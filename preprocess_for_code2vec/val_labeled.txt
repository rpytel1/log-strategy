deleteTopics
 DeleteTopicsResult kafkatest_f7_0(Collection<String> topics)
{    return deleteTopics(topics, new DeleteTopicsOptions());}
f7
0
listTopics
 ListTopicsResult kafkatest_f8_0()
{    return listTopics(new ListTopicsOptions());}
f8
0
describeTopics
 DescribeTopicsResult kafkatest_f9_0(Collection<String> topicNames)
{    return describeTopics(topicNames, new DescribeTopicsOptions());}
f9
0
alterReplicaLogDirs
 AlterReplicaLogDirsResult kafkatest_f17_0(Map<TopicPartitionReplica, String> replicaAssignment)
{    return alterReplicaLogDirs(replicaAssignment, new AlterReplicaLogDirsOptions());}
f17
0
describeLogDirs
 DescribeLogDirsResult kafkatest_f18_0(Collection<Integer> brokers)
{    return describeLogDirs(brokers, new DescribeLogDirsOptions());}
f18
0
describeReplicaLogDirs
 DescribeReplicaLogDirsResult kafkatest_f19_0(Collection<TopicPartitionReplica> replicas)
{    return describeReplicaLogDirs(replicas, new DescribeReplicaLogDirsOptions());}
f19
0
listConsumerGroups
 ListConsumerGroupsResult kafkatest_f27_0()
{    return listConsumerGroups(new ListConsumerGroupsOptions());}
f27
0
listConsumerGroupOffsets
 ListConsumerGroupOffsetsResult kafkatest_f28_0(String groupId)
{    return listConsumerGroupOffsets(groupId, new ListConsumerGroupOffsetsOptions());}
f28
0
deleteConsumerGroups
 DeleteConsumerGroupsResult kafkatest_f29_0(Collection<String> groupIds)
{    return deleteConsumerGroups(groupIds, new DeleteConsumerGroupsOptions());}
f29
0
listPartitionReassignments
 ListPartitionReassignmentsResult kafkatest_f37_0(Set<TopicPartition> partitions, ListPartitionReassignmentsOptions options)
{    return listPartitionReassignments(Optional.of(partitions), options);}
f37
0
listPartitionReassignments
 ListPartitionReassignmentsResult kafkatest_f38_0(ListPartitionReassignmentsOptions options)
{    return listPartitionReassignments(Optional.empty(), options);}
f38
0
create
public static AdminClient kafkatest_f39_0(Properties props)
{    return (AdminClient) Admin.create(props);}
f39
0
configEntry
public ConfigEntry kafkatest_f47_0()
{    return configEntry;}
f47
0
opType
public OpType kafkatest_f48_0()
{    return opType;}
f48
0
equals
public boolean kafkatest_f49_0(final Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    final AlterConfigOp that = (AlterConfigOp) o;    return opType == that.opType && Objects.equals(configEntry, that.configEntry);}
f49
0
values
public Map<TopicPartition, KafkaFuture<Void>> kafkatest_f57_0()
{    return futures;}
f57
0
all
public KafkaFuture<Void> kafkatest_f58_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0]));}
f58
0
values
public Map<TopicPartitionReplica, KafkaFuture<Void>> kafkatest_f59_0()
{    return futures;}
f59
0
value
public String kafkatest_f67_0()
{    return value;}
f67
0
source
public ConfigSource kafkatest_f68_0()
{    return source;}
f68
0
isDefault
public boolean kafkatest_f69_0()
{    return source == ConfigSource.DEFAULT_CONFIG;}
f69
0
value
public String kafkatest_f77_0()
{    return value;}
f77
0
source
public ConfigSource kafkatest_f78_0()
{    return source;}
f78
0
equals
public boolean kafkatest_f79_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ConfigSynonym that = (ConfigSynonym) o;    return Objects.equals(name, that.name) && Objects.equals(value, that.value) && source == that.source;}
f79
0
partitionAssignor
public String kafkatest_f87_0()
{    return partitionAssignor;}
f87
0
state
public ConsumerGroupState kafkatest_f88_0()
{    return state;}
f88
0
coordinator
public Node kafkatest_f89_0()
{    return coordinator;}
f89
0
all
public KafkaFuture<Void> kafkatest_f97_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0]));}
f97
0
renewers
public CreateDelegationTokenOptions kafkatest_f98_0(List<KafkaPrincipal> renewers)
{    this.renewers = renewers;    return this;}
f98
0
renewers
public List<KafkaPrincipal> kafkatest_f99_0()
{    return renewers;}
f99
0
timeoutMs
public CreateTopicsOptions kafkatest_f107_0(Integer timeoutMs)
{    this.timeoutMs = timeoutMs;    return this;}
f107
0
validateOnly
public CreateTopicsOptions kafkatest_f108_0(boolean validateOnly)
{    this.validateOnly = validateOnly;    return this;}
f108
0
shouldValidateOnly
public boolean kafkatest_f109_0()
{    return validateOnly;}
f109
0
all
public KafkaFuture<Collection<AclBinding>> kafkatest_f117_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0])).thenApply(v -> getAclBindings(futures));}
f117
0
getAclBindings
private List<AclBinding> kafkatest_f118_0(Map<AclBindingFilter, KafkaFuture<FilterResults>> futures)
{    List<AclBinding> acls = new ArrayList<>();    for (KafkaFuture<FilterResults> value : futures.values()) {        FilterResults results;        try {            results = value.get();        } catch (Throwable e) {            // have failed if any Future failed.            throw new KafkaException("DeleteAclsResult#all: internal error", e);        }        for (FilterResult result : results.values()) {            if (result.exception() != null)                throw result.exception();            acls.add(result.binding());        }    }    return acls;}
f118
0
partitionResult
public KafkaFuture<Void> kafkatest_f119_0(final TopicPartition partition)
{    final KafkaFutureImpl<Void> result = new KafkaFutureImpl<>();    this.future.whenComplete(new BiConsumer<Map<TopicPartition, Errors>, Throwable>() {        @Override        public void accept(final Map<TopicPartition, Errors> topicPartitions, final Throwable throwable) {            if (throwable != null) {                result.completeExceptionally(throwable);            } else if (!topicPartitions.containsKey(partition)) {                result.completeExceptionally(new IllegalArgumentException("Group offset deletion for partition \"" + partition + "\" was not attempted"));            } else {                final Errors error = topicPartitions.get(partition);                if (error == Errors.NONE) {                    result.complete(null);                } else {                    result.completeExceptionally(error.exception());                }            }        }    });    return result;}
f119
0
all
public KafkaFuture<Void> kafkatest_f127_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0]));}
f127
0
timeoutMs
public DeleteTopicsOptions kafkatest_f128_0(Integer timeoutMs)
{    this.timeoutMs = timeoutMs;    return this;}
f128
0
values
public Map<String, KafkaFuture<Void>> kafkatest_f129_0()
{    return futures;}
f129
0
controller
public KafkaFuture<Node> kafkatest_f137_0()
{    return controller;}
f137
0
clusterId
public KafkaFuture<String> kafkatest_f138_0()
{    return clusterId;}
f138
0
authorizedOperations
public KafkaFuture<Set<AclOperation>> kafkatest_f139_0()
{    return authorizedOperations;}
f139
0
includeAuthorizedOperations
public boolean kafkatest_f147_0()
{    return includeAuthorizedOperations;}
f147
0
describedGroups
public Map<String, KafkaFuture<ConsumerGroupDescription>> kafkatest_f148_0()
{    return futures;}
f148
0
all
public KafkaFuture<Map<String, ConsumerGroupDescription>> kafkatest_f149_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0])).thenApply(new KafkaFuture.BaseFunction<Void, Map<String, ConsumerGroupDescription>>() {        @Override        public Map<String, ConsumerGroupDescription> apply(Void v) {            try {                Map<String, ConsumerGroupDescription> descriptions = new HashMap<>(futures.size());                for (Map.Entry<String, KafkaFuture<ConsumerGroupDescription>> entry : futures.entrySet()) {                    descriptions.put(entry.getKey(), entry.getValue().get());                }                return descriptions;            } catch (InterruptedException | ExecutionException e) {                // that all of the futures completed successfully.                throw new RuntimeException(e);            }        }    });}
f149
0
values
public Map<TopicPartitionReplica, KafkaFuture<ReplicaLogDirInfo>> kafkatest_f157_0()
{    return futures;}
f157
0
all
public KafkaFuture<Map<TopicPartitionReplica, ReplicaLogDirInfo>> kafkatest_f158_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0])).thenApply(new KafkaFuture.BaseFunction<Void, Map<TopicPartitionReplica, ReplicaLogDirInfo>>() {        @Override        public Map<TopicPartitionReplica, ReplicaLogDirInfo> apply(Void v) {            Map<TopicPartitionReplica, ReplicaLogDirInfo> replicaLogDirInfos = new HashMap<>();            for (Map.Entry<TopicPartitionReplica, KafkaFuture<ReplicaLogDirInfo>> entry : futures.entrySet()) {                try {                    replicaLogDirInfos.put(entry.getKey(), entry.getValue().get());                } catch (InterruptedException | ExecutionException e) {                    // This should be unreachable, because allOf ensured that all the futures completed successfully.                    throw new RuntimeException(e);                }            }            return replicaLogDirInfos;        }    });}
f158
0
apply
public Map<TopicPartitionReplica, ReplicaLogDirInfo> kafkatest_f159_0(Void v)
{    Map<TopicPartitionReplica, ReplicaLogDirInfo> replicaLogDirInfos = new HashMap<>();    for (Map.Entry<TopicPartitionReplica, KafkaFuture<ReplicaLogDirInfo>> entry : futures.entrySet()) {        try {            replicaLogDirInfos.put(entry.getKey(), entry.getValue().get());        } catch (InterruptedException | ExecutionException e) {            // This should be unreachable, because allOf ensured that all the futures completed successfully.            throw new RuntimeException(e);        }    }    return replicaLogDirInfos;}
f159
0
includeAuthorizedOperations
public boolean kafkatest_f167_0()
{    return includeAuthorizedOperations;}
f167
0
values
public Map<String, KafkaFuture<TopicDescription>> kafkatest_f168_0()
{    return futures;}
f168
0
all
public KafkaFuture<Map<String, TopicDescription>> kafkatest_f169_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0])).thenApply(new KafkaFuture.BaseFunction<Void, Map<String, TopicDescription>>() {        @Override        public Map<String, TopicDescription> apply(Void v) {            Map<String, TopicDescription> descriptions = new HashMap<>(futures.size());            for (Map.Entry<String, KafkaFuture<TopicDescription>> entry : futures.entrySet()) {                try {                    descriptions.put(entry.getKey(), entry.getValue().get());                } catch (InterruptedException | ExecutionException e) {                    // completed successfully.                    throw new RuntimeException(e);                }            }            return descriptions;        }    });}
f169
0
accept
public void kafkatest_f177_0(Map<TopicPartition, Optional<Throwable>> topicPartitions, Throwable throwable)
{    if (throwable != null) {        result.completeExceptionally(throwable);    } else {        result.complete(topicPartitions.keySet());    }}
f177
0
all
public KafkaFuture<Void> kafkatest_f178_0()
{    return electionResult.all();}
f178
0
expiryTimePeriodMs
public ExpireDelegationTokenOptions kafkatest_f179_0(long expiryTimePeriodMs)
{    this.expiryTimePeriodMs = expiryTimePeriodMs;    return this;}
f179
0
handleCompletedMetadataResponse
public void kafkatest_f187_0(RequestHeader requestHeader, long now, MetadataResponse metadataResponse)
{// Do nothing}
f187
0
requestUpdate
public void kafkatest_f188_0()
{    AdminMetadataManager.this.requestUpdate();}
f188
0
updater
public AdminMetadataUpdater kafkatest_f190_0()
{    return updater;}
f190
0
delayBeforeNextAttemptMs
private long kafkatest_f198_0(long now)
{    long timeSinceAttempt = now - lastMetadataFetchAttemptMs;    return Math.max(0, refreshBackoffMs - timeSinceAttempt);}
f198
0
transitionToUpdatePending
public void kafkatest_f199_0(long now)
{    this.state = State.UPDATE_PENDING;    this.lastMetadataFetchAttemptMs = now;}
f199
0
updateFailed
public voidf200_1Throwable exception)
{    // We depend on pending calls to request another metadata update    this.state = State.QUIESCENT;    if (exception instanceof AuthenticationException) {                this.authException = (AuthenticationException) exception;    } else {            }}
public voidf200
1
createInternal
 static KafkaAdminClient kafkatest_f208_0(AdminClientConfig config, TimeoutProcessorFactory timeoutProcessorFactory)
{    Metrics metrics = null;    NetworkClient networkClient = null;    Time time = Time.SYSTEM;    String clientId = generateClientId(config);    ChannelBuilder channelBuilder = null;    Selector selector = null;    ApiVersions apiVersions = new ApiVersions();    LogContext logContext = createLogContext(clientId);    try {        // Since we only request node information, it's safe to pass true for allowAutoTopicCreation (and it        // simplifies communication with older brokers)        AdminMetadataManager metadataManager = new AdminMetadataManager(logContext, config.getLong(AdminClientConfig.RETRY_BACKOFF_MS_CONFIG), config.getLong(AdminClientConfig.METADATA_MAX_AGE_CONFIG));        List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(config.getList(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG), config.getString(AdminClientConfig.CLIENT_DNS_LOOKUP_CONFIG));        metadataManager.update(Cluster.bootstrap(addresses), time.milliseconds());        List<MetricsReporter> reporters = config.getConfiguredInstances(AdminClientConfig.METRIC_REPORTER_CLASSES_CONFIG, MetricsReporter.class, Collections.singletonMap(AdminClientConfig.CLIENT_ID_CONFIG, clientId));        Map<String, String> metricTags = Collections.singletonMap("client-id", clientId);        MetricConfig metricConfig = new MetricConfig().samples(config.getInt(AdminClientConfig.METRICS_NUM_SAMPLES_CONFIG)).timeWindow(config.getLong(AdminClientConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS).recordLevel(Sensor.RecordingLevel.forName(config.getString(AdminClientConfig.METRICS_RECORDING_LEVEL_CONFIG))).tags(metricTags);        reporters.add(new JmxReporter(JMX_PREFIX));        metrics = new Metrics(metricConfig, reporters, time);        String metricGrpPrefix = "admin-client";        channelBuilder = ClientUtils.createChannelBuilder(config, time);        selector = new Selector(config.getLong(AdminClientConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), metrics, time, metricGrpPrefix, channelBuilder, logContext);        networkClient = new NetworkClient(selector, metadataManager.updater(), clientId, 1, config.getLong(AdminClientConfig.RECONNECT_BACKOFF_MS_CONFIG), config.getLong(AdminClientConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG), config.getInt(AdminClientConfig.SEND_BUFFER_CONFIG), config.getInt(AdminClientConfig.RECEIVE_BUFFER_CONFIG), (int) TimeUnit.HOURS.toMillis(1), ClientDnsLookup.forConfig(config.getString(AdminClientConfig.CLIENT_DNS_LOOKUP_CONFIG)), time, true, apiVersions, logContext);        return new KafkaAdminClient(config, clientId, time, metadataManager, metrics, networkClient, timeoutProcessorFactory, logContext);    } catch (Throwable exc) {        closeQuietly(metrics, "Metrics");        closeQuietly(networkClient, "NetworkClient");        closeQuietly(selector, "Selector");        closeQuietly(channelBuilder, "ChannelBuilder");        throw new KafkaException("Failed to create new KafkaAdminClient", exc);    }}
f208
0
createInternal
 static KafkaAdminClient kafkatest_f209_0(AdminClientConfig config, AdminMetadataManager metadataManager, KafkaClient client, Time time)
{    Metrics metrics = null;    String clientId = generateClientId(config);    try {        metrics = new Metrics(new MetricConfig(), new LinkedList<>(), time);        LogContext logContext = createLogContext(clientId);        return new KafkaAdminClient(config, clientId, time, metadataManager, metrics, client, null, logContext);    } catch (Throwable exc) {        closeQuietly(metrics, "Metrics");        throw new KafkaException("Failed to create new KafkaAdminClient", exc);    }}
f209
0
createLogContext
 static LogContext kafkatest_f210_0(String clientId)
{    return new LogContext("[AdminClient clientId=" + clientId + "] ");}
f210
0
fail
 final voidf218_1long now, Throwable throwable)
{    if (aborted) {        // If the call was aborted while in flight due to a timeout, deliver a        // TimeoutException. In this case, we do not get any more retries - the call has        // failed. We increment tries anyway in order to display an accurate log message.        tries++;        if (log.isDebugEnabled()) {                    }        handleFailure(new TimeoutException("Aborted due to timeout."));        return;    }    // this RPC. That is why 'tries' is not incremented.    if ((throwable instanceof UnsupportedVersionException) && handleUnsupportedVersionException((UnsupportedVersionException) throwable)) {                runnable.enqueue(this, now);        return;    }    tries++;    nextAllowedTryMs = now + retryBackoffMs;    // If the call has timed out, fail.    if (calcTimeoutMsRemainingAsInt(now, deadlineMs) < 0) {        if (log.isDebugEnabled()) {                    }        handleFailure(throwable);        return;    }    // If the exception is not retryable, fail.    if (!(throwable instanceof RetriableException)) {        if (log.isDebugEnabled()) {                    }        handleFailure(throwable);        return;    }    // If we are out of retries, fail.    if (tries > maxRetries) {        if (log.isDebugEnabled()) {                    }        handleFailure(throwable);        return;    }    if (log.isDebugEnabled()) {            }    runnable.enqueue(this, now);}
 final voidf218
1
handleUnsupportedVersionException
 boolean kafkatest_f219_0(UnsupportedVersionException exception)
{    return false;}
f219
0
toString
public String kafkatest_f220_0()
{    return "Call(callName=" + callName + ", deadlineMs=" + deadlineMs + ")";}
f220
0
drainNewCalls
private synchronized void kafkatest_f228_0()
{    if (!newCalls.isEmpty()) {        pendingCalls.addAll(newCalls);        newCalls.clear();    }}
f228
0
maybeDrainPendingCalls
private long kafkatest_f229_0(long now)
{    long pollTimeout = Long.MAX_VALUE;    log.trace("Trying to choose nodes for {} at {}", pendingCalls, now);    Iterator<Call> pendingIter = pendingCalls.iterator();    while (pendingIter.hasNext()) {        Call call = pendingIter.next();        // If the call is being retried, await the proper backoff before finding the node        if (now < call.nextAllowedTryMs) {            pollTimeout = Math.min(pollTimeout, call.nextAllowedTryMs - now);        } else if (maybeDrainPendingCall(call, now)) {            pendingIter.remove();        }    }    return pollTimeout;}
f229
0
maybeDrainPendingCall
private booleanf230_1Call call, long now)
{    try {        Node node = call.nodeProvider.provide();        if (node != null) {            log.trace("Assigned {} to node {}", call, node);            call.curNode = node;            getOrCreateListValue(callsToSend, node).add(call);            return true;        } else {            log.trace("Unable to assign {} to a node.", call);            return false;        }    } catch (Throwable t) {        // Handle authentication errors while choosing nodes.                call.fail(now, t);        return true;    }}
private booleanf230
1
run
public voidf238_1)
{    long now = time.milliseconds();    log.trace("Thread starting");    while (true) {        // Copy newCalls into pendingCalls.        drainNewCalls();        // Check if the AdminClient thread should shut down.        long curHardShutdownTimeMs = hardShutdownTimeMs.get();        if ((curHardShutdownTimeMs != INVALID_SHUTDOWN_TIME) && threadShouldExit(now, curHardShutdownTimeMs))            break;        // Handle timeouts.        TimeoutProcessor timeoutProcessor = timeoutProcessorFactory.create(now);        timeoutPendingCalls(timeoutProcessor);        timeoutCallsToSend(timeoutProcessor);        timeoutCallsInFlight(timeoutProcessor);        long pollTimeout = Math.min(1200000, timeoutProcessor.nextTimeoutMs());        if (curHardShutdownTimeMs != INVALID_SHUTDOWN_TIME) {            pollTimeout = Math.min(pollTimeout, curHardShutdownTimeMs - now);        }        // Choose nodes for our pending calls.        pollTimeout = Math.min(pollTimeout, maybeDrainPendingCalls(now));        long metadataFetchDelayMs = metadataManager.metadataFetchDelayMs(now);        if (metadataFetchDelayMs == 0) {            metadataManager.transitionToUpdatePending(now);            Call metadataCall = makeMetadataCall(now);            if (!maybeDrainPendingCall(metadataCall, now))                pendingCalls.add(metadataCall);        }        pollTimeout = Math.min(pollTimeout, sendEligibleCalls(now));        if (metadataFetchDelayMs > 0) {            pollTimeout = Math.min(pollTimeout, metadataFetchDelayMs);        }        // Ensure that we use a small poll timeout if there are pending calls which need to be sent        if (!pendingCalls.isEmpty())            pollTimeout = Math.min(pollTimeout, retryBackoffMs);        // Wait for network responses.        log.trace("Entering KafkaClient#poll(timeout={})", pollTimeout);        List<ClientResponse> responses = client.poll(pollTimeout, now);        log.trace("KafkaClient#poll retrieved {} response(s)", responses.size());        // unassign calls to disconnected nodes        unassignUnsentCalls(client::connectionFailed);        // Update the current time and handle the latest responses.        now = time.milliseconds();        handleResponses(now, responses);    }    int numTimedOut = 0;    TimeoutProcessor timeoutProcessor = new TimeoutProcessor(Long.MAX_VALUE);    synchronized (this) {        numTimedOut += timeoutProcessor.handleTimeouts(newCalls, "The AdminClient thread has exited.");        newCalls = null;    }    numTimedOut += timeoutProcessor.handleTimeouts(pendingCalls, "The AdminClient thread has exited.");    numTimedOut += timeoutCallsToSend(timeoutProcessor);    numTimedOut += timeoutProcessor.handleTimeouts(correlationIdToCalls.values(), "The AdminClient thread has exited.");    if (numTimedOut > 0) {            }    closeQuietly(client, "KafkaClient");    closeQuietly(metrics, "Metrics");    }
public voidf238
1
enqueue
 voidf239_1Call call, long now)
{    if (log.isDebugEnabled()) {            }    boolean accepted = false;    synchronized (this) {        if (newCalls != null) {            newCalls.add(call);            accepted = true;        }    }    if (accepted) {        // wake the thread if it is in poll()        client.wakeup();    } else {                call.fail(Long.MAX_VALUE, new TimeoutException("The AdminClient thread has exited."));    }}
 voidf239
1
call
 voidf240_1Call call, long now)
{    if (hardShutdownTimeMs.get() != INVALID_SHUTDOWN_TIME) {                call.fail(Long.MAX_VALUE, new TimeoutException("The AdminClient thread is not accepting new calls."));    } else {        enqueue(call, now);    }}
 voidf240
1
createTopics
public CreateTopicsResultf248_1final Collection<NewTopic> newTopics, final CreateTopicsOptions options)
{    final Map<String, KafkaFutureImpl<Void>> topicFutures = new HashMap<>(newTopics.size());    final CreatableTopicCollection topics = new CreatableTopicCollection();    for (NewTopic newTopic : newTopics) {        if (topicNameIsUnrepresentable(newTopic.name())) {            KafkaFutureImpl<Void> future = new KafkaFutureImpl<>();            future.completeExceptionally(new InvalidTopicException("The given topic name '" + newTopic.name() + "' cannot be represented in a request."));            topicFutures.put(newTopic.name(), future);        } else if (!topicFutures.containsKey(newTopic.name())) {            topicFutures.put(newTopic.name(), new KafkaFutureImpl<>());            topics.add(newTopic.convertToCreatableTopic());        }    }    final long now = time.milliseconds();    Call call = new Call("createTopics", calcDeadlineMs(now, options.timeoutMs()), new ControllerNodeProvider()) {        @Override        public AbstractRequest.Builder createRequest(int timeoutMs) {            return new CreateTopicsRequest.Builder(new CreateTopicsRequestData().setTopics(topics).setTimeoutMs(timeoutMs).setValidateOnly(options.shouldValidateOnly()));        }        @Override        public void handleResponse(AbstractResponse abstractResponse) {            CreateTopicsResponse response = (CreateTopicsResponse) abstractResponse;            // Check for controller change            for (Errors error : response.errorCounts().keySet()) {                if (error == Errors.NOT_CONTROLLER) {                    metadataManager.clearController();                    metadataManager.requestUpdate();                    throw error.exception();                }            }            // Handle server responses for particular topics.            for (CreatableTopicResult result : response.data().topics()) {                KafkaFutureImpl<Void> future = topicFutures.get(result.name());                if (future == null) {                                    } else {                    ApiError error = new ApiError(Errors.forCode(result.errorCode()), result.errorMessage());                    ApiException exception = error.exception();                    if (exception != null) {                        future.completeExceptionally(exception);                    } else {                        future.complete(null);                    }                }            }            // The server should send back a response for every topic. But do a sanity check anyway.            for (Map.Entry<String, KafkaFutureImpl<Void>> entry : topicFutures.entrySet()) {                KafkaFutureImpl<Void> future = entry.getValue();                if (!future.isDone()) {                    future.completeExceptionally(new ApiException("The server response did not " + "contain a reference to node " + entry.getKey()));                }            }        }        @Override        void handleFailure(Throwable throwable) {            completeAllExceptionally(topicFutures.values(), throwable);        }    };    if (!topics.isEmpty()) {        runnable.call(call, now);    }    return new CreateTopicsResult(new HashMap<>(topicFutures));}
public CreateTopicsResultf248
1
createRequest
public AbstractRequest.Builder kafkatest_f249_0(int timeoutMs)
{    return new CreateTopicsRequest.Builder(new CreateTopicsRequestData().setTopics(topics).setTimeoutMs(timeoutMs).setValidateOnly(options.shouldValidateOnly()));}
f249
0
handleResponse
public voidf250_1AbstractResponse abstractResponse)
{    CreateTopicsResponse response = (CreateTopicsResponse) abstractResponse;    // Check for controller change    for (Errors error : response.errorCounts().keySet()) {        if (error == Errors.NOT_CONTROLLER) {            metadataManager.clearController();            metadataManager.requestUpdate();            throw error.exception();        }    }    // Handle server responses for particular topics.    for (CreatableTopicResult result : response.data().topics()) {        KafkaFutureImpl<Void> future = topicFutures.get(result.name());        if (future == null) {                    } else {            ApiError error = new ApiError(Errors.forCode(result.errorCode()), result.errorMessage());            ApiException exception = error.exception();            if (exception != null) {                future.completeExceptionally(exception);            } else {                future.complete(null);            }        }    }    // The server should send back a response for every topic. But do a sanity check anyway.    for (Map.Entry<String, KafkaFutureImpl<Void>> entry : topicFutures.entrySet()) {        KafkaFutureImpl<Void> future = entry.getValue();        if (!future.isDone()) {            future.completeExceptionally(new ApiException("The server response did not " + "contain a reference to node " + entry.getKey()));        }    }}
public voidf250
1
handleResponse
 void kafkatest_f258_0(AbstractResponse abstractResponse)
{    MetadataResponse response = (MetadataResponse) abstractResponse;    Map<String, TopicListing> topicListing = new HashMap<>();    for (MetadataResponse.TopicMetadata topicMetadata : response.topicMetadata()) {        String topicName = topicMetadata.topic();        boolean isInternal = topicMetadata.isInternal();        if (!topicMetadata.isInternal() || options.shouldListInternal())            topicListing.put(topicName, new TopicListing(topicName, isInternal));    }    topicListingFuture.complete(topicListing);}
f258
0
handleFailure
 void kafkatest_f259_0(Throwable throwable)
{    topicListingFuture.completeExceptionally(throwable);}
f259
0
describeTopics
public DescribeTopicsResult kafkatest_f260_0(final Collection<String> topicNames, DescribeTopicsOptions options)
{    final Map<String, KafkaFutureImpl<TopicDescription>> topicFutures = new HashMap<>(topicNames.size());    final ArrayList<String> topicNamesList = new ArrayList<>();    for (String topicName : topicNames) {        if (topicNameIsUnrepresentable(topicName)) {            KafkaFutureImpl<TopicDescription> future = new KafkaFutureImpl<>();            future.completeExceptionally(new InvalidTopicException("The given topic name '" + topicName + "' cannot be represented in a request."));            topicFutures.put(topicName, future);        } else if (!topicFutures.containsKey(topicName)) {            topicFutures.put(topicName, new KafkaFutureImpl<>());            topicNamesList.add(topicName);        }    }    final long now = time.milliseconds();    Call call = new Call("describeTopics", calcDeadlineMs(now, options.timeoutMs()), new ControllerNodeProvider()) {        private boolean supportsDisablingTopicCreation = true;        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            if (supportsDisablingTopicCreation)                return new MetadataRequest.Builder(new MetadataRequestData().setTopics(convertToMetadataRequestTopic(topicNamesList)).setAllowAutoTopicCreation(false).setIncludeTopicAuthorizedOperations(options.includeAuthorizedOperations()));            else                return MetadataRequest.Builder.allTopics();        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            MetadataResponse response = (MetadataResponse) abstractResponse;            // Handle server responses for particular topics.            Cluster cluster = response.cluster();            Map<String, Errors> errors = response.errors();            for (Map.Entry<String, KafkaFutureImpl<TopicDescription>> entry : topicFutures.entrySet()) {                String topicName = entry.getKey();                KafkaFutureImpl<TopicDescription> future = entry.getValue();                Errors topicError = errors.get(topicName);                if (topicError != null) {                    future.completeExceptionally(topicError.exception());                    continue;                }                if (!cluster.topics().contains(topicName)) {                    future.completeExceptionally(new UnknownTopicOrPartitionException("Topic " + topicName + " not found."));                    continue;                }                boolean isInternal = cluster.internalTopics().contains(topicName);                List<PartitionInfo> partitionInfos = cluster.partitionsForTopic(topicName);                List<TopicPartitionInfo> partitions = new ArrayList<>(partitionInfos.size());                for (PartitionInfo partitionInfo : partitionInfos) {                    TopicPartitionInfo topicPartitionInfo = new TopicPartitionInfo(partitionInfo.partition(), leader(partitionInfo), Arrays.asList(partitionInfo.replicas()), Arrays.asList(partitionInfo.inSyncReplicas()));                    partitions.add(topicPartitionInfo);                }                partitions.sort(Comparator.comparingInt(TopicPartitionInfo::partition));                TopicDescription topicDescription = new TopicDescription(topicName, isInternal, partitions, validAclOperations(response.topicAuthorizedOperations(topicName).get()));                future.complete(topicDescription);            }        }        private Node leader(PartitionInfo partitionInfo) {            if (partitionInfo.leader() == null || partitionInfo.leader().id() == Node.noNode().id())                return null;            return partitionInfo.leader();        }        @Override        boolean handleUnsupportedVersionException(UnsupportedVersionException exception) {            if (supportsDisablingTopicCreation) {                supportsDisablingTopicCreation = false;                return true;            }            return false;        }        @Override        void handleFailure(Throwable throwable) {            completeAllExceptionally(topicFutures.values(), throwable);        }    };    if (!topicNamesList.isEmpty()) {        runnable.call(call, now);    }    return new DescribeTopicsResult(new HashMap<>(topicFutures));}
f260
0
handleResponse
 void kafkatest_f268_0(AbstractResponse abstractResponse)
{    MetadataResponse response = (MetadataResponse) abstractResponse;    describeClusterFuture.complete(response.brokers());    controllerFuture.complete(controller(response));    clusterIdFuture.complete(response.clusterId());    authorizedOperationsFuture.complete(validAclOperations(response.clusterAuthorizedOperations()));}
f268
0
controller
private Node kafkatest_f269_0(MetadataResponse response)
{    if (response.controller() == null || response.controller().id() == MetadataResponse.NO_CONTROLLER_ID)        return null;    return response.controller();}
f269
0
handleFailure
 void kafkatest_f270_0(Throwable throwable)
{    describeClusterFuture.completeExceptionally(throwable);    controllerFuture.completeExceptionally(throwable);    clusterIdFuture.completeExceptionally(throwable);    authorizedOperationsFuture.completeExceptionally(throwable);}
f270
0
handleFailure
 void kafkatest_f278_0(Throwable throwable)
{    completeAllExceptionally(futures.values(), throwable);}
f278
0
deleteAcls
public DeleteAclsResult kafkatest_f279_0(Collection<AclBindingFilter> filters, DeleteAclsOptions options)
{    final long now = time.milliseconds();    final Map<AclBindingFilter, KafkaFutureImpl<FilterResults>> futures = new HashMap<>();    final List<AclBindingFilter> filterList = new ArrayList<>();    for (AclBindingFilter filter : filters) {        if (futures.get(filter) == null) {            filterList.add(filter);            futures.put(filter, new KafkaFutureImpl<>());        }    }    runnable.call(new Call("deleteAcls", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new DeleteAclsRequest.Builder(filterList);        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            DeleteAclsResponse response = (DeleteAclsResponse) abstractResponse;            List<AclFilterResponse> responses = response.responses();            Iterator<AclFilterResponse> iter = responses.iterator();            for (AclBindingFilter filter : filterList) {                KafkaFutureImpl<FilterResults> future = futures.get(filter);                if (!iter.hasNext()) {                    future.completeExceptionally(new UnknownServerException("The broker reported no deletion result for the given filter."));                } else {                    AclFilterResponse deletion = iter.next();                    if (deletion.error().isFailure()) {                        future.completeExceptionally(deletion.error().exception());                    } else {                        List<FilterResult> filterResults = new ArrayList<>();                        for (AclDeletionResult deletionResult : deletion.deletions()) {                            filterResults.add(new FilterResult(deletionResult.acl(), deletionResult.error().exception()));                        }                        future.complete(new FilterResults(filterResults));                    }                }            }        }        @Override        void handleFailure(Throwable throwable) {            completeAllExceptionally(futures.values(), throwable);        }    }, now);    return new DeleteAclsResult(new HashMap<>(futures));}
f279
0
createRequest
 AbstractRequest.Builder kafkatest_f280_0(int timeoutMs)
{    return new DeleteAclsRequest.Builder(filterList);}
f280
0
handleResponse
 void kafkatest_f288_0(AbstractResponse abstractResponse)
{    DescribeConfigsResponse response = (DescribeConfigsResponse) abstractResponse;    DescribeConfigsResponse.Config config = response.configs().get(resource);    if (config == null) {        brokerFuture.completeExceptionally(new UnknownServerException("Malformed broker response: missing config for " + resource));        return;    }    if (config.error().isFailure())        brokerFuture.completeExceptionally(config.error().exception());    else {        List<ConfigEntry> configEntries = new ArrayList<>();        for (DescribeConfigsResponse.ConfigEntry configEntry : config.entries()) {            configEntries.add(new ConfigEntry(configEntry.name(), configEntry.value(), configSource(configEntry.source()), configEntry.isSensitive(), configEntry.isReadOnly(), configSynonyms(configEntry)));        }        brokerFuture.complete(new Config(configEntries));    }}
f288
0
handleFailure
 void kafkatest_f289_0(Throwable throwable)
{    brokerFuture.completeExceptionally(throwable);}
f289
0
configSynonyms
private List<ConfigEntry.ConfigSynonym> kafkatest_f290_0(DescribeConfigsResponse.ConfigEntry configEntry)
{    List<ConfigEntry.ConfigSynonym> synonyms = new ArrayList<>(configEntry.synonyms().size());    for (DescribeConfigsResponse.ConfigSynonym synonym : configEntry.synonyms()) {        synonyms.add(new ConfigEntry.ConfigSynonym(synonym.name(), synonym.value(), configSource(synonym.source())));    }    return synonyms;}
f290
0
incrementalAlterConfigs
private Map<ConfigResource, KafkaFutureImpl<Void>> kafkatest_f298_0(Map<ConfigResource, Collection<AlterConfigOp>> configs, final AlterConfigsOptions options, Collection<ConfigResource> resources, NodeProvider nodeProvider)
{    final Map<ConfigResource, KafkaFutureImpl<Void>> futures = new HashMap<>();    for (ConfigResource resource : resources) futures.put(resource, new KafkaFutureImpl<>());    final long now = time.milliseconds();    runnable.call(new Call("incrementalAlterConfigs", calcDeadlineMs(now, options.timeoutMs()), nodeProvider) {        @Override        public AbstractRequest.Builder createRequest(int timeoutMs) {            return new IncrementalAlterConfigsRequest.Builder(toIncrementalAlterConfigsRequestData(resources, configs, options.shouldValidateOnly()));        }        @Override        public void handleResponse(AbstractResponse abstractResponse) {            IncrementalAlterConfigsResponse response = (IncrementalAlterConfigsResponse) abstractResponse;            Map<ConfigResource, ApiError> errors = IncrementalAlterConfigsResponse.fromResponseData(response.data());            for (Map.Entry<ConfigResource, KafkaFutureImpl<Void>> entry : futures.entrySet()) {                KafkaFutureImpl<Void> future = entry.getValue();                ApiException exception = errors.get(entry.getKey()).exception();                if (exception != null) {                    future.completeExceptionally(exception);                } else {                    future.complete(null);                }            }        }        @Override        void handleFailure(Throwable throwable) {            completeAllExceptionally(futures.values(), throwable);        }    }, now);    return futures;}
f298
0
createRequest
public AbstractRequest.Builder kafkatest_f299_0(int timeoutMs)
{    return new IncrementalAlterConfigsRequest.Builder(toIncrementalAlterConfigsRequestData(resources, configs, options.shouldValidateOnly()));}
f299
0
handleResponse
public void kafkatest_f300_0(AbstractResponse abstractResponse)
{    IncrementalAlterConfigsResponse response = (IncrementalAlterConfigsResponse) abstractResponse;    Map<ConfigResource, ApiError> errors = IncrementalAlterConfigsResponse.fromResponseData(response.data());    for (Map.Entry<ConfigResource, KafkaFutureImpl<Void>> entry : futures.entrySet()) {        KafkaFutureImpl<Void> future = entry.getValue();        ApiException exception = errors.get(entry.getKey()).exception();        if (exception != null) {            future.completeExceptionally(exception);        } else {            future.complete(null);        }    }}
f300
0
createRequest
public AbstractRequest.Builder kafkatest_f308_0(int timeoutMs)
{    // Query selected partitions in all log directories    return new DescribeLogDirsRequest.Builder(null);}
f308
0
handleResponse
public void kafkatest_f309_0(AbstractResponse abstractResponse)
{    DescribeLogDirsResponse response = (DescribeLogDirsResponse) abstractResponse;    KafkaFutureImpl<Map<String, DescribeLogDirsResponse.LogDirInfo>> future = futures.get(brokerId);    if (response.logDirInfos().size() > 0) {        future.complete(response.logDirInfos());    } else {        // response.logDirInfos() will be empty if and only if the user is not authorized to describe clsuter resource.        future.completeExceptionally(Errors.CLUSTER_AUTHORIZATION_FAILED.exception());    }}
f309
0
handleFailure
 void kafkatest_f310_0(Throwable throwable)
{    completeAllExceptionally(futures.values(), throwable);}
f310
0
handleFailure
 void kafkatest_f318_0(Throwable throwable)
{    completeAllExceptionally(futures.values(), throwable);}
f318
0
deleteRecords
public DeleteRecordsResult kafkatest_f319_0(final Map<TopicPartition, RecordsToDelete> recordsToDelete, final DeleteRecordsOptions options)
{    // requests need to be sent to partitions leader nodes so ...    // ... from the provided map it's needed to create more maps grouping topic/partition per leader    final Map<TopicPartition, KafkaFutureImpl<DeletedRecords>> futures = new HashMap<>(recordsToDelete.size());    for (TopicPartition topicPartition : recordsToDelete.keySet()) {        futures.put(topicPartition, new KafkaFutureImpl<>());    }    // preparing topics list for asking metadata about them    final Set<String> topics = new HashSet<>();    for (TopicPartition topicPartition : recordsToDelete.keySet()) {        topics.add(topicPartition.topic());    }    final long nowMetadata = time.milliseconds();    final long deadline = calcDeadlineMs(nowMetadata, options.timeoutMs());    // asking for topics metadata for getting partitions leaders    runnable.call(new Call("topicsMetadata", deadline, new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new MetadataRequest.Builder(new MetadataRequestData().setTopics(convertToMetadataRequestTopic(topics)).setAllowAutoTopicCreation(false));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            MetadataResponse response = (MetadataResponse) abstractResponse;            Map<String, Errors> errors = response.errors();            Cluster cluster = response.cluster();            // completing futures for topics with errors            for (Map.Entry<String, Errors> topicError : errors.entrySet()) {                for (Map.Entry<TopicPartition, KafkaFutureImpl<DeletedRecords>> future : futures.entrySet()) {                    if (future.getKey().topic().equals(topicError.getKey())) {                        future.getValue().completeExceptionally(topicError.getValue().exception());                    }                }            }            // grouping topic partitions per leader            Map<Node, Map<TopicPartition, Long>> leaders = new HashMap<>();            for (Map.Entry<TopicPartition, RecordsToDelete> entry : recordsToDelete.entrySet()) {                // avoiding to send deletion request for topics with errors                if (!errors.containsKey(entry.getKey().topic())) {                    Node node = cluster.leaderFor(entry.getKey());                    if (node != null) {                        if (!leaders.containsKey(node))                            leaders.put(node, new HashMap<>());                        leaders.get(node).put(entry.getKey(), entry.getValue().beforeOffset());                    } else {                        KafkaFutureImpl<DeletedRecords> future = futures.get(entry.getKey());                        future.completeExceptionally(Errors.LEADER_NOT_AVAILABLE.exception());                    }                }            }            for (final Map.Entry<Node, Map<TopicPartition, Long>> entry : leaders.entrySet()) {                final long nowDelete = time.milliseconds();                final int brokerId = entry.getKey().id();                runnable.call(new Call("deleteRecords", deadline, new ConstantNodeIdProvider(brokerId)) {                    @Override                    AbstractRequest.Builder createRequest(int timeoutMs) {                        return new DeleteRecordsRequest.Builder(timeoutMs, entry.getValue());                    }                    @Override                    void handleResponse(AbstractResponse abstractResponse) {                        DeleteRecordsResponse response = (DeleteRecordsResponse) abstractResponse;                        for (Map.Entry<TopicPartition, DeleteRecordsResponse.PartitionResponse> result : response.responses().entrySet()) {                            KafkaFutureImpl<DeletedRecords> future = futures.get(result.getKey());                            if (result.getValue().error == Errors.NONE) {                                future.complete(new DeletedRecords(result.getValue().lowWatermark));                            } else {                                future.completeExceptionally(result.getValue().error.exception());                            }                        }                    }                    @Override                    void handleFailure(Throwable throwable) {                        completeAllExceptionally(futures.values(), throwable);                    }                }, nowDelete);            }        }        @Override        void handleFailure(Throwable throwable) {            completeAllExceptionally(futures.values(), throwable);        }    }, nowMetadata);    return new DeleteRecordsResult(new HashMap<>(futures));}
f319
0
createRequest
 AbstractRequest.Builder kafkatest_f320_0(int timeoutMs)
{    return new MetadataRequest.Builder(new MetadataRequestData().setTopics(convertToMetadataRequestTopic(topics)).setAllowAutoTopicCreation(false));}
f320
0
handleResponse
 void kafkatest_f328_0(AbstractResponse abstractResponse)
{    CreateDelegationTokenResponse response = (CreateDelegationTokenResponse) abstractResponse;    if (response.hasError()) {        delegationTokenFuture.completeExceptionally(response.error().exception());    } else {        CreateDelegationTokenResponseData data = response.data();        TokenInformation tokenInfo = new TokenInformation(data.tokenId(), new KafkaPrincipal(data.principalType(), data.principalName()), options.renewers(), data.issueTimestampMs(), data.maxTimestampMs(), data.expiryTimestampMs());        DelegationToken token = new DelegationToken(tokenInfo, data.hmac());        delegationTokenFuture.complete(token);    }}
f328
0
handleFailure
 void kafkatest_f329_0(Throwable throwable)
{    delegationTokenFuture.completeExceptionally(throwable);}
f329
0
renewDelegationToken
public RenewDelegationTokenResult kafkatest_f330_0(final byte[] hmac, final RenewDelegationTokenOptions options)
{    final KafkaFutureImpl<Long> expiryTimeFuture = new KafkaFutureImpl<>();    final long now = time.milliseconds();    runnable.call(new Call("renewDelegationToken", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder<RenewDelegationTokenRequest> createRequest(int timeoutMs) {            return new RenewDelegationTokenRequest.Builder(new RenewDelegationTokenRequestData().setHmac(hmac).setRenewPeriodMs(options.renewTimePeriodMs()));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            RenewDelegationTokenResponse response = (RenewDelegationTokenResponse) abstractResponse;            if (response.hasError()) {                expiryTimeFuture.completeExceptionally(response.error().exception());            } else {                expiryTimeFuture.complete(response.expiryTimestamp());            }        }        @Override        void handleFailure(Throwable throwable) {            expiryTimeFuture.completeExceptionally(throwable);        }    }, now);    return new RenewDelegationTokenResult(expiryTimeFuture);}
f330
0
describeDelegationToken
public DescribeDelegationTokenResult kafkatest_f338_0(final DescribeDelegationTokenOptions options)
{    final KafkaFutureImpl<List<DelegationToken>> tokensFuture = new KafkaFutureImpl<>();    final long now = time.milliseconds();    runnable.call(new Call("describeDelegationToken", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new DescribeDelegationTokenRequest.Builder(options.owners());        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            DescribeDelegationTokenResponse response = (DescribeDelegationTokenResponse) abstractResponse;            if (response.hasError()) {                tokensFuture.completeExceptionally(response.error().exception());            } else {                tokensFuture.complete(response.tokens());            }        }        @Override        void handleFailure(Throwable throwable) {            tokensFuture.completeExceptionally(throwable);        }    }, now);    return new DescribeDelegationTokenResult(tokensFuture);}
f338
0
createRequest
 AbstractRequest.Builder kafkatest_f339_0(int timeoutMs)
{    return new DescribeDelegationTokenRequest.Builder(options.owners());}
f339
0
handleResponse
 void kafkatest_f340_0(AbstractResponse abstractResponse)
{    DescribeDelegationTokenResponse response = (DescribeDelegationTokenResponse) abstractResponse;    if (response.hasError()) {        tokensFuture.completeExceptionally(response.error().exception());    } else {        tokensFuture.complete(response.tokens());    }}
f340
0
hasCoordinatorMoved
public boolean kafkatest_f348_0(AbstractResponse response)
{    return response.errorCounts().keySet().stream().anyMatch(error -> error == Errors.NOT_COORDINATOR);}
f348
0
rescheduleTask
private voidf349_1ConsumerGroupOperationContext<?, ?> context, Supplier<Call> nextCall)
{        // Requeue the task so that we can try with new coordinator    context.setNode(null);    Call findCoordinatorCall = getFindCoordinatorCall(context, nextCall);    runnable.call(findCoordinatorCall, time.milliseconds());}
private voidf349
1
createFutures
private static Map<String, KafkaFutureImpl<T>> kafkatest_f350_0(Collection<String> groupIds)
{    return new HashSet<>(groupIds).stream().collect(Collectors.toMap(groupId -> groupId, groupId -> {        if (groupIdIsUnrepresentable(groupId)) {            KafkaFutureImpl<T> future = new KafkaFutureImpl<>();            future.completeExceptionally(new InvalidGroupIdException("The given group id '" + groupId + "' cannot be represented in a request."));            return future;        } else {            return new KafkaFutureImpl<>();        }    }));}
f350
0
handleResponse
 void kafkatest_f358_0(AbstractResponse abstractResponse)
{    final DescribeGroupsResponse response = (DescribeGroupsResponse) abstractResponse;    List<DescribedGroup> describedGroups = response.data().groups();    if (describedGroups.isEmpty()) {        context.getFuture().completeExceptionally(new InvalidGroupIdException("No consumer group found for GroupId: " + context.getGroupId()));        return;    }    if (describedGroups.size() > 1 || !describedGroups.get(0).groupId().equals(context.getGroupId())) {        String ids = Arrays.toString(describedGroups.stream().map(DescribedGroup::groupId).toArray());        context.getFuture().completeExceptionally(new InvalidGroupIdException("DescribeConsumerGroup request for GroupId: " + context.getGroupId() + " returned " + ids));        return;    }    final DescribedGroup describedGroup = describedGroups.get(0);    // If coordinator changed since we fetched it, retry    if (context.hasCoordinatorMoved(response)) {        rescheduleTask(context, () -> getDescribeConsumerGroupsCall(context));        return;    }    final Errors groupError = Errors.forCode(describedGroup.errorCode());    if (handleGroupRequestError(groupError, context.getFuture()))        return;    final String protocolType = describedGroup.protocolType();    if (protocolType.equals(ConsumerProtocol.PROTOCOL_TYPE) || protocolType.isEmpty()) {        final List<DescribedGroupMember> members = describedGroup.members();        final List<MemberDescription> memberDescriptions = new ArrayList<>(members.size());        final Set<AclOperation> authorizedOperations = validAclOperations(describedGroup.authorizedOperations());        for (DescribedGroupMember groupMember : members) {            Set<TopicPartition> partitions = Collections.emptySet();            if (groupMember.memberAssignment().length > 0) {                final Assignment assignment = ConsumerProtocol.deserializeAssignment(ByteBuffer.wrap(groupMember.memberAssignment()));                partitions = new HashSet<>(assignment.partitions());            }            final MemberDescription memberDescription = new MemberDescription(groupMember.memberId(), Optional.ofNullable(groupMember.groupInstanceId()), groupMember.clientId(), groupMember.clientHost(), new MemberAssignment(partitions));            memberDescriptions.add(memberDescription);        }        final ConsumerGroupDescription consumerGroupDescription = new ConsumerGroupDescription(context.getGroupId(), protocolType.isEmpty(), memberDescriptions, describedGroup.protocolData(), ConsumerGroupState.parse(describedGroup.groupState()), context.getNode().get(), authorizedOperations);        context.getFuture().complete(consumerGroupDescription);    }}
f358
0
handleFailure
 void kafkatest_f359_0(Throwable throwable)
{    context.getFuture().completeExceptionally(throwable);}
f359
0
validAclOperations
private Set<AclOperation> kafkatest_f360_0(final int authorizedOperations)
{    if (authorizedOperations == MetadataResponse.AUTHORIZED_OPERATIONS_OMITTED) {        return null;    }    return Utils.from32BitField(authorizedOperations).stream().map(AclOperation::fromCode).filter(operation -> operation != AclOperation.UNKNOWN && operation != AclOperation.ALL && operation != AclOperation.ANY).collect(Collectors.toSet());}
f360
0
createRequest
 AbstractRequest.Builder kafkatest_f368_0(int timeoutMs)
{    return new MetadataRequest.Builder(new MetadataRequestData().setTopics(Collections.emptyList()).setAllowAutoTopicCreation(true));}
f368
0
handleResponse
 void kafkatest_f369_0(AbstractResponse abstractResponse)
{    MetadataResponse metadataResponse = (MetadataResponse) abstractResponse;    Collection<Node> nodes = metadataResponse.brokers();    if (nodes.isEmpty())        throw new StaleMetadataException("Metadata fetch failed due to missing broker list");    HashSet<Node> allNodes = new HashSet<>(nodes);    final ListConsumerGroupsResults results = new ListConsumerGroupsResults(allNodes, all);    for (final Node node : allNodes) {        final long nowList = time.milliseconds();        runnable.call(new Call("listConsumerGroups", deadline, new ConstantNodeIdProvider(node.id())) {            @Override            AbstractRequest.Builder createRequest(int timeoutMs) {                return new ListGroupsRequest.Builder(new ListGroupsRequestData());            }            private void maybeAddConsumerGroup(ListGroupsResponseData.ListedGroup group) {                String protocolType = group.protocolType();                if (protocolType.equals(ConsumerProtocol.PROTOCOL_TYPE) || protocolType.isEmpty()) {                    final String groupId = group.groupId();                    final ConsumerGroupListing groupListing = new ConsumerGroupListing(groupId, protocolType.isEmpty());                    results.addListing(groupListing);                }            }            @Override            void handleResponse(AbstractResponse abstractResponse) {                final ListGroupsResponse response = (ListGroupsResponse) abstractResponse;                synchronized (results) {                    Errors error = Errors.forCode(response.data().errorCode());                    if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.COORDINATOR_NOT_AVAILABLE) {                        throw error.exception();                    } else if (error != Errors.NONE) {                        results.addError(error.exception(), node);                    } else {                        for (ListGroupsResponseData.ListedGroup group : response.data().groups()) {                            maybeAddConsumerGroup(group);                        }                    }                    results.tryComplete(node);                }            }            @Override            void handleFailure(Throwable throwable) {                synchronized (results) {                    results.addError(throwable, node);                    results.tryComplete(node);                }            }        }, nowList);    }}
f369
0
createRequest
 AbstractRequest.Builder kafkatest_f370_0(int timeoutMs)
{    return new ListGroupsRequest.Builder(new ListGroupsRequestData());}
f370
0
handleResponse
 voidf378_1AbstractResponse abstractResponse)
{    final OffsetFetchResponse response = (OffsetFetchResponse) abstractResponse;    final Map<TopicPartition, OffsetAndMetadata> groupOffsetsListing = new HashMap<>();    // If coordinator changed since we fetched it, retry    if (context.hasCoordinatorMoved(response)) {        rescheduleTask(context, () -> getListConsumerGroupOffsetsCall(context));        return;    }    if (handleGroupRequestError(response.error(), context.getFuture()))        return;    for (Map.Entry<TopicPartition, OffsetFetchResponse.PartitionData> entry : response.responseData().entrySet()) {        final TopicPartition topicPartition = entry.getKey();        OffsetFetchResponse.PartitionData partitionData = entry.getValue();        final Errors error = partitionData.error;        if (error == Errors.NONE) {            final Long offset = partitionData.offset;            final String metadata = partitionData.metadata;            final Optional<Integer> leaderEpoch = partitionData.leaderEpoch;            groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, leaderEpoch, metadata));        } else {                    }    }    context.getFuture().complete(groupOffsetsListing);}
 voidf378
1
handleFailure
 void kafkatest_f379_0(Throwable throwable)
{    context.getFuture().completeExceptionally(throwable);}
f379
0
deleteConsumerGroups
public DeleteConsumerGroupsResult kafkatest_f380_0(Collection<String> groupIds, DeleteConsumerGroupsOptions options)
{    final Map<String, KafkaFutureImpl<Void>> futures = createFutures(groupIds);    // all consumer groups this coordinator host    for (final String groupId : groupIds) {        // skip sending request for those futures that already failed.        final KafkaFutureImpl<Void> future = futures.get(groupId);        if (future.isCompletedExceptionally())            continue;        final long startFindCoordinatorMs = time.milliseconds();        final long deadline = calcDeadlineMs(startFindCoordinatorMs, options.timeoutMs());        ConsumerGroupOperationContext<Void, DeleteConsumerGroupsOptions> context = new ConsumerGroupOperationContext<>(groupId, options, deadline, future);        Call findCoordinatorCall = getFindCoordinatorCall(context, () -> KafkaAdminClient.this.getDeleteConsumerGroupsCall(context));        runnable.call(findCoordinatorCall, startFindCoordinatorMs);    }    return new DeleteConsumerGroupsResult(new HashMap<>(futures));}
f380
0
handleResponse
 void kafkatest_f388_0(AbstractResponse abstractResponse)
{    final OffsetDeleteResponse response = (OffsetDeleteResponse) abstractResponse;    // If coordinator changed since we fetched it, retry    if (context.hasCoordinatorMoved(response)) {        rescheduleTask(context, () -> getDeleteConsumerGroupOffsetsCall(context, partitions));        return;    }    // If the error is an error at the group level, the future is failed with it    final Errors groupError = Errors.forCode(response.data.errorCode());    if (handleGroupRequestError(groupError, context.getFuture()))        return;    final Map<TopicPartition, Errors> partitions = new HashMap<>();    response.data.topics().forEach(topic -> {        topic.partitions().forEach(partition -> {            partitions.put(new TopicPartition(topic.name(), partition.partitionIndex()), Errors.forCode(partition.errorCode()));        });    });    context.getFuture().complete(partitions);}
f388
0
handleFailure
 void kafkatest_f389_0(Throwable throwable)
{    context.getFuture().completeExceptionally(throwable);}
f389
0
metrics
public Map<MetricName, ? extends Metric> kafkatest_f390_0()
{    return Collections.unmodifiableMap(this.metrics.metrics());}
f390
0
assertResponseCountMatch
private void kafkatest_f398_0(Map<TopicPartition, ApiException> errors, int receivedResponsesCount)
{    int expectedResponsesCount = topicsToReassignments.values().stream().mapToInt(Map::size).sum();    if (errors.values().stream().noneMatch(Objects::nonNull) && receivedResponsesCount != expectedResponsesCount) {        String quantifier = receivedResponsesCount > expectedResponsesCount ? "many" : "less";        throw new UnknownServerException("The server returned too " + quantifier + " results." + "Expected " + expectedResponsesCount + " but received " + receivedResponsesCount);    }}
f398
0
validateTopicResponses
private int kafkatest_f399_0(List<ReassignableTopicResponse> topicResponses, Map<TopicPartition, ApiException> errors)
{    int receivedResponsesCount = 0;    for (ReassignableTopicResponse topicResponse : topicResponses) {        String topicName = topicResponse.name();        for (ReassignablePartitionResponse partResponse : topicResponse.partitions()) {            Errors partitionError = Errors.forCode(partResponse.errorCode());            TopicPartition tp = new TopicPartition(topicName, partResponse.partitionIndex());            if (partitionError == Errors.NONE) {                errors.put(tp, null);            } else {                errors.put(tp, new ApiError(partitionError, partResponse.errorMessage()).exception());            }            receivedResponsesCount += 1;        }    }    return receivedResponsesCount;}
f399
0
handleFailure
 void kafkatest_f400_0(Throwable throwable)
{    for (KafkaFutureImpl<Void> future : futures.values()) {        future.completeExceptionally(throwable);    }}
f400
0
getRemoveMembersFromGroupCall
private Call kafkatest_f408_0(ConsumerGroupOperationContext<RemoveMemberFromGroupResult, RemoveMemberFromConsumerGroupOptions> context)
{    return new Call("leaveGroup", context.getDeadline(), new ConstantNodeIdProvider(context.getNode().get().id())) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new LeaveGroupRequest.Builder(context.getGroupId(), context.getOptions().getMembers());        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            final LeaveGroupResponse response = (LeaveGroupResponse) abstractResponse;            // If coordinator changed since we fetched it, retry            if (context.hasCoordinatorMoved(response)) {                rescheduleTask(context, () -> getRemoveMembersFromGroupCall(context));                return;            }            // If error is transient coordinator error, retry            Errors error = response.error();            if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.COORDINATOR_NOT_AVAILABLE) {                throw error.exception();            }            final RemoveMemberFromGroupResult membershipChangeResult = new RemoveMemberFromGroupResult(response, context.getOptions().getMembers());            context.getFuture().complete(membershipChangeResult);        }        @Override        void handleFailure(Throwable throwable) {            context.getFuture().completeExceptionally(throwable);        }    };}
f408
0
createRequest
 AbstractRequest.Builder kafkatest_f409_0(int timeoutMs)
{    return new LeaveGroupRequest.Builder(context.getGroupId(), context.getOptions().getMembers());}
f409
0
handleResponse
 void kafkatest_f410_0(AbstractResponse abstractResponse)
{    final LeaveGroupResponse response = (LeaveGroupResponse) abstractResponse;    // If coordinator changed since we fetched it, retry    if (context.hasCoordinatorMoved(response)) {        rescheduleTask(context, () -> getRemoveMembersFromGroupCall(context));        return;    }    // If error is transient coordinator error, retry    Errors error = response.error();    if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.COORDINATOR_NOT_AVAILABLE) {        throw error.exception();    }    final RemoveMemberFromGroupResult membershipChangeResult = new RemoveMemberFromGroupResult(response, context.getOptions().getMembers());    context.getFuture().complete(membershipChangeResult);}
f410
0
errors
public KafkaFuture<Collection<Throwable>> kafkatest_f418_0()
{    return errors;}
f418
0
reassignments
public KafkaFuture<Map<TopicPartition, PartitionReassignment>> kafkatest_f419_0()
{    return future;}
f419
0
timeoutMs
public ListTopicsOptions kafkatest_f420_0(Integer timeoutMs)
{    this.timeoutMs = timeoutMs;    return this;}
f420
0
equals
public boolean kafkatest_f428_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    MemberAssignment that = (MemberAssignment) o;    return Objects.equals(topicPartitions, that.topicPartitions);}
f428
0
hashCode
public int kafkatest_f429_0()
{    return topicPartitions != null ? topicPartitions.hashCode() : 0;}
f429
0
topicPartitions
public Set<TopicPartition> kafkatest_f430_0()
{    return topicPartitions;}
f430
0
assignment
public MemberAssignment kafkatest_f438_0()
{    return assignment;}
f438
0
toString
public String kafkatest_f439_0()
{    return "(memberId=" + memberId + ", groupInstanceId=" + groupInstanceId.orElse("null") + ", clientId=" + clientId + ", host=" + host + ", assignment=" + assignment + ")";}
f439
0
all
public RemoveMemberFromGroupResult kafkatest_f440_0() throws ExecutionException, InterruptedException
{    return future.get();}
f440
0
toString
public String kafkatest_f448_0()
{    return "(totalCount=" + totalCount() + ", newAssignments=" + assignments() + ")";}
f448
0
name
public String kafkatest_f449_0()
{    return name;}
f449
0
numPartitions
public int kafkatest_f450_0()
{    return numPartitions.orElse(NO_PARTITIONS);}
f450
0
addingReplicas
public List<Integer> kafkatest_f458_0()
{    return addingReplicas;}
f458
0
removingReplicas
public List<Integer> kafkatest_f459_0()
{    return removingReplicas;}
f459
0
beforeOffset
public static RecordsToDelete kafkatest_f460_0(long offset)
{    return new RecordsToDelete(offset);}
f460
0
memberFutures
public Map<MemberIdentity, KafkaFuture<Void>> kafkatest_f468_0()
{    return memberFutures;}
f468
0
renewTimePeriodMs
public RenewDelegationTokenOptions kafkatest_f469_0(long renewTimePeriodMs)
{    this.renewTimePeriodMs = renewTimePeriodMs;    return this;}
f469
0
renewTimePeriodMs
public long kafkatest_f470_0()
{    return renewTimePeriodMs;}
f470
0
toString
public String kafkatest_f478_0()
{    return "(name=" + name + ", internal=" + internal + ", partitions=" + Utils.join(partitions, ",") + ", authorizedOperations=" + authorizedOperations + ")";}
f478
0
name
public String kafkatest_f479_0()
{    return name;}
f479
0
isInternal
public boolean kafkatest_f480_0()
{    return internal;}
f480
0
forConfig
public static ClientDnsLookup kafkatest_f488_0(String config)
{    return ClientDnsLookup.valueOf(config.toUpperCase(Locale.ROOT));}
f488
0
toString
public String kafkatest_f489_0()
{    return "ClientRequest(expectResponse=" + expectResponse + ", callback=" + callback + ", destination=" + destination + ", correlationId=" + correlationId + ", clientId=" + clientId + ", createdTimeMs=" + createdTimeMs + ", requestBuilder=" + requestBuilder + ")";}
f489
0
expectResponse
public boolean kafkatest_f490_0()
{    return expectResponse;}
f490
0
requestTimeoutMs
public int kafkatest_f498_0()
{    return requestTimeoutMs;}
f498
0
receivedTimeMs
public long kafkatest_f499_0()
{    return receivedTimeMs;}
f499
0
wasDisconnected
public boolean kafkatest_f500_0()
{    return disconnected;}
f500
0
onComplete
public void kafkatest_f508_0()
{    if (callback != null)        callback.onComplete(this);}
f508
0
toString
public String kafkatest_f509_0()
{    return "ClientResponse(receivedTimeMs=" + receivedTimeMs + ", latencyMs=" + latencyMs + ", disconnected=" + disconnected + ", requestHeader=" + requestHeader + ", responseBody=" + responseBody + ")";}
f509
0
parseAndValidateAddresses
public static List<InetSocketAddress> kafkatest_f510_0(List<String> urls, String clientDnsLookupConfig)
{    return parseAndValidateAddresses(urls, ClientDnsLookup.forConfig(clientDnsLookupConfig));}
f510
0
isConnecting
public boolean kafkatest_f518_0(String id)
{    NodeConnectionState state = nodeState.get(id);    return state != null && state.state == ConnectionState.CONNECTING;}
f518
0
isPreparingConnection
public boolean kafkatest_f519_0(String id)
{    NodeConnectionState state = nodeState.get(id);    return state != null && (state.state == ConnectionState.CONNECTING || state.state == ConnectionState.CHECKING_API_VERSIONS);}
f519
0
connecting
public voidf520_1String id, long now, String host, ClientDnsLookup clientDnsLookup)
{    NodeConnectionState connectionState = nodeState.get(id);    if (connectionState != null && connectionState.host().equals(host)) {        connectionState.lastConnectAttemptMs = now;        connectionState.state = ConnectionState.CONNECTING;        // Move to next resolved address, or if addresses are exhausted, mark node to be re-resolved        connectionState.moveToNextAddress();        return;    } else if (connectionState != null) {            }    // Create a new NodeConnectionState if nodeState does not already contain one    // for the specified id or if the hostname associated with the node id changed.    nodeState.put(id, new NodeConnectionState(ConnectionState.CONNECTING, now, this.reconnectBackoffInitMs, host, clientDnsLookup));}
public voidf520
1
authenticationFailed
public void kafkatest_f528_0(String id, long now, AuthenticationException exception)
{    NodeConnectionState nodeState = nodeState(id);    nodeState.authenticationException = exception;    nodeState.state = ConnectionState.AUTHENTICATION_FAILED;    nodeState.lastConnectAttemptMs = now;    updateReconnectBackoff(nodeState);}
f528
0
isReady
public boolean kafkatest_f529_0(String id, long now)
{    return isReady(nodeState.get(id), now);}
f529
0
isReady
private boolean kafkatest_f530_0(NodeConnectionState state, long now)
{    return state != null && state.state == ConnectionState.READY && state.throttleUntilTimeMs <= now;}
f530
0
connectionState
public ConnectionState kafkatest_f538_0(String id)
{    return nodeState(id).state;}
f538
0
nodeState
private NodeConnectionState kafkatest_f539_0(String id)
{    NodeConnectionState state = this.nodeState.get(id);    if (state == null)        throw new IllegalStateException("No entry found for connection " + id);    return state;}
f539
0
host
public String kafkatest_f540_0()
{    return host;}
f540
0
addDeserializerToConfig
public static Map<String, Object> kafkatest_f548_0(Map<String, Object> configs, Deserializer<?> keyDeserializer, Deserializer<?> valueDeserializer)
{    Map<String, Object> newConfigs = new HashMap<>(configs);    if (keyDeserializer != null)        newConfigs.put(KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializer.getClass());    if (valueDeserializer != null)        newConfigs.put(VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializer.getClass());    return newConfigs;}
f548
0
addDeserializerToConfig
public static Properties kafkatest_f549_0(Properties properties, Deserializer<?> keyDeserializer, Deserializer<?> valueDeserializer)
{    Properties newProperties = new Properties();    newProperties.putAll(properties);    if (keyDeserializer != null)        newProperties.put(KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializer.getClass().getName());    if (valueDeserializer != null)        newProperties.put(VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializer.getClass().getName());    return newProperties;}
f549
0
configNames
public static Set<String> kafkatest_f550_0()
{    return CONFIG.names();}
f550
0
supportedProtocols
 List<RebalanceProtocol> kafkatest_f559_0()
{    return Collections.singletonList(RebalanceProtocol.EAGER);}
f559
0
version
 short kafkatest_f560_0()
{    return (short) 0;}
f560
0
topics
public List<String> kafkatest_f561_0()
{    return topics;}
f561
0
groupAssignment
public Map<String, Assignment> kafkatest_f569_0()
{    return assignments;}
f569
0
id
public byte kafkatest_f570_0()
{    return id;}
f570
0
forId
public static RebalanceProtocol kafkatest_f571_0(byte id)
{    switch(id) {        case 0:            return EAGER;        case 1:            return COOPERATIVE;        default:            throw new IllegalArgumentException("Unknown rebalance protocol id: " + id);    }}
f571
0
timestamp
public long kafkatest_f579_0()
{    return timestamp;}
f579
0
timestampType
public TimestampType kafkatest_f580_0()
{    return timestampType;}
f580
0
checksum
public long kafkatest_f581_0()
{    if (checksum == null)        this.checksum = DefaultRecord.computePartialChecksum(timestamp, serializedKeySize, serializedValueSize);    return this.checksum;}
f581
0
iterator
public Iterator<ConsumerRecord<K, V>> kafkatest_f589_0()
{    return new ConcatenatedIterable<>(records.values()).iterator();}
f589
0
count
public int kafkatest_f590_0()
{    int count = 0;    for (List<ConsumerRecord<K, V>> recs : this.records.values()) count += recs.size();    return count;}
f590
0
iterator
public Iterator<ConsumerRecord<K, V>> kafkatest_f591_0()
{    return new AbstractIterator<ConsumerRecord<K, V>>() {        Iterator<? extends Iterable<ConsumerRecord<K, V>>> iters = iterables.iterator();        Iterator<ConsumerRecord<K, V>> current;        public ConsumerRecord<K, V> makeNext() {            while (current == null || !current.hasNext()) {                if (iters.hasNext())                    current = iters.next().iterator();                else                    return allDone();            }            return current.next();        }    };}
f591
0
adjustAssignment
private void kafkatest_f599_0(final Map<String, Subscription> subscriptions, final Map<String, List<TopicPartition>> assignments)
{    Map<TopicPartition, String> allAddedPartitions = new HashMap<>();    Set<TopicPartition> allRevokedPartitions = new HashSet<>();    for (final Map.Entry<String, List<TopicPartition>> entry : assignments.entrySet()) {        String consumer = entry.getKey();        List<TopicPartition> ownedPartitions = subscriptions.get(consumer).ownedPartitions();        List<TopicPartition> assignedPartitions = entry.getValue();        List<TopicPartition> addedPartitions = new ArrayList<>(assignedPartitions);        addedPartitions.removeAll(ownedPartitions);        for (TopicPartition tp : addedPartitions) {            allAddedPartitions.put(tp, consumer);        }        final Set<TopicPartition> revokedPartitions = new HashSet<>(ownedPartitions);        revokedPartitions.removeAll(assignedPartitions);        allRevokedPartitions.addAll(revokedPartitions);    }    // remove any partitions to be revoked from the current assignment    for (TopicPartition tp : allRevokedPartitions) {        // if partition is being migrated to another consumer, don't assign it there yet        if (allAddedPartitions.containsKey(tp)) {            String assignedConsumer = allAddedPartitions.get(tp);            assignments.get(assignedConsumer).remove(tp);        }    }}
f599
0
ensureCoordinatorReady
protected synchronized booleanf601_1final Timer timer)
{    if (!coordinatorUnknown())        return true;    do {        final RequestFuture<Void> future = lookupCoordinator();        client.poll(future, timer);        if (!future.isDone()) {            // ran out of time            break;        }        if (future.failed()) {            if (future.isRetriable()) {                                client.awaitMetadataUpdate(timer);            } else                throw future.exception();        } else if (coordinator != null && client.isUnavailable(coordinator)) {            // we found the coordinator, but the connection has failed, so mark            // it dead and backoff before retrying discovery            markCoordinatorUnknown();            timer.sleep(rebalanceConfig.retryBackoffMs);        }    } while (coordinatorUnknown() && timer.notExpired());    return !coordinatorUnknown();}
protected synchronized booleanf601
1
lookupCoordinator
protected synchronized RequestFuture<Void>f602_1)
{    if (findCoordinatorFuture == null) {        // find a node to ask about the coordinator        Node node = this.client.leastLoadedNode();        if (node == null) {                        return RequestFuture.noBrokersAvailable();        } else            findCoordinatorFuture = sendFindCoordinatorRequest(node);    }    return findCoordinatorFuture;}
protected synchronized RequestFuture<Void>f602
1
disableHeartbeatThread
private synchronized void kafkatest_f610_0()
{    if (heartbeatThread != null)        heartbeatThread.disable();}
f610
0
closeHeartbeatThread
private voidf611_1)
{    HeartbeatThread thread = null;    synchronized (this) {        if (heartbeatThread == null)            return;        heartbeatThread.close();        thread = heartbeatThread;        heartbeatThread = null;    }    try {        thread.join();    } catch (InterruptedException e) {                throw new InterruptException(e);    }}
private voidf611
1
joinGroupIfNeeded
 boolean kafkatest_f612_0(final Timer timer)
{    while (rejoinNeededOrPending()) {        if (!ensureCoordinatorReady(timer)) {            return false;        }        // still in progress.        if (needsJoinPrepare) {            // need to set the flag before calling onJoinPrepare since the user callback may throw            // exception, in which case upon retry we should not retry onJoinPrepare either.            needsJoinPrepare = false;            onJoinPrepare(generation.generationId, generation.memberId);        }        final RequestFuture<ByteBuffer> future = initiateJoinGroup();        client.poll(future, timer);        if (!future.isDone()) {            // we ran out of time            return false;        }        if (future.succeeded()) {            // Duplicate the buffer in case `onJoinComplete` does not complete and needs to be retried.            ByteBuffer memberAssignment = future.value().duplicate();            onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment);            // We reset the join group future only after the completion callback returns. This ensures            // that if the callback is woken up, we will retry it on the next joinGroupIfNeeded.            resetJoinGroupFuture();            needsJoinPrepare = true;        } else {            resetJoinGroupFuture();            final RuntimeException exception = future.exception();            if (exception instanceof UnknownMemberIdException || exception instanceof RebalanceInProgressException || exception instanceof IllegalGenerationException || exception instanceof MemberIdRequiredException)                continue;            else if (!future.isRetriable())                throw exception;            timer.sleep(rebalanceConfig.retryBackoffMs);        }    }    return true;}
f612
0
onJoinLeader
private RequestFuture<ByteBuffer>f620_1JoinGroupResponse joinResponse)
{    try {        // perform the leader synchronization and send back the assignment for the group        Map<String, ByteBuffer> groupAssignment = performAssignment(joinResponse.data().leader(), joinResponse.data().protocolName(), joinResponse.data().members());        List<SyncGroupRequestData.SyncGroupRequestAssignment> groupAssignmentList = new ArrayList<>();        for (Map.Entry<String, ByteBuffer> assignment : groupAssignment.entrySet()) {            groupAssignmentList.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(assignment.getKey()).setAssignment(Utils.toArray(assignment.getValue())));        }        SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder(new SyncGroupRequestData().setGroupId(rebalanceConfig.groupId).setMemberId(generation.memberId).setGroupInstanceId(this.rebalanceConfig.groupInstanceId.orElse(null)).setGenerationId(generation.generationId).setAssignments(groupAssignmentList));                return sendSyncGroupRequest(requestBuilder);    } catch (RuntimeException e) {        return RequestFuture.failure(e);    }}
private RequestFuture<ByteBuffer>f620
1
sendSyncGroupRequest
private RequestFuture<ByteBuffer> kafkatest_f621_0(SyncGroupRequest.Builder requestBuilder)
{    if (coordinatorUnknown())        return RequestFuture.coordinatorNotAvailable();    return client.send(coordinator, requestBuilder).compose(new SyncGroupResponseHandler());}
f621
0
handle
public voidf622_1SyncGroupResponse syncResponse, RequestFuture<ByteBuffer> future)
{    Errors error = syncResponse.error();    if (error == Errors.NONE) {        sensors.syncLatency.record(response.requestLatencyMs());        future.complete(ByteBuffer.wrap(syncResponse.data.assignment()));    } else {        requestRejoin();        if (error == Errors.GROUP_AUTHORIZATION_FAILED) {            future.raise(GroupAuthorizationException.forGroupId(rebalanceConfig.groupId));        } else if (error == Errors.REBALANCE_IN_PROGRESS) {                        future.raise(error);        } else if (error == Errors.FENCED_INSTANCE_ID) {                        future.raise(error);        } else if (error == Errors.UNKNOWN_MEMBER_ID || error == Errors.ILLEGAL_GENERATION) {                        resetGenerationOnResponseError(ApiKeys.SYNC_GROUP, error);            future.raise(error);        } else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) {                        markCoordinatorUnknown();            future.raise(error);        } else {            future.raise(new KafkaException("Unexpected error from SyncGroup: " + error.message()));        }    }}
public voidf622
1
markCoordinatorUnknown
protected synchronized voidf630_1boolean isDisconnected)
{    if (this.coordinator != null) {                Node oldCoordinator = this.coordinator;        // Mark the coordinator dead before disconnecting requests since the callbacks for any pending        // requests may attempt to do likewise. This also prevents new requests from being sent to the        // coordinator while the disconnect is in progress.        this.coordinator = null;        // Pending callbacks will be invoked with a DisconnectException on the next call to poll.        if (!isDisconnected)            client.disconnectAsync(oldCoordinator);    }}
protected synchronized voidf630
1
generation
protected synchronized Generation kafkatest_f631_0()
{    if (this.state != MemberState.STABLE)        return null;    return generation;}
f631
0
memberId
protected synchronized String kafkatest_f632_0()
{    return generation == null ? JoinGroupRequest.UNKNOWN_MEMBER_ID : generation.memberId;}
f632
0
close
protected voidf640_1Timer timer)
{    try {        closeHeartbeatThread();    } finally {        // needs this lock to complete and terminate after close flag is set.        synchronized (this) {            if (rebalanceConfig.leaveGroupOnClose) {                onLeavePrepare();                maybeLeaveGroup("the consumer is being closed");            }            // At this point, there may be pending commits (async commits or sync commits that were            // interrupted using wakeup) and the leave group request which have been queued, but not            // yet sent to the broker. Wait up to close timeout for these pending requests to be processed.            // If coordinator is not known, requests are aborted.            Node coordinator = checkAndGetCoordinator();            if (coordinator != null && !client.awaitPendingRequests(coordinator, timer))                        }    }}
protected voidf640
1
maybeLeaveGroup
public synchronized RequestFuture<Void>f641_1String leaveReason)
{    RequestFuture<Void> future = null;    // and the membership expiration is only controlled by session timeout.    if (isDynamicMember() && !coordinatorUnknown() && state != MemberState.UNJOINED && generation.hasMemberId()) {        // this is a minimal effort attempt to leave the group. we do not        // attempt any resending if the request fails or times out.                LeaveGroupRequest.Builder request = new LeaveGroupRequest.Builder(rebalanceConfig.groupId, Collections.singletonList(new MemberIdentity().setMemberId(generation.memberId)));        future = client.send(coordinator, request).compose(new LeaveGroupResponseHandler());        client.pollNoWakeup();    }    resetGenerationOnLeaveGroup();    return future;}
public synchronized RequestFuture<Void>f641
1
isDynamicMember
protected boolean kafkatest_f642_0()
{    return !rebalanceConfig.groupInstanceId.isPresent();}
f642
0
enable
public voidf650_1)
{    synchronized (AbstractCoordinator.this) {                this.enabled = true;        heartbeat.resetTimeouts();        AbstractCoordinator.this.notify();    }}
public voidf650
1
disable
public voidf651_1)
{    synchronized (AbstractCoordinator.this) {                this.enabled = false;    }}
public voidf651
1
close
public void kafkatest_f652_0()
{    synchronized (AbstractCoordinator.this) {        this.closed = true;        AbstractCoordinator.this.notify();    }}
f652
0
hashCode
public int kafkatest_f660_0()
{    return Objects.hash(generationId, memberId, protocol);}
f660
0
toString
public String kafkatest_f661_0()
{    return "Generation{" + "generationId=" + generationId + ", memberId='" + memberId + '\'' + ", protocol='" + protocol + '\'' + '}';}
f661
0
heartbeat
public Heartbeat kafkatest_f662_0()
{    return heartbeat;}
f662
0
assign
public Map<String, List<TopicPartition>> kafkatest_f670_0(Map<String, Integer> partitionsPerTopic, Map<String, Subscription> subscriptions)
{    Map<String, List<TopicPartition>> currentAssignment = new HashMap<>();    Map<TopicPartition, ConsumerGenerationPair> prevAssignment = new HashMap<>();    partitionMovements = new PartitionMovements();    prepopulateCurrentAssignments(subscriptions, currentAssignment, prevAssignment);    boolean isFreshAssignment = currentAssignment.isEmpty();    // a mapping of all topic partitions to all consumers that can be assigned to them    final Map<TopicPartition, List<String>> partition2AllPotentialConsumers = new HashMap<>();    // a mapping of all consumers to all potential topic partitions that can be assigned to them    final Map<String, List<TopicPartition>> consumer2AllPotentialPartitions = new HashMap<>();    // initialize partition2AllPotentialConsumers and consumer2AllPotentialPartitions in the following two for loops    for (Entry<String, Integer> entry : partitionsPerTopic.entrySet()) {        for (int i = 0; i < entry.getValue(); ++i) partition2AllPotentialConsumers.put(new TopicPartition(entry.getKey(), i), new ArrayList<>());    }    for (Entry<String, Subscription> entry : subscriptions.entrySet()) {        String consumerId = entry.getKey();        consumer2AllPotentialPartitions.put(consumerId, new ArrayList<>());        entry.getValue().topics().stream().filter(topic -> partitionsPerTopic.get(topic) != null).forEach(topic -> {            for (int i = 0; i < partitionsPerTopic.get(topic); ++i) {                TopicPartition topicPartition = new TopicPartition(topic, i);                consumer2AllPotentialPartitions.get(consumerId).add(topicPartition);                partition2AllPotentialConsumers.get(topicPartition).add(consumerId);            }        });        // add this consumer to currentAssignment (with an empty topic partition assignment) if it does not already exist        if (!currentAssignment.containsKey(consumerId))            currentAssignment.put(consumerId, new ArrayList<>());    }    // a mapping of partition to current consumer    Map<TopicPartition, String> currentPartitionConsumer = new HashMap<>();    for (Map.Entry<String, List<TopicPartition>> entry : currentAssignment.entrySet()) for (TopicPartition topicPartition : entry.getValue()) currentPartitionConsumer.put(topicPartition, entry.getKey());    List<TopicPartition> sortedPartitions = sortPartitions(currentAssignment, prevAssignment.keySet(), isFreshAssignment, partition2AllPotentialConsumers, consumer2AllPotentialPartitions);    // all partitions that need to be assigned (initially set to all partitions but adjusted in the following loop)    List<TopicPartition> unassignedPartitions = new ArrayList<>(sortedPartitions);    boolean revocationRequired = false;    for (Iterator<Entry<String, List<TopicPartition>>> it = currentAssignment.entrySet().iterator(); it.hasNext(); ) {        Map.Entry<String, List<TopicPartition>> entry = it.next();        if (!subscriptions.containsKey(entry.getKey())) {            // if a consumer that existed before (and had some partition assignments) is now removed, remove it from currentAssignment            for (TopicPartition topicPartition : entry.getValue()) currentPartitionConsumer.remove(topicPartition);            it.remove();        } else {            // otherwise (the consumer still exists)            for (Iterator<TopicPartition> partitionIter = entry.getValue().iterator(); partitionIter.hasNext(); ) {                TopicPartition partition = partitionIter.next();                if (!partition2AllPotentialConsumers.containsKey(partition)) {                    // if this topic partition of this consumer no longer exists remove it from currentAssignment of the consumer                    partitionIter.remove();                    currentPartitionConsumer.remove(partition);                } else if (!subscriptions.get(entry.getKey()).topics().contains(partition.topic())) {                    // if this partition cannot remain assigned to its current consumer because the consumer                    // is no longer subscribed to its topic remove it from currentAssignment of the consumer                    partitionIter.remove();                    revocationRequired = true;                } else                    // otherwise, remove the topic partition from those that need to be assigned only if                    // its current consumer is still subscribed to its topic (because it is already assigned                    // and we would want to preserve that assignment as much as possible)                    unassignedPartitions.remove(partition);            }        }    }    // at this point we have preserved all valid topic partition to consumer assignments and removed    // all invalid topic partitions and invalid consumers. Now we need to assign unassignedPartitions    // to consumers so that the topic partition assignments are as balanced as possible.    // an ascending sorted set of consumers based on how many topic partitions are already assigned to them    TreeSet<String> sortedCurrentSubscriptions = new TreeSet<>(new SubscriptionComparator(currentAssignment));    sortedCurrentSubscriptions.addAll(currentAssignment.keySet());    balance(currentAssignment, prevAssignment, sortedPartitions, unassignedPartitions, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer, revocationRequired);    return currentAssignment;}
f670
0
prepopulateCurrentAssignments
private voidf671_1Map<String, Subscription> subscriptions, Map<String, List<TopicPartition>> currentAssignment, Map<TopicPartition, ConsumerGenerationPair> prevAssignment)
{    // we need to process subscriptions' user data with each consumer's reported generation in mind    // higher generations overwrite lower generations in case of a conflict    // note that a conflict could exists only if user data is for different generations    // for each partition we create a sorted map of its consumers by generation    Map<TopicPartition, TreeMap<Integer, String>> sortedPartitionConsumersByGeneration = new HashMap<>();    for (Map.Entry<String, Subscription> subscriptionEntry : subscriptions.entrySet()) {        String consumer = subscriptionEntry.getKey();        MemberData memberData = memberData(subscriptionEntry.getValue());        for (TopicPartition partition : memberData.partitions) {            if (sortedPartitionConsumersByGeneration.containsKey(partition)) {                Map<Integer, String> consumers = sortedPartitionConsumersByGeneration.get(partition);                if (memberData.generation.isPresent() && consumers.containsKey(memberData.generation.get())) {                    // same partition is assigned to two consumers during the same rebalance.                    // log a warning and skip this record                                    } else                    consumers.put(memberData.generation.orElse(DEFAULT_GENERATION), consumer);            } else {                TreeMap<Integer, String> sortedConsumers = new TreeMap<>();                sortedConsumers.put(memberData.generation.orElse(DEFAULT_GENERATION), consumer);                sortedPartitionConsumersByGeneration.put(partition, sortedConsumers);            }        }    }    // current and previous consumers are the last two consumers of each partition in the above sorted map    for (Map.Entry<TopicPartition, TreeMap<Integer, String>> partitionConsumersEntry : sortedPartitionConsumersByGeneration.entrySet()) {        TopicPartition partition = partitionConsumersEntry.getKey();        TreeMap<Integer, String> consumers = partitionConsumersEntry.getValue();        Iterator<Integer> it = consumers.descendingKeySet().iterator();        // let's process the current (most recent) consumer first        String consumer = consumers.get(it.next());        currentAssignment.computeIfAbsent(consumer, k -> new ArrayList<>());        currentAssignment.get(consumer).add(partition);        // now update previous assignment if any        if (it.hasNext()) {            int generation = it.next();            prevAssignment.put(partition, new ConsumerGenerationPair(consumers.get(generation), generation));        }    }}
private voidf671
1
isBalanced
private booleanf672_1Map<String, List<TopicPartition>> currentAssignment, TreeSet<String> sortedCurrentSubscriptions, Map<String, List<TopicPartition>> allSubscriptions)
{    int min = currentAssignment.get(sortedCurrentSubscriptions.first()).size();    int max = currentAssignment.get(sortedCurrentSubscriptions.last()).size();    if (min >= max - 1)        // if minimum and maximum numbers of partitions assigned to consumers differ by at most one return true        return true;    // create a mapping from partitions to the consumer assigned to them    final Map<TopicPartition, String> allPartitions = new HashMap<>();    Set<Entry<String, List<TopicPartition>>> assignments = currentAssignment.entrySet();    for (Map.Entry<String, List<TopicPartition>> entry : assignments) {        List<TopicPartition> topicPartitions = entry.getValue();        for (TopicPartition topicPartition : topicPartitions) {            if (allPartitions.containsKey(topicPartition))                            allPartitions.put(topicPartition, entry.getKey());        }    }    // could but did not get cannot be moved to it (because that would break the balance)    for (String consumer : sortedCurrentSubscriptions) {        List<TopicPartition> consumerPartitions = currentAssignment.get(consumer);        int consumerPartitionCount = consumerPartitions.size();        // skip if this consumer already has all the topic partitions it can get        if (consumerPartitionCount == allSubscriptions.get(consumer).size())            continue;        // otherwise make sure it cannot get any more        List<TopicPartition> potentialTopicPartitions = allSubscriptions.get(consumer);        for (TopicPartition topicPartition : potentialTopicPartitions) {            if (!currentAssignment.get(consumer).contains(topicPartition)) {                String otherConsumer = allPartitions.get(topicPartition);                int otherConsumerPartitionCount = currentAssignment.get(otherConsumer).size();                if (consumerPartitionCount < otherConsumerPartitionCount) {                                        return false;                }            }        }    }    return true;}
private booleanf672
1
performReassignments
private booleanf680_1List<TopicPartition> reassignablePartitions, Map<String, List<TopicPartition>> currentAssignment, Map<TopicPartition, ConsumerGenerationPair> prevAssignment, TreeSet<String> sortedCurrentSubscriptions, Map<String, List<TopicPartition>> consumer2AllPotentialPartitions, Map<TopicPartition, List<String>> partition2AllPotentialConsumers, Map<TopicPartition, String> currentPartitionConsumer)
{    boolean reassignmentPerformed = false;    boolean modified;    // repeat reassignment until no partition can be moved to improve the balance    do {        modified = false;        // reassign all reassignable partitions (starting from the partition with least potential consumers and if needed)        // until the full list is processed or a balance is achieved        Iterator<TopicPartition> partitionIterator = reassignablePartitions.iterator();        while (partitionIterator.hasNext() && !isBalanced(currentAssignment, sortedCurrentSubscriptions, consumer2AllPotentialPartitions)) {            TopicPartition partition = partitionIterator.next();            // the partition must have at least two consumers            if (partition2AllPotentialConsumers.get(partition).size() <= 1)                            // the partition must have a current consumer            String consumer = currentPartitionConsumer.get(partition);            if (consumer == null)                            if (prevAssignment.containsKey(partition) && currentAssignment.get(consumer).size() > currentAssignment.get(prevAssignment.get(partition).consumer).size() + 1) {                reassignPartition(partition, currentAssignment, sortedCurrentSubscriptions, currentPartitionConsumer, prevAssignment.get(partition).consumer);                reassignmentPerformed = true;                modified = true;                continue;            }            // check if a better-suited consumer exist for the partition; if so, reassign it            for (String otherConsumer : partition2AllPotentialConsumers.get(partition)) {                if (currentAssignment.get(consumer).size() > currentAssignment.get(otherConsumer).size() + 1) {                    reassignPartition(partition, currentAssignment, sortedCurrentSubscriptions, currentPartitionConsumer, consumer2AllPotentialPartitions);                    reassignmentPerformed = true;                    modified = true;                    break;                }            }        }    } while (modified);    return reassignmentPerformed;}
private booleanf680
1
reassignPartition
private void kafkatest_f681_0(TopicPartition partition, Map<String, List<TopicPartition>> currentAssignment, TreeSet<String> sortedCurrentSubscriptions, Map<TopicPartition, String> currentPartitionConsumer, Map<String, List<TopicPartition>> consumer2AllPotentialPartitions)
{    // find the new consumer    String newConsumer = null;    for (String anotherConsumer : sortedCurrentSubscriptions) {        if (consumer2AllPotentialPartitions.get(anotherConsumer).contains(partition)) {            newConsumer = anotherConsumer;            break;        }    }    assert newConsumer != null;    reassignPartition(partition, currentAssignment, sortedCurrentSubscriptions, currentPartitionConsumer, newConsumer);}
f681
0
reassignPartition
private void kafkatest_f682_0(TopicPartition partition, Map<String, List<TopicPartition>> currentAssignment, TreeSet<String> sortedCurrentSubscriptions, Map<TopicPartition, String> currentPartitionConsumer, String newConsumer)
{    String consumer = currentPartitionConsumer.get(partition);    // find the correct partition movement considering the stickiness requirement    TopicPartition partitionToBeMoved = partitionMovements.getTheActualPartitionToBeMoved(partition, consumer, newConsumer);    processPartitionMovement(partitionToBeMoved, newConsumer, currentAssignment, sortedCurrentSubscriptions, currentPartitionConsumer);}
f682
0
removeMovementRecordOfPartition
private ConsumerPair kafkatest_f690_0(TopicPartition partition)
{    ConsumerPair pair = partitionMovements.remove(partition);    String topic = partition.topic();    Map<ConsumerPair, Set<TopicPartition>> partitionMovementsForThisTopic = partitionMovementsByTopic.get(topic);    partitionMovementsForThisTopic.get(pair).remove(partition);    if (partitionMovementsForThisTopic.get(pair).isEmpty())        partitionMovementsForThisTopic.remove(pair);    if (partitionMovementsByTopic.get(topic).isEmpty())        partitionMovementsByTopic.remove(topic);    return pair;}
f690
0
addPartitionMovementRecord
private void kafkatest_f691_0(TopicPartition partition, ConsumerPair pair)
{    partitionMovements.put(partition, pair);    String topic = partition.topic();    if (!partitionMovementsByTopic.containsKey(topic))        partitionMovementsByTopic.put(topic, new HashMap<>());    Map<ConsumerPair, Set<TopicPartition>> partitionMovementsForThisTopic = partitionMovementsByTopic.get(topic);    if (!partitionMovementsForThisTopic.containsKey(pair))        partitionMovementsForThisTopic.put(pair, new HashSet<>());    partitionMovementsForThisTopic.get(pair).add(partition);}
f691
0
movePartition
private void kafkatest_f692_0(TopicPartition partition, String oldConsumer, String newConsumer)
{    ConsumerPair pair = new ConsumerPair(oldConsumer, newConsumer);    if (partitionMovements.containsKey(partition)) {        // this partition has previously moved        ConsumerPair existingPair = removeMovementRecordOfPartition(partition);        assert existingPair.dstMemberId.equals(oldConsumer);        if (!existingPair.srcMemberId.equals(newConsumer)) {            // the partition is not moving back to its previous consumer            // return new ConsumerPair2(existingPair.src, newConsumer);            addPartitionMovementRecord(partition, new ConsumerPair(existingPair.srcMemberId, newConsumer));        }    } else        addPartitionMovementRecord(partition, pair);}
f692
0
equals
public boolean kafkatest_f700_0(Object obj)
{    if (obj == null)        return false;    if (!getClass().isInstance(obj))        return false;    ConsumerPair otherPair = (ConsumerPair) obj;    return this.srcMemberId.equals(otherPair.srcMemberId) && this.dstMemberId.equals(otherPair.dstMemberId);}
f700
0
in
private boolean kafkatest_f701_0(Set<ConsumerPair> pairs)
{    for (ConsumerPair pair : pairs) if (this.equals(pair))        return true;    return false;}
f701
0
sendAsyncRequest
public RequestFuture<T2>f702_1Node node, T1 requestData)
{    AbstractRequest.Builder<Req> requestBuilder = prepareRequest(node, requestData);    return client.send(node, requestBuilder).compose(new RequestFutureAdapter<ClientResponse, T2>() {        @Override        @SuppressWarnings("unchecked")        public void onSuccess(ClientResponse value, RequestFuture<T2> future) {            Resp resp;            try {                resp = (Resp) value.responseBody();            } catch (ClassCastException cce) {                                future.raise(cce);                return;            }            log.trace("Received {} {} from broker {}", resp.getClass().getSimpleName(), resp, node);            try {                future.complete(handleResponse(node, requestData, resp));            } catch (RuntimeException e) {                if (!future.isDone()) {                    future.raise(e);                }            }        }        @Override        public void onFailure(RuntimeException e, RequestFuture<T2> future1) {            future1.raise(e);        }    });}
public RequestFuture<T2>f702
1
lookupAssignor
private ConsumerPartitionAssignor kafkatest_f710_0(String name)
{    for (ConsumerPartitionAssignor assignor : this.assignors) {        if (assignor.name().equals(name))            return assignor;    }    return null;}
f710
0
maybeUpdateJoinedSubscription
private void kafkatest_f711_0(Set<TopicPartition> assignedPartitions)
{    if (subscriptions.hasPatternSubscription()) {        // Check if the assignment contains some topics that were not in the original        // subscription, if yes we will obey what leader has decided and add these topics        // into the subscriptions as long as they still match the subscribed pattern        Set<String> addedTopics = new HashSet<>();        // this is a copy because its handed to listener below        for (TopicPartition tp : assignedPartitions) {            if (!joinedSubscription.contains(tp.topic()))                addedTopics.add(tp.topic());        }        if (!addedTopics.isEmpty()) {            Set<String> newSubscription = new HashSet<>(subscriptions.subscription());            Set<String> newJoinedSubscription = new HashSet<>(joinedSubscription);            newSubscription.addAll(addedTopics);            newJoinedSubscription.addAll(addedTopics);            if (this.subscriptions.subscribeFromPattern(newSubscription))                metadata.requestUpdateForNewTopics();            this.joinedSubscription = newJoinedSubscription;        }    }}
f711
0
invokePartitionsAssigned
private Exceptionf712_1final Set<TopicPartition> assignedPartitions)
{        ConsumerRebalanceListener listener = subscriptions.rebalanceListener();    try {        listener.onPartitionsAssigned(assignedPartitions);    } catch (WakeupException | InterruptException e) {        throw e;    } catch (Exception e) {                return e;    }    return null;}
private Exceptionf712
1
performAssignment
protected Map<String, ByteBuffer>f720_1String leaderId, String assignmentStrategy, List<JoinGroupResponseData.JoinGroupResponseMember> allSubscriptions)
{    ConsumerPartitionAssignor assignor = lookupAssignor(assignmentStrategy);    if (assignor == null)        throw new IllegalStateException("Coordinator selected invalid assignment protocol: " + assignmentStrategy);    Set<String> allSubscribedTopics = new HashSet<>();    Map<String, Subscription> subscriptions = new HashMap<>();    // collect all the owned partitions    Map<String, List<TopicPartition>> ownedPartitions = new HashMap<>();    for (JoinGroupResponseData.JoinGroupResponseMember memberSubscription : allSubscriptions) {        Subscription subscription = ConsumerProtocol.deserializeSubscription(ByteBuffer.wrap(memberSubscription.metadata()));        subscription.setGroupInstanceId(Optional.ofNullable(memberSubscription.groupInstanceId()));        subscriptions.put(memberSubscription.memberId(), subscription);        allSubscribedTopics.addAll(subscription.topics());        ownedPartitions.put(memberSubscription.memberId(), subscription.ownedPartitions());    }    // the leader will begin watching for changes to any of the topics the group is interested in,    // which ensures that all metadata changes will eventually be seen    updateGroupSubscription(allSubscribedTopics);    isLeader = true;        Map<String, Assignment> assignments = assignor.assign(metadata.fetch(), new GroupSubscription(subscriptions)).groupAssignment();    if (protocol == RebalanceProtocol.COOPERATIVE) {        validateCooperativeAssignment(ownedPartitions, assignments);    }    // user-customized assignor may have created some topics that are not in the subscription list    // and assign their partitions to the members; in this case we would like to update the leader's    // own metadata with the newly added topics so that it will not trigger a subsequent rebalance    // when these topics gets updated from metadata refresh.    //     // TODO: this is a hack and not something we want to support long-term unless we push regex into the protocol    // we may need to modify the PartitionAssignor API to better support this case.    Set<String> assignedTopics = new HashSet<>();    for (Assignment assigned : assignments.values()) {        for (TopicPartition tp : assigned.partitions()) assignedTopics.add(tp.topic());    }    if (!assignedTopics.containsAll(allSubscribedTopics)) {        Set<String> notAssignedTopics = new HashSet<>(allSubscribedTopics);        notAssignedTopics.removeAll(assignedTopics);            }    if (!allSubscribedTopics.containsAll(assignedTopics)) {        Set<String> newlyAddedTopics = new HashSet<>(assignedTopics);        newlyAddedTopics.removeAll(allSubscribedTopics);                allSubscribedTopics.addAll(assignedTopics);        updateGroupSubscription(allSubscribedTopics);    }    assignmentSnapshot = metadataSnapshot;        Map<String, ByteBuffer> groupAssignment = new HashMap<>();    for (Map.Entry<String, Assignment> assignmentEntry : assignments.entrySet()) {        ByteBuffer buffer = ConsumerProtocol.serializeAssignment(assignmentEntry.getValue());        groupAssignment.put(assignmentEntry.getKey(), buffer);    }    return groupAssignment;}
protected Map<String, ByteBuffer>f720
1
validateCooperativeAssignment
private voidf721_1final Map<String, List<TopicPartition>> ownedPartitions, final Map<String, Assignment> assignments)
{    Set<TopicPartition> totalRevokedPartitions = new HashSet<>();    Set<TopicPartition> totalAddedPartitions = new HashSet<>();    for (final Map.Entry<String, Assignment> entry : assignments.entrySet()) {        final Assignment assignment = entry.getValue();        final Set<TopicPartition> addedPartitions = new HashSet<>(assignment.partitions());        addedPartitions.removeAll(ownedPartitions.get(entry.getKey()));        final Set<TopicPartition> revokedPartitions = new HashSet<>(ownedPartitions.get(entry.getKey()));        revokedPartitions.removeAll(assignment.partitions());        totalAddedPartitions.addAll(addedPartitions);        totalRevokedPartitions.addAll(revokedPartitions);    }    // if there are overlap between revoked partitions and added partitions, it means some partitions    // immediately gets re-assigned to another member while it is still claimed by some member    totalAddedPartitions.retainAll(totalRevokedPartitions);    if (!totalAddedPartitions.isEmpty()) {         however the assignor has reassigned partitions {} which are still owned " + "by some members; return the error code to all members to let them stop", totalAddedPartitions);        throw new IllegalStateException("Assignor supporting the COOPERATIVE protocol violates its requirements");    }}
private voidf721
1
onJoinPrepare
protected voidf722_1int generation, String memberId)
{        // commit offsets prior to rebalance if auto-commit enabled    maybeAutoCommitOffsetsSync(time.timer(rebalanceConfig.rebalanceTimeoutMs));    // the generation / member-id can possibly be reset by the heartbeat thread    // upon getting errors or heartbeat timeouts; in this case whatever is previously    // owned partitions would be lost, we should trigger the callback and cleanup the assignment;    // otherwise we can proceed normally and revoke the partitions depending on the protocol,    // and in that case we should only change the assignment AFTER the revoke callback is triggered    // so that users can still access the previously owned partitions to commit offsets etc.    Exception exception = null;    final Set<TopicPartition> revokedPartitions;    if (generation == Generation.NO_GENERATION.generationId && memberId.equals(Generation.NO_GENERATION.memberId)) {        revokedPartitions = new HashSet<>(subscriptions.assignedPartitions());        if (!revokedPartitions.isEmpty()) {                        exception = invokePartitionsLost(revokedPartitions);            subscriptions.assignFromSubscribed(Collections.emptySet());        }    } else {        switch(protocol) {            case EAGER:                // revoke all partitions                revokedPartitions = new HashSet<>(subscriptions.assignedPartitions());                exception = invokePartitionsRevoked(revokedPartitions);                subscriptions.assignFromSubscribed(Collections.emptySet());                break;            case COOPERATIVE:                // only revoke those partitions that are not in the subscription any more.                Set<TopicPartition> ownedPartitions = new HashSet<>(subscriptions.assignedPartitions());                revokedPartitions = ownedPartitions.stream().filter(tp -> !subscriptions.subscription().contains(tp.topic())).collect(Collectors.toSet());                if (!revokedPartitions.isEmpty()) {                    exception = invokePartitionsRevoked(revokedPartitions);                    ownedPartitions.removeAll(revokedPartitions);                    subscriptions.assignFromSubscribed(ownedPartitions);                }                break;        }    }    isLeader = false;    subscriptions.resetGroupSubscription();    if (exception != null) {        throw new KafkaException("User rebalance callback throws an error", exception);    }}
protected voidf722
1
onSuccess
public void kafkatest_f730_0(Void value)
{    pendingAsyncCommits.decrementAndGet();    doCommitOffsetsAsync(offsets, callback);    client.pollNoWakeup();}
f730
0
onFailure
public void kafkatest_f731_0(RuntimeException e)
{    pendingAsyncCommits.decrementAndGet();    completedOffsetCommits.add(new OffsetCommitCompletion(callback, offsets, new RetriableCommitFailedException(e)));}
f731
0
doCommitOffsetsAsync
private void kafkatest_f732_0(final Map<TopicPartition, OffsetAndMetadata> offsets, final OffsetCommitCallback callback)
{    RequestFuture<Void> future = sendOffsetCommitRequest(offsets);    final OffsetCommitCallback cb = callback == null ? defaultOffsetCommitCallback : callback;    future.addListener(new RequestFutureListener<Void>() {        @Override        public void onSuccess(Void value) {            if (interceptors != null)                interceptors.onCommit(offsets);            completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, null));        }        @Override        public void onFailure(RuntimeException e) {            Exception commitException = e;            if (e instanceof RetriableException) {                commitException = new RetriableCommitFailedException(e);            }            completedOffsetCommits.add(new OffsetCommitCompletion(cb, offsets, commitException));            if (commitException instanceof FencedInstanceIdException) {                asyncCommitFenced.set(true);            }        }    });}
f732
0
sendOffsetCommitRequest
private RequestFuture<Void>f740_1final Map<TopicPartition, OffsetAndMetadata> offsets)
{    if (offsets.isEmpty())        return RequestFuture.voidSuccess();    Node coordinator = checkAndGetCoordinator();    if (coordinator == null)        return RequestFuture.coordinatorNotAvailable();    // create the offset commit request    Map<String, OffsetCommitRequestData.OffsetCommitRequestTopic> requestTopicDataMap = new HashMap<>();    for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : offsets.entrySet()) {        TopicPartition topicPartition = entry.getKey();        OffsetAndMetadata offsetAndMetadata = entry.getValue();        if (offsetAndMetadata.offset() < 0) {            return RequestFuture.failure(new IllegalArgumentException("Invalid offset: " + offsetAndMetadata.offset()));        }        OffsetCommitRequestData.OffsetCommitRequestTopic topic = requestTopicDataMap.getOrDefault(topicPartition.topic(), new OffsetCommitRequestData.OffsetCommitRequestTopic().setName(topicPartition.topic()));        topic.partitions().add(new OffsetCommitRequestData.OffsetCommitRequestPartition().setPartitionIndex(topicPartition.partition()).setCommittedOffset(offsetAndMetadata.offset()).setCommittedLeaderEpoch(offsetAndMetadata.leaderEpoch().orElse(RecordBatch.NO_PARTITION_LEADER_EPOCH)).setCommittedMetadata(offsetAndMetadata.metadata()));        requestTopicDataMap.put(topicPartition.topic(), topic);    }    final Generation generation;    if (subscriptions.partitionsAutoAssigned()) {        generation = generation();        // the only thing we can do is fail the commit and let the user rejoin the group in poll()        if (generation == null) {                        return RequestFuture.failure(new CommitFailedException());        }    } else        generation = Generation.NO_GENERATION;    OffsetCommitRequest.Builder builder = new OffsetCommitRequest.Builder(new OffsetCommitRequestData().setGroupId(this.rebalanceConfig.groupId).setGenerationId(generation.generationId).setMemberId(generation.memberId).setGroupInstanceId(rebalanceConfig.groupInstanceId.orElse(null)).setTopics(new ArrayList<>(requestTopicDataMap.values())));    log.trace("Sending OffsetCommit request with {} to coordinator {}", offsets, coordinator);    return client.send(coordinator, builder).compose(new OffsetCommitResponseHandler(offsets));}
private RequestFuture<Void>f740
1
handle
public voidf741_1OffsetCommitResponse commitResponse, RequestFuture<Void> future)
{    sensors.commitLatency.record(response.requestLatencyMs());    Set<String> unauthorizedTopics = new HashSet<>();    for (OffsetCommitResponseData.OffsetCommitResponseTopic topic : commitResponse.data().topics()) {        for (OffsetCommitResponseData.OffsetCommitResponsePartition partition : topic.partitions()) {            TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());            OffsetAndMetadata offsetAndMetadata = this.offsets.get(tp);            long offset = offsetAndMetadata.offset();            Errors error = Errors.forCode(partition.errorCode());            if (error == Errors.NONE) {                            } else {                if (error.exception() instanceof RetriableException) {                                    } else {                                    }                if (error == Errors.GROUP_AUTHORIZATION_FAILED) {                    future.raise(GroupAuthorizationException.forGroupId(rebalanceConfig.groupId));                    return;                } else if (error == Errors.TOPIC_AUTHORIZATION_FAILED) {                    unauthorizedTopics.add(tp.topic());                } else if (error == Errors.OFFSET_METADATA_TOO_LARGE || error == Errors.INVALID_COMMIT_OFFSET_SIZE) {                    // raise the error to the user                    future.raise(error);                    return;                } else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.UNKNOWN_TOPIC_OR_PARTITION) {                    // just retry                    future.raise(error);                    return;                } else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR || error == Errors.REQUEST_TIMED_OUT) {                    markCoordinatorUnknown();                    future.raise(error);                    return;                } else if (error == Errors.FENCED_INSTANCE_ID) {                                        future.raise(error);                    return;                } else if (error == Errors.REBALANCE_IN_PROGRESS) {                    /* Consumer never tries to commit offset in between join-group and sync-group,                             * and hence on broker-side it is not expected to see a commit offset request                             * during CompletingRebalance phase; if it ever happens then broker would return                             * this error. In this case we should just treat as a fatal CommitFailed exception.                             * However, we do not need to reset generations and just request re-join, such that                             * if the caller decides to proceed and poll, it would still try to proceed and re-join normally.                             */                    requestRejoin();                    future.raise(new CommitFailedException());                    return;                } else if (error == Errors.UNKNOWN_MEMBER_ID || error == Errors.ILLEGAL_GENERATION) {                    // need to reset generation and re-join group                    resetGenerationOnResponseError(ApiKeys.OFFSET_COMMIT, error);                    future.raise(new CommitFailedException());                    return;                } else {                    future.raise(new KafkaException("Unexpected error in commit: " + error.message()));                    return;                }            }        }    }    if (!unauthorizedTopics.isEmpty()) {                future.raise(new TopicAuthorizationException(unauthorizedTopics));    } else {        future.complete(null);    }}
public voidf741
1
sendOffsetFetchRequest
private RequestFuture<Map<TopicPartition, OffsetAndMetadata>>f742_1Set<TopicPartition> partitions)
{    Node coordinator = checkAndGetCoordinator();    if (coordinator == null)        return RequestFuture.coordinatorNotAvailable();        // construct the request    OffsetFetchRequest.Builder requestBuilder = new OffsetFetchRequest.Builder(this.rebalanceConfig.groupId, new ArrayList<>(partitions));    // send the request with a callback    return client.send(coordinator, requestBuilder).compose(new OffsetFetchResponseHandler());}
private RequestFuture<Map<TopicPartition, OffsetAndMetadata>>f742
1
close
public voidf750_1)
{    for (ConsumerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptor.close();        } catch (Exception e) {                    }    }}
public voidf750
1
allowAutoTopicCreation
public boolean kafkatest_f751_0()
{    return allowAutoTopicCreation;}
f751
0
newMetadataRequestBuilder
public synchronized MetadataRequest.Builder kafkatest_f752_0()
{    if (subscription.hasPatternSubscription())        return MetadataRequest.Builder.allTopics();    List<String> topics = new ArrayList<>();    topics.addAll(subscription.groupSubscription());    topics.addAll(transientTopics);    return new MetadataRequest.Builder(topics, allowAutoTopicCreation);}
f752
0
leastLoadedNode
public Node kafkatest_f760_0()
{    lock.lock();    try {        return client.leastLoadedNode(time.milliseconds());    } finally {        lock.unlock();    }}
f760
0
hasReadyNodes
public boolean kafkatest_f761_0(long now)
{    lock.lock();    try {        return client.hasReadyNodes(now);    } finally {        lock.unlock();    }}
f761
0
awaitMetadataUpdate
public boolean kafkatest_f762_0(Timer timer)
{    int version = this.metadata.requestUpdate();    do {        poll(timer);    } while (this.metadata.updateVersion() == version && timer.notExpired());    return this.metadata.updateVersion() > version;}
f762
0
pollNoWakeup
public void kafkatest_f770_0()
{    poll(time.timer(0), null, true);}
f770
0
awaitPendingRequests
public boolean kafkatest_f771_0(Node node, Timer timer)
{    while (hasPendingRequests(node) && timer.notExpired()) {        poll(timer);    }    return !hasPendingRequests(node);}
f771
0
pendingRequestCount
public int kafkatest_f772_0(Node node)
{    lock.lock();    try {        return unsent.requestCount(node) + client.inFlightRequestCount(node.idString());    } finally {        lock.unlock();    }}
f772
0
failExpiredRequests
private void kafkatest_f780_0(long now)
{    // clear all expired unsent requests and fail their corresponding futures    Collection<ClientRequest> expiredRequests = unsent.removeExpiredRequests(now);    for (ClientRequest request : expiredRequests) {        RequestFutureCompletionHandler handler = (RequestFutureCompletionHandler) request.callback();        handler.onFailure(new TimeoutException("Failed to send request after " + request.requestTimeoutMs() + " ms."));    }}
f780
0
failUnsentRequests
private void kafkatest_f781_0(Node node, RuntimeException e)
{    // clear unsent requests to node and fail their corresponding futures    lock.lock();    try {        Collection<ClientRequest> unsentRequests = unsent.remove(node);        for (ClientRequest unsentRequest : unsentRequests) {            RequestFutureCompletionHandler handler = (RequestFutureCompletionHandler) unsentRequest.callback();            handler.onFailure(e);        }    } finally {        lock.unlock();    }}
f781
0
trySend
 long kafkatest_f782_0(long now)
{    long pollDelayMs = maxPollTimeoutMs;    // send any requests that can be sent now    for (Node node : unsent.nodes()) {        Iterator<ClientRequest> iterator = unsent.requestIterator(node);        if (iterator.hasNext())            pollDelayMs = Math.min(pollDelayMs, client.pollDelayMs(node, now));        while (iterator.hasNext()) {            ClientRequest request = iterator.next();            if (client.ready(node, now)) {                client.send(request, now);                iterator.remove();            } else {                // try next node when current node is not ready                break;            }        }    }    return pollDelayMs;}
f782
0
fireCompletion
public voidf790_1)
{    if (e != null) {        future.raise(e);    } else if (response.authenticationException() != null) {        future.raise(response.authenticationException());    } else if (response.wasDisconnected()) {                future.raise(DisconnectException.INSTANCE);    } else if (response.versionMismatch() != null) {        future.raise(response.versionMismatch());    } else {        future.complete(response);    }}
public voidf790
1
onFailure
public void kafkatest_f791_0(RuntimeException e)
{    this.e = e;    pendingCompletion.add(this);}
f791
0
onComplete
public void kafkatest_f792_0(ClientResponse response)
{    this.response = response;    pendingCompletion.add(this);}
f792
0
remove
public Collection<ClientRequest> kafkatest_f800_0(Node node)
{    // queue after it has been removed from the map    synchronized (unsent) {        ConcurrentLinkedQueue<ClientRequest> requests = unsent.remove(node);        return requests == null ? Collections.<ClientRequest>emptyList() : requests;    }}
f800
0
requestIterator
public Iterator<ClientRequest> kafkatest_f801_0(Node node)
{    ConcurrentLinkedQueue<ClientRequest> requests = unsent.get(node);    return requests == null ? Collections.<ClientRequest>emptyIterator() : requests.iterator();}
f801
0
nodes
public Collection<Node> kafkatest_f802_0()
{    return unsent.keySet();}
f802
0
deserializeSubscription
public static Subscription kafkatest_f810_0(ByteBuffer buffer)
{    Short version = deserializeVersion(buffer);    if (version < CONSUMER_PROTOCOL_V0)        throw new SchemaException("Unsupported subscription version: " + version);    switch(version) {        case CONSUMER_PROTOCOL_V0:            return deserializeSubscriptionV0(buffer);        case CONSUMER_PROTOCOL_V1:            return deserializeSubscriptionV1(buffer);        // assume all higher versions can be parsed as V1        default:            return deserializeSubscriptionV1(buffer);    }}
f810
0
serializeAssignmentV0
public static ByteBuffer kafkatest_f811_0(Assignment assignment)
{    Struct struct = new Struct(ASSIGNMENT_V0);    struct.set(USER_DATA_KEY_NAME, assignment.userData());    List<Struct> topicAssignments = new ArrayList<>();    Map<String, List<Integer>> partitionsByTopic = CollectionUtils.groupPartitionsByTopic(assignment.partitions());    for (Map.Entry<String, List<Integer>> topicEntry : partitionsByTopic.entrySet()) {        Struct topicAssignment = new Struct(TOPIC_ASSIGNMENT_V0);        topicAssignment.set(TOPIC_KEY_NAME, topicEntry.getKey());        topicAssignment.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());        topicAssignments.add(topicAssignment);    }    struct.set(TOPIC_PARTITIONS_KEY_NAME, topicAssignments.toArray());    ByteBuffer buffer = ByteBuffer.allocate(CONSUMER_PROTOCOL_HEADER_V0.sizeOf() + ASSIGNMENT_V0.sizeOf(struct));    CONSUMER_PROTOCOL_HEADER_V0.writeTo(buffer);    ASSIGNMENT_V0.write(buffer, struct);    buffer.flip();    return buffer;}
f811
0
serializeAssignmentV1
public static ByteBuffer kafkatest_f812_0(Assignment assignment)
{    Struct struct = new Struct(ASSIGNMENT_V1);    struct.set(USER_DATA_KEY_NAME, assignment.userData());    List<Struct> topicAssignments = new ArrayList<>();    Map<String, List<Integer>> partitionsByTopic = CollectionUtils.groupPartitionsByTopic(assignment.partitions());    for (Map.Entry<String, List<Integer>> topicEntry : partitionsByTopic.entrySet()) {        Struct topicAssignment = new Struct(TOPIC_ASSIGNMENT_V0);        topicAssignment.set(TOPIC_KEY_NAME, topicEntry.getKey());        topicAssignment.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());        topicAssignments.add(topicAssignment);    }    struct.set(TOPIC_PARTITIONS_KEY_NAME, topicAssignments.toArray());    ByteBuffer buffer = ByteBuffer.allocate(CONSUMER_PROTOCOL_HEADER_V1.sizeOf() + ASSIGNMENT_V1.sizeOf(struct));    CONSUMER_PROTOCOL_HEADER_V1.writeTo(buffer);    ASSIGNMENT_V1.write(buffer, struct);    buffer.flip();    return buffer;}
f812
0
sendFetches
public synchronized intf820_1)
{    // Update metrics in case there was an assignment change    sensors.maybeUpdateAssignment(subscriptions);    Map<Node, FetchSessionHandler.FetchRequestData> fetchRequestMap = prepareFetchRequests();    for (Map.Entry<Node, FetchSessionHandler.FetchRequestData> entry : fetchRequestMap.entrySet()) {        final Node fetchTarget = entry.getKey();        final FetchSessionHandler.FetchRequestData data = entry.getValue();        final FetchRequest.Builder request = FetchRequest.Builder.forConsumer(this.maxWaitMs, this.minBytes, data.toSend()).isolationLevel(isolationLevel).setMaxBytes(this.maxBytes).metadata(data.metadata()).toForget(data.toForget()).rackId(clientRackId);        if (log.isDebugEnabled()) {                    }        client.send(fetchTarget, request).addListener(new RequestFutureListener<ClientResponse>() {            @Override            public void onSuccess(ClientResponse resp) {                synchronized (Fetcher.this) {                    try {                        @SuppressWarnings("unchecked")                        FetchResponse<Records> response = (FetchResponse<Records>) resp.responseBody();                        FetchSessionHandler handler = sessionHandler(fetchTarget.id());                        if (handler == null) {                                                        return;                        }                        if (!handler.handleResponse(response)) {                            return;                        }                        Set<TopicPartition> partitions = new HashSet<>(response.responseData().keySet());                        FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);                        for (Map.Entry<TopicPartition, FetchResponse.PartitionData<Records>> entry : response.responseData().entrySet()) {                            TopicPartition partition = entry.getKey();                            FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition);                            if (requestData == null) {                                String message;                                if (data.metadata().isFull()) {                                    message = MessageFormatter.arrayFormat("Response for missing full request partition: partition={}; metadata={}", new Object[] { partition, data.metadata() }).getMessage();                                } else {                                    message = MessageFormatter.arrayFormat("Response for missing session request partition: partition={}; metadata={}; toSend={}; toForget={}", new Object[] { partition, data.metadata(), data.toSend(), data.toForget() }).getMessage();                                }                                // Received fetch response for missing session partition                                throw new IllegalStateException(message);                            } else {                                long fetchOffset = requestData.fetchOffset;                                FetchResponse.PartitionData<Records> partitionData = entry.getValue();                                                                Iterator<? extends RecordBatch> batches = partitionData.records.batches().iterator();                                short responseVersion = resp.requestHeader().apiVersion();                                completedFetches.add(new CompletedFetch(partition, partitionData, metricAggregator, batches, fetchOffset, responseVersion));                            }                        }                        sensors.fetchLatency.record(resp.requestLatencyMs());                    } finally {                        nodesWithPendingFetchRequests.remove(fetchTarget.id());                    }                }            }            @Override            public void onFailure(RuntimeException e) {                synchronized (Fetcher.this) {                    try {                        FetchSessionHandler handler = sessionHandler(fetchTarget.id());                        if (handler != null) {                            handler.handleError(e);                        }                    } finally {                        nodesWithPendingFetchRequests.remove(fetchTarget.id());                    }                }            }        });        this.nodesWithPendingFetchRequests.add(entry.getKey().id());    }    return fetchRequestMap.size();}
public synchronized intf820
1
onSuccess
public voidf821_1ClientResponse resp)
{    synchronized (Fetcher.this) {        try {            @SuppressWarnings("unchecked")            FetchResponse<Records> response = (FetchResponse<Records>) resp.responseBody();            FetchSessionHandler handler = sessionHandler(fetchTarget.id());            if (handler == null) {                                return;            }            if (!handler.handleResponse(response)) {                return;            }            Set<TopicPartition> partitions = new HashSet<>(response.responseData().keySet());            FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);            for (Map.Entry<TopicPartition, FetchResponse.PartitionData<Records>> entry : response.responseData().entrySet()) {                TopicPartition partition = entry.getKey();                FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition);                if (requestData == null) {                    String message;                    if (data.metadata().isFull()) {                        message = MessageFormatter.arrayFormat("Response for missing full request partition: partition={}; metadata={}", new Object[] { partition, data.metadata() }).getMessage();                    } else {                        message = MessageFormatter.arrayFormat("Response for missing session request partition: partition={}; metadata={}; toSend={}; toForget={}", new Object[] { partition, data.metadata(), data.toSend(), data.toForget() }).getMessage();                    }                    // Received fetch response for missing session partition                    throw new IllegalStateException(message);                } else {                    long fetchOffset = requestData.fetchOffset;                    FetchResponse.PartitionData<Records> partitionData = entry.getValue();                                        Iterator<? extends RecordBatch> batches = partitionData.records.batches().iterator();                    short responseVersion = resp.requestHeader().apiVersion();                    completedFetches.add(new CompletedFetch(partition, partitionData, metricAggregator, batches, fetchOffset, responseVersion));                }            }            sensors.fetchLatency.record(resp.requestLatencyMs());        } finally {            nodesWithPendingFetchRequests.remove(fetchTarget.id());        }    }}
public voidf821
1
onFailure
public void kafkatest_f822_0(RuntimeException e)
{    synchronized (Fetcher.this) {        try {            FetchSessionHandler handler = sessionHandler(fetchTarget.id());            if (handler != null) {                handler.handleError(e);            }        } finally {            nodesWithPendingFetchRequests.remove(fetchTarget.id());        }    }}
f822
0
offsetsForTimes
public Map<TopicPartition, OffsetAndTimestamp> kafkatest_f830_0(Map<TopicPartition, Long> timestampsToSearch, Timer timer)
{    metadata.addTransientTopics(topicsForPartitions(timestampsToSearch.keySet()));    try {        Map<TopicPartition, ListOffsetData> fetchedOffsets = fetchOffsetsByTimes(timestampsToSearch, timer, true).fetchedOffsets;        HashMap<TopicPartition, OffsetAndTimestamp> offsetsByTimes = new HashMap<>(timestampsToSearch.size());        for (Map.Entry<TopicPartition, Long> entry : timestampsToSearch.entrySet()) offsetsByTimes.put(entry.getKey(), null);        for (Map.Entry<TopicPartition, ListOffsetData> entry : fetchedOffsets.entrySet()) {            // 'entry.getValue().timestamp' will not be null since we are guaranteed            // to work with a v1 (or later) ListOffset request            ListOffsetData offsetData = entry.getValue();            offsetsByTimes.put(entry.getKey(), new OffsetAndTimestamp(offsetData.offset, offsetData.timestamp, offsetData.leaderEpoch));        }        return offsetsByTimes;    } finally {        metadata.clearTransientTopics();    }}
f830
0
fetchOffsetsByTimes
private ListOffsetResult kafkatest_f831_0(Map<TopicPartition, Long> timestampsToSearch, Timer timer, boolean requireTimestamps)
{    ListOffsetResult result = new ListOffsetResult();    if (timestampsToSearch.isEmpty())        return result;    Map<TopicPartition, Long> remainingToSearch = new HashMap<>(timestampsToSearch);    do {        RequestFuture<ListOffsetResult> future = sendListOffsetsRequests(remainingToSearch, requireTimestamps);        client.poll(future, timer);        if (!future.isDone())            break;        if (future.succeeded()) {            ListOffsetResult value = future.value();            result.fetchedOffsets.putAll(value.fetchedOffsets);            if (value.partitionsToRetry.isEmpty())                return result;            remainingToSearch.keySet().retainAll(value.partitionsToRetry);        } else if (!future.isRetriable()) {            throw future.exception();        }        if (metadata.updateRequested())            client.awaitMetadataUpdate(timer);        else            timer.sleep(retryBackoffMs);    } while (timer.notExpired());    throw new TimeoutException("Failed to get offsets by times in " + timer.elapsedMs() + "ms");}
f831
0
beginningOffsets
public Map<TopicPartition, Long> kafkatest_f832_0(Collection<TopicPartition> partitions, Timer timer)
{    return beginningOrEndOffset(partitions, ListOffsetRequest.EARLIEST_TIMESTAMP, timer);}
f832
0
onFailure
public voidf840_1RuntimeException e)
{    subscriptions.requestFailed(resetTimestamps.keySet(), time.milliseconds() + retryBackoffMs);    metadata.requestUpdate();    if (!(e instanceof RetriableException) && !cachedListOffsetsException.compareAndSet(null, e))        }
public voidf840
1
hasUsableOffsetForLeaderEpochVersion
private boolean kafkatest_f841_0(NodeApiVersions nodeApiVersions)
{    ApiVersionsResponse.ApiVersion apiVersion = nodeApiVersions.apiVersion(ApiKeys.OFFSET_FOR_LEADER_EPOCH);    if (apiVersion == null)        return false;    return OffsetsForLeaderEpochRequest.supportsTopicPermission(apiVersion.maxVersion);}
f841
0
validateOffsetsAsync
private voidf842_1Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate)
{    final Map<Node, Map<TopicPartition, SubscriptionState.FetchPosition>> regrouped = regroupFetchPositionsByLeader(partitionsToValidate);    regrouped.forEach((node, fetchPostitions) -> {        if (node.isEmpty()) {            metadata.requestUpdate();            return;        }        NodeApiVersions nodeApiVersions = apiVersions.get(node.idString());        if (nodeApiVersions == null) {            client.tryConnect(node);            return;        }        if (!hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {                        for (TopicPartition partition : fetchPostitions.keySet()) {                subscriptions.completeValidation(partition);            }            return;        }        subscriptions.setNextAllowedRetry(fetchPostitions.keySet(), time.milliseconds() + requestTimeoutMs);        RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future = offsetsForLeaderEpochClient.sendAsyncRequest(node, partitionsToValidate);        future.addListener(new RequestFutureListener<OffsetsForLeaderEpochClient.OffsetForEpochResult>() {            @Override            public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsResult) {                Map<TopicPartition, OffsetAndMetadata> truncationWithoutResetPolicy = new HashMap<>();                if (!offsetsResult.partitionsToRetry().isEmpty()) {                    subscriptions.setNextAllowedRetry(offsetsResult.partitionsToRetry(), time.milliseconds() + retryBackoffMs);                    metadata.requestUpdate();                }                // For each OffsetsForLeader response, check if the end-offset is lower than our current offset                // for the partition. If so, it means we have experienced log truncation and need to reposition                // that partition's offset.                offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {                    SubscriptionState.FetchPosition requestPosition = fetchPostitions.get(respTopicPartition);                    Optional<OffsetAndMetadata> divergentOffsetOpt = subscriptions.maybeCompleteValidation(respTopicPartition, requestPosition, respEndOffset);                    divergentOffsetOpt.ifPresent(divergentOffset -> {                        truncationWithoutResetPolicy.put(respTopicPartition, divergentOffset);                    });                });                if (!truncationWithoutResetPolicy.isEmpty()) {                    throw new LogTruncationException(truncationWithoutResetPolicy);                }            }            @Override            public void onFailure(RuntimeException e) {                subscriptions.requestFailed(fetchPostitions.keySet(), time.milliseconds() + retryBackoffMs);                metadata.requestUpdate();                if (!(e instanceof RetriableException) && !cachedOffsetForLeaderException.compareAndSet(null, e)) {                                    }            }        });    });}
private voidf842
1
onSuccess
public void kafkatest_f850_0(ClientResponse response, RequestFuture<ListOffsetResult> future)
{    ListOffsetResponse lor = (ListOffsetResponse) response.responseBody();    log.trace("Received ListOffsetResponse {} from broker {}", lor, node);    handleListOffsetResponse(timestampsToSearch, lor, future);}
f850
0
handleListOffsetResponse
private voidf851_1Map<TopicPartition, ListOffsetRequest.PartitionData> timestampsToSearch, ListOffsetResponse listOffsetResponse, RequestFuture<ListOffsetResult> future)
{    Map<TopicPartition, ListOffsetData> fetchedOffsets = new HashMap<>();    Set<TopicPartition> partitionsToRetry = new HashSet<>();    Set<String> unauthorizedTopics = new HashSet<>();    for (Map.Entry<TopicPartition, ListOffsetRequest.PartitionData> entry : timestampsToSearch.entrySet()) {        TopicPartition topicPartition = entry.getKey();        ListOffsetResponse.PartitionData partitionData = listOffsetResponse.responseData().get(topicPartition);        Errors error = partitionData.error;        if (error == Errors.NONE) {            if (partitionData.offsets != null) {                // Handle v0 response                long offset;                if (partitionData.offsets.size() > 1) {                    future.raise(new IllegalStateException("Unexpected partitionData response of length " + partitionData.offsets.size()));                    return;                } else if (partitionData.offsets.isEmpty()) {                    offset = ListOffsetResponse.UNKNOWN_OFFSET;                } else {                    offset = partitionData.offsets.get(0);                }                                if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {                    ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());                    fetchedOffsets.put(topicPartition, offsetData);                }            } else {                // Handle v1 and later response                                if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {                    ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp, partitionData.leaderEpoch);                    fetchedOffsets.put(topicPartition, offsetData);                }            }        } else if (error == Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT) {            // The message format on the broker side is before 0.10.0, which means it does not            // support timestamps. We treat this case the same as if we weren't able to find an            // offset corresponding to the requested timestamp and leave it out of the result.                    } else if (error == Errors.NOT_LEADER_FOR_PARTITION || error == Errors.REPLICA_NOT_AVAILABLE || error == Errors.KAFKA_STORAGE_ERROR || error == Errors.OFFSET_NOT_AVAILABLE || error == Errors.LEADER_NOT_AVAILABLE) {                        partitionsToRetry.add(topicPartition);        } else if (error == Errors.FENCED_LEADER_EPOCH || error == Errors.UNKNOWN_LEADER_EPOCH) {                        partitionsToRetry.add(topicPartition);        } else if (error == Errors.UNKNOWN_TOPIC_OR_PARTITION) {                        partitionsToRetry.add(topicPartition);        } else if (error == Errors.TOPIC_AUTHORIZATION_FAILED) {            unauthorizedTopics.add(topicPartition.topic());        } else {                        partitionsToRetry.add(topicPartition);        }    }    if (!unauthorizedTopics.isEmpty())        future.raise(new TopicAuthorizationException(unauthorizedTopics));    else        future.complete(new ListOffsetResult(fetchedOffsets, partitionsToRetry));}
private voidf851
1
fetchablePartitions
private List<TopicPartition> kafkatest_f852_0()
{    Set<TopicPartition> exclude = new HashSet<>();    if (nextInLineFetch != null && !nextInLineFetch.isConsumed) {        exclude.add(nextInLineFetch.partition);    }    for (CompletedFetch completedFetch : completedFetches) {        exclude.add(completedFetch.partition);    }    return subscriptions.fetchablePartitions(tp -> !exclude.contains(tp));}
f852
0
clearBufferedDataForUnassignedPartitions
public void kafkatest_f860_0(Collection<TopicPartition> assignedPartitions)
{    Iterator<CompletedFetch> completedFetchesItr = completedFetches.iterator();    while (completedFetchesItr.hasNext()) {        CompletedFetch records = completedFetchesItr.next();        TopicPartition tp = records.partition;        if (!assignedPartitions.contains(tp)) {            records.drain();            completedFetchesItr.remove();        }    }    if (nextInLineFetch != null && !assignedPartitions.contains(nextInLineFetch.partition)) {        nextInLineFetch.drain();        nextInLineFetch = null;    }}
f860
0
clearBufferedDataForUnassignedTopics
public void kafkatest_f861_0(Collection<String> assignedTopics)
{    Set<TopicPartition> currentTopicPartitions = new HashSet<>();    for (TopicPartition tp : subscriptions.assignedPartitions()) {        if (assignedTopics.contains(tp.topic())) {            currentTopicPartitions.add(tp);        }    }    clearBufferedDataForUnassignedPartitions(currentTopicPartitions);}
f861
0
sessionHandler
protected FetchSessionHandler kafkatest_f862_0(int node)
{    return sessionHandlers.get(node);}
f862
0
consumeAbortedTransactionsUpTo
private void kafkatest_f870_0(long offset)
{    if (abortedTransactions == null)        return;    while (!abortedTransactions.isEmpty() && abortedTransactions.peek().firstOffset <= offset) {        FetchResponse.AbortedTransaction abortedTransaction = abortedTransactions.poll();        abortedProducerIds.add(abortedTransaction.producerId);    }}
f870
0
isBatchAborted
private boolean kafkatest_f871_0(RecordBatch batch)
{    return batch.isTransactional() && abortedProducerIds.contains(batch.producerId());}
f871
0
abortedTransactions
private PriorityQueue<FetchResponse.AbortedTransaction> kafkatest_f872_0(FetchResponse.PartitionData<?> partition)
{    if (partition.abortedTransactions == null || partition.abortedTransactions.isEmpty())        return null;    PriorityQueue<FetchResponse.AbortedTransaction> abortedTransactions = new PriorityQueue<>(partition.abortedTransactions.size(), Comparator.comparingLong(o -> o.firstOffset));    abortedTransactions.addAll(partition.abortedTransactions);    return abortedTransactions;}
f872
0
recordPartitionLag
private void kafkatest_f880_0(TopicPartition tp, long lag)
{    this.recordsFetchLag.record(lag);    String name = partitionLagMetricName(tp);    Sensor recordsLag = this.metrics.getSensor(name);    if (recordsLag == null) {        Map<String, String> metricTags = topicPartitionTags(tp);        recordsLag = this.metrics.sensor(name);        recordsLag.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLag, metricTags), new Value());        recordsLag.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLagMax, metricTags), new Max());        recordsLag.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLagAvg, metricTags), new Avg());    }    recordsLag.record(lag);}
f880
0
partitionLagMetricName
private static String kafkatest_f881_0(TopicPartition tp)
{    return tp + ".records-lag";}
f881
0
partitionLeadMetricName
private static String kafkatest_f882_0(TopicPartition tp)
{    return tp + ".records-lead";}
f882
0
sentHeartbeat
public void kafkatest_f890_0(long now)
{    this.lastHeartbeatSend = now;    update(now);    heartbeatTimer.reset(rebalanceConfig.heartbeatIntervalMs);}
f890
0
failHeartbeat
public void kafkatest_f891_0()
{    update(time.milliseconds());    heartbeatTimer.reset(rebalanceConfig.retryBackoffMs);}
f891
0
receiveHeartbeat
public void kafkatest_f892_0()
{    update(time.milliseconds());    sessionTimer.reset(rebalanceConfig.sessionTimeoutMs);}
f892
0
lastPollTime
public long kafkatest_f900_0()
{    return pollTimer.currentTimeMs();}
f900
0
prepareRequest
protected AbstractRequest.Builder<OffsetsForLeaderEpochRequest> kafkatest_f903_0(Node node, Map<TopicPartition, SubscriptionState.FetchPosition> requestData)
{    Map<TopicPartition, OffsetsForLeaderEpochRequest.PartitionData> partitionData = new HashMap<>(requestData.size());    requestData.forEach((topicPartition, fetchPosition) -> fetchPosition.offsetEpoch.ifPresent(fetchEpoch -> partitionData.put(topicPartition, new OffsetsForLeaderEpochRequest.PartitionData(fetchPosition.currentLeader.epoch, fetchEpoch))));    return OffsetsForLeaderEpochRequest.Builder.forConsumer(partitionData);}
f903
0
handleResponse
protected OffsetForEpochResult kafkatest_f904_0(Node node, Map<TopicPartition, SubscriptionState.FetchPosition> requestData, OffsetsForLeaderEpochResponse response)
{    Set<TopicPartition> partitionsToRetry = new HashSet<>();    Set<String> unauthorizedTopics = new HashSet<>();    Map<TopicPartition, EpochEndOffset> endOffsets = new HashMap<>();    for (TopicPartition topicPartition : requestData.keySet()) {        EpochEndOffset epochEndOffset = response.responses().get(topicPartition);        if (epochEndOffset == null) {            logger().warn("Missing partition {} from response, ignoring", topicPartition);            partitionsToRetry.add(topicPartition);            continue;        }        Errors error = epochEndOffset.error();        if (error == Errors.NONE) {            logger().debug("Handling OffsetsForLeaderEpoch response for {}. Got offset {} for epoch {}", topicPartition, epochEndOffset.endOffset(), epochEndOffset.leaderEpoch());            endOffsets.put(topicPartition, epochEndOffset);        } else if (error == Errors.NOT_LEADER_FOR_PARTITION || error == Errors.REPLICA_NOT_AVAILABLE || error == Errors.KAFKA_STORAGE_ERROR || error == Errors.OFFSET_NOT_AVAILABLE || error == Errors.LEADER_NOT_AVAILABLE) {            logger().debug("Attempt to fetch offsets for partition {} failed due to {}, retrying.", topicPartition, error);            partitionsToRetry.add(topicPartition);        } else if (error == Errors.FENCED_LEADER_EPOCH || error == Errors.UNKNOWN_LEADER_EPOCH) {            logger().debug("Attempt to fetch offsets for partition {} failed due to {}, retrying.", topicPartition, error);            partitionsToRetry.add(topicPartition);        } else if (error == Errors.UNKNOWN_TOPIC_OR_PARTITION) {            logger().warn("Received unknown topic or partition error in ListOffset request for partition {}", topicPartition);            partitionsToRetry.add(topicPartition);        } else if (error == Errors.TOPIC_AUTHORIZATION_FAILED) {            unauthorizedTopics.add(topicPartition.topic());        } else {            logger().warn("Attempt to fetch offsets for partition {} failed due to: {}, retrying.", topicPartition, error.message());            partitionsToRetry.add(topicPartition);        }    }    if (!unauthorizedTopics.isEmpty())        throw new TopicAuthorizationException(unauthorizedTopics);    else        return new OffsetForEpochResult(endOffsets, partitionsToRetry);}
f904
0
userData
public ByteBuffer kafkatest_f912_0()
{    return userData;}
f912
0
toString
public String kafkatest_f913_0()
{    return "Assignment(" + "partitions=" + partitions + ')';}
f913
0
subscriptionUserData
public ByteBuffer kafkatest_f914_0(Set<String> topics)
{    return oldAssignor.subscription(topics).userData();}
f914
0
isDone
public boolean kafkatest_f922_0()
{    return result.get() != INCOMPLETE_SENTINEL;}
f922
0
awaitDone
public boolean kafkatest_f923_0(long timeout, TimeUnit unit) throws InterruptedException
{    return completedLatch.await(timeout, unit);}
f923
0
value
public T kafkatest_f924_0()
{    if (!succeeded())        throw new IllegalStateException("Attempt to retrieve value from future which hasn't successfully completed");    return (T) result.get();}
f924
0
fireSuccess
private void kafkatest_f932_0()
{    T value = value();    while (true) {        RequestFutureListener<T> listener = listeners.poll();        if (listener == null)            break;        listener.onSuccess(value);    }}
f932
0
fireFailure
private void kafkatest_f933_0()
{    RuntimeException exception = exception();    while (true) {        RequestFutureListener<T> listener = listeners.poll();        if (listener == null)            break;        listener.onFailure(exception);    }}
f933
0
addListener
public void kafkatest_f934_0(RequestFutureListener<T> listener)
{    this.listeners.add(listener);    if (failed())        fireFailure();    else if (succeeded())        fireSuccess();}
f934
0
voidSuccess
public static RequestFuture<Void> kafkatest_f942_0()
{    RequestFuture<Void> future = new RequestFuture<>();    future.complete(null);    return future;}
f942
0
coordinatorNotAvailable
public static RequestFuture<T> kafkatest_f943_0()
{    return failure(Errors.COORDINATOR_NOT_AVAILABLE.exception());}
f943
0
noBrokersAvailable
public static RequestFuture<T> kafkatest_f944_0()
{    return failure(new NoAvailableBrokersException());}
f944
0
subscribe
public synchronized void kafkatest_f952_0(Pattern pattern, ConsumerRebalanceListener listener)
{    registerRebalanceListener(listener);    setSubscriptionType(SubscriptionType.AUTO_PATTERN);    this.subscribedPattern = pattern;}
f952
0
subscribeFromPattern
public synchronized boolean kafkatest_f953_0(Set<String> topics)
{    if (subscriptionType != SubscriptionType.AUTO_PATTERN)        throw new IllegalArgumentException("Attempt to subscribe from pattern while subscription type set to " + subscriptionType);    return changeSubscription(topics);}
f953
0
changeSubscription
private boolean kafkatest_f954_0(Set<String> topicsToSubscribe)
{    if (subscription.equals(topicsToSubscribe))        return false;    subscription = topicsToSubscribe;    if (subscriptionType != SubscriptionType.USER_ASSIGNED) {        groupSubscription = new HashSet<>(groupSubscription);        groupSubscription.addAll(topicsToSubscribe);    } else {        groupSubscription = new HashSet<>(topicsToSubscribe);    }    return true;}
f954
0
hasNoSubscriptionOrUserAssignment
public synchronized boolean kafkatest_f962_0()
{    return this.subscriptionType == SubscriptionType.NONE;}
f962
0
unsubscribe
public synchronized void kafkatest_f963_0()
{    this.subscription = Collections.emptySet();    this.groupSubscription = Collections.emptySet();    this.assignment.clear();    this.subscribedPattern = null;    this.subscriptionType = SubscriptionType.NONE;    this.assignmentId++;}
f963
0
matchesSubscribedPattern
 synchronized boolean kafkatest_f964_0(String topic)
{    Pattern pattern = this.subscribedPattern;    if (hasPatternSubscription() && pattern != null)        return pattern.matcher(topic).matches();    return false;}
f964
0
seek
public void kafkatest_f972_0(TopicPartition tp, long offset)
{    seekValidated(tp, new FetchPosition(offset));}
f972
0
seekUnvalidated
public void kafkatest_f973_0(TopicPartition tp, FetchPosition position)
{    assignedState(tp).seekUnvalidated(position);}
f973
0
maybeSeekUnvalidated
 synchronized voidf974_1TopicPartition tp, long offset, OffsetResetStrategy requestedResetStrategy)
{    TopicPartitionState state = assignedStateOrNull(tp);    if (state == null) {            } else if (!state.awaitingReset()) {            } else if (requestedResetStrategy != state.resetStrategy) {            } else {                state.seekUnvalidated(new FetchPosition(offset));    }}
 synchronized voidf974
1
maybeCompleteValidation
public synchronized Optional<OffsetAndMetadata>f982_1TopicPartition tp, FetchPosition requestPosition, EpochEndOffset epochEndOffset)
{    TopicPartitionState state = assignedStateOrNull(tp);    if (state == null) {            } else if (!state.awaitingValidation()) {            } else {        SubscriptionState.FetchPosition currentPosition = state.position;        if (!currentPosition.equals(requestPosition)) {                    } else if (epochEndOffset.endOffset() < currentPosition.offset) {            if (hasDefaultOffsetResetPolicy()) {                SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition(epochEndOffset.endOffset(), Optional.of(epochEndOffset.leaderEpoch()), currentPosition.currentLeader);                                state.seekValidated(newPosition);            } else {                                return Optional.of(new OffsetAndMetadata(epochEndOffset.endOffset(), Optional.of(epochEndOffset.leaderEpoch()), null));            }        } else {            state.completeValidation();        }    }    return Optional.empty();}
public synchronized Optional<OffsetAndMetadata>f982
1
awaitingValidation
public synchronized boolean kafkatest_f983_0(TopicPartition tp)
{    return assignedState(tp).awaitingValidation();}
f983
0
completeValidation
public synchronized void kafkatest_f984_0(TopicPartition tp)
{    assignedState(tp).completeValidation();}
f984
0
updatePreferredReadReplica
public synchronized void kafkatest_f992_0(TopicPartition tp, int preferredReadReplicaId, Supplier<Long> timeMs)
{    assignedState(tp).updatePreferredReadReplica(preferredReadReplicaId, timeMs);}
f992
0
preferredReadReplica
public synchronized Optional<Integer> kafkatest_f993_0(TopicPartition tp, long timeMs)
{    return assignedState(tp).preferredReadReplica(timeMs);}
f993
0
clearPreferredReadReplica
public synchronized Optional<Integer> kafkatest_f994_0(TopicPartition tp)
{    return assignedState(tp).clearPreferredReadReplica();}
f994
0
resetStrategy
public synchronized OffsetResetStrategy kafkatest_f1002_0(TopicPartition partition)
{    return assignedState(partition).resetStrategy();}
f1002
0
hasAllFetchPositions
public synchronized boolean kafkatest_f1003_0()
{    return assignment.stream().allMatch(state -> state.value().hasValidPosition());}
f1003
0
missingFetchPositions
public synchronized Set<TopicPartition> kafkatest_f1004_0()
{    return collectPartitions(state -> !state.hasPosition(), Collectors.toSet());}
f1004
0
hasValidPosition
public synchronized boolean kafkatest_f1012_0(TopicPartition tp)
{    TopicPartitionState assignedOrNull = assignedStateOrNull(tp);    return assignedOrNull != null && assignedOrNull.hasValidPosition();}
f1012
0
pause
public synchronized void kafkatest_f1013_0(TopicPartition tp)
{    assignedState(tp).pause();}
f1013
0
resume
public synchronized void kafkatest_f1014_0(TopicPartition tp)
{    assignedState(tp).resume();}
f1014
0
clearPreferredReadReplica
private Optional<Integer> kafkatest_f1022_0()
{    if (preferredReadReplica != null) {        int removedReplicaId = this.preferredReadReplica;        this.preferredReadReplica = null;        this.preferredReadReplicaExpireTimeMs = null;        return Optional.of(removedReplicaId);    } else {        return Optional.empty();    }}
f1022
0
reset
private void kafkatest_f1023_0(OffsetResetStrategy strategy)
{    transitionState(FetchStates.AWAIT_RESET, () -> {        this.resetStrategy = strategy;        this.nextRetryTimeMs = null;    });}
f1023
0
maybeValidatePosition
private boolean kafkatest_f1024_0(Metadata.LeaderAndEpoch currentLeaderAndEpoch)
{    if (this.fetchState.equals(FetchStates.AWAIT_RESET)) {        return false;    }    if (currentLeaderAndEpoch.equals(Metadata.LeaderAndEpoch.noLeaderOrEpoch())) {        // Ignore empty LeaderAndEpochs        return false;    }    if (position != null && !position.currentLeader.equals(currentLeaderAndEpoch)) {        FetchPosition newPosition = new FetchPosition(position.offset, position.offsetEpoch, currentLeaderAndEpoch);        validatePosition(newPosition);        preferredReadReplica = null;    }    return this.fetchState.equals(FetchStates.AWAIT_VALIDATION);}
f1024
0
hasValidPosition
private boolean kafkatest_f1032_0()
{    return fetchState.hasValidPosition();}
f1032
0
hasPosition
private boolean kafkatest_f1033_0()
{    return fetchState.hasPosition();}
f1033
0
isPaused
private boolean kafkatest_f1034_0()
{    return paused;}
f1034
0
highWatermark
private void kafkatest_f1042_0(Long highWatermark)
{    this.highWatermark = highWatermark;}
f1042
0
logStartOffset
private void kafkatest_f1043_0(Long logStartOffset)
{    this.logStartOffset = logStartOffset;}
f1043
0
lastStableOffset
private void kafkatest_f1044_0(Long lastStableOffset)
{    this.lastStableOffset = lastStableOffset;}
f1044
0
hasValidPosition
public boolean kafkatest_f1052_0()
{    return true;}
f1052
0
validTransitions
public Collection<FetchState> kafkatest_f1053_0()
{    return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET);}
f1053
0
hasPosition
public boolean kafkatest_f1054_0()
{    return true;}
f1054
0
buildMetrics
private static Metrics kafkatest_f1062_0(ConsumerConfig config, Time time, String clientId)
{    Map<String, String> metricsTags = Collections.singletonMap(CLIENT_ID_METRIC_TAG, clientId);    MetricConfig metricConfig = new MetricConfig().samples(config.getInt(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG)).timeWindow(config.getLong(ConsumerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS).recordLevel(Sensor.RecordingLevel.forName(config.getString(ConsumerConfig.METRICS_RECORDING_LEVEL_CONFIG))).tags(metricsTags);    List<MetricsReporter> reporters = config.getConfiguredInstances(ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, MetricsReporter.class, Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId));    reporters.add(new JmxReporter(JMX_PREFIX));    return new Metrics(metricConfig, reporters, time);}
f1062
0
assignment
public Set<TopicPartition> kafkatest_f1063_0()
{    acquireAndEnsureOpen();    try {        return Collections.unmodifiableSet(this.subscriptions.assignedPartitions());    } finally {        release();    }}
f1063
0
subscription
public Set<String> kafkatest_f1064_0()
{    acquireAndEnsureOpen();    try {        return Collections.unmodifiableSet(new HashSet<>(this.subscriptions.subscription()));    } finally {        release();    }}
f1064
0
poll
public ConsumerRecords<K, V> kafkatest_f1072_0(final Duration timeout)
{    return poll(time.timer(timeout), true);}
f1072
0
poll
private ConsumerRecords<K, V>f1073_1final Timer timer, final boolean includeMetadataInTimeout)
{    acquireAndEnsureOpen();    try {        if (this.subscriptions.hasNoSubscriptionOrUserAssignment()) {            throw new IllegalStateException("Consumer is not subscribed to any topics or assigned any partitions");        }        // poll for new data until the timeout expires        do {            client.maybeTriggerWakeup();            if (includeMetadataInTimeout) {                if (!updateAssignmentMetadataIfNeeded(timer)) {                    return ConsumerRecords.empty();                }            } else {                while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {                                    }            }            final Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);            if (!records.isEmpty()) {                // wakeups or any other errors to be triggered prior to returning the fetched records.                if (fetcher.sendFetches() > 0 || client.hasPendingRequests()) {                    client.pollNoWakeup();                }                return this.interceptors.onConsume(new ConsumerRecords<>(records));            }        } while (timer.notExpired());        return ConsumerRecords.empty();    } finally {        release();    }}
private ConsumerRecords<K, V>f1073
1
updateAssignmentMetadataIfNeeded
 boolean kafkatest_f1074_0(final Timer timer)
{    if (coordinator != null && !coordinator.poll(timer)) {        return false;    }    return updateFetchPositions(timer);}
f1074
0
commitAsync
public voidf1082_1final Map<TopicPartition, OffsetAndMetadata> offsets, OffsetCommitCallback callback)
{    acquireAndEnsureOpen();    try {        maybeThrowInvalidGroupIdException();                offsets.forEach(this::updateLastSeenEpochIfNewer);        coordinator.commitOffsetsAsync(new HashMap<>(offsets), callback);    } finally {        release();    }}
public voidf1082
1
seek
public voidf1083_1TopicPartition partition, long offset)
{    if (offset < 0)        throw new IllegalArgumentException("seek offset must not be a negative number");    acquireAndEnsureOpen();    try {                SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition(offset, // This will ensure we skip validation        Optional.empty(), this.metadata.leaderAndEpoch(partition));        this.subscriptions.seekUnvalidated(partition, newPosition);    } finally {        release();    }}
public voidf1083
1
seek
public voidf1084_1TopicPartition partition, OffsetAndMetadata offsetAndMetadata)
{    long offset = offsetAndMetadata.offset();    if (offset < 0) {        throw new IllegalArgumentException("seek offset must not be a negative number");    }    acquireAndEnsureOpen();    try {        if (offsetAndMetadata.leaderEpoch().isPresent()) {                    } else {                    }        Metadata.LeaderAndEpoch currentLeaderAndEpoch = this.metadata.leaderAndEpoch(partition);        SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition(offsetAndMetadata.offset(), offsetAndMetadata.leaderEpoch(), currentLeaderAndEpoch);        this.updateLastSeenEpochIfNewer(partition, offsetAndMetadata);        this.subscriptions.seekUnvalidated(partition, newPosition);    } finally {        release();    }}
public voidf1084
1
partitionsFor
public List<PartitionInfo> kafkatest_f1092_0(String topic)
{    return partitionsFor(topic, Duration.ofMillis(defaultApiTimeoutMs));}
f1092
0
partitionsFor
public List<PartitionInfo> kafkatest_f1093_0(String topic, Duration timeout)
{    acquireAndEnsureOpen();    try {        Cluster cluster = this.metadata.fetch();        List<PartitionInfo> parts = cluster.partitionsForTopic(topic);        if (!parts.isEmpty())            return parts;        Timer timer = time.timer(timeout);        Map<String, List<PartitionInfo>> topicMetadata = fetcher.getTopicMetadata(new MetadataRequest.Builder(Collections.singletonList(topic), metadata.allowAutoTopicCreation()), timer);        return topicMetadata.get(topic);    } finally {        release();    }}
f1093
0
listTopics
public Map<String, List<PartitionInfo>> kafkatest_f1094_0()
{    return listTopics(Duration.ofMillis(defaultApiTimeoutMs));}
f1094
0
beginningOffsets
public Map<TopicPartition, Long> kafkatest_f1102_0(Collection<TopicPartition> partitions, Duration timeout)
{    acquireAndEnsureOpen();    try {        return fetcher.beginningOffsets(partitions, time.timer(timeout));    } finally {        release();    }}
f1102
0
endOffsets
public Map<TopicPartition, Long> kafkatest_f1103_0(Collection<TopicPartition> partitions)
{    return endOffsets(partitions, Duration.ofMillis(requestTimeoutMs));}
f1103
0
endOffsets
public Map<TopicPartition, Long> kafkatest_f1104_0(Collection<TopicPartition> partitions, Duration timeout)
{    acquireAndEnsureOpen();    try {        return fetcher.endOffsets(partitions, time.timer(timeout));    } finally {        release();    }}
f1104
0
acquireAndEnsureOpen
private void kafkatest_f1112_0()
{    acquire();    if (this.closed) {        release();        throw new IllegalStateException("This consumer has already been closed.");    }}
f1112
0
acquire
private void kafkatest_f1113_0()
{    long threadId = Thread.currentThread().getId();    if (threadId != currentThread.get() && !currentThread.compareAndSet(NO_CURRENT_THREAD, threadId))        throw new ConcurrentModificationException("KafkaConsumer is not safe for multi-threaded access");    refcount.incrementAndGet();}
f1113
0
release
private void kafkatest_f1114_0()
{    if (refcount.decrementAndGet() == 0)        currentThread.set(NO_CURRENT_THREAD);}
f1114
0
subscription
public synchronized Set<String> kafkatest_f1122_0()
{    return this.subscriptions.subscription();}
f1122
0
subscribe
public synchronized void kafkatest_f1123_0(Collection<String> topics)
{    subscribe(topics, new NoOpConsumerRebalanceListener());}
f1123
0
subscribe
public synchronized void kafkatest_f1124_0(Pattern pattern, final ConsumerRebalanceListener listener)
{    ensureNotClosed();    committed.clear();    this.subscriptions.subscribe(pattern, listener);    Set<String> topicsToSubscribe = new HashSet<>();    for (String topic : partitions.keySet()) {        if (pattern.matcher(topic).matches() && !subscriptions.subscription().contains(topic))            topicsToSubscribe.add(topic);    }    ensureNotClosed();    this.subscriptions.subscribeFromPattern(topicsToSubscribe);    final Set<TopicPartition> assignedPartitions = new HashSet<>();    for (final String topic : topicsToSubscribe) {        for (final PartitionInfo info : this.partitions.get(topic)) {            assignedPartitions.add(new TopicPartition(topic, info.partition()));        }    }    subscriptions.assignFromSubscribed(assignedPartitions);}
f1124
0
setException
public synchronized void kafkatest_f1132_0(KafkaException exception)
{    setPollException(exception);}
f1132
0
setPollException
public synchronized void kafkatest_f1133_0(KafkaException exception)
{    this.pollException = exception;}
f1133
0
setOffsetsException
public synchronized void kafkatest_f1134_0(KafkaException exception)
{    this.offsetsException = exception;}
f1134
0
seek
public synchronized void kafkatest_f1142_0(TopicPartition partition, long offset)
{    ensureNotClosed();    subscriptions.seek(partition, offset);}
f1142
0
seek
public void kafkatest_f1143_0(TopicPartition partition, OffsetAndMetadata offsetAndMetadata)
{    ensureNotClosed();    subscriptions.seek(partition, offsetAndMetadata.offset());}
f1143
0
committed
public synchronized OffsetAndMetadata kafkatest_f1144_0(TopicPartition partition)
{    ensureNotClosed();    if (subscriptions.isAssigned(partition)) {        return committed.get(partition);    }    return new OffsetAndMetadata(0);}
f1144
0
updateEndOffsets
public synchronized void kafkatest_f1152_0(final Map<TopicPartition, Long> newOffsets)
{    innerUpdateEndOffsets(newOffsets, true);}
f1152
0
innerUpdateEndOffsets
private void kafkatest_f1153_0(final Map<TopicPartition, Long> newOffsets, final boolean replace)
{    for (final Map.Entry<TopicPartition, Long> entry : newOffsets.entrySet()) {        List<Long> offsets = endOffsets.get(entry.getKey());        if (replace || offsets == null) {            offsets = new ArrayList<>();        }        offsets.add(entry.getValue());        endOffsets.put(entry.getKey(), offsets);    }}
f1153
0
metrics
public synchronized Map<MetricName, ? extends Metric> kafkatest_f1154_0()
{    ensureNotClosed();    return Collections.emptyMap();}
f1154
0
endOffsets
public synchronized Map<TopicPartition, Long> kafkatest_f1162_0(Collection<TopicPartition> partitions)
{    if (offsetsException != null) {        RuntimeException exception = this.offsetsException;        this.offsetsException = null;        throw exception;    }    Map<TopicPartition, Long> result = new HashMap<>();    for (TopicPartition tp : partitions) {        Long endOffset = getEndOffset(endOffsets.get(tp));        if (endOffset == null)            throw new IllegalStateException("The partition " + tp + " does not have an end offset.");        result.put(tp, endOffset);    }    return result;}
f1162
0
close
public synchronized void kafkatest_f1163_0()
{    close(KafkaConsumer.DEFAULT_CLOSE_TIMEOUT_MS, TimeUnit.MILLISECONDS);}
f1163
0
close
public synchronized void kafkatest_f1164_0(long timeout, TimeUnit unit)
{    ensureNotClosed();    this.closed = true;}
f1164
0
resetOffsetPosition
private void kafkatest_f1172_0(TopicPartition tp)
{    OffsetResetStrategy strategy = subscriptions.resetStrategy(tp);    Long offset;    if (strategy == OffsetResetStrategy.EARLIEST) {        offset = beginningOffsets.get(tp);        if (offset == null)            throw new IllegalStateException("MockConsumer didn't have beginning offset specified, but tried to seek to beginning");    } else if (strategy == OffsetResetStrategy.LATEST) {        offset = getEndOffset(endOffsets.get(tp));        if (offset == null)            throw new IllegalStateException("MockConsumer didn't have end offset specified, but tried to seek to end");    } else {        throw new NoOffsetForPartitionException(tp);    }    seek(tp, offset);}
f1172
0
getEndOffset
private Long kafkatest_f1173_0(List<Long> offsets)
{    if (offsets == null || offsets.isEmpty()) {        return null;    }    return offsets.size() > 1 ? offsets.remove(0) : offsets.get(0);}
f1173
0
partitionsFor
public List<PartitionInfo> kafkatest_f1174_0(String topic, Duration timeout)
{    return partitionsFor(topic);}
f1174
0
offset
public long kafkatest_f1182_0()
{    return offset;}
f1182
0
metadata
public String kafkatest_f1183_0()
{    return metadata;}
f1183
0
leaderEpoch
public Optional<Integer> kafkatest_f1184_0()
{    return Optional.ofNullable(leaderEpoch);}
f1184
0
equals
public boolean kafkatest_f1192_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    OffsetAndTimestamp that = (OffsetAndTimestamp) o;    return timestamp == that.timestamp && offset == that.offset && Objects.equals(leaderEpoch, that.leaderEpoch);}
f1192
0
hashCode
public int kafkatest_f1193_0()
{    return Objects.hash(timestamp, offset, leaderEpoch);}
f1193
0
offsetOutOfRangePartitions
public Map<TopicPartition, Long> kafkatest_f1194_0()
{    return offsetOutOfRangePartitions;}
f1194
0
name
public String kafkatest_f1202_0()
{    return "roundrobin";}
f1202
0
name
public String kafkatest_f1203_0()
{    return "sticky";}
f1203
0
onAssignment
public void kafkatest_f1204_0(Assignment assignment, ConsumerGroupMetadata metadata)
{    memberAssignment = assignment.partitions();    this.generation = metadata.generationId();}
f1204
0
metadata
public FetchMetadata kafkatest_f1212_0()
{    return metadata;}
f1212
0
toString
public String kafkatest_f1213_0()
{    if (metadata.isFull()) {        StringBuilder bld = new StringBuilder("FullFetchRequest(");        String prefix = "";        for (TopicPartition partition : toSend.keySet()) {            bld.append(prefix);            bld.append(partition);            prefix = ", ";        }        bld.append(")");        return bld.toString();    } else {        StringBuilder bld = new StringBuilder("IncrementalFetchRequest(toSend=(");        String prefix = "";        for (TopicPartition partition : toSend.keySet()) {            bld.append(prefix);            bld.append(partition);            prefix = ", ";        }        bld.append("), toForget=(");        prefix = "";        for (TopicPartition partition : toForget) {            bld.append(prefix);            bld.append(partition);            prefix = ", ";        }        bld.append("), implied=(");        prefix = "";        for (TopicPartition partition : sessionPartitions.keySet()) {            if (!toSend.containsKey(partition)) {                bld.append(prefix);                bld.append(partition);                prefix = ", ";            }        }        bld.append("))");        return bld.toString();    }}
f1213
0
add
public void kafkatest_f1214_0(TopicPartition topicPartition, PartitionData data)
{    next.put(topicPartition, data);}
f1214
0
handleResponse
public booleanf1222_1FetchResponse<?> response)
{    if (response.error() != Errors.NONE) {                if (response.error() == Errors.FETCH_SESSION_ID_NOT_FOUND) {            nextMetadata = FetchMetadata.INITIAL;        } else {            nextMetadata = nextMetadata.nextCloseExisting();        }        return false;    } else if (nextMetadata.isFull()) {        String problem = verifyFullFetchResponsePartitions(response);        if (problem != null) {                        nextMetadata = FetchMetadata.INITIAL;            return false;        } else if (response.sessionId() == INVALID_SESSION_ID) {                        nextMetadata = FetchMetadata.INITIAL;            return true;        } else {            // The server created a new incremental fetch session.                        nextMetadata = FetchMetadata.newIncremental(response.sessionId());            return true;        }    } else {        String problem = verifyIncrementalFetchResponsePartitions(response);        if (problem != null) {                        nextMetadata = nextMetadata.nextCloseExisting();            return false;        } else if (response.sessionId() == INVALID_SESSION_ID) {            // The incremental fetch session was closed by the server.                        nextMetadata = FetchMetadata.INITIAL;            return true;        } else {            // The incremental fetch session was continued by the server.                        nextMetadata = nextMetadata.nextIncremental();            return true;        }    }}
public booleanf1222
1
handleError
public voidf1223_1Throwable t)
{        nextMetadata = nextMetadata.nextCloseExisting();}
public voidf1223
1
toString
public String kafkatest_f1224_0()
{    return super.toString().toLowerCase(Locale.ROOT);}
f1224
0
isEmpty
public boolean kafkatest_f1232_0(String node)
{    Deque<NetworkClient.InFlightRequest> queue = requests.get(node);    return queue == null || queue.isEmpty();}
f1232
0
count
public int kafkatest_f1233_0()
{    return inFlightRequestCount.get();}
f1233
0
isEmpty
public boolean kafkatest_f1234_0()
{    for (Deque<NetworkClient.InFlightRequest> deque : this.requests.values()) {        if (!deque.isEmpty())            return false;    }    return true;}
f1234
0
handleDisconnection
public void kafkatest_f1242_0(String destination)
{// Do nothing}
f1242
0
handleFatalException
public voidf1243_1KafkaException exception)
{    // We don't fail the broker on failures, but there should be sufficient information in the logs indicating the reason    // for failure.    }
public voidf1243
1
handleCompletedMetadataResponse
public void kafkatest_f1244_0(RequestHeader requestHeader, long now, MetadataResponse response)
{// Do nothing}
f1244
0
lastSeenLeaderEpoch
public Optional<Integer> kafkatest_f1253_0(TopicPartition topicPartition)
{    return Optional.ofNullable(lastSeenLeaderEpochs.get(topicPartition));}
f1253
0
updateLastSeenEpoch
private synchronized booleanf1254_1TopicPartition topicPartition, int epoch, Predicate<Integer> epochTest, boolean setRequestUpdateFlag)
{    Integer oldEpoch = lastSeenLeaderEpochs.get(topicPartition);    log.trace("Determining if we should replace existing epoch {} with new epoch {}", oldEpoch, epoch);    if (oldEpoch == null || epochTest.test(oldEpoch)) {                lastSeenLeaderEpochs.put(topicPartition, epoch);        if (setRequestUpdateFlag) {            this.needUpdate = true;        }        return true;    } else {                return false;    }}
private synchronized booleanf1254
1
updateRequested
public synchronized boolean kafkatest_f1255_0()
{    return this.needUpdate;}
f1255
0
handleMetadataResponse
private MetadataCachef1263_1MetadataResponse metadataResponse, Predicate<MetadataResponse.TopicMetadata> topicsToRetain)
{    Set<String> internalTopics = new HashSet<>();    List<MetadataCache.PartitionInfoAndEpoch> partitions = new ArrayList<>();    for (MetadataResponse.TopicMetadata metadata : metadataResponse.topicMetadata()) {        if (!topicsToRetain.test(metadata))            continue;        if (metadata.error() == Errors.NONE) {            if (metadata.isInternal())                internalTopics.add(metadata.topic());            for (MetadataResponse.PartitionMetadata partitionMetadata : metadata.partitionMetadata()) {                // Even if the partition's metadata includes an error, we need to handle the update to catch new epochs                updatePartitionInfo(metadata.topic(), partitionMetadata, partitionInfo -> {                    int epoch = partitionMetadata.leaderEpoch().orElse(RecordBatch.NO_PARTITION_LEADER_EPOCH);                    partitions.add(new MetadataCache.PartitionInfoAndEpoch(partitionInfo, epoch));                });                if (partitionMetadata.error().exception() instanceof InvalidMetadataException) {                                        requestUpdate();                }            }        } else if (metadata.error().exception() instanceof InvalidMetadataException) {                        requestUpdate();        }    }    return new MetadataCache(metadataResponse.clusterId(), new ArrayList<>(metadataResponse.brokers()), partitions, metadataResponse.topicsByError(Errors.TOPIC_AUTHORIZATION_FAILED), metadataResponse.topicsByError(Errors.INVALID_TOPIC_EXCEPTION), internalTopics, metadataResponse.controller());}
private MetadataCachef1263
1
updatePartitionInfo
private void kafkatest_f1264_0(String topic, MetadataResponse.PartitionMetadata partitionMetadata, Consumer<PartitionInfo> partitionInfoConsumer)
{    TopicPartition tp = new TopicPartition(topic, partitionMetadata.partition());    if (partitionMetadata.leaderEpoch().isPresent()) {        int newEpoch = partitionMetadata.leaderEpoch().get();        // If the received leader epoch is at least the same as the previous one, update the metadata        if (updateLastSeenEpoch(tp, newEpoch, oldEpoch -> newEpoch >= oldEpoch, false)) {            partitionInfoConsumer.accept(MetadataResponse.partitionMetaToInfo(topic, partitionMetadata));        } else {            // Otherwise ignore the new metadata and use the previously cached info            PartitionInfo previousInfo = cache.cluster().partition(tp);            if (previousInfo != null) {                partitionInfoConsumer.accept(previousInfo);            }        }    } else {        // Handle old cluster formats as well as error responses where leader and epoch are missing        lastSeenLeaderEpochs.remove(tp);        partitionInfoConsumer.accept(MetadataResponse.partitionMetaToInfo(topic, partitionMetadata));    }}
f1264
0
maybeThrowAnyException
public synchronized void kafkatest_f1265_0()
{    clearErrorsAndMaybeThrowException(this::recoverableException);}
f1265
0
updateVersion
public synchronized int kafkatest_f1273_0()
{    return this.updateVersion;}
f1273
0
lastSuccessfulUpdate
public synchronized long kafkatest_f1274_0()
{    return this.lastSuccessfulRefreshMs;}
f1274
0
close
public synchronized void kafkatest_f1275_0()
{    this.isClosed = true;}
f1275
0
equals
public boolean kafkatest_f1283_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    LeaderAndEpoch that = (LeaderAndEpoch) o;    if (!leader.equals(that.leader))        return false;    return epoch.equals(that.epoch);}
f1283
0
hashCode
public int kafkatest_f1284_0()
{    int result = leader.hashCode();    result = 31 * result + epoch.hashCode();    return result;}
f1284
0
toString
public String kafkatest_f1285_0()
{    return "LeaderAndEpoch{" + "leader=" + leader + ", epoch=" + epoch.map(Number::toString).orElse("absent") + '}';}
f1285
0
partitionInfo
public PartitionInfo kafkatest_f1293_0()
{    return partitionInfo;}
f1293
0
epoch
public int kafkatest_f1294_0()
{    return epoch;}
f1294
0
equals
public boolean kafkatest_f1295_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    PartitionInfoAndEpoch that = (PartitionInfoAndEpoch) o;    return epoch == that.epoch && Objects.equals(partitionInfo, that.partitionInfo);}
f1295
0
throttleDelayMs
public long kafkatest_f1303_0(Node node, long now)
{    return connectionStates.throttleDelayMs(node.idString(), now);}
f1303
0
pollDelayMs
public long kafkatest_f1304_0(Node node, long now)
{    return connectionStates.pollDelayMs(node.idString(), now);}
f1304
0
connectionFailed
public boolean kafkatest_f1305_0(Node node)
{    return connectionStates.isDisconnected(node.idString());}
f1305
0
poll
public List<ClientResponse>f1313_1long timeout, long now)
{    ensureActive();    if (!abortedSends.isEmpty()) {        // If there are aborted sends because of unsupported version exceptions or disconnects,        // handle them immediately without waiting for Selector#poll.        List<ClientResponse> responses = new ArrayList<>();        handleAbortedSends(responses);        completeResponses(responses);        return responses;    }    long metadataTimeout = metadataUpdater.maybeUpdate(now);    try {        this.selector.poll(Utils.min(timeout, metadataTimeout, defaultRequestTimeoutMs));    } catch (IOException e) {            }    // process completed actions    long updatedNow = this.time.milliseconds();    List<ClientResponse> responses = new ArrayList<>();    handleCompletedSends(responses, updatedNow);    handleCompletedReceives(responses, updatedNow);    handleDisconnections(responses, updatedNow);    handleConnections();    handleInitiateApiVersionRequests(updatedNow);    handleTimedOutRequests(responses, updatedNow);    completeResponses(responses);    return responses;}
public List<ClientResponse>f1313
1
completeResponses
private voidf1314_1List<ClientResponse> responses)
{    for (ClientResponse response : responses) {        try {            response.onComplete();        } catch (Exception e) {                    }    }}
private voidf1314
1
inFlightRequestCount
public int kafkatest_f1315_0()
{    return this.inFlightRequests.count();}
f1315
0
ensureActive
private void kafkatest_f1323_0()
{    if (!active())        throw new DisconnectException("NetworkClient is no longer active, state is " + state);}
f1323
0
close
public voidf1324_1)
{    state.compareAndSet(State.ACTIVE, State.CLOSING);    if (state.compareAndSet(State.CLOSING, State.CLOSED)) {        this.selector.close();        this.metadataUpdater.close();    } else {            }}
public voidf1324
1
leastLoadedNode
public Node kafkatest_f1325_0(long now)
{    List<Node> nodes = this.metadataUpdater.fetchNodes();    if (nodes.isEmpty())        throw new IllegalStateException("There are no nodes in the Kafka cluster");    int inflight = Integer.MAX_VALUE;    Node foundConnecting = null;    Node foundCanConnect = null;    Node foundReady = null;    int offset = this.randOffset.nextInt(nodes.size());    for (int i = 0; i < nodes.size(); i++) {        int idx = (offset + i) % nodes.size();        Node node = nodes.get(idx);        if (canSendRequest(node.idString(), now)) {            int currInflight = this.inFlightRequests.count(node.idString());            if (currInflight == 0) {                // if we find an established connection with no in-flight requests we can stop right away                log.trace("Found least loaded node {} connected with no in-flight requests", node);                return node;            } else if (currInflight < inflight) {                // otherwise if this is the best we have found so far, record that                inflight = currInflight;                foundReady = node;            }        } else if (connectionStates.isPreparingConnection(node.idString())) {            foundConnecting = node;        } else if (canConnect(node, now)) {            foundCanConnect = node;        } else {            log.trace("Removing node {} from least loaded node selection since it is neither ready " + "for sending or connecting", node);        }    }    // which are being established before connecting to new nodes.    if (foundReady != null) {        log.trace("Found least loaded node {} with {} inflight requests", foundReady, inflight);        return foundReady;    } else if (foundConnecting != null) {        log.trace("Found least loaded connecting node {}", foundConnecting);        return foundConnecting;    } else if (foundCanConnect != null) {        log.trace("Found least loaded node {} with no active connection", foundCanConnect);        return foundCanConnect;    } else {        log.trace("Least loaded node selection failed to find an available node");        return null;    }}
f1325
0
handleCompletedReceives
private void kafkatest_f1333_0(List<ClientResponse> responses, long now)
{    for (NetworkReceive receive : this.selector.completedReceives()) {        String source = receive.source();        InFlightRequest req = inFlightRequests.completeNext(source);        Struct responseStruct = parseStructMaybeUpdateThrottleTimeMetrics(receive.payload(), req.header, throttleTimeSensor, now);        if (log.isTraceEnabled()) {            log.trace("Completed receive from node {} for {} with correlation id {}, received {}", req.destination, req.header.apiKey(), req.header.correlationId(), responseStruct);        }        // If the received response includes a throttle delay, throttle the connection.        AbstractResponse body = AbstractResponse.parseResponse(req.header.apiKey(), responseStruct, req.header.apiVersion());        maybeThrottle(body, req.header.apiVersion(), req.destination, now);        if (req.isInternalRequest && body instanceof MetadataResponse)            metadataUpdater.handleCompletedMetadataResponse(req.header, now, (MetadataResponse) body);        else if (req.isInternalRequest && body instanceof ApiVersionsResponse)            handleApiVersionsResponse(responses, req, now, (ApiVersionsResponse) body);        else            responses.add(req.completed(body, now));    }}
f1333
0
handleApiVersionsResponse
private voidf1334_1List<ClientResponse> responses, InFlightRequest req, long now, ApiVersionsResponse apiVersionsResponse)
{    final String node = req.destination;    if (apiVersionsResponse.error() != Errors.NONE) {        if (req.request.version() == 0 || apiVersionsResponse.error() != Errors.UNSUPPORTED_VERSION) {                        this.selector.close(node);            processDisconnection(responses, node, now, ChannelState.LOCAL_CLOSE);        } else {            nodesNeedingApiVersionsFetch.put(node, new ApiVersionsRequest.Builder((short) 0));        }        return;    }    NodeApiVersions nodeVersionInfo = new NodeApiVersions(apiVersionsResponse.apiVersions());    apiVersions.update(node, nodeVersionInfo);    this.connectionStates.ready(node);    }
private voidf1334
1
handleDisconnections
private voidf1335_1List<ClientResponse> responses, long now)
{    for (Map.Entry<String, ChannelState> entry : this.selector.disconnected().entrySet()) {        String node = entry.getKey();                processDisconnection(responses, node, now, entry.getValue());    }    // we got a disconnect so we should probably refresh our metadata and see if that broker is dead    if (this.selector.disconnected().size() > 0)        metadataUpdater.requestUpdate();}
private voidf1335
1
maybeUpdate
public longf1343_1long now)
{    // should we update our metadata?    long timeToNextMetadataUpdate = metadata.timeToNextUpdate(now);    long waitForMetadataFetch = hasFetchInProgress() ? defaultRequestTimeoutMs : 0;    long metadataTimeout = Math.max(timeToNextMetadataUpdate, waitForMetadataFetch);    if (metadataTimeout > 0) {        return metadataTimeout;    }    // Beware that the behavior of this method and the computation of timeouts for poll() are    // highly dependent on the behavior of leastLoadedNode.    Node node = leastLoadedNode(now);    if (node == null) {                return reconnectBackoffMs;    }    return maybeUpdate(now, node);}
public longf1343
1
handleDisconnection
public voidf1344_1String destination)
{    Cluster cluster = metadata.fetch();    // before metadata could be obtained.    if (cluster.isBootstrapConfigured()) {        int nodeId = Integer.parseInt(destination);        Node node = cluster.nodeById(nodeId);        if (node != null)                }    inProgressRequestVersion = null;}
public voidf1344
1
handleFatalException
public void kafkatest_f1345_0(KafkaException fatalException)
{    if (metadata.updateRequested())        metadata.failedUpdate(time.milliseconds(), fatalException);    inProgressRequestVersion = null;}
f1345
0
discoverBrokerVersions
public boolean kafkatest_f1353_0()
{    return discoverBrokerVersions;}
f1353
0
completed
public ClientResponse kafkatest_f1354_0(AbstractResponse response, long timeMs)
{    return new ClientResponse(header, callback, destination, createdTimeMs, timeMs, false, null, null, response);}
f1354
0
disconnected
public ClientResponse kafkatest_f1355_0(long timeMs, AuthenticationException authenticationException)
{    return new ClientResponse(header, callback, destination, createdTimeMs, timeMs, true, null, authenticationException, null);}
f1355
0
latestUsableVersion
public short kafkatest_f1363_0(ApiKeys apiKey, short oldestAllowedVersion, short latestAllowedVersion)
{    ApiVersion usableVersion = supportedVersions.get(apiKey);    if (usableVersion == null)        throw new UnsupportedVersionException("The broker does not support " + apiKey);    return latestUsableVersion(apiKey, usableVersion, oldestAllowedVersion, latestAllowedVersion);}
f1363
0
latestUsableVersion
private short kafkatest_f1364_0(ApiKeys apiKey, ApiVersion supportedVersions, short minAllowedVersion, short maxAllowedVersion)
{    short minVersion = (short) Math.max(minAllowedVersion, supportedVersions.minVersion);    short maxVersion = (short) Math.min(maxAllowedVersion, supportedVersions.maxVersion);    if (minVersion > maxVersion)        throw new UnsupportedVersionException("The broker does not support " + apiKey + " with version in range [" + minAllowedVersion + "," + maxAllowedVersion + "]. The supported" + " range is [" + supportedVersions.minVersion + "," + supportedVersions.maxVersion + "].");    return maxVersion;}
f1364
0
toString
public String kafkatest_f1365_0()
{    return toString(false);}
f1365
0
freeUp
private void kafkatest_f1373_0(int size)
{    while (!this.free.isEmpty() && this.nonPooledAvailableMemory < size) this.nonPooledAvailableMemory += this.free.pollLast().capacity();}
f1373
0
deallocate
public void kafkatest_f1374_0(ByteBuffer buffer, int size)
{    lock.lock();    try {        if (size == this.poolableSize && size == buffer.capacity()) {            buffer.clear();            this.free.add(buffer);        } else {            this.nonPooledAvailableMemory += size;        }        Condition moreMem = this.waiters.peekFirst();        if (moreMem != null)            moreMem.signal();    } finally {        lock.unlock();    }}
f1374
0
deallocate
public void kafkatest_f1375_0(ByteBuffer buffer)
{    deallocate(buffer, buffer.capacity());}
f1375
0
partition
public int kafkatest_f1384_0(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster)
{    if (keyBytes == null) {        return stickyPartitionCache.partition(topic, cluster);    }    List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);    int numPartitions = partitions.size();    // hash the keyBytes to choose a partition    return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;}
f1384
0
onNewBatch
public void kafkatest_f1386_0(String topic, Cluster cluster, int prevPartition)
{    stickyPartitionCache.nextPartition(topic, cluster, prevPartition);}
f1386
0
onCompletion
public voidf1387_1RecordMetadata metadata, Exception e)
{    if (e != null) {        String keyString = (key == null) ? "null" : logAsString ? new String(key, StandardCharsets.UTF_8) : key.length + " bytes";        String valueString = (valueLength == -1) ? "null" : logAsString ? new String(value, StandardCharsets.UTF_8) : valueLength + " bytes";            }}
public voidf1387
1
value
 RecordMetadata kafkatest_f1395_0()
{    if (nextRecordMetadata != null)        return nextRecordMetadata.value();    return new RecordMetadata(result.topicPartition(), this.result.baseOffset(), this.relativeOffset, timestamp(), this.checksum, this.serializedKeySize, this.serializedValueSize);}
f1395
0
timestamp
private long kafkatest_f1396_0()
{    return result.hasLogAppendTime() ? result.logAppendTime() : createTimestamp;}
f1396
0
isDone
public boolean kafkatest_f1397_0()
{    if (nextRecordMetadata != null)        return nextRecordMetadata.isDone();    return this.result.completed();}
f1397
0
isDone
public boolean kafkatest_f1405_0()
{    return finalState() != null;}
f1405
0
done
public booleanf1406_1long baseOffset, long logAppendTime, RuntimeException exception)
{    final FinalState tryFinalState = (exception == null) ? FinalState.SUCCEEDED : FinalState.FAILED;    if (tryFinalState == FinalState.SUCCEEDED) {        log.trace("Successfully produced messages to {} with base offset {}.", topicPartition, baseOffset);    } else {        log.trace("Failed to produce messages to {} with base offset {}.", topicPartition, baseOffset, exception);    }    if (this.finalState.compareAndSet(null, tryFinalState)) {        completeFutureAndFireCallbacks(baseOffset, logAppendTime, exception);        return true;    }    if (this.finalState.get() != FinalState.SUCCEEDED) {        if (tryFinalState == FinalState.SUCCEEDED) {            // Log if a previously unsuccessful batch succeeded later on.                    } else {            // FAILED --> FAILED and ABORTED --> FAILED transitions are ignored.                    }    } else {        // A SUCCESSFUL batch must not attempt another state change.        throw new IllegalStateException("A " + this.finalState.get() + " batch must not attempt another state change to " + tryFinalState);    }    return false;}
public booleanf1406
1
completeFutureAndFireCallbacks
private voidf1407_1long baseOffset, long logAppendTime, RuntimeException exception)
{    // Set the future before invoking the callbacks as we rely on its state for the `onCompletion` call    produceFuture.set(baseOffset, logAppendTime, exception);    // execute callbacks    for (Thunk thunk : thunks) {        try {            if (exception == null) {                RecordMetadata metadata = thunk.future.value();                if (thunk.callback != null)                    thunk.callback.onCompletion(metadata, null);            } else {                if (thunk.callback != null)                    thunk.callback.onCompletion(null, exception);            }        } catch (Exception e) {                    }    }    produceFuture.done();}
private voidf1407
1
reenqueued
 void kafkatest_f1415_0(long now)
{    attempts.getAndIncrement();    lastAttemptMs = Math.max(lastAppendTime, now);    lastAppendTime = Math.max(lastAppendTime, now);    retry = true;}
f1415
0
queueTimeMs
 long kafkatest_f1416_0()
{    return drainedMs - createdMs;}
f1416
0
waitedTimeMs
 long kafkatest_f1417_0(long nowMs)
{    return Math.max(0, nowMs - lastAttemptMs);}
f1417
0
setProducerState
public void kafkatest_f1425_0(ProducerIdAndEpoch producerIdAndEpoch, int baseSequence, boolean isTransactional)
{    recordsBuilder.setProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional);}
f1425
0
resetProducerState
public void kafkatest_f1426_0(ProducerIdAndEpoch producerIdAndEpoch, int baseSequence, boolean isTransactional)
{    reopened = true;    recordsBuilder.reopenAndRewriteProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional);}
f1426
0
closeForRecordAppends
public void kafkatest_f1427_0()
{    recordsBuilder.closeForRecordAppends();}
f1427
0
producerId
public long kafkatest_f1435_0()
{    return recordsBuilder.producerId();}
f1435
0
producerEpoch
public short kafkatest_f1436_0()
{    return recordsBuilder.producerEpoch();}
f1436
0
baseSequence
public int kafkatest_f1437_0()
{    return recordsBuilder.baseSequence();}
f1437
0
baseOffset
public long kafkatest_f1445_0()
{    return baseOffset;}
f1445
0
hasLogAppendTime
public boolean kafkatest_f1446_0()
{    return logAppendTime != RecordBatch.NO_TIMESTAMP;}
f1446
0
logAppendTime
public long kafkatest_f1447_0()
{    return logAppendTime;}
f1447
0
onSendError
public voidf1455_1ProducerRecord<K, V> record, TopicPartition interceptTopicPartition, Exception exception)
{    for (ProducerInterceptor<K, V> interceptor : this.interceptors) {        try {            if (record == null && interceptTopicPartition == null) {                interceptor.onAcknowledgement(null, exception);            } else {                if (interceptTopicPartition == null) {                    interceptTopicPartition = new TopicPartition(record.topic(), record.partition() == null ? RecordMetadata.UNKNOWN_PARTITION : record.partition());                }                interceptor.onAcknowledgement(new RecordMetadata(interceptTopicPartition, -1, -1, RecordBatch.NO_TIMESTAMP, Long.valueOf(-1L), -1, -1), exception);            }        } catch (Exception e) {            // do not propagate interceptor exceptions, just log                    }    }}
public voidf1455
1
close
public voidf1456_1)
{    for (ProducerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptor.close();        } catch (Exception e) {                    }    }}
public voidf1456
1
newMetadataRequestBuilder
public synchronized MetadataRequest.Builder kafkatest_f1457_0()
{    return new MetadataRequest.Builder(new ArrayList<>(topics.keySet()), true);}
f1457
0
close
public synchronized void kafkatest_f1465_0()
{    super.close();    notifyAll();}
f1465
0
getAllTemplates
private List<MetricNameTemplate> kafkatest_f1466_0()
{    List<MetricNameTemplate> l = new ArrayList<>(this.senderMetrics.allTemplates());    return l;}
f1466
0
main
public static void kafkatest_f1467_0(String[] args)
{    Map<String, String> metricTags = Collections.singletonMap("client-id", "client-id");    MetricConfig metricConfig = new MetricConfig().tags(metricTags);    Metrics metrics = new Metrics(metricConfig);    ProducerMetrics metricsRegistry = new ProducerMetrics(metrics);    System.out.println(Metrics.toHtmlTable("kafka.producer", metricsRegistry.getAllTemplates()));}
f1467
0
isMuted
private boolean kafkatest_f1475_0(TopicPartition tp, long now)
{    // Take care to avoid unnecessary map look-ups because this method is a hotspot if producing to a    // large number of partitions    Long throttleUntilTime = muted.get(tp);    if (throttleUntilTime == null)        return false;    if (now >= throttleUntilTime) {        muted.remove(tp);        return false;    }    return true;}
f1475
0
resetNextBatchExpiryTime
public void kafkatest_f1476_0()
{    nextBatchExpiryTimeMs = Long.MAX_VALUE;}
f1476
0
maybeUpdateNextBatchExpiryTime
public voidf1477_1ProducerBatch batch)
{    if (batch.createdMs + deliveryTimeoutMs > 0) {        // the non-negative check is to guard us against potential overflow due to setting        // a large value for deliveryTimeoutMs        nextBatchExpiryTimeMs = Math.min(nextBatchExpiryTimeMs, batch.createdMs + deliveryTimeoutMs);    } else {            }}
public voidf1477
1
shouldStopDrainBatchesForPartition
private boolean kafkatest_f1485_0(ProducerBatch first, TopicPartition tp)
{    ProducerIdAndEpoch producerIdAndEpoch = null;    if (transactionManager != null) {        if (!transactionManager.isSendToPartitionAllowed(tp))            return true;        producerIdAndEpoch = transactionManager.producerIdAndEpoch();        if (!producerIdAndEpoch.isValid())            // we cannot send the batch until we have refreshed the producer id            return true;        if (!first.hasSequence() && transactionManager.hasUnresolvedSequence(first.topicPartition))            // on the client after being sent to the broker at least once.            return true;        int firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);        if (firstInFlightSequence != RecordBatch.NO_SEQUENCE && first.hasSequence() && first.baseSequence() != firstInFlightSequence)            // a fatal broker error). This effectively reduces our in flight request count to 1.            return true;    }    return false;}
f1485
0
drainBatchesForOneNode
private List<ProducerBatch>f1486_1Cluster cluster, Node node, int maxSize, long now)
{    int size = 0;    List<PartitionInfo> parts = cluster.partitionsForNode(node.id());    List<ProducerBatch> ready = new ArrayList<>();    /* to make starvation less likely this loop doesn't start at 0 */    int start = drainIndex = drainIndex % parts.size();    do {        PartitionInfo part = parts.get(drainIndex);        TopicPartition tp = new TopicPartition(part.topic(), part.partition());        this.drainIndex = (this.drainIndex + 1) % parts.size();        // Only proceed if the partition has no in-flight batches.        if (isMuted(tp, now))            continue;        Deque<ProducerBatch> deque = getDeque(tp);        if (deque == null)            continue;        synchronized (deque) {            // invariant: !isMuted(tp,now) && deque != null            ProducerBatch first = deque.peekFirst();            if (first == null)                continue;            // first != null            boolean backoff = first.attempts() > 0 && first.waitedTimeMs(now) < retryBackoffMs;            // Only drain the batch if it is not during backoff period.            if (backoff)                continue;            if (size + first.estimatedSizeInBytes() > maxSize && !ready.isEmpty()) {                // compression; in this case we will still eventually send this batch in a single request                break;            } else {                if (shouldStopDrainBatchesForPartition(first, tp))                    break;                boolean isTransactional = transactionManager != null && transactionManager.isTransactional();                ProducerIdAndEpoch producerIdAndEpoch = transactionManager != null ? transactionManager.producerIdAndEpoch() : null;                ProducerBatch batch = deque.pollFirst();                if (producerIdAndEpoch != null && !batch.hasSequence()) {                    // If the batch already has an assigned sequence, then we should not change the producer id and                    // sequence number, since this may introduce duplicates. In particular, the previous attempt                    // may actually have been accepted, and if we change the producer id and sequence here, this                    // attempt will also be accepted, causing a duplicate.                    //                     // Additionally, we update the next sequence number bound for the partition, and also have                    // the transaction manager track the batch so as to ensure that sequence ordering is maintained                    // even if we receive out of order responses.                    batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);                    transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount);                                        transactionManager.addInFlightBatch(batch);                }                batch.close();                size += batch.records().sizeInBytes();                ready.add(batch);                batch.drained(now);            }        }    } while (start != drainIndex);    return ready;}
private List<ProducerBatch>f1486
1
drain
public Map<Integer, List<ProducerBatch>> kafkatest_f1487_0(Cluster cluster, Set<Node> nodes, int maxSize, long now)
{    if (nodes.isEmpty())        return Collections.emptyMap();    Map<Integer, List<ProducerBatch>> batches = new HashMap<>();    for (Node node : nodes) {        List<ProducerBatch> ready = drainBatchesForOneNode(cluster, node, maxSize, now);        batches.put(node.id(), ready);    }    return batches;}
f1487
0
beginFlush
public void kafkatest_f1495_0()
{    this.flushesInProgress.getAndIncrement();}
f1495
0
appendsInProgress
private boolean kafkatest_f1496_0()
{    return appendsInProgress.get() > 0;}
f1496
0
awaitFlushCompletion
public void kafkatest_f1497_0() throws InterruptedException
{    try {        for (ProducerBatch batch : this.incomplete.copyAll()) batch.produceFuture.await();    } finally {        this.flushesInProgress.decrementAndGet();    }}
f1497
0
close
public void kafkatest_f1505_0()
{    this.closed = true;}
f1505
0
inFlightBatches
public List<ProducerBatch> kafkatest_f1506_0(TopicPartition tp)
{    return inFlightBatches.containsKey(tp) ? inFlightBatches.get(tp) : new ArrayList<>();}
f1506
0
maybeRemoveFromInflightBatches
private void kafkatest_f1507_0(ProducerBatch batch)
{    List<ProducerBatch> batches = inFlightBatches.get(batch.topicPartition);    if (batches != null) {        batches.remove(batch);        if (batches.isEmpty()) {            inFlightBatches.remove(batch.topicPartition);        }    }}
f1507
0
sendProducerData
private longf1515_1long now)
{    Cluster cluster = metadata.fetch();    // get the list of partitions with data ready to send    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);    // if there are any partitions whose leaders are not known yet, force metadata update    if (!result.unknownLeaderTopics.isEmpty()) {        // and request metadata update, since there are messages to send to the topic.        for (String topic : result.unknownLeaderTopics) this.metadata.add(topic);                this.metadata.requestUpdate();    }    // remove any nodes we aren't ready to send to    Iterator<Node> iter = result.readyNodes.iterator();    long notReadyTimeout = Long.MAX_VALUE;    while (iter.hasNext()) {        Node node = iter.next();        if (!this.client.ready(node, now)) {            iter.remove();            notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));        }    }    // create produce requests    Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);    addToInflightBatches(batches);    if (guaranteeMessageOrder) {        // Mute all the partitions drained        for (List<ProducerBatch> batchList : batches.values()) {            for (ProducerBatch batch : batchList) this.accumulator.mutePartition(batch.topicPartition);        }    }    accumulator.resetNextBatchExpiryTime();    List<ProducerBatch> expiredInflightBatches = getExpiredInflightBatches(now);    List<ProducerBatch> expiredBatches = this.accumulator.expiredBatches(now);    expiredBatches.addAll(expiredInflightBatches);    // we need to reset the producer id here.    if (!expiredBatches.isEmpty())        log.trace("Expired {} batches in accumulator", expiredBatches.size());    for (ProducerBatch expiredBatch : expiredBatches) {        String errorMessage = "Expiring " + expiredBatch.recordCount + " record(s) for " + expiredBatch.topicPartition + ":" + (now - expiredBatch.createdMs) + " ms has passed since batch creation";        failBatch(expiredBatch, -1, NO_TIMESTAMP, new TimeoutException(errorMessage), false);        if (transactionManager != null && expiredBatch.inRetry()) {            // This ensures that no new batches are drained until the current in flight batches are fully resolved.            transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);        }    }    sensors.updateProduceRequestMetrics(batches);    // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately    // loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry    // time, and the delay time for checking data availability. Note that the nodes may have data that isn't yet    // sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data    // that aren't ready to send since they would cause busy looping.    long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);    pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);    pollTimeout = Math.max(pollTimeout, 0);    if (!result.readyNodes.isEmpty()) {        log.trace("Nodes with data ready to send: {}", result.readyNodes);        // if some partitions are already ready to be sent, the select time would be 0;        // otherwise if some partition already has some data accumulated but not ready yet,        // the select time will be the time difference between now and its linger expiry time;        // otherwise the select time will be the time difference between now and the metadata expiry time;        pollTimeout = 0;    }    sendProduceRequests(batches, now);    return pollTimeout;}
private longf1515
1
maybeSendAndPollTransactionalRequest
private booleanf1516_1)
{    if (transactionManager.hasInFlightTransactionalRequest()) {        // as long as there are outstanding transactional requests, we simply wait for them to return        client.poll(retryBackoffMs, time.milliseconds());        return true;    }    if (transactionManager.isCompleting() && accumulator.hasIncomplete()) {        if (transactionManager.isAborting())            accumulator.abortUndrainedBatches(new KafkaException("Failing batch since transaction was aborted"));        // be correct which would lead to an OutOfSequenceException.        if (!accumulator.flushInProgress())            accumulator.beginFlush();    }    TransactionManager.TxnRequestHandler nextRequestHandler = transactionManager.nextRequestHandler(accumulator.hasIncomplete());    if (nextRequestHandler == null)        return false;    AbstractRequest.Builder<?> requestBuilder = nextRequestHandler.requestBuilder();    Node targetNode = null;    try {        targetNode = awaitNodeReady(nextRequestHandler.coordinatorType());        if (targetNode == null) {            lookupCoordinatorAndRetry(nextRequestHandler);            return true;        }        if (nextRequestHandler.isRetry())            time.sleep(nextRequestHandler.retryBackoffMs());        long currentTimeMs = time.milliseconds();        ClientRequest clientRequest = client.newClientRequest(targetNode.idString(), requestBuilder, currentTimeMs, true, requestTimeoutMs, nextRequestHandler);                client.send(clientRequest, currentTimeMs);        transactionManager.setInFlightCorrelationId(clientRequest.correlationId());        client.poll(retryBackoffMs, time.milliseconds());        return true;    } catch (IOException e) {                // We break here so that we pick up the FindCoordinator request immediately.        lookupCoordinatorAndRetry(nextRequestHandler);        return true;    }}
private booleanf1516
1
lookupCoordinatorAndRetry
private void kafkatest_f1517_0(TransactionManager.TxnRequestHandler nextRequestHandler)
{    if (nextRequestHandler.needsCoordinator()) {        transactionManager.lookupCoordinator(nextRequestHandler);    } else {        // For non-coordinator requests, sleep here to prevent a tight loop when no node is available        time.sleep(retryBackoffMs);        metadata.requestUpdate();    }    transactionManager.retry(nextRequestHandler);}
f1517
0
handleProduceResponse
private voidf1525_1ClientResponse response, Map<TopicPartition, ProducerBatch> batches, long now)
{    RequestHeader requestHeader = response.requestHeader();    long receivedTimeMs = response.receivedTimeMs();    int correlationId = requestHeader.correlationId();    if (response.wasDisconnected()) {        log.trace("Cancelled request with header {} due to node {} being disconnected", requestHeader, response.destination());        for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION), correlationId, now, 0L);    } else if (response.versionMismatch() != null) {                for (ProducerBatch batch : batches.values()) completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.UNSUPPORTED_VERSION), correlationId, now, 0L);    } else {        log.trace("Received produce response from node {} with correlation id {}", response.destination(), correlationId);        // if we have a response, parse it        if (response.hasResponse()) {            ProduceResponse produceResponse = (ProduceResponse) response.responseBody();            for (Map.Entry<TopicPartition, ProduceResponse.PartitionResponse> entry : produceResponse.responses().entrySet()) {                TopicPartition tp = entry.getKey();                ProduceResponse.PartitionResponse partResp = entry.getValue();                ProducerBatch batch = batches.get(tp);                completeBatch(batch, partResp, correlationId, now, receivedTimeMs + produceResponse.throttleTimeMs());            }            this.sensors.recordLatency(response.destination(), response.requestLatencyMs());        } else {            // this is the acks = 0 case, just complete all requests            for (ProducerBatch batch : batches.values()) {                completeBatch(batch, new ProduceResponse.PartitionResponse(Errors.NONE), correlationId, now, 0L);            }        }    }}
private voidf1525
1
completeBatch
private voidf1526_1ProducerBatch batch, ProduceResponse.PartitionResponse response, long correlationId, long now, long throttleUntilTimeMs)
{    Errors error = response.error;    if (error == Errors.MESSAGE_TOO_LARGE && batch.recordCount > 1 && !batch.isDone() && (batch.magic() >= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) {        // If the batch is too large, we split the batch and send the split batches again. We do not decrement        // the retry attempts in this case.                if (transactionManager != null)            transactionManager.removeInFlightBatch(batch);        this.accumulator.splitAndReenqueue(batch);        maybeRemoveAndDeallocateBatch(batch);        this.sensors.recordBatchSplit();    } else if (error != Errors.NONE) {        if (canRetry(batch, response, now)) {                        if (transactionManager == null) {                reenqueueBatch(batch, now);            } else if (transactionManager.hasProducerIdAndEpoch(batch.producerId(), batch.producerEpoch())) {                // If idempotence is enabled only retry the request if the current producer id is the same as                // the producer id of the batch.                 Sequence number : {}", batch.topicPartition, batch.producerId(), batch.baseSequence());                reenqueueBatch(batch, now);            } else {                failBatch(batch, response, new OutOfOrderSequenceException("Attempted to retry sending a " + "batch but the producer id changed from " + batch.producerId() + " to " + transactionManager.producerIdAndEpoch().producerId + " in the mean time. This batch will be dropped."), false);            }        } else if (error == Errors.DUPLICATE_SEQUENCE_NUMBER) {            // If we have received a duplicate sequence error, it means that the sequence number has advanced beyond            // the sequence of the current batch, and we haven't retained batch metadata on the broker to return            // the correct offset and timestamp.            //             // The only thing we can do is to return success to the user and not return a valid offset and timestamp.            completeBatch(batch, response);        } else {            final RuntimeException exception;            if (error == Errors.TOPIC_AUTHORIZATION_FAILED)                exception = new TopicAuthorizationException(Collections.singleton(batch.topicPartition.topic()));            else if (error == Errors.CLUSTER_AUTHORIZATION_FAILED)                exception = new ClusterAuthorizationException("The producer is not authorized to do idempotent sends");            else                exception = error.exception();            // tell the user the result of their request. We only adjust sequence numbers if the batch didn't exhaust            // its retries -- if it did, we don't know whether the sequence number was accepted or not, and            // thus it is not safe to reassign the sequence.            failBatch(batch, response, exception, batch.attempts() < this.retries);        }        if (error.exception() instanceof InvalidMetadataException) {            if (error.exception() instanceof UnknownTopicOrPartitionException) {                            } else {                            }            metadata.requestUpdate();        }    } else {        completeBatch(batch, response);    }    // Unmute the completed partition.    if (guaranteeMessageOrder)        this.accumulator.unmutePartition(batch.topicPartition, throttleUntilTimeMs);}
private voidf1526
1
reenqueueBatch
private void kafkatest_f1527_0(ProducerBatch batch, long currentTimeMs)
{    this.accumulator.reenqueue(batch, currentTimeMs);    maybeRemoveFromInflightBatches(batch);    this.sensors.recordRetries(batch.topicPartition.topic(), batch.recordCount);}
f1527
0
wakeup
public void kafkatest_f1535_0()
{    this.client.wakeup();}
f1535
0
throttleTimeSensor
public static Sensor kafkatest_f1536_0(SenderMetricsRegistry metrics)
{    Sensor produceThrottleTimeSensor = metrics.sensor("produce-throttle-time");    produceThrottleTimeSensor.add(metrics.produceThrottleTimeAvg, new Avg());    produceThrottleTimeSensor.add(metrics.produceThrottleTimeMax, new Max());    return produceThrottleTimeSensor;}
f1536
0
maybeRegisterTopicMetrics
private void kafkatest_f1537_0(String topic)
{    // if one sensor of the metrics has been registered for the topic,    // then all other sensors should have been registered; and vice versa    String topicRecordsCountName = "topic." + topic + ".records-per-batch";    Sensor topicRecordCount = this.metrics.getSensor(topicRecordsCountName);    if (topicRecordCount == null) {        Map<String, String> metricTags = Collections.singletonMap("topic", topic);        topicRecordCount = this.metrics.sensor(topicRecordsCountName);        MetricName rateMetricName = this.metrics.topicRecordSendRate(metricTags);        MetricName totalMetricName = this.metrics.topicRecordSendTotal(metricTags);        topicRecordCount.add(new Meter(rateMetricName, totalMetricName));        String topicByteRateName = "topic." + topic + ".bytes";        Sensor topicByteRate = this.metrics.sensor(topicByteRateName);        rateMetricName = this.metrics.topicByteRate(metricTags);        totalMetricName = this.metrics.topicByteTotal(metricTags);        topicByteRate.add(new Meter(rateMetricName, totalMetricName));        String topicCompressionRateName = "topic." + topic + ".compression-rate";        Sensor topicCompressionRate = this.metrics.sensor(topicCompressionRateName);        MetricName m = this.metrics.topicCompressionRate(metricTags);        topicCompressionRate.add(m, new Avg());        String topicRetryName = "topic." + topic + ".record-retries";        Sensor topicRetrySensor = this.metrics.sensor(topicRetryName);        rateMetricName = this.metrics.topicRecordRetryRate(metricTags);        totalMetricName = this.metrics.topicRecordRetryTotal(metricTags);        topicRetrySensor.add(new Meter(rateMetricName, totalMetricName));        String topicErrorName = "topic." + topic + ".record-errors";        Sensor topicErrorSensor = this.metrics.sensor(topicErrorName);        rateMetricName = this.metrics.topicRecordErrorRate(metricTags);        totalMetricName = this.metrics.topicRecordErrorTotal(metricTags);        topicErrorSensor.add(new Meter(rateMetricName, totalMetricName));    }}
f1537
0
topicRecordSendRate
public MetricName kafkatest_f1545_0(Map<String, String> tags)
{    return this.metrics.metricInstance(this.topicRecordSendRate, tags);}
f1545
0
topicRecordSendTotal
public MetricName kafkatest_f1546_0(Map<String, String> tags)
{    return this.metrics.metricInstance(this.topicRecordSendTotal, tags);}
f1546
0
topicByteRate
public MetricName kafkatest_f1547_0(Map<String, String> tags)
{    return this.metrics.metricInstance(this.topicByteRate, tags);}
f1547
0
sensor
public Sensor kafkatest_f1555_0(String name)
{    return this.metrics.sensor(name);}
f1555
0
addMetric
public void kafkatest_f1556_0(MetricName m, Measurable measurable)
{    this.metrics.addMetric(m, measurable);}
f1556
0
getSensor
public Sensor kafkatest_f1557_0(String name)
{    return this.metrics.getSensor(name);}
f1557
0
error
public RuntimeException kafkatest_f1565_0()
{    return error;}
f1565
0
isSuccessful
public boolean kafkatest_f1566_0()
{    return error == null;}
f1566
0
isCompleted
public boolean kafkatest_f1567_0()
{    return latch.getCount() == 0L;}
f1567
0
isTransitionValid
private boolean kafkatest_f1575_0(State source, State target)
{    switch(target) {        case INITIALIZING:            return source == UNINITIALIZED;        case READY:            return source == INITIALIZING || source == COMMITTING_TRANSACTION || source == ABORTING_TRANSACTION;        case IN_TRANSACTION:            return source == READY;        case COMMITTING_TRANSACTION:            return source == IN_TRANSACTION;        case ABORTING_TRANSACTION:            return source == IN_TRANSACTION || source == ABORTABLE_ERROR;        case ABORTABLE_ERROR:            return source == IN_TRANSACTION || source == COMMITTING_TRANSACTION || source == ABORTABLE_ERROR;        case FATAL_ERROR:        default:            // producer or do purely non transactional requests.            return true;    }}
f1575
0
initializeTransactions
public synchronized TransactionalRequestResult kafkatest_f1576_0()
{    return handleCachedTransactionRequestResult(() -> {        transitionTo(State.INITIALIZING);        setProducerIdAndEpoch(ProducerIdAndEpoch.NONE);        InitProducerIdRequestData requestData = new InitProducerIdRequestData().setTransactionalId(transactionalId).setTransactionTimeoutMs(transactionTimeoutMs);        InitProducerIdHandler handler = new InitProducerIdHandler(new InitProducerIdRequest.Builder(requestData));        enqueueRequest(handler);        return handler.result;    }, State.INITIALIZING);}
f1576
0
beginTransaction
public synchronized void kafkatest_f1577_0()
{    ensureTransactional();    maybeFailWithError();    transitionTo(State.IN_TRANSACTION);}
f1577
0
isSendToPartitionAllowed
 synchronized boolean kafkatest_f1585_0(TopicPartition tp)
{    if (hasFatalError())        return false;    return !isTransactional() || partitionsInTransaction.contains(tp);}
f1585
0
transactionalId
public String kafkatest_f1586_0()
{    return transactionalId;}
f1586
0
hasProducerId
public boolean kafkatest_f1587_0()
{    return producerIdAndEpoch.isValid();}
f1587
0
isPartitionAdded
 synchronized boolean kafkatest_f1595_0(TopicPartition partition)
{    return partitionsInTransaction.contains(partition);}
f1595
0
isPartitionPendingAdd
 synchronized boolean kafkatest_f1596_0(TopicPartition partition)
{    return newPartitionsInTransaction.contains(partition) || pendingPartitionsInTransaction.contains(partition);}
f1596
0
producerIdAndEpoch
 ProducerIdAndEpoch kafkatest_f1597_0()
{    return producerIdAndEpoch;}
f1597
0
addInFlightBatch
 synchronized void kafkatest_f1605_0(ProducerBatch batch)
{    if (!batch.hasSequence())        throw new IllegalStateException("Can't track batch for partition " + batch.topicPartition + " when sequence is not set.");    topicPartitionBookkeeper.getPartition(batch.topicPartition).inflightBatchesBySequence.add(batch);}
f1605
0
firstInFlightSequence
 synchronized int kafkatest_f1606_0(TopicPartition topicPartition)
{    if (!hasInflightBatches(topicPartition))        return RecordBatch.NO_SEQUENCE;    SortedSet<ProducerBatch> inflightBatches = topicPartitionBookkeeper.getPartition(topicPartition).inflightBatchesBySequence;    if (inflightBatches.isEmpty())        return RecordBatch.NO_SEQUENCE;    else        return inflightBatches.first().baseSequence();}
f1606
0
nextBatchBySequence
 synchronized ProducerBatch kafkatest_f1607_0(TopicPartition topicPartition)
{    SortedSet<ProducerBatch> queue = topicPartitionBookkeeper.getPartition(topicPartition).inflightBatchesBySequence;    return queue.isEmpty() ? null : queue.first();}
f1607
0
handleFailedBatch
public synchronized voidf1615_1ProducerBatch batch, RuntimeException exception, boolean adjustSequenceNumbers)
{    maybeTransitionToErrorState(exception);    if (!hasProducerIdAndEpoch(batch.producerId(), batch.producerEpoch())) {                return;    }    if (exception instanceof OutOfOrderSequenceException && !isTransactional()) {                // Reset the producerId since we have hit an irrecoverable exception and cannot make any guarantees        // about the previously committed message. Note that this will discard the producer id and sequence        // numbers for all existing partitions.        resetProducerId();    } else {        removeInFlightBatch(batch);        if (adjustSequenceNumbers)            adjustSequencesDueToFailedBatch(batch);    }}
public synchronized voidf1615
1
adjustSequencesDueToFailedBatch
private voidf1616_1ProducerBatch batch)
{    if (!topicPartitionBookkeeper.contains(batch.topicPartition))        // reset due to a previous OutOfOrderSequenceException.        return;        int currentSequence = sequenceNumber(batch.topicPartition);    currentSequence -= batch.recordCount;    if (currentSequence < 0)        throw new IllegalStateException("Sequence number for partition " + batch.topicPartition + " is going to become negative: " + currentSequence);    setNextSequence(batch.topicPartition, currentSequence);    topicPartitionBookkeeper.getPartition(batch.topicPartition).resetSequenceNumbers(inFlightBatch -> {        if (inFlightBatch.baseSequence() < batch.baseSequence())            return;        int newSequence = inFlightBatch.baseSequence() - batch.recordCount;        if (newSequence < 0)            throw new IllegalStateException("Sequence number for batch with sequence " + inFlightBatch.baseSequence() + " for partition " + batch.topicPartition + " is going to become negative: " + newSequence);                inFlightBatch.resetProducerState(new ProducerIdAndEpoch(inFlightBatch.producerId(), inFlightBatch.producerEpoch()), newSequence, inFlightBatch.isTransactional());    });}
private voidf1616
1
startSequencesAtBeginning
private voidf1617_1TopicPartition topicPartition)
{    final PrimitiveRef.IntRef sequence = PrimitiveRef.ofInt(0);    topicPartitionBookkeeper.getPartition(topicPartition).resetSequenceNumbers(inFlightBatch -> {                inFlightBatch.resetProducerState(new ProducerIdAndEpoch(inFlightBatch.producerId(), inFlightBatch.producerEpoch()), sequence.value, inFlightBatch.isTransactional());        sequence.value += inFlightBatch.recordCount;    });    setNextSequence(topicPartition, sequence.value);    topicPartitionBookkeeper.getPartition(topicPartition).lastAckedSequence = NO_LAST_ACKED_SEQUENCE_NUMBER;}
private voidf1617
1
nextRequestHandler
 synchronized TxnRequestHandlerf1625_1boolean hasIncompleteBatches)
{    if (!newPartitionsInTransaction.isEmpty())        enqueueRequest(addPartitionsToTransactionHandler());    TxnRequestHandler nextRequestHandler = pendingRequests.peek();    if (nextRequestHandler == null)        return null;    // Do not send the EndTxn until all batches have been flushed    if (nextRequestHandler.isEndTxn() && hasIncompleteBatches)        return null;    pendingRequests.poll();    if (maybeTerminateRequestWithError(nextRequestHandler)) {        log.trace("Not sending transactional request {} because we are in an error state", nextRequestHandler.requestBuilder());        return null;    }    if (nextRequestHandler.isEndTxn() && !transactionStarted) {        nextRequestHandler.result.done();        if (currentState != State.FATAL_ERROR) {                        completeTransaction();        }        nextRequestHandler = pendingRequests.poll();    }    if (nextRequestHandler != null)        log.trace("Request {} dequeued for sending", nextRequestHandler.requestBuilder());    return nextRequestHandler;}
 synchronized TxnRequestHandlerf1625
1
retry
 synchronized void kafkatest_f1626_0(TxnRequestHandler request)
{    request.setRetry();    enqueueRequest(request);}
f1626
0
authenticationFailed
 synchronized void kafkatest_f1627_0(AuthenticationException e)
{    for (TxnRequestHandler request : pendingRequests) request.fatalError(e);}
f1627
0
hasAbortableError
 boolean kafkatest_f1635_0()
{    return currentState == State.ABORTABLE_ERROR;}
f1635
0
transactionContainsPartition
 synchronized boolean kafkatest_f1636_0(TopicPartition topicPartition)
{    return partitionsInTransaction.contains(topicPartition);}
f1636
0
hasPendingOffsetCommits
 synchronized boolean kafkatest_f1637_0()
{    return !pendingTxnOffsetCommits.isEmpty();}
f1637
0
maybeFailWithError
private void kafkatest_f1645_0()
{    if (hasError()) {        // but create a new instance without the call trace since it was not thrown because of the current call        if (lastError instanceof ProducerFencedException) {            throw new ProducerFencedException("The producer has been rejected from the broker because " + "it tried to use an old epoch with the transactionalId");        } else {            throw new KafkaException("Cannot execute transactional method because we are in an error state", lastError);        }    }}
f1645
0
maybeTerminateRequestWithError
private boolean kafkatest_f1646_0(TxnRequestHandler requestHandler)
{    if (hasError()) {        if (hasAbortableError() && requestHandler instanceof FindCoordinatorHandler)            // No harm letting the FindCoordinator request go through if we're expecting to abort            return false;        requestHandler.fail(lastError);        return true;    }    return false;}
f1646
0
enqueueRequest
private voidf1647_1TxnRequestHandler requestHandler)
{        pendingRequests.add(requestHandler);}
private voidf1647
1
fail
 void kafkatest_f1655_0(RuntimeException e)
{    result.setError(e);    result.done();}
f1655
0
reenqueue
 void kafkatest_f1656_0()
{    synchronized (TransactionManager.this) {        this.isRetry = true;        enqueueRequest(this);    }}
f1656
0
retryBackoffMs
 long kafkatest_f1657_0()
{    return retryBackoffMs;}
f1657
0
requestBuilder
 InitProducerIdRequest.Builder kafkatest_f1665_0()
{    return builder;}
f1665
0
priority
 Priority kafkatest_f1666_0()
{    return Priority.INIT_PRODUCER_ID;}
f1666
0
handleResponse
public void kafkatest_f1667_0(AbstractResponse response)
{    InitProducerIdResponse initProducerIdResponse = (InitProducerIdResponse) response;    Errors error = initProducerIdResponse.error();    if (error == Errors.NONE) {        ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(initProducerIdResponse.data.producerId(), initProducerIdResponse.data.producerEpoch());        setProducerIdAndEpoch(producerIdAndEpoch);        transitionTo(State.READY);        lastError = null;        result.done();    } else if (error == Errors.NOT_COORDINATOR || error == Errors.COORDINATOR_NOT_AVAILABLE) {        lookupCoordinator(FindCoordinatorRequest.CoordinatorType.TRANSACTION, transactionalId);        reenqueue();    } else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.CONCURRENT_TRANSACTIONS) {        reenqueue();    } else if (error == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED) {        fatalError(error.exception());    } else {        fatalError(new KafkaException("Unexpected error in InitProducerIdResponse; " + error.message()));    }}
f1667
0
coordinatorType
 FindCoordinatorRequest.CoordinatorType kafkatest_f1675_0()
{    return null;}
f1675
0
coordinatorKey
 String kafkatest_f1676_0()
{    return null;}
f1676
0
handleResponse
public void kafkatest_f1677_0(AbstractResponse response)
{    FindCoordinatorResponse findCoordinatorResponse = (FindCoordinatorResponse) response;    Errors error = findCoordinatorResponse.error();    CoordinatorType coordinatorType = CoordinatorType.forId(builder.data().keyType());    if (error == Errors.NONE) {        Node node = findCoordinatorResponse.node();        switch(coordinatorType) {            case GROUP:                consumerGroupCoordinator = node;                break;            case TRANSACTION:                transactionCoordinator = node;        }        result.done();    } else if (error == Errors.COORDINATOR_NOT_AVAILABLE) {        reenqueue();    } else if (error == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED) {        fatalError(error.exception());    } else if (findCoordinatorResponse.error() == Errors.GROUP_AUTHORIZATION_FAILED) {        abortableError(GroupAuthorizationException.forGroupId(builder.data().key()));    } else {        fatalError(new KafkaException(String.format("Could not find a coordinator with type %s with key %s due to" + "unexpected error: %s", coordinatorType, builder.data().key(), findCoordinatorResponse.data().errorMessage())));    }}
f1677
0
requestBuilder
 TxnOffsetCommitRequest.Builder kafkatest_f1685_0()
{    return builder;}
f1685
0
priority
 Priority kafkatest_f1686_0()
{    return Priority.ADD_PARTITIONS_OR_OFFSETS;}
f1686
0
coordinatorType
 FindCoordinatorRequest.CoordinatorType kafkatest_f1687_0()
{    return FindCoordinatorRequest.CoordinatorType.GROUP;}
f1687
0
configureInflightRequests
private static int kafkatest_f1695_0(ProducerConfig config, boolean idempotenceEnabled)
{    if (idempotenceEnabled && 5 < config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION)) {        throw new ConfigException("Must set " + ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION + " to at most 5" + " to use the idempotent producer.");    }    return config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION);}
f1695
0
configureAcks
private static shortf1696_1ProducerConfig config, boolean idempotenceEnabled, Logger log)
{    boolean userConfiguredAcks = false;    short acks = (short) parseAcks(config.getString(ProducerConfig.ACKS_CONFIG));    if (config.originals().containsKey(ProducerConfig.ACKS_CONFIG)) {        userConfiguredAcks = true;    }    if (idempotenceEnabled && !userConfiguredAcks) {                return -1;    }    if (idempotenceEnabled && acks != -1) {        throw new ConfigException("Must set " + ProducerConfig.ACKS_CONFIG + " to all in order to use the idempotent " + "producer. Otherwise we cannot guarantee idempotence.");    }    return acks;}
private static shortf1696
1
parseAcks
private static int kafkatest_f1697_0(String acksString)
{    try {        return acksString.trim().equalsIgnoreCase("all") ? -1 : Integer.parseInt(acksString.trim());    } catch (NumberFormatException e) {        throw new ConfigException("Invalid configuration value for 'acks': " + acksString);    }}
f1697
0
throwIfProducerClosed
private void kafkatest_f1705_0()
{    if (sender == null || !sender.isRunning())        throw new IllegalStateException("Cannot perform operation after producer has been closed");}
f1705
0
doSend
private Future<RecordMetadata>f1706_1ProducerRecord<K, V> record, Callback callback)
{    TopicPartition tp = null;    try {        throwIfProducerClosed();        // first make sure the metadata for the topic is available        ClusterAndWaitTime clusterAndWaitTime;        try {            clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);        } catch (KafkaException e) {            if (metadata.isClosed())                throw new KafkaException("Producer closed while send in progress", e);            throw e;        }        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);        Cluster cluster = clusterAndWaitTime.cluster;        byte[] serializedKey;        try {            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());        } catch (ClassCastException cce) {            throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() + " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + " specified in key.serializer", cce);        }        byte[] serializedValue;        try {            serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());        } catch (ClassCastException cce) {            throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() + " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + " specified in value.serializer", cce);        }        int partition = partition(record, serializedKey, serializedValue, cluster);        tp = new TopicPartition(record.topic(), partition);        setReadOnly(record.headers());        Header[] headers = record.headers().toArray();        int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(), compressionType, serializedKey, serializedValue, headers);        ensureValidRecordSize(serializedSize);        long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();        if (log.isTraceEnabled()) {            log.trace("Attempting to append record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);        }        // producer callback will make sure to call both 'callback' and interceptor callback        Callback interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);        if (transactionManager != null && transactionManager.isTransactional()) {            transactionManager.failIfNotReadyForSend();        }        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs, true);        if (result.abortForNewBatch) {            int prevPartition = partition;            partitioner.onNewBatch(record.topic(), cluster, prevPartition);            partition = partition(record, serializedKey, serializedValue, cluster);            tp = new TopicPartition(record.topic(), partition);            if (log.isTraceEnabled()) {                log.trace("Retrying append due to new batch creation for topic {} partition {}. The old partition was {}", record.topic(), partition, prevPartition);            }            // producer callback will make sure to call both 'callback' and interceptor callback            interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);            result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs, false);        }        if (transactionManager != null && transactionManager.isTransactional())            transactionManager.maybeAddPartitionToTransaction(tp);        if (result.batchIsFull || result.newBatchCreated) {            log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);            this.sender.wakeup();        }        return result.future;    // handling exceptions and record the errors;    // for API exceptions return them in the future,    // for other exceptions throw directly    } catch (ApiException e) {                if (callback != null)            callback.onCompletion(null, e);        this.errors.record();        this.interceptors.onSendError(record, tp, e);        return new FutureFailure(e);    } catch (InterruptedException e) {        this.errors.record();        this.interceptors.onSendError(record, tp, e);        throw new InterruptException(e);    } catch (BufferExhaustedException e) {        this.errors.record();        this.metrics.sensor("buffer-exhausted-records").record();        this.interceptors.onSendError(record, tp, e);        throw e;    } catch (KafkaException e) {        this.errors.record();        this.interceptors.onSendError(record, tp, e);        throw e;    } catch (Exception e) {        // we notify interceptor about all exceptions, since onSend is called before anything else in this method        this.interceptors.onSendError(record, tp, e);        throw e;    }}
private Future<RecordMetadata>f1706
1
setReadOnly
private void kafkatest_f1707_0(Headers headers)
{    if (headers instanceof RecordHeaders) {        ((RecordHeaders) headers).setReadOnly();    }}
f1707
0
close
private voidf1715_1Duration timeout, boolean swallowException)
{    long timeoutMs = timeout.toMillis();    if (timeoutMs < 0)        throw new IllegalArgumentException("The timeout cannot be negative.");        // this will keep track of the first encountered exception    AtomicReference<Throwable> firstException = new AtomicReference<>();    boolean invokedFromCallback = Thread.currentThread() == this.ioThread;    if (timeoutMs > 0) {        if (invokedFromCallback) {                    } else {            // Try to close gracefully.            if (this.sender != null)                this.sender.initiateClose();            if (this.ioThread != null) {                try {                    this.ioThread.join(timeoutMs);                } catch (InterruptedException t) {                    firstException.compareAndSet(null, new InterruptException(t));                                    }            }        }    }    if (this.sender != null && this.ioThread != null && this.ioThread.isAlive()) {                this.sender.forceClose();        // Only join the sender thread when not calling from callback.        if (!invokedFromCallback) {            try {                this.ioThread.join();            } catch (InterruptedException e) {                firstException.compareAndSet(null, new InterruptException(e));            }        }    }    Utils.closeQuietly(interceptors, "producer interceptors", firstException);    Utils.closeQuietly(metrics, "producer metrics", firstException);    Utils.closeQuietly(keySerializer, "producer keySerializer", firstException);    Utils.closeQuietly(valueSerializer, "producer valueSerializer", firstException);    Utils.closeQuietly(partitioner, "producer partitioner", firstException);    AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics);    Throwable exception = firstException.get();    if (exception != null && !swallowException) {        if (exception instanceof InterruptException) {            throw (InterruptException) exception;        }        throw new KafkaException("Failed to close kafka producer", exception);    }    }
private voidf1715
1
propsToMap
private static Map<String, Object> kafkatest_f1716_0(Properties properties)
{    Map<String, Object> map = new HashMap<>(properties.size());    for (Map.Entry<Object, Object> entry : properties.entrySet()) {        if (entry.getKey() instanceof String) {            String k = (String) entry.getKey();            map.put(k, properties.get(k));        } else {            throw new ConfigException(entry.getKey().toString(), entry.getValue(), "Key must be a string.");        }    }    return map;}
f1716
0
configureClusterResourceListeners
private ClusterResourceListeners kafkatest_f1717_0(Serializer<K> keySerializer, Serializer<V> valueSerializer, List<?>... candidateLists)
{    ClusterResourceListeners clusterResourceListeners = new ClusterResourceListeners();    for (List<?> candidateList : candidateLists) clusterResourceListeners.maybeAddAll(candidateList);    clusterResourceListeners.maybeAdd(keySerializer);    clusterResourceListeners.maybeAdd(valueSerializer);    return clusterResourceListeners;}
f1717
0
isDone
public boolean kafkatest_f1725_0()
{    return true;}
f1725
0
onCompletion
public void kafkatest_f1726_0(RecordMetadata metadata, Exception exception)
{    metadata = metadata != null ? metadata : new RecordMetadata(tp, -1, -1, RecordBatch.NO_TIMESTAMP, Long.valueOf(-1L), -1, -1);    this.interceptors.onAcknowledgement(metadata, exception);    if (this.userCallback != null)        this.userCallback.onCompletion(metadata, exception);}
f1726
0
initTransactions
public void kafkatest_f1727_0()
{    verifyProducerState();    if (this.transactionInitialized) {        throw new IllegalStateException("MockProducer has already been initialized for transactions.");    }    this.transactionInitialized = true;}
f1727
0
send
public synchronized Future<RecordMetadata> kafkatest_f1735_0(ProducerRecord<K, V> record)
{    return send(record, null);}
f1735
0
send
public synchronized Future<RecordMetadata> kafkatest_f1736_0(ProducerRecord<K, V> record, Callback callback)
{    if (this.closed) {        throw new IllegalStateException("MockProducer is already closed.");    }    if (this.producerFenced) {        throw new KafkaException("MockProducer is fenced.", new ProducerFencedException("Fenced"));    }    int partition = 0;    if (!this.cluster.partitionsForTopic(record.topic()).isEmpty())        partition = partition(record, this.cluster);    TopicPartition topicPartition = new TopicPartition(record.topic(), partition);    ProduceRequestResult result = new ProduceRequestResult(topicPartition);    FutureRecordMetadata future = new FutureRecordMetadata(result, 0, RecordBatch.NO_TIMESTAMP, 0L, 0, 0, Time.SYSTEM);    long offset = nextOffset(topicPartition);    Completion completion = new Completion(offset, new RecordMetadata(topicPartition, 0, offset, RecordBatch.NO_TIMESTAMP, Long.valueOf(0L), 0, 0), result, callback);    if (!this.transactionInFlight)        this.sent.add(record);    else        this.uncommittedSends.add(record);    if (autoComplete)        completion.complete(null);    else        this.completions.addLast(completion);    return future;}
f1736
0
nextOffset
private long kafkatest_f1737_0(TopicPartition tp)
{    Long offset = this.offsets.get(tp);    if (offset == null) {        this.offsets.put(tp, 1L);        return 0L;    } else {        Long next = offset + 1;        this.offsets.put(tp, next);        return offset;    }}
f1737
0
fenceProducer
public synchronized void kafkatest_f1745_0()
{    verifyProducerState();    verifyTransactionsInitialized();    this.producerFenced = true;}
f1745
0
fenceProducerOnClose
public void kafkatest_f1746_0()
{    verifyProducerState();    verifyTransactionsInitialized();    this.producerFencedOnClose = true;}
f1746
0
transactionInitialized
public boolean kafkatest_f1747_0()
{    return this.transactionInitialized;}
f1747
0
consumerGroupOffsetsHistory
public synchronized List<Map<String, Map<TopicPartition, OffsetAndMetadata>>> kafkatest_f1755_0()
{    return new ArrayList<>(this.consumerGroupOffsets);}
f1755
0
clear
public synchronized void kafkatest_f1756_0()
{    this.sent.clear();    this.uncommittedSends.clear();    this.completions.clear();    this.consumerGroupOffsets.clear();    this.uncommittedConsumerGroupOffsets.clear();    this.transactionInitialized = false;    this.transactionInFlight = false;    this.transactionCommitted = false;    this.transactionAborted = false;    this.producerFenced = false;}
f1756
0
completeNext
public synchronized boolean kafkatest_f1757_0()
{    return errorNext(null);}
f1757
0
configNames
public static Set<String> kafkatest_f1766_0()
{    return CONFIG.names();}
f1766
0
configDef
public static ConfigDef kafkatest_f1767_0()
{    return new ConfigDef(CONFIG);}
f1767
0
main
public static void kafkatest_f1768_0(String[] args)
{    System.out.println(CONFIG.toHtml());}
f1768
0
equals
public boolean kafkatest_f1776_0(Object o)
{    if (this == o)        return true;    else if (!(o instanceof ProducerRecord))        return false;    ProducerRecord<?, ?> that = (ProducerRecord<?, ?>) o;    return Objects.equals(key, that.key) && Objects.equals(partition, that.partition) && Objects.equals(topic, that.topic) && Objects.equals(headers, that.headers) && Objects.equals(value, that.value) && Objects.equals(timestamp, that.timestamp);}
f1776
0
hashCode
public int kafkatest_f1777_0()
{    int result = topic != null ? topic.hashCode() : 0;    result = 31 * result + (partition != null ? partition.hashCode() : 0);    result = 31 * result + (headers != null ? headers.hashCode() : 0);    result = 31 * result + (key != null ? key.hashCode() : 0);    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + (timestamp != null ? timestamp.hashCode() : 0);    return result;}
f1777
0
hasOffset
public boolean kafkatest_f1778_0()
{    return this.offset != ProduceResponse.INVALID_OFFSET;}
f1778
0
partition
public int kafkatest_f1786_0()
{    return this.topicPartition.partition();}
f1786
0
toString
public String kafkatest_f1787_0()
{    return topicPartition.toString() + "@" + offset;}
f1787
0
partition
public int kafkatest_f1789_0(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster)
{    List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);    int numPartitions = partitions.size();    int nextValue = nextValue(topic);    List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);    if (!availablePartitions.isEmpty()) {        int part = Utils.toPositive(nextValue) % availablePartitions.size();        return availablePartitions.get(part).partition();    } else {        // no partitions are available, give a non-available partition        return Utils.toPositive(nextValue) % numPartitions;    }}
f1789
0
toFilter
public AccessControlEntryFilter kafkatest_f1800_0()
{    return new AccessControlEntryFilter(data);}
f1800
0
toString
public String kafkatest_f1801_0()
{    return data.toString();}
f1801
0
isUnknown
public boolean kafkatest_f1802_0()
{    return data.isUnknown();}
f1802
0
toString
public String kafkatest_f1810_0()
{    return "(principal=" + (principal == null ? "<any>" : principal) + ", host=" + (host == null ? "<any>" : host) + ", operation=" + operation + ", permissionType=" + permissionType + ")";}
f1810
0
isUnknown
 boolean kafkatest_f1811_0()
{    return operation.isUnknown() || permissionType.isUnknown();}
f1811
0
equals
public boolean kafkatest_f1812_0(Object o)
{    if (!(o instanceof AccessControlEntryData))        return false;    AccessControlEntryData other = (AccessControlEntryData) o;    return Objects.equals(principal, other.principal) && Objects.equals(host, other.host) && Objects.equals(operation, other.operation) && Objects.equals(permissionType, other.permissionType);}
f1812
0
matches
public boolean kafkatest_f1820_0(AccessControlEntry other)
{    if ((principal() != null) && (!data.principal().equals(other.principal())))        return false;    if ((host() != null) && (!host().equals(other.host())))        return false;    if ((operation() != AclOperation.ANY) && (!operation().equals(other.operation())))        return false;    return (permissionType() == AclPermissionType.ANY) || (permissionType().equals(other.permissionType()));}
f1820
0
matchesAtMostOne
public boolean kafkatest_f1821_0()
{    return findIndefiniteField() == null;}
f1821
0
findIndefiniteField
public String kafkatest_f1822_0()
{    return data.findIndefiniteField();}
f1822
0
equals
public boolean kafkatest_f1830_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    AclBinding that = (AclBinding) o;    return Objects.equals(pattern, that.pattern) && Objects.equals(entry, that.entry);}
f1830
0
hashCode
public int kafkatest_f1831_0()
{    return Objects.hash(pattern, entry);}
f1831
0
isUnknown
public boolean kafkatest_f1832_0()
{    return patternFilter.isUnknown() || entryFilter.isUnknown();}
f1832
0
hashCode
public int kafkatest_f1840_0()
{    return Objects.hash(patternFilter, entryFilter);}
f1840
0
fromString
public static AclOperation kafkatest_f1841_0(String str) throws IllegalArgumentException
{    try {        return AclOperation.valueOf(str.toUpperCase(Locale.ROOT));    } catch (IllegalArgumentException e) {        return UNKNOWN;    }}
f1841
0
fromCode
public static AclOperation kafkatest_f1842_0(byte code)
{    AclOperation operation = CODE_TO_VALUE.get(code);    if (operation == null) {        return UNKNOWN;    }    return operation;}
f1842
0
get
public V kafkatest_f1850_0(K key)
{    return cache.get(key);}
f1850
0
put
public void kafkatest_f1851_0(K key, V value)
{    cache.put(key, value);}
f1851
0
remove
public boolean kafkatest_f1852_0(K key)
{    return cache.remove(key) != null;}
f1852
0
withPartitions
public Cluster kafkatest_f1860_0(Map<TopicPartition, PartitionInfo> partitions)
{    Map<TopicPartition, PartitionInfo> combinedPartitions = new HashMap<>(this.partitionsByTopicPartition);    combinedPartitions.putAll(partitions);    return new Cluster(clusterResource.clusterId(), this.nodes, combinedPartitions.values(), new HashSet<>(this.unauthorizedTopics), new HashSet<>(this.invalidTopics), new HashSet<>(this.internalTopics), this.controller);}
f1860
0
nodes
public List<Node> kafkatest_f1861_0()
{    return this.nodes;}
f1861
0
nodeById
public Node kafkatest_f1862_0(int id)
{    return this.nodesById.get(id);}
f1862
0
topics
public Set<String> kafkatest_f1870_0()
{    return partitionsByTopic.keySet();}
f1870
0
unauthorizedTopics
public Set<String> kafkatest_f1871_0()
{    return unauthorizedTopics;}
f1871
0
invalidTopics
public Set<String> kafkatest_f1872_0()
{    return invalidTopics;}
f1872
0
clusterId
public String kafkatest_f1880_0()
{    return clusterId;}
f1880
0
toString
public String kafkatest_f1881_0()
{    return "ClusterResource(clusterId=" + clusterId + ")";}
f1881
0
equals
public boolean kafkatest_f1882_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ClusterResource that = (ClusterResource) o;    return Objects.equals(clusterId, that.clusterId);}
f1882
0
getDouble
public Double kafkatest_f1890_0(String key)
{    return (Double) get(key);}
f1890
0
getList
public List<String> kafkatest_f1891_0(String key)
{    return (List<String>) get(key);}
f1891
0
getBoolean
public Boolean kafkatest_f1892_0(String key)
{    return (Boolean) get(key);}
f1892
0
originalsWithPrefix
public Map<String, Object> kafkatest_f1900_0(String prefix)
{    return originalsWithPrefix(prefix, true);}
f1900
0
originalsWithPrefix
public Map<String, Object> kafkatest_f1901_0(String prefix, boolean strip)
{    Map<String, Object> result = new RecordingMap<>(prefix, false);    for (Map.Entry<String, ?> entry : originals.entrySet()) {        if (entry.getKey().startsWith(prefix) && entry.getKey().length() > prefix.length()) {            if (strip)                result.put(entry.getKey().substring(prefix.length()), entry.getValue());            else                result.put(entry.getKey(), entry.getValue());        }    }    return result;}
f1901
0
valuesWithPrefixOverride
public Map<String, Object> kafkatest_f1902_0(String prefix)
{    Map<String, Object> result = new RecordingMap<>(values(), prefix, true);    for (Map.Entry<String, ?> entry : originals.entrySet()) {        if (entry.getKey().startsWith(prefix) && entry.getKey().length() > prefix.length()) {            String keyWithNoPrefix = entry.getKey().substring(prefix.length());            ConfigDef.ConfigKey configKey = definition.configKeys().get(keyWithNoPrefix);            if (configKey != null)                result.put(keyWithNoPrefix, definition.parseValue(configKey, entry.getValue(), true));            else {                String keyWithNoSecondaryPrefix = keyWithNoPrefix.substring(keyWithNoPrefix.indexOf('.') + 1);                configKey = definition.configKeys().get(keyWithNoSecondaryPrefix);                if (configKey != null)                    result.put(keyWithNoPrefix, definition.parseValue(configKey, entry.getValue(), true));            }        }    }    return result;}
f1902
0
getConfiguredInstances
public List<T> kafkatest_f1910_0(String key, Class<T> t, Map<String, Object> configOverrides)
{    return getConfiguredInstances(getList(key), t, configOverrides);}
f1910
0
getConfiguredInstances
public List<T> kafkatest_f1911_0(List<String> classNames, Class<T> t, Map<String, Object> configOverrides)
{    List<T> objects = new ArrayList<>();    if (classNames == null)        return objects;    Map<String, Object> configPairs = originals();    configPairs.putAll(configOverrides);    for (Object klass : classNames) {        Object o = getConfiguredInstance(klass, t, configPairs);        objects.add(t.cast(o));    }    return objects;}
f1911
0
extractPotentialVariables
private Map<String, String> kafkatest_f1912_0(Map<?, ?> configMap)
{    // Variables are tuples of the form "${providerName:[path:]key}". From the configMap we extract the subset of configs with string    // values as potential variables.    Map<String, String> configMapAsString = new HashMap<>();    for (Map.Entry<?, ?> entry : configMap.entrySet()) {        if (entry.getValue() instanceof String)            configMapAsString.put((String) entry.getKey(), (String) entry.getValue());    }    return configMapAsString;}
f1912
0
configValues
public List<ConfigValue> kafkatest_f1920_0()
{    return configValues;}
f1920
0
data
public Map<String, String> kafkatest_f1921_0()
{    return data;}
f1921
0
ttl
public Long kafkatest_f1922_0()
{    return ttl;}
f1922
0
define
public ConfigDef kafkatest_f1930_0(String name, Type type, Object defaultValue, Importance importance, String documentation, String group, int orderInGroup, Width width, String displayName, List<String> dependents, Recommender recommender)
{    return define(name, type, defaultValue, null, importance, documentation, group, orderInGroup, width, displayName, dependents, recommender);}
f1930
0
define
public ConfigDef kafkatest_f1931_0(String name, Type type, Object defaultValue, Importance importance, String documentation, String group, int orderInGroup, Width width, String displayName, List<String> dependents)
{    return define(name, type, defaultValue, null, importance, documentation, group, orderInGroup, width, displayName, dependents, null);}
f1931
0
define
public ConfigDef kafkatest_f1932_0(String name, Type type, Object defaultValue, Importance importance, String documentation, String group, int orderInGroup, Width width, String displayName, Recommender recommender)
{    return define(name, type, defaultValue, null, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList(), recommender);}
f1932
0
define
public ConfigDef kafkatest_f1940_0(String name, Type type, Importance importance, String documentation)
{    return define(name, type, NO_DEFAULT_VALUE, null, importance, documentation);}
f1940
0
defineInternal
public ConfigDef kafkatest_f1941_0(final String name, final Type type, final Object defaultValue, final Importance importance)
{    return define(new ConfigKey(name, type, defaultValue, null, importance, "", "", -1, Width.NONE, name, Collections.<String>emptyList(), null, true));}
f1941
0
configKeys
public Map<String, ConfigKey> kafkatest_f1942_0()
{    return configKeys;}
f1942
0
parseForValidate
 Map<String, Object> kafkatest_f1950_0(Map<String, String> props, Map<String, ConfigValue> configValues)
{    Map<String, Object> parsed = new HashMap<>();    Set<String> configsWithNoParent = getConfigsWithNoParent();    for (String name : configsWithNoParent) {        parseForValidate(name, props, parsed, configValues);    }    return parsed;}
f1950
0
validate
private Map<String, ConfigValue> kafkatest_f1951_0(Map<String, Object> parsed, Map<String, ConfigValue> configValues)
{    Set<String> configsWithNoParent = getConfigsWithNoParent();    for (String name : configsWithNoParent) {        validate(name, parsed, configValues);    }    return configValues;}
f1951
0
undefinedDependentConfigs
private List<String> kafkatest_f1952_0()
{    Set<String> undefinedConfigKeys = new HashSet<>();    for (ConfigKey configKey : configKeys.values()) {        for (String dependent : configKey.dependents) {            if (!configKeys.containsKey(dependent)) {                undefinedConfigKeys.add(dependent);            }        }    }    return new ArrayList<>(undefinedConfigKeys);}
f1952
0
between
public static Range kafkatest_f1960_0(Number min, Number max)
{    return new Range(min, max);}
f1960
0
ensureValid
public void kafkatest_f1961_0(String name, Object o)
{    if (o == null)        throw new ConfigException(name, null, "Value must be non-null");    Number n = (Number) o;    if (min != null && n.doubleValue() < min.doubleValue())        throw new ConfigException(name, o, "Value must be at least " + min);    if (max != null && n.doubleValue() > max.doubleValue())        throw new ConfigException(name, o, "Value must be no more than " + max);}
f1961
0
toString
public String kafkatest_f1962_0()
{    if (min == null && max == null)        return "[...]";    else if (min == null)        return "[...," + max + "]";    else if (max == null)        return "[" + min + ",...]";    else        return "[" + min + ",...," + max + "]";}
f1962
0
toString
public String kafkatest_f1970_0()
{    return "non-null string";}
f1970
0
with
public static LambdaValidator kafkatest_f1971_0(BiConsumer<String, Object> ensureValid, Supplier<String> toStringFunction)
{    return new LambdaValidator(ensureValid, toStringFunction);}
f1971
0
ensureValid
public void kafkatest_f1972_0(String name, Object value)
{    ensureValid.accept(name, value);}
f1972
0
ensureValid
public void kafkatest_f1980_0(String name, Object value)
{    String s = (String) value;    if (s == null) {        // thus we can ok a null value here        return;    } else if (s.isEmpty()) {        throw new ConfigException(name, value, "String may not be empty");    }    // Check name string for illegal characters    ArrayList<Integer> foundIllegalCharacters = new ArrayList<>();    for (int i = 0; i < s.length(); i++) {        if (Character.isISOControl(s.codePointAt(i))) {            foundIllegalCharacters.add(s.codePointAt(i));        }    }    if (!foundIllegalCharacters.isEmpty()) {        throw new ConfigException(name, value, "String may not contain control sequences but had the following ASCII chars: " + Utils.join(foundIllegalCharacters, ", "));    }}
f1980
0
toString
public String kafkatest_f1981_0()
{    return "non-empty string without ISO control characters";}
f1981
0
hasDefault
public boolean kafkatest_f1982_0()
{    return !NO_DEFAULT_VALUE.equals(this.defaultValue);}
f1982
0
toRst
public String kafkatest_f1990_0()
{    StringBuilder b = new StringBuilder();    for (ConfigKey key : sortedConfigs()) {        if (key.internalConfig) {            continue;        }        getConfigKeyRst(key, b);        b.append("\n");    }    return b.toString();}
f1990
0
toEnrichedRst
public String kafkatest_f1991_0()
{    StringBuilder b = new StringBuilder();    String lastKeyGroupName = "";    for (ConfigKey key : sortedConfigs()) {        if (key.internalConfig) {            continue;        }        if (key.group != null) {            if (!lastKeyGroupName.equalsIgnoreCase(key.group)) {                b.append(key.group).append("\n");                char[] underLine = new char[key.group.length()];                Arrays.fill(underLine, '^');                b.append(new String(underLine)).append("\n\n");            }            lastKeyGroupName = key.group;        }        getConfigKeyRst(key, b);        if (key.dependents != null && key.dependents.size() > 0) {            int j = 0;            b.append("  * Dependents: ");            for (String dependent : key.dependents) {                b.append("``");                b.append(dependent);                if (++j == key.dependents.size())                    b.append("``");                else                    b.append("``, ");            }            b.append("\n");        }        b.append("\n");    }    return b.toString();}
f1991
0
getConfigKeyRst
private void kafkatest_f1992_0(ConfigKey key, StringBuilder b)
{    b.append("``").append(key.name).append("``").append("\n");    for (String docLine : key.documentation.split("\n")) {        if (docLine.length() == 0) {            continue;        }        b.append("  ").append(docLine).append("\n\n");    }    b.append("  * Type: ").append(getConfigValue(key, "Type")).append("\n");    if (key.hasDefault()) {        b.append("  * Default: ").append(getConfigValue(key, "Default")).append("\n");    }    if (key.validator != null) {        b.append("  * Valid Values: ").append(getConfigValue(key, "Valid Values")).append("\n");    }    b.append("  * Importance: ").append(getConfigValue(key, "Importance")).append("\n");}
f1992
0
embeddedRecommender
private static Recommender kafkatest_f2000_0(final String keyPrefix, final Recommender base)
{    if (base == null)        return null;    return new Recommender() {        private String unprefixed(String k) {            return k.substring(keyPrefix.length());        }        private Map<String, Object> unprefixed(Map<String, Object> parsedConfig) {            final Map<String, Object> unprefixedParsedConfig = new HashMap<>(parsedConfig.size());            for (Map.Entry<String, Object> e : parsedConfig.entrySet()) {                if (e.getKey().startsWith(keyPrefix)) {                    unprefixedParsedConfig.put(unprefixed(e.getKey()), e.getValue());                }            }            return unprefixedParsedConfig;        }        @Override        public List<Object> validValues(String name, Map<String, Object> parsedConfig) {            return base.validValues(unprefixed(name), unprefixed(parsedConfig));        }        @Override        public boolean visible(String name, Map<String, Object> parsedConfig) {            return base.visible(unprefixed(name), unprefixed(parsedConfig));        }    };}
f2000
0
unprefixed
private String kafkatest_f2001_0(String k)
{    return k.substring(keyPrefix.length());}
f2001
0
unprefixed
private Map<String, Object> kafkatest_f2002_0(Map<String, Object> parsedConfig)
{    final Map<String, Object> unprefixedParsedConfig = new HashMap<>(parsedConfig.size());    for (Map.Entry<String, Object> e : parsedConfig.entrySet()) {        if (e.getKey().startsWith(keyPrefix)) {            unprefixedParsedConfig.put(unprefixed(e.getKey()), e.getValue());        }    }    return unprefixedParsedConfig;}
f2002
0
type
public Type kafkatest_f2010_0()
{    return type;}
f2010
0
name
public String kafkatest_f2011_0()
{    return name;}
f2011
0
isDefault
public boolean kafkatest_f2012_0()
{    return name.isEmpty();}
f2012
0
data
public Map<String, String> kafkatest_f2020_0()
{    return data;}
f2020
0
ttls
public Map<String, Long> kafkatest_f2021_0()
{    return ttls;}
f2021
0
name
public String kafkatest_f2022_0()
{    return name;}
f2022
0
visible
public void kafkatest_f2030_0(boolean visible)
{    this.visible = visible;}
f2030
0
equals
public boolean kafkatest_f2031_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ConfigValue that = (ConfigValue) o;    return Objects.equals(name, that.name) && Objects.equals(value, that.value) && Objects.equals(recommendedValues, that.recommendedValues) && Objects.equals(errorMessages, that.errorMessages) && Objects.equals(visible, that.visible);}
f2031
0
hashCode
public int kafkatest_f2032_0()
{    return Objects.hash(name, value, recommendedValues, errorMessages, visible);}
f2032
0
addClientSaslSupport
public static void kafkatest_f2042_0(ConfigDef config)
{    config.define(SaslConfigs.SASL_KERBEROS_SERVICE_NAME, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM, SaslConfigs.SASL_KERBEROS_SERVICE_NAME_DOC).define(SaslConfigs.SASL_KERBEROS_KINIT_CMD, ConfigDef.Type.STRING, SaslConfigs.DEFAULT_KERBEROS_KINIT_CMD, ConfigDef.Importance.LOW, SaslConfigs.SASL_KERBEROS_KINIT_CMD_DOC).define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR, ConfigDef.Type.DOUBLE, SaslConfigs.DEFAULT_KERBEROS_TICKET_RENEW_WINDOW_FACTOR, ConfigDef.Importance.LOW, SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR_DOC).define(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER, ConfigDef.Type.DOUBLE, SaslConfigs.DEFAULT_KERBEROS_TICKET_RENEW_JITTER, ConfigDef.Importance.LOW, SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER_DOC).define(SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN, ConfigDef.Type.LONG, SaslConfigs.DEFAULT_KERBEROS_MIN_TIME_BEFORE_RELOGIN, ConfigDef.Importance.LOW, SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN_DOC).define(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR, ConfigDef.Type.DOUBLE, SaslConfigs.DEFAULT_LOGIN_REFRESH_WINDOW_FACTOR, Range.between(0.5, 1.0), ConfigDef.Importance.LOW, SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR_DOC).define(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER, ConfigDef.Type.DOUBLE, SaslConfigs.DEFAULT_LOGIN_REFRESH_WINDOW_JITTER, Range.between(0.0, 0.25), ConfigDef.Importance.LOW, SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER_DOC).define(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS, ConfigDef.Type.SHORT, SaslConfigs.DEFAULT_LOGIN_REFRESH_MIN_PERIOD_SECONDS, Range.between(0, 900), ConfigDef.Importance.LOW, SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS_DOC).define(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS, ConfigDef.Type.SHORT, SaslConfigs.DEFAULT_LOGIN_REFRESH_BUFFER_SECONDS, Range.between(0, 3600), ConfigDef.Importance.LOW, SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS_DOC).define(SaslConfigs.SASL_MECHANISM, ConfigDef.Type.STRING, SaslConfigs.DEFAULT_SASL_MECHANISM, ConfigDef.Importance.MEDIUM, SaslConfigs.SASL_MECHANISM_DOC).define(SaslConfigs.SASL_JAAS_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.MEDIUM, SaslConfigs.SASL_JAAS_CONFIG_DOC).define(SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS, ConfigDef.Type.CLASS, null, ConfigDef.Importance.MEDIUM, SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS_DOC).define(SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS, ConfigDef.Type.CLASS, null, ConfigDef.Importance.MEDIUM, SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS_DOC).define(SaslConfigs.SASL_LOGIN_CLASS, ConfigDef.Type.CLASS, null, ConfigDef.Importance.MEDIUM, SaslConfigs.SASL_LOGIN_CLASS_DOC);}
f2042
0
forConfig
public static SslClientAuth kafkatest_f2043_0(String key)
{    if (key == null) {        return SslClientAuth.NONE;    }    String upperCaseKey = key.toUpperCase(Locale.ROOT);    for (SslClientAuth auth : VALUES) {        if (auth.name().equals(upperCaseKey)) {            return auth;        }    }    return null;}
f2043
0
addClientSslSupport
public static void kafkatest_f2044_0(ConfigDef config)
{    config.define(SslConfigs.SSL_PROTOCOL_CONFIG, ConfigDef.Type.STRING, SslConfigs.DEFAULT_SSL_PROTOCOL, ConfigDef.Importance.MEDIUM, SslConfigs.SSL_PROTOCOL_DOC).define(SslConfigs.SSL_PROVIDER_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM, SslConfigs.SSL_PROVIDER_DOC).define(SslConfigs.SSL_CIPHER_SUITES_CONFIG, ConfigDef.Type.LIST, null, ConfigDef.Importance.LOW, SslConfigs.SSL_CIPHER_SUITES_DOC).define(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, ConfigDef.Type.LIST, SslConfigs.DEFAULT_SSL_ENABLED_PROTOCOLS, ConfigDef.Importance.MEDIUM, SslConfigs.SSL_ENABLED_PROTOCOLS_DOC).define(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, ConfigDef.Type.STRING, SslConfigs.DEFAULT_SSL_KEYSTORE_TYPE, ConfigDef.Importance.MEDIUM, SslConfigs.SSL_KEYSTORE_TYPE_DOC).define(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.HIGH, SslConfigs.SSL_KEYSTORE_LOCATION_DOC).define(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.HIGH, SslConfigs.SSL_KEYSTORE_PASSWORD_DOC).define(SslConfigs.SSL_KEY_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.HIGH, SslConfigs.SSL_KEY_PASSWORD_DOC).define(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, ConfigDef.Type.STRING, SslConfigs.DEFAULT_SSL_TRUSTSTORE_TYPE, ConfigDef.Importance.MEDIUM, SslConfigs.SSL_TRUSTSTORE_TYPE_DOC).define(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.HIGH, SslConfigs.SSL_TRUSTSTORE_LOCATION_DOC).define(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.HIGH, SslConfigs.SSL_TRUSTSTORE_PASSWORD_DOC).define(SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG, ConfigDef.Type.STRING, SslConfigs.DEFAULT_SSL_KEYMANGER_ALGORITHM, ConfigDef.Importance.LOW, SslConfigs.SSL_KEYMANAGER_ALGORITHM_DOC).define(SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG, ConfigDef.Type.STRING, SslConfigs.DEFAULT_SSL_TRUSTMANAGER_ALGORITHM, ConfigDef.Importance.LOW, SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_DOC).define(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, ConfigDef.Type.STRING, SslConfigs.DEFAULT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, ConfigDef.Importance.LOW, SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_DOC).define(SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG, ConfigDef.Type.STRING, null, ConfigDef.Importance.LOW, SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_DOC);}
f2044
0
listener
public String kafkatest_f2052_0()
{    return listener;}
f2052
0
securityProtocol
public SecurityProtocol kafkatest_f2053_0()
{    return securityProtocol;}
f2053
0
host
public String kafkatest_f2054_0()
{    return host;}
f2054
0
invalidTopics
public Set<String> kafkatest_f2062_0()
{    return invalidTopics;}
f2062
0
recordTooLargePartitions
public Map<TopicPartition, Long> kafkatest_f2063_0()
{    return recordTooLargePartitions;}
f2063
0
fillInStackTrace
public Throwable kafkatest_f2064_0()
{    return this;}
f2064
0
add
public Headers kafkatest_f2072_0(String key, byte[] value) throws IllegalStateException
{    return add(new RecordHeader(key, value));}
f2072
0
remove
public Headers kafkatest_f2073_0(String key) throws IllegalStateException
{    canWrite();    checkKey(key);    Iterator<Header> iterator = iterator();    while (iterator.hasNext()) {        if (iterator.next().key().equals(key)) {            iterator.remove();        }    }    return this;}
f2073
0
lastHeader
public Header kafkatest_f2074_0(String key)
{    checkKey(key);    for (int i = headers.size() - 1; i >= 0; i--) {        Header header = headers.get(i);        if (header.key().equals(key)) {            return header;        }    }    return null;}
f2074
0
hasNext
public boolean kafkatest_f2082_0()
{    return original.hasNext();}
f2082
0
next
public Header kafkatest_f2083_0()
{    return original.next();}
f2083
0
remove
public void kafkatest_f2084_0()
{    canWrite();    original.remove();}
f2084
0
statusCode
public int kafkatest_f2092_0()
{    return statusCode;}
f2092
0
wrapAndThrow
private static void kafkatest_f2093_0(Throwable t) throws InterruptedException, ExecutionException
{    if (t instanceof CancellationException) {        throw (CancellationException) t;    } else if (t instanceof InterruptedException) {        throw (InterruptedException) t;    } else {        throw new ExecutionException(t);    }}
f2093
0
accept
public void kafkatest_f2094_0(A a, Throwable exception)
{    if (exception != null) {        future.completeExceptionally(exception);    } else {        try {            B b = function.apply(a);            future.complete(b);        } catch (Throwable t) {            future.completeExceptionally(t);        }    }}
f2094
0
whenComplete
public KafkaFuture<T> kafkatest_f2102_0(final BiConsumer<? super T, ? super Throwable> biConsumer)
{    final KafkaFutureImpl<T> future = new KafkaFutureImpl<>();    addWaiter(new WhenCompleteBiConsumer<>(future, biConsumer));    return future;}
f2102
0
addWaiter
protected synchronized void kafkatest_f2103_0(BiConsumer<? super T, ? super Throwable> action)
{    if (exception != null) {        action.accept(null, exception);    } else if (done) {        action.accept(value, null);    } else {        waiters.add(action);    }}
f2103
0
complete
public synchronized boolean kafkatest_f2104_0(T newValue)
{    List<BiConsumer<? super T, ? super Throwable>> oldWaiters;    synchronized (this) {        if (done)            return false;        value = newValue;        done = true;        oldWaiters = waiters;        waiters = null;    }    for (BiConsumer<? super T, ? super Throwable> waiter : oldWaiters) {        waiter.accept(newValue, null);    }    return true;}
f2104
0
isDone
public synchronized boolean kafkatest_f2112_0()
{    return done;}
f2112
0
toString
public String kafkatest_f2113_0()
{    return String.format("KafkaFuture{value=%s,exception=%s,done=%b}", value, exception, done);}
f2113
0
moveToEnd
public void kafkatest_f2114_0(TopicPartition topicPartition)
{    S state = map.remove(topicPartition);    if (state != null)        map.put(topicPartition, state);}
f2114
0
partitionStateMap
public LinkedHashMap<TopicPartition, S> kafkatest_f2122_0()
{    return new LinkedHashMap<>(map);}
f2122
0
partitionStateValues
public List<S> kafkatest_f2123_0()
{    return new ArrayList<>(map.values());}
f2123
0
stateValue
public S kafkatest_f2124_0(TopicPartition topicPartition)
{    return map.get(topicPartition);}
f2124
0
topicPartition
public TopicPartition kafkatest_f2132_0()
{    return topicPartition;}
f2132
0
toString
public String kafkatest_f2133_0()
{    return "PartitionState(" + topicPartition + "=" + value + ')';}
f2133
0
validate
public static void kafkatest_f2134_0(String topic)
{    if (topic.isEmpty())        throw new InvalidTopicException("Topic name is illegal, it can't be empty");    if (topic.equals(".") || topic.equals(".."))        throw new InvalidTopicException("Topic name cannot be \".\" or \"..\"");    if (topic.length() > MAX_NAME_LENGTH)        throw new InvalidTopicException("Topic name is illegal, it can't be longer than " + MAX_NAME_LENGTH + " characters, topic name: " + topic);    if (!containsValidPattern(topic))        throw new InvalidTopicException("Topic name \"" + topic + "\" is illegal, it contains a character other than " + "ASCII alphanumerics, '.', '_' and '-'");}
f2134
0
allOf
public static KafkaFuture<Void> kafkatest_f2142_0(KafkaFuture<?>... futures)
{    KafkaFuture<Void> allOfFuture = new KafkaFutureImpl<>();    AllOfAdapter<Object> allOfWaiter = new AllOfAdapter<>(futures.length, allOfFuture);    for (KafkaFuture<?> future : futures) {        future.addWaiter(allOfWaiter);    }    return allOfFuture;}
f2142
0
bufferToBeReturned
protected void kafkatest_f2143_0(ByteBuffer justAllocated)
{    BufferReference ref = new BufferReference(justAllocated, garbageCollectedBuffers);    BufferMetadata metadata = new BufferMetadata(justAllocated.capacity());    if (buffersInFlight.put(ref, metadata) != null)        // the same identity or we failed to register a released/GC'ed buffer        throw new IllegalStateException("allocated buffer identity " + ref.hashCode + " already registered as in use?!");    log.trace("allocated buffer of size {} and identity {}", sizeBytes, ref.hashCode);}
f2143
0
bufferToBeReleased
protected void kafkatest_f2144_0(ByteBuffer justReleased)
{    // used ro lookup only    BufferReference ref = new BufferReference(justReleased);    BufferMetadata metadata = buffersInFlight.remove(ref);    if (metadata == null)        // in the function arg) so this means either a double free or not our buffer.        throw new IllegalArgumentException("returned buffer " + ref.hashCode + " was never allocated by this pool");    if (metadata.sizeBytes != justReleased.capacity()) {        // this is a bug        throw new IllegalStateException("buffer " + ref.hashCode + " has capacity " + justReleased.capacity() + " but recorded as " + metadata.sizeBytes);    }    log.trace("released buffer of size {} and identity {}", metadata.sizeBytes, ref.hashCode);}
f2144
0
size
public long kafkatest_f2152_0()
{    return Long.MAX_VALUE;}
f2152
0
availableMemory
public long kafkatest_f2153_0()
{    return Long.MAX_VALUE;}
f2153
0
isOutOfMemory
public boolean kafkatest_f2154_0()
{    return false;}
f2154
0
bufferToBeReleased
protected void kafkatest_f2162_0(ByteBuffer justReleased)
{    log.trace("released buffer of size {}", justReleased.capacity());}
f2162
0
toString
public String kafkatest_f2163_0()
{    long allocated = sizeBytes - availableMemory.get();    return "SimpleMemoryPool{" + Utils.formatBytes(allocated) + "/" + Utils.formatBytes(sizeBytes) + " used}";}
f2163
0
maybeRecordEndOfDrySpell
protected void kafkatest_f2164_0()
{    if (oomTimeSensor != null) {        long startOfDrySpell = startOfNoMemPeriod.getAndSet(0);        if (startOfDrySpell != 0) {            // how long were we refusing allocation requests for            // fractional (double) millis            oomTimeSensor.record((System.nanoTime() - startOfDrySpell) / 1000000.0);        }    }}
f2164
0
getTags
private static LinkedHashSet<String> kafkatest_f2172_0(String... keys)
{    LinkedHashSet<String> tags = new LinkedHashSet<>();    Collections.addAll(tags, keys);    return tags;}
f2172
0
name
public String kafkatest_f2173_0()
{    return this.name;}
f2173
0
group
public String kafkatest_f2174_0()
{    return this.group;}
f2174
0
init
public void kafkatest_f2183_0(List<KafkaMetric> metrics)
{    synchronized (LOCK) {        for (KafkaMetric metric : metrics) addAttribute(metric);        for (KafkaMbean mbean : mbeans.values()) reregister(mbean);    }}
f2183
0
containsMbean
public boolean kafkatest_f2184_0(String mbeanName)
{    return mbeans.containsKey(mbeanName);}
f2184
0
metricChange
public void kafkatest_f2185_0(KafkaMetric metric)
{    synchronized (LOCK) {        KafkaMbean mbean = addAttribute(metric);        reregister(mbean);    }}
f2185
0
name
public ObjectName kafkatest_f2193_0()
{    return objectName;}
f2193
0
setAttribute
public void kafkatest_f2194_0(String name, KafkaMetric metric)
{    this.metrics.put(name, metric);}
f2194
0
getAttribute
public Object kafkatest_f2195_0(String name) throws AttributeNotFoundException, MBeanException, ReflectionException
{    if (this.metrics.containsKey(name))        return this.metrics.get(name).metricValue();    else        throw new AttributeNotFoundException("Could not find attribute " + name);}
f2195
0
metricName
public MetricName kafkatest_f2203_0()
{    return this.metricName;}
f2203
0
value
public double kafkatest_f2204_0()
{    return measurableValue(time.milliseconds());}
f2204
0
metricValue
public Object kafkatest_f2205_0()
{    long now = time.milliseconds();    synchronized (this.lock) {        if (this.metricValueProvider instanceof Measurable)            return ((Measurable) metricValueProvider).measure(config, now);        else if (this.metricValueProvider instanceof Gauge)            return ((Gauge<?>) metricValueProvider).value(config, now);        else            throw new IllegalStateException("Not a valid metric: " + this.metricValueProvider.getClass());    }}
f2205
0
timeWindowMs
public long kafkatest_f2213_0()
{    return timeWindowMs;}
f2213
0
timeWindow
public MetricConfig kafkatest_f2214_0(long window, TimeUnit unit)
{    this.timeWindowMs = TimeUnit.MILLISECONDS.convert(window, unit);    return this;}
f2214
0
tags
public Map<String, String> kafkatest_f2215_0()
{    return this.tags;}
f2215
0
metricName
public MetricName kafkatest_f2223_0(String name, String group, String description, Map<String, String> tags)
{    Map<String, String> combinedTag = new LinkedHashMap<>(config.tags());    combinedTag.putAll(tags);    return new MetricName(name, group, description, combinedTag);}
f2223
0
metricName
public MetricName kafkatest_f2224_0(String name, String group, String description)
{    return metricName(name, group, description, new HashMap<String, String>());}
f2224
0
metricName
public MetricName kafkatest_f2225_0(String name, String group)
{    return metricName(name, group, "", new HashMap<String, String>());}
f2225
0
sensor
public Sensor kafkatest_f2233_0(String name, Sensor.RecordingLevel recordingLevel)
{    return sensor(name, null, recordingLevel, (Sensor[]) null);}
f2233
0
sensor
public Sensor kafkatest_f2234_0(String name, Sensor... parents)
{    return this.sensor(name, Sensor.RecordingLevel.INFO, parents);}
f2234
0
sensor
public Sensor kafkatest_f2235_0(String name, Sensor.RecordingLevel recordingLevel, Sensor... parents)
{    return sensor(name, null, recordingLevel, parents);}
f2235
0
addMetric
public void kafkatest_f2243_0(MetricName metricName, MetricConfig config, MetricValueProvider<?> metricValueProvider)
{    KafkaMetric m = new KafkaMetric(new Object(), Objects.requireNonNull(metricName), Objects.requireNonNull(metricValueProvider), config == null ? this.config : config, time);    registerMetric(m);}
f2243
0
addMetric
public void kafkatest_f2244_0(MetricName metricName, MetricValueProvider<?> metricValueProvider)
{    addMetric(metricName, null, metricValueProvider);}
f2244
0
removeMetric
public synchronized KafkaMetricf2245_1MetricName metricName)
{    KafkaMetric metric = this.metrics.remove(metricName);    if (metric != null) {        for (MetricsReporter reporter : reporters) {            try {                reporter.metricRemoval(metric);            } catch (Exception e) {                            }        }        log.trace("Removed metric named {}", metricName);    }    return metric;}
public synchronized KafkaMetricf2245
1
childrenSensors
 Map<Sensor, List<Sensor>> kafkatest_f2253_0()
{    return Collections.unmodifiableMap(childrenSensors);}
f2253
0
metricInstance
public MetricName kafkatest_f2254_0(MetricNameTemplate template, String... keyValue)
{    return metricInstance(template, getTags(keyValue));}
f2254
0
metricInstance
public MetricName kafkatest_f2255_0(MetricNameTemplate template, Map<String, String> tags)
{    // check to make sure that the runtime defined tags contain all the template tags.    Set<String> runtimeTagKeys = new HashSet<>(tags.keySet());    runtimeTagKeys.addAll(config().tags().keySet());    Set<String> templateTagKeys = template.tags();    if (!runtimeTagKeys.equals(templateTagKeys)) {        throw new IllegalArgumentException("For '" + template.name() + "', runtime-defined metric tags do not match the tags in the template. " + "Runtime = " + runtimeTagKeys.toString() + " Template = " + templateTagKeys.toString());    }    return this.metricName(template.name(), template.group(), template.description(), tags);}
f2255
0
equals
public boolean kafkatest_f2263_0(Object obj)
{    if (this == obj)        return true;    if (!(obj instanceof Quota))        return false;    Quota that = (Quota) obj;    return (that.bound == this.bound) && (that.upper == this.upper);}
f2263
0
toString
public String kafkatest_f2264_0()
{    return (upper ? "upper=" : "lower=") + bound;}
f2264
0
metricName
public MetricName kafkatest_f2265_0()
{    return metricName;}
f2265
0
parents
 List<Sensor> kafkatest_f2273_0()
{    return unmodifiableList(asList(parents));}
f2273
0
record
public void kafkatest_f2274_0()
{    if (shouldRecord()) {        record(1.0);    }}
f2274
0
shouldRecord
public boolean kafkatest_f2275_0()
{    return this.recordingLevel.shouldRecord(config.recordLevel().id);}
f2275
0
add
public boolean kafkatest_f2283_0(MetricName metricName, MeasurableStat stat)
{    return add(metricName, stat, null);}
f2283
0
add
public synchronized boolean kafkatest_f2284_0(final MetricName metricName, final MeasurableStat stat, final MetricConfig config)
{    if (hasExpired()) {        return false;    } else if (metrics.containsKey(metricName)) {        return true;    } else {        final KafkaMetric metric = new KafkaMetric(metricLock(), Objects.requireNonNull(metricName), Objects.requireNonNull(stat), config == null ? this.config : config, time);        registry.registerMetric(metric);        metrics.put(metric.metricName(), metric);        stats.add(stat);        return true;    }}
f2284
0
hasExpired
public boolean kafkatest_f2285_0()
{    return (time.milliseconds() - this.lastRecordTime) > this.inactiveSensorExpirationTimeMs;}
f2285
0
forBooleanValues
public static Frequencies kafkatest_f2293_0(MetricName falseMetricName, MetricName trueMetricName)
{    List<Frequency> frequencies = new ArrayList<>();    if (falseMetricName != null) {        frequencies.add(new Frequency(falseMetricName, 0.0));    }    if (trueMetricName != null) {        frequencies.add(new Frequency(trueMetricName, 1.0));    }    if (frequencies.isEmpty()) {        throw new IllegalArgumentException("Must specify at least one metric name");    }    Frequency[] frequencyArray = frequencies.toArray(new Frequency[frequencies.size()]);    return new Frequencies(2, 0.0, 1.0, frequencyArray);}
f2293
0
stats
public List<NamedMeasurable> kafkatest_f2294_0()
{    List<NamedMeasurable> ms = new ArrayList<>(frequencies.length);    for (Frequency frequency : frequencies) {        final double center = frequency.centerValue();        ms.add(new NamedMeasurable(frequency.name(), new Measurable() {            public double measure(MetricConfig config, long now) {                return frequency(config, now, center);            }        }));    }    return ms;}
f2294
0
measure
public double kafkatest_f2295_0(MetricConfig config, long now)
{    return frequency(config, now, center);}
f2295
0
centerValue
public double kafkatest_f2303_0()
{    return this.centerValue;}
f2303
0
record
public void kafkatest_f2304_0(double value)
{    this.hist[binScheme.toBin(value)] += 1.0f;    this.count += 1.0d;}
f2304
0
value
public double kafkatest_f2305_0(double quantile)
{    if (count == 0.0d)        return Double.NaN;    if (quantile > 1.00d)        return Float.POSITIVE_INFINITY;    if (quantile < 0.00d)        return Float.NEGATIVE_INFINITY;    float sum = 0.0f;    float quant = (float) quantile;    for (int i = 0; i < this.hist.length - 1; i++) {        sum += this.hist[i];        if (sum / count > quant)            return binScheme.fromBin(i);    }    return binScheme.fromBin(this.hist.length - 1);}
f2305
0
fromBin
public double kafkatest_f2313_0(int b)
{    if (b > this.bins - 1) {        return Float.POSITIVE_INFINITY;    } else if (b < 0.0000d) {        return Float.NEGATIVE_INFINITY;    } else {        return this.scale * (b * (b + 1.0)) / 2.0;    }}
f2313
0
toBin
public int kafkatest_f2314_0(double x)
{    if (x < 0.0d) {        throw new IllegalArgumentException("Values less than 0.0 not accepted.");    } else if (x > this.max) {        return this.bins - 1;    } else {        return (int) (-0.5 + 0.5 * Math.sqrt(1.0 + 8.0 * x / this.scale));    }}
f2314
0
update
protected void kafkatest_f2315_0(Sample sample, MetricConfig config, double value, long now)
{    sample.value = Math.max(sample.value, value);}
f2315
0
stats
public List<NamedMeasurable> kafkatest_f2323_0()
{    List<NamedMeasurable> ms = new ArrayList<NamedMeasurable>(this.percentiles.length);    for (Percentile percentile : this.percentiles) {        final double pct = percentile.percentile();        ms.add(new NamedMeasurable(percentile.name(), new Measurable() {            public double measure(MetricConfig config, long now) {                return value(config, now, pct / 100.0);            }        }));    }    return ms;}
f2323
0
measure
public double kafkatest_f2324_0(MetricConfig config, long now)
{    return value(config, now, pct / 100.0);}
f2324
0
value
public double kafkatest_f2325_0(MetricConfig config, long now, double quantile)
{    purgeObsoleteSamples(config, now);    float count = 0.0f;    for (Sample sample : this.samples) count += sample.eventCount;    if (count == 0.0f)        return Double.NaN;    float sum = 0.0f;    float quant = (float) quantile;    for (int b = 0; b < buckets; b++) {        for (Sample s : this.samples) {            HistogramSample sample = (HistogramSample) s;            float[] hist = sample.histogram.counts();            sum += hist[b];            if (sum / count > quant)                return binScheme.fromBin(b);        }    }    return Double.POSITIVE_INFINITY;}
f2325
0
windowSize
public long kafkatest_f2333_0(MetricConfig config, long now)
{    // purge old samples before we compute the window size    stat.purgeObsoleteSamples(config, now);    /*         * Here we check the total amount of time elapsed since the oldest non-obsolete window.         * This give the total windowSize of the batch which is the time used for Rate computation.         * However, there is an issue if we do not have sufficient data for e.g. if only 1 second has elapsed in a 30 second         * window, the measured rate will be very high.         * Hence we assume that the elapsed time is always N-1 complete windows plus whatever fraction of the final window is complete.         *         * Note that we could simply count the amount of time elapsed in the current window and add n-1 windows to get the total time,         * but this approach does not account for sleeps. SampledStat only creates samples whenever record is called,         * if no record is called for a period of time that time is not accounted for in windowSize and produces incorrect results.         */    long totalElapsedTimeMs = now - stat.oldest(now).lastWindowMs;    // Check how many full windows of data we have currently retained    int numFullWindows = (int) (totalElapsedTimeMs / config.timeWindowMs());    int minFullWindows = config.samples() - 1;    // If the available windows are less than the minimum required, add the difference to the totalElapsedTime    if (numFullWindows < minFullWindows)        totalElapsedTimeMs += (minFullWindows - numFullWindows) * config.timeWindowMs();    return totalElapsedTimeMs;}
f2333
0
convert
private double kafkatest_f2334_0(long timeMs)
{    switch(unit) {        case NANOSECONDS:            return timeMs * 1000.0 * 1000.0;        case MICROSECONDS:            return timeMs * 1000.0;        case MILLISECONDS:            return timeMs;        case SECONDS:            return timeMs / 1000.0;        case MINUTES:            return timeMs / (60.0 * 1000.0);        case HOURS:            return timeMs / (60.0 * 60.0 * 1000.0);        case DAYS:            return timeMs / (24.0 * 60.0 * 60.0 * 1000.0);        default:            throw new IllegalStateException("Unknown unit: " + unit);    }}
f2334
0
record
public void kafkatest_f2335_0(MetricConfig config, double value, long timeMs)
{    Sample sample = current(timeMs);    if (sample.isComplete(timeMs, config))        sample = advance(config, timeMs);    update(sample, config, value, timeMs);    sample.eventCount += 1;}
f2335
0
isComplete
public boolean kafkatest_f2343_0(long timeMs, MetricConfig config)
{    return timeMs - lastWindowMs >= config.timeWindowMs() || eventCount >= config.eventWindow();}
f2343
0
windowSize
public long kafkatest_f2344_0(MetricConfig config, long now)
{    stat.purgeObsoleteSamples(config, now);    long elapsed = now - stat.oldest(now).lastWindowMs;    return elapsed < config.timeWindowMs() ? config.timeWindowMs() : elapsed;}
f2344
0
measure
public double kafkatest_f2345_0(MetricConfig config, long now)
{    return value;}
f2345
0
reauthenticationLatencyMs
 Long kafkatest_f2354_0()
{    return null;}
f2354
0
getAndClearResponsesReceivedDuringReauthentication
 List<NetworkReceive> kafkatest_f2355_0()
{    return Collections.emptyList();}
f2355
0
connectedClientSupportsReauthentication
 boolean kafkatest_f2356_0()
{    return false;}
f2356
0
requireNonNullMode
private static void kafkatest_f2364_0(Mode mode, SecurityProtocol securityProtocol)
{    if (mode == null)        throw new IllegalArgumentException("`mode` must be non-null if `securityProtocol` is `" + securityProtocol + "`");}
f2364
0
createPrincipalBuilder
private static org.apache.kafka.common.security.auth.PrincipalBuilder kafkatest_f2365_0(Class<?> principalBuilderClass, Map<String, ?> configs)
{    org.apache.kafka.common.security.auth.PrincipalBuilder principalBuilder;    if (principalBuilderClass == null)        principalBuilder = new org.apache.kafka.common.security.auth.DefaultPrincipalBuilder();    else        principalBuilder = (org.apache.kafka.common.security.auth.PrincipalBuilder) Utils.newInstance(principalBuilderClass);    principalBuilder.configure(configs);    return principalBuilder;}
f2365
0
createPrincipalBuilder
public static KafkaPrincipalBuilder kafkatest_f2366_0(Map<String, ?> configs, TransportLayer transportLayer, Authenticator authenticator, KerberosShortNamer kerberosShortNamer, SslPrincipalMapper sslPrincipalMapper)
{    Class<?> principalBuilderClass = (Class<?>) configs.get(BrokerSecurityConfigs.PRINCIPAL_BUILDER_CLASS_CONFIG);    final KafkaPrincipalBuilder builder;    if (principalBuilderClass == null || principalBuilderClass == DefaultKafkaPrincipalBuilder.class) {        builder = new DefaultKafkaPrincipalBuilder(kerberosShortNamer, sslPrincipalMapper);    } else if (KafkaPrincipalBuilder.class.isAssignableFrom(principalBuilderClass)) {        builder = (KafkaPrincipalBuilder) Utils.newInstance(principalBuilderClass);    } else if (org.apache.kafka.common.security.auth.PrincipalBuilder.class.isAssignableFrom(principalBuilderClass)) {        org.apache.kafka.common.security.auth.PrincipalBuilder oldPrincipalBuilder = createPrincipalBuilder(principalBuilderClass, configs);        builder = DefaultKafkaPrincipalBuilder.fromOldPrincipalBuilder(authenticator, transportLayer, oldPrincipalBuilder, kerberosShortNamer);    } else {        throw new InvalidConfigurationException("Type " + principalBuilderClass.getName() + " is not " + "an instance of " + org.apache.kafka.common.security.auth.PrincipalBuilder.class.getName() + " or " + KafkaPrincipalBuilder.class.getName());    }    if (builder instanceof Configurable)        ((Configurable) builder).configure(configs);    return builder;}
f2366
0
state
public void kafkatest_f2374_0(ChannelState state)
{    this.state = state;}
f2374
0
state
public ChannelState kafkatest_f2375_0()
{    return this.state;}
f2375
0
finishConnect
public boolean kafkatest_f2376_0() throws IOException
{    // we need to grab remoteAddr before finishConnect() is called otherwise    // it becomes inaccessible if the connection was refused.    SocketChannel socketChannel = transportLayer.socketChannel();    if (socketChannel != null) {        remoteAddress = socketChannel.getRemoteAddress();    }    boolean connected = transportLayer.finishConnect();    if (connected) {        if (ready()) {            state = ChannelState.READY;        } else if (remoteAddress != null) {            state = new ChannelState(ChannelState.State.AUTHENTICATE, remoteAddress.toString());        } else {            state = ChannelState.AUTHENTICATE;        }    }    return connected;}
f2376
0
delayCloseOnAuthenticationFailure
private void kafkatest_f2384_0()
{    transportLayer.removeInterestOps(SelectionKey.OP_WRITE);}
f2384
0
completeCloseOnAuthenticationFailure
 void kafkatest_f2385_0() throws IOException
{    transportLayer.addInterestOps(SelectionKey.OP_WRITE);    // Invoke the underlying handler to finish up any processing on authentication failure    authenticator.handleAuthenticationFailure();}
f2385
0
isMute
public boolean kafkatest_f2386_0()
{    return muteState != ChannelMuteState.NOT_MUTED;}
f2386
0
write
public Send kafkatest_f2394_0() throws IOException
{    Send result = null;    if (send != null && send(send)) {        result = send;        send = null;    }    return result;}
f2394
0
addNetworkThreadTimeNanos
public void kafkatest_f2395_0(long nanos)
{    networkThreadTimeNanos += nanos;}
f2395
0
getAndResetNetworkThreadTimeNanos
public long kafkatest_f2396_0()
{    long current = networkThreadTimeNanos;    networkThreadTimeNanos = 0;    return current;}
f2396
0
maybeBeginServerReauthentication
public boolean kafkatest_f2404_0(NetworkReceive saslHandshakeNetworkReceive, Supplier<Long> nowNanosSupplier) throws AuthenticationException, IOException
{    if (!ready())        throw new IllegalStateException("KafkaChannel should be \"ready\" when processing SASL Handshake for potential re-authentication");    /*         * Re-authentication is disabled if there is no session expiration time, in         * which case the SASL handshake network receive will be processed normally,         * which results in a failure result being sent to the client. Also, no need to         * check if we are muted since since we are processing a received packet when we         * invoke this.         */    if (authenticator.serverSessionExpirationTimeNanos() == null)        return false;    /*         * We've delayed getting the time as long as possible in case we don't need it,         * but at this point we need it -- so get it now.         */    long nowNanos = nowNanosSupplier.get().longValue();    /*         * Cannot re-authenticate more than once every second; an attempt to do so will         * result in the SASL handshake network receive being processed normally, which         * results in a failure result being sent to the client.         */    if (lastReauthenticationStartNanos != 0 && nowNanos - lastReauthenticationStartNanos < MIN_REAUTH_INTERVAL_ONE_SECOND_NANOS)        return false;    lastReauthenticationStartNanos = nowNanos;    swapAuthenticatorsAndBeginReauthentication(new ReauthenticationContext(authenticator, saslHandshakeNetworkReceive, nowNanos));    return true;}
f2404
0
maybeBeginClientReauthentication
public boolean kafkatest_f2405_0(Supplier<Long> nowNanosSupplier) throws AuthenticationException, IOException
{    if (!ready())        throw new IllegalStateException("KafkaChannel should always be \"ready\" when it is checked for possible re-authentication");    if (muteState != ChannelMuteState.NOT_MUTED || midWrite || authenticator.clientSessionReauthenticationTimeNanos() == null)        return false;    /*         * We've delayed getting the time as long as possible in case we don't need it,         * but at this point we need it -- so get it now.         */    long nowNanos = nowNanosSupplier.get().longValue();    if (nowNanos < authenticator.clientSessionReauthenticationTimeNanos().longValue())        return false;    swapAuthenticatorsAndBeginReauthentication(new ReauthenticationContext(authenticator, receive, nowNanos));    receive = null;    return true;}
f2405
0
reauthenticationLatencyMs
public Long kafkatest_f2406_0()
{    return authenticator.reauthenticationLatencyMs();}
f2406
0
equals
public boolean kafkatest_f2414_0(Object o)
{    if (!(o instanceof ListenerName))        return false;    ListenerName that = (ListenerName) o;    return value.equals(that.value);}
f2414
0
hashCode
public int kafkatest_f2415_0()
{    return value.hashCode();}
f2415
0
toString
public String kafkatest_f2416_0()
{    return "ListenerName(" + value + ")";}
f2416
0
memoryAllocated
public boolean kafkatest_f2424_0()
{    return buffer != null;}
f2424
0
close
public void kafkatest_f2425_0() throws IOException
{    if (buffer != null && buffer != EMPTY_BUFFER) {        memoryPool.release(buffer);        buffer = null;    }}
f2425
0
payload
public ByteBuffer kafkatest_f2426_0()
{    return this.buffer;}
f2426
0
ready
public boolean kafkatest_f2436_0()
{    return true;}
f2436
0
finishConnect
public boolean kafkatest_f2437_0() throws IOException
{    boolean connected = socketChannel.finishConnect();    if (connected)        key.interestOps(key.interestOps() & ~SelectionKey.OP_CONNECT | SelectionKey.OP_READ);    return connected;}
f2437
0
disconnect
public void kafkatest_f2438_0()
{    key.cancel();}
f2438
0
read
public long kafkatest_f2447_0(ByteBuffer[] dsts, int offset, int length) throws IOException
{    return socketChannel.read(dsts, offset, length);}
f2447
0
write
public int kafkatest_f2448_0(ByteBuffer src) throws IOException
{    return socketChannel.write(src);}
f2448
0
write
public long kafkatest_f2449_0(ByteBuffer[] srcs) throws IOException
{    return socketChannel.write(srcs);}
f2449
0
transferFrom
public long kafkatest_f2457_0(FileChannel fileChannel, long position, long count) throws IOException
{    return fileChannel.transferTo(position, count, socketChannel);}
f2457
0
networkReceive
public NetworkReceive kafkatest_f2458_0()
{    return networkReceive;}
f2458
0
previousAuthenticator
public Authenticator kafkatest_f2459_0()
{    return previousAuthenticator;}
f2459
0
close
public void kafkatest_f2467_0()
{    for (LoginManager loginManager : loginManagers.values()) loginManager.release();    loginManagers.clear();    for (AuthenticateCallbackHandler handler : saslCallbackHandlers.values()) handler.close();}
f2467
0
buildTransportLayer
protected TransportLayer kafkatest_f2468_0(String id, SelectionKey key, SocketChannel socketChannel) throws IOException
{    if (this.securityProtocol == SecurityProtocol.SASL_SSL) {        return SslTransportLayer.create(id, key, sslFactory.createSslEngine(socketChannel.socket().getInetAddress().getHostName(), socketChannel.socket().getPort()));    } else {        return new PlaintextTransportLayer(key);    }}
f2468
0
buildServerAuthenticator
protected SaslServerAuthenticator kafkatest_f2469_0(Map<String, ?> configs, Map<String, AuthenticateCallbackHandler> callbackHandlers, String id, TransportLayer transportLayer, Map<String, Subject> subjects, Map<String, Long> connectionsMaxReauthMsByMechanism)
{    return new SaslServerAuthenticator(configs, callbackHandlers, id, subjects, kerberosShortNamer, listenerName, securityProtocol, transportLayer, connectionsMaxReauthMsByMechanism, time);}
f2469
0
clientCallbackHandlerClass
private Class<? extends AuthenticateCallbackHandler> kafkatest_f2477_0()
{    switch(clientSaslMechanism) {        case SaslConfigs.GSSAPI_MECHANISM:            return KerberosClientCallbackHandler.class;        case OAuthBearerLoginModule.OAUTHBEARER_MECHANISM:            return OAuthBearerSaslClientCallbackHandler.class;        default:            return SaslClientCallbackHandler.class;    }}
f2477
0
connect
public voidf2478_1String id, InetSocketAddress address, int sendBufferSize, int receiveBufferSize) throws IOException
{    ensureNotRegistered(id);    SocketChannel socketChannel = SocketChannel.open();    SelectionKey key = null;    try {        configureSocketChannel(socketChannel, sendBufferSize, receiveBufferSize);        boolean connected = doConnect(socketChannel, address);        key = registerChannel(id, socketChannel, SelectionKey.OP_CONNECT);        if (connected) {            // OP_CONNECT won't trigger for immediately connected channels                        immediatelyConnectedKeys.add(key);            key.interestOps(0);        }    } catch (IOException | RuntimeException e) {        if (key != null)            immediatelyConnectedKeys.remove(key);        channels.remove(id);        socketChannel.close();        throw e;    }}
public voidf2478
1
doConnect
protected boolean kafkatest_f2479_0(SocketChannel channel, InetSocketAddress address) throws IOException
{    try {        return channel.connect(address);    } catch (UnresolvedAddressException e) {        throw new IOException("Can't resolve address: " + address, e);    }}
f2479
0
send
public voidf2487_1Send send)
{    String connectionId = send.destination();    KafkaChannel channel = openOrClosingChannelOrFail(connectionId);    if (closingChannels.containsKey(connectionId)) {        // ensure notification via `disconnected`, leave channel in the state in which closing was triggered        this.failedSends.add(connectionId);    } else {        try {            channel.setSend(send);        } catch (Exception e) {            // update the state for consistency, the channel will be discarded after `close`            channel.state(ChannelState.FAILED_SEND);            // ensure notification via `disconnected` when `failedSends` are processed in the next poll            this.failedSends.add(connectionId);            close(channel, CloseMode.DISCARD_NO_NOTIFY);            if (!(e instanceof CancelledKeyException)) {                                throw e;            }        }    }}
public voidf2487
1
poll
public void kafkatest_f2488_0(long timeout) throws IOException
{    if (timeout < 0)        throw new IllegalArgumentException("timeout should be >= 0");    boolean madeReadProgressLastCall = madeReadProgressLastPoll;    clear();    boolean dataInBuffers = !keysWithBufferedRead.isEmpty();    if (hasStagedReceives() || !immediatelyConnectedKeys.isEmpty() || (madeReadProgressLastCall && dataInBuffers))        timeout = 0;    if (!memoryPool.isOutOfMemory() && outOfMemory) {        // we have recovered from memory pressure. unmute any channel not explicitly muted for other reasons        log.trace("Broker no longer low on memory - unmuting incoming sockets");        for (KafkaChannel channel : channels.values()) {            if (channel.isInMutableState() && !explicitlyMutedChannels.contains(channel)) {                channel.maybeUnmute();            }        }        outOfMemory = false;    }    /* check ready keys */    long startSelect = time.nanoseconds();    int numReadyKeys = select(timeout);    long endSelect = time.nanoseconds();    this.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());    if (numReadyKeys > 0 || !immediatelyConnectedKeys.isEmpty() || dataInBuffers) {        Set<SelectionKey> readyKeys = this.nioSelector.selectedKeys();        // Poll from channels that have buffered data (but nothing more from the underlying socket)        if (dataInBuffers) {            // so no channel gets polled twice            keysWithBufferedRead.removeAll(readyKeys);            Set<SelectionKey> toPoll = keysWithBufferedRead;            // poll() calls will repopulate if needed            keysWithBufferedRead = new HashSet<>();            pollSelectionKeys(toPoll, false, endSelect);        }        // Poll from channels where the underlying socket has more data        pollSelectionKeys(readyKeys, false, endSelect);        // Clear all selected keys so that they are included in the ready count for the next select        readyKeys.clear();        pollSelectionKeys(immediatelyConnectedKeys, true, endSelect);        immediatelyConnectedKeys.clear();    } else {        // no work is also "progress"        madeReadProgressLastPoll = true;    }    long endIo = time.nanoseconds();    this.sensors.ioTime.record(endIo - endSelect, time.milliseconds());    // Close channels that were delayed and are now ready to be closed    completeDelayedChannelClose(endIo);    // we use the time at the end of select to ensure that we don't close any connections that    // have just been processed in pollSelectionKeys    maybeCloseOldestConnection(endSelect);    // Add to completedReceives after closing expired connections to avoid removing    // channels with completed receives until all staged receives are completed.    addToCompletedReceives();}
f2488
0
pollSelectionKeys
 voidf2489_1Set<SelectionKey> selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos)
{    for (SelectionKey key : determineHandlingOrder(selectionKeys)) {        KafkaChannel channel = channel(key);        long channelStartTimeNanos = recordTimePerConnection ? time.nanoseconds() : 0;        boolean sendFailed = false;        // register all per-connection metrics at once        sensors.maybeRegisterConnectionMetrics(channel.id());        if (idleExpiryManager != null)            idleExpiryManager.update(channel.id(), currentTimeNanos);        try {            /* complete any connections that have finished their handshake (either normally or immediately) */            if (isImmediatelyConnected || key.isConnectable()) {                if (channel.finishConnect()) {                    this.connected.add(channel.id());                    this.sensors.connectionCreated.record();                    SocketChannel socketChannel = (SocketChannel) key.channel();                                    } else {                    continue;                }            }            /* if channel is not ready finish prepare */            if (channel.isConnected() && !channel.ready()) {                channel.prepare();                if (channel.ready()) {                    long readyTimeMs = time.milliseconds();                    boolean isReauthentication = channel.successfulAuthentications() > 1;                    if (isReauthentication) {                        sensors.successfulReauthentication.record(1.0, readyTimeMs);                        if (channel.reauthenticationLatencyMs() == null)                             continuing...");                        else                            sensors.reauthenticationLatency.record(channel.reauthenticationLatencyMs().doubleValue(), readyTimeMs);                    } else {                        sensors.successfulAuthentication.record(1.0, readyTimeMs);                        if (!channel.connectedClientSupportsReauthentication())                            sensors.successfulAuthenticationNoReauth.record(1.0, readyTimeMs);                    }                                    }                List<NetworkReceive> responsesReceivedDuringReauthentication = channel.getAndClearResponsesReceivedDuringReauthentication();                responsesReceivedDuringReauthentication.forEach(receive -> addToStagedReceives(channel, receive));            }            attemptRead(key, channel);            if (channel.hasBytesBuffered()) {                // this channel has bytes enqueued in intermediary buffers that we could not read                // (possibly because no memory). it may be the case that the underlying socket will                // not come up in the next poll() and so we need to remember this channel for the                // next poll call otherwise data may be stuck in said buffers forever. If we attempt                // to process buffered data and no progress is made, the channel buffered status is                // cleared to avoid the overhead of checking every time.                keysWithBufferedRead.add(key);            }            /* if channel is ready write to any sockets that have space in their buffer and for which we have data */            if (channel.ready() && key.isWritable() && !channel.maybeBeginClientReauthentication(() -> channelStartTimeNanos != 0 ? channelStartTimeNanos : currentTimeNanos)) {                Send send;                try {                    send = channel.write();                } catch (Exception e) {                    sendFailed = true;                    throw e;                }                if (send != null) {                    this.completedSends.add(send);                    this.sensors.recordBytesSent(channel.id(), send.size());                }            }            /* cancel any defunct sockets */            if (!key.isValid())                close(channel, CloseMode.GRACEFUL);        } catch (Exception e) {            String desc = channel.socketDescription();            if (e instanceof IOException) {                            } else if (e instanceof AuthenticationException) {                boolean isReauthentication = channel.successfulAuthentications() > 0;                if (isReauthentication)                    sensors.failedReauthentication.record();                else                    sensors.failedAuthentication.record();                String exceptionMessage = e.getMessage();                if (e instanceof DelayedResponseAuthenticationException)                    exceptionMessage = e.getCause().getMessage();                            } else {                 closing connection", desc, e);            }            if (e instanceof DelayedResponseAuthenticationException)                maybeDelayCloseOnAuthenticationFailure(channel);            else                close(channel, sendFailed ? CloseMode.NOTIFY_ONLY : CloseMode.GRACEFUL);        } finally {            maybeRecordTimePerConnection(channel, channelStartTimeNanos);        }    }}
 voidf2489
1
mute
public void kafkatest_f2497_0(String id)
{    KafkaChannel channel = openOrClosingChannelOrFail(id);    mute(channel);}
f2497
0
mute
private void kafkatest_f2498_0(KafkaChannel channel)
{    channel.mute();    explicitlyMutedChannels.add(channel);}
f2498
0
unmute
public void kafkatest_f2499_0(String id)
{    KafkaChannel channel = openOrClosingChannelOrFail(id);    unmute(channel);}
f2499
0
close
public void kafkatest_f2507_0(String id)
{    KafkaChannel channel = this.channels.get(id);    if (channel != null) {        // There is no disconnect notification for local close, but updating        // channel state here anyway to avoid confusion.        channel.state(ChannelState.LOCAL_CLOSE);        close(channel, CloseMode.DISCARD_NO_NOTIFY);    } else {        KafkaChannel closingChannel = this.closingChannels.remove(id);        // Close any closing channel, leave the channel in the state in which closing was triggered        if (closingChannel != null)            doClose(closingChannel, false);    }}
f2507
0
maybeDelayCloseOnAuthenticationFailure
private void kafkatest_f2508_0(KafkaChannel channel)
{    DelayedAuthenticationFailureClose delayedClose = new DelayedAuthenticationFailureClose(channel, failedAuthenticationDelayMs);    if (delayedClosingChannels != null)        delayedClosingChannels.put(channel.id(), delayedClose);    else        delayedClose.closeNow();}
f2508
0
handleCloseOnAuthenticationFailure
private voidf2509_1KafkaChannel channel)
{    try {        channel.completeCloseOnAuthenticationFailure();    } catch (Exception e) {            } finally {        close(channel, CloseMode.GRACEFUL);    }}
private voidf2509
1
lowestPriorityChannel
public KafkaChannel kafkatest_f2517_0()
{    KafkaChannel channel = null;    if (!closingChannels.isEmpty()) {        channel = closingChannels.values().iterator().next();    } else if (idleExpiryManager != null && !idleExpiryManager.lruConnections.isEmpty()) {        String channelId = idleExpiryManager.lruConnections.keySet().iterator().next();        channel = channel(channelId);    } else if (!channels.isEmpty()) {        channel = channels.values().iterator().next();    }    return channel;}
f2517
0
channel
private KafkaChannel kafkatest_f2518_0(SelectionKey key)
{    return (KafkaChannel) key.attachment();}
f2518
0
hasStagedReceive
private boolean kafkatest_f2519_0(KafkaChannel channel)
{    return stagedReceives.containsKey(channel);}
f2519
0
createMeter
private Meter kafkatest_f2527_0(Metrics metrics, String groupName, Map<String, String> metricTags, String baseName, String descriptiveName)
{    return createMeter(metrics, groupName, metricTags, null, baseName, descriptiveName);}
f2527
0
createIOThreadRatioMeter
private Meter kafkatest_f2528_0(Metrics metrics, String groupName, Map<String, String> metricTags, String baseName, String action)
{    MetricName rateMetricName = metrics.metricName(baseName + "-ratio", groupName, String.format("The fraction of time the I/O thread spent %s", action), metricTags);    MetricName totalMetricName = metrics.metricName(baseName + "time-total", groupName, String.format("The total time the I/O thread spent %s", action), metricTags);    return new Meter(TimeUnit.NANOSECONDS, rateMetricName, totalMetricName);}
f2528
0
sensor
private Sensor kafkatest_f2529_0(String name, Sensor... parents)
{    Sensor sensor = metrics.sensor(name, parents);    sensors.add(sensor);    return sensor;}
f2529
0
pollExpiredConnection
public Map.Entry<String, Long> kafkatest_f2537_0(long currentTimeNanos)
{    if (currentTimeNanos <= nextIdleCloseCheckTime)        return null;    if (lruConnections.isEmpty()) {        nextIdleCloseCheckTime = currentTimeNanos + connectionsMaxIdleNanos;        return null;    }    Map.Entry<String, Long> oldestConnectionEntry = lruConnections.entrySet().iterator().next();    Long connectionLastActiveTime = oldestConnectionEntry.getValue();    nextIdleCloseCheckTime = connectionLastActiveTime + connectionsMaxIdleNanos;    if (currentTimeNanos > nextIdleCloseCheckTime)        return oldestConnectionEntry;    else        return null;}
f2537
0
remove
public void kafkatest_f2538_0(String connectionId)
{    lruConnections.remove(connectionId);}
f2538
0
isOutOfMemory
 boolean kafkatest_f2539_0()
{    return outOfMemory;}
f2539
0
buildChannel
public KafkaChannelf2547_1String id, SelectionKey key, int maxReceiveSize, MemoryPool memoryPool) throws KafkaException
{    try {        SslTransportLayer transportLayer = buildTransportLayer(sslFactory, id, key, peerHost(key));        Supplier<Authenticator> authenticatorCreator = () -> new SslAuthenticator(configs, transportLayer, listenerName, sslPrincipalMapper);        return new KafkaChannel(id, transportLayer, authenticatorCreator, maxReceiveSize, memoryPool != null ? memoryPool : MemoryPool.NONE);    } catch (Exception e) {                throw new KafkaException(e);    }}
public KafkaChannelf2547
1
buildTransportLayer
protected SslTransportLayer kafkatest_f2549_0(SslFactory sslFactory, String id, SelectionKey key, String host) throws IOException
{    SocketChannel socketChannel = (SocketChannel) key.channel();    return SslTransportLayer.create(id, key, sslFactory.createSslEngine(host, socketChannel.socket().getPort()));}
f2549
0
peerHost
private String kafkatest_f2550_0(SelectionKey key)
{    SocketChannel socketChannel = (SocketChannel) key.channel();    return new InetSocketAddress(socketChannel.socket().getInetAddress(), 0).getHostString();}
f2550
0
disconnect
public void kafkatest_f2559_0()
{    key.cancel();}
f2559
0
socketChannel
public SocketChannel kafkatest_f2560_0()
{    return socketChannel;}
f2560
0
selectionKey
public SelectionKey kafkatest_f2561_0()
{    return key;}
f2561
0
doHandshake
private void kafkatest_f2569_0() throws IOException
{    boolean read = key.isReadable();    boolean write = key.isWritable();    handshakeStatus = sslEngine.getHandshakeStatus();    if (!flush(netWriteBuffer)) {        key.interestOps(key.interestOps() | SelectionKey.OP_WRITE);        return;    }    // Throw any pending handshake exception since `netWriteBuffer` has been flushed    maybeThrowSslAuthenticationException();    switch(handshakeStatus) {        case NEED_TASK:            log.trace("SSLHandshake NEED_TASK channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());            handshakeStatus = runDelegatedTasks();            break;        case NEED_WRAP:            log.trace("SSLHandshake NEED_WRAP channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());            handshakeResult = handshakeWrap(write);            if (handshakeResult.getStatus() == Status.BUFFER_OVERFLOW) {                int currentNetWriteBufferSize = netWriteBufferSize();                netWriteBuffer.compact();                netWriteBuffer = Utils.ensureCapacity(netWriteBuffer, currentNetWriteBufferSize);                netWriteBuffer.flip();                if (netWriteBuffer.limit() >= currentNetWriteBufferSize) {                    throw new IllegalStateException("Buffer overflow when available data size (" + netWriteBuffer.limit() + ") >= network buffer size (" + currentNetWriteBufferSize + ")");                }            } else if (handshakeResult.getStatus() == Status.BUFFER_UNDERFLOW) {                throw new IllegalStateException("Should not have received BUFFER_UNDERFLOW during handshake WRAP.");            } else if (handshakeResult.getStatus() == Status.CLOSED) {                throw new EOFException();            }            log.trace("SSLHandshake NEED_WRAP channelId {}, handshakeResult {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, handshakeResult, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());            // we will break here otherwise we can do need_unwrap in the same call.            if (handshakeStatus != HandshakeStatus.NEED_UNWRAP || !flush(netWriteBuffer)) {                key.interestOps(key.interestOps() | SelectionKey.OP_WRITE);                break;            }        case NEED_UNWRAP:            log.trace("SSLHandshake NEED_UNWRAP channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());            do {                handshakeResult = handshakeUnwrap(read, false);                if (handshakeResult.getStatus() == Status.BUFFER_OVERFLOW) {                    int currentAppBufferSize = applicationBufferSize();                    appReadBuffer = Utils.ensureCapacity(appReadBuffer, currentAppBufferSize);                    if (appReadBuffer.position() > currentAppBufferSize) {                        throw new IllegalStateException("Buffer underflow when available data size (" + appReadBuffer.position() + ") > packet buffer size (" + currentAppBufferSize + ")");                    }                }            } while (handshakeResult.getStatus() == Status.BUFFER_OVERFLOW);            if (handshakeResult.getStatus() == Status.BUFFER_UNDERFLOW) {                int currentNetReadBufferSize = netReadBufferSize();                netReadBuffer = Utils.ensureCapacity(netReadBuffer, currentNetReadBufferSize);                if (netReadBuffer.position() >= currentNetReadBufferSize) {                    throw new IllegalStateException("Buffer underflow when there is available data");                }            } else if (handshakeResult.getStatus() == Status.CLOSED) {                throw new EOFException("SSL handshake status CLOSED during handshake UNWRAP");            }            log.trace("SSLHandshake NEED_UNWRAP channelId {}, handshakeResult {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}", channelId, handshakeResult, appReadBuffer.position(), netReadBuffer.position(), netWriteBuffer.position());            // so the selector won't invoke this channel if we don't go through the handshakeFinished here.            if (handshakeStatus != HandshakeStatus.FINISHED) {                if (handshakeStatus == HandshakeStatus.NEED_WRAP) {                    key.interestOps(key.interestOps() | SelectionKey.OP_WRITE);                } else if (handshakeStatus == HandshakeStatus.NEED_UNWRAP) {                    key.interestOps(key.interestOps() & ~SelectionKey.OP_WRITE);                }                break;            }        case FINISHED:            handshakeFinished();            break;        case NOT_HANDSHAKING:            handshakeFinished();            break;        default:            throw new IllegalStateException(String.format("Unexpected status [%s]", handshakeStatus));    }}
f2569
0
renegotiationException
private SSLHandshakeException kafkatest_f2570_0()
{    return new SSLHandshakeException("Renegotiation is not supported");}
f2570
0
closingException
private IllegalStateException kafkatest_f2571_0()
{    throw new IllegalStateException("Channel is in closing state");}
f2571
0
write
public int kafkatest_f2579_0(ByteBuffer src) throws IOException
{    int written = 0;    if (state == State.CLOSING)        throw closingException();    if (state != State.READY)        return written;    if (!flush(netWriteBuffer))        return written;    netWriteBuffer.clear();    SSLEngineResult wrapResult = sslEngine.wrap(src, netWriteBuffer);    netWriteBuffer.flip();    // handle ssl renegotiation    if (wrapResult.getHandshakeStatus() != HandshakeStatus.NOT_HANDSHAKING && wrapResult.getStatus() == Status.OK)        throw renegotiationException();    if (wrapResult.getStatus() == Status.OK) {        written = wrapResult.bytesConsumed();        flush(netWriteBuffer);    } else if (wrapResult.getStatus() == Status.BUFFER_OVERFLOW) {        int currentNetWriteBufferSize = netWriteBufferSize();        netWriteBuffer.compact();        netWriteBuffer = Utils.ensureCapacity(netWriteBuffer, currentNetWriteBufferSize);        netWriteBuffer.flip();        if (netWriteBuffer.limit() >= currentNetWriteBufferSize)            throw new IllegalStateException("SSL BUFFER_OVERFLOW when available data size (" + netWriteBuffer.limit() + ") >= network buffer size (" + currentNetWriteBufferSize + ")");    } else if (wrapResult.getStatus() == Status.BUFFER_UNDERFLOW) {        throw new IllegalStateException("SSL BUFFER_UNDERFLOW during write");    } else if (wrapResult.getStatus() == Status.CLOSED) {        throw new EOFException();    }    return written;}
f2579
0
write
public long kafkatest_f2580_0(ByteBuffer[] srcs, int offset, int length) throws IOException
{    if ((offset < 0) || (length < 0) || (offset > srcs.length - length))        throw new IndexOutOfBoundsException();    int totalWritten = 0;    int i = offset;    while (i < length) {        if (srcs[i].hasRemaining() || hasPendingWrites()) {            int written = write(srcs[i]);            if (written > 0) {                totalWritten += written;            }        }        if (!srcs[i].hasRemaining() && !hasPendingWrites()) {            i++;        } else {            // as we might have reached max socket send buffer size.            break;        }    }    return totalWritten;}
f2580
0
write
public long kafkatest_f2581_0(ByteBuffer[] srcs) throws IOException
{    return write(srcs, 0, srcs.length);}
f2581
0
netWriteBufferSize
protected int kafkatest_f2589_0()
{    return sslEngine.getSession().getPacketBufferSize();}
f2589
0
applicationBufferSize
protected int kafkatest_f2590_0()
{    return sslEngine.getSession().getApplicationBufferSize();}
f2590
0
netReadBuffer
protected ByteBuffer kafkatest_f2591_0()
{    return netReadBuffer;}
f2591
0
transferFrom
public long kafkatest_f2599_0(FileChannel fileChannel, long position, long count) throws IOException
{    return fileChannel.transferTo(position, count, this);}
f2599
0
hasPendingWrites
public static boolean kafkatest_f2600_0(GatheringByteChannel channel)
{    if (channel instanceof TransportLayer)        return ((TransportLayer) channel).hasPendingWrites();    return false;}
f2600
0
noNode
public static Node kafkatest_f2601_0()
{    return NO_NODE;}
f2601
0
hashCode
public int kafkatest_f2609_0()
{    Integer h = this.hash;    if (h == null) {        int result = 31 + ((host == null) ? 0 : host.hashCode());        result = 31 * result + id;        result = 31 * result + port;        result = 31 * result + ((rack == null) ? 0 : rack.hashCode());        this.hash = result;        return result;    } else {        return h;    }}
f2609
0
equals
public boolean kafkatest_f2610_0(Object obj)
{    if (this == obj)        return true;    if (obj == null || getClass() != obj.getClass())        return false;    Node other = (Node) obj;    return id == other.id && port == other.port && Objects.equals(host, other.host) && Objects.equals(rack, other.rack);}
f2610
0
toString
public String kafkatest_f2611_0()
{    return host + ":" + port + " (id: " + idString + " rack: " + rack + ")";}
f2611
0
formatNodeIds
private String kafkatest_f2619_0(Node[] nodes)
{    StringBuilder b = new StringBuilder("[");    if (nodes != null) {        for (int i = 0; i < nodes.length; i++) {            b.append(nodes[i].idString());            if (i < nodes.length - 1)                b.append(',');        }    }    b.append("]");    return b.toString();}
f2619
0
forId
public static ApiKeys kafkatest_f2620_0(int id)
{    if (!hasId(id))        throw new IllegalArgumentException(String.format("Unexpected ApiKeys id `%s`, it should be between `%s` " + "and `%s` (inclusive)", id, MIN_API_KEY, MAX_API_KEY));    return ID_TO_TYPE[id];}
f2620
0
hasId
public static boolean kafkatest_f2621_0(int id)
{    return id >= MIN_API_KEY && id <= MAX_API_KEY;}
f2621
0
schemaFor
private Schema kafkatest_f2629_0(Schema[] versions, short version)
{    if (!isVersionSupported(version))        throw new IllegalArgumentException("Invalid version for API key " + this + ": " + version);    return versions[version];}
f2629
0
isVersionSupported
public boolean kafkatest_f2630_0(short apiVersion)
{    return apiVersion >= oldestVersion() && apiVersion <= latestVersion();}
f2630
0
toHtml
private static String kafkatest_f2631_0()
{    final StringBuilder b = new StringBuilder();    b.append("<table class=\"data-table\"><tbody>\n");    b.append("<tr>");    b.append("<th>Name</th>\n");    b.append("<th>Key</th>\n");    b.append("</tr>");    for (ApiKeys key : ApiKeys.values()) {        b.append("<tr>\n");        b.append("<td>");        b.append("<a href=\"#The_Messages_" + key.name + "\">" + key.name + "</a>");        b.append("</td>");        b.append("<td>");        b.append(key.id);        b.append("</td>");        b.append("</tr>\n");    }    b.append("</table>\n");    return b.toString();}
f2631
0
readLong
public long kafkatest_f2639_0()
{    return buf.getLong();}
f2639
0
readArray
public void kafkatest_f2640_0(byte[] arr)
{    buf.get(arr);}
f2640
0
writeByte
public void kafkatest_f2641_0(byte val)
{    buf.put(val);}
f2641
0
code
public short kafkatest_f2649_0()
{    return this.code;}
f2649
0
maybeThrow
public void kafkatest_f2650_0()
{    if (exception != null) {        throw this.exception;    }}
f2650
0
message
public String kafkatest_f2651_0()
{    if (exception != null)        return exception.getMessage();    return toString();}
f2651
0
schemaToBnfHtml
private static void kafkatest_f2659_0(Schema schema, StringBuilder b, int indentSize)
{    final String indentStr = indentString(indentSize);    final Map<String, Type> subTypes = new LinkedHashMap<>();    // Top level fields    for (BoundField field : schema.fields()) {        if (field.def.type instanceof ArrayOf) {            b.append("[");            b.append(field.def.name);            b.append("] ");            Type innerType = ((ArrayOf) field.def.type).type();            if (!subTypes.containsKey(field.def.name))                subTypes.put(field.def.name, innerType);        } else {            b.append(field.def.name);            b.append(" ");            if (!subTypes.containsKey(field.def.name))                subTypes.put(field.def.name, field.def.type);        }    }    b.append("\n");    // Sub Types/Schemas    for (Map.Entry<String, Type> entry : subTypes.entrySet()) {        if (entry.getValue() instanceof Schema) {            // Complex Schema Type            b.append(indentStr);            b.append(entry.getKey());            b.append(" => ");            schemaToBnfHtml((Schema) entry.getValue(), b, indentSize + 2);        } else {            // Standard Field Type            b.append(indentStr);            b.append(entry.getKey());            b.append(" => ");            b.append(entry.getValue());            b.append("\n");        }    }}
f2659
0
populateSchemaFields
private static void kafkatest_f2660_0(Schema schema, Set<BoundField> fields)
{    for (BoundField field : schema.fields()) {        fields.add(field);        if (field.def.type instanceof ArrayOf) {            Type innerType = ((ArrayOf) field.def.type).type();            if (innerType instanceof Schema)                populateSchemaFields((Schema) innerType, fields);        } else if (field.def.type instanceof Schema)            populateSchemaFields((Schema) field.def.type, fields);    }}
f2660
0
schemaToFieldTableHtml
private static void kafkatest_f2661_0(Schema schema, StringBuilder b)
{    Set<BoundField> fields = new LinkedHashSet<>();    populateSchemaFields(schema, fields);    b.append("<table class=\"data-table\"><tbody>\n");    b.append("<tr>");    b.append("<th>Field</th>\n");    b.append("<th>Description</th>\n");    b.append("</tr>");    for (BoundField field : fields) {        b.append("<tr>\n");        b.append("<td>");        b.append(field.def.name);        b.append("</td>");        b.append("<td>");        b.append(field.def.docString);        b.append("</td>");        b.append("</tr>\n");    }    b.append("</table>\n");}
f2661
0
write
public void kafkatest_f2669_0(ByteBuffer buffer, Object o)
{    if (o == null) {        buffer.putInt(-1);        return;    }    Object[] objs = (Object[]) o;    int size = objs.length;    buffer.putInt(size);    for (Object obj : objs) type.write(buffer, obj);}
f2669
0
read
public Object kafkatest_f2670_0(ByteBuffer buffer)
{    int size = buffer.getInt();    if (size < 0 && isNullable())        return null;    else if (size < 0)        throw new SchemaException("Array size " + size + " cannot be negative");    if (size > buffer.remaining())        throw new SchemaException("Error reading array of size " + size + ", only " + buffer.remaining() + " bytes available");    Object[] objs = new Object[size];    for (int i = 0; i < size; i++) objs[i] = type.read(buffer);    return objs;}
f2670
0
sizeOf
public int kafkatest_f2671_0(Object o)
{    int size = 4;    if (o == null)        return size;    Object[] objs = (Object[]) o;    for (Object obj : objs) size += type.sizeOf(obj);    return size;}
f2671
0
nullableWithFields
public Field kafkatest_f2679_0(Field... fields)
{    Schema elementType = new Schema(fields);    return new Field(name, ArrayOf.nullable(elementType), docString, false, null);}
f2679
0
withFields
public Field kafkatest_f2680_0(String docStringOverride, Field... fields)
{    Schema elementType = new Schema(fields);    return new Field(name, new ArrayOf(elementType), docStringOverride, false, null);}
f2680
0
write
public void kafkatest_f2681_0(ByteBuffer buffer, Object o)
{    Struct r = (Struct) o;    for (BoundField field : fields) {        try {            Object value = field.def.type.validate(r.get(field));            field.def.type.write(buffer, value);        } catch (Exception e) {            throw new SchemaException("Error writing field '" + field.def.name + "': " + (e.getMessage() == null ? e.getClass().getName() : e.getMessage()));        }    }}
f2681
0
validate
public Struct kafkatest_f2689_0(Object item)
{    try {        Struct struct = (Struct) item;        for (BoundField field : fields) {            try {                field.def.type.validate(struct.get(field));            } catch (SchemaException e) {                throw new SchemaException("Invalid value for field '" + field.def.name + "': " + e.getMessage());            }        }        return struct;    } catch (ClassCastException e) {        throw new SchemaException("Not a Struct.");    }}
f2689
0
walk
public void kafkatest_f2690_0(Visitor visitor)
{    Objects.requireNonNull(visitor, "visitor must be non-null");    handleNode(this, visitor);}
f2690
0
handleNode
private static void kafkatest_f2691_0(Type node, Visitor visitor)
{    if (node instanceof Schema) {        Schema schema = (Schema) node;        visitor.visit(schema);        for (BoundField f : schema.fields()) handleNode(f.def.type, visitor);    } else if (node instanceof ArrayOf) {        ArrayOf array = (ArrayOf) node;        visitor.visit(array);        handleNode(array.type(), visitor);    } else {        visitor.visit(node);    }}
f2691
0
get
public Short kafkatest_f2702_0(Field.Int16 field)
{    return getShort(field.name);}
f2702
0
get
public String kafkatest_f2703_0(Field.Str field)
{    return getString(field.name);}
f2703
0
get
public String kafkatest_f2704_0(Field.NullableStr field)
{    return getString(field.name);}
f2704
0
getOrElse
public Integer kafkatest_f2712_0(Field.Int32 field, int alternative)
{    if (hasField(field.name))        return getInt(field.name);    return alternative;}
f2712
0
getOrElse
public String kafkatest_f2713_0(Field.NullableStr field, String alternative)
{    if (hasField(field.name))        return getString(field.name);    return alternative;}
f2713
0
getOrElse
public String kafkatest_f2714_0(Field.Str field, String alternative)
{    if (hasField(field.name))        return getString(field.name);    return alternative;}
f2714
0
getStruct
public Struct kafkatest_f2722_0(BoundField field)
{    return (Struct) get(field);}
f2722
0
getStruct
public Struct kafkatest_f2723_0(String name)
{    return (Struct) get(name);}
f2723
0
getByte
public Byte kafkatest_f2724_0(BoundField field)
{    return (Byte) get(field);}
f2724
0
getLong
public Long kafkatest_f2732_0(BoundField field)
{    return (Long) get(field);}
f2732
0
getLong
public Long kafkatest_f2733_0(String name)
{    return (Long) get(name);}
f2733
0
getUUID
public UUID kafkatest_f2734_0(BoundField field)
{    return (UUID) get(field);}
f2734
0
getBytes
public ByteBuffer kafkatest_f2742_0(BoundField field)
{    Object result = get(field);    if (result instanceof byte[])        return ByteBuffer.wrap((byte[]) result);    return (ByteBuffer) result;}
f2742
0
getBytes
public ByteBuffer kafkatest_f2743_0(String name)
{    Object result = get(name);    if (result instanceof byte[])        return ByteBuffer.wrap((byte[]) result);    return (ByteBuffer) result;}
f2743
0
getByteArray
public byte[] kafkatest_f2744_0(String name)
{    Object result = get(name);    if (result instanceof byte[])        return (byte[]) result;    ByteBuffer buf = (ByteBuffer) result;    byte[] arr = new byte[buf.remaining()];    buf.get(arr);    buf.flip();    return arr;}
f2744
0
set
public Struct kafkatest_f2752_0(Field.UUID def, UUID value)
{    return set(def.name, value);}
f2752
0
set
public Struct kafkatest_f2753_0(Field.Int16 def, short value)
{    return set(def.name, value);}
f2753
0
set
public Struct kafkatest_f2754_0(Field.Bool def, boolean value)
{    return set(def.name, value);}
f2754
0
instance
public Struct kafkatest_f2762_0(BoundField field)
{    validateField(field);    if (field.def.type instanceof Schema) {        return new Struct((Schema) field.def.type);    } else if (field.def.type instanceof ArrayOf) {        ArrayOf array = (ArrayOf) field.def.type;        return new Struct((Schema) array.type());    } else {        throw new SchemaException("Field '" + field.def.name + "' is not a container type, it is of type " + field.def.type);    }}
f2762
0
instance
public Struct kafkatest_f2763_0(String field)
{    return instance(schema.get(field));}
f2763
0
instance
public Struct kafkatest_f2764_0(Field field)
{    return instance(schema.get(field.name));}
f2764
0
hashCode
public int kafkatest_f2772_0()
{    final int prime = 31;    int result = 1;    for (int i = 0; i < this.values.length; i++) {        BoundField f = this.schema.get(i);        if (f.def.type instanceof ArrayOf) {            if (this.get(f) != null) {                Object[] arrayObject = (Object[]) this.get(f);                for (Object arrayItem : arrayObject) result = prime * result + arrayItem.hashCode();            }        } else {            Object field = this.get(f);            if (field != null) {                result = prime * result + field.hashCode();            }        }    }    return result;}
f2772
0
equals
public boolean kafkatest_f2773_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    Struct other = (Struct) obj;    if (schema != other.schema)        return false;    for (int i = 0; i < this.values.length; i++) {        BoundField f = this.schema.get(i);        boolean result;        if (f.def.type instanceof ArrayOf) {            result = Arrays.equals((Object[]) this.get(f), (Object[]) other.get(f));        } else {            Object thisField = this.get(f);            Object otherField = other.get(f);            result = Objects.equals(thisField, otherField);        }        if (!result)            return false;    }    return true;}
f2773
0
isNullable
public boolean kafkatest_f2774_0()
{    return false;}
f2774
0
write
public void kafkatest_f2782_0(ByteBuffer buffer, Object o)
{    buffer.put((Byte) o);}
f2782
0
read
public Object kafkatest_f2783_0(ByteBuffer buffer)
{    return buffer.get();}
f2783
0
sizeOf
public int kafkatest_f2784_0(Object o)
{    return 1;}
f2784
0
validate
public Short kafkatest_f2792_0(Object item)
{    if (item instanceof Short)        return (Short) item;    else        throw new SchemaException(item + " is not a Short.");}
f2792
0
documentation
public String kafkatest_f2793_0()
{    return "Represents an integer between -2<sup>15</sup> and 2<sup>15</sup>-1 inclusive. " + "The values are encoded using two bytes in network byte order (big-endian).";}
f2793
0
write
public void kafkatest_f2794_0(ByteBuffer buffer, Object o)
{    buffer.putInt((Integer) o);}
f2794
0
sizeOf
public int kafkatest_f2802_0(Object o)
{    return 4;}
f2802
0
typeName
public String kafkatest_f2803_0()
{    return "UINT32";}
f2803
0
validate
public Long kafkatest_f2804_0(Object item)
{    if (item instanceof Long)        return (Long) item;    else        throw new SchemaException(item + " is not a Long.");}
f2804
0
write
public void kafkatest_f2812_0(ByteBuffer buffer, Object o)
{    final java.util.UUID uuid = (java.util.UUID) o;    buffer.putLong(uuid.getMostSignificantBits());    buffer.putLong(uuid.getLeastSignificantBits());}
f2812
0
read
public Object kafkatest_f2813_0(ByteBuffer buffer)
{    return new java.util.UUID(buffer.getLong(), buffer.getLong());}
f2813
0
sizeOf
public int kafkatest_f2814_0(Object o)
{    return 16;}
f2814
0
validate
public String kafkatest_f2822_0(Object item)
{    if (item instanceof String)        return (String) item;    else        throw new SchemaException(item + " is not a String.");}
f2822
0
documentation
public String kafkatest_f2823_0()
{    return "Represents a sequence of characters. First the length N is given as an " + INT16 + ". Then N bytes follow which are the UTF-8 encoding of the character sequence. " + "Length must not be negative.";}
f2823
0
isNullable
public boolean kafkatest_f2824_0()
{    return true;}
f2824
0
read
public Object kafkatest_f2832_0(ByteBuffer buffer)
{    int size = buffer.getInt();    if (size < 0)        throw new SchemaException("Bytes size " + size + " cannot be negative");    if (size > buffer.remaining())        throw new SchemaException("Error reading bytes of size " + size + ", only " + buffer.remaining() + " bytes available");    ByteBuffer val = buffer.slice();    val.limit(size);    buffer.position(buffer.position() + size);    return val;}
f2832
0
sizeOf
public int kafkatest_f2833_0(Object o)
{    ByteBuffer buffer = (ByteBuffer) o;    return 4 + buffer.remaining();}
f2833
0
typeName
public String kafkatest_f2834_0()
{    return "BYTES";}
f2834
0
validate
public ByteBuffer kafkatest_f2842_0(Object item)
{    if (item == null)        return null;    if (item instanceof ByteBuffer)        return (ByteBuffer) item;    throw new SchemaException(item + " is not a java.nio.ByteBuffer.");}
f2842
0
documentation
public String kafkatest_f2843_0()
{    return "Represents a raw sequence of bytes or null. For non-null values, first the length N is given as an " + INT32 + ". Then N bytes follow. A null value is encoded with length of -1 and there are no following bytes.";}
f2843
0
isNullable
public boolean kafkatest_f2844_0()
{    return true;}
f2844
0
read
public Integer kafkatest_f2852_0(ByteBuffer buffer)
{    return ByteUtils.readVarint(buffer);}
f2852
0
validate
public Integer kafkatest_f2853_0(Object item)
{    if (item instanceof Integer)        return (Integer) item;    throw new SchemaException(item + " is not an integer");}
f2853
0
typeName
public String kafkatest_f2854_0()
{    return "VARINT";}
f2854
0
documentation
public String kafkatest_f2862_0()
{    return "Represents an integer between -2<sup>63</sup> and 2<sup>63</sup>-1 inclusive. " + "Encoding follows the variable-length zig-zag encoding from " + " <a href=\"http://code.google.com/apis/protocolbuffers/docs/encoding.html\"> Google Protocol Buffers</a>.";}
f2862
0
toHtml
private static String kafkatest_f2863_0()
{    DocumentedType[] types = { BOOLEAN, INT8, INT16, INT32, INT64, UNSIGNED_INT32, VARINT, VARLONG, STRING, NULLABLE_STRING, BYTES, NULLABLE_BYTES, RECORDS, new ArrayOf(STRING) };    final StringBuilder b = new StringBuilder();    b.append("<table class=\"data-table\"><tbody>\n");    b.append("<tr>");    b.append("<th>Type</th>\n");    b.append("<th>Description</th>\n");    b.append("</tr>\n");    for (DocumentedType type : types) {        b.append("<tr>");        b.append("<td>");        b.append(type.typeName());        b.append("</td>");        b.append("<td>");        b.append(type.documentation());        b.append("</td>");        b.append("</tr>\n");    }    b.append("</table>\n");    return b.toString();}
f2863
0
main
public static void kafkatest_f2864_0(String[] args)
{    System.out.println(toHtml());}
f2864
0
ensureValid
public void kafkatest_f2872_0()
{    outerRecord().ensureValid();}
f2872
0
keySize
public int kafkatest_f2873_0()
{    return outerRecord().keySize();}
f2873
0
hasKey
public boolean kafkatest_f2874_0()
{    return outerRecord().hasKey();}
f2874
0
checksumOrNull
public Long kafkatest_f2882_0()
{    return checksum();}
f2882
0
checksum
public long kafkatest_f2883_0()
{    return outerRecord().checksum();}
f2883
0
maxTimestamp
public long kafkatest_f2884_0()
{    return timestamp();}
f2884
0
toString
public String kafkatest_f2892_0()
{    return "LegacyRecordBatch(offset=" + offset() + ", " + outerRecord() + ")";}
f2892
0
writeTo
public void kafkatest_f2893_0(ByteBuffer buffer)
{    writeHeader(buffer, offset(), outerRecord().sizeInBytes());    buffer.put(outerRecord().buffer().duplicate());}
f2893
0
producerId
public long kafkatest_f2894_0()
{    return RecordBatch.NO_PRODUCER_ID;}
f2894
0
isControlBatch
public boolean kafkatest_f2902_0()
{    return false;}
f2902
0
iterator
public Iterator<Record> kafkatest_f2903_0()
{    return iterator(BufferSupplier.NO_CACHING);}
f2903
0
iterator
 CloseableIterator<Record> kafkatest_f2904_0(BufferSupplier bufferSupplier)
{    if (isCompressed())        return new DeepRecordsIterator(this, false, Integer.MAX_VALUE, bufferSupplier);    return new CloseableIterator<Record>() {        private boolean hasNext = true;        @Override        public void close() {        }        @Override        public boolean hasNext() {            return hasNext;        }        @Override        public Record next() {            if (!hasNext)                throw new NoSuchElementException();            hasNext = false;            return AbstractLegacyRecordBatch.this;        }        @Override        public void remove() {            throw new UnsupportedOperationException();        }    };}
f2904
0
makeNext
protected Record kafkatest_f2913_0()
{    if (innerEntries.isEmpty())        return allDone();    AbstractLegacyRecordBatch entry = innerEntries.remove();    // Convert offset to absolute offset if needed.    if (wrapperMagic == RecordBatch.MAGIC_VALUE_V1) {        long absoluteOffset = absoluteBaseOffset + entry.offset();        entry = new BasicLegacyRecordBatch(absoluteOffset, entry.outerRecord());    }    if (entry.isCompressed())        throw new InvalidRecordException("Inner messages must not be compressed");    return entry;}
f2913
0
offset
public long kafkatest_f2915_0()
{    return offset;}
f2915
0
outerRecord
public LegacyRecord kafkatest_f2916_0()
{    return record;}
f2916
0
setTimestampAndUpdateCrc
private void kafkatest_f2924_0(TimestampType timestampType, long timestamp)
{    byte attributes = LegacyRecord.computeAttributes(magic(), compressionType(), timestampType);    buffer.put(LOG_OVERHEAD + LegacyRecord.ATTRIBUTES_OFFSET, attributes);    buffer.putLong(LOG_OVERHEAD + LegacyRecord.TIMESTAMP_OFFSET, timestamp);    long crc = record.computeChecksum();    ByteUtils.writeUnsignedInt(buffer, LOG_OVERHEAD + LegacyRecord.CRC_OFFSET, crc);}
f2924
0
skipKeyValueIterator
public CloseableIterator<Record> kafkatest_f2925_0(BufferSupplier bufferSupplier)
{    return CloseableIterator.wrap(iterator(bufferSupplier));}
f2925
0
writeTo
public void kafkatest_f2926_0(ByteBufferOutputStream outputStream)
{    outputStream.write(buffer.duplicate());}
f2926
0
baseSequence
public int kafkatest_f2934_0()
{    return RecordBatch.NO_SEQUENCE;}
f2934
0
lastSequence
public int kafkatest_f2935_0()
{    return RecordBatch.NO_SEQUENCE;}
f2935
0
countOrNull
public Integer kafkatest_f2936_0()
{    return null;}
f2936
0
hasMatchingMagic
public boolean kafkatest_f2944_0(byte magic)
{    for (RecordBatch batch : batches()) if (batch.magic() != magic)        return false;    return true;}
f2944
0
hasCompatibleMagic
public boolean kafkatest_f2945_0(byte magic)
{    for (RecordBatch batch : batches()) if (batch.magic() > magic)        return false;    return true;}
f2945
0
firstBatch
public RecordBatch kafkatest_f2946_0()
{    Iterator<? extends RecordBatch> iterator = batches().iterator();    if (!iterator.hasNext())        return null;    return iterator.next();}
f2946
0
estimateSizeInBytesUpperBound
public static int kafkatest_f2954_0(byte magic, CompressionType compressionType, byte[] key, byte[] value, Header[] headers)
{    return estimateSizeInBytesUpperBound(magic, compressionType, Utils.wrapNullable(key), Utils.wrapNullable(value), headers);}
f2954
0
estimateSizeInBytesUpperBound
public static int kafkatest_f2955_0(byte magic, CompressionType compressionType, ByteBuffer key, ByteBuffer value, Header[] headers)
{    if (magic >= RecordBatch.MAGIC_VALUE_V2)        return DefaultRecordBatch.estimateBatchSizeUpperBound(key, value, headers);    else if (compressionType != CompressionType.NONE)        return Records.LOG_OVERHEAD + LegacyRecord.recordOverhead(magic) + LegacyRecord.recordSize(magic, key, value);    else        return Records.LOG_OVERHEAD + LegacyRecord.recordSize(magic, key, value);}
f2955
0
recordBatchHeaderSizeInBytes
public static int kafkatest_f2956_0(byte magic, CompressionType compressionType)
{    if (magic > RecordBatch.MAGIC_VALUE_V1) {        return DefaultRecordBatch.RECORD_BATCH_OVERHEAD;    } else if (compressionType != CompressionType.NONE) {        return Records.LOG_OVERHEAD + LegacyRecord.recordOverhead(magic);    } else {        return 0;    }}
f2956
0
close
public void kafkatest_f2966_0()
{    cachedBuffer = null;}
f2966
0
nextBatch
public MutableRecordBatch kafkatest_f2967_0()
{    int remaining = buffer.remaining();    Integer batchSize = nextBatchSize();    if (batchSize == null || remaining < batchSize)        return null;    byte magic = buffer.get(buffer.position() + MAGIC_OFFSET);    ByteBuffer batchSlice = buffer.slice();    batchSlice.limit(batchSize);    buffer.position(buffer.position() + batchSize);    if (magic > RecordBatch.MAGIC_VALUE_V1)        return new DefaultRecordBatch(batchSlice);    else        return new AbstractLegacyRecordBatch.ByteBufferLegacyRecordBatch(batchSlice);}
f2967
0
nextBatchSize
 Integer kafkatest_f2968_0() throws CorruptRecordException
{    int remaining = buffer.remaining();    if (remaining < LOG_OVERHEAD)        return null;    int recordSize = buffer.getInt(buffer.position() + SIZE_OFFSET);    // V0 has the smallest overhead, stricter checking is done later    if (recordSize < LegacyRecord.RECORD_OVERHEAD_V0)        throw new CorruptRecordException(String.format("Record size %d is less than the minimum record overhead (%d)", recordSize, LegacyRecord.RECORD_OVERHEAD_V0));    if (recordSize > maxMessageSize)        throw new CorruptRecordException(String.format("Record size %d exceeds the largest allowable message size (%d).", recordSize, maxMessageSize));    if (remaining < HEADER_SIZE_UP_TO_MAGIC)        return null;    byte magic = buffer.get(buffer.position() + MAGIC_OFFSET);    if (magic < 0 || magic > RecordBatch.CURRENT_MAGIC_VALUE)        throw new CorruptRecordException("Invalid magic found in record: " + magic);    return recordSize + LOG_OVERHEAD;}
f2968
0
forId
public static CompressionType kafkatest_f2976_0(int id)
{    switch(id) {        case 0:            return NONE;        case 1:            return GZIP;        case 2:            return SNAPPY;        case 3:            return LZ4;        case 4:            return ZSTD;        default:            throw new IllegalArgumentException("Unknown compression type id: " + id);    }}
f2976
0
forName
public static CompressionType kafkatest_f2977_0(String name)
{    if (NONE.name.equals(name))        return NONE;    else if (GZIP.name.equals(name))        return GZIP;    else if (SNAPPY.name.equals(name))        return SNAPPY;    else if (LZ4.name.equals(name))        return LZ4;    else if (ZSTD.name.equals(name))        return ZSTD;    else        throw new IllegalArgumentException("Unknown compression name: " + name);}
f2977
0
findConstructor
private static MethodHandle kafkatest_f2978_0(String className, MethodType methodType)
{    try {        return MethodHandles.publicLookup().findConstructor(Class.forName(className), methodType);    } catch (ReflectiveOperationException e) {        throw new RuntimeException(e);    }}
f2978
0
wrapForInput
public InputStream kafkatest_f2986_0(ByteBuffer inputBuffer, byte messageVersion, BufferSupplier decompressionBufferSupplier)
{    try {        return new KafkaLZ4BlockInputStream(inputBuffer, decompressionBufferSupplier, messageVersion == RecordBatch.MAGIC_VALUE_V0);    } catch (Throwable e) {        throw new KafkaException(e);    }}
f2986
0
wrapForOutput
public OutputStream kafkatest_f2987_0(ByteBufferOutputStream buffer, byte messageVersion)
{    try {        return (OutputStream) ZstdConstructors.OUTPUT.invoke(buffer);    } catch (Throwable e) {        throw new KafkaException(e);    }}
f2987
0
wrapForInput
public InputStream kafkatest_f2988_0(ByteBuffer buffer, byte messageVersion, BufferSupplier decompressionBufferSupplier)
{    try {        return (InputStream) ZstdConstructors.INPUT.invoke(new ByteBufferInputStream(buffer));    } catch (Throwable e) {        throw new KafkaException(e);    }}
f2988
0
sequence
public int kafkatest_f2996_0()
{    return sequence;}
f2996
0
sizeInBytes
public int kafkatest_f2997_0()
{    return sizeInBytes;}
f2997
0
timestamp
public long kafkatest_f2998_0()
{    return timestamp;}
f2998
0
hasValue
public boolean kafkatest_f3007_0()
{    return value != null;}
f3007
0
value
public ByteBuffer kafkatest_f3008_0()
{    return value == null ? null : value.duplicate();}
f3008
0
headers
public Header[] kafkatest_f3009_0()
{    return headers;}
f3009
0
readFrom
public static DefaultRecord kafkatest_f3017_0(DataInput input, long baseOffset, long baseTimestamp, int baseSequence, Long logAppendTime) throws IOException
{    int sizeOfBodyInBytes = ByteUtils.readVarint(input);    ByteBuffer recordBuffer = ByteBuffer.allocate(sizeOfBodyInBytes);    input.readFully(recordBuffer.array(), 0, sizeOfBodyInBytes);    int totalSizeInBytes = ByteUtils.sizeOfVarint(sizeOfBodyInBytes) + sizeOfBodyInBytes;    return readFrom(recordBuffer, totalSizeInBytes, sizeOfBodyInBytes, baseOffset, baseTimestamp, baseSequence, logAppendTime);}
f3017
0
readFrom
public static DefaultRecord kafkatest_f3018_0(ByteBuffer buffer, long baseOffset, long baseTimestamp, int baseSequence, Long logAppendTime)
{    int sizeOfBodyInBytes = ByteUtils.readVarint(buffer);    if (buffer.remaining() < sizeOfBodyInBytes)        return null;    int totalSizeInBytes = ByteUtils.sizeOfVarint(sizeOfBodyInBytes) + sizeOfBodyInBytes;    return readFrom(buffer, totalSizeInBytes, sizeOfBodyInBytes, baseOffset, baseTimestamp, baseSequence, logAppendTime);}
f3018
0
readFrom
private static DefaultRecord kafkatest_f3019_0(ByteBuffer buffer, int sizeInBytes, int sizeOfBodyInBytes, long baseOffset, long baseTimestamp, int baseSequence, Long logAppendTime)
{    try {        int recordStart = buffer.position();        byte attributes = buffer.get();        long timestampDelta = ByteUtils.readVarlong(buffer);        long timestamp = baseTimestamp + timestampDelta;        if (logAppendTime != null)            timestamp = logAppendTime;        int offsetDelta = ByteUtils.readVarint(buffer);        long offset = baseOffset + offsetDelta;        int sequence = baseSequence >= 0 ? DefaultRecordBatch.incrementSequence(baseSequence, offsetDelta) : RecordBatch.NO_SEQUENCE;        ByteBuffer key = null;        int keySize = ByteUtils.readVarint(buffer);        if (keySize >= 0) {            key = buffer.slice();            key.limit(keySize);            buffer.position(buffer.position() + keySize);        }        ByteBuffer value = null;        int valueSize = ByteUtils.readVarint(buffer);        if (valueSize >= 0) {            value = buffer.slice();            value.limit(valueSize);            buffer.position(buffer.position() + valueSize);        }        int numHeaders = ByteUtils.readVarint(buffer);        if (numHeaders < 0)            throw new InvalidRecordException("Found invalid number of record headers " + numHeaders);        final Header[] headers;        if (numHeaders == 0)            headers = Record.EMPTY_HEADERS;        else            headers = readHeaders(buffer, numHeaders);        // validate whether we have read all header bytes in the current record        if (buffer.position() - recordStart != sizeOfBodyInBytes)            throw new InvalidRecordException("Invalid record size: expected to read " + sizeOfBodyInBytes + " bytes in record payload, but instead read " + (buffer.position() - recordStart));        return new DefaultRecord(sizeInBytes, attributes, offset, timestamp, sequence, key, value, headers);    } catch (BufferUnderflowException | IllegalArgumentException e) {        throw new InvalidRecordException("Found invalid record structure", e);    }}
f3019
0
readHeaders
private static Header[] kafkatest_f3027_0(ByteBuffer buffer, int numHeaders)
{    Header[] headers = new Header[numHeaders];    for (int i = 0; i < numHeaders; i++) {        int headerKeySize = ByteUtils.readVarint(buffer);        if (headerKeySize < 0)            throw new InvalidRecordException("Invalid negative header key size " + headerKeySize);        String headerKey = Utils.utf8(buffer, headerKeySize);        buffer.position(buffer.position() + headerKeySize);        ByteBuffer headerValue = null;        int headerValueSize = ByteUtils.readVarint(buffer);        if (headerValueSize >= 0) {            headerValue = buffer.slice();            headerValue.limit(headerValueSize);            buffer.position(buffer.position() + headerValueSize);        }        headers[i] = new RecordHeader(headerKey, headerValue);    }    return headers;}
f3027
0
sizeInBytes
public static int kafkatest_f3028_0(int offsetDelta, long timestampDelta, ByteBuffer key, ByteBuffer value, Header[] headers)
{    int bodySize = sizeOfBodyInBytes(offsetDelta, timestampDelta, key, value, headers);    return bodySize + ByteUtils.sizeOfVarint(bodySize);}
f3028
0
sizeInBytes
public static int kafkatest_f3029_0(int offsetDelta, long timestampDelta, int keySize, int valueSize, Header[] headers)
{    int bodySize = sizeOfBodyInBytes(offsetDelta, timestampDelta, keySize, valueSize, headers);    return bodySize + ByteUtils.sizeOfVarint(bodySize);}
f3029
0
firstTimestamp
public long kafkatest_f3037_0()
{    return buffer.getLong(FIRST_TIMESTAMP_OFFSET);}
f3037
0
maxTimestamp
public long kafkatest_f3038_0()
{    return buffer.getLong(MAX_TIMESTAMP_OFFSET);}
f3038
0
timestampType
public TimestampType kafkatest_f3039_0()
{    return (attributes() & TIMESTAMP_TYPE_MASK) == 0 ? TimestampType.CREATE_TIME : TimestampType.LOG_APPEND_TIME;}
f3039
0
compressionType
public CompressionType kafkatest_f3047_0()
{    return CompressionType.forId(attributes() & COMPRESSION_CODEC_MASK);}
f3047
0
sizeInBytes
public int kafkatest_f3048_0()
{    return LOG_OVERHEAD + buffer.getInt(LENGTH_OFFSET);}
f3048
0
count
private int kafkatest_f3049_0()
{    return buffer.getInt(RECORDS_COUNT_OFFSET);}
f3049
0
doReadRecord
protected Record kafkatest_f3057_0(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime) throws IOException
{    return DefaultRecord.readPartiallyFrom(inputStream, skipArray, baseOffset, firstTimestamp, baseSequence, logAppendTime);}
f3057
0
doReadRecord
protected Record kafkatest_f3058_0(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime) throws IOException
{    return DefaultRecord.readFrom(inputStream, baseOffset, firstTimestamp, baseSequence, logAppendTime);}
f3058
0
uncompressedIterator
private CloseableIterator<Record> kafkatest_f3059_0()
{    final ByteBuffer buffer = this.buffer.duplicate();    buffer.position(RECORDS_OFFSET);    return new RecordIterator() {        @Override        protected Record readNext(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime) {            try {                return DefaultRecord.readFrom(buffer, baseOffset, firstTimestamp, baseSequence, logAppendTime);            } catch (BufferUnderflowException e) {                throw new InvalidRecordException("Incorrect declared batch size, premature EOF reached");            }        }        @Override        protected boolean ensureNoneRemaining() {            return !buffer.hasRemaining();        }        @Override        public void close() {        }    };}
f3059
0
setPartitionLeaderEpoch
public void kafkatest_f3068_0(int epoch)
{    buffer.putInt(PARTITION_LEADER_EPOCH_OFFSET, epoch);}
f3068
0
checksum
public long kafkatest_f3069_0()
{    return ByteUtils.readUnsignedInt(buffer, CRC_OFFSET);}
f3069
0
isValid
public boolean kafkatest_f3070_0()
{    return sizeInBytes() >= RECORD_BATCH_OVERHEAD && checksum() == computeChecksum();}
f3070
0
toString
public String kafkatest_f3078_0()
{    return "RecordBatch(magic=" + magic() + ", offsets=[" + baseOffset() + ", " + lastOffset() + "], " + "compression=" + compressionType() + ", timestampType=" + timestampType() + ", crc=" + checksum() + ")";}
f3078
0
sizeInBytes
public static int kafkatest_f3079_0(long baseOffset, Iterable<Record> records)
{    Iterator<Record> iterator = records.iterator();    if (!iterator.hasNext())        return 0;    int size = RECORD_BATCH_OVERHEAD;    Long firstTimestamp = null;    while (iterator.hasNext()) {        Record record = iterator.next();        int offsetDelta = (int) (record.offset() - baseOffset);        if (firstTimestamp == null)            firstTimestamp = record.timestamp();        long timestampDelta = record.timestamp() - firstTimestamp;        size += DefaultRecord.sizeInBytes(offsetDelta, timestampDelta, record.key(), record.value(), record.headers());    }    return size;}
f3079
0
sizeInBytes
public static int kafkatest_f3080_0(Iterable<SimpleRecord> records)
{    Iterator<SimpleRecord> iterator = records.iterator();    if (!iterator.hasNext())        return 0;    int size = RECORD_BATCH_OVERHEAD;    int offsetDelta = 0;    Long firstTimestamp = null;    while (iterator.hasNext()) {        SimpleRecord record = iterator.next();        if (firstTimestamp == null)            firstTimestamp = record.timestamp();        long timestampDelta = record.timestamp() - firstTimestamp;        size += DefaultRecord.sizeInBytes(offsetDelta++, timestampDelta, record.key(), record.value(), record.headers());    }    return size;}
f3080
0
ensureNoneRemaining
protected boolean kafkatest_f3088_0()
{    try {        return inputStream.read() == -1;    } catch (IOException e) {        throw new KafkaException("Error checking for remaining bytes after reading batch", e);    }}
f3088
0
close
public void kafkatest_f3089_0()
{    try {        inputStream.close();    } catch (IOException e) {        throw new KafkaException("Failed to close record stream", e);    }}
f3089
0
toMemoryRecordBatch
protected RecordBatch kafkatest_f3090_0(ByteBuffer buffer)
{    return new DefaultRecordBatch(buffer);}
f3090
0
countOrNull
public Integer kafkatest_f3098_0()
{    return loadBatchHeader().countOrNull();}
f3098
0
isTransactional
public boolean kafkatest_f3099_0()
{    return loadBatchHeader().isTransactional();}
f3099
0
isControlBatch
public boolean kafkatest_f3100_0()
{    return loadBatchHeader().isControlBatch();}
f3100
0
equals
public boolean kafkatest_f3108_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    EndTransactionMarker that = (EndTransactionMarker) o;    return coordinatorEpoch == that.coordinatorEpoch && type == that.type;}
f3108
0
hashCode
public int kafkatest_f3109_0()
{    int result = type != null ? type.hashCode() : 0;    result = 31 * result + coordinatorEpoch;    return result;}
f3109
0
ensureTransactionMarkerControlType
private static void kafkatest_f3110_0(ControlRecordType type)
{    if (type != ControlRecordType.COMMIT && type != ControlRecordType.ABORT)        throw new IllegalArgumentException("Invalid control record type for end transaction marker" + type);}
f3110
0
position
public int kafkatest_f3118_0()
{    return position;}
f3118
0
magic
public byte kafkatest_f3119_0()
{    return magic;}
f3119
0
iterator
public Iterator<Record> kafkatest_f3120_0()
{    return loadFullBatch().iterator();}
f3120
0
loadBatchWithSize
private RecordBatch kafkatest_f3128_0(int size, String description)
{    FileChannel channel = fileRecords.channel();    try {        ByteBuffer buffer = ByteBuffer.allocate(size);        Utils.readFullyOrFail(channel, buffer, position, description);        buffer.rewind();        return toMemoryRecordBatch(buffer);    } catch (IOException e) {        throw new KafkaException("Failed to load record batch at position " + position + " from " + fileRecords, e);    }}
f3128
0
equals
public boolean kafkatest_f3129_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    FileChannelRecordBatch that = (FileChannelRecordBatch) o;    FileChannel channel = fileRecords == null ? null : fileRecords.channel();    FileChannel thatChannel = that.fileRecords == null ? null : that.fileRecords.channel();    return offset == that.offset && position == that.position && batchSize == that.batchSize && Objects.equals(channel, thatChannel);}
f3129
0
hashCode
public int kafkatest_f3130_0()
{    FileChannel channel = fileRecords == null ? null : fileRecords.channel();    int result = Long.hashCode(offset);    result = 31 * result + (channel != null ? channel.hashCode() : 0);    result = 31 * result + position;    result = 31 * result + batchSize;    return result;}
f3130
0
flush
public void kafkatest_f3138_0() throws IOException
{    channel.force(true);}
f3138
0
close
public void kafkatest_f3139_0() throws IOException
{    flush();    trim();    channel.close();}
f3139
0
closeHandlers
public void kafkatest_f3140_0() throws IOException
{    channel.close();}
f3140
0
searchForOffsetWithSize
public LogOffsetPosition kafkatest_f3148_0(long targetOffset, int startingPosition)
{    for (FileChannelRecordBatch batch : batchesFrom(startingPosition)) {        long offset = batch.lastOffset();        if (offset >= targetOffset)            return new LogOffsetPosition(offset, batch.position(), batch.sizeInBytes());    }    return null;}
f3148
0
searchForTimestamp
public TimestampAndOffset kafkatest_f3149_0(long targetTimestamp, int startingPosition, long startingOffset)
{    for (RecordBatch batch : batchesFrom(startingPosition)) {        if (batch.maxTimestamp() >= targetTimestamp) {            // We found a message            for (Record record : batch) {                long timestamp = record.timestamp();                if (timestamp >= targetTimestamp && record.offset() >= startingOffset)                    return new TimestampAndOffset(timestamp, record.offset(), maybeLeaderEpoch(batch.partitionLeaderEpoch()));            }        }    }    return null;}
f3149
0
largestTimestampAfter
public TimestampAndOffset kafkatest_f3150_0(int startingPosition)
{    long maxTimestamp = RecordBatch.NO_TIMESTAMP;    long offsetOfMaxTimestamp = -1L;    int leaderEpochOfMaxTimestamp = RecordBatch.NO_PARTITION_LEADER_EPOCH;    for (RecordBatch batch : batchesFrom(startingPosition)) {        long timestamp = batch.maxTimestamp();        if (timestamp > maxTimestamp) {            maxTimestamp = timestamp;            offsetOfMaxTimestamp = batch.lastOffset();            leaderEpochOfMaxTimestamp = batch.partitionLeaderEpoch();        }    }    return new TimestampAndOffset(maxTimestamp, offsetOfMaxTimestamp, maybeLeaderEpoch(leaderEpochOfMaxTimestamp));}
f3150
0
open
public static FileRecords kafkatest_f3158_0(File file, boolean fileAlreadyExists, int initFileSize, boolean preallocate) throws IOException
{    return open(file, true, fileAlreadyExists, initFileSize, preallocate);}
f3158
0
open
public static FileRecords kafkatest_f3159_0(File file, boolean mutable) throws IOException
{    return open(file, mutable, false, 0, false);}
f3159
0
open
public static FileRecords kafkatest_f3160_0(File file) throws IOException
{    return open(file, true);}
f3160
0
ignoreFlagDescriptorChecksum
public boolean kafkatest_f3168_0()
{    return this.ignoreFlagDescriptorChecksum;}
f3168
0
readHeader
private void kafkatest_f3169_0() throws IOException
{    // read first 6 bytes into buffer to check magic and FLG/BD descriptor flags    if (in.remaining() < 6) {        throw new IOException(PREMATURE_EOS);    }    if (MAGIC != in.getInt()) {        throw new IOException(NOT_SUPPORTED);    }    // mark start of data to checksum    in.mark();    flg = FLG.fromByte(in.get());    maxBlockSize = BD.fromByte(in.get()).getBlockMaximumSize();    if (flg.isContentSizeSet()) {        if (in.remaining() < 8) {            throw new IOException(PREMATURE_EOS);        }        in.position(in.position() + 8);    }    // Old implementations produced incorrect HC checksums    if (ignoreFlagDescriptorChecksum) {        in.position(in.position() + 1);        return;    }    int len = in.position() - in.reset().position();    int hash = CHECKSUM.hash(in, in.position(), len, 0);    in.position(in.position() + len);    if (in.get() != (byte) ((hash >> 8) & 0xFF)) {        throw new IOException(DESCRIPTOR_HASH_MISMATCH);    }}
f3169
0
readBlock
private void kafkatest_f3170_0() throws IOException
{    if (in.remaining() < 4) {        throw new IOException(PREMATURE_EOS);    }    int blockSize = in.getInt();    boolean compressed = (blockSize & LZ4_FRAME_INCOMPRESSIBLE_MASK) == 0;    blockSize &= ~LZ4_FRAME_INCOMPRESSIBLE_MASK;    // Check for EndMark    if (blockSize == 0) {        finished = true;        if (flg.isContentChecksumSet())            // TODO: verify this content checksum            in.getInt();        return;    } else if (blockSize > maxBlockSize) {        throw new IOException(String.format("Block size %s exceeded max: %s", blockSize, maxBlockSize));    }    if (in.remaining() < blockSize) {        throw new IOException(PREMATURE_EOS);    }    if (compressed) {        try {            final int bufferSize = DECOMPRESSOR.decompress(in, in.position(), blockSize, decompressionBuffer, 0, maxBlockSize);            decompressionBuffer.position(0);            decompressionBuffer.limit(bufferSize);            decompressedBuffer = decompressionBuffer;        } catch (LZ4Exception e) {            throw new IOException(e);        }    } else {        decompressedBuffer = in.slice();        decompressedBuffer.limit(blockSize);    }    // verify checksum    if (flg.isBlockChecksumSet()) {        int hash = CHECKSUM.hash(in, in.position(), blockSize, 0);        in.position(in.position() + blockSize);        if (hash != in.getInt()) {            throw new IOException(BLOCK_HASH_MISMATCH);        }    } else {        in.position(in.position() + blockSize);    }}
f3170
0
markSupported
public boolean kafkatest_f3178_0()
{    return false;}
f3178
0
useBrokenFlagDescriptorChecksum
public boolean kafkatest_f3179_0()
{    return this.useBrokenFlagDescriptorChecksum;}
f3179
0
writeHeader
private void kafkatest_f3180_0() throws IOException
{    ByteUtils.writeUnsignedIntLE(buffer, 0, MAGIC);    bufferOffset = 4;    buffer[bufferOffset++] = flg.toByte();    buffer[bufferOffset++] = bd.toByte();    // TODO write uncompressed content size, update flg.validate()    // compute checksum on all descriptor fields    int offset = 4;    int len = bufferOffset - offset;    if (this.useBrokenFlagDescriptorChecksum) {        len += offset;        offset = 0;    }    byte hash = (byte) ((checksum.hash(buffer, offset, len, 0) >> 8) & 0xFF);    buffer[bufferOffset++] = hash;    // write out frame descriptor    out.write(buffer, 0, bufferOffset);    bufferOffset = 0;}
f3180
0
fromByte
public static FLG kafkatest_f3188_0(byte flg)
{    int reserved = (flg >>> 0) & 3;    int contentChecksum = (flg >>> 2) & 1;    int contentSize = (flg >>> 3) & 1;    int blockChecksum = (flg >>> 4) & 1;    int blockIndependence = (flg >>> 5) & 1;    int version = (flg >>> 6) & 3;    return new FLG(reserved, contentChecksum, contentSize, blockChecksum, blockIndependence, version);}
f3188
0
toByte
public byte kafkatest_f3189_0()
{    return (byte) (((reserved & 3) << 0) | ((contentChecksum & 1) << 2) | ((contentSize & 1) << 3) | ((blockChecksum & 1) << 4) | ((blockIndependence & 1) << 5) | ((version & 3) << 6));}
f3189
0
validate
private void kafkatest_f3190_0()
{    if (reserved != 0) {        throw new RuntimeException("Reserved bits must be 0");    }    if (blockIndependence != 1) {        throw new RuntimeException("Dependent block stream is unsupported");    }    if (version != VERSION) {        throw new RuntimeException(String.format("Version %d is unsupported", version));    }}
f3190
0
getBlockMaximumSize
public int kafkatest_f3198_0()
{    return 1 << ((2 * blockSizeValue) + 8);}
f3198
0
toByte
public byte kafkatest_f3199_0()
{    return (byte) (((reserved2 & 15) << 0) | ((blockSizeValue & 7) << 4) | ((reserved3 & 1) << 7));}
f3199
0
sizeInBytes
public int kafkatest_f3200_0()
{    return sizeInBytes;}
f3200
0
buildOverflowBatch
private MemoryRecordsf3208_1int remaining)
{    // We do not have any records left to down-convert. Construct an overflow message for the length remaining.    // This message will be ignored by the consumer because its length will be past the length of maximum    // possible response size.    // DefaultRecordBatch =>    // BaseOffset => Int64    // Length => Int32    // ...    ByteBuffer overflowMessageBatch = ByteBuffer.allocate(Math.max(MIN_OVERFLOW_MESSAGE_LENGTH, Math.min(remaining + 1, MAX_READ_SIZE)));    overflowMessageBatch.putLong(-1L);    // Fill in the length of the overflow batch. A valid batch must be at least as long as the minimum batch    // overhead.    overflowMessageBatch.putInt(Math.max(remaining + 1, DefaultRecordBatch.RECORD_BATCH_OVERHEAD));        return MemoryRecords.readableRecords(overflowMessageBatch);}
private MemoryRecordsf3208
1
writeTo
public longf3209_1GatheringByteChannel channel, long previouslyWritten, int remaining) throws IOException
{    if (convertedRecordsWriter == null || convertedRecordsWriter.completed()) {        MemoryRecords convertedRecords;        try {            // Check if we have more chunks left to down-convert            if (convertedRecordsIterator.hasNext()) {                // Get next chunk of down-converted messages                ConvertedRecords<?> recordsAndStats = convertedRecordsIterator.next();                convertedRecords = (MemoryRecords) recordsAndStats.records();                recordConversionStats.add(recordsAndStats.recordConversionStats());                            } else {                convertedRecords = buildOverflowBatch(remaining);            }        } catch (UnsupportedCompressionTypeException e) {            // We have encountered a compression type which does not support down-conversion (e.g. zstd).            // Since we have already sent at least one batch and we have committed to the fetch size, we            // send an overflow batch. The consumer will read the first few records and then fetch from the            // offset of the batch which has the unsupported compression type. At that time, we will            // send back the UNSUPPORTED_COMPRESSION_TYPE erro which will allow the consumer to fail gracefully.            convertedRecords = buildOverflowBatch(remaining);        }        convertedRecordsWriter = new DefaultRecordsSend(destination(), convertedRecords, Math.min(convertedRecords.sizeInBytes(), remaining));    }    return convertedRecordsWriter.writeTo(channel);}
public longf3209
1
recordConversionStats
public RecordConversionStats kafkatest_f3210_0()
{    return recordConversionStats;}
f3210
0
sizeInBytes
public int kafkatest_f3218_0()
{    return buffer.limit();}
f3218
0
keySize
public int kafkatest_f3219_0()
{    if (magic() == RecordBatch.MAGIC_VALUE_V0)        return buffer.getInt(KEY_SIZE_OFFSET_V0);    else        return buffer.getInt(KEY_SIZE_OFFSET_V1);}
f3219
0
hasKey
public boolean kafkatest_f3220_0()
{    return keySize() >= 0;}
f3220
0
compressionType
public CompressionType kafkatest_f3228_0()
{    return CompressionType.forId(buffer.get(ATTRIBUTES_OFFSET) & COMPRESSION_CODEC_MASK);}
f3228
0
value
public ByteBuffer kafkatest_f3229_0()
{    return Utils.sizeDelimited(buffer, valueSizeOffset());}
f3229
0
key
public ByteBuffer kafkatest_f3230_0()
{    if (magic() == RecordBatch.MAGIC_VALUE_V0)        return Utils.sizeDelimited(buffer, KEY_SIZE_OFFSET_V0);    else        return Utils.sizeDelimited(buffer, KEY_SIZE_OFFSET_V1);}
f3230
0
write
private static void kafkatest_f3238_0(ByteBuffer buffer, byte magic, long timestamp, ByteBuffer key, ByteBuffer value, CompressionType compressionType, TimestampType timestampType)
{    try {        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));        write(out, magic, timestamp, key, value, compressionType, timestampType);    } catch (IOException e) {        throw new KafkaException(e);    }}
f3238
0
write
public static long kafkatest_f3239_0(DataOutputStream out, byte magic, long timestamp, byte[] key, byte[] value, CompressionType compressionType, TimestampType timestampType) throws IOException
{    return write(out, magic, timestamp, wrapNullable(key), wrapNullable(value), compressionType, timestampType);}
f3239
0
write
public static long kafkatest_f3240_0(DataOutputStream out, byte magic, long timestamp, ByteBuffer key, ByteBuffer value, CompressionType compressionType, TimestampType timestampType) throws IOException
{    byte attributes = computeAttributes(magic, compressionType, timestampType);    long crc = computeChecksum(magic, attributes, timestamp, key, value);    write(out, magic, crc, attributes, timestamp, key, value);    return crc;}
f3240
0
recordOverhead
 static int kafkatest_f3248_0(byte magic)
{    if (magic == 0)        return RECORD_OVERHEAD_V0;    else if (magic == 1)        return RECORD_OVERHEAD_V1;    throw new IllegalArgumentException("Invalid magic used in LegacyRecord: " + magic);}
f3248
0
headerSize
 static int kafkatest_f3249_0(byte magic)
{    if (magic == 0)        return HEADER_SIZE_V0;    else if (magic == 1)        return HEADER_SIZE_V1;    throw new IllegalArgumentException("Invalid magic used in LegacyRecord: " + magic);}
f3249
0
keyOffset
private static int kafkatest_f3250_0(byte magic)
{    if (magic == 0)        return KEY_OFFSET_V0;    else if (magic == 1)        return KEY_OFFSET_V1;    throw new IllegalArgumentException("Invalid magic used in LegacyRecord: " + magic);}
f3250
0
firstBatchSize
public Integer kafkatest_f3258_0()
{    if (buffer.remaining() < HEADER_SIZE_UP_TO_MAGIC)        return null;    return new ByteBufferLogInputStream(buffer, Integer.MAX_VALUE).nextBatchSize();}
f3258
0
filterTo
public FilterResult kafkatest_f3259_0(TopicPartition partition, RecordFilter filter, ByteBuffer destinationBuffer, int maxRecordBatchSize, BufferSupplier decompressionBufferSupplier)
{    return filterTo(partition, batches(), filter, destinationBuffer, maxRecordBatchSize, decompressionBufferSupplier);}
f3259
0
filterTo
private static FilterResultf3260_1TopicPartition partition, Iterable<MutableRecordBatch> batches, RecordFilter filter, ByteBuffer destinationBuffer, int maxRecordBatchSize, BufferSupplier decompressionBufferSupplier)
{    FilterResult filterResult = new FilterResult(destinationBuffer);    ByteBufferOutputStream bufferOutputStream = new ByteBufferOutputStream(destinationBuffer);    for (MutableRecordBatch batch : batches) {        long maxOffset = -1L;        BatchRetention batchRetention = filter.checkBatchRetention(batch);        filterResult.bytesRead += batch.sizeInBytes();        if (batchRetention == BatchRetention.DELETE)            continue;        // We use the absolute offset to decide whether to retain the message or not. Due to KAFKA-4298, we have to        // allow for the possibility that a previous version corrupted the log by writing a compressed record batch        // with a magic value not matching the magic of the records (magic < 2). This will be fixed as we        // recopy the messages to the destination buffer.        byte batchMagic = batch.magic();        boolean writeOriginalBatch = true;        List<Record> retainedRecords = new ArrayList<>();        try (final CloseableIterator<Record> iterator = batch.streamingIterator(decompressionBufferSupplier)) {            while (iterator.hasNext()) {                Record record = iterator.next();                filterResult.messagesRead += 1;                if (filter.shouldRetainRecord(batch, record)) {                    // the corrupted batch with correct data.                    if (!record.hasMagic(batchMagic))                        writeOriginalBatch = false;                    if (record.offset() > maxOffset)                        maxOffset = record.offset();                    retainedRecords.add(record);                } else {                    writeOriginalBatch = false;                }            }        }        if (!retainedRecords.isEmpty()) {            if (writeOriginalBatch) {                batch.writeTo(bufferOutputStream);                filterResult.updateRetainedBatchMetadata(batch, retainedRecords.size(), false);            } else {                MemoryRecordsBuilder builder = buildRetainedRecordsInto(batch, retainedRecords, bufferOutputStream);                MemoryRecords records = builder.build();                int filteredBatchSize = records.sizeInBytes();                if (filteredBatchSize > batch.sizeInBytes() && filteredBatchSize > maxRecordBatchSize)                                    MemoryRecordsBuilder.RecordsInfo info = builder.info();                filterResult.updateRetainedBatchMetadata(info.maxTimestamp, info.shallowOffsetOfMaxTimestamp, maxOffset, retainedRecords.size(), filteredBatchSize);            }        } else if (batchRetention == BatchRetention.RETAIN_EMPTY) {            if (batchMagic < RecordBatch.MAGIC_VALUE_V2)                throw new IllegalStateException("Empty batches are only supported for magic v2 and above");            bufferOutputStream.ensureRemaining(DefaultRecordBatch.RECORD_BATCH_OVERHEAD);            DefaultRecordBatch.writeEmptyHeader(bufferOutputStream.buffer(), batchMagic, batch.producerId(), batch.producerEpoch(), batch.baseSequence(), batch.baseOffset(), batch.lastOffset(), batch.partitionLeaderEpoch(), batch.timestampType(), batch.maxTimestamp(), batch.isTransactional(), batch.isControlBatch());            filterResult.updateRetainedBatchMetadata(batch, 0, true);        }        // If we had to allocate a new buffer to fit the filtered buffer (see KAFKA-5316), return early to        // avoid the need for additional allocations.        ByteBuffer outputBuffer = bufferOutputStream.buffer();        if (outputBuffer != destinationBuffer) {            filterResult.outputBuffer = outputBuffer;            return filterResult;        }    }    return filterResult;}
private static FilterResultf3260
1
updateRetainedBatchMetadata
private void kafkatest_f3268_0(long maxTimestamp, long shallowOffsetOfMaxTimestamp, long maxOffset, int messagesRetained, int bytesRetained)
{    validateBatchMetadata(maxTimestamp, shallowOffsetOfMaxTimestamp, maxOffset);    if (maxTimestamp > this.maxTimestamp) {        this.maxTimestamp = maxTimestamp;        this.shallowOffsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp;    }    this.maxOffset = Math.max(maxOffset, this.maxOffset);    this.messagesRetained += messagesRetained;    this.bytesRetained += bytesRetained;}
f3268
0
validateBatchMetadata
private void kafkatest_f3269_0(long maxTimestamp, long shallowOffsetOfMaxTimestamp, long maxOffset)
{    if (maxTimestamp != RecordBatch.NO_TIMESTAMP && shallowOffsetOfMaxTimestamp < 0)        throw new IllegalArgumentException("shallowOffset undefined for maximum timestamp " + maxTimestamp);    if (maxOffset < 0)        throw new IllegalArgumentException("maxOffset undefined");}
f3269
0
outputBuffer
public ByteBuffer kafkatest_f3270_0()
{    return outputBuffer;}
f3270
0
readableRecords
public static MemoryRecords kafkatest_f3278_0(ByteBuffer buffer)
{    return new MemoryRecords(buffer);}
f3278
0
builder
public static MemoryRecordsBuilder kafkatest_f3279_0(ByteBuffer buffer, CompressionType compressionType, TimestampType timestampType, long baseOffset)
{    return builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, timestampType, baseOffset);}
f3279
0
idempotentBuilder
public static MemoryRecordsBuilder kafkatest_f3280_0(ByteBuffer buffer, CompressionType compressionType, long baseOffset, long producerId, short producerEpoch, int baseSequence)
{    return builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, baseOffset, System.currentTimeMillis(), producerId, producerEpoch, baseSequence);}
f3280
0
withRecords
public static MemoryRecords kafkatest_f3288_0(CompressionType compressionType, SimpleRecord... records)
{    return withRecords(RecordBatch.CURRENT_MAGIC_VALUE, compressionType, records);}
f3288
0
withRecords
public static MemoryRecords kafkatest_f3289_0(CompressionType compressionType, int partitionLeaderEpoch, SimpleRecord... records)
{    return withRecords(RecordBatch.CURRENT_MAGIC_VALUE, 0L, compressionType, TimestampType.CREATE_TIME, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, partitionLeaderEpoch, false, records);}
f3289
0
withRecords
public static MemoryRecords kafkatest_f3290_0(byte magic, CompressionType compressionType, SimpleRecord... records)
{    return withRecords(magic, 0L, compressionType, TimestampType.CREATE_TIME, records);}
f3290
0
withTransactionalRecords
public static MemoryRecords kafkatest_f3298_0(byte magic, long initialOffset, CompressionType compressionType, long producerId, short producerEpoch, int baseSequence, int partitionLeaderEpoch, SimpleRecord... records)
{    return withRecords(magic, initialOffset, compressionType, TimestampType.CREATE_TIME, producerId, producerEpoch, baseSequence, partitionLeaderEpoch, true, records);}
f3298
0
withTransactionalRecords
public static MemoryRecords kafkatest_f3299_0(long initialOffset, CompressionType compressionType, long producerId, short producerEpoch, int baseSequence, int partitionLeaderEpoch, SimpleRecord... records)
{    return withTransactionalRecords(RecordBatch.CURRENT_MAGIC_VALUE, initialOffset, compressionType, producerId, producerEpoch, baseSequence, partitionLeaderEpoch, records);}
f3299
0
withRecords
public static MemoryRecords kafkatest_f3300_0(byte magic, long initialOffset, CompressionType compressionType, TimestampType timestampType, SimpleRecord... records)
{    return withRecords(magic, initialOffset, compressionType, timestampType, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, RecordBatch.NO_PARTITION_LEADER_EPOCH, false, records);}
f3300
0
initialCapacity
public int kafkatest_f3308_0()
{    return bufferStream.initialCapacity();}
f3308
0
compressionRatio
public double kafkatest_f3309_0()
{    return actualCompressionRatio;}
f3309
0
compressionType
public CompressionType kafkatest_f3310_0()
{    return compressionType;}
f3310
0
overrideLastOffset
public void kafkatest_f3318_0(long lastOffset)
{    if (builtRecords != null)        throw new IllegalStateException("Cannot override the last offset after the records have been built");    this.lastOffset = lastOffset;}
f3318
0
closeForRecordAppends
public void kafkatest_f3319_0()
{    if (appendStream != CLOSED_STREAM) {        try {            appendStream.close();        } catch (IOException e) {            throw new KafkaException(e);        } finally {            appendStream = CLOSED_STREAM;        }    }}
f3319
0
abort
public void kafkatest_f3320_0()
{    closeForRecordAppends();    buffer().position(initialPosition);    aborted = true;}
f3320
0
appendWithOffset
public Long kafkatest_f3328_0(long offset, long timestamp, ByteBuffer key, ByteBuffer value, Header[] headers)
{    return appendWithOffset(offset, false, timestamp, key, value, headers);}
f3328
0
appendWithOffset
public Long kafkatest_f3329_0(long offset, long timestamp, byte[] key, byte[] value)
{    return appendWithOffset(offset, timestamp, wrapNullable(key), wrapNullable(value), Record.EMPTY_HEADERS);}
f3329
0
appendWithOffset
public Long kafkatest_f3330_0(long offset, long timestamp, ByteBuffer key, ByteBuffer value)
{    return appendWithOffset(offset, timestamp, key, value, Record.EMPTY_HEADERS);}
f3330
0
appendEndTxnMarker
public Long kafkatest_f3338_0(long timestamp, EndTransactionMarker marker)
{    if (producerId == RecordBatch.NO_PRODUCER_ID)        throw new IllegalArgumentException("End transaction marker requires a valid producerId");    if (!isTransactional)        throw new IllegalArgumentException("End transaction marker depends on batch transactional flag being enabled");    ByteBuffer value = marker.serializeValue();    return appendControlRecord(timestamp, marker.controlType(), value);}
f3338
0
appendUncheckedWithOffset
public void kafkatest_f3339_0(long offset, LegacyRecord record)
{    ensureOpenForRecordAppend();    try {        int size = record.sizeInBytes();        AbstractLegacyRecordBatch.writeHeader(appendStream, toInnerOffset(offset), size);        ByteBuffer buffer = record.buffer().duplicate();        appendStream.write(buffer.array(), buffer.arrayOffset(), buffer.limit());        recordWritten(offset, record.timestamp(), size + Records.LOG_OVERHEAD);    } catch (IOException e) {        throw new KafkaException("I/O exception when writing to the append stream, closing", e);    }}
f3339
0
append
public void kafkatest_f3340_0(Record record)
{    appendWithOffset(record.offset(), isControlBatch, record.timestamp(), record.key(), record.value(), record.headers());}
f3340
0
ensureOpenForRecordAppend
private void kafkatest_f3348_0()
{    if (appendStream == CLOSED_STREAM)        throw new IllegalStateException("Tried to append a record, but MemoryRecordsBuilder is closed for record appends");}
f3348
0
ensureOpenForRecordBatchWrite
private void kafkatest_f3349_0()
{    if (isClosed())        throw new IllegalStateException("Tried to write record batch header, but MemoryRecordsBuilder is closed");    if (aborted)        throw new IllegalStateException("Tried to write record batch header, but MemoryRecordsBuilder is aborted");}
f3349
0
estimatedBytesWritten
private int kafkatest_f3350_0()
{    if (compressionType == CompressionType.NONE) {        return batchHeaderSizeInBytes + uncompressedRecordsSizeInBytes;    } else {        // estimate the written bytes to the underlying byte buffer based on uncompressed written bytes        return batchHeaderSizeInBytes + (int) (uncompressedRecordsSizeInBytes * estimatedCompressionRatio * COMPRESSION_RATE_ESTIMATION_FACTOR);    }}
f3350
0
nextSequentialOffset
private long kafkatest_f3358_0()
{    return lastOffset == null ? baseOffset : lastOffset + 1;}
f3358
0
producerId
public long kafkatest_f3359_0()
{    return this.producerId;}
f3359
0
producerEpoch
public short kafkatest_f3360_0()
{    return this.producerEpoch;}
f3360
0
updateRecordConversionStats
private void kafkatest_f3368_0(Send completedSend)
{    // and fold it up appropriately.    if (completedSend instanceof LazyDownConversionRecordsSend) {        if (recordConversionStats == null)            recordConversionStats = new HashMap<>();        LazyDownConversionRecordsSend lazyRecordsSend = (LazyDownConversionRecordsSend) completedSend;        recordConversionStats.put(lazyRecordsSend.topicPartition(), lazyRecordsSend.recordConversionStats());    }}
f3368
0
equals
public boolean kafkatest_f3369_0(Object o)
{    return super.equals(o) && this.keySize == ((PartialDefaultRecord) o).keySize && this.valueSize == ((PartialDefaultRecord) o).valueSize;}
f3369
0
hashCode
public int kafkatest_f3370_0()
{    int result = super.hashCode();    result = 31 * result + keySize;    result = 31 * result + valueSize;    return result;}
f3370
0
headers
public Header[] kafkatest_f3378_0()
{    throw new UnsupportedOperationException("headers is skipped in PartialDefaultRecord");}
f3378
0
makeNext
protected T kafkatest_f3379_0()
{    try {        T batch = logInputStream.nextBatch();        if (batch == null)            return allDone();        return batch;    } catch (IOException e) {        throw new KafkaException(e);    }}
f3379
0
add
public void kafkatest_f3380_0(RecordConversionStats stats)
{    temporaryMemoryBytes += stats.temporaryMemoryBytes;    numRecordsConverted += stats.numRecordsConverted;    conversionTimeNanos += stats.conversionTimeNanos;}
f3380
0
size
public long kafkatest_f3388_0()
{    return maxBytesToWrite;}
f3388
0
records
protected T kafkatest_f3389_0()
{    return records;}
f3389
0
downConvert
protected static ConvertedRecords<MemoryRecords> kafkatest_f3390_0(Iterable<? extends RecordBatch> batches, byte toMagic, long firstOffset, Time time)
{    // maintain the batch along with the decompressed records to avoid the need to decompress again    List<RecordBatchAndRecords> recordBatchAndRecordsList = new ArrayList<>();    int totalSizeEstimate = 0;    long startNanos = time.nanoseconds();    for (RecordBatch batch : batches) {        if (toMagic < RecordBatch.MAGIC_VALUE_V2) {            if (batch.isControlBatch())                continue;            if (batch.compressionType() == CompressionType.ZSTD)                throw new UnsupportedCompressionTypeException("Down-conversion of zstandard-compressed batches " + "is not supported");        }        if (batch.magic() <= toMagic) {            totalSizeEstimate += batch.sizeInBytes();            recordBatchAndRecordsList.add(new RecordBatchAndRecords(batch, null, null));        } else {            List<Record> records = new ArrayList<>();            for (Record record : batch) {                // See the method javadoc for an explanation                if (toMagic > RecordBatch.MAGIC_VALUE_V1 || batch.isCompressed() || record.offset() >= firstOffset)                    records.add(record);            }            if (records.isEmpty())                continue;            final long baseOffset;            if (batch.magic() >= RecordBatch.MAGIC_VALUE_V2 && toMagic >= RecordBatch.MAGIC_VALUE_V2)                baseOffset = batch.baseOffset();            else                baseOffset = records.get(0).offset();            totalSizeEstimate += AbstractRecords.estimateSizeInBytes(toMagic, baseOffset, batch.compressionType(), records);            recordBatchAndRecordsList.add(new RecordBatchAndRecords(batch, records, baseOffset));        }    }    ByteBuffer buffer = ByteBuffer.allocate(totalSizeEstimate);    long temporaryMemoryBytes = 0;    int numRecordsConverted = 0;    for (RecordBatchAndRecords recordBatchAndRecords : recordBatchAndRecordsList) {        temporaryMemoryBytes += recordBatchAndRecords.batch.sizeInBytes();        if (recordBatchAndRecords.batch.magic() <= toMagic) {            buffer = Utils.ensureCapacity(buffer, buffer.position() + recordBatchAndRecords.batch.sizeInBytes());            recordBatchAndRecords.batch.writeTo(buffer);        } else {            MemoryRecordsBuilder builder = convertRecordBatch(toMagic, buffer, recordBatchAndRecords);            buffer = builder.buffer();            temporaryMemoryBytes += builder.uncompressedBytesWritten();            numRecordsConverted += builder.numRecords();        }    }    buffer.flip();    RecordConversionStats stats = new RecordConversionStats(temporaryMemoryBytes, numRecordsConverted, time.nanoseconds() - startNanos);    return new ConvertedRecords<>(MemoryRecords.readableRecords(buffer), stats);}
f3390
0
headers
public Header[] kafkatest_f3398_0()
{    return headers;}
f3398
0
equals
public boolean kafkatest_f3399_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    SimpleRecord that = (SimpleRecord) o;    return timestamp == that.timestamp && Objects.equals(key, that.key) && Objects.equals(value, that.value) && Arrays.equals(headers, that.headers);}
f3399
0
hashCode
public int kafkatest_f3400_0()
{    int result = key != null ? key.hashCode() : 0;    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + Long.hashCode(timestamp);    result = 31 * result + Arrays.hashCode(headers);    return result;}
f3400
0
listenerName
public String kafkatest_f3408_0()
{    return listenerName;}
f3408
0
equals
public boolean kafkatest_f3409_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    DefaultClientMetadata that = (DefaultClientMetadata) o;    return Objects.equals(rackId, that.rackId) && Objects.equals(clientId, that.clientId) && Objects.equals(clientAddress, that.clientAddress) && Objects.equals(principal, that.principal) && Objects.equals(listenerName, that.listenerName);}
f3409
0
hashCode
public int kafkatest_f3410_0()
{    return Objects.hash(rackId, clientId, clientAddress, principal, listenerName);}
f3410
0
close
 void kafkatest_f3418_0() throws IOException
{// No-op by default}
f3418
0
configure
 void kafkatest_f3419_0(Map<String, ?> configs)
{// No-op by default}
f3419
0
comparator
 static Comparator<ReplicaView> kafkatest_f3420_0()
{    return Comparator.comparingLong(ReplicaView::logEndOffset).thenComparing(Comparator.comparingLong(ReplicaView::timeSinceLastCaughtUpMs).reversed()).thenComparing(replicaInfo -> replicaInfo.endpoint().id());}
f3420
0
controllerEpoch
public int kafkatest_f3428_0()
{    return controllerEpoch;}
f3428
0
brokerEpoch
public long kafkatest_f3429_0()
{    return brokerEpoch;}
f3429
0
size
 long kafkatest_f3430_0()
{    return toStruct().sizeOf();}
f3430
0
toString
public String kafkatest_f3438_0(boolean verbose)
{    return toStruct().toString();}
f3438
0
toString
public final String kafkatest_f3439_0()
{    return toString(true);}
f3439
0
getErrorResponse
public AbstractResponse kafkatest_f3440_0(Throwable e)
{    return getErrorResponse(AbstractResponse.DEFAULT_THROTTLE_TIME, e);}
f3440
0
apiErrorCounts
protected Map<Errors, Integer> kafkatest_f3448_0(Map<?, ApiError> errors)
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (ApiError apiError : errors.values()) updateErrorCounts(errorCounts, apiError.error());    return errorCounts;}
f3448
0
updateErrorCounts
protected void kafkatest_f3449_0(Map<Errors, Integer> errorCounts, Errors error)
{    Integer count = errorCounts.get(error);    errorCounts.put(error, count == null ? 1 : count + 1);}
f3449
0
parseResponse
public static AbstractResponse kafkatest_f3450_0(ApiKeys apiKey, Struct struct, short version)
{    switch(apiKey) {        case PRODUCE:            return new ProduceResponse(struct);        case FETCH:            return FetchResponse.parse(struct);        case LIST_OFFSETS:            return new ListOffsetResponse(struct);        case METADATA:            return new MetadataResponse(struct, version);        case OFFSET_COMMIT:            return new OffsetCommitResponse(struct, version);        case OFFSET_FETCH:            return new OffsetFetchResponse(struct, version);        case FIND_COORDINATOR:            return new FindCoordinatorResponse(struct, version);        case JOIN_GROUP:            return new JoinGroupResponse(struct, version);        case HEARTBEAT:            return new HeartbeatResponse(struct, version);        case LEAVE_GROUP:            return new LeaveGroupResponse(struct, version);        case SYNC_GROUP:            return new SyncGroupResponse(struct, version);        case STOP_REPLICA:            return new StopReplicaResponse(struct);        case CONTROLLED_SHUTDOWN:            return new ControlledShutdownResponse(struct, version);        case UPDATE_METADATA:            return new UpdateMetadataResponse(struct);        case LEADER_AND_ISR:            return new LeaderAndIsrResponse(struct);        case DESCRIBE_GROUPS:            return new DescribeGroupsResponse(struct, version);        case LIST_GROUPS:            return new ListGroupsResponse(struct, version);        case SASL_HANDSHAKE:            return new SaslHandshakeResponse(struct, version);        case API_VERSIONS:            return new ApiVersionsResponse(struct);        case CREATE_TOPICS:            return new CreateTopicsResponse(struct, version);        case DELETE_TOPICS:            return new DeleteTopicsResponse(struct, version);        case DELETE_RECORDS:            return new DeleteRecordsResponse(struct);        case INIT_PRODUCER_ID:            return new InitProducerIdResponse(struct, version);        case OFFSET_FOR_LEADER_EPOCH:            return new OffsetsForLeaderEpochResponse(struct);        case ADD_PARTITIONS_TO_TXN:            return new AddPartitionsToTxnResponse(struct);        case ADD_OFFSETS_TO_TXN:            return new AddOffsetsToTxnResponse(struct);        case END_TXN:            return new EndTxnResponse(struct);        case WRITE_TXN_MARKERS:            return new WriteTxnMarkersResponse(struct);        case TXN_OFFSET_COMMIT:            return new TxnOffsetCommitResponse(struct, version);        case DESCRIBE_ACLS:            return new DescribeAclsResponse(struct);        case CREATE_ACLS:            return new CreateAclsResponse(struct);        case DELETE_ACLS:            return new DeleteAclsResponse(struct);        case DESCRIBE_CONFIGS:            return new DescribeConfigsResponse(struct);        case ALTER_CONFIGS:            return new AlterConfigsResponse(struct);        case ALTER_REPLICA_LOG_DIRS:            return new AlterReplicaLogDirsResponse(struct);        case DESCRIBE_LOG_DIRS:            return new DescribeLogDirsResponse(struct);        case SASL_AUTHENTICATE:            return new SaslAuthenticateResponse(struct, version);        case CREATE_PARTITIONS:            return new CreatePartitionsResponse(struct);        case CREATE_DELEGATION_TOKEN:            return new CreateDelegationTokenResponse(struct, version);        case RENEW_DELEGATION_TOKEN:            return new RenewDelegationTokenResponse(struct, version);        case EXPIRE_DELEGATION_TOKEN:            return new ExpireDelegationTokenResponse(struct, version);        case DESCRIBE_DELEGATION_TOKEN:            return new DescribeDelegationTokenResponse(struct, version);        case DELETE_GROUPS:            return new DeleteGroupsResponse(struct, version);        case ELECT_LEADERS:            return new ElectLeadersResponse(struct, version);        case INCREMENTAL_ALTER_CONFIGS:            return new IncrementalAlterConfigsResponse(struct, version);        case ALTER_PARTITION_REASSIGNMENTS:            return new AlterPartitionReassignmentsResponse(struct, version);        case LIST_PARTITION_REASSIGNMENTS:            return new ListPartitionReassignmentsResponse(struct, version);        case OFFSET_DELETE:            return new OffsetDeleteResponse(struct, version);        default:            throw new AssertionError(String.format("ApiKey %s is not currently handled in `parseResponse`, the " + "code should be updated to do so.", apiKey));    }}
f3450
0
transactionalId
public String kafkatest_f3458_0()
{    return transactionalId;}
f3458
0
producerId
public long kafkatest_f3459_0()
{    return producerId;}
f3459
0
producerEpoch
public short kafkatest_f3460_0()
{    return producerEpoch;}
f3460
0
errorCounts
public Map<Errors, Integer> kafkatest_f3468_0()
{    return errorCounts(error);}
f3468
0
toStruct
protected Struct kafkatest_f3469_0(short version)
{    Struct struct = new Struct(ApiKeys.ADD_OFFSETS_TO_TXN.responseSchema(version));    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    struct.set(ERROR_CODE, error.code());    return struct;}
f3469
0
parse
public static AddOffsetsToTxnResponse kafkatest_f3470_0(ByteBuffer buffer, short version)
{    return new AddOffsetsToTxnResponse(ApiKeys.ADD_OFFSETS_TO_TXN.parseResponse(version, buffer));}
f3470
0
producerId
public long kafkatest_f3478_0()
{    return producerId;}
f3478
0
producerEpoch
public short kafkatest_f3479_0()
{    return producerEpoch;}
f3479
0
partitions
public List<TopicPartition> kafkatest_f3480_0()
{    return partitions;}
f3480
0
toStruct
protected Struct kafkatest_f3488_0(short version)
{    Struct struct = new Struct(ApiKeys.ADD_PARTITIONS_TO_TXN.responseSchema(version));    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    Map<String, Map<Integer, Errors>> errorsByTopic = CollectionUtils.groupPartitionDataByTopic(errors);    List<Struct> topics = new ArrayList<>(errorsByTopic.size());    for (Map.Entry<String, Map<Integer, Errors>> entry : errorsByTopic.entrySet()) {        Struct topicErrorCodes = struct.instance(ERRORS_KEY_NAME);        topicErrorCodes.set(TOPIC_NAME, entry.getKey());        List<Struct> partitionArray = new ArrayList<>();        for (Map.Entry<Integer, Errors> partitionErrors : entry.getValue().entrySet()) {            final Struct partitionData = topicErrorCodes.instance(PARTITION_ERRORS).set(PARTITION_ID, partitionErrors.getKey()).set(ERROR_CODE, partitionErrors.getValue().code());            partitionArray.add(partitionData);        }        topicErrorCodes.set(PARTITION_ERRORS, partitionArray.toArray());        topics.add(topicErrorCodes);    }    struct.set(ERRORS_KEY_NAME, topics.toArray());    return struct;}
f3488
0
parse
public static AddPartitionsToTxnResponse kafkatest_f3489_0(ByteBuffer buffer, short version)
{    return new AddPartitionsToTxnResponse(ApiKeys.ADD_PARTITIONS_TO_TXN.parseResponse(version, buffer));}
f3489
0
toString
public String kafkatest_f3490_0()
{    return "AddPartitionsToTxnResponse(" + "errors=" + errors + ", throttleTimeMs=" + throttleTimeMs + ')';}
f3490
0
validateOnly
public boolean kafkatest_f3498_0()
{    return validateOnly;}
f3498
0
toStruct
protected Struct kafkatest_f3499_0()
{    Struct struct = new Struct(ApiKeys.ALTER_CONFIGS.requestSchema(version()));    struct.set(VALIDATE_ONLY_KEY_NAME, validateOnly);    List<Struct> resourceStructs = new ArrayList<>(configs.size());    for (Map.Entry<ConfigResource, Config> entry : configs.entrySet()) {        Struct resourceStruct = struct.instance(RESOURCES_KEY_NAME);        ConfigResource resource = entry.getKey();        resourceStruct.set(RESOURCE_TYPE_KEY_NAME, resource.type().id());        resourceStruct.set(RESOURCE_NAME_KEY_NAME, resource.name());        Config config = entry.getValue();        List<Struct> configEntryStructs = new ArrayList<>(config.entries.size());        for (ConfigEntry configEntry : config.entries) {            Struct configEntriesStruct = resourceStruct.instance(CONFIG_ENTRIES_KEY_NAME);            configEntriesStruct.set(CONFIG_NAME, configEntry.name);            configEntriesStruct.set(CONFIG_VALUE, configEntry.value);            configEntryStructs.add(configEntriesStruct);        }        resourceStruct.set(CONFIG_ENTRIES_KEY_NAME, configEntryStructs.toArray(new Struct[0]));        resourceStructs.add(resourceStruct);    }    struct.set(RESOURCES_KEY_NAME, resourceStructs.toArray(new Struct[0]));    return struct;}
f3499
0
getErrorResponse
public AbstractResponse kafkatest_f3500_0(int throttleTimeMs, Throwable e)
{    short version = version();    switch(version) {        case 0:        case 1:            ApiError error = ApiError.fromThrowable(e);            Map<ConfigResource, ApiError> errors = new HashMap<>(configs.size());            for (ConfigResource resource : configs.keySet()) errors.put(resource, error);            return new AlterConfigsResponse(throttleTimeMs, errors);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", version, this.getClass().getSimpleName(), ApiKeys.ALTER_CONFIGS.latestVersion()));    }}
f3500
0
shouldClientThrottle
public boolean kafkatest_f3508_0(short version)
{    return version >= 1;}
f3508
0
build
public AlterPartitionReassignmentsRequest kafkatest_f3509_0(short version)
{    return new AlterPartitionReassignmentsRequest(data, version);}
f3509
0
toString
public String kafkatest_f3510_0()
{    return data.toString();}
f3510
0
throttleTimeMs
public int kafkatest_f3518_0()
{    return data.throttleTimeMs();}
f3518
0
errorCounts
public Map<Errors, Integer> kafkatest_f3519_0()
{    Map<Errors, Integer> counts = new HashMap<>();    Errors topLevelErr = Errors.forCode(data.errorCode());    counts.put(topLevelErr, counts.getOrDefault(topLevelErr, 0) + 1);    for (ReassignableTopicResponse topicResponse : data.responses()) {        for (ReassignablePartitionResponse partitionResponse : topicResponse.partitions()) {            Errors error = Errors.forCode(partitionResponse.errorCode());            counts.put(error, counts.getOrDefault(error, 0) + 1);        }    }    return counts;}
f3519
0
toStruct
protected Struct kafkatest_f3520_0(short version)
{    return data.toStruct(version);}
f3520
0
schemaVersions
public static Schema[] kafkatest_f3528_0()
{    return new Schema[] { ALTER_REPLICA_LOG_DIRS_RESPONSE_V0, ALTER_REPLICA_LOG_DIRS_RESPONSE_V1 };}
f3528
0
toStruct
protected Struct kafkatest_f3529_0(short version)
{    Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.responseSchema(version));    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    Map<String, Map<Integer, Errors>> responsesByTopic = CollectionUtils.groupPartitionDataByTopic(responses);    List<Struct> topicStructArray = new ArrayList<>();    for (Map.Entry<String, Map<Integer, Errors>> responsesByTopicEntry : responsesByTopic.entrySet()) {        Struct topicStruct = struct.instance(TOPICS_KEY_NAME);        topicStruct.set(TOPIC_NAME, responsesByTopicEntry.getKey());        List<Struct> partitionStructArray = new ArrayList<>();        for (Map.Entry<Integer, Errors> responsesByPartitionEntry : responsesByTopicEntry.getValue().entrySet()) {            Struct partitionStruct = topicStruct.instance(PARTITIONS_KEY_NAME);            Errors response = responsesByPartitionEntry.getValue();            partitionStruct.set(PARTITION_ID, responsesByPartitionEntry.getKey());            partitionStruct.set(ERROR_CODE, response.code());            partitionStructArray.add(partitionStruct);        }        topicStruct.set(PARTITIONS_KEY_NAME, partitionStructArray.toArray());        topicStructArray.add(topicStruct);    }    struct.set(TOPICS_KEY_NAME, topicStructArray.toArray());    return struct;}
f3529
0
throttleTimeMs
public int kafkatest_f3530_0()
{    return throttleTimeMs;}
f3530
0
isFailure
public boolean kafkatest_f3538_0()
{    return !isSuccess();}
f3538
0
isSuccess
public boolean kafkatest_f3539_0()
{    return is(Errors.NONE);}
f3539
0
error
public Errors kafkatest_f3540_0()
{    return error;}
f3540
0
hasUnsupportedRequestVersion
public boolean kafkatest_f3548_0()
{    return unsupportedRequestVersion != null;}
f3548
0
toStruct
protected Struct kafkatest_f3549_0()
{    return new Struct(ApiKeys.API_VERSIONS.requestSchema(version()));}
f3549
0
getErrorResponse
public ApiVersionsResponse kafkatest_f3550_0(int throttleTimeMs, Throwable e)
{    short version = version();    switch(version) {        case 0:            return new ApiVersionsResponse(Errors.forException(e), Collections.emptyList());        case 1:        case 2:            return new ApiVersionsResponse(throttleTimeMs, Errors.forException(e), Collections.emptyList());        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", version, this.getClass().getSimpleName(), ApiKeys.API_VERSIONS.latestVersion()));    }}
f3550
0
apiVersion
public ApiVersion kafkatest_f3558_0(short apiKey)
{    return apiKeyToApiVersion.get(apiKey);}
f3558
0
error
public Errors kafkatest_f3559_0()
{    return error;}
f3559
0
errorCounts
public Map<Errors, Integer> kafkatest_f3560_0()
{    return errorCounts(error);}
f3560
0
getErrorResponse
public AbstractResponse kafkatest_f3568_0(int throttleTimeMs, Throwable e)
{    ControlledShutdownResponseData response = new ControlledShutdownResponseData();    response.setErrorCode(Errors.forException(e).code());    short versionId = version();    switch(versionId) {        case 0:        case 1:        case 2:            return new ControlledShutdownResponse(response);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.CONTROLLED_SHUTDOWN.latestVersion()));    }}
f3568
0
parse
public static ControlledShutdownRequest kafkatest_f3569_0(ByteBuffer buffer, short version)
{    return new ControlledShutdownRequest(ApiKeys.CONTROLLED_SHUTDOWN.parseRequest(version, buffer), version);}
f3569
0
toStruct
protected Struct kafkatest_f3570_0()
{    return data.toStruct(version);}
f3570
0
schemaVersions
public static Schema[] kafkatest_f3578_0()
{    return new Schema[] { CREATE_ACLS_REQUEST_V0, CREATE_ACLS_REQUEST_V1 };}
f3578
0
fromStruct
 static AclCreation kafkatest_f3579_0(Struct struct)
{    ResourcePattern pattern = RequestUtils.resourcePatternromStructFields(struct);    AccessControlEntry entry = RequestUtils.aceFromStructFields(struct);    return new AclCreation(new AclBinding(pattern, entry));}
f3579
0
acl
public AclBinding kafkatest_f3580_0()
{    return acl;}
f3580
0
parse
public static CreateAclsRequest kafkatest_f3588_0(ByteBuffer buffer, short version)
{    return new CreateAclsRequest(ApiKeys.CREATE_ACLS.parseRequest(version, buffer), version);}
f3588
0
validate
private void kafkatest_f3589_0(List<AclCreation> aclCreations)
{    if (version() == 0) {        final boolean unsupported = aclCreations.stream().map(AclCreation::acl).map(AclBinding::pattern).map(ResourcePattern::patternType).anyMatch(patternType -> patternType != PatternType.LITERAL);        if (unsupported) {            throw new UnsupportedVersionException("Version 0 only supports literal resource pattern types");        }    }    final boolean unknown = aclCreations.stream().map(AclCreation::acl).anyMatch(AclBinding::isUnknown);    if (unknown) {        throw new IllegalArgumentException("You can not create ACL bindings with unknown elements");    }}
f3589
0
schemaVersions
public static Schema[] kafkatest_f3590_0()
{    return new Schema[] { CREATE_ACLS_RESPONSE_V0, CREATE_ACLS_RESPONSE_V1 };}
f3590
0
shouldClientThrottle
public boolean kafkatest_f3598_0(short version)
{    return version >= 1;}
f3598
0
parse
public static CreateDelegationTokenRequest kafkatest_f3599_0(ByteBuffer buffer, short version)
{    return new CreateDelegationTokenRequest(ApiKeys.CREATE_DELEGATION_TOKEN.parseRequest(version, buffer), version);}
f3599
0
toStruct
protected Struct kafkatest_f3600_0()
{    return data.toStruct(version());}
f3600
0
data
public CreateDelegationTokenResponseData kafkatest_f3608_0()
{    return data;}
f3608
0
errorCounts
public Map<Errors, Integer> kafkatest_f3609_0()
{    return Collections.singletonMap(error(), 1);}
f3609
0
toStruct
protected Struct kafkatest_f3610_0(short version)
{    return data.toStruct(version);}
f3610
0
toString
public String kafkatest_f3618_0()
{    return "(totalCount=" + totalCount() + ", newAssignments=" + newAssignments() + ")";}
f3618
0
build
public CreatePartitionsRequest kafkatest_f3619_0(short version)
{    return new CreatePartitionsRequest(newPartitions, timeout, validateOnly, version);}
f3619
0
toString
public String kafkatest_f3620_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=CreatePartitionsRequest").append(", newPartitions=").append(newPartitions).append(", timeout=").append(timeout).append(", validateOnly=").append(validateOnly).append(")");    return bld.toString();}
f3620
0
schemaVersions
public static Schema[] kafkatest_f3628_0()
{    return new Schema[] { CREATE_PARTITIONS_RESPONSE_V0, CREATE_PARTITIONS_RESPONSE_V1 };}
f3628
0
toStruct
protected Struct kafkatest_f3629_0(short version)
{    Struct struct = new Struct(ApiKeys.CREATE_PARTITIONS.responseSchema(version));    List<Struct> topicErrors = new ArrayList<>(errors.size());    for (Map.Entry<String, ApiError> error : errors.entrySet()) {        Struct errorStruct = struct.instance(TOPIC_ERRORS_KEY_NAME);        errorStruct.set(TOPIC_NAME, error.getKey());        error.getValue().write(errorStruct);        topicErrors.add(errorStruct);    }    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    struct.set(TOPIC_ERRORS_KEY_NAME, topicErrors.toArray(new Object[topicErrors.size()]));    return struct;}
f3629
0
errors
public Map<String, ApiError> kafkatest_f3630_0()
{    return errors;}
f3630
0
getErrorResponse
public AbstractResponse kafkatest_f3638_0(int throttleTimeMs, Throwable e)
{    CreateTopicsResponseData response = new CreateTopicsResponseData();    if (version() >= 2) {        response.setThrottleTimeMs(throttleTimeMs);    }    ApiError apiError = ApiError.fromThrowable(e);    for (CreatableTopic topic : data.topics()) {        response.topics().add(new CreatableTopicResult().setName(topic.name()).setErrorCode(apiError.error().code()).setErrorMessage(apiError.message()));    }    return new CreateTopicsResponse(response);}
f3638
0
parse
public static CreateTopicsRequest kafkatest_f3639_0(ByteBuffer buffer, short version)
{    return new CreateTopicsRequest(ApiKeys.CREATE_TOPICS.parseRequest(version, buffer), version);}
f3639
0
toStruct
public Struct kafkatest_f3640_0()
{    return data.toStruct(version());}
f3640
0
build
public DeleteAclsRequest kafkatest_f3648_0(short version)
{    return new DeleteAclsRequest(version, filters);}
f3648
0
toString
public String kafkatest_f3649_0()
{    return "(type=DeleteAclsRequest, filters=" + Utils.join(filters, ", ") + ")";}
f3649
0
filters
public List<AclBindingFilter> kafkatest_f3650_0()
{    return filters;}
f3650
0
toString
public String kafkatest_f3658_0()
{    return "(error=" + error + ", acl=" + acl + ")";}
f3658
0
error
public ApiError kafkatest_f3659_0()
{    return error;}
f3659
0
deletions
public Collection<AclDeletionResult> kafkatest_f3660_0()
{    return deletions;}
f3660
0
shouldClientThrottle
public boolean kafkatest_f3668_0(short version)
{    return version >= 1;}
f3668
0
validate
private void kafkatest_f3669_0(short version)
{    if (version == 0) {        final boolean unsupported = responses.stream().flatMap(r -> r.deletions.stream()).map(AclDeletionResult::acl).map(AclBinding::pattern).map(ResourcePattern::patternType).anyMatch(patternType -> patternType != PatternType.LITERAL);        if (unsupported) {            throw new UnsupportedVersionException("Version 0 only supports literal resource pattern types");        }    }    final boolean unknown = responses.stream().flatMap(r -> r.deletions.stream()).map(AclDeletionResult::acl).anyMatch(AclBinding::isUnknown);    if (unknown) {        throw new IllegalArgumentException("Response contains UNKNOWN elements");    }}
f3669
0
build
public DeleteGroupsRequest kafkatest_f3670_0(short version)
{    return new DeleteGroupsRequest(data, version);}
f3670
0
errorCounts
public Map<Errors, Integer> kafkatest_f3678_0()
{    Map<Errors, Integer> counts = new HashMap<>();    for (DeletableGroupResult result : data.results()) {        Errors error = Errors.forCode(result.errorCode());        counts.put(error, counts.getOrDefault(error, 0) + 1);    }    return counts;}
f3678
0
parse
public static DeleteGroupsResponse kafkatest_f3679_0(ByteBuffer buffer, short version)
{    return new DeleteGroupsResponse(ApiKeys.DELETE_GROUPS.parseResponse(version, buffer));}
f3679
0
throttleTimeMs
public int kafkatest_f3680_0()
{    return data.throttleTimeMs();}
f3680
0
partitionOffsets
public Map<TopicPartition, Long> kafkatest_f3688_0()
{    return partitionOffsets;}
f3688
0
parse
public static DeleteRecordsRequest kafkatest_f3689_0(ByteBuffer buffer, short version)
{    return new DeleteRecordsRequest(ApiKeys.DELETE_RECORDS.parseRequest(version, buffer), version);}
f3689
0
schemaVersions
public static Schema[] kafkatest_f3690_0()
{    return new Schema[] { DELETE_RECORDS_RESPONSE_V0, DELETE_RECORDS_RESPONSE_V1 };}
f3690
0
build
public DeleteTopicsRequest kafkatest_f3698_0(short version)
{    return new DeleteTopicsRequest(data, version);}
f3698
0
toString
public String kafkatest_f3699_0()
{    return data.toString();}
f3699
0
toStruct
protected Struct kafkatest_f3700_0()
{    return data.toStruct(version);}
f3700
0
parse
public static DeleteTopicsResponse kafkatest_f3708_0(ByteBuffer buffer, short version)
{    return new DeleteTopicsResponse(ApiKeys.DELETE_TOPICS.parseResponse(version, buffer), version);}
f3708
0
shouldClientThrottle
public boolean kafkatest_f3709_0(short version)
{    return version >= 2;}
f3709
0
schemaVersions
public static Schema[] kafkatest_f3710_0()
{    return new Schema[] { DESCRIBE_ACLS_REQUEST_V0, DESCRIBE_ACLS_REQUEST_V1 };}
f3710
0
schemaVersions
public static Schema[] kafkatest_f3718_0()
{    return new Schema[] { DESCRIBE_ACLS_RESPONSE_V0, DESCRIBE_ACLS_RESPONSE_V1 };}
f3718
0
toStruct
protected Struct kafkatest_f3719_0(short version)
{    validate(version);    Struct struct = new Struct(ApiKeys.DESCRIBE_ACLS.responseSchema(version));    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    error.write(struct);    Map<ResourcePattern, List<AccessControlEntry>> resourceToData = new HashMap<>();    for (AclBinding acl : acls) {        resourceToData.computeIfAbsent(acl.pattern(), k -> new ArrayList<>()).add(acl.entry());    }    List<Struct> resourceStructs = new ArrayList<>();    for (Map.Entry<ResourcePattern, List<AccessControlEntry>> tuple : resourceToData.entrySet()) {        ResourcePattern resource = tuple.getKey();        Struct resourceStruct = struct.instance(RESOURCES_KEY_NAME);        RequestUtils.resourcePatternSetStructFields(resource, resourceStruct);        List<Struct> dataStructs = new ArrayList<>();        for (AccessControlEntry entry : tuple.getValue()) {            Struct dataStruct = resourceStruct.instance(ACLS_KEY_NAME);            RequestUtils.aceSetStructFields(entry, dataStruct);            dataStructs.add(dataStruct);        }        resourceStruct.set(ACLS_KEY_NAME, dataStructs.toArray());        resourceStructs.add(resourceStruct);    }    struct.set(RESOURCES_KEY_NAME, resourceStructs.toArray());    return struct;}
f3719
0
throttleTimeMs
public int kafkatest_f3720_0()
{    return throttleTimeMs;}
f3720
0
includeSynonyms
public Builder kafkatest_f3728_0(boolean includeSynonyms)
{    this.includeSynonyms = includeSynonyms;    return this;}
f3728
0
toResourceToConfigNames
private static Map<ConfigResource, Collection<String>> kafkatest_f3729_0(Collection<ConfigResource> resources)
{    Map<ConfigResource, Collection<String>> result = new HashMap<>(resources.size());    for (ConfigResource resource : resources) result.put(resource, null);    return result;}
f3729
0
build
public DescribeConfigsRequest kafkatest_f3730_0(short version)
{    return new DescribeConfigsRequest(version, resourceToConfigNames, includeSynonyms);}
f3730
0
error
public ApiError kafkatest_f3738_0()
{    return error;}
f3738
0
entries
public Collection<ConfigEntry> kafkatest_f3739_0()
{    return entries;}
f3739
0
name
public String kafkatest_f3740_0()
{    return name;}
f3740
0
value
public String kafkatest_f3748_0()
{    return value;}
f3748
0
source
public ConfigSource kafkatest_f3749_0()
{    return source;}
f3749
0
configs
public Map<ConfigResource, Config> kafkatest_f3750_0()
{    return configs;}
f3750
0
toString
public String kafkatest_f3758_0()
{    return data.toString();}
f3758
0
toStruct
protected Struct kafkatest_f3759_0()
{    return data.toStruct(version());}
f3759
0
data
public DescribeDelegationTokenRequestData kafkatest_f3760_0()
{    return data;}
f3760
0
tokens
public List<DelegationToken> kafkatest_f3768_0()
{    return data.tokens().stream().map(ddt -> new DelegationToken(new TokenInformation(ddt.tokenId(), new KafkaPrincipal(ddt.principalType(), ddt.principalName()), ddt.renewers().stream().map(ddtr -> new KafkaPrincipal(ddtr.principalType(), ddtr.principalName())).collect(Collectors.toList()), ddt.issueTimestamp(), ddt.maxTimestamp(), ddt.expiryTimestamp()), ddt.hmac())).collect(Collectors.toList());}
f3768
0
hasError
public boolean kafkatest_f3769_0()
{    return error() != Errors.NONE;}
f3769
0
shouldClientThrottle
public boolean kafkatest_f3770_0(short version)
{    return version >= 1;}
f3770
0
groupMetadata
public static DescribedGroup kafkatest_f3778_0(final String groupId, final Errors error, final String state, final String protocolType, final String protocol, final List<DescribedGroupMember> members, final Set<Byte> authorizedOperations)
{    DescribedGroup groupMetada = new DescribedGroup();    groupMetada.setGroupId(groupId).setErrorCode(error.code()).setGroupState(state).setProtocolType(protocolType).setProtocolData(protocol).setMembers(members).setAuthorizedOperations(Utils.to32BitField(authorizedOperations));    return groupMetada;}
f3778
0
groupMetadata
public static DescribedGroup kafkatest_f3779_0(final String groupId, final Errors error, final String state, final String protocolType, final String protocol, final List<DescribedGroupMember> members, final int authorizedOperations)
{    DescribedGroup groupMetada = new DescribedGroup();    groupMetada.setGroupId(groupId).setErrorCode(error.code()).setGroupState(state).setProtocolType(protocolType).setProtocolData(protocol).setMembers(members).setAuthorizedOperations(authorizedOperations);    return groupMetada;}
f3779
0
data
public DescribeGroupsResponseData kafkatest_f3780_0()
{    return data;}
f3780
0
schemaVersions
public static Schema[] kafkatest_f3788_0()
{    return new Schema[] { DESCRIBE_LOG_DIRS_REQUEST_V0, DESCRIBE_LOG_DIRS_REQUEST_V1 };}
f3788
0
build
public DescribeLogDirsRequest kafkatest_f3789_0(short version)
{    return new DescribeLogDirsRequest(topicPartitions, version);}
f3789
0
toString
public String kafkatest_f3790_0()
{    StringBuilder builder = new StringBuilder();    builder.append("(type=DescribeLogDirsRequest").append(", topicPartitions=").append(topicPartitions).append(")");    return builder.toString();}
f3790
0
throttleTimeMs
public int kafkatest_f3798_0()
{    return throttleTimeMs;}
f3798
0
errorCounts
public Map<Errors, Integer> kafkatest_f3799_0()
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (LogDirInfo logDirInfo : logDirInfos.values()) updateErrorCounts(errorCounts, logDirInfo.error);    return errorCounts;}
f3799
0
logDirInfos
public Map<String, LogDirInfo> kafkatest_f3800_0()
{    return logDirInfos;}
f3800
0
data
public ElectLeadersRequestData kafkatest_f3808_0()
{    return data;}
f3808
0
getErrorResponse
public AbstractResponse kafkatest_f3809_0(int throttleTimeMs, Throwable e)
{    ApiError apiError = ApiError.fromThrowable(e);    List<ReplicaElectionResult> electionResults = new ArrayList<>();    for (TopicPartitions topic : data.topicPartitions()) {        ReplicaElectionResult electionResult = new ReplicaElectionResult();        electionResult.setTopic(topic.topic());        for (Integer partitionId : topic.partitionId()) {            PartitionResult partitionResult = new PartitionResult();            partitionResult.setPartitionId(partitionId);            partitionResult.setErrorCode(apiError.error().code());            partitionResult.setErrorMessage(apiError.message());            electionResult.partitionResult().add(partitionResult);        }        electionResults.add(electionResult);    }    return new ElectLeadersResponse(throttleTimeMs, apiError.error().code(), electionResults, version());}
f3809
0
parse
public static ElectLeadersRequest kafkatest_f3810_0(ByteBuffer buffer, short version)
{    return new ElectLeadersRequest(ApiKeys.ELECT_LEADERS.parseRequest(version, buffer), version);}
f3810
0
shouldClientThrottle
public boolean kafkatest_f3818_0(short version)
{    return true;}
f3818
0
electLeadersResult
public static Map<TopicPartition, Optional<Throwable>> kafkatest_f3819_0(ElectLeadersResponseData data)
{    Map<TopicPartition, Optional<Throwable>> map = new HashMap<>();    for (ElectLeadersResponseData.ReplicaElectionResult topicResults : data.replicaElectionResults()) {        for (ElectLeadersResponseData.PartitionResult partitionResult : topicResults.partitionResult()) {            Optional<Throwable> value = Optional.empty();            Errors error = Errors.forCode(partitionResult.errorCode());            if (error != Errors.NONE) {                value = Optional.of(error.exception(partitionResult.errorMessage()));            }            map.put(new TopicPartition(topicResults.topic(), partitionResult.partitionId()), value);        }    }    return map;}
f3819
0
schemaVersions
public static Schema[] kafkatest_f3820_0()
{    return new Schema[] { END_TXN_REQUEST_V0, END_TXN_REQUEST_V1 };}
f3820
0
toStruct
protected Struct kafkatest_f3828_0()
{    Struct struct = new Struct(ApiKeys.END_TXN.requestSchema(version()));    struct.set(TRANSACTIONAL_ID, transactionalId);    struct.set(PRODUCER_ID, producerId);    struct.set(PRODUCER_EPOCH, producerEpoch);    struct.set(TRANSACTION_RESULT_KEY_NAME, result.id);    return struct;}
f3828
0
getErrorResponse
public EndTxnResponse kafkatest_f3829_0(int throttleTimeMs, Throwable e)
{    return new EndTxnResponse(throttleTimeMs, Errors.forException(e));}
f3829
0
parse
public static EndTxnRequest kafkatest_f3830_0(ByteBuffer buffer, short version)
{    return new EndTxnRequest(ApiKeys.END_TXN.parseRequest(version, buffer), version);}
f3830
0
shouldClientThrottle
public boolean kafkatest_f3838_0(short version)
{    return version >= 1;}
f3838
0
error
public Errors kafkatest_f3839_0()
{    return error;}
f3839
0
hasError
public boolean kafkatest_f3840_0()
{    return error != Errors.NONE;}
f3840
0
getErrorResponse
public AbstractResponse kafkatest_f3848_0(int throttleTimeMs, Throwable e)
{    return new ExpireDelegationTokenResponse(new ExpireDelegationTokenResponseData().setErrorCode(Errors.forException(e).code()).setThrottleTimeMs(throttleTimeMs));}
f3848
0
hmac
public ByteBuffer kafkatest_f3849_0()
{    return ByteBuffer.wrap(data.hmac());}
f3849
0
expiryTimePeriod
public long kafkatest_f3850_0()
{    return data.expiryTimePeriodMs();}
f3850
0
throttleTimeMs
public int kafkatest_f3858_0()
{    return data.throttleTimeMs();}
f3858
0
hasError
public boolean kafkatest_f3859_0()
{    return error() != Errors.NONE;}
f3859
0
shouldClientThrottle
public boolean kafkatest_f3860_0(short version)
{    return version >= 1;}
f3860
0
newIncremental
public static FetchMetadata kafkatest_f3868_0(int sessionId)
{    return new FetchMetadata(sessionId, nextEpoch(INITIAL_EPOCH));}
f3868
0
nextIncremental
public FetchMetadata kafkatest_f3869_0()
{    return new FetchMetadata(sessionId, nextEpoch(epoch));}
f3869
0
toString
public String kafkatest_f3870_0()
{    StringBuilder bld = new StringBuilder();    if (sessionId == INVALID_SESSION_ID) {        bld.append("(sessionId=INVALID, ");    } else {        bld.append("(sessionId=").append(sessionId).append(", ");    }    if (epoch == INITIAL_EPOCH) {        bld.append("epoch=INITIAL)");    } else if (epoch == FINAL_EPOCH) {        bld.append("epoch=FINAL)");    } else {        bld.append("epoch=").append(epoch).append(")");    }    return bld.toString();}
f3870
0
isolationLevel
public Builder kafkatest_f3878_0(IsolationLevel isolationLevel)
{    this.isolationLevel = isolationLevel;    return this;}
f3878
0
metadata
public Builder kafkatest_f3879_0(FetchMetadata metadata)
{    this.metadata = metadata;    return this;}
f3879
0
rackId
public Builder kafkatest_f3880_0(String rackId)
{    this.rackId = rackId;    return this;}
f3880
0
replicaId
public int kafkatest_f3888_0()
{    return replicaId;}
f3888
0
maxWait
public int kafkatest_f3889_0()
{    return maxWait;}
f3889
0
minBytes
public int kafkatest_f3890_0()
{    return minBytes;}
f3890
0
parse
public static FetchRequest kafkatest_f3898_0(ByteBuffer buffer, short version)
{    return new FetchRequest(ApiKeys.FETCH.parseRequest(version, buffer), version);}
f3898
0
toStruct
protected Struct kafkatest_f3899_0()
{    Struct struct = new Struct(ApiKeys.FETCH.requestSchema(version()));    List<TopicAndPartitionData<PartitionData>> topicsData = TopicAndPartitionData.batchByTopic(fetchData.entrySet().iterator());    struct.set(REPLICA_ID, replicaId);    struct.set(MAX_WAIT_TIME, maxWait);    struct.set(MIN_BYTES, minBytes);    struct.setIfExists(MAX_BYTES, maxBytes);    struct.setIfExists(ISOLATION_LEVEL, isolationLevel.id());    struct.setIfExists(SESSION_ID, metadata.sessionId());    struct.setIfExists(SESSION_EPOCH, metadata.epoch());    List<Struct> topicArray = new ArrayList<>();    for (TopicAndPartitionData<PartitionData> topicEntry : topicsData) {        Struct topicData = struct.instance(TOPICS);        topicData.set(TOPIC_NAME, topicEntry.topic);        List<Struct> partitionArray = new ArrayList<>();        for (Map.Entry<Integer, PartitionData> partitionEntry : topicEntry.partitions.entrySet()) {            PartitionData fetchPartitionData = partitionEntry.getValue();            Struct partitionData = topicData.instance(PARTITIONS);            partitionData.set(PARTITION_ID, partitionEntry.getKey());            partitionData.set(FETCH_OFFSET, fetchPartitionData.fetchOffset);            partitionData.set(PARTITION_MAX_BYTES, fetchPartitionData.maxBytes);            partitionData.setIfExists(LOG_START_OFFSET, fetchPartitionData.logStartOffset);            RequestUtils.setLeaderEpochIfExists(partitionData, CURRENT_LEADER_EPOCH, fetchPartitionData.currentLeaderEpoch);            partitionArray.add(partitionData);        }        topicData.set(PARTITIONS, partitionArray.toArray());        topicArray.add(topicData);    }    struct.set(TOPICS, topicArray.toArray());    if (struct.hasField(FORGOTTEN_TOPICS)) {        Map<String, List<Integer>> topicsToPartitions = new HashMap<>();        for (TopicPartition part : toForget) {            List<Integer> partitions = topicsToPartitions.computeIfAbsent(part.topic(), topic -> new ArrayList<>());            partitions.add(part.partition());        }        List<Struct> toForgetStructs = new ArrayList<>();        for (Map.Entry<String, List<Integer>> entry : topicsToPartitions.entrySet()) {            Struct toForgetStruct = struct.instance(FORGOTTEN_TOPICS);            toForgetStruct.set(TOPIC_NAME, entry.getKey());            toForgetStruct.set(FORGOTTEN_PARTITIONS, entry.getValue().toArray());            toForgetStructs.add(toForgetStruct);        }        struct.set(FORGOTTEN_TOPICS, toForgetStructs.toArray());    }    struct.setIfExists(RACK_ID, rackId);    return struct;}
f3899
0
schemaVersions
public static Schema[] kafkatest_f3900_0()
{    return new Schema[] { FETCH_RESPONSE_V0, FETCH_RESPONSE_V1, FETCH_RESPONSE_V2, FETCH_RESPONSE_V3, FETCH_RESPONSE_V4, FETCH_RESPONSE_V5, FETCH_RESPONSE_V6, FETCH_RESPONSE_V7, FETCH_RESPONSE_V8, FETCH_RESPONSE_V9, FETCH_RESPONSE_V10, FETCH_RESPONSE_V11 };}
f3900
0
toStruct
public Struct kafkatest_f3908_0(short version)
{    return toStruct(version, throttleTimeMs, error, responseData.entrySet().iterator(), sessionId);}
f3908
0
toSend
protected Send kafkatest_f3909_0(String dest, ResponseHeader responseHeader, short apiVersion)
{    Struct responseHeaderStruct = responseHeader.toStruct();    Struct responseBodyStruct = toStruct(apiVersion);    // write the total size and the response header    ByteBuffer buffer = ByteBuffer.allocate(responseHeaderStruct.sizeOf() + 4);    buffer.putInt(responseHeaderStruct.sizeOf() + responseBodyStruct.sizeOf());    responseHeaderStruct.writeTo(buffer);    buffer.rewind();    Queue<Send> sends = new ArrayDeque<>();    sends.add(new ByteBufferSend(dest, buffer));    addResponseData(responseBodyStruct, throttleTimeMs, dest, sends);    return new MultiRecordsSend(dest, sends);}
f3909
0
error
public Errors kafkatest_f3910_0()
{    return error;}
f3910
0
addPartitionData
private static void kafkatest_f3918_0(String dest, Queue<Send> sends, Struct partitionData)
{    Struct header = partitionData.getStruct(PARTITION_HEADER_KEY_NAME);    BaseRecords records = partitionData.getRecords(RECORD_SET_KEY_NAME);    // include the partition header and the size of the record set    ByteBuffer buffer = ByteBuffer.allocate(header.sizeOf() + 4);    header.writeTo(buffer);    buffer.putInt(records.sizeInBytes());    buffer.rewind();    sends.add(new ByteBufferSend(dest, buffer));    // finally the send for the record set itself    sends.add(records.toSend(dest));}
f3918
0
toStruct
private static Struct kafkatest_f3919_0(short version, int throttleTimeMs, Errors error, Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator, int sessionId)
{    Struct struct = new Struct(ApiKeys.FETCH.responseSchema(version));    struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);    struct.setIfExists(ERROR_CODE, error.code());    struct.setIfExists(SESSION_ID, sessionId);    List<FetchRequest.TopicAndPartitionData<PartitionData<T>>> topicsData = FetchRequest.TopicAndPartitionData.batchByTopic(partIterator);    List<Struct> topicArray = new ArrayList<>();    for (FetchRequest.TopicAndPartitionData<PartitionData<T>> topicEntry : topicsData) {        Struct topicData = struct.instance(RESPONSES_KEY_NAME);        topicData.set(TOPIC_NAME, topicEntry.topic);        List<Struct> partitionArray = new ArrayList<>();        for (Map.Entry<Integer, PartitionData<T>> partitionEntry : topicEntry.partitions.entrySet()) {            PartitionData<T> fetchPartitionData = partitionEntry.getValue();            short errorCode = fetchPartitionData.error.code();            // by converting the KafkaStorageException to NotLeaderForPartitionException in the response if FetchRequest version <= 5            if (errorCode == Errors.KAFKA_STORAGE_ERROR.code() && version <= 5)                errorCode = Errors.NOT_LEADER_FOR_PARTITION.code();            Struct partitionData = topicData.instance(PARTITIONS_KEY_NAME);            Struct partitionDataHeader = partitionData.instance(PARTITION_HEADER_KEY_NAME);            partitionDataHeader.set(PARTITION_ID, partitionEntry.getKey());            partitionDataHeader.set(ERROR_CODE, errorCode);            partitionDataHeader.set(HIGH_WATERMARK, fetchPartitionData.highWatermark);            if (partitionDataHeader.hasField(LAST_STABLE_OFFSET)) {                partitionDataHeader.set(LAST_STABLE_OFFSET, fetchPartitionData.lastStableOffset);                if (fetchPartitionData.abortedTransactions == null) {                    partitionDataHeader.set(ABORTED_TRANSACTIONS_KEY_NAME, null);                } else {                    List<Struct> abortedTransactionStructs = new ArrayList<>(fetchPartitionData.abortedTransactions.size());                    for (AbortedTransaction abortedTransaction : fetchPartitionData.abortedTransactions) {                        Struct abortedTransactionStruct = partitionDataHeader.instance(ABORTED_TRANSACTIONS_KEY_NAME);                        abortedTransactionStruct.set(PRODUCER_ID, abortedTransaction.producerId);                        abortedTransactionStruct.set(FIRST_OFFSET, abortedTransaction.firstOffset);                        abortedTransactionStructs.add(abortedTransactionStruct);                    }                    partitionDataHeader.set(ABORTED_TRANSACTIONS_KEY_NAME, abortedTransactionStructs.toArray());                }            }            partitionDataHeader.setIfExists(LOG_START_OFFSET, fetchPartitionData.logStartOffset);            partitionDataHeader.setIfExists(PREFERRED_READ_REPLICA, fetchPartitionData.preferredReadReplica.orElse(-1));            partitionData.set(PARTITION_HEADER_KEY_NAME, partitionDataHeader);            partitionData.set(RECORD_SET_KEY_NAME, fetchPartitionData.records);            partitionArray.add(partitionData);        }        topicData.set(PARTITIONS_KEY_NAME, partitionArray.toArray());        topicArray.add(topicData);    }    struct.set(RESPONSES_KEY_NAME, topicArray.toArray());    return struct;}
f3919
0
sizeOf
public static int kafkatest_f3920_0(short version, Iterator<Map.Entry<TopicPartition, PartitionData<T>>> partIterator)
{    // use arbitrary values here without affecting the result.    return 4 + toStruct(version, 0, Errors.NONE, partIterator, INVALID_SESSION_ID).sizeOf();}
f3920
0
data
public FindCoordinatorRequestData kafkatest_f3928_0()
{    return data;}
f3928
0
id
public byte kafkatest_f3929_0()
{    return id;}
f3929
0
forId
public static CoordinatorType kafkatest_f3930_0(byte id)
{    switch(id) {        case 0:            return GROUP;        case 1:            return TRANSACTION;        default:            throw new IllegalArgumentException("Unknown coordinator type received: " + id);    }}
f3930
0
parse
public static FindCoordinatorResponse kafkatest_f3938_0(ByteBuffer buffer, short version)
{    return new FindCoordinatorResponse(ApiKeys.FIND_COORDINATOR.responseSchema(version).read(buffer), version);}
f3938
0
toString
public String kafkatest_f3939_0()
{    return data.toString();}
f3939
0
shouldClientThrottle
public boolean kafkatest_f3940_0(short version)
{    return version >= 2;}
f3940
0
error
public Errors kafkatest_f3948_0()
{    return Errors.forCode(data.errorCode());}
f3948
0
errorCounts
public Map<Errors, Integer> kafkatest_f3949_0()
{    return Collections.singletonMap(error(), 1);}
f3949
0
toStruct
protected Struct kafkatest_f3950_0(short version)
{    return data.toStruct(version);}
f3950
0
getErrorResponse
public AbstractResponse kafkatest_f3958_0(final int throttleTimeMs, final Throwable e)
{    IncrementalAlterConfigsResponseData response = new IncrementalAlterConfigsResponseData();    ApiError apiError = ApiError.fromThrowable(e);    for (AlterConfigsResource resource : data.resources()) {        response.responses().add(new AlterConfigsResourceResponse().setResourceName(resource.resourceName()).setResourceType(resource.resourceType()).setErrorCode(apiError.error().code()).setErrorMessage(apiError.message()));    }    return new IncrementalAlterConfigsResponse(response);}
f3958
0
toResponseData
public static IncrementalAlterConfigsResponseData kafkatest_f3959_0(final int requestThrottleMs, final Map<ConfigResource, ApiError> results)
{    IncrementalAlterConfigsResponseData responseData = new IncrementalAlterConfigsResponseData();    responseData.setThrottleTimeMs(requestThrottleMs);    for (Map.Entry<ConfigResource, ApiError> entry : results.entrySet()) {        responseData.responses().add(new AlterConfigsResourceResponse().setResourceName(entry.getKey().name()).setResourceType(entry.getKey().type().id()).setErrorCode(entry.getValue().error().code()).setErrorMessage(entry.getValue().message()));    }    return responseData;}
f3959
0
fromResponseData
public static Map<ConfigResource, ApiError> kafkatest_f3960_0(final IncrementalAlterConfigsResponseData data)
{    Map<ConfigResource, ApiError> map = new HashMap<>();    for (AlterConfigsResourceResponse response : data.responses()) {        map.put(new ConfigResource(ConfigResource.Type.forId(response.resourceType()), response.resourceName()), new ApiError(Errors.forCode(response.errorCode()), response.errorMessage()));    }    return map;}
f3960
0
toString
public String kafkatest_f3968_0()
{    return data.toString();}
f3968
0
getErrorResponse
public AbstractResponse kafkatest_f3969_0(int throttleTimeMs, Throwable e)
{    InitProducerIdResponseData response = new InitProducerIdResponseData().setErrorCode(Errors.forException(e).code()).setProducerId(RecordBatch.NO_PRODUCER_ID).setProducerEpoch(RecordBatch.NO_PRODUCER_EPOCH).setThrottleTimeMs(0);    return new InitProducerIdResponse(response);}
f3969
0
parse
public static InitProducerIdRequest kafkatest_f3970_0(ByteBuffer buffer, short version)
{    return new InitProducerIdRequest(ApiKeys.INIT_PRODUCER_ID.parseRequest(version, buffer), version);}
f3970
0
shouldClientThrottle
public boolean kafkatest_f3978_0(short version)
{    return version >= 1;}
f3978
0
id
public byte kafkatest_f3979_0()
{    return id;}
f3979
0
forId
public static IsolationLevel kafkatest_f3980_0(byte id)
{    switch(id) {        case 0:            return READ_UNCOMMITTED;        case 1:            return READ_COMMITTED;        default:            throw new IllegalArgumentException("Unknown isolation level " + id);    }}
f3980
0
parse
public static JoinGroupRequest kafkatest_f3988_0(ByteBuffer buffer, short version)
{    return new JoinGroupRequest(ApiKeys.JOIN_GROUP.parseRequest(version, buffer), version);}
f3988
0
toStruct
protected Struct kafkatest_f3989_0()
{    return data.toStruct(version());}
f3989
0
data
public JoinGroupResponseData kafkatest_f3990_0()
{    return data;}
f3990
0
shouldClientThrottle
public boolean kafkatest_f3998_0(short version)
{    return version >= 3;}
f3998
0
schemaVersions
public static Schema[] kafkatest_f3999_0()
{    return new Schema[] { LEADER_AND_ISR_REQUEST_V0, LEADER_AND_ISR_REQUEST_V1, LEADER_AND_ISR_REQUEST_V2, LEADER_AND_ISR_REQUEST_V3 };}
f3999
0
build
public LeaderAndIsrRequest kafkatest_f4000_0(short version)
{    return new LeaderAndIsrRequest(controllerId, controllerEpoch, brokerEpoch, partitionStates, liveLeaders, version);}
f4000
0
parse
public static LeaderAndIsrRequest kafkatest_f4008_0(ByteBuffer buffer, short version)
{    return new LeaderAndIsrRequest(ApiKeys.LEADER_AND_ISR.parseRequest(version, buffer), version);}
f4008
0
toString
public String kafkatest_f4009_0()
{    return "PartitionState(controllerEpoch=" + basePartitionState.controllerEpoch + ", leader=" + basePartitionState.leader + ", leaderEpoch=" + basePartitionState.leaderEpoch + ", isr=" + Utils.join(basePartitionState.isr, ",") + ", zkVersion=" + basePartitionState.zkVersion + ", replicas=" + Utils.join(basePartitionState.replicas, ",") + ", addingReplicas=" + Utils.join(addingReplicas, ",") + ", removingReplicas=" + Utils.join(removingReplicas, ",") + ", isNew=" + isNew + ")";}
f4009
0
setStruct
private void kafkatest_f4010_0(Struct struct, short version)
{    struct.set(CONTROLLER_EPOCH, basePartitionState.controllerEpoch);    struct.set(LEADER, basePartitionState.leader);    struct.set(LEADER_EPOCH, basePartitionState.leaderEpoch);    struct.set(ISR, basePartitionState.isr.toArray());    struct.set(ZK_VERSION, basePartitionState.zkVersion);    struct.set(REPLICAS, basePartitionState.replicas.toArray());    if (version >= 3) {        struct.set(ADDING_REPLICAS, addingReplicas.toArray());        struct.set(REMOVING_REPLICAS, removingReplicas.toArray());    }    struct.setIfExists(IS_NEW, isNew);}
f4010
0
build
public LeaveGroupRequest kafkatest_f4018_0(short version)
{    final LeaveGroupRequestData data;    // Starting from version 3, all the leave group request will be in batch.    if (version >= 3) {        data = new LeaveGroupRequestData().setGroupId(groupId).setMembers(members);    } else {        if (members.size() != 1) {            throw new UnsupportedVersionException("Version " + version + " leave group request only " + "supports single member instance than " + members.size() + " members");        }        data = new LeaveGroupRequestData().setGroupId(groupId).setMemberId(members.get(0).memberId());    }    return new LeaveGroupRequest(data, version);}
f4018
0
toString
public String kafkatest_f4019_0()
{    return "(type=LeaveGroupRequest" + ", groupId=" + groupId + ", members=" + MessageUtil.deepToString(members.iterator()) + ")";}
f4019
0
data
public LeaveGroupRequestData kafkatest_f4020_0()
{    return data;}
f4020
0
topLevelError
public Errors kafkatest_f4028_0()
{    return Errors.forCode(data.errorCode());}
f4028
0
getError
private static Errors kafkatest_f4029_0(Errors topLevelError, List<MemberResponse> memberResponses)
{    if (topLevelError != Errors.NONE) {        return topLevelError;    } else {        for (MemberResponse memberResponse : memberResponses) {            Errors memberError = Errors.forCode(memberResponse.errorCode());            if (memberError != Errors.NONE) {                return memberError;            }        }        return Errors.NONE;    }}
f4029
0
errorCounts
public Map<Errors, Integer> kafkatest_f4030_0()
{    Map<Errors, Integer> combinedErrorCounts = new HashMap<>();    // Top level error.    Errors topLevelError = Errors.forCode(data.errorCode());    if (topLevelError != Errors.NONE) {        updateErrorCounts(combinedErrorCounts, topLevelError);    }    // Member level error.    for (MemberResponse memberResponse : data.members()) {        Errors memberError = Errors.forCode(memberResponse.errorCode());        if (memberError != Errors.NONE) {            updateErrorCounts(combinedErrorCounts, memberError);        }    }    return combinedErrorCounts;}
f4030
0
getErrorResponse
public ListGroupsResponse kafkatest_f4038_0(int throttleTimeMs, Throwable e)
{    short versionId = version();    switch(versionId) {        case 0:            return new ListGroupsResponse(new ListGroupsResponseData().setGroups(Collections.emptyList()).setErrorCode(Errors.forException(e).code()));        case 1:        case 2:            return new ListGroupsResponse(new ListGroupsResponseData().setGroups(Collections.emptyList()).setErrorCode(Errors.forException(e).code()).setThrottleTimeMs(throttleTimeMs));        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.LIST_GROUPS.latestVersion()));    }}
f4038
0
parse
public static ListGroupsRequest kafkatest_f4039_0(ByteBuffer buffer, short version)
{    return new ListGroupsRequest(ApiKeys.LIST_GROUPS.parseRequest(version, buffer), version);}
f4039
0
toStruct
protected Struct kafkatest_f4040_0()
{    return new Struct(ApiKeys.LIST_GROUPS.requestSchema(version()));}
f4040
0
forReplica
public static Builder kafkatest_f4048_0(short allowedVersion, int replicaId)
{    return new Builder((short) 0, allowedVersion, replicaId, IsolationLevel.READ_UNCOMMITTED);}
f4048
0
forConsumer
public static Builder kafkatest_f4049_0(boolean requireTimestamp, IsolationLevel isolationLevel)
{    short minVersion = 0;    if (isolationLevel == IsolationLevel.READ_COMMITTED)        minVersion = 2;    else if (requireTimestamp)        minVersion = 1;    return new Builder(minVersion, ApiKeys.LIST_OFFSETS.latestVersion(), CONSUMER_REPLICA_ID, isolationLevel);}
f4049
0
setTargetTimes
public Builder kafkatest_f4050_0(Map<TopicPartition, PartitionData> partitionTimestamps)
{    this.partitionTimestamps = partitionTimestamps;    return this;}
f4050
0
duplicatePartitions
public Set<TopicPartition> kafkatest_f4058_0()
{    return duplicatePartitions;}
f4058
0
parse
public static ListOffsetRequest kafkatest_f4059_0(ByteBuffer buffer, short version)
{    return new ListOffsetRequest(ApiKeys.LIST_OFFSETS.parseRequest(version, buffer), version);}
f4059
0
toStruct
protected Struct kafkatest_f4060_0()
{    short version = version();    Struct struct = new Struct(ApiKeys.LIST_OFFSETS.requestSchema(version));    Map<String, Map<Integer, PartitionData>> topicsData = CollectionUtils.groupPartitionDataByTopic(partitionTimestamps);    struct.set(REPLICA_ID, replicaId);    struct.setIfExists(ISOLATION_LEVEL, isolationLevel.id());    List<Struct> topicArray = new ArrayList<>();    for (Map.Entry<String, Map<Integer, PartitionData>> topicEntry : topicsData.entrySet()) {        Struct topicData = struct.instance(TOPICS);        topicData.set(TOPIC_NAME, topicEntry.getKey());        List<Struct> partitionArray = new ArrayList<>();        for (Map.Entry<Integer, PartitionData> partitionEntry : topicEntry.getValue().entrySet()) {            PartitionData offsetPartitionData = partitionEntry.getValue();            Struct partitionData = topicData.instance(PARTITIONS);            partitionData.set(PARTITION_ID, partitionEntry.getKey());            partitionData.set(TIMESTAMP, offsetPartitionData.timestamp);            partitionData.setIfExists(MAX_NUM_OFFSETS, offsetPartitionData.maxNumOffsets);            RequestUtils.setLeaderEpochIfExists(partitionData, CURRENT_LEADER_EPOCH, offsetPartitionData.currentLeaderEpoch);            partitionArray.add(partitionData);        }        topicData.set(PARTITIONS, partitionArray.toArray());        topicArray.add(topicData);    }    struct.set(TOPICS, topicArray.toArray());    return struct;}
f4060
0
toString
public String kafkatest_f4068_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=ListOffsetResponse").append(", throttleTimeMs=").append(throttleTimeMs).append(", responseData=").append(responseData).append(")");    return bld.toString();}
f4068
0
shouldClientThrottle
public boolean kafkatest_f4069_0(short version)
{    return version >= 3;}
f4069
0
build
public ListPartitionReassignmentsRequest kafkatest_f4070_0(short version)
{    return new ListPartitionReassignmentsRequest(data, version);}
f4070
0
shouldClientThrottle
public boolean kafkatest_f4078_0(short version)
{    return true;}
f4078
0
throttleTimeMs
public int kafkatest_f4079_0()
{    return data.throttleTimeMs();}
f4079
0
errorCounts
public Map<Errors, Integer> kafkatest_f4080_0()
{    Map<Errors, Integer> counts = new HashMap<>();    Errors topLevelErr = Errors.forCode(data.errorCode());    counts.put(topLevelErr, 1);    return counts;}
f4080
0
data
public MetadataRequestData kafkatest_f4088_0()
{    return data;}
f4088
0
getErrorResponse
public AbstractResponse kafkatest_f4089_0(int throttleTimeMs, Throwable e)
{    Errors error = Errors.forException(e);    MetadataResponseData responseData = new MetadataResponseData();    if (topics() != null) {        for (String topic : topics()) responseData.topics().add(new MetadataResponseData.MetadataResponseTopic().setName(topic).setErrorCode(error.code()).setIsInternal(false).setPartitions(Collections.emptyList()));    }    short versionId = version();    switch(versionId) {        case 0:        case 1:        case 2:            return new MetadataResponse(responseData);        case 3:        case 4:        case 5:        case 6:        case 7:        case 8:            responseData.setThrottleTimeMs(throttleTimeMs);            return new MetadataResponse(responseData);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.METADATA.latestVersion()));    }}
f4089
0
isAllTopics
public boolean kafkatest_f4090_0()
{    return (data.topics() == null) || // In version 0, an empty topic list indicates    (data.topics().isEmpty() && version == 0);// "request metadata for all topics."}
f4090
0
errors
public Map<String, Errors> kafkatest_f4098_0()
{    Map<String, Errors> errors = new HashMap<>();    for (MetadataResponseTopic metadata : data.topics()) {        if (metadata.errorCode() != Errors.NONE.code())            errors.put(metadata.name(), Errors.forCode(metadata.errorCode()));    }    return errors;}
f4098
0
errorCounts
public Map<Errors, Integer> kafkatest_f4099_0()
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (MetadataResponseTopic metadata : data.topics()) updateErrorCounts(errorCounts, Errors.forCode(metadata.errorCode()));    return errorCounts;}
f4099
0
topicsByError
public Set<String> kafkatest_f4100_0(Errors error)
{    Set<String> errorTopics = new HashSet<>();    for (MetadataResponseTopic metadata : data.topics()) {        if (metadata.errorCode() == error.code())            errorTopics.add(metadata.name());    }    return errorTopics;}
f4100
0
controller
public Node kafkatest_f4108_0()
{    return holder().controller;}
f4108
0
clusterId
public String kafkatest_f4109_0()
{    return this.data.clusterId();}
f4109
0
parse
public static MetadataResponse kafkatest_f4110_0(ByteBuffer buffer, short version)
{    return new MetadataResponse(ApiKeys.METADATA.responseSchema(version).read(buffer), version);}
f4110
0
hashCode
public int kafkatest_f4118_0()
{    return Objects.hash(error, topic, isInternal, partitionMetadata, authorizedOperations);}
f4118
0
toString
public String kafkatest_f4119_0()
{    return "TopicMetadata{" + "error=" + error + ", topic='" + topic + '\'' + ", isInternal=" + isInternal + ", partitionMetadata=" + partitionMetadata + ", authorizedOperations=" + authorizedOperations + '}';}
f4119
0
error
public Errors kafkatest_f4120_0()
{    return error;}
f4120
0
toString
public String kafkatest_f4128_0()
{    return "(type=PartitionMetadata" + ", error=" + error + ", partition=" + partition + ", leader=" + leader + ", leaderEpoch=" + leaderEpoch + ", replicas=" + Utils.join(replicas, ",") + ", isr=" + Utils.join(isr, ",") + ", offlineReplicas=" + Utils.join(offlineReplicas, ",") + ')';}
f4128
0
createBrokers
private Collection<Node> kafkatest_f4129_0(MetadataResponseData data)
{    return data.brokers().valuesList().stream().map(b -> new Node(b.nodeId(), b.host(), b.port(), b.rack())).collect(Collectors.toList());}
f4129
0
createTopicMetadata
private Collection<TopicMetadata> kafkatest_f4130_0(MetadataResponseData data, Map<Integer, Node> brokerMap)
{    List<TopicMetadata> topicMetadataList = new ArrayList<>();    for (MetadataResponseTopic topicMetadata : data.topics()) {        Errors topicError = Errors.forCode(topicMetadata.errorCode());        String topic = topicMetadata.name();        boolean isInternal = topicMetadata.isInternal();        List<PartitionMetadata> partitionMetadataList = new ArrayList<>();        for (MetadataResponsePartition partitionMetadata : topicMetadata.partitions()) {            Errors partitionError = Errors.forCode(partitionMetadata.errorCode());            int partitionIndex = partitionMetadata.partitionIndex();            int leader = partitionMetadata.leaderId();            Optional<Integer> leaderEpoch = RequestUtils.getLeaderEpoch(partitionMetadata.leaderEpoch());            Node leaderNode = leader == -1 ? null : brokerMap.get(leader);            List<Node> replicaNodes = convertToNodes(brokerMap, partitionMetadata.replicaNodes());            List<Node> isrNodes = convertToNodes(brokerMap, partitionMetadata.isrNodes());            List<Node> offlineNodes = convertToNodes(brokerMap, partitionMetadata.offlineReplicas());            partitionMetadataList.add(new PartitionMetadata(partitionError, partitionIndex, leaderNode, leaderEpoch, replicaNodes, isrNodes, offlineNodes));        }        topicMetadataList.add(new TopicMetadata(topicError, topic, isInternal, partitionMetadataList, topicMetadata.topicAuthorizedOperations()));    }    return topicMetadataList;}
f4130
0
data
public OffsetCommitRequestData kafkatest_f4138_0()
{    return data;}
f4138
0
offsets
public Map<TopicPartition, Long> kafkatest_f4139_0()
{    Map<TopicPartition, Long> offsets = new HashMap<>();    for (OffsetCommitRequestTopic topic : data.topics()) {        for (OffsetCommitRequestData.OffsetCommitRequestPartition partition : topic.partitions()) {            offsets.put(new TopicPartition(topic.name(), partition.partitionIndex()), partition.committedOffset());        }    }    return offsets;}
f4139
0
getErrorResponseTopics
public static List<OffsetCommitResponseTopic> kafkatest_f4140_0(List<OffsetCommitRequestTopic> requestTopics, Errors e)
{    List<OffsetCommitResponseTopic> responseTopicData = new ArrayList<>();    for (OffsetCommitRequestTopic entry : requestTopics) {        List<OffsetCommitResponsePartition> responsePartitions = new ArrayList<>();        for (OffsetCommitRequestData.OffsetCommitRequestPartition requestPartition : entry.partitions()) {            responsePartitions.add(new OffsetCommitResponsePartition().setPartitionIndex(requestPartition.partitionIndex()).setErrorCode(e.code()));        }        responseTopicData.add(new OffsetCommitResponseTopic().setName(entry.name()).setPartitions(responsePartitions));    }    return responseTopicData;}
f4140
0
toString
public String kafkatest_f4148_0()
{    return data.toString();}
f4148
0
throttleTimeMs
public int kafkatest_f4149_0()
{    return data.throttleTimeMs();}
f4149
0
shouldClientThrottle
public boolean kafkatest_f4150_0(short version)
{    return version >= 4;}
f4150
0
errorCounts
public Map<Errors, Integer> kafkatest_f4158_0()
{    Map<Errors, Integer> counts = new HashMap<>();    counts.put(Errors.forCode(data.errorCode()), 1);    for (OffsetDeleteResponseTopic topic : data.topics()) {        for (OffsetDeleteResponsePartition partition : topic.partitions()) {            Errors error = Errors.forCode(partition.errorCode());            counts.put(error, counts.getOrDefault(error, 0) + 1);        }    }    return counts;}
f4158
0
parse
public static OffsetDeleteResponse kafkatest_f4159_0(ByteBuffer buffer, short version)
{    return new OffsetDeleteResponse(ApiKeys.OFFSET_DELETE.parseResponse(version, buffer));}
f4159
0
throttleTimeMs
public int kafkatest_f4160_0()
{    return data.throttleTimeMs();}
f4160
0
getErrorResponse
public OffsetFetchResponse kafkatest_f4168_0(Errors error)
{    return getErrorResponse(AbstractResponse.DEFAULT_THROTTLE_TIME, error);}
f4168
0
getErrorResponse
public OffsetFetchResponse kafkatest_f4169_0(int throttleTimeMs, Errors error)
{    short versionId = version();    Map<TopicPartition, OffsetFetchResponse.PartitionData> responsePartitions = new HashMap<>();    if (versionId < 2) {        OffsetFetchResponse.PartitionData partitionError = new OffsetFetchResponse.PartitionData(OffsetFetchResponse.INVALID_OFFSET, Optional.empty(), OffsetFetchResponse.NO_METADATA, error);        for (OffsetFetchRequestTopic topic : this.data.topics()) {            for (int partitionIndex : topic.partitionIndexes()) {                responsePartitions.put(new TopicPartition(topic.name(), partitionIndex), partitionError);            }        }    }    switch(versionId) {        case 0:        case 1:        case 2:            return new OffsetFetchResponse(error, responsePartitions);        case 3:        case 4:        case 5:            return new OffsetFetchResponse(throttleTimeMs, error, responsePartitions);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.OFFSET_FETCH.latestVersion()));    }}
f4169
0
getErrorResponse
public OffsetFetchResponse kafkatest_f4170_0(int throttleTimeMs, Throwable e)
{    return getErrorResponse(throttleTimeMs, Errors.forException(e));}
f4170
0
throttleTimeMs
public int kafkatest_f4178_0()
{    return data.throttleTimeMs();}
f4178
0
hasError
public boolean kafkatest_f4179_0()
{    return error != Errors.NONE;}
f4179
0
error
public Errors kafkatest_f4180_0()
{    return error;}
f4180
0
replicaId
public int kafkatest_f4188_0()
{    return replicaId;}
f4188
0
forConsumer
public static Builder kafkatest_f4189_0(Map<TopicPartition, PartitionData> epochsByPartition)
{    // compatibility, we only send this request when we can guarantee the relaxed permissions.    return new Builder((short) 3, ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion(), epochsByPartition, CONSUMER_REPLICA_ID);}
f4189
0
forFollower
public static Builder kafkatest_f4190_0(short version, Map<TopicPartition, PartitionData> epochsByPartition, int replicaId)
{    return new Builder(version, version, epochsByPartition, replicaId);}
f4190
0
supportsTopicPermission
public static boolean kafkatest_f4198_0(short latestUsableVersion)
{    return latestUsableVersion >= 3;}
f4198
0
schemaVersions
public static Schema[] kafkatest_f4199_0()
{    return new Schema[] { OFFSET_FOR_LEADER_EPOCH_RESPONSE_V0, OFFSET_FOR_LEADER_EPOCH_RESPONSE_V1, OFFSET_FOR_LEADER_EPOCH_RESPONSE_V2, OFFSET_FOR_LEADER_EPOCH_RESPONSE_V3 };}
f4199
0
responses
public Map<TopicPartition, EpochEndOffset> kafkatest_f4200_0()
{    return epochEndOffsetsByPartition;}
f4200
0
forMagic
public static Builder kafkatest_f4208_0(byte magic, short acks, int timeout, Map<TopicPartition, MemoryRecords> partitionRecords, String transactionalId)
{    // Message format upgrades correspond with a bump in the produce request version. Older    // message format versions are generally not supported by the produce request versions    // following the bump.    final short minVersion;    final short maxVersion;    if (magic < RecordBatch.MAGIC_VALUE_V2) {        minVersion = 2;        maxVersion = 2;    } else {        minVersion = 3;        maxVersion = ApiKeys.PRODUCE.latestVersion();    }    return new Builder(minVersion, maxVersion, acks, timeout, partitionRecords, transactionalId);}
f4208
0
build
public ProduceRequest kafkatest_f4209_0(short version)
{    return build(version, true);}
f4209
0
buildUnsafe
public ProduceRequest kafkatest_f4210_0(short version)
{    return build(version, false);}
f4210
0
errorCounts
public Map<Errors, Integer> kafkatest_f4218_0(Throwable e)
{    Errors error = Errors.forException(e);    return Collections.singletonMap(error, partitions().size());}
f4218
0
partitions
private Collection<TopicPartition> kafkatest_f4219_0()
{    return partitionSizes.keySet();}
f4219
0
acks
public short kafkatest_f4220_0()
{    return acks;}
f4220
0
parse
public static ProduceRequest kafkatest_f4228_0(ByteBuffer buffer, short version)
{    return new ProduceRequest(ApiKeys.PRODUCE.parseRequest(version, buffer), version);}
f4228
0
requiredMagicForVersion
public static byte kafkatest_f4229_0(short produceRequestVersion)
{    switch(produceRequestVersion) {        case 0:        case 1:            return RecordBatch.MAGIC_VALUE_V0;        case 2:            return RecordBatch.MAGIC_VALUE_V1;        case 3:        case 4:        case 5:        case 6:        case 7:            return RecordBatch.MAGIC_VALUE_V2;        default:            // to update this method on a bump to the produce request version.            throw new IllegalArgumentException("Magic value to use for produce request version " + produceRequestVersion + " is not known");    }}
f4229
0
schemaVersions
public static Schema[] kafkatest_f4230_0()
{    return new Schema[] { PRODUCE_RESPONSE_V0, PRODUCE_RESPONSE_V1, PRODUCE_RESPONSE_V2, PRODUCE_RESPONSE_V3, PRODUCE_RESPONSE_V4, PRODUCE_RESPONSE_V5, PRODUCE_RESPONSE_V6, PRODUCE_RESPONSE_V7 };}
f4230
0
parse
public static RenewDelegationTokenRequest kafkatest_f4238_0(ByteBuffer buffer, short version)
{    return new RenewDelegationTokenRequest(ApiKeys.RENEW_DELEGATION_TOKEN.parseRequest(version, buffer), version);}
f4238
0
toStruct
protected Struct kafkatest_f4239_0()
{    return data.toStruct(version());}
f4239
0
data
public RenewDelegationTokenRequestData kafkatest_f4240_0()
{    return data;}
f4240
0
error
public Errors kafkatest_f4248_0()
{    return Errors.forCode(data.errorCode());}
f4248
0
expiryTimestamp
public long kafkatest_f4249_0()
{    return data.expiryTimestampMs();}
f4249
0
hasError
public boolean kafkatest_f4250_0()
{    return error() != Errors.NONE;}
f4250
0
principal
public KafkaPrincipal kafkatest_f4258_0()
{    return principal;}
f4258
0
clientAddress
public InetAddress kafkatest_f4259_0()
{    return clientAddress;}
f4259
0
requestType
public int kafkatest_f4260_0()
{    return header.apiKey().id;}
f4260
0
correlationId
public int kafkatest_f4268_0()
{    return correlationId;}
f4268
0
toResponseHeader
public ResponseHeader kafkatest_f4269_0()
{    return new ResponseHeader(correlationId);}
f4269
0
parse
public static RequestHeader kafkatest_f4270_0(ByteBuffer buffer)
{    try {        short apiKey = buffer.getShort();        short apiVersion = buffer.getShort();        Schema schema = schema(apiKey, apiVersion);        buffer.rewind();        return new RequestHeader(schema.read(buffer));    } catch (InvalidRequestException e) {        throw e;    } catch (Throwable ex) {        throw new InvalidRequestException("Error parsing request header. Our best guess of the apiKey is: " + buffer.getShort(0), ex);    }}
f4270
0
resourcePatternFilterSetStructFields
 static void kafkatest_f4278_0(ResourcePatternFilter patternFilter, Struct struct)
{    struct.set(RESOURCE_TYPE, patternFilter.resourceType().code());    struct.set(RESOURCE_NAME_FILTER, patternFilter.name());    struct.setIfExists(RESOURCE_PATTERN_TYPE_FILTER, patternFilter.patternType().code());}
f4278
0
aceFromStructFields
 static AccessControlEntry kafkatest_f4279_0(Struct struct)
{    String principal = struct.get(PRINCIPAL);    String host = struct.get(HOST);    byte operation = struct.get(OPERATION);    byte permissionType = struct.get(PERMISSION_TYPE);    return new AccessControlEntry(principal, host, AclOperation.fromCode(operation), AclPermissionType.fromCode(permissionType));}
f4279
0
aceSetStructFields
 static void kafkatest_f4280_0(AccessControlEntry data, Struct struct)
{    struct.set(PRINCIPAL, data.principal());    struct.set(HOST, data.host());    struct.set(OPERATION, data.operation().code());    struct.set(PERMISSION_TYPE, data.permissionType().code());}
f4280
0
correlationId
public int kafkatest_f4288_0()
{    return correlationId;}
f4288
0
parse
public static ResponseHeader kafkatest_f4289_0(ByteBuffer buffer)
{    return new ResponseHeader(SCHEMA.read(buffer));}
f4289
0
build
public SaslAuthenticateRequest kafkatest_f4290_0(short version)
{    return new SaslAuthenticateRequest(data, version);}
f4290
0
errorMessage
public String kafkatest_f4298_0()
{    return data.errorMessage();}
f4298
0
sessionLifetimeMs
public long kafkatest_f4299_0()
{    return data.sessionLifetimeMs();}
f4299
0
saslAuthBytes
public byte[] kafkatest_f4300_0()
{    return data.authBytes();}
f4300
0
toStruct
protected Struct kafkatest_f4308_0()
{    return data.toStruct(version);}
f4308
0
error
public Errors kafkatest_f4309_0()
{    return Errors.forCode(data.errorCode());}
f4309
0
errorCounts
public Map<Errors, Integer> kafkatest_f4310_0()
{    return Collections.singletonMap(Errors.forCode(data.errorCode()), 1);}
f4310
0
deletePartitions
public boolean kafkatest_f4318_0()
{    return deletePartitions;}
f4318
0
partitions
public Collection<TopicPartition> kafkatest_f4319_0()
{    return partitions;}
f4319
0
parse
public static StopReplicaRequest kafkatest_f4320_0(ByteBuffer buffer, short version)
{    return new StopReplicaRequest(ApiKeys.STOP_REPLICA.parseRequest(version, buffer), version);}
f4320
0
toString
public String kafkatest_f4328_0()
{    return "StopReplicaResponse(" + "responses=" + responses + ", error=" + error + ")";}
f4328
0
build
public SyncGroupRequest kafkatest_f4329_0(short version)
{    if (data.groupInstanceId() != null && version < 3) {        throw new UnsupportedVersionException("The broker sync group protocol version " + version + " does not support usage of config group.instance.id.");    }    return new SyncGroupRequest(data, version);}
f4329
0
toString
public String kafkatest_f4330_0()
{    return data.toString();}
f4330
0
toStruct
protected Struct kafkatest_f4338_0(short version)
{    return data.toStruct(version);}
f4338
0
parse
public static SyncGroupResponse kafkatest_f4339_0(ByteBuffer buffer, short version)
{    return new SyncGroupResponse(ApiKeys.SYNC_GROUP.parseResponse(version, buffer));}
f4339
0
shouldClientThrottle
public boolean kafkatest_f4340_0(short version)
{    return version >= 2;}
f4340
0
getErrorResponse
public TxnOffsetCommitResponse kafkatest_f4348_0(int throttleTimeMs, Throwable e)
{    List<TxnOffsetCommitResponseTopic> responseTopicData = getErrorResponseTopics(data.topics(), Errors.forException(e));    return new TxnOffsetCommitResponse(new TxnOffsetCommitResponseData().setThrottleTimeMs(throttleTimeMs).setTopics(responseTopicData));}
f4348
0
parse
public static TxnOffsetCommitRequest kafkatest_f4349_0(ByteBuffer buffer, short version)
{    return new TxnOffsetCommitRequest(ApiKeys.TXN_OFFSET_COMMIT.parseRequest(version, buffer), version);}
f4349
0
toString
public String kafkatest_f4350_0()
{    return "CommittedOffset(" + "offset=" + offset + ", leaderEpoch=" + leaderEpoch + ", metadata='" + metadata + "')";}
f4350
0
toString
public String kafkatest_f4358_0()
{    return data.toString();}
f4358
0
shouldClientThrottle
public boolean kafkatest_f4359_0(short version)
{    return version >= 1;}
f4359
0
schemaVersions
public static Schema[] kafkatest_f4360_0()
{    return new Schema[] { UPDATE_METADATA_REQUEST_V0, UPDATE_METADATA_REQUEST_V1, UPDATE_METADATA_REQUEST_V2, UPDATE_METADATA_REQUEST_V3, UPDATE_METADATA_REQUEST_V4, UPDATE_METADATA_REQUEST_V5 };}
f4360
0
getErrorResponse
public AbstractResponse kafkatest_f4368_0(int throttleTimeMs, Throwable e)
{    short versionId = version();    if (versionId <= 5)        return new UpdateMetadataResponse(Errors.forException(e));    else        throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.UPDATE_METADATA.latestVersion()));}
f4368
0
partitionStates
public Map<TopicPartition, PartitionState> kafkatest_f4369_0()
{    return partitionStates;}
f4369
0
liveBrokers
public Set<Broker> kafkatest_f4370_0()
{    return liveBrokers;}
f4370
0
producerId
public long kafkatest_f4378_0()
{    return producerId;}
f4378
0
producerEpoch
public short kafkatest_f4379_0()
{    return producerEpoch;}
f4379
0
coordinatorEpoch
public int kafkatest_f4380_0()
{    return coordinatorEpoch;}
f4380
0
toStruct
protected Struct kafkatest_f4388_0()
{    Struct struct = new Struct(ApiKeys.WRITE_TXN_MARKERS.requestSchema(version()));    Object[] markersArray = new Object[markers.size()];    int i = 0;    for (TxnMarkerEntry entry : markers) {        Struct markerStruct = struct.instance(TXN_MARKERS_KEY_NAME);        markerStruct.set(PRODUCER_ID_KEY_NAME, entry.producerId);        markerStruct.set(PRODUCER_EPOCH_KEY_NAME, entry.producerEpoch);        markerStruct.set(COORDINATOR_EPOCH_KEY_NAME, entry.coordinatorEpoch);        markerStruct.set(TRANSACTION_RESULT_KEY_NAME, entry.result.id);        Map<String, List<Integer>> mappedPartitions = CollectionUtils.groupPartitionsByTopic(entry.partitions);        Object[] partitionsArray = new Object[mappedPartitions.size()];        int j = 0;        for (Map.Entry<String, List<Integer>> topicAndPartitions : mappedPartitions.entrySet()) {            Struct topicPartitionsStruct = markerStruct.instance(TOPICS_KEY_NAME);            topicPartitionsStruct.set(TOPIC_NAME, topicAndPartitions.getKey());            topicPartitionsStruct.set(PARTITIONS_KEY_NAME, topicAndPartitions.getValue().toArray());            partitionsArray[j++] = topicPartitionsStruct;        }        markerStruct.set(TOPICS_KEY_NAME, partitionsArray);        markersArray[i++] = markerStruct;    }    struct.set(TXN_MARKERS_KEY_NAME, markersArray);    return struct;}
f4388
0
getErrorResponse
public WriteTxnMarkersResponse kafkatest_f4389_0(int throttleTimeMs, Throwable e)
{    Errors error = Errors.forException(e);    Map<Long, Map<TopicPartition, Errors>> errors = new HashMap<>(markers.size());    for (TxnMarkerEntry entry : markers) {        Map<TopicPartition, Errors> errorsPerPartition = new HashMap<>(entry.partitions.size());        for (TopicPartition partition : entry.partitions) errorsPerPartition.put(partition, error);        errors.put(entry.producerId, errorsPerPartition);    }    return new WriteTxnMarkersResponse(errors);}
f4389
0
parse
public static WriteTxnMarkersRequest kafkatest_f4390_0(ByteBuffer buffer, short version)
{    return new WriteTxnMarkersRequest(ApiKeys.WRITE_TXN_MARKERS.parseRequest(version, buffer), version);}
f4390
0
code
public byte kafkatest_f4398_0()
{    return code;}
f4398
0
isUnknown
public boolean kafkatest_f4399_0()
{    return this == UNKNOWN;}
f4399
0
isSpecific
public boolean kafkatest_f4400_0()
{    return this != UNKNOWN && this != ANY && this != MATCH;}
f4400
0
equals
public boolean kafkatest_f4408_0(Object o)
{    if (!(o instanceof Resource))        return false;    Resource other = (Resource) o;    return resourceType.equals(other.resourceType) && Objects.equals(name, other.name);}
f4408
0
hashCode
public int kafkatest_f4409_0()
{    return Objects.hash(resourceType, name);}
f4409
0
resourceType
public ResourceType kafkatest_f4410_0()
{    return resourceType;}
f4410
0
findIndefiniteField
public String kafkatest_f4418_0()
{    if (resourceType == ResourceType.ANY)        return "Resource type is ANY.";    if (resourceType == ResourceType.UNKNOWN)        return "Resource type is UNKNOWN.";    if (name == null)        return "Resource name is NULL.";    return null;}
f4418
0
resourceType
public ResourceType kafkatest_f4419_0()
{    return resourceType;}
f4419
0
name
public String kafkatest_f4420_0()
{    return name;}
f4420
0
resourceType
public ResourceType kafkatest_f4428_0()
{    return resourceType;}
f4428
0
name
public String kafkatest_f4429_0()
{    return name;}
f4429
0
patternType
public PatternType kafkatest_f4430_0()
{    return patternType;}
f4430
0
fromCode
public static ResourceType kafkatest_f4438_0(byte code)
{    ResourceType resourceType = CODE_TO_VALUE.get(code);    if (resourceType == null) {        return UNKNOWN;    }    return resourceType;}
f4438
0
code
public byte kafkatest_f4439_0()
{    return code;}
f4439
0
isUnknown
public boolean kafkatest_f4440_0()
{    return this == UNKNOWN;}
f4440
0
tokenAuthenticated
public void kafkatest_f4450_0(boolean tokenAuthenticated)
{    this.tokenAuthenticated = tokenAuthenticated;}
f4450
0
tokenAuthenticated
public boolean kafkatest_f4451_0()
{    return tokenAuthenticated;}
f4451
0
securityProtocol
public SecurityProtocol kafkatest_f4452_0()
{    return SecurityProtocol.PLAINTEXT;}
f4452
0
equals
public boolean kafkatest_f4460_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    return extensionsMap.equals(((SaslExtensions) o).extensionsMap);}
f4460
0
toString
public String kafkatest_f4461_0()
{    return extensionsMap.toString();}
f4461
0
hashCode
public int kafkatest_f4462_0()
{    return extensionsMap.hashCode();}
f4462
0
clientAddress
public InetAddress kafkatest_f4470_0()
{    return clientAddress;}
f4470
0
listenerName
public String kafkatest_f4471_0()
{    return listenerName;}
f4471
0
configure
public void kafkatest_f4472_0(Map<String, ?> configs, String contextName, Configuration configuration, AuthenticateCallbackHandler loginCallbackHandler)
{    this.contextName = contextName;    this.configuration = configuration;    this.loginCallbackHandler = loginCallbackHandler;}
f4472
0
get
public C kafkatest_f4482_0(String username)
{    return credentials.get(username);}
f4482
0
put
public C kafkatest_f4483_0(String username, C credential)
{    return credentials.put(username, credential);}
f4483
0
remove
public C kafkatest_f4484_0(String username)
{    return credentials.remove(username);}
f4484
0
serviceName
public String kafkatest_f4492_0()
{    return "kafka";}
f4492
0
acquireLoginManager
public static LoginManager kafkatest_f4494_0(JaasContext jaasContext, String saslMechanism, Class<? extends Login> defaultLoginClass, Map<String, ?> configs) throws LoginException
{    Class<? extends Login> loginClass = configuredClassOrDefault(configs, jaasContext, saslMechanism, SaslConfigs.SASL_LOGIN_CLASS, defaultLoginClass);    Class<? extends AuthenticateCallbackHandler> defaultLoginCallbackHandlerClass = OAuthBearerLoginModule.OAUTHBEARER_MECHANISM.equals(saslMechanism) ? OAuthBearerUnsecuredLoginCallbackHandler.class : AbstractLogin.DefaultLoginCallbackHandler.class;    Class<? extends AuthenticateCallbackHandler> loginCallbackClass = configuredClassOrDefault(configs, jaasContext, saslMechanism, SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS, defaultLoginCallbackHandlerClass);    synchronized (LoginManager.class) {        LoginManager loginManager;        Password jaasConfigValue = jaasContext.dynamicJaasConfig();        if (jaasConfigValue != null) {            LoginMetadata<Password> loginMetadata = new LoginMetadata<>(jaasConfigValue, loginClass, loginCallbackClass);            loginManager = DYNAMIC_INSTANCES.get(loginMetadata);            if (loginManager == null) {                loginManager = new LoginManager(jaasContext, saslMechanism, configs, loginMetadata);                DYNAMIC_INSTANCES.put(loginMetadata, loginManager);            }        } else {            LoginMetadata<String> loginMetadata = new LoginMetadata<>(jaasContext.name(), loginClass, loginCallbackClass);            loginManager = STATIC_INSTANCES.get(loginMetadata);            if (loginManager == null) {                loginManager = new LoginManager(jaasContext, saslMechanism, configs, loginMetadata);                STATIC_INSTANCES.put(loginMetadata, loginManager);            }        }        SecurityUtils.addConfiguredSecurityProviders(configs);        return loginManager.acquire();    }}
f4494
0
subject
public Subject kafkatest_f4495_0()
{    return login.subject();}
f4495
0
hashCode
public int kafkatest_f4503_0()
{    return Objects.hash(configInfo, loginClass, loginCallbackClass);}
f4503
0
equals
public boolean kafkatest_f4504_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    LoginMetadata<?> loginMetadata = (LoginMetadata<?>) o;    return Objects.equals(configInfo, loginMetadata.configInfo) && Objects.equals(loginClass, loginMetadata.loginClass) && Objects.equals(loginCallbackClass, loginMetadata.loginCallbackClass);}
f4504
0
createSaslClient
private SaslClientf4505_1)
{    try {        return Subject.doAs(subject, (PrivilegedExceptionAction<SaslClient>) () -> {            String[] mechs = { mechanism };            service={};serviceHostname={};mechs={}", clientPrincipalName, servicePrincipal, host, Arrays.toString(mechs));            return Sasl.createSaslClient(mechs, clientPrincipalName, servicePrincipal, host, configs, callbackHandler);        });    } catch (PrivilegedActionException e) {        throw new SaslAuthenticationException("Failed to create SaslClient with mechanism " + mechanism, e.getCause());    }}
private SaslClientf4505
1
nextRequestHeader
private RequestHeader kafkatest_f4513_0(ApiKeys apiKey, short version)
{    String clientId = (String) configs.get(CommonClientConfigs.CLIENT_ID_CONFIG);    currentRequestHeader = new RequestHeader(apiKey, version, clientId, correlationId++);    return currentRequestHeader;}
f4513
0
createSaslHandshakeRequest
protected SaslHandshakeRequest kafkatest_f4514_0(short version)
{    return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism(mechanism)).build(version);}
f4514
0
saslAuthenticateVersion
protected void kafkatest_f4515_0(ApiVersionsResponse apiVersionsResponse)
{    ApiVersion authenticateVersion = apiVersionsResponse.apiVersion(ApiKeys.SASL_AUTHENTICATE.id);    if (authenticateVersion != null)        this.saslAuthenticateVersion = (short) Math.min(authenticateVersion.maxVersion, ApiKeys.SASL_AUTHENTICATE.latestVersion());}
f4515
0
close
public void kafkatest_f4523_0() throws IOException
{    if (saslClient != null)        saslClient.dispose();}
f4523
0
receiveToken
private byte[] kafkatest_f4524_0() throws IOException
{    if (saslAuthenticateVersion == DISABLE_KAFKA_SASL_AUTHENTICATE_HEADER) {        return receiveResponseOrToken();    } else {        SaslAuthenticateResponse response = (SaslAuthenticateResponse) receiveKafkaResponse();        if (response != null) {            Errors error = response.error();            if (error != Errors.NONE) {                setSaslState(SaslState.FAILED);                String errMsg = response.errorMessage();                throw errMsg == null ? error.exception() : error.exception(errMsg);            }            long sessionLifetimeMs = response.sessionLifetimeMs();            if (sessionLifetimeMs > 0L)                reauthInfo.positiveSessionLifetimeMs = sessionLifetimeMs;            return Utils.copyArray(response.saslAuthBytes());        } else            return null;    }}
f4524
0
createSaslToken
private byte[] kafkatest_f4525_0(final byte[] saslToken, boolean isInitial) throws SaslException
{    if (saslToken == null)        throw new IllegalSaslStateException("Error authenticating with the Kafka Broker: received a `null` saslToken.");    try {        if (isInitial && !saslClient.hasInitialResponse())            return saslToken;        else            return Subject.doAs(subject, (PrivilegedExceptionAction<byte[]>) () -> saslClient.evaluateChallenge(saslToken));    } catch (PrivilegedActionException e) {        String error = "An error: (" + e + ") occurred when evaluating SASL token received from the Kafka Broker.";        KerberosError kerberosError = KerberosError.fromException(e);        // Try to provide hints to use about what went wrong so they can fix their configuration.        if (kerberosError == KerberosError.SERVER_NOT_FOUND) {            error += " This may be caused by Java's being unable to resolve the Kafka Broker's" + " hostname correctly. You may want to try to adding" + " '-Dsun.net.spi.nameservice.provider.1=dns,sun' to your client's JVMFLAGS environment." + " Users must configure FQDN of kafka brokers when authenticating using SASL and" + " `socketChannel.socket().getInetAddress().getHostName()` must match the hostname in `principal/hostname@realm`";        }        error += " Kafka Client will go to AUTHENTICATION_FAILED state.";        // Unwrap the SaslException inside `PrivilegedActionException`        Throwable cause = e.getCause();        // and all other failures as fatal SaslAuthenticationException.        if (kerberosError != null && kerberosError.retriable())            throw new SaslException(error, cause);        else            throw new SaslAuthenticationException(error, cause);    }}
f4525
0
getAndClearResponsesReceivedDuringReauthentication
public List<NetworkReceive> kafkatest_f4533_0()
{    if (pendingAuthenticatedReceives.isEmpty())        return Collections.emptyList();    List<NetworkReceive> retval = pendingAuthenticatedReceives;    pendingAuthenticatedReceives = new ArrayList<>();    return retval;}
f4533
0
setAuthenticationEndAndSessionReauthenticationTimes
public voidf4534_1long nowNanos)
{    authenticationEndNanos = nowNanos;    long sessionLifetimeMsToUse = 0;    if (positiveSessionLifetimeMs != null) {        // pick a random percentage between 85% and 95% for session re-authentication        double pctWindowFactorToTakeNetworkLatencyAndClockDriftIntoAccount = 0.85;        double pctWindowJitterToAvoidReauthenticationStormAcrossManyChannelsSimultaneously = 0.10;        double pctToUse = pctWindowFactorToTakeNetworkLatencyAndClockDriftIntoAccount + RNG.nextDouble() * pctWindowJitterToAvoidReauthenticationStormAcrossManyChannelsSimultaneously;        sessionLifetimeMsToUse = (long) (positiveSessionLifetimeMs.longValue() * pctToUse);        clientSessionReauthenticationTimeNanos = authenticationEndNanos + 1000 * 1000 * sessionLifetimeMsToUse;            } else        }
public voidf4534
1
reauthenticationLatencyMs
public Long kafkatest_f4535_0()
{    return reauthenticating() ? Long.valueOf(Math.round((authenticationEndNanos - reauthenticationBeginNanos) / 1000.0 / 1000.0)) : null;}
f4535
0
complete
public boolean kafkatest_f4544_0()
{    return saslState == SaslState.COMPLETE;}
f4544
0
handleAuthenticationFailure
public void kafkatest_f4545_0() throws IOException
{    sendAuthenticationFailureResponse();}
f4545
0
close
public void kafkatest_f4546_0() throws IOException
{    if (principalBuilder instanceof Closeable)        Utils.closeQuietly((Closeable) principalBuilder, "principal builder");    if (saslServer != null)        saslServer.dispose();}
f4546
0
flushNetOutBuffer
private boolean kafkatest_f4554_0() throws IOException
{    if (!netOutBuffer.completed())        netOutBuffer.writeTo(transportLayer);    return netOutBuffer.completed();}
f4554
0
serverAddress
private InetAddress kafkatest_f4555_0()
{    return transportLayer.socketChannel().socket().getLocalAddress();}
f4555
0
clientAddress
private InetAddress kafkatest_f4556_0()
{    return transportLayer.socketChannel().socket().getInetAddress();}
f4556
0
sendAuthenticationFailureResponse
private void kafkatest_f4564_0() throws IOException
{    if (authenticationFailureSend == null)        return;    sendKafkaResponse(authenticationFailureSend);    authenticationFailureSend = null;}
f4564
0
sendKafkaResponse
private void kafkatest_f4565_0(RequestContext context, AbstractResponse response) throws IOException
{    sendKafkaResponse(context.buildResponse(response));}
f4565
0
sendKafkaResponse
private void kafkatest_f4566_0(Send send) throws IOException
{    netOutBuffer = send;    flushNetOutBufferAndUpdateInterestOps();}
f4566
0
zeroIfNegative
private long kafkatest_f4574_0(long value)
{    return Math.max(0L, value);}
f4574
0
configure
public void kafkatest_f4575_0(Map<String, ?> configs, String mechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    this.mechanism = mechanism;}
f4575
0
handle
public void kafkatest_f4576_0(Callback[] callbacks) throws UnsupportedCallbackException
{    for (Callback callback : callbacks) {        if (callback instanceof RealmCallback)            handleRealmCallback((RealmCallback) callback);        else if (callback instanceof AuthorizeCallback && mechanism.equals(SaslConfigs.GSSAPI_MECHANISM))            handleAuthorizeCallback((AuthorizeCallback) callback);        else            throw new UnsupportedCallbackException(callback);    }}
f4576
0
load
 static JaasContext kafkatest_f4585_0(JaasContext.Type contextType, String listenerContextName, String globalContextName, Password dynamicJaasConfig)
{    if (dynamicJaasConfig != null) {        JaasConfig jaasConfig = new JaasConfig(globalContextName, dynamicJaasConfig.value());        AppConfigurationEntry[] contextModules = jaasConfig.getAppConfigurationEntry(globalContextName);        if (contextModules == null || contextModules.length == 0)            throw new IllegalArgumentException("JAAS config property does not contain any login modules");        else if (contextModules.length != 1)            throw new IllegalArgumentException("JAAS config property contains " + contextModules.length + " login modules, should be 1 module");        return new JaasContext(globalContextName, contextType, jaasConfig, dynamicJaasConfig);    } else        return defaultContext(contextType, listenerContextName, globalContextName);}
f4585
0
defaultContext
private static JaasContextf4586_1JaasContext.Type contextType, String listenerContextName, String globalContextName)
{    String jaasConfigFile = System.getProperty(JaasUtils.JAVA_LOGIN_CONFIG_PARAM);    if (jaasConfigFile == null) {        if (contextType == Type.CLIENT) {                    } else {                    }    }    Configuration jaasConfig = Configuration.getConfiguration();    AppConfigurationEntry[] configEntries = null;    String contextName = globalContextName;    if (listenerContextName != null) {        configEntries = jaasConfig.getAppConfigurationEntry(listenerContextName);        if (configEntries != null)            contextName = listenerContextName;    }    if (configEntries == null)        configEntries = jaasConfig.getAppConfigurationEntry(globalContextName);    if (configEntries == null) {        String listenerNameText = listenerContextName == null ? "" : " or '" + listenerContextName + "'";        String errorMessage = "Could not find a '" + globalContextName + "'" + listenerNameText + " entry in the JAAS " + "configuration. System property '" + JaasUtils.JAVA_LOGIN_CONFIG_PARAM + "' is " + (jaasConfigFile == null ? "not set" : jaasConfigFile);        throw new IllegalArgumentException(errorMessage);    }    return new JaasContext(contextName, contextType, jaasConfig, null);}
private static JaasContextf4586
1
name
public String kafkatest_f4587_0()
{    return name;}
f4587
0
configure
public void kafkatest_f4595_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    if (!saslMechanism.equals(SaslConfigs.GSSAPI_MECHANISM))        throw new IllegalStateException("Kerberos callback handler should only be used with GSSAPI");}
f4595
0
handle
public void kafkatest_f4596_0(Callback[] callbacks) throws UnsupportedCallbackException
{    for (Callback callback : callbacks) {        if (callback instanceof NameCallback) {            NameCallback nc = (NameCallback) callback;            nc.setName(nc.getDefaultName());        } else if (callback instanceof PasswordCallback) {            String errorMessage = "Could not login: the client is being asked for a password, but the Kafka" + " client code does not currently support obtaining a password from the user.";            errorMessage += " Make sure -Djava.security.auth.login.config property passed to JVM and" + " the client is configured to use a ticket cache (using" + " the JAAS configuration setting 'useTicketCache=true)'. Make sure you are using" + " FQDN of the Kafka broker you are trying to connect to.";            throw new UnsupportedCallbackException(callback, errorMessage);        } else if (callback instanceof RealmCallback) {            RealmCallback rc = (RealmCallback) callback;            rc.setText(rc.getDefaultText());        } else if (callback instanceof AuthorizeCallback) {            AuthorizeCallback ac = (AuthorizeCallback) callback;            String authId = ac.getAuthenticationID();            String authzId = ac.getAuthorizationID();            ac.setAuthorized(authId.equals(authzId));            if (ac.isAuthorized())                ac.setAuthorizedID(authzId);        } else {            throw new UnsupportedCallbackException(callback, "Unrecognized SASL ClientCallback");        }    }}
f4596
0
retriable
public boolean kafkatest_f4598_0()
{    return retriable;}
f4598
0
getServiceName
private static String kafkatest_f4606_0(Map<String, ?> configs, String contextName, Configuration configuration)
{    List<AppConfigurationEntry> configEntries = Arrays.asList(configuration.getAppConfigurationEntry(contextName));    String jaasServiceName = JaasContext.configEntryOption(configEntries, JaasUtils.SERVICE_NAME, null);    String configServiceName = (String) configs.get(SaslConfigs.SASL_KERBEROS_SERVICE_NAME);    if (jaasServiceName != null && configServiceName != null && !jaasServiceName.equals(configServiceName)) {        String message = String.format("Conflicting serviceName values found in JAAS and Kafka configs " + "value in JAAS file %s, value in Kafka config %s", jaasServiceName, configServiceName);        throw new IllegalArgumentException(message);    }    if (jaasServiceName != null)        return jaasServiceName;    if (configServiceName != null)        return configServiceName;    throw new IllegalArgumentException("No serviceName defined in either JAAS or Kafka config");}
f4606
0
getRefreshTime
private longf4607_1KerberosTicket tgt)
{    long start = tgt.getStartTime().getTime();    long expires = tgt.getEndTime().getTime();            long proposedRefresh = start + (long) ((expires - start) * (ticketRenewWindowFactor + (ticketRenewJitter * RNG.nextDouble())));    if (proposedRefresh > expires)        // proposedRefresh is too far in the future: it's after ticket expires: simply return now.        return currentWallTime();    else        return proposedRefresh;}
private longf4607
1
getTGT
private KerberosTicketf4608_1)
{    Set<KerberosTicket> tickets = subject.getPrivateCredentials(KerberosTicket.class);    for (KerberosTicket ticket : tickets) {        KerberosPrincipal server = ticket.getServer();        if (server.getName().equals("krbtgt/" + server.getRealm() + "@" + server.getRealm())) {                        return ticket;        }    }    return null;}
private KerberosTicketf4608
1
hostName
public String kafkatest_f4616_0()
{    return hostName;}
f4616
0
realm
public String kafkatest_f4617_0()
{    return realm;}
f4617
0
toString
public String kafkatest_f4618_0()
{    StringBuilder buf = new StringBuilder();    if (isDefault) {        buf.append("DEFAULT");    } else {        buf.append("RULE:[");        buf.append(numOfComponents);        buf.append(':');        buf.append(format);        buf.append(']');        if (match != null) {            buf.append('(');            buf.append(match);            buf.append(')');        }        if (fromPattern != null) {            buf.append("s/");            buf.append(fromPattern);            buf.append('/');            buf.append(toPattern);            buf.append('/');            if (repeat) {                buf.append('g');            }        }        if (toLowerCase) {            buf.append("/L");        }    }    return buf.toString();}
f4618
0
loginRefreshWindowFactor
public double kafkatest_f4626_0()
{    return loginRefreshWindowFactor;}
f4626
0
loginRefreshWindowJitter
public double kafkatest_f4627_0()
{    return loginRefreshWindowJitter;}
f4627
0
loginRefreshMinPeriodSeconds
public short kafkatest_f4628_0()
{    return loginRefreshMinPeriodSeconds;}
f4628
0
contextName
public String kafkatest_f4636_0()
{    return contextName;}
f4636
0
configuration
public Configuration kafkatest_f4637_0()
{    return configuration;}
f4637
0
callbackHandler
public AuthenticateCallbackHandler kafkatest_f4638_0()
{    return callbackHandler;}
f4638
0
isLogoutRequiredBeforeLoggingBackIn
private boolean kafkatest_f4646_0()
{    return !expiringCredentialRefreshConfig.loginRefreshReloginAllowedBeforeLogout();}
f4646
0
extensions
public SaslExtensions kafkatest_f4647_0()
{    return saslExtensions;}
f4647
0
toBytes
public byte[] kafkatest_f4648_0()
{    String authzid = authorizationId.isEmpty() ? "" : "a=" + authorizationId;    String extensions = extensionsMessage();    if (extensions.length() > 0)        extensions = SEPARATOR + extensions;    String message = String.format("n,%s,%sauth=Bearer %s%s%s%s", authzid, SEPARATOR, tokenValue, extensions, SEPARATOR, SEPARATOR);    return message.getBytes(StandardCharsets.UTF_8);}
f4648
0
startTimeMs
public Long kafkatest_f4656_0()
{    return token.startTimeMs();}
f4656
0
expireTimeMs
public long kafkatest_f4657_0()
{    return token.lifetimeMs();}
f4657
0
absoluteLastRefreshTimeMs
public Long kafkatest_f4658_0()
{    return null;}
f4658
0
evaluateChallenge
public byte[]f4666_1byte[] challenge) throws SaslException
{    try {        OAuthBearerTokenCallback callback = new OAuthBearerTokenCallback();        switch(state) {            case SEND_CLIENT_FIRST_MESSAGE:                if (challenge != null && challenge.length != 0)                    throw new SaslException("Expected empty challenge");                callbackHandler().handle(new Callback[] { callback });                SaslExtensions extensions = retrieveCustomExtensions();                setState(State.RECEIVE_SERVER_FIRST_MESSAGE);                return new OAuthBearerClientInitialResponse(callback.token().value(), extensions).toBytes();            case RECEIVE_SERVER_FIRST_MESSAGE:                if (challenge != null && challenge.length != 0) {                    String jsonErrorResponse = new String(challenge, StandardCharsets.UTF_8);                    if (log.isDebugEnabled())                                            setState(State.RECEIVE_SERVER_MESSAGE_AFTER_FAILURE);                    return new byte[] { BYTE_CONTROL_A };                }                callbackHandler().handle(new Callback[] { callback });                if (log.isDebugEnabled())                                    setState(State.COMPLETE);                return null;            default:                throw new IllegalSaslStateException("Unexpected challenge in Sasl client state " + state);        }    } catch (SaslException e) {        setState(State.FAILED);        throw e;    } catch (IOException | UnsupportedCallbackException e) {        setState(State.FAILED);        throw new SaslException(e.getMessage(), e);    }}
public byte[]f4666
1
isComplete
public boolean kafkatest_f4667_0()
{    return state == State.COMPLETE;}
f4667
0
unwrap
public byte[] kafkatest_f4668_0(byte[] incoming, int offset, int len)
{    if (!isComplete())        throw new IllegalStateException("Authentication exchange has not completed");    return Arrays.copyOfRange(incoming, offset, offset + len);}
f4668
0
configure
public void kafkatest_f4677_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    if (!OAuthBearerLoginModule.OAUTHBEARER_MECHANISM.equals(saslMechanism))        throw new IllegalArgumentException(String.format("Unexpected SASL mechanism: %s", saslMechanism));    configured = true;}
f4677
0
handle
public void kafkatest_f4678_0(Callback[] callbacks) throws IOException, UnsupportedCallbackException
{    if (!configured())        throw new IllegalStateException("Callback handler not configured");    for (Callback callback : callbacks) {        if (callback instanceof OAuthBearerTokenCallback)            handleCallback((OAuthBearerTokenCallback) callback);        else if (callback instanceof SaslExtensionsCallback)            handleCallback((SaslExtensionsCallback) callback, Subject.getSubject(AccessController.getContext()));        else            throw new UnsupportedCallbackException(callback);    }}
f4678
0
close
public void kafkatest_f4679_0()
{// empty}
f4679
0
getNegotiatedProperty
public Object kafkatest_f4687_0(String propName)
{    if (!complete)        throw new IllegalStateException("Authentication exchange has not completed");    if (NEGOTIATED_PROPERTY_KEY_TOKEN.equals(propName))        return tokenForNegotiatedProperty;    if (SaslInternalConfigs.CREDENTIAL_LIFETIME_MS_SASL_NEGOTIATED_PROPERTY_KEY.equals(propName))        return tokenForNegotiatedProperty.lifetimeMs();    return extensions.map().get(propName);}
f4687
0
isComplete
public boolean kafkatest_f4688_0()
{    return complete;}
f4688
0
unwrap
public byte[] kafkatest_f4689_0(byte[] incoming, int offset, int len)
{    if (!complete)        throw new IllegalStateException("Authentication exchange has not completed");    return Arrays.copyOfRange(incoming, offset, offset + len);}
f4689
0
createSaslServer
public SaslServer kafkatest_f4697_0(String mechanism, String protocol, String serverName, Map<String, ?> props, CallbackHandler callbackHandler)
{    String[] mechanismNamesCompatibleWithPolicy = getMechanismNames(props);    for (int i = 0; i < mechanismNamesCompatibleWithPolicy.length; i++) {        if (mechanismNamesCompatibleWithPolicy[i].equals(mechanism)) {            return new OAuthBearerSaslServer(callbackHandler);        }    }    return null;}
f4697
0
getMechanismNames
public String[] kafkatest_f4698_0(Map<String, ?> props)
{    return OAuthBearerSaslServer.mechanismNamesCompatibleWithPolicy(props);}
f4698
0
initialize
public static void kafkatest_f4699_0()
{    Security.addProvider(new OAuthBearerSaslServerProvider());}
f4699
0
startTimeMs
public Long kafkatest_f4707_0()
{    return startTimeMs;}
f4707
0
lifetimeMs
public long kafkatest_f4708_0()
{    return lifetime;}
f4708
0
scope
public Set<String> kafkatest_f4709_0() throws OAuthBearerIllegalTokenException
{    return scope;}
f4709
0
issuedAt
public Number kafkatest_f4717_0() throws OAuthBearerIllegalTokenException
{    return claim("iat", Number.class);}
f4717
0
subject
public String kafkatest_f4718_0() throws OAuthBearerIllegalTokenException
{    return claim("sub", String.class);}
f4718
0
toMap
public static Map<String, Object> kafkatest_f4719_0(String split) throws OAuthBearerIllegalTokenException
{    Map<String, Object> retval = new HashMap<>();    try {        byte[] decode = Base64.getDecoder().decode(split);        JsonNode jsonNode = new ObjectMapper().readTree(decode);        if (jsonNode == null)            throw new OAuthBearerIllegalTokenException(OAuthBearerValidationResult.newFailure("malformed JSON"));        for (Iterator<Entry<String, JsonNode>> iterator = jsonNode.fields(); iterator.hasNext(); ) {            Entry<String, JsonNode> entry = iterator.next();            retval.put(entry.getKey(), convert(entry.getValue()));        }        return Collections.unmodifiableMap(retval);    } catch (IllegalArgumentException e) {        // potentially thrown by java.util.Base64.Decoder implementations        throw new OAuthBearerIllegalTokenException(OAuthBearerValidationResult.newFailure("malformed Base64 URL encoded value"));    } catch (IOException e) {        throw new OAuthBearerIllegalTokenException(OAuthBearerValidationResult.newFailure("malformed JSON"));    }}
f4719
0
configure
public void kafkatest_f4727_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    if (!OAuthBearerLoginModule.OAUTHBEARER_MECHANISM.equals(saslMechanism))        throw new IllegalArgumentException(String.format("Unexpected SASL mechanism: %s", saslMechanism));    if (Objects.requireNonNull(jaasConfigEntries).size() != 1 || jaasConfigEntries.get(0) == null)        throw new IllegalArgumentException(String.format("Must supply exactly 1 non-null JAAS mechanism configuration (size was %d)", jaasConfigEntries.size()));    this.moduleOptions = Collections.unmodifiableMap((Map<String, String>) jaasConfigEntries.get(0).getOptions());    configured = true;}
f4727
0
handle
public void kafkatest_f4728_0(Callback[] callbacks) throws IOException, UnsupportedCallbackException
{    if (!configured())        throw new IllegalStateException("Callback handler not configured");    for (Callback callback : callbacks) {        if (callback instanceof OAuthBearerTokenCallback)            try {                handleTokenCallback((OAuthBearerTokenCallback) callback);            } catch (KafkaException e) {                throw new IOException(e.getMessage(), e);            }        else if (callback instanceof SaslExtensionsCallback)            try {                handleExtensionsCallback((SaslExtensionsCallback) callback);            } catch (KafkaException e) {                throw new IOException(e.getMessage(), e);            }        else            throw new UnsupportedCallbackException(callback);    }}
f4728
0
close
public void kafkatest_f4729_0()
{// empty}
f4729
0
option
private String kafkatest_f4737_0(String key)
{    if (!configured)        throw new IllegalStateException("Callback handler not configured");    return moduleOptions.get(Objects.requireNonNull(key));}
f4737
0
claimOrHeaderJsonText
private String kafkatest_f4738_0(String claimName, Number claimValue)
{    return QUOTE + escape(claimName) + QUOTE + ":" + claimValue;}
f4738
0
claimOrHeaderJsonText
private String kafkatest_f4739_0(String claimName, String claimValue)
{    return QUOTE + escape(claimName) + QUOTE + ":" + QUOTE + escape(claimValue) + QUOTE;}
f4739
0
close
public void kafkatest_f4747_0()
{// empty}
f4747
0
handleCallback
private voidf4748_1OAuthBearerValidatorCallback callback)
{    String tokenValue = callback.tokenValue();    if (tokenValue == null)        throw new IllegalArgumentException("Callback missing required token value");    String principalClaimName = principalClaimName();    String scopeClaimName = scopeClaimName();    List<String> requiredScope = requiredScope();    int allowableClockSkewMs = allowableClockSkewMs();    OAuthBearerUnsecuredJws unsecuredJwt = new OAuthBearerUnsecuredJws(tokenValue, principalClaimName, scopeClaimName);    long now = time.milliseconds();    OAuthBearerValidationUtils.validateClaimForExistenceAndType(unsecuredJwt, true, principalClaimName, String.class).throwExceptionIfFailed();    OAuthBearerValidationUtils.validateIssuedAt(unsecuredJwt, false, now, allowableClockSkewMs).throwExceptionIfFailed();    OAuthBearerValidationUtils.validateExpirationTime(unsecuredJwt, now, allowableClockSkewMs).throwExceptionIfFailed();    OAuthBearerValidationUtils.validateTimeConsistency(unsecuredJwt).throwExceptionIfFailed();    OAuthBearerValidationUtils.validateScope(unsecuredJwt, requiredScope).throwExceptionIfFailed();        callback.token(unsecuredJwt);}
private voidf4748
1
principalClaimName
private String kafkatest_f4749_0()
{    String principalClaimNameValue = option(PRINCIPAL_CLAIM_NAME_OPTION);    String principalClaimName = principalClaimNameValue != null && !principalClaimNameValue.trim().isEmpty() ? principalClaimNameValue.trim() : "sub";    return principalClaimName;}
f4749
0
success
public boolean kafkatest_f4757_0()
{    return success;}
f4757
0
failureDescription
public String kafkatest_f4758_0()
{    return failureDescription;}
f4758
0
failureScope
public String kafkatest_f4759_0()
{    return failureScope;}
f4759
0
confirmNonNegative
private static int kafkatest_f4767_0(int allowableClockSkewMs) throws OAuthBearerConfigException
{    if (allowableClockSkewMs < 0)        throw new OAuthBearerConfigException(String.format("Allowable clock skew must not be negative: %d", allowableClockSkewMs));    return allowableClockSkewMs;}
f4767
0
doesNotExistResult
private static OAuthBearerValidationResult kafkatest_f4768_0(boolean required, String claimName)
{    return required ? OAuthBearerValidationResult.newFailure(String.format("Required claim missing: %s", claimName)) : OAuthBearerValidationResult.newSuccess();}
f4768
0
token
public OAuthBearerToken kafkatest_f4769_0()
{    return token;}
f4769
0
login
public booleanf4777_1) throws LoginException
{    if (loginState == LoginState.LOGGED_IN_NOT_COMMITTED) {        if (tokenRequiringCommit != null)            throw new IllegalStateException(String.format("Already have an uncommitted token with private credential token count=%d", committedTokenCount()));        else            throw new IllegalStateException("Already logged in without a token");    }    if (loginState == LoginState.COMMITTED) {        if (myCommittedToken != null)            throw new IllegalStateException(String.format("Already have a committed token with private credential token count=%d; must login on another login context or logout here first before reusing the same login context", committedTokenCount()));        else            throw new IllegalStateException("Login has already been committed without a token");    }    identifyToken();    if (tokenRequiringCommit != null)        identifyExtensions();    else            loginState = LoginState.LOGGED_IN_NOT_COMMITTED;     invoke commit() to commit it; current committed token count={}", committedTokenCount());    return true;}
public booleanf4777
1
identifyToken
private voidf4778_1) throws LoginException
{    OAuthBearerTokenCallback tokenCallback = new OAuthBearerTokenCallback();    try {        callbackHandler.handle(new Callback[] { tokenCallback });    } catch (IOException | UnsupportedCallbackException e) {                throw new LoginException("An internal error occurred while retrieving token from callback handler");    }    tokenRequiringCommit = tokenCallback.token();    if (tokenCallback.errorCode() != null) {                throw new LoginException(tokenCallback.errorDescription());    }}
private voidf4778
1
identifyExtensions
private voidf4779_1) throws LoginException
{    SaslExtensionsCallback extensionsCallback = new SaslExtensionsCallback();    try {        callbackHandler.handle(new Callback[] { extensionsCallback });        extensionsRequiringCommit = extensionsCallback.extensions();    } catch (IOException e) {                throw new LoginException("An internal error occurred while retrieving SASL extensions from callback handler");    } catch (UnsupportedCallbackException e) {        extensionsRequiringCommit = EMPTY_EXTENSIONS;            }    if (extensionsRequiringCommit == null) {                throw new LoginException("Extensions cannot be null.");    }}
private voidf4779
1
errorUri
public String kafkatest_f4787_0()
{    return errorUri;}
f4787
0
token
public void kafkatest_f4788_0(OAuthBearerToken token)
{    this.token = token;    this.errorCode = null;    this.errorDescription = null;    this.errorUri = null;}
f4788
0
error
public void kafkatest_f4789_0(String errorCode, String errorDescription, String errorUri)
{    if (Objects.requireNonNull(errorCode).isEmpty())        throw new IllegalArgumentException("error code must not be empty");    this.errorCode = errorCode;    this.errorDescription = errorDescription;    this.errorUri = errorUri;    this.token = null;}
f4789
0
evaluateResponse
public byte[] kafkatest_f4797_0(byte[] responseBytes) throws SaslAuthenticationException
{    /*         * Message format (from https://tools.ietf.org/html/rfc4616):         *         * message   = [authzid] UTF8NUL authcid UTF8NUL passwd         * authcid   = 1*SAFE ; MUST accept up to 255 octets         * authzid   = 1*SAFE ; MUST accept up to 255 octets         * passwd    = 1*SAFE ; MUST accept up to 255 octets         * UTF8NUL   = %x00 ; UTF-8 encoded NUL character         *         * SAFE      = UTF1 / UTF2 / UTF3 / UTF4         *                ;; any UTF-8 encoded Unicode character except NUL         */    String response = new String(responseBytes, StandardCharsets.UTF_8);    List<String> tokens = extractTokens(response);    String authorizationIdFromClient = tokens.get(0);    String username = tokens.get(1);    String password = tokens.get(2);    if (username.isEmpty()) {        throw new SaslAuthenticationException("Authentication failed: username not specified");    }    if (password.isEmpty()) {        throw new SaslAuthenticationException("Authentication failed: password not specified");    }    NameCallback nameCallback = new NameCallback("username", username);    PlainAuthenticateCallback authenticateCallback = new PlainAuthenticateCallback(password.toCharArray());    try {        callbackHandler.handle(new Callback[] { nameCallback, authenticateCallback });    } catch (Throwable e) {        throw new SaslAuthenticationException("Authentication failed: credentials for user could not be verified", e);    }    if (!authenticateCallback.authenticated())        throw new SaslAuthenticationException("Authentication failed: Invalid username or password");    if (!authorizationIdFromClient.isEmpty() && !authorizationIdFromClient.equals(username))        throw new SaslAuthenticationException("Authentication failed: Client requested an authorization id that is different from username");    this.authorizationId = username;    complete = true;    return new byte[0];}
f4797
0
extractTokens
private List<String> kafkatest_f4798_0(String string)
{    List<String> tokens = new ArrayList<>();    int startIndex = 0;    for (int i = 0; i < 4; ++i) {        int endIndex = string.indexOf("\u0000", startIndex);        if (endIndex == -1) {            tokens.add(string.substring(startIndex));            break;        }        tokens.add(string.substring(startIndex, endIndex));        startIndex = endIndex + 1;    }    if (tokens.size() != 3)        throw new SaslAuthenticationException("Invalid SASL/PLAIN response: expected 3 tokens, got " + tokens.size());    return tokens;}
f4798
0
getAuthorizationID
public String kafkatest_f4799_0()
{    if (!complete)        throw new IllegalStateException("Authentication exchange has not completed");    return authorizationId;}
f4799
0
initialize
public static void kafkatest_f4808_0()
{    Security.addProvider(new PlainSaslServerProvider());}
f4808
0
configure
public void kafkatest_f4809_0(Map<String, ?> configs, String mechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    this.jaasConfigEntries = jaasConfigEntries;}
f4809
0
handle
public void kafkatest_f4810_0(Callback[] callbacks) throws IOException, UnsupportedCallbackException
{    String username = null;    for (Callback callback : callbacks) {        if (callback instanceof NameCallback)            username = ((NameCallback) callback).getDefaultName();        else if (callback instanceof PlainAuthenticateCallback) {            PlainAuthenticateCallback plainCallback = (PlainAuthenticateCallback) callback;            boolean authenticated = authenticate(username, plainCallback.password());            plainCallback.authenticated(authenticated);        } else            throw new UnsupportedCallbackException(callback);    }}
f4810
0
commit
public boolean kafkatest_f4819_0()
{    return true;}
f4819
0
abort
public boolean kafkatest_f4820_0()
{    return false;}
f4820
0
credentialToString
public static String kafkatest_f4821_0(ScramCredential credential)
{    return String.format("%s=%s,%s=%s,%s=%s,%s=%d", SALT, Base64.getEncoder().encodeToString(credential.salt()), STORED_KEY, Base64.getEncoder().encodeToString(credential.storedKey()), SERVER_KEY, Base64.getEncoder().encodeToString(credential.serverKey()), ITERATIONS, credential.iterations());}
f4821
0
hi
public byte[] kafkatest_f4829_0(byte[] str, byte[] salt, int iterations) throws InvalidKeyException
{    mac.init(new SecretKeySpec(str, mac.getAlgorithm()));    mac.update(salt);    byte[] u1 = mac.doFinal(new byte[] { 0, 0, 0, 1 });    byte[] prev = u1;    byte[] result = u1;    for (int i = 2; i <= iterations; i++) {        byte[] ui = hmac(str, prev);        result = xor(result, ui);        prev = ui;    }    return result;}
f4829
0
normalize
public byte[] kafkatest_f4830_0(String str)
{    return toBytes(str);}
f4830
0
saltedPassword
public byte[] kafkatest_f4831_0(String password, byte[] salt, int iterations) throws InvalidKeyException
{    return hi(normalize(password), salt, iterations);}
f4831
0
authMessage
private byte[] kafkatest_f4839_0(ClientFirstMessage clientFirstMessage, ServerFirstMessage serverFirstMessage, ClientFinalMessage clientFinalMessage)
{    return toBytes(authMessage(clientFirstMessage.clientFirstMessageBare(), serverFirstMessage.toMessage(), clientFinalMessage.clientFinalMessageWithoutProof()));}
f4839
0
storedKey
public byte[] kafkatest_f4840_0(byte[] clientSignature, byte[] clientProof)
{    return hash(xor(clientSignature, clientProof));}
f4840
0
serverKey
public byte[] kafkatest_f4841_0(byte[] saltedPassword) throws InvalidKeyException
{    return hmac(saltedPassword, toBytes("Server Key"));}
f4841
0
macAlgorithm
public String kafkatest_f4849_0()
{    return macAlgorithm;}
f4849
0
minIterations
public int kafkatest_f4850_0()
{    return minIterations;}
f4850
0
forMechanismName
public static ScramMechanism kafkatest_f4851_0(String mechanismName)
{    return MECHANISMS_MAP.get(mechanismName);}
f4851
0
gs2Header
public String kafkatest_f4859_0()
{    return "n," + authorizationId + ",";}
f4859
0
extensions
public ScramExtensions kafkatest_f4860_0()
{    return extensions;}
f4860
0
clientFirstMessageBare
public String kafkatest_f4861_0()
{    String extensionStr = Utils.mkString(extensions.map(), "", "", "=", ",");    if (extensionStr.isEmpty())        return String.format("n=%s,r=%s", saslName, nonce);    else        return String.format("n=%s,r=%s,%s", saslName, nonce, extensionStr);}
f4861
0
proof
public byte[] kafkatest_f4869_0()
{    return proof;}
f4869
0
proof
public void kafkatest_f4870_0(byte[] proof)
{    this.proof = proof;}
f4870
0
clientFinalMessageWithoutProof
public String kafkatest_f4871_0()
{    return String.format("c=%s,r=%s", Base64.getEncoder().encodeToString(channelBinding), nonce);}
f4871
0
isComplete
public boolean kafkatest_f4879_0()
{    return state == State.COMPLETE;}
f4879
0
unwrap
public byte[] kafkatest_f4880_0(byte[] incoming, int offset, int len)
{    if (!isComplete())        throw new IllegalStateException("Authentication exchange has not completed");    return Arrays.copyOfRange(incoming, offset, offset + len);}
f4880
0
wrap
public byte[] kafkatest_f4881_0(byte[] outgoing, int offset, int len)
{    if (!isComplete())        throw new IllegalStateException("Authentication exchange has not completed");    return Arrays.copyOfRange(outgoing, offset, offset + len);}
f4881
0
evaluateResponse
public byte[]f4890_1byte[] response) throws SaslException, SaslAuthenticationException
{    try {        switch(state) {            case RECEIVE_CLIENT_FIRST_MESSAGE:                this.clientFirstMessage = new ClientFirstMessage(response);                this.scramExtensions = clientFirstMessage.extensions();                if (!SUPPORTED_EXTENSIONS.containsAll(scramExtensions.map().keySet())) {                                    }                String serverNonce = formatter.secureRandomString();                try {                    String saslName = clientFirstMessage.saslName();                    this.username = formatter.username(saslName);                    NameCallback nameCallback = new NameCallback("username", username);                    ScramCredentialCallback credentialCallback;                    if (scramExtensions.tokenAuthenticated()) {                        DelegationTokenCredentialCallback tokenCallback = new DelegationTokenCredentialCallback();                        credentialCallback = tokenCallback;                        callbackHandler.handle(new Callback[] { nameCallback, tokenCallback });                        if (tokenCallback.tokenOwner() == null)                            throw new SaslException("Token Authentication failed: Invalid tokenId : " + username);                        this.authorizationId = tokenCallback.tokenOwner();                        this.tokenExpiryTimestamp = tokenCallback.tokenExpiryTimestamp();                    } else {                        credentialCallback = new ScramCredentialCallback();                        callbackHandler.handle(new Callback[] { nameCallback, credentialCallback });                        this.authorizationId = username;                        this.tokenExpiryTimestamp = null;                    }                    this.scramCredential = credentialCallback.scramCredential();                    if (scramCredential == null)                        throw new SaslException("Authentication failed: Invalid user credentials");                    String authorizationIdFromClient = clientFirstMessage.authorizationId();                    if (!authorizationIdFromClient.isEmpty() && !authorizationIdFromClient.equals(username))                        throw new SaslAuthenticationException("Authentication failed: Client requested an authorization id that is different from username");                    if (scramCredential.iterations() < mechanism.minIterations())                        throw new SaslException("Iterations " + scramCredential.iterations() + " is less than the minimum " + mechanism.minIterations() + " for " + mechanism);                    this.serverFirstMessage = new ServerFirstMessage(clientFirstMessage.nonce(), serverNonce, scramCredential.salt(), scramCredential.iterations());                    setState(State.RECEIVE_CLIENT_FINAL_MESSAGE);                    return serverFirstMessage.toBytes();                } catch (SaslException | AuthenticationException e) {                    throw e;                } catch (Throwable e) {                    throw new SaslException("Authentication failed: Credentials could not be obtained", e);                }            case RECEIVE_CLIENT_FINAL_MESSAGE:                try {                    ClientFinalMessage clientFinalMessage = new ClientFinalMessage(response);                    verifyClientProof(clientFinalMessage);                    byte[] serverKey = scramCredential.serverKey();                    byte[] serverSignature = formatter.serverSignature(serverKey, clientFirstMessage, serverFirstMessage, clientFinalMessage);                    ServerFinalMessage serverFinalMessage = new ServerFinalMessage(null, serverSignature);                    clearCredentials();                    setState(State.COMPLETE);                    return serverFinalMessage.toBytes();                } catch (InvalidKeyException e) {                    throw new SaslException("Authentication failed: Invalid client final message", e);                }            default:                throw new IllegalSaslStateException("Unexpected challenge in Sasl server state " + state);        }    } catch (SaslException | AuthenticationException e) {        clearCredentials();        setState(State.FAILED);        throw e;    }}
public byte[]f4890
1
getAuthorizationID
public String kafkatest_f4891_0()
{    if (!isComplete())        throw new IllegalStateException("Authentication exchange has not completed");    return authorizationId;}
f4891
0
getMechanismName
public String kafkatest_f4892_0()
{    return mechanism.mechanismName();}
f4892
0
createSaslServer
public SaslServer kafkatest_f4901_0(String mechanism, String protocol, String serverName, Map<String, ?> props, CallbackHandler cbh) throws SaslException
{    if (!ScramMechanism.isScram(mechanism)) {        throw new SaslException(String.format("Requested mechanism '%s' is not supported. Supported mechanisms are '%s'.", mechanism, ScramMechanism.mechanismNames()));    }    try {        return new ScramSaslServer(ScramMechanism.forMechanismName(mechanism), props, cbh);    } catch (NoSuchAlgorithmException e) {        throw new SaslException("Hash algorithm not supported for mechanism " + mechanism, e);    }}
f4901
0
getMechanismNames
public String[] kafkatest_f4902_0(Map<String, ?> props)
{    Collection<String> mechanisms = ScramMechanism.mechanismNames();    return mechanisms.toArray(new String[mechanisms.size()]);}
f4902
0
initialize
public static void kafkatest_f4903_0()
{    Security.addProvider(new ScramSaslServerProvider());}
f4903
0
scramCredential
public ScramCredential kafkatest_f4912_0()
{    return scramCredential;}
f4912
0
extensions
public Map<String, String> kafkatest_f4913_0()
{    return extensions;}
f4913
0
extensions
public void kafkatest_f4914_0(Map<String, String> extensions)
{    this.extensions = extensions;}
f4914
0
createSSLContext
private SSLContextf4923_1)
{    try {        SSLContext sslContext;        if (provider != null)            sslContext = SSLContext.getInstance(protocol, provider);        else            sslContext = SSLContext.getInstance(protocol);        KeyManager[] keyManagers = null;        if (keystore != null || kmfAlgorithm != null) {            String kmfAlgorithm = this.kmfAlgorithm != null ? this.kmfAlgorithm : KeyManagerFactory.getDefaultAlgorithm();            KeyManagerFactory kmf = KeyManagerFactory.getInstance(kmfAlgorithm);            if (keystore != null) {                KeyStore ks = keystore.load();                Password keyPassword = keystore.keyPassword != null ? keystore.keyPassword : keystore.password;                kmf.init(ks, keyPassword.value().toCharArray());            } else {                kmf.init(null, null);            }            keyManagers = kmf.getKeyManagers();        }        String tmfAlgorithm = this.tmfAlgorithm != null ? this.tmfAlgorithm : TrustManagerFactory.getDefaultAlgorithm();        TrustManagerFactory tmf = TrustManagerFactory.getInstance(tmfAlgorithm);        KeyStore ts = truststore == null ? null : truststore.load();        tmf.init(ts);        sslContext.init(keyManagers, tmf.getTrustManagers(), this.secureRandomImplementation);                return sslContext;    } catch (Exception e) {        throw new KafkaException(e);    }}
private SSLContextf4923
1
createKeystore
private static SecurityStore kafkatest_f4924_0(String type, String path, Password password, Password keyPassword)
{    if (path == null && password != null) {        throw new KafkaException("SSL key store is not specified, but key store password is specified.");    } else if (path != null && password == null) {        throw new KafkaException("SSL key store is specified, but key store password is not specified.");    } else if (path != null && password != null) {        return new SecurityStore(type, path, password, keyPassword);    } else        // path == null, clients may use this path with brokers that don't require client auth        return null;}
f4924
0
createTruststore
private static SecurityStore kafkatest_f4925_0(String type, String path, Password password)
{    if (path == null && password != null) {        throw new KafkaException("SSL trust store is not specified, but trust store password is specified.");    } else if (path != null) {        return new SecurityStore(type, path, password, null);    } else        return null;}
f4925
0
lastModifiedMs
private Longf4933_1String path)
{    try {        return Files.getLastModifiedTime(Paths.get(path)).toMillis();    } catch (IOException e) {                return null;    }}
private Longf4933
1
modified
 boolean kafkatest_f4934_0()
{    Long modifiedMs = lastModifiedMs(path);    return modifiedMs != null && !Objects.equals(modifiedMs, this.fileLastModifiedMs);}
f4934
0
toString
public String kafkatest_f4935_0()
{    return "SecurityStore(" + "path=" + path + ", modificationTime=" + (fileLastModifiedMs == null ? null : new Date(fileLastModifiedMs)) + ")";}
f4935
0
sslEngineBuilder
public SslEngineBuilder kafkatest_f4943_0()
{    return sslEngineBuilder;}
f4943
0
copyMapEntries
private static void kafkatest_f4944_0(Map<K, V> destMap, Map<K, ? extends V> srcMap, Set<K> keySet)
{    for (K k : keySet) {        copyMapEntry(destMap, srcMap, k);    }}
f4944
0
copyMapEntry
private static void kafkatest_f4945_0(Map<K, V> destMap, Map<K, ? extends V> srcMap, K key)
{    if (srcMap.containsKey(key)) {        destMap.put(key, srcMap.get(key));    }}
f4945
0
beginHandshake
 void kafkatest_f4953_0() throws SSLException
{    sslEngine.beginHandshake();}
f4953
0
handshake
 void kafkatest_f4954_0(SslEngineValidator peerValidator) throws SSLException
{    SSLEngineResult.HandshakeStatus handshakeStatus = sslEngine.getHandshakeStatus();    while (true) {        switch(handshakeStatus) {            case NEED_WRAP:                handshakeResult = sslEngine.wrap(EMPTY_BUF, netBuffer);                switch(handshakeResult.getStatus()) {                    case OK:                        break;                    case BUFFER_OVERFLOW:                        netBuffer.compact();                        netBuffer = Utils.ensureCapacity(netBuffer, sslEngine.getSession().getPacketBufferSize());                        netBuffer.flip();                        break;                    case BUFFER_UNDERFLOW:                    case CLOSED:                    default:                        throw new SSLException("Unexpected handshake status: " + handshakeResult.getStatus());                }                return;            case NEED_UNWRAP:                if (// no data to unwrap, return to process peer                peerValidator.netBuffer.position() == 0)                    return;                // unwrap the data from peer                peerValidator.netBuffer.flip();                handshakeResult = sslEngine.unwrap(peerValidator.netBuffer, appBuffer);                peerValidator.netBuffer.compact();                handshakeStatus = handshakeResult.getHandshakeStatus();                switch(handshakeResult.getStatus()) {                    case OK:                        break;                    case BUFFER_OVERFLOW:                        appBuffer = Utils.ensureCapacity(appBuffer, sslEngine.getSession().getApplicationBufferSize());                        break;                    case BUFFER_UNDERFLOW:                        netBuffer = Utils.ensureCapacity(netBuffer, sslEngine.getSession().getPacketBufferSize());                        break;                    case CLOSED:                    default:                        throw new SSLException("Unexpected handshake status: " + handshakeResult.getStatus());                }                break;            case NEED_TASK:                sslEngine.getDelegatedTask().run();                handshakeStatus = sslEngine.getHandshakeStatus();                break;            case FINISHED:                return;            case NOT_HANDSHAKING:                if (handshakeResult.getHandshakeStatus() != SSLEngineResult.HandshakeStatus.FINISHED)                    throw new SSLException("Did not finish handshake");                return;            default:                throw new IllegalStateException("Unexpected handshake status " + handshakeStatus);        }    }}
f4954
0
complete
 boolean kafkatest_f4955_0()
{    return sslEngine.getHandshakeStatus() == SSLEngineResult.HandshakeStatus.FINISHED || sslEngine.getHandshakeStatus() == SSLEngineResult.HandshakeStatus.NOT_HANDSHAKING;}
f4955
0
escapeLiteralBackReferences
private String kafkatest_f4963_0(final String unescaped, final int numCapturingGroups)
{    if (numCapturingGroups == 0) {        return unescaped;    }    String value = unescaped;    final Matcher backRefMatcher = BACK_REFERENCE_PATTERN.matcher(value);    while (backRefMatcher.find()) {        final String backRefNum = backRefMatcher.group(1);        if (backRefNum.startsWith("0")) {            continue;        }        final int originalBackRefIndex = Integer.parseInt(backRefNum);        int backRefIndex = originalBackRefIndex;        // we want to truncate the 1 and get 0.        while (backRefIndex > numCapturingGroups && backRefIndex >= 10) {            backRefIndex /= 10;        }        if (backRefIndex > numCapturingGroups) {            final StringBuilder sb = new StringBuilder(value.length() + 1);            final int groupStart = backRefMatcher.start(1);            sb.append(value.substring(0, groupStart - 1));            sb.append("\\");            sb.append(value.substring(groupStart - 1));            value = sb.toString();        }    }    return value;}
f4963
0
toString
public String kafkatest_f4964_0()
{    StringBuilder buf = new StringBuilder();    if (isDefault) {        buf.append("DEFAULT");    } else {        buf.append("RULE:");        if (pattern != null) {            buf.append(pattern);        }        if (replacement != null) {            buf.append("/");            buf.append(replacement);        }        if (toLowerCase) {            buf.append("/L");        } else if (toUpperCase) {            buf.append("/U");        }    }    return buf.toString();}
f4964
0
tokenInfo
public TokenInformation kafkatest_f4965_0()
{    return tokenInformation;}
f4965
0
updateCache
public void kafkatest_f4973_0(DelegationToken token, Map<String, ScramCredential> scramCredentialMap)
{    // Update TokenCache    String tokenId = token.tokenInfo().tokenId();    addToken(tokenId, token.tokenInfo());    String hmac = token.hmacAsBase64String();    // Update Scram Credentials    updateCredentials(tokenId, scramCredentialMap);    // Update hmac-id cache    hmacTokenIdCache.put(hmac, tokenId);    tokenIdHmacCache.put(tokenId, hmac);}
f4973
0
removeCache
public void kafkatest_f4974_0(String tokenId)
{    removeToken(tokenId);    updateCredentials(tokenId, new HashMap<>());}
f4974
0
tokenIdForHmac
public String kafkatest_f4975_0(String base64hmac)
{    return hmacTokenIdCache.get(base64hmac);}
f4975
0
tokenOwner
public void kafkatest_f4983_0(String tokenOwner)
{    this.tokenOwner = tokenOwner;}
f4983
0
tokenOwner
public String kafkatest_f4984_0()
{    return tokenOwner;}
f4984
0
tokenExpiryTimestamp
public void kafkatest_f4985_0(Long tokenExpiryTimestamp)
{    this.tokenExpiryTimestamp = tokenExpiryTimestamp;}
f4985
0
setExpiryTimestamp
public void kafkatest_f4993_0(long expiryTimestamp)
{    this.expiryTimestamp = expiryTimestamp;}
f4993
0
tokenId
public String kafkatest_f4994_0()
{    return tokenId;}
f4994
0
maxTimestamp
public long kafkatest_f4995_0()
{    return maxTimestamp;}
f4995
0
serialize
public byte[] kafkatest_f5003_0(String topic, ByteBuffer data)
{    if (data == null)        return null;    data.rewind();    if (data.hasArray()) {        byte[] arr = data.array();        if (data.arrayOffset() == 0 && arr.length == data.remaining()) {            return arr;        }    }    byte[] ret = new byte[data.remaining()];    data.get(ret, 0, ret.length);    data.rewind();    return ret;}
f5003
0
deserialize
public Bytes kafkatest_f5004_0(String topic, byte[] data)
{    if (data == null)        return null;    return new Bytes(data);}
f5004
0
serialize
public byte[] kafkatest_f5005_0(String topic, Bytes data)
{    if (data == null)        return null;    return data.get();}
f5005
0
deserialize
public T kafkatest_f5013_0(String topic, byte[] data)
{    return deserializer.deserialize(topic, data);}
f5013
0
close
public void kafkatest_f5014_0()
{    deserializer.close();}
f5014
0
ensureExtended
public static ExtendedDeserializer<T> kafkatest_f5015_0(Deserializer<T> deserializer)
{    return deserializer == null ? null : deserializer instanceof ExtendedDeserializer ? (ExtendedDeserializer<T>) deserializer : new ExtendedDeserializer.Wrapper<>(deserializer);}
f5015
0
deserialize
public Integer kafkatest_f5023_0(String topic, byte[] data)
{    if (data == null)        return null;    if (data.length != 4) {        throw new SerializationException("Size of data received by IntegerDeserializer is not 4");    }    int value = 0;    for (byte b : data) {        value <<= 8;        value |= b & 0xFF;    }    return value;}
f5023
0
serialize
public byte[] kafkatest_f5024_0(String topic, Integer data)
{    if (data == null)        return null;    return new byte[] { (byte) (data >>> 24), (byte) (data >>> 16), (byte) (data >>> 8), data.byteValue() };}
f5024
0
deserialize
public Long kafkatest_f5025_0(String topic, byte[] data)
{    if (data == null)        return null;    if (data.length != 8) {        throw new SerializationException("Size of data received by LongDeserializer is not 8");    }    long value = 0;    for (byte b : data) {        value <<= 8;        value |= b & 0xFF;    }    return value;}
f5025
0
serdeFrom
public static Serde<T> kafkatest_f5033_0(Class<T> type)
{    if (String.class.isAssignableFrom(type)) {        return (Serde<T>) String();    }    if (Short.class.isAssignableFrom(type)) {        return (Serde<T>) Short();    }    if (Integer.class.isAssignableFrom(type)) {        return (Serde<T>) Integer();    }    if (Long.class.isAssignableFrom(type)) {        return (Serde<T>) Long();    }    if (Float.class.isAssignableFrom(type)) {        return (Serde<T>) Float();    }    if (Double.class.isAssignableFrom(type)) {        return (Serde<T>) Double();    }    if (byte[].class.isAssignableFrom(type)) {        return (Serde<T>) ByteArray();    }    if (ByteBuffer.class.isAssignableFrom(type)) {        return (Serde<T>) ByteBuffer();    }    if (Bytes.class.isAssignableFrom(type)) {        return (Serde<T>) Bytes();    }    if (UUID.class.isAssignableFrom(type)) {        return (Serde<T>) UUID();    }    // TODO: we can also serializes objects of type T using generic Java serialization by default    throw new IllegalArgumentException("Unknown class for built-in serializer. Supported types are: " + "String, Short, Integer, Long, Float, Double, ByteArray, ByteBuffer, Bytes, UUID");}
f5033
0
serdeFrom
public static Serde<T> kafkatest_f5034_0(final Serializer<T> serializer, final Deserializer<T> deserializer)
{    if (serializer == null) {        throw new IllegalArgumentException("serializer must not be null");    }    if (deserializer == null) {        throw new IllegalArgumentException("deserializer must not be null");    }    return new WrapperSerde<>(serializer, deserializer);}
f5034
0
Long
public static Serde<Long> kafkatest_f5035_0()
{    return new LongSerde();}
f5035
0
UUID
public static Serde<UUID> kafkatest_f5043_0()
{    return new UUIDSerde();}
f5043
0
ByteArray
public static Serde<byte[]> kafkatest_f5044_0()
{    return new ByteArraySerde();}
f5044
0
configure
 void kafkatest_f5045_0(Map<String, ?> configs, boolean isKey)
{// intentionally left blank}
f5045
0
serialize
public byte[] kafkatest_f5053_0(String topic, String data)
{    try {        if (data == null)            return null;        else            return data.getBytes(encoding);    } catch (UnsupportedEncodingException e) {        throw new SerializationException("Error when serializing string to byte[] due to unsupported encoding " + encoding);    }}
f5053
0
configure
public void kafkatest_f5054_0(Map<String, ?> configs, boolean isKey)
{    String propertyName = isKey ? "key.deserializer.encoding" : "value.deserializer.encoding";    Object encodingValue = configs.get(propertyName);    if (encodingValue == null)        encodingValue = configs.get("deserializer.encoding");    if (encodingValue != null && encodingValue instanceof String)        encoding = (String) encodingValue;}
f5054
0
deserialize
public UUID kafkatest_f5055_0(String topic, byte[] data)
{    try {        if (data == null)            return null;        else            return UUID.fromString(new String(data, encoding));    } catch (UnsupportedEncodingException e) {        throw new SerializationException("Error when deserializing byte[] to UUID due to unsupported encoding " + encoding, e);    } catch (IllegalArgumentException e) {        throw new SerializationException("Error parsing data into UUID", e);    }}
f5055
0
partition
public int kafkatest_f5063_0()
{    return partition;}
f5063
0
leader
public Node kafkatest_f5064_0()
{    return leader;}
f5064
0
replicas
public List<Node> kafkatest_f5065_0()
{    return replicas;}
f5065
0
hashCode
public int kafkatest_f5073_0()
{    if (hash != 0) {        return hash;    }    final int prime = 31;    int result = 1;    result = prime * result + topic.hashCode();    result = prime * result + partition;    result = prime * result + brokerId;    this.hash = result;    return result;}
f5073
0
equals
public boolean kafkatest_f5074_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    TopicPartitionReplica other = (TopicPartitionReplica) obj;    return partition == other.partition && brokerId == other.brokerId && topic.equals(other.topic);}
f5074
0
toString
public String kafkatest_f5075_0()
{    return String.format("%s-%d-%d", topic, partition, brokerId);}
f5075
0
getCommitId
public static String kafkatest_f5083_0()
{    return COMMIT_ID;}
f5083
0
registerAppInfo
public static synchronized voidf5084_1String prefix, String id, Metrics metrics, long nowMs)
{    try {        ObjectName name = new ObjectName(prefix + ":type=app-info,id=" + Sanitizer.jmxSanitize(id));        AppInfo mBean = new AppInfo(nowMs);        ManagementFactory.getPlatformMBeanServer().registerMBean(mBean, name);        // prefix will be added later by JmxReporter        registerMetrics(metrics, mBean);    } catch (JMException e) {            }}
public static synchronized voidf5084
1
unregisterAppInfo
public static synchronized voidf5085_1String prefix, String id, Metrics metrics)
{    MBeanServer server = ManagementFactory.getPlatformMBeanServer();    try {        ObjectName name = new ObjectName(prefix + ":type=app-info,id=" + Sanitizer.jmxSanitize(id));        if (server.isRegistered(name))            server.unregisterMBean(name);        unregisterMetrics(metrics);    } catch (JMException e) {            }}
public static synchronized voidf5085
1
read
public int kafkatest_f5093_0()
{    if (!buffer.hasRemaining()) {        return -1;    }    return buffer.get() & 0xFF;}
f5093
0
read
public int kafkatest_f5094_0(byte[] bytes, int off, int len)
{    if (!buffer.hasRemaining()) {        return -1;    }    len = Math.min(len, buffer.remaining());    buffer.get(bytes, off, len);    return len;}
f5094
0
write
public void kafkatest_f5095_0(int b)
{    ensureRemaining(1);    buffer.put((byte) b);}
f5095
0
initialCapacity
public int kafkatest_f5103_0()
{    return initialCapacity;}
f5103
0
ensureRemaining
public void kafkatest_f5104_0(int remainingBytesRequired)
{    if (remainingBytesRequired > buffer.remaining())        expandBuffer(remainingBytesRequired);}
f5104
0
expandBuffer
private void kafkatest_f5105_0(int remainingRequired)
{    int expandSize = Math.max((int) (buffer.limit() * REALLOCATION_FACTOR), buffer.position() + remainingRequired);    ByteBuffer temp = ByteBuffer.allocate(expandSize);    int limit = limit();    buffer.flip();    temp.put(buffer);    buffer.limit(limit);    // reset the old buffer's position so that the partial data in the new buffer cannot be mistakenly consumed    // we should ideally only do this for the original buffer, but the additional complexity doesn't seem worth it    buffer.position(initialPosition);    buffer = temp;}
f5105
0
compare
public int kafkatest_f5113_0(byte[] buffer1, byte[] buffer2)
{    return compare(buffer1, 0, buffer1.length, buffer2, 0, buffer2.length);}
f5113
0
compare
public int kafkatest_f5114_0(final byte[] buffer1, int offset1, int length1, final byte[] buffer2, int offset2, int length2)
{    // short circuit equal case    if (buffer1 == buffer2 && offset1 == offset2 && length1 == length2) {        return 0;    }    // similar to Arrays.compare() but considers offset and length    int end1 = offset1 + length1;    int end2 = offset2 + length2;    for (int i = offset1, j = offset2; i < end1 && j < end2; i++, j++) {        int a = buffer1[i] & 0xff;        int b = buffer2[j] & 0xff;        if (a != b) {            return a - b;        }    }    return length1 - length2;}
f5114
0
readUnsignedInt
public static long kafkatest_f5115_0(ByteBuffer buffer)
{    return buffer.getInt() & 0xffffffffL;}
f5115
0
readUnsignedVarint
public static int kafkatest_f5123_0(ByteBuffer buffer)
{    int value = 0;    int i = 0;    int b;    while (((b = buffer.get()) & 0x80) != 0) {        value |= (b & 0x7f) << i;        i += 7;        if (i > 28)            throw illegalVarintException(value);    }    value |= b << i;    return value;}
f5123
0
readUnsignedVarint
public static int kafkatest_f5124_0(DataInput in) throws IOException
{    int value = 0;    int i = 0;    int b;    while (((b = in.readByte()) & 0x80) != 0) {        value |= (b & 0x7f) << i;        i += 7;        if (i > 28)            throw illegalVarintException(value);    }    value |= b << i;    return value;}
f5124
0
readVarint
public static int kafkatest_f5125_0(ByteBuffer buffer)
{    int value = readUnsignedVarint(buffer);    return (value >>> 1) ^ -(value & 1);}
f5125
0
writeVarlong
public static void kafkatest_f5133_0(long value, DataOutput out) throws IOException
{    long v = (value << 1) ^ (value >> 63);    while ((v & 0xffffffffffffff80L) != 0L) {        out.writeByte(((int) v & 0x7f) | 0x80);        v >>>= 7;    }    out.writeByte((byte) v);}
f5133
0
writeVarlong
public static void kafkatest_f5134_0(long value, ByteBuffer buffer)
{    long v = (value << 1) ^ (value >> 63);    while ((v & 0xffffffffffffff80L) != 0L) {        byte b = (byte) ((v & 0x7f) | 0x80);        buffer.put(b);        v >>>= 7;    }    buffer.put((byte) v);}
f5134
0
sizeOfUnsignedVarint
public static int kafkatest_f5135_0(int value)
{    int bytes = 1;    while ((value & 0xffffff80) != 0L) {        bytes += 1;        value >>>= 7;    }    return bytes;}
f5135
0
updateLong
public static void kafkatest_f5143_0(Checksum checksum, long input)
{    checksum.update((byte) (input >> 56));    checksum.update((byte) (input >> 48));    checksum.update((byte) (input >> 40));    checksum.update((byte) (input >> 32));    checksum.update((byte) (input >> 24));    checksum.update((byte) (input >> 16));    checksum.update((byte) (input >> 8));    checksum.update((byte) input);}
f5143
0
hasNext
public boolean kafkatest_f5144_0()
{    return true;}
f5144
0
next
public T kafkatest_f5145_0()
{    T next = list.get(i);    i = (i + 1) % list.size();    return next;}
f5145
0
groupPartitionDataByTopic
public static Map<String, Map<Integer, T>> kafkatest_f5154_0(Map<TopicPartition, ? extends T> data)
{    Map<String, Map<Integer, T>> dataByTopic = new HashMap<>();    for (Map.Entry<TopicPartition, ? extends T> entry : data.entrySet()) {        String topic = entry.getKey().topic();        int partition = entry.getKey().partition();        Map<Integer, T> topicData = dataByTopic.computeIfAbsent(topic, t -> new HashMap<>());        topicData.put(partition, entry.getValue());    }    return dataByTopic;}
f5154
0
groupPartitionsByTopic
public static Map<String, List<Integer>> kafkatest_f5155_0(Collection<TopicPartition> partitions)
{    Map<String, List<Integer>> partitionsByTopic = new HashMap<>();    for (TopicPartition tp : partitions) {        String topic = tp.topic();        List<Integer> topicData = partitionsByTopic.computeIfAbsent(topic, t -> new ArrayList<>());        topicData.add(tp.partition());    }    return partitionsByTopic;}
f5155
0
containsKey
public boolean kafkatest_f5156_0(Object k)
{    return map.containsKey(k);}
f5156
0
clear
public synchronized void kafkatest_f5164_0()
{    this.map = Collections.emptyMap();}
f5164
0
put
public synchronized V kafkatest_f5165_0(K k, V v)
{    Map<K, V> copy = new HashMap<K, V>(this.map);    V prev = copy.put(k, v);    this.map = Collections.unmodifiableMap(copy);    return prev;}
f5165
0
putAll
public synchronized void kafkatest_f5166_0(Map<? extends K, ? extends V> entries)
{    Map<K, V> copy = new HashMap<K, V>(this.map);    copy.putAll(entries);    this.map = Collections.unmodifiableMap(copy);}
f5166
0
crc32
public static long kafkatest_f5174_0(ByteBuffer buffer, int offset, int size)
{    Crc32 crc = new Crc32();    Checksums.update(crc, buffer, offset, size);    return crc.getValue();}
f5174
0
getValue
public long kafkatest_f5175_0()
{    return (~crc) & 0xffffffffL;}
f5175
0
reset
public void kafkatest_f5176_0()
{    crc = 0xffffffff;}
f5176
0
execute
public void kafkatest_f5184_0(int statusCode, String message)
{    Runtime.getRuntime().halt(statusCode);}
f5184
0
execute
public void kafkatest_f5185_0(int statusCode, String message)
{    System.exit(statusCode);}
f5185
0
exit
public static void kafkatest_f5186_0(int statusCode)
{    exit(statusCode, null);}
f5186
0
removeEldestEntry
protected boolean kafkatest_f5194_0(final Map.Entry<K, V> eldest)
{    return false;}
f5194
0
remove
public V kafkatest_f5195_0(final Object key)
{    throw new UnsupportedOperationException("Removing from registeredStores is not allowed");}
f5195
0
remove
public boolean kafkatest_f5196_0(final Object key, final Object value)
{    throw new UnsupportedOperationException("Removing from registeredStores is not allowed");}
f5196
0
addToListTail
private static void kafkatest_f5204_0(Element head, Element[] elements, int elementIdx)
{    int oldTailIdx = head.prev();    Element element = indexToElement(head, elements, elementIdx);    Element oldTail = indexToElement(head, elements, oldTailIdx);    head.setPrev(elementIdx);    oldTail.setNext(elementIdx);    element.setPrev(oldTailIdx);    element.setNext(HEAD_INDEX);}
f5204
0
removeFromList
private static void kafkatest_f5205_0(Element head, Element[] elements, int elementIdx)
{    Element element = indexToElement(head, elements, elementIdx);    elements[elementIdx] = null;    int prevIdx = element.prev();    int nextIdx = element.next();    Element prev = indexToElement(head, elements, prevIdx);    Element next = indexToElement(head, elements, nextIdx);    prev.setNext(nextIdx);    next.setPrev(prevIdx);    element.setNext(INVALID_INDEX);    element.setPrev(INVALID_INDEX);}
f5205
0
hasNext
public boolean kafkatest_f5206_0()
{    return cursor != size;}
f5206
0
add
public void kafkatest_f5214_0(E e)
{    throw new UnsupportedOperationException();}
f5214
0
listIterator
public ListIterator<E> kafkatest_f5215_0(int index)
{    if (index < 0 || index > size) {        throw new IndexOutOfBoundsException();    }    return ImplicitLinkedHashCollection.this.listIterator(index);}
f5215
0
size
public int kafkatest_f5216_0()
{    return size;}
f5216
0
listIterator
private ListIterator<E> kafkatest_f5224_0(int index)
{    return new ImplicitLinkedHashCollectionIterator(index);}
f5224
0
slot
 final int kafkatest_f5225_0(Element[] curElements, Object e)
{    return (e.hashCode() & 0x7fffffff) % curElements.length;}
f5225
0
findIndexOfEqualElement
private final int kafkatest_f5226_0(Object key)
{    if (key == null || size == 0) {        return INVALID_INDEX;    }    int slot = slot(elements, key);    for (int seen = 0; seen < elements.length; seen++) {        Element element = elements[slot];        if (element == null) {            return INVALID_INDEX;        }        if (key.equals(element)) {            return slot;        }        slot = (slot + 1) % elements.length;    }    return INVALID_INDEX;}
f5226
0
changeCapacity
private void kafkatest_f5234_0(int newCapacity)
{    Element[] newElements = new Element[newCapacity];    HeadElement newHead = new HeadElement();    int oldSize = size;    for (Iterator<E> iter = iterator(); iter.hasNext(); ) {        Element element = iter.next();        iter.remove();        int newSlot = addInternal(element, newElements);        addToListTail(newHead, newElements, newSlot);    }    this.elements = newElements;    this.head = newHead;    this.size = oldSize;}
f5234
0
remove
public final boolean kafkatest_f5235_0(Object key)
{    int slot = findElementToRemove(key);    if (slot == INVALID_INDEX) {        return false;    }    removeElementAtSlot(slot);    return true;}
f5235
0
findElementToRemove
 int kafkatest_f5236_0(Object key)
{    return findIndexOfEqualElement(key);}
f5236
0
valuesList
public List<E> kafkatest_f5244_0()
{    return new ImplicitLinkedHashCollectionListView();}
f5244
0
valuesSet
public Set<E> kafkatest_f5245_0()
{    return new ImplicitLinkedHashCollectionSetView();}
f5245
0
addInternal
 int kafkatest_f5246_0(Element newElement, Element[] addElements)
{    int slot = slot(addElements, newElement);    for (int seen = 0; seen < addElements.length; seen++) {        Element element = addElements[slot];        if (element == null) {            addElements[slot] = newElement;            return slot;        }        if (element == newElement) {            return INVALID_INDEX;        }        slot = (slot + 1) % addElements.length;    }    throw new RuntimeException("Not enough hash table slots to add a new element.");}
f5246
0
nonDaemon
public static KafkaThread kafkatest_f5254_0(final String name, Runnable runnable)
{    return new KafkaThread(name, runnable, false);}
f5254
0
configureThread
private voidf5255_1final String name, boolean daemon)
{    setDaemon(daemon);    setUncaughtExceptionHandler(new UncaughtExceptionHandler() {        public void uncaughtException(Thread t, Throwable e) {                    }    });}
private voidf5255
1
uncaughtException
public voidf5256_1Thread t, Throwable e)
{    }
public voidf5256
1
isDebugEnabled
public boolean kafkatest_f5264_0(Marker marker)
{    return logger.isDebugEnabled(marker);}
f5264
0
isInfoEnabled
public boolean kafkatest_f5265_0()
{    return logger.isInfoEnabled();}
f5265
0
isInfoEnabled
public boolean kafkatest_f5266_0(Marker marker)
{    return logger.isInfoEnabled(marker);}
f5266
0
trace
public void kafkatest_f5274_0(String format, Object... args)
{    if (logger.isTraceEnabled()) {        writeLog(null, LocationAwareLogger.TRACE_INT, format, args, null);    }}
f5274
0
trace
public void kafkatest_f5275_0(String msg, Throwable t)
{    if (logger.isTraceEnabled()) {        writeLog(null, LocationAwareLogger.TRACE_INT, msg, null, t);    }}
f5275
0
trace
public void kafkatest_f5276_0(Marker marker, String msg)
{    if (logger.isTraceEnabled()) {        writeLog(marker, LocationAwareLogger.TRACE_INT, msg, null, null);    }}
f5276
0
debug
public voidf5284_1String format, Object... args)
{    if (logger.isDebugEnabled()) {        writeLog(null, LocationAware    }}
public voidf5284
1
debug
public voidf5285_1String msg, Throwable t)
{    if (logger.isDebugEnabled()) {        writeLog(null, LocationAware    }}
public voidf5285
1
debug
public voidf5286_1Marker marker, String msg)
{    if (logger.isDebugEnabled()) {        writeLog(marker, LocationAware    }}
public voidf5286
1
warn
public voidf5294_1String format, Object... args)
{    writeLog(null, LocationAware}
public voidf5294
1
warn
public voidf5295_1String msg, Throwable t)
{    writeLog(null, LocationAware}
public voidf5295
1
warn
public voidf5296_1Marker marker, String msg)
{    writeLog(marker, LocationAware}
public voidf5296
1
error
public voidf5304_1String format, Object... args)
{    writeLog(null, LocationAware}
public voidf5304
1
error
public voidf5305_1String msg, Throwable t)
{    writeLog(null, LocationAware}
public voidf5305
1
error
public voidf5306_1Marker marker, String msg)
{    writeLog(marker, LocationAware}
public voidf5306
1
info
public voidf5314_1String format, Object... args)
{    writeLog(null, LocationAware}
public voidf5314
1
info
public voidf5315_1String msg, Throwable t)
{    writeLog(null, LocationAware}
public voidf5315
1
info
public voidf5316_1Marker marker, String msg)
{    writeLog(marker, LocationAware}
public voidf5316
1
isTraceEnabled
public boolean kafkatest_f5324_0(Marker marker)
{    return logger.isTraceEnabled(marker);}
f5324
0
isDebugEnabled
public boolean kafkatest_f5325_0()
{    return logger.isDebugEnabled();}
f5325
0
isDebugEnabled
public boolean kafkatest_f5326_0(Marker marker)
{    return logger.isDebugEnabled(marker);}
f5326
0
trace
public void kafkatest_f5334_0(String message, Object arg)
{    if (logger.isTraceEnabled()) {        logger.trace(addPrefix(message), arg);    }}
f5334
0
trace
public void kafkatest_f5335_0(String message, Object arg1, Object arg2)
{    if (logger.isTraceEnabled()) {        logger.trace(addPrefix(message), arg1, arg2);    }}
f5335
0
trace
public void kafkatest_f5336_0(String message, Object... args)
{    if (logger.isTraceEnabled()) {        logger.trace(addPrefix(message), args);    }}
f5336
0
debug
public voidf5344_1String message, Object arg)
{    if (logger.isDebugEnabled()) {            }}
public voidf5344
1
debug
public voidf5345_1String message, Object arg1, Object arg2)
{    if (logger.isDebugEnabled()) {            }}
public voidf5345
1
debug
public voidf5346_1String message, Object... args)
{    if (logger.isDebugEnabled()) {            }}
public voidf5346
1
warn
public voidf5354_1String message, Object arg)
{    }
public voidf5354
1
warn
public voidf5355_1String message, Object arg1, Object arg2)
{    }
public voidf5355
1
warn
public voidf5356_1String message, Object... args)
{    }
public voidf5356
1
error
public voidf5364_1String message, Object arg)
{    }
public voidf5364
1
error
public voidf5365_1String message, Object arg1, Object arg2)
{    }
public voidf5365
1
error
public voidf5366_1String message, Object... args)
{    }
public voidf5366
1
info
public voidf5374_1String message, Object arg)
{    }
public voidf5374
1
info
public voidf5375_1String message, Object arg1, Object arg2)
{    }
public voidf5375
1
info
public voidf5376_1String message, Object... args)
{    }
public voidf5376
1
createSignalHandler
private Objectf5384_1final Map<String, Object> jvmSignalHandlers)
{    InvocationHandler invocationHandler = new InvocationHandler() {        private String getName(Object signal) throws ReflectiveOperationException {            return (String) signalGetNameMethod.invoke(signal);        }        private void handle(Object signalHandler, Object signal) throws ReflectiveOperationException {            signalHandlerHandleMethod.invoke(signalHandler, signal);        }        @Override        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {            Object signal = args[0];                        Object handler = jvmSignalHandlers.get(getName(signal));            if (handler != null)                handle(handler, signal);            return null;        }    };    return Proxy.newProxyInstance(Utils.getContextOrKafkaClassLoader(), new Class[] { signalHandlerClass }, invocationHandler);}
private Objectf5384
1
getName
private String kafkatest_f5385_0(Object signal) throws ReflectiveOperationException
{    return (String) signalGetNameMethod.invoke(signal);}
f5385
0
handle
private void kafkatest_f5386_0(Object signalHandler, Object signal) throws ReflectiveOperationException
{    signalHandlerHandleMethod.invoke(signalHandler, signal);}
f5386
0
ofInt
public static IntRef kafkatest_f5394_0(int value)
{    return new IntRef(value);}
f5394
0
getValue
public long kafkatest_f5395_0()
{    long ret = crc;    return (~ret) & 0xffffffffL;}
f5395
0
reset
public void kafkatest_f5396_0()
{    crc = 0xffffffff;}
f5396
0
resourceType
public static ResourceType kafkatest_f5404_0(String name)
{    ResourceType resourceType = NAME_TO_RESOURCE_TYPES.get(name);    return resourceType == null ? ResourceType.UNKNOWN : resourceType;}
f5404
0
operation
public static AclOperation kafkatest_f5405_0(String name)
{    AclOperation operation = NAME_TO_OPERATIONS.get(name);    return operation == null ? AclOperation.UNKNOWN : operation;}
f5405
0
toPascalCase
private static String kafkatest_f5406_0(String name)
{    StringBuilder builder = new StringBuilder();    boolean capitalizeNext = true;    for (char c : name.toCharArray()) {        if (c == '_')            capitalizeNext = true;        else if (capitalizeNext) {            builder.append(Character.toUpperCase(c));            capitalizeNext = false;        } else            builder.append(Character.toLowerCase(c));    }    return builder.toString();}
f5406
0
execString
protected String[] kafkatest_f5414_0()
{    return command;}
f5414
0
parseExecResult
protected void kafkatest_f5415_0(BufferedReader reader) throws IOException
{    output = new StringBuffer();    char[] buf = new char[512];    int nRead;    while ((nRead = reader.read(buf, 0, buf.length)) > 0) {        output.append(buf, 0, nRead);    }}
f5415
0
output
public String kafkatest_f5416_0()
{    return (output == null) ? "" : output.toString();}
f5416
0
nanoseconds
public long kafkatest_f5424_0()
{    return System.nanoTime();}
f5424
0
sleep
public void kafkatest_f5425_0(long ms)
{    Utils.sleep(ms);}
f5425
0
waitObject
public void kafkatest_f5426_0(Object obj, Supplier<Boolean> condition, long deadlineMs) throws InterruptedException
{    synchronized (obj) {        while (true) {            if (condition.get())                return;            long currentTimeMs = milliseconds();            if (currentTimeMs >= deadlineMs)                throw new TimeoutException("Condition not satisfied before deadline");            obj.wait(deadlineMs - currentTimeMs);        }    }}
f5426
0
update
public void kafkatest_f5434_0()
{    update(time.milliseconds());}
f5434
0
update
public void kafkatest_f5435_0(long currentTimeMs)
{    this.currentTimeMs = Math.max(currentTimeMs, this.currentTimeMs);}
f5435
0
remainingMs
public long kafkatest_f5436_0()
{    return Math.max(0, deadlineMs - currentTimeMs);}
f5436
0
utf8
public static String kafkatest_f5444_0(ByteBuffer buffer, int offset, int length)
{    if (buffer.hasArray())        return new String(buffer.array(), buffer.arrayOffset() + buffer.position() + offset, length, StandardCharsets.UTF_8);    else        return utf8(toArray(buffer, offset, length));}
f5444
0
utf8
public static byte[] kafkatest_f5445_0(String string)
{    return string.getBytes(StandardCharsets.UTF_8);}
f5445
0
abs
public static int kafkatest_f5446_0(int n)
{    return (n == Integer.MIN_VALUE) ? 0 : Math.abs(n);}
f5446
0
wrapNullable
public static ByteBuffer kafkatest_f5454_0(byte[] array)
{    return array == null ? null : ByteBuffer.wrap(array);}
f5454
0
toArray
public static byte[] kafkatest_f5455_0(ByteBuffer buffer, int offset, int size)
{    byte[] dest = new byte[size];    if (buffer.hasArray()) {        System.arraycopy(buffer.array(), buffer.position() + buffer.arrayOffset() + offset, dest, 0, size);    } else {        int pos = buffer.position();        buffer.position(pos + offset);        buffer.get(dest);        buffer.position(pos);    }    return dest;}
f5455
0
copyArray
public static byte[] kafkatest_f5456_0(byte[] src)
{    return Arrays.copyOf(src, src.length);}
f5456
0
getPort
public static Integer kafkatest_f5464_0(String address)
{    Matcher matcher = HOST_PORT_PATTERN.matcher(address);    return matcher.matches() ? Integer.parseInt(matcher.group(2)) : null;}
f5464
0
validHostPattern
public static boolean kafkatest_f5465_0(String address)
{    return VALID_HOST_CHARACTERS.matcher(address).matches();}
f5465
0
formatAddress
public static String kafkatest_f5466_0(String host, Integer port)
{    return host.contains(":") ? // IPv6    "[" + host + "]:" + port : host + ":" + port;}
f5466
0
stackTrace
public static String kafkatest_f5474_0(Throwable e)
{    StringWriter sw = new StringWriter();    PrintWriter pw = new PrintWriter(sw);    e.printStackTrace(pw);    return sw.toString();}
f5474
0
readBytes
public static byte[] kafkatest_f5475_0(ByteBuffer buffer, int offset, int length)
{    byte[] dest = new byte[length];    if (buffer.hasArray()) {        System.arraycopy(buffer.array(), buffer.arrayOffset() + offset, dest, 0, length);    } else {        buffer.mark();        buffer.position(offset);        buffer.get(dest, 0, length);        buffer.reset();    }    return dest;}
f5475
0
readBytes
public static byte[] kafkatest_f5476_0(ByteBuffer buffer)
{    return Utils.readBytes(buffer, 0, buffer.limit());}
f5476
0
setValue
public V kafkatest_f5484_0(final V value)
{    throw new UnsupportedOperationException();}
f5484
0
mkMap
public static Map<K, V> kafkatest_f5485_0(final Map.Entry<K, V>... entries)
{    final LinkedHashMap<K, V> result = new LinkedHashMap<>();    for (final Map.Entry<K, V> entry : entries) {        result.put(entry.getKey(), entry.getValue());    }    return result;}
f5485
0
mkProperties
public static Properties kafkatest_f5486_0(final Map<String, String> properties)
{    final Properties result = new Properties();    for (final Map.Entry<String, String> entry : properties.entrySet()) {        result.setProperty(entry.getKey(), entry.getValue());    }    return result;}
f5486
0
atomicMoveWithFallback
public static voidf5494_1Path source, Path target) throws IOException
{    try {        Files.move(source, target, StandardCopyOption.ATOMIC_MOVE);    } catch (IOException outer) {        try {            Files.move(source, target, StandardCopyOption.REPLACE_EXISTING);                    } catch (IOException inner) {            inner.addSuppressed(outer);            throw inner;        }    }}
public static voidf5494
1
closeAll
public static void kafkatest_f5495_0(Closeable... closeables) throws IOException
{    IOException exception = null;    for (Closeable closeable : closeables) {        try {            if (closeable != null)                closeable.close();        } catch (IOException e) {            if (exception != null)                exception.addSuppressed(e);            else                exception = e;        }    }    if (exception != null)        throw exception;}
f5495
0
closeQuietly
public static voidf5496_1AutoCloseable closeable, String name)
{    if (closeable != null) {        try {            closeable.close();        } catch (Throwable t) {                    }    }}
public static voidf5496
1
writeTo
public static void kafkatest_f5504_0(DataOutput out, ByteBuffer buffer, int length) throws IOException
{    if (buffer.hasArray()) {        out.write(buffer.array(), buffer.position() + buffer.arrayOffset(), length);    } else {        int pos = buffer.position();        for (int i = pos; i < length + pos; i++) out.writeByte(buffer.get(i));    }}
f5504
0
toList
public static List<T> kafkatest_f5505_0(Iterator<T> iterator)
{    List<T> res = new ArrayList<>();    while (iterator.hasNext()) res.add(iterator.next());    return res;}
f5505
0
concatListsUnmodifiable
public static List<T> kafkatest_f5506_0(List<T> left, List<T> right)
{    return concatLists(left, right, Collections::unmodifiableList);}
f5506
0
aclBindingDeleteResults
public Collection<AclBindingDeleteResult> kafkatest_f5514_0()
{    return aclBindingDeleteResults;}
f5514
0
aclBinding
public AclBinding kafkatest_f5515_0()
{    return aclBinding;}
f5515
0
exception
public Optional<ApiException> kafkatest_f5516_0()
{    return exception == null ? Optional.empty() : Optional.of(exception);}
f5516
0
toString
public String kafkatest_f5524_0()
{    return "Action(" + ", resourcePattern='" + resourcePattern + '\'' + ", operation='" + operation + '\'' + ", resourceReferenceCount='" + resourceReferenceCount + '\'' + ", logIfAllowed='" + logIfAllowed + '\'' + ", logIfDenied='" + logIfDenied + '\'' + ')';}
f5524
0
configs
public Map<String, String> kafkatest_f5525_0()
{    return configs;}
f5525
0
resource
public ConfigResource kafkatest_f5526_0()
{    return resource;}
f5526
0
fetchNodes
public List<Node> kafkatest_f5534_0()
{    return cluster.nodes();}
f5534
0
isUpdateNeeded
public boolean kafkatest_f5535_0()
{    return false;}
f5535
0
update
public void kafkatest_f5536_0(Time time, MockClient.MetadataUpdate update)
{    throw new UnsupportedOperationException();}
f5536
0
setUp
public void kafkatest_f5544_0()
{    final Collection<ConfigEntry> entries = new ArrayList<>();    entries.add(E1);    entries.add(E2);    config = new Config(entries);}
f5544
0
shouldGetEntry
public void kafkatest_f5545_0()
{    assertThat(config.get("a"), is(E1));    assertThat(config.get("c"), is(E2));}
f5545
0
shouldReturnNullOnGetUnknownEntry
public void kafkatest_f5546_0()
{    assertThat(config.get("unknown"), is(nullValue()));}
f5546
0
mockCluster
private static Cluster kafkatest_f5554_0()
{    HashMap<Integer, Node> nodes = new HashMap<>();    nodes.put(0, new Node(0, "localhost", 8121));    nodes.put(1, new Node(1, "localhost", 8122));    nodes.put(2, new Node(2, "localhost", 8123));    return new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptySet(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));}
f5554
0
testGetOrCreateListValue
public void kafkatest_f5555_0()
{    Map<String, List<String>> map = new HashMap<>();    List<String> fooList = KafkaAdminClient.getOrCreateListValue(map, "foo");    assertNotNull(fooList);    fooList.add("a");    fooList.add("b");    List<String> fooList2 = KafkaAdminClient.getOrCreateListValue(map, "foo");    assertEquals(fooList, fooList2);    assertTrue(fooList2.contains("a"));    assertTrue(fooList2.contains("b"));    List<String> barList = KafkaAdminClient.getOrCreateListValue(map, "bar");    assertNotNull(barList);    assertTrue(barList.isEmpty());}
f5555
0
testCalcTimeoutMsRemainingAsInt
public void kafkatest_f5556_0()
{    assertEquals(0, KafkaAdminClient.calcTimeoutMsRemainingAsInt(1000, 1000));    assertEquals(100, KafkaAdminClient.calcTimeoutMsRemainingAsInt(1000, 1100));    assertEquals(Integer.MAX_VALUE, KafkaAdminClient.calcTimeoutMsRemainingAsInt(0, Long.MAX_VALUE));    assertEquals(Integer.MIN_VALUE, KafkaAdminClient.calcTimeoutMsRemainingAsInt(Long.MAX_VALUE, 0));}
f5556
0
testCloseAdminClient
public void kafkatest_f5564_0()
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {    }}
f5564
0
prepareOffsetDeleteResponse
private static OffsetDeleteResponse kafkatest_f5565_0(Errors error)
{    return new OffsetDeleteResponse(new OffsetDeleteResponseData().setErrorCode(error.code()).setTopics(new OffsetDeleteResponseTopicCollection()));}
f5565
0
prepareOffsetDeleteResponse
private static OffsetDeleteResponse kafkatest_f5566_0(String topic, int partition, Errors error)
{    return new OffsetDeleteResponse(new OffsetDeleteResponseData().setErrorCode(Errors.NONE.code()).setTopics(new OffsetDeleteResponseTopicCollection(Stream.of(new OffsetDeleteResponseTopic().setName(topic).setPartitions(new OffsetDeleteResponsePartitionCollection(Collections.singletonList(new OffsetDeleteResponsePartition().setPartitionIndex(partition).setErrorCode(error.code())).iterator()))).collect(Collectors.toList()).iterator())));}
f5566
0
testCreateTopics
public void kafkatest_f5574_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(body -> body instanceof CreateTopicsRequest, prepareCreateTopicsResponse("myTopic", Errors.NONE));        KafkaFuture<Void> future = env.adminClient().createTopics(Collections.singleton(new NewTopic("myTopic", Collections.singletonMap(0, asList(0, 1, 2)))), new CreateTopicsOptions().timeoutMs(10000)).all();        future.get();    }}
f5574
0
testCreateTopicsRetryBackoff
public void kafkatest_f5575_0() throws Exception
{    Cluster cluster = mockCluster(0);    MockTime time = new MockTime();    int retryBackoff = 100;    try (final AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(time, cluster, newStrMap(AdminClientConfig.RETRY_BACKOFF_MS_CONFIG, "" + retryBackoff))) {        MockClient mockClient = env.kafkaClient();        mockClient.setNodeApiVersions(NodeApiVersions.create());        AtomicLong firstAttemptTime = new AtomicLong(0);        AtomicLong secondAttemptTime = new AtomicLong(0);        mockClient.prepareResponse(body -> {            firstAttemptTime.set(time.milliseconds());            return body instanceof CreateTopicsRequest;        }, null, true);        mockClient.prepareResponse(body -> {            secondAttemptTime.set(time.milliseconds());            return body instanceof CreateTopicsRequest;        }, prepareCreateTopicsResponse("myTopic", Errors.NONE));        KafkaFuture<Void> future = env.adminClient().createTopics(Collections.singleton(new NewTopic("myTopic", Collections.singletonMap(0, asList(0, 1, 2)))), new CreateTopicsOptions().timeoutMs(10000)).all();        // Wait until the first attempt has failed, then advance the time        TestUtils.waitForCondition(() -> mockClient.numAwaitingResponses() == 1, "Failed awaiting CreateTopics first request failure");        // Wait until the retry call added to the queue in AdminClient        TestUtils.waitForCondition(() -> ((KafkaAdminClient) env.adminClient()).numPendingCalls() == 1, "Failed to add retry CreateTopics call");        time.sleep(retryBackoff);        future.get();        long actualRetryBackoff = secondAttemptTime.get() - firstAttemptTime.get();        assertEquals("CreateTopics retry did not await expected backoff", retryBackoff, actualRetryBackoff);    }}
f5575
0
testCreateTopicsHandleNotControllerException
public void kafkatest_f5576_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponseFrom(prepareCreateTopicsResponse("myTopic", Errors.NOT_CONTROLLER), env.cluster().nodeById(0));        env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(), env.cluster().clusterResource().clusterId(), 1, Collections.<MetadataResponse.TopicMetadata>emptyList()));        env.kafkaClient().prepareResponseFrom(prepareCreateTopicsResponse("myTopic", Errors.NONE), env.cluster().nodeById(1));        KafkaFuture<Void> future = env.adminClient().createTopics(Collections.singleton(new NewTopic("myTopic", Collections.singletonMap(0, asList(0, 1, 2)))), new CreateTopicsOptions().timeoutMs(10000)).all();        future.get();    }}
f5576
0
testDeleteAcls
public void kafkatest_f5584_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Test a call where one filter has an error.        env.kafkaClient().prepareResponse(new DeleteAclsResponse(0, asList(new AclFilterResponse(asList(new AclDeletionResult(ACL1), new AclDeletionResult(ACL2))), new AclFilterResponse(new ApiError(Errors.SECURITY_DISABLED, "No security"), Collections.<AclDeletionResult>emptySet()))));        DeleteAclsResult results = env.adminClient().deleteAcls(asList(FILTER1, FILTER2));        Map<AclBindingFilter, KafkaFuture<FilterResults>> filterResults = results.values();        FilterResults filter1Results = filterResults.get(FILTER1).get();        assertEquals(null, filter1Results.values().get(0).exception());        assertEquals(ACL1, filter1Results.values().get(0).binding());        assertEquals(null, filter1Results.values().get(1).exception());        assertEquals(ACL2, filter1Results.values().get(1).binding());        TestUtils.assertFutureError(filterResults.get(FILTER2), SecurityDisabledException.class);        TestUtils.assertFutureError(results.all(), SecurityDisabledException.class);        // Test a call where one deletion result has an error.        env.kafkaClient().prepareResponse(new DeleteAclsResponse(0, asList(new AclFilterResponse(asList(new AclDeletionResult(ACL1), new AclDeletionResult(new ApiError(Errors.SECURITY_DISABLED, "No security"), ACL2))), new AclFilterResponse(Collections.<AclDeletionResult>emptySet()))));        results = env.adminClient().deleteAcls(asList(FILTER1, FILTER2));        assertTrue(results.values().get(FILTER2).get().values().isEmpty());        TestUtils.assertFutureError(results.all(), SecurityDisabledException.class);        // Test a call where there are no errors.        env.kafkaClient().prepareResponse(new DeleteAclsResponse(0, asList(new AclFilterResponse(asList(new AclDeletionResult(ACL1))), new AclFilterResponse(asList(new AclDeletionResult(ACL2))))));        results = env.adminClient().deleteAcls(asList(FILTER1, FILTER2));        Collection<AclBinding> deleted = results.all().get();        assertCollectionIs(deleted, ACL1, ACL2);    }}
f5584
0
testElectLeaders
public void kafkatest_f5585_0() throws Exception
{    TopicPartition topic1 = new TopicPartition("topic", 0);    TopicPartition topic2 = new TopicPartition("topic", 2);    try (AdminClientUnitTestEnv env = mockClientEnv()) {        for (ElectionType electionType : ElectionType.values()) {            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());            // Test a call where one partition has an error.            ApiError value = ApiError.fromThrowable(new ClusterAuthorizationException(null));            List<ReplicaElectionResult> electionResults = new ArrayList<>();            ReplicaElectionResult electionResult = new ReplicaElectionResult();            electionResult.setTopic(topic1.topic());            // Add partition 1 result            PartitionResult partition1Result = new PartitionResult();            partition1Result.setPartitionId(topic1.partition());            partition1Result.setErrorCode(value.error().code());            partition1Result.setErrorMessage(value.message());            electionResult.partitionResult().add(partition1Result);            // Add partition 2 result            PartitionResult partition2Result = new PartitionResult();            partition2Result.setPartitionId(topic2.partition());            partition2Result.setErrorCode(value.error().code());            partition2Result.setErrorMessage(value.message());            electionResult.partitionResult().add(partition2Result);            electionResults.add(electionResult);            env.kafkaClient().prepareResponse(new ElectLeadersResponse(0, Errors.NONE.code(), electionResults));            ElectLeadersResult results = env.adminClient().electLeaders(electionType, new HashSet<>(asList(topic1, topic2)));            assertEquals(results.partitions().get().get(topic2).get().getClass(), ClusterAuthorizationException.class);            // Test a call where there are no errors. By mutating the internal of election results            partition1Result.setErrorCode(ApiError.NONE.error().code());            partition1Result.setErrorMessage(ApiError.NONE.message());            partition2Result.setErrorCode(ApiError.NONE.error().code());            partition2Result.setErrorMessage(ApiError.NONE.message());            env.kafkaClient().prepareResponse(new ElectLeadersResponse(0, Errors.NONE.code(), electionResults));            results = env.adminClient().electLeaders(electionType, new HashSet<>(asList(topic1, topic2)));            assertFalse(results.partitions().get().get(topic1).isPresent());            assertFalse(results.partitions().get().get(topic2).isPresent());            // Now try a timeout            results = env.adminClient().electLeaders(electionType, new HashSet<>(asList(topic1, topic2)), new ElectLeadersOptions().timeoutMs(100));            TestUtils.assertFutureError(results.partitions(), TimeoutException.class);        }    }}
f5585
0
testHandleTimeout
public voidf5586_1) throws Exception
{    HashMap<Integer, Node> nodes = new HashMap<>();    MockTime time = new MockTime();    nodes.put(0, new Node(0, "localhost", 8121));    Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptySet(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(time, cluster, AdminClientConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG, "1", AdminClientConfig.RECONNECT_BACKOFF_MS_CONFIG, "1")) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        assertEquals(time, env.time());        assertEquals(env.time(), ((KafkaAdminClient) env.adminClient()).time());        // Make a request with an extremely short timeout.        // Then wait for it to fail by not supplying any response.                final ListTopicsResult result = env.adminClient().listTopics(new ListTopicsOptions().timeoutMs(1000));        TestUtils.waitForCondition(new TestCondition() {            @Override            public boolean conditionMet() {                return env.kafkaClient().hasInFlightRequests();            }        }, "Timed out waiting for inFlightRequests");        time.sleep(5000);        TestUtils.waitForCondition(new TestCondition() {            @Override            public boolean conditionMet() {                return result.listings().isDone();            }        }, "Timed out waiting for listTopics to complete");        TestUtils.assertFutureError(result.listings(), TimeoutException.class);                // The next request should succeed.        time.sleep(5000);        env.kafkaClient().prepareResponse(new DescribeConfigsResponse(0, Collections.singletonMap(new ConfigResource(ConfigResource.Type.TOPIC, "foo"), new DescribeConfigsResponse.Config(ApiError.NONE, Collections.emptySet()))));        DescribeConfigsResult result2 = env.adminClient().describeConfigs(Collections.singleton(new ConfigResource(ConfigResource.Type.TOPIC, "foo")));        time.sleep(5000);        result2.values().get(new ConfigResource(ConfigResource.Type.TOPIC, "foo")).get();    }}
public voidf5586
1
testListConsumerGroupsMetadataFailure
public void kafkatest_f5594_0() throws Exception
{    final HashMap<Integer, Node> nodes = new HashMap<>();    Node node0 = new Node(0, "localhost", 8121);    Node node1 = new Node(1, "localhost", 8122);    Node node2 = new Node(2, "localhost", 8123);    nodes.put(0, node0);    nodes.put(1, node1);    nodes.put(2, node2);    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.emptyList(), Collections.emptySet(), Collections.emptySet(), nodes.get(0));    final Time time = new MockTime();    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(time, cluster, AdminClientConfig.RETRIES_CONFIG, "0")) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Empty metadata causes the request to fail since we have no list of brokers        // to send the ListGroups requests to        env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(Collections.emptyList(), env.cluster().clusterResource().clusterId(), -1, Collections.emptyList()));        final ListConsumerGroupsResult result = env.adminClient().listConsumerGroups();        TestUtils.assertFutureError(result.all(), KafkaException.class);    }}
f5594
0
testDescribeConsumerGroups
public void kafkatest_f5595_0() throws Exception
{    final HashMap<Integer, Node> nodes = new HashMap<>();    nodes.put(0, new Node(0, "localhost", 8121));    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptyList(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Retriable FindCoordinatorResponse errors should be retried        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.COORDINATOR_NOT_AVAILABLE, Node.noNode()));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.COORDINATOR_LOAD_IN_PROGRESS, Node.noNode()));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.NONE, env.cluster().controller()));        DescribeGroupsResponseData data = new DescribeGroupsResponseData();        // Retriable errors should be retried        data.groups().add(DescribeGroupsResponse.groupMetadata("group-0", Errors.COORDINATOR_LOAD_IN_PROGRESS, "", "", "", Collections.emptyList(), Collections.emptySet()));        env.kafkaClient().prepareResponse(new DescribeGroupsResponse(data));        data = new DescribeGroupsResponseData();        data.groups().add(DescribeGroupsResponse.groupMetadata("group-0", Errors.COORDINATOR_NOT_AVAILABLE, "", "", "", Collections.emptyList(), Collections.emptySet()));        env.kafkaClient().prepareResponse(new DescribeGroupsResponse(data));        /*             * We need to return two responses here, one with NOT_COORDINATOR error when calling describe consumer group             * api using coordinator that has moved. This will retry whole operation. So we need to again respond with a             * FindCoordinatorResponse.             */        data = new DescribeGroupsResponseData();        data.groups().add(DescribeGroupsResponse.groupMetadata("group-0", Errors.NOT_COORDINATOR, "", "", "", Collections.emptyList(), Collections.emptySet()));        env.kafkaClient().prepareResponse(new DescribeGroupsResponse(data));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.NONE, env.cluster().controller()));        data = new DescribeGroupsResponseData();        TopicPartition myTopicPartition0 = new TopicPartition("my_topic", 0);        TopicPartition myTopicPartition1 = new TopicPartition("my_topic", 1);        TopicPartition myTopicPartition2 = new TopicPartition("my_topic", 2);        final List<TopicPartition> topicPartitions = new ArrayList<>();        topicPartitions.add(0, myTopicPartition0);        topicPartitions.add(1, myTopicPartition1);        topicPartitions.add(2, myTopicPartition2);        final ByteBuffer memberAssignment = ConsumerProtocol.serializeAssignment(new ConsumerPartitionAssignor.Assignment(topicPartitions));        byte[] memberAssignmentBytes = new byte[memberAssignment.remaining()];        memberAssignment.get(memberAssignmentBytes);        DescribedGroupMember memberOne = DescribeGroupsResponse.groupMember("0", "instance1", "clientId0", "clientHost", memberAssignmentBytes, null);        DescribedGroupMember memberTwo = DescribeGroupsResponse.groupMember("1", "instance2", "clientId1", "clientHost", memberAssignmentBytes, null);        List<MemberDescription> expectedMemberDescriptions = new ArrayList<>();        expectedMemberDescriptions.add(convertToMemberDescriptions(memberOne, new MemberAssignment(new HashSet<>(topicPartitions))));        expectedMemberDescriptions.add(convertToMemberDescriptions(memberTwo, new MemberAssignment(new HashSet<>(topicPartitions))));        data.groups().add(DescribeGroupsResponse.groupMetadata("group-0", Errors.NONE, "", ConsumerProtocol.PROTOCOL_TYPE, "", asList(memberOne, memberTwo), Collections.emptySet()));        env.kafkaClient().prepareResponse(new DescribeGroupsResponse(data));        final DescribeConsumerGroupsResult result = env.adminClient().describeConsumerGroups(singletonList("group-0"));        final ConsumerGroupDescription groupDescription = result.describedGroups().get("group-0").get();        assertEquals(1, result.describedGroups().size());        assertEquals("group-0", groupDescription.groupId());        assertEquals(2, groupDescription.members().size());        assertEquals(expectedMemberDescriptions, groupDescription.members());    }}
f5595
0
testDescribeMultipleConsumerGroups
public void kafkatest_f5596_0() throws Exception
{    final HashMap<Integer, Node> nodes = new HashMap<>();    nodes.put(0, new Node(0, "localhost", 8121));    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptyList(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.NONE, env.cluster().controller()));        TopicPartition myTopicPartition0 = new TopicPartition("my_topic", 0);        TopicPartition myTopicPartition1 = new TopicPartition("my_topic", 1);        TopicPartition myTopicPartition2 = new TopicPartition("my_topic", 2);        final List<TopicPartition> topicPartitions = new ArrayList<>();        topicPartitions.add(0, myTopicPartition0);        topicPartitions.add(1, myTopicPartition1);        topicPartitions.add(2, myTopicPartition2);        final ByteBuffer memberAssignment = ConsumerProtocol.serializeAssignment(new ConsumerPartitionAssignor.Assignment(topicPartitions));        byte[] memberAssignmentBytes = new byte[memberAssignment.remaining()];        memberAssignment.get(memberAssignmentBytes);        DescribeGroupsResponseData group0Data = new DescribeGroupsResponseData();        group0Data.groups().add(DescribeGroupsResponse.groupMetadata("group-0", Errors.NONE, "", ConsumerProtocol.PROTOCOL_TYPE, "", asList(DescribeGroupsResponse.groupMember("0", null, "clientId0", "clientHost", memberAssignmentBytes, null), DescribeGroupsResponse.groupMember("1", null, "clientId1", "clientHost", memberAssignmentBytes, null)), Collections.emptySet()));        DescribeGroupsResponseData groupConnectData = new DescribeGroupsResponseData();        group0Data.groups().add(DescribeGroupsResponse.groupMetadata("group-connect-0", Errors.NONE, "", "connect", "", asList(DescribeGroupsResponse.groupMember("0", null, "clientId0", "clientHost", memberAssignmentBytes, null), DescribeGroupsResponse.groupMember("1", null, "clientId1", "clientHost", memberAssignmentBytes, null)), Collections.emptySet()));        env.kafkaClient().prepareResponse(new DescribeGroupsResponse(group0Data));        env.kafkaClient().prepareResponse(new DescribeGroupsResponse(groupConnectData));        Collection<String> groups = new HashSet<>();        groups.add("group-0");        groups.add("group-connect-0");        final DescribeConsumerGroupsResult result = env.adminClient().describeConsumerGroups(groups);        assertEquals(2, result.describedGroups().size());        assertEquals(groups, result.describedGroups().keySet());    }}
f5596
0
testDeleteConsumerGroupOffsetsFindCoordinatorNonRetriableErrors
public void kafkatest_f5604_0() throws Exception
{    // Non-retriable FindCoordinatorResponse errors throw an exception    final Map<Integer, Node> nodes = new HashMap<>();    nodes.put(0, new Node(0, "localhost", 8121));    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptyList(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));    final String groupId = "group-0";    final TopicPartition tp1 = new TopicPartition("foo", 0);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.GROUP_AUTHORIZATION_FAILED, Node.noNode()));        final DeleteConsumerGroupOffsetsResult errorResult = env.adminClient().deleteConsumerGroupOffsets(groupId, Stream.of(tp1).collect(Collectors.toSet()));        TestUtils.assertFutureError(errorResult.all(), GroupAuthorizationException.class);        TestUtils.assertFutureError(errorResult.partitionResult(tp1), GroupAuthorizationException.class);    }}
f5604
0
testIncrementalAlterConfigs
public void kafkatest_f5605_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // test error scenarios        IncrementalAlterConfigsResponseData responseData = new IncrementalAlterConfigsResponseData();        responseData.responses().add(new AlterConfigsResourceResponse().setResourceName("").setResourceType(ConfigResource.Type.BROKER.id()).setErrorCode(Errors.CLUSTER_AUTHORIZATION_FAILED.code()).setErrorMessage("authorization error"));        responseData.responses().add(new AlterConfigsResourceResponse().setResourceName("topic1").setResourceType(ConfigResource.Type.TOPIC.id()).setErrorCode(Errors.INVALID_REQUEST.code()).setErrorMessage("Config value append is not allowed for config"));        env.kafkaClient().prepareResponse(new IncrementalAlterConfigsResponse(responseData));        ConfigResource brokerResource = new ConfigResource(ConfigResource.Type.BROKER, "");        ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, "topic1");        AlterConfigOp alterConfigOp1 = new AlterConfigOp(new ConfigEntry("log.segment.bytes", "1073741"), AlterConfigOp.OpType.SET);        AlterConfigOp alterConfigOp2 = new AlterConfigOp(new ConfigEntry("compression.type", "gzip"), AlterConfigOp.OpType.APPEND);        final Map<ConfigResource, Collection<AlterConfigOp>> configs = new HashMap<>();        configs.put(brokerResource, singletonList(alterConfigOp1));        configs.put(topicResource, singletonList(alterConfigOp2));        AlterConfigsResult result = env.adminClient().incrementalAlterConfigs(configs);        TestUtils.assertFutureError(result.values().get(brokerResource), ClusterAuthorizationException.class);        TestUtils.assertFutureError(result.values().get(topicResource), InvalidRequestException.class);        // Test a call where there are no errors.        responseData = new IncrementalAlterConfigsResponseData();        responseData.responses().add(new AlterConfigsResourceResponse().setResourceName("").setResourceType(ConfigResource.Type.BROKER.id()).setErrorCode(Errors.NONE.code()).setErrorMessage(ApiError.NONE.message()));        env.kafkaClient().prepareResponse(new IncrementalAlterConfigsResponse(responseData));        env.adminClient().incrementalAlterConfigs(Collections.singletonMap(brokerResource, singletonList(alterConfigOp1))).all().get();    }}
f5605
0
testRemoveMembersFromGroup
public void kafkatest_f5606_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        final String instanceOne = "instance-1";        final String instanceTwo = "instance-2";        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        MemberResponse responseOne = new MemberResponse().setGroupInstanceId(instanceOne).setErrorCode(Errors.UNKNOWN_MEMBER_ID.code());        MemberResponse responseTwo = new MemberResponse().setGroupInstanceId(instanceTwo).setErrorCode(Errors.NONE.code());        List<MemberResponse> memberResponses = Arrays.asList(responseOne, responseTwo);        // Retriable FindCoordinatorResponse errors should be retried        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.COORDINATOR_NOT_AVAILABLE, Node.noNode()));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.COORDINATOR_LOAD_IN_PROGRESS, Node.noNode()));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.NONE, env.cluster().controller()));        // Retriable errors should be retried        env.kafkaClient().prepareResponse(null, true);        env.kafkaClient().prepareResponse(new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.COORDINATOR_NOT_AVAILABLE.code())));        env.kafkaClient().prepareResponse(new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.COORDINATOR_LOAD_IN_PROGRESS.code())));        // Inject a top-level non-retriable error        env.kafkaClient().prepareResponse(new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.UNKNOWN_SERVER_ERROR.code())));        String groupId = "groupId";        List<String> membersToRemove = Arrays.asList(instanceOne, instanceTwo);        final MembershipChangeResult unknownErrorResult = env.adminClient().removeMemberFromConsumerGroup(groupId, new RemoveMemberFromConsumerGroupOptions(membersToRemove));        RemoveMemberFromGroupResult result = unknownErrorResult.all();        assertTrue(result.hasError());        assertEquals(Errors.UNKNOWN_SERVER_ERROR, result.topLevelError());        Map<MemberIdentity, KafkaFuture<Void>> memberFutures = result.memberFutures();        assertEquals(2, memberFutures.size());        for (Map.Entry<MemberIdentity, KafkaFuture<Void>> entry : memberFutures.entrySet()) {            KafkaFuture<Void> memberFuture = entry.getValue();            assertTrue(memberFuture.isCompletedExceptionally());            try {                memberFuture.get();                fail("get() should throw exception");            } catch (ExecutionException | InterruptedException e0) {                assertTrue(e0.getCause() instanceof UnknownServerException);            }        }        // Inject one member level error.        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.NONE, env.cluster().controller()));        env.kafkaClient().prepareResponse(new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.NONE.code()).setMembers(memberResponses)));        final MembershipChangeResult memberLevelErrorResult = env.adminClient().removeMemberFromConsumerGroup(groupId, new RemoveMemberFromConsumerGroupOptions(membersToRemove));        result = memberLevelErrorResult.all();        assertTrue(result.hasError());        assertEquals(Errors.NONE, result.topLevelError());        memberFutures = result.memberFutures();        assertEquals(2, memberFutures.size());        for (Map.Entry<MemberIdentity, KafkaFuture<Void>> entry : memberFutures.entrySet()) {            KafkaFuture<Void> memberFuture = entry.getValue();            if (entry.getKey().groupInstanceId().equals(instanceOne)) {                try {                    memberFuture.get();                    fail("get() should throw ExecutionException");                } catch (ExecutionException | InterruptedException e0) {                    assertTrue(e0.getCause() instanceof UnknownMemberIdException);                }            } else {                assertFalse(memberFuture.isCompletedExceptionally());            }        }        // Return success.        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.NONE, env.cluster().controller()));        env.kafkaClient().prepareResponse(new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.NONE.code()).setMembers(Collections.singletonList(responseTwo))));        final MembershipChangeResult noErrorResult = env.adminClient().removeMemberFromConsumerGroup(groupId, new RemoveMemberFromConsumerGroupOptions(membersToRemove));        result = noErrorResult.all();        assertFalse(result.hasError());        assertEquals(Errors.NONE, result.topLevelError());        memberFutures = result.memberFutures();        assertEquals(1, memberFutures.size());        for (Map.Entry<MemberIdentity, KafkaFuture<Void>> entry : memberFutures.entrySet()) {            assertFalse(entry.getValue().isCompletedExceptionally());        }    }}
f5606
0
failuresInjected
public synchronized int kafkatest_f5614_0()
{    return failuresInjected;}
f5614
0
callHasExpired
 booleanf5615_1KafkaAdminClient.Call call)
{    if ((!call.isInternal()) && shouldInjectFailure()) {                return true;    } else {        boolean ret = super.callHasExpired(call);                return ret;    }}
 booleanf5615
1
testEqualsWithoutGroupInstanceId
public void kafkatest_f5616_0()
{    MemberDescription dynamicMemberDescription = new MemberDescription(MEMBER_ID, CLIENT_ID, HOST, ASSIGNMENT);    MemberDescription identityDescription = new MemberDescription(MEMBER_ID, CLIENT_ID, HOST, ASSIGNMENT);    assertNotEquals(STATIC_MEMBER_DESCRIPTION, dynamicMemberDescription);    assertNotEquals(STATIC_MEMBER_DESCRIPTION.hashCode(), dynamicMemberDescription.hashCode());    // Check self equality.    assertEquals(dynamicMemberDescription, dynamicMemberDescription);    assertEquals(dynamicMemberDescription, identityDescription);    assertEquals(dynamicMemberDescription.hashCode(), identityDescription.hashCode());}
f5616
0
describeCluster
public DescribeClusterResult kafkatest_f5624_0(DescribeClusterOptions options)
{    KafkaFutureImpl<Collection<Node>> nodesFuture = new KafkaFutureImpl<>();    KafkaFutureImpl<Node> controllerFuture = new KafkaFutureImpl<>();    KafkaFutureImpl<String> brokerIdFuture = new KafkaFutureImpl<>();    KafkaFutureImpl<Set<AclOperation>> authorizedOperationsFuture = new KafkaFutureImpl<>();    if (timeoutNextRequests > 0) {        nodesFuture.completeExceptionally(new TimeoutException());        controllerFuture.completeExceptionally(new TimeoutException());        brokerIdFuture.completeExceptionally(new TimeoutException());        authorizedOperationsFuture.completeExceptionally(new TimeoutException());        --timeoutNextRequests;    } else {        nodesFuture.complete(brokers);        controllerFuture.complete(controller);        brokerIdFuture.complete(clusterId);        authorizedOperationsFuture.complete(Collections.emptySet());    }    return new DescribeClusterResult(nodesFuture, controllerFuture, brokerIdFuture, authorizedOperationsFuture);}
f5624
0
createTopics
public CreateTopicsResult kafkatest_f5625_0(Collection<NewTopic> newTopics, CreateTopicsOptions options)
{    Map<String, KafkaFuture<Void>> createTopicResult = new HashMap<>();    if (timeoutNextRequests > 0) {        for (final NewTopic newTopic : newTopics) {            String topicName = newTopic.name();            KafkaFutureImpl<Void> future = new KafkaFutureImpl<>();            future.completeExceptionally(new TimeoutException());            createTopicResult.put(topicName, future);        }        --timeoutNextRequests;        return new CreateTopicsResult(createTopicResult);    }    for (final NewTopic newTopic : newTopics) {        KafkaFutureImpl<Void> future = new KafkaFutureImpl<>();        String topicName = newTopic.name();        if (allTopics.containsKey(topicName)) {            future.completeExceptionally(new TopicExistsException(String.format("Topic %s exists already.", topicName)));            createTopicResult.put(topicName, future);            continue;        }        int replicationFactor = newTopic.replicationFactor();        List<Node> replicas = new ArrayList<>(replicationFactor);        for (int i = 0; i < replicationFactor; ++i) {            replicas.add(brokers.get(i));        }        int numberOfPartitions = newTopic.numPartitions();        List<TopicPartitionInfo> partitions = new ArrayList<>(numberOfPartitions);        for (int p = 0; p < numberOfPartitions; ++p) {            partitions.add(new TopicPartitionInfo(p, brokers.get(0), replicas, Collections.emptyList()));        }        allTopics.put(topicName, new TopicMetadata(false, partitions, newTopic.configs()));        future.complete(null);        createTopicResult.put(topicName, future);    }    return new CreateTopicsResult(createTopicResult);}
f5625
0
listTopics
public ListTopicsResult kafkatest_f5626_0(ListTopicsOptions options)
{    Map<String, TopicListing> topicListings = new HashMap<>();    if (timeoutNextRequests > 0) {        KafkaFutureImpl<Map<String, TopicListing>> future = new KafkaFutureImpl<>();        future.completeExceptionally(new TimeoutException());        --timeoutNextRequests;        return new ListTopicsResult(future);    }    for (Map.Entry<String, TopicMetadata> topicDescription : allTopics.entrySet()) {        String topicName = topicDescription.getKey();        if (topicDescription.getValue().fetchesRemainingUntilVisible > 0) {            topicDescription.getValue().fetchesRemainingUntilVisible--;        } else {            topicListings.put(topicName, new TopicListing(topicName, topicDescription.getValue().isInternalTopic));        }    }    KafkaFutureImpl<Map<String, TopicListing>> future = new KafkaFutureImpl<>();    future.complete(topicListings);    return new ListTopicsResult(future);}
f5626
0
describeDelegationToken
public DescribeDelegationTokenResult kafkatest_f5634_0(DescribeDelegationTokenOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5634
0
describeConsumerGroups
public DescribeConsumerGroupsResult kafkatest_f5635_0(Collection<String> groupIds, DescribeConsumerGroupsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5635
0
listConsumerGroups
public ListConsumerGroupsResult kafkatest_f5636_0(ListConsumerGroupsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5636
0
describeAcls
public DescribeAclsResult kafkatest_f5644_0(AclBindingFilter filter, DescribeAclsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5644
0
deleteAcls
public DeleteAclsResult kafkatest_f5645_0(Collection<AclBindingFilter> filters, DeleteAclsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5645
0
describeConfigs
public DescribeConfigsResult kafkatest_f5646_0(Collection<ConfigResource> resources, DescribeConfigsOptions options)
{    Map<ConfigResource, KafkaFuture<Config>> configescriptions = new HashMap<>();    for (ConfigResource resource : resources) {        if (resource.type() == ConfigResource.Type.TOPIC) {            Map<String, String> configs = allTopics.get(resource.name()).configs;            List<ConfigEntry> configEntries = new ArrayList<>();            for (Map.Entry<String, String> entry : configs.entrySet()) {                configEntries.add(new ConfigEntry(entry.getKey(), entry.getValue()));            }            KafkaFutureImpl<Config> future = new KafkaFutureImpl<>();            future.complete(new Config(configEntries));            configescriptions.put(resource, future);        } else {            throw new UnsupportedOperationException("Not implemented yet");        }    }    return new DescribeConfigsResult(configescriptions);}
f5646
0
setMockMetrics
public void kafkatest_f5655_0(MetricName name, Metric metric)
{    mockMetrics.put(name, metric);}
f5655
0
metrics
public Map<MetricName, ? extends Metric> kafkatest_f5656_0()
{    return mockMetrics;}
f5656
0
setFetchesRemainingUntilVisible
public void kafkatest_f5657_0(String topicName, int fetchesRemainingUntilVisible)
{    TopicMetadata metadata = allTopics.get(topicName);    if (metadata == null) {        throw new RuntimeException("No such topic as " + topicName);    }    metadata.fetchesRemainingUntilVisible = fetchesRemainingUntilVisible;}
f5657
0
testParseAndValidateAddressesWithReverseLookup
public void kafkatest_f5665_0()
{    checkWithoutLookup("127.0.0.1:8000");    checkWithoutLookup("localhost:8080");    checkWithoutLookup("[::1]:8000");    checkWithoutLookup("[2001:db8:85a3:8d3:1319:8a2e:370:7348]:1234", "localhost:10000");    // With lookup of example.com, either one or two addresses are expected depending on    // whether ipv4 and ipv6 are enabled    List<InetSocketAddress> validatedAddresses = checkWithLookup(Arrays.asList("example.com:10000"));    assertTrue("Unexpected addresses " + validatedAddresses, validatedAddresses.size() >= 1);    List<String> validatedHostNames = validatedAddresses.stream().map(InetSocketAddress::getHostName).collect(Collectors.toList());    List<String> expectedHostNames = Arrays.asList("93.184.216.34", "2606:2800:220:1:248:1893:25c8:1946");    assertTrue("Unexpected addresses " + validatedHostNames, expectedHostNames.containsAll(validatedHostNames));    validatedAddresses.forEach(address -> assertEquals(10000, address.getPort()));}
f5665
0
testInvalidConfig
public void kafkatest_f5666_0()
{    ClientUtils.parseAndValidateAddresses(Arrays.asList("localhost:10000"), "random.value");}
f5666
0
testNoPort
public void kafkatest_f5667_0()
{    checkWithoutLookup("127.0.0.1");}
f5667
0
setup
public void kafkatest_f5675_0()
{    this.connectionStates = new ClusterConnectionStates(reconnectBackoffMs, reconnectBackoffMax, new LogContext());}
f5675
0
testClusterConnectionStateChanges
public void kafkatest_f5676_0()
{    assertTrue(connectionStates.canConnect(nodeId1, time.milliseconds()));    // Start connecting to Node and check state    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.DEFAULT);    assertEquals(connectionStates.connectionState(nodeId1), ConnectionState.CONNECTING);    assertTrue(connectionStates.isConnecting(nodeId1));    assertFalse(connectionStates.isReady(nodeId1, time.milliseconds()));    assertFalse(connectionStates.isBlackedOut(nodeId1, time.milliseconds()));    assertFalse(connectionStates.hasReadyNodes(time.milliseconds()));    time.sleep(100);    // Successful connection    connectionStates.ready(nodeId1);    assertEquals(connectionStates.connectionState(nodeId1), ConnectionState.READY);    assertTrue(connectionStates.isReady(nodeId1, time.milliseconds()));    assertTrue(connectionStates.hasReadyNodes(time.milliseconds()));    assertFalse(connectionStates.isConnecting(nodeId1));    assertFalse(connectionStates.isBlackedOut(nodeId1, time.milliseconds()));    assertEquals(connectionStates.connectionDelay(nodeId1, time.milliseconds()), Long.MAX_VALUE);    time.sleep(15000);    // Disconnected from broker    connectionStates.disconnected(nodeId1, time.milliseconds());    assertEquals(connectionStates.connectionState(nodeId1), ConnectionState.DISCONNECTED);    assertTrue(connectionStates.isDisconnected(nodeId1));    assertTrue(connectionStates.isBlackedOut(nodeId1, time.milliseconds()));    assertFalse(connectionStates.isConnecting(nodeId1));    assertFalse(connectionStates.hasReadyNodes(time.milliseconds()));    assertFalse(connectionStates.canConnect(nodeId1, time.milliseconds()));    // After disconnecting we expect a backoff value equal to the reconnect.backoff.ms setting (plus minus 20% jitter)    double backoffTolerance = reconnectBackoffMs * reconnectBackoffJitter;    long currentBackoff = connectionStates.connectionDelay(nodeId1, time.milliseconds());    assertEquals(reconnectBackoffMs, currentBackoff, backoffTolerance);    time.sleep(currentBackoff + 1);    // after waiting for the current backoff value we should be allowed to connect again    assertTrue(connectionStates.canConnect(nodeId1, time.milliseconds()));}
f5676
0
testMultipleNodeConnectionStates
public void kafkatest_f5677_0()
{    // Check initial state, allowed to connect to all nodes, but no nodes shown as ready    assertTrue(connectionStates.canConnect(nodeId1, time.milliseconds()));    assertTrue(connectionStates.canConnect(nodeId2, time.milliseconds()));    assertFalse(connectionStates.hasReadyNodes(time.milliseconds()));    // Start connecting one node and check that the pool only shows ready nodes after    // successful connect    connectionStates.connecting(nodeId2, time.milliseconds(), "localhost", ClientDnsLookup.DEFAULT);    assertFalse(connectionStates.hasReadyNodes(time.milliseconds()));    time.sleep(1000);    connectionStates.ready(nodeId2);    assertTrue(connectionStates.hasReadyNodes(time.milliseconds()));    // Connect second node and check that both are shown as ready, pool should immediately    // show ready nodes, since node2 is already connected    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.DEFAULT);    assertTrue(connectionStates.hasReadyNodes(time.milliseconds()));    time.sleep(1000);    connectionStates.ready(nodeId1);    assertTrue(connectionStates.hasReadyNodes(time.milliseconds()));    time.sleep(12000);    // disconnect nodes and check proper state of pool throughout    connectionStates.disconnected(nodeId2, time.milliseconds());    assertTrue(connectionStates.hasReadyNodes(time.milliseconds()));    assertTrue(connectionStates.isBlackedOut(nodeId2, time.milliseconds()));    assertFalse(connectionStates.isBlackedOut(nodeId1, time.milliseconds()));    time.sleep(connectionStates.connectionDelay(nodeId2, time.milliseconds()));    // by the time node1 disconnects node2 should have been unblocked again    connectionStates.disconnected(nodeId1, time.milliseconds() + 1);    assertTrue(connectionStates.isBlackedOut(nodeId1, time.milliseconds()));    assertFalse(connectionStates.isBlackedOut(nodeId2, time.milliseconds()));    assertFalse(connectionStates.hasReadyNodes(time.milliseconds()));}
f5677
0
testMultipleIPsWithDefault
public void kafkatest_f5685_0() throws UnknownHostException
{    assertEquals(2, ClientUtils.resolve(hostTwoIps, ClientDnsLookup.USE_ALL_DNS_IPS).size());    connectionStates.connecting(nodeId1, time.milliseconds(), hostTwoIps, ClientDnsLookup.DEFAULT);    InetAddress currAddress = connectionStates.currentAddress(nodeId1);    connectionStates.connecting(nodeId1, time.milliseconds(), hostTwoIps, ClientDnsLookup.DEFAULT);    assertSame(currAddress, connectionStates.currentAddress(nodeId1));}
f5685
0
testMultipleIPsWithUseAll
public void kafkatest_f5686_0() throws UnknownHostException
{    assertEquals(2, ClientUtils.resolve(hostTwoIps, ClientDnsLookup.USE_ALL_DNS_IPS).size());    connectionStates.connecting(nodeId1, time.milliseconds(), hostTwoIps, ClientDnsLookup.USE_ALL_DNS_IPS);    InetAddress addr1 = connectionStates.currentAddress(nodeId1);    connectionStates.connecting(nodeId1, time.milliseconds(), hostTwoIps, ClientDnsLookup.USE_ALL_DNS_IPS);    InetAddress addr2 = connectionStates.currentAddress(nodeId1);    assertNotSame(addr1, addr2);    connectionStates.connecting(nodeId1, time.milliseconds(), hostTwoIps, ClientDnsLookup.USE_ALL_DNS_IPS);    InetAddress addr3 = connectionStates.currentAddress(nodeId1);    assertSame(addr1, addr3);}
f5686
0
testHostResolveChange
public void kafkatest_f5687_0() throws UnknownHostException, ReflectiveOperationException
{    assertEquals(2, ClientUtils.resolve(hostTwoIps, ClientDnsLookup.USE_ALL_DNS_IPS).size());    connectionStates.connecting(nodeId1, time.milliseconds(), hostTwoIps, ClientDnsLookup.DEFAULT);    InetAddress addr1 = connectionStates.currentAddress(nodeId1);    // reflection to simulate host change in DNS lookup    Method nodeStateMethod = connectionStates.getClass().getDeclaredMethod("nodeState", String.class);    nodeStateMethod.setAccessible(true);    Object nodeState = nodeStateMethod.invoke(connectionStates, nodeId1);    Field hostField = nodeState.getClass().getDeclaredField("host");    hostField.setAccessible(true);    hostField.set(nodeState, "localhost");    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.DEFAULT);    InetAddress addr2 = connectionStates.currentAddress(nodeId1);    assertNotSame(addr1, addr2);}
f5687
0
testOldConstructor
public void kafkatest_f5695_0()
{    String topic = "topic";    int partition = 0;    long offset = 23;    String key = "key";    String value = "value";    ConsumerRecord<String, String> record = new ConsumerRecord<>(topic, partition, offset, key, value);    assertEquals(topic, record.topic());    assertEquals(partition, record.partition());    assertEquals(offset, record.offset());    assertEquals(key, record.key());    assertEquals(value, record.value());    assertEquals(TimestampType.NO_TIMESTAMP_TYPE, record.timestampType());    assertEquals(ConsumerRecord.NO_TIMESTAMP, record.timestamp());    assertEquals(ConsumerRecord.NULL_CHECKSUM, record.checksum());    assertEquals(ConsumerRecord.NULL_SIZE, record.serializedKeySize());    assertEquals(ConsumerRecord.NULL_SIZE, record.serializedValueSize());    assertEquals(Optional.empty(), record.leaderEpoch());    assertEquals(new RecordHeaders(), record.headers());}
f5695
0
testNullChecksumInConstructor
public void kafkatest_f5696_0()
{    String key = "key";    String value = "value";    long timestamp = 242341324L;    ConsumerRecord<String, String> record = new ConsumerRecord<>("topic", 0, 23L, timestamp, TimestampType.CREATE_TIME, null, key.length(), value.length(), key, value, new RecordHeaders());    assertEquals(DefaultRecord.computePartialChecksum(timestamp, key.length(), value.length()), record.checksum());}
f5696
0
createAssignor
public AbstractStickyAssignor kafkatest_f5697_0()
{    return new CooperativeStickyAssignor();}
f5697
0
testTimeoutAndRetryJoinGroupIfNeeded
public void kafkatest_f5705_0() throws Exception
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(mockTime.timer(0));    ExecutorService executor = Executors.newFixedThreadPool(1);    try {        Timer firstAttemptTimer = mockTime.timer(REQUEST_TIMEOUT_MS);        Future<Boolean> firstAttempt = executor.submit(() -> coordinator.joinGroupIfNeeded(firstAttemptTimer));        mockTime.sleep(REQUEST_TIMEOUT_MS);        assertFalse(firstAttempt.get());        assertTrue(consumerClient.hasPendingRequests(coordinatorNode));        mockClient.respond(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));        mockClient.prepareResponse(syncGroupResponse(Errors.NONE));        Timer secondAttemptTimer = mockTime.timer(REQUEST_TIMEOUT_MS);        Future<Boolean> secondAttempt = executor.submit(() -> coordinator.joinGroupIfNeeded(secondAttemptTimer));        assertTrue(secondAttempt.get());    } finally {        executor.shutdownNow();        executor.awaitTermination(1000, TimeUnit.MILLISECONDS);    }}
f5705
0
testGroupMaxSizeExceptionIsFatal
public void kafkatest_f5706_0()
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(mockTime.timer(0));    mockClient.prepareResponse(joinGroupFollowerResponse(defaultGeneration, memberId, JoinGroupResponse.UNKNOWN_MEMBER_ID, Errors.GROUP_MAX_SIZE_REACHED));    RequestFuture<ByteBuffer> future = coordinator.sendJoinGroupRequest();    assertTrue(consumerClient.poll(future, mockTime.timer(REQUEST_TIMEOUT_MS)));    assertTrue(future.exception().getClass().isInstance(Errors.GROUP_MAX_SIZE_REACHED.exception()));    assertFalse(future.isRetriable());}
f5706
0
testJoinGroupRequestTimeout
public void kafkatest_f5707_0()
{    setupCoordinator(RETRY_BACKOFF_MS, REBALANCE_TIMEOUT_MS, Optional.empty());    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(mockTime.timer(0));    RequestFuture<ByteBuffer> future = coordinator.sendJoinGroupRequest();    mockTime.sleep(REQUEST_TIMEOUT_MS + 1);    assertFalse(consumerClient.poll(future, mockTime.timer(0)));    mockTime.sleep(REBALANCE_TIMEOUT_MS - REQUEST_TIMEOUT_MS + 5000);    assertTrue(consumerClient.poll(future, mockTime.timer(0)));}
f5707
0
checkLeaveGroupRequestSent
private void kafkatest_f5715_0(Optional<String> groupInstanceId)
{    setupCoordinator(RETRY_BACKOFF_MS, Integer.MAX_VALUE, groupInstanceId);    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(syncGroupResponse(Errors.NONE));    final RuntimeException e = new RuntimeException();    // raise the error when the coordinator tries to send leave group request.    mockClient.prepareResponse(body -> {        if (body instanceof LeaveGroupRequest)            throw e;        return false;    }, heartbeatResponse(Errors.UNKNOWN_SERVER_ERROR));    try {        coordinator.ensureActiveGroup();        coordinator.close();        if (coordinator.isDynamicMember()) {            fail("Expected leavegroup to raise an error.");        }    } catch (RuntimeException exception) {        if (coordinator.isDynamicMember()) {            assertEquals(exception, e);        } else {            fail("Coordinator with group.instance.id set shouldn't send leave group request.");        }    }}
f5715
0
testHandleNormalLeaveGroupResponse
public void kafkatest_f5716_0()
{    MemberResponse memberResponse = new MemberResponse().setMemberId(memberId).setErrorCode(Errors.NONE.code());    LeaveGroupResponse response = leaveGroupResponse(Collections.singletonList(memberResponse));    RequestFuture<Void> leaveGroupFuture = setupLeaveGroup(response);    assertNotNull(leaveGroupFuture);    assertTrue(leaveGroupFuture.succeeded());}
f5716
0
testHandleMultipleMembersLeaveGroupResponse
public void kafkatest_f5717_0()
{    MemberResponse memberResponse = new MemberResponse().setMemberId(memberId).setErrorCode(Errors.NONE.code());    LeaveGroupResponse response = leaveGroupResponse(Arrays.asList(memberResponse, memberResponse));    RequestFuture<Void> leaveGroupFuture = setupLeaveGroup(response);    assertNotNull(leaveGroupFuture);    assertTrue(leaveGroupFuture.exception() instanceof IllegalStateException);}
f5717
0
testWakeupAfterJoinGroupSent
public void kafkatest_f5725_0() throws Exception
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(new MockClient.RequestMatcher() {        private int invocations = 0;        @Override        public boolean matches(AbstractRequest body) {            invocations++;            boolean isJoinGroupRequest = body instanceof JoinGroupRequest;            if (isJoinGroupRequest && invocations == 1)                // simulate wakeup before the request returns                throw new WakeupException();            return isJoinGroupRequest;        }    }, joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(syncGroupResponse(Errors.NONE));    AtomicBoolean heartbeatReceived = prepareFirstHeartbeat();    try {        coordinator.ensureActiveGroup();        fail("Should have woken up from ensureActiveGroup()");    } catch (WakeupException e) {    }    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(0, coordinator.onJoinCompleteInvokes);    assertFalse(heartbeatReceived.get());    coordinator.ensureActiveGroup();    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(1, coordinator.onJoinCompleteInvokes);    awaitFirstHeartbeat(heartbeatReceived);}
f5725
0
matches
public boolean kafkatest_f5726_0(AbstractRequest body)
{    invocations++;    boolean isJoinGroupRequest = body instanceof JoinGroupRequest;    if (isJoinGroupRequest && invocations == 1)        // simulate wakeup before the request returns        throw new WakeupException();    return isJoinGroupRequest;}
f5726
0
testWakeupAfterJoinGroupSentExternalCompletion
public void kafkatest_f5727_0() throws Exception
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(new MockClient.RequestMatcher() {        private int invocations = 0;        @Override        public boolean matches(AbstractRequest body) {            invocations++;            boolean isJoinGroupRequest = body instanceof JoinGroupRequest;            if (isJoinGroupRequest && invocations == 1)                // simulate wakeup before the request returns                throw new WakeupException();            return isJoinGroupRequest;        }    }, joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(syncGroupResponse(Errors.NONE));    AtomicBoolean heartbeatReceived = prepareFirstHeartbeat();    try {        coordinator.ensureActiveGroup();        fail("Should have woken up from ensureActiveGroup()");    } catch (WakeupException e) {    }    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(0, coordinator.onJoinCompleteInvokes);    assertFalse(heartbeatReceived.get());    // the join group completes in this poll()    consumerClient.poll(mockTime.timer(0));    coordinator.ensureActiveGroup();    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(1, coordinator.onJoinCompleteInvokes);    awaitFirstHeartbeat(heartbeatReceived);}
f5727
0
testWakeupAfterSyncGroupReceived
public void kafkatest_f5735_0() throws Exception
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(body -> {        boolean isSyncGroupRequest = body instanceof SyncGroupRequest;        if (isSyncGroupRequest)            // wakeup after the request returns            consumerClient.wakeup();        return isSyncGroupRequest;    }, syncGroupResponse(Errors.NONE));    AtomicBoolean heartbeatReceived = prepareFirstHeartbeat();    try {        coordinator.ensureActiveGroup();        fail("Should have woken up from ensureActiveGroup()");    } catch (WakeupException e) {    }    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(0, coordinator.onJoinCompleteInvokes);    assertFalse(heartbeatReceived.get());    coordinator.ensureActiveGroup();    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(1, coordinator.onJoinCompleteInvokes);    awaitFirstHeartbeat(heartbeatReceived);}
f5735
0
testWakeupAfterSyncGroupReceivedExternalCompletion
public void kafkatest_f5736_0() throws Exception
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(body -> {        boolean isSyncGroupRequest = body instanceof SyncGroupRequest;        if (isSyncGroupRequest)            // wakeup after the request returns            consumerClient.wakeup();        return isSyncGroupRequest;    }, syncGroupResponse(Errors.NONE));    AtomicBoolean heartbeatReceived = prepareFirstHeartbeat();    try {        coordinator.ensureActiveGroup();        fail("Should have woken up from ensureActiveGroup()");    } catch (WakeupException e) {    }    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(0, coordinator.onJoinCompleteInvokes);    assertFalse(heartbeatReceived.get());    coordinator.ensureActiveGroup();    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(1, coordinator.onJoinCompleteInvokes);    awaitFirstHeartbeat(heartbeatReceived);}
f5736
0
testWakeupInOnJoinComplete
public void kafkatest_f5737_0() throws Exception
{    setupCoordinator();    coordinator.wakeupOnJoinComplete = true;    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(syncGroupResponse(Errors.NONE));    AtomicBoolean heartbeatReceived = prepareFirstHeartbeat();    try {        coordinator.ensureActiveGroup();        fail("Should have woken up from ensureActiveGroup()");    } catch (WakeupException e) {    }    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(0, coordinator.onJoinCompleteInvokes);    assertFalse(heartbeatReceived.get());    // the join group completes in this poll()    coordinator.wakeupOnJoinComplete = false;    consumerClient.poll(mockTime.timer(0));    coordinator.ensureActiveGroup();    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(1, coordinator.onJoinCompleteInvokes);    awaitFirstHeartbeat(heartbeatReceived);}
f5737
0
joinGroupResponse
private JoinGroupResponse kafkatest_f5745_0(Errors error)
{    return joinGroupFollowerResponse(JoinGroupResponse.UNKNOWN_GENERATION_ID, JoinGroupResponse.UNKNOWN_MEMBER_ID, JoinGroupResponse.UNKNOWN_MEMBER_ID, error);}
f5745
0
syncGroupResponse
private SyncGroupResponse kafkatest_f5746_0(Errors error)
{    return new SyncGroupResponse(new SyncGroupResponseData().setErrorCode(error.code()).setAssignment(new byte[0]));}
f5746
0
leaveGroupResponse
private LeaveGroupResponse kafkatest_f5747_0(List<MemberResponse> members)
{    return new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.NONE.code()).setMembers(members));}
f5747
0
testMemberInfoSortingSomeGroupInstanceId
public void kafkatest_f5755_0()
{    MemberInfo m1 = new MemberInfo("a", Optional.empty());    MemberInfo m2 = new MemberInfo("b", Optional.of("y"));    MemberInfo m3 = new MemberInfo("c", Optional.of("x"));    List<MemberInfo> memberInfoList = Arrays.asList(m1, m2, m3);    assertEquals(Arrays.asList(m3, m2, m1), Utils.sorted(memberInfoList));}
f5755
0
testMergeSortManyMemberInfo
public void kafkatest_f5756_0()
{    Random rand = new Random();    int bound = 2;    List<MemberInfo> memberInfoList = new ArrayList<>();    List<MemberInfo> staticMemberList = new ArrayList<>();    List<MemberInfo> dynamicMemberList = new ArrayList<>();    for (int i = 0; i < 100; i++) {        // Need to make sure all the ids are defined as 3-digits otherwise        // the comparison result will break.        String id = Integer.toString(i + 100);        Optional<String> groupInstanceId = rand.nextInt(bound) < bound / 2 ? Optional.of(id) : Optional.empty();        MemberInfo m = new MemberInfo(id, groupInstanceId);        memberInfoList.add(m);        if (m.groupInstanceId.isPresent()) {            staticMemberList.add(m);        } else {            dynamicMemberList.add(m);        }    }    staticMemberList.addAll(dynamicMemberList);    Collections.shuffle(memberInfoList);    assertEquals(staticMemberList, Utils.sorted(memberInfoList));}
f5756
0
setUp
public void kafkatest_f5757_0()
{    assignor = createAssignor();    if (subscriptions != null) {        subscriptions.clear();    } else {        subscriptions = new HashMap<>();    }}
f5757
0
testMultipleConsumersMixedTopicSubscriptions
public void kafkatest_f5765_0()
{    String topic1 = "topic1";    String topic2 = "topic2";    String consumer1 = "consumer1";    String consumer2 = "consumer2";    String consumer3 = "consumer3";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, 3);    partitionsPerTopic.put(topic2, 2);    subscriptions.put(consumer1, new Subscription(topics(topic1)));    subscriptions.put(consumer2, new Subscription(topics(topic1, topic2)));    subscriptions.put(consumer3, new Subscription(topics(topic1)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(partitions(tp(topic1, 0), tp(topic1, 2)), assignment.get(consumer1));    assertEquals(partitions(tp(topic2, 0), tp(topic2, 1)), assignment.get(consumer2));    assertEquals(partitions(tp(topic1, 1)), assignment.get(consumer3));    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(isFullyBalanced(assignment));}
f5765
0
testTwoConsumersTwoTopicsSixPartitions
public void kafkatest_f5766_0()
{    String topic1 = "topic1";    String topic2 = "topic2";    String consumer1 = "consumer1";    String consumer2 = "consumer2";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, 3);    partitionsPerTopic.put(topic2, 3);    subscriptions.put(consumer1, new Subscription(topics(topic1, topic2)));    subscriptions.put(consumer2, new Subscription(topics(topic1, topic2)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(partitions(tp(topic1, 0), tp(topic1, 2), tp(topic2, 1)), assignment.get(consumer1));    assertEquals(partitions(tp(topic1, 1), tp(topic2, 0), tp(topic2, 2)), assignment.get(consumer2));    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(isFullyBalanced(assignment));}
f5766
0
testAddRemoveConsumerOneTopic
public void kafkatest_f5767_0()
{    String consumer1 = "consumer1";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 3);    subscriptions.put(consumer1, new Subscription(topics(topic)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(partitions(tp(topic, 0), tp(topic, 1), tp(topic, 2)), assignment.get(consumer1));    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(isFullyBalanced(assignment));    String consumer2 = "consumer2";    subscriptions.put(consumer1, buildSubscription(topics(topic), assignment.get(consumer1)));    subscriptions.put(consumer2, buildSubscription(topics(topic), Collections.emptyList()));    assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertEquals(partitions(tp(topic, 2), tp(topic, 1)), assignment.get(consumer1));    assertEquals(partitions(tp(topic, 0)), assignment.get(consumer2));    assertTrue(isFullyBalanced(assignment));    assertTrue(assignor.isSticky());    subscriptions.remove(consumer1);    subscriptions.put(consumer2, buildSubscription(topics(topic), assignment.get(consumer2)));    assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(new HashSet<>(partitions(tp(topic, 2), tp(topic, 1), tp(topic, 0))), new HashSet<>(assignment.get(consumer2)));    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(isFullyBalanced(assignment));    assertTrue(assignor.isSticky());}
f5767
0
testMoveExistingAssignments
public void kafkatest_f5775_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    for (int i = 1; i <= 6; i++) partitionsPerTopic.put(String.format("topic%02d", i), 1);    subscriptions.put("consumer01", buildSubscription(topics("topic01", "topic02"), partitions(tp("topic01", 0))));    subscriptions.put("consumer02", buildSubscription(topics("topic01", "topic02", "topic03", "topic04"), partitions(tp("topic02", 0), tp("topic03", 0))));    subscriptions.put("consumer03", buildSubscription(topics("topic02", "topic03", "topic04", "topic05", "topic06"), partitions(tp("topic04", 0), tp("topic05", 0), tp("topic06", 0))));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);}
f5775
0
testStickiness
public void kafkatest_f5776_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put("topic01", 3);    String consumer1 = "consumer01";    String consumer2 = "consumer02";    String consumer3 = "consumer03";    String consumer4 = "consumer04";    subscriptions.put(consumer1, new Subscription(topics("topic01")));    subscriptions.put(consumer2, new Subscription(topics("topic01")));    subscriptions.put(consumer3, new Subscription(topics("topic01")));    subscriptions.put(consumer4, new Subscription(topics("topic01")));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    Map<String, TopicPartition> partitionsAssigned = new HashMap<>();    Set<Map.Entry<String, List<TopicPartition>>> assignments = assignment.entrySet();    for (Map.Entry<String, List<TopicPartition>> entry : assignments) {        String consumer = entry.getKey();        List<TopicPartition> topicPartitions = entry.getValue();        int size = topicPartitions.size();        assertTrue("Consumer " + consumer + " is assigned more topic partitions than expected.", size <= 1);        if (size == 1)            partitionsAssigned.put(consumer, topicPartitions.get(0));    }    // removing the potential group leader    subscriptions.remove(consumer1);    subscriptions.put(consumer2, buildSubscription(topics("topic01"), assignment.get(consumer2)));    subscriptions.put(consumer3, buildSubscription(topics("topic01"), assignment.get(consumer3)));    subscriptions.put(consumer4, buildSubscription(topics("topic01"), assignment.get(consumer4)));    assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(assignor.isSticky());    assignments = assignment.entrySet();    for (Map.Entry<String, List<TopicPartition>> entry : assignments) {        String consumer = entry.getKey();        List<TopicPartition> topicPartitions = entry.getValue();        assertEquals("Consumer " + consumer + " is assigned more topic partitions than expected.", 1, topicPartitions.size());        assertTrue("Stickiness was not honored for consumer " + consumer, (!partitionsAssigned.containsKey(consumer)) || (assignment.get(consumer).contains(partitionsAssigned.get(consumer))));    }}
f5776
0
testAssignmentUpdatedForDeletedTopic
public void kafkatest_f5777_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put("topic01", 1);    partitionsPerTopic.put("topic03", 100);    subscriptions = Collections.singletonMap(consumerId, new Subscription(topics("topic01", "topic02", "topic03")));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(assignment.values().stream().mapToInt(topicPartitions -> topicPartitions.size()).sum(), 1 + 100);    assertEquals(Collections.singleton(consumerId), assignment.keySet());    assertTrue(isFullyBalanced(assignment));}
f5777
0
topics
protected static List<String> kafkatest_f5785_0(String... topics)
{    return Arrays.asList(topics);}
f5785
0
partitions
protected static List<TopicPartition> kafkatest_f5786_0(TopicPartition... partitions)
{    return Arrays.asList(partitions);}
f5786
0
tp
protected static TopicPartition kafkatest_f5787_0(String topic, int partition)
{    return new TopicPartition(topic, partition);}
f5787
0
testSelectRebalanceProtcol
public void kafkatest_f5795_0()
{    List<ConsumerPartitionAssignor> assignors = new ArrayList<>();    assignors.add(new MockPartitionAssignor(Collections.singletonList(ConsumerPartitionAssignor.RebalanceProtocol.EAGER)));    assignors.add(new MockPartitionAssignor(Collections.singletonList(COOPERATIVE)));    // no commonly supported protocols    assertThrows(IllegalArgumentException.class, () -> buildCoordinator(rebalanceConfig, new Metrics(), assignors, false));    assignors.clear();    assignors.add(new MockPartitionAssignor(Arrays.asList(ConsumerPartitionAssignor.RebalanceProtocol.EAGER, COOPERATIVE)));    assignors.add(new MockPartitionAssignor(Arrays.asList(ConsumerPartitionAssignor.RebalanceProtocol.EAGER, COOPERATIVE)));    // select higher indexed (more advanced) protocols    try (ConsumerCoordinator coordinator = buildCoordinator(rebalanceConfig, new Metrics(), assignors, false)) {        assertEquals(COOPERATIVE, coordinator.getProtocol());    }}
f5795
0
testNormalHeartbeat
public void kafkatest_f5796_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // normal heartbeat    time.sleep(sessionTimeoutMs);    // should send out the heartbeat    RequestFuture<Void> future = coordinator.sendHeartbeatRequest();    assertEquals(1, consumerClient.pendingRequestCount());    assertFalse(future.isDone());    client.prepareResponse(heartbeatResponse(Errors.NONE));    consumerClient.poll(time.timer(0));    assertTrue(future.isDone());    assertTrue(future.succeeded());}
f5796
0
testGroupDescribeUnauthorized
public void kafkatest_f5797_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.GROUP_AUTHORIZATION_FAILED));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));}
f5797
0
testIllegalGeneration
public void kafkatest_f5806_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // illegal_generation will cause re-partition    subscriptions.subscribe(singleton(topic1), rebalanceListener);    subscriptions.assignFromSubscribed(Collections.singletonList(t1p));    time.sleep(sessionTimeoutMs);    // should send out the heartbeat    RequestFuture<Void> future = coordinator.sendHeartbeatRequest();    assertEquals(1, consumerClient.pendingRequestCount());    assertFalse(future.isDone());    client.prepareResponse(heartbeatResponse(Errors.ILLEGAL_GENERATION));    time.sleep(sessionTimeoutMs);    consumerClient.poll(time.timer(0));    assertTrue(future.isDone());    assertTrue(future.failed());    assertEquals(Errors.ILLEGAL_GENERATION.exception(), future.exception());    assertTrue(coordinator.rejoinNeededOrPending());    coordinator.poll(time.timer(0));    assertEquals(1, rebalanceListener.lostCount);    assertEquals(Collections.singleton(t1p), rebalanceListener.lost);}
f5806
0
testUnknownMemberId
public void kafkatest_f5807_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // illegal_generation will cause re-partition    subscriptions.subscribe(singleton(topic1), rebalanceListener);    subscriptions.assignFromSubscribed(Collections.singletonList(t1p));    time.sleep(sessionTimeoutMs);    // should send out the heartbeat    RequestFuture<Void> future = coordinator.sendHeartbeatRequest();    assertEquals(1, consumerClient.pendingRequestCount());    assertFalse(future.isDone());    client.prepareResponse(heartbeatResponse(Errors.UNKNOWN_MEMBER_ID));    time.sleep(sessionTimeoutMs);    consumerClient.poll(time.timer(0));    assertTrue(future.isDone());    assertTrue(future.failed());    assertEquals(Errors.UNKNOWN_MEMBER_ID.exception(), future.exception());    assertTrue(coordinator.rejoinNeededOrPending());    coordinator.poll(time.timer(0));    assertEquals(1, rebalanceListener.lostCount);    assertEquals(Collections.singleton(t1p), rebalanceListener.lost);}
f5807
0
testCoordinatorDisconnect
public void kafkatest_f5808_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // coordinator disconnect will mark coordinator as unknown    time.sleep(sessionTimeoutMs);    // should send out the heartbeat    RequestFuture<Void> future = coordinator.sendHeartbeatRequest();    assertEquals(1, consumerClient.pendingRequestCount());    assertFalse(future.isDone());    // return disconnected    client.prepareResponse(heartbeatResponse(Errors.NONE), true);    time.sleep(sessionTimeoutMs);    consumerClient.poll(time.timer(0));    assertTrue(future.isDone());    assertTrue(future.failed());    assertTrue(future.exception() instanceof DisconnectException);    assertTrue(coordinator.coordinatorUnknown());}
f5808
0
testForceMetadataRefreshForPatternSubscriptionDuringRebalance
public void kafkatest_f5816_0()
{    // Set up a non-leader consumer with pattern subscription and a cluster containing one topic matching the    // pattern.    final String consumerId = "consumer";    subscriptions.subscribe(Pattern.compile(".*"), rebalanceListener);    client.updateMetadata(TestUtils.metadataUpdateWith(1, singletonMap(topic1, 1)));    coordinator.maybeUpdateSubscriptionMetadata();    assertEquals(singleton(topic1), subscriptions.subscription());    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // Instrument the test so that metadata will contain two topics after next refresh.    client.prepareMetadataUpdate(metadataResponse);    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(body -> {        SyncGroupRequest sync = (SyncGroupRequest) body;        return sync.data.memberId().equals(consumerId) && sync.data.generationId() == 1 && sync.groupAssignments().isEmpty();    }, syncGroupResponse(singletonList(t1p), Errors.NONE));    partitionAssignor.prepare(singletonMap(consumerId, singletonList(t1p)));    // This will trigger rebalance.    coordinator.poll(time.timer(Long.MAX_VALUE));    // Make sure that the metadata was refreshed during the rebalance and thus subscriptions now contain two topics.    final Set<String> updatedSubscriptionSet = new HashSet<>(Arrays.asList(topic1, topic2));    assertEquals(updatedSubscriptionSet, subscriptions.subscription());    // Refresh the metadata again. Since there have been no changes since the last refresh, it won't trigger    // rebalance again.    metadata.requestUpdate();    consumerClient.poll(time.timer(Long.MAX_VALUE));    assertFalse(coordinator.rejoinNeededOrPending());}
f5816
0
testWakeupDuringJoin
public void kafkatest_f5817_0()
{    final String consumerId = "leader";    final List<TopicPartition> owned = Collections.emptyList();    final List<TopicPartition> assigned = Arrays.asList(t1p);    subscriptions.subscribe(singleton(topic1), rebalanceListener);    // ensure metadata is up-to-date for leader    client.updateMetadata(metadataResponse);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    Map<String, List<String>> memberSubscriptions = singletonMap(consumerId, singletonList(topic1));    partitionAssignor.prepare(singletonMap(consumerId, assigned));    // prepare only the first half of the join and then trigger the wakeup    client.prepareResponse(joinGroupLeaderResponse(1, consumerId, memberSubscriptions, Errors.NONE));    consumerClient.wakeup();    try {        coordinator.poll(time.timer(Long.MAX_VALUE));    } catch (WakeupException e) {    // ignore    }    // now complete the second half    client.prepareResponse(syncGroupResponse(assigned, Errors.NONE));    coordinator.poll(time.timer(Long.MAX_VALUE));    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(toSet(assigned), subscriptions.assignedPartitions());    assertEquals(0, rebalanceListener.revokedCount);    assertNull(rebalanceListener.revoked);    assertEquals(1, rebalanceListener.assignedCount);    assertEquals(getAdded(owned, assigned), rebalanceListener.assigned);}
f5817
0
testNormalJoinGroupFollower
public void kafkatest_f5818_0()
{    final String consumerId = "consumer";    final Set<String> subscription = singleton(topic1);    final List<TopicPartition> owned = Collections.emptyList();    final List<TopicPartition> assigned = Arrays.asList(t1p);    subscriptions.subscribe(subscription, rebalanceListener);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // normal join group    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(body -> {        SyncGroupRequest sync = (SyncGroupRequest) body;        return sync.data.memberId().equals(consumerId) && sync.data.generationId() == 1 && sync.groupAssignments().isEmpty();    }, syncGroupResponse(assigned, Errors.NONE));    coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(toSet(assigned), subscriptions.assignedPartitions());    assertEquals(subscription, subscriptions.groupSubscription());    assertEquals(0, rebalanceListener.revokedCount);    assertNull(rebalanceListener.revoked);    assertEquals(1, rebalanceListener.assignedCount);    assertEquals(getAdded(owned, assigned), rebalanceListener.assigned);}
f5818
0
testUnknownMemberIdOnSyncGroup
public void kafkatest_f5826_0()
{    final String consumerId = "consumer";    subscriptions.subscribe(singleton(topic1), rebalanceListener);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // join initially, but let coordinator returns unknown member id    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(Collections.emptyList(), Errors.UNKNOWN_MEMBER_ID));    // now we should see a new join with the empty UNKNOWN_MEMBER_ID    client.prepareResponse(body -> {        JoinGroupRequest joinRequest = (JoinGroupRequest) body;        return joinRequest.data().memberId().equals(JoinGroupRequest.UNKNOWN_MEMBER_ID);    }, joinGroupFollowerResponse(2, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(singletonList(t1p), Errors.NONE));    coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(singleton(t1p), subscriptions.assignedPartitions());}
f5826
0
testRebalanceInProgressOnSyncGroup
public void kafkatest_f5827_0()
{    final String consumerId = "consumer";    subscriptions.subscribe(singleton(topic1), rebalanceListener);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // join initially, but let coordinator rebalance on sync    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(Collections.emptyList(), Errors.REBALANCE_IN_PROGRESS));    // then let the full join/sync finish successfully    client.prepareResponse(joinGroupFollowerResponse(2, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(singletonList(t1p), Errors.NONE));    coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(singleton(t1p), subscriptions.assignedPartitions());}
f5827
0
testIllegalGenerationOnSyncGroup
public void kafkatest_f5828_0()
{    final String consumerId = "consumer";    subscriptions.subscribe(singleton(topic1), rebalanceListener);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // join initially, but let coordinator rebalance on sync    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(Collections.emptyList(), Errors.ILLEGAL_GENERATION));    // then let the full join/sync finish successfully    client.prepareResponse(body -> {        JoinGroupRequest joinRequest = (JoinGroupRequest) body;        return joinRequest.data().memberId().equals(JoinGroupRequest.UNKNOWN_MEMBER_ID);    }, joinGroupFollowerResponse(2, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(singletonList(t1p), Errors.NONE));    coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(singleton(t1p), subscriptions.assignedPartitions());}
f5828
0
unavailableTopicTest
private void kafkatest_f5836_0(boolean patternSubscribe, Set<String> unavailableTopicsInLastMetadata)
{    final String consumerId = "consumer";    if (patternSubscribe)        subscriptions.subscribe(Pattern.compile("test.*"), rebalanceListener);    else        subscriptions.subscribe(singleton(topic1), rebalanceListener);    client.prepareMetadataUpdate(TestUtils.metadataUpdateWith("kafka-cluster", 1, Collections.singletonMap(topic1, Errors.UNKNOWN_TOPIC_OR_PARTITION), Collections.emptyMap()));    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    Map<String, List<String>> memberSubscriptions = singletonMap(consumerId, singletonList(topic1));    partitionAssignor.prepare(Collections.emptyMap());    client.prepareResponse(joinGroupLeaderResponse(1, consumerId, memberSubscriptions, Errors.NONE));    client.prepareResponse(syncGroupResponse(Collections.emptyList(), Errors.NONE));    coordinator.poll(time.timer(Long.MAX_VALUE));    assertFalse(coordinator.rejoinNeededOrPending());    // callback not triggered since there's nothing to be assigned    assertEquals(Collections.emptySet(), rebalanceListener.assigned);    assertTrue("Metadata refresh not requested for unavailable partitions", metadata.updateRequested());    Map<String, Errors> topicErrors = new HashMap<>();    for (String topic : unavailableTopicsInLastMetadata) topicErrors.put(topic, Errors.UNKNOWN_TOPIC_OR_PARTITION);    client.prepareMetadataUpdate(TestUtils.metadataUpdateWith("kafka-cluster", 1, topicErrors, singletonMap(topic1, 1)));    consumerClient.poll(time.timer(0));    client.prepareResponse(joinGroupLeaderResponse(2, consumerId, memberSubscriptions, Errors.NONE));    client.prepareResponse(syncGroupResponse(singletonList(t1p), Errors.NONE));    coordinator.poll(time.timer(Long.MAX_VALUE));    assertFalse("Metadata refresh requested unnecessarily", metadata.updateRequested());    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(singleton(t1p), rebalanceListener.assigned);}
f5836
0
testExcludeInternalTopicsConfigOption
public void kafkatest_f5837_0()
{    testInternalTopicInclusion(false);}
f5837
0
testIncludeInternalTopicsConfigOption
public void kafkatest_f5838_0()
{    testInternalTopicInclusion(true);}
f5838
0
testInFlightRequestsFailedAfterCoordinatorMarkedDead
private void kafkatest_f5846_0(Errors error)
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // Send two async commits and fail the first one with an error.    // This should cause a coordinator disconnect which will cancel the second request.    MockCommitCallback firstCommitCallback = new MockCommitCallback();    MockCommitCallback secondCommitCallback = new MockCommitCallback();    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), firstCommitCallback);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), secondCommitCallback);    respondToOffsetCommitRequest(singletonMap(t1p, 100L), error);    consumerClient.pollNoWakeup();    // second poll since coordinator disconnect is async    consumerClient.pollNoWakeup();    coordinator.invokeCompletedOffsetCommitCallbacks();    assertTrue(coordinator.coordinatorUnknown());    assertTrue(firstCommitCallback.exception instanceof RetriableCommitFailedException);    assertTrue(secondCommitCallback.exception instanceof RetriableCommitFailedException);}
f5846
0
testAutoCommitDynamicAssignment
public void kafkatest_f5847_0()
{    final String consumerId = "consumer";    try (ConsumerCoordinator coordinator = buildCoordinator(rebalanceConfig, new Metrics(), assignors, true)) {        subscriptions.subscribe(singleton(topic1), rebalanceListener);        joinAsFollowerAndReceiveAssignment(consumerId, coordinator, singletonList(t1p));        subscriptions.seek(t1p, 100);        prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.NONE);        time.sleep(autoCommitIntervalMs);        coordinator.poll(time.timer(Long.MAX_VALUE));        assertFalse(client.hasPendingResponses());    }}
f5847
0
testAutoCommitRetryBackoff
public void kafkatest_f5848_0()
{    final String consumerId = "consumer";    try (ConsumerCoordinator coordinator = buildCoordinator(rebalanceConfig, new Metrics(), assignors, true)) {        subscriptions.subscribe(singleton(topic1), rebalanceListener);        joinAsFollowerAndReceiveAssignment(consumerId, coordinator, singletonList(t1p));        subscriptions.seek(t1p, 100);        time.sleep(autoCommitIntervalMs);        // Send an offset commit, but let it fail with a retriable error        prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.NOT_COORDINATOR);        coordinator.poll(time.timer(Long.MAX_VALUE));        assertTrue(coordinator.coordinatorUnknown());        // After the disconnect, we should rediscover the coordinator        client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));        coordinator.poll(time.timer(Long.MAX_VALUE));        subscriptions.seek(t1p, 200);        // Until the retry backoff has expired, we should not retry the offset commit        time.sleep(retryBackoffMs / 2);        coordinator.poll(time.timer(Long.MAX_VALUE));        assertEquals(0, client.inFlightRequestCount());        // Once the backoff expires, we should retry        time.sleep(retryBackoffMs / 2);        coordinator.poll(time.timer(Long.MAX_VALUE));        assertEquals(1, client.inFlightRequestCount());        respondToOffsetCommitRequest(singletonMap(t1p, 200L), Errors.NONE);    }}
f5848
0
testCommitOffsetAsyncFailedWithDefaultCallback
public void kafkatest_f5856_0()
{    int invokedBeforeTest = mockOffsetCommitCallback.invoked;    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.COORDINATOR_NOT_AVAILABLE);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), mockOffsetCommitCallback);    coordinator.invokeCompletedOffsetCommitCallbacks();    assertEquals(invokedBeforeTest + 1, mockOffsetCommitCallback.invoked);    assertTrue(mockOffsetCommitCallback.exception instanceof RetriableCommitFailedException);}
f5856
0
testCommitOffsetAsyncCoordinatorNotAvailable
public void kafkatest_f5857_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // async commit with coordinator not available    MockCommitCallback cb = new MockCommitCallback();    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.COORDINATOR_NOT_AVAILABLE);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), cb);    coordinator.invokeCompletedOffsetCommitCallbacks();    assertTrue(coordinator.coordinatorUnknown());    assertEquals(1, cb.invoked);    assertTrue(cb.exception instanceof RetriableCommitFailedException);}
f5857
0
testCommitOffsetAsyncNotCoordinator
public void kafkatest_f5858_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // async commit with not coordinator    MockCommitCallback cb = new MockCommitCallback();    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.COORDINATOR_NOT_AVAILABLE);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), cb);    coordinator.invokeCompletedOffsetCommitCallbacks();    assertTrue(coordinator.coordinatorUnknown());    assertEquals(1, cb.invoked);    assertTrue(cb.exception instanceof RetriableCommitFailedException);}
f5858
0
testRetryCommitUnknownTopicOrPartition
public void kafkatest_f5866_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    client.prepareResponse(offsetCommitResponse(singletonMap(t1p, Errors.UNKNOWN_TOPIC_OR_PARTITION)));    client.prepareResponse(offsetCommitResponse(singletonMap(t1p, Errors.NONE)));    assertTrue(coordinator.commitOffsetsSync(singletonMap(t1p, new OffsetAndMetadata(100L, "metadata")), time.timer(10000)));}
f5866
0
testCommitOffsetMetadataTooLarge
public void kafkatest_f5867_0()
{    // since offset metadata is provided by the user, we have to propagate the exception so they can handle it    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.OFFSET_METADATA_TOO_LARGE);    coordinator.commitOffsetsSync(singletonMap(t1p, new OffsetAndMetadata(100L, "metadata")), time.timer(Long.MAX_VALUE));}
f5867
0
testCommitOffsetIllegalGeneration
public void kafkatest_f5868_0()
{    // we cannot retry if a rebalance occurs before the commit completed    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.ILLEGAL_GENERATION);    coordinator.commitOffsetsSync(singletonMap(t1p, new OffsetAndMetadata(100L, "metadata")), time.timer(Long.MAX_VALUE));}
f5868
0
testTopicAuthorizationFailedInOffsetFetch
public void kafkatest_f5876_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    OffsetFetchResponse.PartitionData data = new OffsetFetchResponse.PartitionData(-1, Optional.empty(), "", Errors.TOPIC_AUTHORIZATION_FAILED);    client.prepareResponse(new OffsetFetchResponse(Errors.NONE, singletonMap(t1p, data)));    TopicAuthorizationException exception = assertThrows(TopicAuthorizationException.class, () -> coordinator.fetchCommittedOffsets(singleton(t1p), time.timer(Long.MAX_VALUE)));    assertEquals(singleton(topic1), exception.unauthorizedTopics());}
f5876
0
testRefreshOffsetLoadInProgress
public void kafkatest_f5877_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    subscriptions.assignFromUser(singleton(t1p));    client.prepareResponse(offsetFetchResponse(Errors.COORDINATOR_LOAD_IN_PROGRESS));    client.prepareResponse(offsetFetchResponse(t1p, Errors.NONE, "", 100L));    coordinator.refreshCommittedOffsetsIfNeeded(time.timer(Long.MAX_VALUE));    assertEquals(Collections.emptySet(), subscriptions.missingFetchPositions());    assertTrue(subscriptions.hasAllFetchPositions());    assertEquals(100L, subscriptions.position(t1p).offset);}
f5877
0
testRefreshOffsetsGroupNotAuthorized
public void kafkatest_f5878_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    subscriptions.assignFromUser(singleton(t1p));    client.prepareResponse(offsetFetchResponse(Errors.GROUP_AUTHORIZATION_FAILED));    try {        coordinator.refreshCommittedOffsetsIfNeeded(time.timer(Long.MAX_VALUE));        fail("Expected group authorization error");    } catch (GroupAuthorizationException e) {        assertEquals(groupId, e.groupId());    }}
f5878
0
run
public void kafkatest_f5886_0()
{    // Poll as fast as possible to reproduce ConcurrentModificationException    while (!doStop.get()) {        try {            int size = ((Double) metric.metricValue()).intValue();            observedSize.set(size);        } catch (Exception e) {            exceptionHolder.set(e);            return;        }    }}
f5886
0
conditionMet
public boolean kafkatest_f5887_0()
{    return observedSize.get() == totalPartitions || exceptionHolder.get() != null;}
f5887
0
testCloseDynamicAssignment
public void kafkatest_f5888_0() throws Exception
{    try (ConsumerCoordinator coordinator = prepareCoordinatorForCloseTest(true, true, Optional.empty())) {        gracefulCloseTest(coordinator, true);    }}
f5888
0
testCloseNoResponseForCommit
public void kafkatest_f5896_0() throws Exception
{    try (ConsumerCoordinator coordinator = prepareCoordinatorForCloseTest(true, true, groupInstanceId)) {        time.sleep(autoCommitIntervalMs);        closeVerifyTimeout(coordinator, Long.MAX_VALUE, requestTimeoutMs, requestTimeoutMs);    }}
f5896
0
testCloseNoResponseForLeaveGroup
public void kafkatest_f5897_0() throws Exception
{    try (ConsumerCoordinator coordinator = prepareCoordinatorForCloseTest(true, false, Optional.empty())) {        closeVerifyTimeout(coordinator, Long.MAX_VALUE, requestTimeoutMs, requestTimeoutMs);    }}
f5897
0
testCloseNoWait
public void kafkatest_f5898_0() throws Exception
{    try (ConsumerCoordinator coordinator = prepareCoordinatorForCloseTest(true, true, groupInstanceId)) {        time.sleep(autoCommitIntervalMs);        closeVerifyTimeout(coordinator, 0, 0, 0);    }}
f5898
0
makeCoordinatorUnknown
private void kafkatest_f5906_0(ConsumerCoordinator coordinator, Errors error)
{    time.sleep(sessionTimeoutMs);    coordinator.sendHeartbeatRequest();    client.prepareResponse(heartbeatResponse(error));    time.sleep(sessionTimeoutMs);    consumerClient.poll(time.timer(0));    assertTrue(coordinator.coordinatorUnknown());}
f5906
0
closeVerifyTimeout
private void kafkatest_f5907_0(final ConsumerCoordinator coordinator, final long closeTimeoutMs, final long expectedMinTimeMs, final long expectedMaxTimeMs) throws Exception
{    ExecutorService executor = Executors.newSingleThreadExecutor();    try {        boolean coordinatorUnknown = coordinator.coordinatorUnknown();        // Run close on a different thread. Coordinator is locked by this thread, so it is        // not safe to use the coordinator from the main thread until the task completes.        Future<?> future = executor.submit(new Runnable() {            @Override            public void run() {                coordinator.close(time.timer(Math.min(closeTimeoutMs, requestTimeoutMs)));            }        });        // at least one request. Otherwise, sleep for a short time.        if (!coordinatorUnknown)            client.waitForRequests(1, 1000);        else            Thread.sleep(200);        if (expectedMinTimeMs > 0) {            time.sleep(expectedMinTimeMs - 1);            try {                future.get(500, TimeUnit.MILLISECONDS);                fail("Close completed ungracefully without waiting for timeout");            } catch (TimeoutException e) {            // Expected timeout            }        }        if (expectedMaxTimeMs >= 0)            time.sleep(expectedMaxTimeMs - expectedMinTimeMs + 2);        future.get(2000, TimeUnit.MILLISECONDS);    } finally {        executor.shutdownNow();    }}
f5907
0
run
public void kafkatest_f5908_0()
{    coordinator.close(time.timer(Math.min(closeTimeoutMs, requestTimeoutMs)));}
f5908
0
joinGroupFollowerResponse
private JoinGroupResponse kafkatest_f5916_0(int generationId, String memberId, String leaderId, Errors error)
{    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(error.code()).setGenerationId(generationId).setProtocolName(partitionAssignor.name()).setLeader(leaderId).setMemberId(memberId).setMembers(Collections.emptyList()));}
f5916
0
syncGroupResponse
private SyncGroupResponse kafkatest_f5917_0(List<TopicPartition> partitions, Errors error)
{    ByteBuffer buf = ConsumerProtocol.serializeAssignment(new ConsumerPartitionAssignor.Assignment(partitions));    return new SyncGroupResponse(new SyncGroupResponseData().setErrorCode(error.code()).setAssignment(Utils.toArray(buf)));}
f5917
0
offsetCommitResponse
private OffsetCommitResponse kafkatest_f5918_0(Map<TopicPartition, Errors> responseData)
{    return new OffsetCommitResponse(responseData);}
f5918
0
prepareOffsetCommitRequestDisconnect
private void kafkatest_f5926_0(Map<TopicPartition, Long> expectedOffsets)
{    prepareOffsetCommitRequest(expectedOffsets, Errors.NONE, true);}
f5926
0
prepareOffsetCommitRequest
private void kafkatest_f5927_0(final Map<TopicPartition, Long> expectedOffsets, Errors error, boolean disconnected)
{    Map<TopicPartition, Errors> errors = partitionErrors(expectedOffsets.keySet(), error);    client.prepareResponse(offsetCommitRequestMatcher(expectedOffsets), offsetCommitResponse(errors), disconnected);}
f5927
0
partitionErrors
private Map<TopicPartition, Errors> kafkatest_f5928_0(Collection<TopicPartition> partitions, Errors error)
{    final Map<TopicPartition, Errors> errors = new HashMap<>();    for (TopicPartition partition : partitions) {        errors.put(partition, error);    }    return errors;}
f5928
0
onConsume
public ConsumerRecords<K, V> kafkatest_f5937_0(ConsumerRecords<K, V> records)
{    onConsumeCount++;    if (throwExceptionOnConsume)        throw new KafkaException("Injected exception in FilterConsumerInterceptor.onConsume.");    // filters out topic/partitions with partition == FILTER_PARTITION    Map<TopicPartition, List<ConsumerRecord<K, V>>> recordMap = new HashMap<>();    for (TopicPartition tp : records.partitions()) {        if (tp.partition() != filterPartition)            recordMap.put(tp, records.records(tp));    }    return new ConsumerRecords<K, V>(recordMap);}
f5937
0
onCommit
public void kafkatest_f5938_0(Map<TopicPartition, OffsetAndMetadata> offsets)
{    onCommitCount++;    if (throwExceptionOnCommit)        throw new KafkaException("Injected exception in FilterConsumerInterceptor.onCommit.");}
f5938
0
injectOnConsumeError
public void kafkatest_f5940_0(boolean on)
{    throwExceptionOnConsume = on;}
f5940
0
testNormalSubscription
public void kafkatest_f5948_0()
{    subscription.subscribe(Utils.mkSet("foo", "bar", "__consumer_offsets"), new NoOpConsumerRebalanceListener());    subscription.groupSubscribe(Utils.mkSet("baz"));    testBasicSubscription(Utils.mkSet("foo", "bar", "baz"), Utils.mkSet("__consumer_offsets"));}
f5948
0
testTransientTopics
public void kafkatest_f5949_0()
{    subscription.subscribe(singleton("foo"), new NoOpConsumerRebalanceListener());    ConsumerMetadata metadata = newConsumerMetadata(false);    metadata.update(TestUtils.metadataUpdateWith(1, singletonMap("foo", 1)), time.milliseconds());    assertFalse(metadata.updateRequested());    metadata.addTransientTopics(singleton("foo"));    assertFalse(metadata.updateRequested());    metadata.addTransientTopics(singleton("bar"));    assertTrue(metadata.updateRequested());    Map<String, Integer> topicPartitionCounts = new HashMap<>();    topicPartitionCounts.put("foo", 1);    topicPartitionCounts.put("bar", 1);    metadata.update(TestUtils.metadataUpdateWith(1, topicPartitionCounts), time.milliseconds());    assertFalse(metadata.updateRequested());    assertEquals(Utils.mkSet("foo", "bar"), new HashSet<>(metadata.fetch().topics()));    metadata.clearTransientTopics();    metadata.update(TestUtils.metadataUpdateWith(1, topicPartitionCounts), time.milliseconds());    assertEquals(singleton("foo"), new HashSet<>(metadata.fetch().topics()));}
f5949
0
testBasicSubscription
private void kafkatest_f5950_0(Set<String> expectedTopics, Set<String> expectedInternalTopics)
{    Set<String> allTopics = new HashSet<>();    allTopics.addAll(expectedTopics);    allTopics.addAll(expectedInternalTopics);    ConsumerMetadata metadata = newConsumerMetadata(false);    MetadataRequest.Builder builder = metadata.newMetadataRequestBuilder();    assertEquals(allTopics, new HashSet<>(builder.topics()));    List<MetadataResponse.TopicMetadata> topics = new ArrayList<>();    for (String expectedTopic : expectedTopics) topics.add(topicMetadata(expectedTopic, false));    for (String expectedInternalTopic : expectedInternalTopics) topics.add(topicMetadata(expectedInternalTopic, true));    MetadataResponse response = MetadataResponse.prepareResponse(singletonList(node), "clusterId", node.id(), topics);    metadata.update(response, time.milliseconds());    assertEquals(allTopics, metadata.fetch().topics());}
f5950
0
testTimeoutUnsentRequest
public void kafkatest_f5958_0()
{    // Delay connection to the node so that the request remains unsent    client.delayReady(node, 1000);    RequestFuture<ClientResponse> future = consumerClient.send(node, heartbeat(), 500);    consumerClient.pollNoWakeup();    // Ensure the request is pending, but hasn't been sent    assertTrue(consumerClient.hasPendingRequests());    assertFalse(client.hasInFlightRequests());    time.sleep(501);    consumerClient.pollNoWakeup();    assertFalse(consumerClient.hasPendingRequests());    assertTrue(future.failed());    assertTrue(future.exception() instanceof TimeoutException);}
f5958
0
doNotBlockIfPollConditionIsSatisfied
public void kafkatest_f5959_0()
{    NetworkClient mockNetworkClient = mock(NetworkClient.class);    ConsumerNetworkClient consumerClient = new ConsumerNetworkClient(new LogContext(), mockNetworkClient, metadata, time, 100, 1000, Integer.MAX_VALUE);    // expect poll, but with no timeout    consumerClient.poll(time.timer(Long.MAX_VALUE), () -> false);    verify(mockNetworkClient).poll(eq(0L), anyLong());}
f5959
0
blockWhenPollConditionNotSatisfied
public void kafkatest_f5960_0()
{    long timeout = 4000L;    NetworkClient mockNetworkClient = mock(NetworkClient.class);    ConsumerNetworkClient consumerClient = new ConsumerNetworkClient(new LogContext(), mockNetworkClient, metadata, time, 100, 1000, Integer.MAX_VALUE);    when(mockNetworkClient.inFlightRequestCount()).thenReturn(1);    consumerClient.poll(time.timer(timeout), () -> true);    verify(mockNetworkClient).poll(eq(timeout), anyLong());}
f5960
0
testMetadataFailurePropagated
public void kafkatest_f5968_0()
{    KafkaException metadataException = new KafkaException();    metadata.failedUpdate(time.milliseconds(), metadataException);    try {        consumerClient.poll(time.timer(Duration.ZERO));        fail("Expected poll to throw exception");    } catch (Exception e) {        assertEquals(metadataException, e);    }}
f5968
0
testFutureCompletionOutsidePoll
public void kafkatest_f5969_0() throws Exception
{    // Tests the scenario in which the request that is being awaited in one thread    // is received and completed in another thread.    final RequestFuture<ClientResponse> future = consumerClient.send(node, heartbeat());    // dequeue and send the request    consumerClient.pollNoWakeup();    client.enableBlockingUntilWakeup(2);    Thread t1 = new Thread() {        @Override        public void run() {            consumerClient.pollNoWakeup();        }    };    t1.start();    // Sleep a little so that t1 is blocking in poll    Thread.sleep(50);    Thread t2 = new Thread() {        @Override        public void run() {            consumerClient.poll(future);        }    };    t2.start();    // Sleep a little so that t2 is awaiting the network client lock    Thread.sleep(50);    // Simulate a network response and return from the poll in t1    client.respond(heartbeatResponse(Errors.NONE));    client.wakeup();    // Both threads should complete since t1 should wakeup t2    t1.join();    t2.join();    assertTrue(future.succeeded());}
f5969
0
run
public void kafkatest_f5970_0()
{    consumerClient.pollNoWakeup();}
f5970
0
heartbeat
private HeartbeatRequest.Builder kafkatest_f5978_0()
{    return new HeartbeatRequest.Builder(new HeartbeatRequestData().setGroupId("group").setGenerationId(1).setMemberId("memberId"));}
f5978
0
heartbeatResponse
private HeartbeatResponse kafkatest_f5979_0(Errors error)
{    return new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(error.code()));}
f5979
0
serializeDeserializeMetadata
public void kafkatest_f5980_0()
{    Subscription subscription = new Subscription(Arrays.asList("foo", "bar"), ByteBuffer.wrap(new byte[0]));    ByteBuffer buffer = ConsumerProtocol.serializeSubscription(subscription);    Subscription parsedSubscription = ConsumerProtocol.deserializeSubscription(buffer);    assertEquals(subscription.topics(), parsedSubscription.topics());    assertEquals(0, parsedSubscription.userData().limit());    assertFalse(parsedSubscription.groupInstanceId().isPresent());}
f5980
0
deserializeFutureAssignmentVersion
public void kafkatest_f5988_0()
{    // verify that a new version which adds a field is still parseable    short version = 100;    Schema assignmentSchemaV100 = new Schema(new Field(TOPIC_PARTITIONS_KEY_NAME, new ArrayOf(TOPIC_ASSIGNMENT_V0)), new Field(USER_DATA_KEY_NAME, Type.BYTES), new Field("foo", Type.STRING));    Struct assignmentV100 = new Struct(assignmentSchemaV100);    assignmentV100.set(TOPIC_PARTITIONS_KEY_NAME, new Object[] { new Struct(TOPIC_ASSIGNMENT_V0).set(ConsumerProtocol.TOPIC_KEY_NAME, tp1.topic()).set(ConsumerProtocol.PARTITIONS_KEY_NAME, new Object[] { tp1.partition() }) });    assignmentV100.set(USER_DATA_KEY_NAME, ByteBuffer.wrap(new byte[0]));    assignmentV100.set("foo", "bar");    Struct headerV100 = new Struct(CONSUMER_PROTOCOL_HEADER_SCHEMA);    headerV100.set(VERSION_KEY_NAME, version);    ByteBuffer buffer = ByteBuffer.allocate(assignmentV100.sizeOf() + headerV100.sizeOf());    headerV100.writeTo(buffer);    assignmentV100.writeTo(buffer);    buffer.flip();    Assignment assignment = ConsumerProtocol.deserializeAssignment(buffer);    assertEquals(toSet(Collections.singletonList(tp1)), toSet(assignment.partitions()));}
f5988
0
setup
public void kafkatest_f5989_0()
{    records = buildRecords(1L, 3, 1);    nextRecords = buildRecords(4L, 2, 4);    emptyRecords = buildRecords(0L, 0, 0);    partialRecords = buildRecords(4L, 1, 0);    partialRecords.buffer().putInt(Records.SIZE_OFFSET, 10000);}
f5989
0
assignFromUser
private void kafkatest_f5990_0(Set<TopicPartition> partitions)
{    subscriptions.assignFromUser(partitions);    client.updateMetadata(initialUpdateResponse);}
f5990
0
testFetchError
public void kafkatest_f5998_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NOT_LEADER_FOR_PARTITION, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> partitionRecords = fetchedRecords();    assertFalse(partitionRecords.containsKey(tp0));}
f5998
0
matchesOffset
private MockClient.RequestMatcher kafkatest_f5999_0(final TopicPartition tp, final long offset)
{    return new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            FetchRequest fetch = (FetchRequest) body;            return fetch.fetchData().containsKey(tp) && fetch.fetchData().get(tp).fetchOffset == offset;        }    };}
f5999
0
matches
public boolean kafkatest_f6000_0(AbstractRequest body)
{    FetchRequest fetch = (FetchRequest) body;    return fetch.fetchData().containsKey(tp) && fetch.fetchData().get(tp).fetchOffset == offset;}
f6000
0
testHeaders
public void kafkatest_f6008_0()
{    buildFetcher();    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 1L);    builder.append(0L, "key".getBytes(), "value-1".getBytes());    Header[] headersArray = new Header[1];    headersArray[0] = new RecordHeader("headerKey", "headerValue".getBytes(StandardCharsets.UTF_8));    builder.append(0L, "key".getBytes(), "value-2".getBytes(), headersArray);    Header[] headersArray2 = new Header[2];    headersArray2[0] = new RecordHeader("headerKey", "headerValue".getBytes(StandardCharsets.UTF_8));    headersArray2[1] = new RecordHeader("headerKey", "headerValue2".getBytes(StandardCharsets.UTF_8));    builder.append(0L, "key".getBytes(), "value-3".getBytes(), headersArray2);    MemoryRecords memoryRecords = builder.build();    List<ConsumerRecord<byte[], byte[]>> records;    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 1);    client.prepareResponse(matchesOffset(tp0, 1), fullFetchResponse(tp0, memoryRecords, Errors.NONE, 100L, 0));    assertEquals(1, fetcher.sendFetches());    consumerClient.poll(time.timer(0));    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> recordsByPartition = fetchedRecords();    records = recordsByPartition.get(tp0);    assertEquals(3, records.size());    Iterator<ConsumerRecord<byte[], byte[]>> recordIterator = records.iterator();    ConsumerRecord<byte[], byte[]> record = recordIterator.next();    assertNull(record.headers().lastHeader("headerKey"));    record = recordIterator.next();    assertEquals("headerValue", new String(record.headers().lastHeader("headerKey").value(), StandardCharsets.UTF_8));    assertEquals("headerKey", record.headers().lastHeader("headerKey").key());    record = recordIterator.next();    assertEquals("headerValue2", new String(record.headers().lastHeader("headerKey").value(), StandardCharsets.UTF_8));    assertEquals("headerKey", record.headers().lastHeader("headerKey").key());}
f6008
0
testFetchMaxPollRecords
public void kafkatest_f6009_0()
{    buildFetcher(2);    List<ConsumerRecord<byte[], byte[]>> records;    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 1);    client.prepareResponse(matchesOffset(tp0, 1), fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    client.prepareResponse(matchesOffset(tp0, 4), fullFetchResponse(tp0, this.nextRecords, Errors.NONE, 100L, 0));    assertEquals(1, fetcher.sendFetches());    consumerClient.poll(time.timer(0));    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> recordsByPartition = fetchedRecords();    records = recordsByPartition.get(tp0);    assertEquals(2, records.size());    assertEquals(3L, subscriptions.position(tp0).offset);    assertEquals(1, records.get(0).offset());    assertEquals(2, records.get(1).offset());    assertEquals(0, fetcher.sendFetches());    consumerClient.poll(time.timer(0));    recordsByPartition = fetchedRecords();    records = recordsByPartition.get(tp0);    assertEquals(1, records.size());    assertEquals(4L, subscriptions.position(tp0).offset);    assertEquals(3, records.get(0).offset());    assertTrue(fetcher.sendFetches() > 0);    consumerClient.poll(time.timer(0));    recordsByPartition = fetchedRecords();    records = recordsByPartition.get(tp0);    assertEquals(2, records.size());    assertEquals(6L, subscriptions.position(tp0).offset);    assertEquals(4, records.get(0).offset());    assertEquals(5, records.get(1).offset());}
f6009
0
testFetchAfterPartitionWithFetchedRecordsIsUnassigned
public void kafkatest_f6010_0()
{    buildFetcher(2);    List<ConsumerRecord<byte[], byte[]>> records;    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 1);    // Returns 3 records while `max.poll.records` is configured to 2    client.prepareResponse(matchesOffset(tp0, 1), fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    assertEquals(1, fetcher.sendFetches());    consumerClient.poll(time.timer(0));    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> recordsByPartition = fetchedRecords();    records = recordsByPartition.get(tp0);    assertEquals(2, records.size());    assertEquals(3L, subscriptions.position(tp0).offset);    assertEquals(1, records.get(0).offset());    assertEquals(2, records.get(1).offset());    assignFromUser(singleton(tp1));    client.prepareResponse(matchesOffset(tp1, 4), fullFetchResponse(tp1, this.nextRecords, Errors.NONE, 100L, 0));    subscriptions.seek(tp1, 4);    assertEquals(1, fetcher.sendFetches());    consumerClient.poll(time.timer(0));    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertNull(fetchedRecords.get(tp0));    records = fetchedRecords.get(tp1);    assertEquals(2, records.size());    assertEquals(6L, subscriptions.position(tp1).offset);    assertEquals(4, records.get(0).offset());    assertEquals(5, records.get(1).offset());}
f6010
0
testFetchOnPausedPartition
public void kafkatest_f6018_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    subscriptions.pause(tp0);    assertFalse(fetcher.sendFetches() > 0);    assertTrue(client.requests().isEmpty());}
f6018
0
testFetchOnCompletedFetchesForPausedAndResumedPartitions
public void kafkatest_f6019_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    subscriptions.pause(tp0);    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertEquals("Should not return any records when partition is paused", 0, fetchedRecords.size());    assertTrue("Should still contain completed fetches", fetcher.hasCompletedFetches());    assertFalse("Should not have any available (non-paused) completed fetches", fetcher.hasAvailableFetches());    assertNull(fetchedRecords.get(tp0));    assertEquals(0, fetcher.sendFetches());    subscriptions.resume(tp0);    assertTrue("Should have available (non-paused) completed fetches", fetcher.hasAvailableFetches());    consumerClient.poll(time.timer(0));    fetchedRecords = fetchedRecords();    assertEquals("Should return records when partition is resumed", 1, fetchedRecords.size());    assertNotNull(fetchedRecords.get(tp0));    assertEquals(3, fetchedRecords.get(tp0).size());    consumerClient.poll(time.timer(0));    fetchedRecords = fetchedRecords();    assertEquals("Should not return records after previously paused partitions are fetched", 0, fetchedRecords.size());    assertFalse("Should no longer contain completed fetches", fetcher.hasCompletedFetches());}
f6019
0
testFetchOnCompletedFetchesForSomePausedPartitions
public void kafkatest_f6020_0()
{    buildFetcher();    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords;    assignFromUser(Utils.mkSet(tp0, tp1));    // seek to tp0 and tp1 in two polls to generate 2 complete requests and responses    // #1 seek, request, poll, response    subscriptions.seek(tp0, 1);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    // #2 seek, request, poll, response    subscriptions.seek(tp1, 1);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp1, this.nextRecords, Errors.NONE, 100L, 0));    subscriptions.pause(tp0);    consumerClient.poll(time.timer(0));    fetchedRecords = fetchedRecords();    assertEquals("Should return completed fetch for unpaused partitions", 1, fetchedRecords.size());    assertTrue("Should still contain completed fetches", fetcher.hasCompletedFetches());    assertNotNull(fetchedRecords.get(tp1));    assertNull(fetchedRecords.get(tp0));    fetchedRecords = fetchedRecords();    assertEquals("Should return no records for remaining paused partition", 0, fetchedRecords.size());    assertTrue("Should still contain completed fetches", fetcher.hasCompletedFetches());}
f6020
0
testEpochSetInFetchRequest
public void kafkatest_f6028_0()
{    buildFetcher();    subscriptions.assignFromUser(singleton(tp0));    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), Collections.singletonMap(topicName, 4), tp -> 99);    client.updateMetadata(metadataResponse);    subscriptions.seek(tp0, 10);    assertEquals(1, fetcher.sendFetches());    // Check for epoch in outgoing request    MockClient.RequestMatcher matcher = body -> {        if (body instanceof FetchRequest) {            FetchRequest fetchRequest = (FetchRequest) body;            fetchRequest.fetchData().values().forEach(partitionData -> {                assertTrue("Expected Fetcher to set leader epoch in request", partitionData.currentLeaderEpoch.isPresent());                assertEquals("Expected leader epoch to match epoch from metadata update", 99, partitionData.currentLeaderEpoch.get().longValue());            });            return true;        } else {            fail("Should have seen FetchRequest");            return false;        }    };    client.prepareResponse(matcher, fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.pollNoWakeup();}
f6028
0
testFetchOffsetOutOfRange
public void kafkatest_f6029_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.OFFSET_OUT_OF_RANGE, 100L, 0));    consumerClient.poll(time.timer(0));    assertEquals(0, fetcher.fetchedRecords().size());    assertTrue(subscriptions.isOffsetResetNeeded(tp0));    assertNull(subscriptions.validPosition(tp0));    assertNotNull(subscriptions.position(tp0));}
f6029
0
testStaleOutOfRangeError
public void kafkatest_f6030_0()
{    // verify that an out of range error which arrives after a seek    // does not cause us to reset our position or throw an exception    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.OFFSET_OUT_OF_RANGE, 100L, 0));    subscriptions.seek(tp0, 1);    consumerClient.poll(time.timer(0));    assertEquals(0, fetcher.fetchedRecords().size());    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertEquals(1, subscriptions.position(tp0).offset);}
f6030
0
testUpdateFetchPositionNoOpWithPositionSet
public void kafkatest_f6038_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 5L);    fetcher.resetOffsetsIfNeeded();    assertFalse(client.hasInFlightRequests());    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(5, subscriptions.position(tp0).offset);}
f6038
0
testUpdateFetchPositionResetToDefaultOffset
public void kafkatest_f6039_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0);    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.EARLIEST_TIMESTAMP), listOffsetResponse(Errors.NONE, 1L, 5L));    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(5, subscriptions.position(tp0).offset);}
f6039
0
testUpdateFetchPositionResetToLatestOffset
public void kafkatest_f6040_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.LATEST);    client.updateMetadata(initialUpdateResponse);    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP), listOffsetResponse(Errors.NONE, 1L, 5L));    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(5, subscriptions.position(tp0).offset);}
f6040
0
testUpdateFetchPositionDisconnect
public void kafkatest_f6048_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.LATEST);    // First request gets a disconnect    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP), listOffsetResponse(Errors.NONE, 1L, 5L), true);    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.hasValidPosition(tp0));    // Expect a metadata refresh    client.prepareMetadataUpdate(initialUpdateResponse);    consumerClient.pollNoWakeup();    assertFalse(client.hasPendingMetadataUpdates());    // No retry until the backoff passes    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(client.hasInFlightRequests());    assertFalse(subscriptions.hasValidPosition(tp0));    // Next one succeeds    time.sleep(retryBackoffMs);    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP), listOffsetResponse(Errors.NONE, 1L, 5L));    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(5, subscriptions.position(tp0).offset);}
f6048
0
testAssignmentChangeWithInFlightReset
public void kafkatest_f6049_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.LATEST);    // Send the ListOffsets request to reset the position    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.hasValidPosition(tp0));    assertTrue(client.hasInFlightRequests());    // Now we have an assignment change    assignFromUser(singleton(tp1));    // The response returns and is discarded    client.respond(listOffsetResponse(Errors.NONE, 1L, 5L));    consumerClient.pollNoWakeup();    assertFalse(client.hasPendingResponses());    assertFalse(client.hasInFlightRequests());    assertFalse(subscriptions.isAssigned(tp0));}
f6049
0
testSeekWithInFlightReset
public void kafkatest_f6050_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.LATEST);    // Send the ListOffsets request to reset the position    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.hasValidPosition(tp0));    assertTrue(client.hasInFlightRequests());    // Now we get a seek from the user    subscriptions.seek(tp0, 237);    // The response returns and is discarded    client.respond(listOffsetResponse(Errors.NONE, 1L, 5L));    consumerClient.pollNoWakeup();    assertFalse(client.hasPendingResponses());    assertFalse(client.hasInFlightRequests());    assertEquals(237L, subscriptions.position(tp0).offset);}
f6050
0
testGetAllTopics
public void kafkatest_f6058_0()
{    // sending response before request, as getTopicMetadata is a blocking call    buildFetcher();    assignFromUser(singleton(tp0));    client.prepareResponse(newMetadataResponse(topicName, Errors.NONE));    Map<String, List<PartitionInfo>> allTopics = fetcher.getAllTopicMetadata(time.timer(5000L));    assertEquals(initialUpdateResponse.topicMetadata().size(), allTopics.size());}
f6058
0
testGetAllTopicsDisconnect
public void kafkatest_f6059_0()
{    // first try gets a disconnect, next succeeds    buildFetcher();    assignFromUser(singleton(tp0));    client.prepareResponse(null, true);    client.prepareResponse(newMetadataResponse(topicName, Errors.NONE));    Map<String, List<PartitionInfo>> allTopics = fetcher.getAllTopicMetadata(time.timer(5000L));    assertEquals(initialUpdateResponse.topicMetadata().size(), allTopics.size());}
f6059
0
testGetAllTopicsTimeout
public void kafkatest_f6060_0()
{    // since no response is prepared, the request should timeout    buildFetcher();    assignFromUser(singleton(tp0));    fetcher.getAllTopicMetadata(time.timer(50L));}
f6060
0
testFetcherLeadMetric
public void kafkatest_f6068_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    MetricName minLeadMetric = metrics.metricInstance(metricsRegistry.recordsLeadMin);    Map<String, String> tags = new HashMap<>(2);    tags.put("topic", tp0.topic());    tags.put("partition", String.valueOf(tp0.partition()));    MetricName partitionLeadMetric = metrics.metricName("records-lead", metricGroup, "", tags);    Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();    KafkaMetric recordsFetchLeadMin = allMetrics.get(minLeadMetric);    // recordsFetchLeadMin should be initialized to NaN    assertEquals(Double.NaN, (Double) recordsFetchLeadMin.metricValue(), EPSILON);    // recordsFetchLeadMin should be position - logStartOffset after receiving an empty FetchResponse    fetchRecords(tp0, MemoryRecords.EMPTY, Errors.NONE, 100L, -1L, 0L, 0);    assertEquals(0L, (Double) recordsFetchLeadMin.metricValue(), EPSILON);    KafkaMetric partitionLead = allMetrics.get(partitionLeadMetric);    assertEquals(0L, (Double) partitionLead.metricValue(), EPSILON);    // recordsFetchLeadMin should be position - logStartOffset after receiving a non-empty FetchResponse    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    for (int v = 0; v < 3; v++) {        builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, "key".getBytes(), ("value-" + v).getBytes());    }    fetchRecords(tp0, builder.build(), Errors.NONE, 200L, -1L, 0L, 0);    assertEquals(0L, (Double) recordsFetchLeadMin.metricValue(), EPSILON);    assertEquals(3L, (Double) partitionLead.metricValue(), EPSILON);    // verify de-registration of partition lag    subscriptions.unsubscribe();    fetcher.sendFetches();    assertFalse(allMetrics.containsKey(partitionLeadMetric));}
f6068
0
testReadCommittedLagMetric
public void kafkatest_f6069_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    MetricName maxLagMetric = metrics.metricInstance(metricsRegistry.recordsLagMax);    Map<String, String> tags = new HashMap<>();    tags.put("topic", tp0.topic());    tags.put("partition", String.valueOf(tp0.partition()));    MetricName partitionLagMetric = metrics.metricName("records-lag", metricGroup, tags);    Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();    KafkaMetric recordsFetchLagMax = allMetrics.get(maxLagMetric);    // recordsFetchLagMax should be initialized to NaN    assertEquals(Double.NaN, (Double) recordsFetchLagMax.metricValue(), EPSILON);    // recordsFetchLagMax should be lso - fetchOffset after receiving an empty FetchResponse    fetchRecords(tp0, MemoryRecords.EMPTY, Errors.NONE, 100L, 50L, 0);    assertEquals(50, (Double) recordsFetchLagMax.metricValue(), EPSILON);    KafkaMetric partitionLag = allMetrics.get(partitionLagMetric);    assertEquals(50, (Double) partitionLag.metricValue(), EPSILON);    // recordsFetchLagMax should be lso - offset of the last message after receiving a non-empty FetchResponse    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    for (int v = 0; v < 3; v++) builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, "key".getBytes(), ("value-" + v).getBytes());    fetchRecords(tp0, builder.build(), Errors.NONE, 200L, 150L, 0);    assertEquals(147, (Double) recordsFetchLagMax.metricValue(), EPSILON);    assertEquals(147, (Double) partitionLag.metricValue(), EPSILON);    // verify de-registration of partition lag    subscriptions.unsubscribe();    fetcher.sendFetches();    assertFalse(allMetrics.containsKey(partitionLagMetric));}
f6069
0
testFetchResponseMetrics
public void kafkatest_f6070_0()
{    buildFetcher();    String topic1 = "foo";    String topic2 = "bar";    TopicPartition tp1 = new TopicPartition(topic1, 0);    TopicPartition tp2 = new TopicPartition(topic2, 0);    subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(topic1, 1);    partitionCounts.put(topic2, 1);    client.updateMetadata(TestUtils.metadataUpdateWith(1, partitionCounts));    int expectedBytes = 0;    LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> fetchPartitionData = new LinkedHashMap<>();    for (TopicPartition tp : Utils.mkSet(tp1, tp2)) {        subscriptions.seek(tp, 0);        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 0L);        for (int v = 0; v < 3; v++) builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, "key".getBytes(), ("value-" + v).getBytes());        MemoryRecords records = builder.build();        for (Record record : records.records()) expectedBytes += record.sizeInBytes();        fetchPartitionData.put(tp, new FetchResponse.PartitionData<>(Errors.NONE, 15L, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));    }    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(new FetchResponse<>(Errors.NONE, fetchPartitionData, 0, INVALID_SESSION_ID));    consumerClient.poll(time.timer(0));    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertEquals(3, fetchedRecords.get(tp1).size());    assertEquals(3, fetchedRecords.get(tp2).size());    Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();    KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));    KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));    assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);    assertEquals(6, (Double) recordsCountAverage.metricValue(), EPSILON);}
f6070
0
testGetOffsetsForTimesTimeout
public void kafkatest_f6078_0()
{    try {        buildFetcher();        fetcher.offsetsForTimes(Collections.singletonMap(new TopicPartition(topicName, 2), 1000L), time.timer(100L));        fail("Should throw timeout exception.");    } catch (TimeoutException e) {    // let it go.    }}
f6078
0
testGetOffsetsForTimes
public void kafkatest_f6079_0()
{    buildFetcher();    // Empty map    assertTrue(fetcher.offsetsForTimes(new HashMap<TopicPartition, Long>(), time.timer(100L)).isEmpty());    // Unknown Offset    testGetOffsetsForTimesWithUnknownOffset();    // Error code none with unknown offset    testGetOffsetsForTimesWithError(Errors.NONE, Errors.NONE, -1L, 100L, null, 100L);    // Error code none with known offset    testGetOffsetsForTimesWithError(Errors.NONE, Errors.NONE, 10L, 100L, 10L, 100L);    // Test both of partition has error.    testGetOffsetsForTimesWithError(Errors.NOT_LEADER_FOR_PARTITION, Errors.INVALID_REQUEST, 10L, 100L, 10L, 100L);    // Test the second partition has error.    testGetOffsetsForTimesWithError(Errors.NONE, Errors.NOT_LEADER_FOR_PARTITION, 10L, 100L, 10L, 100L);    // Test different errors.    testGetOffsetsForTimesWithError(Errors.NOT_LEADER_FOR_PARTITION, Errors.NONE, 10L, 100L, 10L, 100L);    testGetOffsetsForTimesWithError(Errors.UNKNOWN_TOPIC_OR_PARTITION, Errors.NONE, 10L, 100L, 10L, 100L);    testGetOffsetsForTimesWithError(Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT, Errors.NONE, 10L, 100L, null, 100L);    testGetOffsetsForTimesWithError(Errors.BROKER_NOT_AVAILABLE, Errors.NONE, 10L, 100L, 10L, 100L);}
f6079
0
testGetOffsetsFencedLeaderEpoch
public void kafkatest_f6080_0()
{    buildFetcher();    subscriptions.assignFromUser(singleton(tp0));    client.updateMetadata(initialUpdateResponse);    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.LATEST);    client.prepareResponse(listOffsetResponse(Errors.FENCED_LEADER_EPOCH, 1L, 5L));    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertTrue(subscriptions.isOffsetResetNeeded(tp0));    assertFalse(subscriptions.isFetchable(tp0));    assertFalse(subscriptions.hasValidPosition(tp0));    assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));}
f6080
0
testReadCommittedWithCommittedAndAbortedTransactions
public void kafkatest_f6088_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    long pid1 = 1L;    long pid2 = 2L;    // Appends for producer 1 (eventually committed)    appendTransactionalRecords(buffer, pid1, 0L, new SimpleRecord("commit1-1".getBytes(), "value".getBytes()), new SimpleRecord("commit1-2".getBytes(), "value".getBytes()));    // Appends for producer 2 (eventually aborted)    appendTransactionalRecords(buffer, pid2, 2L, new SimpleRecord("abort2-1".getBytes(), "value".getBytes()));    // commit producer 1    commitTransaction(buffer, pid1, 3L);    // append more for producer 2 (eventually aborted)    appendTransactionalRecords(buffer, pid2, 4L, new SimpleRecord("abort2-2".getBytes(), "value".getBytes()));    // abort producer 2    abortTransaction(buffer, pid2, 5L);    abortedTransactions.add(new FetchResponse.AbortedTransaction(pid2, 2L));    // New transaction for producer 1 (eventually aborted)    appendTransactionalRecords(buffer, pid1, 6L, new SimpleRecord("abort1-1".getBytes(), "value".getBytes()));    // New transaction for producer 2 (eventually committed)    appendTransactionalRecords(buffer, pid2, 7L, new SimpleRecord("commit2-1".getBytes(), "value".getBytes()));    // Add messages for producer 1 (eventually aborted)    appendTransactionalRecords(buffer, pid1, 8L, new SimpleRecord("abort1-2".getBytes(), "value".getBytes()));    // abort producer 1    abortTransaction(buffer, pid1, 9L);    abortedTransactions.add(new FetchResponse.AbortedTransaction(1, 6));    // commit producer 2    commitTransaction(buffer, pid2, 10L);    buffer.flip();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.containsKey(tp0));    // There are only 3 committed records    List<ConsumerRecord<byte[], byte[]>> fetchedConsumerRecords = fetchedRecords.get(tp0);    Set<String> fetchedKeys = new HashSet<>();    for (ConsumerRecord<byte[], byte[]> consumerRecord : fetchedConsumerRecords) {        fetchedKeys.add(new String(consumerRecord.key(), StandardCharsets.UTF_8));    }    assertEquals(Utils.mkSet("commit1-1", "commit1-2", "commit2-1"), fetchedKeys);}
f6088
0
testMultipleAbortMarkers
public void kafkatest_f6089_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    int currentOffset = 0;    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "abort1-1".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "abort1-2".getBytes(), "value".getBytes()));    currentOffset += abortTransaction(buffer, 1L, currentOffset);    // Duplicate abort -- should be ignored.    currentOffset += abortTransaction(buffer, 1L, currentOffset);    // Now commit a transaction.    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "commit1-1".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "commit1-2".getBytes(), "value".getBytes()));    commitTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    abortedTransactions.add(new FetchResponse.AbortedTransaction(1, 0));    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.containsKey(tp0));    assertEquals(fetchedRecords.get(tp0).size(), 2);    List<ConsumerRecord<byte[], byte[]>> fetchedConsumerRecords = fetchedRecords.get(tp0);    Set<String> committedKeys = new HashSet<>(Arrays.asList("commit1-1", "commit1-2"));    Set<String> actuallyCommittedKeys = new HashSet<>();    for (ConsumerRecord<byte[], byte[]> consumerRecord : fetchedConsumerRecords) {        actuallyCommittedKeys.add(new String(consumerRecord.key(), StandardCharsets.UTF_8));    }    assertTrue(actuallyCommittedKeys.equals(committedKeys));}
f6089
0
testReadCommittedAbortMarkerWithNoData
public void kafkatest_f6090_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new StringDeserializer(), new StringDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    long producerId = 1L;    abortTransaction(buffer, producerId, 5L);    appendTransactionalRecords(buffer, producerId, 6L, new SimpleRecord("6".getBytes(), null), new SimpleRecord("7".getBytes(), null), new SimpleRecord("8".getBytes(), null));    commitTransaction(buffer, producerId, 9L);    buffer.flip();    // send the fetch    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    // prepare the response. the aborted transactions begin at offsets which are no longer in the log    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    abortedTransactions.add(new FetchResponse.AbortedTransaction(producerId, 0L));    client.prepareResponse(fullFetchResponseWithAbortedTransactions(MemoryRecords.readableRecords(buffer), abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<String, String>>> allFetchedRecords = fetchedRecords();    assertTrue(allFetchedRecords.containsKey(tp0));    List<ConsumerRecord<String, String>> fetchedRecords = allFetchedRecords.get(tp0);    assertEquals(3, fetchedRecords.size());    assertEquals(Arrays.asList(6L, 7L, 8L), collectRecordOffsets(fetchedRecords));}
f6090
0
testConsumingViaIncrementalFetchRequests
public void kafkatest_f6098_0()
{    buildFetcher(2);    List<ConsumerRecord<byte[], byte[]>> records;    assignFromUser(new HashSet<>(Arrays.asList(tp0, tp1)));    subscriptions.seekValidated(tp0, new SubscriptionState.FetchPosition(0, Optional.empty(), metadata.leaderAndEpoch(tp0)));    subscriptions.seekValidated(tp1, new SubscriptionState.FetchPosition(1, Optional.empty(), metadata.leaderAndEpoch(tp1)));    // Fetch some records and establish an incremental fetch session.    LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions1 = new LinkedHashMap<>();    partitions1.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 2L, 2, 0L, null, this.records));    partitions1.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100L, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, emptyRecords));    FetchResponse resp1 = new FetchResponse<>(Errors.NONE, partitions1, 0, 123);    client.prepareResponse(resp1);    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertFalse(fetchedRecords.containsKey(tp1));    records = fetchedRecords.get(tp0);    assertEquals(2, records.size());    assertEquals(3L, subscriptions.position(tp0).offset);    assertEquals(1L, subscriptions.position(tp1).offset);    assertEquals(1, records.get(0).offset());    assertEquals(2, records.get(1).offset());    // There is still a buffered record.    assertEquals(0, fetcher.sendFetches());    fetchedRecords = fetchedRecords();    assertFalse(fetchedRecords.containsKey(tp1));    records = fetchedRecords.get(tp0);    assertEquals(1, records.size());    assertEquals(3, records.get(0).offset());    assertEquals(4L, subscriptions.position(tp0).offset);    // The second response contains no new records.    LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions2 = new LinkedHashMap<>();    FetchResponse resp2 = new FetchResponse<>(Errors.NONE, partitions2, 0, 123);    client.prepareResponse(resp2);    assertEquals(1, fetcher.sendFetches());    consumerClient.poll(time.timer(0));    fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.isEmpty());    assertEquals(4L, subscriptions.position(tp0).offset);    assertEquals(1L, subscriptions.position(tp1).offset);    // The third response contains some new records for tp0.    LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions3 = new LinkedHashMap<>();    partitions3.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100L, 4, 0L, null, this.nextRecords));    FetchResponse resp3 = new FetchResponse<>(Errors.NONE, partitions3, 0, 123);    client.prepareResponse(resp3);    assertEquals(1, fetcher.sendFetches());    consumerClient.poll(time.timer(0));    fetchedRecords = fetchedRecords();    assertFalse(fetchedRecords.containsKey(tp1));    records = fetchedRecords.get(tp0);    assertEquals(2, records.size());    assertEquals(6L, subscriptions.position(tp0).offset);    assertEquals(1L, subscriptions.position(tp1).offset);    assertEquals(4, records.get(0).offset());    assertEquals(5, records.get(1).offset());}
f6098
0
testFetcherConcurrency
public void kafkatest_f6099_0() throws Exception
{    int numPartitions = 20;    Set<TopicPartition> topicPartitions = new HashSet<>();    for (int i = 0; i < numPartitions; i++) topicPartitions.add(new TopicPartition(topicName, i));    LogContext logContext = new LogContext();    buildDependencies(new MetricConfig(), Long.MAX_VALUE, new SubscriptionState(logContext, OffsetResetStrategy.EARLIEST), logContext);    fetcher = new Fetcher<byte[], byte[]>(new LogContext(), consumerClient, minBytes, maxBytes, maxWaitMs, fetchSize, 2 * numPartitions, true, "", new ByteArrayDeserializer(), new ByteArrayDeserializer(), metadata, subscriptions, metrics, metricsRegistry, time, retryBackoffMs, requestTimeoutMs, IsolationLevel.READ_UNCOMMITTED, apiVersions) {        @Override        protected FetchSessionHandler sessionHandler(int id) {            final FetchSessionHandler handler = super.sessionHandler(id);            if (handler == null)                return null;            else {                return new FetchSessionHandler(new LogContext(), id) {                    @Override                    public Builder newBuilder() {                        verifySessionPartitions();                        return handler.newBuilder();                    }                    @Override                    public boolean handleResponse(FetchResponse response) {                        verifySessionPartitions();                        return handler.handleResponse(response);                    }                    @Override                    public void handleError(Throwable t) {                        verifySessionPartitions();                        handler.handleError(t);                    }                    // Verify that session partitions can be traversed safely.                    private void verifySessionPartitions() {                        try {                            Field field = FetchSessionHandler.class.getDeclaredField("sessionPartitions");                            field.setAccessible(true);                            LinkedHashMap<?, ?> sessionPartitions = (LinkedHashMap<?, ?>) field.get(handler);                            for (Map.Entry<?, ?> entry : sessionPartitions.entrySet()) {                                // If `sessionPartitions` are modified on another thread, Thread.yield will increase the                                // possibility of ConcurrentModificationException if appropriate synchronization is not used.                                Thread.yield();                            }                        } catch (Exception e) {                            throw new RuntimeException(e);                        }                    }                };            }        }    };    MetadataResponse initialMetadataResponse = TestUtils.metadataUpdateWith(1, singletonMap(topicName, numPartitions));    client.updateMetadata(initialMetadataResponse);    fetchSize = 10000;    assignFromUser(topicPartitions);    topicPartitions.forEach(tp -> subscriptions.seek(tp, 0L));    AtomicInteger fetchesRemaining = new AtomicInteger(1000);    executorService = Executors.newSingleThreadExecutor();    Future<?> future = executorService.submit(() -> {        while (fetchesRemaining.get() > 0) {            synchronized (consumerClient) {                if (!client.requests().isEmpty()) {                    ClientRequest request = client.requests().peek();                    FetchRequest fetchRequest = (FetchRequest) request.requestBuilder().build();                    LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseMap = new LinkedHashMap<>();                    for (Map.Entry<TopicPartition, FetchRequest.PartitionData> entry : fetchRequest.fetchData().entrySet()) {                        TopicPartition tp = entry.getKey();                        long offset = entry.getValue().fetchOffset;                        responseMap.put(tp, new FetchResponse.PartitionData<>(Errors.NONE, offset + 2L, offset + 2, 0L, null, buildRecords(offset, 2, offset)));                    }                    client.respondToRequest(request, new FetchResponse<>(Errors.NONE, responseMap, 0, 123));                    consumerClient.poll(time.timer(0));                }            }        }        return fetchesRemaining.get();    });    Map<TopicPartition, Long> nextFetchOffsets = topicPartitions.stream().collect(Collectors.toMap(Function.identity(), t -> 0L));    while (fetchesRemaining.get() > 0 && !future.isDone()) {        if (fetcher.sendFetches() == 1) {            synchronized (consumerClient) {                consumerClient.poll(time.timer(0));            }        }        if (fetcher.hasCompletedFetches()) {            Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();            if (!fetchedRecords.isEmpty()) {                fetchesRemaining.decrementAndGet();                fetchedRecords.entrySet().forEach(entry -> {                    TopicPartition tp = entry.getKey();                    List<ConsumerRecord<byte[], byte[]>> records = entry.getValue();                    assertEquals(2, records.size());                    long nextOffset = nextFetchOffsets.get(tp);                    assertEquals(nextOffset, records.get(0).offset());                    assertEquals(nextOffset + 1, records.get(1).offset());                    nextFetchOffsets.put(tp, nextOffset + 2);                });            }        }    }    assertEquals(0, future.get());}
f6099
0
sessionHandler
protected FetchSessionHandler kafkatest_f6100_0(int id)
{    final FetchSessionHandler handler = super.sessionHandler(id);    if (handler == null)        return null;    else {        return new FetchSessionHandler(new LogContext(), id) {            @Override            public Builder newBuilder() {                verifySessionPartitions();                return handler.newBuilder();            }            @Override            public boolean handleResponse(FetchResponse response) {                verifySessionPartitions();                return handler.handleResponse(response);            }            @Override            public void handleError(Throwable t) {                verifySessionPartitions();                handler.handleError(t);            }            // Verify that session partitions can be traversed safely.            private void verifySessionPartitions() {                try {                    Field field = FetchSessionHandler.class.getDeclaredField("sessionPartitions");                    field.setAccessible(true);                    LinkedHashMap<?, ?> sessionPartitions = (LinkedHashMap<?, ?>) field.get(handler);                    for (Map.Entry<?, ?> entry : sessionPartitions.entrySet()) {                        // If `sessionPartitions` are modified on another thread, Thread.yield will increase the                        // possibility of ConcurrentModificationException if appropriate synchronization is not used.                        Thread.yield();                    }                } catch (Exception e) {                    throw new RuntimeException(e);                }            }        };    }}
f6100
0
buildRecords
private MemoryRecords kafkatest_f6108_0(long baseOffset, int count, long firstMessageId)
{    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, baseOffset);    for (int i = 0; i < count; i++) builder.append(0L, "key".getBytes(), ("value-" + (firstMessageId + i)).getBytes());    return builder.build();}
f6108
0
appendTransactionalRecords
private int kafkatest_f6109_0(ByteBuffer buffer, long pid, long baseOffset, int baseSequence, SimpleRecord... records)
{    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.CREATE_TIME, baseOffset, time.milliseconds(), pid, (short) 0, baseSequence, true, RecordBatch.NO_PARTITION_LEADER_EPOCH);    for (SimpleRecord record : records) {        builder.append(record);    }    builder.build();    return records.length;}
f6109
0
appendTransactionalRecords
private int kafkatest_f6110_0(ByteBuffer buffer, long pid, long baseOffset, SimpleRecord... records)
{    return appendTransactionalRecords(buffer, pid, baseOffset, (int) baseOffset, records);}
f6110
0
testOffsetValidationHandlesSeekWithInflightOffsetForLeaderRequest
public void kafkatest_f6118_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(tp0.topic(), 4);    final int epochOne = 1;    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochOne), 0L);    // Offset validation requires OffsetForLeaderEpoch request v3 or higher    Node node = metadata.fetch().nodes().get(0);    apiVersions.update(node.idString(), NodeApiVersions.create());    Metadata.LeaderAndEpoch leaderAndEpoch = new Metadata.LeaderAndEpoch(metadata.leaderAndEpoch(tp0).leader, Optional.of(epochOne));    subscriptions.seekUnvalidated(tp0, new SubscriptionState.FetchPosition(0, Optional.of(epochOne), leaderAndEpoch));    fetcher.validateOffsetsIfNeeded();    consumerClient.poll(time.timer(Duration.ZERO));    assertTrue(subscriptions.awaitingValidation(tp0));    assertTrue(client.hasInFlightRequests());    // While the OffsetForLeaderEpoch request is in-flight, we seek to a different offset.    subscriptions.seekUnvalidated(tp0, new SubscriptionState.FetchPosition(5, Optional.of(epochOne), leaderAndEpoch));    assertTrue(subscriptions.awaitingValidation(tp0));    client.respond(request -> {        OffsetsForLeaderEpochRequest epochRequest = (OffsetsForLeaderEpochRequest) request;        OffsetsForLeaderEpochRequest.PartitionData partitionData = epochRequest.epochsByTopicPartition().get(tp0);        return partitionData.currentLeaderEpoch.equals(Optional.of(epochOne)) && partitionData.leaderEpoch == epochOne;    }, new OffsetsForLeaderEpochResponse(singletonMap(tp0, new EpochEndOffset(0, 0L))));    consumerClient.poll(time.timer(Duration.ZERO));    // The response should be ignored since we were validating a different position.    assertTrue(subscriptions.awaitingValidation(tp0));}
f6118
0
testOffsetValidationFencing
public void kafkatest_f6119_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(tp0.topic(), 4);    final int epochOne = 1;    final int epochTwo = 2;    final int epochThree = 3;    // Start with metadata, epoch=1    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochOne), 0L);    // Offset validation requires OffsetForLeaderEpoch request v3 or higher    Node node = metadata.fetch().nodes().get(0);    apiVersions.update(node.idString(), NodeApiVersions.create());    // Seek with a position and leader+epoch    Metadata.LeaderAndEpoch leaderAndEpoch = new Metadata.LeaderAndEpoch(metadata.leaderAndEpoch(tp0).leader, Optional.of(epochOne));    subscriptions.seekValidated(tp0, new SubscriptionState.FetchPosition(0, Optional.of(epochOne), leaderAndEpoch));    // Update metadata to epoch=2, enter validation    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochTwo), 0L);    fetcher.validateOffsetsIfNeeded();    assertTrue(subscriptions.awaitingValidation(tp0));    // Update the position to epoch=3, as we would from a fetch    subscriptions.completeValidation(tp0);    SubscriptionState.FetchPosition nextPosition = new SubscriptionState.FetchPosition(10, Optional.of(epochTwo), new Metadata.LeaderAndEpoch(leaderAndEpoch.leader, Optional.of(epochTwo)));    subscriptions.position(tp0, nextPosition);    subscriptions.maybeValidatePositionForCurrentLeader(tp0, new Metadata.LeaderAndEpoch(leaderAndEpoch.leader, Optional.of(epochThree)));    // Prepare offset list response from async validation with epoch=2    Map<TopicPartition, EpochEndOffset> endOffsetMap = new HashMap<>();    endOffsetMap.put(tp0, new EpochEndOffset(Errors.NONE, epochTwo, 10L));    OffsetsForLeaderEpochResponse resp = new OffsetsForLeaderEpochResponse(endOffsetMap);    client.prepareResponse(resp);    consumerClient.pollNoWakeup();    assertTrue("Expected validation to fail since leader epoch changed", subscriptions.awaitingValidation(tp0));    // Next round of validation, should succeed in validating the position    fetcher.validateOffsetsIfNeeded();    endOffsetMap.clear();    endOffsetMap.put(tp0, new EpochEndOffset(Errors.NONE, epochThree, 10L));    resp = new OffsetsForLeaderEpochResponse(endOffsetMap);    client.prepareResponse(resp);    consumerClient.pollNoWakeup();    assertFalse("Expected validation to succeed with latest epoch", subscriptions.awaitingValidation(tp0));}
f6119
0
testTruncationDetected
public void kafkatest_f6120_0()
{    // Create some records that include a leader epoch (1)    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.CREATE_TIME, 0L, RecordBatch.NO_TIMESTAMP, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, // record epoch is earlier than the leader epoch on the client    1);    builder.appendWithOffset(0L, 0L, "key".getBytes(), "value-1".getBytes());    builder.appendWithOffset(1L, 0L, "key".getBytes(), "value-2".getBytes());    builder.appendWithOffset(2L, 0L, "key".getBytes(), "value-3".getBytes());    MemoryRecords records = builder.build();    buildFetcher();    assignFromUser(singleton(tp0));    // Initialize the epoch=2    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(tp0.topic(), 4);    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> 2);    metadata.update(metadataResponse, 0L);    // Offset validation requires OffsetForLeaderEpoch request v3 or higher    Node node = metadata.fetch().nodes().get(0);    apiVersions.update(node.idString(), NodeApiVersions.create());    // Seek    Metadata.LeaderAndEpoch leaderAndEpoch = new Metadata.LeaderAndEpoch(metadata.leaderAndEpoch(tp0).leader, Optional.of(1));    subscriptions.seekValidated(tp0, new SubscriptionState.FetchPosition(0, Optional.of(1), leaderAndEpoch));    // Check for truncation, this should cause tp0 to go into validation    fetcher.validateOffsetsIfNeeded();    // No fetches sent since we entered validation    assertEquals(0, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    assertTrue(subscriptions.awaitingValidation(tp0));    // Prepare OffsetForEpoch response then check that we update the subscription position correctly.    Map<TopicPartition, EpochEndOffset> endOffsetMap = new HashMap<>();    endOffsetMap.put(tp0, new EpochEndOffset(Errors.NONE, 1, 10L));    OffsetsForLeaderEpochResponse resp = new OffsetsForLeaderEpochResponse(endOffsetMap);    client.prepareResponse(resp);    consumerClient.pollNoWakeup();    assertFalse(subscriptions.awaitingValidation(tp0));    // Fetch again, now it works    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(fullFetchResponse(tp0, records, Errors.NONE, 100L, 0));    consumerClient.pollNoWakeup();    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> partitionRecords = fetchedRecords();    assertTrue(partitionRecords.containsKey(tp0));    assertEquals(subscriptions.position(tp0).offset, 3L);    assertOptional(subscriptions.position(tp0).offsetEpoch, value -> assertEquals(value.intValue(), 1));}
f6120
0
fullFetchResponse
private FetchResponse<MemoryRecords> kafkatest_f6128_0(TopicPartition tp, MemoryRecords records, Errors error, long hw, int throttleTime)
{    return fullFetchResponse(tp, records, error, hw, FetchResponse.INVALID_LAST_STABLE_OFFSET, throttleTime);}
f6128
0
fullFetchResponse
private FetchResponse<MemoryRecords> kafkatest_f6129_0(TopicPartition tp, MemoryRecords records, Errors error, long hw, long lastStableOffset, int throttleTime)
{    Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = Collections.singletonMap(tp, new FetchResponse.PartitionData<>(error, hw, lastStableOffset, 0L, null, records));    return new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions), throttleTime, INVALID_SESSION_ID);}
f6129
0
fullFetchResponse
private FetchResponse<MemoryRecords> kafkatest_f6130_0(TopicPartition tp, MemoryRecords records, Errors error, long hw, long lastStableOffset, int throttleTime, Optional<Integer> preferredReplicaId)
{    Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = Collections.singletonMap(tp, new FetchResponse.PartitionData<>(error, hw, lastStableOffset, 0L, preferredReplicaId, null, records));    return new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions), throttleTime, INVALID_SESSION_ID);}
f6130
0
buildFetcher
private void kafkatest_f6138_0(MetricConfig metricConfig, OffsetResetStrategy offsetResetStrategy, Deserializer<K> keyDeserializer, Deserializer<V> valueDeserializer, int maxPollRecords, IsolationLevel isolationLevel)
{    buildFetcher(metricConfig, offsetResetStrategy, keyDeserializer, valueDeserializer, maxPollRecords, isolationLevel, Long.MAX_VALUE);}
f6138
0
buildFetcher
private void kafkatest_f6139_0(MetricConfig metricConfig, OffsetResetStrategy offsetResetStrategy, Deserializer<K> keyDeserializer, Deserializer<V> valueDeserializer, int maxPollRecords, IsolationLevel isolationLevel, long metadataExpireMs)
{    LogContext logContext = new LogContext();    SubscriptionState subscriptionState = new SubscriptionState(logContext, offsetResetStrategy);    buildFetcher(metricConfig, keyDeserializer, valueDeserializer, maxPollRecords, isolationLevel, metadataExpireMs, subscriptionState, logContext);}
f6139
0
buildFetcher
private void kafkatest_f6140_0(SubscriptionState subscriptionState, LogContext logContext)
{    buildFetcher(new MetricConfig(), new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_UNCOMMITTED, Long.MAX_VALUE, subscriptionState, logContext);}
f6140
0
testSessionTimeoutExpired
public void kafkatest_f6148_0()
{    heartbeat.sentHeartbeat(time.milliseconds());    time.sleep(sessionTimeoutMs + 5);    assertTrue(heartbeat.sessionTimeoutExpired(time.milliseconds()));}
f6148
0
testResetSession
public void kafkatest_f6149_0()
{    heartbeat.sentHeartbeat(time.milliseconds());    time.sleep(sessionTimeoutMs + 5);    heartbeat.resetSessionTimeout();    assertFalse(heartbeat.sessionTimeoutExpired(time.milliseconds()));    // Resetting the session timeout should not reset the poll timeout    time.sleep(maxPollIntervalMs + 1);    heartbeat.resetSessionTimeout();    assertTrue(heartbeat.pollTimeoutExpired(time.milliseconds()));}
f6149
0
testResetTimeouts
public void kafkatest_f6150_0()
{    time.sleep(maxPollIntervalMs);    assertTrue(heartbeat.sessionTimeoutExpired(time.milliseconds()));    assertEquals(0, heartbeat.timeToNextHeartbeat(time.milliseconds()));    assertTrue(heartbeat.pollTimeoutExpired(time.milliseconds()));    heartbeat.resetTimeouts();    assertFalse(heartbeat.sessionTimeoutExpired(time.milliseconds()));    assertEquals(heartbeatIntervalMs, heartbeat.timeToNextHeartbeat(time.milliseconds()));    assertFalse(heartbeat.pollTimeoutExpired(time.milliseconds()));}
f6150
0
testUnexpectedEmptyResponse
public void kafkatest_f6158_0()
{    Map<TopicPartition, SubscriptionState.FetchPosition> positionMap = new HashMap<>();    positionMap.put(tp0, new SubscriptionState.FetchPosition(0, Optional.of(1), new Metadata.LeaderAndEpoch(Node.noNode(), Optional.of(1))));    OffsetsForLeaderEpochClient offsetClient = newOffsetClient();    RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future = offsetClient.sendAsyncRequest(Node.noNode(), positionMap);    OffsetsForLeaderEpochResponse resp = new OffsetsForLeaderEpochResponse(Collections.emptyMap());    client.prepareResponse(resp);    consumerClient.pollNoWakeup();    OffsetsForLeaderEpochClient.OffsetForEpochResult result = future.value();    assertFalse(result.partitionsToRetry().isEmpty());    assertTrue(result.endOffsets().isEmpty());}
f6158
0
testOkResponse
public void kafkatest_f6159_0()
{    Map<TopicPartition, SubscriptionState.FetchPosition> positionMap = new HashMap<>();    positionMap.put(tp0, new SubscriptionState.FetchPosition(0, Optional.of(1), new Metadata.LeaderAndEpoch(Node.noNode(), Optional.of(1))));    OffsetsForLeaderEpochClient offsetClient = newOffsetClient();    RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future = offsetClient.sendAsyncRequest(Node.noNode(), positionMap);    Map<TopicPartition, EpochEndOffset> endOffsetMap = new HashMap<>();    endOffsetMap.put(tp0, new EpochEndOffset(Errors.NONE, 1, 10L));    client.prepareResponse(new OffsetsForLeaderEpochResponse(endOffsetMap));    consumerClient.pollNoWakeup();    OffsetsForLeaderEpochClient.OffsetForEpochResult result = future.value();    assertTrue(result.partitionsToRetry().isEmpty());    assertTrue(result.endOffsets().containsKey(tp0));    assertEquals(result.endOffsets().get(tp0).error(), Errors.NONE);    assertEquals(result.endOffsets().get(tp0).leaderEpoch(), 1);    assertEquals(result.endOffsets().get(tp0).endOffset(), 10L);}
f6159
0
testUnauthorizedTopic
public void kafkatest_f6160_0()
{    Map<TopicPartition, SubscriptionState.FetchPosition> positionMap = new HashMap<>();    positionMap.put(tp0, new SubscriptionState.FetchPosition(0, Optional.of(1), new Metadata.LeaderAndEpoch(Node.noNode(), Optional.of(1))));    OffsetsForLeaderEpochClient offsetClient = newOffsetClient();    RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future = offsetClient.sendAsyncRequest(Node.noNode(), positionMap);    Map<TopicPartition, EpochEndOffset> endOffsetMap = new HashMap<>();    endOffsetMap.put(tp0, new EpochEndOffset(Errors.TOPIC_AUTHORIZATION_FAILED, -1, -1));    client.prepareResponse(new OffsetsForLeaderEpochResponse(endOffsetMap));    consumerClient.pollNoWakeup();    assertTrue(future.failed());    assertEquals(future.exception().getClass(), TopicAuthorizationException.class);    assertTrue(((TopicAuthorizationException) future.exception()).unauthorizedTopics().contains(tp0.topic()));}
f6160
0
shouldInstantiateFromListOfOldAndNewClassTypes
public void kafkatest_f6168_0()
{    Properties props = new Properties();    props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    classTypes = Arrays.asList(StickyAssignor.class, OldPartitionAssignor.class);    props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, classTypes);    KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props, new StringDeserializer(), new StringDeserializer());    consumer.close();}
f6168
0
shouldThrowKafkaExceptionOnListWithNonAssignorClassType
public void kafkatest_f6169_0()
{    Properties props = new Properties();    props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    classTypes = Arrays.asList(StickyAssignor.class, OldPartitionAssignor.class, String.class);    props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, classTypes);    assertThrows(KafkaException.class, () -> new KafkaConsumer<>(props, new StringDeserializer(), new StringDeserializer()));}
f6169
0
testOnAssignment
public void kafkatest_f6170_0()
{    OldPartitionAssignor oldAssignor = new OldPartitionAssignor();    ConsumerPartitionAssignor adaptedAssignor = new PartitionAssignorAdapter(oldAssignor);    TopicPartition tp1 = new TopicPartition("tp1", 1);    TopicPartition tp2 = new TopicPartition("tp2", 2);    List<TopicPartition> partitions = Arrays.asList(tp1, tp2);    adaptedAssignor.onAssignment(new Assignment(partitions), new ConsumerGroupMetadata("", 1, "", Optional.empty()));    assertEquals(oldAssignor.partitions, partitions);}
f6170
0
testVoidFuture
public void kafkatest_f6178_0()
{    RequestFuture<Void> future = new RequestFuture<>();    future.complete(null);    assertTrue(future.isDone());    assertNull(future.value());}
f6178
0
testRuntimeExceptionInComplete
public void kafkatest_f6179_0()
{    RequestFuture<Exception> future = new RequestFuture<>();    future.complete(new RuntimeException());}
f6179
0
invokeCompleteAfterAlreadyComplete
public void kafkatest_f6180_0()
{    RequestFuture<Void> future = new RequestFuture<>();    future.complete(null);    future.complete(null);}
f6180
0
listenerInvokedIfAddedAfterFutureCompletion
public void kafkatest_f6188_0()
{    RequestFuture<Void> future = new RequestFuture<>();    future.complete(null);    MockRequestFutureListener<Void> listener = new MockRequestFutureListener<>();    future.addListener(listener);    assertOnSuccessInvoked(listener);}
f6188
0
listenerInvokedIfAddedAfterFutureFailure
public void kafkatest_f6189_0()
{    RequestFuture<Void> future = new RequestFuture<>();    future.raise(new RuntimeException());    MockRequestFutureListener<Void> listener = new MockRequestFutureListener<>();    future.addListener(listener);    assertOnFailureInvoked(listener);}
f6189
0
listenersInvokedIfAddedBeforeAndAfterFailure
public void kafkatest_f6190_0()
{    RequestFuture<Void> future = new RequestFuture<>();    MockRequestFutureListener<Void> beforeListener = new MockRequestFutureListener<>();    future.addListener(beforeListener);    future.raise(new RuntimeException());    MockRequestFutureListener<Void> afterListener = new MockRequestFutureListener<>();    future.addListener(afterListener);    assertOnFailureInvoked(beforeListener);    assertOnFailureInvoked(afterListener);}
f6190
0
onSuccess
public void kafkatest_f6198_0(T value)
{    numOnSuccessCalls.incrementAndGet();}
f6198
0
onFailure
public void kafkatest_f6199_0(RuntimeException e)
{    numOnFailureCalls.incrementAndGet();}
f6199
0
partitionAssignment
public void kafkatest_f6200_0()
{    state.assignFromUser(singleton(tp0));    assertEquals(singleton(tp0), state.assignedPartitions());    assertEquals(1, state.numAssignedPartitions());    assertFalse(state.hasAllFetchPositions());    state.seek(tp0, 1);    assertTrue(state.isFetchable(tp0));    assertEquals(1L, state.position(tp0).offset);    state.assignFromUser(Collections.emptySet());    assertTrue(state.assignedPartitions().isEmpty());    assertEquals(0, state.numAssignedPartitions());    assertFalse(state.isAssigned(tp0));    assertFalse(state.isFetchable(tp0));}
f6200
0
cantAssignPartitionForUnsubscribedTopics
public void kafkatest_f6208_0()
{    state.subscribe(singleton(topic), rebalanceListener);    assertFalse(state.checkAssignmentMatchedSubscription(Collections.singletonList(t1p0)));}
f6208
0
cantAssignPartitionForUnmatchedPattern
public void kafkatest_f6209_0()
{    state.subscribe(Pattern.compile(".*t"), rebalanceListener);    state.subscribeFromPattern(new HashSet<>(Collections.singletonList(topic)));    assertFalse(state.checkAssignmentMatchedSubscription(Collections.singletonList(t1p0)));}
f6209
0
cantChangePositionForNonAssignedPartition
public void kafkatest_f6210_0()
{    state.position(tp0, new SubscriptionState.FetchPosition(1, Optional.empty(), leaderAndEpoch));}
f6210
0
unsubscription
public void kafkatest_f6218_0()
{    state.subscribe(Pattern.compile(".*"), rebalanceListener);    state.subscribeFromPattern(new HashSet<>(Arrays.asList(topic, topic1)));    assertTrue(state.checkAssignmentMatchedSubscription(singleton(tp1)));    state.assignFromSubscribed(singleton(tp1));    assertEquals(singleton(tp1), state.assignedPartitions());    assertEquals(1, state.numAssignedPartitions());    state.unsubscribe();    assertEquals(0, state.subscription().size());    assertTrue(state.assignedPartitions().isEmpty());    assertEquals(0, state.numAssignedPartitions());    state.assignFromUser(singleton(tp0));    assertEquals(singleton(tp0), state.assignedPartitions());    assertEquals(1, state.numAssignedPartitions());    state.unsubscribe();    assertEquals(0, state.subscription().size());    assertTrue(state.assignedPartitions().isEmpty());    assertEquals(0, state.numAssignedPartitions());}
f6218
0
testPreferredReadReplicaLease
public void kafkatest_f6219_0()
{    state.assignFromUser(Collections.singleton(tp0));    // Default state    assertFalse(state.preferredReadReplica(tp0, 0L).isPresent());    // Set the preferred replica with lease    state.updatePreferredReadReplica(tp0, 42, () -> 10L);    TestUtils.assertOptional(state.preferredReadReplica(tp0, 9L), value -> assertEquals(value.intValue(), 42));    TestUtils.assertOptional(state.preferredReadReplica(tp0, 10L), value -> assertEquals(value.intValue(), 42));    assertFalse(state.preferredReadReplica(tp0, 11L).isPresent());    // Unset the preferred replica    state.clearPreferredReadReplica(tp0);    assertFalse(state.preferredReadReplica(tp0, 9L).isPresent());    assertFalse(state.preferredReadReplica(tp0, 11L).isPresent());    // Set to new preferred replica with lease    state.updatePreferredReadReplica(tp0, 43, () -> 20L);    TestUtils.assertOptional(state.preferredReadReplica(tp0, 11L), value -> assertEquals(value.intValue(), 43));    TestUtils.assertOptional(state.preferredReadReplica(tp0, 20L), value -> assertEquals(value.intValue(), 43));    assertFalse(state.preferredReadReplica(tp0, 21L).isPresent());    // Set to new preferred replica without clearing first    state.updatePreferredReadReplica(tp0, 44, () -> 30L);    TestUtils.assertOptional(state.preferredReadReplica(tp0, 30L), value -> assertEquals(value.intValue(), 44));    assertFalse(state.preferredReadReplica(tp0, 31L).isPresent());}
f6219
0
testSeekUnvalidatedWithNoOffsetEpoch
public void kafkatest_f6220_0()
{    Node broker1 = new Node(1, "localhost", 9092);    state.assignFromUser(Collections.singleton(tp0));    // Seek with no offset epoch requires no validation no matter what the current leader is    state.seekUnvalidated(tp0, new SubscriptionState.FetchPosition(0L, Optional.empty(), new Metadata.LeaderAndEpoch(broker1, Optional.of(5))));    assertTrue(state.hasValidPosition(tp0));    assertFalse(state.awaitingValidation(tp0));    assertFalse(state.maybeValidatePositionForCurrentLeader(tp0, new Metadata.LeaderAndEpoch(broker1, Optional.empty())));    assertTrue(state.hasValidPosition(tp0));    assertFalse(state.awaitingValidation(tp0));    assertFalse(state.maybeValidatePositionForCurrentLeader(tp0, new Metadata.LeaderAndEpoch(broker1, Optional.of(10))));    assertTrue(state.hasValidPosition(tp0));    assertFalse(state.awaitingValidation(tp0));}
f6220
0
testMaybeCompleteValidationAfterOffsetReset
public void kafkatest_f6228_0()
{    Node broker1 = new Node(1, "localhost", 9092);    state.assignFromUser(Collections.singleton(tp0));    int currentEpoch = 10;    long initialOffset = 10L;    int initialOffsetEpoch = 5;    SubscriptionState.FetchPosition initialPosition = new SubscriptionState.FetchPosition(initialOffset, Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, initialPosition);    assertTrue(state.awaitingValidation(tp0));    state.requestOffsetReset(tp0);    Optional<OffsetAndMetadata> divergentOffsetMetadataOpt = state.maybeCompleteValidation(tp0, initialPosition, new EpochEndOffset(initialOffsetEpoch, initialOffset + 5));    assertEquals(Optional.empty(), divergentOffsetMetadataOpt);    assertFalse(state.awaitingValidation(tp0));    assertTrue(state.isOffsetResetNeeded(tp0));}
f6228
0
testTruncationDetectionWithResetPolicy
public void kafkatest_f6229_0()
{    Node broker1 = new Node(1, "localhost", 9092);    state.assignFromUser(Collections.singleton(tp0));    int currentEpoch = 10;    long initialOffset = 10L;    int initialOffsetEpoch = 5;    long divergentOffset = 5L;    int divergentOffsetEpoch = 7;    SubscriptionState.FetchPosition initialPosition = new SubscriptionState.FetchPosition(initialOffset, Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, initialPosition);    assertTrue(state.awaitingValidation(tp0));    Optional<OffsetAndMetadata> divergentOffsetMetadata = state.maybeCompleteValidation(tp0, initialPosition, new EpochEndOffset(divergentOffsetEpoch, divergentOffset));    assertEquals(Optional.empty(), divergentOffsetMetadata);    assertFalse(state.awaitingValidation(tp0));    SubscriptionState.FetchPosition updatedPosition = new SubscriptionState.FetchPosition(divergentOffset, Optional.of(divergentOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    assertEquals(updatedPosition, state.position(tp0));}
f6229
0
testTruncationDetectionWithoutResetPolicy
public void kafkatest_f6230_0()
{    Node broker1 = new Node(1, "localhost", 9092);    state = new SubscriptionState(new LogContext(), OffsetResetStrategy.NONE);    state.assignFromUser(Collections.singleton(tp0));    int currentEpoch = 10;    long initialOffset = 10L;    int initialOffsetEpoch = 5;    long divergentOffset = 5L;    int divergentOffsetEpoch = 7;    SubscriptionState.FetchPosition initialPosition = new SubscriptionState.FetchPosition(initialOffset, Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, initialPosition);    assertTrue(state.awaitingValidation(tp0));    Optional<OffsetAndMetadata> divergentOffsetMetadata = state.maybeCompleteValidation(tp0, initialPosition, new EpochEndOffset(divergentOffsetEpoch, divergentOffset));    assertEquals(Optional.of(new OffsetAndMetadata(divergentOffset, Optional.of(divergentOffsetEpoch), "")), divergentOffsetMetadata);    assertTrue(state.awaitingValidation(tp0));}
f6230
0
shouldIgnoreGroupInstanceIdForEmptyGroupId
public void kafkatest_f6238_0()
{    Map<String, Object> config = new HashMap<>();    config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    config.put(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG, "instance_id");    KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());    consumer.close();}
f6238
0
testSubscription
public void kafkatest_f6239_0()
{    KafkaConsumer<byte[], byte[]> consumer = newConsumer(groupId);    consumer.subscribe(singletonList(topic));    assertEquals(singleton(topic), consumer.subscription());    assertTrue(consumer.assignment().isEmpty());    consumer.subscribe(Collections.<String>emptyList());    assertTrue(consumer.subscription().isEmpty());    assertTrue(consumer.assignment().isEmpty());    consumer.assign(singletonList(tp0));    assertTrue(consumer.subscription().isEmpty());    assertEquals(singleton(tp0), consumer.assignment());    consumer.unsubscribe();    assertTrue(consumer.subscription().isEmpty());    assertTrue(consumer.assignment().isEmpty());    consumer.close();}
f6239
0
testSubscriptionOnNullTopicCollection
public void kafkatest_f6240_0()
{    try (KafkaConsumer<byte[], byte[]> consumer = newConsumer(groupId)) {        consumer.subscribe((List<String>) null);    }}
f6240
0
testAssignOnNullTopicInPartition
public void kafkatest_f6248_0()
{    try (KafkaConsumer<byte[], byte[]> consumer = newConsumer((String) null)) {        consumer.assign(singleton(new TopicPartition(null, 0)));    }}
f6248
0
testAssignOnEmptyTopicInPartition
public void kafkatest_f6249_0()
{    try (KafkaConsumer<byte[], byte[]> consumer = newConsumer((String) null)) {        consumer.assign(singleton(new TopicPartition("  ", 0)));    }}
f6249
0
testInterceptorConstructorClose
public void kafkatest_f6250_0() throws Exception
{    try {        Properties props = new Properties();        // test with client ID assigned by KafkaConsumer        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");        props.setProperty(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, MockConsumerInterceptor.class.getName());        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props, new StringDeserializer(), new StringDeserializer());        assertEquals(1, MockConsumerInterceptor.INIT_COUNT.get());        assertEquals(0, MockConsumerInterceptor.CLOSE_COUNT.get());        consumer.close();        assertEquals(1, MockConsumerInterceptor.INIT_COUNT.get());        assertEquals(1, MockConsumerInterceptor.CLOSE_COUNT.get());        // Cluster metadata will only be updated on calling poll.        Assert.assertNull(MockConsumerInterceptor.CLUSTER_META.get());    } finally {        // cleanup since we are using mutable static variables in MockConsumerInterceptor        MockConsumerInterceptor.resetCounters();    }}
f6250
0
verifyDeprecatedPollDoesNotTimeOutDuringMetadataUpdate
public void kafkatest_f6258_0()
{    final Time time = new MockTime();    final SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    final ConsumerMetadata metadata = createMetadata(subscription);    final MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    final ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    final KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    prepareRebalance(client, node, assignor, singletonList(tp0), null);    consumer.poll(0L);    // The underlying client SHOULD get a fetch request    final Queue<ClientRequest> requests = client.requests();    Assert.assertEquals(1, requests.size());    final Class<? extends AbstractRequest.Builder> aClass = requests.peek().requestBuilder().getClass();    Assert.assertEquals(FetchRequest.Builder.class, aClass);}
f6258
0
verifyNoCoordinatorLookupForManualAssignmentWithSeek
public void kafkatest_f6259_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.assign(singleton(tp0));    consumer.seekToBeginning(singleton(tp0));    // there shouldn't be any need to lookup the coordinator or fetch committed offsets.    // we just lookup the starting position and send the record fetch.    client.prepareResponse(listOffsetsResponse(Collections.singletonMap(tp0, 50L)));    client.prepareResponse(fetchResponse(tp0, 50L, 5));    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));    assertEquals(5, records.count());    assertEquals(55L, consumer.position(tp0));    consumer.close(Duration.ofMillis(0));}
f6259
0
testFetchProgressWithMissingPartitionPosition
public void kafkatest_f6260_0()
{    // Verifies that we can make progress on one partition while we are awaiting    // a reset on another partition.    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 2));    KafkaConsumer<String, String> consumer = newConsumerNoAutoCommit(time, client, subscription, metadata);    consumer.assign(Arrays.asList(tp0, tp1));    consumer.seekToEnd(singleton(tp0));    consumer.seekToBeginning(singleton(tp1));    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            ListOffsetRequest request = (ListOffsetRequest) body;            Map<TopicPartition, ListOffsetRequest.PartitionData> timestamps = request.partitionTimestamps();            return timestamps.get(tp0).timestamp == ListOffsetRequest.LATEST_TIMESTAMP && timestamps.get(tp1).timestamp == ListOffsetRequest.EARLIEST_TIMESTAMP;        }    }, listOffsetsResponse(Collections.singletonMap(tp0, 50L), Collections.singletonMap(tp1, Errors.NOT_LEADER_FOR_PARTITION)));    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            FetchRequest request = (FetchRequest) body;            return request.fetchData().keySet().equals(singleton(tp0)) && request.fetchData().get(tp0).fetchOffset == 50L;        }    }, fetchResponse(tp0, 50L, 5));    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));    assertEquals(5, records.count());    assertEquals(singleton(tp0), records.partitions());}
f6260
0
testCommitsFetchedDuringAssign
public void kafkatest_f6268_0()
{    long offset1 = 10000;    long offset2 = 20000;    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 2));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.assign(singletonList(tp0));    // lookup coordinator    client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);    Node coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    // fetch offset for one topic    client.prepareResponseFrom(offsetResponse(Collections.singletonMap(tp0, offset1), Errors.NONE), coordinator);    assertEquals(offset1, consumer.committed(tp0).offset());    consumer.assign(Arrays.asList(tp0, tp1));    // fetch offset for two topics    Map<TopicPartition, Long> offsets = new HashMap<>();    offsets.put(tp0, offset1);    client.prepareResponseFrom(offsetResponse(offsets, Errors.NONE), coordinator);    assertEquals(offset1, consumer.committed(tp0).offset());    offsets.remove(tp0);    offsets.put(tp1, offset2);    client.prepareResponseFrom(offsetResponse(offsets, Errors.NONE), coordinator);    assertEquals(offset2, consumer.committed(tp1).offset());    consumer.close(Duration.ofMillis(0));}
f6268
0
testAutoCommitSentBeforePositionUpdate
public void kafkatest_f6269_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    Node coordinator = prepareRebalance(client, node, assignor, singletonList(tp0), null);    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    consumer.poll(Duration.ZERO);    // respond to the outstanding fetch so that we have data available on the next poll    client.respondFrom(fetchResponse(tp0, 0, 5), node);    client.poll(0, time.milliseconds());    time.sleep(autoCommitIntervalMs);    client.prepareResponseFrom(fetchResponse(tp0, 5, 0), node);    // no data has been returned to the user yet, so the committed offset should be 0    AtomicBoolean commitReceived = prepareOffsetCommitResponse(client, coordinator, tp0, 0);    consumer.poll(Duration.ZERO);    assertTrue(commitReceived.get());    consumer.close(Duration.ofMillis(0));}
f6269
0
testRegexSubscription
public void kafkatest_f6270_0()
{    String unmatchedTopic = "unmatched";    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(topic, 1);    partitionCounts.put(unmatchedTopic, 1);    initMetadata(client, partitionCounts);    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    prepareRebalance(client, node, singleton(topic), assignor, singletonList(tp0), null);    consumer.subscribe(Pattern.compile(topic), getConsumerRebalanceListener(consumer));    client.prepareMetadataUpdate(TestUtils.metadataUpdateWith(1, partitionCounts));    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    assertEquals(singleton(topic), consumer.subscription());    assertEquals(singleton(tp0), consumer.assignment());    consumer.close(Duration.ofMillis(0));}
f6270
0
testManualAssignmentChangeWithAutoCommitEnabled
public void kafkatest_f6278_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    Map<String, Integer> tpCounts = new HashMap<>();    tpCounts.put(topic, 1);    tpCounts.put(topic2, 1);    initMetadata(client, tpCounts);    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    // lookup coordinator    client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);    Node coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    // manual assignment    consumer.assign(singleton(tp0));    consumer.seekToBeginning(singleton(tp0));    // fetch offset for one topic    client.prepareResponseFrom(offsetResponse(Collections.singletonMap(tp0, 0L), Errors.NONE), coordinator);    assertEquals(0, consumer.committed(tp0).offset());    // verify that assignment immediately changes    assertTrue(consumer.assignment().equals(singleton(tp0)));    // there shouldn't be any need to lookup the coordinator or fetch committed offsets.    // we just lookup the starting position and send the record fetch.    client.prepareResponse(listOffsetsResponse(Collections.singletonMap(tp0, 10L)));    client.prepareResponse(fetchResponse(tp0, 10L, 1));    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));    assertEquals(1, records.count());    assertEquals(11L, consumer.position(tp0));    // mock the offset commit response for to be revoked partitions    AtomicBoolean commitReceived = prepareOffsetCommitResponse(client, coordinator, tp0, 11);    // new manual assignment    consumer.assign(singleton(t2p0));    // verify that assignment immediately changes    assertTrue(consumer.assignment().equals(singleton(t2p0)));    // verify that the offset commits occurred as expected    assertTrue(commitReceived.get());    client.requests().clear();    consumer.close();}
f6278
0
testManualAssignmentChangeWithAutoCommitDisabled
public void kafkatest_f6279_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    Map<String, Integer> tpCounts = new HashMap<>();    tpCounts.put(topic, 1);    tpCounts.put(topic2, 1);    initMetadata(client, tpCounts);    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, false, groupInstanceId);    // lookup coordinator    client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);    Node coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    // manual assignment    consumer.assign(singleton(tp0));    consumer.seekToBeginning(singleton(tp0));    // fetch offset for one topic    client.prepareResponseFrom(offsetResponse(Collections.singletonMap(tp0, 0L), Errors.NONE), coordinator);    assertEquals(0, consumer.committed(tp0).offset());    // verify that assignment immediately changes    assertTrue(consumer.assignment().equals(singleton(tp0)));    // there shouldn't be any need to lookup the coordinator or fetch committed offsets.    // we just lookup the starting position and send the record fetch.    client.prepareResponse(listOffsetsResponse(Collections.singletonMap(tp0, 10L)));    client.prepareResponse(fetchResponse(tp0, 10L, 1));    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));    assertEquals(1, records.count());    assertEquals(11L, consumer.position(tp0));    // new manual assignment    consumer.assign(singleton(t2p0));    // verify that assignment immediately changes    assertTrue(consumer.assignment().equals(singleton(t2p0)));    // the auto commit is disabled, so no offset commit request should be sent    for (ClientRequest req : client.requests()) assertTrue(req.requestBuilder().apiKey() != ApiKeys.OFFSET_COMMIT);    client.requests().clear();    consumer.close();}
f6279
0
testOffsetOfPausedPartitions
public void kafkatest_f6280_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 2));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    // lookup coordinator    client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);    Node coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    // manual assignment    Set<TopicPartition> partitions = Utils.mkSet(tp0, tp1);    consumer.assign(partitions);    // verify consumer's assignment    assertEquals(partitions, consumer.assignment());    consumer.pause(partitions);    consumer.seekToEnd(partitions);    // fetch and verify committed offset of two partitions    Map<TopicPartition, Long> offsets = new HashMap<>();    offsets.put(tp0, 0L);    offsets.put(tp1, 0L);    client.prepareResponseFrom(offsetResponse(offsets, Errors.NONE), coordinator);    assertEquals(0, consumer.committed(tp0).offset());    offsets.remove(tp0);    offsets.put(tp1, 0L);    client.prepareResponseFrom(offsetResponse(offsets, Errors.NONE), coordinator);    assertEquals(0, consumer.committed(tp1).offset());    // fetch and verify consumer's position in the two partitions    final Map<TopicPartition, Long> offsetResponse = new HashMap<>();    offsetResponse.put(tp0, 3L);    offsetResponse.put(tp1, 3L);    client.prepareResponse(listOffsetsResponse(offsetResponse));    assertEquals(3L, consumer.position(tp0));    assertEquals(3L, consumer.position(tp1));    client.requests().clear();    consumer.unsubscribe();    consumer.close();}
f6280
0
testCloseInterrupt
public void kafkatest_f6288_0() throws Exception
{    consumerCloseTest(Long.MAX_VALUE, Collections.emptyList(), 0, true);}
f6288
0
testCloseShouldBeIdempotent
public void kafkatest_f6289_0()
{    KafkaConsumer<byte[], byte[]> consumer = newConsumer((String) null);    consumer.close();    consumer.close();    consumer.close();}
f6289
0
testOperationsBySubscribingConsumerWithDefaultGroupId
public void kafkatest_f6290_0()
{    try {        newConsumer(null, Optional.of(Boolean.TRUE));        fail("Expected an InvalidConfigurationException");    } catch (KafkaException e) {        assertEquals(InvalidConfigurationException.class, e.getCause().getClass());    }    try {        newConsumer((String) null).subscribe(Collections.singleton(topic));        fail("Expected an InvalidGroupIdException");    } catch (InvalidGroupIdException e) {    // OK, expected    }    try {        newConsumer((String) null).committed(tp0);        fail("Expected an InvalidGroupIdException");    } catch (InvalidGroupIdException e) {    // OK, expected    }    try {        newConsumer((String) null).commitAsync();        fail("Expected an InvalidGroupIdException");    } catch (InvalidGroupIdException e) {    // OK, expected    }    try {        newConsumer((String) null).commitSync();        fail("Expected an InvalidGroupIdException");    } catch (InvalidGroupIdException e) {    // OK, expected    }}
f6290
0
conditionMet
public boolean kafkatest_f6298_0()
{    return closeException.get() != null;}
f6298
0
testPartitionsForAuthenticationFailure
public void kafkatest_f6299_0()
{    final KafkaConsumer<String, String> consumer = consumerWithPendingAuthenticationError();    consumer.partitionsFor("some other topic");}
f6299
0
testBeginningOffsetsAuthenticationFailure
public void kafkatest_f6300_0()
{    final KafkaConsumer<String, String> consumer = consumerWithPendingAuthenticationError();    consumer.beginningOffsets(Collections.singleton(tp0));}
f6300
0
getConsumerRebalanceListener
private ConsumerRebalanceListener kafkatest_f6308_0(final KafkaConsumer<String, String> consumer)
{    return new ConsumerRebalanceListener() {        @Override        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {        }        @Override        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {            // set initial position so we don't need a lookup            for (TopicPartition partition : partitions) consumer.seek(partition, 0);        }    };}
f6308
0
onPartitionsAssigned
public void kafkatest_f6310_0(Collection<TopicPartition> partitions)
{    // set initial position so we don't need a lookup    for (TopicPartition partition : partitions) consumer.seek(partition, 0);}
f6310
0
getExceptionConsumerRebalanceListener
private ConsumerRebalanceListener kafkatest_f6311_0()
{    return new ConsumerRebalanceListener() {        @Override        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {            throw new RuntimeException("boom!");        }        @Override        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {            throw new RuntimeException("boom!");        }    };}
f6311
0
matches
public boolean kafkatest_f6319_0(AbstractRequest body)
{    heartbeatReceived.set(true);    return true;}
f6319
0
prepareOffsetCommitResponse
private AtomicBoolean kafkatest_f6320_0(MockClient client, Node coordinator, final Map<TopicPartition, Long> partitionOffsets)
{    final AtomicBoolean commitReceived = new AtomicBoolean(true);    Map<TopicPartition, Errors> response = new HashMap<>();    for (TopicPartition partition : partitionOffsets.keySet()) response.put(partition, Errors.NONE);    client.prepareResponseFrom(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            OffsetCommitRequest commitRequest = (OffsetCommitRequest) body;            Map<TopicPartition, Long> commitErrors = commitRequest.offsets();            for (Map.Entry<TopicPartition, Long> partitionOffset : partitionOffsets.entrySet()) {                // verify that the expected offset has been committed                if (!commitErrors.get(partitionOffset.getKey()).equals(partitionOffset.getValue())) {                    commitReceived.set(false);                    return false;                }            }            return true;        }    }, offsetCommitResponse(response), coordinator);    return commitReceived;}
f6320
0
matches
public boolean kafkatest_f6321_0(AbstractRequest body)
{    OffsetCommitRequest commitRequest = (OffsetCommitRequest) body;    Map<TopicPartition, Long> commitErrors = commitRequest.offsets();    for (Map.Entry<TopicPartition, Long> partitionOffset : partitionOffsets.entrySet()) {        // verify that the expected offset has been committed        if (!commitErrors.get(partitionOffset.getKey()).equals(partitionOffset.getValue())) {            commitReceived.set(false);            return false;        }    }    return true;}
f6321
0
fetchResponse
private FetchResponse<MemoryRecords> kafkatest_f6329_0(Map<TopicPartition, FetchInfo> fetches)
{    LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> tpResponses = new LinkedHashMap<>();    for (Map.Entry<TopicPartition, FetchInfo> fetchEntry : fetches.entrySet()) {        TopicPartition partition = fetchEntry.getKey();        long fetchOffset = fetchEntry.getValue().offset;        int fetchCount = fetchEntry.getValue().count;        final MemoryRecords records;        if (fetchCount == 0) {            records = MemoryRecords.EMPTY;        } else {            MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, fetchOffset);            for (int i = 0; i < fetchCount; i++) builder.append(0L, ("key-" + i).getBytes(), ("value-" + i).getBytes());            records = builder.build();        }        tpResponses.put(partition, new FetchResponse.PartitionData<>(Errors.NONE, 0, FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));    }    return new FetchResponse<>(Errors.NONE, tpResponses, 0, INVALID_SESSION_ID);}
f6329
0
fetchResponse
private FetchResponse kafkatest_f6330_0(TopicPartition partition, long fetchOffset, int count)
{    FetchInfo fetchInfo = new FetchInfo(fetchOffset, count);    return fetchResponse(Collections.singletonMap(partition, fetchInfo));}
f6330
0
newConsumer
private KafkaConsumer<String, String> kafkatest_f6331_0(Time time, KafkaClient client, SubscriptionState subscription, ConsumerMetadata metadata, ConsumerPartitionAssignor assignor, boolean autoCommitEnabled, Optional<String> groupInstanceId)
{    return newConsumer(time, client, subscription, metadata, assignor, autoCommitEnabled, groupId, groupInstanceId);}
f6331
0
testInvalidNegativeOffset
public void kafkatest_f6339_0()
{    new OffsetAndMetadata(-239L, Optional.of(15), "");}
f6339
0
testSerializationRoundtrip
public void kafkatest_f6340_0() throws IOException, ClassNotFoundException
{    checkSerde(new OffsetAndMetadata(239L, Optional.of(15), "blah"));    checkSerde(new OffsetAndMetadata(239L, "blah"));    checkSerde(new OffsetAndMetadata(239L));}
f6340
0
checkSerde
private void kafkatest_f6341_0(OffsetAndMetadata offsetAndMetadata) throws IOException, ClassNotFoundException
{    byte[] bytes = Serializer.serialize(offsetAndMetadata);    OffsetAndMetadata deserialized = (OffsetAndMetadata) Serializer.deserialize(bytes);    assertEquals(offsetAndMetadata, deserialized);}
f6341
0
testOneConsumerMultipleTopics
public void kafkatest_f6349_0()
{    Map<String, Integer> partitionsPerTopic = setupPartitionsPerTopicWithTwoTopics(1, 2);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumer1, new Subscription(topics(topic1, topic2))));    assertEquals(Collections.singleton(consumer1), assignment.keySet());    assertAssignment(partitions(tp(topic1, 0), tp(topic2, 0), tp(topic2, 1)), assignment.get(consumer1));}
f6349
0
testTwoConsumersOneTopicOnePartition
public void kafkatest_f6350_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, 1);    Map<String, Subscription> consumers = new HashMap<>();    consumers.put(consumer1, new Subscription(topics(topic1)));    consumers.put(consumer2, new Subscription(topics(topic1)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, consumers);    assertAssignment(partitions(tp(topic1, 0)), assignment.get(consumer1));    assertAssignment(Collections.emptyList(), assignment.get(consumer2));}
f6350
0
testTwoConsumersOneTopicTwoPartitions
public void kafkatest_f6351_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, 2);    Map<String, Subscription> consumers = new HashMap<>();    consumers.put(consumer1, new Subscription(topics(topic1)));    consumers.put(consumer2, new Subscription(topics(topic1)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, consumers);    assertAssignment(partitions(tp(topic1, 0)), assignment.get(consumer1));    assertAssignment(partitions(tp(topic1, 1)), assignment.get(consumer2));}
f6351
0
assertAssignment
private void kafkatest_f6359_0(List<TopicPartition> expected, List<TopicPartition> actual)
{    // order doesn't matter for assignment, so convert to a set    assertEquals(new HashSet<>(expected), new HashSet<>(actual));}
f6359
0
setupPartitionsPerTopicWithTwoTopics
private Map<String, Integer> kafkatest_f6360_0(int numberOfPartitions1, int numberOfPartitions2)
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, numberOfPartitions1);    partitionsPerTopic.put(topic2, numberOfPartitions2);    return partitionsPerTopic;}
f6360
0
topics
private static List<String> kafkatest_f6361_0(String... topics)
{    return Arrays.asList(topics);}
f6361
0
testTwoConsumersOneTopicOnePartition
public void kafkatest_f6369_0()
{    String consumer1 = "consumer1";    String consumer2 = "consumer2";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 1);    Map<String, Subscription> consumers = new HashMap<>();    consumers.put(consumer1, new Subscription(topics(topic)));    consumers.put(consumer2, new Subscription(topics(topic)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, consumers);    assertEquals(partitions(tp(topic, 0)), assignment.get(consumer1));    assertEquals(Collections.<TopicPartition>emptyList(), assignment.get(consumer2));}
f6369
0
testTwoConsumersOneTopicTwoPartitions
public void kafkatest_f6370_0()
{    String consumer1 = "consumer1";    String consumer2 = "consumer2";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 2);    Map<String, Subscription> consumers = new HashMap<>();    consumers.put(consumer1, new Subscription(topics(topic)));    consumers.put(consumer2, new Subscription(topics(topic)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, consumers);    assertEquals(partitions(tp(topic, 0)), assignment.get(consumer1));    assertEquals(partitions(tp(topic, 1)), assignment.get(consumer2));}
f6370
0
testMultipleConsumersMixedTopics
public void kafkatest_f6371_0()
{    String topic1 = "topic1";    String topic2 = "topic2";    String consumer1 = "consumer1";    String consumer2 = "consumer2";    String consumer3 = "consumer3";    Map<String, Integer> partitionsPerTopic = setupPartitionsPerTopicWithTwoTopics(3, 2);    Map<String, Subscription> consumers = new HashMap<>();    consumers.put(consumer1, new Subscription(topics(topic1)));    consumers.put(consumer2, new Subscription(topics(topic1, topic2)));    consumers.put(consumer3, new Subscription(topics(topic1)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, consumers);    assertEquals(partitions(tp(topic1, 0)), assignment.get(consumer1));    assertEquals(partitions(tp(topic1, 1), tp(topic2, 0), tp(topic2, 1)), assignment.get(consumer2));    assertEquals(partitions(tp(topic1, 2)), assignment.get(consumer3));}
f6371
0
tp
private static TopicPartition kafkatest_f6379_0(String topic, int partition)
{    return new TopicPartition(topic, partition);}
f6379
0
setupPartitionsPerTopicWithTwoTopics
private Map<String, Integer> kafkatest_f6380_0(int numberOfPartitions1, int numberOfPartitions2)
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, numberOfPartitions1);    partitionsPerTopic.put(topic2, numberOfPartitions2);    return partitionsPerTopic;}
f6380
0
createAssignor
public AbstractStickyAssignor kafkatest_f6381_0()
{    return new StickyAssignor();}
f6381
0
toSet
private static Set<TopicPartition> kafkatest_f6389_0(TopicPartition... arr)
{    TreeSet<TopicPartition> set = new TreeSet<>(new Comparator<TopicPartition>() {        @Override        public int compare(TopicPartition o1, TopicPartition o2) {            return o1.toString().compareTo(o2.toString());        }    });    set.addAll(Arrays.asList(arr));    return set;}
f6389
0
compare
public int kafkatest_f6390_0(TopicPartition o1, TopicPartition o2)
{    return o1.toString().compareTo(o2.toString());}
f6390
0
testFindMissing
public void kafkatest_f6391_0()
{    TopicPartition foo0 = new TopicPartition("foo", 0);    TopicPartition foo1 = new TopicPartition("foo", 1);    TopicPartition bar0 = new TopicPartition("bar", 0);    TopicPartition bar1 = new TopicPartition("bar", 1);    TopicPartition baz0 = new TopicPartition("baz", 0);    TopicPartition baz1 = new TopicPartition("baz", 1);    assertEquals(toSet(), FetchSessionHandler.findMissing(toSet(foo0), toSet(foo0)));    assertEquals(toSet(foo0), FetchSessionHandler.findMissing(toSet(foo0), toSet(foo1)));    assertEquals(toSet(foo0, foo1), FetchSessionHandler.findMissing(toSet(foo0, foo1), toSet(baz0)));    assertEquals(toSet(bar1, foo0, foo1), FetchSessionHandler.findMissing(toSet(foo0, foo1, bar0, bar1), toSet(bar0, baz0, baz1)));    assertEquals(toSet(), FetchSessionHandler.findMissing(toSet(foo0, foo1, bar0, bar1, baz1), toSet(foo0, foo1, bar0, bar1, baz0, baz1)));}
f6391
0
testDoubleBuild
public void kafkatest_f6399_0()
{    FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);    FetchSessionHandler.Builder builder = handler.newBuilder();    builder.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder.build();    try {        builder.build();        fail("Expected calling build twice to fail.");    } catch (Throwable t) {    // expected    }}
f6399
0
testIncrementalPartitionRemoval
public void kafkatest_f6400_0()
{    FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);    FetchSessionHandler.Builder builder = handler.newBuilder();    builder.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));    builder.add(new TopicPartition("bar", 0), new FetchRequest.PartitionData(20, 120, 220, Optional.empty()));    FetchSessionHandler.FetchRequestData data = builder.build();    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 110, 210), new ReqEntry("bar", 0, 20, 120, 220)), data.toSend(), data.sessionPartitions());    assertTrue(data.metadata().isFull());    FetchResponse<MemoryRecords> resp = new FetchResponse<>(Errors.NONE, respMap(new RespEntry("foo", 0, 10, 20), new RespEntry("foo", 1, 10, 20), new RespEntry("bar", 0, 10, 20)), 0, 123);    handler.handleResponse(resp);    // Test an incremental fetch request which removes two partitions.    FetchSessionHandler.Builder builder2 = handler.newBuilder();    builder2.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));    FetchSessionHandler.FetchRequestData data2 = builder2.build();    assertFalse(data2.metadata().isFull());    assertEquals(123, data2.metadata().sessionId());    assertEquals(1, data2.metadata().epoch());    assertMapEquals(reqMap(new ReqEntry("foo", 1, 10, 110, 210)), data2.sessionPartitions());    assertMapEquals(reqMap(), data2.toSend());    ArrayList<TopicPartition> expectedToForget2 = new ArrayList<>();    expectedToForget2.add(new TopicPartition("foo", 0));    expectedToForget2.add(new TopicPartition("bar", 0));    assertListEquals(expectedToForget2, data2.toForget());    // A FETCH_SESSION_ID_NOT_FOUND response triggers us to close the session.    // The next request is a session establishing FULL request.    FetchResponse<MemoryRecords> resp2 = new FetchResponse<>(Errors.FETCH_SESSION_ID_NOT_FOUND, respMap(), 0, INVALID_SESSION_ID);    handler.handleResponse(resp2);    FetchSessionHandler.Builder builder3 = handler.newBuilder();    builder3.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    FetchSessionHandler.FetchRequestData data3 = builder3.build();    assertTrue(data3.metadata().isFull());    assertEquals(INVALID_SESSION_ID, data3.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data3.metadata().epoch());    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200)), data3.sessionPartitions(), data3.toSend());}
f6400
0
setup
public void kafkatest_f6401_0()
{    inFlightRequests = new InFlightRequests(12);    correlationId = 0;}
f6401
0
addRequest
private int kafkatest_f6409_0(String destination, long sendTimeMs, int requestTimeoutMs)
{    int correlationId = this.correlationId;    this.correlationId += 1;    RequestHeader requestHeader = new RequestHeader(ApiKeys.METADATA, (short) 0, "clientId", correlationId);    NetworkClient.InFlightRequest ifr = new NetworkClient.InFlightRequest(requestHeader, requestTimeoutMs, 0, destination, null, false, false, null, null, sendTimeMs);    inFlightRequests.add(ifr);    return correlationId;}
f6409
0
emptyMetadataResponse
private static MetadataResponse kafkatest_f6410_0()
{    return MetadataResponse.prepareResponse(Collections.emptyList(), null, -1, Collections.emptyList());}
f6410
0
testMetadataUpdateAfterClose
public void kafkatest_f6411_0()
{    metadata.close();    metadata.update(emptyMetadataResponse(), 1000);}
f6411
0
testMaybeRequestUpdate
public void kafkatest_f6419_0()
{    TopicPartition tp = new TopicPartition("topic-1", 0);    metadata.update(emptyMetadataResponse(), 0L);    assertTrue(metadata.updateLastSeenEpochIfNewer(tp, 1));    assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 1);    metadata.update(emptyMetadataResponse(), 1L);    assertFalse(metadata.updateLastSeenEpochIfNewer(tp, 1));    assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 1);    metadata.update(emptyMetadataResponse(), 2L);    assertFalse(metadata.updateLastSeenEpochIfNewer(tp, 0));    assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 1);    metadata.update(emptyMetadataResponse(), 3L);    assertTrue(metadata.updateLastSeenEpochIfNewer(tp, 2));    assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 2);}
f6419
0
testOutOfBandEpochUpdate
public void kafkatest_f6420_0()
{    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put("topic-1", 5);    TopicPartition tp = new TopicPartition("topic-1", 0);    metadata.update(emptyMetadataResponse(), 0L);    assertTrue(metadata.updateLastSeenEpochIfNewer(tp, 99));    // Update epoch to 100    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 100);    metadata.update(metadataResponse, 10L);    assertNotNull(metadata.fetch().partition(tp));    assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    // Simulate a leader epoch from another response, like a fetch response (not yet implemented)    assertTrue(metadata.updateLastSeenEpochIfNewer(tp, 101));    // Cache of partition stays, but current partition info is not available since it's stale    assertNotNull(metadata.fetch().partition(tp));    assertEquals(metadata.fetch().partitionCountForTopic("topic-1").longValue(), 5);    assertFalse(metadata.partitionInfoIfCurrent(tp).isPresent());    assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 101);    // Metadata with older epoch is rejected, metadata state is unchanged    metadata.update(metadataResponse, 20L);    assertNotNull(metadata.fetch().partition(tp));    assertEquals(metadata.fetch().partitionCountForTopic("topic-1").longValue(), 5);    assertFalse(metadata.partitionInfoIfCurrent(tp).isPresent());    assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 101);    // Metadata with equal or newer epoch is accepted    metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 101);    metadata.update(metadataResponse, 30L);    assertNotNull(metadata.fetch().partition(tp));    assertEquals(metadata.fetch().partitionCountForTopic("topic-1").longValue(), 5);    assertTrue(metadata.partitionInfoIfCurrent(tp).isPresent());    assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 101);}
f6420
0
testNoEpoch
public void kafkatest_f6421_0()
{    metadata.update(emptyMetadataResponse(), 0L);    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), Collections.singletonMap("topic-1", 1));    metadata.update(metadataResponse, 10L);    TopicPartition tp = new TopicPartition("topic-1", 0);    // no epoch    assertFalse(metadata.lastSeenLeaderEpoch(tp).isPresent());    // still works    assertTrue(metadata.partitionInfoIfCurrent(tp).isPresent());    assertEquals(metadata.partitionInfoIfCurrent(tp).get().partitionInfo().partition(), 0);    assertEquals(metadata.partitionInfoIfCurrent(tp).get().partitionInfo().leader().id(), 0);}
f6421
0
isConnected
public boolean kafkatest_f6429_0(String idString)
{    return connectionState(idString).state == ConnectionState.State.CONNECTED;}
f6429
0
connectionState
private ConnectionState kafkatest_f6430_0(String idString)
{    ConnectionState connectionState = connections.get(idString);    if (connectionState == null) {        connectionState = new ConnectionState();        connections.put(idString, connectionState);    }    return connectionState;}
f6430
0
isReady
public boolean kafkatest_f6431_0(Node node, long now)
{    return connectionState(node.idString()).isReady(now);}
f6431
0
authenticationFailed
public void kafkatest_f6439_0(Node node, long blackoutMs)
{    pendingAuthenticationErrors.remove(node);    authenticationErrors.put(node, (AuthenticationException) Errors.SASL_AUTHENTICATION_FAILED.exception());    disconnect(node.idString());    blackout(node, blackoutMs);}
f6439
0
createPendingAuthenticationError
public void kafkatest_f6440_0(Node node, long blackoutMs)
{    pendingAuthenticationErrors.put(node, blackoutMs);}
f6440
0
connectionFailed
public boolean kafkatest_f6441_0(Node node)
{    return connectionState(node.idString()).isBackingOff(time.milliseconds());}
f6441
0
elapsedTimeMs
private long kafkatest_f6449_0(long currentTimeMs, long startTimeMs)
{    return Math.max(0, currentTimeMs - startTimeMs);}
f6449
0
checkTimeoutOfPendingRequests
private void kafkatest_f6450_0(long nowMs)
{    ClientRequest request = requests.peek();    while (request != null && elapsedTimeMs(nowMs, request.createdTimeMs()) > request.requestTimeoutMs()) {        disconnect(request.destination());        requests.poll();        request = requests.peek();    }}
f6450
0
requests
public Queue<ClientRequest> kafkatest_f6451_0()
{    return this.requests;}
f6451
0
prepareResponseFrom
public void kafkatest_f6459_0(AbstractResponse response, Node node)
{    prepareResponseFrom(ALWAYS_TRUE, response, node, false, false);}
f6459
0
prepareResponse
public void kafkatest_f6460_0(RequestMatcher matcher, AbstractResponse response)
{    prepareResponse(matcher, response, false);}
f6460
0
prepareResponseFrom
public void kafkatest_f6461_0(RequestMatcher matcher, AbstractResponse response, Node node)
{    prepareResponseFrom(matcher, response, node, false, false);}
f6461
0
reset
public void kafkatest_f6469_0()
{    connections.clear();    requests.clear();    responses.clear();    futureResponses.clear();    metadataUpdates.clear();    authenticationErrors.clear();}
f6469
0
hasPendingMetadataUpdates
public boolean kafkatest_f6470_0()
{    return !metadataUpdates.isEmpty();}
f6470
0
numAwaitingResponses
public int kafkatest_f6471_0()
{    return futureResponses.size();}
f6471
0
hasInFlightRequests
public boolean kafkatest_f6479_0(String node)
{    return inFlightRequestCount(node) > 0;}
f6479
0
hasReadyNodes
public boolean kafkatest_f6480_0(long now)
{    return connections.values().stream().anyMatch(cxn -> cxn.isReady(now));}
f6480
0
newClientRequest
public ClientRequest kafkatest_f6481_0(String nodeId, AbstractRequest.Builder<?> requestBuilder, long createdTimeMs, boolean expectResponse)
{    return newClientRequest(nodeId, requestBuilder, createdTimeMs, expectResponse, 5000, null);}
f6481
0
topics
private Set<String> kafkatest_f6489_0()
{    return updateResponse.topicMetadata().stream().map(MetadataResponse.TopicMetadata::topic).collect(Collectors.toSet());}
f6489
0
fetchNodes
public List<Node> kafkatest_f6492_0()
{    return metadata.fetch().nodes();}
f6492
0
isUpdateNeeded
public boolean kafkatest_f6493_0()
{    return metadata.updateRequested();}
f6493
0
setReadyDelayed
 void kafkatest_f6501_0(long untilMs)
{    readyDelayedUntilMs = untilMs;}
f6501
0
isReady
 boolean kafkatest_f6502_0(long now)
{    return state == State.CONNECTED && notThrottled(now);}
f6502
0
isReadyDelayed
 boolean kafkatest_f6503_0(long now)
{    return now < readyDelayedUntilMs;}
f6503
0
createNetworkClientWithStaticNodes
private NetworkClient kafkatest_f6511_0()
{    return new NetworkClient(selector, metadataUpdater, "mock-static", Integer.MAX_VALUE, 0, 0, 64 * 1024, 64 * 1024, defaultRequestTimeoutMs, ClientDnsLookup.DEFAULT, time, true, new ApiVersions(), new LogContext());}
f6511
0
createNetworkClientWithNoVersionDiscovery
private NetworkClient kafkatest_f6512_0()
{    return new NetworkClient(selector, metadataUpdater, "mock", Integer.MAX_VALUE, reconnectBackoffMsTest, reconnectBackoffMaxMsTest, 64 * 1024, 64 * 1024, defaultRequestTimeoutMs, ClientDnsLookup.DEFAULT, time, false, new ApiVersions(), new LogContext());}
f6512
0
setup
public void kafkatest_f6513_0()
{    selector.reset();}
f6513
0
checkSimpleRequestResponse
private void kafkatest_f6521_0(NetworkClient networkClient)
{    // has to be before creating any request, as it may send ApiVersionsRequest and its response is mocked with correlation id 0    awaitReady(networkClient, node);    ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000, Collections.emptyMap());    TestCallbackHandler handler = new TestCallbackHandler();    ClientRequest request = networkClient.newClientRequest(node.idString(), builder, time.milliseconds(), true, defaultRequestTimeoutMs, handler);    networkClient.send(request, time.milliseconds());    networkClient.poll(1, time.milliseconds());    assertEquals(1, networkClient.inFlightRequestCount());    ResponseHeader respHeader = new ResponseHeader(request.correlationId());    Struct resp = new Struct(ApiKeys.PRODUCE.responseSchema(ApiKeys.PRODUCE.latestVersion()));    resp.set("responses", new Object[0]);    Struct responseHeaderStruct = respHeader.toStruct();    int size = responseHeaderStruct.sizeOf() + resp.sizeOf();    ByteBuffer buffer = ByteBuffer.allocate(size);    responseHeaderStruct.writeTo(buffer);    resp.writeTo(buffer);    buffer.flip();    selector.completeReceive(new NetworkReceive(node.idString(), buffer));    List<ClientResponse> responses = networkClient.poll(1, time.milliseconds());    assertEquals(1, responses.size());    assertTrue("The handler should have executed.", handler.executed);    assertTrue("Should have a response body.", handler.response.hasResponse());    assertEquals("Should be correlated to the original request", request.correlationId(), handler.response.requestHeader().correlationId());}
f6521
0
setExpectedApiVersionsResponse
private void kafkatest_f6522_0(ApiVersionsResponse response)
{    short apiVersionsResponseVersion = response.apiVersion(ApiKeys.API_VERSIONS.id).maxVersion;    ByteBuffer buffer = response.serialize(apiVersionsResponseVersion, new ResponseHeader(0));    selector.delayedReceive(new DelayedReceive(node.idString(), new NetworkReceive(node.idString(), buffer)));}
f6522
0
awaitReady
private void kafkatest_f6523_0(NetworkClient client, Node node)
{    if (client.discoverBrokerVersions()) {        setExpectedApiVersionsResponse(ApiVersionsResponse.defaultApiVersionsResponse());    }    while (!client.ready(node, time.milliseconds())) client.poll(1, time.milliseconds());    selector.clear();}
f6523
0
sendResponse
private void kafkatest_f6531_0(int correlationId, Struct response)
{    ResponseHeader respHeader = new ResponseHeader(correlationId);    Struct responseHeaderStruct = respHeader.toStruct();    int size = responseHeaderStruct.sizeOf() + response.sizeOf();    ByteBuffer buffer = ByteBuffer.allocate(size);    responseHeaderStruct.writeTo(buffer);    response.writeTo(buffer);    buffer.flip();    selector.completeReceive(new NetworkReceive(node.idString(), buffer));}
f6531
0
sendThrottledProduceResponse
private void kafkatest_f6532_0(int correlationId, int throttleMs)
{    Struct resp = new Struct(ApiKeys.PRODUCE.responseSchema(ApiKeys.PRODUCE.latestVersion()));    resp.set("responses", new Object[0]);    resp.set(CommonFields.THROTTLE_TIME_MS, throttleMs);    sendResponse(correlationId, resp);}
f6532
0
testLeastLoadedNode
public void kafkatest_f6533_0()
{    client.ready(node, time.milliseconds());    assertFalse(client.isReady(node, time.milliseconds()));    assertEquals(node, client.leastLoadedNode(time.milliseconds()));    awaitReady(client, node);    client.poll(1, time.milliseconds());    assertTrue("The client should be ready", client.isReady(node, time.milliseconds()));    // leastloadednode should be our single node    Node leastNode = client.leastLoadedNode(time.milliseconds());    assertEquals("There should be one leastloadednode", leastNode.id(), node.id());    // sleep for longer than reconnect backoff    time.sleep(reconnectBackoffMsTest);    // CLOSE node    selector.serverDisconnect(node.idString());    client.poll(1, time.milliseconds());    assertFalse("After we forced the disconnection the client is no longer ready.", client.ready(node, time.milliseconds()));    leastNode = client.leastLoadedNode(time.milliseconds());    assertNull("There should be NO leastloadednode", leastNode);}
f6533
0
testDisconnectDuringUserMetadataRequest
public void kafkatest_f6541_0()
{    // this test ensures that the default metadata updater does not intercept a user-initiated    // metadata request when the remote node disconnects with the request in-flight.    awaitReady(client, node);    MetadataRequest.Builder builder = new MetadataRequest.Builder(Collections.<String>emptyList(), true);    long now = time.milliseconds();    ClientRequest request = client.newClientRequest(node.idString(), builder, now, true);    client.send(request, now);    client.poll(defaultRequestTimeoutMs, now);    assertEquals(1, client.inFlightRequestCount(node.idString()));    assertTrue(client.hasInFlightRequests(node.idString()));    assertTrue(client.hasInFlightRequests());    selector.close(node.idString());    List<ClientResponse> responses = client.poll(defaultRequestTimeoutMs, time.milliseconds());    assertEquals(1, responses.size());    assertTrue(responses.iterator().next().wasDisconnected());}
f6541
0
testServerDisconnectAfterInternalApiVersionRequest
public void kafkatest_f6542_0() throws Exception
{    awaitInFlightApiVersionRequest();    selector.serverDisconnect(node.idString());    // The failed ApiVersion request should not be forwarded to upper layers    List<ClientResponse> responses = client.poll(0, time.milliseconds());    assertFalse(client.hasInFlightRequests(node.idString()));    assertTrue(responses.isEmpty());}
f6542
0
testClientDisconnectAfterInternalApiVersionRequest
public void kafkatest_f6543_0() throws Exception
{    awaitInFlightApiVersionRequest();    client.disconnect(node.idString());    assertFalse(client.hasInFlightRequests(node.idString()));    // The failed ApiVersion request should not be forwarded to upper layers    List<ClientResponse> responses = client.poll(0, time.milliseconds());    assertTrue(responses.isEmpty());}
f6543
0
getAndClearFailure
public KafkaException kafkatest_f6551_0()
{    KafkaException failure = this.failure;    this.failure = null;    return failure;}
f6551
0
testUnsupportedVersionsToString
public void kafkatest_f6552_0()
{    NodeApiVersions versions = new NodeApiVersions(Collections.<ApiVersion>emptyList());    StringBuilder bld = new StringBuilder();    String prefix = "(";    for (ApiKeys apiKey : ApiKeys.values()) {        bld.append(prefix).append(apiKey.name).append("(").append(apiKey.id).append("): UNSUPPORTED");        prefix = ", ";    }    bld.append(")");    assertEquals(bld.toString(), versions.toString());}
f6552
0
testUnknownApiVersionsToString
public void kafkatest_f6553_0()
{    ApiVersion unknownApiVersion = new ApiVersion((short) 337, (short) 0, (short) 1);    NodeApiVersions versions = new NodeApiVersions(Collections.singleton(unknownApiVersion));    assertTrue(versions.toString().endsWith("UNKNOWN(337): 0 to 1)"));}
f6553
0
teardown
public void kafkatest_f6561_0()
{    this.metrics.close();}
f6561
0
testSimple
public void kafkatest_f6562_0() throws Exception
{    long totalMemory = 64 * 1024;    int size = 1024;    BufferPool pool = new BufferPool(totalMemory, size, metrics, time, metricGroup);    ByteBuffer buffer = pool.allocate(size, maxBlockTimeMs);    assertEquals("Buffer size should equal requested size.", size, buffer.limit());    assertEquals("Unallocated memory should have shrunk", totalMemory - size, pool.unallocatedMemory());    assertEquals("Available memory should have shrunk", totalMemory - size, pool.availableMemory());    buffer.putInt(1);    buffer.flip();    pool.deallocate(buffer);    assertEquals("All memory should be available", totalMemory, pool.availableMemory());    assertEquals("But now some is on the free list", totalMemory - size, pool.unallocatedMemory());    buffer = pool.allocate(size, maxBlockTimeMs);    assertEquals("Recycled buffer should be cleared.", 0, buffer.position());    assertEquals("Recycled buffer should be cleared.", buffer.capacity(), buffer.limit());    pool.deallocate(buffer);    assertEquals("All memory should be available", totalMemory, pool.availableMemory());    assertEquals("Still a single buffer on the free list", totalMemory - size, pool.unallocatedMemory());    buffer = pool.allocate(2 * size, maxBlockTimeMs);    pool.deallocate(buffer);    assertEquals("All memory should be available", totalMemory, pool.availableMemory());    assertEquals("Non-standard size didn't go to the free list.", totalMemory - size, pool.unallocatedMemory());}
f6562
0
testCantAllocateMoreMemoryThanWeHave
public void kafkatest_f6563_0() throws Exception
{    BufferPool pool = new BufferPool(1024, 512, metrics, time, metricGroup);    ByteBuffer buffer = pool.allocate(1024, maxBlockTimeMs);    assertEquals(1024, buffer.limit());    pool.deallocate(buffer);    pool.allocate(1025, maxBlockTimeMs);}
f6563
0
testBlockTimeout
public void kafkatest_f6571_0() throws Exception
{    BufferPool pool = new BufferPool(10, 1, metrics, Time.SYSTEM, metricGroup);    ByteBuffer buffer1 = pool.allocate(1, maxBlockTimeMs);    ByteBuffer buffer2 = pool.allocate(1, maxBlockTimeMs);    ByteBuffer buffer3 = pool.allocate(1, maxBlockTimeMs);    // The first two buffers will be de-allocated within maxBlockTimeMs since the most recent allocation    delayedDeallocate(pool, buffer1, maxBlockTimeMs / 2);    delayedDeallocate(pool, buffer2, maxBlockTimeMs);    // The third buffer will be de-allocated after maxBlockTimeMs since the most recent allocation    delayedDeallocate(pool, buffer3, maxBlockTimeMs / 2 * 5);    long beginTimeMs = Time.SYSTEM.milliseconds();    try {        pool.allocate(10, maxBlockTimeMs);        fail("The buffer allocated more memory than its maximum value 10");    } catch (TimeoutException e) {    // this is good    }    // Thread scheduling sometimes means that deallocation varies by this point    assertTrue("available memory " + pool.availableMemory(), pool.availableMemory() >= 8 && pool.availableMemory() <= 10);    long durationMs = Time.SYSTEM.milliseconds() - beginTimeMs;    assertTrue("TimeoutException should not throw before maxBlockTimeMs", durationMs >= maxBlockTimeMs);    assertTrue("TimeoutException should throw soon after maxBlockTimeMs", durationMs < maxBlockTimeMs + 1000);}
f6571
0
testCleanupMemoryAvailabilityWaiterOnBlockTimeout
public void kafkatest_f6572_0() throws Exception
{    BufferPool pool = new BufferPool(2, 1, metrics, time, metricGroup);    pool.allocate(1, maxBlockTimeMs);    try {        pool.allocate(2, maxBlockTimeMs);        fail("The buffer allocated more memory than its maximum value 2");    } catch (TimeoutException e) {    // this is good    }    assertEquals(0, pool.queued());    assertEquals(1, pool.availableMemory());}
f6572
0
testCleanupMemoryAvailabilityWaiterOnInterruption
public void kafkatest_f6573_0() throws Exception
{    BufferPool pool = new BufferPool(2, 1, metrics, time, metricGroup);    long blockTime = 5000;    pool.allocate(1, maxBlockTimeMs);    Thread t1 = new Thread(new BufferPoolAllocator(pool, blockTime));    Thread t2 = new Thread(new BufferPoolAllocator(pool, blockTime));    // start thread t1 which will try to allocate more memory on to the Buffer pool    t1.start();    // sleep for 500ms. Condition variable c1 associated with pool.allocate() by thread t1 will be inserted in the waiters queue.    Thread.sleep(500);    Deque<Condition> waiters = pool.waiters();    // get the condition object associated with pool.allocate() by thread t1    Condition c1 = waiters.getFirst();    // start thread t2 which will try to allocate more memory on to the Buffer pool    t2.start();    // sleep for 500ms. Condition variable c2 associated with pool.allocate() by thread t2 will be inserted in the waiters queue. The waiters queue will have 2 entries c1 and c2.    Thread.sleep(500);    t1.interrupt();    // sleep for 500ms.    Thread.sleep(500);    // get the condition object associated with allocate() by thread t2    Condition c2 = waiters.getLast();    t2.interrupt();    assertNotEquals(c1, c2);    t1.join();    t2.join();    // both the allocate() called by threads t1 and t2 should have been interrupted and the waiters queue should be empty    assertEquals(pool.queued(), 0);}
f6573
0
allocateByteBuffer
protected ByteBuffer kafkatest_f6581_0(int size)
{    throw new OutOfMemoryError();}
f6581
0
run
public void kafkatest_f6582_0()
{    try {        for (int i = 0; i < iterations; i++) {            int size;            if (TestUtils.RANDOM.nextBoolean())                // allocate poolable size                size = pool.poolableSize();            else                // allocate a random size                size = TestUtils.RANDOM.nextInt((int) pool.totalMemory());            ByteBuffer buffer = pool.allocate(size, maxBlockTimeMs);            pool.deallocate(buffer);        }        success.set(true);    } catch (Exception e) {        e.printStackTrace();    }}
f6582
0
testKeyPartitionIsStable
public void kafkatest_f6583_0()
{    final Partitioner partitioner = new DefaultPartitioner();    final Cluster cluster = new Cluster("clusterId", asList(NODES), PARTITIONS, Collections.<String>emptySet(), Collections.<String>emptySet());    int partition = partitioner.partition("test", null, KEY_BYTES, null, null, cluster);    assertEquals("Same key should yield same partition", partition, partitioner.partition("test", null, KEY_BYTES, null, null, cluster));}
f6583
0
testBatchCannotCompleteTwice
public void kafkatest_f6591_0() throws Exception
{    ProducerBatch batch = new ProducerBatch(new TopicPartition("topic", 1), memoryRecordsBuilder, now);    MockCallback callback = new MockCallback();    FutureRecordMetadata future = batch.tryAppend(now, null, new byte[10], Record.EMPTY_HEADERS, callback, now);    batch.done(500L, 10L, null);    assertEquals(1, callback.invocations);    assertNull(callback.exception);    assertNotNull(callback.metadata);    try {        batch.done(1000L, 20L, null);        fail("Expected exception from done");    } catch (IllegalStateException e) {    // expected    }    RecordMetadata recordMetadata = future.get();    assertEquals(500L, recordMetadata.offset());    assertEquals(10L, recordMetadata.timestamp());}
f6591
0
testAppendedChecksumMagicV0AndV1
public void kafkatest_f6592_0()
{    for (byte magic : Arrays.asList(MAGIC_VALUE_V0, MAGIC_VALUE_V1)) {        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(128), magic, CompressionType.NONE, TimestampType.CREATE_TIME, 0L);        ProducerBatch batch = new ProducerBatch(new TopicPartition("topic", 1), builder, now);        byte[] key = "hi".getBytes();        byte[] value = "there".getBytes();        FutureRecordMetadata future = batch.tryAppend(now, key, value, Record.EMPTY_HEADERS, null, now);        assertNotNull(future);        byte attributes = LegacyRecord.computeAttributes(magic, CompressionType.NONE, TimestampType.CREATE_TIME);        long expectedChecksum = LegacyRecord.computeChecksum(magic, attributes, now, key, value);        assertEquals(expectedChecksum, future.checksumOrNull().longValue());    }}
f6592
0
testSplitPreservesHeaders
public void kafkatest_f6593_0()
{    for (CompressionType compressionType : CompressionType.values()) {        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), MAGIC_VALUE_V2, compressionType, TimestampType.CREATE_TIME, 0L);        ProducerBatch batch = new ProducerBatch(new TopicPartition("topic", 1), builder, now);        Header header = new RecordHeader("header-key", "header-value".getBytes());        while (true) {            FutureRecordMetadata future = batch.tryAppend(now, "hi".getBytes(), "there".getBytes(), new Header[] { header }, null, now);            if (future == null) {                break;            }        }        Deque<ProducerBatch> batches = batch.split(200);        assertTrue("This batch should be split to multiple small batches.", batches.size() >= 2);        for (ProducerBatch splitProducerBatch : batches) {            for (RecordBatch splitBatch : splitProducerBatch.records().batches()) {                for (Record record : splitBatch) {                    assertTrue("Header size should be 1.", record.headers().length == 1);                    assertTrue("Header key should be 'header-key'.", record.headers()[0].key().equals("header-key"));                    assertTrue("Header value should be 'header-value'.", new String(record.headers()[0].value()).equals("header-value"));                }            }        }    }}
f6593
0
injectOnSendError
public void kafkatest_f6603_0(boolean on)
{    throwExceptionOnSend = on;}
f6603
0
injectOnAcknowledgementError
public void kafkatest_f6604_0(boolean on)
{    throwExceptionOnAck = on;}
f6604
0
testOnSendChain
public void kafkatest_f6605_0()
{    List<ProducerInterceptor<Integer, String>> interceptorList = new ArrayList<>();    // we are testing two different interceptors by configuring the same interceptor differently, which is not    // how it would be done in KafkaProducer, but ok for testing interceptor callbacks    AppendProducerInterceptor interceptor1 = new AppendProducerInterceptor("One");    AppendProducerInterceptor interceptor2 = new AppendProducerInterceptor("Two");    interceptorList.add(interceptor1);    interceptorList.add(interceptor2);    ProducerInterceptors<Integer, String> interceptors = new ProducerInterceptors<>(interceptorList);    // verify that onSend() mutates the record as expected    ProducerRecord<Integer, String> interceptedRecord = interceptors.onSend(producerRecord);    assertEquals(2, onSendCount);    assertEquals(producerRecord.topic(), interceptedRecord.topic());    assertEquals(producerRecord.partition(), interceptedRecord.partition());    assertEquals(producerRecord.key(), interceptedRecord.key());    assertEquals(interceptedRecord.value(), producerRecord.value().concat("One").concat("Two"));    // onSend() mutates the same record the same way    ProducerRecord<Integer, String> anotherRecord = interceptors.onSend(producerRecord);    assertEquals(4, onSendCount);    assertEquals(interceptedRecord, anotherRecord);    // verify that if one of the interceptors throws an exception, other interceptors' callbacks are still called    interceptor1.injectOnSendError(true);    ProducerRecord<Integer, String> partInterceptRecord = interceptors.onSend(producerRecord);    assertEquals(6, onSendCount);    assertEquals(partInterceptRecord.value(), producerRecord.value().concat("Two"));    // verify the record remains valid if all onSend throws an exception    interceptor2.injectOnSendError(true);    ProducerRecord<Integer, String> noInterceptRecord = interceptors.onSend(producerRecord);    assertEquals(producerRecord, noInterceptRecord);    interceptors.close();}
f6605
0
testTopicExpiry
public void kafkatest_f6613_0()
{    // Test that topic is expired if not used within the expiry interval    long time = 0;    String topic1 = "topic1";    metadata.add(topic1);    metadata.update(responseWithCurrentTopics(), time);    assertTrue(metadata.containsTopic(topic1));    time += ProducerMetadata.TOPIC_EXPIRY_MS;    metadata.update(responseWithCurrentTopics(), time);    assertFalse("Unused topic not expired", metadata.containsTopic(topic1));    // Test that topic is not expired if used within the expiry interval    metadata.add("topic2");    metadata.update(responseWithCurrentTopics(), time);    for (int i = 0; i < 3; i++) {        time += ProducerMetadata.TOPIC_EXPIRY_MS / 2;        metadata.update(responseWithCurrentTopics(), time);        assertTrue("Topic expired even though in use", metadata.containsTopic("topic2"));        metadata.add("topic2");    }}
f6613
0
testMetadataWaitAbortedOnFatalException
public void kafkatest_f6614_0() throws Exception
{    Time time = new MockTime();    metadata.failedUpdate(time.milliseconds(), new AuthenticationException("Fatal exception from test"));    assertThrows(AuthenticationException.class, () -> metadata.awaitUpdate(0, 1000));}
f6614
0
responseWithCurrentTopics
private MetadataResponse kafkatest_f6615_0()
{    return responseWithTopics(metadata.topics());}
f6615
0
testAppendLarge
private void kafkatest_f6623_0(CompressionType compressionType) throws Exception
{    int batchSize = 512;    byte[] value = new byte[2 * batchSize];    RecordAccumulator accum = createTestRecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);    accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    assertEquals("Our partition's leader should be ready", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);    Deque<ProducerBatch> batches = accum.batches().get(tp1);    assertEquals(1, batches.size());    ProducerBatch producerBatch = batches.peek();    List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());    assertEquals(1, recordBatches.size());    MutableRecordBatch recordBatch = recordBatches.get(0);    assertEquals(0L, recordBatch.baseOffset());    List<Record> records = TestUtils.toList(recordBatch);    assertEquals(1, records.size());    Record record = records.get(0);    assertEquals(0L, record.offset());    assertEquals(ByteBuffer.wrap(key), record.key());    assertEquals(ByteBuffer.wrap(value), record.value());    assertEquals(0L, record.timestamp());}
f6623
0
testAppendLargeOldMessageFormatCompressed
public void kafkatest_f6624_0() throws Exception
{    testAppendLargeOldMessageFormat(CompressionType.GZIP);}
f6624
0
testAppendLargeOldMessageFormatNonCompressed
public void kafkatest_f6625_0() throws Exception
{    testAppendLargeOldMessageFormat(CompressionType.NONE);}
f6625
0
testFlush
public void kafkatest_f6633_0() throws Exception
{    int lingerMs = Integer.MAX_VALUE;    final RecordAccumulator accum = createTestRecordAccumulator(4 * 1024 + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 64 * 1024, CompressionType.NONE, lingerMs);    for (int i = 0; i < 100; i++) {        accum.append(new TopicPartition(topic, i % 3), 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);        assertTrue(accum.hasIncomplete());    }    RecordAccumulator.ReadyCheckResult result = accum.ready(cluster, time.milliseconds());    assertEquals("No nodes should be ready.", 0, result.readyNodes.size());    accum.beginFlush();    result = accum.ready(cluster, time.milliseconds());    // drain and deallocate all batches    Map<Integer, List<ProducerBatch>> results = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertTrue(accum.hasIncomplete());    for (List<ProducerBatch> batches : results.values()) for (ProducerBatch batch : batches) accum.deallocate(batch);    // should be complete with no unsent records.    accum.awaitFlushCompletion();    assertFalse(accum.hasUndrained());    assertFalse(accum.hasIncomplete());}
f6633
0
delayedInterrupt
private void kafkatest_f6634_0(final Thread thread, final long delayMs)
{    Thread t = new Thread() {        public void run() {            Time.SYSTEM.sleep(delayMs);            thread.interrupt();        }    };    t.start();}
f6634
0
run
public void kafkatest_f6635_0()
{    Time.SYSTEM.sleep(delayMs);    thread.interrupt();}
f6635
0
testExpiredBatchSingleMaxValue
public void kafkatest_f6643_0() throws InterruptedException
{    doExpireBatchSingle(Integer.MAX_VALUE);}
f6643
0
testExpiredBatches
public void kafkatest_f6644_0() throws InterruptedException
{    long retryBackoffMs = 100L;    int lingerMs = 30;    int requestTimeout = 60;    int deliveryTimeoutMs = 3200;    // test case assumes that the records do not fill the batch completely    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(deliveryTimeoutMs, batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    int appends = expectedNumAppends(batchSize);    // Test batches not in retry    for (int i = 0; i < appends; i++) {        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);        assertEquals("No partitions should be ready.", 0, accum.ready(cluster, time.milliseconds()).readyNodes.size());    }    // Make the batches ready due to batch full    accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, 0, false);    Set<Node> readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    assertEquals("Our partition's leader should be ready", Collections.singleton(node1), readyNodes);    // Advance the clock to expire the batch.    time.sleep(deliveryTimeoutMs + 1);    accum.mutePartition(tp1);    List<ProducerBatch> expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("The batches will be muted no matter if the partition is muted or not", 2, expiredBatches.size());    accum.unmutePartition(tp1, 0L);    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("All batches should have been expired earlier", 0, expiredBatches.size());    assertEquals("No partitions should be ready.", 0, accum.ready(cluster, time.milliseconds()).readyNodes.size());    // Advance the clock to make the next batch ready due to linger.ms    time.sleep(lingerMs);    assertEquals("Our partition's leader should be ready", Collections.singleton(node1), readyNodes);    time.sleep(requestTimeout + 1);    accum.mutePartition(tp1);    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("The batch should not be expired when metadata is still available and partition is muted", 0, expiredBatches.size());    accum.unmutePartition(tp1, 0L);    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("All batches should have been expired", 0, expiredBatches.size());    assertEquals("No partitions should be ready.", 0, accum.ready(cluster, time.milliseconds()).readyNodes.size());    // Test batches in retry.    // Create a retried batch    accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, 0, false);    time.sleep(lingerMs);    readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    assertEquals("Our partition's leader should be ready", Collections.singleton(node1), readyNodes);    Map<Integer, List<ProducerBatch>> drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertEquals("There should be only one batch.", drained.get(node1.id()).size(), 1);    time.sleep(1000L);    accum.reenqueue(drained.get(node1.id()).get(0), time.milliseconds());    // test expiration.    time.sleep(requestTimeout + retryBackoffMs);    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("The batch should not be expired.", 0, expiredBatches.size());    time.sleep(1L);    accum.mutePartition(tp1);    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("The batch should not be expired when the partition is muted", 0, expiredBatches.size());    accum.unmutePartition(tp1, 0L);    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("All batches should have been expired.", 0, expiredBatches.size());    // Test that when being throttled muted batches are expired before the throttle time is over.    accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, 0, false);    time.sleep(lingerMs);    readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    assertEquals("Our partition's leader should be ready", Collections.singleton(node1), readyNodes);    // Advance the clock to expire the batch.    time.sleep(requestTimeout + 1);    accum.mutePartition(tp1);    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("The batch should not be expired when the partition is muted", 0, expiredBatches.size());    long throttleTimeMs = 100L;    accum.unmutePartition(tp1, time.milliseconds() + throttleTimeMs);    // The batch shouldn't be expired yet.    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("The batch should not be expired when the partition is muted", 0, expiredBatches.size());    // Once the throttle time is over, the batch can be expired.    time.sleep(throttleTimeMs);    expiredBatches = accum.expiredBatches(time.milliseconds());    assertEquals("All batches should have been expired earlier", 0, expiredBatches.size());    assertEquals("No partitions should be ready.", 1, accum.ready(cluster, time.milliseconds()).readyNodes.size());}
f6644
0
testMutedPartitions
public void kafkatest_f6645_0() throws InterruptedException
{    long now = time.milliseconds();    // test case assumes that the records do not fill the batch completely    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, 10);    int appends = expectedNumAppends(batchSize);    for (int i = 0; i < appends; i++) {        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);        assertEquals("No partitions should be ready.", 0, accum.ready(cluster, now).readyNodes.size());    }    time.sleep(2000);    // Test ready with muted partition    accum.mutePartition(tp1);    RecordAccumulator.ReadyCheckResult result = accum.ready(cluster, time.milliseconds());    assertEquals("No node should be ready", 0, result.readyNodes.size());    // Test ready without muted partition    accum.unmutePartition(tp1, 0L);    result = accum.ready(cluster, time.milliseconds());    assertTrue("The batch should be ready", result.readyNodes.size() > 0);    // Test drain with muted partition    accum.mutePartition(tp1);    Map<Integer, List<ProducerBatch>> drained = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertEquals("No batch should have been drained", 0, drained.get(node1.id()).size());    // Test drain without muted partition.    accum.unmutePartition(tp1, 0L);    drained = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertTrue("The batch should have been drained.", drained.get(node1.id()).size() > 0);}
f6645
0
testStickyBatches
public void kafkatest_f6653_0() throws Exception
{    long now = time.milliseconds();    // Test case assumes that the records do not fill the batch completely    int batchSize = 1025;    Partitioner partitioner = new DefaultPartitioner();    RecordAccumulator accum = createTestRecordAccumulator(3200, batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10L * batchSize, CompressionType.NONE, 10);    int expectedAppends = expectedNumAppendsNoKey(batchSize);    // Create first batch    int partition = partitioner.partition(topic, null, null, "value", value, cluster);    TopicPartition tp = new TopicPartition(topic, partition);    accum.append(tp, 0L, null, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    int appends = 1;    boolean switchPartition = false;    while (!switchPartition) {        // Append to the first batch        partition = partitioner.partition(topic, null, null, "value", value, cluster);        tp = new TopicPartition(topic, partition);        RecordAccumulator.RecordAppendResult result = accum.append(tp, 0L, null, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, true);        Deque<ProducerBatch> partitionBatches1 = accum.batches().get(tp1);        Deque<ProducerBatch> partitionBatches2 = accum.batches().get(tp2);        Deque<ProducerBatch> partitionBatches3 = accum.batches().get(tp3);        int numBatches = (partitionBatches1 == null ? 0 : partitionBatches1.size()) + (partitionBatches2 == null ? 0 : partitionBatches2.size()) + (partitionBatches3 == null ? 0 : partitionBatches3.size());        // Only one batch is created because the partition is sticky.        assertEquals(1, numBatches);        switchPartition = result.abortForNewBatch;        // We only appended if we do not retry.        if (!switchPartition) {            appends++;            assertEquals("No partitions should be ready.", 0, accum.ready(cluster, now).readyNodes.size());        }    }    // Batch should be full.    assertEquals(1, accum.ready(cluster, time.milliseconds()).readyNodes.size());    assertEquals(appends, expectedAppends);    switchPartition = false;    // KafkaProducer would call this method in this case, make second batch    partitioner.onNewBatch(topic, cluster, partition);    partition = partitioner.partition(topic, null, null, "value", value, cluster);    tp = new TopicPartition(topic, partition);    accum.append(tp, 0L, null, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    appends++;    // These appends all go into the second batch    while (!switchPartition) {        partition = partitioner.partition(topic, null, null, "value", value, cluster);        tp = new TopicPartition(topic, partition);        RecordAccumulator.RecordAppendResult result = accum.append(tp, 0L, null, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, true);        Deque<ProducerBatch> partitionBatches1 = accum.batches().get(tp1);        Deque<ProducerBatch> partitionBatches2 = accum.batches().get(tp2);        Deque<ProducerBatch> partitionBatches3 = accum.batches().get(tp3);        int numBatches = (partitionBatches1 == null ? 0 : partitionBatches1.size()) + (partitionBatches2 == null ? 0 : partitionBatches2.size()) + (partitionBatches3 == null ? 0 : partitionBatches3.size());        // Only two batches because the new partition is also sticky.        assertEquals(2, numBatches);        switchPartition = result.abortForNewBatch;        // We only appended if we do not retry.        if (!switchPartition) {            appends++;        }    }    // There should be two full batches now.    assertEquals(appends, 2 * expectedAppends);}
f6653
0
prepareSplitBatches
private int kafkatest_f6654_0(RecordAccumulator accum, long seed, int recordSize, int numRecords) throws InterruptedException
{    Random random = new Random();    random.setSeed(seed);    // First set the compression ratio estimation to be good.    CompressionRatioEstimator.setEstimation(tp1.topic(), CompressionType.GZIP, 0.1f);    // Append 20 records of 100 bytes size with poor compression ratio should make the batch too big.    for (int i = 0; i < numRecords; i++) {        accum.append(tp1, 0L, null, bytesWithPoorCompression(random, recordSize), Record.EMPTY_HEADERS, null, 0, false);    }    RecordAccumulator.ReadyCheckResult result = accum.ready(cluster, time.milliseconds());    assertFalse(result.readyNodes.isEmpty());    Map<Integer, List<ProducerBatch>> batches = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertEquals(1, batches.size());    assertEquals(1, batches.values().iterator().next().size());    ProducerBatch batch = batches.values().iterator().next().get(0);    int numSplitBatches = accum.splitAndReenqueue(batch);    accum.deallocate(batch);    return numSplitBatches;}
f6654
0
completeOrSplitBatches
private BatchDrainedResult kafkatest_f6655_0(RecordAccumulator accum, int batchSize)
{    int numSplit = 0;    int numBatches = 0;    boolean batchDrained;    do {        batchDrained = false;        RecordAccumulator.ReadyCheckResult result = accum.ready(cluster, time.milliseconds());        Map<Integer, List<ProducerBatch>> batches = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, time.milliseconds());        for (List<ProducerBatch> batchList : batches.values()) {            for (ProducerBatch batch : batchList) {                batchDrained = true;                numBatches++;                if (batch.estimatedSizeInBytes() > batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD) {                    accum.splitAndReenqueue(batch);                    // release the resource of the original big batch.                    numSplit++;                } else {                    batch.done(0L, 0L, null);                }                accum.deallocate(batch);            }        }    } while (batchDrained);    return new BatchDrainedResult(numSplit, numBatches);}
f6655
0
tearDown
public void kafkatest_f6663_0()
{    this.metrics.close();}
f6663
0
testSimple
public void kafkatest_f6664_0() throws Exception
{    long offset = 0;    Future<RecordMetadata> future = accumulator.append(tp0, 0L, "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // connect    sender.runOnce();    // send produce request    sender.runOnce();    assertEquals("We should have a single produce request in flight.", 1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    assertTrue(client.hasInFlightRequests());    client.respond(produceResponse(tp0, offset, Errors.NONE, 0));    sender.runOnce();    assertEquals("All requests completed.", 0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());    assertFalse(client.hasInFlightRequests());    sender.runOnce();    assertTrue("Request should be completed", future.isDone());    assertEquals(offset, future.get().offset());}
f6664
0
testMessageFormatDownConversion
public void kafkatest_f6665_0() throws Exception
{    // this test case verifies the behavior when the version of the produce request supported by the    // broker changes after the record set is created    long offset = 0;    // start off support produce request v3    apiVersions.update("0", NodeApiVersions.create());    Future<RecordMetadata> future = accumulator.append(tp0, 0L, "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // now the partition leader supports only v2    apiVersions.update("0", NodeApiVersions.create(Collections.singleton(new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE, (short) 0, (short) 2))));    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            ProduceRequest request = (ProduceRequest) body;            if (request.version() != 2)                return false;            MemoryRecords records = request.partitionRecordsOrFail().get(tp0);            return records != null && records.sizeInBytes() > 0 && records.hasMatchingMagic(RecordBatch.MAGIC_VALUE_V1);        }    }, produceResponse(tp0, offset, Errors.NONE, 0));    // connect    sender.runOnce();    // send produce request    sender.runOnce();    assertTrue("Request should be completed", future.isDone());    assertEquals(offset, future.get().offset());}
f6665
0
testAppendInExpiryCallback
public void kafkatest_f6673_0() throws InterruptedException
{    int messagesPerBatch = 10;    final AtomicInteger expiryCallbackCount = new AtomicInteger(0);    final AtomicReference<Exception> unexpectedException = new AtomicReference<>();    final byte[] key = "key".getBytes();    final byte[] value = "value".getBytes();    final long maxBlockTimeMs = 1000;    Callback callback = new Callback() {        @Override        public void onCompletion(RecordMetadata metadata, Exception exception) {            if (exception instanceof TimeoutException) {                expiryCallbackCount.incrementAndGet();                try {                    accumulator.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);                } catch (InterruptedException e) {                    throw new RuntimeException("Unexpected interruption", e);                }            } else if (exception != null)                unexpectedException.compareAndSet(null, exception);        }    };    for (int i = 0; i < messagesPerBatch; i++) accumulator.append(tp1, 0L, key, value, null, callback, maxBlockTimeMs, false);    // Advance the clock to expire the first batch.    time.sleep(10000);    Node clusterNode = metadata.fetch().nodes().get(0);    Map<Integer, List<ProducerBatch>> drainedBatches = accumulator.drain(metadata.fetch(), Collections.singleton(clusterNode), Integer.MAX_VALUE, time.milliseconds());    sender.addToInflightBatches(drainedBatches);    // Disconnect the target node for the pending produce request. This will ensure that sender will try to    // expire the batch.    client.disconnect(clusterNode.idString());    client.blackout(clusterNode, 100);    // We should try to flush the batch, but we expire it instead without sending anything.    sender.runOnce();    assertEquals("Callbacks not invoked for expiry", messagesPerBatch, expiryCallbackCount.get());    assertNull("Unexpected exception", unexpectedException.get());    // Make sure that the reconds were appended back to the batch.    assertTrue(accumulator.batches().containsKey(tp1));    assertEquals(1, accumulator.batches().get(tp1).size());    assertEquals(messagesPerBatch, accumulator.batches().get(tp1).peekFirst().recordCount);}
f6673
0
onCompletion
public void kafkatest_f6674_0(RecordMetadata metadata, Exception exception)
{    if (exception instanceof TimeoutException) {        expiryCallbackCount.incrementAndGet();        try {            accumulator.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);        } catch (InterruptedException e) {            throw new RuntimeException("Unexpected interruption", e);        }    } else if (exception != null)        unexpectedException.compareAndSet(null, exception);}
f6674
0
testMetadataTopicExpiry
public void kafkatest_f6675_0() throws Exception
{    long offset = 0;    client.updateMetadata(TestUtils.metadataUpdateWith(1, Collections.singletonMap("test", 2)));    Future<RecordMetadata> future = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertTrue("Topic not added to metadata", metadata.containsTopic(tp0.topic()));    client.updateMetadata(TestUtils.metadataUpdateWith(1, Collections.singletonMap("test", 2)));    // send produce request    sender.runOnce();    client.respond(produceResponse(tp0, offset++, Errors.NONE, 0));    sender.runOnce();    assertEquals("Request completed.", 0, client.inFlightRequestCount());    assertFalse(client.hasInFlightRequests());    assertEquals(0, sender.inFlightBatches(tp0).size());    sender.runOnce();    assertTrue("Request should be completed", future.isDone());    assertTrue("Topic not retained in metadata list", metadata.containsTopic(tp0.topic()));    time.sleep(ProducerMetadata.TOPIC_EXPIRY_MS);    client.updateMetadata(TestUtils.metadataUpdateWith(1, Collections.singletonMap("test", 2)));    assertFalse("Unused topic has not been expired", metadata.containsTopic(tp0.topic()));    future = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertTrue("Topic not added to metadata", metadata.containsTopic(tp0.topic()));    client.updateMetadata(TestUtils.metadataUpdateWith(1, Collections.singletonMap("test", 2)));    // send produce request    sender.runOnce();    client.respond(produceResponse(tp0, offset++, Errors.NONE, 0));    sender.runOnce();    assertEquals("Request completed.", 0, client.inFlightRequestCount());    assertFalse(client.hasInFlightRequests());    assertEquals(0, sender.inFlightBatches(tp0).size());    sender.runOnce();    assertTrue("Request should be completed", future.isDone());}
f6675
0
testMustNotRetryOutOfOrderSequenceForNextBatch
public void kafkatest_f6683_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    assertEquals(0, transactionManager.sequenceNumber(tp0).longValue());    // Send first ProduceRequest with multiple messages.    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);    sender.runOnce();    String nodeId = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(nodeId), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    // make sure the next sequence number accounts for multi-message batches.    assertEquals(2, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(0, tp0, Errors.NONE, 0);    sender.runOnce();    // Send second ProduceRequest    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(3, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    assertTrue(request1.isDone());    assertEquals(0, request1.get().offset());    assertFalse(request2.isDone());    assertTrue(client.isReady(node, time.milliseconds()));    // This OutOfOrderSequence is fatal since it is returned for the batch succeeding the last acknowledged batch.    sendIdempotentProducerResponse(2, tp0, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1L);    sender.runOnce();    assertFutureFailure(request2, OutOfOrderSequenceException.class);}
f6683
0
testCorrectHandlingOfOutOfOrderResponses
public void kafkatest_f6684_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    assertEquals(0, transactionManager.sequenceNumber(tp0).longValue());    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    String nodeId = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(nodeId), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    // Send second ProduceRequest    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertEquals(2, client.inFlightRequestCount());    assertEquals(2, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertFalse(request1.isDone());    assertFalse(request2.isDone());    assertTrue(client.isReady(node, time.milliseconds()));    ClientRequest firstClientRequest = client.requests().peek();    ClientRequest secondClientRequest = (ClientRequest) client.requests().toArray()[1];    client.respondToRequest(secondClientRequest, produceResponse(tp0, -1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1));    // receive response 1    sender.runOnce();    Deque<ProducerBatch> queuedBatches = accumulator.batches().get(tp0);    // Make sure that we are queueing the second batch first.    assertEquals(1, queuedBatches.size());    assertEquals(1, queuedBatches.peekFirst().baseSequence());    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    client.respondToRequest(firstClientRequest, produceResponse(tp0, -1, Errors.NOT_LEADER_FOR_PARTITION, -1));    // receive response 0    sender.runOnce();    // Make sure we requeued both batches in the correct order.    assertEquals(2, queuedBatches.size());    assertEquals(0, queuedBatches.peekFirst().baseSequence());    assertEquals(1, queuedBatches.peekLast().baseSequence());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertEquals(0, client.inFlightRequestCount());    assertFalse(request1.isDone());    assertFalse(request2.isDone());    // send request 0    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    // don't do anything, only one inflight allowed once we are retrying.    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    // Make sure that the requests are sent in order, even though the previous responses were not in order.    sendIdempotentProducerResponse(0, tp0, Errors.NONE, 0L);    // receive response 0    sender.runOnce();    assertEquals(OptionalInt.of(0), transactionManager.lastAckedSequence(tp0));    assertEquals(0, client.inFlightRequestCount());    assertTrue(request1.isDone());    assertEquals(0, request1.get().offset());    // send request 1    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    sendIdempotentProducerResponse(1, tp0, Errors.NONE, 1L);    // receive response 1    sender.runOnce();    assertFalse(client.hasInFlightRequests());    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    assertTrue(request2.isDone());    assertEquals(1, request2.get().offset());}
f6684
0
testCorrectHandlingOfOutOfOrderResponsesWhenSecondSucceeds
public void kafkatest_f6685_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    assertEquals(0, transactionManager.sequenceNumber(tp0).longValue());    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    String nodeId = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(nodeId), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    // Send second ProduceRequest    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertEquals(2, client.inFlightRequestCount());    assertFalse(request1.isDone());    assertFalse(request2.isDone());    assertTrue(client.isReady(node, time.milliseconds()));    ClientRequest firstClientRequest = client.requests().peek();    ClientRequest secondClientRequest = (ClientRequest) client.requests().toArray()[1];    client.respondToRequest(secondClientRequest, produceResponse(tp0, 1, Errors.NONE, 1));    // receive response 1    sender.runOnce();    assertTrue(request2.isDone());    assertEquals(1, request2.get().offset());    assertFalse(request1.isDone());    Deque<ProducerBatch> queuedBatches = accumulator.batches().get(tp0);    assertEquals(0, queuedBatches.size());    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    client.respondToRequest(firstClientRequest, produceResponse(tp0, -1, Errors.REQUEST_TIMED_OUT, -1));    // receive response 0    sender.runOnce();    // Make sure we requeued both batches in the correct order.    assertEquals(1, queuedBatches.size());    assertEquals(0, queuedBatches.peekFirst().baseSequence());    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    assertEquals(0, client.inFlightRequestCount());    // resend request 0    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    // Make sure we handle the out of order successful responses correctly.    sendIdempotentProducerResponse(0, tp0, Errors.NONE, 0L);    // receive response 0    sender.runOnce();    assertEquals(0, queuedBatches.size());    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    assertEquals(0, client.inFlightRequestCount());    assertFalse(client.hasInFlightRequests());    assertTrue(request1.isDone());    assertEquals(0, request1.get().offset());}
f6685
0
testForceCloseWithProducerIdReset
public void kafkatest_f6693_0() throws Exception
{    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(1L, (short) 0));    setupWithTransactionState(transactionManager);    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, 10, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // connect and send.    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    Map<TopicPartition, OffsetAndError> responses = new LinkedHashMap<>();    responses.put(tp1, new OffsetAndError(-1, Errors.NOT_LEADER_FOR_PARTITION));    responses.put(tp0, new OffsetAndError(-1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER));    client.respond(produceResponse(responses));    sender.runOnce();    assertTrue(failedResponse.isDone());    assertFalse("Expected transaction state to be reset upon receiving an OutOfOrderSequenceException", transactionManager.hasProducerId());    // initiate force close    sender.forceClose();    // this should not block    sender.runOnce();    // run main loop to test forceClose flag    sender.run();    assertTrue("Pending batches are not aborted.", !accumulator.hasUndrained());    assertTrue(successfulResponse.isDone());}
f6693
0
testBatchesDrainedWithOldProducerIdShouldFailWithOutOfOrderSequenceOnSubsequentRetry
public void kafkatest_f6694_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));    setupWithTransactionState(transactionManager);    int maxRetries = 10;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // connect.    sender.runOnce();    // send.    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    Map<TopicPartition, OffsetAndError> responses = new LinkedHashMap<>();    responses.put(tp1, new OffsetAndError(-1, Errors.NOT_LEADER_FOR_PARTITION));    responses.put(tp0, new OffsetAndError(-1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER));    client.respond(produceResponse(responses));    sender.runOnce();    assertTrue(failedResponse.isDone());    assertFalse("Expected transaction state to be reset upon receiving an OutOfOrderSequenceException", transactionManager.hasProducerId());    prepareAndReceiveInitProducerId(producerId + 1, Errors.NONE);    assertEquals(producerId + 1, transactionManager.producerIdAndEpoch().producerId);    // send request to tp1 with the old producerId    sender.runOnce();    assertFalse(successfulResponse.isDone());    // The response comes back with a retriable error.    client.respond(produceResponse(tp1, 0, Errors.NOT_LEADER_FOR_PARTITION, -1));    sender.runOnce();    assertTrue(successfulResponse.isDone());    // exception.    try {        successfulResponse.get();        fail("Should have raised an OutOfOrderSequenceException");    } catch (Exception e) {        assertTrue(e.getCause() instanceof OutOfOrderSequenceException);    }}
f6694
0
testCorrectHandlingOfDuplicateSequenceError
public void kafkatest_f6695_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    assertEquals(0, transactionManager.sequenceNumber(tp0).longValue());    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    String nodeId = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(nodeId), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    // Send second ProduceRequest    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertEquals(2, client.inFlightRequestCount());    assertEquals(2, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertFalse(request1.isDone());    assertFalse(request2.isDone());    assertTrue(client.isReady(node, time.milliseconds()));    ClientRequest firstClientRequest = client.requests().peek();    ClientRequest secondClientRequest = (ClientRequest) client.requests().toArray()[1];    client.respondToRequest(secondClientRequest, produceResponse(tp0, 1000, Errors.NONE, 0));    // receive response 1    sender.runOnce();    assertEquals(OptionalLong.of(1000), transactionManager.lastAckedOffset(tp0));    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    client.respondToRequest(firstClientRequest, produceResponse(tp0, ProduceResponse.INVALID_OFFSET, Errors.DUPLICATE_SEQUENCE_NUMBER, 0));    // receive response 0    sender.runOnce();    // Make sure that the last ack'd sequence doesn't change.    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    assertEquals(OptionalLong.of(1000), transactionManager.lastAckedOffset(tp0));    assertFalse(client.hasInFlightRequests());    RecordMetadata unknownMetadata = request1.get();    assertFalse(unknownMetadata.hasOffset());    assertEquals(-1L, unknownMetadata.offset());}
f6695
0
testClusterAuthorizationExceptionInProduceRequest
public void kafkatest_f6703_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    // cluster authorization is a fatal error for the producer    Future<RecordMetadata> future = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            return body instanceof ProduceRequest && ((ProduceRequest) body).hasIdempotentRecords();        }    }, produceResponse(tp0, -1, Errors.CLUSTER_AUTHORIZATION_FAILED, 0));    sender.runOnce();    assertFutureFailure(future, ClusterAuthorizationException.class);    // cluster authorization errors are fatal, so we should continue seeing it on future sends    assertTrue(transactionManager.hasFatalError());    assertSendFailure(ClusterAuthorizationException.class);}
f6703
0
matches
public boolean kafkatest_f6704_0(AbstractRequest body)
{    return body instanceof ProduceRequest && ((ProduceRequest) body).hasIdempotentRecords();}
f6704
0
testCancelInFlightRequestAfterFatalError
public void kafkatest_f6705_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    // cluster authorization is a fatal error for the producer    Future<RecordMetadata> future1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    Future<RecordMetadata> future2 = accumulator.append(tp1, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    client.respond(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            return body instanceof ProduceRequest && ((ProduceRequest) body).hasIdempotentRecords();        }    }, produceResponse(tp0, -1, Errors.CLUSTER_AUTHORIZATION_FAILED, 0));    sender.runOnce();    assertTrue(transactionManager.hasFatalError());    assertFutureFailure(future1, ClusterAuthorizationException.class);    sender.runOnce();    assertFutureFailure(future2, ClusterAuthorizationException.class);    // Should be fine if the second response eventually returns    client.respond(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            return body instanceof ProduceRequest && ((ProduceRequest) body).hasIdempotentRecords();        }    }, produceResponse(tp1, 0, Errors.NONE, 0));    sender.runOnce();}
f6705
0
matches
public boolean kafkatest_f6713_0(AbstractRequest body)
{    if (body instanceof ProduceRequest) {        ProduceRequest request = (ProduceRequest) body;        MemoryRecords records = request.partitionRecordsOrFail().get(tp0);        Iterator<MutableRecordBatch> batchIterator = records.batches().iterator();        assertTrue(batchIterator.hasNext());        RecordBatch batch = batchIterator.next();        assertFalse(batchIterator.hasNext());        assertEquals(0, batch.baseSequence());        assertEquals(producerId, batch.producerId());        assertEquals(0, batch.producerEpoch());        return true;    }    return false;}
f6713
0
testAbortRetryWhenProducerIdChanges
public void kafkatest_f6714_0() throws InterruptedException
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));    setupWithTransactionState(transactionManager);    int maxRetries = 10;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // connect.    sender.runOnce();    // send.    sender.runOnce();    String id = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(id), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    assertTrue("Client ready status should be true", client.isReady(node, time.milliseconds()));    client.disconnect(id);    assertEquals(0, client.inFlightRequestCount());    assertFalse("Client ready status should be false", client.isReady(node, time.milliseconds()));    transactionManager.resetProducerId();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId + 1, (short) 0));    // receive error    sender.runOnce();    // reconnect    sender.runOnce();    // nothing to do, since the pid has changed. We should check the metrics for errors.    sender.runOnce();    assertEquals("Expected requests to be aborted after pid change", 0, client.inFlightRequestCount());    KafkaMetric recordErrors = m.metrics().get(senderMetrics.recordErrorRate);    assertTrue("Expected non-zero value for record send errors", (Double) recordErrors.metricValue() > 0);    assertTrue(responseFuture.isDone());    assertEquals(0, (long) transactionManager.sequenceNumber(tp0));}
f6714
0
testResetWhenOutOfOrderSequenceReceived
public void kafkatest_f6715_0() throws InterruptedException
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));    setupWithTransactionState(transactionManager);    int maxRetries = 10;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // connect.    sender.runOnce();    // send.    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    client.respond(produceResponse(tp0, 0, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, 0));    sender.runOnce();    assertTrue(responseFuture.isDone());    assertEquals(0, sender.inFlightBatches(tp0).size());    assertFalse("Expected transaction state to be reset upon receiving an OutOfOrderSequenceException", transactionManager.hasProducerId());}
f6715
0
testExpiredBatchDoesNotSplitOnMessageTooLargeError
public void kafkatest_f6723_0() throws Exception
{    long deliverTimeoutMs = 1500L;    // create a producer batch with more than one record so it is eligible for splitting    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key1".getBytes(), "value1".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key2".getBytes(), "value2".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // send request    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    // return a MESSAGE_TOO_LARGE error    client.respond(produceResponse(tp0, -1, Errors.MESSAGE_TOO_LARGE, -1));    time.sleep(deliverTimeoutMs);    // expire the batch and process the response    sender.runOnce();    assertTrue(request1.isDone());    assertTrue(request2.isDone());    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());    // run again and must not split big batch and resend anything.    sender.runOnce();    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());}
f6723
0
testResetNextBatchExpiry
public void kafkatest_f6724_0() throws Exception
{    client = spy(new MockClient(time, metadata));    setupWithTransactionState(null);    accumulator.append(tp0, 0L, "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);    sender.runOnce();    sender.runOnce();    time.setCurrentTimeMs(time.milliseconds() + accumulator.getDeliveryTimeoutMs() + 1);    sender.runOnce();    InOrder inOrder = inOrder(client);    inOrder.verify(client, atLeastOnce()).ready(any(), anyLong());    inOrder.verify(client, atLeastOnce()).newClientRequest(anyString(), any(), anyLong(), anyBoolean(), anyInt(), any());    inOrder.verify(client, atLeastOnce()).send(any(), anyLong());    inOrder.verify(client).poll(eq(0L), anyLong());    inOrder.verify(client).poll(eq(accumulator.getDeliveryTimeoutMs()), anyLong());    inOrder.verify(client).poll(geq(1L), anyLong());}
f6724
0
testExpiredBatchesInMultiplePartitions
public void kafkatest_f6725_0() throws Exception
{    long deliveryTimeoutMs = 1500L;    setupWithTransactionState(null, true, null);    // Send multiple ProduceRequest across multiple partitions.    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "k1".getBytes(), "v1".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    Future<RecordMetadata> request2 = accumulator.append(tp1, time.milliseconds(), "k2".getBytes(), "v2".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // Send request.    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals("Expect one in-flight batch in accumulator", 1, sender.inFlightBatches(tp0).size());    Map<TopicPartition, ProduceResponse.PartitionResponse> responseMap = new HashMap<>();    responseMap.put(tp0, new ProduceResponse.PartitionResponse(Errors.NONE, 0L, 0L, 0L));    client.respond(new ProduceResponse(responseMap));    // Successfully expire both batches.    time.sleep(deliveryTimeoutMs);    sender.runOnce();    assertEquals("Expect zero in-flight batch in accumulator", 0, sender.inFlightBatches(tp0).size());    try {        request1.get();        fail("The expired batch should throw a TimeoutException");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    try {        request2.get();        fail("The expired batch should throw a TimeoutException");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }}
f6725
0
deallocate
public void kafkatest_f6733_0(ByteBuffer buffer, int size)
{    if (!allocatedBuffers.containsKey(buffer)) {        throw new IllegalStateException("Deallocating a buffer that is not allocated");    }    allocatedBuffers.remove(buffer);    super.deallocate(buffer, size);}
f6733
0
allMatch
public boolean kafkatest_f6734_0()
{    return allocatedBuffers.isEmpty();}
f6734
0
produceRequestMatcher
private MockClient.RequestMatcher kafkatest_f6735_0(final TopicPartition tp, final ProducerIdAndEpoch producerIdAndEpoch, final int sequence, final boolean isTransactional)
{    return new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            if (!(body instanceof ProduceRequest))                return false;            ProduceRequest request = (ProduceRequest) body;            Map<TopicPartition, MemoryRecords> recordsMap = request.partitionRecordsOrFail();            MemoryRecords records = recordsMap.get(tp);            if (records == null)                return false;            List<MutableRecordBatch> batches = TestUtils.toList(records.batches());            if (batches.isEmpty() || batches.size() > 1)                return false;            MutableRecordBatch batch = batches.get(0);            return batch.baseOffset() == 0L && batch.baseSequence() == sequence && batch.producerId() == producerIdAndEpoch.producerId && batch.producerEpoch() == producerIdAndEpoch.epoch && batch.isTransactional() == isTransactional;        }    };}
f6735
0
prepareAndReceiveInitProducerId
private void kafkatest_f6743_0(long producerId, Errors error)
{    short producerEpoch = 0;    if (error != Errors.NONE)        producerEpoch = RecordBatch.NO_PRODUCER_EPOCH;    client.prepareResponse(body -> {        return body instanceof InitProducerIdRequest && ((InitProducerIdRequest) body).data.transactionalId() == null;    }, initProducerIdResponse(producerId, producerEpoch, error));    sender.runOnce();}
f6743
0
initProducerIdResponse
private InitProducerIdResponse kafkatest_f6744_0(long producerId, short producerEpoch, Errors error)
{    InitProducerIdResponseData responseData = new InitProducerIdResponseData().setErrorCode(error.code()).setProducerEpoch(producerEpoch).setProducerId(producerId).setThrottleTimeMs(0);    return new InitProducerIdResponse(responseData);}
f6744
0
doInitTransactions
private void kafkatest_f6745_0(TransactionManager transactionManager, ProducerIdAndEpoch producerIdAndEpoch)
{    transactionManager.initializeTransactions();    prepareFindCoordinatorResponse(Errors.NONE);    sender.runOnce();    sender.runOnce();    prepareInitProducerResponse(Errors.NONE, producerIdAndEpoch.producerId, producerIdAndEpoch.epoch);    sender.runOnce();    assertTrue(transactionManager.hasProducerId());}
f6745
0
testEndTxnNotSentIfIncompleteBatches
public void kafkatest_f6753_0()
{    long pid = 13131L;    short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    prepareAddPartitionsToTxn(tp0, Errors.NONE);    sender.runOnce();    assertTrue(transactionManager.isPartitionAdded(tp0));    transactionManager.beginCommit();    assertNull(transactionManager.nextRequestHandler(true));    assertTrue(transactionManager.nextRequestHandler(false).isEndTxn());}
f6753
0
testFailIfNotReadyForSendNoProducerId
public void kafkatest_f6754_0()
{    transactionManager.failIfNotReadyForSend();}
f6754
0
testFailIfNotReadyForSendIdempotentProducer
public void kafkatest_f6755_0()
{    TransactionManager idempotentTransactionManager = new TransactionManager();    idempotentTransactionManager.failIfNotReadyForSend();}
f6755
0
testHasOngoingTransactionFatalError
public void kafkatest_f6763_0()
{    long pid = 13131L;    short epoch = 1;    TopicPartition partition = new TopicPartition("foo", 0);    assertFalse(transactionManager.hasOngoingTransaction());    doInitTransactions(pid, epoch);    assertFalse(transactionManager.hasOngoingTransaction());    transactionManager.beginTransaction();    assertTrue(transactionManager.hasOngoingTransaction());    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(partition);    assertTrue(transactionManager.hasOngoingTransaction());    prepareAddPartitionsToTxn(partition, Errors.NONE);    sender.runOnce();    transactionManager.transitionToFatalError(new KafkaException());    assertFalse(transactionManager.hasOngoingTransaction());}
f6763
0
testMaybeAddPartitionToTransaction
public void kafkatest_f6764_0()
{    long pid = 13131L;    short epoch = 1;    TopicPartition partition = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(partition);    assertTrue(transactionManager.hasPartitionsToAdd());    assertFalse(transactionManager.isPartitionAdded(partition));    assertTrue(transactionManager.isPartitionPendingAdd(partition));    prepareAddPartitionsToTxn(partition, Errors.NONE);    sender.runOnce();    assertFalse(transactionManager.hasPartitionsToAdd());    assertTrue(transactionManager.isPartitionAdded(partition));    assertFalse(transactionManager.isPartitionPendingAdd(partition));    // adding the partition again should not have any effect    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(partition);    assertFalse(transactionManager.hasPartitionsToAdd());    assertTrue(transactionManager.isPartitionAdded(partition));    assertFalse(transactionManager.isPartitionPendingAdd(partition));}
f6764
0
testAddPartitionToTransactionOverridesRetryBackoffForConcurrentTransactions
public void kafkatest_f6765_0()
{    long pid = 13131L;    short epoch = 1;    TopicPartition partition = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(partition);    assertTrue(transactionManager.hasPartitionsToAdd());    assertFalse(transactionManager.isPartitionAdded(partition));    assertTrue(transactionManager.isPartitionPendingAdd(partition));    prepareAddPartitionsToTxn(partition, Errors.CONCURRENT_TRANSACTIONS);    sender.runOnce();    TransactionManager.TxnRequestHandler handler = transactionManager.nextRequestHandler(false);    assertNotNull(handler);    assertEquals(20, handler.retryBackoffMs());}
f6765
0
testIsSendToPartitionAllowedWithInFlightPartitionAddAfterAbortableError
public void kafkatest_f6773_0()
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    // Send the AddPartitionsToTxn request and leave it in-flight    sender.runOnce();    transactionManager.transitionToAbortableError(new KafkaException());    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    assertTrue(transactionManager.hasAbortableError());}
f6773
0
testIsSendToPartitionAllowedWithPendingPartitionAfterFatalError
public void kafkatest_f6774_0()
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    transactionManager.transitionToFatalError(new KafkaException());    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    assertTrue(transactionManager.hasFatalError());}
f6774
0
testIsSendToPartitionAllowedWithInFlightPartitionAddAfterFatalError
public void kafkatest_f6775_0()
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    // Send the AddPartitionsToTxn request and leave it in-flight    sender.runOnce();    transactionManager.transitionToFatalError(new KafkaException());    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    assertTrue(transactionManager.hasFatalError());}
f6775
0
testBatchCompletedAfterProducerReset
public void kafkatest_f6783_0()
{    final long producerId = 13131L;    final short epoch = 1;    ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(producerId, epoch);    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);    ProducerBatch b1 = writeIdempotentBatchWithValue(transactionManager, tp0, "1");    // The producerId might be reset due to a failure on another partition    ProducerIdAndEpoch updatedProducerIdAndEpoch = new ProducerIdAndEpoch(producerId + 1, epoch);    transactionManager.resetProducerId();    transactionManager.setProducerIdAndEpoch(updatedProducerIdAndEpoch);    ProducerBatch b2 = writeIdempotentBatchWithValue(transactionManager, tp0, "2");    assertEquals(1, transactionManager.sequenceNumber(tp0).intValue());    // If the request returns successfully, we should ignore the response and not update any state    ProduceResponse.PartitionResponse b1Response = new ProduceResponse.PartitionResponse(Errors.NONE, 500L, time.milliseconds(), 0L);    transactionManager.handleCompletedBatch(b1, b1Response);    assertEquals(1, transactionManager.sequenceNumber(tp0).intValue());    assertEquals(b2, transactionManager.nextBatchBySequence(tp0));}
f6783
0
writeIdempotentBatchWithValue
private ProducerBatch kafkatest_f6784_0(TransactionManager manager, TopicPartition tp, String value)
{    int seq = manager.sequenceNumber(tp);    manager.incrementSequenceNumber(tp, 1);    ProducerBatch batch = batchWithValue(tp, value);    batch.setProducerState(manager.producerIdAndEpoch(), seq, false);    manager.addInFlightBatch(batch);    batch.close();    return batch;}
f6784
0
batchWithValue
private ProducerBatch kafkatest_f6785_0(TopicPartition tp, String value)
{    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(64), CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    long currentTimeMs = time.milliseconds();    ProducerBatch batch = new ProducerBatch(tp, builder, currentTimeMs);    batch.tryAppend(currentTimeMs, new byte[0], value.getBytes(), new Header[0], null, currentTimeMs);    return batch;}
f6785
0
testLookupCoordinatorOnDisconnectAfterSend
public void kafkatest_f6793_0()
{    // This is called from the initTransactions method in the producer as the first order of business.    // It finds the coordinator and then gets a PID.    final long pid = 13131L;    final short epoch = 1;    TransactionalRequestResult initPidResult = transactionManager.initializeTransactions();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    // find coordinator    sender.runOnce();    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    prepareInitPidResponse(Errors.NONE, true, pid, epoch);    // send pid to coordinator, should get disconnected before receiving the response, and resend the    // FindCoordinator and InitPid requests.    sender.runOnce();    assertNull(transactionManager.coordinator(CoordinatorType.TRANSACTION));    assertFalse(initPidResult.isCompleted());    assertFalse(transactionManager.hasProducerId());    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    assertFalse(initPidResult.isCompleted());    prepareInitPidResponse(Errors.NONE, false, pid, epoch);    // get pid and epoch    sender.runOnce();    // The future should only return after the second round of retries succeed.    assertTrue(initPidResult.isCompleted());    assertTrue(transactionManager.hasProducerId());    assertEquals(pid, transactionManager.producerIdAndEpoch().producerId);    assertEquals(epoch, transactionManager.producerIdAndEpoch().epoch);}
f6793
0
testLookupCoordinatorOnDisconnectBeforeSend
public void kafkatest_f6794_0()
{    // This is called from the initTransactions method in the producer as the first order of business.    // It finds the coordinator and then gets a PID.    final long pid = 13131L;    final short epoch = 1;    TransactionalRequestResult initPidResult = transactionManager.initializeTransactions();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    // one loop to realize we need a coordinator.    sender.runOnce();    // next loop to find coordintor.    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    client.disconnect(brokerNode.idString());    client.blackout(brokerNode, 100);    // send pid to coordinator. Should get disconnected before the send and resend the FindCoordinator    // and InitPid requests.    sender.runOnce();    // waiting for the blackout period for the node to expire.    time.sleep(110);    assertNull(transactionManager.coordinator(CoordinatorType.TRANSACTION));    assertFalse(initPidResult.isCompleted());    assertFalse(transactionManager.hasProducerId());    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    assertFalse(initPidResult.isCompleted());    prepareInitPidResponse(Errors.NONE, false, pid, epoch);    // get pid and epoch    sender.runOnce();    // The future should only return after the second round of retries succeed.    assertTrue(initPidResult.isCompleted());    assertTrue(transactionManager.hasProducerId());    assertEquals(pid, transactionManager.producerIdAndEpoch().producerId);    assertEquals(epoch, transactionManager.producerIdAndEpoch().epoch);}
f6794
0
testLookupCoordinatorOnNotCoordinatorError
public void kafkatest_f6795_0()
{    // This is called from the initTransactions method in the producer as the first order of business.    // It finds the coordinator and then gets a PID.    final long pid = 13131L;    final short epoch = 1;    TransactionalRequestResult initPidResult = transactionManager.initializeTransactions();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    // find coordinator    sender.runOnce();    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    prepareInitPidResponse(Errors.NOT_COORDINATOR, false, pid, epoch);    // send pid, get not coordinator. Should resend the FindCoordinator and InitPid requests    sender.runOnce();    assertNull(transactionManager.coordinator(CoordinatorType.TRANSACTION));    assertFalse(initPidResult.isCompleted());    assertFalse(transactionManager.hasProducerId());    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    assertFalse(initPidResult.isCompleted());    prepareInitPidResponse(Errors.NONE, false, pid, epoch);    // get pid and epoch    sender.runOnce();    // The future should only return after the second round of retries succeed.    assertTrue(initPidResult.isCompleted());    assertTrue(transactionManager.hasProducerId());    assertEquals(pid, transactionManager.producerIdAndEpoch().producerId);    assertEquals(epoch, transactionManager.producerIdAndEpoch().epoch);}
f6795
0
testRecoveryFromAbortableErrorTransactionNotStarted
public void kafkatest_f6803_0() throws Exception
{    final long pid = 13131L;    final short epoch = 1;    final TopicPartition unauthorizedPartition = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(unauthorizedPartition);    Future<RecordMetadata> responseFuture = accumulator.append(unauthorizedPartition, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(singletonMap(unauthorizedPartition, Errors.TOPIC_AUTHORIZATION_FAILED));    sender.runOnce();    assertTrue(transactionManager.hasAbortableError());    transactionManager.beginAbort();    sender.runOnce();    assertTrue(responseFuture.isDone());    assertFutureFailed(responseFuture);    // No partitions added, so no need to prepare EndTxn response    sender.runOnce();    assertTrue(transactionManager.isReady());    assertFalse(transactionManager.hasPartitionsToAdd());    assertFalse(accumulator.hasIncomplete());    // ensure we can now start a new transaction    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(singletonMap(tp0, Errors.NONE));    sender.runOnce();    assertTrue(transactionManager.isPartitionAdded(tp0));    assertFalse(transactionManager.hasPartitionsToAdd());    transactionManager.beginCommit();    prepareProduceResponse(Errors.NONE, pid, epoch);    sender.runOnce();    assertTrue(responseFuture.isDone());    assertNotNull(responseFuture.get());    prepareEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertTrue(transactionManager.isReady());}
f6803
0
testRecoveryFromAbortableErrorTransactionStarted
public void kafkatest_f6804_0() throws Exception
{    final long pid = 13131L;    final short epoch = 1;    final TopicPartition unauthorizedPartition = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    prepareAddPartitionsToTxn(tp0, Errors.NONE);    Future<RecordMetadata> authorizedTopicProduceFuture = accumulator.append(unauthorizedPartition, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertTrue(transactionManager.isPartitionAdded(tp0));    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(unauthorizedPartition);    Future<RecordMetadata> unauthorizedTopicProduceFuture = accumulator.append(unauthorizedPartition, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(singletonMap(unauthorizedPartition, Errors.TOPIC_AUTHORIZATION_FAILED));    sender.runOnce();    assertTrue(transactionManager.hasAbortableError());    assertTrue(transactionManager.isPartitionAdded(tp0));    assertFalse(transactionManager.isPartitionAdded(unauthorizedPartition));    assertFalse(authorizedTopicProduceFuture.isDone());    assertFalse(unauthorizedTopicProduceFuture.isDone());    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    transactionManager.beginAbort();    sender.runOnce();    // neither produce request has been sent, so they should both be failed immediately    assertFutureFailed(authorizedTopicProduceFuture);    assertFutureFailed(unauthorizedTopicProduceFuture);    assertTrue(transactionManager.isReady());    assertFalse(transactionManager.hasPartitionsToAdd());    assertFalse(accumulator.hasIncomplete());    // ensure we can now start a new transaction    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    FutureRecordMetadata nextTransactionFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(singletonMap(tp0, Errors.NONE));    sender.runOnce();    assertTrue(transactionManager.isPartitionAdded(tp0));    assertFalse(transactionManager.hasPartitionsToAdd());    transactionManager.beginCommit();    prepareProduceResponse(Errors.NONE, pid, epoch);    sender.runOnce();    assertTrue(nextTransactionFuture.isDone());    assertNotNull(nextTransactionFuture.get());    prepareEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertTrue(transactionManager.isReady());}
f6804
0
testRecoveryFromAbortableErrorProduceRequestInRetry
public void kafkatest_f6805_0() throws Exception
{    final long pid = 13131L;    final short epoch = 1;    final TopicPartition unauthorizedPartition = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    prepareAddPartitionsToTxn(tp0, Errors.NONE);    Future<RecordMetadata> authorizedTopicProduceFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertTrue(transactionManager.isPartitionAdded(tp0));    accumulator.beginFlush();    prepareProduceResponse(Errors.REQUEST_TIMED_OUT, pid, epoch);    sender.runOnce();    assertFalse(authorizedTopicProduceFuture.isDone());    assertTrue(accumulator.hasIncomplete());    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(unauthorizedPartition);    Future<RecordMetadata> unauthorizedTopicProduceFuture = accumulator.append(unauthorizedPartition, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(singletonMap(unauthorizedPartition, Errors.TOPIC_AUTHORIZATION_FAILED));    sender.runOnce();    assertTrue(transactionManager.hasAbortableError());    assertTrue(transactionManager.isPartitionAdded(tp0));    assertFalse(transactionManager.isPartitionAdded(unauthorizedPartition));    assertFalse(authorizedTopicProduceFuture.isDone());    prepareProduceResponse(Errors.NONE, pid, epoch);    sender.runOnce();    assertFutureFailed(unauthorizedTopicProduceFuture);    assertTrue(authorizedTopicProduceFuture.isDone());    assertNotNull(authorizedTopicProduceFuture.get());    assertTrue(authorizedTopicProduceFuture.isDone());    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    transactionManager.beginAbort();    sender.runOnce();    // neither produce request has been sent, so they should both be failed immediately    assertTrue(transactionManager.isReady());    assertFalse(transactionManager.hasPartitionsToAdd());    assertFalse(accumulator.hasIncomplete());    // ensure we can now start a new transaction    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    FutureRecordMetadata nextTransactionFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(singletonMap(tp0, Errors.NONE));    sender.runOnce();    assertTrue(transactionManager.isPartitionAdded(tp0));    assertFalse(transactionManager.hasPartitionsToAdd());    transactionManager.beginCommit();    prepareProduceResponse(Errors.NONE, pid, epoch);    sender.runOnce();    assertTrue(nextTransactionFuture.isDone());    assertNotNull(nextTransactionFuture.get());    prepareEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertTrue(transactionManager.isReady());}
f6805
0
testCommitTransactionWithUnsentProduceRequest
public void kafkatest_f6813_0() throws Exception
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(tp0, Errors.NONE);    sender.runOnce();    assertTrue(accumulator.hasUndrained());    // committing the transaction should cause the unsent batch to be flushed    transactionManager.beginCommit();    sender.runOnce();    assertFalse(accumulator.hasUndrained());    assertTrue(accumulator.hasIncomplete());    assertFalse(transactionManager.hasInFlightTransactionalRequest());    assertFalse(responseFuture.isDone());    // until the produce future returns, we will not send EndTxn    sender.runOnce();    assertFalse(accumulator.hasUndrained());    assertTrue(accumulator.hasIncomplete());    assertFalse(transactionManager.hasInFlightTransactionalRequest());    assertFalse(responseFuture.isDone());    // now the produce response returns    sendProduceResponse(Errors.NONE, pid, epoch);    sender.runOnce();    assertTrue(responseFuture.isDone());    assertFalse(accumulator.hasUndrained());    assertFalse(accumulator.hasIncomplete());    assertFalse(transactionManager.hasInFlightTransactionalRequest());    // now we send EndTxn    sender.runOnce();    assertTrue(transactionManager.hasInFlightTransactionalRequest());    sendEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertFalse(transactionManager.hasInFlightTransactionalRequest());    assertTrue(transactionManager.isReady());}
f6813
0
testCommitTransactionWithInFlightProduceRequest
public void kafkatest_f6814_0() throws Exception
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(tp0, Errors.NONE);    sender.runOnce();    assertTrue(accumulator.hasUndrained());    accumulator.beginFlush();    sender.runOnce();    assertFalse(accumulator.hasUndrained());    assertTrue(accumulator.hasIncomplete());    assertFalse(transactionManager.hasInFlightTransactionalRequest());    // now we begin the commit with the produce request still pending    transactionManager.beginCommit();    sender.runOnce();    assertFalse(accumulator.hasUndrained());    assertTrue(accumulator.hasIncomplete());    assertFalse(transactionManager.hasInFlightTransactionalRequest());    assertFalse(responseFuture.isDone());    // until the produce future returns, we will not send EndTxn    sender.runOnce();    assertFalse(accumulator.hasUndrained());    assertTrue(accumulator.hasIncomplete());    assertFalse(transactionManager.hasInFlightTransactionalRequest());    assertFalse(responseFuture.isDone());    // now the produce response returns    sendProduceResponse(Errors.NONE, pid, epoch);    sender.runOnce();    assertTrue(responseFuture.isDone());    assertFalse(accumulator.hasUndrained());    assertFalse(accumulator.hasIncomplete());    assertFalse(transactionManager.hasInFlightTransactionalRequest());    // now we send EndTxn    sender.runOnce();    assertTrue(transactionManager.hasInFlightTransactionalRequest());    sendEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertFalse(transactionManager.hasInFlightTransactionalRequest());    assertTrue(transactionManager.isReady());}
f6814
0
testFindCoordinatorAllowedInAbortableErrorState
public void kafkatest_f6815_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    // Send AddPartitionsRequest    sender.runOnce();    transactionManager.transitionToAbortableError(new KafkaException());    sendAddPartitionsToTxnResponse(Errors.NOT_COORDINATOR, tp0, epoch, pid);    // AddPartitions returns    sender.runOnce();    assertTrue(transactionManager.hasAbortableError());    assertNull(transactionManager.coordinator(CoordinatorType.TRANSACTION));    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    // FindCoordinator handled    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    assertTrue(transactionManager.hasAbortableError());}
f6815
0
shouldNotAddPartitionsToTransactionWhenTopicAuthorizationFailed
public void kafkatest_f6823_0() throws Exception
{    verifyAddPartitionsFailsWithPartitionLevelError(Errors.TOPIC_AUTHORIZATION_FAILED);}
f6823
0
shouldNotSendAbortTxnRequestWhenOnlyAddPartitionsRequestFailed
public void kafkatest_f6824_0()
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    prepareAddPartitionsToTxnResponse(Errors.TOPIC_AUTHORIZATION_FAILED, tp0, epoch, pid);    // Send AddPartitionsRequest    sender.runOnce();    TransactionalRequestResult abortResult = transactionManager.beginAbort();    assertFalse(abortResult.isCompleted());    sender.runOnce();    assertTrue(abortResult.isCompleted());    assertTrue(abortResult.isSuccessful());}
f6824
0
shouldNotSendAbortTxnRequestWhenOnlyAddOffsetsRequestFailed
public void kafkatest_f6825_0()
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    offsets.put(tp1, new OffsetAndMetadata(1));    final String consumerGroupId = "myconsumergroup";    transactionManager.sendOffsetsToTransaction(offsets, consumerGroupId);    TransactionalRequestResult abortResult = transactionManager.beginAbort();    prepareAddOffsetsToTxnResponse(Errors.GROUP_AUTHORIZATION_FAILED, consumerGroupId, pid, epoch);    // Send AddOffsetsToTxnRequest    sender.runOnce();    assertFalse(abortResult.isCompleted());    sender.runOnce();    assertTrue(transactionManager.isReady());    assertTrue(abortResult.isCompleted());    assertTrue(abortResult.isSuccessful());}
f6825
0
testDropCommitOnBatchExpiry
public void kafkatest_f6833_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    assertFalse(transactionManager.transactionContainsPartition(tp0));    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    // send addPartitions.    sender.runOnce();    // Check that only addPartitions was sent.    assertTrue(transactionManager.transactionContainsPartition(tp0));    assertTrue(transactionManager.isSendToPartitionAllowed(tp0));    assertFalse(responseFuture.isDone());    TransactionalRequestResult commitResult = transactionManager.beginCommit();    // Sleep 10 seconds to make sure that the batches in the queue would be expired if they can't be drained.    time.sleep(10000);    // Disconnect the target node for the pending produce request. This will ensure that sender will try to    // expire the batch.    Node clusterNode = metadata.fetch().nodes().get(0);    client.disconnect(clusterNode.idString());    // We should try to flush the produce, but expire it instead without sending anything.    sender.runOnce();    assertTrue(responseFuture.isDone());    try {        // make sure the produce was expired.        responseFuture.get();        fail("Expected to get a TimeoutException since the queued ProducerBatch should have been expired");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    // the commit shouldn't be completed without being sent since the produce request failed.    sender.runOnce();    assertTrue(commitResult.isCompleted());    // the commit shouldn't succeed since the produce request failed.    assertFalse(commitResult.isSuccessful());    assertTrue(transactionManager.hasAbortableError());    assertTrue(transactionManager.hasOngoingTransaction());    assertFalse(transactionManager.isCompleting());    assertTrue(transactionManager.transactionContainsPartition(tp0));    TransactionalRequestResult abortResult = transactionManager.beginAbort();    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    // send the abort.    sender.runOnce();    assertTrue(abortResult.isCompleted());    assertTrue(abortResult.isSuccessful());    assertFalse(transactionManager.hasOngoingTransaction());    assertFalse(transactionManager.transactionContainsPartition(tp0));}
f6833
0
testTransitionToFatalErrorWhenRetriedBatchIsExpired
public void kafkatest_f6834_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    assertFalse(transactionManager.transactionContainsPartition(tp0));    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    // send addPartitions.    sender.runOnce();    // Check that only addPartitions was sent.    assertTrue(transactionManager.transactionContainsPartition(tp0));    assertTrue(transactionManager.isSendToPartitionAllowed(tp0));    prepareProduceResponse(Errors.NOT_LEADER_FOR_PARTITION, pid, epoch);    // send the produce request.    sender.runOnce();    assertFalse(responseFuture.isDone());    TransactionalRequestResult commitResult = transactionManager.beginCommit();    // Sleep 10 seconds to make sure that the batches in the queue would be expired if they can't be drained.    time.sleep(10000);    // Disconnect the target node for the pending produce request. This will ensure that sender will try to    // expire the batch.    Node clusterNode = metadata.fetch().nodes().get(0);    client.disconnect(clusterNode.idString());    client.blackout(clusterNode, 100);    // We should try to flush the produce, but expire it instead without sending anything.    sender.runOnce();    assertTrue(responseFuture.isDone());    try {        // make sure the produce was expired.        responseFuture.get();        fail("Expected to get a TimeoutException since the queued ProducerBatch should have been expired");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    // Transition to fatal error since we have unresolved batches.    sender.runOnce();    // Fail the queued transactional requests    sender.runOnce();    assertTrue(commitResult.isCompleted());    // the commit should have been dropped.    assertFalse(commitResult.isSuccessful());    assertTrue(transactionManager.hasFatalError());    assertFalse(transactionManager.hasOngoingTransaction());}
f6834
0
testResetProducerIdAfterWithoutPendingInflightRequests
public void kafkatest_f6835_0()
{    TransactionManager manager = new TransactionManager(logContext, null, transactionTimeoutMs, DEFAULT_RETRY_BACKOFF_MS);    long producerId = 15L;    short epoch = 5;    ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(producerId, epoch);    manager.setProducerIdAndEpoch(producerIdAndEpoch);    // Nothing to resolve, so no reset is needed    manager.resetProducerIdIfNeeded();    assertEquals(producerIdAndEpoch, manager.producerIdAndEpoch());    TopicPartition tp0 = new TopicPartition("foo", 0);    assertEquals(Integer.valueOf(0), manager.sequenceNumber(tp0));    ProducerBatch b1 = writeIdempotentBatchWithValue(manager, tp0, "1");    assertEquals(Integer.valueOf(1), manager.sequenceNumber(tp0));    manager.handleCompletedBatch(b1, new ProduceResponse.PartitionResponse(Errors.NONE, 500L, time.milliseconds(), 0L));    assertEquals(OptionalInt.of(0), manager.lastAckedSequence(tp0));    // Marking sequence numbers unresolved without inflight requests is basically a no-op.    manager.markSequenceUnresolved(tp0);    manager.resetProducerIdIfNeeded();    assertEquals(producerIdAndEpoch, manager.producerIdAndEpoch());    assertFalse(manager.hasUnresolvedSequences());    // We have a new batch which fails with a timeout    ProducerBatch b2 = writeIdempotentBatchWithValue(manager, tp0, "2");    assertEquals(Integer.valueOf(2), manager.sequenceNumber(tp0));    manager.markSequenceUnresolved(tp0);    manager.handleFailedBatch(b2, new TimeoutException(), false);    assertTrue(manager.hasUnresolvedSequences());    // We only had one inflight batch, so we should be able to clear the unresolved status    // and reset the producerId    manager.resetProducerIdIfNeeded();    assertFalse(manager.hasUnresolvedSequences());    assertFalse(manager.hasProducerId());}
f6835
0
verifyAddPartitionsFailsWithPartitionLevelError
private void kafkatest_f6843_0(final Errors error) throws InterruptedException
{    final long pid = 1L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxn(tp0, error);    // attempt send addPartitions.    sender.runOnce();    assertTrue(transactionManager.hasError());    assertFalse(transactionManager.transactionContainsPartition(tp0));}
f6843
0
prepareAddPartitionsToTxn
private void kafkatest_f6844_0(final Map<TopicPartition, Errors> errors)
{    client.prepareResponse(body -> {        AddPartitionsToTxnRequest request = (AddPartitionsToTxnRequest) body;        assertEquals(new HashSet<>(request.partitions()), new HashSet<>(errors.keySet()));        return true;    }, new AddPartitionsToTxnResponse(0, errors));}
f6844
0
prepareAddPartitionsToTxn
private void kafkatest_f6845_0(final TopicPartition tp, final Errors error)
{    prepareAddPartitionsToTxn(Collections.singletonMap(tp, error));}
f6845
0
addPartitionsRequestMatcher
private MockClient.RequestMatcher kafkatest_f6853_0(final TopicPartition topicPartition, final short epoch, final long pid)
{    return body -> {        AddPartitionsToTxnRequest addPartitionsToTxnRequest = (AddPartitionsToTxnRequest) body;        assertEquals(pid, addPartitionsToTxnRequest.producerId());        assertEquals(epoch, addPartitionsToTxnRequest.producerEpoch());        assertEquals(singletonList(topicPartition), addPartitionsToTxnRequest.partitions());        assertEquals(transactionalId, addPartitionsToTxnRequest.transactionalId());        return true;    };}
f6853
0
prepareEndTxnResponse
private void kafkatest_f6854_0(Errors error, final TransactionResult result, final long pid, final short epoch)
{    this.prepareEndTxnResponse(error, result, pid, epoch, false);}
f6854
0
prepareEndTxnResponse
private void kafkatest_f6855_0(Errors error, final TransactionResult result, final long pid, final short epoch, final boolean shouldDisconnect)
{    client.prepareResponse(endTxnMatcher(result, pid, epoch), new EndTxnResponse(0, error), shouldDisconnect);}
f6855
0
assertFatalError
private void kafkatest_f6863_0(Class<? extends RuntimeException> cause)
{    assertTrue(transactionManager.hasError());    try {        transactionManager.beginAbort();        fail("Should have raised " + cause.getSimpleName());    } catch (KafkaException e) {        assertTrue(cause.isAssignableFrom(e.getCause().getClass()));        assertTrue(transactionManager.hasError());    }    // Transaction abort cannot clear fatal error state    try {        transactionManager.beginAbort();        fail("Should have raised " + cause.getSimpleName());    } catch (KafkaException e) {        assertTrue(cause.isAssignableFrom(e.getCause().getClass()));        assertTrue(transactionManager.hasError());    }}
f6863
0
assertFutureFailed
private void kafkatest_f6864_0(Future<RecordMetadata> future) throws InterruptedException
{    assertTrue(future.isDone());    try {        future.get();        fail("Expected produce future to throw");    } catch (ExecutionException e) {    // expected    }}
f6864
0
testMetricsReporterAutoGeneratedClientId
public void kafkatest_f6865_0()
{    Properties props = new Properties();    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    props.setProperty(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());    KafkaProducer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());    MockMetricsReporter mockMetricsReporter = (MockMetricsReporter) producer.metrics.reporters().get(0);    Assert.assertEquals(producer.getClientId(), mockMetricsReporter.clientId);    producer.close();}
f6865
0
shouldCloseProperlyAndThrowIfInterrupted
public void kafkatest_f6873_0() throws Exception
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    configs.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MockPartitioner.class.getName());    configs.put(ProducerConfig.BATCH_SIZE_CONFIG, "1");    Time time = new MockTime();    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap("topic", 1));    ProducerMetadata metadata = newMetadata(0, Long.MAX_VALUE);    MockClient client = new MockClient(time, metadata);    client.updateMetadata(initialUpdateResponse);    final Producer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, client, null, time);    ExecutorService executor = Executors.newSingleThreadExecutor();    final AtomicReference<Exception> closeException = new AtomicReference<>();    try {        Future<?> future = executor.submit(() -> {            producer.send(new ProducerRecord<>("topic", "key", "value"));            try {                producer.close();                fail("Close should block and throw.");            } catch (Exception e) {                closeException.set(e);            }        });        // Close producer should not complete until send succeeds        try {            future.get(100, TimeUnit.MILLISECONDS);            fail("Close completed without waiting for send");        } catch (java.util.concurrent.TimeoutException expected) {        /* ignore */        }        // Ensure send has started        client.waitForRequests(1, 1000);        assertTrue("Close terminated prematurely", future.cancel(true));        TestUtils.waitForCondition(() -> closeException.get() != null, "InterruptException did not occur within timeout.");        assertTrue("Expected exception not thrown " + closeException, closeException.get() instanceof InterruptException);    } finally {        executor.shutdownNow();    }}
f6873
0
testOsDefaultSocketBufferSizes
public void kafkatest_f6874_0()
{    Map<String, Object> config = new HashMap<>();    config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    config.put(ProducerConfig.SEND_BUFFER_CONFIG, Selectable.USE_DEFAULT_BUFFER_SIZE);    config.put(ProducerConfig.RECEIVE_BUFFER_CONFIG, Selectable.USE_DEFAULT_BUFFER_SIZE);    new KafkaProducer<>(config, new ByteArraySerializer(), new ByteArraySerializer()).close();}
f6874
0
testInvalidSocketSendBufferSize
public void kafkatest_f6875_0()
{    Map<String, Object> config = new HashMap<>();    config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    config.put(ProducerConfig.SEND_BUFFER_CONFIG, -2);    new KafkaProducer<>(config, new ByteArraySerializer(), new ByteArraySerializer());}
f6875
0
testMetadataTimeoutWithPartitionOutOfRange
public void kafkatest_f6883_0() throws Exception
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    configs.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, 60000);    // Create a record with a partition higher than the initial (outdated) partition range    ProducerRecord<String, String> record = new ProducerRecord<>(topic, 2, null, "value");    ProducerMetadata metadata = mock(ProducerMetadata.class);    MockTime mockTime = new MockTime();    AtomicInteger invocationCount = new AtomicInteger(0);    when(metadata.fetch()).then(invocation -> {        invocationCount.incrementAndGet();        if (invocationCount.get() == 5) {            mockTime.setCurrentTimeMs(mockTime.milliseconds() + 70000);        }        return onePartitionCluster;    });    KafkaProducer<String, String> producer = new KafkaProducer<String, String>(configs, new StringSerializer(), new StringSerializer(), metadata, new MockClient(Time.SYSTEM, metadata), null, mockTime) {        @Override        Sender newSender(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata) {            // give Sender its own Metadata instance so that we can isolate Metadata calls from KafkaProducer            return super.newSender(logContext, kafkaClient, newMetadata(0, 100_000));        }    };    // Four request updates where the requested partition is out of range, at which point the timeout expires    // and a TimeoutException is thrown    Future future = producer.send(record);    verify(metadata, times(4)).requestUpdate();    verify(metadata, times(4)).awaitUpdate(anyInt(), anyLong());    verify(metadata, times(5)).fetch();    try {        future.get();    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    } finally {        producer.close(Duration.ofMillis(0));    }}
f6883
0
newSender
 Sender kafkatest_f6884_0(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata)
{    // give Sender its own Metadata instance so that we can isolate Metadata calls from KafkaProducer    return super.newSender(logContext, kafkaClient, newMetadata(0, 100_000));}
f6884
0
testTopicRefreshInMetadata
public void kafkatest_f6885_0() throws InterruptedException
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    configs.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, "600000");    long refreshBackoffMs = 500L;    long metadataExpireMs = 60000L;    final Time time = new MockTime();    final ProducerMetadata metadata = new ProducerMetadata(refreshBackoffMs, metadataExpireMs, new LogContext(), new ClusterResourceListeners(), time);    final String topic = "topic";    try (KafkaProducer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, new MockClient(time, metadata), null, time)) {        AtomicBoolean running = new AtomicBoolean(true);        Thread t = new Thread(() -> {            long startTimeMs = System.currentTimeMillis();            while (running.get()) {                while (!metadata.updateRequested() && System.currentTimeMillis() - startTimeMs < 100) Thread.yield();                MetadataResponse updateResponse = TestUtils.metadataUpdateWith("kafka-cluster", 1, singletonMap(topic, Errors.UNKNOWN_TOPIC_OR_PARTITION), emptyMap());                metadata.update(updateResponse, time.milliseconds());                time.sleep(60 * 1000L);            }        });        t.start();        assertThrows(TimeoutException.class, () -> producer.partitionsFor(topic));        running.set(false);        t.join();    }}
f6885
0
testInitTransactionTimeout
public void kafkatest_f6893_0()
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "bad-transaction");    configs.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, 5);    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");    Time time = new MockTime(1);    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap("topic", 1));    ProducerMetadata metadata = newMetadata(0, Long.MAX_VALUE);    metadata.update(initialUpdateResponse, time.milliseconds());    MockClient client = new MockClient(time, metadata);    try (Producer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, client, null, time)) {        assertThrows(TimeoutException.class, producer::initTransactions);    }}
f6893
0
testInitTransactionWhileThrottled
public void kafkatest_f6894_0()
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "some.id");    configs.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, 10000);    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");    Time time = new MockTime(1);    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap("topic", 1));    ProducerMetadata metadata = newMetadata(0, Long.MAX_VALUE);    MockClient client = new MockClient(time, metadata);    client.updateMetadata(initialUpdateResponse);    Node node = metadata.fetch().nodes().get(0);    client.throttle(node, 5000);    client.prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, host1));    client.prepareResponse(initProducerIdResponse(1L, (short) 5, Errors.NONE));    try (Producer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, client, null, time)) {        producer.initTransactions();    }}
f6894
0
initProducerIdResponse
private InitProducerIdResponse kafkatest_f6895_0(long producerId, short producerEpoch, Errors error)
{    InitProducerIdResponseData responseData = new InitProducerIdResponseData().setErrorCode(error.code()).setProducerEpoch(producerEpoch).setProducerId(producerId).setThrottleTimeMs(0);    return new InitProducerIdResponse(responseData);}
f6895
0
newMetadata
private ProducerMetadata kafkatest_f6903_0(long refreshBackoffMs, long expirationMs)
{    return new ProducerMetadata(refreshBackoffMs, expirationMs, new LogContext(), new ClusterResourceListeners(), Time.SYSTEM);}
f6903
0
buildMockProducer
private void kafkatest_f6904_0(boolean autoComplete)
{    this.producer = new MockProducer<>(autoComplete, new MockSerializer(), new MockSerializer());}
f6904
0
cleanup
public void kafkatest_f6905_0()
{    if (this.producer != null && !this.producer.closed())        this.producer.close();}
f6905
0
shouldThrowOnSendOffsetsToTransactionIfTransactionsNotInitialized
public void kafkatest_f6913_0()
{    buildMockProducer(true);    producer.sendOffsetsToTransaction(null, null);}
f6913
0
shouldThrowOnSendOffsetsToTransactionTransactionIfNoTransactionGotStarted
public void kafkatest_f6914_0()
{    buildMockProducer(true);    producer.initTransactions();    try {        producer.sendOffsetsToTransaction(null, null);        fail("Should have thrown as producer has no open transaction");    } catch (IllegalStateException e) {    }}
f6914
0
shouldThrowOnCommitIfTransactionsNotInitialized
public void kafkatest_f6915_0()
{    buildMockProducer(true);    producer.commitTransaction();}
f6915
0
shouldThrowFenceProducerIfTransactionsNotInitialized
public void kafkatest_f6923_0()
{    buildMockProducer(true);    producer.fenceProducer();}
f6923
0
shouldThrowOnBeginTransactionsIfProducerGotFenced
public void kafkatest_f6924_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.fenceProducer();    try {        producer.beginTransaction();        fail("Should have thrown as producer is fenced off");    } catch (ProducerFencedException e) {    }}
f6924
0
shouldThrowOnSendIfProducerGotFenced
public void kafkatest_f6925_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.fenceProducer();    try {        producer.send(null);        fail("Should have thrown as producer is fenced off");    } catch (KafkaException e) {        assertTrue("The root cause of the exception should be ProducerFenced", e.getCause() instanceof ProducerFencedException);    }}
f6925
0
shouldPreserveCommittedMessagesOnAbortIfTransactionsAreEnabled
public void kafkatest_f6933_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    producer.send(record1);    producer.send(record2);    producer.commitTransaction();    producer.beginTransaction();    producer.abortTransaction();    List<ProducerRecord<byte[], byte[]>> expectedResult = new ArrayList<>();    expectedResult.add(record1);    expectedResult.add(record2);    assertThat(producer.history(), equalTo(expectedResult));}
f6933
0
shouldPublishConsumerGroupOffsetsOnlyAfterCommitIfTransactionsAreEnabled
public void kafkatest_f6934_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    String group1 = "g1";    Map<TopicPartition, OffsetAndMetadata> group1Commit = new HashMap<TopicPartition, OffsetAndMetadata>() {        {            put(new TopicPartition(topic, 0), new OffsetAndMetadata(42L, null));            put(new TopicPartition(topic, 1), new OffsetAndMetadata(73L, null));        }    };    String group2 = "g2";    Map<TopicPartition, OffsetAndMetadata> group2Commit = new HashMap<TopicPartition, OffsetAndMetadata>() {        {            put(new TopicPartition(topic, 0), new OffsetAndMetadata(101L, null));            put(new TopicPartition(topic, 1), new OffsetAndMetadata(21L, null));        }    };    producer.sendOffsetsToTransaction(group1Commit, group1);    producer.sendOffsetsToTransaction(group2Commit, group2);    assertTrue(producer.consumerGroupOffsetsHistory().isEmpty());    Map<String, Map<TopicPartition, OffsetAndMetadata>> expectedResult = new HashMap<>();    expectedResult.put(group1, group1Commit);    expectedResult.put(group2, group2Commit);    producer.commitTransaction();    assertThat(producer.consumerGroupOffsetsHistory(), equalTo(Collections.singletonList(expectedResult)));}
f6934
0
shouldThrowOnNullConsumerGroupIdWhenSendOffsetsToTransaction
public void kafkatest_f6935_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    try {        producer.sendOffsetsToTransaction(Collections.<TopicPartition, OffsetAndMetadata>emptyMap(), null);        fail("Should have thrown NullPointerException");    } catch (NullPointerException e) {    }}
f6935
0
shouldThrowOnSendIfProducerIsClosed
public void kafkatest_f6943_0()
{    buildMockProducer(true);    producer.close();    try {        producer.send(null);        fail("Should have thrown as producer is already closed");    } catch (IllegalStateException e) {    }}
f6943
0
shouldThrowOnBeginTransactionIfProducerIsClosed
public void kafkatest_f6944_0()
{    buildMockProducer(true);    producer.close();    try {        producer.beginTransaction();        fail("Should have thrown as producer is already closed");    } catch (IllegalStateException e) {    }}
f6944
0
shouldThrowSendOffsetsToTransactionIfProducerIsClosed
public void kafkatest_f6945_0()
{    buildMockProducer(true);    producer.close();    try {        producer.sendOffsetsToTransaction(null, null);        fail("Should have thrown as producer is already closed");    } catch (IllegalStateException e) {    }}
f6945
0
shouldNotBeFlushedAfterFlush
public void kafkatest_f6953_0()
{    buildMockProducer(false);    producer.send(record1);    producer.flush();    assertTrue(producer.flushed());}
f6953
0
isError
private boolean kafkatest_f6954_0(Future<?> future)
{    try {        future.get();        return false;    } catch (Exception e) {        return true;    }}
f6954
0
testEqualsAndHashCode
public void kafkatest_f6955_0()
{    ProducerRecord<String, Integer> producerRecord = new ProducerRecord<>("test", 1, "key", 1);    assertEquals(producerRecord, producerRecord);    assertEquals(producerRecord.hashCode(), producerRecord.hashCode());    ProducerRecord<String, Integer> equalRecord = new ProducerRecord<>("test", 1, "key", 1);    assertEquals(producerRecord, equalRecord);    assertEquals(producerRecord.hashCode(), equalRecord.hashCode());    ProducerRecord<String, Integer> topicMisMatch = new ProducerRecord<>("test-1", 1, "key", 1);    assertFalse(producerRecord.equals(topicMisMatch));    ProducerRecord<String, Integer> partitionMismatch = new ProducerRecord<>("test", 2, "key", 1);    assertFalse(producerRecord.equals(partitionMismatch));    ProducerRecord<String, Integer> keyMisMatch = new ProducerRecord<>("test", 1, "key-1", 1);    assertFalse(producerRecord.equals(keyMisMatch));    ProducerRecord<String, Integer> valueMisMatch = new ProducerRecord<>("test", 1, "key", 2);    assertFalse(producerRecord.equals(valueMisMatch));    ProducerRecord<String, Integer> nullFieldsRecord = new ProducerRecord<>("topic", null, null, null, null, null);    assertEquals(nullFieldsRecord, nullFieldsRecord);    assertEquals(nullFieldsRecord.hashCode(), nullFieldsRecord.hashCode());}
f6955
0
asyncRequest
public ProduceRequestResult kafkatest_f6963_0(final long baseOffset, final RuntimeException error, final long timeout)
{    final ProduceRequestResult request = new ProduceRequestResult(topicPartition);    Thread thread = new Thread() {        public void run() {            try {                sleep(timeout);                request.set(baseOffset, RecordBatch.NO_TIMESTAMP, error);                request.done();            } catch (InterruptedException e) {            }        }    };    thread.start();    return request;}
f6963
0
run
public void kafkatest_f6964_0()
{    try {        sleep(timeout);        request.set(baseOffset, RecordBatch.NO_TIMESTAMP, error);        request.done();    } catch (InterruptedException e) {    }}
f6964
0
testRoundRobinWithUnavailablePartitions
public void kafkatest_f6965_0()
{    // Intentionally make the partition list not in partition order to test the edge    // cases.    List<PartitionInfo> partitions = asList(new PartitionInfo("test", 1, null, NODES, NODES), new PartitionInfo("test", 2, NODES[1], NODES, NODES), new PartitionInfo("test", 0, NODES[0], NODES, NODES));    // When there are some unavailable partitions, we want to make sure that (1) we    // always pick an available partition,    // and (2) the available partitions are selected in a round robin way.    int countForPart0 = 0;    int countForPart2 = 0;    Partitioner partitioner = new RoundRobinPartitioner();    Cluster cluster = new Cluster("clusterId", asList(NODES[0], NODES[1], NODES[2]), partitions, Collections.<String>emptySet(), Collections.<String>emptySet());    for (int i = 1; i <= 100; i++) {        int part = partitioner.partition("test", null, null, null, null, cluster);        assertTrue("We should never choose a leader-less node in round robin", part == 0 || part == 2);        if (part == 0)            countForPart0++;        else            countForPart2++;    }    assertEquals("The distribution between two available partitions should be even", countForPart0, countForPart2);}
f6965
0
testMatchesAtMostOne
public void kafkatest_f6973_0()
{    assertNull(ACL1.toFilter().findIndefiniteField());    assertNull(ACL2.toFilter().findIndefiniteField());    assertNull(ACL3.toFilter().findIndefiniteField());    assertFalse(ANY_ANONYMOUS.matchesAtMostOne());    assertFalse(ANY_DENY.matchesAtMostOne());    assertFalse(ANY_MYTOPIC.matchesAtMostOne());}
f6973
0
shouldNotThrowOnUnknownPatternType
public void kafkatest_f6974_0()
{    new AclBinding(new ResourcePattern(ResourceType.TOPIC, "foo", PatternType.UNKNOWN), ACL1.entry());}
f6974
0
shouldNotThrowOnUnknownResourceType
public void kafkatest_f6975_0()
{    new AclBinding(new ResourcePattern(ResourceType.UNKNOWN, "foo", PatternType.LITERAL), ACL1.entry());}
f6975
0
testIsUnknown
public void kafkatest_f6983_0() throws Exception
{    for (AclPermissionTypeTestInfo info : INFOS) {        assertEquals(info.ty + " was supposed to have unknown == " + info.unknown, info.unknown, info.ty.isUnknown());    }}
f6983
0
testCode
public void kafkatest_f6984_0() throws Exception
{    assertEquals(AclPermissionType.values().length, INFOS.length);    for (AclPermissionTypeTestInfo info : INFOS) {        assertEquals(info.ty + " was supposed to have code == " + info.code, info.code, info.ty.code());        assertEquals("AclPermissionType.fromCode(" + info.code + ") was supposed to be " + info.ty, info.ty, AclPermissionType.fromCode((byte) info.code));    }    assertEquals(AclPermissionType.UNKNOWN, AclPermissionType.fromCode((byte) 120));}
f6984
0
testName
public void kafkatest_f6985_0() throws Exception
{    for (AclPermissionTypeTestInfo info : INFOS) {        assertEquals("AclPermissionType.fromString(" + info.name + ") was supposed to be " + info.ty, info.ty, AclPermissionType.fromString(info.name));    }    assertEquals(AclPermissionType.UNKNOWN, AclPermissionType.fromString("something"));}
f6985
0
shouldMatchWhereResourceTypeIsAny
public void kafkatest_f6993_0()
{    assertTrue(new ResourcePatternFilter(ANY, "Name", PREFIXED).matches(new ResourcePattern(TOPIC, "Name", PREFIXED)));}
f6993
0
shouldMatchWhereResourceNameIsAny
public void kafkatest_f6994_0()
{    assertTrue(new ResourcePatternFilter(TOPIC, null, PREFIXED).matches(new ResourcePattern(TOPIC, "Name", PREFIXED)));}
f6994
0
shouldMatchWherePatternTypeIsAny
public void kafkatest_f6995_0()
{    assertTrue(new ResourcePatternFilter(TOPIC, null, PatternType.ANY).matches(new ResourcePattern(TOPIC, "Name", PREFIXED)));}
f6995
0
shouldNotMatchLiteralWildcardTheWayAround
public void kafkatest_f7003_0()
{    assertFalse(new ResourcePatternFilter(TOPIC, "*", LITERAL).matches(new ResourcePattern(TOPIC, "Name", LITERAL)));}
f7003
0
shouldNotMatchLiteralWildcardIfFilterHasPatternTypeOfAny
public void kafkatest_f7004_0()
{    assertFalse(new ResourcePatternFilter(TOPIC, "Name", PatternType.ANY).matches(new ResourcePattern(TOPIC, "*", LITERAL)));}
f7004
0
shouldMatchLiteralWildcardIfFilterHasPatternTypeOfMatch
public void kafkatest_f7005_0()
{    assertTrue(new ResourcePatternFilter(TOPIC, "Name", PatternType.MATCH).matches(new ResourcePattern(TOPIC, "*", LITERAL)));}
f7005
0
shouldThrowIfPatternTypeIsAny
public void kafkatest_f7013_0()
{    new ResourcePattern(ResourceType.TOPIC, "name", PatternType.ANY);}
f7013
0
shouldThrowIfResourceNameIsNull
public void kafkatest_f7014_0()
{    new ResourcePattern(ResourceType.TOPIC, null, PatternType.ANY);}
f7014
0
testPutGet
public void kafkatest_f7015_0()
{    Cache<String, String> cache = new LRUCache<>(4);    cache.put("a", "b");    cache.put("c", "d");    cache.put("e", "f");    cache.put("g", "h");    assertEquals(4, cache.size());    assertEquals("b", cache.get("a"));    assertEquals("d", cache.get("c"));    assertEquals("f", cache.get("e"));    assertEquals("h", cache.get("g"));}
f7015
0
testValuesWithPrefixOverride
public void kafkatest_f7023_0()
{    String prefix = "prefix.";    Properties props = new Properties();    props.put("sasl.mechanism", "PLAIN");    props.put("prefix.sasl.mechanism", "GSSAPI");    props.put("prefix.sasl.kerberos.kinit.cmd", "/usr/bin/kinit2");    props.put("prefix.ssl.truststore.location", "my location");    props.put("sasl.kerberos.service.name", "service name");    props.put("ssl.keymanager.algorithm", "algorithm");    TestSecurityConfig config = new TestSecurityConfig(props);    Map<String, Object> valuesWithPrefixOverride = config.valuesWithPrefixOverride(prefix);    // prefix overrides global    assertTrue(config.unused().contains("prefix.sasl.mechanism"));    assertTrue(config.unused().contains("sasl.mechanism"));    assertEquals("GSSAPI", valuesWithPrefixOverride.get("sasl.mechanism"));    assertFalse(config.unused().contains("sasl.mechanism"));    assertFalse(config.unused().contains("prefix.sasl.mechanism"));    // prefix overrides default    assertTrue(config.unused().contains("prefix.sasl.kerberos.kinit.cmd"));    assertFalse(config.unused().contains("sasl.kerberos.kinit.cmd"));    assertEquals("/usr/bin/kinit2", valuesWithPrefixOverride.get("sasl.kerberos.kinit.cmd"));    assertFalse(config.unused().contains("sasl.kerberos.kinit.cmd"));    assertFalse(config.unused().contains("prefix.sasl.kerberos.kinit.cmd"));    // prefix override with no default    assertTrue(config.unused().contains("prefix.ssl.truststore.location"));    assertFalse(config.unused().contains("ssl.truststore.location"));    assertEquals("my location", valuesWithPrefixOverride.get("ssl.truststore.location"));    assertFalse(config.unused().contains("ssl.truststore.location"));    assertFalse(config.unused().contains("prefix.ssl.truststore.location"));    // global overrides default    assertTrue(config.unused().contains("ssl.keymanager.algorithm"));    assertEquals("algorithm", valuesWithPrefixOverride.get("ssl.keymanager.algorithm"));    assertFalse(config.unused().contains("ssl.keymanager.algorithm"));    // global with no default    assertTrue(config.unused().contains("sasl.kerberos.service.name"));    assertEquals("service name", valuesWithPrefixOverride.get("sasl.kerberos.service.name"));    assertFalse(config.unused().contains("sasl.kerberos.service.name"));    // unset with default    assertFalse(config.unused().contains("sasl.kerberos.min.time.before.relogin"));    assertEquals(SaslConfigs.DEFAULT_KERBEROS_MIN_TIME_BEFORE_RELOGIN, valuesWithPrefixOverride.get("sasl.kerberos.min.time.before.relogin"));    assertFalse(config.unused().contains("sasl.kerberos.min.time.before.relogin"));    // unset with no default    assertFalse(config.unused().contains("ssl.key.password"));    assertNull(valuesWithPrefixOverride.get("ssl.key.password"));    assertFalse(config.unused().contains("ssl.key.password"));}
f7023
0
testValuesWithSecondaryPrefix
public void kafkatest_f7024_0()
{    String prefix = "listener.name.listener1.";    Password saslJaasConfig1 = new Password("test.myLoginModule1 required;");    Password saslJaasConfig2 = new Password("test.myLoginModule2 required;");    Password saslJaasConfig3 = new Password("test.myLoginModule3 required;");    Properties props = new Properties();    props.put("listener.name.listener1.test-mechanism.sasl.jaas.config", saslJaasConfig1.value());    props.put("test-mechanism.sasl.jaas.config", saslJaasConfig2.value());    props.put("sasl.jaas.config", saslJaasConfig3.value());    props.put("listener.name.listener1.gssapi.sasl.kerberos.kinit.cmd", "/usr/bin/kinit2");    props.put("listener.name.listener1.gssapi.sasl.kerberos.service.name", "testkafka");    props.put("listener.name.listener1.gssapi.sasl.kerberos.min.time.before.relogin", "60000");    props.put("ssl.provider", "TEST");    TestSecurityConfig config = new TestSecurityConfig(props);    Map<String, Object> valuesWithPrefixOverride = config.valuesWithPrefixOverride(prefix);    // prefix with mechanism overrides global    assertTrue(config.unused().contains("listener.name.listener1.test-mechanism.sasl.jaas.config"));    assertTrue(config.unused().contains("test-mechanism.sasl.jaas.config"));    assertEquals(saslJaasConfig1, valuesWithPrefixOverride.get("test-mechanism.sasl.jaas.config"));    assertEquals(saslJaasConfig3, valuesWithPrefixOverride.get("sasl.jaas.config"));    assertFalse(config.unused().contains("listener.name.listener1.test-mechanism.sasl.jaas.config"));    assertFalse(config.unused().contains("test-mechanism.sasl.jaas.config"));    assertFalse(config.unused().contains("sasl.jaas.config"));    // prefix with mechanism overrides default    assertFalse(config.unused().contains("sasl.kerberos.kinit.cmd"));    assertTrue(config.unused().contains("listener.name.listener1.gssapi.sasl.kerberos.kinit.cmd"));    assertFalse(config.unused().contains("gssapi.sasl.kerberos.kinit.cmd"));    assertFalse(config.unused().contains("sasl.kerberos.kinit.cmd"));    assertEquals("/usr/bin/kinit2", valuesWithPrefixOverride.get("gssapi.sasl.kerberos.kinit.cmd"));    assertFalse(config.unused().contains("listener.name.listener1.sasl.kerberos.kinit.cmd"));    // prefix override for mechanism with no default    assertFalse(config.unused().contains("sasl.kerberos.service.name"));    assertTrue(config.unused().contains("listener.name.listener1.gssapi.sasl.kerberos.service.name"));    assertFalse(config.unused().contains("gssapi.sasl.kerberos.service.name"));    assertFalse(config.unused().contains("sasl.kerberos.service.name"));    assertEquals("testkafka", valuesWithPrefixOverride.get("gssapi.sasl.kerberos.service.name"));    assertFalse(config.unused().contains("listener.name.listener1.gssapi.sasl.kerberos.service.name"));    // unset with no default    assertTrue(config.unused().contains("ssl.provider"));    assertNull(valuesWithPrefixOverride.get("gssapi.ssl.provider"));    assertTrue(config.unused().contains("ssl.provider"));}
f7024
0
testValuesWithPrefixAllOrNothing
public void kafkatest_f7025_0()
{    String prefix1 = "prefix1.";    String prefix2 = "prefix2.";    Properties props = new Properties();    props.put("sasl.mechanism", "PLAIN");    props.put("prefix1.sasl.mechanism", "GSSAPI");    props.put("prefix1.sasl.kerberos.kinit.cmd", "/usr/bin/kinit2");    props.put("prefix1.ssl.truststore.location", "my location");    props.put("sasl.kerberos.service.name", "service name");    props.put("ssl.keymanager.algorithm", "algorithm");    TestSecurityConfig config = new TestSecurityConfig(props);    Map<String, Object> valuesWithPrefixAllOrNothing1 = config.valuesWithPrefixAllOrNothing(prefix1);    // All prefixed values are there    assertEquals("GSSAPI", valuesWithPrefixAllOrNothing1.get("sasl.mechanism"));    assertEquals("/usr/bin/kinit2", valuesWithPrefixAllOrNothing1.get("sasl.kerberos.kinit.cmd"));    assertEquals("my location", valuesWithPrefixAllOrNothing1.get("ssl.truststore.location"));    // Non-prefixed values are missing    assertFalse(valuesWithPrefixAllOrNothing1.containsKey("sasl.kerberos.service.name"));    assertFalse(valuesWithPrefixAllOrNothing1.containsKey("ssl.keymanager.algorithm"));    Map<String, Object> valuesWithPrefixAllOrNothing2 = config.valuesWithPrefixAllOrNothing(prefix2);    assertTrue(valuesWithPrefixAllOrNothing2.containsKey("sasl.kerberos.service.name"));    assertTrue(valuesWithPrefixAllOrNothing2.containsKey("ssl.keymanager.algorithm"));}
f7025
0
testConfigProvidersPropsAsParam
public void kafkatest_f7033_0()
{    // Test Case: Valid Test Case for ConfigProviders as a separate variable    Properties providers = new Properties();    providers.put("config.providers", "file");    providers.put("config.providers.file.class", MockFileConfigProvider.class.getName());    Properties props = new Properties();    props.put("sasl.kerberos.key", "${file:/usr/kerberos:key}");    props.put("sasl.kerberos.password", "${file:/usr/kerberos:password}");    TestIndirectConfigResolution config = new TestIndirectConfigResolution(props, convertPropertiesToMap(providers));    assertEquals(config.originals().get("sasl.kerberos.key"), "testKey");    assertEquals(config.originals().get("sasl.kerberos.password"), "randomPassword");}
f7033
0
testImmutableOriginalsWithConfigProvidersProps
public void kafkatest_f7034_0()
{    // Test Case: Valid Test Case for ConfigProviders as a separate variable    Properties providers = new Properties();    providers.put("config.providers", "file");    providers.put("config.providers.file.class", "org.apache.kafka.common.config.provider.MockFileConfigProvider");    Properties props = new Properties();    props.put("sasl.kerberos.key", "${file:/usr/kerberos:key}");    Map<?, ?> immutableMap = Collections.unmodifiableMap(props);    Map<String, ?> provMap = convertPropertiesToMap(providers);    TestIndirectConfigResolution config = new TestIndirectConfigResolution(immutableMap, provMap);    assertEquals(config.originals().get("sasl.kerberos.key"), "testKey");}
f7034
0
testAutoConfigResolutionWithMultipleConfigProviders
public void kafkatest_f7035_0()
{    // Test Case: Valid Test Case With Multiple ConfigProviders as a separate variable    Properties providers = new Properties();    providers.put("config.providers", "file,vault");    providers.put("config.providers.file.class", MockFileConfigProvider.class.getName());    providers.put("config.providers.vault.class", MockVaultConfigProvider.class.getName());    Properties props = new Properties();    props.put("sasl.kerberos.key", "${file:/usr/kerberos:key}");    props.put("sasl.kerberos.password", "${file:/usr/kerberos:password}");    props.put("sasl.truststore.key", "${vault:/usr/truststore:truststoreKey}");    props.put("sasl.truststore.password", "${vault:/usr/truststore:truststorePassword}");    TestIndirectConfigResolution config = new TestIndirectConfigResolution(props, convertPropertiesToMap(providers));    assertEquals(config.originals().get("sasl.kerberos.key"), "testKey");    assertEquals(config.originals().get("sasl.kerberos.password"), "randomPassword");    assertEquals(config.originals().get("sasl.truststore.key"), "testTruststoreKey");    assertEquals(config.originals().get("sasl.truststore.password"), "randomtruststorePassword");}
f7035
0
overrideProps
private static Map<String, Object> kafkatest_f7043_0(Object classProp, Object listProp)
{    Map<String, Object> props = new HashMap<>();    if (classProp != null)        props.put("class.prop", classProp);    if (listProp != null)        props.put("list.prop", listProp);    return props;}
f7043
0
configure
public void kafkatest_f7044_0(Map<String, ?> configs)
{    FakeMetricsReporterConfig config = new FakeMetricsReporterConfig(configs);    // Calling getString() should have the side effect of marking that config as used.    config.getString(FakeMetricsReporterConfig.EXTRA_CONFIG);}
f7044
0
testBasicTypes
public void kafkatest_f7045_0()
{    ConfigDef def = new ConfigDef().define("a", Type.INT, 5, Range.between(0, 14), Importance.HIGH, "docs").define("b", Type.LONG, Importance.HIGH, "docs").define("c", Type.STRING, "hello", Importance.HIGH, "docs").define("d", Type.LIST, Importance.HIGH, "docs").define("e", Type.DOUBLE, Importance.HIGH, "docs").define("f", Type.CLASS, Importance.HIGH, "docs").define("g", Type.BOOLEAN, Importance.HIGH, "docs").define("h", Type.BOOLEAN, Importance.HIGH, "docs").define("i", Type.BOOLEAN, Importance.HIGH, "docs").define("j", Type.PASSWORD, Importance.HIGH, "docs");    Properties props = new Properties();    props.put("a", "1   ");    props.put("b", 2);    props.put("d", " a , b, c");    props.put("e", 42.5d);    props.put("f", String.class.getName());    props.put("g", "true");    props.put("h", "FalSE");    props.put("i", "TRUE");    props.put("j", "password");    Map<String, Object> vals = def.parse(props);    assertEquals(1, vals.get("a"));    assertEquals(2L, vals.get("b"));    assertEquals("hello", vals.get("c"));    assertEquals(asList("a", "b", "c"), vals.get("d"));    assertEquals(42.5d, vals.get("e"));    assertEquals(String.class, vals.get("f"));    assertEquals(true, vals.get("g"));    assertEquals(false, vals.get("h"));    assertEquals(true, vals.get("i"));    assertEquals(new Password("password"), vals.get("j"));    assertEquals(Password.HIDDEN, vals.get("j").toString());}
f7045
0
testInvalidDefaultRange
public void kafkatest_f7053_0()
{    new ConfigDef().define("name", Type.INT, -1, Range.between(0, 10), Importance.HIGH, "docs");}
f7053
0
testInvalidDefaultString
public void kafkatest_f7054_0()
{    new ConfigDef().define("name", Type.STRING, "bad", ValidString.in("valid", "values"), Importance.HIGH, "docs");}
f7054
0
testNestedClass
public void kafkatest_f7055_0()
{    // getName(), not getSimpleName() or getCanonicalName(), is the version that should be able to locate the class    Map<String, Object> props = Collections.<String, Object>singletonMap("name", NestedClass.class.getName());    new ConfigDef().define("name", Type.CLASS, Importance.HIGH, "docs").parse(props);}
f7055
0
testValidateCannotParse
public void kafkatest_f7063_0()
{    Map<String, ConfigValue> expected = new HashMap<>();    String errorMessageB = "Invalid value non_integer for configuration a: Not a number of type INT";    ConfigValue configA = new ConfigValue("a", null, Collections.emptyList(), Arrays.asList(errorMessageB));    expected.put("a", configA);    ConfigDef def = new ConfigDef().define("a", Type.INT, Importance.HIGH, "docs");    Map<String, String> props = new HashMap<>();    props.put("a", "non_integer");    List<ConfigValue> configs = def.validate(props);    for (ConfigValue config : configs) {        String name = config.name();        ConfigValue expectedConfig = expected.get(name);        assertEquals(expectedConfig, config);    }}
f7063
0
testCanAddInternalConfig
public void kafkatest_f7064_0() throws Exception
{    final String configName = "internal.config";    final ConfigDef configDef = new ConfigDef().defineInternal(configName, Type.STRING, "", Importance.LOW);    final HashMap<String, String> properties = new HashMap<>();    properties.put(configName, "value");    final List<ConfigValue> results = configDef.validate(properties);    final ConfigValue configValue = results.get(0);    assertEquals("value", configValue.value());    assertEquals(configName, configValue.name());}
f7064
0
testInternalConfigDoesntShowUpInDocs
public void kafkatest_f7065_0() throws Exception
{    final String name = "my.config";    final ConfigDef configDef = new ConfigDef().defineInternal(name, Type.STRING, "", Importance.LOW);    assertFalse(configDef.toHtmlTable().contains("my.config"));    assertFalse(configDef.toEnrichedRst().contains("my.config"));    assertFalse(configDef.toRst().contains("my.config"));}
f7065
0
toRst
public void kafkatest_f7073_0()
{    final ConfigDef def = new ConfigDef().define("opt1", Type.STRING, "a", ValidString.in("a", "b", "c"), Importance.HIGH, "docs1").define("opt2", Type.INT, Importance.MEDIUM, "docs2").define("opt3", Type.LIST, Arrays.asList("a", "b"), Importance.LOW, "docs3");    final String expectedRst = "" + "``opt2``\n" + "  docs2\n" + "\n" + "  * Type: int\n" + "  * Importance: medium\n" + "\n" + "``opt1``\n" + "  docs1\n" + "\n" + "  * Type: string\n" + "  * Default: a\n" + "  * Valid Values: [a, b, c]\n" + "  * Importance: high\n" + "\n" + "``opt3``\n" + "  docs3\n" + "\n" + "  * Type: list\n" + "  * Default: a,b\n" + "  * Importance: low\n" + "\n";    assertEquals(expectedRst, def.toRst());}
f7073
0
toEnrichedRst
public void kafkatest_f7074_0()
{    final ConfigDef def = new ConfigDef().define("opt1.of.group1", Type.STRING, "a", ValidString.in("a", "b", "c"), Importance.HIGH, "Doc doc.", "Group One", 0, Width.NONE, "..", Collections.<String>emptyList()).define("opt2.of.group1", Type.INT, ConfigDef.NO_DEFAULT_VALUE, Importance.MEDIUM, "Doc doc doc.", "Group One", 1, Width.NONE, "..", Arrays.asList("some.option1", "some.option2")).define("opt2.of.group2", Type.BOOLEAN, false, Importance.HIGH, "Doc doc doc doc.", "Group Two", 1, Width.NONE, "..", Collections.<String>emptyList()).define("opt1.of.group2", Type.BOOLEAN, false, Importance.HIGH, "Doc doc doc doc doc.", "Group Two", 0, Width.NONE, "..", Collections.singletonList("some.option")).define("poor.opt", Type.STRING, "foo", Importance.HIGH, "Doc doc doc doc.");    final String expectedRst = "" + "``poor.opt``\n" + "  Doc doc doc doc.\n" + "\n" + "  * Type: string\n" + "  * Default: foo\n" + "  * Importance: high\n" + "\n" + "Group One\n" + "^^^^^^^^^\n" + "\n" + "``opt1.of.group1``\n" + "  Doc doc.\n" + "\n" + "  * Type: string\n" + "  * Default: a\n" + "  * Valid Values: [a, b, c]\n" + "  * Importance: high\n" + "\n" + "``opt2.of.group1``\n" + "  Doc doc doc.\n" + "\n" + "  * Type: int\n" + "  * Importance: medium\n" + "  * Dependents: ``some.option1``, ``some.option2``\n" + "\n" + "Group Two\n" + "^^^^^^^^^\n" + "\n" + "``opt1.of.group2``\n" + "  Doc doc doc doc doc.\n" + "\n" + "  * Type: boolean\n" + "  * Default: false\n" + "  * Importance: high\n" + "  * Dependents: ``some.option``\n" + "\n" + "``opt2.of.group2``\n" + "  Doc doc doc doc.\n" + "\n" + "  * Type: boolean\n" + "  * Default: false\n" + "  * Importance: high\n" + "\n";    assertEquals(expectedRst, def.toEnrichedRst());}
f7074
0
testConvertValueToStringBoolean
public void kafkatest_f7075_0()
{    assertEquals("true", ConfigDef.convertToString(true, Type.BOOLEAN));    assertNull(ConfigDef.convertToString(null, Type.BOOLEAN));}
f7075
0
testConvertValueToStringClass
public void kafkatest_f7083_0() throws ClassNotFoundException
{    String actual = ConfigDef.convertToString(ConfigDefTest.class, Type.CLASS);    assertEquals("org.apache.kafka.common.config.ConfigDefTest", actual);    // Additionally validate that we can look up this class by this name    assertEquals(ConfigDefTest.class, Class.forName(actual));    assertNull(ConfigDef.convertToString(null, Type.CLASS));}
f7083
0
testConvertValueToStringNestedClass
public void kafkatest_f7084_0() throws ClassNotFoundException
{    String actual = ConfigDef.convertToString(NestedClass.class, Type.CLASS);    assertEquals("org.apache.kafka.common.config.ConfigDefTest$NestedClass", actual);    // Additionally validate that we can look up this class by this name    assertEquals(NestedClass.class, Class.forName(actual));}
f7084
0
testClassWithAlias
public void kafkatest_f7085_0()
{    final String alias = "PluginAlias";    ClassLoader originalClassLoader = Thread.currentThread().getContextClassLoader();    try {        // Could try to use the Plugins class from Connect here, but this should simulate enough        // of the aliasing logic to suffice for this test.        Thread.currentThread().setContextClassLoader(new ClassLoader(originalClassLoader) {            @Override            public Class<?> loadClass(String name, boolean resolve) throws ClassNotFoundException {                if (alias.equals(name)) {                    return NestedClass.class;                } else {                    return super.loadClass(name, resolve);                }            }        });        ConfigDef.parseType("Test config", alias, Type.CLASS);    } finally {        Thread.currentThread().setContextClassLoader(originalClassLoader);    }}
f7085
0
testReplaceMultipleVariablesInValue
public void kafkatest_f7093_0() throws Exception
{    ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, "hello, ${test:testPath:testKey}; goodbye, ${test:testPath:testKeyWithTTL}!!!"));    Map<String, String> data = result.data();    assertEquals("hello, testResult; goodbye, testResultWithTTL!!!", data.get(MY_KEY));}
f7093
0
testNoReplacement
public void kafkatest_f7094_0() throws Exception
{    ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, "${test:testPath:missingKey}"));    Map<String, String> data = result.data();    assertEquals("${test:testPath:missingKey}", data.get(MY_KEY));}
f7094
0
testSingleLevelOfIndirection
public void kafkatest_f7095_0() throws Exception
{    ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, "${test:testPath:testIndirection}"));    Map<String, String> data = result.data();    assertEquals("${test:testPath:testResult}", data.get(MY_KEY));}
f7095
0
testGetOneKeyAtPath
public void kafkatest_f7105_0() throws Exception
{    ConfigData configData = configProvider.get("dummy", Collections.singleton("testKey"));    Map<String, String> result = new HashMap<>();    result.put("testKey", "testResult");    assertEquals(result, configData.data());    assertEquals(null, configData.ttl());}
f7105
0
testEmptyPath
public void kafkatest_f7106_0() throws Exception
{    ConfigData configData = configProvider.get("", Collections.singleton("testKey"));    assertTrue(configData.data().isEmpty());    assertEquals(null, configData.ttl());}
f7106
0
testEmptyPathWithKey
public void kafkatest_f7107_0() throws Exception
{    ConfigData configData = configProvider.get("");    assertTrue(configData.data().isEmpty());    assertEquals(null, configData.ttl());}
f7107
0
testSaslLoginRefreshDefaults
public void kafkatest_f7115_0()
{    Map<String, Object> vals = new ConfigDef().withClientSaslSupport().parse(Collections.emptyMap());    assertEquals(SaslConfigs.DEFAULT_LOGIN_REFRESH_WINDOW_FACTOR, vals.get(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR));    assertEquals(SaslConfigs.DEFAULT_LOGIN_REFRESH_WINDOW_JITTER, vals.get(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER));    assertEquals(SaslConfigs.DEFAULT_LOGIN_REFRESH_MIN_PERIOD_SECONDS, vals.get(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS));    assertEquals(SaslConfigs.DEFAULT_LOGIN_REFRESH_BUFFER_SECONDS, vals.get(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS));}
f7115
0
testSaslLoginRefreshMinValuesAreValid
public void kafkatest_f7116_0()
{    Map<Object, Object> props = new HashMap<>();    props.put(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR, "0.5");    props.put(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER, "0.0");    props.put(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS, "0");    props.put(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS, "0");    Map<String, Object> vals = new ConfigDef().withClientSaslSupport().parse(props);    assertEquals(Double.valueOf("0.5"), vals.get(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR));    assertEquals(Double.valueOf("0.0"), vals.get(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER));    assertEquals(Short.valueOf("0"), vals.get(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS));    assertEquals(Short.valueOf("0"), vals.get(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS));}
f7116
0
testSaslLoginRefreshMaxValuesAreValid
public void kafkatest_f7117_0()
{    Map<Object, Object> props = new HashMap<>();    props.put(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR, "1.0");    props.put(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER, "0.25");    props.put(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS, "900");    props.put(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS, "3600");    Map<String, Object> vals = new ConfigDef().withClientSaslSupport().parse(props);    assertEquals(Double.valueOf("1.0"), vals.get(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR));    assertEquals(Double.valueOf("0.25"), vals.get(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER));    assertEquals(Short.valueOf("900"), vals.get(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS));    assertEquals(Short.valueOf("3600"), vals.get(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS));}
f7117
0
testSaslLoginRefreshBufferSecondsMaxValueIsReallyMaximum
public void kafkatest_f7125_0()
{    Map<Object, Object> props = new HashMap<>();    props.put(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS, "3601");    new ConfigDef().withClientSaslSupport().parse(props);}
f7125
0
testAdd
public void kafkatest_f7126_0()
{    Headers headers = new RecordHeaders();    headers.add(new RecordHeader("key", "value".getBytes()));    Header header = headers.iterator().next();    assertHeader("key", "value", header);    headers.add(new RecordHeader("key2", "value2".getBytes()));    assertHeader("key2", "value2", headers.lastHeader("key2"));    assertEquals(2, getCount(headers));}
f7126
0
testRemove
public void kafkatest_f7127_0()
{    Headers headers = new RecordHeaders();    headers.add(new RecordHeader("key", "value".getBytes()));    assertTrue(headers.iterator().hasNext());    headers.remove("key");    assertFalse(headers.iterator().hasNext());}
f7127
0
assertHeader
 static void kafkatest_f7135_0(String key, String value, Header actual)
{    assertEquals(key, actual.key());    assertTrue(Arrays.equals(value.getBytes(), actual.value()));}
f7135
0
testSet
public void kafkatest_f7136_0()
{    PartitionStates<String> states = new PartitionStates<>();    LinkedHashMap<TopicPartition, String> map = createMap();    states.set(map);    LinkedHashMap<TopicPartition, String> expected = new LinkedHashMap<>();    expected.put(new TopicPartition("foo", 2), "foo 2");    expected.put(new TopicPartition("foo", 0), "foo 0");    expected.put(new TopicPartition("blah", 2), "blah 2");    expected.put(new TopicPartition("blah", 1), "blah 1");    expected.put(new TopicPartition("baz", 2), "baz 2");    expected.put(new TopicPartition("baz", 3), "baz 3");    checkState(states, expected);    states.set(new LinkedHashMap<TopicPartition, String>());    checkState(states, new LinkedHashMap<TopicPartition, String>());}
f7136
0
createMap
private LinkedHashMap<TopicPartition, String> kafkatest_f7137_0()
{    LinkedHashMap<TopicPartition, String> map = new LinkedHashMap<>();    map.put(new TopicPartition("foo", 2), "foo 2");    map.put(new TopicPartition("blah", 2), "blah 2");    map.put(new TopicPartition("blah", 1), "blah 1");    map.put(new TopicPartition("baz", 2), "baz 2");    map.put(new TopicPartition("foo", 0), "foo 0");    map.put(new TopicPartition("baz", 3), "baz 3");    return map;}
f7137
0
shouldThrowOnInvalidTopicNames
public void kafkatest_f7145_0()
{    char[] longString = new char[250];    Arrays.fill(longString, 'a');    String[] invalidTopicNames = { "", "foo bar", "..", "foo:bar", "foo=bar", ".", new String(longString) };    for (String topicName : invalidTopicNames) {        try {            Topic.validate(topicName);            fail("No exception was thrown for topic with invalid name: " + topicName);        } catch (InvalidTopicException e) {        // Good        }    }}
f7145
0
shouldRecognizeInvalidCharactersInTopicNames
public void kafkatest_f7146_0()
{    char[] invalidChars = { '/', '\\', ',', '\u0000', ':', '"', '\'', ';', '*', '?', ' ', '\t', '\r', '\n', '=' };    for (char c : invalidChars) {        String topicName = "Is " + c + "illegal";        assertFalse(Topic.containsValidPattern(topicName));    }}
f7146
0
testTopicHasCollisionChars
public void kafkatest_f7147_0()
{    List<String> falseTopics = Arrays.asList("start", "end", "middle", "many");    List<String> trueTopics = Arrays.asList(".start", "end.", "mid.dle", ".ma.ny.", "_start", "end_", "mid_dle", "_ma_ny.");    for (String topic : falseTopics) assertFalse(Topic.hasCollisionChars(topic));    for (String topic : trueTopics) assertTrue(Topic.hasCollisionChars(topic));}
f7147
0
testAllOfFuturesHandlesZeroFutures
public void kafkatest_f7155_0() throws Exception
{    KafkaFuture<Void> allFuture = KafkaFuture.allOf();    assertTrue(allFuture.isDone());    assertFalse(allFuture.isCancelled());    assertFalse(allFuture.isCompletedExceptionally());    allFuture.get();}
f7155
0
testFutureTimeoutWithZeroWait
public void kafkatest_f7156_0() throws Exception
{    final KafkaFutureImpl<String> future = new KafkaFutureImpl<>();    future.get(0, TimeUnit.MILLISECONDS);}
f7156
0
testZeroSize
public void kafkatest_f7157_0() throws Exception
{    new GarbageCollectedMemoryPool(0, 7, true, null);}
f7157
0
testReleaseNull
public void kafkatest_f7165_0() throws Exception
{    GarbageCollectedMemoryPool pool = new GarbageCollectedMemoryPool(1000, 10, true, null);    pool.release(null);}
f7165
0
testReleaseForeignBuffer
public void kafkatest_f7166_0() throws Exception
{    GarbageCollectedMemoryPool pool = new GarbageCollectedMemoryPool(1000, 10, true, null);    ByteBuffer fellOffATruck = ByteBuffer.allocate(1);    pool.release(fellOffATruck);}
f7166
0
testDoubleFree
public void kafkatest_f7167_0() throws Exception
{    GarbageCollectedMemoryPool pool = new GarbageCollectedMemoryPool(1000, 10, false, null);    ByteBuffer buffer = pool.tryAllocate(5);    Assert.assertNotNull(buffer);    // so far so good    pool.release(buffer);    try {        pool.release(buffer);        Assert.fail("2nd release() should have failed");    } catch (IllegalArgumentException e) {    // as expected    } catch (Throwable t) {        Assert.fail("expected an IllegalArgumentException. instead got " + t);    }}
f7167
0
testCreateTopicsVersions
public void kafkatest_f7175_0() throws Exception
{    testAllMessageRoundTrips(new CreateTopicsRequestData().setTimeoutMs(1000).setTopics(new CreateTopicsRequestData.CreatableTopicCollection()));}
f7175
0
testDescribeAclsRequest
public void kafkatest_f7176_0() throws Exception
{    testAllMessageRoundTrips(new DescribeAclsRequestData().setResourceType((byte) 42).setResourceNameFilter(null).setResourcePatternType((byte) 3).setPrincipalFilter("abc").setHostFilter(null).setOperation((byte) 0).setPermissionType((byte) 0));}
f7176
0
testMetadataVersions
public void kafkatest_f7177_0() throws Exception
{    testAllMessageRoundTrips(new MetadataRequestData().setTopics(Arrays.asList(new MetadataRequestData.MetadataRequestTopic().setName("foo"), new MetadataRequestData.MetadataRequestTopic().setName("bar"))));    testAllMessageRoundTripsFromVersion((short) 1, new MetadataRequestData().setTopics(null).setAllowAutoTopicCreation(true).setIncludeClusterAuthorizedOperations(false).setIncludeTopicAuthorizedOperations(false));    testAllMessageRoundTripsFromVersion((short) 4, new MetadataRequestData().setTopics(null).setAllowAutoTopicCreation(false).setIncludeClusterAuthorizedOperations(false).setIncludeTopicAuthorizedOperations(false));}
f7177
0
testLeaderAndIsrVersions
public void kafkatest_f7185_0() throws Exception
{    // Version 3 adds two new fields - AddingReplicas and RemovingReplicas    LeaderAndIsrRequestData.LeaderAndIsrRequestTopicState partitionStateNoAddingRemovingReplicas = new LeaderAndIsrRequestData.LeaderAndIsrRequestTopicState().setName("topic").setPartitionStatesV0(Collections.singletonList(new LeaderAndIsrRequestData.LeaderAndIsrRequestPartition().setPartitionIndex(0).setReplicas(Collections.singletonList(0))));    LeaderAndIsrRequestData.LeaderAndIsrRequestTopicState partitionStateWithAddingRemovingReplicas = new LeaderAndIsrRequestData.LeaderAndIsrRequestTopicState().setName("topic").setPartitionStatesV0(Collections.singletonList(new LeaderAndIsrRequestData.LeaderAndIsrRequestPartition().setPartitionIndex(0).setReplicas(Collections.singletonList(0)).setAddingReplicas(Collections.singletonList(1)).setRemovingReplicas(Collections.singletonList(1))));    testAllMessageRoundTripsBetweenVersions((short) 2, (short) 3, new LeaderAndIsrRequestData().setTopicStates(Collections.singletonList(partitionStateWithAddingRemovingReplicas)), new LeaderAndIsrRequestData().setTopicStates(Collections.singletonList(partitionStateNoAddingRemovingReplicas)));    testAllMessageRoundTripsFromVersion((short) 3, new LeaderAndIsrRequestData().setTopicStates(Collections.singletonList(partitionStateWithAddingRemovingReplicas)));}
f7185
0
testOffsetCommitRequestVersions
public void kafkatest_f7186_0() throws Exception
{    String groupId = "groupId";    String topicName = "topic";    String metadata = "metadata";    int partition = 2;    int offset = 100;    testAllMessageRoundTrips(new OffsetCommitRequestData().setGroupId(groupId).setTopics(Collections.singletonList(new OffsetCommitRequestTopic().setName(topicName).setPartitions(Collections.singletonList(new OffsetCommitRequestPartition().setPartitionIndex(partition).setCommittedMetadata(metadata).setCommittedOffset(offset))))));    Supplier<OffsetCommitRequestData> request = () -> new OffsetCommitRequestData().setGroupId(groupId).setMemberId("memberId").setGroupInstanceId("instanceId").setTopics(Collections.singletonList(new OffsetCommitRequestTopic().setName(topicName).setPartitions(Collections.singletonList(new OffsetCommitRequestPartition().setPartitionIndex(partition).setCommittedLeaderEpoch(10).setCommittedMetadata(metadata).setCommittedOffset(offset).setCommitTimestamp(20))))).setRetentionTimeMs(20);    for (short version = 0; version <= ApiKeys.OFFSET_COMMIT.latestVersion(); version++) {        OffsetCommitRequestData requestData = request.get();        if (version < 1) {            requestData.setMemberId("");            requestData.setGenerationId(-1);        }        if (version != 1) {            requestData.topics().get(0).partitions().get(0).setCommitTimestamp(-1);        }        if (version < 2 || version > 4) {            requestData.setRetentionTimeMs(-1);        }        if (version < 6) {            requestData.topics().get(0).partitions().get(0).setCommittedLeaderEpoch(-1);        }        if (version < 7) {            requestData.setGroupInstanceId(null);        }        if (version == 1) {            testEquivalentMessageRoundTrip(version, requestData);        } else if (version >= 2 && version <= 4) {            testAllMessageRoundTripsBetweenVersions(version, (short) 4, requestData, requestData);        } else {            testAllMessageRoundTripsFromVersion(version, requestData);        }    }}
f7186
0
testOffsetCommitResponseVersions
public void kafkatest_f7187_0() throws Exception
{    Supplier<OffsetCommitResponseData> response = () -> new OffsetCommitResponseData().setTopics(singletonList(new OffsetCommitResponseTopic().setName("topic").setPartitions(singletonList(new OffsetCommitResponsePartition().setPartitionIndex(1).setErrorCode(Errors.UNKNOWN_MEMBER_ID.code()))))).setThrottleTimeMs(20);    for (short version = 0; version <= ApiKeys.OFFSET_COMMIT.latestVersion(); version++) {        OffsetCommitResponseData responseData = response.get();        if (version < 3) {            responseData.setThrottleTimeMs(0);        }        testAllMessageRoundTripsFromVersion(version, responseData);    }}
f7187
0
testMessageRoundTrip
private void kafkatest_f7195_0(short version, Message message, Message expected) throws Exception
{    testByteBufferRoundTrip(version, message, expected);    testStructRoundTrip(version, message, expected);}
f7195
0
testEquivalentMessageRoundTrip
private void kafkatest_f7196_0(short version, Message message) throws Exception
{    testStructRoundTrip(version, message, message);    testByteBufferRoundTrip(version, message, message);}
f7196
0
testByteBufferRoundTrip
private void kafkatest_f7197_0(short version, Message message, Message expected) throws Exception
{    int size = message.size(version);    ByteBuffer buf = ByteBuffer.allocate(size);    ByteBufferAccessor byteBufferAccessor = new ByteBufferAccessor(buf);    message.write(byteBufferAccessor, version);    assertEquals(size, buf.position());    Message message2 = message.getClass().newInstance();    buf.flip();    message2.read(byteBufferAccessor, version);    assertEquals(size, buf.position());    assertEquals(expected, message2);    assertEquals(expected.hashCode(), message2.hashCode());    assertEquals(expected.toString(), message2.toString());}
f7197
0
compareTypes
private static void kafkatest_f7205_0(NamedType typeA, NamedType typeB)
{    List<NamedType> listA = flatten(typeA);    List<NamedType> listB = flatten(typeB);    if (listA.size() != listB.size()) {        throw new RuntimeException("Can't match up structures: typeA has " + Utils.join(listA, ", ") + ", but typeB has " + Utils.join(listB, ", "));    }    for (int i = 0; i < listA.size(); i++) {        NamedType entryA = listA.get(i);        NamedType entryB = listB.get(i);        if (!entryA.hasSimilarType(entryB)) {            throw new RuntimeException("Type " + entryA + " in schema A " + "does not match type " + entryB + " in schema B.");        }        if (entryA.type.isNullable() != entryB.type.isNullable()) {            throw new RuntimeException(String.format("Type %s in Schema A is %s, but type %s in " + "Schema B is %s", entryA, entryA.type.isNullable() ? "nullable" : "non-nullable", entryB, entryB.type.isNullable() ? "nullable" : "non-nullable"));        }        if (entryA.type instanceof ArrayOf) {            compareTypes(new NamedType(entryA.name, ((ArrayOf) entryA.type).type()), new NamedType(entryB.name, ((ArrayOf) entryB.type).type()));        }    }}
f7205
0
flatten
private static List<NamedType> kafkatest_f7206_0(NamedType type)
{    if (!(type.type instanceof Schema)) {        return singletonList(type);    }    Schema schema = (Schema) type.type;    ArrayList<NamedType> results = new ArrayList<>();    for (BoundField field : schema.fields()) {        results.addAll(flatten(new NamedType(field.def.name, field.def.type)));    }    return results;}
f7206
0
testDefaultValues
public void kafkatest_f7207_0() throws Exception
{    verifySizeRaisesUve((short) 0, "validateOnly", new CreateTopicsRequestData().setValidateOnly(true));    verifySizeSucceeds((short) 0, new CreateTopicsRequestData().setValidateOnly(false));    verifySizeSucceeds((short) 0, new OffsetCommitRequestData().setRetentionTimeMs(123));    verifySizeRaisesUve((short) 5, "forgotten", new FetchRequestData().setForgotten(singletonList(new FetchRequestData.ForgottenTopic().setName("foo"))));}
f7207
0
shouldImplementJVMMethods
public void kafkatest_f7215_0()
{    final UUID uuid = UUID.randomUUID();    final TestUUIDData a = new TestUUIDData();    a.setProcessId(uuid);    final TestUUIDData b = new TestUUIDData();    b.setProcessId(uuid);    Assert.assertEquals(a, b);    Assert.assertEquals(a.hashCode(), b.hashCode());    // just tagging this on here    Assert.assertEquals(a.toString(), b.toString());}
f7215
0
testJmxRegistration
public void kafkatest_f7221_0() throws Exception
{    Metrics metrics = new Metrics();    MBeanServer server = ManagementFactory.getPlatformMBeanServer();    try {        JmxReporter reporter = new JmxReporter();        metrics.addReporter(reporter);        assertFalse(server.isRegistered(new ObjectName(":type=grp1")));        Sensor sensor = metrics.sensor("kafka.requests");        sensor.add(metrics.metricName("pack.bean1.avg", "grp1"), new Avg());        sensor.add(metrics.metricName("pack.bean2.total", "grp2"), new CumulativeSum());        assertTrue(server.isRegistered(new ObjectName(":type=grp1")));        assertEquals(Double.NaN, server.getAttribute(new ObjectName(":type=grp1"), "pack.bean1.avg"));        assertTrue(server.isRegistered(new ObjectName(":type=grp2")));        assertEquals(0.0, server.getAttribute(new ObjectName(":type=grp2"), "pack.bean2.total"));        MetricName metricName = metrics.metricName("pack.bean1.avg", "grp1");        String mBeanName = JmxReporter.getMBeanName("", metricName);        assertTrue(reporter.containsMbean(mBeanName));        metrics.removeMetric(metricName);        assertFalse(reporter.containsMbean(mBeanName));        assertFalse(server.isRegistered(new ObjectName(":type=grp1")));        assertTrue(server.isRegistered(new ObjectName(":type=grp2")));        assertEquals(0.0, server.getAttribute(new ObjectName(":type=grp2"), "pack.bean2.total"));        metricName = metrics.metricName("pack.bean2.total", "grp2");        metrics.removeMetric(metricName);        assertFalse(reporter.containsMbean(mBeanName));        assertFalse(server.isRegistered(new ObjectName(":type=grp1")));        assertFalse(server.isRegistered(new ObjectName(":type=grp2")));    } finally {        metrics.close();    }}
f7221
0
testJmxRegistrationSanitization
public void kafkatest_f7222_0() throws Exception
{    Metrics metrics = new Metrics();    MBeanServer server = ManagementFactory.getPlatformMBeanServer();    try {        metrics.addReporter(new JmxReporter());        Sensor sensor = metrics.sensor("kafka.requests");        sensor.add(metrics.metricName("name", "group", "desc", "id", "foo*"), new CumulativeSum());        sensor.add(metrics.metricName("name", "group", "desc", "id", "foo+"), new CumulativeSum());        sensor.add(metrics.metricName("name", "group", "desc", "id", "foo?"), new CumulativeSum());        sensor.add(metrics.metricName("name", "group", "desc", "id", "foo:"), new CumulativeSum());        sensor.add(metrics.metricName("name", "group", "desc", "id", "foo%"), new CumulativeSum());        assertTrue(server.isRegistered(new ObjectName(":type=group,id=\"foo\\*\"")));        assertEquals(0.0, server.getAttribute(new ObjectName(":type=group,id=\"foo\\*\""), "name"));        assertTrue(server.isRegistered(new ObjectName(":type=group,id=\"foo+\"")));        assertEquals(0.0, server.getAttribute(new ObjectName(":type=group,id=\"foo+\""), "name"));        assertTrue(server.isRegistered(new ObjectName(":type=group,id=\"foo\\?\"")));        assertEquals(0.0, server.getAttribute(new ObjectName(":type=group,id=\"foo\\?\""), "name"));        assertTrue(server.isRegistered(new ObjectName(":type=group,id=\"foo:\"")));        assertEquals(0.0, server.getAttribute(new ObjectName(":type=group,id=\"foo:\""), "name"));        assertTrue(server.isRegistered(new ObjectName(":type=group,id=foo%")));        assertEquals(0.0, server.getAttribute(new ObjectName(":type=group,id=foo%"), "name"));        metrics.removeMetric(metrics.metricName("name", "group", "desc", "id", "foo*"));        metrics.removeMetric(metrics.metricName("name", "group", "desc", "id", "foo+"));        metrics.removeMetric(metrics.metricName("name", "group", "desc", "id", "foo?"));        metrics.removeMetric(metrics.metricName("name", "group", "desc", "id", "foo:"));        metrics.removeMetric(metrics.metricName("name", "group", "desc", "id", "foo%"));        assertFalse(server.isRegistered(new ObjectName(":type=group,id=\"foo\\*\"")));        assertFalse(server.isRegistered(new ObjectName(":type=group,id=foo+")));        assertFalse(server.isRegistered(new ObjectName(":type=group,id=\"foo\\?\"")));        assertFalse(server.isRegistered(new ObjectName(":type=group,id=\"foo:\"")));        assertFalse(server.isRegistered(new ObjectName(":type=group,id=foo%")));    } finally {        metrics.close();    }}
f7222
0
testSetAttribute
public void kafkatest_f7230_0() throws Exception
{    try {        mBeanServer.setAttribute(objectName(countMetricName), new Attribute("anything", 1));        fail("setAttribute should have failed");    } catch (RuntimeMBeanException e) {        assertThat(e.getCause(), instanceOf(UnsupportedOperationException.class));    }}
f7230
0
testSetAttributes
public void kafkatest_f7231_0() throws Exception
{    try {        mBeanServer.setAttributes(objectName(countMetricName), new AttributeList(1));        fail("setAttributes should have failed");    } catch (Exception e) {        assertThat(e.getCause(), instanceOf(UnsupportedOperationException.class));    }}
f7231
0
objectName
private ObjectName kafkatest_f7232_0(MetricName metricName) throws Exception
{    return new ObjectName(JmxReporter.getMBeanName("", metricName));}
f7232
0
verifyStats
private void kafkatest_f7240_0(Function<KafkaMetric, Double> metricValueFunc)
{    ConstantMeasurable measurable = new ConstantMeasurable();    metrics.addMetric(metrics.metricName("direct.measurable", "grp1", "The fraction of time an appender waits for space allocation."), measurable);    Sensor s = metrics.sensor("test.sensor");    s.add(metrics.metricName("test.avg", "grp1"), new Avg());    s.add(metrics.metricName("test.max", "grp1"), new Max());    s.add(metrics.metricName("test.min", "grp1"), new Min());    s.add(new Meter(TimeUnit.SECONDS, metrics.metricName("test.rate", "grp1"), metrics.metricName("test.total", "grp1")));    s.add(new Meter(TimeUnit.SECONDS, new WindowedCount(), metrics.metricName("test.occurences", "grp1"), metrics.metricName("test.occurences.total", "grp1")));    s.add(metrics.metricName("test.count", "grp1"), new WindowedCount());    s.add(new Percentiles(100, -100, 100, BucketSizing.CONSTANT, new Percentile(metrics.metricName("test.median", "grp1"), 50.0), new Percentile(metrics.metricName("test.perc99_9", "grp1"), 99.9)));    Sensor s2 = metrics.sensor("test.sensor2");    s2.add(metrics.metricName("s2.total", "grp1"), new CumulativeSum());    s2.record(5.0);    int sum = 0;    int count = 10;    for (int i = 0; i < count; i++) {        s.record(i);        sum += i;    }    // prior to any time passing    double elapsedSecs = (config.timeWindowMs() * (config.samples() - 1)) / 1000.0;    assertEquals(String.format("Occurrences(0...%d) = %f", count, count / elapsedSecs), count / elapsedSecs, metricValueFunc.apply(metrics.metrics().get(metrics.metricName("test.occurences", "grp1"))), EPS);    // pretend 2 seconds passed...    long sleepTimeMs = 2;    time.sleep(sleepTimeMs * 1000);    elapsedSecs += sleepTimeMs;    assertEquals("s2 reflects the constant value", 5.0, metricValueFunc.apply(metrics.metrics().get(metrics.metricName("s2.total", "grp1"))), EPS);    assertEquals("Avg(0...9) = 4.5", 4.5, metricValueFunc.apply(metrics.metrics().get(metrics.metricName("test.avg", "grp1"))), EPS);    assertEquals("Max(0...9) = 9", count - 1, metricValueFunc.apply(metrics.metrics().get(metrics.metricName("test.max", "grp1"))), EPS);    assertEquals("Min(0...9) = 0", 0.0, metricValueFunc.apply(metrics.metrics().get(metrics.metricName("test.min", "grp1"))), EPS);    assertEquals("Rate(0...9) = 1.40625", sum / elapsedSecs, metricValueFunc.apply(metrics.metrics().get(metrics.metricName("test.rate", "grp1"))), EPS);    assertEquals(String.format("Occurrences(0...%d) = %f", count, count / elapsedSecs), count / elapsedSecs, metricValueFunc.apply(metrics.metrics().get(metrics.metricName("test.occurences", "grp1"))), EPS);    assertEquals("Count(0...9) = 10", (double) count, metricValueFunc.apply(metrics.metrics().get(metrics.metricName("test.count", "grp1"))), EPS);}
f7240
0
testHierarchicalSensors
public void kafkatest_f7241_0()
{    Sensor parent1 = metrics.sensor("test.parent1");    parent1.add(metrics.metricName("test.parent1.count", "grp1"), new WindowedCount());    Sensor parent2 = metrics.sensor("test.parent2");    parent2.add(metrics.metricName("test.parent2.count", "grp1"), new WindowedCount());    Sensor child1 = metrics.sensor("test.child1", parent1, parent2);    child1.add(metrics.metricName("test.child1.count", "grp1"), new WindowedCount());    Sensor child2 = metrics.sensor("test.child2", parent1);    child2.add(metrics.metricName("test.child2.count", "grp1"), new WindowedCount());    Sensor grandchild = metrics.sensor("test.grandchild", child1);    grandchild.add(metrics.metricName("test.grandchild.count", "grp1"), new WindowedCount());    /* increment each sensor one time */    parent1.record();    parent2.record();    child1.record();    child2.record();    grandchild.record();    double p1 = (double) parent1.metrics().get(0).metricValue();    double p2 = (double) parent2.metrics().get(0).metricValue();    double c1 = (double) child1.metrics().get(0).metricValue();    double c2 = (double) child2.metrics().get(0).metricValue();    double gc = (double) grandchild.metrics().get(0).metricValue();    /* each metric should have a count equal to one + its children's count */    assertEquals(1.0, gc, EPS);    assertEquals(1.0 + gc, c1, EPS);    assertEquals(1.0, c2, EPS);    assertEquals(1.0 + c1, p2, EPS);    assertEquals(1.0 + c1 + c2, p1, EPS);    assertEquals(Arrays.asList(child1, child2), metrics.childrenSensors().get(parent1));    assertEquals(Arrays.asList(child1), metrics.childrenSensors().get(parent2));    assertNull(metrics.childrenSensors().get(grandchild));}
f7241
0
testBadSensorHierarchy
public void kafkatest_f7242_0()
{    Sensor p = metrics.sensor("parent");    Sensor c1 = metrics.sensor("child1", p);    Sensor c2 = metrics.sensor("child2", p);    // should fail    metrics.sensor("gc", c1, c2);}
f7242
0
testSampledStatReturnsNaNWhenNoValuesExist
public void kafkatest_f7250_0()
{    // This is tested by having a SampledStat with expired Stats,    // because their values get reset to the initial values.    Max max = new Max();    Min min = new Min();    Avg avg = new Avg();    long windowMs = 100;    int samples = 2;    MetricConfig config = new MetricConfig().timeWindow(windowMs, TimeUnit.MILLISECONDS).samples(samples);    max.record(config, 50, time.milliseconds());    min.record(config, 50, time.milliseconds());    avg.record(config, 50, time.milliseconds());    time.sleep(samples * windowMs);    assertEquals(Double.NaN, max.measure(config, time.milliseconds()), EPS);    assertEquals(Double.NaN, min.measure(config, time.milliseconds()), EPS);    assertEquals(Double.NaN, avg.measure(config, time.milliseconds()), EPS);}
f7250
0
testSampledStatReturnsInitialValueWhenNoValuesExist
public void kafkatest_f7251_0()
{    WindowedCount count = new WindowedCount();    WindowedSum sampledTotal = new WindowedSum();    long windowMs = 100;    int samples = 2;    MetricConfig config = new MetricConfig().timeWindow(windowMs, TimeUnit.MILLISECONDS).samples(samples);    count.record(config, 50, time.milliseconds());    sampledTotal.record(config, 50, time.milliseconds());    time.sleep(samples * windowMs);    assertEquals(0, count.measure(config, time.milliseconds()), EPS);    assertEquals(0.0, sampledTotal.measure(config, time.milliseconds()), EPS);}
f7251
0
testDuplicateMetricName
public void kafkatest_f7252_0()
{    metrics.sensor("test").add(metrics.metricName("test", "grp1"), new Avg());    metrics.sensor("test2").add(metrics.metricName("test", "grp1"), new CumulativeSum());}
f7252
0
measure
private Double kafkatest_f7260_0(Measurable rate, MetricConfig config)
{    return rate.measure(config, time.milliseconds());}
f7260
0
testMetricInstances
public void kafkatest_f7261_0()
{    MetricName n1 = metrics.metricInstance(SampleMetrics.METRIC1, "key1", "value1", "key2", "value2");    Map<String, String> tags = new HashMap<String, String>();    tags.put("key1", "value1");    tags.put("key2", "value2");    MetricName n2 = metrics.metricInstance(SampleMetrics.METRIC2, tags);    assertEquals("metric names created in two different ways should be equal", n1, n2);    try {        metrics.metricInstance(SampleMetrics.METRIC1, "key1");        fail("Creating MetricName with an odd number of keyValue should fail");    } catch (IllegalArgumentException e) {    // this is expected    }    Map<String, String> parentTagsWithValues = new HashMap<>();    parentTagsWithValues.put("parent-tag", "parent-tag-value");    Map<String, String> childTagsWithValues = new HashMap<>();    childTagsWithValues.put("child-tag", "child-tag-value");    try (Metrics inherited = new Metrics(new MetricConfig().tags(parentTagsWithValues), Arrays.asList(new JmxReporter()), time, true)) {        MetricName inheritedMetric = inherited.metricInstance(SampleMetrics.METRIC_WITH_INHERITED_TAGS, childTagsWithValues);        Map<String, String> filledOutTags = inheritedMetric.tags();        assertEquals("parent-tag should be set properly", filledOutTags.get("parent-tag"), "parent-tag-value");        assertEquals("child-tag should be set properly", filledOutTags.get("child-tag"), "child-tag-value");        try {            inherited.metricInstance(SampleMetrics.METRIC_WITH_INHERITED_TAGS, parentTagsWithValues);            fail("Creating MetricName should fail if the child metrics are not defined at runtime");        } catch (IllegalArgumentException e) {        // this is expected        }        try {            Map<String, String> runtimeTags = new HashMap<>();            runtimeTags.put("child-tag", "child-tag-value");            runtimeTags.put("tag-not-in-template", "unexpected-value");            inherited.metricInstance(SampleMetrics.METRIC_WITH_INHERITED_TAGS, runtimeTags);            fail("Creating MetricName should fail if there is a tag at runtime that is not in the template");        } catch (IllegalArgumentException e) {        // this is expected        }    }}
f7261
0
testConcurrentReadUpdate
public void kafkatest_f7262_0() throws Exception
{    final Random random = new Random();    final Deque<Sensor> sensors = new ConcurrentLinkedDeque<>();    metrics = new Metrics(new MockTime(10));    SensorCreator sensorCreator = new SensorCreator(metrics);    final AtomicBoolean alive = new AtomicBoolean(true);    executorService = Executors.newSingleThreadExecutor();    executorService.submit(new ConcurrentMetricOperation(alive, "record", () -> sensors.forEach(sensor -> sensor.record(random.nextInt(10000)))));    for (int i = 0; i < 10000; i++) {        if (sensors.size() > 5) {            Sensor sensor = random.nextBoolean() ? sensors.removeFirst() : sensors.removeLast();            metrics.removeSensor(sensor.name());        }        StatType statType = StatType.forId(random.nextInt(StatType.values().length));        sensors.add(sensorCreator.createSensor(statType, i));        for (Sensor sensor : sensors) {            for (KafkaMetric metric : sensor.metrics()) {                assertNotNull("Invalid metric value", metric.metricValue());            }        }    }    alive.set(false);}
f7262
0
testDeprecatedMetricValueMethod
public void kafkatest_f7273_0()
{    verifyStats(KafkaMetric::value);}
f7273
0
testRecordLevelEnum
public void kafkatest_f7274_0()
{    Sensor.RecordingLevel configLevel = Sensor.RecordingLevel.INFO;    assertTrue(Sensor.RecordingLevel.INFO.shouldRecord(configLevel.id));    assertFalse(Sensor.RecordingLevel.DEBUG.shouldRecord(configLevel.id));    configLevel = Sensor.RecordingLevel.DEBUG;    assertTrue(Sensor.RecordingLevel.INFO.shouldRecord(configLevel.id));    assertTrue(Sensor.RecordingLevel.DEBUG.shouldRecord(configLevel.id));    assertEquals(Sensor.RecordingLevel.valueOf(Sensor.RecordingLevel.DEBUG.toString()), Sensor.RecordingLevel.DEBUG);    assertEquals(Sensor.RecordingLevel.valueOf(Sensor.RecordingLevel.INFO.toString()), Sensor.RecordingLevel.INFO);}
f7274
0
testShouldRecord
public void kafkatest_f7275_0()
{    MetricConfig debugConfig = new MetricConfig().recordLevel(Sensor.RecordingLevel.DEBUG);    MetricConfig infoConfig = new MetricConfig().recordLevel(Sensor.RecordingLevel.INFO);    Sensor infoSensor = new Sensor(null, "infoSensor", null, debugConfig, new SystemTime(), 0, Sensor.RecordingLevel.INFO);    assertTrue(infoSensor.shouldRecord());    infoSensor = new Sensor(null, "infoSensor", null, debugConfig, new SystemTime(), 0, Sensor.RecordingLevel.DEBUG);    assertTrue(infoSensor.shouldRecord());    Sensor debugSensor = new Sensor(null, "debugSensor", null, infoConfig, new SystemTime(), 0, Sensor.RecordingLevel.INFO);    assertTrue(debugSensor.shouldRecord());    debugSensor = new Sensor(null, "debugSensor", null, infoConfig, new SystemTime(), 0, Sensor.RecordingLevel.DEBUG);    assertFalse(debugSensor.shouldRecord());}
f7275
0
testFrequencyCenterValueBelowMin
public void kafkatest_f7283_0()
{    new Frequencies(4, 1.0, 4.0, freq("1", 1.0), freq("2", -20.0));}
f7283
0
testMoreFrequencyParametersThanBuckets
public void kafkatest_f7284_0()
{    new Frequencies(1, 1.0, 4.0, freq("1", 1.0), freq("2", -20.0));}
f7284
0
testBooleanFrequencies
public void kafkatest_f7285_0()
{    MetricName metricTrue = name("true");    MetricName metricFalse = name("false");    Frequencies frequencies = Frequencies.forBooleanValues(metricFalse, metricTrue);    final NamedMeasurable falseMetric = frequencies.stats().get(0);    final NamedMeasurable trueMetric = frequencies.stats().get(1);    // Record 2 windows worth of values    for (int i = 0; i != 25; ++i) {        frequencies.record(config, 0.0, time.milliseconds());    }    for (int i = 0; i != 75; ++i) {        frequencies.record(config, 1.0, time.milliseconds());    }    assertEquals(0.25, falseMetric.stat().measure(config, time.milliseconds()), DELTA);    assertEquals(0.75, trueMetric.stat().measure(config, time.milliseconds()), DELTA);    // Record 2 more windows worth of values    for (int i = 0; i != 40; ++i) {        frequencies.record(config, 0.0, time.milliseconds());    }    for (int i = 0; i != 60; ++i) {        frequencies.record(config, 1.0, time.milliseconds());    }    assertEquals(0.40, falseMetric.stat().measure(config, time.milliseconds()), DELTA);    assertEquals(0.60, trueMetric.stat().measure(config, time.milliseconds()), DELTA);}
f7285
0
checkBinningConsistency
private void kafkatest_f7293_0(BinScheme scheme)
{    for (int bin = 0; bin < scheme.bins(); bin++) {        double fromBin = scheme.fromBin(bin);        int binAgain = scheme.toBin(fromBin + EPS);        assertEquals("unbinning and rebinning the bin " + bin + " gave a different result (" + fromBin + " was placed in bin " + binAgain + " )", bin, binAgain);    }}
f7293
0
main
public static void kafkatest_f7294_0(String[] args)
{    Random random = new Random();    System.out.println("[-100, 100]:");    for (BinScheme scheme : Arrays.asList(new ConstantBinScheme(1000, -100, 100), new ConstantBinScheme(100, -100, 100), new ConstantBinScheme(10, -100, 100))) {        Histogram h = new Histogram(scheme);        for (int i = 0; i < 10000; i++) h.record(200.0 * random.nextDouble() - 100.0);        for (double quantile = 0.0; quantile < 1.0; quantile += 0.05) System.out.printf("%5.2f: %.1f, ", quantile, h.value(quantile));        System.out.println();    }    System.out.println("[0, 1000]");    for (BinScheme scheme : Arrays.asList(new LinearBinScheme(1000, 1000), new LinearBinScheme(100, 1000), new LinearBinScheme(10, 1000))) {        Histogram h = new Histogram(scheme);        for (int i = 0; i < 10000; i++) h.record(1000.0 * random.nextDouble());        for (double quantile = 0.0; quantile < 1.0; quantile += 0.05) System.out.printf("%5.2f: %.1f, ", quantile, h.value(quantile));        System.out.println();    }}
f7294
0
testMeter
public void kafkatest_f7295_0()
{    Map<String, String> emptyTags = Collections.emptyMap();    MetricName rateMetricName = new MetricName("rate", "test", "", emptyTags);    MetricName totalMetricName = new MetricName("total", "test", "", emptyTags);    Meter meter = new Meter(rateMetricName, totalMetricName);    List<NamedMeasurable> stats = meter.stats();    assertEquals(2, stats.size());    NamedMeasurable total = stats.get(0);    NamedMeasurable rate = stats.get(1);    assertEquals(rateMetricName, rate.name());    assertEquals(totalMetricName, total.name());    Rate rateStat = (Rate) rate.stat();    CumulativeSum totalStat = (CumulativeSum) total.stat();    MetricConfig config = new MetricConfig();    double nextValue = 0.0;    double expectedTotal = 0.0;    long now = 0;    double intervalMs = 100;    double delta = 5.0;    // for time windows and that the total is cumulative.    for (int i = 1; i <= 100; i++) {        for (; now < i * 1000; now += intervalMs, nextValue += delta) {            expectedTotal += nextValue;            meter.record(config, nextValue, now);        }        assertEquals(expectedTotal, totalStat.measure(config, now), EPS);        long windowSizeMs = rateStat.windowSize(config, now);        long windowStartMs = Math.max(now - windowSizeMs, 0);        double sampledTotal = 0.0;        double prevValue = nextValue - delta;        for (long timeMs = now - 100; timeMs >= windowStartMs; timeMs -= intervalMs, prevValue -= delta) sampledTotal += prevValue;        assertEquals(sampledTotal * 1000 / windowSizeMs, rateStat.measure(config, now), EPS);    }}
f7295
0
buildPrincipal
public Principal kafkatest_f7303_0(TransportLayer transportLayer, Authenticator authenticator) throws KafkaException
{    return new Principal() {        @Override        public String getName() {            return PRINCIPAL_NAME;        }    };}
f7303
0
getName
public String kafkatest_f7304_0()
{    return PRINCIPAL_NAME;}
f7304
0
configure
public void kafkatest_f7306_0(Map<String, ?> configs)
{    configured = true;}
f7306
0
createEchoServer
public static NioEchoServer kafkatest_f7314_0(ListenerName listenerName, SecurityProtocol securityProtocol, AbstractConfig serverConfig, CredentialCache credentialCache, int failedAuthenticationDelayMs, Time time) throws Exception
{    NioEchoServer server = new NioEchoServer(listenerName, securityProtocol, serverConfig, "localhost", null, credentialCache, failedAuthenticationDelayMs, time);    server.start();    return server;}
f7314
0
createEchoServer
public static NioEchoServer kafkatest_f7315_0(ListenerName listenerName, SecurityProtocol securityProtocol, AbstractConfig serverConfig, CredentialCache credentialCache, int failedAuthenticationDelayMs, Time time, DelegationTokenCache tokenCache) throws Exception
{    NioEchoServer server = new NioEchoServer(listenerName, securityProtocol, serverConfig, "localhost", null, credentialCache, failedAuthenticationDelayMs, time, tokenCache);    server.start();    return server;}
f7315
0
createSelector
public static Selector kafkatest_f7316_0(ChannelBuilder channelBuilder, Time time)
{    return new Selector(5000, new Metrics(), time, "MetricGroup", channelBuilder, new LogContext());}
f7316
0
credentialCache
public CredentialCache kafkatest_f7324_0()
{    return credentialCache;}
f7324
0
tokenCache
public DelegationTokenCache kafkatest_f7325_0()
{    return tokenCache;}
f7325
0
metricValue
public double kafkatest_f7326_0(String name)
{    for (Map.Entry<MetricName, KafkaMetric> entry : metrics.metrics().entrySet()) {        if (entry.getKey().name().equals(name))            return (double) entry.getValue().metricValue();    }    throw new IllegalStateException("Metric not found, " + name + ", found=" + metrics.metrics().keySet());}
f7326
0
maybeBeginServerReauthentication
private static boolean kafkatest_f7334_0(KafkaChannel channel, NetworkReceive networkReceive, Time time)
{    try {        if (TestUtils.apiKeyFrom(networkReceive) == ApiKeys.SASL_HANDSHAKE) {            return channel.maybeBeginServerReauthentication(networkReceive, () -> time.nanoseconds());        }    } catch (Exception e) {    // ignore    }    return false;}
f7334
0
id
private String kafkatest_f7335_0(SocketChannel channel)
{    return channel.socket().getLocalAddress().getHostAddress() + ":" + channel.socket().getLocalPort() + "-" + channel.socket().getInetAddress().getHostAddress() + ":" + channel.socket().getPort();}
f7335
0
channel
private KafkaChannel kafkatest_f7336_0(String id)
{    KafkaChannel channel = selector.channel(id);    return channel == null ? selector.closingChannel(id) : channel;}
f7336
0
testCloseBeforeConfigureIsIdempotent
public void kafkatest_f7344_0()
{    SaslChannelBuilder builder = createChannelBuilder(SecurityProtocol.SASL_PLAINTEXT);    builder.close();    assertTrue(builder.loginManagers().isEmpty());    builder.close();    assertTrue(builder.loginManagers().isEmpty());}
f7344
0
testCloseAfterConfigIsIdempotent
public void kafkatest_f7345_0()
{    SaslChannelBuilder builder = createChannelBuilder(SecurityProtocol.SASL_PLAINTEXT);    builder.configure(new HashMap<String, Object>());    assertNotNull(builder.loginManagers().get("PLAIN"));    builder.close();    assertTrue(builder.loginManagers().isEmpty());    builder.close();    assertTrue(builder.loginManagers().isEmpty());}
f7345
0
testLoginManagerReleasedIfConfigureThrowsException
public void kafkatest_f7346_0()
{    SaslChannelBuilder builder = createChannelBuilder(SecurityProtocol.SASL_SSL);    try {        // Use invalid config so that an exception is thrown        builder.configure(Collections.singletonMap(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, "1"));        fail("Exception should have been thrown");    } catch (KafkaException e) {        assertTrue(builder.loginManagers().isEmpty());    }    builder.close();    assertTrue(builder.loginManagers().isEmpty());}
f7346
0
testCantSendWithoutConnecting
public void kafkatest_f7354_0() throws Exception
{    selector.send(createSend("0", "test"));    selector.poll(1000L);}
f7354
0
testNoRouteToHost
public void kafkatest_f7355_0() throws Exception
{    selector.connect("0", new InetSocketAddress("some.invalid.hostname.foo.bar.local", server.port), BUFFER_SIZE, BUFFER_SIZE);}
f7355
0
testConnectionRefused
public void kafkatest_f7356_0() throws Exception
{    String node = "0";    ServerSocket nonListeningSocket = new ServerSocket(0);    int nonListeningPort = nonListeningSocket.getLocalPort();    selector.connect(node, new InetSocketAddress("localhost", nonListeningPort), BUFFER_SIZE, BUFFER_SIZE);    while (selector.disconnected().containsKey(node)) {        assertEquals(ChannelState.NOT_CONNECTED, selector.disconnected().get(node));        selector.poll(1000L);    }    nonListeningSocket.close();}
f7356
0
buildChannel
public KafkaChannel kafkatest_f7364_0(String id, SelectionKey key, int maxReceiveSize, MemoryPool memoryPool) throws KafkaException
{    throw new RuntimeException("Test exception");}
f7364
0
testCloseConnectionInClosingState
public void kafkatest_f7366_0() throws Exception
{    KafkaChannel channel = createConnectionWithStagedReceives(5);    String id = channel.id();    // Mute to allow channel to be expired even if more data is available for read    selector.mute(id);    // The max idle time is 5000ms    time.sleep(6000);    selector.poll(0);    assertNull("Channel not expired", selector.channel(id));    assertEquals(channel, selector.closingChannel(id));    assertEquals(ChannelState.EXPIRED, channel.state());    selector.close(id);    assertNull("Channel not removed from channels", selector.channel(id));    assertNull("Channel not removed from closingChannels", selector.closingChannel(id));    assertTrue("Unexpected disconnect notification", selector.disconnected().isEmpty());    assertEquals(ChannelState.EXPIRED, channel.state());    assertNull(channel.selectionKey().attachment());    selector.poll(0);    assertTrue("Unexpected disconnect notification", selector.disconnected().isEmpty());}
f7366
0
testCloseOldestConnection
public void kafkatest_f7367_0() throws Exception
{    String id = "0";    blockingConnect(id);    // The max idle time is 5000ms    time.sleep(6000);    selector.poll(0);    assertTrue("The idle connection should have been closed", selector.disconnected().containsKey(id));    assertEquals(ChannelState.EXPIRED, selector.disconnected().get(id));}
f7367
0
testCloseOldestConnectionWithOneStagedReceive
public void kafkatest_f7375_0() throws Exception
{    verifyCloseOldestConnectionWithStagedReceives(1);}
f7375
0
testCloseOldestConnectionWithMultipleStagedReceives
public void kafkatest_f7376_0() throws Exception
{    verifyCloseOldestConnectionWithStagedReceives(5);}
f7376
0
createConnectionWithStagedReceives
private KafkaChannel kafkatest_f7377_0(int maxStagedReceives) throws Exception
{    String id = "0";    blockingConnect(id);    KafkaChannel channel = selector.channel(id);    int retries = 100;    do {        selector.mute(id);        for (int i = 0; i <= maxStagedReceives; i++) {            selector.send(createSend(id, String.valueOf(i)));            do {                selector.poll(1000);            } while (selector.completedSends().isEmpty());        }        selector.unmute(id);        do {            selector.poll(1000);        } while (selector.completedReceives().isEmpty());    } while ((selector.numStagedReceives(channel) == 0 || channel.hasBytesBuffered()) && --retries > 0);    assertTrue("No staged receives after 100 attempts", selector.numStagedReceives(channel) > 0);    // We want to return without any bytes buffered to ensure that channel will be closed after idle time    assertFalse("Channel has bytes buffered", channel.hasBytesBuffered());    return channel;}
f7377
0
testLowestPriorityChannel
public void kafkatest_f7385_0() throws Exception
{    int conns = 5;    InetSocketAddress addr = new InetSocketAddress("localhost", server.port);    for (int i = 0; i < conns; i++) {        connect(String.valueOf(i), addr);    }    assertNotNull(selector.lowestPriorityChannel());    for (int i = conns - 1; i >= 0; i--) {        if (i != 2)            assertEquals("", blockingRequest(String.valueOf(i), ""));        time.sleep(10);    }    assertEquals("2", selector.lowestPriorityChannel().id());    Field field = Selector.class.getDeclaredField("closingChannels");    field.setAccessible(true);    Map<String, KafkaChannel> closingChannels = (Map<String, KafkaChannel>) field.get(selector);    closingChannels.put("3", selector.channel("3"));    assertEquals("3", selector.lowestPriorityChannel().id());    closingChannels.remove("3");    for (int i = 0; i < conns; i++) {        selector.close(String.valueOf(i));    }    assertNull(selector.lowestPriorityChannel());}
f7385
0
testMetricsCleanupOnSelectorClose
public void kafkatest_f7386_0() throws Exception
{    Metrics metrics = new Metrics();    Selector selector = new ImmediatelyConnectingSelector(5000, metrics, time, "MetricGroup", channelBuilder, new LogContext()) {        @Override        public void close(String id) {            throw new RuntimeException();        }    };    assertTrue(metrics.metrics().size() > 1);    String id = "0";    selector.connect(id, new InetSocketAddress("localhost", server.port), BUFFER_SIZE, BUFFER_SIZE);    // Close the selector and ensure a RuntimeException has been throw    assertThrows(RuntimeException.class, selector::close);    // We should only have one remaining metric for kafka-metrics-count, which is a global metric    assertEquals(1, metrics.metrics().size());}
f7386
0
close
public void kafkatest_f7387_0(String id)
{    throw new RuntimeException();}
f7387
0
verifyNonEmptyImmediatelyConnectedKeys
private void kafkatest_f7395_0(Selector selector) throws Exception
{    Field field = Selector.class.getDeclaredField("immediatelyConnectedKeys");    field.setAccessible(true);    Collection<?> immediatelyConnectedKeys = (Collection<?>) field.get(selector);    assertFalse(immediatelyConnectedKeys.isEmpty());}
f7395
0
verifyEmptyImmediatelyConnectedKeys
private void kafkatest_f7396_0(Selector selector) throws Exception
{    Field field = Selector.class.getDeclaredField("immediatelyConnectedKeys");    ensureEmptySelectorField(selector, field);}
f7396
0
verifySelectorEmpty
protected void kafkatest_f7397_0() throws Exception
{    verifySelectorEmpty(this.selector);}
f7397
0
testConnectionWithCustomKeyManager
public void kafkatest_f7405_0() throws Exception
{    TestProviderCreator testProviderCreator = new TestProviderCreator();    int requestSize = 100 * 1024;    final String node = "0";    String request = TestUtils.randomString(requestSize);    Map<String, Object> sslServerConfigs = TestSslUtils.createSslConfig(TestKeyManagerFactory.ALGORITHM, TestTrustManagerFactory.ALGORITHM);    sslServerConfigs.put(SecurityConfig.SECURITY_PROVIDERS_CONFIG, testProviderCreator.getClass().getName());    EchoServer server = new EchoServer(SecurityProtocol.SSL, sslServerConfigs);    server.start();    Time time = new MockTime();    File trustStoreFile = new File(TestKeyManagerFactory.TestKeyManager.mockTrustStoreFile);    Map<String, Object> sslClientConfigs = TestSslUtils.createSslConfig(true, true, Mode.CLIENT, trustStoreFile, "client");    ChannelBuilder channelBuilder = new TestSslChannelBuilder(Mode.CLIENT);    channelBuilder.configure(sslClientConfigs);    Metrics metrics = new Metrics();    Selector selector = new Selector(5000, metrics, time, "MetricGroup", channelBuilder, new LogContext());    selector.connect(node, new InetSocketAddress("localhost", server.port), BUFFER_SIZE, BUFFER_SIZE);    while (!selector.connected().contains(node)) selector.poll(10000L);    while (!selector.isChannelReady(node)) selector.poll(10000L);    selector.send(createSend(node, request));    waitForBytesBuffered(selector, node);    selector.close(node);    super.verifySelectorEmpty(selector);    Security.removeProvider(testProviderCreator.getProvider().getName());    selector.close();    server.close();    metrics.close();}
f7405
0
testDisconnectWithIntermediateBufferedBytes
public void kafkatest_f7406_0() throws Exception
{    int requestSize = 100 * 1024;    final String node = "0";    String request = TestUtils.randomString(requestSize);    this.selector.close();    this.channelBuilder = new TestSslChannelBuilder(Mode.CLIENT);    this.channelBuilder.configure(sslClientConfigs);    this.selector = new Selector(5000, metrics, time, "MetricGroup", channelBuilder, new LogContext());    connect(node, new InetSocketAddress("localhost", server.port));    selector.send(createSend(node, request));    waitForBytesBuffered(selector, node);    selector.close(node);    verifySelectorEmpty();}
f7406
0
waitForBytesBuffered
private void kafkatest_f7407_0(Selector selector, String node) throws Exception
{    TestUtils.waitForCondition(new TestCondition() {        @Override        public boolean conditionMet() {            try {                selector.poll(0L);                return selector.channel(node).hasBytesBuffered();            } catch (IOException e) {                throw new RuntimeException(e);            }        }    }, 2000L, "Failed to reach socket state with bytes buffered");}
f7407
0
connect
protected void kafkatest_f7415_0(String node, InetSocketAddress serverAddr) throws IOException
{    blockingConnect(node, serverAddr);}
f7415
0
createSender
private SslSender kafkatest_f7416_0(InetSocketAddress serverAddress, byte[] payload)
{    return new SslSender(serverAddress, payload);}
f7416
0
buildTransportLayer
protected SslTransportLayer kafkatest_f7417_0(SslFactory sslFactory, String id, SelectionKey key, String host) throws IOException
{    SocketChannel socketChannel = (SocketChannel) key.channel();    SSLEngine sslEngine = sslFactory.createSslEngine(host, socketChannel.socket().getPort());    TestSslTransportLayer transportLayer = new TestSslTransportLayer(id, key, sslEngine);    return transportLayer;}
f7417
0
setup
public void kafkatest_f7425_0() throws Exception
{    // Create certificates for use by client and server. Add server cert to client truststore and vice versa.    serverCertStores = new CertStores(true, "server", "localhost");    clientCertStores = new CertStores(false, "client", "localhost");    sslServerConfigs = serverCertStores.getTrustingConfig(clientCertStores);    sslClientConfigs = clientCertStores.getTrustingConfig(serverCertStores);    this.channelBuilder = new SslChannelBuilder(Mode.CLIENT, null, false);    this.channelBuilder.configure(sslClientConfigs);    this.selector = new Selector(5000, new Metrics(), time, "MetricGroup", channelBuilder, new LogContext());}
f7425
0
teardown
public void kafkatest_f7426_0() throws Exception
{    if (selector != null)        this.selector.close();    if (server != null)        this.server.close();}
f7426
0
testValidEndpointIdentificationSanDns
public void kafkatest_f7427_0() throws Exception
{    String node = "0";    server = createEchoServer(SecurityProtocol.SSL);    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, "HTTPS");    createSelector(sslClientConfigs);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);    server.verifyAuthenticationMetrics(1, 0);}
f7427
0
testClientAuthenticationRequiredValidProvided
public void kafkatest_f7435_0() throws Exception
{    String node = "0";    sslServerConfigs.put(BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG, "required");    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);}
f7435
0
testListenerConfigOverride
public void kafkatest_f7436_0() throws Exception
{    String node = "0";    ListenerName clientListenerName = new ListenerName("client");    sslServerConfigs.put(BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG, "required");    sslServerConfigs.put(clientListenerName.configPrefix() + BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG, "none");    // `client` listener is not configured at this point, so client auth should be required    server = createEchoServer(SecurityProtocol.SSL);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    // Connect with client auth should work fine    createSelector(sslClientConfigs);    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);    selector.close();    // Remove client auth, so connection should fail    sslClientConfigs.remove(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG);    sslClientConfigs.remove(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG);    sslClientConfigs.remove(SslConfigs.SSL_KEY_PASSWORD_CONFIG);    createSelector(sslClientConfigs);    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(selector, node, ChannelState.State.AUTHENTICATION_FAILED);    selector.close();    server.close();    // Listener-specific config should be used and client auth should be disabled    server = createEchoServer(clientListenerName, SecurityProtocol.SSL);    addr = new InetSocketAddress("localhost", server.port());    // Connect without client auth should work fine now    createSelector(sslClientConfigs);    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);}
f7436
0
testClientAuthenticationRequiredUntrustedProvided
public void kafkatest_f7437_0() throws Exception
{    String node = "0";    sslServerConfigs = serverCertStores.getUntrustingConfig();    sslServerConfigs.put(BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG, "required");    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(selector, node, ChannelState.State.AUTHENTICATION_FAILED);    server.verifyAuthenticationMetrics(0, 1);}
f7437
0
testInvalidKeystorePassword
public void kafkatest_f7445_0() throws Exception
{    try (SslChannelBuilder channelBuilder = new SslChannelBuilder(Mode.CLIENT, null, false)) {        sslClientConfigs.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "invalid");        channelBuilder.configure(sslClientConfigs);        fail("SSL channel configured with invalid keystore password");    } catch (KafkaException e) {    // Expected exception    }}
f7445
0
testNullTruststorePassword
public void kafkatest_f7446_0() throws Exception
{    String node = "0";    sslClientConfigs.remove(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);    sslServerConfigs.remove(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);}
f7446
0
testInvalidKeyPassword
public void kafkatest_f7447_0() throws Exception
{    String node = "0";    sslServerConfigs.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, new Password("invalid"));    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(selector, node, ChannelState.State.AUTHENTICATION_FAILED);    server.verifyAuthenticationMetrics(0, 1);}
f7447
0
testNetWriteBufferResize
public void kafkatest_f7455_0() throws Exception
{    String node = "0";    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs, null, 10, null);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 64000, 10);}
f7455
0
testApplicationBufferResize
public void kafkatest_f7456_0() throws Exception
{    String node = "0";    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs, null, null, 10);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 64000, 10);}
f7456
0
testNetworkThreadTimeRecorded
public void kafkatest_f7457_0() throws Exception
{    selector.close();    this.selector = new Selector(NetworkReceive.UNLIMITED, Selector.NO_IDLE_TIMEOUT_MS, new Metrics(), Time.SYSTEM, "MetricGroup", new HashMap<String, String>(), false, true, channelBuilder, MemoryPool.NONE, new LogContext());    String node = "0";    server = createEchoServer(SecurityProtocol.SSL);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    String message = TestUtils.randomString(1024 * 1024);    NetworkTestUtils.waitForChannelReady(selector, node);    final KafkaChannel channel = selector.channel(node);    assertTrue("SSL handshake time not recorded", channel.getAndResetNetworkThreadTimeNanos() > 0);    assertEquals("Time not reset", 0, channel.getAndResetNetworkThreadTimeNanos());    selector.mute(node);    selector.send(new NetworkSend(node, ByteBuffer.wrap(message.getBytes())));    while (selector.completedSends().isEmpty()) {        selector.poll(100L);    }    long sendTimeNanos = channel.getAndResetNetworkThreadTimeNanos();    assertTrue("Send time not recorded: " + sendTimeNanos, sendTimeNanos > 0);    assertEquals("Time not reset", 0, channel.getAndResetNetworkThreadTimeNanos());    assertFalse("Unexpected bytes buffered", channel.hasBytesBuffered());    assertEquals(0, selector.completedReceives().size());    selector.unmute(node);    // Wait for echo server to send the message back    TestUtils.waitForCondition(new TestCondition() {        @Override        public boolean conditionMet() {            try {                selector.poll(100L);                assertEquals(0, selector.numStagedReceives(channel));            } catch (IOException e) {                return false;            }            return !selector.completedReceives().isEmpty();        }    }, "Timed out waiting for a message to receive from echo server");    long receiveTimeNanos = channel.getAndResetNetworkThreadTimeNanos();    assertTrue("Receive time not recorded: " + receiveTimeNanos, receiveTimeNanos > 0);}
f7457
0
testIOExceptionsDuringHandshake
private void kafkatest_f7465_0(FailureAction readFailureAction, FailureAction flushFailureAction) throws Exception
{    TestSslChannelBuilder channelBuilder = new TestSslChannelBuilder(Mode.CLIENT);    boolean done = false;    for (int i = 1; i <= 100; i++) {        String node = String.valueOf(i);        channelBuilder.readFailureAction = readFailureAction;        channelBuilder.flushFailureAction = flushFailureAction;        channelBuilder.failureIndex = i;        channelBuilder.configure(sslClientConfigs);        this.selector = new Selector(5000, new Metrics(), time, "MetricGroup", channelBuilder, new LogContext());        InetSocketAddress addr = new InetSocketAddress("localhost", server.port());        selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);        for (int j = 0; j < 30; j++) {            selector.poll(1000L);            KafkaChannel channel = selector.channel(node);            if (channel != null && channel.ready()) {                done = true;                break;            }            if (selector.disconnected().containsKey(node)) {                ChannelState.State state = selector.disconnected().get(node).state();                assertTrue("Unexpected channel state " + state, state == ChannelState.State.AUTHENTICATE || state == ChannelState.State.READY);                break;            }        }        KafkaChannel channel = selector.channel(node);        if (channel != null)            assertTrue("Channel not ready or disconnected:" + channel.state().state(), channel.ready());    }    assertTrue("Too many invocations of read/write during SslTransportLayer.handshake()", done);}
f7465
0
testPeerNotifiedOfHandshakeFailure
public void kafkatest_f7466_0() throws Exception
{    sslServerConfigs = serverCertStores.getUntrustingConfig();    sslServerConfigs.put(BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG, "required");    // Test without delay and a couple of delay counts to ensure delay applies to handshake failure    for (int i = 0; i < 3; i++) {        String node = "0";        TestSslChannelBuilder serverChannelBuilder = new TestSslChannelBuilder(Mode.SERVER);        serverChannelBuilder.configure(sslServerConfigs);        serverChannelBuilder.flushDelayCount = i;        server = new NioEchoServer(ListenerName.forSecurityProtocol(SecurityProtocol.SSL), SecurityProtocol.SSL, new TestSecurityConfig(sslServerConfigs), "localhost", serverChannelBuilder, null, time);        server.start();        createSelector(sslClientConfigs);        InetSocketAddress addr = new InetSocketAddress("localhost", server.port());        selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);        NetworkTestUtils.waitForChannelClose(selector, node, ChannelState.State.AUTHENTICATION_FAILED);        server.close();        selector.close();    }}
f7466
0
testCloseSsl
public void kafkatest_f7467_0() throws Exception
{    testClose(SecurityProtocol.SSL, new SslChannelBuilder(Mode.CLIENT, null, false));}
f7467
0
verifyInvalidReconfigure
private void kafkatest_f7475_0(ListenerReconfigurable reconfigurable, Map<String, Object> invalidConfigs, String errorMessage)
{    try {        reconfigurable.validateReconfiguration(invalidConfigs);        fail("Should have failed validation with an exception: " + errorMessage);    } catch (KafkaException e) {    // expected exception    }    try {        reconfigurable.reconfigure(invalidConfigs);        fail("Should have failed to reconfigure: " + errorMessage);    } catch (KafkaException e) {    // expected exception    }}
f7475
0
createSelector
private Selector kafkatest_f7476_0(Map<String, Object> sslClientConfigs)
{    return createSelector(sslClientConfigs, null, null, null);}
f7476
0
createSelector
private Selector kafkatest_f7477_0(Map<String, Object> sslClientConfigs, final Integer netReadBufSize, final Integer netWriteBufSize, final Integer appBufSize)
{    TestSslChannelBuilder channelBuilder = new TestSslChannelBuilder(Mode.CLIENT);    channelBuilder.configureBufferSizes(netReadBufSize, netWriteBufSize, appBufSize);    this.channelBuilder = channelBuilder;    this.channelBuilder.configure(sslClientConfigs);    this.selector = new Selector(5000, new Metrics(), time, "MetricGroup", channelBuilder, new LogContext());    return selector;}
f7477
0
applicationBufferSize
protected int kafkatest_f7485_0()
{    return appBufSize.updateAndGet(super.applicationBufferSize(), true);}
f7485
0
readFromSocketChannel
protected int kafkatest_f7486_0() throws IOException
{    if (numReadsRemaining.decrementAndGet() == 0 && !ready())        readFailureAction.run();    return super.readFromSocketChannel();}
f7486
0
flush
protected boolean kafkatest_f7487_0(ByteBuffer buf) throws IOException
{    if (numFlushesRemaining.decrementAndGet() == 0 && !ready())        flushFailureAction.run();    else if (numDelayedFlushesRemaining.getAndDecrement() != 0)        return false;    resetDelayedFlush();    return super.flush(buf);}
f7487
0
testResponseThrottleTime
public void kafkatest_f7495_0()
{    List<ApiKeys> authenticationKeys = Arrays.asList(ApiKeys.SASL_HANDSHAKE, ApiKeys.SASL_AUTHENTICATE);    for (ApiKeys apiKey : ApiKeys.values()) {        Schema responseSchema = apiKey.responseSchema(apiKey.latestVersion());        BoundField throttleTimeField = responseSchema.get(CommonFields.THROTTLE_TIME_MS.name);        if (apiKey.clusterAction || authenticationKeys.contains(apiKey))            assertNull("Unexpected throttle time field: " + apiKey, throttleTimeField);        else            assertNotNull("Throttle time field missing: " + apiKey, throttleTimeField);    }}
f7495
0
testUniqueErrorCodes
public void kafkatest_f7496_0()
{    Set<Short> codeSet = new HashSet<>();    for (Errors error : Errors.values()) {        codeSet.add(error.code());    }    assertEquals("Error codes must be unique", codeSet.size(), Errors.values().length);}
f7496
0
testUniqueExceptions
public void kafkatest_f7497_0()
{    Set<Class> exceptionSet = new HashSet<>();    for (Errors error : Errors.values()) {        if (error != Errors.NONE)            exceptionSet.add(error.exception().getClass());    }    // Ignore NONE    assertEquals("Exceptions must be unique", exceptionSet.size(), Errors.values().length - 1);}
f7497
0
validateUtf8Length
private void kafkatest_f7505_0(String string)
{    byte[] arr = string.getBytes(StandardCharsets.UTF_8);    assertEquals(arr.length, MessageUtil.serializedUtf8Length(string));}
f7505
0
testDeepToString
public void kafkatest_f7506_0()
{    assertEquals("[1, 2, 3]", MessageUtil.deepToString(Arrays.asList(1, 2, 3).iterator()));    assertEquals("[foo]", MessageUtil.deepToString(Arrays.asList("foo").iterator()));}
f7506
0
testDelayedAllocationSchemaDetection
public void kafkatest_f7507_0() throws Exception
{    // verifies that schemas known to retain a reference to the underlying byte buffer are correctly detected.    for (ApiKeys key : ApiKeys.values()) {        if (key == ApiKeys.PRODUCE || key == ApiKeys.JOIN_GROUP || key == ApiKeys.SYNC_GROUP || key == ApiKeys.SASL_AUTHENTICATE || key == ApiKeys.EXPIRE_DELEGATION_TOKEN || key == ApiKeys.RENEW_DELEGATION_TOKEN) {            assertTrue(key + " should require delayed allocation", key.requiresDelayedAllocation);        } else {            assertFalse(key + " should not require delayed allocation", key.requiresDelayedAllocation);        }    }}
f7507
0
testReadNegativeArraySize
public void kafkatest_f7515_0()
{    Type type = new ArrayOf(Type.INT8);    int size = 10;    ByteBuffer invalidBuffer = ByteBuffer.allocate(4 + size);    invalidBuffer.putInt(-1);    for (int i = 0; i < size; i++) invalidBuffer.put((byte) i);    invalidBuffer.rewind();    try {        type.read(invalidBuffer);        fail("Array size not validated");    } catch (SchemaException e) {    // Expected exception    }}
f7515
0
testReadStringSizeTooLarge
public void kafkatest_f7516_0()
{    byte[] stringBytes = "foo".getBytes();    ByteBuffer invalidBuffer = ByteBuffer.allocate(2 + stringBytes.length);    invalidBuffer.putShort((short) (stringBytes.length * 5));    invalidBuffer.put(stringBytes);    invalidBuffer.rewind();    try {        Type.STRING.read(invalidBuffer);        fail("String size not validated");    } catch (SchemaException e) {    // Expected exception    }    invalidBuffer.rewind();    try {        Type.NULLABLE_STRING.read(invalidBuffer);        fail("String size not validated");    } catch (SchemaException e) {    // Expected exception    }}
f7516
0
testReadNegativeStringSize
public void kafkatest_f7517_0()
{    byte[] stringBytes = "foo".getBytes();    ByteBuffer invalidBuffer = ByteBuffer.allocate(2 + stringBytes.length);    invalidBuffer.putShort((short) -1);    invalidBuffer.put(stringBytes);    invalidBuffer.rewind();    try {        Type.STRING.read(invalidBuffer);        fail("String size not validated");    } catch (SchemaException e) {    // Expected exception    }}
f7517
0
testReadWhenOptionalDataMissingAtTheEndIsTolerated
public void kafkatest_f7525_0()
{    Schema oldSchema = new Schema(new Field("field1", Type.NULLABLE_STRING));    Schema newSchema = new Schema(true, new Field("field1", Type.NULLABLE_STRING), new Field("field2", Type.NULLABLE_STRING, "", true, "default"), new Field("field3", Type.NULLABLE_STRING, "", true, null), new Field("field4", Type.NULLABLE_BYTES, "", true, ByteBuffer.allocate(0)), new Field("field5", Type.INT64, "doc", true, Long.MAX_VALUE));    String value = "foo bar baz";    Struct oldFormat = new Struct(oldSchema).set("field1", value);    ByteBuffer buffer = ByteBuffer.allocate(oldSchema.sizeOf(oldFormat));    oldFormat.writeTo(buffer);    buffer.flip();    Struct newFormat = newSchema.read(buffer);    assertEquals(value, newFormat.get("field1"));    assertEquals("default", newFormat.get("field2"));    assertEquals(null, newFormat.get("field3"));    assertEquals(ByteBuffer.allocate(0), newFormat.get("field4"));    assertEquals(Long.MAX_VALUE, newFormat.get("field5"));}
f7525
0
testReadWhenOptionalDataMissingAtTheEndIsNotTolerated
public void kafkatest_f7526_0()
{    Schema oldSchema = new Schema(new Field("field1", Type.NULLABLE_STRING));    Schema newSchema = new Schema(new Field("field1", Type.NULLABLE_STRING), new Field("field2", Type.NULLABLE_STRING, "", true, "default"));    String value = "foo bar baz";    Struct oldFormat = new Struct(oldSchema).set("field1", value);    ByteBuffer buffer = ByteBuffer.allocate(oldSchema.sizeOf(oldFormat));    oldFormat.writeTo(buffer);    buffer.flip();    SchemaException e = assertThrows(SchemaException.class, () -> newSchema.read(buffer));    e.getMessage().contains("Error reading field 'field2': java.nio.BufferUnderflowException");}
f7526
0
testReadWithMissingNonOptionalExtraDataAtTheEnd
public void kafkatest_f7527_0()
{    Schema oldSchema = new Schema(new Field("field1", Type.NULLABLE_STRING));    Schema newSchema = new Schema(true, new Field("field1", Type.NULLABLE_STRING), new Field("field2", Type.NULLABLE_STRING));    String value = "foo bar baz";    Struct oldFormat = new Struct(oldSchema).set("field1", value);    ByteBuffer buffer = ByteBuffer.allocate(oldSchema.sizeOf(oldFormat));    oldFormat.writeTo(buffer);    buffer.flip();    SchemaException e = assertThrows(SchemaException.class, () -> newSchema.read(buffer));    e.getMessage().contains("Missing value for field 'field2' which has no default value");}
f7527
0
testSetPartitionLeaderEpochNotAllowedV0
public void kafkatest_f7535_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V0, 0L, CompressionType.GZIP, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    ByteBufferLegacyRecordBatch batch = new ByteBufferLegacyRecordBatch(records.buffer());    batch.setPartitionLeaderEpoch(15);}
f7535
0
testSetPartitionLeaderEpochNotAllowedV1
public void kafkatest_f7536_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V1, 0L, CompressionType.GZIP, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    ByteBufferLegacyRecordBatch batch = new ByteBufferLegacyRecordBatch(records.buffer());    batch.setPartitionLeaderEpoch(15);}
f7536
0
testSetLogAppendTimeV1
public void kafkatest_f7537_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V1, 0L, CompressionType.GZIP, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    long logAppendTime = 15L;    ByteBufferLegacyRecordBatch batch = new ByteBufferLegacyRecordBatch(records.buffer());    batch.setMaxTimestamp(TimestampType.LOG_APPEND_TIME, logAppendTime);    assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());    assertEquals(logAppendTime, batch.maxTimestamp());    assertTrue(batch.isValid());    List<MutableRecordBatch> recordBatches = Utils.toList(records.batches().iterator());    assertEquals(1, recordBatches.size());    assertEquals(TimestampType.LOG_APPEND_TIME, recordBatches.get(0).timestampType());    assertEquals(logAppendTime, recordBatches.get(0).maxTimestamp());    for (Record record : records.records()) assertEquals(logAppendTime, record.timestamp());}
f7537
0
testLZ4FramingMagicV0
public void kafkatest_f7545_0()
{    ByteBuffer buffer = ByteBuffer.allocate(256);    KafkaLZ4BlockOutputStream out = (KafkaLZ4BlockOutputStream) CompressionType.LZ4.wrapForOutput(new ByteBufferOutputStream(buffer), RecordBatch.MAGIC_VALUE_V0);    assertTrue(out.useBrokenFlagDescriptorChecksum());    buffer.rewind();    KafkaLZ4BlockInputStream in = (KafkaLZ4BlockInputStream) CompressionType.LZ4.wrapForInput(buffer, RecordBatch.MAGIC_VALUE_V0, BufferSupplier.NO_CACHING);    assertTrue(in.ignoreFlagDescriptorChecksum());}
f7545
0
testLZ4FramingMagicV1
public void kafkatest_f7546_0()
{    ByteBuffer buffer = ByteBuffer.allocate(256);    KafkaLZ4BlockOutputStream out = (KafkaLZ4BlockOutputStream) CompressionType.LZ4.wrapForOutput(new ByteBufferOutputStream(buffer), RecordBatch.MAGIC_VALUE_V1);    assertFalse(out.useBrokenFlagDescriptorChecksum());    buffer.rewind();    KafkaLZ4BlockInputStream in = (KafkaLZ4BlockInputStream) CompressionType.LZ4.wrapForInput(buffer, RecordBatch.MAGIC_VALUE_V1, BufferSupplier.create());    assertFalse(in.ignoreFlagDescriptorChecksum());}
f7546
0
testParseUnknownType
public void kafkatest_f7547_0()
{    ByteBuffer buffer = ByteBuffer.allocate(32);    buffer.putShort(ControlRecordType.CURRENT_CONTROL_RECORD_KEY_VERSION);    buffer.putShort((short) 337);    buffer.flip();    ControlRecordType type = ControlRecordType.parse(buffer);    assertEquals(ControlRecordType.UNKNOWN, type);}
f7547
0
testInvalidRecordCountTooManyNonCompressedV2
public void kafkatest_f7555_0()
{    long now = System.currentTimeMillis();    DefaultRecordBatch batch = recordsWithInvalidRecordCount(RecordBatch.MAGIC_VALUE_V2, now, CompressionType.NONE, 5);    // batch validation is a part of normal workflow for LogValidator.validateMessagesAndAssignOffsets    for (Record record : batch) {        record.isValid();    }}
f7555
0
testInvalidRecordCountTooLittleNonCompressedV2
public void kafkatest_f7556_0()
{    long now = System.currentTimeMillis();    DefaultRecordBatch batch = recordsWithInvalidRecordCount(RecordBatch.MAGIC_VALUE_V2, now, CompressionType.NONE, 2);    // batch validation is a part of normal workflow for LogValidator.validateMessagesAndAssignOffsets    for (Record record : batch) {        record.isValid();    }}
f7556
0
testInvalidRecordCountTooManyCompressedV2
public void kafkatest_f7557_0()
{    long now = System.currentTimeMillis();    DefaultRecordBatch batch = recordsWithInvalidRecordCount(RecordBatch.MAGIC_VALUE_V2, now, CompressionType.GZIP, 5);    // batch validation is a part of normal workflow for LogValidator.validateMessagesAndAssignOffsets    for (Record record : batch) {        record.isValid();    }}
f7557
0
testStreamingIteratorConsistency
public void kafkatest_f7565_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V2, 0L, CompressionType.GZIP, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    DefaultRecordBatch batch = new DefaultRecordBatch(records.buffer());    try (CloseableIterator<Record> streamingIterator = batch.streamingIterator(BufferSupplier.create())) {        TestUtils.checkEquals(streamingIterator, batch.iterator());    }}
f7565
0
testSkipKeyValueIteratorCorrectness
public void kafkatest_f7566_0()
{    Header[] headers = { new RecordHeader("k1", "v1".getBytes()), new RecordHeader("k2", "v2".getBytes()) };    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V2, 0L, CompressionType.LZ4, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()), new SimpleRecord(1000L, "abc".getBytes(), "0".getBytes()), new SimpleRecord(9999L, "abc".getBytes(), "0".getBytes(), headers));    DefaultRecordBatch batch = new DefaultRecordBatch(records.buffer());    try (CloseableIterator<Record> streamingIterator = batch.skipKeyValueIterator(BufferSupplier.NO_CACHING)) {        assertEquals(Arrays.asList(new PartialDefaultRecord(9, (byte) 0, 0L, 1L, -1, 1, 1), new PartialDefaultRecord(9, (byte) 0, 1L, 2L, -1, 1, 1), new PartialDefaultRecord(9, (byte) 0, 2L, 3L, -1, 1, 1), new PartialDefaultRecord(12, (byte) 0, 3L, 1000L, -1, 3, 1), new PartialDefaultRecord(25, (byte) 0, 4L, 9999L, -1, 3, 1)), Utils.toList(streamingIterator));    }}
f7566
0
testIncrementSequence
public void kafkatest_f7567_0()
{    assertEquals(10, DefaultRecordBatch.incrementSequence(5, 5));    assertEquals(0, DefaultRecordBatch.incrementSequence(Integer.MAX_VALUE, 1));    assertEquals(4, DefaultRecordBatch.incrementSequence(Integer.MAX_VALUE - 5, 10));}
f7567
0
testInvalidValueSize
public void kafkatest_f7575_0() throws IOException
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    // use a value size larger than the full message    int valueSize = 105;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    // null key    ByteUtils.writeVarint(-1, buf);    ByteUtils.writeVarint(valueSize, buf);    buf.position(buf.limit());    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7575
0
testInvalidValueSizePartial
public void kafkatest_f7576_0() throws IOException
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    // use a value size larger than the full message    int valueSize = 105;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    // null key    ByteUtils.writeVarint(-1, buf);    ByteUtils.writeVarint(valueSize, buf);    buf.position(buf.limit());    buf.flip();    DataInputStream inputStream = new DataInputStream(new ByteBufferInputStream(buf));    DefaultRecord.readPartiallyFrom(inputStream, skipArray, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7576
0
testInvalidNumHeaders
public void kafkatest_f7577_0() throws IOException
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    // null key    ByteUtils.writeVarint(-1, buf);    // null value    ByteUtils.writeVarint(-1, buf);    // -1 num.headers, not allowed    ByteUtils.writeVarint(-1, buf);    buf.position(buf.limit());    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7577
0
testUnderflowReadingTimestamp
public void kafkatest_f7585_0()
{    byte attributes = 0;    int sizeOfBodyInBytes = 1;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7585
0
testUnderflowReadingVarlong
public void kafkatest_f7586_0()
{    byte attributes = 0;    // one byte for attributes, one byte for partial timestamp    int sizeOfBodyInBytes = 2;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes) + 1);    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    // needs 2 bytes to represent    ByteUtils.writeVarlong(156, buf);    buf.position(buf.limit() - 1);    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7586
0
testInvalidVarlong
public void kafkatest_f7587_0()
{    byte attributes = 0;    // one byte for attributes, 10 bytes for max timestamp    int sizeOfBodyInBytes = 11;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes) + 1);    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    int recordStartPosition = buf.position();    buf.put(attributes);    // takes 10 bytes    ByteUtils.writeVarlong(Long.MAX_VALUE, buf);    // use an invalid final byte    buf.put(recordStartPosition + 10, Byte.MIN_VALUE);    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7587
0
testWriteTo
public void kafkatest_f7595_0() throws IOException
{    if (compression == CompressionType.ZSTD && magic < MAGIC_VALUE_V2)        return;    try (FileRecords fileRecords = FileRecords.open(tempFile())) {        fileRecords.append(MemoryRecords.withRecords(magic, compression, new SimpleRecord("foo".getBytes())));        fileRecords.flush();        FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());        FileChannelRecordBatch batch = logInputStream.nextBatch();        assertNotNull(batch);        assertEquals(magic, batch.magic());        ByteBuffer buffer = ByteBuffer.allocate(128);        batch.writeTo(buffer);        buffer.flip();        MemoryRecords memRecords = MemoryRecords.readableRecords(buffer);        List<Record> records = Utils.toList(memRecords.records().iterator());        assertEquals(1, records.size());        Record record0 = records.get(0);        assertTrue(record0.hasMagic(magic));        assertEquals("foo", Utils.utf8(record0.value(), record0.valueSize()));    }}
f7595
0
testSimpleBatchIteration
public void kafkatest_f7596_0() throws IOException
{    if (compression == CompressionType.ZSTD && magic < MAGIC_VALUE_V2)        return;    try (FileRecords fileRecords = FileRecords.open(tempFile())) {        SimpleRecord firstBatchRecord = new SimpleRecord(3241324L, "a".getBytes(), "foo".getBytes());        SimpleRecord secondBatchRecord = new SimpleRecord(234280L, "b".getBytes(), "bar".getBytes());        fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));        fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));        fileRecords.flush();        FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());        FileChannelRecordBatch firstBatch = logInputStream.nextBatch();        assertGenericRecordBatchData(firstBatch, 0L, 3241324L, firstBatchRecord);        assertNoProducerData(firstBatch);        FileChannelRecordBatch secondBatch = logInputStream.nextBatch();        assertGenericRecordBatchData(secondBatch, 1L, 234280L, secondBatchRecord);        assertNoProducerData(secondBatch);        assertNull(logInputStream.nextBatch());    }}
f7596
0
testBatchIterationWithMultipleRecordsPerBatch
public void kafkatest_f7597_0() throws IOException
{    if (magic < MAGIC_VALUE_V2 && compression == CompressionType.NONE)        return;    if (compression == CompressionType.ZSTD && magic < MAGIC_VALUE_V2)        return;    try (FileRecords fileRecords = FileRecords.open(tempFile())) {        SimpleRecord[] firstBatchRecords = new SimpleRecord[] { new SimpleRecord(3241324L, "a".getBytes(), "1".getBytes()), new SimpleRecord(234280L, "b".getBytes(), "2".getBytes()) };        SimpleRecord[] secondBatchRecords = new SimpleRecord[] { new SimpleRecord(238423489L, "c".getBytes(), "3".getBytes()), new SimpleRecord(897839L, null, "4".getBytes()), new SimpleRecord(8234020L, "e".getBytes(), null) };        fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecords));        fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecords));        fileRecords.flush();        FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());        FileChannelRecordBatch firstBatch = logInputStream.nextBatch();        assertNoProducerData(firstBatch);        assertGenericRecordBatchData(firstBatch, 0L, 3241324L, firstBatchRecords);        FileChannelRecordBatch secondBatch = logInputStream.nextBatch();        assertNoProducerData(secondBatch);        assertGenericRecordBatchData(secondBatch, 1L, 238423489L, secondBatchRecords);        assertNull(logInputStream.nextBatch());    }}
f7597
0
data
public static Collection<Object[]> kafkatest_f7605_0()
{    List<Object[]> values = new ArrayList<>();    for (byte magic : asList(MAGIC_VALUE_V0, MAGIC_VALUE_V1, MAGIC_VALUE_V2)) for (CompressionType type : CompressionType.values()) values.add(new Object[] { magic, type });    return values;}
f7605
0
setup
public void kafkatest_f7606_0() throws IOException
{    this.fileRecords = createFileRecords(values);    this.time = new MockTime();}
f7606
0
testAppendProtectsFromOverflow
public void kafkatest_f7607_0() throws Exception
{    File fileMock = mock(File.class);    FileChannel fileChannelMock = mock(FileChannel.class);    when(fileChannelMock.size()).thenReturn((long) Integer.MAX_VALUE);    FileRecords records = new FileRecords(fileMock, fileChannelMock, 0, Integer.MAX_VALUE, false);    append(records, values);}
f7607
0
testSearch
public void kafkatest_f7615_0() throws IOException
{    // append a new message with a high offset    SimpleRecord lastMessage = new SimpleRecord("test".getBytes());    fileRecords.append(MemoryRecords.withRecords(50L, CompressionType.NONE, lastMessage));    List<RecordBatch> batches = batches(fileRecords);    int position = 0;    int message1Size = batches.get(0).sizeInBytes();    assertEquals("Should be able to find the first message by its offset", new FileRecords.LogOffsetPosition(0L, position, message1Size), fileRecords.searchForOffsetWithSize(0, 0));    position += message1Size;    int message2Size = batches.get(1).sizeInBytes();    assertEquals("Should be able to find second message when starting from 0", new FileRecords.LogOffsetPosition(1L, position, message2Size), fileRecords.searchForOffsetWithSize(1, 0));    assertEquals("Should be able to find second message starting from its offset", new FileRecords.LogOffsetPosition(1L, position, message2Size), fileRecords.searchForOffsetWithSize(1, position));    position += message2Size + batches.get(2).sizeInBytes();    int message4Size = batches.get(3).sizeInBytes();    assertEquals("Should be able to find fourth message from a non-existent offset", new FileRecords.LogOffsetPosition(50L, position, message4Size), fileRecords.searchForOffsetWithSize(3, position));    assertEquals("Should be able to find fourth message by correct offset", new FileRecords.LogOffsetPosition(50L, position, message4Size), fileRecords.searchForOffsetWithSize(50, position));}
f7615
0
testIteratorWithLimits
public void kafkatest_f7616_0() throws IOException
{    RecordBatch batch = batches(fileRecords).get(1);    int start = fileRecords.searchForOffsetWithSize(1, 0).position;    int size = batch.sizeInBytes();    FileRecords slice = fileRecords.slice(start, size);    assertEquals(Collections.singletonList(batch), batches(slice));    FileRecords slice2 = fileRecords.slice(start, size - 1);    assertEquals(Collections.emptyList(), batches(slice2));}
f7616
0
testTruncate
public void kafkatest_f7617_0() throws IOException
{    RecordBatch batch = batches(fileRecords).get(0);    int end = fileRecords.searchForOffsetWithSize(1, 0).position;    fileRecords.truncateTo(end);    assertEquals(Collections.singletonList(batch), batches(fileRecords));    assertEquals(batch.sizeInBytes(), fileRecords.sizeInBytes());}
f7617
0
testSearchForTimestamp
public void kafkatest_f7625_0() throws IOException
{    for (RecordVersion version : RecordVersion.values()) {        testSearchForTimestamp(version);    }}
f7625
0
testSearchForTimestamp
private void kafkatest_f7626_0(RecordVersion version) throws IOException
{    File temp = tempFile();    FileRecords fileRecords = FileRecords.open(temp, false, 1024 * 1024, true);    appendWithOffsetAndTimestamp(fileRecords, version, 10L, 5, 0);    appendWithOffsetAndTimestamp(fileRecords, version, 11L, 6, 1);    assertFoundTimestamp(new FileRecords.TimestampAndOffset(10L, 5, Optional.of(0)), fileRecords.searchForTimestamp(9L, 0, 0L), version);    assertFoundTimestamp(new FileRecords.TimestampAndOffset(10L, 5, Optional.of(0)), fileRecords.searchForTimestamp(10L, 0, 0L), version);    assertFoundTimestamp(new FileRecords.TimestampAndOffset(11L, 6, Optional.of(1)), fileRecords.searchForTimestamp(11L, 0, 0L), version);    assertNull(fileRecords.searchForTimestamp(12L, 0, 0L));}
f7626
0
assertFoundTimestamp
private void kafkatest_f7627_0(FileRecords.TimestampAndOffset expected, FileRecords.TimestampAndOffset actual, RecordVersion version)
{    if (version == RecordVersion.V0) {        assertNull("Expected no match for message format v0", actual);    } else {        assertNotNull("Expected to find timestamp for message format " + version, actual);        assertEquals("Expected matching timestamps for message format" + version, expected.timestamp, actual.timestamp);        assertEquals("Expected matching offsets for message format " + version, expected.offset, actual.offset);        Optional<Integer> expectedLeaderEpoch = version.value >= RecordVersion.V2.value ? expected.leaderEpoch : Optional.empty();        assertEquals("Non-matching leader epoch for version " + version, expectedLeaderEpoch, actual.leaderEpoch);    }}
f7627
0
createFileRecords
private FileRecords kafkatest_f7635_0(byte[][] values) throws IOException
{    FileRecords fileRecords = FileRecords.open(tempFile());    append(fileRecords, values);    return fileRecords;}
f7635
0
append
private void kafkatest_f7636_0(FileRecords fileRecords, byte[][] values) throws IOException
{    long offset = 0L;    for (byte[] value : values) {        ByteBuffer buffer = ByteBuffer.allocate(128);        MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.CREATE_TIME, offset);        builder.appendWithOffset(offset++, System.currentTimeMillis(), null, value);        fileRecords.append(builder.build());    }    fileRecords.flush();}
f7636
0
toString
public String kafkatest_f7637_0()
{    return "Payload{" + "size=" + payload.length + ", name='" + name + '\'' + '}';}
f7637
0
testArrayBackedBuffer
public void kafkatest_f7645_0() throws IOException
{    byte[] compressed = compressedBytes();    testDecompression(ByteBuffer.wrap(compressed));}
f7645
0
testArrayBackedBufferSlice
public void kafkatest_f7646_0() throws IOException
{    byte[] compressed = compressedBytes();    int sliceOffset = 12;    ByteBuffer buffer = ByteBuffer.allocate(compressed.length + sliceOffset + 123);    buffer.position(sliceOffset);    buffer.put(compressed).flip();    buffer.position(sliceOffset);    ByteBuffer slice = buffer.slice();    testDecompression(slice);    int offset = 42;    buffer = ByteBuffer.allocate(compressed.length + sliceOffset + offset);    buffer.position(sliceOffset + offset);    buffer.put(compressed).flip();    buffer.position(sliceOffset);    slice = buffer.slice();    slice.position(offset);    testDecompression(slice);}
f7646
0
testDirectBuffer
public void kafkatest_f7647_0() throws IOException
{    byte[] compressed = compressedBytes();    ByteBuffer buffer;    buffer = ByteBuffer.allocateDirect(compressed.length);    buffer.put(compressed).flip();    testDecompression(buffer);    int offset = 42;    buffer = ByteBuffer.allocateDirect(compressed.length + offset + 123);    buffer.position(offset);    buffer.put(compressed).flip();    buffer.position(offset);    testDecompression(buffer);}
f7647
0
doTestConversion
private void kafkatest_f7655_0(boolean testConversionOverflow) throws IOException
{    List<Long> offsets = asList(0L, 2L, 3L, 9L, 11L, 15L, 16L, 17L, 22L, 24L);    Header[] headers = { new RecordHeader("headerKey1", "headerValue1".getBytes()), new RecordHeader("headerKey2", "headerValue2".getBytes()), new RecordHeader("headerKey3", "headerValue3".getBytes()) };    List<SimpleRecord> records = asList(new SimpleRecord(1L, "k1".getBytes(), "hello".getBytes()), new SimpleRecord(2L, "k2".getBytes(), "goodbye".getBytes()), new SimpleRecord(3L, "k3".getBytes(), "hello again".getBytes()), new SimpleRecord(4L, "k4".getBytes(), "goodbye for now".getBytes()), new SimpleRecord(5L, "k5".getBytes(), "hello again".getBytes()), new SimpleRecord(6L, "k6".getBytes(), "I sense indecision".getBytes()), new SimpleRecord(7L, "k7".getBytes(), "what now".getBytes()), new SimpleRecord(8L, "k8".getBytes(), "running out".getBytes(), headers), new SimpleRecord(9L, "k9".getBytes(), "ok, almost done".getBytes()), new SimpleRecord(10L, "k10".getBytes(), "finally".getBytes(), headers));    assertEquals("incorrect test setup", offsets.size(), records.size());    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, 0L);    for (int i = 0; i < 3; i++) builder.appendWithOffset(offsets.get(i), records.get(i));    builder.close();    builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, 0L);    for (int i = 3; i < 6; i++) builder.appendWithOffset(offsets.get(i), records.get(i));    builder.close();    builder = MemoryRecords.builder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, 0L);    for (int i = 6; i < 10; i++) builder.appendWithOffset(offsets.get(i), records.get(i));    builder.close();    buffer.flip();    MemoryRecords recordsToConvert = MemoryRecords.readableRecords(buffer);    int numBytesToConvert = recordsToConvert.sizeInBytes();    if (testConversionOverflow)        numBytesToConvert *= 2;    MemoryRecords convertedRecords = convertRecords(recordsToConvert, toMagic, numBytesToConvert);    verifyDownConvertedRecords(records, offsets, convertedRecords, compressionType, toMagic);}
f7655
0
convertRecords
private static MemoryRecords kafkatest_f7656_0(MemoryRecords recordsToConvert, byte toMagic, int bytesToConvert) throws IOException
{    try (FileRecords inputRecords = FileRecords.open(tempFile())) {        inputRecords.append(recordsToConvert);        inputRecords.flush();        LazyDownConversionRecords lazyRecords = new LazyDownConversionRecords(new TopicPartition("test", 1), inputRecords, toMagic, 0L, Time.SYSTEM);        LazyDownConversionRecordsSend lazySend = lazyRecords.toSend("foo");        File outputFile = tempFile();        FileChannel channel = FileChannel.open(outputFile.toPath(), StandardOpenOption.READ, StandardOpenOption.WRITE);        int written = 0;        while (written < bytesToConvert) written += lazySend.writeTo(channel, written, bytesToConvert - written);        FileRecords convertedRecords = FileRecords.open(outputFile, true, (int) channel.size(), false);        ByteBuffer convertedRecordsBuffer = ByteBuffer.allocate(convertedRecords.sizeInBytes());        convertedRecords.readInto(convertedRecordsBuffer, 0);        // cleanup        convertedRecords.close();        channel.close();        return MemoryRecords.readableRecords(convertedRecordsBuffer);    }}
f7656
0
verifyDownConvertedRecords
private static void kafkatest_f7657_0(List<SimpleRecord> initialRecords, List<Long> initialOffsets, MemoryRecords downConvertedRecords, CompressionType compressionType, byte toMagic)
{    int i = 0;    for (RecordBatch batch : downConvertedRecords.batches()) {        assertTrue("Magic byte should be lower than or equal to " + toMagic, batch.magic() <= toMagic);        if (batch.magic() == RecordBatch.MAGIC_VALUE_V0)            assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());        else            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());        assertEquals("Compression type should not be affected by conversion", compressionType, batch.compressionType());        for (Record record : batch) {            assertTrue("Inner record should have magic " + toMagic, record.hasMagic(batch.magic()));            assertEquals("Offset should not change", initialOffsets.get(i).longValue(), record.offset());            assertEquals("Key should not change", utf8(initialRecords.get(i).key()), utf8(record.key()));            assertEquals("Value should not change", utf8(initialRecords.get(i).value()), utf8(record.value()));            assertFalse(record.hasTimestampType(TimestampType.LOG_APPEND_TIME));            if (batch.magic() == RecordBatch.MAGIC_VALUE_V0) {                assertEquals(RecordBatch.NO_TIMESTAMP, record.timestamp());                assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                assertTrue(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));            } else if (batch.magic() == RecordBatch.MAGIC_VALUE_V1) {                assertEquals("Timestamp should not change", initialRecords.get(i).timestamp(), record.timestamp());                assertTrue(record.hasTimestampType(TimestampType.CREATE_TIME));                assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));            } else {                assertEquals("Timestamp should not change", initialRecords.get(i).timestamp(), record.timestamp());                assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                assertArrayEquals("Headers should not change", initialRecords.get(i).headers(), record.headers());            }            i += 1;        }    }    assertEquals(initialOffsets.size(), i);}
f7657
0
testWriteTransactionalNotAllowedMagicV0
public void kafkatest_f7665_0()
{    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    long pid = 9809;    short epoch = 15;    int sequence = 2342;    new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V0, compressionType, TimestampType.CREATE_TIME, 0L, 0L, pid, epoch, sequence, true, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());}
f7665
0
testWriteTransactionalNotAllowedMagicV1
public void kafkatest_f7666_0()
{    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    long pid = 9809;    short epoch = 15;    int sequence = 2342;    new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType, TimestampType.CREATE_TIME, 0L, 0L, pid, epoch, sequence, true, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());}
f7666
0
testWriteControlBatchNotAllowedMagicV0
public void kafkatest_f7667_0()
{    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    long pid = 9809;    short epoch = 15;    int sequence = 2342;    new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V0, compressionType, TimestampType.CREATE_TIME, 0L, 0L, pid, epoch, sequence, false, true, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());}
f7667
0
testEstimatedSizeInBytes
public void kafkatest_f7675_0()
{    ByteBuffer buffer = ByteBuffer.allocate(1024);    buffer.position(bufferOffset);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    int previousEstimate = 0;    for (int i = 0; i < 10; i++) {        builder.append(new SimpleRecord(i, ("" + i).getBytes()));        int currentEstimate = builder.estimatedSizeInBytes();        assertTrue(currentEstimate > previousEstimate);        previousEstimate = currentEstimate;    }    int bytesWrittenBeforeClose = builder.estimatedSizeInBytes();    MemoryRecords records = builder.build();    assertEquals(records.sizeInBytes(), builder.estimatedSizeInBytes());    if (compressionType == CompressionType.NONE)        assertEquals(records.sizeInBytes(), bytesWrittenBeforeClose);}
f7675
0
testCompressionRateV1
public void kafkatest_f7676_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V1;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(1024);    buffer.position(bufferOffset);    LegacyRecord[] records = new LegacyRecord[] { LegacyRecord.create(magic, 0L, "a".getBytes(), "1".getBytes()), LegacyRecord.create(magic, 1L, "b".getBytes(), "2".getBytes()), LegacyRecord.create(magic, 2L, "c".getBytes(), "3".getBytes()) };    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    int uncompressedSize = 0;    for (LegacyRecord record : records) {        uncompressedSize += record.sizeInBytes() + Records.LOG_OVERHEAD;        builder.append(record);    }    MemoryRecords built = builder.build();    if (compressionType == CompressionType.NONE) {        assertEquals(1.0, builder.compressionRatio(), 0.00001);    } else {        int compressedSize = built.sizeInBytes() - Records.LOG_OVERHEAD - LegacyRecord.RECORD_OVERHEAD_V1;        double computedCompressionRate = (double) compressedSize / uncompressedSize;        assertEquals(computedCompressionRate, builder.compressionRatio(), 0.00001);    }}
f7676
0
buildUsingLogAppendTime
public void kafkatest_f7677_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V1;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(1024);    buffer.position(bufferOffset);    long logAppendTime = System.currentTimeMillis();    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    builder.append(0L, "a".getBytes(), "1".getBytes());    builder.append(0L, "b".getBytes(), "2".getBytes());    builder.append(0L, "c".getBytes(), "3".getBytes());    MemoryRecords records = builder.build();    MemoryRecordsBuilder.RecordsInfo info = builder.info();    assertEquals(logAppendTime, info.maxTimestamp);    if (compressionType != CompressionType.NONE)        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);    else        assertEquals(0L, info.shallowOffsetOfMaxTimestamp);    for (RecordBatch batch : records.batches()) {        assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());        for (Record record : batch) assertEquals(logAppendTime, record.timestamp());    }}
f7677
0
shouldThrowIllegalStateExceptionOnBuildWhenAborted
public void kafkatest_f7685_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V0;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    builder.abort();    assertThrows(IllegalStateException.class, builder::build);}
f7685
0
shouldResetBufferToInitialPositionOnAbort
public void kafkatest_f7686_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V0;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    builder.append(0L, "a".getBytes(), "1".getBytes());    builder.abort();    assertEquals(bufferOffset, builder.buffer().position());}
f7686
0
shouldThrowIllegalStateExceptionOnCloseWhenAborted
public void kafkatest_f7687_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V0;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    builder.abort();    try {        builder.close();        fail("Should have thrown IllegalStateException");    } catch (IllegalStateException e) {    // ok    }}
f7687
0
testHasRoomForMethodWithHeaders
public void kafkatest_f7695_0()
{    if (magic >= RecordBatch.MAGIC_VALUE_V2) {        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(100), magic, compression, TimestampType.CREATE_TIME, 0L);        RecordHeaders headers = new RecordHeaders();        headers.add("hello", "world.world".getBytes());        headers.add("hello", "world.world".getBytes());        headers.add("hello", "world.world".getBytes());        headers.add("hello", "world.world".getBytes());        headers.add("hello", "world.world".getBytes());        builder.append(logAppendTime, "key".getBytes(), "value".getBytes());        // Make sure that hasRoomFor accounts for header sizes by letting a record without headers pass, but stopping        // a record with a large number of headers.        assertTrue(builder.hasRoomFor(logAppendTime, "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS));        assertFalse(builder.hasRoomFor(logAppendTime, "key".getBytes(), "value".getBytes(), headers.toArray()));    }}
f7695
0
testChecksum
public void kafkatest_f7696_0()
{    // we get reasonable coverage with uncompressed and one compression type    if (compression != CompressionType.NONE && compression != CompressionType.LZ4)        return;    SimpleRecord[] records = { new SimpleRecord(283843L, "key1".getBytes(), "value1".getBytes()), new SimpleRecord(1234L, "key2".getBytes(), "value2".getBytes()) };    RecordBatch batch = MemoryRecords.withRecords(magic, compression, records).batches().iterator().next();    long expectedChecksum;    if (magic == RecordBatch.MAGIC_VALUE_V0) {        if (compression == CompressionType.NONE)            expectedChecksum = 1978725405L;        else            expectedChecksum = 66944826L;    } else if (magic == RecordBatch.MAGIC_VALUE_V1) {        if (compression == CompressionType.NONE)            expectedChecksum = 109425508L;        else            expectedChecksum = 1407303399L;    } else {        if (compression == CompressionType.NONE)            expectedChecksum = 3851219455L;        else            expectedChecksum = 2745969314L;    }    assertEquals("Unexpected checksum for magic " + magic + " and compression type " + compression, expectedChecksum, batch.checksum());}
f7696
0
testFilterToPreservesPartitionLeaderEpoch
public void kafkatest_f7697_0()
{    if (magic >= RecordBatch.MAGIC_VALUE_V2) {        int partitionLeaderEpoch = 67;        ByteBuffer buffer = ByteBuffer.allocate(2048);        MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 0L, RecordBatch.NO_TIMESTAMP, partitionLeaderEpoch);        builder.append(10L, null, "a".getBytes());        builder.append(11L, "1".getBytes(), "b".getBytes());        builder.append(12L, null, "c".getBytes());        ByteBuffer filtered = ByteBuffer.allocate(2048);        builder.build().filterTo(new TopicPartition("foo", 0), new RetainNonNullKeysFilter(), filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);        filtered.flip();        MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);        List<MutableRecordBatch> batches = TestUtils.toList(filteredRecords.batches());        assertEquals(1, batches.size());        MutableRecordBatch firstBatch = batches.get(0);        assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());    }}
f7697
0
checkBatchRetention
protected BatchRetention kafkatest_f7705_0(RecordBatch batch)
{    return deleteRetention;}
f7705
0
shouldRetainRecord
protected boolean kafkatest_f7706_0(RecordBatch recordBatch, Record record)
{    return false;}
f7706
0
testBuildEndTxnMarker
public void kafkatest_f7707_0()
{    if (magic >= RecordBatch.MAGIC_VALUE_V2) {        long producerId = 73;        short producerEpoch = 13;        long initialOffset = 983L;        int coordinatorEpoch = 347;        int partitionLeaderEpoch = 29;        EndTransactionMarker marker = new EndTransactionMarker(ControlRecordType.COMMIT, coordinatorEpoch);        MemoryRecords records = MemoryRecords.withEndTransactionMarker(initialOffset, System.currentTimeMillis(), partitionLeaderEpoch, producerId, producerEpoch, marker);        // verify that buffer allocation was precise        assertEquals(records.buffer().remaining(), records.buffer().capacity());        List<MutableRecordBatch> batches = TestUtils.toList(records.batches());        assertEquals(1, batches.size());        RecordBatch batch = batches.get(0);        assertTrue(batch.isControlBatch());        assertEquals(producerId, batch.producerId());        assertEquals(producerEpoch, batch.producerEpoch());        assertEquals(initialOffset, batch.baseOffset());        assertEquals(partitionLeaderEpoch, batch.partitionLeaderEpoch());        assertTrue(batch.isValid());        List<Record> createdRecords = TestUtils.toList(batch);        assertEquals(1, createdRecords.size());        Record record = createdRecords.get(0);        assertTrue(record.isValid());        EndTransactionMarker deserializedMarker = EndTransactionMarker.deserialize(record);        assertEquals(ControlRecordType.COMMIT, deserializedMarker.controlType());        assertEquals(coordinatorEpoch, deserializedMarker.coordinatorEpoch());    }}
f7707
0
testFilterToPreservesLogAppendTime
public void kafkatest_f7715_0()
{    assumeAtLeastV2OrNotZstd();    long logAppendTime = System.currentTimeMillis();    ByteBuffer buffer = ByteBuffer.allocate(2048);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, pid, epoch, firstSequence);    builder.append(10L, null, "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.LOG_APPEND_TIME, 1L, logAppendTime, pid, epoch, firstSequence);    builder.append(11L, "1".getBytes(), "b".getBytes());    builder.append(12L, null, "c".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.LOG_APPEND_TIME, 3L, logAppendTime, pid, epoch, firstSequence);    builder.append(13L, null, "d".getBytes());    builder.append(14L, "4".getBytes(), "e".getBytes());    builder.append(15L, "5".getBytes(), "f".getBytes());    builder.close();    buffer.flip();    ByteBuffer filtered = ByteBuffer.allocate(2048);    MemoryRecords.readableRecords(buffer).filterTo(new TopicPartition("foo", 0), new RetainNonNullKeysFilter(), filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);    filtered.flip();    MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);    List<MutableRecordBatch> batches = TestUtils.toList(filteredRecords.batches());    assertEquals(magic < RecordBatch.MAGIC_VALUE_V2 && compression == CompressionType.NONE ? 3 : 2, batches.size());    for (RecordBatch batch : batches) {        assertEquals(compression, batch.compressionType());        if (magic > RecordBatch.MAGIC_VALUE_V0) {            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());            assertEquals(logAppendTime, batch.maxTimestamp());        }    }}
f7715
0
testNextBatchSize
public void kafkatest_f7716_0()
{    assumeAtLeastV2OrNotZstd();    ByteBuffer buffer = ByteBuffer.allocate(2048);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, pid, epoch, firstSequence);    builder.append(10L, null, "abc".getBytes());    builder.close();    buffer.flip();    int size = buffer.remaining();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assertEquals(size, records.firstBatchSize().intValue());    assertEquals(0, buffer.position());    // size not in buffer    buffer.limit(1);    assertNull(records.firstBatchSize());    // magic not in buffer    buffer.limit(Records.LOG_OVERHEAD);    assertNull(records.firstBatchSize());    // payload not in buffer    buffer.limit(Records.HEADER_SIZE_UP_TO_MAGIC);    assertEquals(size, records.firstBatchSize().intValue());    buffer.limit(size);    byte magic = buffer.get(Records.MAGIC_OFFSET);    buffer.put(Records.MAGIC_OFFSET, (byte) 10);    assertThrows(CorruptRecordException.class, records::firstBatchSize);    buffer.put(Records.MAGIC_OFFSET, magic);    buffer.put(Records.SIZE_OFFSET + 3, (byte) 0);    assertThrows(CorruptRecordException.class, records::firstBatchSize);}
f7716
0
testWithRecords
public void kafkatest_f7717_0()
{    Supplier<MemoryRecords> recordsSupplier = () -> MemoryRecords.withRecords(magic, compression, new SimpleRecord(10L, "key1".getBytes(), "value1".getBytes()));    if (compression != CompressionType.ZSTD || magic >= MAGIC_VALUE_V2) {        MemoryRecords memoryRecords = recordsSupplier.get();        String key = Utils.utf8(memoryRecords.batches().iterator().next().iterator().next().key());        assertEquals("key1", key);    } else {        assertThrows(IllegalArgumentException.class, recordsSupplier::get);    }}
f7717
0
testCompressedIterationWithEmptyRecords
public void kafkatest_f7725_0() throws Exception
{    ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);    OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue), RecordBatch.MAGIC_VALUE_V1);    gzipOutput.close();    emptyCompressedValue.flip();    ByteBuffer buffer = ByteBuffer.allocate(128);    DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));    AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());    LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue), CompressionType.GZIP, TimestampType.CREATE_TIME);    buffer.flip();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    if (records.records().iterator().hasNext())        fail("Iteration should have caused invalid record error");}
f7725
0
testIsValidWithTooSmallBuffer
public void kafkatest_f7726_0()
{    ByteBuffer buffer = ByteBuffer.allocate(2);    LegacyRecord record = new LegacyRecord(buffer);    assertFalse(record.isValid());    record.ensureValid();}
f7726
0
testIsValidWithChecksumMismatch
public void kafkatest_f7727_0()
{    ByteBuffer buffer = ByteBuffer.allocate(4);    // set checksum    buffer.putInt(2);    LegacyRecord record = new LegacyRecord(buffer);    assertFalse(record.isValid());    record.ensureValid();}
f7727
0
shouldReturnAllKeysWhenMagicIsCurrentValueAndThrottleMsIsDefaultThrottle
public void kafkatest_f7735_0()
{    ApiVersionsResponse response = ApiVersionsResponse.apiVersionsResponse(AbstractResponse.DEFAULT_THROTTLE_TIME, RecordBatch.CURRENT_MAGIC_VALUE);    assertEquals(Utils.mkSet(ApiKeys.values()), apiKeysInResponse(response));    assertEquals(AbstractResponse.DEFAULT_THROTTLE_TIME, response.throttleTimeMs());}
f7735
0
shouldHaveCorrectDefaultApiVersionsResponse
public void kafkatest_f7736_0()
{    Collection<ApiVersionsResponse.ApiVersion> apiVersions = ApiVersionsResponse.defaultApiVersionsResponse().apiVersions();    assertEquals("API versions for all API keys must be maintained.", apiVersions.size(), ApiKeys.values().length);    for (ApiKeys key : ApiKeys.values()) {        ApiVersionsResponse.ApiVersion version = ApiVersionsResponse.defaultApiVersionsResponse().apiVersion(key.id);        assertNotNull("Could not find ApiVersion for API " + key.name, version);        assertEquals("Incorrect min version for Api " + key.name, version.minVersion, key.oldestVersion());        assertEquals("Incorrect max version for Api " + key.name, version.maxVersion, key.latestVersion());        // Check if versions less than min version are indeed set as null, i.e., deprecated.        for (int i = 0; i < version.minVersion; ++i) {            assertNull("Request version " + i + " for API " + version.apiKey + " must be null", key.requestSchemas[i]);            assertNull("Response version " + i + " for API " + version.apiKey + " must be null", key.responseSchemas[i]);        }        // Check if versions between min and max versions are non null, i.e., valid.        for (int i = version.minVersion; i <= version.maxVersion; ++i) {            assertNotNull("Request version " + i + " for API " + version.apiKey + " must not be null", key.requestSchemas[i]);            assertNotNull("Response version " + i + " for API " + version.apiKey + " must not be null", key.responseSchemas[i]);        }    }}
f7736
0
verifyApiKeysForMagic
private void kafkatest_f7737_0(final ApiVersionsResponse response, final byte maxMagic)
{    for (final ApiVersionsResponse.ApiVersion version : response.apiVersions()) {        assertTrue(ApiKeys.forId(version.apiKey).minRequiredInterBrokerMagic <= maxMagic);    }}
f7737
0
testLeaderAndIsrRequestNormalization
public void kafkatest_f7745_0()
{    Set<TopicPartition> tps = generateRandomTopicPartitions(10, 10);    Map<TopicPartition, LeaderAndIsrRequest.PartitionState> partitionStates = new HashMap<>();    for (TopicPartition tp : tps) {        partitionStates.put(tp, new LeaderAndIsrRequest.PartitionState(0, 0, 0, Collections.emptyList(), 0, Collections.emptyList(), false));    }    LeaderAndIsrRequest.Builder builder = new LeaderAndIsrRequest.Builder((short) 2, 0, 0, 0, partitionStates, Collections.emptySet());    Assert.assertTrue(builder.build((short) 2).size() < builder.build((short) 1).size());}
f7745
0
testUpdateMetadataRequestNormalization
public void kafkatest_f7746_0()
{    Set<TopicPartition> tps = generateRandomTopicPartitions(10, 10);    Map<TopicPartition, UpdateMetadataRequest.PartitionState> partitionStates = new HashMap<>();    for (TopicPartition tp : tps) {        partitionStates.put(tp, new UpdateMetadataRequest.PartitionState(0, 0, 0, Collections.emptyList(), 0, Collections.emptyList(), Collections.emptyList()));    }    UpdateMetadataRequest.Builder builder = new UpdateMetadataRequest.Builder((short) 5, 0, 0, 0, partitionStates, Collections.emptySet());    Assert.assertTrue(builder.build((short) 5).size() < builder.build((short) 4).size());}
f7746
0
testStopReplicaRequestNormalization
public void kafkatest_f7747_0()
{    Set<TopicPartition> tps = generateRandomTopicPartitions(10, 10);    Map<TopicPartition, UpdateMetadataRequest.PartitionState> partitionStates = new HashMap<>();    for (TopicPartition tp : tps) {        partitionStates.put(tp, new UpdateMetadataRequest.PartitionState(0, 0, 0, Collections.emptyList(), 0, Collections.emptyList(), Collections.emptyList()));    }    StopReplicaRequest.Builder builder = new StopReplicaRequest.Builder((short) 5, 0, 0, 0, false, tps);    Assert.assertTrue(builder.build((short) 1).size() < builder.build((short) 0).size());}
f7747
0
shouldThrowOnV0IfPrefixed
public void kafkatest_f7755_0()
{    new DeleteAclsRequest(V0, aclFilters(PREFIXED_FILTER));}
f7755
0
shouldThrowOnUnknownElements
public void kafkatest_f7756_0()
{    new DeleteAclsRequest(V1, aclFilters(UNKNOWN_FILTER));}
f7756
0
shouldRoundTripLiteralV0
public void kafkatest_f7757_0()
{    final DeleteAclsRequest original = new DeleteAclsRequest(V0, aclFilters(LITERAL_FILTER));    final Struct struct = original.toStruct();    final DeleteAclsRequest result = new DeleteAclsRequest(struct, V0);    assertRequestEquals(original, result);}
f7757
0
shouldRoundTripV1
public void kafkatest_f7765_0()
{    final DeleteAclsResponse original = new DeleteAclsResponse(100, aclResponses(LITERAL_RESPONSE, PREFIXED_RESPONSE));    final Struct struct = original.toStruct(V1);    final DeleteAclsResponse result = new DeleteAclsResponse(struct);    assertResponseEquals(original, result);}
f7765
0
assertResponseEquals
private static void kafkatest_f7766_0(final DeleteAclsResponse original, final DeleteAclsResponse actual)
{    assertEquals("Number of responses wrong", original.responses().size(), actual.responses().size());    for (int idx = 0; idx != original.responses().size(); ++idx) {        final List<AclBinding> originalBindings = original.responses().get(idx).deletions().stream().map(AclDeletionResult::acl).collect(Collectors.toList());        final List<AclBinding> actualBindings = actual.responses().get(idx).deletions().stream().map(AclDeletionResult::acl).collect(Collectors.toList());        assertEquals(originalBindings, actualBindings);    }}
f7766
0
aclResponses
private static List<AclFilterResponse> kafkatest_f7767_0(final AclFilterResponse... responses)
{    return Arrays.asList(responses);}
f7767
0
shouldRoundTripAnyV0AsLiteral
public void kafkatest_f7775_0()
{    final DescribeAclsRequest original = new DescribeAclsRequest(ANY_FILTER, V0);    final DescribeAclsRequest expected = new DescribeAclsRequest(new AclBindingFilter(new ResourcePatternFilter(ANY_FILTER.patternFilter().resourceType(), ANY_FILTER.patternFilter().name(), PatternType.LITERAL), ANY_FILTER.entryFilter()), V0);    final Struct struct = original.toStruct();    final DescribeAclsRequest result = new DescribeAclsRequest(struct, V0);    assertRequestEquals(expected, result);}
f7775
0
shouldRoundTripLiteralV1
public void kafkatest_f7776_0()
{    final DescribeAclsRequest original = new DescribeAclsRequest(LITERAL_FILTER, V1);    final Struct struct = original.toStruct();    final DescribeAclsRequest result = new DescribeAclsRequest(struct, V1);    assertRequestEquals(original, result);}
f7776
0
shouldRoundTripPrefixedV1
public void kafkatest_f7777_0()
{    final DescribeAclsRequest original = new DescribeAclsRequest(PREFIXED_FILTER, V1);    final Struct struct = original.toStruct();    final DescribeAclsRequest result = new DescribeAclsRequest(struct, V1);    assertRequestEquals(original, result);}
f7777
0
aclBindings
private static List<AclBinding> kafkatest_f7785_0(final AclBinding... bindings)
{    return Arrays.asList(bindings);}
f7785
0
testRequestVersionCompatibilityFailBuild
public void kafkatest_f7786_0()
{    new HeartbeatRequest.Builder(new HeartbeatRequestData().setGroupId("groupId").setMemberId("consumerId").setGroupInstanceId("groupInstanceId")).build((short) 2);}
f7786
0
shouldAcceptValidGroupInstanceIds
public void kafkatest_f7787_0()
{    String maxLengthString = TestUtils.randomString(249);    String[] validGroupInstanceIds = { "valid", "INSTANCE", "gRoUp", "ar6", "VaL1d", "_0-9_.", "...", maxLengthString };    for (String instanceId : validGroupInstanceIds) {        JoinGroupRequest.validateGroupInstanceId(instanceId);    }}
f7787
0
testToString
public void kafkatest_f7795_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.CLUSTER_AUTHORIZATION_FAILED);    LeaderAndIsrResponse response = new LeaderAndIsrResponse(Errors.NONE, errors);    String responseStr = response.toString();    assertTrue(responseStr.contains(LeaderAndIsrResponse.class.getSimpleName()));    assertTrue(responseStr.contains(errors.toString()));    assertTrue(responseStr.contains(Errors.NONE.name()));}
f7795
0
setUp
public void kafkatest_f7796_0()
{    members = Arrays.asList(new MemberIdentity().setMemberId(memberIdOne).setGroupInstanceId(instanceIdOne), new MemberIdentity().setMemberId(memberIdTwo).setGroupInstanceId(instanceIdTwo));    builder = new LeaveGroupRequest.Builder(groupId, members);}
f7796
0
testMultiLeaveConstructor
public void kafkatest_f7797_0()
{    final LeaveGroupRequestData expectedData = new LeaveGroupRequestData().setGroupId(groupId).setMembers(members);    for (short version = 0; version <= ApiKeys.LEAVE_GROUP.latestVersion(); version++) {        try {            LeaveGroupRequest request = builder.build(version);            if (version <= 2) {                fail("Older version " + version + " request data should not be created due to non-single members");            }            assertEquals(expectedData, request.data());            assertEquals(members, request.members());            LeaveGroupResponse expectedResponse = new LeaveGroupResponse(Collections.emptyList(), Errors.COORDINATOR_LOAD_IN_PROGRESS, throttleTimeMs, version);            assertEquals(expectedResponse, request.getErrorResponse(throttleTimeMs, Errors.COORDINATOR_LOAD_IN_PROGRESS.exception()));        } catch (UnsupportedVersionException e) {            assertTrue(e.getMessage().contains("leave group request only supports single member instance"));        }    }}
f7797
0
testEqualityWithMemberResponses
public void kafkatest_f7805_0()
{    for (short version = 0; version <= ApiKeys.LEAVE_GROUP.latestVersion(); version++) {        List<MemberResponse> localResponses = version > 2 ? memberResponses : memberResponses.subList(0, 1);        LeaveGroupResponse primaryResponse = new LeaveGroupResponse(localResponses, Errors.NONE, throttleTimeMs, version);        // The order of members should not alter result data.        Collections.reverse(localResponses);        LeaveGroupResponse reversedResponse = new LeaveGroupResponse(localResponses, Errors.NONE, throttleTimeMs, version);        assertEquals(primaryResponse, primaryResponse);        assertEquals(primaryResponse, reversedResponse);        assertEquals(primaryResponse.hashCode(), reversedResponse.hashCode());    }}
f7805
0
testEmptyMeansAllTopicsV0
public void kafkatest_f7806_0()
{    MetadataRequestData data = new MetadataRequestData();    MetadataRequest parsedRequest = new MetadataRequest(data, (short) 0);    assertTrue(parsedRequest.isAllTopics());    assertNull(parsedRequest.topics());}
f7806
0
testEmptyMeansEmptyForVersionsAboveV0
public void kafkatest_f7807_0()
{    for (int i = 1; i < MetadataRequestData.SCHEMAS.length; i++) {        MetadataRequestData data = new MetadataRequestData();        data.setAllowAutoTopicCreation(true);        MetadataRequest parsedRequest = new MetadataRequest(data, (short) i);        assertFalse(parsedRequest.isAllTopics());        assertEquals(Collections.emptyList(), parsedRequest.topics());    }}
f7807
0
testConstructorWithStruct
public void kafkatest_f7815_0()
{    OffsetCommitResponseData data = new OffsetCommitResponseData().setTopics(Arrays.asList(new OffsetCommitResponseTopic().setPartitions(Collections.singletonList(new OffsetCommitResponsePartition().setPartitionIndex(partitionOne).setErrorCode(errorOne.code()))), new OffsetCommitResponseTopic().setPartitions(Collections.singletonList(new OffsetCommitResponsePartition().setPartitionIndex(partitionTwo).setErrorCode(errorTwo.code()))))).setThrottleTimeMs(throttleTimeMs);    for (short version = 0; version <= ApiKeys.OFFSET_COMMIT.latestVersion(); version++) {        OffsetCommitResponse response = new OffsetCommitResponse(data.toStruct(version), version);        assertEquals(expectedErrorCounts, response.errorCounts());        if (version >= 3) {            assertEquals(throttleTimeMs, response.throttleTimeMs());        } else {            assertEquals(DEFAULT_THROTTLE_TIME, response.throttleTimeMs());        }        assertEquals(version >= 4, response.shouldClientThrottle(version));    }}
f7815
0
setUp
public void kafkatest_f7816_0()
{    partitions = Arrays.asList(new TopicPartition(topicOne, partitionOne), new TopicPartition(topicTwo, partitionTwo));    builder = new OffsetFetchRequest.Builder(groupId, partitions);}
f7816
0
testConstructor
public void kafkatest_f7817_0()
{    assertFalse(builder.isAllTopicPartitions());    int throttleTimeMs = 10;    Map<TopicPartition, PartitionData> expectedData = new HashMap<>();    for (TopicPartition partition : partitions) {        expectedData.put(partition, new PartitionData(OffsetFetchResponse.INVALID_OFFSET, Optional.empty(), OffsetFetchResponse.NO_METADATA, Errors.NONE));    }    for (short version = 0; version <= ApiKeys.OFFSET_FETCH.latestVersion(); version++) {        OffsetFetchRequest request = builder.build(version);        assertFalse(request.isAllPartitions());        assertEquals(groupId, request.groupId());        assertEquals(partitions, request.partitions());        OffsetFetchResponse response = request.getErrorResponse(throttleTimeMs, Errors.NONE);        assertEquals(Errors.NONE, response.error());        assertFalse(response.hasError());        assertEquals(Collections.singletonMap(Errors.NONE, 1), response.errorCounts());        if (version <= 1) {            assertEquals(expectedData, response.responseData());        }        if (version >= 3) {            assertEquals(throttleTimeMs, response.throttleTimeMs());        } else {            assertEquals(DEFAULT_THROTTLE_TIME, response.throttleTimeMs());        }    }}
f7817
0
testForConsumerRequiresVersion3
public void kafkatest_f7825_0()
{    OffsetsForLeaderEpochRequest.Builder builder = OffsetsForLeaderEpochRequest.Builder.forConsumer(Collections.emptyMap());    for (short version = 0; version < 3; version++) {        final short v = version;        assertThrows(UnsupportedVersionException.class, () -> builder.build(v));    }    for (short version = 3; version < ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion(); version++) {        OffsetsForLeaderEpochRequest request = builder.build((short) 3);        assertEquals(OffsetsForLeaderEpochRequest.CONSUMER_REPLICA_ID, request.replicaId());    }}
f7825
0
testDefaultReplicaId
public void kafkatest_f7826_0()
{    for (short version = 0; version < ApiKeys.OFFSET_FOR_LEADER_EPOCH.latestVersion(); version++) {        int replicaId = 1;        OffsetsForLeaderEpochRequest.Builder builder = OffsetsForLeaderEpochRequest.Builder.forFollower(version, Collections.emptyMap(), replicaId);        OffsetsForLeaderEpochRequest request = builder.build();        OffsetsForLeaderEpochRequest parsed = (OffsetsForLeaderEpochRequest) AbstractRequest.parseRequest(ApiKeys.OFFSET_FOR_LEADER_EPOCH, version, request.toStruct());        if (version < 3)            assertEquals(OffsetsForLeaderEpochRequest.DEBUGGING_REPLICA_ID, parsed.replicaId());        else            assertEquals(replicaId, parsed.replicaId());    }}
f7826
0
shouldBeFlaggedAsTransactionalWhenTransactionalRecords
public void kafkatest_f7827_0() throws Exception
{    final MemoryRecords memoryRecords = MemoryRecords.withTransactionalRecords(0, CompressionType.NONE, 1L, (short) 1, 1, 1, simpleRecord);    final ProduceRequest request = ProduceRequest.Builder.forCurrentMagic((short) -1, 10, Collections.singletonMap(new TopicPartition("topic", 1), memoryRecords)).build();    assertTrue(request.hasTransactionalRecords());}
f7827
0
testV3AndAboveCannotUseMagicV0
public void kafkatest_f7835_0()
{    ByteBuffer buffer = ByteBuffer.allocate(256);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V0, CompressionType.NONE, TimestampType.NO_TIMESTAMP_TYPE, 0L);    builder.append(10L, null, "a".getBytes());    Map<TopicPartition, MemoryRecords> produceData = new HashMap<>();    produceData.put(new TopicPartition("test", 0), builder.build());    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forCurrentMagic((short) 1, 5000, produceData);    assertThrowsInvalidRecordExceptionForAllVersions(requestBuilder);}
f7835
0
testV3AndAboveCannotUseMagicV1
public void kafkatest_f7836_0()
{    ByteBuffer buffer = ByteBuffer.allocate(256);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V1, CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    builder.append(10L, null, "a".getBytes());    Map<TopicPartition, MemoryRecords> produceData = new HashMap<>();    produceData.put(new TopicPartition("test", 0), builder.build());    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forCurrentMagic((short) 1, 5000, produceData);    assertThrowsInvalidRecordExceptionForAllVersions(requestBuilder);}
f7836
0
testV6AndBelowCannotUseZStdCompression
public void kafkatest_f7837_0()
{    ByteBuffer buffer = ByteBuffer.allocate(256);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, CompressionType.ZSTD, TimestampType.CREATE_TIME, 0L);    builder.append(10L, null, "a".getBytes());    Map<TopicPartition, MemoryRecords> produceData = new HashMap<>();    produceData.put(new TopicPartition("test", 0), builder.build());    // Can't create ProduceRequest instance with version within [3, 7)    for (short version = 3; version < 7; version++) {        ProduceRequest.Builder requestBuilder = new ProduceRequest.Builder(version, version, (short) 1, 5000, produceData, null);        assertThrowsInvalidRecordExceptionForAllVersions(requestBuilder);    }    // Works fine with current version (>= 7)    ProduceRequest.Builder.forCurrentMagic((short) 1, 5000, produceData);}
f7837
0
testRequestHeader
public void kafkatest_f7845_0()
{    RequestHeader header = new RequestHeader(ApiKeys.FIND_COORDINATOR, (short) 1, "", 10);    ByteBuffer buffer = toBuffer(header.toStruct());    RequestHeader deserialized = RequestHeader.parse(buffer);    assertEquals(header, deserialized);}
f7845
0
testRequestHeaderWithNullClientId
public void kafkatest_f7846_0()
{    RequestHeader header = new RequestHeader(ApiKeys.FIND_COORDINATOR, (short) 1, null, 10);    Struct headerStruct = header.toStruct();    ByteBuffer buffer = toBuffer(headerStruct);    RequestHeader deserialized = RequestHeader.parse(buffer);    assertEquals(header.apiKey(), deserialized.apiKey());    assertEquals(header.apiVersion(), deserialized.apiVersion());    assertEquals(header.correlationId(), deserialized.correlationId());    // null defaults to ""    assertEquals("", deserialized.clientId());}
f7846
0
testSerialization
public void kafkatest_f7847_0() throws Exception
{    checkRequest(createFindCoordinatorRequest(0), true);    checkRequest(createFindCoordinatorRequest(1), true);    checkErrorResponse(createFindCoordinatorRequest(0), new UnknownServerException(), true);    checkErrorResponse(createFindCoordinatorRequest(1), new UnknownServerException(), true);    checkResponse(createFindCoordinatorResponse(), 0, true);    checkResponse(createFindCoordinatorResponse(), 1, true);    checkRequest(createControlledShutdownRequest(), true);    checkResponse(createControlledShutdownResponse(), 1, true);    checkErrorResponse(createControlledShutdownRequest(), new UnknownServerException(), true);    checkErrorResponse(createControlledShutdownRequest(0), new UnknownServerException(), true);    checkRequest(createFetchRequest(4), true);    checkResponse(createFetchResponse(), 4, true);    List<TopicPartition> toForgetTopics = new ArrayList<>();    toForgetTopics.add(new TopicPartition("foo", 0));    toForgetTopics.add(new TopicPartition("foo", 2));    toForgetTopics.add(new TopicPartition("bar", 0));    checkRequest(createFetchRequest(7, new FetchMetadata(123, 456), toForgetTopics), true);    checkResponse(createFetchResponse(123), 7, true);    checkResponse(createFetchResponse(Errors.FETCH_SESSION_ID_NOT_FOUND, 123), 7, true);    checkErrorResponse(createFetchRequest(4), new UnknownServerException(), true);    checkRequest(createHeartBeatRequest(), true);    checkErrorResponse(createHeartBeatRequest(), new UnknownServerException(), true);    checkResponse(createHeartBeatResponse(), 0, true);    checkRequest(createJoinGroupRequest(1), true);    checkErrorResponse(createJoinGroupRequest(0), new UnknownServerException(), true);    checkErrorResponse(createJoinGroupRequest(1), new UnknownServerException(), true);    checkResponse(createJoinGroupResponse(), 0, true);    checkRequest(createLeaveGroupRequest(), true);    checkErrorResponse(createLeaveGroupRequest(), new UnknownServerException(), true);    checkResponse(createLeaveGroupResponse(), 0, true);    checkRequest(createListGroupsRequest(), true);    checkErrorResponse(createListGroupsRequest(), new UnknownServerException(), true);    checkResponse(createListGroupsResponse(), 0, true);    checkRequest(createDescribeGroupRequest(), true);    checkErrorResponse(createDescribeGroupRequest(), new UnknownServerException(), true);    checkResponse(createDescribeGroupResponse(), 0, true);    checkRequest(createDeleteGroupsRequest(), true);    checkErrorResponse(createDeleteGroupsRequest(), new UnknownServerException(), true);    checkResponse(createDeleteGroupsResponse(), 0, true);    checkRequest(createListOffsetRequest(1), true);    checkErrorResponse(createListOffsetRequest(1), new UnknownServerException(), true);    checkResponse(createListOffsetResponse(1), 1, true);    checkRequest(createListOffsetRequest(2), true);    checkErrorResponse(createListOffsetRequest(2), new UnknownServerException(), true);    checkResponse(createListOffsetResponse(2), 2, true);    checkRequest(MetadataRequest.Builder.allTopics().build((short) 2), true);    checkRequest(createMetadataRequest(1, Collections.singletonList("topic1")), true);    checkErrorResponse(createMetadataRequest(1, Collections.singletonList("topic1")), new UnknownServerException(), true);    checkResponse(createMetadataResponse(), 2, true);    checkErrorResponse(createMetadataRequest(2, Collections.singletonList("topic1")), new UnknownServerException(), true);    checkResponse(createMetadataResponse(), 3, true);    checkErrorResponse(createMetadataRequest(3, Collections.singletonList("topic1")), new UnknownServerException(), true);    checkResponse(createMetadataResponse(), 4, true);    checkErrorResponse(createMetadataRequest(4, Collections.singletonList("topic1")), new UnknownServerException(), true);    checkRequest(OffsetFetchRequest.Builder.allTopicPartitions("group1").build(), true);    checkErrorResponse(OffsetFetchRequest.Builder.allTopicPartitions("group1").build(), new NotCoordinatorException("Not Coordinator"), true);    checkRequest(createOffsetFetchRequest(0), true);    checkRequest(createOffsetFetchRequest(1), true);    checkRequest(createOffsetFetchRequest(2), true);    checkRequest(OffsetFetchRequest.Builder.allTopicPartitions("group1").build(), true);    checkErrorResponse(createOffsetFetchRequest(0), new UnknownServerException(), true);    checkErrorResponse(createOffsetFetchRequest(1), new UnknownServerException(), true);    checkErrorResponse(createOffsetFetchRequest(2), new UnknownServerException(), true);    checkResponse(createOffsetFetchResponse(), 0, true);    checkRequest(createProduceRequest(2), true);    checkErrorResponse(createProduceRequest(2), new UnknownServerException(), true);    checkRequest(createProduceRequest(3), true);    checkErrorResponse(createProduceRequest(3), new UnknownServerException(), true);    checkResponse(createProduceResponse(), 2, true);    checkRequest(createStopReplicaRequest(0, true), true);    checkRequest(createStopReplicaRequest(0, false), true);    checkErrorResponse(createStopReplicaRequest(0, true), new UnknownServerException(), true);    checkRequest(createStopReplicaRequest(1, true), true);    checkRequest(createStopReplicaRequest(1, false), true);    checkErrorResponse(createStopReplicaRequest(1, true), new UnknownServerException(), true);    checkResponse(createStopReplicaResponse(), 0, true);    checkRequest(createLeaderAndIsrRequest(0), true);    checkErrorResponse(createLeaderAndIsrRequest(0), new UnknownServerException(), false);    checkRequest(createLeaderAndIsrRequest(1), true);    checkErrorResponse(createLeaderAndIsrRequest(1), new UnknownServerException(), false);    checkResponse(createLeaderAndIsrResponse(), 0, true);    checkRequest(createSaslHandshakeRequest(), true);    checkErrorResponse(createSaslHandshakeRequest(), new UnknownServerException(), true);    checkResponse(createSaslHandshakeResponse(), 0, true);    checkRequest(createSaslAuthenticateRequest(), true);    checkErrorResponse(createSaslAuthenticateRequest(), new UnknownServerException(), true);    checkResponse(createSaslAuthenticateResponse(), 0, true);    checkResponse(createSaslAuthenticateResponse(), 1, true);    checkRequest(createApiVersionRequest(), true);    checkErrorResponse(createApiVersionRequest(), new UnknownServerException(), true);    checkResponse(createApiVersionResponse(), 0, true);    checkRequest(createCreateTopicRequest(0), true);    checkErrorResponse(createCreateTopicRequest(0), new UnknownServerException(), true);    checkResponse(createCreateTopicResponse(), 0, true);    checkRequest(createCreateTopicRequest(1), true);    checkErrorResponse(createCreateTopicRequest(1), new UnknownServerException(), true);    checkResponse(createCreateTopicResponse(), 1, true);    checkRequest(createDeleteTopicsRequest(), true);    checkErrorResponse(createDeleteTopicsRequest(), new UnknownServerException(), true);    checkResponse(createDeleteTopicsResponse(), 0, true);    checkRequest(createInitPidRequest(), true);    checkErrorResponse(createInitPidRequest(), new UnknownServerException(), true);    checkResponse(createInitPidResponse(), 0, true);    checkRequest(createAddPartitionsToTxnRequest(), true);    checkResponse(createAddPartitionsToTxnResponse(), 0, true);    checkErrorResponse(createAddPartitionsToTxnRequest(), new UnknownServerException(), true);    checkRequest(createAddOffsetsToTxnRequest(), true);    checkResponse(createAddOffsetsToTxnResponse(), 0, true);    checkErrorResponse(createAddOffsetsToTxnRequest(), new UnknownServerException(), true);    checkRequest(createEndTxnRequest(), true);    checkResponse(createEndTxnResponse(), 0, true);    checkErrorResponse(createEndTxnRequest(), new UnknownServerException(), true);    checkRequest(createWriteTxnMarkersRequest(), true);    checkResponse(createWriteTxnMarkersResponse(), 0, true);    checkErrorResponse(createWriteTxnMarkersRequest(), new UnknownServerException(), true);    checkRequest(createTxnOffsetCommitRequest(), true);    checkResponse(createTxnOffsetCommitResponse(), 0, true);    checkErrorResponse(createTxnOffsetCommitRequest(), new UnknownServerException(), true);    checkOlderFetchVersions();    checkResponse(createMetadataResponse(), 0, true);    checkResponse(createMetadataResponse(), 1, true);    checkErrorResponse(createMetadataRequest(1, Collections.singletonList("topic1")), new UnknownServerException(), true);    checkRequest(createOffsetCommitRequest(0), true);    checkErrorResponse(createOffsetCommitRequest(0), new UnknownServerException(), true);    checkRequest(createOffsetCommitRequest(1), true);    checkErrorResponse(createOffsetCommitRequest(1), new UnknownServerException(), true);    checkRequest(createOffsetCommitRequest(2), true);    checkErrorResponse(createOffsetCommitRequest(2), new UnknownServerException(), true);    checkRequest(createOffsetCommitRequest(3), true);    checkErrorResponse(createOffsetCommitRequest(3), new UnknownServerException(), true);    checkRequest(createOffsetCommitRequest(4), true);    checkErrorResponse(createOffsetCommitRequest(4), new UnknownServerException(), true);    checkResponse(createOffsetCommitResponse(), 4, true);    checkRequest(createOffsetCommitRequest(5), true);    checkErrorResponse(createOffsetCommitRequest(5), new UnknownServerException(), true);    checkResponse(createOffsetCommitResponse(), 5, true);    checkRequest(createJoinGroupRequest(0), true);    checkRequest(createUpdateMetadataRequest(0, null), false);    checkErrorResponse(createUpdateMetadataRequest(0, null), new UnknownServerException(), true);    checkRequest(createUpdateMetadataRequest(1, null), false);    checkRequest(createUpdateMetadataRequest(1, "rack1"), false);    checkErrorResponse(createUpdateMetadataRequest(1, null), new UnknownServerException(), true);    checkRequest(createUpdateMetadataRequest(2, "rack1"), false);    checkRequest(createUpdateMetadataRequest(2, null), false);    checkErrorResponse(createUpdateMetadataRequest(2, "rack1"), new UnknownServerException(), true);    checkRequest(createUpdateMetadataRequest(3, "rack1"), false);    checkRequest(createUpdateMetadataRequest(3, null), false);    checkErrorResponse(createUpdateMetadataRequest(3, "rack1"), new UnknownServerException(), true);    checkRequest(createUpdateMetadataRequest(4, "rack1"), false);    checkRequest(createUpdateMetadataRequest(4, null), false);    checkErrorResponse(createUpdateMetadataRequest(4, "rack1"), new UnknownServerException(), true);    checkRequest(createUpdateMetadataRequest(5, "rack1"), false);    checkRequest(createUpdateMetadataRequest(5, null), false);    checkErrorResponse(createUpdateMetadataRequest(5, "rack1"), new UnknownServerException(), true);    checkResponse(createUpdateMetadataResponse(), 0, true);    checkRequest(createListOffsetRequest(0), true);    checkErrorResponse(createListOffsetRequest(0), new UnknownServerException(), true);    checkResponse(createListOffsetResponse(0), 0, true);    checkRequest(createLeaderEpochRequestForReplica(0, 1), true);    checkRequest(createLeaderEpochRequestForConsumer(), true);    checkResponse(createLeaderEpochResponse(), 0, true);    checkErrorResponse(createLeaderEpochRequestForConsumer(), new UnknownServerException(), true);    checkRequest(createAddPartitionsToTxnRequest(), true);    checkErrorResponse(createAddPartitionsToTxnRequest(), new UnknownServerException(), true);    checkResponse(createAddPartitionsToTxnResponse(), 0, true);    checkRequest(createAddOffsetsToTxnRequest(), true);    checkErrorResponse(createAddOffsetsToTxnRequest(), new UnknownServerException(), true);    checkResponse(createAddOffsetsToTxnResponse(), 0, true);    checkRequest(createEndTxnRequest(), true);    checkErrorResponse(createEndTxnRequest(), new UnknownServerException(), true);    checkResponse(createEndTxnResponse(), 0, true);    checkRequest(createWriteTxnMarkersRequest(), true);    checkErrorResponse(createWriteTxnMarkersRequest(), new UnknownServerException(), true);    checkResponse(createWriteTxnMarkersResponse(), 0, true);    checkRequest(createTxnOffsetCommitRequest(), true);    checkErrorResponse(createTxnOffsetCommitRequest(), new UnknownServerException(), true);    checkResponse(createTxnOffsetCommitResponse(), 0, true);    checkRequest(createListAclsRequest(), true);    checkErrorResponse(createListAclsRequest(), new SecurityDisabledException("Security is not enabled."), true);    checkResponse(createDescribeAclsResponse(), ApiKeys.DESCRIBE_ACLS.latestVersion(), true);    checkRequest(createCreateAclsRequest(), true);    checkErrorResponse(createCreateAclsRequest(), new SecurityDisabledException("Security is not enabled."), true);    checkResponse(createCreateAclsResponse(), ApiKeys.CREATE_ACLS.latestVersion(), true);    checkRequest(createDeleteAclsRequest(), true);    checkErrorResponse(createDeleteAclsRequest(), new SecurityDisabledException("Security is not enabled."), true);    checkResponse(createDeleteAclsResponse(), ApiKeys.DELETE_ACLS.latestVersion(), true);    checkRequest(createAlterConfigsRequest(), false);    checkErrorResponse(createAlterConfigsRequest(), new UnknownServerException(), true);    checkResponse(createAlterConfigsResponse(), 0, false);    checkRequest(createDescribeConfigsRequest(0), true);    checkRequest(createDescribeConfigsRequestWithConfigEntries(0), false);    checkErrorResponse(createDescribeConfigsRequest(0), new UnknownServerException(), true);    checkResponse(createDescribeConfigsResponse(), 0, false);    checkRequest(createDescribeConfigsRequest(1), true);    checkRequest(createDescribeConfigsRequestWithConfigEntries(1), false);    checkErrorResponse(createDescribeConfigsRequest(1), new UnknownServerException(), true);    checkResponse(createDescribeConfigsResponse(), 1, false);    checkDescribeConfigsResponseVersions();    checkRequest(createCreatePartitionsRequest(), true);    checkRequest(createCreatePartitionsRequestWithAssignments(), false);    checkErrorResponse(createCreatePartitionsRequest(), new InvalidTopicException(), true);    checkResponse(createCreatePartitionsResponse(), 0, true);    checkRequest(createCreateTokenRequest(), true);    checkErrorResponse(createCreateTokenRequest(), new UnknownServerException(), true);    checkResponse(createCreateTokenResponse(), 0, true);    checkRequest(createDescribeTokenRequest(), true);    checkErrorResponse(createDescribeTokenRequest(), new UnknownServerException(), true);    checkResponse(createDescribeTokenResponse(), 0, true);    checkRequest(createExpireTokenRequest(), true);    checkErrorResponse(createExpireTokenRequest(), new UnknownServerException(), true);    checkResponse(createExpireTokenResponse(), 0, true);    checkRequest(createRenewTokenRequest(), true);    checkErrorResponse(createRenewTokenRequest(), new UnknownServerException(), true);    checkResponse(createRenewTokenResponse(), 0, true);    checkRequest(createElectLeadersRequest(), true);    checkRequest(createElectLeadersRequestNullPartitions(), true);    checkErrorResponse(createElectLeadersRequest(), new UnknownServerException(), true);    checkResponse(createElectLeadersResponse(), 1, true);    checkRequest(createIncrementalAlterConfigsRequest(), true);    checkErrorResponse(createIncrementalAlterConfigsRequest(), new UnknownServerException(), true);    checkResponse(createIncrementalAlterConfigsResponse(), 0, true);    checkRequest(createAlterPartitionReassignmentsRequest(), true);    checkErrorResponse(createAlterPartitionReassignmentsRequest(), new UnknownServerException(), true);    checkResponse(createAlterPartitionReassignmentsResponse(), 0, true);    checkRequest(createListPartitionReassignmentsRequest(), true);    checkErrorResponse(createListPartitionReassignmentsRequest(), new UnknownServerException(), true);    checkResponse(createListPartitionReassignmentsResponse(), 0, true);    checkRequest(createOffsetDeleteRequest(), true);    checkErrorResponse(createOffsetDeleteRequest(), new UnknownServerException(), true);    checkResponse(createOffsetDeleteResponse(), 0, true);}
f7847
0
deserialize
private AbstractRequestResponse kafkatest_f7855_0(AbstractRequestResponse req, Struct struct, short version) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException
{    ByteBuffer buffer = toBuffer(struct);    Method deserializer = req.getClass().getDeclaredMethod("parse", ByteBuffer.class, Short.TYPE);    return (AbstractRequestResponse) deserializer.invoke(null, buffer, version);}
f7855
0
cannotUseFindCoordinatorV0ToFindTransactionCoordinator
public void kafkatest_f7856_0()
{    FindCoordinatorRequest.Builder builder = new FindCoordinatorRequest.Builder(new FindCoordinatorRequestData().setKeyType(CoordinatorType.TRANSACTION.id).setKey("foobar"));    builder.build((short) 0);}
f7856
0
produceRequestToStringTest
public void kafkatest_f7857_0()
{    ProduceRequest request = createProduceRequest(ApiKeys.PRODUCE.latestVersion());    assertEquals(1, request.partitionRecordsOrFail().size());    assertFalse(request.toString(false).contains("partitionSizes"));    assertTrue(request.toString(false).contains("numPartitions=1"));    assertTrue(request.toString(true).contains("partitionSizes"));    assertFalse(request.toString(true).contains("numPartitions"));    request.clearPartitionRecords();    try {        request.partitionRecordsOrFail();        fail("partitionRecordsOrFail should fail after clearPartitionRecords()");    } catch (IllegalStateException e) {    // OK    }    // `toString` should behave the same after `clearPartitionRecords`    assertFalse(request.toString(false).contains("partitionSizes"));    assertTrue(request.toString(false).contains("numPartitions=1"));    assertTrue(request.toString(true).contains("partitionSizes"));    assertFalse(request.toString(true).contains("numPartitions"));}
f7857
0
testControlledShutdownResponse
public void kafkatest_f7865_0()
{    ControlledShutdownResponse response = createControlledShutdownResponse();    short version = ApiKeys.CONTROLLED_SHUTDOWN.latestVersion();    Struct struct = response.toStruct(version);    ByteBuffer buffer = toBuffer(struct);    ControlledShutdownResponse deserialized = ControlledShutdownResponse.parse(buffer, version);    assertEquals(response.error(), deserialized.error());    assertEquals(response.data().remainingPartitions(), deserialized.data().remainingPartitions());}
f7865
0
testCreateTopicRequestV0FailsIfValidateOnly
public void kafkatest_f7866_0()
{    createCreateTopicRequest(0, true);}
f7866
0
testCreateTopicRequestV3FailsIfNoPartitionsOrReplicas
public void kafkatest_f7867_0()
{    final UnsupportedVersionException exception = assertThrows(UnsupportedVersionException.class, () -> {        CreateTopicsRequestData data = new CreateTopicsRequestData().setTimeoutMs(123).setValidateOnly(false);        data.topics().add(new CreatableTopic().setName("foo").setNumPartitions(CreateTopicsRequest.NO_NUM_PARTITIONS).setReplicationFactor((short) 1));        data.topics().add(new CreatableTopic().setName("bar").setNumPartitions(1).setReplicationFactor(CreateTopicsRequest.NO_REPLICATION_FACTOR));        new Builder(data).build((short) 3);    });    assertTrue(exception.getMessage().contains("supported in CreateTopicRequest version 4+"));    assertTrue(exception.getMessage().contains("[foo, bar]"));}
f7867
0
createFindCoordinatorResponse
private FindCoordinatorResponse kafkatest_f7875_0()
{    Node node = new Node(10, "host1", 2014);    return FindCoordinatorResponse.prepareResponse(Errors.NONE, node);}
f7875
0
createFetchRequest
private FetchRequest kafkatest_f7876_0(int version, FetchMetadata metadata, List<TopicPartition> toForget)
{    LinkedHashMap<TopicPartition, FetchRequest.PartitionData> fetchData = new LinkedHashMap<>();    fetchData.put(new TopicPartition("test1", 0), new FetchRequest.PartitionData(100, 0L, 1000000, Optional.of(15)));    fetchData.put(new TopicPartition("test2", 0), new FetchRequest.PartitionData(200, 0L, 1000000, Optional.of(25)));    return FetchRequest.Builder.forConsumer(100, 100000, fetchData).metadata(metadata).setMaxBytes(1000).toForget(toForget).build((short) version);}
f7876
0
createFetchRequest
private FetchRequest kafkatest_f7877_0(int version, IsolationLevel isolationLevel)
{    LinkedHashMap<TopicPartition, FetchRequest.PartitionData> fetchData = new LinkedHashMap<>();    fetchData.put(new TopicPartition("test1", 0), new FetchRequest.PartitionData(100, 0L, 1000000, Optional.of(15)));    fetchData.put(new TopicPartition("test2", 0), new FetchRequest.PartitionData(200, 0L, 1000000, Optional.of(25)));    return FetchRequest.Builder.forConsumer(100, 100000, fetchData).isolationLevel(isolationLevel).setMaxBytes(1000).build((short) version);}
f7877
0
createJoinGroupResponse
private JoinGroupResponse kafkatest_f7885_0()
{    List<JoinGroupResponseData.JoinGroupResponseMember> members = Arrays.asList(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId("consumer1").setMetadata(new byte[0]), new JoinGroupResponseData.JoinGroupResponseMember().setMemberId("consumer2").setMetadata(new byte[0]));    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(Errors.NONE.code()).setGenerationId(1).setProtocolName("range").setLeader("leader").setMemberId("consumer1").setMembers(members));}
f7885
0
createListGroupsRequest
private ListGroupsRequest kafkatest_f7886_0()
{    return new ListGroupsRequest.Builder(new ListGroupsRequestData()).build();}
f7886
0
createListGroupsResponse
private ListGroupsResponse kafkatest_f7887_0()
{    return new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.NONE.code()).setGroups(Collections.singletonList(new ListGroupsResponseData.ListedGroup().setGroupId("test-group").setProtocolType("consumer"))));}
f7887
0
createListOffsetResponse
private ListOffsetResponse kafkatest_f7895_0(int version)
{    if (version == 0) {        Map<TopicPartition, ListOffsetResponse.PartitionData> responseData = new HashMap<>();        responseData.put(new TopicPartition("test", 0), new ListOffsetResponse.PartitionData(Errors.NONE, asList(100L)));        return new ListOffsetResponse(responseData);    } else if (version == 1 || version == 2) {        Map<TopicPartition, ListOffsetResponse.PartitionData> responseData = new HashMap<>();        responseData.put(new TopicPartition("test", 0), new ListOffsetResponse.PartitionData(Errors.NONE, 10000L, 100L, Optional.of(27)));        return new ListOffsetResponse(responseData);    } else {        throw new IllegalArgumentException("Illegal ListOffsetResponse version " + version);    }}
f7895
0
createMetadataRequest
private MetadataRequest kafkatest_f7896_0(int version, List<String> topics)
{    return new MetadataRequest.Builder(topics, true).build((short) version);}
f7896
0
createMetadataResponse
private MetadataResponse kafkatest_f7897_0()
{    Node node = new Node(1, "host1", 1001);    List<Node> replicas = asList(node);    List<Node> isr = asList(node);    List<Node> offlineReplicas = asList();    List<MetadataResponse.TopicMetadata> allTopicMetadata = new ArrayList<>();    allTopicMetadata.add(new MetadataResponse.TopicMetadata(Errors.NONE, "__consumer_offsets", true, asList(new MetadataResponse.PartitionMetadata(Errors.NONE, 1, node, Optional.of(5), replicas, isr, offlineReplicas))));    allTopicMetadata.add(new MetadataResponse.TopicMetadata(Errors.LEADER_NOT_AVAILABLE, "topic2", false, Collections.emptyList()));    allTopicMetadata.add(new MetadataResponse.TopicMetadata(Errors.NONE, "topic3", false, asList(new MetadataResponse.PartitionMetadata(Errors.LEADER_NOT_AVAILABLE, 0, null, Optional.empty(), replicas, isr, offlineReplicas))));    return MetadataResponse.prepareResponse(asList(node), null, MetadataResponse.NO_CONTROLLER_ID, allTopicMetadata);}
f7897
0
createStopReplicaResponse
private StopReplicaResponse kafkatest_f7905_0()
{    Map<TopicPartition, Errors> responses = new HashMap<>();    responses.put(new TopicPartition("test", 0), Errors.NONE);    return new StopReplicaResponse(Errors.NONE, responses);}
f7905
0
createControlledShutdownRequest
private ControlledShutdownRequest kafkatest_f7906_0()
{    ControlledShutdownRequestData data = new ControlledShutdownRequestData().setBrokerId(10).setBrokerEpoch(0L);    return new ControlledShutdownRequest.Builder(data, ApiKeys.CONTROLLED_SHUTDOWN.latestVersion()).build();}
f7906
0
createControlledShutdownRequest
private ControlledShutdownRequest kafkatest_f7907_0(int version)
{    ControlledShutdownRequestData data = new ControlledShutdownRequestData().setBrokerId(10).setBrokerEpoch(0L);    return new ControlledShutdownRequest.Builder(data, ApiKeys.CONTROLLED_SHUTDOWN.latestVersion()).build((short) version);}
f7907
0
createSaslAuthenticateRequest
private SaslAuthenticateRequest kafkatest_f7915_0()
{    SaslAuthenticateRequestData data = new SaslAuthenticateRequestData().setAuthBytes(new byte[0]);    return new SaslAuthenticateRequest(data);}
f7915
0
createSaslAuthenticateResponse
private SaslAuthenticateResponse kafkatest_f7916_0()
{    SaslAuthenticateResponseData data = new SaslAuthenticateResponseData().setErrorCode(Errors.NONE.code()).setAuthBytes(new byte[0]).setSessionLifetimeMs(Long.MAX_VALUE);    return new SaslAuthenticateResponse(data);}
f7916
0
createApiVersionRequest
private ApiVersionsRequest kafkatest_f7917_0()
{    return new ApiVersionsRequest.Builder().build();}
f7917
0
createInitPidResponse
private InitProducerIdResponse kafkatest_f7925_0()
{    InitProducerIdResponseData responseData = new InitProducerIdResponseData().setErrorCode(Errors.NONE.code()).setProducerEpoch((short) 3).setProducerId(3332).setThrottleTimeMs(0);    return new InitProducerIdResponse(responseData);}
f7925
0
createOffsetForLeaderEpochPartitionData
private Map<TopicPartition, OffsetsForLeaderEpochRequest.PartitionData> kafkatest_f7926_0()
{    Map<TopicPartition, OffsetsForLeaderEpochRequest.PartitionData> epochs = new HashMap<>();    epochs.put(new TopicPartition("topic1", 0), new OffsetsForLeaderEpochRequest.PartitionData(Optional.of(0), 1));    epochs.put(new TopicPartition("topic1", 1), new OffsetsForLeaderEpochRequest.PartitionData(Optional.of(0), 1));    epochs.put(new TopicPartition("topic2", 2), new OffsetsForLeaderEpochRequest.PartitionData(Optional.empty(), 3));    return epochs;}
f7926
0
createLeaderEpochRequestForConsumer
private OffsetsForLeaderEpochRequest kafkatest_f7927_0()
{    Map<TopicPartition, OffsetsForLeaderEpochRequest.PartitionData> epochs = createOffsetForLeaderEpochPartitionData();    return OffsetsForLeaderEpochRequest.Builder.forConsumer(epochs).build();}
f7927
0
createEndTxnResponse
private EndTxnResponse kafkatest_f7935_0()
{    return new EndTxnResponse(0, Errors.NONE);}
f7935
0
createWriteTxnMarkersRequest
private WriteTxnMarkersRequest kafkatest_f7936_0()
{    return new WriteTxnMarkersRequest.Builder(Collections.singletonList(new WriteTxnMarkersRequest.TxnMarkerEntry(21L, (short) 42, 73, TransactionResult.ABORT, Collections.singletonList(new TopicPartition("topic", 73))))).build();}
f7936
0
createWriteTxnMarkersResponse
private WriteTxnMarkersResponse kafkatest_f7937_0()
{    final Map<TopicPartition, Errors> errorPerPartitions = new HashMap<>();    errorPerPartitions.put(new TopicPartition("topic", 73), Errors.NONE);    final Map<Long, Map<TopicPartition, Errors>> response = new HashMap<>();    response.put(21L, errorPerPartitions);    return new WriteTxnMarkersResponse(response);}
f7937
0
createDeleteAclsResponse
private DeleteAclsResponse kafkatest_f7945_0()
{    List<AclFilterResponse> responses = new ArrayList<>();    responses.add(new AclFilterResponse(Utils.mkSet(new AclDeletionResult(new AclBinding(new ResourcePattern(ResourceType.TOPIC, "mytopic3", PatternType.LITERAL), new AccessControlEntry("User:ANONYMOUS", "*", AclOperation.DESCRIBE, AclPermissionType.ALLOW))), new AclDeletionResult(new AclBinding(new ResourcePattern(ResourceType.TOPIC, "mytopic4", PatternType.LITERAL), new AccessControlEntry("User:ANONYMOUS", "*", AclOperation.DESCRIBE, AclPermissionType.DENY))))));    responses.add(new AclFilterResponse(new ApiError(Errors.SECURITY_DISABLED, "No security"), Collections.<AclDeletionResult>emptySet()));    return new DeleteAclsResponse(0, responses);}
f7945
0
createDescribeConfigsRequest
private DescribeConfigsRequest kafkatest_f7946_0(int version)
{    return new DescribeConfigsRequest.Builder(asList(new ConfigResource(ConfigResource.Type.BROKER, "0"), new ConfigResource(ConfigResource.Type.TOPIC, "topic"))).build((short) version);}
f7946
0
createDescribeConfigsRequestWithConfigEntries
private DescribeConfigsRequest kafkatest_f7947_0(int version)
{    Map<ConfigResource, Collection<String>> resources = new HashMap<>();    resources.put(new ConfigResource(ConfigResource.Type.BROKER, "0"), asList("foo", "bar"));    resources.put(new ConfigResource(ConfigResource.Type.TOPIC, "topic"), null);    resources.put(new ConfigResource(ConfigResource.Type.TOPIC, "topic a"), Collections.emptyList());    return new DescribeConfigsRequest.Builder(resources).build((short) version);}
f7947
0
createCreateTokenResponse
private CreateDelegationTokenResponse kafkatest_f7955_0()
{    CreateDelegationTokenResponseData data = new CreateDelegationTokenResponseData().setThrottleTimeMs(20).setErrorCode(Errors.NONE.code()).setPrincipalType("User").setPrincipalName("user1").setIssueTimestampMs(System.currentTimeMillis()).setExpiryTimestampMs(System.currentTimeMillis()).setMaxTimestampMs(System.currentTimeMillis()).setTokenId("token1").setHmac("test".getBytes());    return new CreateDelegationTokenResponse(data);}
f7955
0
createRenewTokenRequest
private RenewDelegationTokenRequest kafkatest_f7956_0()
{    RenewDelegationTokenRequestData data = new RenewDelegationTokenRequestData().setHmac("test".getBytes()).setRenewPeriodMs(System.currentTimeMillis());    return new RenewDelegationTokenRequest.Builder(data).build();}
f7956
0
createRenewTokenResponse
private RenewDelegationTokenResponse kafkatest_f7957_0()
{    RenewDelegationTokenResponseData data = new RenewDelegationTokenResponseData().setThrottleTimeMs(20).setErrorCode(Errors.NONE.code()).setExpiryTimestampMs(System.currentTimeMillis());    return new RenewDelegationTokenResponse(data);}
f7957
0
createIncrementalAlterConfigsRequest
private IncrementalAlterConfigsRequest kafkatest_f7965_0()
{    IncrementalAlterConfigsRequestData data = new IncrementalAlterConfigsRequestData();    AlterableConfig alterableConfig = new AlterableConfig().setName("retention.ms").setConfigOperation((byte) 0).setValue("100");    IncrementalAlterConfigsRequestData.AlterableConfigCollection alterableConfigs = new IncrementalAlterConfigsRequestData.AlterableConfigCollection();    alterableConfigs.add(alterableConfig);    data.resources().add(new AlterConfigsResource().setResourceName("testtopic").setResourceType(ResourceType.TOPIC.code()).setConfigs(alterableConfigs));    return new IncrementalAlterConfigsRequest.Builder(data).build((short) 0);}
f7965
0
createIncrementalAlterConfigsResponse
private IncrementalAlterConfigsResponse kafkatest_f7966_0()
{    IncrementalAlterConfigsResponseData data = new IncrementalAlterConfigsResponseData();    data.responses().add(new AlterConfigsResourceResponse().setResourceName("testtopic").setResourceType(ResourceType.TOPIC.code()).setErrorCode(Errors.INVALID_REQUEST.code()).setErrorMessage("Duplicate Keys"));    return new IncrementalAlterConfigsResponse(data);}
f7966
0
createAlterPartitionReassignmentsRequest
private AlterPartitionReassignmentsRequest kafkatest_f7967_0()
{    AlterPartitionReassignmentsRequestData data = new AlterPartitionReassignmentsRequestData();    data.topics().add(new AlterPartitionReassignmentsRequestData.ReassignableTopic().setName("topic").setPartitions(Collections.singletonList(new AlterPartitionReassignmentsRequestData.ReassignablePartition().setPartitionIndex(0).setReplicas(null))));    return new AlterPartitionReassignmentsRequest.Builder(data).build((short) 0);}
f7967
0
testErrorCountsNoTopLevelError
public void kafkatest_f7975_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.CLUSTER_AUTHORIZATION_FAILED);    StopReplicaResponse response = new StopReplicaResponse(Errors.NONE, errors);    Map<Errors, Integer> errorCounts = response.errorCounts();    assertEquals(2, errorCounts.size());    assertEquals(1, errorCounts.get(Errors.NONE).intValue());    assertEquals(1, errorCounts.get(Errors.CLUSTER_AUTHORIZATION_FAILED).intValue());}
f7975
0
testToString
public void kafkatest_f7976_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.CLUSTER_AUTHORIZATION_FAILED);    StopReplicaResponse response = new StopReplicaResponse(Errors.NONE, errors);    String responseStr = response.toString();    assertTrue(responseStr.contains(StopReplicaResponse.class.getSimpleName()));    assertTrue(responseStr.contains(errors.toString()));    assertTrue(responseStr.contains(Errors.NONE.name()));}
f7976
0
testRequestVersionCompatibilityFailBuild
public void kafkatest_f7977_0()
{    new SyncGroupRequest.Builder(new SyncGroupRequestData().setGroupId("groupId").setMemberId("consumerId").setGroupInstanceId("groupInstanceId")).build((short) 2);}
f7977
0
shouldMatchWhereResourceTypeIsAny
public void kafkatest_f7985_0()
{    assertTrue(new ResourceFilter(ANY, "Name").matches(new Resource(TOPIC, "Name")));}
f7985
0
shouldMatchWhereResourceNameIsAny
public void kafkatest_f7986_0()
{    assertTrue(new ResourceFilter(TOPIC, null).matches(new Resource(TOPIC, "Name")));}
f7986
0
shouldMatchIfExactMatch
public void kafkatest_f7987_0()
{    assertTrue(new ResourceFilter(TOPIC, "Name").matches(new Resource(TOPIC, "Name")));}
f7987
0
testUseOldPrincipalBuilderForPlaintextIfProvided
public void kafkatest_f7995_0() throws Exception
{    TransportLayer transportLayer = mock(TransportLayer.class);    Authenticator authenticator = mock(Authenticator.class);    PrincipalBuilder oldPrincipalBuilder = mock(PrincipalBuilder.class);    when(oldPrincipalBuilder.buildPrincipal(any(), any())).thenReturn(new DummyPrincipal("foo"));    DefaultKafkaPrincipalBuilder builder = DefaultKafkaPrincipalBuilder.fromOldPrincipalBuilder(authenticator, transportLayer, oldPrincipalBuilder, null);    KafkaPrincipal principal = builder.build(new PlaintextAuthenticationContext(InetAddress.getLocalHost(), SecurityProtocol.PLAINTEXT.name()));    assertEquals(KafkaPrincipal.USER_TYPE, principal.getPrincipalType());    assertEquals("foo", principal.getName());    builder.close();    verify(oldPrincipalBuilder).buildPrincipal(transportLayer, authenticator);    verify(oldPrincipalBuilder).close();}
f7995
0
testReturnAnonymousPrincipalForPlaintext
public void kafkatest_f7996_0() throws Exception
{    try (DefaultKafkaPrincipalBuilder builder = new DefaultKafkaPrincipalBuilder(null, null)) {        assertEquals(KafkaPrincipal.ANONYMOUS, builder.build(new PlaintextAuthenticationContext(InetAddress.getLocalHost(), SecurityProtocol.PLAINTEXT.name())));    }}
f7996
0
testUseOldPrincipalBuilderForSslIfProvided
public void kafkatest_f7997_0() throws Exception
{    TransportLayer transportLayer = mock(TransportLayer.class);    Authenticator authenticator = mock(Authenticator.class);    PrincipalBuilder oldPrincipalBuilder = mock(PrincipalBuilder.class);    SSLSession session = mock(SSLSession.class);    when(oldPrincipalBuilder.buildPrincipal(any(), any())).thenReturn(new DummyPrincipal("foo"));    DefaultKafkaPrincipalBuilder builder = DefaultKafkaPrincipalBuilder.fromOldPrincipalBuilder(authenticator, transportLayer, oldPrincipalBuilder, null);    KafkaPrincipal principal = builder.build(new SslAuthenticationContext(session, InetAddress.getLocalHost(), SecurityProtocol.PLAINTEXT.name()));    assertEquals(KafkaPrincipal.USER_TYPE, principal.getPrincipalType());    assertEquals("foo", principal.getName());    builder.close();    verify(oldPrincipalBuilder).buildPrincipal(transportLayer, authenticator);    verify(oldPrincipalBuilder).close();}
f7997
0
setup
public void kafkatest_f8005_0() throws Exception
{    LoginManager.closeAll();    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    saslServerConfigs = new HashMap<>();    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, Arrays.asList("PLAIN"));    saslClientConfigs = new HashMap<>();    saslClientConfigs.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "PLAIN");    testJaasConfig = TestJaasConfig.createConfiguration("PLAIN", Arrays.asList("PLAIN"));    testJaasConfig.setClientOptions("PLAIN", TestJaasConfig.USERNAME, "anotherpassword");    server = createEchoServer(securityProtocol);}
f8005
0
teardown
public void kafkatest_f8006_0() throws Exception
{    if (server != null)        server.close();}
f8006
0
testConsumerWithInvalidCredentials
public void kafkatest_f8007_0()
{    Map<String, Object> props = new HashMap<>(saslClientConfigs);    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:" + server.port());    props.put(ConsumerConfig.GROUP_ID_CONFIG, "");    StringDeserializer deserializer = new StringDeserializer();    try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props, deserializer, deserializer)) {        consumer.subscribe(Arrays.asList(topic));        consumer.poll(Duration.ofSeconds(10));        fail("Expected an authentication error!");    } catch (SaslAuthenticationException e) {    // OK    } catch (Exception e) {        throw new AssertionError("Expected only an authentication error, but another error occurred.", e);    }}
f8007
0
testClientLoginManager
public void kafkatest_f8015_0() throws Exception
{    Map<String, ?> configs = Collections.singletonMap("sasl.jaas.config", dynamicPlainContext);    JaasContext dynamicContext = JaasContext.loadClientContext(configs);    JaasContext staticContext = JaasContext.loadClientContext(Collections.emptyMap());    LoginManager dynamicLogin = LoginManager.acquireLoginManager(dynamicContext, "PLAIN", DefaultLogin.class, configs);    assertEquals(dynamicPlainContext, dynamicLogin.cacheKey());    LoginManager staticLogin = LoginManager.acquireLoginManager(staticContext, "SCRAM-SHA-256", DefaultLogin.class, configs);    assertNotSame(dynamicLogin, staticLogin);    assertEquals("KafkaClient", staticLogin.cacheKey());    assertSame(dynamicLogin, LoginManager.acquireLoginManager(dynamicContext, "PLAIN", DefaultLogin.class, configs));    assertSame(staticLogin, LoginManager.acquireLoginManager(staticContext, "SCRAM-SHA-256", DefaultLogin.class, configs));    verifyLoginManagerRelease(dynamicLogin, 2, dynamicContext, configs);    verifyLoginManagerRelease(staticLogin, 2, staticContext, configs);}
f8015
0
testServerLoginManager
public void kafkatest_f8016_0() throws Exception
{    Map<String, Object> configs = new HashMap<>();    configs.put("plain.sasl.jaas.config", dynamicPlainContext);    configs.put("digest-md5.sasl.jaas.config", dynamicDigestContext);    ListenerName listenerName = new ListenerName("listener1");    JaasContext plainJaasContext = JaasContext.loadServerContext(listenerName, "PLAIN", configs);    JaasContext digestJaasContext = JaasContext.loadServerContext(listenerName, "DIGEST-MD5", configs);    JaasContext scramJaasContext = JaasContext.loadServerContext(listenerName, "SCRAM-SHA-256", configs);    LoginManager dynamicPlainLogin = LoginManager.acquireLoginManager(plainJaasContext, "PLAIN", DefaultLogin.class, configs);    assertEquals(dynamicPlainContext, dynamicPlainLogin.cacheKey());    LoginManager dynamicDigestLogin = LoginManager.acquireLoginManager(digestJaasContext, "DIGEST-MD5", DefaultLogin.class, configs);    assertNotSame(dynamicPlainLogin, dynamicDigestLogin);    assertEquals(dynamicDigestContext, dynamicDigestLogin.cacheKey());    LoginManager staticScramLogin = LoginManager.acquireLoginManager(scramJaasContext, "SCRAM-SHA-256", DefaultLogin.class, configs);    assertNotSame(dynamicPlainLogin, staticScramLogin);    assertEquals("KafkaServer", staticScramLogin.cacheKey());    assertSame(dynamicPlainLogin, LoginManager.acquireLoginManager(plainJaasContext, "PLAIN", DefaultLogin.class, configs));    assertSame(dynamicDigestLogin, LoginManager.acquireLoginManager(digestJaasContext, "DIGEST-MD5", DefaultLogin.class, configs));    assertSame(staticScramLogin, LoginManager.acquireLoginManager(scramJaasContext, "SCRAM-SHA-256", DefaultLogin.class, configs));    verifyLoginManagerRelease(dynamicPlainLogin, 2, plainJaasContext, configs);    verifyLoginManagerRelease(dynamicDigestLogin, 2, digestJaasContext, configs);    verifyLoginManagerRelease(staticScramLogin, 2, scramJaasContext, configs);}
f8016
0
verifyLoginManagerRelease
private void kafkatest_f8017_0(LoginManager loginManager, int acquireCount, JaasContext jaasContext, Map<String, ?> configs) throws Exception
{    // Release all except one reference and verify that the loginManager is still cached    for (int i = 0; i < acquireCount - 1; i++) loginManager.release();    assertSame(loginManager, LoginManager.acquireLoginManager(jaasContext, "PLAIN", DefaultLogin.class, configs));    // Release all references and verify that new LoginManager is created on next acquire    for (// release all references    int i = 0; // release all references    i < 2; // release all references    i++) loginManager.release();    LoginManager newLoginManager = LoginManager.acquireLoginManager(jaasContext, "PLAIN", DefaultLogin.class, configs);    assertNotSame(loginManager, newLoginManager);    newLoginManager.release();}
f8017
0
createSelector
private void kafkatest_f8025_0(SecurityProtocol securityProtocol, Map<String, Object> clientConfigs)
{    if (selector != null) {        selector.close();        selector = null;    }    String saslMechanism = (String) saslClientConfigs.get(SaslConfigs.SASL_MECHANISM);    this.channelBuilder = ChannelBuilders.clientChannelBuilder(securityProtocol, JaasContext.Type.CLIENT, new TestSecurityConfig(clientConfigs), null, saslMechanism, time, true);    this.selector = NetworkTestUtils.createSelector(channelBuilder, time);}
f8025
0
createEchoServer
private NioEchoServer kafkatest_f8026_0(SecurityProtocol securityProtocol) throws Exception
{    return createEchoServer(ListenerName.forSecurityProtocol(securityProtocol), securityProtocol);}
f8026
0
createEchoServer
private NioEchoServer kafkatest_f8027_0(ListenerName listenerName, SecurityProtocol securityProtocol) throws Exception
{    if (failedAuthenticationDelayMs != -1)        return NetworkTestUtils.createEchoServer(listenerName, securityProtocol, new TestSecurityConfig(saslServerConfigs), credentialCache, failedAuthenticationDelayMs, time);    else        return NetworkTestUtils.createEchoServer(listenerName, securityProtocol, new TestSecurityConfig(saslServerConfigs), credentialCache, time);}
f8027
0
testInvalidPasswordSaslPlain
public void kafkatest_f8035_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    TestJaasConfig jaasConfig = configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    jaasConfig.setClientOptions("PLAIN", TestJaasConfig.USERNAME, "invalidpassword");    server = createEchoServer(securityProtocol);    createAndCheckClientAuthenticationFailure(securityProtocol, node, "PLAIN", "Authentication failed: Invalid username or password");    server.verifyAuthenticationMetrics(0, 1);    server.verifyReauthenticationMetrics(0, 0);}
f8035
0
testInvalidUsernameSaslPlain
public void kafkatest_f8036_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    TestJaasConfig jaasConfig = configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    jaasConfig.setClientOptions("PLAIN", "invaliduser", TestJaasConfig.PASSWORD);    server = createEchoServer(securityProtocol);    createAndCheckClientAuthenticationFailure(securityProtocol, node, "PLAIN", "Authentication failed: Invalid username or password");    server.verifyAuthenticationMetrics(0, 1);    server.verifyReauthenticationMetrics(0, 0);}
f8036
0
testMissingUsernameSaslPlain
public void kafkatest_f8037_0() throws Exception
{    String node = "0";    TestJaasConfig jaasConfig = configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    jaasConfig.setClientOptions("PLAIN", null, "mypassword");    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    server = createEchoServer(securityProtocol);    createSelector(securityProtocol, saslClientConfigs);    InetSocketAddress addr = new InetSocketAddress("127.0.0.1", server.port());    try {        selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);        fail("SASL/PLAIN channel created without username");    } catch (IOException e) {        // Expected exception        assertTrue("Channels not closed", selector.channels().isEmpty());        for (SelectionKey key : selector.keys()) assertFalse("Key not cancelled", key.isValid());    }}
f8037
0
testValidSaslScramSha256
public void kafkatest_f8046_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("SCRAM-SHA-256", Arrays.asList("SCRAM-SHA-256"));    server = createEchoServer(securityProtocol);    updateScramCredentialCache(TestJaasConfig.USERNAME, TestJaasConfig.PASSWORD);    checkAuthenticationAndReauthentication(securityProtocol, "0");}
f8046
0
testValidSaslScramMechanisms
public void kafkatest_f8047_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("SCRAM-SHA-256", new ArrayList<>(ScramMechanism.mechanismNames()));    server = createEchoServer(securityProtocol);    updateScramCredentialCache(TestJaasConfig.USERNAME, TestJaasConfig.PASSWORD);    for (String mechanism : ScramMechanism.mechanismNames()) {        saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, mechanism);        createAndCheckClientConnection(securityProtocol, "node-" + mechanism);    }}
f8047
0
testInvalidPasswordSaslScram
public void kafkatest_f8048_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    TestJaasConfig jaasConfig = configureMechanisms("SCRAM-SHA-256", Arrays.asList("SCRAM-SHA-256"));    Map<String, Object> options = new HashMap<>();    options.put("username", TestJaasConfig.USERNAME);    options.put("password", "invalidpassword");    jaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_CLIENT, ScramLoginModule.class.getName(), options);    String node = "0";    server = createEchoServer(securityProtocol);    updateScramCredentialCache(TestJaasConfig.USERNAME, TestJaasConfig.PASSWORD);    createAndCheckClientAuthenticationFailure(securityProtocol, node, "SCRAM-SHA-256", null);    server.verifyAuthenticationMetrics(0, 1);    server.verifyReauthenticationMetrics(0, 0);}
f8048
0
testUnauthenticatedApiVersionsRequestOverPlaintextHandshakeVersion1
public void kafkatest_f8056_0() throws Exception
{    testUnauthenticatedApiVersionsRequest(SecurityProtocol.SASL_PLAINTEXT, (short) 1);}
f8056
0
testUnauthenticatedApiVersionsRequestOverSslHandshakeVersion0
public void kafkatest_f8057_0() throws Exception
{    testUnauthenticatedApiVersionsRequest(SecurityProtocol.SASL_SSL, (short) 0);}
f8057
0
testUnauthenticatedApiVersionsRequestOverSslHandshakeVersion1
public void kafkatest_f8058_0() throws Exception
{    testUnauthenticatedApiVersionsRequest(SecurityProtocol.SASL_SSL, (short) 1);}
f8058
0
testClientAuthenticateCallbackHandler
public void kafkatest_f8066_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    TestJaasConfig jaasConfig = configureMechanisms("PLAIN", Collections.singletonList("PLAIN"));    saslClientConfigs.put(SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS, TestClientCallbackHandler.class.getName());    // remove username, password in login context    jaasConfig.setClientOptions("PLAIN", "", "");    Map<String, Object> options = new HashMap<>();    options.put("user_" + TestClientCallbackHandler.USERNAME, TestClientCallbackHandler.PASSWORD);    jaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_SERVER, PlainLoginModule.class.getName(), options);    server = createEchoServer(securityProtocol);    createAndCheckClientConnection(securityProtocol, "good");    options.clear();    options.put("user_" + TestClientCallbackHandler.USERNAME, "invalid-password");    jaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_SERVER, PlainLoginModule.class.getName(), options);    createAndCheckClientConnectionFailure(securityProtocol, "invalid");}
f8066
0
testServerAuthenticateCallbackHandler
public void kafkatest_f8067_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    TestJaasConfig jaasConfig = configureMechanisms("PLAIN", Collections.singletonList("PLAIN"));    jaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_SERVER, PlainLoginModule.class.getName(), new HashMap<String, Object>());    String callbackPrefix = ListenerName.forSecurityProtocol(securityProtocol).saslMechanismConfigPrefix("PLAIN");    saslServerConfigs.put(callbackPrefix + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, TestServerCallbackHandler.class.getName());    server = createEchoServer(securityProtocol);    // Set client username/password to the values used by `TestServerCallbackHandler`    jaasConfig.setClientOptions("PLAIN", TestServerCallbackHandler.USERNAME, TestServerCallbackHandler.PASSWORD);    createAndCheckClientConnection(securityProtocol, "good");    // Set client username/password to the invalid values    jaasConfig.setClientOptions("PLAIN", TestJaasConfig.USERNAME, "invalid-password");    createAndCheckClientConnectionFailure(securityProtocol, "invalid");}
f8067
0
testAuthenticateCallbackHandlerMechanisms
public void kafkatest_f8068_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    TestJaasConfig jaasConfig = configureMechanisms("DIGEST-MD5", Arrays.asList("DIGEST-MD5", "PLAIN"));    // Connections should fail using the digest callback handler if listener.mechanism prefix not specified    saslServerConfigs.put("plain." + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, TestServerCallbackHandler.class);    saslServerConfigs.put("digest-md5." + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, DigestServerCallbackHandler.class);    server = createEchoServer(securityProtocol);    createAndCheckClientConnectionFailure(securityProtocol, "invalid");    // Connections should succeed using the server callback handler associated with the listener    ListenerName listener = ListenerName.forSecurityProtocol(securityProtocol);    saslServerConfigs.remove("plain." + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS);    saslServerConfigs.remove("digest-md5." + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS);    saslServerConfigs.put(listener.saslMechanismConfigPrefix("plain") + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, TestServerCallbackHandler.class);    saslServerConfigs.put(listener.saslMechanismConfigPrefix("digest-md5") + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, DigestServerCallbackHandler.class);    server = createEchoServer(securityProtocol);    // Verify that DIGEST-MD5 (currently configured for client) works with `DigestServerCallbackHandler`    createAndCheckClientConnection(securityProtocol, "good-digest-md5");    // Verify that PLAIN works with `TestServerCallbackHandler`    jaasConfig.setClientOptions("PLAIN", TestServerCallbackHandler.USERNAME, TestServerCallbackHandler.PASSWORD);    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "PLAIN");    createAndCheckClientConnection(securityProtocol, "good-plain");}
f8068
0
testServerDynamicJaasConfiguration
public void kafkatest_f8076_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "PLAIN");    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, Arrays.asList("PLAIN"));    Map<String, Object> serverOptions = new HashMap<>();    serverOptions.put("user_user1", "user1-secret");    serverOptions.put("user_user2", "user2-secret");    saslServerConfigs.put("listener.name.sasl_ssl.plain." + SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", serverOptions));    TestJaasConfig staticJaasConfig = new TestJaasConfig();    staticJaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_SERVER, PlainLoginModule.class.getName(), Collections.emptyMap());    staticJaasConfig.setClientOptions("PLAIN", "user1", "user1-secret");    Configuration.setConfiguration(staticJaasConfig);    server = createEchoServer(securityProtocol);    // Check that 'user1' can connect with static Jaas config    createAndCheckClientConnection(securityProtocol, "1");    // Check that user 'user2' can also connect with a Jaas config override    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "user2", "user2-secret"));    createAndCheckClientConnection(securityProtocol, "2");}
f8076
0
testJaasConfigurationForListener
public void kafkatest_f8077_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "PLAIN");    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, Arrays.asList("PLAIN"));    TestJaasConfig staticJaasConfig = new TestJaasConfig();    Map<String, Object> globalServerOptions = new HashMap<>();    globalServerOptions.put("user_global1", "gsecret1");    globalServerOptions.put("user_global2", "gsecret2");    staticJaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_SERVER, PlainLoginModule.class.getName(), globalServerOptions);    Map<String, Object> clientListenerServerOptions = new HashMap<>();    clientListenerServerOptions.put("user_client1", "csecret1");    clientListenerServerOptions.put("user_client2", "csecret2");    String clientJaasEntryName = "client." + TestJaasConfig.LOGIN_CONTEXT_SERVER;    staticJaasConfig.createOrUpdateEntry(clientJaasEntryName, PlainLoginModule.class.getName(), clientListenerServerOptions);    Configuration.setConfiguration(staticJaasConfig);    // Listener-specific credentials    server = createEchoServer(new ListenerName("client"), securityProtocol);    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "client1", "csecret1"));    createAndCheckClientConnection(securityProtocol, "1");    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "global1", "gsecret1"));    createAndCheckClientConnectionFailure(securityProtocol, "2");    server.close();    // Global credentials as there is no listener-specific JAAS entry    server = createEchoServer(new ListenerName("other"), securityProtocol);    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "global1", "gsecret1"));    createAndCheckClientConnection(securityProtocol, "3");    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "client1", "csecret1"));    createAndCheckClientConnectionFailure(securityProtocol, "4");}
f8077
0
oldSaslPlainPlaintextServerWithoutSaslAuthenticateHeader
public void kafkatest_f8078_0() throws Exception
{    verifySaslAuthenticateHeaderInterop(false, true, SecurityProtocol.SASL_PLAINTEXT, "PLAIN");}
f8078
0
oldSaslPlainPlaintextServerWithoutSaslAuthenticateHeaderFailure
public void kafkatest_f8086_0() throws Exception
{    verifySaslAuthenticateHeaderInteropWithFailure(false, true, SecurityProtocol.SASL_PLAINTEXT, "PLAIN");}
f8086
0
oldSaslPlainPlaintextClientWithoutSaslAuthenticateHeaderFailure
public void kafkatest_f8087_0() throws Exception
{    verifySaslAuthenticateHeaderInteropWithFailure(true, false, SecurityProtocol.SASL_PLAINTEXT, "PLAIN");}
f8087
0
oldSaslScramPlaintextServerWithoutSaslAuthenticateHeaderFailure
public void kafkatest_f8088_0() throws Exception
{    verifySaslAuthenticateHeaderInteropWithFailure(false, true, SecurityProtocol.SASL_PLAINTEXT, "SCRAM-SHA-256");}
f8088
0
testCannotReauthenticateWithDifferentMechanism
public void kafkatest_f8096_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("DIGEST-MD5", Arrays.asList("DIGEST-MD5", "PLAIN"));    configureDigestMd5ServerCallback(securityProtocol);    server = createEchoServer(securityProtocol);    String saslMechanism = (String) saslClientConfigs.get(SaslConfigs.SASL_MECHANISM);    Map<String, ?> configs = new TestSecurityConfig(saslClientConfigs).values();    this.channelBuilder = new AlternateSaslChannelBuilder(Mode.CLIENT, Collections.singletonMap(saslMechanism, JaasContext.loadClientContext(configs)), securityProtocol, null, false, saslMechanism, true, credentialCache, null, time);    this.channelBuilder.configure(configs);    // initial authentication must succeed    this.selector = NetworkTestUtils.createSelector(channelBuilder, time);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    checkClientConnection(node);    // ensure metrics are as expected before trying to re-authenticate    server.verifyAuthenticationMetrics(1, 0);    server.verifyReauthenticationMetrics(0, 0);    /*         * Now re-authenticate with a different mechanism and ensure it fails. We have         * to sleep long enough so that the next write will trigger a re-authentication.         */    delay((long) (CONNECTIONS_MAX_REAUTH_MS_VALUE * 1.1));    try {        checkClientConnection(node);        fail("Re-authentication with a different mechanism should have failed but did not");    } catch (AssertionError e) {        // ignore, expected        server.verifyAuthenticationMetrics(1, 0);        server.verifyReauthenticationMetrics(0, 1);    }}
f8096
0
testCannotReauthenticateAgainFasterThanOneSecond
public void kafkatest_f8097_0() throws Exception
{    String node = "0";    time = new MockTime();    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms(OAuthBearerLoginModule.OAUTHBEARER_MECHANISM, Arrays.asList(OAuthBearerLoginModule.OAUTHBEARER_MECHANISM));    server = createEchoServer(securityProtocol);    try {        createClientConnection(securityProtocol, node);        checkClientConnection(node);        server.verifyAuthenticationMetrics(1, 0);        server.verifyReauthenticationMetrics(0, 0);        /*             * Now sleep long enough so that the next write will cause re-authentication,             * which we expect to succeed.             */        time.sleep((long) (CONNECTIONS_MAX_REAUTH_MS_VALUE * 1.1));        checkClientConnection(node);        server.verifyAuthenticationMetrics(1, 0);        server.verifyReauthenticationMetrics(1, 0);        /*             * Now sleep long enough so that the next write will cause re-authentication,             * but this time we expect re-authentication to not occur since it has been too             * soon. The checkClientConnection() call should return an error saying it             * expected the one byte-plus-node response but got the SaslHandshakeRequest             * instead             */        time.sleep((long) (CONNECTIONS_MAX_REAUTH_MS_VALUE * 1.1));        NetworkTestUtils.checkClientConnection(selector, node, 1, 1);        fail("Expected a failure when trying to re-authenticate to quickly, but that did not occur");    } catch (AssertionError e) {        String expectedResponseTextRegex = "\\w-" + node;        String receivedResponseTextRegex = ".*" + OAuthBearerLoginModule.OAUTHBEARER_MECHANISM;        assertTrue("Should have received the SaslHandshakeRequest bytes back since we re-authenticated too quickly, " + "but instead we got our generated message echoed back, implying re-auth succeeded when it " + "should not have: " + e, e.getMessage().matches(".*\\<\\[" + expectedResponseTextRegex + "]>.*\\<\\[" + receivedResponseTextRegex + "]>"));        // unchanged        server.verifyReauthenticationMetrics(1, 0);    } finally {        selector.close();        selector = null;    }}
f8097
0
testRepeatedValidSaslPlainOverSsl
public void kafkatest_f8098_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    /*         * Make sure 85% of this value is at least 1 second otherwise it is possible for         * the client to start re-authenticating but the server does not start due to         * the 1-second minimum. If this happens the SASL HANDSHAKE request that was         * injected to start re-authentication will be echoed back to the client instead         * of the data that the client explicitly sent, and then the client will not         * recognize that data and will throw an assertion error.         */    saslServerConfigs.put(BrokerSecurityConfigs.CONNECTIONS_MAX_REAUTH_MS, new Double(1.1 * 1000L / 0.85).longValue());    server = createEchoServer(securityProtocol);    createClientConnection(securityProtocol, node);    checkClientConnection(node);    server.verifyAuthenticationMetrics(1, 0);    server.verifyReauthenticationMetrics(0, 0);    double successfulReauthentications = 0;    int desiredNumReauthentications = 5;    long startMs = Time.SYSTEM.milliseconds();    // stop after 15 seconds    long timeoutMs = startMs + 1000 * 15;    while (successfulReauthentications < desiredNumReauthentications && Time.SYSTEM.milliseconds() < timeoutMs) {        checkClientConnection(node);        successfulReauthentications = server.metricValue("successful-reauthentication-total");    }    server.verifyReauthenticationMetrics(desiredNumReauthentications, 0);}
f8098
0
buildServerAuthenticator
protected SaslServerAuthenticator kafkatest_f8106_0(Map<String, ?> configs, Map<String, AuthenticateCallbackHandler> callbackHandlers, String id, TransportLayer transportLayer, Map<String, Subject> subjects, Map<String, Long> connectionsMaxReauthMsByMechanism)
{    return new SaslServerAuthenticator(configs, callbackHandlers, id, subjects, null, listenerName, securityProtocol, transportLayer, connectionsMaxReauthMsByMechanism, time) {        @Override        protected ApiVersionsResponse apiVersionsResponse() {            List<ApiVersion> apiVersions = new ArrayList<>(ApiVersionsResponse.defaultApiVersionsResponse().apiVersions());            for (Iterator<ApiVersion> it = apiVersions.iterator(); it.hasNext(); ) {                ApiVersion apiVersion = it.next();                if (apiVersion.apiKey == ApiKeys.SASL_AUTHENTICATE.id) {                    it.remove();                    break;                }            }            return new ApiVersionsResponse(0, Errors.NONE, apiVersions);        }        @Override        protected void enableKafkaSaslAuthenticateHeaders(boolean flag) {        // Don't enable Kafka SASL_AUTHENTICATE headers        }    };}
f8106
0
apiVersionsResponse
protected ApiVersionsResponse kafkatest_f8107_0()
{    List<ApiVersion> apiVersions = new ArrayList<>(ApiVersionsResponse.defaultApiVersionsResponse().apiVersions());    for (Iterator<ApiVersion> it = apiVersions.iterator(); it.hasNext(); ) {        ApiVersion apiVersion = it.next();        if (apiVersion.apiKey == ApiKeys.SASL_AUTHENTICATE.id) {            it.remove();            break;        }    }    return new ApiVersionsResponse(0, Errors.NONE, apiVersions);}
f8107
0
enableKafkaSaslAuthenticateHeaders
protected void kafkatest_f8108_0(boolean flag)
{// Don't enable Kafka SASL_AUTHENTICATE headers}
f8108
0
configureDigestMd5ServerCallback
private void kafkatest_f8116_0(SecurityProtocol securityProtocol)
{    String callbackPrefix = ListenerName.forSecurityProtocol(securityProtocol).saslMechanismConfigPrefix("DIGEST-MD5");    saslServerConfigs.put(callbackPrefix + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, TestDigestLoginModule.DigestServerCallbackHandler.class);}
f8116
0
createSelector
private void kafkatest_f8117_0(SecurityProtocol securityProtocol, Map<String, Object> clientConfigs)
{    if (selector != null) {        selector.close();        selector = null;    }    String saslMechanism = (String) saslClientConfigs.get(SaslConfigs.SASL_MECHANISM);    this.channelBuilder = ChannelBuilders.clientChannelBuilder(securityProtocol, JaasContext.Type.CLIENT, new TestSecurityConfig(clientConfigs), null, saslMechanism, time, true);    this.selector = NetworkTestUtils.createSelector(channelBuilder, time);}
f8117
0
createEchoServer
private NioEchoServer kafkatest_f8118_0(SecurityProtocol securityProtocol) throws Exception
{    return createEchoServer(ListenerName.forSecurityProtocol(securityProtocol), securityProtocol);}
f8118
0
createAndCheckClientConnectionFailure
private ChannelState kafkatest_f8126_0(SecurityProtocol securityProtocol, String node) throws Exception
{    try {        createClientConnection(securityProtocol, node);        ChannelState finalState = NetworkTestUtils.waitForChannelClose(selector, node, ChannelState.State.AUTHENTICATION_FAILED);        return finalState;    } finally {        closeClientConnectionIfNecessary();    }}
f8126
0
checkAuthenticationAndReauthentication
private void kafkatest_f8127_0(SecurityProtocol securityProtocol, String node) throws Exception
{    try {        createClientConnection(securityProtocol, node);        checkClientConnection(node);        server.verifyAuthenticationMetrics(1, 0);        /*             * Now re-authenticate the connection. First we have to sleep long enough so             * that the next write will cause re-authentication, which we expect to succeed.             */        delay((long) (CONNECTIONS_MAX_REAUTH_MS_VALUE * 1.1));        server.verifyReauthenticationMetrics(0, 0);        checkClientConnection(node);        server.verifyReauthenticationMetrics(1, 0);    } finally {        closeClientConnectionIfNecessary();    }}
f8127
0
sendKafkaRequestReceiveResponse
private AbstractResponse kafkatest_f8128_0(String node, ApiKeys apiKey, AbstractRequest request) throws IOException
{    RequestHeader header = new RequestHeader(apiKey, request.version(), "someclient", nextCorrelationId++);    Send send = request.toSend(node, header);    selector.send(send);    ByteBuffer responseBuffer = waitForResponse();    return NetworkClient.parseResponse(responseBuffer, header);}
f8128
0
createApiVersionsRequestV0
private ApiVersionsRequest kafkatest_f8136_0()
{    return new ApiVersionsRequest.Builder((short) 0).build();}
f8136
0
updateTokenCredentialCache
private void kafkatest_f8137_0(String username, String password) throws NoSuchAlgorithmException
{    for (String mechanism : (List<String>) saslServerConfigs.get(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG)) {        ScramMechanism scramMechanism = ScramMechanism.forMechanismName(mechanism);        if (scramMechanism != null) {            ScramFormatter formatter = new ScramFormatter(scramMechanism);            ScramCredential credential = formatter.generateCredential(password, 4096);            server.tokenCache().credentialCache(scramMechanism.mechanismName()).put(username, credential);        }    }}
f8137
0
delay
private static void kafkatest_f8138_0(long delayMillis) throws InterruptedException
{    final long startTime = System.currentTimeMillis();    while ((System.currentTimeMillis() - startTime) < delayMillis) Thread.sleep(CONNECTIONS_MAX_REAUTH_MS_VALUE / 5);}
f8138
0
handle
public void kafkatest_f8148_0(Callback[] callbacks)
{    if (!configured)        throw new IllegalStateException("Login callback handler not configured");    for (Callback callback : callbacks) {        if (callback instanceof NameCallback)            ((NameCallback) callback).setName(TestJaasConfig.USERNAME);        else if (callback instanceof PasswordCallback)            ((PasswordCallback) callback).setPassword(TestJaasConfig.PASSWORD.toCharArray());    }}
f8148
0
initialize
public void kafkatest_f8150_0(Subject subject, CallbackHandler callbackHandler, Map<String, ?> sharedState, Map<String, ?> options)
{    try {        NameCallback nameCallback = new NameCallback("name:");        PasswordCallback passwordCallback = new PasswordCallback("password:", false);        callbackHandler.handle(new Callback[] { nameCallback, passwordCallback });        subject.getPublicCredentials().add(nameCallback.getName());        subject.getPrivateCredentials().add(new String(passwordCallback.getPassword()));    } catch (Exception e) {        throw new SaslAuthenticationException("Login initialization failed", e);    }}
f8150
0
handle
public void kafkatest_f8151_0(Callback[] callbacks) throws IOException, UnsupportedCallbackException
{    DELEGATE.handle(callbacks);    // now change any returned token to have a different principal name    if (callbacks.length > 0)        for (Callback callback : callbacks) {            if (callback instanceof OAuthBearerTokenCallback) {                OAuthBearerTokenCallback oauthBearerTokenCallback = (OAuthBearerTokenCallback) callback;                OAuthBearerToken token = oauthBearerTokenCallback.token();                if (token != null) {                    String changedPrincipalNameToUse = token.principalName() + String.valueOf(++numInvocations);                    String headerJson = "{" + claimOrHeaderJsonText("alg", "none") + "}";                    /*                             * Use a short lifetime so the background refresh thread replaces it before we                             * re-authenticate                             */                    String lifetimeSecondsValueToUse = "1";                    String claimsJson;                    try {                        claimsJson = String.format("{%s,%s,%s}", expClaimText(Long.parseLong(lifetimeSecondsValueToUse)), claimOrHeaderJsonText("iat", time.milliseconds() / 1000.0), claimOrHeaderJsonText("sub", changedPrincipalNameToUse));                    } catch (NumberFormatException e) {                        throw new OAuthBearerConfigException(e.getMessage());                    }                    try {                        Encoder urlEncoderNoPadding = Base64.getUrlEncoder().withoutPadding();                        OAuthBearerUnsecuredJws jws = new OAuthBearerUnsecuredJws(String.format("%s.%s.", urlEncoderNoPadding.encodeToString(headerJson.getBytes(StandardCharsets.UTF_8)), urlEncoderNoPadding.encodeToString(claimsJson.getBytes(StandardCharsets.UTF_8))), "sub", "scope");                        oauthBearerTokenCallback.token(jws);                    } catch (OAuthBearerIllegalTokenException e) {                        // occurs if the principal claim doesn't exist or has an empty value                        throw new OAuthBearerConfigException(e.getMessage(), e);                    }                }            }        }}
f8151
0
testOversizeRequest
public void kafkatest_f8159_0() throws IOException
{    TransportLayer transportLayer = mock(TransportLayer.class);    Map<String, ?> configs = Collections.singletonMap(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, Collections.singletonList(SCRAM_SHA_256.mechanismName()));    SaslServerAuthenticator authenticator = setupAuthenticator(configs, transportLayer, SCRAM_SHA_256.mechanismName());    when(transportLayer.read(any(ByteBuffer.class))).then(invocation -> {        invocation.<ByteBuffer>getArgument(0).putInt(SaslServerAuthenticator.MAX_RECEIVE_SIZE + 1);        return 4;    });    authenticator.authenticate();    verify(transportLayer).read(any(ByteBuffer.class));}
f8159
0
testUnexpectedRequestType
public void kafkatest_f8160_0() throws IOException
{    TransportLayer transportLayer = mock(TransportLayer.class);    Map<String, ?> configs = Collections.singletonMap(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, Collections.singletonList(SCRAM_SHA_256.mechanismName()));    SaslServerAuthenticator authenticator = setupAuthenticator(configs, transportLayer, SCRAM_SHA_256.mechanismName());    final RequestHeader header = new RequestHeader(ApiKeys.METADATA, (short) 0, "clientId", 13243);    final Struct headerStruct = header.toStruct();    when(transportLayer.read(any(ByteBuffer.class))).then(invocation -> {        invocation.<ByteBuffer>getArgument(0).putInt(headerStruct.sizeOf());        return 4;    }).then(invocation -> {        // serialize only the request header. the authenticator should not parse beyond this        headerStruct.writeTo(invocation.getArgument(0));        return headerStruct.sizeOf();    });    try {        authenticator.authenticate();        fail("Expected authenticate() to raise an exception");    } catch (IllegalSaslStateException e) {    // expected exception    }    verify(transportLayer, times(2)).read(any(ByteBuffer.class));}
f8160
0
setupAuthenticator
private SaslServerAuthenticator kafkatest_f8161_0(Map<String, ?> configs, TransportLayer transportLayer, String mechanism) throws IOException
{    TestJaasConfig jaasConfig = new TestJaasConfig();    jaasConfig.addEntry("jaasContext", PlainLoginModule.class.getName(), new HashMap<String, Object>());    Map<String, JaasContext> jaasContexts = Collections.singletonMap(mechanism, new JaasContext("jaasContext", JaasContext.Type.SERVER, jaasConfig, null));    Map<String, Subject> subjects = Collections.singletonMap(mechanism, new Subject());    Map<String, AuthenticateCallbackHandler> callbackHandlers = Collections.singletonMap(mechanism, new SaslServerCallbackHandler());    return new SaslServerAuthenticator(configs, callbackHandlers, "node", subjects, null, new ListenerName("ssl"), SecurityProtocol.SASL_SSL, transportLayer, Collections.emptyMap(), Time.SYSTEM);}
f8161
0
getAppConfigurationEntry
public AppConfigurationEntry[] kafkatest_f8171_0(String name)
{    return entryMap.get(name);}
f8171
0
loginModule
private static String kafkatest_f8172_0(String mechanism)
{    String loginModule;    switch(mechanism) {        case "PLAIN":            loginModule = PlainLoginModule.class.getName();            break;        case "DIGEST-MD5":            loginModule = TestDigestLoginModule.class.getName();            break;        case "OAUTHBEARER":            loginModule = OAuthBearerLoginModule.class.getName();            break;        default:            if (ScramMechanism.isScram(mechanism))                loginModule = ScramLoginModule.class.getName();            else                throw new IllegalArgumentException("Unsupported mechanism " + mechanism);    }    return loginModule;}
f8172
0
defaultClientOptions
public static Map<String, Object> kafkatest_f8173_0(String mechanism)
{    switch(mechanism) {        case "OAUTHBEARER":            Map<String, Object> options = new HashMap<>();            options.put("unsecuredLoginStringClaim_sub", USERNAME);            return options;        default:            return defaultClientOptions();    }}
f8173
0
testMultipleOptions
public void kafkatest_f8181_0() throws Exception
{    Map<String, Object> options = new HashMap<>();    for (int i = 0; i < 10; i++) options.put("propName" + i, "propValue" + i);    checkConfiguration("test.testMultipleOptions", LoginModuleControlFlag.SUFFICIENT, options);}
f8181
0
testQuotedOptionValue
public void kafkatest_f8182_0() throws Exception
{    Map<String, Object> options = new HashMap<>();    options.put("propName", "prop value");    options.put("propName2", "value1 = 1, value2 = 2");    String config = String.format("test.testQuotedOptionValue required propName=\"%s\" propName2=\"%s\";", options.get("propName"), options.get("propName2"));    checkConfiguration(config, "test.testQuotedOptionValue", LoginModuleControlFlag.REQUIRED, options);}
f8182
0
testQuotedOptionName
public void kafkatest_f8183_0() throws Exception
{    Map<String, Object> options = new HashMap<>();    options.put("prop name", "propValue");    String config = "test.testQuotedOptionName required \"prop name\"=propValue;";    checkConfiguration(config, "test.testQuotedOptionName", LoginModuleControlFlag.REQUIRED, options);}
f8183
0
testNumericOptionWithQuotes
public void kafkatest_f8191_0() throws Exception
{    Map<String, Object> options = new HashMap<>();    options.put("option1", "3");    String config = "test.testNumericOptionWithQuotes required option1=\"3\";";    checkConfiguration(config, "test.testNumericOptionWithQuotes", LoginModuleControlFlag.REQUIRED, options);}
f8191
0
testLoadForServerWithListenerNameOverride
public void kafkatest_f8192_0() throws IOException
{    writeConfiguration(Arrays.asList("KafkaServer { test.LoginModuleDefault required; };", "plaintext.KafkaServer { test.LoginModuleOverride requisite; };"));    JaasContext context = JaasContext.loadServerContext(new ListenerName("plaintext"), "SOME-MECHANISM", Collections.emptyMap());    assertEquals("plaintext.KafkaServer", context.name());    assertEquals(JaasContext.Type.SERVER, context.type());    assertEquals(1, context.configurationEntries().size());    checkEntry(context.configurationEntries().get(0), "test.LoginModuleOverride", LoginModuleControlFlag.REQUISITE, Collections.emptyMap());}
f8192
0
testLoadForServerWithListenerNameAndFallback
public void kafkatest_f8193_0() throws IOException
{    writeConfiguration(Arrays.asList("KafkaServer { test.LoginModule required; };", "other.KafkaServer { test.LoginModuleOther requisite; };"));    JaasContext context = JaasContext.loadServerContext(new ListenerName("plaintext"), "SOME-MECHANISM", Collections.emptyMap());    assertEquals("KafkaServer", context.name());    assertEquals(JaasContext.Type.SERVER, context.type());    assertEquals(1, context.configurationEntries().size());    checkEntry(context.configurationEntries().get(0), "test.LoginModule", LoginModuleControlFlag.REQUIRED, Collections.emptyMap());}
f8193
0
checkEntry
private void kafkatest_f8201_0(AppConfigurationEntry entry, String loginModule, LoginModuleControlFlag controlFlag, Map<String, ?> options)
{    assertEquals(loginModule, entry.getLoginModuleName());    assertEquals(controlFlag, entry.getControlFlag());    assertEquals(options, entry.getOptions());}
f8201
0
checkConfiguration
private void kafkatest_f8202_0(String jaasConfigProp, String loginModule, LoginModuleControlFlag controlFlag, Map<String, Object> options) throws Exception
{    AppConfigurationEntry dynamicEntry = configurationEntry(JaasContext.Type.CLIENT, jaasConfigProp);    checkEntry(dynamicEntry, loginModule, controlFlag, options);    assertNull("Static configuration updated", Configuration.getConfiguration().getAppConfigurationEntry(JaasContext.Type.CLIENT.name()));    writeConfiguration(JaasContext.Type.SERVER.name(), jaasConfigProp);    AppConfigurationEntry staticEntry = configurationEntry(JaasContext.Type.SERVER, null);    checkEntry(staticEntry, loginModule, controlFlag, options);}
f8202
0
checkInvalidConfiguration
private void kafkatest_f8203_0(String jaasConfigProp) throws IOException
{    try {        writeConfiguration(JaasContext.Type.SERVER.name(), jaasConfigProp);        AppConfigurationEntry entry = configurationEntry(JaasContext.Type.SERVER, null);        fail("Invalid JAAS configuration file didn't throw exception, entry=" + entry);    } catch (SecurityException e) {    // Expected exception    }    try {        AppConfigurationEntry entry = configurationEntry(JaasContext.Type.CLIENT, jaasConfigProp);        fail("Invalid JAAS configuration property didn't throw exception, entry=" + entry);    } catch (IllegalArgumentException e) {    // Expected exception    }}
f8203
0
getCreateMs
public long kafkatest_f8211_0()
{    return time.milliseconds();}
f8211
0
getExpireTimeMs
public long kafkatest_f8212_0()
{    return time.milliseconds() + lifetimeMillis;}
f8212
0
createNewExpiringCredential
public void kafkatest_f8213_0()
{    if (!clientReloginAllowedBeforeLogout)        /*                 * Was preceded by logout                 */        expiringCredential = internalNewExpiringCredential();    else {        boolean initialLogin = expiringCredential == null;        if (initialLogin)            // no logout immediately after the initial login            this.expiringCredential = internalNewExpiringCredential();        else            /*                     * This is at least the second invocation of login; we will move the credential                     * over upon logout, which should be invoked next                     */            this.tmpExpiringCredential = internalNewExpiringCredential();    }}
f8213
0
toString
public String kafkatest_f8221_0()
{    return String.format("startTimeMs=%d, expireTimeMs=%d, absoluteLastRefreshTimeMs=%s", startTimeMs(), expireTimeMs(), absoluteLastRefreshTimeMs());}
f8221
0
login
public void kafkatest_f8222_0() throws LoginException
{    /*             * Here is where we get the functionality of a mock while simultaneously             * performing the creation of an expiring credential             */    mockLoginContext.login();    testExpiringCredentialRefreshingLogin.createNewExpiringCredential();}
f8222
0
logout
public void kafkatest_f8223_0() throws LoginException
{    /*             * Here is where we get the functionality of a mock while simultaneously             * performing the removal of an expiring credential             */    mockLoginContext.logout();    testExpiringCredentialRefreshingLogin.clearExpiringCredential();}
f8223
0
refresherThreadDone
public void kafkatest_f8231_0()
{    refresherThreadDoneFuture.complete(null);}
f8231
0
refresherThreadStartedFuture
public Future<?> kafkatest_f8232_0()
{    return refresherThreadStartedFuture;}
f8232
0
refresherThreadDoneFuture
public Future<?> kafkatest_f8233_0()
{    return refresherThreadDoneFuture;}
f8233
0
addWaiters
private static List<KafkaFutureImpl<Long>> kafkatest_f8241_0(MockScheduler mockScheduler, long refreshEveryMillis, int numWaiters)
{    List<KafkaFutureImpl<Long>> retvalWaiters = new ArrayList<>(numWaiters);    for (int i = 1; i <= numWaiters; ++i) {        KafkaFutureImpl<Long> waiter = new KafkaFutureImpl<Long>();        mockScheduler.addWaiter(i * refreshEveryMillis, waiter);        retvalWaiters.add(waiter);    }    return retvalWaiters;}
f8241
0
refreshConfigThatPerformsReloginEveryGivenPercentageOfLifetime
private static ExpiringCredentialRefreshConfig kafkatest_f8242_0(double refreshWindowFactor, short minPeriodSeconds, short bufferSeconds, boolean clientReloginAllowedBeforeLogout)
{    Map<Object, Object> configs = new HashMap<>();    configs.put(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR, refreshWindowFactor);    configs.put(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER, 0);    configs.put(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS, minPeriodSeconds);    configs.put(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS, bufferSeconds);    return new ExpiringCredentialRefreshConfig(new ConfigDef().withClientSaslSupport().parse(configs), clientReloginAllowedBeforeLogout);}
f8242
0
testBuildClientResponseToBytes
public void kafkatest_f8243_0() throws Exception
{    String expectedMesssage = "n,,\u0001auth=Bearer 123.345.567\u0001nineteen=42\u0001\u0001";    Map<String, String> extensions = new HashMap<>();    extensions.put("nineteen", "42");    OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse("123.345.567", new SaslExtensions(extensions));    String message = new String(response.toBytes(), StandardCharsets.UTF_8);    assertEquals(expectedMesssage, message);}
f8243
0
testNoExtensionsFromTokenAndNullExtensions
public void kafkatest_f8251_0() throws Exception
{    OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse("token", null);    assertTrue(response.extensions().map().isEmpty());}
f8251
0
testValidateNullExtensions
public void kafkatest_f8252_0() throws Exception
{    OAuthBearerClientInitialResponse.validateExtensions(null);}
f8252
0
configured
public boolean kafkatest_f8253_0()
{    return configured;}
f8253
0
testAttachesExtensionsToFirstClientMessage
public void kafkatest_f8262_0() throws Exception
{    String expectedToken = new String(new OAuthBearerClientInitialResponse("", testExtensions).toBytes(), StandardCharsets.UTF_8);    OAuthBearerSaslClient client = new OAuthBearerSaslClient(new ExtensionsCallbackHandler(false));    String message = new String(client.evaluateChallenge("".getBytes()), StandardCharsets.UTF_8);    assertEquals(expectedToken, message);}
f8262
0
testNoExtensionsDoesNotAttachAnythingToFirstClientMessage
public void kafkatest_f8263_0() throws Exception
{    TEST_PROPERTIES.clear();    testExtensions = new SaslExtensions(TEST_PROPERTIES);    String expectedToken = new String(new OAuthBearerClientInitialResponse("", new SaslExtensions(TEST_PROPERTIES)).toBytes(), StandardCharsets.UTF_8);    OAuthBearerSaslClient client = new OAuthBearerSaslClient(new ExtensionsCallbackHandler(false));    String message = new String(client.evaluateChallenge("".getBytes()), StandardCharsets.UTF_8);    assertEquals(expectedToken, message);}
f8263
0
testWrapsExtensionsCallbackHandlingErrorInSaslExceptionInFirstClientMessage
public void kafkatest_f8264_0()
{    OAuthBearerSaslClient client = new OAuthBearerSaslClient(new ExtensionsCallbackHandler(true));    try {        client.evaluateChallenge("".getBytes());        fail("Should have failed with " + SaslException.class.getName());    } catch (SaslException e) {        // assert it has caught our expected exception        assertEquals(ConfigException.class, e.getCause().getClass());        assertEquals(errorMessage, e.getCause().getMessage());    }}
f8264
0
handle
public void kafkatest_f8272_0(Callback[] callbacks) throws UnsupportedCallbackException
{    for (Callback callback : callbacks) {        if (callback instanceof OAuthBearerValidatorCallback) {            OAuthBearerValidatorCallback validationCallback = (OAuthBearerValidatorCallback) callback;            validationCallback.token(new OAuthBearerTokenMock());        } else if (callback instanceof OAuthBearerExtensionsValidatorCallback) {            OAuthBearerExtensionsValidatorCallback extensionsCallback = (OAuthBearerExtensionsValidatorCallback) callback;            extensionsCallback.error("firstKey", "is not valid");            extensionsCallback.error("secondKey", "is not valid either");        } else            throw new UnsupportedCallbackException(callback);    }}
f8272
0
authorizatonIdEqualsAuthenticationId
public void kafkatest_f8273_0() throws Exception
{    byte[] nextChallenge = saslServer.evaluateResponse(clientInitialResponse(USER));    assertTrue("Next challenge is not empty", nextChallenge.length == 0);}
f8273
0
authorizatonIdNotEqualsAuthenticationId
public void kafkatest_f8274_0() throws Exception
{    saslServer.evaluateResponse(clientInitialResponse(USER + "x"));}
f8274
0
validCompactSerialization
public void kafkatest_f8282_0()
{    String subject = "foo";    long issuedAt = 100;    long expirationTime = issuedAt + 60 * 60;    List<String> scope = Arrays.asList("scopeValue1", "scopeValue2");    String validCompactSerialization = compactSerialization(subject, issuedAt, expirationTime, scope);    OAuthBearerUnsecuredJws jws = new OAuthBearerUnsecuredJws(validCompactSerialization, "sub", "scope");    assertEquals(1, jws.header().size());    assertEquals("none", jws.header().get("alg"));    assertEquals(4, jws.claims().size());    assertEquals(subject, jws.claims().get("sub"));    assertEquals(subject, jws.principalName());    assertEquals(issuedAt, Number.class.cast(jws.claims().get("iat")).longValue());    assertEquals(expirationTime, Number.class.cast(jws.claims().get("exp")).longValue());    assertEquals(expirationTime * 1000, jws.lifetimeMs());    assertEquals(scope, jws.claims().get("scope"));    assertEquals(new HashSet<>(scope), jws.scope());    assertEquals(3, jws.splits().size());    assertEquals(validCompactSerialization.split("\\.")[0], jws.splits().get(0));    assertEquals(validCompactSerialization.split("\\.")[1], jws.splits().get(1));    assertEquals("", jws.splits().get(2));}
f8282
0
missingPrincipal
public void kafkatest_f8283_0()
{    String subject = null;    long issuedAt = 100;    Long expirationTime = null;    List<String> scope = Arrays.asList("scopeValue1", "scopeValue2");    String validCompactSerialization = compactSerialization(subject, issuedAt, expirationTime, scope);    new OAuthBearerUnsecuredJws(validCompactSerialization, "sub", "scope");}
f8283
0
blankPrincipalName
public void kafkatest_f8284_0()
{    String subject = "   ";    long issuedAt = 100;    long expirationTime = issuedAt + 60 * 60;    List<String> scope = Arrays.asList("scopeValue1", "scopeValue2");    String validCompactSerialization = compactSerialization(subject, issuedAt, expirationTime, scope);    new OAuthBearerUnsecuredJws(validCompactSerialization, "sub", "scope");}
f8284
0
throwsErrorOnInvalidExtensionName
public void kafkatest_f8292_0() throws IOException, UnsupportedCallbackException
{    Map<String, String> options = new HashMap<>();    options.put("unsecuredLoginExtension_test.Id", "1");    OAuthBearerUnsecuredLoginCallbackHandler callbackHandler = createCallbackHandler(options, new MockTime());    SaslExtensionsCallback callback = new SaslExtensionsCallback();    callbackHandler.handle(new Callback[] { callback });}
f8292
0
throwsErrorOnInvalidExtensionValue
public void kafkatest_f8293_0() throws IOException, UnsupportedCallbackException
{    Map<String, String> options = new HashMap<>();    options.put("unsecuredLoginExtension_testId", "Çalifornia");    OAuthBearerUnsecuredLoginCallbackHandler callbackHandler = createCallbackHandler(options, new MockTime());    SaslExtensionsCallback callback = new SaslExtensionsCallback();    callbackHandler.handle(new Callback[] { callback });}
f8293
0
minimalToken
public void kafkatest_f8294_0() throws IOException, UnsupportedCallbackException
{    Map<String, String> options = new HashMap<>();    String user = "user";    options.put("unsecuredLoginStringClaim_sub", user);    MockTime mockTime = new MockTime();    OAuthBearerUnsecuredLoginCallbackHandler callbackHandler = createCallbackHandler(options, mockTime);    OAuthBearerTokenCallback callback = new OAuthBearerTokenCallback();    callbackHandler.handle(new Callback[] { callback });    OAuthBearerUnsecuredJws jws = (OAuthBearerUnsecuredJws) callback.token();    assertNotNull("create token failed", jws);    long startMs = mockTime.milliseconds();    confirmCorrectValues(jws, user, startMs, 1000 * 60 * 60);    assertEquals(new HashSet<>(Arrays.asList("sub", "iat", "exp")), jws.claims().keySet());}
f8294
0
missingRequiredScope
public void kafkatest_f8302_0() throws IOException, UnsupportedCallbackException
{    String claimsJson = "{" + SUB_CLAIM_TEXT + comma(EXPIRATION_TIME_CLAIM_TEXT) + comma(SCOPE_CLAIM_TEXT) + "}";    confirmFailsValidation(UNSECURED_JWT_HEADER_JSON, claimsJson, MODULE_OPTIONS_MAP_REQUIRE_ADDITIONAL_SCOPE, "[scope1, scope2]");}
f8302
0
confirmFailsValidation
private static void kafkatest_f8303_0(String headerJson, String claimsJson, Map<String, String> moduleOptionsMap) throws OAuthBearerConfigException, OAuthBearerIllegalTokenException, IOException, UnsupportedCallbackException
{    confirmFailsValidation(headerJson, claimsJson, moduleOptionsMap, null);}
f8303
0
confirmFailsValidation
private static void kafkatest_f8304_0(String headerJson, String claimsJson, Map<String, String> moduleOptionsMap, String optionalFailureScope) throws OAuthBearerConfigException, OAuthBearerIllegalTokenException
{    Object validationResultObj = validationResult(headerJson, claimsJson, moduleOptionsMap);    assertTrue(validationResultObj instanceof OAuthBearerValidatorCallback);    OAuthBearerValidatorCallback callback = (OAuthBearerValidatorCallback) validationResultObj;    assertNull(callback.token());    assertNull(callback.errorOpenIDConfiguration());    if (optionalFailureScope == null) {        assertEquals("invalid_token", callback.errorStatus());        assertNull(callback.errorScope());    } else {        assertEquals("insufficient_scope", callback.errorStatus());        assertEquals(optionalFailureScope, callback.errorScope());    }}
f8304
0
validateIssuedAt
public void kafkatest_f8312_0()
{    long nowMs = TIME.milliseconds();    double nowClaimValue = ((double) nowMs) / 1000;    for (boolean exists : new boolean[] { true, false }) {        StringBuilder sb = new StringBuilder("{");        appendJsonText(sb, "exp", nowClaimValue);        appendCommaJsonText(sb, "sub", "principalName");        if (exists)            appendCommaJsonText(sb, "iat", nowClaimValue);        sb.append("}");        String compactSerialization = HEADER_COMPACT_SERIALIZATION + Base64.getUrlEncoder().withoutPadding().encodeToString(sb.toString().getBytes(StandardCharsets.UTF_8)) + ".";        OAuthBearerUnsecuredJws testJwt = new OAuthBearerUnsecuredJws(compactSerialization, "sub", "scope");        for (boolean required : new boolean[] { true, false }) {            for (int allowableClockSkewMs : new int[] { 0, 5, 10, 20 }) {                for (long whenCheckOffsetMs : new long[] { -10, 0, 10 }) {                    long whenCheckMs = nowMs + whenCheckOffsetMs;                    OAuthBearerValidationResult result = OAuthBearerValidationUtils.validateIssuedAt(testJwt, required, whenCheckMs, allowableClockSkewMs);                    if (required && !exists)                        assertTrue("useErrorValue || required && !exists", isFailureWithMessageAndNoFailureScope(result));                    else if (!required && !exists)                        assertTrue("!required && !exists", isSuccess(result));                    else if (// issued in future                    nowClaimValue * 1000 > whenCheckMs + allowableClockSkewMs)                        assertTrue(assertionFailureMessage(nowClaimValue, allowableClockSkewMs, whenCheckMs), isFailureWithMessageAndNoFailureScope(result));                    else                        assertTrue(assertionFailureMessage(nowClaimValue, allowableClockSkewMs, whenCheckMs), isSuccess(result));                }            }        }    }}
f8312
0
validateExpirationTime
public void kafkatest_f8313_0()
{    long nowMs = TIME.milliseconds();    double nowClaimValue = ((double) nowMs) / 1000;    StringBuilder sb = new StringBuilder("{");    appendJsonText(sb, "exp", nowClaimValue);    appendCommaJsonText(sb, "sub", "principalName");    sb.append("}");    String compactSerialization = HEADER_COMPACT_SERIALIZATION + Base64.getUrlEncoder().withoutPadding().encodeToString(sb.toString().getBytes(StandardCharsets.UTF_8)) + ".";    OAuthBearerUnsecuredJws testJwt = new OAuthBearerUnsecuredJws(compactSerialization, "sub", "scope");    for (int allowableClockSkewMs : new int[] { 0, 5, 10, 20 }) {        for (long whenCheckOffsetMs : new long[] { -10, 0, 10 }) {            long whenCheckMs = nowMs + whenCheckOffsetMs;            OAuthBearerValidationResult result = OAuthBearerValidationUtils.validateExpirationTime(testJwt, whenCheckMs, allowableClockSkewMs);            if (// expired            whenCheckMs - allowableClockSkewMs >= nowClaimValue * 1000)                assertTrue(assertionFailureMessage(nowClaimValue, allowableClockSkewMs, whenCheckMs), isFailureWithMessageAndNoFailureScope(result));            else                assertTrue(assertionFailureMessage(nowClaimValue, allowableClockSkewMs, whenCheckMs), isSuccess(result));        }    }}
f8313
0
validateExpirationTimeAndIssuedAtConsistency
public void kafkatest_f8314_0() throws OAuthBearerIllegalTokenException
{    long nowMs = TIME.milliseconds();    double nowClaimValue = ((double) nowMs) / 1000;    for (boolean issuedAtExists : new boolean[] { true, false }) {        if (!issuedAtExists) {            StringBuilder sb = new StringBuilder("{");            appendJsonText(sb, "exp", nowClaimValue);            appendCommaJsonText(sb, "sub", "principalName");            sb.append("}");            String compactSerialization = HEADER_COMPACT_SERIALIZATION + Base64.getUrlEncoder().withoutPadding().encodeToString(sb.toString().getBytes(StandardCharsets.UTF_8)) + ".";            OAuthBearerUnsecuredJws testJwt = new OAuthBearerUnsecuredJws(compactSerialization, "sub", "scope");            assertTrue(isSuccess(OAuthBearerValidationUtils.validateTimeConsistency(testJwt)));        } else            for (int expirationTimeOffset = -1; expirationTimeOffset <= 1; ++expirationTimeOffset) {                StringBuilder sb = new StringBuilder("{");                appendJsonText(sb, "iat", nowClaimValue);                appendCommaJsonText(sb, "exp", nowClaimValue + expirationTimeOffset);                appendCommaJsonText(sb, "sub", "principalName");                sb.append("}");                String compactSerialization = HEADER_COMPACT_SERIALIZATION + Base64.getUrlEncoder().withoutPadding().encodeToString(sb.toString().getBytes(StandardCharsets.UTF_8)) + ".";                OAuthBearerUnsecuredJws testJwt = new OAuthBearerUnsecuredJws(compactSerialization, "sub", "scope");                OAuthBearerValidationResult result = OAuthBearerValidationUtils.validateTimeConsistency(testJwt);                if (expirationTimeOffset <= 0)                    assertTrue(isFailureWithMessageAndNoFailureScope(result));                else                    assertTrue(isSuccess(result));            }    }}
f8314
0
appendJsonText
private static void kafkatest_f8322_0(StringBuilder sb, String claimName, Number claimValue)
{    sb.append(QUOTE).append(escape(claimName)).append(QUOTE).append(":").append(claimValue);}
f8322
0
escape
private static String kafkatest_f8323_0(String jsonStringValue)
{    return jsonStringValue.replace("\"", "\\\"").replace("\\", "\\\\");}
f8323
0
scopeJson
private static String kafkatest_f8324_0(List<String> scope)
{    StringBuilder scopeJsonBuilder = new StringBuilder("\"scope\":[");    int initialLength = scopeJsonBuilder.length();    for (String scopeValue : scope) {        if (scopeJsonBuilder.length() > initialLength)            scopeJsonBuilder.append(',');        scopeJsonBuilder.append('"').append(scopeValue).append('"');    }    scopeJsonBuilder.append(']');    return scopeJsonBuilder.toString();}
f8324
0
handleCallback
private void kafkatest_f8332_0(OAuthBearerTokenCallback callback) throws IOException
{    if (callback.token() != null)        throw new IllegalArgumentException("Callback had a token already");    if (tokens.length > index)        callback.token(tokens[index++]);    else        throw new IOException("no more tokens");}
f8332
0
handleExtensionsCallback
private void kafkatest_f8333_0(SaslExtensionsCallback callback) throws IOException, UnsupportedCallbackException
{    if (extensions.length > extensionsIndex) {        SaslExtensions extension = extensions[extensionsIndex++];        if (extension == RAISE_UNSUPPORTED_CB_EXCEPTION_FLAG) {            throw new UnsupportedCallbackException(callback);        }        callback.extensions(extension);    } else        throw new IOException("no more extensions");}
f8333
0
login1Commit1Login2Commit2Logout1Login3Commit3Logout2
public void kafkatest_f8334_0() throws LoginException
{    /*         * Invoke login()/commit() on loginModule1; invoke login/commit() on         * loginModule2; invoke logout() on loginModule1; invoke login()/commit() on         * loginModule3; invoke logout() on loginModule2         */    Subject subject = new Subject();    Set<Object> privateCredentials = subject.getPrivateCredentials();    Set<Object> publicCredentials = subject.getPublicCredentials();    // Create callback handler    OAuthBearerToken[] tokens = new OAuthBearerToken[] { mock(OAuthBearerToken.class), mock(OAuthBearerToken.class), mock(OAuthBearerToken.class) };    SaslExtensions[] extensions = new SaslExtensions[] { mock(SaslExtensions.class), mock(SaslExtensions.class), mock(SaslExtensions.class) };    TestCallbackHandler testTokenCallbackHandler = new TestCallbackHandler(tokens, extensions);    // Create login modules    OAuthBearerLoginModule loginModule1 = new OAuthBearerLoginModule();    loginModule1.initialize(subject, testTokenCallbackHandler, Collections.emptyMap(), Collections.emptyMap());    OAuthBearerLoginModule loginModule2 = new OAuthBearerLoginModule();    loginModule2.initialize(subject, testTokenCallbackHandler, Collections.emptyMap(), Collections.emptyMap());    OAuthBearerLoginModule loginModule3 = new OAuthBearerLoginModule();    loginModule3.initialize(subject, testTokenCallbackHandler, Collections.emptyMap(), Collections.emptyMap());    // Should start with nothing    assertEquals(0, privateCredentials.size());    assertEquals(0, publicCredentials.size());    loginModule1.login();    // Should still have nothing until commit() is called    assertEquals(0, privateCredentials.size());    assertEquals(0, publicCredentials.size());    loginModule1.commit();    // Now we should have the first token and extensions    assertEquals(1, privateCredentials.size());    assertEquals(1, publicCredentials.size());    assertSame(tokens[0], privateCredentials.iterator().next());    assertSame(extensions[0], publicCredentials.iterator().next());    // Now login on loginModule2 to get the second token    // loginModule2 does not support the extensions callback and will raise UnsupportedCallbackException    loginModule2.login();    // Should still have just the first token and extensions    assertEquals(1, privateCredentials.size());    assertEquals(1, publicCredentials.size());    assertSame(tokens[0], privateCredentials.iterator().next());    assertSame(extensions[0], publicCredentials.iterator().next());    loginModule2.commit();    // Should have the first and second tokens at this point    assertEquals(2, privateCredentials.size());    assertEquals(2, publicCredentials.size());    Iterator<Object> iterator = privateCredentials.iterator();    Iterator<Object> publicIterator = publicCredentials.iterator();    assertNotSame(tokens[2], iterator.next());    assertNotSame(tokens[2], iterator.next());    assertNotSame(extensions[2], publicIterator.next());    assertNotSame(extensions[2], publicIterator.next());    // finally logout() on loginModule1    loginModule1.logout();    // Now we should have just the second token and extension    assertEquals(1, privateCredentials.size());    assertEquals(1, publicCredentials.size());    assertSame(tokens[1], privateCredentials.iterator().next());    assertSame(extensions[1], publicCredentials.iterator().next());    // Now login on loginModule3 to get the third token    loginModule3.login();    // Should still have just the second token and extensions    assertEquals(1, privateCredentials.size());    assertEquals(1, publicCredentials.size());    assertSame(tokens[1], privateCredentials.iterator().next());    assertSame(extensions[1], publicCredentials.iterator().next());    loginModule3.commit();    // Should have the second and third tokens at this point    assertEquals(2, privateCredentials.size());    assertEquals(2, publicCredentials.size());    iterator = privateCredentials.iterator();    publicIterator = publicCredentials.iterator();    assertNotSame(tokens[0], iterator.next());    assertNotSame(tokens[0], iterator.next());    assertNotSame(extensions[0], publicIterator.next());    assertNotSame(extensions[0], publicIterator.next());    // finally logout() on loginModule2    loginModule2.logout();    // Now we should have just the third token    assertEquals(1, privateCredentials.size());    assertEquals(1, publicCredentials.size());    assertSame(tokens[2], privateCredentials.iterator().next());    assertSame(extensions[2], publicCredentials.iterator().next());    verifyZeroInteractions((Object[]) tokens);    verifyZeroInteractions((Object[]) extensions);}
f8334
0
scope
public Set<String> kafkatest_f8342_0()
{    return null;}
f8342
0
principalName
public String kafkatest_f8343_0()
{    return null;}
f8343
0
lifetimeMs
public long kafkatest_f8344_0()
{    return lifetimeMillis;}
f8344
0
lifetimeMs
public long kafkatest_f8352_0()
{    return 0;}
f8352
0
testError
public void kafkatest_f8353_0()
{    String errorCode = "errorCode";    String errorDescription = "errorDescription";    String errorUri = "errorUri";    OAuthBearerTokenCallback callback = new OAuthBearerTokenCallback();    callback.error(errorCode, errorDescription, errorUri);    assertEquals(errorCode, callback.errorCode());    assertEquals(errorDescription, callback.errorDescription());    assertEquals(errorUri, callback.errorUri());    assertNull(callback.token());}
f8353
0
testToken
public void kafkatest_f8354_0()
{    OAuthBearerTokenCallback callback = new OAuthBearerTokenCallback();    callback.token(TOKEN);    assertSame(TOKEN, callback.token());    assertNull(callback.errorCode());    assertNull(callback.errorDescription());    assertNull(callback.errorUri());}
f8354
0
scope
public Set<String> kafkatest_f8362_0()
{    return Collections.emptySet();}
f8362
0
principalName
public String kafkatest_f8363_0()
{    return "principalName";}
f8363
0
lifetimeMs
public long kafkatest_f8364_0()
{    return 0;}
f8364
0
saslMessage
private byte[] kafkatest_f8372_0(String authorizationId, String userName, String password)
{    String nul = "\u0000";    String message = String.format("%s%s%s%s%s", authorizationId, nul, userName, nul, password);    return message.getBytes(StandardCharsets.UTF_8);}
f8372
0
setUp
public void kafkatest_f8373_0()
{    this.map = new HashMap<>();    this.map.put("what", "42");    this.map.put("who", "me");}
f8373
0
testReturnedMapIsImmutable
public void kafkatest_f8374_0()
{    SaslExtensions extensions = new SaslExtensions(this.map);    extensions.map().put("hello", "test");}
f8374
0
scramCredentialCache
public void kafkatest_f8382_0() throws Exception
{    CredentialCache cache = new CredentialCache();    ScramCredentialUtils.createCache(cache, Arrays.asList("SCRAM-SHA-512", "PLAIN"));    assertNotNull("Cache not created for enabled mechanism", cache.cache(ScramMechanism.SCRAM_SHA_512.mechanismName(), ScramCredential.class));    assertNull("Cache created for disabled mechanism", cache.cache(ScramMechanism.SCRAM_SHA_256.mechanismName(), ScramCredential.class));    CredentialCache.Cache<ScramCredential> sha512Cache = cache.cache(ScramMechanism.SCRAM_SHA_512.mechanismName(), ScramCredential.class);    ScramFormatter formatter = new ScramFormatter(ScramMechanism.SCRAM_SHA_512);    ScramCredential credentialA = formatter.generateCredential("password", 4096);    sha512Cache.put("userA", credentialA);    assertEquals(credentialA, sha512Cache.get("userA"));    assertNull("Invalid user credential", sha512Cache.get("userB"));}
f8382
0
rfc7677Example
public void kafkatest_f8383_0() throws Exception
{    ScramFormatter formatter = new ScramFormatter(ScramMechanism.SCRAM_SHA_256);    String password = "pencil";    String c1 = "n,,n=user,r=rOprNGfwEbeRWgbNEkqO";    String s1 = "r=rOprNGfwEbeRWgbNEkqO%hvYDpWUa2RaTCAfuxFIlj)hNlF$k0,s=W22ZaJ0SNY7soEsUEjb6gQ==,i=4096";    String c2 = "c=biws,r=rOprNGfwEbeRWgbNEkqO%hvYDpWUa2RaTCAfuxFIlj)hNlF$k0,p=dHzbZapWIk4jUhN+Ute9ytag9zjfMHgsqmmiz7AndVQ=";    String s2 = "v=6rriTRBi23WpRR/wtup+mMhUZUn/dB5nLTJRsjl95G4=";    ClientFirstMessage clientFirst = new ClientFirstMessage(formatter.toBytes(c1));    ServerFirstMessage serverFirst = new ServerFirstMessage(formatter.toBytes(s1));    ClientFinalMessage clientFinal = new ClientFinalMessage(formatter.toBytes(c2));    ServerFinalMessage serverFinal = new ServerFinalMessage(formatter.toBytes(s2));    String username = clientFirst.saslName();    assertEquals("user", username);    String clientNonce = clientFirst.nonce();    assertEquals("rOprNGfwEbeRWgbNEkqO", clientNonce);    String serverNonce = serverFirst.nonce().substring(clientNonce.length());    assertEquals("%hvYDpWUa2RaTCAfuxFIlj)hNlF$k0", serverNonce);    byte[] salt = serverFirst.salt();    assertArrayEquals(Base64.getDecoder().decode("W22ZaJ0SNY7soEsUEjb6gQ=="), salt);    int iterations = serverFirst.iterations();    assertEquals(4096, iterations);    byte[] channelBinding = clientFinal.channelBinding();    assertArrayEquals(Base64.getDecoder().decode("biws"), channelBinding);    byte[] serverSignature = serverFinal.serverSignature();    assertArrayEquals(Base64.getDecoder().decode("6rriTRBi23WpRR/wtup+mMhUZUn/dB5nLTJRsjl95G4="), serverSignature);    byte[] saltedPassword = formatter.saltedPassword(password, salt, iterations);    byte[] serverKey = formatter.serverKey(saltedPassword);    byte[] computedProof = formatter.clientProof(saltedPassword, clientFirst, serverFirst, clientFinal);    assertArrayEquals(clientFinal.proof(), computedProof);    byte[] computedSignature = formatter.serverSignature(serverKey, clientFirst, serverFirst, clientFinal);    assertArrayEquals(serverFinal.serverSignature(), computedSignature);    // Minimum iterations defined in RFC-7677    assertEquals(4096, ScramMechanism.SCRAM_SHA_256.minIterations());}
f8383
0
saslName
public void kafkatest_f8384_0() throws Exception
{    String[] usernames = { "user1", "123", "1,2", "user=A", "user==B", "user,1", "user 1", ",", "=", ",=", "==" };    ScramFormatter formatter = new ScramFormatter(ScramMechanism.SCRAM_SHA_256);    for (String username : usernames) {        String saslName = formatter.saslName(username);        // There should be no commas in saslName (comma is used as field separator in SASL messages)        assertEquals(-1, saslName.indexOf(','));        // There should be no "=" in the saslName apart from those used in encoding (comma is =2C and equals is =3D)        assertEquals(-1, saslName.replace("=2C", "").replace("=3D", "").indexOf('='));        assertEquals(username, formatter.username(saslName));    }}
f8384
0
validServerFinalMessage
public void kafkatest_f8392_0() throws SaslException
{    String serverSignature = randomBytesAsString();    ServerFinalMessage m = new ServerFinalMessage("unknown-user", null);    checkServerFinalMessage(m, "unknown-user", null);    m = new ServerFinalMessage(null, toBytes(serverSignature));    checkServerFinalMessage(m, null, serverSignature);    // Default format used by Kafka clients for successful final message    String str = String.format("v=%s", serverSignature);    m = createScramMessage(ServerFinalMessage.class, str);    checkServerFinalMessage(m, null, serverSignature);    m = new ServerFinalMessage(m.toBytes());    checkServerFinalMessage(m, null, serverSignature);    // Default format used by Kafka clients for final message with error    str = "e=other-error";    m = createScramMessage(ServerFinalMessage.class, str);    checkServerFinalMessage(m, "other-error", null);    m = new ServerFinalMessage(m.toBytes());    checkServerFinalMessage(m, "other-error", null);    // Optional extension    for (String extension : VALID_EXTENSIONS) {        str = String.format("v=%s,%s", serverSignature, extension);        checkServerFinalMessage(createScramMessage(ServerFinalMessage.class, str), null, serverSignature);    }}
f8392
0
invalidServerFinalMessage
public void kafkatest_f8393_0()
{    String serverSignature = randomBytesAsString();    // Invalid error    String invalid = "e=error1,error2";    checkInvalidScramMessage(ServerFinalMessage.class, invalid);    // Invalid server signature    invalid = String.format("v=1=23");    checkInvalidScramMessage(ServerFinalMessage.class, invalid);    // Invalid extensions    for (String extension : INVALID_EXTENSIONS) {        invalid = String.format("v=%s,%s", serverSignature, extension);        checkInvalidScramMessage(ServerFinalMessage.class, invalid);        invalid = String.format("e=unknown-user,%s", extension);        checkInvalidScramMessage(ServerFinalMessage.class, invalid);    }}
f8393
0
randomBytesAsString
private String kafkatest_f8394_0()
{    return Base64.getEncoder().encodeToString(formatter.secureRandomBytes());}
f8394
0
setUp
public void kafkatest_f8402_0() throws Exception
{    mechanism = ScramMechanism.SCRAM_SHA_256;    formatter = new ScramFormatter(mechanism);    CredentialCache.Cache<ScramCredential> credentialCache = new CredentialCache().createCache(mechanism.mechanismName(), ScramCredential.class);    credentialCache.put(USER_A, formatter.generateCredential("passwordA", 4096));    credentialCache.put(USER_B, formatter.generateCredential("passwordB", 4096));    ScramServerCallbackHandler callbackHandler = new ScramServerCallbackHandler(credentialCache, new DelegationTokenCache(ScramMechanism.mechanismNames()));    saslServer = new ScramSaslServer(mechanism, new HashMap<String, Object>(), callbackHandler);}
f8402
0
noAuthorizationIdSpecified
public void kafkatest_f8403_0() throws Exception
{    byte[] nextChallenge = saslServer.evaluateResponse(clientFirstMessage(USER_A, null));    assertTrue("Next challenge is empty", nextChallenge.length > 0);}
f8403
0
authorizatonIdEqualsAuthenticationId
public void kafkatest_f8404_0() throws Exception
{    byte[] nextChallenge = saslServer.evaluateResponse(clientFirstMessage(USER_A, USER_A));    assertTrue("Next challenge is empty", nextChallenge.length > 0);}
f8404
0
getCertificateChain
public X509Certificate[] kafkatest_f8414_0(String s)
{    return new X509Certificate[] { this.certificate };}
f8414
0
getPrivateKey
public PrivateKey kafkatest_f8415_0(String s)
{    return this.keyPair.getPrivate();}
f8415
0
getProvider
public Provider kafkatest_f8416_0()
{    if (provider == null) {        provider = new TestPlainSaslServerProvider();    }    return provider;}
f8416
0
testSslFactoryWithIncorrectProviderClassConfiguration
public void kafkatest_f8432_0()
{    // An exception is thrown as the algorithm is not registered through a provider    Map<String, Object> serverSslConfig = TestSslUtils.createSslConfig(TestKeyManagerFactory.ALGORITHM, TestTrustManagerFactory.ALGORITHM);    serverSslConfig.put(SecurityConfig.SECURITY_PROVIDERS_CONFIG, "com.fake.ProviderClass1,com.fake.ProviderClass2");    SslFactory sslFactory = new SslFactory(Mode.SERVER);    sslFactory.configure(serverSslConfig);}
f8432
0
testSslFactoryWithoutPasswordConfiguration
public void kafkatest_f8433_0() throws Exception
{    File trustStoreFile = File.createTempFile("truststore", ".jks");    Map<String, Object> serverSslConfig = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile, "server");    // unset the password    serverSslConfig.remove(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);    SslFactory sslFactory = new SslFactory(Mode.SERVER);    try {        sslFactory.configure(serverSslConfig);    } catch (Exception e) {        fail("An exception was thrown when configuring the truststore without a password: " + e);    }}
f8433
0
testClientMode
public void kafkatest_f8434_0() throws Exception
{    File trustStoreFile = File.createTempFile("truststore", ".jks");    Map<String, Object> clientSslConfig = TestSslUtils.createSslConfig(false, true, Mode.CLIENT, trustStoreFile, "client");    SslFactory sslFactory = new SslFactory(Mode.CLIENT);    sslFactory.configure(clientSslConfig);    // host and port are hints    SSLEngine engine = sslFactory.createSslEngine("localhost", 0);    assertTrue(engine.getUseClientMode());}
f8434
0
sslKeyStore
private SslEngineBuilder.SecurityStore kafkatest_f8442_0(Map<String, Object> sslConfig)
{    return new SslEngineBuilder.SecurityStore((String) sslConfig.get(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG), (String) sslConfig.get(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG), (Password) sslConfig.get(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG), (Password) sslConfig.get(SslConfigs.SSL_KEY_PASSWORD_CONFIG));}
f8442
0
sslTrustStore
private SslEngineBuilder.SecurityStore kafkatest_f8443_0(Map<String, Object> sslConfig)
{    return new SslEngineBuilder.SecurityStore((String) sslConfig.get(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG), (String) sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG), (Password) sslConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG), null);}
f8443
0
testValidRules
public void kafkatest_f8444_0()
{    testValidRule("DEFAULT");    testValidRule("RULE:^CN=(.*?),OU=ServiceUsers.*$/$1/");    testValidRule("RULE:^CN=(.*?),OU=ServiceUsers.*$/$1/L, DEFAULT");    testValidRule("RULE:^CN=(.*?),OU=(.*?),O=(.*?),L=(.*?),ST=(.*?),C=(.*?)$/$1@$2/");    testValidRule("RULE:^.*[Cc][Nn]=([a-zA-Z0-9.]*).*$/$1/L");    testValidRule("RULE:^cn=(.?),ou=(.?),dc=(.?),dc=(.?)$/$1@$2/U");    testValidRule("RULE:^CN=([^,ADEFLTU,]+)(,.*|$)/$1/");    testValidRule("RULE:^CN=([^,DEFAULT,]+)(,.*|$)/$1/");}
f8444
0
allSerdesShouldRoundtripInput
public void kafkatest_f8452_0()
{    for (Map.Entry<Class<?>, List<Object>> test : testData.entrySet()) {        try (Serde<Object> serde = Serdes.serdeFrom((Class<Object>) test.getKey())) {            for (Object value : test.getValue()) {                assertEquals("Should get the original " + test.getKey().getSimpleName() + " after serialization and deserialization", value, serde.deserializer().deserialize(topic, serde.serializer().serialize(topic, value)));            }        }    }}
f8452
0
allSerdesShouldSupportNull
public void kafkatest_f8453_0()
{    for (Class<?> cls : testData.keySet()) {        try (Serde<?> serde = Serdes.serdeFrom(cls)) {            assertThat("Should support null in " + cls.getSimpleName() + " serialization", serde.serializer().serialize(topic, null), nullValue());            assertThat("Should support null in " + cls.getSimpleName() + " deserialization", serde.deserializer().deserialize(topic, null), nullValue());        }    }}
f8453
0
testSerdeFromUnknown
public void kafkatest_f8454_0()
{    Serdes.serdeFrom(DummyClass.class);}
f8454
0
checkValues
private void kafkatest_f8462_0(TopicPartition deSerTP)
{    // assert deserialized values are same as original    assertEquals("partition number should be " + partNum + " but got " + deSerTP.partition(), partNum, deSerTP.partition());    assertEquals("topic should be " + topicName + " but got " + deSerTP.topic(), topicName, deSerTP.topic());}
f8462
0
testSerializationRoundtrip
public void kafkatest_f8463_0() throws IOException, ClassNotFoundException
{    // assert TopicPartition is serializable and deserialization renders the clone of original properly    TopicPartition origTp = new TopicPartition(topicName, partNum);    byte[] byteArray = Serializer.serialize(origTp);    // deserialize the byteArray and check if the values are same as original    Object deserializedObject = Serializer.deserialize(byteArray);    assertTrue(deserializedObject instanceof TopicPartition);    checkValues((TopicPartition) deserializedObject);}
f8463
0
testTopiPartitionSerializationCompatibility
public void kafkatest_f8464_0() throws IOException, ClassNotFoundException
{    // assert serialized TopicPartition object in file (serializedData/topicPartitionSerializedfile) is    // deserializable into TopicPartition and is compatible    Object deserializedObject = Serializer.deserialize(fileName);    assertTrue(deserializedObject instanceof TopicPartition);    checkValues((TopicPartition) deserializedObject);}
f8464
0
registerAppInfo
private void kafkatest_f8472_0() throws JMException
{    assertEquals(EXPECTED_COMMIT_VERSION, AppInfoParser.getCommitId());    assertEquals(EXPECTED_VERSION, AppInfoParser.getVersion());    AppInfoParser.registerAppInfo(METRICS_PREFIX, METRICS_ID, metrics, EXPECTED_START_MS);    assertTrue(mBeanServer.isRegistered(expectedAppObjectName()));    assertEquals(EXPECTED_COMMIT_VERSION, metrics.metric(metrics.metricName("commit-id", "app-info")).metricValue());    assertEquals(EXPECTED_VERSION, metrics.metric(metrics.metricName("version", "app-info")).metricValue());    assertEquals(EXPECTED_START_MS, metrics.metric(metrics.metricName("start-time-ms", "app-info")).metricValue());}
f8472
0
expectedAppObjectName
private ObjectName kafkatest_f8473_0() throws MalformedObjectNameException
{    return new ObjectName(METRICS_PREFIX + ":type=app-info,id=" + METRICS_ID);}
f8473
0
testExpandByteBufferOnPositionIncrease
public void kafkatest_f8474_0() throws Exception
{    testExpandByteBufferOnPositionIncrease(ByteBuffer.allocate(16));}
f8474
0
testWriteByteBuffer
private void kafkatest_f8482_0(ByteBuffer input) throws IOException
{    long value = 234239230L;    input.putLong(value);    input.flip();    ByteBufferOutputStream output = new ByteBufferOutputStream(ByteBuffer.allocate(32));    output.write(input);    assertEquals(8, input.position());    assertEquals(8, output.position());    assertEquals(value, output.buffer().getLong(0));    output.close();}
f8482
0
testReadUnsignedIntLEFromArray
public void kafkatest_f8483_0()
{    byte[] array1 = { 0x01, 0x02, 0x03, 0x04, 0x05 };    assertEquals(0x04030201, ByteUtils.readUnsignedIntLE(array1, 0));    assertEquals(0x05040302, ByteUtils.readUnsignedIntLE(array1, 1));    byte[] array2 = { (byte) 0xf1, (byte) 0xf2, (byte) 0xf3, (byte) 0xf4, (byte) 0xf5, (byte) 0xf6 };    assertEquals(0xf4f3f2f1, ByteUtils.readUnsignedIntLE(array2, 0));    assertEquals(0xf6f5f4f3, ByteUtils.readUnsignedIntLE(array2, 2));}
f8483
0
testReadUnsignedIntLEFromInputStream
public void kafkatest_f8484_0() throws IOException
{    byte[] array1 = { 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09 };    ByteArrayInputStream is1 = new ByteArrayInputStream(array1);    assertEquals(0x04030201, ByteUtils.readUnsignedIntLE(is1));    assertEquals(0x08070605, ByteUtils.readUnsignedIntLE(is1));    byte[] array2 = { (byte) 0xf1, (byte) 0xf2, (byte) 0xf3, (byte) 0xf4, (byte) 0xf5, (byte) 0xf6, (byte) 0xf7, (byte) 0xf8 };    ByteArrayInputStream is2 = new ByteArrayInputStream(array2);    assertEquals(0xf4f3f2f1, ByteUtils.readUnsignedIntLE(is2));    assertEquals(0xf8f7f6f5, ByteUtils.readUnsignedIntLE(is2));}
f8484
0
testInvalidVarlong
public void kafkatest_f8492_0()
{    // varlong encoding has one overflow byte    ByteBuffer buf = ByteBuffer.wrap(new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x01 });    ByteUtils.readVarlong(buf);}
f8492
0
assertUnsignedVarintSerde
private void kafkatest_f8493_0(int value, byte[] expectedEncoding) throws IOException
{    ByteBuffer buf = ByteBuffer.allocate(32);    ByteUtils.writeUnsignedVarint(value, buf);    buf.flip();    assertArrayEquals(expectedEncoding, Utils.toArray(buf));    assertEquals(value, ByteUtils.readUnsignedVarint(buf.duplicate()));    buf.rewind();    DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buf));    ByteUtils.writeUnsignedVarint(value, out);    buf.flip();    assertArrayEquals(expectedEncoding, Utils.toArray(buf));    DataInputStream in = new DataInputStream(new ByteBufferInputStream(buf));    assertEquals(value, ByteUtils.readUnsignedVarint(in));}
f8493
0
assertVarintSerde
private void kafkatest_f8494_0(int value, byte[] expectedEncoding) throws IOException
{    ByteBuffer buf = ByteBuffer.allocate(32);    ByteUtils.writeVarint(value, buf);    buf.flip();    assertArrayEquals(expectedEncoding, Utils.toArray(buf));    assertEquals(value, ByteUtils.readVarint(buf.duplicate()));    buf.rewind();    DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buf));    ByteUtils.writeVarint(value, out);    buf.flip();    assertArrayEquals(expectedEncoding, Utils.toArray(buf));    DataInputStream in = new DataInputStream(new ByteBufferInputStream(buf));    assertEquals(value, ByteUtils.readVarint(in));}
f8494
0
testSubtractMapRemovesSecondMapsKeys
public void kafkatest_f8502_0()
{    Map<String, String> mainMap = new HashMap<>();    mainMap.put("one", "1");    mainMap.put("two", "2");    mainMap.put("three", "3");    Map<String, String> secondaryMap = new HashMap<>();    secondaryMap.put("one", "4");    secondaryMap.put("two", "5");    Map<String, String> newMap = subtractMap(mainMap, secondaryMap);    // original map should not be modified    assertEquals(3, mainMap.size());    assertEquals(1, newMap.size());    assertTrue(newMap.containsKey("three"));    assertEquals("3", newMap.get("three"));}
f8502
0
testSubtractMapDoesntRemoveAnythingWhenEmptyMap
public void kafkatest_f8503_0()
{    Map<String, String> mainMap = new HashMap<>();    mainMap.put("one", "1");    mainMap.put("two", "2");    mainMap.put("three", "3");    Map<String, String> secondaryMap = new HashMap<>();    Map<String, String> newMap = subtractMap(mainMap, secondaryMap);    assertEquals(3, newMap.size());    assertEquals("1", newMap.get("one"));    assertEquals("2", newMap.get("two"));    assertEquals("3", newMap.get("three"));    assertNotSame(newMap, mainMap);}
f8503
0
testUpdate
public void kafkatest_f8504_0()
{    final byte[] bytes = "Any String you want".getBytes();    final int len = bytes.length;    Checksum crc1 = Crc32C.create();    Checksum crc2 = Crc32C.create();    Checksum crc3 = Crc32C.create();    crc1.update(bytes, 0, len);    for (int i = 0; i < len; i++) crc2.update(bytes[i]);    crc3.update(bytes, 0, len / 2);    crc3.update(bytes, len / 2, len - len / 2);    assertEquals("Crc values should be the same", crc1.getValue(), crc2.getValue());    assertEquals("Crc values should be the same", crc1.getValue(), crc3.getValue());}
f8504
0
prev
public int kafkatest_f8512_0()
{    return prev;}
f8512
0
setPrev
public void kafkatest_f8513_0(int prev)
{    this.prev = prev;}
f8513
0
next
public int kafkatest_f8514_0()
{    return next;}
f8514
0
expectTraversal
 static void kafkatest_f8522_0(Iterator<TestElement> iter, Iterator<Integer> expectedIter)
{    int i = 0;    while (iter.hasNext()) {        TestElement element = iter.next();        Assert.assertTrue("Iterator yieled " + (i + 1) + " elements, but only " + i + " were expected.", expectedIter.hasNext());        Integer expected = expectedIter.next();        Assert.assertEquals("Iterator value number " + (i + 1) + " was incorrect.", expected.intValue(), element.val);        i = i + 1;    }    Assert.assertFalse("Iterator yieled " + i + " elements, but at least " + (i + 1) + " were expected.", expectedIter.hasNext());}
f8522
0
testTraversal
public void kafkatest_f8523_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>();    expectTraversal(coll.iterator());    assertTrue(coll.add(new TestElement(2)));    expectTraversal(coll.iterator(), 2);    assertTrue(coll.add(new TestElement(1)));    expectTraversal(coll.iterator(), 2, 1);    assertTrue(coll.add(new TestElement(100)));    expectTraversal(coll.iterator(), 2, 1, 100);    assertTrue(coll.remove(new TestElement(1)));    expectTraversal(coll.iterator(), 2, 100);    assertTrue(coll.add(new TestElement(1)));    expectTraversal(coll.iterator(), 2, 100, 1);    Iterator<TestElement> iter = coll.iterator();    iter.next();    iter.next();    iter.remove();    iter.next();    assertFalse(iter.hasNext());    expectTraversal(coll.iterator(), 2, 1);    List<TestElement> list = new ArrayList<>();    list.add(new TestElement(1));    list.add(new TestElement(2));    assertTrue(coll.removeAll(list));    assertFalse(coll.removeAll(list));    expectTraversal(coll.iterator());    assertEquals(0, coll.size());    assertTrue(coll.isEmpty());}
f8523
0
testSetViewGet
public void kafkatest_f8524_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>();    coll.add(new TestElement(1));    coll.add(new TestElement(2));    coll.add(new TestElement(3));    Set<TestElement> set = coll.valuesSet();    assertTrue(set.contains(new TestElement(1)));    assertTrue(set.contains(new TestElement(2)));    assertTrue(set.contains(new TestElement(3)));    assertEquals(3, set.size());}
f8524
0
testCollisions
public void kafkatest_f8532_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>(5);    assertEquals(11, coll.numSlots());    assertTrue(coll.add(new TestElement(11)));    assertTrue(coll.add(new TestElement(0)));    assertTrue(coll.add(new TestElement(22)));    assertTrue(coll.add(new TestElement(33)));    assertEquals(11, coll.numSlots());    expectTraversal(coll.iterator(), 11, 0, 22, 33);    assertTrue(coll.remove(new TestElement(22)));    expectTraversal(coll.iterator(), 11, 0, 33);    assertEquals(3, coll.size());    assertFalse(coll.isEmpty());}
f8532
0
testEnlargement
public void kafkatest_f8533_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>(5);    assertEquals(11, coll.numSlots());    for (int i = 0; i < 6; i++) {        assertTrue(coll.add(new TestElement(i)));    }    assertEquals(23, coll.numSlots());    assertEquals(6, coll.size());    expectTraversal(coll.iterator(), 0, 1, 2, 3, 4, 5);    for (int i = 0; i < 6; i++) {        assertTrue("Failed to find element " + i, coll.contains(new TestElement(i)));    }    coll.remove(new TestElement(3));    assertEquals(23, coll.numSlots());    assertEquals(5, coll.size());    expectTraversal(coll.iterator(), 0, 1, 2, 4, 5);}
f8533
0
testManyInsertsAndDeletes
public void kafkatest_f8534_0()
{    Random random = new Random(123);    LinkedHashSet<Integer> existing = new LinkedHashSet<>();    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>();    for (int i = 0; i < 100; i++) {        addRandomElement(random, existing, coll);        addRandomElement(random, existing, coll);        addRandomElement(random, existing, coll);        removeRandomElement(random, existing, coll);        expectTraversal(coll.iterator(), existing.iterator());    }}
f8534
0
testTraversal
public void kafkatest_f8542_0()
{    ImplicitLinkedHashMultiCollection<TestElement> multiSet = new ImplicitLinkedHashMultiCollection<>();    expectExactTraversal(multiSet.iterator());    TestElement e1 = new TestElement(1);    TestElement e2 = new TestElement(1);    TestElement e3 = new TestElement(2);    assertTrue(multiSet.add(e1));    assertTrue(multiSet.add(e2));    assertTrue(multiSet.add(e3));    expectExactTraversal(multiSet.iterator(), e1, e2, e3);    assertTrue(multiSet.remove(e2));    expectExactTraversal(multiSet.iterator(), e1, e3);    assertTrue(multiSet.remove(e1));    expectExactTraversal(multiSet.iterator(), e3);}
f8542
0
expectExactTraversal
 static void kafkatest_f8543_0(Iterator<TestElement> iterator, TestElement... sequence)
{    int i = 0;    while (iterator.hasNext()) {        TestElement element = iterator.next();        assertTrue("Iterator yieled " + (i + 1) + " elements, but only " + sequence.length + " were expected.", i < sequence.length);        if (sequence[i] != element) {            fail("Iterator value number " + (i + 1) + " was incorrect.");        }        i = i + 1;    }    assertTrue("Iterator yieled " + (i + 1) + " elements, but " + sequence.length + " were expected.", i == sequence.length);}
f8543
0
testEnlargement
public void kafkatest_f8544_0()
{    ImplicitLinkedHashMultiCollection<TestElement> multiSet = new ImplicitLinkedHashMultiCollection<>(5);    assertEquals(11, multiSet.numSlots());    TestElement[] testElements = { new TestElement(100), new TestElement(101), new TestElement(102), new TestElement(100), new TestElement(101), new TestElement(105) };    for (int i = 0; i < testElements.length; i++) {        assertTrue(multiSet.add(testElements[i]));    }    for (int i = 0; i < testElements.length; i++) {        assertFalse(multiSet.add(testElements[i]));    }    assertEquals(23, multiSet.numSlots());    assertEquals(testElements.length, multiSet.size());    expectExactTraversal(multiSet.iterator(), testElements);    multiSet.remove(testElements[1]);    assertEquals(23, multiSet.numSlots());    assertEquals(5, multiSet.size());    expectExactTraversal(multiSet.iterator(), testElements[0], testElements[2], testElements[3], testElements[4], testElements[5]);}
f8544
0
testRegister
public void kafkatest_f8552_0() throws ReflectiveOperationException
{    new LoggingSignalHandler().register();}
f8552
0
testUnmap
public void kafkatest_f8553_0() throws Exception
{    File file = TestUtils.tempFile();    try (FileChannel channel = FileChannel.open(file.toPath())) {        MappedByteBuffer map = channel.map(FileChannel.MapMode.READ_ONLY, 0, 0);        MappedByteBuffers.unmap(file.getAbsolutePath(), map);    }}
f8553
0
time
public Time kafkatest_f8554_0()
{    return time;}
f8554
0
nanoseconds
public long kafkatest_f8562_0()
{    maybeSleep(autoTickMs);    return highResTimeNs.get();}
f8562
0
maybeSleep
private void kafkatest_f8563_0(long ms)
{    if (ms != 0)        sleep(ms);}
f8563
0
sleep
public void kafkatest_f8564_0(long ms)
{    timeMs.addAndGet(ms);    highResTimeNs.addAndGet(TimeUnit.MILLISECONDS.toNanos(ms));    tick();}
f8564
0
testJmxSanitize
public void kafkatest_f8572_0() throws MalformedObjectNameException
{    int unquoted = 0;    for (int i = 0; i < 65536; i++) {        char c = (char) i;        String value = "value" + c;        String jmxSanitizedValue = Sanitizer.jmxSanitize(value);        if (jmxSanitizedValue.equals(value))            unquoted++;        verifyJmx(jmxSanitizedValue, i);        String encodedValue = Sanitizer.sanitize(value);        verifyJmx(encodedValue, i);        // jmxSanitize should not sanitize URL-encoded values        assertEquals(encodedValue, Sanitizer.jmxSanitize(encodedValue));    }    // a-zA-Z0-9-_% space and tab    assertEquals(68, unquoted);}
f8572
0
verifyJmx
private void kafkatest_f8573_0(String sanitizedValue, int c) throws MalformedObjectNameException
{    Object mbean = new TestStat();    MBeanServer server = ManagementFactory.getPlatformMBeanServer();    ObjectName objectName = new ObjectName("test:key=" + sanitizedValue);    try {        server.registerMBean(mbean, objectName);        server.unregisterMBean(objectName);    } catch (OperationsException | MBeanException e) {        fail("Could not register char=\\u" + c);    }}
f8573
0
getValue
public int kafkatest_f8574_0()
{    return 1;}
f8574
0
serialize
public static byte[] kafkatest_f8582_0(Object toSerialize) throws IOException
{    ByteArrayOutputStream arrayOutputStream = new ByteArrayOutputStream();    try (ObjectOutputStream ooStream = new ObjectOutputStream(arrayOutputStream)) {        ooStream.writeObject(toSerialize);        return arrayOutputStream.toByteArray();    }}
f8582
0
deserialize
public static Object kafkatest_f8583_0(InputStream inputStream) throws IOException, ClassNotFoundException
{    try (ObjectInputStream objectInputStream = new ObjectInputStream(inputStream)) {        return objectInputStream.readObject();    }}
f8583
0
deserialize
public static Object kafkatest_f8584_0(byte[] byteArray) throws IOException, ClassNotFoundException
{    ByteArrayInputStream arrayInputStream = new ByteArrayInputStream(byteArray);    return deserialize(arrayInputStream);}
f8584
0
testTimerUpdateAndReset
public void kafkatest_f8592_0()
{    Timer timer = time.timer(500);    timer.sleep(200);    assertEquals(300, timer.remainingMs());    assertEquals(200, timer.elapsedMs());    timer.updateAndReset(400);    assertEquals(400, timer.remainingMs());    assertEquals(0, timer.elapsedMs());    timer.sleep(400);    assertTrue(timer.isExpired());    timer.updateAndReset(200);    assertEquals(200, timer.remainingMs());    assertEquals(0, timer.elapsedMs());    assertFalse(timer.isExpired());}
f8592
0
testTimerResetUsesCurrentTime
public void kafkatest_f8593_0()
{    Timer timer = time.timer(500);    timer.sleep(200);    assertEquals(300, timer.remainingMs());    assertEquals(200, timer.elapsedMs());    time.sleep(300);    timer.reset(500);    assertEquals(500, timer.remainingMs());    timer.update();    assertEquals(200, timer.remainingMs());}
f8593
0
testTimeoutOverflow
public void kafkatest_f8594_0()
{    Timer timer = time.timer(Long.MAX_VALUE);    assertEquals(Long.MAX_VALUE - timer.currentTimeMs(), timer.remainingMs());    assertEquals(0, timer.elapsedMs());}
f8594
0
testGetPort
public void kafkatest_f8602_0()
{    assertEquals(8000, getPort("127.0.0.1:8000").intValue());    assertEquals(8080, getPort("mydomain.com:8080").intValue());    assertEquals(8080, getPort("MyDomain.com:8080").intValue());    assertEquals(1234, getPort("[::1]:1234").intValue());    assertEquals(5678, getPort("[2001:db8:85a3:8d3:1319:8a2e:370:7348]:5678").intValue());    assertEquals(5678, getPort("[2001:DB8:85A3:8D3:1319:8A2E:370:7348]:5678").intValue());    assertEquals(5678, getPort("[fe80::b1da:69ca:57f7:63d8%3]:5678").intValue());}
f8602
0
testFormatAddress
public void kafkatest_f8603_0()
{    assertEquals("127.0.0.1:8000", formatAddress("127.0.0.1", 8000));    assertEquals("mydomain.com:8080", formatAddress("mydomain.com", 8080));    assertEquals("[::1]:1234", formatAddress("::1", 1234));    assertEquals("[2001:db8:85a3:8d3:1319:8a2e:370:7348]:5678", formatAddress("2001:db8:85a3:8d3:1319:8a2e:370:7348", 5678));}
f8603
0
testFormatBytes
public void kafkatest_f8604_0()
{    assertEquals("-1", formatBytes(-1));    assertEquals("1023 B", formatBytes(1023));    assertEquals("1 KB", formatBytes(1024));    assertEquals("1024 KB", formatBytes((1024 * 1024) - 1));    assertEquals("1 MB", formatBytes(1024 * 1024));    assertEquals("1.1 MB", formatBytes((long) (1.1 * 1024 * 1024)));    assertEquals("10 MB", formatBytes(10 * 1024 * 1024));}
f8604
0
utf8ByteBufferSerde
public void kafkatest_f8612_0()
{    doTestUtf8ByteBuffer(ByteBuffer.allocate(20));    doTestUtf8ByteBuffer(ByteBuffer.allocateDirect(20));}
f8612
0
doTestUtf8ByteBuffer
private void kafkatest_f8613_0(ByteBuffer utf8Buffer)
{    String utf8String = "A\u00ea\u00f1\u00fcC";    byte[] utf8Bytes = utf8String.getBytes(StandardCharsets.UTF_8);    utf8Buffer.position(4);    utf8Buffer.put(utf8Bytes);    utf8Buffer.position(4);    assertEquals(utf8String, Utils.utf8(utf8Buffer, utf8Bytes.length));    assertEquals(4, utf8Buffer.position());    utf8Buffer.position(0);    assertEquals(utf8String, Utils.utf8(utf8Buffer, 4, utf8Bytes.length));    assertEquals(0, utf8Buffer.position());}
f8613
0
subTest
private void kafkatest_f8614_0(ByteBuffer buffer)
{    // The first byte should be 'A'    assertEquals('A', (Utils.readBytes(buffer, 0, 1))[0]);    // The offset is 2, so the first 2 bytes should be skipped.    byte[] results = Utils.readBytes(buffer, 2, 3);    assertEquals('y', results[0]);    assertEquals(' ', results[1]);    assertEquals('S', results[2]);    assertEquals(3, results.length);    // test readBytes without offset and length specified.    results = Utils.readBytes(buffer);    assertEquals('A', results[0]);    assertEquals('t', results[buffer.limit() - 1]);    assertEquals(buffer.limit(), results.length);}
f8614
0
fileChannelMockExpectReadWithRandomBytes
private String kafkatest_f8622_0(final FileChannel channelMock, final int bufferSize) throws IOException
{    final int step = 20;    final Random random = new Random();    int remainingBytes = bufferSize;    OngoingStubbing<Integer> when = when(channelMock.read(any(), anyLong()));    StringBuilder expectedBufferContent = new StringBuilder();    while (remainingBytes > 0) {        final int bytesRead = remainingBytes < step ? remainingBytes : random.nextInt(step);        final String stringRead = IntStream.range(0, bytesRead).mapToObj(i -> "a").collect(Collectors.joining());        expectedBufferContent.append(stringRead);        when = when.then(invocation -> {            ByteBuffer buffer = invocation.getArgument(0);            buffer.put(stringRead.getBytes());            return bytesRead;        });        remainingBytes -= bytesRead;    }    return expectedBufferContent.toString();}
f8622
0
close
public void kafkatest_f8623_0() throws IOException
{    closed = true;    if (closeException != null)        throw closeException;}
f8623
0
createCloseables
 static TestCloseable[] kafkatest_f8624_0(boolean... exceptionOnClose)
{    TestCloseable[] closeables = new TestCloseable[exceptionOnClose.length];    for (int i = 0; i < closeables.length; i++) closeables[i] = new TestCloseable(i, exceptionOnClose[i]);    return closeables;}
f8624
0
main
public static void kafkatest_f8632_0(String[] args) throws Exception
{    final int iters = Integer.parseInt(args[0]);    double x = 0.0;    long start = System.nanoTime();    for (int i = 0; i < iters; i++) x += Math.sqrt(x);    System.out.println(x);    System.out.println("sqrt: " + (System.nanoTime() - start) / (double) iters);    // test clocks    systemMillis(iters);    systemNanos(iters);    long total = 0;    start = System.nanoTime();    total += systemMillis(iters);    System.out.println("System.currentTimeMillis(): " + (System.nanoTime() - start) / iters);    start = System.nanoTime();    total += systemNanos(iters);    System.out.println("System.nanoTime(): " + (System.nanoTime() - start) / iters);    System.out.println(total);    // test random    int n = 0;    Random random = new Random();    start = System.nanoTime();    for (int i = 0; i < iters; i++) {        n += random.nextInt();    }    System.out.println(n);    System.out.println("random: " + (System.nanoTime() - start) / iters);    float[] floats = new float[1024];    for (int i = 0; i < floats.length; i++) floats[i] = random.nextFloat();    Arrays.sort(floats);    int loc = 0;    start = System.nanoTime();    for (int i = 0; i < iters; i++) loc += Arrays.binarySearch(floats, floats[i % floats.length]);    System.out.println(loc);    System.out.println("binary search: " + (System.nanoTime() - start) / iters);    final Time time = Time.SYSTEM;    final AtomicBoolean done = new AtomicBoolean(false);    final Object lock = new Object();    Thread t1 = new Thread() {        public void run() {            time.sleep(1);            int counter = 0;            long start = time.nanoseconds();            for (int i = 0; i < iters; i++) {                synchronized (lock) {                    counter++;                }            }            System.out.println("synchronized: " + ((time.nanoseconds() - start) / iters));            System.out.println(counter);            done.set(true);        }    };    Thread t2 = new Thread() {        public void run() {            int counter = 0;            while (!done.get()) {                time.sleep(1);                synchronized (lock) {                    counter += 1;                }            }            System.out.println("Counter: " + counter);        }    };    t1.start();    t2.start();    t1.join();    t2.join();    System.out.println("Testing locks");    done.set(false);    final ReentrantLock lock2 = new ReentrantLock();    Thread t3 = new Thread() {        public void run() {            time.sleep(1);            int counter = 0;            long start = time.nanoseconds();            for (int i = 0; i < iters; i++) {                lock2.lock();                counter++;                lock2.unlock();            }            System.out.println("lock: " + ((time.nanoseconds() - start) / iters));            System.out.println(counter);            done.set(true);        }    };    Thread t4 = new Thread() {        public void run() {            int counter = 0;            while (!done.get()) {                time.sleep(1);                lock2.lock();                counter++;                lock2.unlock();            }            System.out.println("Counter: " + counter);        }    };    t3.start();    t4.start();    t3.join();    t4.join();    Map<String, Integer> values = new HashMap<String, Integer>();    for (int i = 0; i < 100; i++) values.put(Integer.toString(i), i);    System.out.println("HashMap:");    benchMap(2, 1000000, values);    System.out.println("ConcurentHashMap:");    benchMap(2, 1000000, new ConcurrentHashMap<String, Integer>(values));    System.out.println("CopyOnWriteMap:");    benchMap(2, 1000000, new CopyOnWriteMap<String, Integer>(values));}
f8632
0
run
public void kafkatest_f8633_0()
{    time.sleep(1);    int counter = 0;    long start = time.nanoseconds();    for (int i = 0; i < iters; i++) {        synchronized (lock) {            counter++;        }    }    System.out.println("synchronized: " + ((time.nanoseconds() - start) / iters));    System.out.println(counter);    done.set(true);}
f8633
0
run
public void kafkatest_f8634_0()
{    int counter = 0;    while (!done.get()) {        time.sleep(1);        synchronized (lock) {            counter += 1;        }    }    System.out.println("Counter: " + counter);}
f8634
0
clusterResource
public ClusterResource kafkatest_f8642_0()
{    return clusterResource;}
f8642
0
configure
public void kafkatest_f8643_0(Map<String, ?> configs)
{    // clientId must be in configs    Object clientIdValue = configs.get(ConsumerConfig.CLIENT_ID_CONFIG);    if (clientIdValue == null)        throw new ConfigException("Mock consumer interceptor expects configuration " + ProducerConfig.CLIENT_ID_CONFIG);}
f8643
0
onConsume
public ConsumerRecords<String, String> kafkatest_f8644_0(ConsumerRecords<String, String> records)
{    // This will ensure that we get the cluster metadata when onConsume is called for the first time    // as subsequent compareAndSet operations will fail.    CLUSTER_ID_BEFORE_ON_CONSUME.compareAndSet(NO_CLUSTER_ID, CLUSTER_META.get());    Map<TopicPartition, List<ConsumerRecord<String, String>>> recordMap = new HashMap<>();    for (TopicPartition tp : records.partitions()) {        List<ConsumerRecord<String, String>> lst = new ArrayList<>();        for (ConsumerRecord<String, String> record : records.records(tp)) {            lst.add(new ConsumerRecord<>(record.topic(), record.partition(), record.offset(), record.timestamp(), record.timestampType(), record.checksum(), record.serializedKeySize(), record.serializedValueSize(), record.key(), record.value().toUpperCase(Locale.ROOT)));        }        recordMap.put(tp, lst);    }    return new ConsumerRecords<String, String>(recordMap);}
f8644
0
close
public void kafkatest_f8652_0()
{    closeCount.incrementAndGet();}
f8652
0
onUpdate
public void kafkatest_f8653_0(ClusterResource clusterResource)
{    clusterMeta.set(clusterResource);}
f8653
0
init
public void kafkatest_f8654_0(List<KafkaMetric> metrics)
{    INIT_COUNT.incrementAndGet();}
f8654
0
onAcknowledgement
public void kafkatest_f8665_0(RecordMetadata metadata, Exception exception)
{    // This will ensure that we get the cluster metadata when onAcknowledgement is called for the first time    // as subsequent compareAndSet operations will fail.    CLUSTER_ID_BEFORE_ON_ACKNOWLEDGEMENT.compareAndSet(NO_CLUSTER_ID, CLUSTER_META.get());    if (exception != null) {        ON_ERROR_COUNT.incrementAndGet();        if (metadata != null) {            ON_ERROR_WITH_METADATA_COUNT.incrementAndGet();        }    } else if (metadata != null)        ON_SUCCESS_COUNT.incrementAndGet();}
f8665
0
close
public void kafkatest_f8666_0()
{    CLOSE_COUNT.incrementAndGet();}
f8666
0
resetCounters
public static void kafkatest_f8667_0()
{    INIT_COUNT.set(0);    CLOSE_COUNT.set(0);    ONSEND_COUNT.set(0);    ON_SUCCESS_COUNT.set(0);    ON_ERROR_COUNT.set(0);    ON_ERROR_WITH_METADATA_COUNT.set(0);    CLUSTER_META.set(null);    CLUSTER_ID_BEFORE_ON_ACKNOWLEDGEMENT.set(NO_CLUSTER_ID);}
f8667
0
poll
public void kafkatest_f8677_0(long timeout) throws IOException
{    completeInitiatedSends();    completeDelayedReceives();    time.sleep(timeout);}
f8677
0
completeInitiatedSends
private void kafkatest_f8678_0() throws IOException
{    for (Send send : initiatedSends) {        completeSend(send);    }    this.initiatedSends.clear();}
f8678
0
completeSend
private void kafkatest_f8679_0(Send send) throws IOException
{    // Consume the send so that we will be able to send more requests to the destination    try (ByteBufferChannel discardChannel = new ByteBufferChannel(send.size())) {        while (!send.completed()) {            send.writeTo(discardChannel);        }        completedSends.add(send);    }}
f8679
0
connected
public List<String> kafkatest_f8687_0()
{    List<String> currentConnected = new ArrayList<>(connected);    connected.clear();    return currentConnected;}
f8687
0
isChannelReady
public boolean kafkatest_f8692_0(String id)
{    return true;}
f8692
0
reset
public void kafkatest_f8693_0()
{    clear();    initiatedSends.clear();    delayedReceives.clear();}
f8693
0
createKeyStore
public static void kafkatest_f8701_0(String filename, Password password, String alias, Key privateKey, Certificate cert) throws GeneralSecurityException, IOException
{    KeyStore ks = createEmptyKeyStore();    ks.setKeyEntry(alias, privateKey, password.value().toCharArray(), new Certificate[] { cert });    saveKeyStore(ks, filename, password);}
f8701
0
createKeyStore
public static void kafkatest_f8702_0(String filename, Password password, Password keyPassword, String alias, Key privateKey, Certificate cert) throws GeneralSecurityException, IOException
{    KeyStore ks = createEmptyKeyStore();    ks.setKeyEntry(alias, privateKey, keyPassword.value().toCharArray(), new Certificate[] { cert });    saveKeyStore(ks, filename, password);}
f8702
0
createTrustStore
public static void kafkatest_f8703_0(String filename, Password password, Map<String, T> certs) throws GeneralSecurityException, IOException
{    KeyStore ks = KeyStore.getInstance("JKS");    try (InputStream in = Files.newInputStream(Paths.get(filename))) {        ks.load(in, password.value().toCharArray());    } catch (EOFException e) {        ks = createEmptyKeyStore();    }    for (Map.Entry<String, T> cert : certs.entrySet()) {        ks.setCertificateEntry(cert.getKey(), cert.getValue());    }    saveKeyStore(ks, filename, password);}
f8703
0
generate
public X509Certificate kafkatest_f8711_0(String dn, KeyPair keyPair) throws CertificateException
{    try {        Security.addProvider(new BouncyCastleProvider());        AlgorithmIdentifier sigAlgId = new DefaultSignatureAlgorithmIdentifierFinder().find(algorithm);        AlgorithmIdentifier digAlgId = new DefaultDigestAlgorithmIdentifierFinder().find(sigAlgId);        AsymmetricKeyParameter privateKeyAsymKeyParam = PrivateKeyFactory.createKey(keyPair.getPrivate().getEncoded());        SubjectPublicKeyInfo subPubKeyInfo = SubjectPublicKeyInfo.getInstance(keyPair.getPublic().getEncoded());        ContentSigner sigGen = new BcRSAContentSignerBuilder(sigAlgId, digAlgId).build(privateKeyAsymKeyParam);        X500Name name = new X500Name(dn);        Date from = new Date();        Date to = new Date(from.getTime() + days * 86400000L);        BigInteger sn = new BigInteger(64, new SecureRandom());        X509v3CertificateBuilder v3CertGen = new X509v3CertificateBuilder(name, sn, from, to, name, subPubKeyInfo);        if (subjectAltName != null)            v3CertGen.addExtension(Extension.subjectAlternativeName, false, subjectAltName);        X509CertificateHolder certificateHolder = v3CertGen.build(sigGen);        return new JcaX509CertificateConverter().setProvider("BC").getCertificate(certificateHolder);    } catch (CertificateException ce) {        throw ce;    } catch (Exception e) {        throw new CertificateException(e);    }}
f8711
0
singletonCluster
public static Cluster kafkatest_f8712_0()
{    return clusterWith(1);}
f8712
0
singletonCluster
public static Cluster kafkatest_f8713_0(final String topic, final int partitions)
{    return clusterWith(1, topic, partitions);}
f8713
0
clusterWith
public static Cluster kafkatest_f8721_0(final int nodes, final String topic, final int partitions)
{    return clusterWith(nodes, Collections.singletonMap(topic, partitions));}
f8721
0
randomBytes
public static byte[] kafkatest_f8722_0(final int size)
{    final byte[] bytes = new byte[size];    SEEDED_RANDOM.nextBytes(bytes);    return bytes;}
f8722
0
randomString
public static String kafkatest_f8723_0(final int len)
{    final StringBuilder b = new StringBuilder();    for (int i = 0; i < len; i++) b.append(LETTERS_AND_DIGITS.charAt(SEEDED_RANDOM.nextInt(LETTERS_AND_DIGITS.length())));    return b.toString();}
f8723
0
consumerConfig
public static Properties kafkatest_f8731_0(final String bootstrapServers, final String groupId, final Class keyDeserializer, final Class valueDeserializer, final Properties additional)
{    final Properties consumerConfig = new Properties();    consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);    consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);    consumerConfig.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializer);    consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializer);    consumerConfig.putAll(additional);    return consumerConfig;}
f8731
0
consumerConfig
public static Properties kafkatest_f8732_0(final String bootstrapServers, final String groupId, final Class keyDeserializer, final Class valueDeserializer)
{    return consumerConfig(bootstrapServers, groupId, keyDeserializer, valueDeserializer, new Properties());}
f8732
0
consumerConfig
public static Properties kafkatest_f8733_0(final String bootstrapServers, final Class keyDeserializer, final Class valueDeserializer)
{    return consumerConfig(bootstrapServers, UUID.randomUUID().toString(), keyDeserializer, valueDeserializer, new Properties());}
f8733
0
checkEquals
public static void kafkatest_f8741_0(Set<T> c1, Set<T> c2, String firstDesc, String secondDesc)
{    if (!c1.equals(c2)) {        Set<T> missing1 = new HashSet<>(c2);        missing1.removeAll(c1);        Set<T> missing2 = new HashSet<>(c1);        missing2.removeAll(c2);        fail(String.format("Sets not equal, missing %s=%s, missing %s=%s", firstDesc, missing1, secondDesc, missing2));    }}
f8741
0
toList
public static List<T> kafkatest_f8742_0(Iterable<? extends T> iterable)
{    List<T> list = new ArrayList<>();    for (T item : iterable) list.add(item);    return list;}
f8742
0
toSet
public static Set<T> kafkatest_f8743_0(Collection<T> collection)
{    return new HashSet<>(collection);}
f8743
0
reconfigure
public void kafkatest_f8751_0(Map<String, String> props)
{    stop();    start(props);}
f8751
0
validate
public Config kafkatest_f8752_0(Map<String, String> connectorConfigs)
{    ConfigDef configDef = config();    if (null == configDef) {        throw new ConnectException(String.format("%s.config() must return a ConfigDef that is not null.", this.getClass().getName()));    }    List<ConfigValue> configValues = configDef.validate(connectorConfigs);    return new Config(configValues);}
f8752
0
topic
public String kafkatest_f8753_0()
{    return topic;}
f8753
0
toString
public String kafkatest_f8761_0()
{    return "ConnectRecord{" + "topic='" + topic + '\'' + ", kafkaPartition=" + kafkaPartition + ", key=" + key + ", keySchema=" + keySchema + ", value=" + value + ", valueSchema=" + valueSchema + ", timestamp=" + timestamp + ", headers=" + headers + '}';}
f8761
0
equals
public boolean kafkatest_f8762_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ConnectRecord that = (ConnectRecord) o;    return Objects.equals(kafkaPartition, that.kafkaPartition) && Objects.equals(topic, that.topic) && Objects.equals(keySchema, that.keySchema) && Objects.equals(key, that.key) && Objects.equals(valueSchema, that.valueSchema) && Objects.equals(value, that.value) && Objects.equals(timestamp, that.timestamp) && Objects.equals(headers, that.headers);}
f8762
0
hashCode
public int kafkatest_f8763_0()
{    int result = topic != null ? topic.hashCode() : 0;    result = 31 * result + (kafkaPartition != null ? kafkaPartition.hashCode() : 0);    result = 31 * result + (keySchema != null ? keySchema.hashCode() : 0);    result = 31 * result + (key != null ? key.hashCode() : 0);    result = 31 * result + (valueSchema != null ? valueSchema.hashCode() : 0);    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + (timestamp != null ? timestamp.hashCode() : 0);    result = 31 * result + headers.hashCode();    return result;}
f8763
0
defaultValue
public Object kafkatest_f8771_0()
{    return defaultValue;}
f8771
0
name
public String kafkatest_f8772_0()
{    return name;}
f8772
0
version
public Integer kafkatest_f8773_0()
{    return version;}
f8773
0
validateValue
public static void kafkatest_f8781_0(String name, Schema schema, Object value)
{    if (value == null) {        if (!schema.isOptional())            throw new DataException("Invalid value: null used for required field: \"" + name + "\", schema type: " + schema.type());        return;    }    List<Class> expectedClasses = expectedClassesFor(schema);    if (expectedClasses == null)        throw new DataException("Invalid Java object for schema type " + schema.type() + ": " + value.getClass() + " for field: \"" + name + "\"");    boolean foundMatch = false;    for (Class<?> expectedClass : expectedClasses) {        if (expectedClass.isInstance(value)) {            foundMatch = true;            break;        }    }    if (!foundMatch)        throw new DataException("Invalid Java object for schema type " + schema.type() + ": " + value.getClass() + " for field: \"" + name + "\"");    switch(schema.type()) {        case STRUCT:            Struct struct = (Struct) value;            if (!struct.schema().equals(schema))                throw new DataException("Struct schemas do not match.");            struct.validate();            break;        case ARRAY:            List<?> array = (List<?>) value;            for (Object entry : array) validateValue(schema.valueSchema(), entry);            break;        case MAP:            Map<?, ?> map = (Map<?, ?>) value;            for (Map.Entry<?, ?> entry : map.entrySet()) {                validateValue(schema.keySchema(), entry.getKey());                validateValue(schema.valueSchema(), entry.getValue());            }            break;    }}
f8781
0
expectedClassesFor
private static List<Class> kafkatest_f8782_0(Schema schema)
{    List<Class> expectedClasses = LOGICAL_TYPE_CLASSES.get(schema.name());    if (expectedClasses == null)        expectedClasses = SCHEMA_TYPE_CLASSES.get(schema.type());    return expectedClasses;}
f8782
0
validateValue
public void kafkatest_f8783_0(Object value)
{    validateValue(this, value);}
f8783
0
toLogical
public static java.util.Date kafkatest_f8791_0(Schema schema, int value)
{    if (!(LOGICAL_NAME.equals(schema.name())))        throw new DataException("Requested conversion of Date object but the schema does not match.");    return new java.util.Date(value * MILLIS_PER_DAY);}
f8791
0
builder
public static SchemaBuilder kafkatest_f8792_0(int scale)
{    return SchemaBuilder.bytes().name(LOGICAL_NAME).parameter(SCALE_FIELD, Integer.toString(scale)).version(1);}
f8792
0
schema
public static Schema kafkatest_f8793_0(int scale)
{    return builder(scale).build();}
f8793
0
hashCode
public int kafkatest_f8801_0()
{    return Objects.hash(name, index, schema);}
f8801
0
toString
public String kafkatest_f8802_0()
{    return "Field{" + "name=" + name + ", index=" + index + ", schema=" + schema + "}";}
f8802
0
getName
public String kafkatest_f8803_0()
{    return name;}
f8803
0
optional
public SchemaBuilder kafkatest_f8811_0()
{    checkCanSet(OPTIONAL_FIELD, optional, true);    optional = true;    return this;}
f8811
0
required
public SchemaBuilder kafkatest_f8812_0()
{    checkCanSet(OPTIONAL_FIELD, optional, false);    optional = false;    return this;}
f8812
0
defaultValue
public Object kafkatest_f8813_0()
{    return defaultValue;}
f8813
0
parameters
public Map<String, String> kafkatest_f8821_0()
{    return parameters == null ? null : Collections.unmodifiableMap(parameters);}
f8821
0
parameter
public SchemaBuilder kafkatest_f8822_0(String propertyName, String propertyValue)
{    // can print their properties in a consistent order.    if (parameters == null)        parameters = new LinkedHashMap<>();    parameters.put(propertyName, propertyValue);    return this;}
f8822
0
parameters
public SchemaBuilder kafkatest_f8823_0(Map<String, String> props)
{    // Avoid creating an empty set of properties so we never have an empty map    if (props.isEmpty())        return this;    if (parameters == null)        parameters = new LinkedHashMap<>();    parameters.putAll(props);    return this;}
f8823
0
float64
public static SchemaBuilder kafkatest_f8831_0()
{    return new SchemaBuilder(Type.FLOAT64);}
f8831
0
bool
public static SchemaBuilder kafkatest_f8832_0()
{    return new SchemaBuilder(Type.BOOLEAN);}
f8832
0
string
public static SchemaBuilder kafkatest_f8833_0()
{    return new SchemaBuilder(Type.STRING);}
f8833
0
keySchema
public Schema kafkatest_f8841_0()
{    return keySchema;}
f8841
0
valueSchema
public Schema kafkatest_f8842_0()
{    return valueSchema;}
f8842
0
build
public Schema kafkatest_f8843_0()
{    return new ConnectSchema(type, isOptional(), defaultValue, name, version, doc, parameters == null ? null : Collections.unmodifiableMap(parameters), fields == null ? null : Collections.unmodifiableList(new ArrayList<Field>(fields.values())), keySchema, valueSchema);}
f8843
0
projectArray
private static Object kafkatest_f8851_0(Schema source, Object record, Schema target) throws SchemaProjectorException
{    List<?> array = (List<?>) record;    List<Object> retArray = new ArrayList<>();    for (Object entry : array) {        retArray.add(project(source.valueSchema(), entry, target.valueSchema()));    }    return retArray;}
f8851
0
projectMap
private static Object kafkatest_f8852_0(Schema source, Object record, Schema target) throws SchemaProjectorException
{    Map<?, ?> map = (Map<?, ?>) record;    Map<Object, Object> retMap = new HashMap<>();    for (Map.Entry<?, ?> entry : map.entrySet()) {        Object key = entry.getKey();        Object value = entry.getValue();        Object retKey = project(source.keySchema(), key, target.keySchema());        Object retValue = project(source.valueSchema(), value, target.valueSchema());        retMap.put(retKey, retValue);    }    return retMap;}
f8852
0
projectPrimitive
private static Object kafkatest_f8853_0(Schema source, Object record, Schema target) throws SchemaProjectorException
{    assert source.type().isPrimitive();    assert target.type().isPrimitive();    Object result;    if (isPromotable(source.type(), target.type()) && record instanceof Number) {        Number numberRecord = (Number) record;        switch(target.type()) {            case INT8:                result = numberRecord.byteValue();                break;            case INT16:                result = numberRecord.shortValue();                break;            case INT32:                result = numberRecord.intValue();                break;            case INT64:                result = numberRecord.longValue();                break;            case FLOAT32:                result = numberRecord.floatValue();                break;            case FLOAT64:                result = numberRecord.doubleValue();                break;            default:                throw new SchemaProjectorException("Not promotable type.");        }    } else {        result = record;    }    return result;}
f8853
0
getInt32
public Integer kafkatest_f8861_0(String fieldName)
{    return (Integer) getCheckType(fieldName, Schema.Type.INT32);}
f8861
0
getInt64
public Long kafkatest_f8862_0(String fieldName)
{    return (Long) getCheckType(fieldName, Schema.Type.INT64);}
f8862
0
getFloat32
public Float kafkatest_f8863_0(String fieldName)
{    return (Float) getCheckType(fieldName, Schema.Type.FLOAT32);}
f8863
0
put
public Struct kafkatest_f8871_0(String fieldName, Object value)
{    Field field = lookupField(fieldName);    return put(field, value);}
f8871
0
put
public Struct kafkatest_f8872_0(Field field, Object value)
{    if (null == field)        throw new DataException("field cannot be null.");    ConnectSchema.validateValue(field.name(), field.schema(), value);    values[field.index()] = value;    return this;}
f8872
0
validate
public void kafkatest_f8873_0()
{    for (Field field : schema.fields()) {        Schema fieldSchema = field.schema();        Object value = values[field.index()];        if (value == null && (fieldSchema.isOptional() || fieldSchema.defaultValue() != null))            continue;        ConnectSchema.validateValue(field.name(), fieldSchema, value);    }}
f8873
0
toLogical
public static java.util.Date kafkatest_f8881_0(Schema schema, int value)
{    if (!(LOGICAL_NAME.equals(schema.name())))        throw new DataException("Requested conversion of Date object but the schema does not match.");    if (value < 0 || value > MILLIS_PER_DAY)        throw new DataException("Time values must use number of milliseconds greater than 0 and less than 86400000");    return new java.util.Date(value);}
f8881
0
builder
public static SchemaBuilder kafkatest_f8882_0()
{    return SchemaBuilder.int64().name(LOGICAL_NAME).version(1);}
f8882
0
fromLogical
public static long kafkatest_f8883_0(Schema schema, java.util.Date value)
{    if (!(LOGICAL_NAME.equals(schema.name())))        throw new DataException("Requested conversion of Timestamp object but the schema does not match.");    return value.getTime();}
f8883
0
convertToDouble
public static Double kafkatest_f8891_0(Schema schema, Object value) throws DataException
{    return (Double) convertTo(Schema.OPTIONAL_FLOAT64_SCHEMA, schema, value);}
f8891
0
convertToString
public static String kafkatest_f8892_0(Schema schema, Object value)
{    return (String) convertTo(Schema.OPTIONAL_STRING_SCHEMA, schema, value);}
f8892
0
convertToList
public static List<?> kafkatest_f8893_0(Schema schema, Object value)
{    return (List<?>) convertTo(ARRAY_SELECTOR_SCHEMA, schema, value);}
f8893
0
parseString
public static SchemaAndValue kafkatest_f8901_0(String value)
{    if (value == null) {        return NULL_SCHEMA_AND_VALUE;    }    if (value.isEmpty()) {        return new SchemaAndValue(Schema.STRING_SCHEMA, value);    }    Parser parser = new Parser(value);    return parse(parser, false);}
f8901
0
convertTo
protected static Object kafkatest_f8902_0(Schema toSchema, Schema fromSchema, Object value) throws DataException
{    if (value == null) {        if (toSchema.isOptional()) {            return null;        }        throw new DataException("Unable to convert a null value to a schema that requires a value");    }    switch(toSchema.type()) {        case BYTES:            if (Decimal.LOGICAL_NAME.equals(toSchema.name())) {                if (value instanceof ByteBuffer) {                    value = Utils.toArray((ByteBuffer) value);                }                if (value instanceof byte[]) {                    return Decimal.toLogical(toSchema, (byte[]) value);                }                if (value instanceof BigDecimal) {                    return value;                }                if (value instanceof Number) {                    // Not already a decimal, so treat it as a double ...                    double converted = ((Number) value).doubleValue();                    return new BigDecimal(converted);                }                if (value instanceof String) {                    return new BigDecimal(value.toString()).doubleValue();                }            }            if (value instanceof ByteBuffer) {                return Utils.toArray((ByteBuffer) value);            }            if (value instanceof byte[]) {                return value;            }            if (value instanceof BigDecimal) {                return Decimal.fromLogical(toSchema, (BigDecimal) value);            }            break;        case STRING:            StringBuilder sb = new StringBuilder();            append(sb, value, false);            return sb.toString();        case BOOLEAN:            if (value instanceof Boolean) {                return value;            }            if (value instanceof String) {                SchemaAndValue parsed = parseString(value.toString());                if (parsed.value() instanceof Boolean) {                    return parsed.value();                }            }            return asLong(value, fromSchema, null) == 0L ? Boolean.FALSE : Boolean.TRUE;        case INT8:            if (value instanceof Byte) {                return value;            }            return (byte) asLong(value, fromSchema, null);        case INT16:            if (value instanceof Short) {                return value;            }            return (short) asLong(value, fromSchema, null);        case INT32:            if (Date.LOGICAL_NAME.equals(toSchema.name())) {                if (value instanceof String) {                    SchemaAndValue parsed = parseString(value.toString());                    value = parsed.value();                }                if (value instanceof java.util.Date) {                    if (fromSchema != null) {                        String fromSchemaName = fromSchema.name();                        if (Date.LOGICAL_NAME.equals(fromSchemaName)) {                            return value;                        }                        if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {                            // Just get the number of days from this timestamp                            long millis = ((java.util.Date) value).getTime();                            // truncates                            int days = (int) (millis / MILLIS_PER_DAY);                            return Date.toLogical(toSchema, days);                        }                    }                }                long numeric = asLong(value, fromSchema, null);                return Date.toLogical(toSchema, (int) numeric);            }            if (Time.LOGICAL_NAME.equals(toSchema.name())) {                if (value instanceof String) {                    SchemaAndValue parsed = parseString(value.toString());                    value = parsed.value();                }                if (value instanceof java.util.Date) {                    if (fromSchema != null) {                        String fromSchemaName = fromSchema.name();                        if (Time.LOGICAL_NAME.equals(fromSchemaName)) {                            return value;                        }                        if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {                            // Just get the time portion of this timestamp                            Calendar calendar = Calendar.getInstance(UTC);                            calendar.setTime((java.util.Date) value);                            calendar.set(Calendar.YEAR, 1970);                            // Months are zero-based                            calendar.set(Calendar.MONTH, 0);                            calendar.set(Calendar.DAY_OF_MONTH, 1);                            return Time.toLogical(toSchema, (int) calendar.getTimeInMillis());                        }                    }                }                long numeric = asLong(value, fromSchema, null);                return Time.toLogical(toSchema, (int) numeric);            }            if (value instanceof Integer) {                return value;            }            return (int) asLong(value, fromSchema, null);        case INT64:            if (Timestamp.LOGICAL_NAME.equals(toSchema.name())) {                if (value instanceof String) {                    SchemaAndValue parsed = parseString(value.toString());                    value = parsed.value();                }                if (value instanceof java.util.Date) {                    java.util.Date date = (java.util.Date) value;                    if (fromSchema != null) {                        String fromSchemaName = fromSchema.name();                        if (Date.LOGICAL_NAME.equals(fromSchemaName)) {                            int days = Date.fromLogical(fromSchema, date);                            long millis = days * MILLIS_PER_DAY;                            return Timestamp.toLogical(toSchema, millis);                        }                        if (Time.LOGICAL_NAME.equals(fromSchemaName)) {                            long millis = Time.fromLogical(fromSchema, date);                            return Timestamp.toLogical(toSchema, millis);                        }                        if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {                            return value;                        }                    }                }                long numeric = asLong(value, fromSchema, null);                return Timestamp.toLogical(toSchema, numeric);            }            if (value instanceof Long) {                return value;            }            return asLong(value, fromSchema, null);        case FLOAT32:            if (value instanceof Float) {                return value;            }            return (float) asDouble(value, fromSchema, null);        case FLOAT64:            if (value instanceof Double) {                return value;            }            return asDouble(value, fromSchema, null);        case ARRAY:            if (value instanceof String) {                SchemaAndValue schemaAndValue = parseString(value.toString());                value = schemaAndValue.value();            }            if (value instanceof List) {                return value;            }            break;        case MAP:            if (value instanceof String) {                SchemaAndValue schemaAndValue = parseString(value.toString());                value = schemaAndValue.value();            }            if (value instanceof Map) {                return value;            }            break;        case STRUCT:            if (value instanceof Struct) {                Struct struct = (Struct) value;                return struct;            }    }    throw new DataException("Unable to convert " + value + " (" + value.getClass() + ") to " + toSchema);}
f8902
0
asLong
protected static long kafkatest_f8903_0(Object value, Schema fromSchema, Throwable error)
{    try {        if (value instanceof Number) {            Number number = (Number) value;            return number.longValue();        }        if (value instanceof String) {            return new BigDecimal(value.toString()).longValue();        }    } catch (NumberFormatException e) {        error = e;    // fall through    }    if (fromSchema != null) {        String schemaName = fromSchema.name();        if (value instanceof java.util.Date) {            if (Date.LOGICAL_NAME.equals(schemaName)) {                return Date.fromLogical(fromSchema, (java.util.Date) value);            }            if (Time.LOGICAL_NAME.equals(schemaName)) {                return Time.fromLogical(fromSchema, (java.util.Date) value);            }            if (Timestamp.LOGICAL_NAME.equals(schemaName)) {                return Timestamp.fromLogical(fromSchema, (java.util.Date) value);            }        }        throw new DataException("Unable to convert " + value + " (" + value.getClass() + ") to " + fromSchema, error);    }    throw new DataException("Unable to convert " + value + " (" + value.getClass() + ") to a number", error);}
f8903
0
alignListEntriesWithSchema
protected static List<Object> kafkatest_f8911_0(Schema schema, List<Object> input)
{    if (schema == null) {        return input;    }    Schema valueSchema = schema.valueSchema();    List<Object> result = new ArrayList<>();    for (Object value : input) {        Object newValue = convertTo(valueSchema, null, value);        result.add(newValue);    }    return result;}
f8911
0
alignMapKeysAndValuesWithSchema
protected static Map<Object, Object> kafkatest_f8912_0(Schema mapSchema, Map<Object, Object> input)
{    if (mapSchema == null) {        return input;    }    Schema keySchema = mapSchema.keySchema();    Schema valueSchema = mapSchema.valueSchema();    Map<Object, Object> result = new LinkedHashMap<>();    for (Map.Entry<?, ?> entry : input.entrySet()) {        Object newKey = convertTo(keySchema, null, entry.getKey());        Object newValue = convertTo(valueSchema, null, entry.getValue());        result.put(newKey, newValue);    }    return result;}
f8912
0
canDetect
public boolean kafkatest_f8913_0(Object value)
{    if (value == null) {        optional = true;        return true;    }    Schema schema = inferSchema(value);    if (schema == null) {        return false;    }    if (knownType == null) {        knownType = schema.type();    } else if (knownType != schema.type()) {        return false;    }    return true;}
f8913
0
next
public String kafkatest_f8921_0()
{    if (nextToken != null) {        previousToken = nextToken;        nextToken = null;    } else {        previousToken = consumeNextToken();    }    return previousToken;}
f8921
0
consumeNextToken
private String kafkatest_f8922_0() throws NoSuchElementException
{    boolean escaped = false;    int start = iter.getIndex();    char c = iter.current();    while (c != CharacterIterator.DONE) {        switch(c) {            case '\\':                escaped = !escaped;                break;            case ':':            case ',':            case '{':            case '}':            case '[':            case ']':            case '\"':                if (!escaped) {                    if (start < iter.getIndex()) {                        // Return the previous token                        return original.substring(start, iter.getIndex());                    }                    // Consume and return this delimiter as a token                    iter.next();                    return original.substring(start, start + 1);                }                // escaped, so continue                escaped = false;                break;            default:                // If escaped, then we don't care what was escaped                escaped = false;                break;        }        c = iter.next();    }    return original.substring(start, iter.getIndex());}
f8922
0
previous
public String kafkatest_f8923_0()
{    return previousToken;}
f8923
0
with
public Header kafkatest_f8931_0(Schema schema, Object value)
{    return new ConnectHeader(key, new SchemaAndValue(schema, value));}
f8931
0
hashCode
public int kafkatest_f8932_0()
{    return Objects.hash(key, schemaAndValue);}
f8932
0
equals
public boolean kafkatest_f8933_0(Object obj)
{    if (obj == this) {        return true;    }    if (obj instanceof Header) {        Header that = (Header) obj;        return Objects.equals(this.key, that.key()) && Objects.equals(this.schema(), that.schema()) && Objects.equals(this.value(), that.value());    }    return false;}
f8933
0
add
public Headers kafkatest_f8941_0(Header header)
{    Objects.requireNonNull(header, "Unable to add a null header.");    if (headers == null) {        headers = new LinkedList<>();    }    headers.add(header);    return this;}
f8941
0
addWithoutValidating
protected Headers kafkatest_f8942_0(String key, Object value, Schema schema)
{    return add(new ConnectHeader(key, new SchemaAndValue(schema, value)));}
f8942
0
add
public Headers kafkatest_f8943_0(String key, SchemaAndValue schemaAndValue)
{    checkSchemaMatches(schemaAndValue);    return add(new ConnectHeader(key, schemaAndValue != null ? schemaAndValue : SchemaAndValue.NULL));}
f8943
0
addLong
public Headers kafkatest_f8951_0(String key, long value)
{    return addWithoutValidating(key, value, Schema.INT64_SCHEMA);}
f8951
0
addFloat
public Headers kafkatest_f8952_0(String key, float value)
{    return addWithoutValidating(key, value, Schema.FLOAT32_SCHEMA);}
f8952
0
addDouble
public Headers kafkatest_f8953_0(String key, double value)
{    return addWithoutValidating(key, value, Schema.FLOAT64_SCHEMA);}
f8953
0
lastWithName
public Header kafkatest_f8961_0(String key)
{    checkKey(key);    if (headers != null) {        ListIterator<Header> iter = headers.listIterator(headers.size());        while (iter.hasPrevious()) {            Header header = iter.previous();            if (key.equals(header.key())) {                return header;            }        }    }    return null;}
f8961
0
allWithName
public Iterator<Header> kafkatest_f8962_0(String key)
{    return new FilterByKeyIterator(iterator(), key);}
f8962
0
iterator
public Iterator<Header> kafkatest_f8963_0()
{    if (headers != null) {        return headers.iterator();    }    return EMPTY_ITERATOR;}
f8963
0
toString
public String kafkatest_f8971_0()
{    return "ConnectHeaders(headers=" + (headers != null ? headers : "") + ")";}
f8971
0
duplicate
public ConnectHeaders kafkatest_f8972_0()
{    return new ConnectHeaders(this);}
f8972
0
checkKey
private void kafkatest_f8973_0(String key)
{    Objects.requireNonNull(key, "Header key cannot be null");}
f8973
0
clusterDetails
 ConnectClusterDetails kafkatest_f8981_0()
{    throw new UnsupportedOperationException();}
f8981
0
name
public String kafkatest_f8982_0()
{    return name;}
f8982
0
connectorState
public ConnectorState kafkatest_f8983_0()
{    return connectorState;}
f8983
0
timestampType
public TimestampType kafkatest_f8991_0()
{    return timestampType;}
f8991
0
newRecord
public SinkRecord kafkatest_f8992_0(String topic, Integer kafkaPartition, Schema keySchema, Object key, Schema valueSchema, Object value, Long timestamp)
{    return newRecord(topic, kafkaPartition, keySchema, key, valueSchema, value, timestamp, headers().duplicate());}
f8992
0
newRecord
public SinkRecord kafkatest_f8993_0(String topic, Integer kafkaPartition, Schema keySchema, Object key, Schema valueSchema, Object value, Long timestamp, Iterable<Header> headers)
{    return new SinkRecord(topic, kafkaPartition, keySchema, key, valueSchema, value, kafkaOffset(), timestamp, timestampType, headers);}
f8993
0
sourcePartition
public Map<String, ?> kafkatest_f9004_0()
{    return sourcePartition;}
f9004
0
sourceOffset
public Map<String, ?> kafkatest_f9005_0()
{    return sourceOffset;}
f9005
0
newRecord
public SourceRecord kafkatest_f9006_0(String topic, Integer kafkaPartition, Schema keySchema, Object key, Schema valueSchema, Object value, Long timestamp)
{    return newRecord(topic, kafkaPartition, keySchema, key, valueSchema, value, timestamp, headers().duplicate());}
f9006
0
newConfigDef
public static ConfigDef kafkatest_f9014_0()
{    return new ConfigDef().define(TYPE_CONFIG, Type.STRING, ConfigDef.NO_DEFAULT_VALUE, in(ConverterType.KEY.getName(), ConverterType.VALUE.getName(), ConverterType.HEADER.getName()), Importance.LOW, TYPE_DOC);}
f9014
0
type
public ConverterType kafkatest_f9015_0()
{    return ConverterType.withName(getString(TYPE_CONFIG));}
f9015
0
withName
public static ConverterType kafkatest_f9016_0(String name)
{    if (name == null) {        return null;    }    return NAME_TO_TYPE.get(name.toLowerCase(Locale.getDefault()));}
f9016
0
configure
public void kafkatest_f9024_0(Map<String, ?> configs)
{    StringConverterConfig conf = new StringConverterConfig(configs);    String encoding = conf.encoding();    Map<String, Object> serializerConfigs = new HashMap<>(configs);    Map<String, Object> deserializerConfigs = new HashMap<>(configs);    serializerConfigs.put("serializer.encoding", encoding);    deserializerConfigs.put("deserializer.encoding", encoding);    boolean isKey = conf.type() == ConverterType.KEY;    serializer.configure(serializerConfigs, isKey);    deserializer.configure(deserializerConfigs, isKey);}
f9024
0
configure
public void kafkatest_f9025_0(Map<String, ?> configs, boolean isKey)
{    Map<String, Object> conf = new HashMap<>(configs);    conf.put(StringConverterConfig.TYPE_CONFIG, isKey ? ConverterType.KEY.getName() : ConverterType.VALUE.getName());    configure(conf);}
f9025
0
fromConnectData
public byte[] kafkatest_f9026_0(String topic, Schema schema, Object value)
{    try {        return serializer.serialize(topic, value == null ? null : value.toString());    } catch (SerializationException e) {        throw new DataException("Failed to serialize to a string: ", e);    }}
f9026
0
testDefaultReconfigure
public void kafkatest_f9034_0()
{    TestConnector conn = new TestConnector(false);    conn.reconfigure(Collections.<String, String>emptyMap());    assertEquals(conn.stopOrder, 0);    assertEquals(conn.configureOrder, 1);}
f9034
0
testReconfigureStopException
public void kafkatest_f9035_0()
{    TestConnector conn = new TestConnector(true);    conn.reconfigure(Collections.<String, String>emptyMap());}
f9035
0
version
public String kafkatest_f9036_0()
{    return "1.0";}
f9036
0
testValidateValueMatchingType
public void kafkatest_f9044_0()
{    ConnectSchema.validateValue(Schema.INT8_SCHEMA, (byte) 1);    ConnectSchema.validateValue(Schema.INT16_SCHEMA, (short) 1);    ConnectSchema.validateValue(Schema.INT32_SCHEMA, 1);    ConnectSchema.validateValue(Schema.INT64_SCHEMA, (long) 1);    ConnectSchema.validateValue(Schema.FLOAT32_SCHEMA, 1.f);    ConnectSchema.validateValue(Schema.FLOAT64_SCHEMA, 1.);    ConnectSchema.validateValue(Schema.BOOLEAN_SCHEMA, true);    ConnectSchema.validateValue(Schema.STRING_SCHEMA, "a string");    ConnectSchema.validateValue(Schema.BYTES_SCHEMA, "a byte array".getBytes());    ConnectSchema.validateValue(Schema.BYTES_SCHEMA, ByteBuffer.wrap("a byte array".getBytes()));    ConnectSchema.validateValue(SchemaBuilder.array(Schema.INT32_SCHEMA).build(), Arrays.asList(1, 2, 3));    ConnectSchema.validateValue(SchemaBuilder.map(Schema.INT32_SCHEMA, Schema.STRING_SCHEMA).build(), Collections.singletonMap(1, "value"));    // Struct tests the basic struct layout + complex field types + nested structs    Struct structValue = new Struct(STRUCT_SCHEMA).put("first", 1).put("second", "foo").put("array", Arrays.asList(1, 2, 3)).put("map", Collections.singletonMap(1, "value")).put("nested", new Struct(FLAT_STRUCT_SCHEMA).put("field", 12));    ConnectSchema.validateValue(STRUCT_SCHEMA, structValue);}
f9044
0
testValidateValueMatchingLogicalType
public void kafkatest_f9045_0()
{    ConnectSchema.validateValue(Decimal.schema(2), new BigDecimal(new BigInteger("156"), 2));    ConnectSchema.validateValue(Date.SCHEMA, new java.util.Date(0));    ConnectSchema.validateValue(Time.SCHEMA, new java.util.Date(0));    ConnectSchema.validateValue(Timestamp.SCHEMA, new java.util.Date(0));}
f9045
0
testValidateValueMismatchInt8
public void kafkatest_f9046_0()
{    ConnectSchema.validateValue(Schema.INT8_SCHEMA, 1);}
f9046
0
testValidateValueMismatchBytes
public void kafkatest_f9054_0()
{    ConnectSchema.validateValue(Schema.BYTES_SCHEMA, new Object[] { 1, "foo" });}
f9054
0
testValidateValueMismatchArray
public void kafkatest_f9055_0()
{    ConnectSchema.validateValue(SchemaBuilder.array(Schema.INT32_SCHEMA).build(), Arrays.asList("a", "b", "c"));}
f9055
0
testValidateValueMismatchArraySomeMatch
public void kafkatest_f9056_0()
{    // Even if some match the right type, this should fail if any mismatch. In this case, type erasure loses    // the fact that the list is actually List<Object>, but we couldn't tell if only checking the first element    ConnectSchema.validateValue(SchemaBuilder.array(Schema.INT32_SCHEMA).build(), Arrays.asList(1, 2, "c"));}
f9056
0
testValidateValueMismatchDate
public void kafkatest_f9064_0()
{    ConnectSchema.validateValue(Date.SCHEMA, 1000L);}
f9064
0
testValidateValueMismatchTime
public void kafkatest_f9065_0()
{    ConnectSchema.validateValue(Time.SCHEMA, 1000L);}
f9065
0
testValidateValueMismatchTimestamp
public void kafkatest_f9066_0()
{    ConnectSchema.validateValue(Timestamp.SCHEMA, 1000L);}
f9066
0
testFromLogical
public void kafkatest_f9074_0()
{    assertEquals(0, Date.fromLogical(Date.SCHEMA, EPOCH.getTime()));    assertEquals(10000, Date.fromLogical(Date.SCHEMA, EPOCH_PLUS_TEN_THOUSAND_DAYS.getTime()));}
f9074
0
testFromLogicalInvalidSchema
public void kafkatest_f9075_0()
{    Date.fromLogical(Date.builder().name("invalid").build(), EPOCH.getTime());}
f9075
0
testFromLogicalInvalidHasTimeComponents
public void kafkatest_f9076_0()
{    Date.fromLogical(Date.SCHEMA, EPOCH_PLUS_TIME_COMPONENT.getTime());}
f9076
0
defaultValue
public Object kafkatest_f9084_0()
{    return null;}
f9084
0
name
public String kafkatest_f9085_0()
{    return "fake";}
f9085
0
version
public Integer kafkatest_f9086_0()
{    return null;}
f9086
0
testEquality
public void kafkatest_f9094_0()
{    Field field1 = new Field("name", 0, Schema.INT8_SCHEMA);    Field field2 = new Field("name", 0, Schema.INT8_SCHEMA);    Field differentName = new Field("name2", 0, Schema.INT8_SCHEMA);    Field differentIndex = new Field("name", 1, Schema.INT8_SCHEMA);    Field differentSchema = new Field("name", 0, Schema.INT16_SCHEMA);    assertEquals(field1, field2);    assertNotEquals(field1, differentName);    assertNotEquals(field1, differentIndex);    assertNotEquals(field1, differentSchema);}
f9094
0
testInt8Builder
public void kafkatest_f9095_0()
{    Schema schema = SchemaBuilder.int8().build();    assertTypeAndDefault(schema, Schema.Type.INT8, false, null);    assertNoMetadata(schema);    schema = SchemaBuilder.int8().name(NAME).optional().defaultValue((byte) 12).version(VERSION).doc(DOC).build();    assertTypeAndDefault(schema, Schema.Type.INT8, true, (byte) 12);    assertMetadata(schema, NAME, VERSION, DOC, NO_PARAMS);}
f9095
0
testInt8BuilderInvalidDefault
public void kafkatest_f9096_0()
{    SchemaBuilder.int8().defaultValue("invalid");}
f9096
0
testFloatBuilderInvalidDefault
public void kafkatest_f9104_0()
{    SchemaBuilder.float32().defaultValue("invalid");}
f9104
0
testDoubleBuilder
public void kafkatest_f9105_0()
{    Schema schema = SchemaBuilder.float64().build();    assertTypeAndDefault(schema, Schema.Type.FLOAT64, false, null);    assertNoMetadata(schema);    schema = SchemaBuilder.float64().name(NAME).optional().defaultValue(12.0).version(VERSION).doc(DOC).build();    assertTypeAndDefault(schema, Schema.Type.FLOAT64, true, 12.0);    assertMetadata(schema, NAME, VERSION, DOC, NO_PARAMS);}
f9105
0
testDoubleBuilderInvalidDefault
public void kafkatest_f9106_0()
{    SchemaBuilder.float64().defaultValue("invalid");}
f9106
0
testStructBuilder
public void kafkatest_f9114_0()
{    Schema schema = SchemaBuilder.struct().field("field1", Schema.INT8_SCHEMA).field("field2", Schema.INT8_SCHEMA).build();    assertTypeAndDefault(schema, Schema.Type.STRUCT, false, null);    assertEquals(2, schema.fields().size());    assertEquals("field1", schema.fields().get(0).name());    assertEquals(0, schema.fields().get(0).index());    assertEquals(Schema.INT8_SCHEMA, schema.fields().get(0).schema());    assertEquals("field2", schema.fields().get(1).name());    assertEquals(1, schema.fields().get(1).index());    assertEquals(Schema.INT8_SCHEMA, schema.fields().get(1).schema());    assertNoMetadata(schema);}
f9114
0
testNonStructCantHaveFields
public void kafkatest_f9115_0()
{    SchemaBuilder.int8().field("field", SchemaBuilder.int8().build());}
f9115
0
testArrayBuilder
public void kafkatest_f9116_0()
{    Schema schema = SchemaBuilder.array(Schema.INT8_SCHEMA).build();    assertTypeAndDefault(schema, Schema.Type.ARRAY, false, null);    assertEquals(schema.valueSchema(), Schema.INT8_SCHEMA);    assertNoMetadata(schema);    // Default value    List<Byte> defArray = Arrays.asList((byte) 1, (byte) 2);    schema = SchemaBuilder.array(Schema.INT8_SCHEMA).defaultValue(defArray).build();    assertTypeAndDefault(schema, Schema.Type.ARRAY, false, defArray);    assertEquals(schema.valueSchema(), Schema.INT8_SCHEMA);    assertNoMetadata(schema);}
f9116
0
testFieldNameNull
public void kafkatest_f9124_0()
{    Schema schema = SchemaBuilder.struct().field(null, Schema.STRING_SCHEMA).build();}
f9124
0
testFieldSchemaNull
public void kafkatest_f9125_0()
{    Schema schema = SchemaBuilder.struct().field("fieldName", null).build();}
f9125
0
testArraySchemaNull
public void kafkatest_f9126_0()
{    Schema schema = SchemaBuilder.array(null).build();}
f9126
0
testNumericTypeProjection
public void kafkatest_f9134_0()
{    Schema[] promotableSchemas = { Schema.INT8_SCHEMA, Schema.INT16_SCHEMA, Schema.INT32_SCHEMA, Schema.INT64_SCHEMA, Schema.FLOAT32_SCHEMA, Schema.FLOAT64_SCHEMA };    Schema[] promotableOptionalSchemas = { Schema.OPTIONAL_INT8_SCHEMA, Schema.OPTIONAL_INT16_SCHEMA, Schema.OPTIONAL_INT32_SCHEMA, Schema.OPTIONAL_INT64_SCHEMA, Schema.OPTIONAL_FLOAT32_SCHEMA, Schema.OPTIONAL_FLOAT64_SCHEMA };    Object[] values = { (byte) 127, (short) 255, 32767, 327890L, 1.2F, 1.2345 };    Map<Object, List<?>> expectedProjected = new HashMap<>();    expectedProjected.put(values[0], Arrays.asList((byte) 127, (short) 127, 127, 127L, 127.F, 127.));    expectedProjected.put(values[1], Arrays.asList((short) 255, 255, 255L, 255.F, 255.));    expectedProjected.put(values[2], Arrays.asList(32767, 32767L, 32767.F, 32767.));    expectedProjected.put(values[3], Arrays.asList(327890L, 327890.F, 327890.));    expectedProjected.put(values[4], Arrays.asList(1.2F, 1.2));    expectedProjected.put(values[5], Arrays.asList(1.2345));    Object promoted;    for (int i = 0; i < promotableSchemas.length; ++i) {        Schema source = promotableSchemas[i];        List<?> expected = expectedProjected.get(values[i]);        for (int j = i; j < promotableSchemas.length; ++j) {            Schema target = promotableSchemas[j];            promoted = SchemaProjector.project(source, values[i], target);            if (target.type() == Type.FLOAT64) {                assertEquals((Double) (expected.get(j - i)), (double) promoted, 1e-6);            } else {                assertEquals(expected.get(j - i), promoted);            }        }        for (int j = i; j < promotableOptionalSchemas.length; ++j) {            Schema target = promotableOptionalSchemas[j];            promoted = SchemaProjector.project(source, values[i], target);            if (target.type() == Type.FLOAT64) {                assertEquals((Double) (expected.get(j - i)), (double) promoted, 1e-6);            } else {                assertEquals(expected.get(j - i), promoted);            }        }    }    for (int i = 0; i < promotableOptionalSchemas.length; ++i) {        Schema source = promotableSchemas[i];        List<?> expected = expectedProjected.get(values[i]);        for (int j = i; j < promotableOptionalSchemas.length; ++j) {            Schema target = promotableOptionalSchemas[j];            promoted = SchemaProjector.project(source, values[i], target);            if (target.type() == Type.FLOAT64) {                assertEquals((Double) (expected.get(j - i)), (double) promoted, 1e-6);            } else {                assertEquals(expected.get(j - i), promoted);            }        }    }    Schema[] nonPromotableSchemas = { Schema.BOOLEAN_SCHEMA, Schema.BYTES_SCHEMA, Schema.STRING_SCHEMA };    for (Schema promotableSchema : promotableSchemas) {        for (Schema nonPromotableSchema : nonPromotableSchemas) {            Object dummy = new Object();            try {                SchemaProjector.project(promotableSchema, dummy, nonPromotableSchema);                fail("Cannot promote " + promotableSchema.type() + " to " + nonPromotableSchema.type());            } catch (DataException e) {            // expected            }        }    }}
f9134
0
testPrimitiveOptionalProjection
public void kafkatest_f9135_0()
{    verifyOptionalProjection(Schema.OPTIONAL_BOOLEAN_SCHEMA, Type.BOOLEAN, false, true, false, true);    verifyOptionalProjection(Schema.OPTIONAL_BOOLEAN_SCHEMA, Type.BOOLEAN, false, true, false, false);    byte[] bytes = { (byte) 1, (byte) 2 };    byte[] defaultBytes = { (byte) 3, (byte) 4 };    verifyOptionalProjection(Schema.OPTIONAL_BYTES_SCHEMA, Type.BYTES, bytes, defaultBytes, bytes, true);    verifyOptionalProjection(Schema.OPTIONAL_BYTES_SCHEMA, Type.BYTES, bytes, defaultBytes, bytes, false);    verifyOptionalProjection(Schema.OPTIONAL_STRING_SCHEMA, Type.STRING, "abc", "def", "abc", true);    verifyOptionalProjection(Schema.OPTIONAL_STRING_SCHEMA, Type.STRING, "abc", "def", "abc", false);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.INT8, (byte) 12, (byte) 127, (byte) 12, true);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.INT8, (byte) 12, (byte) 127, (byte) 12, false);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.INT16, (byte) 12, (short) 127, (short) 12, true);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.INT16, (byte) 12, (short) 127, (short) 12, false);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.INT32, (byte) 12, 12789, 12, true);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.INT32, (byte) 12, 12789, 12, false);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.INT64, (byte) 12, 127890L, 12L, true);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.INT64, (byte) 12, 127890L, 12L, false);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.FLOAT32, (byte) 12, 3.45F, 12.F, true);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.FLOAT32, (byte) 12, 3.45F, 12.F, false);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.FLOAT64, (byte) 12, 3.4567, 12., true);    verifyOptionalProjection(Schema.OPTIONAL_INT8_SCHEMA, Type.FLOAT64, (byte) 12, 3.4567, 12., false);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.INT16, (short) 12, (short) 127, (short) 12, true);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.INT16, (short) 12, (short) 127, (short) 12, false);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.INT32, (short) 12, 12789, 12, true);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.INT32, (short) 12, 12789, 12, false);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.INT64, (short) 12, 127890L, 12L, true);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.INT64, (short) 12, 127890L, 12L, false);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.FLOAT32, (short) 12, 3.45F, 12.F, true);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.FLOAT32, (short) 12, 3.45F, 12.F, false);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.FLOAT64, (short) 12, 3.4567, 12., true);    verifyOptionalProjection(Schema.OPTIONAL_INT16_SCHEMA, Type.FLOAT64, (short) 12, 3.4567, 12., false);    verifyOptionalProjection(Schema.OPTIONAL_INT32_SCHEMA, Type.INT32, 12, 12789, 12, true);    verifyOptionalProjection(Schema.OPTIONAL_INT32_SCHEMA, Type.INT32, 12, 12789, 12, false);    verifyOptionalProjection(Schema.OPTIONAL_INT32_SCHEMA, Type.INT64, 12, 127890L, 12L, true);    verifyOptionalProjection(Schema.OPTIONAL_INT32_SCHEMA, Type.INT64, 12, 127890L, 12L, false);    verifyOptionalProjection(Schema.OPTIONAL_INT32_SCHEMA, Type.FLOAT32, 12, 3.45F, 12.F, true);    verifyOptionalProjection(Schema.OPTIONAL_INT32_SCHEMA, Type.FLOAT32, 12, 3.45F, 12.F, false);    verifyOptionalProjection(Schema.OPTIONAL_INT32_SCHEMA, Type.FLOAT64, 12, 3.4567, 12., true);    verifyOptionalProjection(Schema.OPTIONAL_INT32_SCHEMA, Type.FLOAT64, 12, 3.4567, 12., false);    verifyOptionalProjection(Schema.OPTIONAL_INT64_SCHEMA, Type.INT64, 12L, 127890L, 12L, true);    verifyOptionalProjection(Schema.OPTIONAL_INT64_SCHEMA, Type.INT64, 12L, 127890L, 12L, false);    verifyOptionalProjection(Schema.OPTIONAL_INT64_SCHEMA, Type.FLOAT32, 12L, 3.45F, 12.F, true);    verifyOptionalProjection(Schema.OPTIONAL_INT64_SCHEMA, Type.FLOAT32, 12L, 3.45F, 12.F, false);    verifyOptionalProjection(Schema.OPTIONAL_INT64_SCHEMA, Type.FLOAT64, 12L, 3.4567, 12., true);    verifyOptionalProjection(Schema.OPTIONAL_INT64_SCHEMA, Type.FLOAT64, 12L, 3.4567, 12., false);    verifyOptionalProjection(Schema.OPTIONAL_FLOAT32_SCHEMA, Type.FLOAT32, 12.345F, 3.45F, 12.345F, true);    verifyOptionalProjection(Schema.OPTIONAL_FLOAT32_SCHEMA, Type.FLOAT32, 12.345F, 3.45F, 12.345F, false);    verifyOptionalProjection(Schema.OPTIONAL_FLOAT32_SCHEMA, Type.FLOAT64, 12.345F, 3.4567, 12.345, true);    verifyOptionalProjection(Schema.OPTIONAL_FLOAT32_SCHEMA, Type.FLOAT64, 12.345F, 3.4567, 12.345, false);    verifyOptionalProjection(Schema.OPTIONAL_FLOAT32_SCHEMA, Type.FLOAT64, 12.345, 3.4567, 12.345, true);    verifyOptionalProjection(Schema.OPTIONAL_FLOAT32_SCHEMA, Type.FLOAT64, 12.345, 3.4567, 12.345, false);}
f9135
0
testStructAddField
public void kafkatest_f9136_0()
{    Schema source = SchemaBuilder.struct().field("field", Schema.INT32_SCHEMA).build();    Struct sourceStruct = new Struct(source);    sourceStruct.put("field", 1);    Schema target = SchemaBuilder.struct().field("field", Schema.INT32_SCHEMA).field("field2", SchemaBuilder.int32().defaultValue(123).build()).build();    Struct targetStruct = (Struct) SchemaProjector.project(source, sourceStruct, target);    assertEquals(1, (int) targetStruct.getInt32("field"));    assertEquals(123, (int) targetStruct.getInt32("field2"));    Schema incompatibleTargetSchema = SchemaBuilder.struct().field("field", Schema.INT32_SCHEMA).field("field2", Schema.INT32_SCHEMA).build();    try {        SchemaProjector.project(source, sourceStruct, incompatibleTargetSchema);        fail("Incompatible schema.");    } catch (DataException e) {    // expected    }}
f9136
0
testProjectMissingDefaultValuedStructField
public void kafkatest_f9144_0()
{    final Schema source = SchemaBuilder.struct().build();    final Schema target = SchemaBuilder.struct().field("id", SchemaBuilder.int64().defaultValue(42L).build()).build();    assertEquals(42L, (long) ((Struct) SchemaProjector.project(source, new Struct(source), target)).getInt64("id"));}
f9144
0
testProjectMissingOptionalStructField
public void kafkatest_f9145_0()
{    final Schema source = SchemaBuilder.struct().build();    final Schema target = SchemaBuilder.struct().field("id", SchemaBuilder.OPTIONAL_INT64_SCHEMA).build();    assertEquals(null, ((Struct) SchemaProjector.project(source, new Struct(source), target)).getInt64("id"));}
f9145
0
testProjectMissingRequiredField
public void kafkatest_f9146_0()
{    final Schema source = SchemaBuilder.struct().build();    final Schema target = SchemaBuilder.struct().field("id", SchemaBuilder.INT64_SCHEMA).build();    SchemaProjector.project(source, new Struct(source), target);}
f9146
0
testInvalidStructFieldValue
public void kafkatest_f9154_0()
{    new Struct(NESTED_SCHEMA).put("nested", new Struct(NESTED_CHILD_SCHEMA));}
f9154
0
testMissingFieldValidation
public void kafkatest_f9155_0()
{    // Required int8 field    Schema schema = SchemaBuilder.struct().field("field", REQUIRED_FIELD_SCHEMA).build();    Struct struct = new Struct(schema);    struct.validate();}
f9155
0
testMissingOptionalFieldValidation
public void kafkatest_f9156_0()
{    Schema schema = SchemaBuilder.struct().field("field", OPTIONAL_FIELD_SCHEMA).build();    Struct struct = new Struct(schema);    struct.validate();}
f9156
0
testPutNullField
public void kafkatest_f9164_0()
{    final String fieldName = "fieldName";    Schema testSchema = SchemaBuilder.struct().field(fieldName, Schema.STRING_SCHEMA);    Struct struct = new Struct(testSchema);    assertThrows(DataException.class, () -> struct.put((Field) null, "valid"));}
f9164
0
testInvalidPutIncludesFieldName
public void kafkatest_f9165_0()
{    final String fieldName = "fieldName";    Schema testSchema = SchemaBuilder.struct().field(fieldName, Schema.STRING_SCHEMA);    Struct struct = new Struct(testSchema);    Exception e = assertThrows(DataException.class, () -> struct.put(fieldName, null));    assertEquals("Invalid value: null used for required field: \"fieldName\", schema type: STRING", e.getMessage());}
f9165
0
testBuilder
public void kafkatest_f9166_0()
{    Schema plain = Date.SCHEMA;    assertEquals(Date.LOGICAL_NAME, plain.name());    assertEquals(1, (Object) plain.version());}
f9166
0
testFromLogicalInvalidHasDateComponents
public void kafkatest_f9174_0()
{    Time.fromLogical(Time.SCHEMA, EPOCH_PLUS_DATE_COMPONENT.getTime());}
f9174
0
testToLogical
public void kafkatest_f9175_0()
{    assertEquals(EPOCH.getTime(), Time.toLogical(Time.SCHEMA, 0));    assertEquals(EPOCH_PLUS_TEN_THOUSAND_MILLIS.getTime(), Time.toLogical(Time.SCHEMA, 10000));}
f9175
0
testToLogicalInvalidSchema
public void kafkatest_f9176_0()
{    Time.toLogical(Time.builder().name("invalid").build(), 0);}
f9176
0
shouldConvertMapWithStringKeys
public void kafkatest_f9184_0()
{    assertRoundTrip(STRING_MAP_SCHEMA, STRING_MAP_SCHEMA, STRING_MAP);}
f9184
0
shouldParseStringOfMapWithStringValuesWithoutWhitespaceAsMap
public void kafkatest_f9185_0()
{    SchemaAndValue result = roundTrip(STRING_MAP_SCHEMA, "{\"foo\":\"123\",\"bar\":\"baz\"}");    assertEquals(STRING_MAP_SCHEMA, result.schema());    assertEquals(STRING_MAP, result.value());}
f9185
0
shouldParseStringOfMapWithStringValuesWithWhitespaceAsMap
public void kafkatest_f9186_0()
{    SchemaAndValue result = roundTrip(STRING_MAP_SCHEMA, "{ \"foo\" : \"123\", \n\"bar\" : \"baz\" } ");    assertEquals(STRING_MAP_SCHEMA, result.schema());    assertEquals(STRING_MAP, result.value());}
f9186
0
shouldConvertListWithIntegerValues
public void kafkatest_f9194_0()
{    assertRoundTrip(INT_LIST_SCHEMA, INT_LIST_SCHEMA, INT_LIST);}
f9194
0
shouldConvertStringOfListWithOnlyNumericElementTypesIntoListOfLargestNumericType
public void kafkatest_f9195_0()
{    int thirdValue = Short.MAX_VALUE + 1;    List<?> list = Values.convertToList(Schema.STRING_SCHEMA, "[1, 2, " + thirdValue + "]");    assertEquals(3, list.size());    assertEquals(1, ((Number) list.get(0)).intValue());    assertEquals(2, ((Number) list.get(1)).intValue());    assertEquals(thirdValue, ((Number) list.get(2)).intValue());}
f9195
0
shouldConvertStringOfListWithMixedElementTypesIntoListWithDifferentElementTypes
public void kafkatest_f9196_0()
{    String str = "[1, 2, \"three\"]";    List<?> list = Values.convertToList(Schema.STRING_SCHEMA, str);    assertEquals(3, list.size());    assertEquals(1, ((Number) list.get(0)).intValue());    assertEquals(2, ((Number) list.get(1)).intValue());    assertEquals("three", list.get(2));}
f9196
0
shouldFailToParseStringOfMapWithIntValuesWithBlankEntries
public void kafkatest_f9204_0()
{    Values.convertToList(Schema.STRING_SCHEMA, " { \"foo\" :  \"1234567890\" ,, \"bar\" : \"0\",  \"baz\" : \"boz\" }  ");}
f9204
0
shouldFailToParseStringOfEmptyMap
public void kafkatest_f9205_0()
{    Values.convertToList(Schema.STRING_SCHEMA, " { }  ");}
f9205
0
shouldParseStringsWithoutDelimiters
public void kafkatest_f9206_0()
{    // assertParsed("");    assertParsed("  ");    assertParsed("simple");    assertParsed("simple string");    assertParsed("simple \n\t\bstring");    assertParsed("'simple' string");    assertParsed("si\\mple");    assertParsed("si\\\\mple");}
f9206
0
assertParsed
protected void kafkatest_f9215_0(String input, String... expectedTokens)
{    Parser parser = new Parser(input);    if (!parser.hasNext()) {        assertEquals(1, expectedTokens.length);        assertTrue(expectedTokens[0].isEmpty());        return;    }    for (String expectedToken : expectedTokens) {        assertTrue(parser.hasNext());        int position = parser.mark();        assertEquals(expectedToken, parser.next());        assertEquals(position + expectedToken.length(), parser.position());        assertEquals(expectedToken, parser.previous());        parser.rewindTo(position);        assertEquals(position, parser.position());        assertEquals(expectedToken, parser.next());        int newPosition = parser.mark();        assertEquals(position + expectedToken.length(), newPosition);        assertEquals(expectedToken, parser.previous());    }    assertFalse(parser.hasNext());    // Rewind and try consuming expected tokens ...    parser.rewindTo(0);    assertConsumable(parser, expectedTokens);    // Parse again and try consuming expected tokens ...    parser = new Parser(input);    assertConsumable(parser, expectedTokens);}
f9215
0
assertConsumable
protected void kafkatest_f9216_0(Parser parser, String... expectedTokens)
{    for (String expectedToken : expectedTokens) {        if (!expectedToken.trim().isEmpty()) {            int position = parser.mark();            assertTrue(parser.canConsume(expectedToken.trim()));            parser.rewindTo(position);            assertTrue(parser.canConsume(expectedToken.trim(), true));            parser.rewindTo(position);            assertTrue(parser.canConsume(expectedToken, false));        }    }}
f9216
0
roundTrip
protected SchemaAndValue kafkatest_f9217_0(Schema desiredSchema, String currentValue)
{    return roundTrip(desiredSchema, new SchemaAndValue(Schema.STRING_SCHEMA, currentValue));}
f9217
0
shouldHaveToString
public void kafkatest_f9225_0()
{    // empty    assertNotNull(headers.toString());    // not empty    populate(headers);    assertNotNull(headers.toString());}
f9225
0
shouldRetainLatestWhenEmpty
public void kafkatest_f9226_0()
{    headers.retainLatest(other);    headers.retainLatest(key);    headers.retainLatest();    assertTrue(headers.isEmpty());}
f9226
0
shouldAddMultipleHeadersWithSameKeyAndRetainLatest
public void kafkatest_f9227_0()
{    populate(headers);    Header header = headers.lastWithName(key);    assertHeader(header, key, Schema.STRING_SCHEMA, "third");    iter = headers.allWithName(key);    assertNextHeader(iter, key, Schema.BOOLEAN_SCHEMA, true);    assertNextHeader(iter, key, Schema.INT32_SCHEMA, 0);    assertNextHeader(iter, key, Schema.OPTIONAL_STRING_SCHEMA, null);    assertNextHeader(iter, key, Schema.STRING_SCHEMA, "third");    assertNoNextHeader(iter);    iter = headers.allWithName(other);    assertOnlyNextHeader(iter, other, Schema.STRING_SCHEMA, "other value");    headers.retainLatest(other);    assertOnlySingleHeader(other, Schema.STRING_SCHEMA, "other value");    headers.retainLatest(key);    assertOnlySingleHeader(key, Schema.STRING_SCHEMA, "third");    headers.retainLatest();    assertOnlySingleHeader(other, Schema.STRING_SCHEMA, "other value");    assertOnlySingleHeader(key, Schema.STRING_SCHEMA, "third");}
f9227
0
shouldTransformHeadersWhenEmpty
public void kafkatest_f9235_0()
{    headers.apply(appendToKey("-suffix"));    headers.apply(key, appendToKey("-suffix"));    assertTrue(headers.isEmpty());}
f9235
0
shouldTransformHeaders
public void kafkatest_f9236_0()
{    populate(headers);    iter = headers.allWithName(key);    assertNextHeader(iter, key, Schema.BOOLEAN_SCHEMA, true);    assertNextHeader(iter, key, Schema.INT32_SCHEMA, 0);    assertNextHeader(iter, key, Schema.OPTIONAL_STRING_SCHEMA, null);    assertNextHeader(iter, key, Schema.STRING_SCHEMA, "third");    assertNoNextHeader(iter);    iter = headers.allWithName(other);    assertOnlyNextHeader(iter, other, Schema.STRING_SCHEMA, "other value");    // Transform the headers    assertEquals(5, headers.size());    headers.apply(appendToKey("-suffix"));    assertEquals(5, headers.size());    assertNoHeaderWithKey(key);    assertNoHeaderWithKey(other);    String altKey = key + "-suffix";    iter = headers.allWithName(altKey);    assertNextHeader(iter, altKey, Schema.BOOLEAN_SCHEMA, true);    assertNextHeader(iter, altKey, Schema.INT32_SCHEMA, 0);    assertNextHeader(iter, altKey, Schema.OPTIONAL_STRING_SCHEMA, null);    assertNextHeader(iter, altKey, Schema.STRING_SCHEMA, "third");    assertNoNextHeader(iter);    iter = headers.allWithName(other + "-suffix");    assertOnlyNextHeader(iter, other + "-suffix", Schema.STRING_SCHEMA, "other value");}
f9236
0
shouldTransformHeadersWithKey
public void kafkatest_f9237_0()
{    populate(headers);    iter = headers.allWithName(key);    assertNextHeader(iter, key, Schema.BOOLEAN_SCHEMA, true);    assertNextHeader(iter, key, Schema.INT32_SCHEMA, 0);    assertNextHeader(iter, key, Schema.OPTIONAL_STRING_SCHEMA, null);    assertNextHeader(iter, key, Schema.STRING_SCHEMA, "third");    assertNoNextHeader(iter);    iter = headers.allWithName(other);    assertOnlyNextHeader(iter, other, Schema.STRING_SCHEMA, "other value");    // Transform the headers    assertEquals(5, headers.size());    headers.apply(key, appendToKey("-suffix"));    assertEquals(5, headers.size());    assertNoHeaderWithKey(key);    String altKey = key + "-suffix";    iter = headers.allWithName(altKey);    assertNextHeader(iter, altKey, Schema.BOOLEAN_SCHEMA, true);    assertNextHeader(iter, altKey, Schema.INT32_SCHEMA, 0);    assertNextHeader(iter, altKey, Schema.OPTIONAL_STRING_SCHEMA, null);    assertNextHeader(iter, altKey, Schema.STRING_SCHEMA, "third");    assertNoNextHeader(iter);    iter = headers.allWithName(other);    assertOnlyNextHeader(iter, other, Schema.STRING_SCHEMA, "other value");}
f9237
0
shouldNotValidateNullValuesWithBuiltInTypes
public void kafkatest_f9245_0()
{    assertSchemaDoesNotMatch(Schema.BOOLEAN_SCHEMA, null);    assertSchemaDoesNotMatch(Schema.BYTES_SCHEMA, null);    assertSchemaDoesNotMatch(Schema.INT8_SCHEMA, null);    assertSchemaDoesNotMatch(Schema.INT16_SCHEMA, null);    assertSchemaDoesNotMatch(Schema.INT32_SCHEMA, null);    assertSchemaDoesNotMatch(Schema.INT64_SCHEMA, null);    assertSchemaDoesNotMatch(Schema.FLOAT32_SCHEMA, null);    assertSchemaDoesNotMatch(Schema.FLOAT64_SCHEMA, null);    assertSchemaDoesNotMatch(Schema.STRING_SCHEMA, null);    assertSchemaDoesNotMatch(SchemaBuilder.array(Schema.STRING_SCHEMA), null);    assertSchemaDoesNotMatch(SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.INT32_SCHEMA), null);    assertSchemaDoesNotMatch(SchemaBuilder.struct(), null);}
f9245
0
shouldNotValidateMismatchedValuesWithBuiltInTypes
public void kafkatest_f9246_0()
{    assertSchemaDoesNotMatch(Schema.BOOLEAN_SCHEMA, 0L);    assertSchemaDoesNotMatch(Schema.BYTES_SCHEMA, "oops");    assertSchemaDoesNotMatch(Schema.INT8_SCHEMA, 1.0f);    assertSchemaDoesNotMatch(Schema.INT16_SCHEMA, 1.0f);    assertSchemaDoesNotMatch(Schema.INT32_SCHEMA, 0L);    assertSchemaDoesNotMatch(Schema.INT64_SCHEMA, 1.0f);    assertSchemaDoesNotMatch(Schema.FLOAT32_SCHEMA, 1L);    assertSchemaDoesNotMatch(Schema.FLOAT64_SCHEMA, 1L);    assertSchemaDoesNotMatch(Schema.STRING_SCHEMA, true);    assertSchemaDoesNotMatch(SchemaBuilder.array(Schema.STRING_SCHEMA), "value");    assertSchemaDoesNotMatch(SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.INT32_SCHEMA), "value");    assertSchemaDoesNotMatch(SchemaBuilder.struct(), new ArrayList<String>());}
f9246
0
shouldAddDate
public void kafkatest_f9247_0()
{    java.util.Date dateObj = EPOCH_PLUS_TEN_THOUSAND_DAYS.getTime();    int days = Date.fromLogical(Date.SCHEMA, dateObj);    headers.addDate(key, dateObj);    Header header = headers.lastWithName(key);    assertEquals(days, (int) Values.convertToInteger(header.schema(), header.value()));    assertSame(dateObj, Values.convertToDate(header.schema(), header.value()));    headers.addInt(other, days);    header = headers.lastWithName(other);    assertEquals(days, (int) Values.convertToInteger(header.schema(), header.value()));    assertEquals(dateObj, Values.convertToDate(header.schema(), header.value()));}
f9247
0
addHeader
protected void kafkatest_f9255_0(String key, Schema schema, Object value)
{    headers.add(key, value, schema);    Header header = headers.lastWithName(key);    assertNotNull(header);    assertHeader(header, key, schema, value);}
f9255
0
assertNoHeaderWithKey
protected void kafkatest_f9256_0(String key)
{    assertNoNextHeader(headers.allWithName(key));}
f9256
0
assertContainsHeader
protected void kafkatest_f9257_0(String key, Schema schema, Object value)
{    Header expected = new ConnectHeader(key, new SchemaAndValue(schema, value));    Iterator<Header> iter = headers.allWithName(key);    while (iter.hasNext()) {        Header header = iter.next();        if (header.equals(expected))            return;    }    fail("Should have found header " + expected);}
f9257
0
withString
protected Header kafkatest_f9265_0(String value)
{    return withValue(Schema.STRING_SCHEMA, value);}
f9265
0
shouldAllowNullValues
public void kafkatest_f9266_0()
{    withValue(Schema.OPTIONAL_STRING_SCHEMA, null);}
f9266
0
shouldAllowNullSchema
public void kafkatest_f9267_0()
{    withValue(null, null);    assertNull(header.schema());    assertNull(header.value());    String value = "non-null value";    withValue(null, value);    assertNull(header.schema());    assertSame(value, header.value());}
f9267
0
shouldDuplicateRecordUsingNewHeaders
public void kafkatest_f9275_0()
{    Headers newHeaders = new ConnectHeaders().addString("h3", "hv3");    SinkRecord duplicate = record.newRecord(TOPIC_NAME, PARTITION_NUMBER, Schema.STRING_SCHEMA, "key", Schema.BOOLEAN_SCHEMA, false, KAFKA_TIMESTAMP, newHeaders);    assertEquals(TOPIC_NAME, duplicate.topic());    assertEquals(PARTITION_NUMBER, duplicate.kafkaPartition());    assertEquals(Schema.STRING_SCHEMA, duplicate.keySchema());    assertEquals("key", duplicate.key());    assertEquals(Schema.BOOLEAN_SCHEMA, duplicate.valueSchema());    assertEquals(false, duplicate.value());    assertEquals(KAFKA_OFFSET, duplicate.kafkaOffset());    assertEquals(KAFKA_TIMESTAMP, duplicate.timestamp());    assertEquals(TS_TYPE, duplicate.timestampType());    assertNotNull(duplicate.headers());    assertEquals(newHeaders, duplicate.headers());    assertSame(newHeaders, duplicate.headers());    assertNotSame(record.headers(), duplicate.headers());    assertNotEquals(record.headers(), duplicate.headers());}
f9275
0
shouldModifyRecordHeader
public void kafkatest_f9276_0()
{    assertTrue(record.headers().isEmpty());    record.headers().addInt("intHeader", 100);    assertEquals(1, record.headers().size());    Header header = record.headers().lastWithName("intHeader");    assertEquals(100, (int) Values.convertToInteger(header.schema(), header.value()));}
f9276
0
beforeEach
public void kafkatest_f9277_0()
{    record = new SourceRecord(SOURCE_PARTITION, SOURCE_OFFSET, TOPIC_NAME, PARTITION_NUMBER, Schema.STRING_SCHEMA, "key", Schema.BOOLEAN_SCHEMA, false, KAFKA_TIMESTAMP, null);}
f9277
0
shouldConvertNullValue
public void kafkatest_f9285_0()
{    assertRoundTrip(Schema.STRING_SCHEMA, null);    assertRoundTrip(Schema.OPTIONAL_STRING_SCHEMA, null);}
f9285
0
shouldConvertSimpleString
public void kafkatest_f9286_0()
{    assertRoundTrip(Schema.STRING_SCHEMA, "simple");}
f9286
0
shouldConvertEmptyString
public void kafkatest_f9287_0()
{    assertRoundTrip(Schema.STRING_SCHEMA, "");}
f9287
0
shouldConvertMapWithStringKeysAndIntegerValues
public void kafkatest_f9295_0()
{    assertRoundTrip(STRING_INT_MAP_SCHEMA, STRING_INT_MAP);}
f9295
0
shouldParseStringOfMapWithIntValuesWithoutWhitespaceAsMap
public void kafkatest_f9296_0()
{    SchemaAndValue result = roundTrip(Schema.STRING_SCHEMA, "{\"foo\":1234567890,\"bar\":0,\"baz\":-987654321}");    assertEquals(STRING_INT_MAP_SCHEMA, result.schema());    assertEquals(STRING_INT_MAP, result.value());}
f9296
0
shouldParseStringOfMapWithIntValuesWithWhitespaceAsMap
public void kafkatest_f9297_0()
{    SchemaAndValue result = roundTrip(Schema.STRING_SCHEMA, " { \"foo\" :  1234567890 , \"bar\" : 0,  \"baz\" : -987654321 }  ");    assertEquals(STRING_INT_MAP_SCHEMA, result.schema());    assertEquals(STRING_INT_MAP, result.value());}
f9297
0
assertRoundTrip
protected void kafkatest_f9305_0(Schema schema, Object value)
{    byte[] serialized = converter.fromConnectHeader(TOPIC, HEADER, schema, value);    SchemaAndValue result = converter.toConnectHeader(TOPIC, HEADER, serialized);    if (value == null) {        assertNull(serialized);        assertNull(result.schema());        assertNull(result.value());    } else {        assertNotNull(serialized);        assertEquals(value, result.value());        assertEquals(schema, result.schema());        byte[] serialized2 = converter.fromConnectHeader(TOPIC, HEADER, result.schema(), result.value());        SchemaAndValue result2 = converter.toConnectHeader(TOPIC, HEADER, serialized2);        assertNotNull(serialized2);        assertEquals(schema, result2.schema());        assertEquals(value, result2.value());        assertEquals(result, result2);        assertArrayEquals(serialized, serialized);    }}
f9305
0
testStringToBytes
public void kafkatest_f9306_0() throws UnsupportedEncodingException
{    assertArrayEquals(SAMPLE_STRING.getBytes("UTF8"), converter.fromConnectData(TOPIC, Schema.STRING_SCHEMA, SAMPLE_STRING));}
f9306
0
testNonStringToBytes
public void kafkatest_f9307_0() throws UnsupportedEncodingException
{    assertArrayEquals("true".getBytes("UTF8"), converter.fromConnectData(TOPIC, Schema.BOOLEAN_SCHEMA, true));}
f9307
0
testNonStringHeaderValueToBytes
public void kafkatest_f9315_0() throws UnsupportedEncodingException
{    assertArrayEquals("true".getBytes("UTF8"), converter.fromConnectHeader(TOPIC, "hdr", Schema.BOOLEAN_SCHEMA, true));}
f9315
0
testNullHeaderValueToBytes
public void kafkatest_f9316_0()
{    assertEquals(null, converter.fromConnectHeader(TOPIC, "hdr", Schema.OPTIONAL_STRING_SCHEMA, null));}
f9316
0
testGroupPartitions
public void kafkatest_f9317_0()
{    List<List<Integer>> grouped = ConnectorUtils.groupPartitions(FIVE_ELEMENTS, 1);    assertEquals(Arrays.asList(FIVE_ELEMENTS), grouped);    grouped = ConnectorUtils.groupPartitions(FIVE_ELEMENTS, 2);    assertEquals(Arrays.asList(Arrays.asList(1, 2, 3), Arrays.asList(4, 5)), grouped);    grouped = ConnectorUtils.groupPartitions(FIVE_ELEMENTS, 3);    assertEquals(Arrays.asList(Arrays.asList(1, 2), Arrays.asList(3, 4), Arrays.asList(5)), grouped);    grouped = ConnectorUtils.groupPartitions(FIVE_ELEMENTS, 5);    assertEquals(Arrays.asList(Arrays.asList(1), Arrays.asList(2), Arrays.asList(3), Arrays.asList(4), Arrays.asList(5)), grouped);    grouped = ConnectorUtils.groupPartitions(FIVE_ELEMENTS, 7);    assertEquals(Arrays.asList(Arrays.asList(1), Arrays.asList(2), Arrays.asList(3), Arrays.asList(4), Arrays.asList(5), Collections.emptyList(), Collections.emptyList()), grouped);}
f9317
0
commit
public boolean kafkatest_f9327_0() throws LoginException
{    return authenticated;}
f9327
0
abort
public boolean kafkatest_f9328_0() throws LoginException
{    return true;}
f9328
0
logout
public boolean kafkatest_f9329_0() throws LoginException
{    return true;}
f9329
0
testUnknownLoginModule
public void kafkatest_f9337_0() throws IOException
{    setupJaasConfig("KafkaConnect1", "/tmp/testcrednetial", true);    Configuration.setConfiguration(null);    setMock("Basic", "user", "password", true);    jaasBasicAuthFilter.filter(requestContext);}
f9337
0
testUnknownCredentialsFile
public void kafkatest_f9338_0() throws IOException
{    setupJaasConfig("KafkaConnect", "/tmp/testcrednetial", true);    Configuration.setConfiguration(null);    setMock("Basic", "user", "password", true);    jaasBasicAuthFilter.filter(requestContext);}
f9338
0
testEmptyCredentialsFile
public void kafkatest_f9339_0() throws IOException
{    File jaasConfigFile = File.createTempFile("ks-jaas-", ".conf");    jaasConfigFile.deleteOnExit();    System.setProperty(JaasUtils.JAVA_LOGIN_CONFIG_PARAM, jaasConfigFile.getPath());    setupJaasConfig("KafkaConnect", "", true);    Configuration.setConfiguration(null);    setMock("Basic", "user", "password", true);    jaasBasicAuthFilter.filter(requestContext);}
f9339
0
taskClass
public Class<? extends Task> kafkatest_f9347_0()
{    return FileStreamSinkTask.class;}
f9347
0
taskConfigs
public List<Map<String, String>> kafkatest_f9348_0(int maxTasks)
{    ArrayList<Map<String, String>> configs = new ArrayList<>();    for (int i = 0; i < maxTasks; i++) {        Map<String, String> config = new HashMap<>();        if (filename != null)            config.put(FILE_CONFIG, filename);        configs.add(config);    }    return configs;}
f9348
0
stop
public void kafkatest_f9349_0()
{// Nothing to do since FileStreamSinkConnector has no background monitoring.}
f9349
0
version
public String kafkatest_f9357_0()
{    return AppInfoParser.getVersion();}
f9357
0
start
public void kafkatest_f9358_0(Map<String, String> props)
{    AbstractConfig parsedConfig = new AbstractConfig(CONFIG_DEF, props);    filename = parsedConfig.getString(FILE_CONFIG);    List<String> topics = parsedConfig.getList(TOPIC_CONFIG);    if (topics.size() != 1) {        throw new ConfigException("'topic' in FileStreamSourceConnector configuration requires definition of a single topic");    }    topic = topics.get(0);    batchSize = parsedConfig.getInt(TASK_BATCH_SIZE_CONFIG);}
f9358
0
taskClass
public Class<? extends Task> kafkatest_f9359_0()
{    return FileStreamSourceTask.class;}
f9359
0
stop
public voidf9367_1)
{    log.trace("Stopping");    synchronized (this) {        try {            if (stream != null && stream != System.in) {                stream.close();                log.trace("Closed input stream");            }        } catch (IOException e) {                    }        this.notify();    }}
public voidf9367
1
offsetKey
private Map<String, String> kafkatest_f9368_0(String filename)
{    return Collections.singletonMap(FILENAME_FIELD, filename);}
f9368
0
offsetValue
private Map<String, Long> kafkatest_f9369_0(Long pos)
{    return Collections.singletonMap(POSITION_FIELD, pos);}
f9369
0
testPutFlush
public void kafkatest_f9377_0()
{    HashMap<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    final String newLine = System.getProperty("line.separator");    // We do not call task.start() since it would override the output stream    task.put(Arrays.asList(new SinkRecord("topic1", 0, null, null, Schema.STRING_SCHEMA, "line1", 1)));    offsets.put(new TopicPartition("topic1", 0), new OffsetAndMetadata(1L));    task.flush(offsets);    assertEquals("line1" + newLine, os.toString());    task.put(Arrays.asList(new SinkRecord("topic1", 0, null, null, Schema.STRING_SCHEMA, "line2", 2), new SinkRecord("topic2", 0, null, null, Schema.STRING_SCHEMA, "line3", 1)));    offsets.put(new TopicPartition("topic1", 0), new OffsetAndMetadata(2L));    offsets.put(new TopicPartition("topic2", 0), new OffsetAndMetadata(1L));    task.flush(offsets);    assertEquals("line1" + newLine + "line2" + newLine + "line3" + newLine, os.toString());}
f9377
0
testStart
public void kafkatest_f9378_0() throws IOException
{    task = new FileStreamSinkTask();    Map<String, String> props = new HashMap<>();    props.put(FileStreamSinkConnector.FILE_CONFIG, outputFile);    task.start(props);    HashMap<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    task.put(Arrays.asList(new SinkRecord("topic1", 0, null, null, Schema.STRING_SCHEMA, "line0", 1)));    offsets.put(new TopicPartition("topic1", 0), new OffsetAndMetadata(1L));    task.flush(offsets);    int numLines = 3;    String[] lines = new String[numLines];    int i = 0;    try (BufferedReader reader = Files.newBufferedReader(Paths.get(outputFile))) {        lines[i++] = reader.readLine();        task.put(Arrays.asList(new SinkRecord("topic1", 0, null, null, Schema.STRING_SCHEMA, "line1", 2), new SinkRecord("topic2", 0, null, null, Schema.STRING_SCHEMA, "line2", 1)));        offsets.put(new TopicPartition("topic1", 0), new OffsetAndMetadata(2L));        offsets.put(new TopicPartition("topic2", 0), new OffsetAndMetadata(1L));        task.flush(offsets);        lines[i++] = reader.readLine();        lines[i++] = reader.readLine();    }    while (--i >= 0) {        assertEquals("line" + i, lines[i]);    }}
f9378
0
setup
public void kafkatest_f9379_0()
{    connector = new FileStreamSourceConnector();    ctx = createMock(ConnectorContext.class);    connector.initialize(ctx);    sourceProperties = new HashMap<>();    sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, SINGLE_TOPIC);    sourceProperties.put(FileStreamSourceConnector.FILE_CONFIG, FILENAME);}
f9379
0
testInvalidBatchSize
public void kafkatest_f9387_0()
{    sourceProperties.put(FileStreamSourceConnector.TASK_BATCH_SIZE_CONFIG, "abcd");    connector.start(sourceProperties);}
f9387
0
setup
public void kafkatest_f9388_0() throws IOException
{    tempFile = File.createTempFile("file-stream-source-task-test", null);    config = new HashMap<>();    config.put(FileStreamSourceConnector.FILE_CONFIG, tempFile.getAbsolutePath());    config.put(FileStreamSourceConnector.TOPIC_CONFIG, TOPIC);    config.put(FileStreamSourceConnector.TASK_BATCH_SIZE_CONFIG, String.valueOf(FileStreamSourceConnector.DEFAULT_TASK_BATCH_SIZE));    task = new FileStreamSourceTask();    offsetStorageReader = createMock(OffsetStorageReader.class);    context = createMock(SourceTaskContext.class);    task.initialize(context);}
f9388
0
teardown
public void kafkatest_f9389_0()
{    tempFile.delete();    if (verifyMocks)        verifyAll();}
f9389
0
convert
public Object kafkatest_f9397_0(Schema schema, JsonNode value)
{    return (byte) value.intValue();}
f9397
0
convert
public Object kafkatest_f9398_0(Schema schema, JsonNode value)
{    return (short) value.intValue();}
f9398
0
convert
public Object kafkatest_f9399_0(Schema schema, JsonNode value)
{    return value.intValue();}
f9399
0
convert
public Object kafkatest_f9407_0(Schema schema, JsonNode value)
{    if (!value.isObject())        throw new DataException("Structs should be encoded as JSON objects, but found " + value.getNodeType());    // We only have ISchema here but need Schema, so we need to materialize the actual schema. Using ISchema    // avoids having to materialize the schema for non-Struct types but it cannot be avoided for Structs since    // they require a schema to be provided at construction. However, the schema is only a SchemaBuilder during    // translation of schemas to JSON; during the more common translation of data to JSON, the call to schema.schema()    // just returns the schema Object and has no overhead.    Struct result = new Struct(schema.schema());    for (Field field : schema.fields()) result.put(field, convertToConnect(field.schema(), value.get(field.name())));    return result;}
f9407
0
convert
public Object kafkatest_f9408_0(Schema schema, Object value)
{    if (!(value instanceof byte[]))        throw new DataException("Invalid type for Decimal, underlying representation should be bytes but was " + value.getClass());    return Decimal.toLogical(schema, (byte[]) value);}
f9408
0
convert
public Object kafkatest_f9409_0(Schema schema, Object value)
{    if (!(value instanceof Integer))        throw new DataException("Invalid type for Date, underlying representation should be int32 but was " + value.getClass());    return Date.toLogical(schema, (int) value);}
f9409
0
configure
public void kafkatest_f9417_0(Map<String, ?> configs)
{    JsonConverterConfig config = new JsonConverterConfig(configs);    enableSchemas = config.schemasEnabled();    cacheSize = config.schemaCacheSize();    boolean isKey = config.type() == ConverterType.KEY;    serializer.configure(configs, isKey);    deserializer.configure(configs, isKey);    fromConnectSchemaCache = new SynchronizedCache<>(new LRUCache<Schema, ObjectNode>(cacheSize));    toConnectSchemaCache = new SynchronizedCache<>(new LRUCache<JsonNode, Schema>(cacheSize));}
f9417
0
configure
public void kafkatest_f9418_0(Map<String, ?> configs, boolean isKey)
{    Map<String, Object> conf = new HashMap<>(configs);    conf.put(StringConverterConfig.TYPE_CONFIG, isKey ? ConverterType.KEY.getName() : ConverterType.VALUE.getName());    configure(conf);}
f9418
0
close
public void kafkatest_f9419_0()
{// do nothing}
f9419
0
convertToJsonWithoutEnvelope
private JsonNode kafkatest_f9427_0(Schema schema, Object value)
{    return convertToJson(schema, value);}
f9427
0
convertToJson
private static JsonNode kafkatest_f9428_0(Schema schema, Object logicalValue)
{    if (logicalValue == null) {        if (// Any schema is valid and we don't have a default, so treat this as an optional schema        schema == null)            return null;        if (schema.defaultValue() != null)            return convertToJson(schema, schema.defaultValue());        if (schema.isOptional())            return JsonNodeFactory.instance.nullNode();        throw new DataException("Conversion error: null value for field that is required and has no default value");    }    Object value = logicalValue;    if (schema != null && schema.name() != null) {        LogicalTypeConverter logicalConverter = TO_JSON_LOGICAL_CONVERTERS.get(schema.name());        if (logicalConverter != null)            value = logicalConverter.convert(schema, logicalValue);    }    try {        final Schema.Type schemaType;        if (schema == null) {            schemaType = ConnectSchema.schemaType(value.getClass());            if (schemaType == null)                throw new DataException("Java class " + value.getClass() + " does not have corresponding schema type.");        } else {            schemaType = schema.type();        }        switch(schemaType) {            case INT8:                return JsonNodeFactory.instance.numberNode((Byte) value);            case INT16:                return JsonNodeFactory.instance.numberNode((Short) value);            case INT32:                return JsonNodeFactory.instance.numberNode((Integer) value);            case INT64:                return JsonNodeFactory.instance.numberNode((Long) value);            case FLOAT32:                return JsonNodeFactory.instance.numberNode((Float) value);            case FLOAT64:                return JsonNodeFactory.instance.numberNode((Double) value);            case BOOLEAN:                return JsonNodeFactory.instance.booleanNode((Boolean) value);            case STRING:                CharSequence charSeq = (CharSequence) value;                return JsonNodeFactory.instance.textNode(charSeq.toString());            case BYTES:                if (value instanceof byte[])                    return JsonNodeFactory.instance.binaryNode((byte[]) value);                else if (value instanceof ByteBuffer)                    return JsonNodeFactory.instance.binaryNode(((ByteBuffer) value).array());                else                    throw new DataException("Invalid type for bytes type: " + value.getClass());            case ARRAY:                {                    Collection collection = (Collection) value;                    ArrayNode list = JsonNodeFactory.instance.arrayNode();                    for (Object elem : collection) {                        Schema valueSchema = schema == null ? null : schema.valueSchema();                        JsonNode fieldValue = convertToJson(valueSchema, elem);                        list.add(fieldValue);                    }                    return list;                }            case MAP:                {                    Map<?, ?> map = (Map<?, ?>) value;                    // If true, using string keys and JSON object; if false, using non-string keys and Array-encoding                    boolean objectMode;                    if (schema == null) {                        objectMode = true;                        for (Map.Entry<?, ?> entry : map.entrySet()) {                            if (!(entry.getKey() instanceof String)) {                                objectMode = false;                                break;                            }                        }                    } else {                        objectMode = schema.keySchema().type() == Schema.Type.STRING;                    }                    ObjectNode obj = null;                    ArrayNode list = null;                    if (objectMode)                        obj = JsonNodeFactory.instance.objectNode();                    else                        list = JsonNodeFactory.instance.arrayNode();                    for (Map.Entry<?, ?> entry : map.entrySet()) {                        Schema keySchema = schema == null ? null : schema.keySchema();                        Schema valueSchema = schema == null ? null : schema.valueSchema();                        JsonNode mapKey = convertToJson(keySchema, entry.getKey());                        JsonNode mapValue = convertToJson(valueSchema, entry.getValue());                        if (objectMode)                            obj.set(mapKey.asText(), mapValue);                        else                            list.add(JsonNodeFactory.instance.arrayNode().add(mapKey).add(mapValue));                    }                    return objectMode ? obj : list;                }            case STRUCT:                {                    Struct struct = (Struct) value;                    if (!struct.schema().equals(schema))                        throw new DataException("Mismatching schema.");                    ObjectNode obj = JsonNodeFactory.instance.objectNode();                    for (Field field : schema.fields()) {                        obj.set(field.name(), convertToJson(field.schema(), struct.get(field)));                    }                    return obj;                }        }        throw new DataException("Couldn't convert " + value + " to JSON.");    } catch (ClassCastException e) {        String schemaTypeStr = (schema != null) ? schema.type().toString() : "unknown schema";        throw new DataException("Invalid type for " + schemaTypeStr + ": " + value.getClass());    }}
f9428
0
convertToConnect
private static Object kafkatest_f9429_0(Schema schema, JsonNode jsonValue)
{    final Schema.Type schemaType;    if (schema != null) {        schemaType = schema.type();        if (jsonValue.isNull()) {            if (schema.defaultValue() != null)                // any logical type conversions should already have been applied                return schema.defaultValue();            if (schema.isOptional())                return null;            throw new DataException("Invalid null value for required " + schemaType + " field");        }    } else {        switch(jsonValue.getNodeType()) {            case NULL:                // Special case. With no schema                return null;            case BOOLEAN:                schemaType = Schema.Type.BOOLEAN;                break;            case NUMBER:                if (jsonValue.isIntegralNumber())                    schemaType = Schema.Type.INT64;                else                    schemaType = Schema.Type.FLOAT64;                break;            case ARRAY:                schemaType = Schema.Type.ARRAY;                break;            case OBJECT:                schemaType = Schema.Type.MAP;                break;            case STRING:                schemaType = Schema.Type.STRING;                break;            case BINARY:            case MISSING:            case POJO:            default:                schemaType = null;                break;        }    }    final JsonToConnectTypeConverter typeConverter = TO_CONNECT_CONVERTERS.get(schemaType);    if (typeConverter == null)        throw new DataException("Unknown schema type: " + String.valueOf(schemaType));    Object converted = typeConverter.convert(schema, jsonValue);    if (schema != null && schema.name() != null) {        LogicalTypeConverter logicalConverter = TO_CONNECT_LOGICAL_CONVERTERS.get(schema.name());        if (logicalConverter != null)            converted = logicalConverter.convert(schema, converted);    }    return converted;}
f9429
0
setUp
public void kafkatest_f9437_0()
{    converter.configure(Collections.emptyMap(), false);}
f9437
0
testConnectSchemaMetadataTranslation
public void kafkatest_f9438_0()
{    // this validates the non-type fields are translated and handled properly    assertEquals(new SchemaAndValue(Schema.BOOLEAN_SCHEMA, true), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"boolean\" }, \"payload\": true }".getBytes()));    assertEquals(new SchemaAndValue(Schema.OPTIONAL_BOOLEAN_SCHEMA, null), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"boolean\", \"optional\": true }, \"payload\": null }".getBytes()));    assertEquals(new SchemaAndValue(SchemaBuilder.bool().defaultValue(true).build(), true), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"boolean\", \"default\": true }, \"payload\": null }".getBytes()));    assertEquals(new SchemaAndValue(SchemaBuilder.bool().required().name("bool").version(2).doc("the documentation").parameter("foo", "bar").build(), true), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"boolean\", \"optional\": false, \"name\": \"bool\", \"version\": 2, \"doc\": \"the documentation\", \"parameters\": { \"foo\": \"bar\" }}, \"payload\": true }".getBytes()));}
f9438
0
booleanToConnect
public void kafkatest_f9439_0()
{    assertEquals(new SchemaAndValue(Schema.BOOLEAN_SCHEMA, true), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"boolean\" }, \"payload\": true }".getBytes()));    assertEquals(new SchemaAndValue(Schema.BOOLEAN_SCHEMA, false), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"boolean\" }, \"payload\": false }".getBytes()));}
f9439
0
stringToConnect
public void kafkatest_f9447_0()
{    assertEquals(new SchemaAndValue(Schema.STRING_SCHEMA, "foo-bar-baz"), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"string\" }, \"payload\": \"foo-bar-baz\" }".getBytes()));}
f9447
0
arrayToConnect
public void kafkatest_f9448_0()
{    byte[] arrayJson = "{ \"schema\": { \"type\": \"array\", \"items\": { \"type\" : \"int32\" } }, \"payload\": [1, 2, 3] }".getBytes();    assertEquals(new SchemaAndValue(SchemaBuilder.array(Schema.INT32_SCHEMA).build(), Arrays.asList(1, 2, 3)), converter.toConnectData(TOPIC, arrayJson));}
f9448
0
mapToConnectStringKeys
public void kafkatest_f9449_0()
{    byte[] mapJson = "{ \"schema\": { \"type\": \"map\", \"keys\": { \"type\" : \"string\" }, \"values\": { \"type\" : \"int32\" } }, \"payload\": { \"key1\": 12, \"key2\": 15} }".getBytes();    Map<String, Integer> expected = new HashMap<>();    expected.put("key1", 12);    expected.put("key2", 15);    assertEquals(new SchemaAndValue(SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.INT32_SCHEMA).build(), expected), converter.toConnectData(TOPIC, mapJson));}
f9449
0
decimalToConnectOptionalWithDefaultValue
public void kafkatest_f9457_0()
{    BigDecimal reference = new BigDecimal(new BigInteger("156"), 2);    Schema schema = Decimal.builder(2).optional().defaultValue(reference).build();    String msg = "{ \"schema\": { \"type\": \"bytes\", \"name\": \"org.apache.kafka.connect.data.Decimal\", \"version\": 1, \"optional\": true, \"default\": \"AJw=\", \"parameters\": { \"scale\": \"2\" } }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertEquals(reference, schemaAndValue.value());}
f9457
0
dateToConnect
public void kafkatest_f9458_0()
{    Schema schema = Date.SCHEMA;    GregorianCalendar calendar = new GregorianCalendar(1970, Calendar.JANUARY, 1, 0, 0, 0);    calendar.setTimeZone(TimeZone.getTimeZone("UTC"));    calendar.add(Calendar.DATE, 10000);    java.util.Date reference = calendar.getTime();    String msg = "{ \"schema\": { \"type\": \"int32\", \"name\": \"org.apache.kafka.connect.data.Date\", \"version\": 1 }, \"payload\": 10000 }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    java.util.Date converted = (java.util.Date) schemaAndValue.value();    assertEquals(schema, schemaAndValue.schema());    assertEquals(reference, converted);}
f9458
0
dateToConnectOptional
public void kafkatest_f9459_0()
{    Schema schema = Date.builder().optional().schema();    String msg = "{ \"schema\": { \"type\": \"int32\", \"name\": \"org.apache.kafka.connect.data.Date\", \"version\": 1, \"optional\": true }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertNull(schemaAndValue.value());}
f9459
0
timestampToConnectOptional
public void kafkatest_f9467_0()
{    Schema schema = Timestamp.builder().optional().schema();    String msg = "{ \"schema\": { \"type\": \"int64\", \"name\": \"org.apache.kafka.connect.data.Timestamp\", \"version\": 1, \"optional\": true }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertNull(schemaAndValue.value());}
f9467
0
timestampToConnectWithDefaultValue
public void kafkatest_f9468_0()
{    Schema schema = Timestamp.builder().defaultValue(new java.util.Date(42)).schema();    String msg = "{ \"schema\": { \"type\": \"int64\", \"name\": \"org.apache.kafka.connect.data.Timestamp\", \"version\": 1, \"default\": 42 }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertEquals(new java.util.Date(42), schemaAndValue.value());}
f9468
0
timestampToConnectOptionalWithDefaultValue
public void kafkatest_f9469_0()
{    Schema schema = Timestamp.builder().optional().defaultValue(new java.util.Date(42)).schema();    String msg = "{ \"schema\": { \"type\": \"int64\", \"name\": \"org.apache.kafka.connect.data.Timestamp\", \"version\": 1,  \"optional\": true, \"default\": 42 }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertEquals(new java.util.Date(42), schemaAndValue.value());}
f9469
0
floatToJson
public void kafkatest_f9477_0()
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Schema.FLOAT32_SCHEMA, 12.34f));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"float\", \"optional\": false }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertEquals(12.34f, converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).floatValue(), 0.001);}
f9477
0
doubleToJson
public void kafkatest_f9478_0()
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Schema.FLOAT64_SCHEMA, 12.34));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"double\", \"optional\": false }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertEquals(12.34, converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).doubleValue(), 0.001);}
f9478
0
bytesToJson
public void kafkatest_f9479_0() throws IOException
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Schema.BYTES_SCHEMA, "test-string".getBytes()));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"bytes\", \"optional\": false }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertEquals(ByteBuffer.wrap("test-string".getBytes()), ByteBuffer.wrap(converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).binaryValue()));}
f9479
0
dateToJson
public void kafkatest_f9487_0()
{    GregorianCalendar calendar = new GregorianCalendar(1970, Calendar.JANUARY, 1, 0, 0, 0);    calendar.setTimeZone(TimeZone.getTimeZone("UTC"));    calendar.add(Calendar.DATE, 10000);    java.util.Date date = calendar.getTime();    JsonNode converted = parse(converter.fromConnectData(TOPIC, Date.SCHEMA, date));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"int32\", \"optional\": false, \"name\": \"org.apache.kafka.connect.data.Date\", \"version\": 1 }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    JsonNode payload = converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME);    assertTrue(payload.isInt());    assertEquals(10000, payload.intValue());}
f9487
0
timeToJson
public void kafkatest_f9488_0()
{    GregorianCalendar calendar = new GregorianCalendar(1970, Calendar.JANUARY, 1, 0, 0, 0);    calendar.setTimeZone(TimeZone.getTimeZone("UTC"));    calendar.add(Calendar.MILLISECOND, 14400000);    java.util.Date date = calendar.getTime();    JsonNode converted = parse(converter.fromConnectData(TOPIC, Time.SCHEMA, date));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"int32\", \"optional\": false, \"name\": \"org.apache.kafka.connect.data.Time\", \"version\": 1 }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    JsonNode payload = converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME);    assertTrue(payload.isInt());    assertEquals(14400000, payload.longValue());}
f9488
0
timestampToJson
public void kafkatest_f9489_0()
{    GregorianCalendar calendar = new GregorianCalendar(1970, Calendar.JANUARY, 1, 0, 0, 0);    calendar.setTimeZone(TimeZone.getTimeZone("UTC"));    calendar.add(Calendar.MILLISECOND, 2000000000);    calendar.add(Calendar.MILLISECOND, 2000000000);    java.util.Date date = calendar.getTime();    JsonNode converted = parse(converter.fromConnectData(TOPIC, Timestamp.SCHEMA, date));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"int64\", \"optional\": false, \"name\": \"org.apache.kafka.connect.data.Timestamp\", \"version\": 1 }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    JsonNode payload = converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME);    assertTrue(payload.isLong());    assertEquals(4000000000L, payload.longValue());}
f9489
0
noSchemaToConnect
public void kafkatest_f9497_0()
{    Map<String, Boolean> props = Collections.singletonMap("schemas.enable", false);    converter.configure(props, true);    assertEquals(new SchemaAndValue(null, true), converter.toConnectData(TOPIC, "true".getBytes()));}
f9497
0
noSchemaToJson
public void kafkatest_f9498_0()
{    Map<String, Boolean> props = Collections.singletonMap("schemas.enable", false);    converter.configure(props, true);    JsonNode converted = parse(converter.fromConnectData(TOPIC, null, true));    assertTrue(converted.isBoolean());    assertEquals(true, converted.booleanValue());}
f9498
0
testCacheSchemaToJsonConversion
public void kafkatest_f9499_0()
{    Cache<Schema, ObjectNode> cache = Whitebox.getInternalState(converter, "fromConnectSchemaCache");    assertEquals(0, cache.size());    // Repeated conversion of the same schema, even if the schema object is different should return the same Java    // object    converter.fromConnectData(TOPIC, SchemaBuilder.bool().build(), true);    assertEquals(1, cache.size());    converter.fromConnectData(TOPIC, SchemaBuilder.bool().build(), true);    assertEquals(1, cache.size());    // Validate that a similar, but different schema correctly returns a different schema.    converter.fromConnectData(TOPIC, SchemaBuilder.bool().optional().build(), true);    assertEquals(2, cache.size());}
f9499
0
assertStructSchemaEqual
private void kafkatest_f9507_0(Schema schema, Struct struct)
{    converter.fromConnectData(TOPIC, schema, struct);    assertEquals(schema, struct.schema());}
f9507
0
main
public static voidf9508_1String[] args)
{    if (args.length < 1 || Arrays.asList(args).contains("--help")) {                Exit.exit(1);    }    try {        WorkerInfo initInfo = new WorkerInfo();        initInfo.logAll();        String workerPropsFile = args[0];        Map<String, String> workerProps = !workerPropsFile.isEmpty() ? Utils.propsToStringMap(Utils.loadProps(workerPropsFile)) : Collections.emptyMap();        ConnectDistributed connectDistributed = new ConnectDistributed();        Connect connect = connectDistributed.startConnect(workerProps);        // Shutdown will be triggered by Ctrl-C or via HTTP shutdown request        connect.awaitStop();    } catch (Throwable t) {                Exit.exit(2);    }}
public static voidf9508
1
startConnect
public Connectf9509_1Map<String, String> workerProps)
{        Plugins plugins = new Plugins(workerProps);    plugins.compareAndSwapWithDelegatingLoader();    DistributedConfig config = new DistributedConfig(workerProps);    String kafkaClusterId = ConnectUtils.lookupKafkaClusterId(config);        RestServer rest = new RestServer(config);    rest.initializeServer();    URI advertisedUrl = rest.advertisedUrl();    String workerId = advertisedUrl.getHost() + ":" + advertisedUrl.getPort();    KafkaOffsetBackingStore offsetBackingStore = new KafkaOffsetBackingStore();    offsetBackingStore.configure(config);    ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy = plugins.newPlugin(config.getString(WorkerConfig.CONNECTOR_CLIENT_POLICY_CLASS_CONFIG), config, ConnectorClientConfigOverridePolicy.class);    Worker worker = new Worker(workerId, time, plugins, config, offsetBackingStore, connectorClientConfigOverridePolicy);    WorkerConfigTransformer configTransformer = worker.configTransformer();    Converter internalValueConverter = worker.getInternalValueConverter();    StatusBackingStore statusBackingStore = new KafkaStatusBackingStore(time, internalValueConverter);    statusBackingStore.configure(config);    ConfigBackingStore configBackingStore = new KafkaConfigBackingStore(internalValueConverter, config, configTransformer);    DistributedHerder herder = new DistributedHerder(config, time, worker, kafkaClusterId, statusBackingStore, configBackingStore, advertisedUrl.toString(), connectorClientConfigOverridePolicy);    final Connect connect = new Connect(herder, rest);        try {        connect.start();    } catch (Exception e) {                connect.stop();        Exit.exit(3);    }    return connect;}
public Connectf9509
1
configure
public voidf9518_1Map<String, ?> configs)
{    }
public voidf9518
1
policyName
protected String kafkatest_f9519_0()
{    return "None";}
f9519
0
isAllowed
protected boolean kafkatest_f9520_0(ConfigValue configValue)
{    return false;}
f9520
0
fromConnectHeader
public byte[] kafkatest_f9530_0(String topic, String headerKey, Schema schema, Object value)
{    return fromConnectData(topic, schema, value);}
f9530
0
toConnectHeader
public SchemaAndValue kafkatest_f9531_0(String topic, String headerKey, byte[] value)
{    return toConnectData(topic, value);}
f9531
0
close
public void kafkatest_f9532_0()
{// do nothing}
f9532
0
toConnectHeader
public SchemaAndValue kafkatest_f9540_0(String topic, String headerKey, byte[] value)
{    return toConnectData(topic, value);}
f9540
0
configDef
public static ConfigDef kafkatest_f9542_0()
{    return CONFIG;}
f9542
0
kafkaClusterId
public String kafkatest_f9543_0()
{    return kafkaClusterId;}
f9543
0
onStartup
public void kafkatest_f9551_0(ConnectorTaskId id)
{    statusBackingStore.put(new TaskStatus(id, TaskStatus.State.RUNNING, workerId, generation()));}
f9551
0
onFailure
public void kafkatest_f9552_0(ConnectorTaskId id, Throwable cause)
{    statusBackingStore.putSafe(new TaskStatus(id, TaskStatus.State.FAILED, workerId, generation(), trace(cause)));}
f9552
0
onShutdown
public void kafkatest_f9553_0(ConnectorTaskId id)
{    statusBackingStore.putSafe(new TaskStatus(id, TaskStatus.State.UNASSIGNED, workerId, generation()));}
f9553
0
connectorInfo
public ConnectorInfo kafkatest_f9561_0(String connector)
{    final ClusterConfigState configState = configBackingStore.snapshot();    if (!configState.contains(connector))        return null;    Map<String, String> config = configState.rawConnectorConfig(connector);    return new ConnectorInfo(connector, config, configState.tasks(connector), connectorTypeForClass(config.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG)));}
f9561
0
connectorStatus
public ConnectorStateInfo kafkatest_f9562_0(String connName)
{    ConnectorStatus connector = statusBackingStore.get(connName);    if (connector == null)        throw new NotFoundException("No status found for connector " + connName);    Collection<TaskStatus> tasks = statusBackingStore.getAll(connName);    ConnectorStateInfo.ConnectorState connectorState = new ConnectorStateInfo.ConnectorState(connector.state().toString(), connector.workerId(), connector.trace());    List<ConnectorStateInfo.TaskState> taskStates = new ArrayList<>();    for (TaskStatus status : tasks) {        taskStates.add(new ConnectorStateInfo.TaskState(status.id().task(), status.state().toString(), status.workerId(), status.trace()));    }    Collections.sort(taskStates);    Map<String, String> conf = config(connName);    return new ConnectorStateInfo(connName, connectorState, taskStates, conf == null ? ConnectorType.UNKNOWN : connectorTypeForClass(conf.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG)));}
f9562
0
taskStatus
public ConnectorStateInfo.TaskState kafkatest_f9563_0(ConnectorTaskId id)
{    TaskStatus status = statusBackingStore.get(id);    if (status == null)        throw new NotFoundException("No status found for task " + id);    return new ConnectorStateInfo.TaskState(id.task(), status.state().toString(), status.workerId(), status.trace());}
f9563
0
convertConfigValue
private static ConfigValueInfo kafkatest_f9571_0(ConfigValue configValue, Type type)
{    String value = ConfigDef.convertToString(configValue.value(), type);    List<String> recommendedValues = new LinkedList<>();    if (type == Type.LIST) {        for (Object object : configValue.recommendedValues()) {            recommendedValues.add(ConfigDef.convertToString(object, Type.STRING));        }    } else {        for (Object object : configValue.recommendedValues()) {            recommendedValues.add(ConfigDef.convertToString(object, type));        }    }    return new ConfigValueInfo(configValue.name(), value, recommendedValues, configValue.errorMessages(), configValue.visible());}
f9571
0
getConnector
protected Connector kafkatest_f9572_0(String connType)
{    if (tempConnectors.containsKey(connType)) {        return tempConnectors.get(connType);    } else {        Connector connector = plugins().newConnector(connType);        tempConnectors.put(connType, connector);        return connector;    }}
f9572
0
connectorTypeForClass
public ConnectorType kafkatest_f9573_0(String connClass)
{    return ConnectorType.from(getConnector(connClass).getClass());}
f9573
0
workerId
public String kafkatest_f9581_0()
{    return workerId;}
f9581
0
generation
public int kafkatest_f9582_0()
{    return generation;}
f9582
0
toString
public String kafkatest_f9583_0()
{    return "Status{" + "id=" + id + ", state=" + state + ", workerId='" + workerId + '\'' + ", generation=" + generation + '}';}
f9583
0
workerId
public String kafkatest_f9591_0()
{    return workerId;}
f9591
0
metrics
public Metrics kafkatest_f9592_0()
{    return metrics;}
f9592
0
registry
public ConnectMetricsRegistry kafkatest_f9593_0()
{    return registry;}
f9593
0
hashCode
public int kafkatest_f9601_0()
{    return hc;}
f9601
0
equals
public boolean kafkatest_f9602_0(Object obj)
{    if (obj == this)        return true;    if (obj instanceof MetricGroupId) {        MetricGroupId that = (MetricGroupId) obj;        return this.groupName.equals(that.groupName) && this.tags.equals(that.tags);    }    return false;}
f9602
0
toString
public String kafkatest_f9603_0()
{    return str;}
f9603
0
addImmutableValueMetric
public void kafkatest_f9611_0(MetricNameTemplate nameTemplate, final T value)
{    MetricName metricName = metricName(nameTemplate);    if (metrics().metric(metricName) == null) {        metrics().addMetric(metricName, new Gauge<T>() {            @Override            public T value(MetricConfig config, long now) {                return value;            }        });    }}
f9611
0
value
public T kafkatest_f9612_0(MetricConfig config, long now)
{    return value;}
f9612
0
sensor
public Sensor kafkatest_f9613_0(String name)
{    return sensor(name, null, Sensor.RecordingLevel.INFO);}
f9613
0
createTemplate
private MetricNameTemplate kafkatest_f9621_0(String name, String group, String doc, Set<String> tags)
{    MetricNameTemplate template = new MetricNameTemplate(name, group, doc, tags);    allTemplates.add(template);    return template;}
f9621
0
getAllTemplates
public List<MetricNameTemplate> kafkatest_f9622_0()
{    return Collections.unmodifiableList(allTemplates);}
f9622
0
connectorTagName
public String kafkatest_f9623_0()
{    return CONNECTOR_TAG_NAME;}
f9623
0
taskErrorHandlingGroupName
public String kafkatest_f9631_0()
{    return TASK_ERROR_HANDLING_GROUP_NAME;}
f9631
0
get
public Object kafkatest_f9632_0(String key)
{    return super.get(key);}
f9632
0
configDef
public static ConfigDef kafkatest_f9633_0()
{    int orderInGroup = 0;    int orderInErrorGroup = 0;    return new ConfigDef().define(NAME_CONFIG, Type.STRING, ConfigDef.NO_DEFAULT_VALUE, nonEmptyStringWithoutControlChars(), Importance.HIGH, NAME_DOC, COMMON_GROUP, ++orderInGroup, Width.MEDIUM, NAME_DISPLAY).define(CONNECTOR_CLASS_CONFIG, Type.STRING, Importance.HIGH, CONNECTOR_CLASS_DOC, COMMON_GROUP, ++orderInGroup, Width.LONG, CONNECTOR_CLASS_DISPLAY).define(TASKS_MAX_CONFIG, Type.INT, TASKS_MAX_DEFAULT, atLeast(TASKS_MIN_CONFIG), Importance.HIGH, TASKS_MAX_DOC, COMMON_GROUP, ++orderInGroup, Width.SHORT, TASK_MAX_DISPLAY).define(KEY_CONVERTER_CLASS_CONFIG, Type.CLASS, null, Importance.LOW, KEY_CONVERTER_CLASS_DOC, COMMON_GROUP, ++orderInGroup, Width.SHORT, KEY_CONVERTER_CLASS_DISPLAY).define(VALUE_CONVERTER_CLASS_CONFIG, Type.CLASS, null, Importance.LOW, VALUE_CONVERTER_CLASS_DOC, COMMON_GROUP, ++orderInGroup, Width.SHORT, VALUE_CONVERTER_CLASS_DISPLAY).define(HEADER_CONVERTER_CLASS_CONFIG, Type.CLASS, HEADER_CONVERTER_CLASS_DEFAULT, Importance.LOW, HEADER_CONVERTER_CLASS_DOC, COMMON_GROUP, ++orderInGroup, Width.SHORT, HEADER_CONVERTER_CLASS_DISPLAY).define(TRANSFORMS_CONFIG, Type.LIST, Collections.emptyList(), ConfigDef.CompositeValidator.of(new ConfigDef.NonNullValidator(), new ConfigDef.Validator() {        @SuppressWarnings("unchecked")        @Override        public void ensureValid(String name, Object value) {            final List<String> transformAliases = (List<String>) value;            if (transformAliases.size() > new HashSet<>(transformAliases).size()) {                throw new ConfigException(name, value, "Duplicate alias provided.");            }        }        @Override        public String toString() {            return "unique transformation aliases";        }    }), Importance.LOW, TRANSFORMS_DOC, TRANSFORMS_GROUP, ++orderInGroup, Width.LONG, TRANSFORMS_DISPLAY).define(CONFIG_RELOAD_ACTION_CONFIG, Type.STRING, CONFIG_RELOAD_ACTION_RESTART, in(CONFIG_RELOAD_ACTION_NONE, CONFIG_RELOAD_ACTION_RESTART), Importance.LOW, CONFIG_RELOAD_ACTION_DOC, COMMON_GROUP, ++orderInGroup, Width.MEDIUM, CONFIG_RELOAD_ACTION_DISPLAY).define(ERRORS_RETRY_TIMEOUT_CONFIG, Type.LONG, ERRORS_RETRY_TIMEOUT_DEFAULT, Importance.MEDIUM, ERRORS_RETRY_TIMEOUT_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.MEDIUM, ERRORS_RETRY_TIMEOUT_DISPLAY).define(ERRORS_RETRY_MAX_DELAY_CONFIG, Type.LONG, ERRORS_RETRY_MAX_DELAY_DEFAULT, Importance.MEDIUM, ERRORS_RETRY_MAX_DELAY_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.MEDIUM, ERRORS_RETRY_MAX_DELAY_DISPLAY).define(ERRORS_TOLERANCE_CONFIG, Type.STRING, ERRORS_TOLERANCE_DEFAULT.value(), in(ToleranceType.NONE.value(), ToleranceType.ALL.value()), Importance.MEDIUM, ERRORS_TOLERANCE_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.SHORT, ERRORS_TOLERANCE_DISPLAY).define(ERRORS_LOG_ENABLE_CONFIG, Type.BOOLEAN, ERRORS_LOG_ENABLE_DEFAULT, Importance.MEDIUM, ERRORS_LOG_ENABLE_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.SHORT, ERRORS_LOG_ENABLE_DISPLAY).define(ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, Type.BOOLEAN, ERRORS_LOG_INCLUDE_MESSAGES_DEFAULT, Importance.MEDIUM, ERRORS_LOG_INCLUDE_MESSAGES_DOC, ERROR_GROUP, ++orderInErrorGroup, Width.SHORT, ERRORS_LOG_INCLUDE_MESSAGES_DISPLAY);}
f9633
0
includeRecordDetailsInErrorLog
public boolean kafkatest_f9641_0()
{    return getBoolean(ERRORS_LOG_INCLUDE_MESSAGES_CONFIG);}
f9641
0
transformations
public List<Transformation<R>> kafkatest_f9642_0()
{    final List<String> transformAliases = getList(TRANSFORMS_CONFIG);    final List<Transformation<R>> transformations = new ArrayList<>(transformAliases.size());    for (String alias : transformAliases) {        final String prefix = TRANSFORMS_CONFIG + "." + alias + ".";        try {            @SuppressWarnings("unchecked")            final Transformation<R> transformation = getClass(prefix + "type").asSubclass(Transformation.class).getDeclaredConstructor().newInstance();            transformation.configure(originalsWithPrefix(prefix));            transformations.add(transformation);        } catch (Exception e) {            throw new ConnectException(e);        }    }    return transformations;}
f9642
0
enrich
public static ConfigDef kafkatest_f9643_0(Plugins plugins, ConfigDef baseConfigDef, Map<String, String> props, boolean requireFullConfig)
{    Object transformAliases = ConfigDef.parseType(TRANSFORMS_CONFIG, props.get(TRANSFORMS_CONFIG), Type.LIST);    if (!(transformAliases instanceof List)) {        return baseConfigDef;    }    ConfigDef newDef = new ConfigDef(baseConfigDef);    LinkedHashSet<?> uniqueTransformAliases = new LinkedHashSet<>((List<?>) transformAliases);    for (Object o : uniqueTransformAliases) {        if (!(o instanceof String)) {            throw new ConfigException("Item in " + TRANSFORMS_CONFIG + " property is not of " + "type String");        }        String alias = (String) o;        final String prefix = TRANSFORMS_CONFIG + "." + alias + ".";        final String group = TRANSFORMS_GROUP + ": " + alias;        int orderInGroup = 0;        final String transformationTypeConfig = prefix + "type";        final ConfigDef.Validator typeValidator = new ConfigDef.Validator() {            @Override            public void ensureValid(String name, Object value) {                getConfigDefFromTransformation(transformationTypeConfig, (Class) value);            }        };        newDef.define(transformationTypeConfig, Type.CLASS, ConfigDef.NO_DEFAULT_VALUE, typeValidator, Importance.HIGH, "Class for the '" + alias + "' transformation.", group, orderInGroup++, Width.LONG, "Transformation type for " + alias, Collections.<String>emptyList(), new TransformationClassRecommender(plugins));        final ConfigDef transformationConfigDef;        try {            final String className = props.get(transformationTypeConfig);            final Class<?> cls = (Class<?>) ConfigDef.parseType(transformationTypeConfig, className, Type.CLASS);            transformationConfigDef = getConfigDefFromTransformation(transformationTypeConfig, cls);        } catch (ConfigException e) {            if (requireFullConfig) {                throw e;            } else {                continue;            }        }        newDef.embed(prefix, group, orderInGroup, transformationConfigDef);    }    return newDef;}
f9643
0
connectorConfig
public Map<String, String> kafkatest_f9651_0(String connector)
{    Map<String, String> configs = connectorConfigs.get(connector);    if (configTransformer != null) {        configs = configTransformer.transform(connector, configs);    }    return configs;}
f9651
0
rawConnectorConfig
public Map<String, String> kafkatest_f9652_0(String connector)
{    return connectorConfigs.get(connector);}
f9652
0
targetState
public TargetState kafkatest_f9653_0(String connector)
{    return connectorTargetStates.get(connector);}
f9653
0
equals
public boolean kafkatest_f9661_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ClusterConfigState that = (ClusterConfigState) o;    return offset == that.offset && Objects.equals(connectorTaskCounts, that.connectorTaskCounts) && Objects.equals(connectorConfigs, that.connectorConfigs) && Objects.equals(connectorTargetStates, that.connectorTargetStates) && Objects.equals(taskConfigs, that.taskConfigs) && Objects.equals(inconsistentConnectors, that.inconsistentConnectors) && Objects.equals(configTransformer, that.configTransformer);}
f9661
0
hashCode
public int kafkatest_f9662_0()
{    return Objects.hash(offset, connectorTaskCounts, connectorConfigs, connectorTargetStates, taskConfigs, inconsistentConnectors, configTransformer);}
f9662
0
serializeMetadata
public static ByteBuffer kafkatest_f9663_0(WorkerState workerState)
{    Struct struct = new Struct(CONFIG_STATE_V0);    struct.set(URL_KEY_NAME, workerState.url());    struct.set(CONFIG_OFFSET_KEY_NAME, workerState.offset());    ByteBuffer buffer = ByteBuffer.allocate(CONNECT_PROTOCOL_HEADER_V0.sizeOf() + CONFIG_STATE_V0.sizeOf(struct));    CONNECT_PROTOCOL_HEADER_V0.writeTo(buffer);    CONFIG_STATE_V0.write(buffer, struct);    buffer.flip();    return buffer;}
f9663
0
error
public short kafkatest_f9671_0()
{    return error;}
f9671
0
leader
public String kafkatest_f9672_0()
{    return leader;}
f9672
0
leaderUrl
public String kafkatest_f9673_0()
{    return leaderUrl;}
f9673
0
compatibility
public static ConnectProtocolCompatibility kafkatest_f9681_0(String name)
{    return Arrays.stream(ConnectProtocolCompatibility.values()).filter(mode -> mode.name().equalsIgnoreCase(name)).findFirst().orElseThrow(() -> new IllegalArgumentException("Unknown Connect protocol compatibility mode: " + name));}
f9681
0
toString
public String kafkatest_f9682_0()
{    return name().toLowerCase(Locale.ROOT);}
f9682
0
fromProtocol
public static ConnectProtocolCompatibility kafkatest_f9683_0(String protocolName)
{    return Arrays.stream(ConnectProtocolCompatibility.values()).filter(mode -> mode.protocol().equalsIgnoreCase(protocolName)).findFirst().orElseThrow(() -> new IllegalArgumentException("Not found Connect protocol compatibility mode for protocol: " + protocolName));}
f9683
0
tick
public voidf9691_1)
{    try {        // Joining and immediately leaving for failure to read configs is exceedingly impolite        if (!canReadConfigs && !readConfigToEnd(workerSyncTimeoutMs))            // Safe to return and tick immediately because readConfigToEnd will do the backoff for us            return;        member.ensureActive();        // Ensure we're in a good state in our group. If not restart and everything should be setup to rejoin        if (!handleRebalanceCompleted())            return;    } catch (WakeupException e) {        // unless we're in the group.        return;    }    // Process any external requests    final long now = time.milliseconds();    long nextRequestTimeoutMs = Long.MAX_VALUE;    while (true) {        final DistributedHerderRequest next = peekWithoutException();        if (next == null) {            break;        } else if (now >= next.at) {            requests.pollFirst();        } else {            nextRequestTimeoutMs = next.at - now;            break;        }        try {            next.action().call();            next.callback().onCompletion(null, null);        } catch (Throwable t) {            next.callback().onCompletion(t, null);        }    }    if (scheduledRebalance < Long.MAX_VALUE) {        nextRequestTimeoutMs = Math.min(nextRequestTimeoutMs, Math.max(scheduledRebalance - now, 0));        rebalanceResolved = false;            }    // Process any configuration updates    AtomicReference<Set<String>> connectorConfigUpdatesCopy = new AtomicReference<>();    AtomicReference<Set<String>> connectorTargetStateChangesCopy = new AtomicReference<>();    AtomicReference<Set<ConnectorTaskId>> taskConfigUpdatesCopy = new AtomicReference<>();    boolean shouldReturn;    if (member.currentProtocolVersion() == CONNECT_PROTOCOL_V0) {        shouldReturn = updateConfigsWithEager(connectorConfigUpdatesCopy, connectorTargetStateChangesCopy);        // been set to retain the old workflow        if (shouldReturn) {            return;        }        if (connectorConfigUpdatesCopy.get() != null) {            processConnectorConfigUpdates(connectorConfigUpdatesCopy.get());        }        if (connectorTargetStateChangesCopy.get() != null) {            processTargetStateChanges(connectorTargetStateChangesCopy.get());        }    } else {        shouldReturn = updateConfigsWithIncrementalCooperative(connectorConfigUpdatesCopy, connectorTargetStateChangesCopy, taskConfigUpdatesCopy);        if (connectorConfigUpdatesCopy.get() != null) {            processConnectorConfigUpdates(connectorConfigUpdatesCopy.get());        }        if (connectorTargetStateChangesCopy.get() != null) {            processTargetStateChanges(connectorTargetStateChangesCopy.get());        }        if (taskConfigUpdatesCopy.get() != null) {            processTaskConfigUpdatesWithIncrementalCooperative(taskConfigUpdatesCopy.get());        }        if (shouldReturn) {            return;        }    }    // Let the group take any actions it needs to    try {        member.poll(nextRequestTimeoutMs);        // Ensure we're in a good state in our group. If not restart and everything should be setup to rejoin        handleRebalanceCompleted();    } catch (WakeupException e) {    // FIXME should not be WakeupException    // Ignore. Just indicates we need to check the exit flag, for requested actions, etc.    }}
public voidf9691
1
updateConfigsWithEager
private synchronized booleanf9692_1AtomicReference<Set<String>> connectorConfigUpdatesCopy, AtomicReference<Set<String>> connectorTargetStateChangesCopy)
{    // This branch is here to avoid creating a snapshot if not needed    if (needsReconfigRebalance || !connectorConfigUpdates.isEmpty() || !connectorTargetStateChanges.isEmpty()) {        // Connector reconfigs only need local updates since there is no coordination between workers required.        // However, if connectors were added or removed, work needs to be rebalanced since we have more work        // items to distribute among workers.        configState = configBackingStore.snapshot();        if (needsReconfigRebalance) {            // Task reconfigs require a rebalance. Request the rebalance, clean out state, and then restart            // this loop, which will then ensure the rebalance occurs without any other requests being            // processed until it completes.                        member.requestRejoin();            needsReconfigRebalance = false;            // Any connector config updates or target state changes will be addressed during the rebalance too            connectorConfigUpdates.clear();            connectorTargetStateChanges.clear();            return true;        } else {            if (!connectorConfigUpdates.isEmpty()) {                // We can't start/stop while locked since starting connectors can cause task updates that will                // require writing configs, which in turn make callbacks into this class from another thread that                // require acquiring a lock. This leads to deadlock. Instead, just copy the info we need and process                // the updates after unlocking.                connectorConfigUpdatesCopy.set(connectorConfigUpdates);                connectorConfigUpdates = new HashSet<>();            }            if (!connectorTargetStateChanges.isEmpty()) {                // Similarly for target state changes which can cause connectors to be restarted                connectorTargetStateChangesCopy.set(connectorTargetStateChanges);                connectorTargetStateChanges = new HashSet<>();            }        }    }    return false;}
private synchronized booleanf9692
1
updateConfigsWithIncrementalCooperative
private synchronized booleanf9693_1AtomicReference<Set<String>> connectorConfigUpdatesCopy, AtomicReference<Set<String>> connectorTargetStateChangesCopy, AtomicReference<Set<ConnectorTaskId>> taskConfigUpdatesCopy)
{    boolean retValue = false;    // This branch is here to avoid creating a snapshot if not needed    if (needsReconfigRebalance || !connectorConfigUpdates.isEmpty() || !connectorTargetStateChanges.isEmpty() || !taskConfigUpdates.isEmpty()) {        // Connector reconfigs only need local updates since there is no coordination between workers required.        // However, if connectors were added or removed, work needs to be rebalanced since we have more work        // items to distribute among workers.        configState = configBackingStore.snapshot();        if (needsReconfigRebalance) {                        member.requestRejoin();            needsReconfigRebalance = false;            retValue = true;        }        if (!connectorConfigUpdates.isEmpty()) {            // We can't start/stop while locked since starting connectors can cause task updates that will            // require writing configs, which in turn make callbacks into this class from another thread that            // require acquiring a lock. This leads to deadlock. Instead, just copy the info we need and process            // the updates after unlocking.            connectorConfigUpdatesCopy.set(connectorConfigUpdates);            connectorConfigUpdates = new HashSet<>();        }        if (!connectorTargetStateChanges.isEmpty()) {            // Similarly for target state changes which can cause connectors to be restarted            connectorTargetStateChangesCopy.set(connectorTargetStateChanges);            connectorTargetStateChanges = new HashSet<>();        }        if (!taskConfigUpdates.isEmpty()) {            // Similarly for task config updates            taskConfigUpdatesCopy.set(taskConfigUpdates);            taskConfigUpdates = new HashSet<>();        }    }    return retValue;}
private synchronized booleanf9693
1
connectorInfo
public void kafkatest_f9701_0(final String connName, final Callback<ConnectorInfo> callback)
{    log.trace("Submitting connector info request {}", connName);    addRequest(new Callable<Void>() {        @Override        public Void call() throws Exception {            if (checkRebalanceNeeded(callback))                return null;            if (!configState.contains(connName)) {                callback.onCompletion(new NotFoundException("Connector " + connName + " not found"), null);            } else {                callback.onCompletion(null, connectorInfo(connName));            }            return null;        }    }, forwardErrorCallback(callback));}
f9701
0
call
public Void kafkatest_f9702_0() throws Exception
{    if (checkRebalanceNeeded(callback))        return null;    if (!configState.contains(connName)) {        callback.onCompletion(new NotFoundException("Connector " + connName + " not found"), null);    } else {        callback.onCompletion(null, connectorInfo(connName));    }    return null;}
f9702
0
config
protected Map<String, String> kafkatest_f9703_0(String connName)
{    return configState.connectorConfig(connName);}
f9703
0
requestTaskReconfiguration
public voidf9711_1final String connName)
{    log.trace("Submitting connector task reconfiguration request {}", connName);    addRequest(new Callable<Void>() {        @Override        public Void call() throws Exception {            reconfigureConnectorTasksWithRetry(connName);            return null;        }    }, new Callback<Void>() {        @Override        public void onCompletion(Throwable error, Void result) {            if (error != null) {                                            }        }    });}
public voidf9711
1
call
public Void kafkatest_f9712_0() throws Exception
{    reconfigureConnectorTasksWithRetry(connName);    return null;}
f9712
0
onCompletion
public voidf9713_1Throwable error, Void result)
{    if (error != null) {                    }}
public voidf9713
1
restartTask
public void kafkatest_f9721_0(final ConnectorTaskId id, final Callback<Void> callback)
{    addRequest(new Callable<Void>() {        @Override        public Void call() throws Exception {            if (checkRebalanceNeeded(callback))                return null;            if (!configState.connectors().contains(id.connector())) {                callback.onCompletion(new NotFoundException("Unknown connector: " + id.connector()), null);                return null;            }            if (configState.taskConfig(id) == null) {                callback.onCompletion(new NotFoundException("Unknown task: " + id), null);                return null;            }            if (assignment.tasks().contains(id)) {                try {                    worker.stopAndAwaitTask(id);                    if (startTask(id))                        callback.onCompletion(null, null);                    else                        callback.onCompletion(new ConnectException("Failed to start task: " + id), null);                } catch (Throwable t) {                    callback.onCompletion(t, null);                }            } else if (isLeader()) {                callback.onCompletion(new NotAssignedException("Cannot restart task since it is not assigned to this member", member.ownerUrl(id)), null);            } else {                callback.onCompletion(new NotLeaderException("Cannot restart task since it is not assigned to this member", leaderUrl()), null);            }            return null;        }    }, forwardErrorCallback(callback));}
f9721
0
call
public Void kafkatest_f9722_0() throws Exception
{    if (checkRebalanceNeeded(callback))        return null;    if (!configState.connectors().contains(id.connector())) {        callback.onCompletion(new NotFoundException("Unknown connector: " + id.connector()), null);        return null;    }    if (configState.taskConfig(id) == null) {        callback.onCompletion(new NotFoundException("Unknown task: " + id), null);        return null;    }    if (assignment.tasks().contains(id)) {        try {            worker.stopAndAwaitTask(id);            if (startTask(id))                callback.onCompletion(null, null);            else                callback.onCompletion(new ConnectException("Failed to start task: " + id), null);        } catch (Throwable t) {            callback.onCompletion(t, null);        }    } else if (isLeader()) {        callback.onCompletion(new NotAssignedException("Cannot restart task since it is not assigned to this member", member.ownerUrl(id)), null);    } else {        callback.onCompletion(new NotLeaderException("Cannot restart task since it is not assigned to this member", leaderUrl()), null);    }    return null;}
f9722
0
generation
public int kafkatest_f9723_0()
{    return generation;}
f9723
0
assignmentDifference
private static Collection<T> kafkatest_f9731_0(Collection<T> update, Collection<T> running)
{    if (running.isEmpty()) {        return update;    }    HashSet<T> diff = new HashSet<>(update);    diff.removeAll(running);    return diff;}
f9731
0
startTask
private booleanf9732_1ConnectorTaskId taskId)
{        return worker.startTask(taskId, configState, configState.connectorConfig(taskId.connector()), configState.taskConfig(taskId), this, configState.targetState(taskId.connector()));}
private booleanf9732
1
getTaskStartingCallable
private Callable<Void>f9733_1final ConnectorTaskId taskId)
{    return new Callable<Void>() {        @Override        public Void call() throws Exception {            try {                startTask(taskId);            } catch (Throwable t) {                                onFailure(taskId, t);            }            return null;        }    };}
private Callable<Void>f9733
1
call
public Voidf9741_1) throws Exception
{    try {        worker.stopConnector(connectorName);    } catch (Throwable t) {            }    return null;}
public Voidf9741
1
reconfigureConnectorTasksWithRetry
private voidf9742_1final String connName)
{    reconfigureConnector(connName, new Callback<Void>() {        @Override        public void onCompletion(Throwable error, Void result) {            // from the HTTP request forwarding thread.            if (error != null) {                                addRequest(RECONFIGURE_CONNECTOR_TASKS_BACKOFF_MS, new Callable<Void>() {                    @Override                    public Void call() throws Exception {                        reconfigureConnectorTasksWithRetry(connName);                        return null;                    }                }, new Callback<Void>() {                    @Override                    public void onCompletion(Throwable error, Void result) {                                                                    }                });            }        }    });}
private voidf9742
1
onCompletion
public voidf9743_1Throwable error, Void result)
{    // from the HTTP request forwarding thread.    if (error != null) {                addRequest(RECONFIGURE_CONNECTOR_TASKS_BACKOFF_MS, new Callable<Void>() {            @Override            public Void call() throws Exception {                reconfigureConnectorTasksWithRetry(connName);                return null;            }        }, new Callback<Void>() {            @Override            public void onCompletion(Throwable error, Void result) {                                            }        });    }}
public voidf9743
1
peekWithoutException
private DistributedHerderRequest kafkatest_f9751_0()
{    try {        return requests.isEmpty() ? null : requests.first();    } catch (NoSuchElementException e) {    // Ignore exception. Should be rare. Means that the collection became empty between    // checking the size and retrieving the first element.    }    return null;}
f9751
0
onConnectorConfigRemove
public voidf9752_1String connector)
{        synchronized (DistributedHerder.this) {        // rebalance after connector removal to ensure that existing tasks are balanced among workers        if (configState.contains(connector))            needsReconfigRebalance = true;        connectorConfigUpdates.add(connector);    }    member.wakeup();}
public voidf9752
1
onConnectorConfigUpdate
public voidf9753_1String connector)
{        // a rebalance, so we need to be careful about what operation we request.    synchronized (DistributedHerder.this) {        if (!configState.contains(connector))            needsReconfigRebalance = true;        connectorConfigUpdates.add(connector);    }    member.wakeup();}
public voidf9753
1
hashCode
public int kafkatest_f9761_0()
{    return Objects.hash(at, seq);}
f9761
0
forwardErrorCallback
private static final Callback<Void> kafkatest_f9762_0(final Callback<?> callback)
{    return new Callback<Void>() {        @Override        public void onCompletion(Throwable error, Void result) {            if (error != null)                callback.onCompletion(error, null);        }    };}
f9762
0
onCompletion
public void kafkatest_f9763_0(Throwable error, Void result)
{    if (error != null)        callback.onCompletion(error, null);}
f9763
0
metricValue
public Double kafkatest_f9771_0(long now)
{    return lastRebalanceCompletedAtMillis == Long.MIN_VALUE ? Double.POSITIVE_INFINITY : (double) (now - lastRebalanceCompletedAtMillis);}
f9771
0
close
 void kafkatest_f9772_0()
{    metricGroup.close();}
f9772
0
rebalanceStarted
 void kafkatest_f9773_0(long now)
{    rebalanceStartedAtMillis = now;    rebalancing = true;}
f9773
0
sorted
private static List<T> kafkatest_f9781_0(Collection<T> members)
{    List<T> res = new ArrayList<>(members);    Collections.sort(res);    return res;}
f9781
0
version
public short kafkatest_f9782_0()
{    return version;}
f9782
0
revokedConnectors
public Collection<String> kafkatest_f9783_0()
{    return revokedConnectorIds;}
f9783
0
taskAssignments
private static Collection<Struct> kafkatest_f9791_0(Map<String, Collection<Integer>> assignments)
{    return assignments == null ? null : assignments.entrySet().stream().map(connectorEntry -> {        Struct taskAssignment = new Struct(CONNECTOR_ASSIGNMENT_V1);        taskAssignment.set(CONNECTOR_KEY_NAME, connectorEntry.getKey());        taskAssignment.set(TASKS_KEY_NAME, connectorEntry.getValue().toArray());        return taskAssignment;    }).collect(Collectors.toList());}
f9791
0
extractConnectors
private static Collection<String> kafkatest_f9792_0(Struct struct, String key)
{    assert REVOKED_KEY_NAME.equals(key) || ASSIGNMENT_KEY_NAME.equals(key);    Object[] connectors = struct.getArray(key);    if (connectors == null) {        return Collections.emptyList();    }    List<String> connectorIds = new ArrayList<>();    for (Object structObj : connectors) {        Struct assignment = (Struct) structObj;        String connector = assignment.getString(CONNECTOR_KEY_NAME);        for (Object taskIdObj : assignment.getArray(TASKS_KEY_NAME)) {            Integer taskId = (Integer) taskIdObj;            if (taskId == CONNECTOR_TASK) {                connectorIds.add(connector);            }        }    }    return connectorIds;}
f9792
0
extractTasks
private static Collection<ConnectorTaskId> kafkatest_f9793_0(Struct struct, String key)
{    assert REVOKED_KEY_NAME.equals(key) || ASSIGNMENT_KEY_NAME.equals(key);    Object[] tasks = struct.getArray(key);    if (tasks == null) {        return Collections.emptyList();    }    List<ConnectorTaskId> tasksIds = new ArrayList<>();    for (Object structObj : tasks) {        Struct assignment = (Struct) structObj;        String connector = assignment.getString(CONNECTOR_KEY_NAME);        for (Object taskIdObj : assignment.getArray(TASKS_KEY_NAME)) {            Integer taskId = (Integer) taskIdObj;            if (taskId != CONNECTOR_TASK) {                tasksIds.add(new ConnectorTaskId(connector, taskId));            }        }    }    return tasksIds;}
f9793
0
handleLostAssignments
protected voidf9801_1ConnectorsAndTasks lostAssignments, ConnectorsAndTasks newSubmissions, List<WorkerLoad> completeWorkerAssignment)
{    if (lostAssignments.isEmpty()) {        return;    }    final long now = time.milliseconds();        if (scheduledRebalance > 0 && now >= scheduledRebalance) {        // delayed rebalance expired and it's time to assign resources        Optional<WorkerLoad> candidateWorkerLoad = Optional.empty();        if (!candidateWorkersForReassignment.isEmpty()) {            candidateWorkerLoad = pickCandidateWorkerForReassignment(completeWorkerAssignment);        }        if (candidateWorkerLoad.isPresent()) {            WorkerLoad workerLoad = candidateWorkerLoad.get();            lostAssignments.connectors().forEach(workerLoad::assign);            lostAssignments.tasks().forEach(workerLoad::assign);        } else {            newSubmissions.connectors().addAll(lostAssignments.connectors());            newSubmissions.tasks().addAll(lostAssignments.tasks());        }        candidateWorkersForReassignment.clear();        scheduledRebalance = 0;        delay = 0;    } else {        candidateWorkersForReassignment.addAll(candidateWorkersForReassignment(completeWorkerAssignment));        if (now < scheduledRebalance) {            // a delayed rebalance is in progress, but it's not yet time to reassign            // unaccounted resources            delay = calculateDelay(now);        } else {            // This means scheduledRebalance == 0            // We could also also extract the current minimum delay from the group, to make            // independent of consecutive leader failures, but this optimization is skipped            // at the moment            delay = maxDelay;        }        scheduledRebalance = now + delay;    }}
protected voidf9801
1
candidateWorkersForReassignment
private Set<String> kafkatest_f9802_0(List<WorkerLoad> completeWorkerAssignment)
{    return completeWorkerAssignment.stream().filter(WorkerLoad::isEmpty).map(WorkerLoad::worker).collect(Collectors.toSet());}
f9802
0
pickCandidateWorkerForReassignment
private Optional<WorkerLoad> kafkatest_f9803_0(List<WorkerLoad> completeWorkerAssignment)
{    Map<String, WorkerLoad> activeWorkers = completeWorkerAssignment.stream().collect(Collectors.toMap(WorkerLoad::worker, Function.identity()));    return candidateWorkersForReassignment.stream().map(activeWorkers::get).filter(Objects::nonNull).findFirst();}
f9803
0
assignConnectors
protected voidf9811_1List<WorkerLoad> workerAssignment, Collection<String> connectors)
{    workerAssignment.sort(WorkerLoad.connectorComparator());    WorkerLoad first = workerAssignment.get(0);    Iterator<String> load = connectors.iterator();    while (load.hasNext()) {        int firstLoad = first.connectorsSize();        int upTo = IntStream.range(0, workerAssignment.size()).filter(i -> workerAssignment.get(i).connectorsSize() > firstLoad).findFirst().orElse(workerAssignment.size());        for (WorkerLoad worker : workerAssignment.subList(0, upTo)) {            String connector = load.next();                        worker.assign(connector);            if (!load.hasNext()) {                break;            }        }    }}
protected voidf9811
1
assignTasks
protected voidf9812_1List<WorkerLoad> workerAssignment, Collection<ConnectorTaskId> tasks)
{    workerAssignment.sort(WorkerLoad.taskComparator());    WorkerLoad first = workerAssignment.get(0);    Iterator<ConnectorTaskId> load = tasks.iterator();    while (load.hasNext()) {        int firstLoad = first.tasksSize();        int upTo = IntStream.range(0, workerAssignment.size()).filter(i -> workerAssignment.get(i).tasksSize() > firstLoad).findFirst().orElse(workerAssignment.size());        for (WorkerLoad worker : workerAssignment.subList(0, upTo)) {            ConnectorTaskId task = load.next();                        worker.assign(task);            if (!load.hasNext()) {                break;            }        }    }}
protected voidf9812
1
workerAssignment
private static List<WorkerLoad> kafkatest_f9813_0(Map<String, ExtendedWorkerState> memberConfigs, ConnectorsAndTasks toExclude)
{    ConnectorsAndTasks ignore = new ConnectorsAndTasks.Builder().with(new HashSet<>(toExclude.connectors()), new HashSet<>(toExclude.tasks())).build();    return memberConfigs.entrySet().stream().map(e -> new WorkerLoad.Builder(e.getKey()).with(e.getValue().assignment().connectors().stream().filter(v -> !ignore.connectors().contains(v)).collect(Collectors.toList()), e.getValue().assignment().tasks().stream().filter(v -> !ignore.tasks().contains(v)).collect(Collectors.toList())).build()).collect(Collectors.toList());}
f9813
0
requestRejoin
public void kafkatest_f9821_0()
{    rejoinRequested = true;}
f9821
0
protocolType
public String kafkatest_f9822_0()
{    return "connect";}
f9822
0
ensureCoordinatorReady
protected synchronized boolean kafkatest_f9823_0(final Timer timer)
{    return super.ensureCoordinatorReady(timer);}
f9823
0
isLeader
private boolean kafkatest_f9831_0()
{    return assignmentSnapshot != null && memberId().equals(assignmentSnapshot.leader());}
f9831
0
ownerUrl
public String kafkatest_f9832_0(String connector)
{    if (rejoinNeededOrPending() || !isLeader())        return null;    return leaderState().ownerUrl(connector);}
f9832
0
ownerUrl
public String kafkatest_f9833_0(ConnectorTaskId task)
{    if (rejoinNeededOrPending() || !isLeader())        return null;    return leaderState().ownerUrl(task);}
f9833
0
measure
public double kafkatest_f9841_0(MetricConfig config, long now)
{    return assignmentSnapshot.tasks().size();}
f9841
0
invertAssignment
public static Map<V, K> kafkatest_f9842_0(Map<K, Collection<V>> assignment)
{    Map<V, K> inverted = new HashMap<>();    for (Map.Entry<K, Collection<V>> assignmentEntry : assignment.entrySet()) {        K key = assignmentEntry.getKey();        for (V value : assignmentEntry.getValue()) inverted.put(value, key);    }    return inverted;}
f9842
0
ownerUrl
private String kafkatest_f9843_0(ConnectorTaskId id)
{    String ownerId = taskOwners.get(id);    if (ownerId == null)        return null;    return allMembers.get(ownerId).url();}
f9843
0
isEmpty
public boolean kafkatest_f9851_0()
{    return connectors.isEmpty() && tasks.isEmpty();}
f9851
0
toString
public String kafkatest_f9852_0()
{    return "{ connectorIds=" + connectors + ", taskIds=" + tasks + '}';}
f9852
0
withCopies
public WorkerLoad.Builder kafkatest_f9853_0(Collection<String> connectors, Collection<ConnectorTaskId> tasks)
{    withConnectors = new ArrayList<>(Objects.requireNonNull(connectors, "connectors may be empty but not null"));    withTasks = new ArrayList<>(Objects.requireNonNull(tasks, "tasks may be empty but not null"));    return this;}
f9853
0
assign
public void kafkatest_f9861_0(String connector)
{    connectors.add(connector);}
f9861
0
assign
public void kafkatest_f9862_0(ConnectorTaskId task)
{    tasks.add(task);}
f9862
0
size
public int kafkatest_f9863_0()
{    return connectors.size() + tasks.size();}
f9863
0
ensureActive
public void kafkatest_f9871_0()
{    coordinator.poll(0);}
f9871
0
poll
public void kafkatest_f9872_0(long timeout)
{    if (timeout < 0)        throw new IllegalArgumentException("Timeout must not be negative");    coordinator.poll(timeout);}
f9872
0
wakeup
public void kafkatest_f9873_0()
{    this.client.wakeup();}
f9873
0
createAndSetup
public static DeadLetterQueueReporterf9881_1Map<String, Object> adminProps, ConnectorTaskId id, SinkConnectorConfig sinkConfig, Map<String, Object> producerProps, ErrorHandlingMetrics errorHandlingMetrics)
{    String topic = sinkConfig.dlqTopicName();    try (Admin admin = Admin.create(adminProps)) {        if (!admin.listTopics().names().get().contains(topic)) {                        NewTopic schemaTopicRequest = new NewTopic(topic, DLQ_NUM_DESIRED_PARTITIONS, sinkConfig.dlqTopicReplicationFactor());            admin.createTopics(singleton(schemaTopicRequest)).all().get();        }    } catch (InterruptedException e) {        throw new ConnectException("Could not initialize dead letter queue with topic=" + topic, e);    } catch (ExecutionException e) {        if (!(e.getCause() instanceof TopicExistsException)) {            throw new ConnectException("Could not initialize dead letter queue with topic=" + topic, e);        }    }    KafkaProducer<byte[], byte[]> dlqProducer = new KafkaProducer<>(producerProps);    return new DeadLetterQueueReporter(dlqProducer, sinkConfig, id, errorHandlingMetrics);}
public static DeadLetterQueueReporterf9881
1
report
public voidf9882_1ProcessingContext context)
{    final String dlqTopicName = connConfig.dlqTopicName();    if (dlqTopicName.isEmpty()) {        return;    }    errorHandlingMetrics.recordDeadLetterQueueProduceRequest();    ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();    if (originalMessage == null) {        errorHandlingMetrics.recordDeadLetterQueueProduceFailed();        return;    }    ProducerRecord<byte[], byte[]> producerRecord;    if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {        producerRecord = new ProducerRecord<>(dlqTopicName, null, originalMessage.key(), originalMessage.value(), originalMessage.headers());    } else {        producerRecord = new ProducerRecord<>(dlqTopicName, null, originalMessage.timestamp(), originalMessage.key(), originalMessage.value(), originalMessage.headers());    }    if (connConfig.isDlqContextHeadersEnabled()) {        populateContextHeaders(producerRecord, context);    }    this.kafkaProducer.send(producerRecord, (metadata, exception) -> {        if (exception != null) {                        errorHandlingMetrics.recordDeadLetterQueueProduceFailed();        }    });}
public voidf9882
1
populateContextHeaders
 void kafkatest_f9883_0(ProducerRecord<byte[], byte[]> producerRecord, ProcessingContext context)
{    Headers headers = producerRecord.headers();    if (context.consumerRecord() != null) {        headers.add(ERROR_HEADER_ORIG_TOPIC, toBytes(context.consumerRecord().topic()));        headers.add(ERROR_HEADER_ORIG_PARTITION, toBytes(context.consumerRecord().partition()));        headers.add(ERROR_HEADER_ORIG_OFFSET, toBytes(context.consumerRecord().offset()));    }    headers.add(ERROR_HEADER_CONNECTOR_NAME, toBytes(connectorTaskId.connector()));    headers.add(ERROR_HEADER_TASK_ID, toBytes(String.valueOf(connectorTaskId.task())));    headers.add(ERROR_HEADER_STAGE, toBytes(context.stage().name()));    headers.add(ERROR_HEADER_EXECUTING_CLASS, toBytes(context.executingClass().getName()));    if (context.error() != null) {        headers.add(ERROR_HEADER_EXCEPTION, toBytes(context.error().getClass().getName()));        headers.add(ERROR_HEADER_EXCEPTION_MESSAGE, toBytes(context.error().getMessage()));        byte[] trace;        if ((trace = stacktrace(context.error())) != null) {            headers.add(ERROR_HEADER_EXCEPTION_STACK_TRACE, trace);        }    }}
f9883
0
recordRetry
public void kafkatest_f9891_0()
{    retries.record();}
f9891
0
recordErrorLogged
public void kafkatest_f9892_0()
{    errorsLogged.record();}
f9892
0
recordDeadLetterQueueProduceRequest
public void kafkatest_f9893_0()
{    dlqProduceRequests.record();}
f9893
0
consumerRecord
public ConsumerRecord<byte[], byte[]> kafkatest_f9901_0()
{    return consumedMessage;}
f9901
0
sourceRecord
public SourceRecord kafkatest_f9902_0()
{    return sourceRecord;}
f9902
0
sourceRecord
public void kafkatest_f9903_0(SourceRecord record)
{    this.sourceRecord = record;    reset();}
f9903
0
toString
public String kafkatest_f9911_0(boolean includeMessage)
{    StringBuilder builder = new StringBuilder();    builder.append("Executing stage '");    builder.append(stage().name());    builder.append("' with class '");    builder.append(executingClass() == null ? "null" : executingClass().getName());    builder.append('\'');    if (includeMessage && sourceRecord() != null) {        builder.append(", where source record is = ");        builder.append(sourceRecord());    } else if (includeMessage && consumerRecord() != null) {        ConsumerRecord<byte[], byte[]> msg = consumerRecord();        builder.append(", where consumed record is ");        builder.append("{topic='").append(msg.topic()).append('\'');        builder.append(", partition=").append(msg.partition());        builder.append(", offset=").append(msg.offset());        if (msg.timestampType() == TimestampType.CREATE_TIME || msg.timestampType() == TimestampType.LOG_APPEND_TIME) {            builder.append(", timestamp=").append(msg.timestamp());            builder.append(", timestampType=").append(msg.timestampType());        }        builder.append("}");    }    builder.append('.');    return builder.toString();}
f9911
0
attempt
public void kafkatest_f9912_0(int attempt)
{    this.attempt = attempt;}
f9912
0
attempt
public int kafkatest_f9913_0()
{    return attempt;}
f9913
0
markAsFailed
 void kafkatest_f9921_0()
{    errorHandlingMetrics.recordErrorTimestamp();    totalFailures++;}
f9921
0
withinToleranceLimits
 boolean kafkatest_f9922_0()
{    switch(errorToleranceType) {        case NONE:            if (totalFailures > 0)                return false;        case ALL:            return true;        default:            throw new ConfigException("Unknown tolerance type: {}", errorToleranceType);    }}
f9922
0
checkRetry
 boolean kafkatest_f9923_0(long startTime)
{    return (time.milliseconds() - startTime) < errorRetryTimeout;}
f9923
0
value
public String kafkatest_f9931_0()
{    return name().toLowerCase(Locale.ROOT);}
f9931
0
kafkaClusterId
public String kafkatest_f9932_0()
{    return kafkaClusterId;}
f9932
0
connectors
public Collection<String> kafkatest_f9933_0()
{    FutureCallback<Collection<String>> connectorsCallback = new FutureCallback<>();    herder.connectors(connectorsCallback);    try {        return connectorsCallback.get(herderRequestTimeoutMs, TimeUnit.MILLISECONDS);    } catch (InterruptedException | ExecutionException | TimeoutException e) {        throw new ConnectException("Failed to retrieve list of connectors", e);    }}
f9933
0
hashCode
public int kafkatest_f9941_0()
{    return Objects.hash(created, result);}
f9941
0
requestTaskReconfiguration
public void kafkatest_f9942_0()
{    // Local herder runs in memory in this process    // Distributed herder will forward the request to the leader if needed    herder.requestTaskReconfiguration(connectorName);}
f9942
0
raiseError
public void kafkatest_f9943_0(Exception e)
{    herder.onFailure(connectorName, e);}
f9943
0
connectorLoader
public ClassLoader kafkatest_f9951_0(Connector connector)
{    return connectorLoader(connector.getClass().getName());}
f9951
0
connectorLoader
public ClassLoaderf9952_1String connectorClassOrAlias)
{        String fullName = aliases.containsKey(connectorClassOrAlias) ? aliases.get(connectorClassOrAlias) : connectorClassOrAlias;    SortedMap<PluginDesc<?>, ClassLoader> inner = pluginLoaders.get(fullName);    if (inner == null) {                return this;    }    return inner.get(inner.lastKey());}
public ClassLoaderf9952
1
newPluginClassLoader
private static PluginClassLoader kafkatest_f9953_0(final URL pluginLocation, final URL[] urls, final ClassLoader parent)
{    return AccessController.doPrivileged((PrivilegedAction<PluginClassLoader>) () -> new PluginClassLoader(pluginLocation, urls, parent));}
f9953
0
scanPluginPath
private PluginScanResult kafkatest_f9961_0(ClassLoader loader, URL[] urls) throws InstantiationException, IllegalAccessException
{    ConfigurationBuilder builder = new ConfigurationBuilder();    builder.setClassLoaders(new ClassLoader[] { loader });    builder.addUrls(urls);    builder.setScanners(new SubTypesScanner());    builder.useParallelExecutor();    Reflections reflections = new InternalReflections(builder);    return new PluginScanResult(getPluginDesc(reflections, Connector.class, loader), getPluginDesc(reflections, Converter.class, loader), getPluginDesc(reflections, HeaderConverter.class, loader), getPluginDesc(reflections, Transformation.class, loader), getServiceLoaderPluginDesc(ConfigProvider.class, loader), getServiceLoaderPluginDesc(ConnectRestExtension.class, loader), getServiceLoaderPluginDesc(ConnectorClientConfigOverridePolicy.class, loader));}
f9961
0
getPluginDesc
private Collection<PluginDesc<T>>f9962_1Reflections reflections, Class<T> klass, ClassLoader loader) throws InstantiationException, IllegalAccessException
{    Set<Class<? extends T>> plugins = reflections.getSubTypesOf(klass);    Collection<PluginDesc<T>> result = new ArrayList<>();    for (Class<? extends T> plugin : plugins) {        if (PluginUtils.isConcrete(plugin)) {            result.add(new PluginDesc<>(plugin, versionFor(plugin), loader));        } else {                    }    }    return result;}
private Collection<PluginDesc<T>>f9962
1
getServiceLoaderPluginDesc
private Collection<PluginDesc<T>> kafkatest_f9963_0(Class<T> klass, ClassLoader loader)
{    ServiceLoader<T> serviceLoader = ServiceLoader.load(klass, loader);    Collection<PluginDesc<T>> result = new ArrayList<>();    for (T pluginImpl : serviceLoader) {        result.add(new PluginDesc<>((Class<? extends T>) pluginImpl.getClass(), versionFor(pluginImpl), loader));    }    return result;}
f9963
0
getResources
public Enumeration<URL> kafkatest_f9971_0(String name) throws IOException
{    if (serviceLoaderManifestForPlugin(name)) {        // PluginClassLoader to limit its resource search to only its own URL paths.        return null;    } else {        return super.getResources(name);    }}
f9971
0
serviceLoaderManifestForPlugin
 static boolean kafkatest_f9972_0(String name)
{    return PLUGIN_MANIFEST_FILES.contains(name);}
f9972
0
location
public String kafkatest_f9973_0()
{    return pluginLocation.toString();}
f9973
0
typeName
public String kafkatest_f9981_0()
{    return typeName;}
f9981
0
location
public String kafkatest_f9982_0()
{    return location;}
f9982
0
equals
public boolean kafkatest_f9983_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof PluginDesc)) {        return false;    }    PluginDesc<?> that = (PluginDesc<?>) o;    return Objects.equals(klass, that.klass) && Objects.equals(version, that.version) && type == that.type;}
f9983
0
compareAndSwapLoaders
public static ClassLoader kafkatest_f9991_0(ClassLoader loader)
{    ClassLoader current = Thread.currentThread().getContextClassLoader();    if (!current.equals(loader)) {        Thread.currentThread().setContextClassLoader(loader);    }    return current;}
f9991
0
currentThreadLoader
public ClassLoader kafkatest_f9992_0()
{    return Thread.currentThread().getContextClassLoader();}
f9992
0
compareAndSwapWithDelegatingLoader
public ClassLoader kafkatest_f9993_0()
{    ClassLoader current = Thread.currentThread().getContextClassLoader();    if (!current.equals(delegatingLoader)) {        Thread.currentThread().setContextClassLoader(delegatingLoader);    }    return current;}
f9993
0
connectorClass
public Class<? extends Connector> kafkatest_f10001_0(String connectorClassOrAlias)
{    Class<? extends Connector> klass;    try {        klass = pluginClass(delegatingLoader, connectorClassOrAlias, Connector.class);    } catch (ClassNotFoundException e) {        List<PluginDesc<Connector>> matches = new ArrayList<>();        for (PluginDesc<Connector> plugin : delegatingLoader.connectors()) {            Class<?> pluginClass = plugin.pluginClass();            String simpleName = pluginClass.getSimpleName();            if (simpleName.equals(connectorClassOrAlias) || simpleName.equals(connectorClassOrAlias + "Connector")) {                matches.add(plugin);            }        }        if (matches.isEmpty()) {            throw new ConnectException("Failed to find any class that implements Connector and which name matches " + connectorClassOrAlias + ", available connectors are: " + pluginNames(delegatingLoader.connectors()));        }        if (matches.size() > 1) {            throw new ConnectException("More than one connector matches alias " + connectorClassOrAlias + ". Please use full package and class name instead. Classes found: " + pluginNames(matches));        }        PluginDesc<Connector> entry = matches.get(0);        klass = entry.pluginClass();    }    return klass;}
f10001
0
newTask
public Task kafkatest_f10002_0(Class<? extends Task> taskClass)
{    return newPlugin(taskClass);}
f10002
0
newConverter
public Converterf10003_1AbstractConfig config, String classPropertyName, ClassLoaderUsage classLoaderUsage)
{    if (!config.originals().containsKey(classPropertyName) && !isInternalConverter(classPropertyName)) {        // it does not represent an internal converter (which has a default available)        return null;    }    Converter plugin = null;    switch(classLoaderUsage) {        case CURRENT_CLASSLOADER:            // Attempt to load first with the current classloader, and plugins as a fallback.            // Note: we can't use config.getConfiguredInstance because Converter doesn't implement Configurable, and even if it did            // we have to remove the property prefixes before calling config(...) and we still always want to call Converter.config.            plugin = getInstance(config, classPropertyName, Converter.class);            break;        case PLUGINS:            // Attempt to load with the plugin class loader, which uses the current classloader as a fallback            String converterClassOrAlias = config.getClass(classPropertyName).getName();            Class<? extends Converter> klass;            try {                klass = pluginClass(delegatingLoader, converterClassOrAlias, Converter.class);            } catch (ClassNotFoundException e) {                throw new ConnectException("Failed to find any class that implements Converter and which name matches " + converterClassOrAlias + ", available converters are: " + pluginNames(delegatingLoader.converters()));            }            plugin = newPlugin(klass);            break;    }    if (plugin == null) {        throw new ConnectException("Unable to instantiate the Converter specified in '" + classPropertyName + "'");    }    // Determine whether this is a key or value converter based upon the supplied property name ...    @SuppressWarnings("deprecation")    final boolean isKeyConverter = WorkerConfig.KEY_CONVERTER_CLASS_CONFIG.equals(classPropertyName) || WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG.equals(classPropertyName);    // Configure the Converter using only the old configuration mechanism ...    String configPrefix = classPropertyName + ".";    Map<String, Object> converterConfig = config.originalsWithPrefix(configPrefix);        // WorkerConfig class    if (plugin instanceof JsonConverter && isInternalConverter(classPropertyName)) {        // or internal.value.converter.schemas.enable, we can safely default them to false        if (!converterConfig.containsKey(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG)) {            converterConfig.put(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG, false);        }    }    plugin.configure(converterConfig, isKeyConverter);    return plugin;}
public Converterf10003
1
converters
public Collection<PluginDesc<Converter>> kafkatest_f10011_0()
{    return converters;}
f10011
0
headerConverters
public Collection<PluginDesc<HeaderConverter>> kafkatest_f10012_0()
{    return headerConverters;}
f10012
0
transformations
public Collection<PluginDesc<Transformation>> kafkatest_f10013_0()
{    return transformations;}
f10013
0
accept
public boolean kafkatest_f10021_0(Path path)
{    return Files.isDirectory(path) || isArchive(path) || isClassFile(path);}
f10021
0
shouldLoadInIsolation
public static boolean kafkatest_f10022_0(String name)
{    return !(BLACKLIST.matcher(name).matches() && !WHITELIST.matcher(name).matches());}
f10022
0
isConcrete
public static boolean kafkatest_f10023_0(Class<?> klass)
{    int mod = klass.getModifiers();    return !Modifier.isAbstract(mod) && !Modifier.isInterface(mod);}
f10023
0
prunePluginName
private static String kafkatest_f10031_0(PluginDesc<?> plugin, String suffix)
{    String simple = plugin.pluginClass().getSimpleName();    int pos = simple.lastIndexOf(suffix);    if (pos > 0) {        return simple.substring(0, pos);    }    return simple;}
f10031
0
getConfiguration
public Configuration kafkatest_f10032_0()
{    return resourceConfig.getConfiguration();}
f10032
0
property
public ResourceConfig kafkatest_f10033_0(String name, Object value)
{    return resourceConfig.property(name, value);}
f10033
0
register
public ResourceConfig kafkatest_f10041_0(Class<?> componentClass)
{    if (allowedToRegister(componentClass)) {        resourceConfig.register(componentClass);    }    return resourceConfig;}
f10041
0
allowedToRegister
private booleanf10042_1Object component)
{    if (resourceConfig.isRegistered(component)) {                return NOT_ALLOWED_TO_REGISTER;    }    return ALLOWED_TO_REGISTER;}
private booleanf10042
1
allowedToRegister
private booleanf10043_1Class<?> componentClass)
{    if (resourceConfig.isRegistered(componentClass)) {                return NOT_ALLOWED_TO_REGISTER;    }    return ALLOWED_TO_REGISTER;}
private booleanf10043
1
name
public String kafkatest_f10051_0()
{    return name;}
f10051
0
groups
public List<String> kafkatest_f10052_0()
{    return groups;}
f10052
0
errorCount
public int kafkatest_f10053_0()
{    return errorCount;}
f10053
0
defaultValue
public String kafkatest_f10061_0()
{    return defaultValue;}
f10061
0
documentation
public String kafkatest_f10062_0()
{    return documentation;}
f10062
0
group
public String kafkatest_f10063_0()
{    return group;}
f10063
0
toString
public String kafkatest_f10071_0()
{    StringBuffer sb = new StringBuffer();    sb.append("[").append(name).append(",").append(type).append(",").append(required).append(",").append(defaultValue).append(",").append(importance).append(",").append(documentation).append(",").append(group).append(",").append(orderInGroup).append(",").append(width).append(",").append(displayName).append(",").append(dependents).append("]");    return sb.toString();}
f10071
0
name
public String kafkatest_f10072_0()
{    return name;}
f10072
0
value
public String kafkatest_f10073_0()
{    return value;}
f10073
0
type
public ConnectorType kafkatest_f10081_0()
{    return type;}
f10081
0
config
public Map<String, String> kafkatest_f10082_0()
{    return config;}
f10082
0
tasks
public List<ConnectorTaskId> kafkatest_f10083_0()
{    return tasks;}
f10083
0
toString
public String kafkatest_f10091_0()
{    final StringBuilder sb = new StringBuilder("ConnectorPluginInfo{");    sb.append("className='").append(className).append('\'');    sb.append(", type=").append(type);    sb.append(", version='").append(version).append('\'');    sb.append('}');    return sb.toString();}
f10091
0
name
public String kafkatest_f10092_0()
{    return name;}
f10092
0
connector
public ConnectorState kafkatest_f10093_0()
{    return connector;}
f10093
0
equals
public boolean kafkatest_f10101_0(Object o)
{    if (o == this)        return true;    if (!(o instanceof TaskState))        return false;    TaskState other = (TaskState) o;    return compareTo(other) == 0;}
f10101
0
hashCode
public int kafkatest_f10102_0()
{    return Objects.hash(id);}
f10102
0
from
public static ConnectorType kafkatest_f10103_0(Class<? extends Connector> clazz)
{    if (SinkConnector.class.isAssignableFrom(clazz)) {        return SINK;    }    if (SourceConnector.class.isAssignableFrom(clazz)) {        return SOURCE;    }    return UNKNOWN;}
f10103
0
message
public String kafkatest_f10111_0()
{    return message;}
f10111
0
equals
public boolean kafkatest_f10112_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ErrorMessage that = (ErrorMessage) o;    return Objects.equals(errorCode, that.errorCode) && Objects.equals(message, that.message);}
f10112
0
hashCode
public int kafkatest_f10113_0()
{    return Objects.hash(errorCode, message);}
f10113
0
toResponse
public Responsef10121_1Exception exception)
{        if (exception instanceof ConnectRestException) {        ConnectRestException restException = (ConnectRestException) exception;        return Response.status(restException.statusCode()).entity(new ErrorMessage(restException.errorCode(), restException.getMessage())).build();    }    if (exception instanceof NotFoundException) {        return Response.status(Response.Status.NOT_FOUND).entity(new ErrorMessage(Response.Status.NOT_FOUND.getStatusCode(), exception.getMessage())).build();    }    if (exception instanceof AlreadyExistsException) {        return Response.status(Response.Status.CONFLICT).entity(new ErrorMessage(Response.Status.CONFLICT.getStatusCode(), exception.getMessage())).build();    }    if (!log.isDebugEnabled()) {            }    final int statusCode;    if (exception instanceof WebApplicationException) {        Response.StatusType statusInfo = ((WebApplicationException) exception).getResponse().getStatusInfo();        statusCode = statusInfo.getStatusCode();    } else {        statusCode = Response.Status.INTERNAL_SERVER_ERROR.getStatusCode();    }    return Response.status(statusCode).entity(new ErrorMessage(statusCode, exception.getMessage())).build();}
public Responsef10121
1
statusCode
public int kafkatest_f10122_0()
{    return statusCode;}
f10122
0
errorCode
public int kafkatest_f10123_0()
{    return errorCode;}
f10123
0
getConnectorConfig
public Map<String, String> kafkatest_f10131_0(@PathParam("connector") final String connector, @Context final HttpHeaders headers, kafkatest_f10131_0("forward") final Boolean forward) throws Throwable
{    FutureCallback<Map<String, String>> cb = new FutureCallback<>();    herder.connectorConfig(connector, cb);    return completeOrForwardRequest(cb, "/connectors/" + connector + "/config", "GET", headers, null, forward);}
f10131
0
getConnectorStatus
public ConnectorStateInfo kafkatest_f10132_0(@PathParam("connector") final String connector) throws Throwable
{    return herder.connectorStatus(connector);}
f10132
0
putConnectorConfig
public Response kafkatest_f10133_0(@PathParam("connector") final String connector, @Context final HttpHeaders headers, kafkatest_f10133_0("forward") final Boolean forward, final Map<String, String> connectorConfig) throws Throwable
{    FutureCallback<Herder.Created<ConnectorInfo>> cb = new FutureCallback<>();    checkAndPutConnectorConfigName(connector, connectorConfig);    herder.putConnectorConfig(connector, connectorConfig, true, cb);    Herder.Created<ConnectorInfo> createdInfo = completeOrForwardRequest(cb, "/connectors/" + connector + "/config", "PUT", headers, connectorConfig, new TypeReference<ConnectorInfo>() {    }, new CreatedConnectorInfoTranslator(), forward);    Response.ResponseBuilder response;    if (createdInfo.created()) {        URI location = UriBuilder.fromUri("/connectors").path(connector).build();        response = Response.created(location);    } else {        response = Response.ok();    }    return response.entity(createdInfo.result()).build();}
f10133
0
destroyConnector
public void kafkatest_f10141_0(@PathParam("connector") final String connector, @Context final HttpHeaders headers, kafkatest_f10141_0("forward") final Boolean forward) throws Throwable
{    FutureCallback<Herder.Created<ConnectorInfo>> cb = new FutureCallback<>();    herder.deleteConnectorConfig(connector, cb);    completeOrForwardRequest(cb, "/connectors/" + connector, "DELETE", headers, null, forward);}
f10141
0
checkAndPutConnectorConfigName
private void kafkatest_f10142_0(String connectorName, Map<String, String> connectorConfig)
{    String includedName = connectorConfig.get(ConnectorConfig.NAME_CONFIG);    if (includedName != null) {        if (!includedName.equals(connectorName))            throw new BadRequestException("Connector name configuration (" + includedName + ") doesn't match connector name in the URL (" + connectorName + ")");    } else {        connectorConfig.put(ConnectorConfig.NAME_CONFIG, connectorName);    }}
f10142
0
completeOrForwardRequest
private Tf10143_1FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body, TypeReference<U> resultType, Translator<T, U> translator, Boolean forward) throws Throwable
{    try {        return cb.get(REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);    } catch (ExecutionException e) {        Throwable cause = e.getCause();        if (cause instanceof RequestTargetException) {            if (forward == null || forward) {                // the only time we allow recursive forwarding is when no forward flag has                // been set, which should only be seen by the first worker to handle a user request.                // this gives two total hops to resolve the request before giving up.                boolean recursiveForward = forward == null;                RequestTargetException targetException = (RequestTargetException) cause;                String forwardUrl = UriBuilder.fromUri(targetException.forwardUrl()).path(path).queryParam("forward", recursiveForward).build().toString();                                return translator.translate(RestClient.httpRequest(forwardUrl, method, headers, body, resultType, config));            } else {                // we don't, it probably means that a rebalance has taken place.                throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(), "Cannot complete request because of a conflicting operation (e.g. worker rebalance)");            }        } else if (cause instanceof RebalanceNeededException) {            throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(), "Cannot complete request momentarily due to stale configuration (typically caused by a concurrent config change)");        }        throw cause;    } catch (TimeoutException e) {        // error is the best option        throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), "Request timed out");    } catch (InterruptedException e) {        throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), "Request interrupted");    }}
private Tf10143
1
convertHttpFieldsToMap
private static Map<String, String> kafkatest_f10151_0(HttpFields httpFields)
{    Map<String, String> headers = new HashMap<String, String>();    if (httpFields == null || httpFields.size() == 0)        return headers;    for (HttpField field : httpFields) {        headers.put(field.getName(), field.getValue());    }    return headers;}
f10151
0
status
public int kafkatest_f10152_0()
{    return status;}
f10152
0
headers
public Map<String, String> kafkatest_f10153_0()
{    return headers;}
f10153
0
stop
public voidf10161_1)
{        try {        for (ConnectRestExtension connectRestExtension : connectRestExtensions) {            try {                connectRestExtension.close();            } catch (IOException e) {                            }        }        jettyServer.stop();        jettyServer.join();    } catch (Exception e) {        jettyServer.destroy();        throw new ConnectException("Unable to stop REST server", e);    }    }
public voidf10161
1
advertisedUrl
public URIf10162_1)
{    UriBuilder builder = UriBuilder.fromUri(jettyServer.getURI());    String advertisedSecurityProtocol = determineAdvertisedProtocol();    ServerConnector serverConnector = findConnector(advertisedSecurityProtocol);    builder.scheme(advertisedSecurityProtocol);    String advertisedHostname = config.getString(WorkerConfig.REST_ADVERTISED_HOST_NAME_CONFIG);    if (advertisedHostname != null && !advertisedHostname.isEmpty())        builder.host(advertisedHostname);    else if (serverConnector != null && serverConnector.getHost() != null && serverConnector.getHost().length() > 0)        builder.host(serverConnector.getHost());    Integer advertisedPort = config.getInt(WorkerConfig.REST_ADVERTISED_PORT_CONFIG);    if (advertisedPort != null)        builder.port(advertisedPort);    else if (serverConnector != null && serverConnector.getPort() > 0)        builder.port(serverConnector.getPort());        return builder.build();}
public URIf10162
1
determineAdvertisedProtocol
 String kafkatest_f10163_0()
{    String advertisedSecurityProtocol = config.getString(WorkerConfig.REST_ADVERTISED_LISTENER_CONFIG);    if (advertisedSecurityProtocol == null) {        String listeners = (String) config.originals().get(WorkerConfig.LISTENERS_CONFIG);        if (listeners == null)            return PROTOCOL_HTTP;        else            listeners = listeners.toLowerCase(Locale.ENGLISH);        if (listeners.contains(String.format("%s://", PROTOCOL_HTTP)))            return PROTOCOL_HTTP;        else if (listeners.contains(String.format("%s://", PROTOCOL_HTTPS)))            return PROTOCOL_HTTPS;        else            return PROTOCOL_HTTP;    } else {        return advertisedSecurityProtocol.toLowerCase(Locale.ENGLISH);    }}
f10163
0
configureSslContextFactoryTrustStore
protected static void kafkatest_f10171_0(SslContextFactory ssl, Map<String, Object> sslConfigValues)
{    ssl.setTrustStoreType((String) getOrDefault(sslConfigValues, SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, SslConfigs.DEFAULT_SSL_TRUSTSTORE_TYPE));    String sslTruststoreLocation = (String) sslConfigValues.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG);    if (sslTruststoreLocation != null)        ssl.setTrustStorePath(sslTruststoreLocation);    Password sslTruststorePassword = (Password) sslConfigValues.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG);    if (sslTruststorePassword != null)        ssl.setTrustStorePassword(sslTruststorePassword.value());}
f10171
0
configureSslContextFactoryAlgorithms
protected static void kafkatest_f10172_0(SslContextFactory ssl, Map<String, Object> sslConfigValues)
{    List<String> sslEnabledProtocols = (List<String>) getOrDefault(sslConfigValues, SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, Arrays.asList(COMMA_WITH_WHITESPACE.split(SslConfigs.DEFAULT_SSL_ENABLED_PROTOCOLS)));    ssl.setIncludeProtocols(sslEnabledProtocols.toArray(new String[sslEnabledProtocols.size()]));    String sslProvider = (String) sslConfigValues.get(SslConfigs.SSL_PROVIDER_CONFIG);    if (sslProvider != null)        ssl.setProvider(sslProvider);    ssl.setProtocol((String) getOrDefault(sslConfigValues, SslConfigs.SSL_PROTOCOL_CONFIG, SslConfigs.DEFAULT_SSL_PROTOCOL));    List<String> sslCipherSuites = (List<String>) sslConfigValues.get(SslConfigs.SSL_CIPHER_SUITES_CONFIG);    if (sslCipherSuites != null)        ssl.setIncludeCipherSuites(sslCipherSuites.toArray(new String[sslCipherSuites.size()]));    ssl.setKeyManagerFactoryAlgorithm((String) getOrDefault(sslConfigValues, SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG, SslConfigs.DEFAULT_SSL_KEYMANGER_ALGORITHM));    String sslSecureRandomImpl = (String) sslConfigValues.get(SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG);    if (sslSecureRandomImpl != null)        ssl.setSecureRandomAlgorithm(sslSecureRandomImpl);    ssl.setTrustManagerFactoryAlgorithm((String) getOrDefault(sslConfigValues, SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG, SslConfigs.DEFAULT_SSL_TRUSTMANAGER_ALGORITHM));}
f10172
0
configureSslContextFactoryEndpointIdentification
protected static void kafkatest_f10173_0(SslContextFactory ssl, Map<String, Object> sslConfigValues)
{    String sslEndpointIdentificationAlg = (String) sslConfigValues.get(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG);    if (sslEndpointIdentificationAlg != null)        ssl.setEndpointIdentificationAlgorithm(sslEndpointIdentificationAlg);}
f10173
0
isDlqContextHeadersEnabled
public boolean kafkatest_f10181_0()
{    return getBoolean(DLQ_CONTEXT_HEADERS_ENABLE_CONFIG);}
f10181
0
main
public static void kafkatest_f10182_0(String[] args)
{    System.out.println(config.toHtml());}
f10182
0
configDef
public static ConfigDef kafkatest_f10183_0()
{    return config;}
f10183
0
stop
public synchronized voidf10191_1)
{        requestExecutorService.shutdown();    try {        if (!requestExecutorService.awaitTermination(30, TimeUnit.SECONDS))            requestExecutorService.shutdownNow();    } catch (InterruptedException e) {    // ignore    }    // the tasks.    for (String connName : connectors()) {        removeConnectorTasks(connName);        worker.stopConnector(connName);    }    stopServices();    }
public synchronized voidf10191
1
generation
public int kafkatest_f10192_0()
{    return 0;}
f10192
0
connectors
public synchronized void kafkatest_f10193_0(Callback<Collection<String>> callback)
{    callback.onCompletion(null, connectors());}
f10193
0
requestTaskReconfiguration
public synchronized voidf10201_1String connName)
{    if (!worker.connectorNames().contains(connName)) {                return;    }    updateConnectorTasks(connName);}
public synchronized voidf10201
1
taskConfigs
public synchronized void kafkatest_f10202_0(String connName, Callback<List<TaskInfo>> callback)
{    if (!configState.contains(connName)) {        callback.onCompletion(new NotFoundException("Connector " + connName + " not found", null), null);        return;    }    List<TaskInfo> result = new ArrayList<>();    for (ConnectorTaskId taskId : configState.tasks(connName)) result.add(new TaskInfo(taskId, configState.rawTaskConfig(taskId)));    callback.onCompletion(null, result);}
f10202
0
putTaskConfigs
public void kafkatest_f10203_0(String connName, List<Map<String, String>> configs, Callback<Void> callback)
{    throw new UnsupportedOperationException("Kafka Connect in standalone mode does not support externally setting task configurations.");}
f10203
0
removeConnectorTasks
private void kafkatest_f10211_0(String connName)
{    Collection<ConnectorTaskId> tasks = configState.tasks(connName);    if (!tasks.isEmpty()) {        worker.stopAndAwaitTasks(tasks);        configBackingStore.removeTaskConfigs(connName);    }}
f10211
0
updateConnectorTasks
private voidf10212_1String connName)
{    if (!worker.isRunning(connName)) {                return;    }    List<Map<String, String>> newTaskConfigs = recomputeTaskConfigs(connName);    List<Map<String, String>> oldTaskConfigs = configState.allTaskConfigs(connName);    if (!newTaskConfigs.equals(oldTaskConfigs)) {        removeConnectorTasks(connName);        List<Map<String, String>> rawTaskConfigs = reverseTransform(connName, configState, newTaskConfigs);        configBackingStore.putTaskConfigs(connName, rawTaskConfigs);        createConnectorTasks(connName, configState.targetState(connName));    }}
private voidf10212
1
onConnectorConfigRemove
public void kafkatest_f10213_0(String connector)
{    synchronized (StandaloneHerder.this) {        configState = configBackingStore.snapshot();    }}
f10213
0
durationRatio
public double kafkatest_f10221_0(State ratioState, long now)
{    return lastState.get().durationRatio(ratioState, now);}
f10221
0
currentState
public State kafkatest_f10222_0()
{    return lastState.get().state;}
f10222
0
newState
public StateChange kafkatest_f10223_0(State state, long now)
{    if (this.state == null) {        return new StateChange(state, now, 0L, 0L, 0L, 0L, 0L);    }    if (state == this.state) {        return this;    }    long unassignedTime = this.unassignedTotalTimeMs;    long runningTime = this.runningTotalTimeMs;    long pausedTime = this.pausedTotalTimeMs;    long failedTime = this.failedTotalTimeMs;    long destroyedTime = this.destroyedTotalTimeMs;    long duration = now - startTime;    switch(this.state) {        case UNASSIGNED:            unassignedTime += duration;            break;        case RUNNING:            runningTime += duration;            break;        case PAUSED:            pausedTime += duration;            break;        case FAILED:            failedTime += duration;            break;        case DESTROYED:            destroyedTime += duration;            break;    }    return new StateChange(state, now, unassignedTime, runningTime, pausedTime, failedTime, destroyedTime);}
f10223
0
configTransformer
public WorkerConfigTransformer kafkatest_f10231_0()
{    return workerConfigTransformer;}
f10231
0
herder
protected Herder kafkatest_f10232_0()
{    return herder;}
f10232
0
start
public voidf10233_1)
{        offsetBackingStore.start();    sourceTaskOffsetCommitter = new SourceTaskOffsetCommitter(config);    }
public voidf10233
1
isRunning
public boolean kafkatest_f10241_0(String connName)
{    WorkerConnector workerConnector = connectors.get(connName);    return workerConnector != null && workerConnector.isRunning();}
f10241
0
startTask
public booleanf10242_1ConnectorTaskId id, ClusterConfigState configState, Map<String, String> connProps, Map<String, String> taskProps, TaskStatus.Listener statusListener, TargetState initialState)
{    final WorkerTask workerTask;    try (LoggingContext loggingContext = LoggingContext.forTask(id)) {                if (tasks.containsKey(id))            throw new ConnectException("Task already exists in this worker: " + id);        ClassLoader savedLoader = plugins.currentThreadLoader();        try {            final ConnectorConfig connConfig = new ConnectorConfig(plugins, connProps);            String connType = connConfig.getString(ConnectorConfig.CONNECTOR_CLASS_CONFIG);            ClassLoader connectorLoader = plugins.delegatingLoader().connectorLoader(connType);            savedLoader = Plugins.compareAndSwapLoaders(connectorLoader);            final TaskConfig taskConfig = new TaskConfig(taskProps);            final Class<? extends Task> taskClass = taskConfig.getClass(TaskConfig.TASK_CLASS_CONFIG).asSubclass(Task.class);            final Task task = plugins.newTask(taskClass);                        // By maintaining connector's specific class loader for this thread here, we first            // search for converters within the connector dependencies.            // If any of these aren't found, that means the connector didn't configure specific converters,            // so we should instantiate based upon the worker configuration            Converter keyConverter = plugins.newConverter(connConfig, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);            Converter valueConverter = plugins.newConverter(connConfig, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);            HeaderConverter headerConverter = plugins.newHeaderConverter(connConfig, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);            if (keyConverter == null) {                keyConverter = plugins.newConverter(config, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);                            } else {                            }            if (valueConverter == null) {                valueConverter = plugins.newConverter(config, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);                            } else {                            }            if (headerConverter == null) {                headerConverter = plugins.newHeaderConverter(config, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);                            } else {                            }            workerTask = buildWorkerTask(configState, connConfig, id, task, statusListener, initialState, keyConverter, valueConverter, headerConverter, connectorLoader);            workerTask.initialize(taskConfig);            Plugins.compareAndSwapLoaders(savedLoader);        } catch (Throwable t) {                        // Can't be put in a finally block because it needs to be swapped before the call on            // statusListener            Plugins.compareAndSwapLoaders(savedLoader);            workerMetricsGroup.recordTaskFailure();            statusListener.onFailure(id, t);            return false;        }        WorkerTask existing = tasks.putIfAbsent(id, workerTask);        if (existing != null)            throw new ConnectException("Task already exists in this worker: " + id);        executor.submit(workerTask);        if (workerTask instanceof WorkerSourceTask) {            sourceTaskOffsetCommitter.schedule(id, (WorkerSourceTask) workerTask);        }        workerMetricsGroup.recordTaskSuccess();        return true;    }}
public booleanf10242
1
buildWorkerTask
private WorkerTaskf10243_1ClusterConfigState configState, ConnectorConfig connConfig, ConnectorTaskId id, Task task, TaskStatus.Listener statusListener, TargetState initialState, Converter keyConverter, Converter valueConverter, HeaderConverter headerConverter, ClassLoader loader)
{    ErrorHandlingMetrics errorHandlingMetrics = errorHandlingMetrics(id);    final Class<? extends Connector> connectorClass = plugins.connectorClass(connConfig.getString(ConnectorConfig.CONNECTOR_CLASS_CONFIG));    RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(connConfig.errorRetryTimeout(), connConfig.errorMaxDelayInMillis(), connConfig.errorToleranceType(), Time.SYSTEM);    retryWithToleranceOperator.metrics(errorHandlingMetrics);    // Decide which type of worker task we need based on the type of task.    if (task instanceof SourceTask) {        retryWithToleranceOperator.reporters(sourceTaskReporters(id, connConfig, errorHandlingMetrics));        TransformationChain<SourceRecord> transformationChain = new TransformationChain<>(connConfig.<SourceRecord>transformations(), retryWithToleranceOperator);                OffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetBackingStore, id.connector(), internalKeyConverter, internalValueConverter);        OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetBackingStore, id.connector(), internalKeyConverter, internalValueConverter);        Map<String, Object> producerProps = producerConfigs(id, "connector-producer-" + id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);        KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);        // Note we pass the configState as it performs dynamic transformations under the covers        return new WorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter, headerConverter, transformationChain, producer, offsetReader, offsetWriter, config, configState, metrics, loader, time, retryWithToleranceOperator);    } else if (task instanceof SinkTask) {        TransformationChain<SinkRecord> transformationChain = new TransformationChain<>(connConfig.<SinkRecord>transformations(), retryWithToleranceOperator);                SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());        retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));        Map<String, Object> consumerProps = consumerConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);        KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);        return new WorkerSinkTask(id, (SinkTask) task, statusListener, initialState, config, configState, metrics, keyConverter, valueConverter, headerConverter, transformationChain, consumer, loader, time, retryWithToleranceOperator);    } else {                throw new ConnectException("Tasks must be a subclass of either SourceTask or SinkTask");    }}
private WorkerTaskf10243
1
stopTask
private voidf10251_1ConnectorTaskId taskId)
{    try (LoggingContext loggingContext = LoggingContext.forTask(taskId)) {        WorkerTask task = tasks.get(taskId);        if (task == null) {                        return;        }                if (task instanceof WorkerSourceTask)            sourceTaskOffsetCommitter.remove(task.id());        ClassLoader savedLoader = plugins.currentThreadLoader();        try {            savedLoader = Plugins.compareAndSwapLoaders(task.loader());            task.stop();        } finally {            Plugins.compareAndSwapLoaders(savedLoader);        }    }}
private voidf10251
1
stopTasks
private void kafkatest_f10252_0(Collection<ConnectorTaskId> ids)
{    // stop the tasks that have not explicitly been stopped.    for (ConnectorTaskId taskId : ids) {        stopTask(taskId);    }}
f10252
0
awaitStopTask
private voidf10253_1ConnectorTaskId taskId, long timeout)
{    try (LoggingContext loggingContext = LoggingContext.forTask(taskId)) {        WorkerTask task = tasks.remove(taskId);        if (task == null) {                        return;        }        if (!task.awaitStop(timeout)) {                        task.cancel();        } else {                    }    }}
private voidf10253
1
getPlugins
public Plugins kafkatest_f10261_0()
{    return plugins;}
f10261
0
workerId
public String kafkatest_f10262_0()
{    return workerId;}
f10262
0
metrics
public ConnectMetrics kafkatest_f10263_0()
{    return metrics;}
f10263
0
recordConnectorStartupSuccess
 void kafkatest_f10271_0()
{    connectorStartupAttempts.record(1.0);    connectorStartupSuccesses.record(1.0);    connectorStartupResults.record(1.0);}
f10271
0
recordTaskFailure
 void kafkatest_f10272_0()
{    taskStartupAttempts.record(1.0);    taskStartupFailures.record(1.0);    taskStartupResults.record(0.0);}
f10272
0
recordTaskSuccess
 void kafkatest_f10273_0()
{    taskStartupAttempts.record(1.0);    taskStartupSuccesses.record(1.0);    taskStartupResults.record(1.0);}
f10273
0
transform
public Map<String, String> kafkatest_f10281_0(Map<String, String> configs)
{    return transform(null, configs);}
f10281
0
transform
public Map<String, String> kafkatest_f10282_0(String connectorName, Map<String, String> configs)
{    if (configs == null)        return null;    ConfigTransformerResult result = configTransformer.transform(configs);    if (connectorName != null) {        String key = ConnectorConfig.CONFIG_RELOAD_ACTION_CONFIG;        String action = (String) ConfigDef.parseType(key, configs.get(key), ConfigDef.Type.STRING);        if (action == null) {            // The default action is "restart".            action = ConnectorConfig.CONFIG_RELOAD_ACTION_RESTART;        }        ConfigReloadAction reloadAction = ConfigReloadAction.valueOf(action.toUpperCase(Locale.ROOT));        if (reloadAction == ConfigReloadAction.RESTART) {            scheduleReload(connectorName, result.ttls());        }    }    return result.data();}
f10282
0
scheduleReload
private void kafkatest_f10283_0(String connectorName, Map<String, Long> ttls)
{    for (Map.Entry<String, Long> entry : ttls.entrySet()) {        scheduleReload(connectorName, entry.getKey(), entry.getValue());    }}
f10283
0
resume
private void kafkatest_f10291_0()
{    if (doStart())        statusListener.onResume(connName);}
f10291
0
start
private void kafkatest_f10292_0()
{    if (doStart())        statusListener.onStartup(connName);}
f10292
0
isRunning
public boolean kafkatest_f10293_0()
{    return state == State.STARTED;}
f10293
0
metrics
 ConnectorMetricsGroup kafkatest_f10301_0()
{    return metrics;}
f10301
0
toString
public String kafkatest_f10302_0()
{    return "WorkerConnector{" + "id=" + connName + '}';}
f10302
0
metricValue
public String kafkatest_f10303_0(long now)
{    return state.toString().toLowerCase(Locale.getDefault());}
f10303
0
isUnassigned
 boolean kafkatest_f10311_0()
{    return state == AbstractStatus.State.UNASSIGNED;}
f10311
0
isRunning
 boolean kafkatest_f10312_0()
{    return state == AbstractStatus.State.RUNNING;}
f10312
0
isPaused
 boolean kafkatest_f10313_0()
{    return state == AbstractStatus.State.PAUSED;}
f10313
0
stop
public void kafkatest_f10321_0()
{    // Offset commit is handled upon exit in work thread    super.stop();    consumer.wakeup();}
f10321
0
close
protected voidf10322_1)
{    // passed in    try {        task.stop();    } catch (Throwable t) {            }    if (consumer != null) {        try {            consumer.close();        } catch (Throwable t) {                    }    }    try {        transformationChain.close();    } catch (Throwable t) {            }}
protected voidf10322
1
releaseResources
protected void kafkatest_f10323_0()
{    sinkTaskMetricsGroup.close();}
f10323
0
isCommitting
 boolean kafkatest_f10331_0()
{    return committing;}
f10331
0
doCommitSync
private voidf10332_1Map<TopicPartition, OffsetAndMetadata> offsets, int seqno)
{        try {        consumer.commitSync(offsets);        onCommitCompleted(null, seqno, offsets);    } catch (WakeupException e) {        // retry the commit to ensure offsets get pushed, then propagate the wakeup up to poll        doCommitSync(offsets, seqno);        throw e;    } catch (KafkaException e) {        onCommitCompleted(e, seqno, offsets);    }}
private voidf10332
1
doCommitAsync
private voidf10333_1Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)
{        OffsetCommitCallback cb = new OffsetCommitCallback() {        @Override        public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception error) {            onCommitCompleted(error, seqno, offsets);        }    };    consumer.commitAsync(offsets, cb);}
private voidf10333
1
convertHeadersFor
private Headers kafkatest_f10341_0(ConsumerRecord<byte[], byte[]> record)
{    Headers result = new ConnectHeaders();    org.apache.kafka.common.header.Headers recordHeaders = record.headers();    if (recordHeaders != null) {        String topic = record.topic();        for (org.apache.kafka.common.header.Header recordHeader : recordHeaders) {            SchemaAndValue schemaAndValue = headerConverter.toConnectHeader(topic, recordHeader.key(), recordHeader.value());            result.add(recordHeader.key(), schemaAndValue);        }    }    return result;}
f10341
0
resumeAll
private void kafkatest_f10342_0()
{    for (TopicPartition tp : consumer.assignment()) if (!context.pausedPartitions().contains(tp))        consumer.resume(singleton(tp));}
f10342
0
pauseAll
private void kafkatest_f10343_0()
{    consumer.pause(consumer.assignment());}
f10343
0
sinkTaskMetricsGroup
 SinkTaskMetricsGroup kafkatest_f10351_0()
{    return sinkTaskMetricsGroup;}
f10351
0
getNextCommit
 long kafkatest_f10352_0()
{    return nextCommit;}
f10352
0
onPartitionsAssigned
public voidf10353_1Collection<TopicPartition> partitions)
{        lastCommittedOffsets = new HashMap<>();    currentOffsets = new HashMap<>();    for (TopicPartition tp : partitions) {        long pos = consumer.position(tp);        lastCommittedOffsets.put(tp, new OffsetAndMetadata(pos));        currentOffsets.put(tp, new OffsetAndMetadata(pos));            }    sinkTaskMetricsGroup.assignedOffsets(currentOffsets);    // If we paused everything for redelivery (which is no longer relevant since we discarded the data), make    // sure anything we paused that the task didn't request to be paused *and* which we still own is resumed.    // Also make sure our tracking of paused partitions is updated to remove any partitions we no longer own.    pausedForRedelivery = false;    // Ensure that the paused partitions contains only assigned partitions and repause as necessary    context.pausedPartitions().retainAll(partitions);    if (shouldPause())        pauseAll();    else if (!context.pausedPartitions().isEmpty())        consumer.pause(context.pausedPartitions());    // need to guard against invoking the user's callback method during that period.    if (rebalanceException == null || rebalanceException instanceof WakeupException) {        try {            openPartitions(partitions);            // Rewind should be applied only if openPartitions succeeds.            rewind();        } catch (RuntimeException e) {            // The consumer swallows exceptions raised in the rebalance listener, so we need to store            // exceptions and rethrow when poll() returns.            rebalanceException = e;        }    }}
public voidf10353
1
recordOffsetSequenceNumber
 void kafkatest_f10361_0(int seqNum)
{    offsetSeqNum.record(seqNum);}
f10361
0
recordConsumedOffsets
 void kafkatest_f10362_0(Map<TopicPartition, OffsetAndMetadata> offsets)
{    consumedOffsets.putAll(offsets);    computeSinkRecordLag();}
f10362
0
recordCommittedOffsets
 void kafkatest_f10363_0(Map<TopicPartition, OffsetAndMetadata> offsets)
{    committedOffsets = offsets;    computeSinkRecordLag();}
f10363
0
offset
public voidf10371_1TopicPartition tp, long offset)
{        offsets.put(tp, offset);}
public voidf10371
1
clearOffsets
public void kafkatest_f10372_0()
{    offsets.clear();}
f10372
0
offsets
public Map<TopicPartition, Long> kafkatest_f10373_0()
{    return offsets;}
f10373
0
isCommitRequested
public boolean kafkatest_f10381_0()
{    return commitRequested;}
f10381
0
clearCommitRequest
public void kafkatest_f10382_0()
{    commitRequested = false;}
f10382
0
toString
public String kafkatest_f10383_0()
{    return "WorkerSinkTaskContext{" + "id=" + sinkTask.id + '}';}
f10383
0
poll
protected List<SourceRecord>f10391_1) throws InterruptedException
{    try {        return task.poll();    } catch (RetriableException | org.apache.kafka.common.errors.RetriableException e) {                // Do nothing. Let the framework poll whenever it's ready.        return null;    }}
protected List<SourceRecord>f10391
1
convertTransformedRecord
private ProducerRecord<byte[], byte[]> kafkatest_f10392_0(SourceRecord record)
{    if (record == null) {        return null;    }    RecordHeaders headers = retryWithToleranceOperator.execute(() -> convertHeaderFor(record), Stage.HEADER_CONVERTER, headerConverter.getClass());    byte[] key = retryWithToleranceOperator.execute(() -> keyConverter.fromConnectData(record.topic(), record.keySchema(), record.key()), Stage.KEY_CONVERTER, keyConverter.getClass());    byte[] value = retryWithToleranceOperator.execute(() -> valueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value()), Stage.VALUE_CONVERTER, valueConverter.getClass());    if (retryWithToleranceOperator.failed()) {        return null;    }    return new ProducerRecord<>(record.topic(), record.kafkaPartition(), ConnectUtils.checkAndConvertTimestamp(record.timestamp()), key, value, headers);}
f10392
0
sendRecords
private booleanf10393_1)
{    int processed = 0;    recordBatch(toSend.size());    final SourceRecordWriteCounter counter = new SourceRecordWriteCounter(toSend.size(), sourceTaskMetricsGroup);    for (final SourceRecord preTransformRecord : toSend) {        maybeThrowProducerSendException();        retryWithToleranceOperator.sourceRecord(preTransformRecord);        final SourceRecord record = transformationChain.apply(preTransformRecord);        final ProducerRecord<byte[], byte[]> producerRecord = convertTransformedRecord(record);        if (producerRecord == null || retryWithToleranceOperator.failed()) {            counter.skipRecord();            commitTaskRecord(preTransformRecord);            continue;        }        log.trace("{} Appending record with key {}, value {}", this, record.key(), record.value());        // messages and update the offsets.        synchronized (this) {            if (!lastSendFailed) {                if (!flushing) {                    outstandingMessages.put(producerRecord, producerRecord);                } else {                    outstandingMessagesBacklog.put(producerRecord, producerRecord);                }                // Offsets are converted & serialized in the OffsetWriter                offsetWriter.offset(record.sourcePartition(), record.sourceOffset());            }        }        try {            final String topic = producerRecord.topic();            producer.send(producerRecord, new Callback() {                @Override                public void onCompletion(RecordMetadata recordMetadata, Exception e) {                    if (e != null) {                                                                        producerSendException.compareAndSet(null, e);                    } else {                        recordSent(producerRecord);                        counter.completeRecord();                        log.trace("{} Wrote record successfully: topic {} partition {} offset {}", WorkerSourceTask.this, recordMetadata.topic(), recordMetadata.partition(), recordMetadata.offset());                        commitTaskRecord(preTransformRecord);                    }                }            });            lastSendFailed = false;        } catch (org.apache.kafka.common.errors.RetriableException e) {                        toSend = toSend.subList(processed, toSend.size());            lastSendFailed = true;            counter.retryRemaining();            return false;        } catch (KafkaException e) {            throw new ConnectException("Unrecoverable exception trying to send", e);        }        processed++;    }    toSend = null;    return true;}
private booleanf10393
1
finishFailedFlush
private synchronized void kafkatest_f10401_0()
{    offsetWriter.cancelFlush();    outstandingMessages.putAll(outstandingMessagesBacklog);    outstandingMessagesBacklog.clear();    flushing = false;}
f10401
0
finishSuccessfulFlush
private synchronized void kafkatest_f10402_0()
{    // If we were successful, we can just swap instead of replacing items back into the original map    IdentityHashMap<ProducerRecord<byte[], byte[]>, ProducerRecord<byte[], byte[]>> temp = outstandingMessages;    outstandingMessages = outstandingMessagesBacklog;    outstandingMessagesBacklog = temp;    flushing = false;}
f10402
0
toString
public String kafkatest_f10403_0()
{    return "WorkerSourceTask{" + "id=" + id + '}';}
f10403
0
recordPoll
 void kafkatest_f10411_0(int batchSize, long duration)
{    sourceRecordPoll.record(batchSize);    pollTime.record(duration);    activeRecordCount += batchSize;    sourceRecordActiveCount.record(activeRecordCount);}
f10411
0
recordWrite
 void kafkatest_f10412_0(int recordCount)
{    sourceRecordWrite.record(recordCount);    activeRecordCount -= recordCount;    activeRecordCount = Math.max(0, activeRecordCount);    sourceRecordActiveCount.record(activeRecordCount);}
f10412
0
metricGroup
protected MetricGroup kafkatest_f10413_0()
{    return metricGroup;}
f10413
0
awaitStop
public boolean kafkatest_f10421_0(long timeoutMs)
{    try {        return shutdownLatch.await(timeoutMs, TimeUnit.MILLISECONDS);    } catch (InterruptedException e) {        return false;    }}
f10421
0
isStopping
protected boolean kafkatest_f10422_0()
{    return stopping;}
f10422
0
doClose
private voidf10423_1)
{    try {        close();    } catch (Throwable t) {                throw t;    }}
private voidf10423
1
awaitUnpause
protected boolean kafkatest_f10431_0() throws InterruptedException
{    synchronized (this) {        while (targetState == TargetState.PAUSED) {            if (stopping)                return false;            this.wait();        }        return true;    }}
f10431
0
transitionTo
public void kafkatest_f10432_0(TargetState state)
{    synchronized (this) {        // ignore the state change if we are stopping        if (stopping)            return;        this.targetState = state;        this.notifyAll();    }}
f10432
0
recordCommitSuccess
protected void kafkatest_f10433_0(long duration)
{    taskMetricsGroup.recordCommit(duration, true, null);}
f10433
0
recordCommit
 void kafkatest_f10441_0(long duration, boolean success, Throwable error)
{    if (success) {        commitTime.record(duration);        commitAttempts.record(1.0d);    } else {        commitAttempts.record(0.0d);    }}
f10441
0
recordBatch
 void kafkatest_f10442_0(int size)
{    batchSize.record(size);}
f10442
0
onStartup
public void kafkatest_f10443_0(ConnectorTaskId id)
{    taskStateTimer.changeState(State.RUNNING, time.milliseconds());    delegateListener.onStartup(id);}
f10443
0
configure
public void kafkatest_f10451_0(WorkerConfig config)
{    super.configure(config);    file = new File(config.getString(StandaloneConfig.OFFSET_STORAGE_FILE_FILENAME_CONFIG));}
f10451
0
start
public synchronized voidf10452_1)
{    super.start();        load();}
public synchronized voidf10452
1
stop
public synchronized voidf10453_1)
{    super.stop();    // Nothing to do since this doesn't maintain any outstanding connections/data    }
public synchronized voidf10453
1
start
public voidf10461_1)
{        // Before startup, callbacks are *not* invoked. You can grab a snapshot after starting -- just take care that    // updates can continue to occur in the background    configLog.start();    started = true;    }
public voidf10461
1
stop
public voidf10462_1)
{        configLog.stop();    }
public voidf10462
1
snapshot
public ClusterConfigState kafkatest_f10463_0()
{    synchronized (lock) {        // immutable configs        return new ClusterConfigState(offset, new HashMap<>(connectorTaskCounts), new HashMap<>(connectorConfigs), new HashMap<>(connectorTargetStates), new HashMap<>(taskConfigs), new HashSet<>(inconsistent), configTransformer);    }}
f10463
0
putTargetState
public voidf10471_1String connector, TargetState state)
{    Struct connectTargetState = new Struct(TARGET_STATE_V0);    connectTargetState.put("state", state.name());    byte[] serializedTargetState = converter.fromConnectData(topic, TARGET_STATE_V0, connectTargetState);        configLog.send(TARGET_STATE_KEY(connector), serializedTargetState);}
public voidf10471
1
setupAndCreateKafkaBasedLog
 KafkaBasedLog<String, byte[]> kafkatest_f10472_0(String topic, final WorkerConfig config)
{    Map<String, Object> originals = config.originals();    Map<String, Object> producerProps = new HashMap<>(originals);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);    Map<String, Object> consumerProps = new HashMap<>(originals);    consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());    Map<String, Object> adminProps = new HashMap<>(originals);    NewTopic topicDescription = TopicAdmin.defineTopic(topic).compacted().partitions(1).replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).build();    return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);}
f10472
0
createKafkaBasedLog
private KafkaBasedLog<String, byte[]>f10473_1String topic, Map<String, Object> producerProps, Map<String, Object> consumerProps, Callback<ConsumerRecord<String, byte[]>> consumedCallback, final NewTopic topicDescription, final Map<String, Object> adminProps)
{    Runnable createTopics = new Runnable() {        @Override        public void run() {                        try (TopicAdmin admin = new TopicAdmin(adminProps)) {                admin.createTopics(topicDescription);            }        }    };    return new KafkaBasedLog<>(topic, producerProps, consumerProps, consumedCallback, Time.SYSTEM, createTopics);}
private KafkaBasedLog<String, byte[]>f10473
1
createKafkaBasedLog
private KafkaBasedLog<byte[], byte[]>f10481_1String topic, Map<String, Object> producerProps, Map<String, Object> consumerProps, Callback<ConsumerRecord<byte[], byte[]>> consumedCallback, final NewTopic topicDescription, final Map<String, Object> adminProps)
{    Runnable createTopics = new Runnable() {        @Override        public void run() {                        try (TopicAdmin admin = new TopicAdmin(adminProps)) {                admin.createTopics(topicDescription);            }        }    };    return new KafkaBasedLog<>(topic, producerProps, consumerProps, consumedCallback, Time.SYSTEM, createTopics);}
private KafkaBasedLog<byte[], byte[]>f10481
1
run
public voidf10482_1)
{        try (TopicAdmin admin = new TopicAdmin(adminProps)) {        admin.createTopics(topicDescription);    }}
public voidf10482
1
start
public voidf10483_1)
{        offsetLog.start();    }
public voidf10483
1
isCancelled
public synchronized boolean kafkatest_f10491_0()
{    return false;}
f10491
0
isDone
public synchronized boolean kafkatest_f10492_0()
{    return completed;}
f10492
0
get
public synchronized Void kafkatest_f10493_0() throws InterruptedException, ExecutionException
{    while (!completed) {        this.wait();    }    if (exception != null)        throw new ExecutionException(exception);    return null;}
f10493
0
put
public void kafkatest_f10501_0(final ConnectorStatus status)
{    sendConnectorStatus(status, false);}
f10501
0
putSafe
public void kafkatest_f10502_0(final ConnectorStatus status)
{    sendConnectorStatus(status, true);}
f10502
0
put
public void kafkatest_f10503_0(final TaskStatus status)
{    sendTaskStatus(status, false);}
f10503
0
remove
private synchronized void kafkatest_f10511_0(String connector)
{    CacheEntry<ConnectorStatus> removed = connectors.remove(connector);    if (removed != null)        removed.delete();    Map<Integer, CacheEntry<TaskStatus>> tasks = this.tasks.remove(connector);    if (tasks != null) {        for (CacheEntry<TaskStatus> taskEntry : tasks.values()) taskEntry.delete();    }}
f10511
0
getOrAdd
private synchronized CacheEntry<TaskStatus> kafkatest_f10512_0(ConnectorTaskId task)
{    CacheEntry<TaskStatus> entry = tasks.get(task.connector(), task.task());    if (entry == null) {        entry = new CacheEntry<>();        tasks.put(task.connector(), task.task(), entry);    }    return entry;}
f10512
0
remove
private synchronized void kafkatest_f10513_0(ConnectorTaskId id)
{    CacheEntry<TaskStatus> removed = tasks.remove(id.connector(), id.task());    if (removed != null)        removed.delete();}
f10513
0
parseConnectorStatusKey
private String kafkatest_f10521_0(String key)
{    return key.substring(CONNECTOR_STATUS_PREFIX.length());}
f10521
0
parseConnectorTaskId
private ConnectorTaskIdf10522_1String key)
{    String[] parts = key.split("-");    if (parts.length < 4)        return null;    try {        int taskNum = Integer.parseInt(parts[parts.length - 1]);        String connectorName = Utils.join(Arrays.copyOfRange(parts, 2, parts.length - 1), "-");        return new ConnectorTaskId(connectorName, taskNum);    } catch (NumberFormatException e) {                return null;    }}
private ConnectorTaskIdf10522
1
readConnectorStatus
private voidf10523_1String key, byte[] value)
{    String connector = parseConnectorStatusKey(key);    if (connector == null || connector.isEmpty()) {                return;    }    if (value == null) {        log.trace("Removing status for connector {}", connector);        remove(connector);        return;    }    ConnectorStatus status = parseConnectorStatus(connector, value);    if (status == null)        return;    synchronized (this) {        log.trace("Received connector {} status update {}", connector, status);        CacheEntry<ConnectorStatus> entry = getOrAdd(connector);        entry.put(status);    }}
private voidf10523
1
canWriteSafely
public boolean kafkatest_f10531_0(T status)
{    return value == null || value.workerId().equals(status.workerId()) || value.generation() <= status.generation();}
f10531
0
canWriteSafely
public boolean kafkatest_f10532_0(T status, int sequence)
{    return canWriteSafely(status) && this.sequence == sequence;}
f10532
0
snapshot
public synchronized ClusterConfigState kafkatest_f10535_0()
{    Map<String, Integer> connectorTaskCounts = new HashMap<>();    Map<String, Map<String, String>> connectorConfigs = new HashMap<>();    Map<String, TargetState> connectorTargetStates = new HashMap<>();    Map<ConnectorTaskId, Map<String, String>> taskConfigs = new HashMap<>();    for (Map.Entry<String, ConnectorState> connectorStateEntry : connectors.entrySet()) {        String connector = connectorStateEntry.getKey();        ConnectorState connectorState = connectorStateEntry.getValue();        connectorTaskCounts.put(connector, connectorState.taskConfigs.size());        connectorConfigs.put(connector, connectorState.connConfig);        connectorTargetStates.put(connector, connectorState.targetState);        taskConfigs.putAll(connectorState.taskConfigs);    }    return new ClusterConfigState(ClusterConfigState.NO_OFFSET, connectorTaskCounts, connectorConfigs, connectorTargetStates, taskConfigs, Collections.<String>emptySet(), configTransformer);}
f10535
0
taskConfigListAsMap
private static Map<ConnectorTaskId, Map<String, String>> kafkatest_f10544_0(String connector, List<Map<String, String>> configs)
{    int index = 0;    Map<ConnectorTaskId, Map<String, String>> result = new TreeMap<>();    for (Map<String, String> taskConfigMap : configs) {        result.put(new ConnectorTaskId(connector, index++), taskConfigMap);    }    return result;}
f10544
0
start
public void kafkatest_f10546_0()
{    executor = Executors.newSingleThreadExecutor();}
f10546
0
stop
public void kafkatest_f10547_0()
{    if (executor != null) {        executor.shutdown();        // Best effort wait for any get() and set() tasks (and caller's callbacks) to complete.        try {            executor.awaitTermination(30, TimeUnit.SECONDS);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();        }        if (!executor.shutdownNow().isEmpty()) {            throw new ConnectException("Failed to stop MemoryOffsetBackingStore. Exiting without cleanly " + "shutting down pending tasks and/or callbacks.");        }        executor = null;    }}
f10547
0
putSafe
public synchronized void kafkatest_f10559_0(TaskStatus status)
{    put(status);}
f10559
0
get
public synchronized TaskStatus kafkatest_f10560_0(ConnectorTaskId id)
{    return tasks.get(id.connector(), id.task());}
f10560
0
get
public synchronized ConnectorStatus kafkatest_f10561_0(String connector)
{    return connectors.get(connector);}
f10561
0
doFlush
public Future<Void>f10570_1final Callback<Void> callback)
{    final long flushId;    // Serialize    final Map<ByteBuffer, ByteBuffer> offsetsSerialized;    synchronized (this) {        flushId = currentFlushId;        try {            offsetsSerialized = new HashMap<>(toFlush.size());            for (Map.Entry<Map<String, Object>, Map<String, Object>> entry : toFlush.entrySet()) {                // Offsets are specified as schemaless to the converter, using whatever internal schema is appropriate                // for that data. The only enforcement of the format is here.                OffsetUtils.validateFormat(entry.getKey());                OffsetUtils.validateFormat(entry.getValue());                // When serializing the key, we add in the namespace information so the key is [namespace, real key]                byte[] key = keyConverter.fromConnectData(namespace, null, Arrays.asList(namespace, entry.getKey()));                ByteBuffer keyBuffer = (key != null) ? ByteBuffer.wrap(key) : null;                byte[] value = valueConverter.fromConnectData(namespace, null, entry.getValue());                ByteBuffer valueBuffer = (value != null) ? ByteBuffer.wrap(value) : null;                offsetsSerialized.put(keyBuffer, valueBuffer);            }        } catch (Throwable t) {            // Must handle errors properly here or the writer will be left mid-flush forever and be            // unable to make progress.                                    callback.onCompletion(t, null);            return null;        }        // And submit the data            }    return backingStore.set(offsetsSerialized, new Callback<Void>() {        @Override        public void onCompletion(Throwable error, Void result) {            boolean isCurrent = handleFinishWrite(flushId, error, result);            if (isCurrent && callback != null) {                callback.onCompletion(error, result);            }        }    });}
public Future<Void>f10570
1
onCompletion
public void kafkatest_f10571_0(Throwable error, Void result)
{    boolean isCurrent = handleFinishWrite(flushId, error, result);    if (isCurrent && callback != null) {        callback.onCompletion(error, result);    }}
f10571
0
cancelFlush
public synchronized void kafkatest_f10572_0()
{    // call stack and callbacks from the write request to underlying storage    if (flushing()) {        // Just recombine the data and place it back in the primary storage        toFlush.putAll(data);        data = toFlush;        currentFlushId++;        toFlush = null;    }}
f10572
0
taskConfigs
public List<Map<String, String>>f10580_1int maxTasks)
{        return Collections.singletonList(config);}
public List<Map<String, String>>f10580
1
stop
public void kafkatest_f10581_0()
{    if (executor != null) {        executor.shutdownNow();        try {            if (!executor.awaitTermination(20, TimeUnit.SECONDS))                throw new RuntimeException("Failed timely termination of scheduler");        } catch (InterruptedException e) {            throw new RuntimeException("Task was interrupted during shutdown");        }    }}
f10581
0
config
public ConfigDef kafkatest_f10582_0()
{    return new ConfigDef();}
f10582
0
taskConfigs
public List<Map<String, String>> kafkatest_f10590_0(int maxTasks)
{    return delegate.taskConfigs(maxTasks);}
f10590
0
stop
public void kafkatest_f10591_0()
{    delegate.stop();}
f10591
0
config
public ConfigDef kafkatest_f10592_0()
{    return delegate.config();}
f10592
0
validate
public Config kafkatest_f10602_0(Map<String, String> connectorConfigs)
{    return delegate.validate(connectorConfigs);}
f10602
0
version
public String kafkatest_f10603_0()
{    return delegate.version();}
f10603
0
start
public void kafkatest_f10604_0(Map<String, String> props)
{    delegate.start(props);}
f10604
0
version
public String kafkatest_f10613_0()
{    return AppInfoParser.getVersion();}
f10613
0
start
public void kafkatest_f10614_0(Map<String, String> props)
{    this.config = props;}
f10614
0
taskClass
public Class<? extends Task> kafkatest_f10615_0()
{    return SchemaSourceTask.class;}
f10615
0
printHtml
private static void kafkatest_f10624_0(PrintStream out) throws NoSuchFieldException, IllegalAccessException, InstantiationException
{    for (final DocInfo docInfo : TRANSFORMATIONS) {        printTransformationHtml(out, docInfo);    }}
f10624
0
main
public static void kafkatest_f10625_0(String... args) throws Exception
{    printHtml(System.out);}
f10625
0
version
public String kafkatest_f10626_0()
{    return AppInfoParser.getVersion();}
f10626
0
flush
public void kafkatest_f10635_0(Map<TopicPartition, OffsetAndMetadata> offsets)
{    long nowMs = System.currentTimeMillis();    for (Map<String, Object> data : unflushed) {        data.put("time_ms", nowMs);        data.put("flushed", true);        String dataJson;        try {            dataJson = JSON_SERDE.writeValueAsString(data);        } catch (JsonProcessingException e) {            dataJson = "Bad data can't be written as json: " + e.getMessage();        }        System.out.println(dataJson);    }    unflushed.clear();}
f10635
0
version
public String kafkatest_f10637_0()
{    return AppInfoParser.getVersion();}
f10637
0
start
public void kafkatest_f10638_0(Map<String, String> props)
{    this.config = props;}
f10638
0
stop
public void kafkatest_f10647_0()
{    throttler.wakeup();}
f10647
0
connector
public String kafkatest_f10648_0()
{    return connector;}
f10648
0
task
public int kafkatest_f10649_0()
{    return task;}
f10649
0
onCompletion
public void kafkatest_f10657_0(Throwable error, U result)
{    this.exception = error;    this.result = convert(result);    if (underlying != null)        underlying.onCompletion(error, this.result);    finishedLatch.countDown();}
f10657
0
cancel
public boolean kafkatest_f10658_0(boolean b)
{    return false;}
f10658
0
isCancelled
public boolean kafkatest_f10659_0()
{    return false;}
f10659
0
readToEnd
public void kafkatest_f10668_0(Callback<Void> callback)
{    log.trace("Starting read to end log for topic {}", topic);    producer.flush();    synchronized (this) {        readLogEndOffsetCallbacks.add(callback);    }    consumer.wakeup();}
f10668
0
flush
public void kafkatest_f10669_0()
{    producer.flush();}
f10669
0
readToEnd
public Future<Void> kafkatest_f10670_0()
{    FutureCallback<Void> future = new FutureCallback<>(null);    readToEnd(future);    return future;}
f10670
0
toString
public String kafkatest_f10678_0()
{    return text;}
f10678
0
clear
public static void kafkatest_f10679_0()
{    MDC.clear();}
f10679
0
forConnector
public static LoggingContext kafkatest_f10680_0(String connectorName)
{    Objects.requireNonNull(connectorName);    LoggingContext context = new LoggingContext();    MDC.put(CONNECTOR_CONTEXT, prefixFor(connectorName, Scope.WORKER, null));    return context;}
f10680
0
createDir
public Dir kafkatest_f10688_0(final URL url) throws Exception
{    return emptyVfsDir(url);}
f10688
0
emptyVfsDir
private static Dir kafkatest_f10689_0(final URL url)
{    return new Dir() {        @Override        public String getPath() {            return url.toExternalForm();        }        @Override        public Iterable<File> getFiles() {            return Collections.emptyList();        }        @Override        public void close() {        }    };}
f10689
0
getPath
public String kafkatest_f10690_0()
{    return url.toExternalForm();}
f10690
0
startGracefulShutdown
public voidf10699_1)
{        isRunning.set(false);}
public voidf10699
1
awaitShutdown
public boolean kafkatest_f10700_0(long timeout, TimeUnit unit) throws InterruptedException
{    return shutdownLatch.await(timeout, unit);}
f10700
0
forceShutdown
public voidf10701_1) throws InterruptedException
{        isRunning.set(false);    interrupt();}
public voidf10701
1
replicationFactor
public NewTopicBuilder kafkatest_f10709_0(short replicationFactor)
{    this.replicationFactor = replicationFactor;    return this;}
f10709
0
compacted
public NewTopicBuilder kafkatest_f10710_0()
{    this.configs.put(CLEANUP_POLICY_CONFIG, CLEANUP_POLICY_COMPACT);    return this;}
f10710
0
minInSyncReplicas
public NewTopicBuilder kafkatest_f10711_0(short minInSyncReplicas)
{    this.configs.put(MIN_INSYNC_REPLICAS_CONFIG, Short.toString(minInSyncReplicas));    return this;}
f10711
0
bootstrapServers
private String kafkatest_f10719_0()
{    Object servers = adminConfig.get(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG);    return servers != null ? servers.toString() : "<unknown>";}
f10719
0
testValidOverride
protected void kafkatest_f10720_0(Map<String, Object> clientConfig)
{    List<ConfigValue> configValues = configValues(clientConfig);    assertNoError(configValues);}
f10720
0
testInvalidOverride
protected void kafkatest_f10721_0(Map<String, Object> clientConfig)
{    List<ConfigValue> configValues = configValues(clientConfig);    assertError(configValues);}
f10721
0
testPrincipalPlusOtherConfigs
public void kafkatest_f10729_0()
{    Map<String, Object> clientConfig = new HashMap<>();    clientConfig.put(SaslConfigs.SASL_JAAS_CONFIG, "test");    clientConfig.put(ProducerConfig.ACKS_CONFIG, "none");    testInvalidOverride(clientConfig);}
f10729
0
policyToTest
protected ConnectorClientConfigOverridePolicy kafkatest_f10730_0()
{    return principalConnectorClientConfigOverridePolicy;}
f10730
0
setUp
public void kafkatest_f10731_0()
{    converter.configure(Collections.<String, String>emptyMap(), false);}
f10731
0
samples
public Double[] kafkatest_f10739_0()
{    return new Double[] { Double.MIN_VALUE, 1234.31, Double.MAX_VALUE };}
f10739
0
schema
protected Schema kafkatest_f10740_0()
{    return Schema.OPTIONAL_FLOAT64_SCHEMA;}
f10740
0
createConverter
protected NumberConverter<Double> kafkatest_f10741_0()
{    return new DoubleConverter();}
f10741
0
createConverter
protected NumberConverter<Integer> kafkatest_f10749_0()
{    return new IntegerConverter();}
f10749
0
createSerializer
protected Serializer<Integer> kafkatest_f10750_0()
{    return new IntegerSerializer();}
f10750
0
samples
public Long[] kafkatest_f10751_0()
{    return new Long[] { Long.MIN_VALUE, 1234L, Long.MAX_VALUE };}
f10751
0
testSerializingIncorrectType
public void kafkatest_f10759_0()
{    converter.fromConnectData(TOPIC, schema, "not a valid number");}
f10759
0
testSerializingIncorrectHeader
public void kafkatest_f10760_0()
{    converter.fromConnectHeader(TOPIC, HEADER_NAME, schema, "not a valid number");}
f10760
0
testNullToBytes
public void kafkatest_f10761_0()
{    assertEquals(null, converter.fromConnectData(TOPIC, schema, null));}
f10761
0
testCreateWithAllowedOverridesForPrincipalPolicy
public void kafkatest_f10770_0() throws Exception
{    Map<String, String> props = basicConnectorConfig();    props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "PLAIN");    assertPassCreateConnector("Principal", props);}
f10770
0
testCreateWithAllowedOverridesForAllPolicy
public void kafkatest_f10771_0() throws Exception
{    // setup up props for the sink connector    Map<String, String> props = basicConnectorConfig();    props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + CommonClientConfigs.CLIENT_ID_CONFIG, "test");    assertPassCreateConnector("All", props);}
f10771
0
connectClusterWithPolicy
private EmbeddedConnectCluster kafkatest_f10772_0(String policy) throws IOException
{    // setup Connect worker properties    Map<String, String> workerProps = new HashMap<>();    workerProps.put(OFFSET_COMMIT_INTERVAL_MS_CONFIG, String.valueOf(5_000));    workerProps.put(WorkerConfig.CONNECTOR_CLIENT_POLICY_CLASS_CONFIG, policy);    // setup Kafka broker properties    Properties exampleBrokerProps = new Properties();    exampleBrokerProps.put("auto.create.topics.enable", "false");    // build a Connect cluster backed by Kafka and Zk    EmbeddedConnectCluster connect = new EmbeddedConnectCluster.Builder().name("connect-cluster").numWorkers(NUM_WORKERS).numBrokers(1).workerProps(workerProps).brokerProps(exampleBrokerProps).build();    // start the clusters    connect.start();    return connect;}
f10772
0
expectedRecords
public void kafkatest_f10780_0(int expected)
{    expectedRecords = expected;    recordsRemainingLatch = new CountDownLatch(expected);}
f10780
0
expectedCommits
public void kafkatest_f10781_0(int expected)
{    expectedCommits = expected;    recordsToCommitLatch = new CountDownLatch(expected);}
f10781
0
record
public void kafkatest_f10782_0()
{    if (recordsRemainingLatch != null) {        recordsRemainingLatch.countDown();    }}
f10782
0
expectedStarts
public StartAndStopLatch kafkatest_f10790_0(int expectedStarts)
{    return expectedStarts(expectedStarts, true);}
f10790
0
expectedStarts
public StartAndStopLatch kafkatest_f10791_0(int expectedStarts, boolean includeTasks)
{    List<StartAndStopLatch> taskLatches = null;    if (includeTasks) {        taskLatches = taskHandles.values().stream().map(task -> task.expectedStarts(expectedStarts)).collect(Collectors.toList());    }    return startAndStopCounter.expectedStarts(expectedStarts, taskLatches);}
f10791
0
expectedStops
public StartAndStopLatch kafkatest_f10792_0(int expectedStops)
{    return expectedStops(expectedStops, true);}
f10792
0
setup
public void kafkatest_f10800_0() throws IOException
{    // setup Connect cluster with defaults    connect = new EmbeddedConnectCluster.Builder().build();    // start Connect cluster    connect.start();    // get connector handles before starting test.    connectorHandle = RuntimeHandles.get().connectorHandle(CONNECTOR_NAME);}
f10800
0
close
public void kafkatest_f10801_0()
{    RuntimeHandles.get().deleteConnector(CONNECTOR_NAME);    connect.stop();}
f10801
0
testSkipRetryAndDLQWithHeaders
public voidf10802_1) throws Exception
{    // create test topic    connect.kafka().createTopic("test-topic");    // setup connector config    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put(TOPICS_CONFIG, "test-topic");    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(TRANSFORMS_CONFIG, "failing_transform");    props.put("transforms.failing_transform.type", FaultyPassthrough.class.getName());    // log all errors, along with message metadata    props.put(ERRORS_LOG_ENABLE_CONFIG, "true");    props.put(ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, "true");    // produce bad messages into dead letter queue    props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);    props.put(DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, "true");    props.put(DLQ_TOPIC_REPLICATION_FACTOR_CONFIG, "1");    // tolerate all erros    props.put(ERRORS_TOLERANCE_CONFIG, "all");    // retry for up to one second    props.put(ERRORS_RETRY_TIMEOUT_CONFIG, "1000");    // set expected records to successfully reach the task    connectorHandle.taskHandle(TASK_ID).expectedRecords(EXPECTED_CORRECT_RECORDS);    connect.configureConnector(CONNECTOR_NAME, props);    waitForCondition(this::checkForPartitionAssignment, CONNECTOR_SETUP_DURATION_MS, "Connector task was not assigned a partition.");    // produce some strings into test topic    for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {        connect.kafka().produce("test-topic", "key-" + i, "value-" + i);    }    // consume all records from test topic        int i = 0;    for (ConsumerRecord<byte[], byte[]> rec : connect.kafka().consume(NUM_RECORDS_PRODUCED, CONSUME_MAX_DURATION_MS, "test-topic")) {        String k = new String(rec.key());        String v = new String(rec.value());                assertEquals("Unexpected key", k, "key-" + i);        assertEquals("Unexpected value", v, "value-" + i);        i++;    }    // wait for records to reach the task    connectorHandle.taskHandle(TASK_ID).awaitRecords(CONSUME_MAX_DURATION_MS);    // consume failed records from dead letter queue topic        ConsumerRecords<byte[], byte[]> messages = connect.kafka().consume(EXPECTED_INCORRECT_RECORDS, CONSUME_MAX_DURATION_MS, DLQ_TOPIC);    for (ConsumerRecord<byte[], byte[]> recs : messages) {                assertTrue(recs.headers().toArray().length > 0);        assertValue("test-topic", recs.headers(), ERROR_HEADER_ORIG_TOPIC);        assertValue(RetriableException.class.getName(), recs.headers(), ERROR_HEADER_EXCEPTION);        assertValue("Error when value='value-7'", recs.headers(), ERROR_HEADER_EXCEPTION_MESSAGE);    }    connect.deleteConnector(CONNECTOR_NAME);}
public voidf10802
1
testSourceConnector
public void kafkatest_f10812_0() throws Exception
{    // create test topic    connect.kafka().createTopic("test-topic", NUM_TOPIC_PARTITIONS);    // setup up props for the sink connector    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put("topic", "test-topic");    props.put("throughput", String.valueOf(500));    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    // expect all records to be produced by the connector    connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);    // expect all records to be produced by the connector    connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);    // start a source connector    connect.configureConnector(CONNECTOR_NAME, props);    // wait for the connector tasks to produce enough records    connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);    // wait for the connector tasks to commit enough records    connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);    // consume all records from the source topic or fail, to ensure that they were correctly produced    int recordNum = connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, "test-topic").count();    assertTrue("Not enough records produced by source connector. Expected at least: " + NUM_RECORDS_PRODUCED + " + but got " + recordNum, recordNum >= NUM_RECORDS_PRODUCED);    // delete connector    connect.deleteConnector(CONNECTOR_NAME);}
f10812
0
checkForPartitionAssignment
private booleanf10813_1)
{    try {        ConnectorStateInfo info = connect.connectorStatus(CONNECTOR_NAME);        return info != null && info.tasks().size() == NUM_TASKS && connectorHandle.tasks().stream().allMatch(th -> th.partitionsAssigned() == 1);    } catch (Exception e) {        // Log the exception and return that the partitions were not assigned                return false;    }}
private booleanf10813
1
start
public voidf10814_1Map<String, String> props)
{    connectorHandle = RuntimeHandles.get().connectorHandle(props.get("name"));    connectorName = props.get("name");    commonConfigs = props;        connectorHandle.recordConnectorStart();}
public voidf10814
1
put
public void kafkatest_f10822_0(Collection<SinkRecord> records)
{    for (SinkRecord rec : records) {        taskHandle.record();        TopicPartition tp = cachedTopicPartitions.computeIfAbsent(rec.topic(), v -> new HashMap<>()).computeIfAbsent(rec.kafkaPartition(), v -> new TopicPartition(rec.topic(), rec.kafkaPartition()));        committedOffsets.put(tp, committedOffsets.getOrDefault(tp, 0L) + 1);        log.trace("Task {} obtained record (key='{}' value='{}')", taskId, rec.key(), rec.value());    }}
f10822
0
preCommit
public Map<TopicPartition, OffsetAndMetadata>f10823_1Map<TopicPartition, OffsetAndMetadata> offsets)
{    for (TopicPartition tp : assignments) {        Long recordsSinceLastCommit = committedOffsets.get(tp);        if (recordsSinceLastCommit == null) {                    } else {            taskHandle.commit(recordsSinceLastCommit.intValue());                        taskHandle.commit((int) (long) recordsSinceLastCommit);            committedOffsets.put(tp, 0L);        }    }    return offsets;}
public Map<TopicPartition, OffsetAndMetadata>f10823
1
stop
public void kafkatest_f10824_0()
{    taskHandle.recordTaskStop();}
f10824
0
poll
public List<SourceRecord> kafkatest_f10832_0()
{    if (!stopped) {        if (throttler.shouldThrottle(seqno - startingSeqno, System.currentTimeMillis())) {            throttler.throttle();        }        taskHandle.record(batchSize);        return LongStream.range(0, batchSize).mapToObj(i -> new SourceRecord(Collections.singletonMap("task.id", taskId), Collections.singletonMap("saved", ++seqno), topicName, null, Schema.STRING_SCHEMA, "key-" + taskId + "-" + seqno, Schema.STRING_SCHEMA, "value-" + taskId + "-" + seqno)).collect(Collectors.toList());    }    return null;}
f10832
0
commit
public voidf10833_1)
{    // TODO: save progress outside the offset topic, potentially in the task handle}
public voidf10833
1
commitRecord
public void kafkatest_f10834_0(SourceRecord record)
{    log.trace("Committing record: {}", record);    taskHandle.commit();}
f10834
0
testRemovingWorker
public void kafkatest_f10842_0() throws Exception
{    // create test topic    connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);    // setup up props for the source connector    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put("throughput", String.valueOf(1));    props.put("messages.per.poll", String.valueOf(10));    props.put(TOPIC_CONFIG, TOPIC_NAME);    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    waitForCondition(() -> this.assertWorkersUp(3), WORKER_SETUP_DURATION_MS, "Connect workers did not start in time.");    // start a source connector    IntStream.range(0, 4).forEachOrdered(i -> {        try {            connect.configureConnector(CONNECTOR_NAME + i, props);        } catch (IOException e) {            throw new ConnectException(e);        }    });    waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not start in time.");    connect.removeWorker();    waitForCondition(() -> this.assertWorkersUp(2), WORKER_SETUP_DURATION_MS, "Connect workers did not start in time.");    waitForCondition(this::assertConnectorAndTasksAreUnique, WORKER_SETUP_DURATION_MS, "Connect and tasks are imbalanced between the workers.");}
f10842
0
assertConnectorAndTasksRunning
private Optional<Boolean>f10843_1String connectorName, int numTasks)
{    try {        ConnectorStateInfo info = connect.connectorStatus(connectorName);        boolean result = info != null && info.tasks().size() == numTasks && info.connector().state().equals(AbstractStatus.State.RUNNING.toString()) && info.tasks().stream().allMatch(s -> s.state().equals(AbstractStatus.State.RUNNING.toString()));                return Optional.of(result);    } catch (Exception e) {                return Optional.empty();    }}
private Optional<Boolean>f10843
1
assertWorkersUp
private booleanf10844_1int numWorkers)
{    try {        int numUp = connect.activeWorkers().size();        return numUp == numWorkers;    } catch (Exception e) {                return false;    }}
private booleanf10844
1
get
public static RuntimeHandles kafkatest_f10854_0()
{    return INSTANCE;}
f10854
0
connectorHandle
public ConnectorHandle kafkatest_f10855_0(String connectorName)
{    return connectorHandles.computeIfAbsent(connectorName, k -> new ConnectorHandle(connectorName));}
f10855
0
deleteConnector
public void kafkatest_f10856_0(String connectorName)
{    connectorHandles.remove(connectorName);}
f10856
0
expectedRestarts
public StartAndStopLatch kafkatest_f10864_0(int expectedRestarts, List<StartAndStopLatch> dependents)
{    return expectedRestarts(expectedRestarts, expectedRestarts, dependents);}
f10864
0
expectedStarts
public StartAndStopLatch kafkatest_f10865_0(int expectedStarts)
{    return expectedRestarts(expectedStarts, 0);}
f10865
0
expectedStarts
public StartAndStopLatch kafkatest_f10866_0(int expectedStarts, List<StartAndStopLatch> dependents)
{    return expectedRestarts(expectedStarts, 0, dependents);}
f10866
0
shouldExpectRestarts
public void kafkatest_f10874_0() throws Exception
{    waiters = Executors.newSingleThreadExecutor();    latch = counter.expectedRestarts(1);    Future<Boolean> future = asyncAwait(100, TimeUnit.MILLISECONDS);    clock.sleep(1000);    counter.recordStop();    counter.recordStart();    assertTrue(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
f10874
0
shouldFailToWaitForRestartThatNeverHappens
public void kafkatest_f10875_0() throws Exception
{    waiters = Executors.newSingleThreadExecutor();    latch = counter.expectedRestarts(1);    Future<Boolean> future = asyncAwait(100, TimeUnit.MILLISECONDS);    clock.sleep(1000);    // Record a stop but NOT a start    counter.recordStop();    assertFalse(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
f10875
0
asyncAwait
private Future<Boolean> kafkatest_f10876_0(long duration, TimeUnit unit)
{    return waiters.submit(() -> {        try {            return latch.await(duration, unit);        } catch (InterruptedException e) {            Thread.interrupted();            return false;        }    });}
f10876
0
shouldReturnTrueWhenAwaitingForStartAndStopToComplete
public void kafkatest_f10884_0() throws Throwable
{    latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);    future = asyncAwait(100);    latch.recordStart();    latch.recordStop();    clock.sleep(10);    assertTrue(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
f10884
0
shouldReturnFalseWhenAwaitingForDependentLatchToComplete
public void kafkatest_f10885_0() throws Throwable
{    StartAndStopLatch depLatch = new StartAndStopLatch(1, 1, this::complete, null, clock);    dependents = Collections.singletonList(depLatch);    latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);    future = asyncAwait(100);    latch.recordStart();    latch.recordStop();    clock.sleep(10);    assertFalse(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
f10885
0
shouldReturnTrueWhenAwaitingForStartAndStopAndDependentLatch
public void kafkatest_f10886_0() throws Throwable
{    StartAndStopLatch depLatch = new StartAndStopLatch(1, 1, this::complete, null, clock);    dependents = Collections.singletonList(depLatch);    latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);    future = asyncAwait(100);    latch.recordStart();    latch.recordStop();    depLatch.recordStart();    depLatch.recordStop();    clock.sleep(10);    assertTrue(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
f10886
0
expectedRecords
public void kafkatest_f10894_0(int expected)
{    expectedRecords = expected;    recordsRemainingLatch = new CountDownLatch(expected);}
f10894
0
expectedCommits
public void kafkatest_f10895_0(int expected)
{    expectedRecords = expected;    recordsToCommitLatch = new CountDownLatch(expected);}
f10895
0
partitionsAssigned
public void kafkatest_f10896_0(int numPartitions)
{    partitionsAssigned.set(numPartitions);}
f10896
0
expectedStarts
public StartAndStopLatch kafkatest_f10904_0(int expectedStarts)
{    return startAndStopCounter.expectedStarts(expectedStarts);}
f10904
0
expectedStops
public StartAndStopLatch kafkatest_f10905_0(int expectedStops)
{    return startAndStopCounter.expectedStops(expectedStops);}
f10905
0
toString
public String kafkatest_f10906_0()
{    return "Handle{" + "taskId='" + taskId + '\'' + '}';}
f10906
0
testConfigValidationInvalidTopics
public void kafkatest_f10914_0()
{    AbstractHerder herder = createConfigValidationHerder(TestSinkConnector.class, noneConnectorClientConfigOverridePolicy);    replayAll();    Map<String, String> config = new HashMap<>();    config.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, TestSinkConnector.class.getName());    config.put(SinkConnectorConfig.TOPICS_CONFIG, "topic1,topic2");    config.put(SinkConnectorConfig.TOPICS_REGEX_CONFIG, "topic.*");    herder.validateConnectorConfig(config);    verifyAll();}
f10914
0
testConfigValidationTransformsExtendResults
public void kafkatest_f10915_0()
{    AbstractHerder herder = createConfigValidationHerder(TestSourceConnector.class, noneConnectorClientConfigOverridePolicy);    // 2 transform aliases defined -> 2 plugin lookups    Set<PluginDesc<Transformation>> transformations = new HashSet<>();    transformations.add(new PluginDesc<Transformation>(SampleTransformation.class, "1.0", classLoader));    EasyMock.expect(plugins.transformations()).andReturn(transformations).times(2);    replayAll();    // Define 2 transformations. One has a class defined and so can get embedded configs, the other is missing    // class info that should generate an error.    Map<String, String> config = new HashMap<>();    config.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, TestSourceConnector.class.getName());    config.put(ConnectorConfig.NAME_CONFIG, "connector-name");    config.put(ConnectorConfig.TRANSFORMS_CONFIG, "xformA,xformB");    config.put(ConnectorConfig.TRANSFORMS_CONFIG + ".xformA.type", SampleTransformation.class.getName());    // connector required config    config.put("required", "value");    ConfigInfos result = herder.validateConnectorConfig(config);    assertEquals(herder.connectorTypeForClass(config.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG)), ConnectorType.SOURCE);    // We expect there to be errors due to the missing name and .... Note that these assertions depend heavily on    // the config fields for SourceConnectorConfig, but we expect these to change rarely.    assertEquals(TestSourceConnector.class.getName(), result.name());    // Each transform also gets its own group    List<String> expectedGroups = Arrays.asList(ConnectorConfig.COMMON_GROUP, ConnectorConfig.TRANSFORMS_GROUP, ConnectorConfig.ERROR_GROUP, "Transforms: xformA", "Transforms: xformB");    assertEquals(expectedGroups, result.groups());    assertEquals(2, result.errorCount());    // Base connector config has 13 fields, connector's configs add 2, 2 type fields from the transforms, and    // 1 from the valid transformation's config    assertEquals(18, result.values().size());    // Should get 2 type fields from the transforms, first adds its own config since it has a valid class    assertEquals("transforms.xformA.type", result.values().get(13).configValue().name());    assertTrue(result.values().get(13).configValue().errors().isEmpty());    assertEquals("transforms.xformA.subconfig", result.values().get(14).configValue().name());    assertEquals("transforms.xformB.type", result.values().get(15).configValue().name());    assertFalse(result.values().get(15).configValue().errors().isEmpty());    verifyAll();}
f10915
0
testConfigValidationPrincipalOnlyOverride
public void kafkatest_f10916_0()
{    AbstractHerder herder = createConfigValidationHerder(TestSourceConnector.class, new PrincipalConnectorClientConfigOverridePolicy());    replayAll();    Map<String, String> config = new HashMap<>();    config.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, TestSourceConnector.class.getName());    config.put(ConnectorConfig.NAME_CONFIG, "connector-name");    // connector required config    config.put("required", "value");    String ackConfigKey = producerOverrideKey(ProducerConfig.ACKS_CONFIG);    String saslConfigKey = producerOverrideKey(SaslConfigs.SASL_JAAS_CONFIG);    config.put(ackConfigKey, "none");    config.put(saslConfigKey, "jaas_config");    ConfigInfos result = herder.validateConnectorConfig(config);    assertEquals(herder.connectorTypeForClass(config.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG)), ConnectorType.SOURCE);    // We expect there to be errors due to now allowed override policy for ACKS.... Note that these assertions depend heavily on    // the config fields for SourceConnectorConfig, but we expect these to change rarely.    assertEquals(TestSourceConnector.class.getName(), result.name());    // Each transform also gets its own group    List<String> expectedGroups = Arrays.asList(ConnectorConfig.COMMON_GROUP, ConnectorConfig.TRANSFORMS_GROUP, ConnectorConfig.ERROR_GROUP);    assertEquals(expectedGroups, result.groups());    assertEquals(1, result.errorCount());    // Base connector config has 13 fields, connector's configs add 2, and 2 producer overrides    assertEquals(17, result.values().size());    assertTrue(result.values().stream().anyMatch(configInfo -> ackConfigKey.equals(configInfo.configValue().name()) && !configInfo.configValue().errors().isEmpty()));    assertTrue(result.values().stream().anyMatch(configInfo -> saslConfigKey.equals(configInfo.configValue().name()) && configInfo.configValue().errors().isEmpty()));    verifyAll();}
f10916
0
config
public ConfigDef kafkatest_f10925_0()
{    return new ConfigDef().define("subconfig", ConfigDef.Type.STRING, "default", ConfigDef.Importance.LOW, "docs");}
f10925
0
producerOverrideKey
private static String kafkatest_f10927_0(String config)
{    return ConnectorConfig.CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX + config;}
f10927
0
setUp
public void kafkatest_f10928_0()
{    metrics = new ConnectMetrics("worker1", new WorkerConfig(WorkerConfig.baseConfigDef(), DEFAULT_WORKER_CONFIG), new MockTime());}
f10928
0
testMetricGroupIdIdentity
public void kafkatest_f10936_0()
{    MetricGroupId id1 = metrics.groupId("name", "k1", "v1");    MetricGroupId id2 = metrics.groupId("name", "k1", "v1");    MetricGroupId id3 = metrics.groupId("name", "k1", "v1", "k2", "v2");    assertEquals(id1.hashCode(), id2.hashCode());    assertEquals(id1, id2);    assertEquals(id1.toString(), id2.toString());    assertEquals(id1.groupName(), id2.groupName());    assertEquals(id1.tags(), id2.tags());    assertNotNull(id1.tags());    assertNotEquals(id1, id3);}
f10936
0
testMetricGroupIdWithoutTags
public void kafkatest_f10937_0()
{    MetricGroupId id1 = metrics.groupId("name");    MetricGroupId id2 = metrics.groupId("name");    assertEquals(id1.hashCode(), id2.hashCode());    assertEquals(id1, id2);    assertEquals(id1.toString(), id2.toString());    assertEquals(id1.groupName(), id2.groupName());    assertEquals(id1.tags(), id2.tags());    assertNotNull(id1.tags());    assertNotNull(id2.tags());}
f10937
0
testRecreateWithClose
public void kafkatest_f10938_0()
{    final Sensor originalSensor = addToGroup(metrics, false);    final Sensor recreatedSensor = addToGroup(metrics, true);    // because we closed the metricGroup, we get a brand-new sensor    assertNotSame(originalSensor, recreatedSensor);}
f10938
0
config
public ConfigDef kafkatest_f10946_0()
{    return new ConfigDef().define("magic.number", ConfigDef.Type.INT, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.Range.atLeast(42), ConfigDef.Importance.HIGH, "");}
f10946
0
noTransforms
public void kafkatest_f10947_0()
{    Map<String, String> props = new HashMap<>();    props.put("name", "test");    props.put("connector.class", TestConnector.class.getName());    new ConnectorConfig(MOCK_PLUGINS, props);}
f10947
0
danglingTransformAlias
public void kafkatest_f10948_0()
{    Map<String, String> props = new HashMap<>();    props.put("name", "test");    props.put("connector.class", TestConnector.class.getName());    props.put("transforms", "dangler");    new ConnectorConfig(MOCK_PLUGINS, props);}
f10948
0
setup
public void kafkatest_f10956_0()
{    configStorage = mock(KafkaConfigBackingStore.class);    configState = new ClusterConfigState(1L, Collections.singletonMap(connectorId1, 1), Collections.singletonMap(connectorId1, new HashMap<>()), Collections.singletonMap(connectorId1, TargetState.STARTED), Collections.singletonMap(taskId1x0, new HashMap<>()), Collections.emptySet());}
f10956
0
teardown
public void kafkatest_f10957_0()
{    verifyNoMoreInteractions(configStorage);}
f10957
0
testEagerToEagerMetadata
public void kafkatest_f10958_0()
{    when(configStorage.snapshot()).thenReturn(configState);    ExtendedWorkerState workerState = new ExtendedWorkerState(LEADER_URL, configStorage.snapshot().offset(), null);    ByteBuffer metadata = ConnectProtocol.serializeMetadata(workerState);    ConnectProtocol.WorkerState state = ConnectProtocol.deserializeMetadata(metadata);    assertEquals(LEADER_URL, state.url());    assertEquals(1, state.offset());    verify(configStorage).snapshot();}
f10958
0
setUp
public void kafkatest_f10966_0() throws Exception
{    time = new MockTime();    metrics = new MockConnectMetrics(time);    worker = PowerMock.createMock(Worker.class);    EasyMock.expect(worker.isSinkConnector(CONN1)).andStubReturn(Boolean.TRUE);    // Default to the old protocol unless specified otherwise    connectProtocolVersion = CONNECT_PROTOCOL_V0;    herder = PowerMock.createPartialMock(DistributedHerder.class, new String[] { "backoff", "connectorTypeForClass", "updateDeletedConnectorStatus" }, new DistributedConfig(HERDER_CONFIG), worker, WORKER_ID, KAFKA_CLUSTER_ID, statusBackingStore, configBackingStore, member, MEMBER_URL, metrics, time, noneConnectorClientConfigOverridePolicy);    configUpdateListener = herder.new ConfigUpdateListener();    rebalanceListener = herder.new RebalanceListener(time);    plugins = PowerMock.createMock(Plugins.class);    conn1SinkConfig = new SinkConnectorConfig(plugins, CONN1_CONFIG);    conn1SinkConfigUpdated = new SinkConnectorConfig(plugins, CONN1_CONFIG_UPDATED);    EasyMock.expect(herder.connectorTypeForClass(BogusSourceConnector.class.getName())).andReturn(ConnectorType.SOURCE).anyTimes();    pluginLoader = PowerMock.createMock(PluginClassLoader.class);    delegatingLoader = PowerMock.createMock(DelegatingClassLoader.class);    PowerMock.mockStatic(Plugins.class);    PowerMock.expectPrivate(herder, "updateDeletedConnectorStatus").andVoid().anyTimes();}
f10966
0
tearDown
public void kafkatest_f10967_0()
{    if (metrics != null)        metrics.stop();}
f10967
0
testJoinAssignment
public void kafkatest_f10968_0() throws Exception
{    // Join group and get assignment    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    EasyMock.expect(worker.getPlugins()).andReturn(plugins);    expectRebalance(1, Arrays.asList(CONN1), Arrays.asList(TASK1));    expectPostRebalanceCatchup(SNAPSHOT);    worker.startConnector(EasyMock.eq(CONN1), EasyMock.<Map<String, String>>anyObject(), EasyMock.<ConnectorContext>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    EasyMock.expect(worker.isRunning(CONN1)).andReturn(true);    EasyMock.expect(worker.connectorTaskConfigs(CONN1, conn1SinkConfig)).andReturn(TASK_CONFIGS);    worker.startTask(EasyMock.eq(TASK1), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.tick();    time.sleep(1000L);    assertStatistics(3, 1, 100, 1000L);    PowerMock.verifyAll();}
f10968
0
testCreateConnectorFailedBasicValidation
public void kafkatest_f10976_0() throws Exception
{    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    HashMap<String, String> config = new HashMap<>(CONN2_CONFIG);    config.remove(ConnectorConfig.NAME_CONFIG);    member.wakeup();    PowerMock.expectLastCall();    // config validation    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    EasyMock.expect(connectorMock.config()).andStubReturn(new ConfigDef());    ConfigValue validatedValue = new ConfigValue("foo.bar");    EasyMock.expect(connectorMock.validate(config)).andReturn(new Config(singletonList(validatedValue)));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    // CONN2 creation should fail    Capture<Throwable> error = EasyMock.newCapture();    putConnectorCallback.onCompletion(EasyMock.capture(error), EasyMock.<Herder.Created<ConnectorInfo>>isNull());    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // No immediate action besides this -- change will be picked up via the config log    PowerMock.replayAll();    herder.putConnectorConfig(CONN2, config, false, putConnectorCallback);    herder.tick();    assertTrue(error.hasCaptured());    assertTrue(error.getValue() instanceof BadRequestException);    time.sleep(1000L);    assertStatistics(3, 1, 100, 1000L);    PowerMock.verifyAll();}
f10976
0
testCreateConnectorFailedCustomValidation
public void kafkatest_f10977_0() throws Exception
{    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    member.wakeup();    PowerMock.expectLastCall();    // config validation    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    ConfigDef configDef = new ConfigDef();    configDef.define("foo.bar", ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, "foo.bar doc");    EasyMock.expect(connectorMock.config()).andReturn(configDef);    ConfigValue validatedValue = new ConfigValue("foo.bar");    validatedValue.addErrorMessage("Failed foo.bar validation");    EasyMock.expect(connectorMock.validate(CONN2_CONFIG)).andReturn(new Config(singletonList(validatedValue)));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    // CONN2 creation should fail    Capture<Throwable> error = EasyMock.newCapture();    putConnectorCallback.onCompletion(EasyMock.capture(error), EasyMock.<Herder.Created<ConnectorInfo>>isNull());    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // No immediate action besides this -- change will be picked up via the config log    PowerMock.replayAll();    herder.putConnectorConfig(CONN2, CONN2_CONFIG, false, putConnectorCallback);    herder.tick();    assertTrue(error.hasCaptured());    assertTrue(error.getValue() instanceof BadRequestException);    time.sleep(1000L);    assertStatistics(3, 1, 100, 1000L);    PowerMock.verifyAll();}
f10977
0
testConnectorNameConflictsWithWorkerGroupId
public void kafkatest_f10978_0() throws Exception
{    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    member.wakeup();    PowerMock.expectLastCall();    Map<String, String> config = new HashMap<>(CONN2_CONFIG);    config.put(ConnectorConfig.NAME_CONFIG, "test-group");    // config validation    Connector connectorMock = PowerMock.createMock(SinkConnector.class);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    EasyMock.expect(connectorMock.config()).andReturn(new ConfigDef());    EasyMock.expect(connectorMock.validate(config)).andReturn(new Config(Collections.<ConfigValue>emptyList()));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    // CONN2 creation should fail because the worker group id (connect-test-group) conflicts with    // the consumer group id we would use for this sink    Capture<Throwable> error = EasyMock.newCapture();    putConnectorCallback.onCompletion(EasyMock.capture(error), EasyMock.isNull(Herder.Created.class));    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // No immediate action besides this -- change will be picked up via the config log    PowerMock.replayAll();    herder.putConnectorConfig(CONN2, config, false, putConnectorCallback);    herder.tick();    assertTrue(error.hasCaptured());    assertTrue(error.getValue() instanceof BadRequestException);    time.sleep(1000L);    assertStatistics(3, 1, 100, 1000L);    PowerMock.verifyAll();}
f10978
0
testRestartUnknownTask
public void kafkatest_f10986_0() throws Exception
{    // get the initial assignment    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    member.wakeup();    PowerMock.expectLastCall();    member.ensureActive();    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    FutureCallback<Void> callback = new FutureCallback<>();    herder.tick();    herder.restartTask(new ConnectorTaskId("blah", 0), callback);    herder.tick();    try {        callback.get(1000L, TimeUnit.MILLISECONDS);        fail("Expected NotLeaderException to be raised");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof NotFoundException);    }    PowerMock.verifyAll();}
f10986
0
testRequestProcessingOrder
public void kafkatest_f10987_0()
{    final DistributedHerder.DistributedHerderRequest req1 = herder.addRequest(100, null, null);    final DistributedHerder.DistributedHerderRequest req2 = herder.addRequest(10, null, null);    final DistributedHerder.DistributedHerderRequest req3 = herder.addRequest(200, null, null);    final DistributedHerder.DistributedHerderRequest req4 = herder.addRequest(200, null, null);    // lowest delay    assertEquals(req2, herder.requests.pollFirst());    // next lowest delay    assertEquals(req1, herder.requests.pollFirst());    // same delay as req4, but added first    assertEquals(req3, herder.requests.pollFirst());    assertEquals(req4, herder.requests.pollFirst());}
f10987
0
testRestartTaskRedirectToLeader
public void kafkatest_f10988_0() throws Exception
{    // get the initial assignment    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // now handle the task restart    member.wakeup();    PowerMock.expectLastCall();    member.ensureActive();    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.tick();    FutureCallback<Void> callback = new FutureCallback<>();    herder.restartTask(TASK0, callback);    herder.tick();    try {        callback.get(1000L, TimeUnit.MILLISECONDS);        fail("Expected NotLeaderException to be raised");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof NotLeaderException);    }    PowerMock.verifyAll();}
f10988
0
testConnectorResumedRunningTaskOnly
public void kafkatest_f10996_0() throws Exception
{    // even if we don't own the connector, we should still propagate target state    // changes to the worker so that tasks will transition correctly    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    EasyMock.expect(worker.connectorNames()).andStubReturn(Collections.<String>emptySet());    // join    expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));    expectPostRebalanceCatchup(SNAPSHOT_PAUSED_CONN1);    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.PAUSED));    PowerMock.expectLastCall().andReturn(true);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // handle the state change    member.wakeup();    member.ensureActive();    PowerMock.expectLastCall();    EasyMock.expect(configBackingStore.snapshot()).andReturn(SNAPSHOT);    PowerMock.expectLastCall();    worker.setTargetState(CONN1, TargetState.STARTED);    PowerMock.expectLastCall();    EasyMock.expect(worker.isRunning(CONN1)).andReturn(false);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    // join    herder.tick();    // state changes to paused    configUpdateListener.onConnectorTargetStateChange(CONN1);    // apply state change    herder.tick();    PowerMock.verifyAll();}
f10996
0
testTaskConfigAdded
public void kafkatest_f10997_0()
{    // Task config always requires rebalance    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    // join    expectRebalance(-1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // apply config    member.wakeup();    member.ensureActive();    PowerMock.expectLastCall();    // Checks for config updates and starts rebalance    EasyMock.expect(configBackingStore.snapshot()).andReturn(SNAPSHOT);    member.requestRejoin();    PowerMock.expectLastCall();    // Performs rebalance and gets new assignment    expectRebalance(Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList(), ConnectProtocol.Assignment.NO_ERROR, 1, Collections.<String>emptyList(), Arrays.asList(TASK0));    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    // join    herder.tick();    // read updated config    configUpdateListener.onTaskConfigUpdate(Arrays.asList(TASK0, TASK1, TASK2));    // apply config    herder.tick();    // do rebalance    herder.tick();    PowerMock.verifyAll();}
f10997
0
testJoinLeaderCatchUpFails
public void kafkatest_f10998_0() throws Exception
{    // Join group and as leader fail to do assignment    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList(), ConnectProtocol.Assignment.CONFIG_MISMATCH, 1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    // Reading to end of log times out    configBackingStore.refresh(EasyMock.anyLong(), EasyMock.anyObject(TimeUnit.class));    EasyMock.expectLastCall().andThrow(new TimeoutException());    member.maybeLeaveGroup(EasyMock.eq("taking too long to read the log"));    EasyMock.expectLastCall();    PowerMock.expectPrivate(herder, "backoff", DistributedConfig.WORKER_UNSYNC_BACKOFF_MS_DEFAULT);    member.requestRejoin();    // After backoff, restart the process and this time succeed    expectRebalance(1, Arrays.asList(CONN1), Arrays.asList(TASK1));    expectPostRebalanceCatchup(SNAPSHOT);    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    worker.startConnector(EasyMock.eq(CONN1), EasyMock.<Map<String, String>>anyObject(), EasyMock.<ConnectorContext>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    EasyMock.expect(worker.getPlugins()).andReturn(plugins);    EasyMock.expect(worker.connectorTaskConfigs(CONN1, conn1SinkConfig)).andReturn(TASK_CONFIGS);    worker.startTask(EasyMock.eq(TASK1), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    EasyMock.expect(worker.isRunning(CONN1)).andReturn(true);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.tick();    time.sleep(1000L);    assertStatistics("leaderUrl", true, 3, 0, Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY);    herder.tick();    time.sleep(2000L);    assertStatistics("leaderUrl", false, 3, 1, 100, 2000L);    PowerMock.verifyAll();}
f10998
0
answer
public Object kafkatest_f11006_0() throws Throwable
{    ExtendedAssignment assignment;    if (!revokedConnectors.isEmpty() || !revokedTasks.isEmpty()) {        rebalanceListener.onRevoked("leader", revokedConnectors, revokedTasks);    }    if (connectProtocolVersion == CONNECT_PROTOCOL_V0) {        assignment = new ExtendedAssignment(connectProtocolVersion, error, "leader", "leaderUrl", offset, assignedConnectors, assignedTasks, Collections.emptyList(), Collections.emptyList(), 0);    } else {        assignment = new ExtendedAssignment(connectProtocolVersion, error, "leader", "leaderUrl", offset, assignedConnectors, assignedTasks, new ArrayList<>(revokedConnectors), new ArrayList<>(revokedTasks), delay);    }    rebalanceListener.onAssigned(assignment, 3);    time.sleep(100L);    return null;}
f11006
0
expectPostRebalanceCatchup
private void kafkatest_f11007_0(final ClusterConfigState readToEndSnapshot) throws TimeoutException
{    configBackingStore.refresh(EasyMock.anyLong(), EasyMock.anyObject(TimeUnit.class));    EasyMock.expectLastCall();    EasyMock.expect(configBackingStore.snapshot()).andReturn(readToEndSnapshot);}
f11007
0
assertStatistics
private void kafkatest_f11008_0(int expectedEpoch, int completedRebalances, double rebalanceTime, double millisSinceLastRebalance)
{    String expectedLeader = completedRebalances <= 0 ? null : "leaderUrl";    assertStatistics(expectedLeader, false, expectedEpoch, completedRebalances, rebalanceTime, millisSinceLastRebalance);}
f11008
0
testTaskAssignmentWhenLeaderLeavesPermanently
public void kafkatest_f11016_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    when(coordinator.configSnapshot()).thenReturn(configState);    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    // First assignment with 3 workers and 2 connectors configured but not yet assigned    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    memberConfigs.put("worker3", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(2, 8, 0, 0, "worker1", "worker2", "worker3");    // Second assignment with two workers remaining in the group. The worker that left the    // group was the leader. The new leader has no previous assignments and is not tracking a    // delay upon a leader's exit    applyAssignments(returnedAssignments);    assignments.remove("worker1");    leader = "worker2";    leaderUrl = expectedLeaderUrl(leader);    memberConfigs = memberConfigs(leader, offset, assignments);    // The fact that the leader bounces means that the assignor starts from a clean slate    initAssignor();    // Capture needs to be reset to point to the new assignor    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(1, 3, 0, 0, "worker2", "worker3");    // Third (incidental) assignment with still only one worker in the group.    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker2", "worker3");    verify(coordinator, times(rebalanceNum)).configSnapshot();    verify(coordinator, times(rebalanceNum)).leaderState(any());}
f11016
0
testTaskAssignmentWhenLeaderBounces
public void kafkatest_f11017_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    when(coordinator.configSnapshot()).thenReturn(configState);    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    // First assignment with 3 workers and 2 connectors configured but not yet assigned    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    memberConfigs.put("worker3", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(2, 8, 0, 0, "worker1", "worker2", "worker3");    // Second assignment with two workers remaining in the group. The worker that left the    // group was the leader. The new leader has no previous assignments and is not tracking a    // delay upon a leader's exit    applyAssignments(returnedAssignments);    assignments.remove("worker1");    leader = "worker2";    leaderUrl = expectedLeaderUrl(leader);    memberConfigs = memberConfigs(leader, offset, assignments);    // The fact that the leader bounces means that the assignor starts from a clean slate    initAssignor();    // Capture needs to be reset to point to the new assignor    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(1, 3, 0, 0, "worker2", "worker3");    // Third assignment with the previous leader returning as a follower. In this case, the    // arrival of the previous leader is treated as an arrival of a new worker. Reassignment    // happens immediately, first with a revocation    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    memberConfigs.put("worker1", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 2, "worker1", "worker2", "worker3");    // Fourth assignment after revocations    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 2, 0, 0, "worker1", "worker2", "worker3");    verify(coordinator, times(rebalanceNum)).configSnapshot();    verify(coordinator, times(rebalanceNum)).leaderState(any());}
f11017
0
testTaskAssignmentWhenFirstAssignmentAttemptFails
public void kafkatest_f11018_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    when(coordinator.configSnapshot()).thenReturn(configState);    doThrow(new RuntimeException("Unable to send computed assignment with SyncGroupRequest")).when(assignor).serializeAssignments(assignmentsCapture.capture());    // First assignment with 2 workers and 2 connectors configured but not yet assigned    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    try {        assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    } catch (RuntimeException e) {        RequestFuture.failure(e);    }    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    // This was the assignment that should have been sent, but didn't make it after all the way    assertDelay(0, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(2, 8, 0, 0, "worker1", "worker2");    // Second assignment happens with members returning the same assignments (memberConfigs)    // as the first time. The assignor can not tell whether it was the assignment that failed    // or the workers that were bounced. Therefore it goes into assignment freeze for    // the new assignments for a rebalance delay period    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertDelay(rebalanceDelay, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1", "worker2");    time.sleep(rebalanceDelay / 2);    // Third (incidental) assignment with still only one worker in the group. Max delay has not    // been reached yet    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay / 2, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1", "worker2");    time.sleep(rebalanceDelay / 2 + 1);    // Fourth assignment after delay expired. Finally all the new assignments are assigned    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(2, 8, 0, 0, "worker1", "worker2");    verify(coordinator, times(rebalanceNum)).configSnapshot();    verify(coordinator, times(rebalanceNum)).leaderState(any());}
f11018
0
testLostAssignmentHandlingWithMoreThanOneCandidates
public void kafkatest_f11026_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    assertTrue(assignor.candidateWorkersForReassignment.isEmpty());    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    Map<String, WorkerLoad> configuredAssignment = new HashMap<>();    configuredAssignment.put("worker0", workerLoad("worker0", 0, 2, 0, 4));    configuredAssignment.put("worker2", workerLoad("worker2", 4, 2, 8, 4));    ConnectorsAndTasks newSubmissions = new ConnectorsAndTasks.Builder().build();    // No lost assignments    assignor.handleLostAssignments(new ConnectorsAndTasks.Builder().build(), newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    String flakyWorker = "worker1";    WorkerLoad lostLoad = workerLoad(flakyWorker, 2, 2, 4, 4);    String newWorker = "worker3";    ConnectorsAndTasks lostAssignments = new ConnectorsAndTasks.Builder().withCopies(lostLoad.connectors(), lostLoad.tasks()).build();    // Lost assignments detected - A new worker also has joined that is not the returning worker    configuredAssignment.put(newWorker, new WorkerLoad.Builder(newWorker).build());    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.singleton(newWorker), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay / 2);    rebalanceDelay /= 2;    // Now two new workers have joined    configuredAssignment.put(flakyWorker, new WorkerLoad.Builder(flakyWorker).build());    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    Set<String> expectedWorkers = new HashSet<>();    expectedWorkers.addAll(Arrays.asList(newWorker, flakyWorker));    assertThat("Wrong set of workers for reassignments", expectedWorkers, is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay);    // The new workers have new assignments, other than the lost ones    configuredAssignment.put(flakyWorker, workerLoad(flakyWorker, 6, 2, 8, 4));    configuredAssignment.put(newWorker, workerLoad(newWorker, 8, 2, 12, 4));    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    // newWorker joined first, so should be picked up first as a candidate for reassignment    assertTrue("Wrong assignment of lost connectors", configuredAssignment.getOrDefault(newWorker, new WorkerLoad.Builder(flakyWorker).build()).connectors().containsAll(lostAssignments.connectors()));    assertTrue("Wrong assignment of lost tasks", configuredAssignment.getOrDefault(newWorker, new WorkerLoad.Builder(flakyWorker).build()).tasks().containsAll(lostAssignments.tasks()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);}
f11026
0
testLostAssignmentHandlingWhenWorkerBouncesBackButFinallyLeaves
public void kafkatest_f11027_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    assertTrue(assignor.candidateWorkersForReassignment.isEmpty());    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    Map<String, WorkerLoad> configuredAssignment = new HashMap<>();    configuredAssignment.put("worker0", workerLoad("worker0", 0, 2, 0, 4));    configuredAssignment.put("worker2", workerLoad("worker2", 4, 2, 8, 4));    ConnectorsAndTasks newSubmissions = new ConnectorsAndTasks.Builder().build();    // No lost assignments    assignor.handleLostAssignments(new ConnectorsAndTasks.Builder().build(), newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    String veryFlakyWorker = "worker1";    WorkerLoad lostLoad = workerLoad(veryFlakyWorker, 2, 2, 4, 4);    ConnectorsAndTasks lostAssignments = new ConnectorsAndTasks.Builder().withCopies(lostLoad.connectors(), lostLoad.tasks()).build();    // Lost assignments detected - No candidate worker has appeared yet (worker with no assignments)    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay / 2);    rebalanceDelay /= 2;    // A new worker (probably returning worker) has joined    configuredAssignment.put(veryFlakyWorker, new WorkerLoad.Builder(veryFlakyWorker).build());    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.singleton(veryFlakyWorker), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay);    // The returning worker leaves permanently after joining briefly during the delay    configuredAssignment.remove(veryFlakyWorker);    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertTrue("Wrong assignment of lost connectors", newSubmissions.connectors().containsAll(lostAssignments.connectors()));    assertTrue("Wrong assignment of lost tasks", newSubmissions.tasks().containsAll(lostAssignments.tasks()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);}
f11027
0
emptyWorkerLoad
private WorkerLoad kafkatest_f11028_0(String worker)
{    return new WorkerLoad.Builder(worker).build();}
f11028
0
connectorConfigs
private static Map<String, Map<String, String>> kafkatest_f11036_0(int start, int connectorNum)
{    return IntStream.range(start, connectorNum + 1).mapToObj(i -> new SimpleEntry<>("connector" + i, new HashMap<String, String>())).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
f11036
0
connectorTargetStates
private static Map<String, TargetState> kafkatest_f11037_0(int start, int connectorNum, TargetState state)
{    return IntStream.range(start, connectorNum + 1).mapToObj(i -> new SimpleEntry<>("connector" + i, state)).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
f11037
0
taskConfigs
private static Map<ConnectorTaskId, Map<String, String>> kafkatest_f11038_0(int start, int connectorNum, int taskNum)
{    return IntStream.range(start, taskNum + 1).mapToObj(i -> new SimpleEntry<>(new ConnectorTaskId("connector" + i / connectorNum + 1, i), new HashMap<String, String>())).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
f11038
0
assertNoDuplicateInAssignment
private void kafkatest_f11046_0(Map<String, ExtendedWorkerState> existingAssignment)
{    List<String> existingConnectors = existingAssignment.values().stream().flatMap(a -> a.assignment().connectors().stream()).collect(Collectors.toList());    Set<String> existingUniqueConnectors = new HashSet<>(existingConnectors);    existingConnectors.removeAll(existingUniqueConnectors);    assertThat("Connectors should be unique in assignments but duplicates where found", Collections.emptyList(), is(existingConnectors));    List<ConnectorTaskId> existingTasks = existingAssignment.values().stream().flatMap(a -> a.assignment().tasks().stream()).collect(Collectors.toList());    Set<ConnectorTaskId> existingUniqueTasks = new HashSet<>(existingTasks);    existingTasks.removeAll(existingUniqueTasks);    assertThat("Tasks should be unique in assignments but duplicates where found", Collections.emptyList(), is(existingTasks));}
f11046
0
mode
public static Iterable<?> kafkatest_f11047_0()
{    return Arrays.asList(new Object[][] { { COMPATIBLE, 2 } });}
f11047
0
setup
public void kafkatest_f11048_0()
{    LogContext loggerFactory = new LogContext();    this.time = new MockTime();    this.metadata = new Metadata(0, Long.MAX_VALUE, loggerFactory, new ClusterResourceListeners());    this.client = new MockClient(time, metadata);    this.client.updateMetadata(TestUtils.metadataUpdateWith(1, Collections.singletonMap("topic", 1)));    this.node = metadata.fetch().nodes().get(0);    this.consumerClient = new ConsumerNetworkClient(loggerFactory, client, metadata, time, retryBackoffMs, requestTimeoutMs, heartbeatIntervalMs);    this.metrics = new Metrics(time);    this.rebalanceListener = new MockRebalanceListener();    this.leaderId = "worker1";    this.memberId = "worker2";    this.anotherMemberId = "worker3";    this.leaderUrl = expectedUrl(leaderId);    this.memberUrl = expectedUrl(memberId);    this.anotherMemberUrl = expectedUrl(anotherMemberId);    this.generationId = 3;    this.offset = 10L;    this.configStorageCalls = 0;    this.rebalanceConfig = new GroupRebalanceConfig(sessionTimeoutMs, rebalanceTimeoutMs, heartbeatIntervalMs, groupId, Optional.empty(), retryBackoffMs, true);    this.coordinator = new WorkerCoordinator(rebalanceConfig, loggerFactory, consumerClient, metrics, "worker" + groupId, time, expectedUrl(leaderId), configStorage, rebalanceListener, compatibility, rebalanceDelay);    configState1 = clusterConfigState(offset, 2, 4);}
f11048
0
testTaskAssignmentWhenWorkerBounces
public void kafkatest_f11056_0()
{    when(configStorage.snapshot()).thenReturn(configState1);    // First assignment distributes configured connectors and tasks    coordinator.metadata();    ++configStorageCalls;    List<JoinGroupResponseMember> responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, null);    addJoinGroupResponseMember(responseMembers, memberId, offset, null);    addJoinGroupResponseMember(responseMembers, anotherMemberId, offset, null);    Map<String, ByteBuffer> result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    ExtendedAssignment leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId1), 3, Collections.emptyList(), 0, leaderAssignment);    ExtendedAssignment memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId2), 3, Collections.emptyList(), 0, memberAssignment);    ExtendedAssignment anotherMemberAssignment = deserializeAssignment(result, anotherMemberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 2, Collections.emptyList(), 0, anotherMemberAssignment);    // Second rebalance detects a worker is missing    coordinator.metadata();    ++configStorageCalls;    responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, leaderAssignment);    addJoinGroupResponseMember(responseMembers, memberId, offset, memberAssignment);    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, memberAssignment);    rebalanceDelay /= 2;    time.sleep(rebalanceDelay);    // A third rebalance before the delay expires won't change the assignments even if the    // member returns in the meantime    addJoinGroupResponseMember(responseMembers, anotherMemberId, offset, null);    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, memberAssignment);    anotherMemberAssignment = deserializeAssignment(result, anotherMemberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, anotherMemberAssignment);    time.sleep(rebalanceDelay + 1);    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    // A rebalance after the delay expires re-assigns the lost tasks the the returning member    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, memberAssignment);    anotherMemberAssignment = deserializeAssignment(result, anotherMemberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 2, Collections.emptyList(), 0, anotherMemberAssignment);    verify(configStorage, times(configStorageCalls)).snapshot();}
f11056
0
onAssigned
public void kafkatest_f11057_0(ExtendedAssignment assignment, int generation)
{    this.assignment = assignment;    assignedCount++;}
f11057
0
onRevoked
public void kafkatest_f11058_0(String leader, Collection<String> connectors, Collection<ConnectorTaskId> tasks)
{    if (connectors.isEmpty() && tasks.isEmpty()) {        return;    }    this.revokedLeader = leader;    this.revokedConnectors = connectors;    this.revokedTasks = tasks;    revokedCount++;}
f11058
0
matches
public boolean kafkatest_f11066_0(AbstractRequest body)
{    SyncGroupRequest sync = (SyncGroupRequest) body;    return sync.data.memberId().equals(consumerId) && sync.data.generationId() == 1 && sync.groupAssignments().containsKey(consumerId);}
f11066
0
testNormalJoinGroupFollower
public void kafkatest_f11067_0()
{    EasyMock.expect(configStorage.snapshot()).andReturn(configState1);    PowerMock.replayAll();    final String memberId = "member";    client.prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, node));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // normal join group    client.prepareResponse(joinGroupFollowerResponse(1, memberId, "leader", Errors.NONE));    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            SyncGroupRequest sync = (SyncGroupRequest) body;            return sync.data.memberId().equals(memberId) && sync.data.generationId() == 1 && sync.data.assignments().isEmpty();        }    }, syncGroupResponse(ConnectProtocol.Assignment.NO_ERROR, "leader", 1L, Collections.<String>emptyList(), Collections.singletonList(taskId1x0), Errors.NONE));    coordinator.ensureActiveGroup();    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(0, rebalanceListener.revokedCount);    assertEquals(1, rebalanceListener.assignedCount);    assertFalse(rebalanceListener.assignment.failed());    assertEquals(1L, rebalanceListener.assignment.offset());    assertEquals(Collections.emptyList(), rebalanceListener.assignment.connectors());    assertEquals(Collections.singletonList(taskId1x0), rebalanceListener.assignment.tasks());    PowerMock.verifyAll();}
f11067
0
matches
public boolean kafkatest_f11068_0(AbstractRequest body)
{    SyncGroupRequest sync = (SyncGroupRequest) body;    return sync.data.memberId().equals(memberId) && sync.data.generationId() == 1 && sync.data.assignments().isEmpty();}
f11068
0
joinGroupFollowerResponse
private JoinGroupResponse kafkatest_f11076_0(int generationId, String memberId, String leaderId, Errors error)
{    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(error.code()).setGenerationId(generationId).setProtocolName(WorkerCoordinator.DEFAULT_SUBPROTOCOL).setLeader(leaderId).setMemberId(memberId).setMembers(Collections.emptyList()));}
f11076
0
syncGroupResponse
private SyncGroupResponse kafkatest_f11077_0(short assignmentError, String leader, long configOffset, List<String> connectorIds, List<ConnectorTaskId> taskIds, Errors error)
{    ConnectProtocol.Assignment assignment = new ConnectProtocol.Assignment(assignmentError, leader, LEADER_URL, configOffset, connectorIds, taskIds);    ByteBuffer buf = ConnectProtocol.serializeAssignment(assignment);    return new SyncGroupResponse(new SyncGroupResponseData().setErrorCode(error.code()).setAssignment(Utils.toArray(buf)));}
f11077
0
onAssigned
public void kafkatest_f11078_0(ExtendedAssignment assignment, int generation)
{    this.assignment = assignment;    assignedCount++;}
f11078
0
testErrorHandlingInSourceTasksWthBadConverter
public void kafkatest_f11086_0() throws Exception
{    Map<String, String> reportProps = new HashMap<>();    reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, "true");    reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, "true");    LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);    RetryWithToleranceOperator retryWithToleranceOperator = operator();    retryWithToleranceOperator.metrics(errorHandlingMetrics);    retryWithToleranceOperator.reporters(singletonList(reporter));    createSourceTask(initialState, retryWithToleranceOperator, badConverter());    // valid json    Schema valSchema = SchemaBuilder.struct().field("val", Schema.INT32_SCHEMA).build();    Struct struct1 = new Struct(valSchema).put("val", 1234);    SourceRecord record1 = new SourceRecord(emptyMap(), emptyMap(), TOPIC, PARTITION1, valSchema, struct1);    Struct struct2 = new Struct(valSchema).put("val", 6789);    SourceRecord record2 = new SourceRecord(emptyMap(), emptyMap(), TOPIC, PARTITION1, valSchema, struct2);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(false);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(false);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(true);    EasyMock.expect(workerSourceTask.commitOffsets()).andReturn(true);    offsetWriter.offset(EasyMock.anyObject(), EasyMock.anyObject());    EasyMock.expectLastCall().times(2);    sourceTask.initialize(EasyMock.anyObject());    EasyMock.expectLastCall();    sourceTask.start(EasyMock.anyObject());    EasyMock.expectLastCall();    EasyMock.expect(sourceTask.poll()).andReturn(singletonList(record1));    EasyMock.expect(sourceTask.poll()).andReturn(singletonList(record2));    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andReturn(null).times(2);    PowerMock.replayAll();    workerSourceTask.initialize(TASK_CONFIG);    workerSourceTask.execute();    // two records were consumed from Kafka    assertSourceMetricValue("source-record-poll-total", 2.0);    // only one was written to the task    assertSourceMetricValue("source-record-write-total", 0.0);    // one record completely failed (converter issues)    assertErrorHandlingMetricValue("total-record-errors", 0.0);    // 2 failures in the transformation, and 1 in the converter    assertErrorHandlingMetricValue("total-record-failures", 8.0);    // one record completely failed (converter issues), and thus was skipped    assertErrorHandlingMetricValue("total-records-skipped", 0.0);    PowerMock.verifyAll();}
f11086
0
assertSinkMetricValue
private void kafkatest_f11087_0(String name, double expected)
{    ConnectMetrics.MetricGroup sinkTaskGroup = workerSinkTask.sinkTaskMetricsGroup().metricGroup();    double measured = metrics.currentMetricValueAsDouble(sinkTaskGroup, name);    assertEquals(expected, measured, 0.001d);}
f11087
0
assertSourceMetricValue
private void kafkatest_f11088_0(String name, double expected)
{    ConnectMetrics.MetricGroup sinkTaskGroup = workerSourceTask.sourceTaskMetricsGroup().metricGroup();    double measured = metrics.currentMetricValueAsDouble(sinkTaskGroup, name);    assertEquals(expected, measured, 0.001d);}
f11088
0
fromConnectData
public byte[]f11096_1String topic, Schema schema, Object value)
{    if (value == null) {        return super.fromConnectData(topic, schema, null);    }    invocations++;    if (invocations % 3 == 0) {                return super.fromConnectData(topic, schema, value);    } else {                throw new RetriableException("Bad invocations " + invocations + " for mod 3");    }}
public byte[]f11096
1
apply
public Rf11097_1R record)
{    invocations++;    if (invocations % mod == 0) {                return record;    } else {                throw new RetriableException("Bad invocations " + invocations + " for mod " + mod);    }}
public Rf11097
1
config
public ConfigDef kafkatest_f11098_0()
{    return CONFIG_DEF;}
f11098
0
testReportDLQTwice
public void kafkatest_f11106_0()
{    DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);    ProcessingContext context = processingContext();    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andReturn(metadata).times(2);    replay(producer);    deadLetterQueueReporter.report(context);    deadLetterQueueReporter.report(context);    PowerMock.verifyAll();}
f11106
0
testLogOnDisabledLogReporter
public void kafkatest_f11107_0()
{    LogReporter logReporter = new LogReporter(TASK_ID, config(emptyMap()), errorHandlingMetrics);    ProcessingContext context = processingContext();    context.error(new RuntimeException());    // reporting a context without an error should not cause any errors.    logReporter.report(context);    assertErrorHandlingMetricValue("total-errors-logged", 0.0);}
f11107
0
testLogOnEnabledLogReporter
public void kafkatest_f11108_0()
{    LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, "true")), errorHandlingMetrics);    ProcessingContext context = processingContext();    context.error(new RuntimeException());    // reporting a context without an error should not cause any errors.    logReporter.report(context);    assertErrorHandlingMetricValue("total-errors-logged", 1.0);}
f11108
0
processingContext
private ProcessingContext kafkatest_f11116_0()
{    ProcessingContext context = new ProcessingContext();    context.consumerRecord(new ConsumerRecord<>(TOPIC, 5, 100, new byte[] { 'a', 'b' }, new byte[] { 'x' }));    context.currentContext(Stage.KEY_CONVERTER, JsonConverter.class);    return context;}
f11116
0
config
private SinkConnectorConfig kafkatest_f11117_0(Map<String, String> configProps)
{    Map<String, String> props = new HashMap<>();    props.put(ConnectorConfig.NAME_CONFIG, "test");    props.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, SinkTask.class.getName());    props.putAll(configProps);    return new SinkConnectorConfig(plugins, props);}
f11117
0
assertErrorHandlingMetricValue
private void kafkatest_f11118_0(String name, double expected)
{    ConnectMetrics.MetricGroup sinkTaskGroup = errorHandlingMetrics.metricGroup();    double measured = metrics.currentMetricValueAsDouble(sinkTaskGroup, name);    assertEquals(expected, measured, 0.001d);}
f11118
0
testThrowExceptionInTaskPoll
public void kafkatest_f11126_0()
{    testHandleExceptionInStage(Stage.TASK_POLL, new Exception());}
f11126
0
testThrowExceptionInKafkaConsume
public void kafkatest_f11127_0()
{    testHandleExceptionInStage(Stage.KAFKA_CONSUME, new Exception());}
f11127
0
testThrowExceptionInKafkaProduce
public void kafkatest_f11128_0()
{    testHandleExceptionInStage(Stage.KAFKA_PRODUCE, new Exception());}
f11128
0
execAndHandleNonRetriableError
public void kafkatest_f11136_0(int numRetriableExceptionsThrown, long expectedWait, Exception e) throws Exception
{    MockTime time = new MockTime(0, 0, 0);    RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(6000, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time);    retryWithToleranceOperator.metrics(errorHandlingMetrics);    EasyMock.expect(mockOperation.call()).andThrow(e).times(numRetriableExceptionsThrown);    EasyMock.expect(mockOperation.call()).andReturn("Success");    replay(mockOperation);    String result = retryWithToleranceOperator.execAndHandleError(mockOperation, Exception.class);    assertTrue(retryWithToleranceOperator.failed());    assertNull(result);    assertEquals(expectedWait, time.hiResClockMs());    PowerMock.verifyAll();}
f11136
0
testCheckRetryLimit
public void kafkatest_f11137_0()
{    MockTime time = new MockTime(0, 0, 0);    RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(500, 100, NONE, time);    time.setCurrentTimeMs(100);    assertTrue(retryWithToleranceOperator.checkRetry(0));    time.setCurrentTimeMs(200);    assertTrue(retryWithToleranceOperator.checkRetry(0));    time.setCurrentTimeMs(400);    assertTrue(retryWithToleranceOperator.checkRetry(0));    time.setCurrentTimeMs(499);    assertTrue(retryWithToleranceOperator.checkRetry(0));    time.setCurrentTimeMs(501);    assertFalse(retryWithToleranceOperator.checkRetry(0));    time.setCurrentTimeMs(600);    assertFalse(retryWithToleranceOperator.checkRetry(0));}
f11137
0
testBackoffLimit
public void kafkatest_f11138_0()
{    MockTime time = new MockTime(0, 0, 0);    RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(5, 5000, NONE, time);    long prevTs = time.hiResClockMs();    retryWithToleranceOperator.backoff(1, 5000);    assertEquals(300, time.hiResClockMs() - prevTs);    prevTs = time.hiResClockMs();    retryWithToleranceOperator.backoff(2, 5000);    assertEquals(600, time.hiResClockMs() - prevTs);    prevTs = time.hiResClockMs();    retryWithToleranceOperator.backoff(3, 5000);    assertEquals(1200, time.hiResClockMs() - prevTs);    prevTs = time.hiResClockMs();    retryWithToleranceOperator.backoff(4, 5000);    assertEquals(2400, time.hiResClockMs() - prevTs);    prevTs = time.hiResClockMs();    retryWithToleranceOperator.backoff(5, 5000);    assertEquals(500, time.hiResClockMs() - prevTs);    prevTs = time.hiResClockMs();    retryWithToleranceOperator.backoff(6, 5000);    assertEquals(0, time.hiResClockMs() - prevTs);    PowerMock.verifyAll();}
f11138
0
answer
public Void kafkatest_f11146_0()
{    callback.getValue().onCompletion(null, expectedConnectors);    return null;}
f11146
0
connectorConfig
public void kafkatest_f11147_0()
{    final String connName = "sink6";    final Map<String, String> expectedConfig = Collections.singletonMap("key", "value");    Capture<Callback<Map<String, String>>> callback = EasyMock.newCapture();    herder.connectorConfig(EasyMock.eq(connName), EasyMock.capture(callback));    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() {            callback.getValue().onCompletion(null, expectedConfig);            return null;        }    });    EasyMock.replay(herder);    assertEquals(expectedConfig, connectClusterState.connectorConfig(connName));}
f11147
0
answer
public Void kafkatest_f11148_0()
{    callback.getValue().onCompletion(null, expectedConfig);    return null;}
f11148
0
testPluginDescWithSystemClassLoader
public void kafkatest_f11156_0()
{    String location = "classpath";    PluginDesc<SinkConnector> connectorDesc = new PluginDesc<>(SinkConnector.class, regularVersion, systemLoader);    assertPluginDesc(connectorDesc, SinkConnector.class, regularVersion, location);    PluginDesc<Converter> converterDesc = new PluginDesc<>(Converter.class, snaphotVersion, systemLoader);    assertPluginDesc(converterDesc, Converter.class, snaphotVersion, location);    PluginDesc<Transformation> transformDesc = new PluginDesc<>(Transformation.class, noVersion, systemLoader);    assertPluginDesc(transformDesc, Transformation.class, noVersion, location);}
f11156
0
testPluginDescWithNullVersion
public void kafkatest_f11157_0()
{    String nullVersion = "null";    PluginDesc<SourceConnector> connectorDesc = new PluginDesc<>(SourceConnector.class, null, pluginLoader);    assertPluginDesc(connectorDesc, SourceConnector.class, nullVersion, pluginLoader.location());    String location = "classpath";    PluginDesc<Converter> converterDesc = new PluginDesc<>(Converter.class, null, systemLoader);    assertPluginDesc(converterDesc, Converter.class, nullVersion, location);}
f11157
0
testPluginDescEquality
public void kafkatest_f11158_0()
{    PluginDesc<Connector> connectorDescPluginPath = new PluginDesc<>(Connector.class, snaphotVersion, pluginLoader);    PluginDesc<Connector> connectorDescClasspath = new PluginDesc<>(Connector.class, snaphotVersion, systemLoader);    assertEquals(connectorDescPluginPath, connectorDescClasspath);    assertEquals(connectorDescPluginPath.hashCode(), connectorDescClasspath.hashCode());    PluginDesc<Converter> converterDescPluginPath = new PluginDesc<>(Converter.class, noVersion, pluginLoader);    PluginDesc<Converter> converterDescClasspath = new PluginDesc<>(Converter.class, noVersion, systemLoader);    assertEquals(converterDescPluginPath, converterDescClasspath);    assertEquals(converterDescPluginPath.hashCode(), converterDescClasspath.hashCode());    PluginDesc<Transformation> transformDescPluginPath = new PluginDesc<>(Transformation.class, null, pluginLoader);    PluginDesc<Transformation> transformDescClasspath = new PluginDesc<>(Transformation.class, noVersion, pluginLoader);    assertNotEquals(transformDescPluginPath, transformDescClasspath);}
f11158
0
shouldInstantiateAndConfigureInternalConverters
public void kafkatest_f11166_0()
{    instantiateAndConfigureInternalConverter(WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);    // Validate schemas.enable is defaulted to false for internal converter    assertEquals(false, internalConverter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));    // Validate internal converter properties can still be set    assertEquals("bar1", internalConverter.configs.get("extra.config"));    instantiateAndConfigureInternalConverter(WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);    // Validate schemas.enable is defaulted to false for internal converter    assertEquals(false, internalConverter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));    // Validate internal converter properties can still be set    assertEquals("bar2", internalConverter.configs.get("extra.config"));}
f11166
0
shouldInstantiateAndConfigureExplicitlySetHeaderConverterWithCurrentClassLoader
public void kafkatest_f11167_0()
{    assertNotNull(props.get(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG));    HeaderConverter headerConverter = plugins.newHeaderConverter(config, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);    assertNotNull(headerConverter);    assertTrue(headerConverter instanceof TestHeaderConverter);    this.headerConverter = (TestHeaderConverter) headerConverter;    // Validate extra configs got passed through to overridden converters    assertConverterType(ConverterType.HEADER, this.headerConverter.configs);    assertEquals("baz", this.headerConverter.configs.get("extra.config"));    headerConverter = plugins.newHeaderConverter(config, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);    assertNotNull(headerConverter);    assertTrue(headerConverter instanceof TestHeaderConverter);    this.headerConverter = (TestHeaderConverter) headerConverter;    // Validate extra configs got passed through to overridden converters    assertConverterType(ConverterType.HEADER, this.headerConverter.configs);    assertEquals("baz", this.headerConverter.configs.get("extra.config"));}
f11167
0
shouldInstantiateAndConfigureConnectRestExtension
public void kafkatest_f11168_0()
{    props.put(WorkerConfig.REST_EXTENSION_CLASSES_CONFIG, TestConnectRestExtension.class.getName());    createConfig();    List<ConnectRestExtension> connectRestExtensions = plugins.newPlugins(config.getList(WorkerConfig.REST_EXTENSION_CLASSES_CONFIG), config, ConnectRestExtension.class);    assertNotNull(connectRestExtensions);    assertEquals("One Rest Extension expected", 1, connectRestExtensions.size());    assertNotNull(connectRestExtensions.get(0));    assertTrue("Should be instance of TestConnectRestExtension", connectRestExtensions.get(0) instanceof TestConnectRestExtension);    assertNotNull(((TestConnectRestExtension) connectRestExtensions.get(0)).configs);    assertEquals(config.originals(), ((TestConnectRestExtension) connectRestExtensions.get(0)).configs);}
f11168
0
configure
public void kafkatest_f11176_0(Map<String, ?> configs, boolean isKey)
{    this.configs = configs;}
f11176
0
fromConnectData
public byte[] kafkatest_f11177_0(String topic, Schema schema, Object value)
{    return new byte[0];}
f11177
0
toConnectData
public SchemaAndValue kafkatest_f11178_0(String topic, byte[] value)
{    return null;}
f11178
0
setUp
public void kafkatest_f11189_0() throws Exception
{    pluginPath = rootDir.newFolder("plugins").toPath().toRealPath();}
f11189
0
testJavaLibraryClasses
public void kafkatest_f11190_0()
{    assertFalse(PluginUtils.shouldLoadInIsolation("java."));    assertFalse(PluginUtils.shouldLoadInIsolation("java.lang.Object"));    assertFalse(PluginUtils.shouldLoadInIsolation("java.lang.String"));    assertFalse(PluginUtils.shouldLoadInIsolation("java.util.HashMap$Entry"));    assertFalse(PluginUtils.shouldLoadInIsolation("java.io.Serializable"));    assertFalse(PluginUtils.shouldLoadInIsolation("javax.rmi."));    assertFalse(PluginUtils.shouldLoadInIsolation("javax.management.loading.ClassLoaderRepository"));    assertFalse(PluginUtils.shouldLoadInIsolation("org.omg.CORBA."));    assertFalse(PluginUtils.shouldLoadInIsolation("org.omg.CORBA.Object"));    assertFalse(PluginUtils.shouldLoadInIsolation("org.w3c.dom."));    assertFalse(PluginUtils.shouldLoadInIsolation("org.w3c.dom.traversal.TreeWalker"));    assertFalse(PluginUtils.shouldLoadInIsolation("org.xml.sax."));    assertFalse(PluginUtils.shouldLoadInIsolation("org.xml.sax.EntityResolver"));}
f11190
0
testThirdPartyClasses
public void kafkatest_f11191_0()
{    assertFalse(PluginUtils.shouldLoadInIsolation("org.slf4j."));    assertFalse(PluginUtils.shouldLoadInIsolation("org.slf4j.LoggerFactory"));}
f11191
0
testOrderOfPluginUrlsWithJars
public void kafkatest_f11199_0() throws Exception
{    createBasicDirectoryLayout();    // Here this method is just used to create the files. The result is not used.    createBasicExpectedUrls();    List<Path> actual = PluginUtils.pluginUrls(pluginPath);    // 'simple-transform.jar' is created first. In many cases, without sorting within the    // PluginUtils, this jar will be placed before 'another-transform.jar'. However this is    // not guaranteed because a DirectoryStream does not maintain a certain order in its    // results. Besides this test case, sorted order in every call to assertUrls below.    int i = Arrays.toString(actual.toArray()).indexOf("another-transform.jar");    int j = Arrays.toString(actual.toArray()).indexOf("simple-transform.jar");    assertTrue(i < j);}
f11199
0
testPluginUrlsWithZips
public void kafkatest_f11200_0() throws Exception
{    createBasicDirectoryLayout();    List<Path> expectedUrls = new ArrayList<>();    expectedUrls.add(Files.createFile(pluginPath.resolve("connectorA/my-sink.zip")));    expectedUrls.add(Files.createFile(pluginPath.resolve("connectorB/a-source.zip")));    expectedUrls.add(Files.createFile(pluginPath.resolve("transformC/simple-transform.zip")));    expectedUrls.add(Files.createFile(pluginPath.resolve("transformC/deps/another-transform.zip")));    assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));}
f11200
0
testPluginUrlsWithClasses
public void kafkatest_f11201_0() throws Exception
{    Files.createDirectories(pluginPath.resolve("org/apache/kafka/converters"));    Files.createDirectories(pluginPath.resolve("com/mycompany/transforms"));    Files.createDirectories(pluginPath.resolve("edu/research/connectors"));    Files.createFile(pluginPath.resolve("org/apache/kafka/converters/README.txt"));    Files.createFile(pluginPath.resolve("org/apache/kafka/converters/AlienFormat.class"));    Files.createDirectories(pluginPath.resolve("com/mycompany/transforms/Blackhole.class"));    Files.createDirectories(pluginPath.resolve("edu/research/connectors/HalSink.class"));    List<Path> expectedUrls = new ArrayList<>();    expectedUrls.add(pluginPath);    assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));}
f11201
0
currentMetricValue
public Object kafkatest_f11209_0(MetricGroup metricGroup, String name)
{    return currentMetricValue(this, metricGroup, name);}
f11209
0
currentMetricValueAsDouble
public double kafkatest_f11210_0(MetricGroup metricGroup, String name)
{    Object value = currentMetricValue(metricGroup, name);    return value instanceof Double ? ((Double) value).doubleValue() : Double.NaN;}
f11210
0
currentMetricValueAsString
public String kafkatest_f11211_0(MetricGroup metricGroup, String name)
{    Object value = currentMetricValue(metricGroup, name);    return value instanceof String ? (String) value : null;}
f11211
0
close
public void kafkatest_f11219_0()
{// do nothing}
f11219
0
currentMetricValue
public Object kafkatest_f11220_0(MetricName metricName)
{    KafkaMetric metric = metricsByName.get(metricName);    return metric != null ? metric.metricValue() : null;}
f11220
0
testToStringIsLowerCase
public void kafkatest_f11221_0()
{    for (ConnectorType ct : ConnectorType.values()) {        String shouldBeLower = ct.toString();        assertFalse(shouldBeLower.isEmpty());        for (Character c : shouldBeLower.toCharArray()) {            assertTrue(Character.isLowerCase(c));        }    }}
f11221
0
testValidateConfigWithNonExistentAlias
public void kafkatest_f11229_0() throws Throwable
{    herder.validateConnectorConfig(EasyMock.eq(props));    PowerMock.expectLastCall().andAnswer((IAnswer<ConfigInfos>) () -> {        ConfigDef connectorConfigDef = ConnectorConfig.configDef();        List<ConfigValue> connectorConfigValues = connectorConfigDef.validate(props);        Connector connector = new ConnectorPluginsResourceTestConnector();        Config config = connector.validate(props);        ConfigDef configDef = connector.config();        Map<String, ConfigDef.ConfigKey> configKeys = configDef.configKeys();        List<ConfigValue> configValues = config.configValues();        Map<String, ConfigDef.ConfigKey> resultConfigKeys = new HashMap<>(configKeys);        resultConfigKeys.putAll(connectorConfigDef.configKeys());        configValues.addAll(connectorConfigValues);        return AbstractHerder.generateResult(ConnectorPluginsResourceTestConnector.class.getName(), resultConfigKeys, configValues, Collections.singletonList("Test"));    });    PowerMock.replayAll();    connectorPluginsResource.validateConfigs("ConnectorPluginsTest", props);    PowerMock.verifyAll();}
f11229
0
testListConnectorPlugins
public void kafkatest_f11230_0() throws Exception
{    expectPlugins();    Set<ConnectorPluginInfo> connectorPlugins = new HashSet<>(connectorPluginsResource.listConnectorPlugins());    assertFalse(connectorPlugins.contains(newInfo(Connector.class, "0.0")));    assertFalse(connectorPlugins.contains(newInfo(SourceConnector.class, "0.0")));    assertFalse(connectorPlugins.contains(newInfo(SinkConnector.class, "0.0")));    assertFalse(connectorPlugins.contains(newInfo(VerifiableSourceConnector.class)));    assertFalse(connectorPlugins.contains(newInfo(VerifiableSinkConnector.class)));    assertFalse(connectorPlugins.contains(newInfo(MockSourceConnector.class)));    assertFalse(connectorPlugins.contains(newInfo(MockSinkConnector.class)));    assertFalse(connectorPlugins.contains(newInfo(MockConnector.class)));    assertFalse(connectorPlugins.contains(newInfo(SchemaSourceConnector.class)));    assertTrue(connectorPlugins.contains(newInfo(ConnectorPluginsResourceTestConnector.class)));    PowerMock.verifyAll();}
f11230
0
testConnectorPluginsIncludesTypeAndVersionInformation
public void kafkatest_f11231_0() throws Exception
{    expectPlugins();    ConnectorPluginInfo sinkInfo = newInfo(TestSinkConnector.class);    ConnectorPluginInfo sourceInfo = newInfo(TestSourceConnector.class);    ConnectorPluginInfo unknownInfo = newInfo(ConnectorPluginsResourceTestConnector.class);    assertEquals(ConnectorType.SINK, sinkInfo.type());    assertEquals(ConnectorType.SOURCE, sourceInfo.type());    assertEquals(ConnectorType.UNKNOWN, unknownInfo.type());    assertEquals(TestSinkConnector.VERSION, sinkInfo.version());    assertEquals(TestSourceConnector.VERSION, sourceInfo.version());    final ObjectMapper objectMapper = new ObjectMapper();    String serializedSink = objectMapper.writeValueAsString(ConnectorType.SINK);    String serializedSource = objectMapper.writeValueAsString(ConnectorType.SOURCE);    String serializedUnknown = objectMapper.writeValueAsString(ConnectorType.UNKNOWN);    assertTrue(serializedSink.contains("sink"));    assertTrue(serializedSource.contains("source"));    assertTrue(serializedUnknown.contains("unknown"));    assertEquals(ConnectorType.SINK, objectMapper.readValue(serializedSink, ConnectorType.class));    assertEquals(ConnectorType.SOURCE, objectMapper.readValue(serializedSource, ConnectorType.class));    assertEquals(ConnectorType.UNKNOWN, objectMapper.readValue(serializedUnknown, ConnectorType.class));}
f11231
0
validValues
public List<Object> kafkatest_f11241_0(String name, Map<String, Object> parsedConfig)
{    return Arrays.asList(1, 2, 3);}
f11241
0
visible
public boolean kafkatest_f11242_0(String name, Map<String, Object> parsedConfig)
{    return true;}
f11242
0
validValues
public List<Object> kafkatest_f11243_0(String name, Map<String, Object> parsedConfig)
{    return Arrays.asList("a", "b", "c");}
f11243
0
testExpandConnectorsWithConnectorNotFound
public void kafkatest_f11251_0() throws Throwable
{    EasyMock.expect(herder.connectors()).andReturn(Arrays.asList(CONNECTOR2_NAME, CONNECTOR_NAME));    ConnectorStateInfo connector = EasyMock.mock(ConnectorStateInfo.class);    ConnectorStateInfo connector2 = EasyMock.mock(ConnectorStateInfo.class);    EasyMock.expect(herder.connectorStatus(CONNECTOR2_NAME)).andReturn(connector2);    EasyMock.expect(herder.connectorStatus(CONNECTOR_NAME)).andThrow(EasyMock.mock(NotFoundException.class));    forward = EasyMock.mock(UriInfo.class);    MultivaluedMap<String, String> queryParams = new MultivaluedHashMap<>();    queryParams.putSingle("expand", "status");    EasyMock.expect(forward.getQueryParameters()).andReturn(queryParams).anyTimes();    EasyMock.replay(forward);    PowerMock.replayAll();    Map<String, Map<String, Object>> expanded = (Map<String, Map<String, Object>>) connectorsResource.listConnectors(forward, NULL_HEADERS).getEntity();    // Ordering isn't guaranteed, compare sets    assertEquals(Collections.singleton(CONNECTOR2_NAME), expanded.keySet());    assertEquals(connector2, expanded.get(CONNECTOR2_NAME).get("status"));    PowerMock.verifyAll();}
f11251
0
testCreateConnector
public void kafkatest_f11252_0() throws Throwable
{    CreateConnectorRequest body = new CreateConnectorRequest(CONNECTOR_NAME, Collections.singletonMap(ConnectorConfig.NAME_CONFIG, CONNECTOR_NAME));    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.putConnectorConfig(EasyMock.eq(CONNECTOR_NAME), EasyMock.eq(body.config()), EasyMock.eq(false), EasyMock.capture(cb));    expectAndCallbackResult(cb, new Herder.Created<>(true, new ConnectorInfo(CONNECTOR_NAME, CONNECTOR_CONFIG, CONNECTOR_TASK_NAMES, ConnectorType.SOURCE)));    PowerMock.replayAll();    connectorsResource.createConnector(FORWARD, NULL_HEADERS, body);    PowerMock.verifyAll();}
f11252
0
testCreateConnectorNotLeader
public void kafkatest_f11253_0() throws Throwable
{    CreateConnectorRequest body = new CreateConnectorRequest(CONNECTOR_NAME, Collections.singletonMap(ConnectorConfig.NAME_CONFIG, CONNECTOR_NAME));    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.putConnectorConfig(EasyMock.eq(CONNECTOR_NAME), EasyMock.eq(body.config()), EasyMock.eq(false), EasyMock.capture(cb));    expectAndCallbackNotLeaderException(cb);    // Should forward request    EasyMock.expect(RestClient.httpRequest(EasyMock.eq("http://leader:8083/connectors?forward=false"), EasyMock.eq("POST"), EasyMock.isNull(), EasyMock.eq(body), EasyMock.<TypeReference>anyObject(), EasyMock.anyObject(WorkerConfig.class))).andReturn(new RestClient.HttpResponse<>(201, new HashMap<String, String>(), new ConnectorInfo(CONNECTOR_NAME, CONNECTOR_CONFIG, CONNECTOR_TASK_NAMES, ConnectorType.SOURCE)));    PowerMock.replayAll();    connectorsResource.createConnector(FORWARD, NULL_HEADERS, body);    PowerMock.verifyAll();}
f11253
0
testDeleteConnectorNotLeader
public void kafkatest_f11261_0() throws Throwable
{    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.deleteConnectorConfig(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(cb));    expectAndCallbackNotLeaderException(cb);    // Should forward request    EasyMock.expect(RestClient.httpRequest("http://leader:8083/connectors/" + CONNECTOR_NAME + "?forward=false", "DELETE", NULL_HEADERS, null, null, null)).andReturn(new RestClient.HttpResponse<>(204, new HashMap<String, String>(), null));    PowerMock.replayAll();    connectorsResource.destroyConnector(CONNECTOR_NAME, NULL_HEADERS, FORWARD);    PowerMock.verifyAll();}
f11261
0
testDeleteConnectorNotFound
public void kafkatest_f11262_0() throws Throwable
{    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.deleteConnectorConfig(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(cb));    expectAndCallbackException(cb, new NotFoundException("not found"));    PowerMock.replayAll();    connectorsResource.destroyConnector(CONNECTOR_NAME, NULL_HEADERS, FORWARD);    PowerMock.verifyAll();}
f11262
0
testGetConnector
public void kafkatest_f11263_0() throws Throwable
{    final Capture<Callback<ConnectorInfo>> cb = Capture.newInstance();    herder.connectorInfo(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(cb));    expectAndCallbackResult(cb, new ConnectorInfo(CONNECTOR_NAME, CONNECTOR_CONFIG, CONNECTOR_TASK_NAMES, ConnectorType.SOURCE));    PowerMock.replayAll();    ConnectorInfo connInfo = connectorsResource.getConnector(CONNECTOR_NAME, NULL_HEADERS, FORWARD);    assertEquals(new ConnectorInfo(CONNECTOR_NAME, CONNECTOR_CONFIG, CONNECTOR_TASK_NAMES, ConnectorType.SOURCE), connInfo);    PowerMock.verifyAll();}
f11263
0
testPutConnectorConfigNameMismatch
public void kafkatest_f11271_0() throws Throwable
{    Map<String, String> connConfig = new HashMap<>(CONNECTOR_CONFIG);    connConfig.put(ConnectorConfig.NAME_CONFIG, "mismatched-name");    connectorsResource.putConnectorConfig(CONNECTOR_NAME, NULL_HEADERS, FORWARD, connConfig);}
f11271
0
testCreateConnectorConfigNameMismatch
public void kafkatest_f11272_0() throws Throwable
{    Map<String, String> connConfig = new HashMap<>();    connConfig.put(ConnectorConfig.NAME_CONFIG, "mismatched-name");    CreateConnectorRequest request = new CreateConnectorRequest(CONNECTOR_NAME, connConfig);    connectorsResource.createConnector(FORWARD, NULL_HEADERS, request);}
f11272
0
testGetConnectorTaskConfigs
public void kafkatest_f11273_0() throws Throwable
{    final Capture<Callback<List<TaskInfo>>> cb = Capture.newInstance();    herder.taskConfigs(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(cb));    expectAndCallbackResult(cb, TASK_INFOS);    PowerMock.replayAll();    List<TaskInfo> taskInfos = connectorsResource.getTaskConfigs(CONNECTOR_NAME, NULL_HEADERS, FORWARD);    assertEquals(TASK_INFOS, taskInfos);    PowerMock.verifyAll();}
f11273
0
testRestartTaskLeaderRedirect
public void kafkatest_f11281_0() throws Throwable
{    ConnectorTaskId taskId = new ConnectorTaskId(CONNECTOR_NAME, 0);    final Capture<Callback<Void>> cb = Capture.newInstance();    herder.restartTask(EasyMock.eq(taskId), EasyMock.capture(cb));    expectAndCallbackNotLeaderException(cb);    EasyMock.expect(RestClient.httpRequest(EasyMock.eq("http://leader:8083/connectors/" + CONNECTOR_NAME + "/tasks/0/restart?forward=true"), EasyMock.eq("POST"), EasyMock.isNull(), EasyMock.isNull(), EasyMock.<TypeReference>anyObject(), EasyMock.anyObject(WorkerConfig.class))).andReturn(new RestClient.HttpResponse<>(202, new HashMap<String, String>(), null));    PowerMock.replayAll();    connectorsResource.restartTask(CONNECTOR_NAME, 0, NULL_HEADERS, null);    PowerMock.verifyAll();}
f11281
0
testRestartTaskOwnerRedirect
public void kafkatest_f11282_0() throws Throwable
{    ConnectorTaskId taskId = new ConnectorTaskId(CONNECTOR_NAME, 0);    final Capture<Callback<Void>> cb = Capture.newInstance();    herder.restartTask(EasyMock.eq(taskId), EasyMock.capture(cb));    String ownerUrl = "http://owner:8083";    expectAndCallbackException(cb, new NotAssignedException("not owner test", ownerUrl));    EasyMock.expect(RestClient.httpRequest(EasyMock.eq("http://owner:8083/connectors/" + CONNECTOR_NAME + "/tasks/0/restart?forward=false"), EasyMock.eq("POST"), EasyMock.isNull(), EasyMock.isNull(), EasyMock.<TypeReference>anyObject(), EasyMock.anyObject(WorkerConfig.class))).andReturn(new RestClient.HttpResponse<>(202, new HashMap<String, String>(), null));    PowerMock.replayAll();    connectorsResource.restartTask(CONNECTOR_NAME, 0, NULL_HEADERS, true);    PowerMock.verifyAll();}
f11282
0
expectAndCallbackResult
private void kafkatest_f11283_0(final Capture<Callback<T>> cb, final T value)
{    PowerMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            cb.getValue().onCompletion(null, value);            return null;        }    });}
f11283
0
baseWorkerProps
private Map<String, String> kafkatest_f11291_0()
{    Map<String, String> workerProps = new HashMap<>();    workerProps.put(DistributedConfig.STATUS_STORAGE_TOPIC_CONFIG, "status-topic");    workerProps.put(DistributedConfig.CONFIG_TOPIC_CONFIG, "config-topic");    workerProps.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");    workerProps.put(DistributedConfig.GROUP_ID_CONFIG, "connect-test-group");    workerProps.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    workerProps.put(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    workerProps.put(WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    workerProps.put(WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    workerProps.put(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG, "connect-offsets");    workerProps.put(WorkerConfig.LISTENERS_CONFIG, "HTTP://localhost:0");    return workerProps;}
f11291
0
testCORSEnabled
public void kafkatest_f11292_0() throws IOException
{    checkCORSRequest("*", "http://bar.com", "http://bar.com", "PUT");}
f11292
0
testCORSDisabled
public void kafkatest_f11293_0() throws IOException
{    checkCORSRequest("", "http://bar.com", null, null);}
f11293
0
testCreateClientSideSslContextFactory
public void kafkatest_f11301_0()
{    Map<String, String> configMap = new HashMap<>(DEFAULT_CONFIG);    configMap.put("ssl.keystore.location", "/path/to/keystore");    configMap.put("ssl.keystore.password", "123456");    configMap.put("ssl.key.password", "123456");    configMap.put("ssl.truststore.location", "/path/to/truststore");    configMap.put("ssl.truststore.password", "123456");    configMap.put("ssl.provider", "SunJSSE");    configMap.put("ssl.cipher.suites", "SSL_RSA_WITH_RC4_128_SHA,SSL_RSA_WITH_RC4_128_MD5");    configMap.put("ssl.secure.random.implementation", "SHA1PRNG");    configMap.put("ssl.client.auth", "required");    configMap.put("ssl.endpoint.identification.algorithm", "HTTPS");    configMap.put("ssl.keystore.type", "JKS");    configMap.put("ssl.protocol", "TLS");    configMap.put("ssl.truststore.type", "JKS");    configMap.put("ssl.enabled.protocols", "TLSv1.2,TLSv1.1,TLSv1");    configMap.put("ssl.keymanager.algorithm", "SunX509");    configMap.put("ssl.trustmanager.algorithm", "PKIX");    DistributedConfig config = new DistributedConfig(configMap);    SslContextFactory ssl = SSLUtils.createClientSideSslContextFactory(config);    Assert.assertEquals("file:///path/to/keystore", ssl.getKeyStorePath());    Assert.assertEquals("file:///path/to/truststore", ssl.getTrustStorePath());    Assert.assertEquals("SunJSSE", ssl.getProvider());    Assert.assertArrayEquals(new String[] { "SSL_RSA_WITH_RC4_128_SHA", "SSL_RSA_WITH_RC4_128_MD5" }, ssl.getIncludeCipherSuites());    Assert.assertEquals("SHA1PRNG", ssl.getSecureRandomAlgorithm());    Assert.assertFalse(ssl.getNeedClientAuth());    Assert.assertFalse(ssl.getWantClientAuth());    Assert.assertEquals("JKS", ssl.getKeyStoreType());    Assert.assertEquals("JKS", ssl.getTrustStoreType());    Assert.assertEquals("TLS", ssl.getProtocol());    Assert.assertArrayEquals(new String[] { "TLSv1.2", "TLSv1.1", "TLSv1" }, ssl.getIncludeProtocols());    Assert.assertEquals("SunX509", ssl.getKeyManagerFactoryAlgorithm());    Assert.assertEquals("PKIX", ssl.getTrustManagerFactoryAlgorithm());}
f11301
0
testCreateServerSideSslContextFactoryDefaultValues
public void kafkatest_f11302_0()
{    Map<String, String> configMap = new HashMap<>(DEFAULT_CONFIG);    configMap.put(StandaloneConfig.OFFSET_STORAGE_FILE_FILENAME_CONFIG, "/tmp/offset/file");    configMap.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    configMap.put(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    configMap.put(WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    configMap.put(WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    configMap.put("ssl.keystore.location", "/path/to/keystore");    configMap.put("ssl.keystore.password", "123456");    configMap.put("ssl.key.password", "123456");    configMap.put("ssl.truststore.location", "/path/to/truststore");    configMap.put("ssl.truststore.password", "123456");    configMap.put("ssl.provider", "SunJSSE");    configMap.put("ssl.cipher.suites", "SSL_RSA_WITH_RC4_128_SHA,SSL_RSA_WITH_RC4_128_MD5");    configMap.put("ssl.secure.random.implementation", "SHA1PRNG");    DistributedConfig config = new DistributedConfig(configMap);    SslContextFactory ssl = SSLUtils.createServerSideSslContextFactory(config);    Assert.assertEquals(SslConfigs.DEFAULT_SSL_KEYSTORE_TYPE, ssl.getKeyStoreType());    Assert.assertEquals(SslConfigs.DEFAULT_SSL_TRUSTSTORE_TYPE, ssl.getTrustStoreType());    Assert.assertEquals(SslConfigs.DEFAULT_SSL_PROTOCOL, ssl.getProtocol());    Assert.assertArrayEquals(Arrays.asList(SslConfigs.DEFAULT_SSL_ENABLED_PROTOCOLS.split("\\s*,\\s*")).toArray(), ssl.getIncludeProtocols());    Assert.assertEquals(SslConfigs.DEFAULT_SSL_KEYMANGER_ALGORITHM, ssl.getKeyManagerFactoryAlgorithm());    Assert.assertEquals(SslConfigs.DEFAULT_SSL_TRUSTMANAGER_ALGORITHM, ssl.getTrustManagerFactoryAlgorithm());    Assert.assertFalse(ssl.getNeedClientAuth());    Assert.assertFalse(ssl.getWantClientAuth());}
f11302
0
testCreateClientSideSslContextFactoryDefaultValues
public void kafkatest_f11303_0()
{    Map<String, String> configMap = new HashMap<>(DEFAULT_CONFIG);    configMap.put(StandaloneConfig.OFFSET_STORAGE_FILE_FILENAME_CONFIG, "/tmp/offset/file");    configMap.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    configMap.put(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    configMap.put(WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    configMap.put(WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.json.JsonConverter");    configMap.put("ssl.keystore.location", "/path/to/keystore");    configMap.put("ssl.keystore.password", "123456");    configMap.put("ssl.key.password", "123456");    configMap.put("ssl.truststore.location", "/path/to/truststore");    configMap.put("ssl.truststore.password", "123456");    configMap.put("ssl.provider", "SunJSSE");    configMap.put("ssl.cipher.suites", "SSL_RSA_WITH_RC4_128_SHA,SSL_RSA_WITH_RC4_128_MD5");    configMap.put("ssl.secure.random.implementation", "SHA1PRNG");    DistributedConfig config = new DistributedConfig(configMap);    SslContextFactory ssl = SSLUtils.createClientSideSslContextFactory(config);    Assert.assertEquals(SslConfigs.DEFAULT_SSL_KEYSTORE_TYPE, ssl.getKeyStoreType());    Assert.assertEquals(SslConfigs.DEFAULT_SSL_TRUSTSTORE_TYPE, ssl.getTrustStoreType());    Assert.assertEquals(SslConfigs.DEFAULT_SSL_PROTOCOL, ssl.getProtocol());    Assert.assertArrayEquals(Arrays.asList(SslConfigs.DEFAULT_SSL_ENABLED_PROTOCOLS.split("\\s*,\\s*")).toArray(), ssl.getIncludeProtocols());    Assert.assertEquals(SslConfigs.DEFAULT_SSL_KEYMANGER_ALGORITHM, ssl.getKeyManagerFactoryAlgorithm());    Assert.assertEquals(SslConfigs.DEFAULT_SSL_TRUSTMANAGER_ALGORITHM, ssl.getTrustManagerFactoryAlgorithm());    Assert.assertFalse(ssl.getNeedClientAuth());    Assert.assertFalse(ssl.getWantClientAuth());}
f11303
0
testCreateConnectorFailedCustomValidation
public void kafkatest_f11311_0()
{    connector = PowerMock.createMock(BogusSourceConnector.class);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    ConfigDef configDef = new ConfigDef();    configDef.define("foo.bar", ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, "foo.bar doc");    EasyMock.expect(connectorMock.config()).andReturn(configDef);    ConfigValue validatedValue = new ConfigValue("foo.bar");    validatedValue.addErrorMessage("Failed foo.bar validation");    Map<String, String> config = connectorConfig(SourceSink.SOURCE);    EasyMock.expect(connectorMock.validate(config)).andReturn(new Config(singletonList(validatedValue)));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    createCallback.onCompletion(EasyMock.<BadRequestException>anyObject(), EasyMock.<Herder.Created<ConnectorInfo>>isNull());    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, config, false, createCallback);    PowerMock.verifyAll();}
f11311
0
testCreateConnectorAlreadyExists
public void kafkatest_f11312_0() throws Exception
{    connector = PowerMock.createMock(BogusSourceConnector.class);    // First addition should succeed    expectAdd(SourceSink.SOURCE);    Map<String, String> config = connectorConfig(SourceSink.SOURCE);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    expectConfigValidation(connectorMock, true, config, config);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(2);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    // No new connector is created    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    // Second should fail    createCallback.onCompletion(EasyMock.<AlreadyExistsException>anyObject(), EasyMock.<Herder.Created<ConnectorInfo>>isNull());    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, config, false, createCallback);    herder.putConnectorConfig(CONNECTOR_NAME, config, false, createCallback);    PowerMock.verifyAll();}
f11312
0
testCreateSinkConnector
public void kafkatest_f11313_0() throws Exception
{    connector = PowerMock.createMock(BogusSinkConnector.class);    expectAdd(SourceSink.SINK);    Map<String, String> config = connectorConfig(SourceSink.SINK);    Connector connectorMock = PowerMock.createMock(SinkConnector.class);    expectConfigValidation(connectorMock, true, config);    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, config, false, createCallback);    PowerMock.verifyAll();}
f11313
0
testPutConnectorConfig
public void kafkatest_f11321_0() throws Exception
{    Map<String, String> connConfig = connectorConfig(SourceSink.SOURCE);    Map<String, String> newConnConfig = new HashMap<>(connConfig);    newConnConfig.put("foo", "bar");    Callback<Map<String, String>> connectorConfigCb = PowerMock.createMock(Callback.class);    Callback<Herder.Created<ConnectorInfo>> putConnectorConfigCb = PowerMock.createMock(Callback.class);    // Create    connector = PowerMock.createMock(BogusSourceConnector.class);    expectAdd(SourceSink.SOURCE);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    expectConfigValidation(connectorMock, true, connConfig);    // Should get first config    connectorConfigCb.onCompletion(null, connConfig);    EasyMock.expectLastCall();    // Update config, which requires stopping and restarting    worker.stopConnector(CONNECTOR_NAME);    EasyMock.expectLastCall().andReturn(true);    Capture<Map<String, String>> capturedConfig = EasyMock.newCapture();    worker.startConnector(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(capturedConfig), EasyMock.<ConnectorContext>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    EasyMock.expectLastCall().andReturn(true);    EasyMock.expect(worker.isRunning(CONNECTOR_NAME)).andReturn(true);    // Generate same task config, which should result in no additional action to restart tasks    EasyMock.expect(worker.connectorTaskConfigs(CONNECTOR_NAME, new SourceConnectorConfig(plugins, newConnConfig))).andReturn(singletonList(taskConfig(SourceSink.SOURCE)));    worker.isSinkConnector(CONNECTOR_NAME);    EasyMock.expectLastCall().andReturn(false);    ConnectorInfo newConnInfo = new ConnectorInfo(CONNECTOR_NAME, newConnConfig, Arrays.asList(new ConnectorTaskId(CONNECTOR_NAME, 0)), ConnectorType.SOURCE);    putConnectorConfigCb.onCompletion(null, new Herder.Created<>(false, newConnInfo));    EasyMock.expectLastCall();    // Should get new config    expectConfigValidation(connectorMock, false, newConnConfig);    connectorConfigCb.onCompletion(null, newConnConfig);    EasyMock.expectLastCall();    EasyMock.expect(worker.getPlugins()).andReturn(plugins).anyTimes();    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, connConfig, false, createCallback);    herder.connectorConfig(CONNECTOR_NAME, connectorConfigCb);    herder.putConnectorConfig(CONNECTOR_NAME, newConnConfig, true, putConnectorConfigCb);    assertEquals("bar", capturedConfig.getValue().get("foo"));    herder.connectorConfig(CONNECTOR_NAME, connectorConfigCb);    PowerMock.verifyAll();}
f11321
0
testPutTaskConfigs
public void kafkatest_f11322_0()
{    Callback<Void> cb = PowerMock.createMock(Callback.class);    PowerMock.replayAll();    herder.putTaskConfigs(CONNECTOR_NAME, Arrays.asList(singletonMap("config", "value")), cb);    PowerMock.verifyAll();}
f11322
0
testCorruptConfig
public void kafkatest_f11323_0()
{    Map<String, String> config = new HashMap<>();    config.put(ConnectorConfig.NAME_CONFIG, CONNECTOR_NAME);    config.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, BogusSinkConnector.class.getName());    config.put(SinkConnectorConfig.TOPICS_CONFIG, TOPICS_LIST_STR);    Connector connectorMock = PowerMock.createMock(SinkConnector.class);    String error = "This is an error in your config!";    List<String> errors = new ArrayList<>(singletonList(error));    String key = "foo.invalid.key";    EasyMock.expect(connectorMock.validate(config)).andReturn(new Config(Arrays.asList(new ConfigValue(key, null, Collections.emptyList(), errors))));    ConfigDef configDef = new ConfigDef();    configDef.define(key, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, "");    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(worker.getPlugins()).andStubReturn(plugins);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    EasyMock.expect(connectorMock.config()).andStubReturn(configDef);    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    Callback<Herder.Created<ConnectorInfo>> callback = PowerMock.createMock(Callback.class);    Capture<BadRequestException> capture = Capture.newInstance();    callback.onCompletion(EasyMock.capture(capture), EasyMock.isNull(Herder.Created.class));    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, config, true, callback);    assertEquals(capture.getValue().getMessage(), "Connector configuration is invalid and contains the following 1 error(s):\n" + error + "\n" + "You can also find the above list of errors at the endpoint `/{connectorType}/config/validate`");    PowerMock.verifyAll();}
f11323
0
currentStateIsNullWhenNotInitialized
public void kafkatest_f11331_0()
{    assertNull(tracker.currentState());}
f11331
0
currentState
public void kafkatest_f11332_0()
{    for (State state : State.values()) {        tracker.changeState(state, time.milliseconds());        assertEquals(state, tracker.currentState());    }}
f11332
0
calculateDurations
public void kafkatest_f11333_0()
{    tracker.changeState(State.UNASSIGNED, time.milliseconds());    time.sleep(1000L);    assertEquals(1.0d, tracker.durationRatio(State.UNASSIGNED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.RUNNING, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.PAUSED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.FAILED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.DESTROYED, time.milliseconds()), DELTA);    tracker.changeState(State.RUNNING, time.milliseconds());    time.sleep(3000L);    assertEquals(0.25d, tracker.durationRatio(State.UNASSIGNED, time.milliseconds()), DELTA);    assertEquals(0.75d, tracker.durationRatio(State.RUNNING, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.PAUSED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.FAILED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.DESTROYED, time.milliseconds()), DELTA);    tracker.changeState(State.PAUSED, time.milliseconds());    time.sleep(4000L);    assertEquals(0.125d, tracker.durationRatio(State.UNASSIGNED, time.milliseconds()), DELTA);    assertEquals(0.375d, tracker.durationRatio(State.RUNNING, time.milliseconds()), DELTA);    assertEquals(0.500d, tracker.durationRatio(State.PAUSED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.FAILED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.DESTROYED, time.milliseconds()), DELTA);    tracker.changeState(State.RUNNING, time.milliseconds());    time.sleep(8000L);    assertEquals(0.0625d, tracker.durationRatio(State.UNASSIGNED, time.milliseconds()), DELTA);    assertEquals(0.6875d, tracker.durationRatio(State.RUNNING, time.milliseconds()), DELTA);    assertEquals(0.2500d, tracker.durationRatio(State.PAUSED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.FAILED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.DESTROYED, time.milliseconds()), DELTA);    tracker.changeState(State.FAILED, time.milliseconds());    time.sleep(16000L);    assertEquals(0.03125d, tracker.durationRatio(State.UNASSIGNED, time.milliseconds()), DELTA);    assertEquals(0.34375d, tracker.durationRatio(State.RUNNING, time.milliseconds()), DELTA);    assertEquals(0.12500d, tracker.durationRatio(State.PAUSED, time.milliseconds()), DELTA);    assertEquals(0.50000d, tracker.durationRatio(State.FAILED, time.milliseconds()), DELTA);    assertEquals(0.0d, tracker.durationRatio(State.DESTROYED, time.milliseconds()), DELTA);}
f11333
0
config
public ConfigDef kafkatest_f11345_0()
{    return new ConfigDef().define("required", ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, "required docs").define("optional", ConfigDef.Type.STRING, "defaultVal", ConfigDef.Importance.HIGH, "optional docs");}
f11345
0
testEmbeddedConfigCast
public void kafkatest_f11346_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", Cast.Value.class.getName());    connProps.put("transforms.example.spec", "int8");    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
f11346
0
testEmbeddedConfigExtractField
public void kafkatest_f11347_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", ExtractField.Value.class.getName());    connProps.put("transforms.example.field", "field");    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
f11347
0
testEmbeddedConfigTimestampConverter
public void kafkatest_f11355_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", TimestampConverter.Value.class.getName());    connProps.put("transforms.example.target.type", "unix");    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
f11355
0
testEmbeddedConfigTimestampRouter
public void kafkatest_f11356_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", TimestampRouter.class.getName());    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
f11356
0
testEmbeddedConfigValueToKey
public void kafkatest_f11357_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", ValueToKey.class.getName());    connProps.put("transforms.example.fields", "field");    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
f11357
0
get
public ConfigData kafkatest_f11366_0(String path, Set<String> keys)
{    if (path.equals(TEST_PATH)) {        if (keys.contains(TEST_KEY)) {            return new ConfigData(Collections.singletonMap(TEST_KEY, TEST_RESULT));        } else if (keys.contains(TEST_KEY_WITH_TTL)) {            return new ConfigData(Collections.singletonMap(TEST_KEY_WITH_TTL, TEST_RESULT_WITH_TTL), 1L);        } else if (keys.contains(TEST_KEY_WITH_LONGER_TTL)) {            return new ConfigData(Collections.singletonMap(TEST_KEY_WITH_LONGER_TTL, TEST_RESULT_WITH_LONGER_TTL), 10L);        }    }    return new ConfigData(Collections.emptyMap());}
f11366
0
subscribe
public void kafkatest_f11367_0(String path, Set<String> keys, ConfigChangeCallback callback)
{    throw new UnsupportedOperationException();}
f11367
0
unsubscribe
public void kafkatest_f11368_0(String path, Set<String> keys)
{    throw new UnsupportedOperationException();}
f11368
0
testStartupPaused
public void kafkatest_f11377_0()
{    connector.version();    expectLastCall().andReturn(VERSION);    connector.initialize(EasyMock.notNull(ConnectorContext.class));    expectLastCall();    // connector never gets started    listener.onPause(CONNECTOR);    expectLastCall();    listener.onShutdown(CONNECTOR);    expectLastCall();    replayAll();    WorkerConnector workerConnector = new WorkerConnector(CONNECTOR, connector, ctx, metrics, listener);    workerConnector.initialize(connectorConfig);    assertInitializedMetric(workerConnector);    workerConnector.transitionTo(TargetState.PAUSED);    assertPausedMetric(workerConnector);    workerConnector.shutdown();    assertStoppedMetric(workerConnector);    verifyAll();}
f11377
0
testStartupFailure
public void kafkatest_f11378_0()
{    RuntimeException exception = new RuntimeException();    connector.version();    expectLastCall().andReturn(VERSION);    connector.initialize(EasyMock.notNull(ConnectorContext.class));    expectLastCall();    connector.start(CONFIG);    expectLastCall().andThrow(exception);    listener.onFailure(CONNECTOR, exception);    expectLastCall();    listener.onShutdown(CONNECTOR);    expectLastCall();    replayAll();    WorkerConnector workerConnector = new WorkerConnector(CONNECTOR, connector, ctx, metrics, listener);    workerConnector.initialize(connectorConfig);    assertInitializedMetric(workerConnector);    workerConnector.transitionTo(TargetState.STARTED);    assertFailedMetric(workerConnector);    workerConnector.shutdown();    assertStoppedMetric(workerConnector);    verifyAll();}
f11378
0
testShutdownFailure
public void kafkatest_f11379_0()
{    RuntimeException exception = new RuntimeException();    connector.version();    expectLastCall().andReturn(VERSION);    connector.initialize(EasyMock.notNull(ConnectorContext.class));    expectLastCall();    connector.start(CONFIG);    expectLastCall();    listener.onStartup(CONNECTOR);    expectLastCall();    connector.stop();    expectLastCall().andThrow(exception);    listener.onFailure(CONNECTOR, exception);    expectLastCall();    replayAll();    WorkerConnector workerConnector = new WorkerConnector(CONNECTOR, connector, ctx, metrics, listener);    workerConnector.initialize(connectorConfig);    assertInitializedMetric(workerConnector);    workerConnector.transitionTo(TargetState.STARTED);    assertRunningMetric(workerConnector);    workerConnector.shutdown();    assertFailedMetric(workerConnector);    verifyAll();}
f11379
0
setUp
public void kafkatest_f11387_0()
{    time = new MockTime();    Map<String, String> workerProps = new HashMap<>();    workerProps.put("key.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("value.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.key.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.value.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.key.converter.schemas.enable", "false");    workerProps.put("internal.value.converter.schemas.enable", "false");    workerProps.put("offset.storage.file.filename", "/tmp/connect.offsets");    workerConfig = new StandaloneConfig(workerProps);    pluginLoader = PowerMock.createMock(PluginClassLoader.class);    metrics = new MockConnectMetrics(time);    recordsReturnedTp1 = 0;    recordsReturnedTp3 = 0;}
f11387
0
createTask
private void kafkatest_f11388_0(TargetState initialState)
{    workerTask = new WorkerSinkTask(taskId, sinkTask, statusListener, initialState, workerConfig, ClusterConfigState.EMPTY, metrics, keyConverter, valueConverter, headerConverter, transformationChain, consumer, pluginLoader, time, RetryWithToleranceOperatorTest.NOOP_OPERATOR);}
f11388
0
tearDown
public void kafkatest_f11389_0()
{    if (metrics != null)        metrics.stop();}
f11389
0
testRequestCommit
public void kafkatest_f11397_0() throws Exception
{    createTask(initialState);    expectInitializeTask();    expectPollInitialAssignment();    expectConsumerPoll(1);    expectConversionAndTransformation(1);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    final Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    offsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    offsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    sinkTask.preCommit(offsets);    EasyMock.expectLastCall().andReturn(offsets);    final Capture<OffsetCommitCallback> callback = EasyMock.newCapture();    consumer.commitAsync(EasyMock.eq(offsets), EasyMock.capture(callback));    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callback.getValue().onComplete(offsets, null);            return null;        }    });    expectConsumerPoll(0);    sinkTask.put(Collections.<SinkRecord>emptyList());    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    workerTask.initializeAndStart();    // Initial assignment    time.sleep(30000L);    workerTask.iteration();    assertSinkMetricValue("partition-count", 2);    // First record delivered    workerTask.iteration();    assertSinkMetricValue("partition-count", 2);    assertSinkMetricValue("sink-record-read-total", 1.0);    assertSinkMetricValue("sink-record-send-total", 1.0);    assertSinkMetricValue("sink-record-active-count", 1.0);    assertSinkMetricValue("sink-record-active-count-max", 1.0);    assertSinkMetricValue("sink-record-active-count-avg", 0.333333);    assertSinkMetricValue("offset-commit-seq-no", 0.0);    assertSinkMetricValue("offset-commit-completion-total", 0.0);    assertSinkMetricValue("offset-commit-skip-total", 0.0);    assertTaskMetricValue("status", "running");    assertTaskMetricValue("running-ratio", 1.0);    assertTaskMetricValue("pause-ratio", 0.0);    assertTaskMetricValue("batch-size-max", 1.0);    assertTaskMetricValue("batch-size-avg", 0.5);    assertTaskMetricValue("offset-commit-failure-percentage", 0.0);    assertTaskMetricValue("offset-commit-success-percentage", 0.0);    // Grab the commit time prior to requesting a commit.    // This time should advance slightly after committing.    // KAFKA-8229    final long previousCommitValue = workerTask.getNextCommit();    sinkTaskContext.getValue().requestCommit();    assertTrue(sinkTaskContext.getValue().isCommitRequested());    assertNotEquals(offsets, Whitebox.<Map<TopicPartition, OffsetAndMetadata>>getInternalState(workerTask, "lastCommittedOffsets"));    time.sleep(10000L);    // triggers the commit    workerTask.iteration();    time.sleep(10000L);    // should have been cleared    assertFalse(sinkTaskContext.getValue().isCommitRequested());    assertEquals(offsets, Whitebox.<Map<TopicPartition, OffsetAndMetadata>>getInternalState(workerTask, "lastCommittedOffsets"));    assertEquals(0, workerTask.commitFailures());    // Assert the next commit time advances slightly, the amount it advances    // is the normal commit time less the two sleeps since it started each    // of those sleeps were 10 seconds.    // KAFKA-8229    assertEquals("Should have only advanced by 40 seconds", previousCommitValue + (WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_DEFAULT - 10000L * 2), workerTask.getNextCommit());    assertSinkMetricValue("partition-count", 2);    assertSinkMetricValue("sink-record-read-total", 1.0);    assertSinkMetricValue("sink-record-send-total", 1.0);    assertSinkMetricValue("sink-record-active-count", 0.0);    assertSinkMetricValue("sink-record-active-count-max", 1.0);    assertSinkMetricValue("sink-record-active-count-avg", 0.2);    assertSinkMetricValue("offset-commit-seq-no", 1.0);    assertSinkMetricValue("offset-commit-completion-total", 1.0);    assertSinkMetricValue("offset-commit-skip-total", 0.0);    assertTaskMetricValue("status", "running");    assertTaskMetricValue("running-ratio", 1.0);    assertTaskMetricValue("pause-ratio", 0.0);    assertTaskMetricValue("batch-size-max", 1.0);    assertTaskMetricValue("batch-size-avg", 0.33333);    assertTaskMetricValue("offset-commit-max-time-ms", 0.0);    assertTaskMetricValue("offset-commit-avg-time-ms", 0.0);    assertTaskMetricValue("offset-commit-failure-percentage", 0.0);    assertTaskMetricValue("offset-commit-success-percentage", 1.0);    PowerMock.verifyAll();}
f11397
0
answer
public Void kafkatest_f11398_0() throws Throwable
{    callback.getValue().onComplete(offsets, null);    return null;}
f11398
0
testPreCommit
public void kafkatest_f11399_0() throws Exception
{    createTask(initialState);    expectInitializeTask();    // iter 1    expectPollInitialAssignment();    // iter 2    expectConsumerPoll(2);    expectConversionAndTransformation(2);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    final Map<TopicPartition, OffsetAndMetadata> workerStartingOffsets = new HashMap<>();    workerStartingOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET));    workerStartingOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    final Map<TopicPartition, OffsetAndMetadata> workerCurrentOffsets = new HashMap<>();    workerCurrentOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 2));    workerCurrentOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    final Map<TopicPartition, OffsetAndMetadata> taskOffsets = new HashMap<>();    // act like FIRST_OFFSET+2 has not yet been flushed by the task    taskOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    // should be ignored because > current offset    taskOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET + 1));    // should be ignored because this partition is not assigned    taskOffsets.put(new TopicPartition(TOPIC, 3), new OffsetAndMetadata(FIRST_OFFSET));    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = new HashMap<>();    committableOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    committableOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    sinkTask.preCommit(workerCurrentOffsets);    EasyMock.expectLastCall().andReturn(taskOffsets);    // Expect extra invalid topic partition to be filtered, which causes the consumer assignment to be logged    EasyMock.expect(consumer.assignment()).andReturn(workerCurrentOffsets.keySet());    final Capture<OffsetCommitCallback> callback = EasyMock.newCapture();    consumer.commitAsync(EasyMock.eq(committableOffsets), EasyMock.capture(callback));    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callback.getValue().onComplete(committableOffsets, null);            return null;        }    });    expectConsumerPoll(0);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    workerTask.initializeAndStart();    // iter 1 -- initial assignment    workerTask.iteration();    assertEquals(workerStartingOffsets, Whitebox.<Map<TopicPartition, OffsetAndMetadata>>getInternalState(workerTask, "currentOffsets"));    // iter 2 -- deliver 2 records    workerTask.iteration();    assertEquals(workerCurrentOffsets, Whitebox.<Map<TopicPartition, OffsetAndMetadata>>getInternalState(workerTask, "currentOffsets"));    assertEquals(workerStartingOffsets, Whitebox.<Map<TopicPartition, OffsetAndMetadata>>getInternalState(workerTask, "lastCommittedOffsets"));    sinkTaskContext.getValue().requestCommit();    // iter 3 -- commit    workerTask.iteration();    assertEquals(committableOffsets, Whitebox.<Map<TopicPartition, OffsetAndMetadata>>getInternalState(workerTask, "lastCommittedOffsets"));    PowerMock.verifyAll();}
f11399
0
run
public void kafkatest_f11407_0()
{    callback.onComplete(offsets, null);    asyncCallbackRan.set(true);}
f11407
0
answer
public ConsumerRecords<byte[], byte[]> kafkatest_f11408_0() throws Throwable
{    // Rebalance always begins with revoking current partitions ...    rebalanceListener.getValue().onPartitionsRevoked(originalPartitions);    // Respond to the rebalance    Map<TopicPartition, Long> offsets = new HashMap<>();    offsets.put(TOPIC_PARTITION, rebalanceOffsets.get(TOPIC_PARTITION).offset());    offsets.put(TOPIC_PARTITION2, rebalanceOffsets.get(TOPIC_PARTITION2).offset());    offsets.put(TOPIC_PARTITION3, rebalanceOffsets.get(TOPIC_PARTITION3).offset());    sinkTaskContext.getValue().offset(offsets);    rebalanceListener.getValue().onPartitionsAssigned(rebalancedPartitions);    rebalanced.set(true);    // Run the previous async commit handler    asyncCallbackRunner.get().run();    // And prep the two records to return    long timestamp = RecordBatch.NO_TIMESTAMP;    TimestampType timestampType = TimestampType.NO_TIMESTAMP_TYPE;    List<ConsumerRecord<byte[], byte[]>> records = new ArrayList<>();    records.add(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturnedTp1 + 1, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));    records.add(new ConsumerRecord<>(TOPIC, PARTITION3, FIRST_OFFSET + recordsReturnedTp3 + 1, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));    recordsReturnedTp1 += 1;    recordsReturnedTp3 += 1;    return new ConsumerRecords<>(Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), records));}
f11408
0
answer
public Void kafkatest_f11409_0() throws Throwable
{    callback.getValue().onComplete(postRebalanceCurrentOffsets, null);    return null;}
f11409
0
expectRebalanceRevocationError
private void kafkatest_f11417_0(RuntimeException e)
{    final List<TopicPartition> partitions = asList(TOPIC_PARTITION, TOPIC_PARTITION2);    sinkTask.close(new HashSet<>(partitions));    EasyMock.expectLastCall().andThrow(e);    sinkTask.preCommit(EasyMock.<Map<TopicPartition, OffsetAndMetadata>>anyObject());    EasyMock.expectLastCall().andReturn(Collections.emptyMap());    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            rebalanceListener.getValue().onPartitionsRevoked(partitions);            return ConsumerRecords.empty();        }    });}
f11417
0
answer
public ConsumerRecords<byte[], byte[]> kafkatest_f11418_0() throws Throwable
{    rebalanceListener.getValue().onPartitionsRevoked(partitions);    return ConsumerRecords.empty();}
f11418
0
expectRebalanceAssignmentError
private void kafkatest_f11419_0(RuntimeException e)
{    final List<TopicPartition> partitions = asList(TOPIC_PARTITION, TOPIC_PARTITION2);    sinkTask.close(new HashSet<>(partitions));    EasyMock.expectLastCall();    sinkTask.preCommit(EasyMock.<Map<TopicPartition, OffsetAndMetadata>>anyObject());    EasyMock.expectLastCall().andReturn(Collections.emptyMap());    EasyMock.expect(consumer.position(TOPIC_PARTITION)).andReturn(FIRST_OFFSET);    EasyMock.expect(consumer.position(TOPIC_PARTITION2)).andReturn(FIRST_OFFSET);    sinkTask.open(partitions);    EasyMock.expectLastCall().andThrow(e);    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            rebalanceListener.getValue().onPartitionsRevoked(partitions);            rebalanceListener.getValue().onPartitionsAssigned(partitions);            return ConsumerRecords.empty();        }    });}
f11419
0
expectConversionAndTransformation
private void kafkatest_f11427_0(final int numMessages)
{    expectConversionAndTransformation(numMessages, null);}
f11427
0
expectConversionAndTransformation
private void kafkatest_f11428_0(final int numMessages, final String topicPrefix)
{    EasyMock.expect(keyConverter.toConnectData(TOPIC, RAW_KEY)).andReturn(new SchemaAndValue(KEY_SCHEMA, KEY)).times(numMessages);    EasyMock.expect(valueConverter.toConnectData(TOPIC, RAW_VALUE)).andReturn(new SchemaAndValue(VALUE_SCHEMA, VALUE)).times(numMessages);    final Capture<SinkRecord> recordCapture = EasyMock.newCapture();    EasyMock.expect(transformationChain.apply(EasyMock.capture(recordCapture))).andAnswer(new IAnswer<SinkRecord>() {        @Override        public SinkRecord answer() {            SinkRecord origRecord = recordCapture.getValue();            return topicPrefix != null && !topicPrefix.isEmpty() ? origRecord.newRecord(topicPrefix + origRecord.topic(), origRecord.kafkaPartition(), origRecord.keySchema(), origRecord.key(), origRecord.valueSchema(), origRecord.value(), origRecord.timestamp()) : origRecord;        }    }).times(numMessages);}
f11428
0
answer
public SinkRecord kafkatest_f11429_0()
{    SinkRecord origRecord = recordCapture.getValue();    return topicPrefix != null && !topicPrefix.isEmpty() ? origRecord.newRecord(topicPrefix + origRecord.topic(), origRecord.kafkaPartition(), origRecord.keySchema(), origRecord.key(), origRecord.valueSchema(), origRecord.value(), origRecord.timestamp()) : origRecord;}
f11429
0
setup
public void kafkatest_f11437_0()
{    super.setup();    time = new MockTime();    metrics = new MockConnectMetrics();    Map<String, String> workerProps = new HashMap<>();    workerProps.put("key.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("value.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.key.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.value.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.key.converter.schemas.enable", "false");    workerProps.put("internal.value.converter.schemas.enable", "false");    workerProps.put("offset.storage.file.filename", "/tmp/connect.offsets");    pluginLoader = PowerMock.createMock(PluginClassLoader.class);    workerConfig = new StandaloneConfig(workerProps);    workerTask = new WorkerSinkTask(taskId, sinkTask, statusListener, initialState, workerConfig, ClusterConfigState.EMPTY, metrics, keyConverter, valueConverter, headerConverter, new TransformationChain<>(Collections.emptyList(), RetryWithToleranceOperatorTest.NOOP_OPERATOR), consumer, pluginLoader, time, RetryWithToleranceOperatorTest.NOOP_OPERATOR);    recordsReturned = 0;}
f11437
0
tearDown
public void kafkatest_f11438_0()
{    if (metrics != null)        metrics.stop();}
f11438
0
testPollsInBackground
public void kafkatest_f11439_0() throws Exception
{    expectInitializeTask();    expectPollInitialAssignment();    Capture<Collection<SinkRecord>> capturedRecords = expectPolls(1L);    expectStopTask();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    workerTask.initializeAndStart();    // First iteration initializes partition assignment    workerTask.iteration();    // Then we iterate to fetch data    for (int i = 0; i < 10; i++) {        workerTask.iteration();    }    workerTask.stop();    workerTask.close();    // Verify contents match expected values, i.e. that they were translated properly. With max    // batch size 1 and poll returns 1 message at a time, we should have a matching # of batches    assertEquals(10, capturedRecords.getValues().size());    int offset = 0;    for (Collection<SinkRecord> recs : capturedRecords.getValues()) {        assertEquals(1, recs.size());        for (SinkRecord rec : recs) {            SinkRecord referenceSinkRecord = new SinkRecord(TOPIC, PARTITION, KEY_SCHEMA, KEY, VALUE_SCHEMA, VALUE, FIRST_OFFSET + offset, TIMESTAMP, TIMESTAMP_TYPE);            assertEquals(referenceSinkRecord, rec);            offset++;        }    }    PowerMock.verifyAll();}
f11439
0
answer
public Object kafkatest_f11447_0() throws Throwable
{    try {        sinkTaskContext.getValue().pause(UNASSIGNED_TOPIC_PARTITION);        fail("Trying to pause unassigned partition should have thrown an Connect exception");    } catch (ConnectException e) {    // expected    }    sinkTaskContext.getValue().pause(TOPIC_PARTITION, TOPIC_PARTITION2);    return null;}
f11447
0
answer
public Object kafkatest_f11448_0() throws Throwable
{    try {        sinkTaskContext.getValue().resume(UNASSIGNED_TOPIC_PARTITION);        fail("Trying to resume unassigned partition should have thrown an Connect exception");    } catch (ConnectException e) {    // expected    }    sinkTaskContext.getValue().resume(TOPIC_PARTITION, TOPIC_PARTITION2);    return null;}
f11448
0
testRewind
public void kafkatest_f11449_0() throws Exception
{    expectInitializeTask();    expectPollInitialAssignment();    final long startOffset = 40L;    final Map<TopicPartition, Long> offsets = new HashMap<>();    expectOnePoll().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            offsets.put(TOPIC_PARTITION, startOffset);            sinkTaskContext.getValue().offset(offsets);            return null;        }    });    consumer.seek(TOPIC_PARTITION, startOffset);    EasyMock.expectLastCall();    expectOnePoll().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            Map<TopicPartition, Long> offsets = sinkTaskContext.getValue().offsets();            assertEquals(0, offsets.size());            return null;        }    });    expectStopTask();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    workerTask.initializeAndStart();    workerTask.iteration();    workerTask.iteration();    workerTask.iteration();    workerTask.stop();    workerTask.close();    PowerMock.verifyAll();}
f11449
0
expectStopTask
private void kafkatest_f11457_0() throws Exception
{    sinkTask.stop();    PowerMock.expectLastCall();    // No offset commit since it happens in the mocked worker thread, but the main thread does need to wake up the    // consumer so it exits quickly    consumer.wakeup();    PowerMock.expectLastCall();    consumer.close();    PowerMock.expectLastCall();}
f11457
0
expectPolls
private Capture<Collection<SinkRecord>> kafkatest_f11458_0(final long pollDelayMs) throws Exception
{    // Stub out all the consumer stream/iterator responses, which we just want to verify occur,    // but don't care about the exact details here.    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andStubAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            // "Sleep" so time will progress            time.sleep(pollDelayMs);            ConsumerRecords<byte[], byte[]> records = new ConsumerRecords<>(Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), Arrays.asList(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturned, TIMESTAMP, TIMESTAMP_TYPE, 0L, 0, 0, RAW_KEY, RAW_VALUE))));            recordsReturned++;            return records;        }    });    EasyMock.expect(keyConverter.toConnectData(TOPIC, RAW_KEY)).andReturn(new SchemaAndValue(KEY_SCHEMA, KEY)).anyTimes();    EasyMock.expect(valueConverter.toConnectData(TOPIC, RAW_VALUE)).andReturn(new SchemaAndValue(VALUE_SCHEMA, VALUE)).anyTimes();    final Capture<SinkRecord> recordCapture = EasyMock.newCapture();    EasyMock.expect(transformationChain.apply(EasyMock.capture(recordCapture))).andAnswer((IAnswer<SinkRecord>) () -> recordCapture.getValue()).anyTimes();    Capture<Collection<SinkRecord>> capturedRecords = EasyMock.newCapture(CaptureType.ALL);    sinkTask.put(EasyMock.capture(capturedRecords));    EasyMock.expectLastCall().anyTimes();    return capturedRecords;}
f11458
0
answer
public ConsumerRecords<byte[], byte[]> kafkatest_f11459_0() throws Throwable
{    // "Sleep" so time will progress    time.sleep(pollDelayMs);    ConsumerRecords<byte[], byte[]> records = new ConsumerRecords<>(Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), Arrays.asList(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturned, TIMESTAMP, TIMESTAMP_TYPE, 0L, 0, 0, RAW_KEY, RAW_VALUE))));    recordsReturned++;    return records;}
f11459
0
tearDown
public void kafkatest_f11467_0()
{    if (metrics != null)        metrics.stop();}
f11467
0
createWorkerTask
private void kafkatest_f11468_0()
{    createWorkerTask(TargetState.STARTED);}
f11468
0
createWorkerTask
private void kafkatest_f11469_0(TargetState initialState)
{    workerTask = new WorkerSourceTask(taskId, sourceTask, statusListener, initialState, keyConverter, valueConverter, headerConverter, transformationChain, producer, offsetReader, offsetWriter, config, clusterConfigState, metrics, plugins.delegatingLoader(), Time.SYSTEM, RetryWithToleranceOperatorTest.NOOP_OPERATOR);}
f11469
0
testCommitFailure
public void kafkatest_f11477_0() throws Exception
{    // Test that the task commits properly when prompted    createWorkerTask();    sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));    EasyMock.expectLastCall();    sourceTask.start(TASK_PROPS);    EasyMock.expectLastCall();    statusListener.onStartup(taskId);    EasyMock.expectLastCall();    // We'll wait for some data, then trigger a flush    final CountDownLatch pollLatch = expectPolls(1);    expectOffsetFlush(true);    sourceTask.stop();    EasyMock.expectLastCall();    expectOffsetFlush(false);    statusListener.onShutdown(taskId);    EasyMock.expectLastCall();    producer.close(EasyMock.anyObject(Duration.class));    EasyMock.expectLastCall();    transformationChain.close();    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    Future<?> taskFuture = executor.submit(workerTask);    assertTrue(awaitLatch(pollLatch));    assertTrue(workerTask.commitOffsets());    workerTask.stop();    assertTrue(workerTask.awaitStop(1000));    taskFuture.get();    assertPollMetrics(1);    PowerMock.verifyAll();}
f11477
0
testSendRecordsConvertsData
public void kafkatest_f11478_0() throws Exception
{    createWorkerTask();    List<SourceRecord> records = new ArrayList<>();    // Can just use the same record for key and value    records.add(new SourceRecord(PARTITION, OFFSET, "topic", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD));    Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();    PowerMock.replayAll();    Whitebox.setInternalState(workerTask, "toSend", records);    Whitebox.invokeMethod(workerTask, "sendRecords");    assertEquals(SERIALIZED_KEY, sent.getValue().key());    assertEquals(SERIALIZED_RECORD, sent.getValue().value());    PowerMock.verifyAll();}
f11478
0
testSendRecordsPropagatesTimestamp
public void kafkatest_f11479_0() throws Exception
{    final Long timestamp = System.currentTimeMillis();    createWorkerTask();    List<SourceRecord> records = Collections.singletonList(new SourceRecord(PARTITION, OFFSET, "topic", null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp));    Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();    PowerMock.replayAll();    Whitebox.setInternalState(workerTask, "toSend", records);    Whitebox.invokeMethod(workerTask, "sendRecords");    assertEquals(timestamp, sent.getValue().timestamp());    PowerMock.verifyAll();}
f11479
0
testMetricsGroup
public void kafkatest_f11487_0()
{    SourceTaskMetricsGroup group = new SourceTaskMetricsGroup(taskId, metrics);    SourceTaskMetricsGroup group1 = new SourceTaskMetricsGroup(taskId1, metrics);    for (int i = 0; i != 10; ++i) {        group.recordPoll(100, 1000 + i * 100);        group.recordWrite(10);    }    for (int i = 0; i != 20; ++i) {        group1.recordPoll(100, 1000 + i * 100);        group1.recordWrite(10);    }    assertEquals(1900.0, metrics.currentMetricValueAsDouble(group.metricGroup(), "poll-batch-max-time-ms"), 0.001d);    assertEquals(1450.0, metrics.currentMetricValueAsDouble(group.metricGroup(), "poll-batch-avg-time-ms"), 0.001d);    assertEquals(33.333, metrics.currentMetricValueAsDouble(group.metricGroup(), "source-record-poll-rate"), 0.001d);    assertEquals(1000, metrics.currentMetricValueAsDouble(group.metricGroup(), "source-record-poll-total"), 0.001d);    assertEquals(3.3333, metrics.currentMetricValueAsDouble(group.metricGroup(), "source-record-write-rate"), 0.001d);    assertEquals(100, metrics.currentMetricValueAsDouble(group.metricGroup(), "source-record-write-total"), 0.001d);    assertEquals(900.0, metrics.currentMetricValueAsDouble(group.metricGroup(), "source-record-active-count"), 0.001d);    // Close the group    group.close();    for (MetricName metricName : group.metricGroup().metrics().metrics().keySet()) {        // Metrics for this group should no longer exist        assertFalse(group.metricGroup().groupId().includes(metricName));    }    // Sensors for this group should no longer exist    assertNull(group.metricGroup().metrics().getSensor("sink-record-read"));    assertNull(group.metricGroup().metrics().getSensor("sink-record-send"));    assertNull(group.metricGroup().metrics().getSensor("sink-record-active-count"));    assertNull(group.metricGroup().metrics().getSensor("partition-count"));    assertNull(group.metricGroup().metrics().getSensor("offset-seq-number"));    assertNull(group.metricGroup().metrics().getSensor("offset-commit-completion"));    assertNull(group.metricGroup().metrics().getSensor("offset-commit-completion-skip"));    assertNull(group.metricGroup().metrics().getSensor("put-batch-time"));    assertEquals(2900.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), "poll-batch-max-time-ms"), 0.001d);    assertEquals(1950.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), "poll-batch-avg-time-ms"), 0.001d);    assertEquals(66.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), "source-record-poll-rate"), 0.001d);    assertEquals(2000, metrics.currentMetricValueAsDouble(group1.metricGroup(), "source-record-poll-total"), 0.001d);    assertEquals(6.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), "source-record-write-rate"), 0.001d);    assertEquals(200, metrics.currentMetricValueAsDouble(group1.metricGroup(), "source-record-write-total"), 0.001d);    assertEquals(1800.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), "source-record-active-count"), 0.001d);}
f11487
0
expectPolls
private CountDownLatch kafkatest_f11488_0(int minimum, final AtomicInteger count) throws InterruptedException
{    final CountDownLatch latch = new CountDownLatch(minimum);    // Note that we stub these to allow any number of calls because the thread will continue to    // run. The count passed in + latch returned just makes sure we get *at least* that number of    // calls    EasyMock.expect(sourceTask.poll()).andStubAnswer(new IAnswer<List<SourceRecord>>() {        @Override        public List<SourceRecord> answer() throws Throwable {            count.incrementAndGet();            latch.countDown();            Thread.sleep(10);            return RECORDS;        }    });    // Fallout of the poll() call    expectSendRecordAnyTimes();    return latch;}
f11488
0
answer
public List<SourceRecord> kafkatest_f11489_0() throws Throwable
{    count.incrementAndGet();    latch.countDown();    Thread.sleep(10);    return RECORDS;}
f11489
0
expectSendRecord
private Capture<ProducerRecord<byte[], byte[]>> kafkatest_f11497_0(boolean anyTimes, boolean isRetry, boolean sendSuccess, boolean commitSuccess) throws InterruptedException
{    expectConvertKeyValue(anyTimes);    expectApplyTransformationChain(anyTimes);    Capture<ProducerRecord<byte[], byte[]>> sent = EasyMock.newCapture();    // 1. Offset data is passed to the offset storage.    if (!isRetry) {        offsetWriter.offset(PARTITION, OFFSET);        if (anyTimes)            PowerMock.expectLastCall().anyTimes();        else            PowerMock.expectLastCall();    }    // 2. Converted data passed to the producer, which will need callbacks invoked for flush to work    IExpectationSetters<Future<RecordMetadata>> expect = EasyMock.expect(producer.send(EasyMock.capture(sent), EasyMock.capture(producerCallbacks)));    IAnswer<Future<RecordMetadata>> expectResponse = new IAnswer<Future<RecordMetadata>>() {        @Override        public Future<RecordMetadata> answer() throws Throwable {            synchronized (producerCallbacks) {                for (org.apache.kafka.clients.producer.Callback cb : producerCallbacks.getValues()) {                    if (sendSuccess) {                        cb.onCompletion(new RecordMetadata(new TopicPartition("foo", 0), 0, 0, 0L, 0L, 0, 0), null);                    } else {                        cb.onCompletion(null, new TopicAuthorizationException("foo"));                    }                }                producerCallbacks.reset();            }            return sendFuture;        }    };    if (anyTimes)        expect.andStubAnswer(expectResponse);    else        expect.andAnswer(expectResponse);    if (sendSuccess) {        // 3. As a result of a successful producer send callback, we'll notify the source task of the record commit        expectTaskCommitRecord(anyTimes, commitSuccess);    }    return sent;}
f11497
0
answer
public Future<RecordMetadata> kafkatest_f11498_0() throws Throwable
{    synchronized (producerCallbacks) {        for (org.apache.kafka.clients.producer.Callback cb : producerCallbacks.getValues()) {            if (sendSuccess) {                cb.onCompletion(new RecordMetadata(new TopicPartition("foo", 0), 0, 0, 0L, 0L, 0, 0), null);            } else {                cb.onCompletion(null, new TopicAuthorizationException("foo"));            }        }        producerCallbacks.reset();    }    return sendFuture;}
f11498
0
expectConvertKeyValue
private void kafkatest_f11499_0(boolean anyTimes)
{    IExpectationSetters<byte[]> convertKeyExpect = EasyMock.expect(keyConverter.fromConnectData(TOPIC, KEY_SCHEMA, KEY));    if (anyTimes)        convertKeyExpect.andStubReturn(SERIALIZED_KEY);    else        convertKeyExpect.andReturn(SERIALIZED_KEY);    IExpectationSetters<byte[]> convertValueExpect = EasyMock.expect(valueConverter.fromConnectData(TOPIC, RECORD_SCHEMA, RECORD));    if (anyTimes)        convertValueExpect.andStubReturn(SERIALIZED_RECORD);    else        convertValueExpect.andReturn(SERIALIZED_RECORD);}
f11499
0
setup
public void kafkatest_f11507_0()
{    metrics = new MockConnectMetrics();    retryWithToleranceOperator = RetryWithToleranceOperatorTest.NOOP_OPERATOR;}
f11507
0
tearDown
public void kafkatest_f11508_0()
{    if (metrics != null)        metrics.stop();}
f11508
0
standardStartup
public void kafkatest_f11509_0()
{    ConnectorTaskId taskId = new ConnectorTaskId("foo", 0);    WorkerTask workerTask = partialMockBuilder(WorkerTask.class).withConstructor(ConnectorTaskId.class, TaskStatus.Listener.class, TargetState.class, ClassLoader.class, ConnectMetrics.class, RetryWithToleranceOperator.class).withArgs(taskId, statusListener, TargetState.STARTED, loader, metrics, retryWithToleranceOperator).addMockedMethod("initialize").addMockedMethod("execute").addMockedMethod("close").createStrictMock();    workerTask.initialize(TASK_CONFIG);    expectLastCall();    workerTask.execute();    expectLastCall();    statusListener.onStartup(taskId);    expectLastCall();    workerTask.close();    expectLastCall();    workerTask.releaseResources();    EasyMock.expectLastCall();    statusListener.onShutdown(taskId);    expectLastCall();    replay(workerTask);    workerTask.initialize(TASK_CONFIG);    workerTask.run();    workerTask.stop();    workerTask.awaitStop(1000L);    verify(workerTask);}
f11509
0
assertPausedMetric
protected void kafkatest_f11517_0(TaskMetricsGroup metricsGroup)
{    assertEquals(AbstractStatus.State.PAUSED, metricsGroup.state());}
f11517
0
assertRunningMetric
protected void kafkatest_f11518_0(TaskMetricsGroup metricsGroup)
{    assertEquals(AbstractStatus.State.RUNNING, metricsGroup.state());}
f11518
0
assertStoppedMetric
protected void kafkatest_f11519_0(TaskMetricsGroup metricsGroup)
{    assertEquals(AbstractStatus.State.UNASSIGNED, metricsGroup.state());}
f11519
0
testAddRemoveTask
public void kafkatest_f11527_0() throws Exception
{    expectConverters();    expectStartStorage();    EasyMock.expect(workerTask.id()).andStubReturn(TASK_ID);    EasyMock.expect(plugins.currentThreadLoader()).andReturn(delegatingLoader).times(2);    PowerMock.expectNew(WorkerSourceTask.class, EasyMock.eq(TASK_ID), EasyMock.eq(task), anyObject(TaskStatus.Listener.class), EasyMock.eq(TargetState.STARTED), anyObject(JsonConverter.class), anyObject(JsonConverter.class), anyObject(JsonConverter.class), EasyMock.eq(new TransformationChain<>(Collections.emptyList(), NOOP_OPERATOR)), anyObject(KafkaProducer.class), anyObject(OffsetStorageReader.class), anyObject(OffsetStorageWriter.class), EasyMock.eq(config), anyObject(ClusterConfigState.class), anyObject(ConnectMetrics.class), anyObject(ClassLoader.class), anyObject(Time.class), anyObject(RetryWithToleranceOperator.class)).andReturn(workerTask);    Map<String, String> origProps = new HashMap<>();    origProps.put(TaskConfig.TASK_CLASS_CONFIG, TestSourceTask.class.getName());    TaskConfig taskConfig = new TaskConfig(origProps);    // We should expect this call, but the pluginLoader being swapped in is only mocked.    // EasyMock.expect(pluginLoader.loadClass(TestSourceTask.class.getName()))    // .andReturn((Class) TestSourceTask.class);    EasyMock.expect(plugins.newTask(TestSourceTask.class)).andReturn(task);    EasyMock.expect(task.version()).andReturn("1.0");    workerTask.initialize(taskConfig);    EasyMock.expectLastCall();    // Expect that the worker will create converters and will find them using the current classloader ...    assertNotNull(taskKeyConverter);    assertNotNull(taskValueConverter);    assertNotNull(taskHeaderConverter);    expectTaskKeyConverters(ClassLoaderUsage.CURRENT_CLASSLOADER, taskKeyConverter);    expectTaskValueConverters(ClassLoaderUsage.CURRENT_CLASSLOADER, taskValueConverter);    expectTaskHeaderConverter(ClassLoaderUsage.CURRENT_CLASSLOADER, taskHeaderConverter);    EasyMock.expect(executorService.submit(workerTask)).andReturn(null);    EasyMock.expect(plugins.delegatingLoader()).andReturn(delegatingLoader);    EasyMock.expect(delegatingLoader.connectorLoader(WorkerTestConnector.class.getName())).andReturn(pluginLoader);    EasyMock.expect(Plugins.compareAndSwapLoaders(pluginLoader)).andReturn(delegatingLoader).times(2);    EasyMock.expect(workerTask.loader()).andReturn(pluginLoader);    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader).times(2);    plugins.connectorClass(WorkerTestConnector.class.getName());    EasyMock.expectLastCall().andReturn(WorkerTestConnector.class);    // Remove    workerTask.stop();    EasyMock.expectLastCall();    EasyMock.expect(workerTask.awaitStop(EasyMock.anyLong())).andStubReturn(true);    EasyMock.expectLastCall();    expectStopStorage();    PowerMock.replayAll();    worker = new Worker(WORKER_ID, new MockTime(), plugins, config, offsetBackingStore, executorService, noneConnectorClientConfigOverridePolicy);    worker.start();    assertStatistics(worker, 0, 0);    assertStartupStatistics(worker, 0, 0, 0, 0);    assertEquals(Collections.emptySet(), worker.taskIds());    worker.startTask(TASK_ID, ClusterConfigState.EMPTY, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED);    assertStatistics(worker, 0, 1);    assertStartupStatistics(worker, 0, 0, 1, 0);    assertEquals(new HashSet<>(Arrays.asList(TASK_ID)), worker.taskIds());    worker.stopAndAwaitTask(TASK_ID);    assertStatistics(worker, 0, 0);    assertStartupStatistics(worker, 0, 0, 1, 0);    assertEquals(Collections.emptySet(), worker.taskIds());    // Nothing should be left, so this should effectively be a nop    worker.stop();    assertStatistics(worker, 0, 0);    assertStartupStatistics(worker, 0, 0, 1, 0);    PowerMock.verifyAll();}
f11527
0
testStartTaskFailure
public void kafkatest_f11528_0()
{    expectConverters();    expectStartStorage();    Map<String, String> origProps = new HashMap<>();    origProps.put(TaskConfig.TASK_CLASS_CONFIG, "missing.From.This.Workers.Classpath");    EasyMock.expect(plugins.currentThreadLoader()).andReturn(delegatingLoader);    EasyMock.expect(plugins.delegatingLoader()).andReturn(delegatingLoader);    EasyMock.expect(delegatingLoader.connectorLoader(WorkerTestConnector.class.getName())).andReturn(pluginLoader);    // We would normally expect this since the plugin loader would have been swapped in. However, since we mock out    // all classloader changes, the call actually goes to the normal default classloader. However, this works out    // fine since we just wanted a ClassNotFoundException anyway.    // EasyMock.expect(pluginLoader.loadClass(origProps.get(TaskConfig.TASK_CLASS_CONFIG)))    // .andThrow(new ClassNotFoundException());    EasyMock.expect(Plugins.compareAndSwapLoaders(pluginLoader)).andReturn(delegatingLoader);    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    taskStatusListener.onFailure(EasyMock.eq(TASK_ID), EasyMock.<ConfigException>anyObject());    EasyMock.expectLastCall();    PowerMock.replayAll();    worker = new Worker(WORKER_ID, new MockTime(), plugins, config, offsetBackingStore, noneConnectorClientConfigOverridePolicy);    worker.start();    assertStatistics(worker, 0, 0);    assertStartupStatistics(worker, 0, 0, 0, 0);    assertFalse(worker.startTask(TASK_ID, ClusterConfigState.EMPTY, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED));    assertStartupStatistics(worker, 0, 0, 1, 1);    assertStatistics(worker, 0, 0);    assertStartupStatistics(worker, 0, 0, 1, 1);    assertEquals(Collections.emptySet(), worker.taskIds());    PowerMock.verifyAll();}
f11528
0
testCleanupTasksOnStop
public void kafkatest_f11529_0() throws Exception
{    expectConverters();    expectStartStorage();    EasyMock.expect(workerTask.id()).andStubReturn(TASK_ID);    EasyMock.expect(plugins.currentThreadLoader()).andReturn(delegatingLoader).times(2);    PowerMock.expectNew(WorkerSourceTask.class, EasyMock.eq(TASK_ID), EasyMock.eq(task), anyObject(TaskStatus.Listener.class), EasyMock.eq(TargetState.STARTED), anyObject(JsonConverter.class), anyObject(JsonConverter.class), anyObject(JsonConverter.class), EasyMock.eq(new TransformationChain<>(Collections.emptyList(), NOOP_OPERATOR)), anyObject(KafkaProducer.class), anyObject(OffsetStorageReader.class), anyObject(OffsetStorageWriter.class), anyObject(WorkerConfig.class), anyObject(ClusterConfigState.class), anyObject(ConnectMetrics.class), EasyMock.eq(pluginLoader), anyObject(Time.class), anyObject(RetryWithToleranceOperator.class)).andReturn(workerTask);    Map<String, String> origProps = new HashMap<>();    origProps.put(TaskConfig.TASK_CLASS_CONFIG, TestSourceTask.class.getName());    TaskConfig taskConfig = new TaskConfig(origProps);    // We should expect this call, but the pluginLoader being swapped in is only mocked.    // EasyMock.expect(pluginLoader.loadClass(TestSourceTask.class.getName()))    // .andReturn((Class) TestSourceTask.class);    EasyMock.expect(plugins.newTask(TestSourceTask.class)).andReturn(task);    EasyMock.expect(task.version()).andReturn("1.0");    workerTask.initialize(taskConfig);    EasyMock.expectLastCall();    // Expect that the worker will create converters and will not initially find them using the current classloader ...    assertNotNull(taskKeyConverter);    assertNotNull(taskValueConverter);    assertNotNull(taskHeaderConverter);    expectTaskKeyConverters(ClassLoaderUsage.CURRENT_CLASSLOADER, null);    expectTaskKeyConverters(ClassLoaderUsage.PLUGINS, taskKeyConverter);    expectTaskValueConverters(ClassLoaderUsage.CURRENT_CLASSLOADER, null);    expectTaskValueConverters(ClassLoaderUsage.PLUGINS, taskValueConverter);    expectTaskHeaderConverter(ClassLoaderUsage.CURRENT_CLASSLOADER, null);    expectTaskHeaderConverter(ClassLoaderUsage.PLUGINS, taskHeaderConverter);    EasyMock.expect(executorService.submit(workerTask)).andReturn(null);    EasyMock.expect(plugins.delegatingLoader()).andReturn(delegatingLoader);    EasyMock.expect(delegatingLoader.connectorLoader(WorkerTestConnector.class.getName())).andReturn(pluginLoader);    EasyMock.expect(Plugins.compareAndSwapLoaders(pluginLoader)).andReturn(delegatingLoader).times(2);    EasyMock.expect(workerTask.loader()).andReturn(pluginLoader);    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader).times(2);    plugins.connectorClass(WorkerTestConnector.class.getName());    EasyMock.expectLastCall().andReturn(WorkerTestConnector.class);    // Remove on Worker.stop()    workerTask.stop();    EasyMock.expectLastCall();    EasyMock.expect(workerTask.awaitStop(EasyMock.anyLong())).andReturn(true);    // Note that in this case we *do not* commit offsets since it's an unclean shutdown    EasyMock.expectLastCall();    expectStopStorage();    PowerMock.replayAll();    worker = new Worker(WORKER_ID, new MockTime(), plugins, config, offsetBackingStore, executorService, noneConnectorClientConfigOverridePolicy);    worker.start();    assertStatistics(worker, 0, 0);    worker.startTask(TASK_ID, ClusterConfigState.EMPTY, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED);    assertStatistics(worker, 0, 1);    worker.stop();    assertStatistics(worker, 0, 0);    PowerMock.verifyAll();}
f11529
0
testConsumerConfigsClientOverridesWithNonePolicy
public void kafkatest_f11537_0()
{    Map<String, String> props = new HashMap<>(workerProps);    props.put("consumer.auto.offset.reset", "latest");    props.put("consumer.max.poll.records", "5000");    WorkerConfig configWithOverrides = new StandaloneConfig(props);    Map<String, Object> connConfig = new HashMap<String, Object>();    connConfig.put("max.poll.records", "5000");    connConfig.put("max.poll.interval.ms", "1000");    EasyMock.expect(connectorConfig.originalsWithPrefix(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX)).andReturn(connConfig);    PowerMock.replayAll();    Worker.consumerConfigs(new ConnectorTaskId("test", 1), configWithOverrides, connectorConfig, null, noneConnectorClientConfigOverridePolicy);}
f11537
0
testAdminConfigsClientOverridesWithAllPolicy
public void kafkatest_f11538_0()
{    Map<String, String> props = new HashMap<>(workerProps);    props.put("admin.client.id", "testid");    props.put("admin.metadata.max.age.ms", "5000");    WorkerConfig configWithOverrides = new StandaloneConfig(props);    Map<String, Object> connConfig = new HashMap<String, Object>();    connConfig.put("metadata.max.age.ms", "10000");    Map<String, String> expectedConfigs = new HashMap<>();    expectedConfigs.put("bootstrap.servers", "localhost:9092");    expectedConfigs.put("client.id", "testid");    expectedConfigs.put("metadata.max.age.ms", "10000");    EasyMock.expect(connectorConfig.originalsWithPrefix(ConnectorConfig.CONNECTOR_CLIENT_ADMIN_OVERRIDES_PREFIX)).andReturn(connConfig);    PowerMock.replayAll();    assertEquals(expectedConfigs, Worker.adminConfigs(new ConnectorTaskId("test", 1), configWithOverrides, connectorConfig, null, allConnectorClientConfigOverridePolicy));}
f11538
0
testAdminConfigsClientOverridesWithNonePolicy
public void kafkatest_f11539_0()
{    Map<String, String> props = new HashMap<>(workerProps);    props.put("admin.client.id", "testid");    props.put("admin.metadata.max.age.ms", "5000");    WorkerConfig configWithOverrides = new StandaloneConfig(props);    Map<String, Object> connConfig = new HashMap<String, Object>();    connConfig.put("metadata.max.age.ms", "10000");    EasyMock.expect(connectorConfig.originalsWithPrefix(ConnectorConfig.CONNECTOR_CLIENT_ADMIN_OVERRIDES_PREFIX)).andReturn(connConfig);    PowerMock.replayAll();    Worker.adminConfigs(new ConnectorTaskId("test", 1), configWithOverrides, connectorConfig, null, noneConnectorClientConfigOverridePolicy);}
f11539
0
expectTaskKeyConverters
private void kafkatest_f11547_0(ClassLoaderUsage classLoaderUsage, Converter returning)
{    EasyMock.expect(plugins.newConverter(anyObject(AbstractConfig.class), eq(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG), eq(classLoaderUsage))).andReturn(returning);}
f11547
0
expectTaskValueConverters
private void kafkatest_f11548_0(ClassLoaderUsage classLoaderUsage, Converter returning)
{    EasyMock.expect(plugins.newConverter(anyObject(AbstractConfig.class), eq(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG), eq(classLoaderUsage))).andReturn(returning);}
f11548
0
expectTaskHeaderConverter
private void kafkatest_f11549_0(ClassLoaderUsage classLoaderUsage, HeaderConverter returning)
{    EasyMock.expect(plugins.newHeaderConverter(anyObject(AbstractConfig.class), eq(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG), eq(classLoaderUsage))).andReturn(returning);}
f11549
0
configure
public void kafkatest_f11561_0(Map<String, ?> configs, boolean isKey)
{    this.configs = configs;}
f11561
0
fromConnectData
public byte[] kafkatest_f11562_0(String topic, Schema schema, Object value)
{    return new byte[0];}
f11562
0
toConnectData
public SchemaAndValue kafkatest_f11563_0(String topic, byte[] value)
{    return null;}
f11563
0
newConnectors
public static List<String> kafkatest_f11571_0(int start, int end)
{    return IntStream.range(start, end).mapToObj(i -> "connector" + i).collect(Collectors.toList());}
f11571
0
newTasks
public static List<ConnectorTaskId> kafkatest_f11572_0(int start, int end)
{    return IntStream.range(start, end).mapToObj(i -> new ConnectorTaskId("task", i)).collect(Collectors.toList());}
f11572
0
clusterConfigState
public static ClusterConfigState kafkatest_f11573_0(long offset, int connectorNum, int taskNum)
{    return new ClusterConfigState(offset, connectorTaskCounts(1, connectorNum, taskNum), connectorConfigs(1, connectorNum), connectorTargetStates(1, connectorNum, TargetState.STARTED), taskConfigs(0, connectorNum, connectorNum * taskNum), Collections.emptySet());}
f11573
0
assertAssignment
public static void kafkatest_f11581_0(String expectedLeader, long expectedOffset, List<String> expectedAssignedConnectors, int expectedAssignedTaskNum, List<String> expectedRevokedConnectors, int expectedRevokedTaskNum, ExtendedAssignment assignment)
{    assertAssignment(false, expectedLeader, expectedOffset, expectedAssignedConnectors, expectedAssignedTaskNum, expectedRevokedConnectors, expectedRevokedTaskNum, 0, assignment);}
f11581
0
assertAssignment
public static void kafkatest_f11582_0(String expectedLeader, long expectedOffset, List<String> expectedAssignedConnectors, int expectedAssignedTaskNum, List<String> expectedRevokedConnectors, int expectedRevokedTaskNum, int expectedDelay, ExtendedAssignment assignment)
{    assertAssignment(false, expectedLeader, expectedOffset, expectedAssignedConnectors, expectedAssignedTaskNum, expectedRevokedConnectors, expectedRevokedTaskNum, expectedDelay, assignment);}
f11582
0
assertAssignment
public static void kafkatest_f11583_0(boolean expectFailed, String expectedLeader, long expectedOffset, List<String> expectedAssignedConnectors, int expectedAssignedTaskNum, List<String> expectedRevokedConnectors, int expectedRevokedTaskNum, int expectedDelay, ExtendedAssignment assignment)
{    assertNotNull("Assignment can't be null", assignment);    assertEquals("Wrong status in " + assignment, expectFailed, assignment.failed());    assertEquals("Wrong leader in " + assignment, expectedLeader, assignment.leader());    assertEquals("Wrong leaderUrl in " + assignment, expectedLeaderUrl(expectedLeader), assignment.leaderUrl());    assertEquals("Wrong offset in " + assignment, expectedOffset, assignment.offset());    assertThat("Wrong set of assigned connectors in " + assignment, assignment.connectors(), is(expectedAssignedConnectors));    assertEquals("Wrong number of assigned tasks in " + assignment, expectedAssignedTaskNum, assignment.tasks().size());    assertThat("Wrong set of revoked connectors in " + assignment, assignment.revokedConnectors(), is(expectedRevokedConnectors));    assertEquals("Wrong number of revoked tasks in " + assignment, expectedRevokedTaskNum, assignment.revokedTasks().size());    assertEquals("Wrong rebalance delay in " + assignment, expectedDelay, assignment.delay());}
f11583
0
setUp
public void kafkatest_f11591_0()
{    configStorage = PowerMock.createPartialMock(KafkaConfigBackingStore.class, new String[] { "createKafkaBasedLog" }, converter, DEFAULT_DISTRIBUTED_CONFIG, null);    Whitebox.setInternalState(configStorage, "configLog", storeLog);    configStorage.setUpdateListener(configUpdateListener);}
f11591
0
testStartStop
public void kafkatest_f11592_0() throws Exception
{    expectConfigure();    expectStart(Collections.emptyList(), Collections.emptyMap());    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    assertEquals(TOPIC, capturedTopic.getValue());    assertEquals("org.apache.kafka.common.serialization.StringSerializer", capturedProducerProps.getValue().get(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.ByteArraySerializer", capturedProducerProps.getValue().get(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.StringDeserializer", capturedConsumerProps.getValue().get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.ByteArrayDeserializer", capturedConsumerProps.getValue().get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG));    assertEquals(TOPIC, capturedNewTopic.getValue().name());    assertEquals(1, capturedNewTopic.getValue().numPartitions());    assertEquals(TOPIC_REPLICATION_FACTOR, capturedNewTopic.getValue().replicationFactor());    configStorage.start();    configStorage.stop();    PowerMock.verifyAll();}
f11592
0
testPutConnectorConfig
public void kafkatest_f11593_0() throws Exception
{    expectConfigure();    expectStart(Collections.emptyList(), Collections.emptyMap());    expectConvertWriteAndRead(CONNECTOR_CONFIG_KEYS.get(0), KafkaConfigBackingStore.CONNECTOR_CONFIGURATION_V0, CONFIGS_SERIALIZED.get(0), "properties", SAMPLE_CONFIGS.get(0));    configUpdateListener.onConnectorConfigUpdate(CONNECTOR_IDS.get(0));    EasyMock.expectLastCall();    expectConvertWriteAndRead(CONNECTOR_CONFIG_KEYS.get(1), KafkaConfigBackingStore.CONNECTOR_CONFIGURATION_V0, CONFIGS_SERIALIZED.get(1), "properties", SAMPLE_CONFIGS.get(1));    configUpdateListener.onConnectorConfigUpdate(CONNECTOR_IDS.get(1));    EasyMock.expectLastCall();    // Config deletion    expectConnectorRemoval(CONNECTOR_CONFIG_KEYS.get(1), TARGET_STATE_KEYS.get(1));    configUpdateListener.onConnectorConfigRemove(CONNECTOR_IDS.get(1));    EasyMock.expectLastCall();    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    configStorage.start();    // Null before writing    ClusterConfigState configState = configStorage.snapshot();    assertEquals(-1, configState.offset());    assertNull(configState.connectorConfig(CONNECTOR_IDS.get(0)));    assertNull(configState.connectorConfig(CONNECTOR_IDS.get(1)));    // Writing should block until it is written and read back from Kafka    configStorage.putConnectorConfig(CONNECTOR_IDS.get(0), SAMPLE_CONFIGS.get(0));    configState = configStorage.snapshot();    assertEquals(1, configState.offset());    assertEquals(SAMPLE_CONFIGS.get(0), configState.connectorConfig(CONNECTOR_IDS.get(0)));    assertNull(configState.connectorConfig(CONNECTOR_IDS.get(1)));    // Second should also block and all configs should still be available    configStorage.putConnectorConfig(CONNECTOR_IDS.get(1), SAMPLE_CONFIGS.get(1));    configState = configStorage.snapshot();    assertEquals(2, configState.offset());    assertEquals(SAMPLE_CONFIGS.get(0), configState.connectorConfig(CONNECTOR_IDS.get(0)));    assertEquals(SAMPLE_CONFIGS.get(1), configState.connectorConfig(CONNECTOR_IDS.get(1)));    // Deletion should remove the second one we added    configStorage.removeConnectorConfig(CONNECTOR_IDS.get(1));    configState = configStorage.snapshot();    assertEquals(4, configState.offset());    assertEquals(SAMPLE_CONFIGS.get(0), configState.connectorConfig(CONNECTOR_IDS.get(0)));    assertNull(configState.connectorConfig(CONNECTOR_IDS.get(1)));    assertNull(configState.targetState(CONNECTOR_IDS.get(1)));    configStorage.stop();    PowerMock.verifyAll();}
f11593
0
testRestore
public void kafkatest_f11601_0() throws Exception
{    // Restoring data should notify only of the latest values after loading is complete. This also validates    // that inconsistent state is ignored.    expectConfigure();    // Overwrite each type at least once to ensure we see the latest data after loading    List<ConsumerRecord<String, byte[]>> existingRecords = Arrays.asList(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0)), new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(1)), new ConsumerRecord<>(TOPIC, 0, 2, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(1), CONFIGS_SERIALIZED.get(2)), new ConsumerRecord<>(TOPIC, 0, 3, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(3)), new ConsumerRecord<>(TOPIC, 0, 4, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, COMMIT_TASKS_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(4)), // Connector after root update should make it through, task update shouldn't    new ConsumerRecord<>(TOPIC, 0, 5, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(5)), new ConsumerRecord<>(TOPIC, 0, 6, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(6)));    LinkedHashMap<byte[], Struct> deserialized = new LinkedHashMap<>();    deserialized.put(CONFIGS_SERIALIZED.get(0), CONNECTOR_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(1), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(2), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(3), CONNECTOR_CONFIG_STRUCTS.get(1));    deserialized.put(CONFIGS_SERIALIZED.get(4), TASKS_COMMIT_STRUCT_TWO_TASK_CONNECTOR);    deserialized.put(CONFIGS_SERIALIZED.get(5), CONNECTOR_CONFIG_STRUCTS.get(2));    deserialized.put(CONFIGS_SERIALIZED.get(6), TASK_CONFIG_STRUCTS.get(1));    logOffset = 7;    expectStart(existingRecords, deserialized);    // Shouldn't see any callbacks since this is during startup    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    configStorage.start();    // Should see a single connector and its config should be the last one seen anywhere in the log    ClusterConfigState configState = configStorage.snapshot();    // Should always be next to be read, even if uncommitted    assertEquals(7, configState.offset());    assertEquals(Arrays.asList(CONNECTOR_IDS.get(0)), new ArrayList<>(configState.connectors()));    assertEquals(TargetState.STARTED, configState.targetState(CONNECTOR_IDS.get(0)));    // CONNECTOR_CONFIG_STRUCTS[2] -> SAMPLE_CONFIGS[2]    assertEquals(SAMPLE_CONFIGS.get(2), configState.connectorConfig(CONNECTOR_IDS.get(0)));    // Should see 2 tasks for that connector. Only config updates before the root key update should be reflected    assertEquals(Arrays.asList(TASK_IDS.get(0), TASK_IDS.get(1)), configState.tasks(CONNECTOR_IDS.get(0)));    // Both TASK_CONFIG_STRUCTS[0] -> SAMPLE_CONFIGS[0]    assertEquals(SAMPLE_CONFIGS.get(0), configState.taskConfig(TASK_IDS.get(0)));    assertEquals(SAMPLE_CONFIGS.get(0), configState.taskConfig(TASK_IDS.get(1)));    assertEquals(Collections.EMPTY_SET, configState.inconsistentConnectors());    configStorage.stop();    PowerMock.verifyAll();}
f11601
0
testRestoreConnectorDeletion
public void kafkatest_f11602_0() throws Exception
{    // Restoring data should notify only of the latest values after loading is complete. This also validates    // that inconsistent state is ignored.    expectConfigure();    // Overwrite each type at least once to ensure we see the latest data after loading    List<ConsumerRecord<String, byte[]>> existingRecords = Arrays.asList(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0)), new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(1)), new ConsumerRecord<>(TOPIC, 0, 2, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(1), CONFIGS_SERIALIZED.get(2)), new ConsumerRecord<>(TOPIC, 0, 3, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(3)), new ConsumerRecord<>(TOPIC, 0, 4, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TARGET_STATE_KEYS.get(0), CONFIGS_SERIALIZED.get(4)), new ConsumerRecord<>(TOPIC, 0, 5, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, COMMIT_TASKS_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(5)));    LinkedHashMap<byte[], Struct> deserialized = new LinkedHashMap<>();    deserialized.put(CONFIGS_SERIALIZED.get(0), CONNECTOR_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(1), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(2), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(3), null);    deserialized.put(CONFIGS_SERIALIZED.get(4), null);    deserialized.put(CONFIGS_SERIALIZED.get(5), TASKS_COMMIT_STRUCT_TWO_TASK_CONNECTOR);    logOffset = 6;    expectStart(existingRecords, deserialized);    // Shouldn't see any callbacks since this is during startup    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    configStorage.start();    // Should see a single connector and its config should be the last one seen anywhere in the log    ClusterConfigState configState = configStorage.snapshot();    // Should always be next to be read, even if uncommitted    assertEquals(6, configState.offset());    assertTrue(configState.connectors().isEmpty());    configStorage.stop();    PowerMock.verifyAll();}
f11602
0
testRestoreZeroTasks
public void kafkatest_f11603_0() throws Exception
{    // Restoring data should notify only of the latest values after loading is complete. This also validates    // that inconsistent state is ignored.    expectConfigure();    // Overwrite each type at least once to ensure we see the latest data after loading    List<ConsumerRecord<String, byte[]>> existingRecords = Arrays.asList(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0)), new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(1)), new ConsumerRecord<>(TOPIC, 0, 2, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(1), CONFIGS_SERIALIZED.get(2)), new ConsumerRecord<>(TOPIC, 0, 3, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(3)), new ConsumerRecord<>(TOPIC, 0, 4, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, COMMIT_TASKS_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(4)), // Connector after root update should make it through, task update shouldn't    new ConsumerRecord<>(TOPIC, 0, 5, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(5)), new ConsumerRecord<>(TOPIC, 0, 6, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(6)), new ConsumerRecord<>(TOPIC, 0, 7, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, COMMIT_TASKS_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(7)));    LinkedHashMap<byte[], Struct> deserialized = new LinkedHashMap<>();    deserialized.put(CONFIGS_SERIALIZED.get(0), CONNECTOR_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(1), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(2), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(3), CONNECTOR_CONFIG_STRUCTS.get(1));    deserialized.put(CONFIGS_SERIALIZED.get(4), TASKS_COMMIT_STRUCT_TWO_TASK_CONNECTOR);    deserialized.put(CONFIGS_SERIALIZED.get(5), CONNECTOR_CONFIG_STRUCTS.get(2));    deserialized.put(CONFIGS_SERIALIZED.get(6), TASK_CONFIG_STRUCTS.get(1));    deserialized.put(CONFIGS_SERIALIZED.get(7), TASKS_COMMIT_STRUCT_ZERO_TASK_CONNECTOR);    logOffset = 8;    expectStart(existingRecords, deserialized);    // Shouldn't see any callbacks since this is during startup    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    configStorage.start();    // Should see a single connector and its config should be the last one seen anywhere in the log    ClusterConfigState configState = configStorage.snapshot();    // Should always be next to be read, even if uncommitted    assertEquals(8, configState.offset());    assertEquals(Arrays.asList(CONNECTOR_IDS.get(0)), new ArrayList<>(configState.connectors()));    // CONNECTOR_CONFIG_STRUCTS[2] -> SAMPLE_CONFIGS[2]    assertEquals(SAMPLE_CONFIGS.get(2), configState.connectorConfig(CONNECTOR_IDS.get(0)));    // Should see 0 tasks for that connector.    assertEquals(Collections.emptyList(), configState.tasks(CONNECTOR_IDS.get(0)));    // Both TASK_CONFIG_STRUCTS[0] -> SAMPLE_CONFIGS[0]    assertEquals(Collections.EMPTY_SET, configState.inconsistentConnectors());    configStorage.stop();    PowerMock.verifyAll();}
f11603
0
expectConvertWriteRead
private void kafkatest_f11611_0(final String configKey, final Schema valueSchema, final byte[] serialized, final String dataFieldName, final Object dataFieldValue)
{    final Capture<Struct> capturedRecord = EasyMock.newCapture();    if (serialized != null)        EasyMock.expect(converter.fromConnectData(EasyMock.eq(TOPIC), EasyMock.eq(valueSchema), EasyMock.capture(capturedRecord))).andReturn(serialized);    storeLog.send(EasyMock.eq(configKey), EasyMock.aryEq(serialized));    PowerMock.expectLastCall();    EasyMock.expect(converter.toConnectData(EasyMock.eq(TOPIC), EasyMock.aryEq(serialized))).andAnswer(new IAnswer<SchemaAndValue>() {        @Override        public SchemaAndValue answer() throws Throwable {            if (dataFieldName != null)                assertEquals(dataFieldValue, capturedRecord.getValue().get(dataFieldName));            // Note null schema because default settings for internal serialization are schema-less            return new SchemaAndValue(null, serialized == null ? null : structToMap(capturedRecord.getValue()));        }    });}
f11611
0
answer
public SchemaAndValue kafkatest_f11612_0() throws Throwable
{    if (dataFieldName != null)        assertEquals(dataFieldValue, capturedRecord.getValue().get(dataFieldName));    // Note null schema because default settings for internal serialization are schema-less    return new SchemaAndValue(null, serialized == null ? null : structToMap(capturedRecord.getValue()));}
f11612
0
expectReadToEnd
private void kafkatest_f11613_0(final LinkedHashMap<String, byte[]> serializedConfigs)
{    EasyMock.expect(storeLog.readToEnd()).andAnswer(new IAnswer<Future<Void>>() {        @Override        public Future<Void> answer() throws Throwable {            TestFuture<Void> future = new TestFuture<Void>();            for (Map.Entry<String, byte[]> entry : serializedConfigs.entrySet()) capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 0, logOffset++, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, entry.getKey(), entry.getValue()));            future.resolveOnGet((Void) null);            return future;        }    });}
f11613
0
testReloadOnStart
public void kafkatest_f11621_0() throws Exception
{    expectConfigure();    expectStart(Arrays.asList(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY.array(), TP0_VALUE.array()), new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), TP1_VALUE.array()), new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY.array(), TP0_VALUE_NEW.array()), new ConsumerRecord<>(TOPIC, 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), TP1_VALUE_NEW.array())));    expectStop();    PowerMock.replayAll();    store.configure(DEFAULT_DISTRIBUTED_CONFIG);    store.start();    HashMap<ByteBuffer, ByteBuffer> data = Whitebox.getInternalState(store, "data");    assertEquals(TP0_VALUE_NEW, data.get(TP0_KEY));    assertEquals(TP1_VALUE_NEW, data.get(TP1_KEY));    store.stop();    PowerMock.verifyAll();}
f11621
0
testGetSet
public void kafkatest_f11622_0() throws Exception
{    expectConfigure();    expectStart(Collections.emptyList());    expectStop();    // First get() against an empty store    final Capture<Callback<Void>> firstGetReadToEndCallback = EasyMock.newCapture();    storeLog.readToEnd(EasyMock.capture(firstGetReadToEndCallback));    PowerMock.expectLastCall().andAnswer(() -> {        firstGetReadToEndCallback.getValue().onCompletion(null, null);        return null;    });    // Set offsets    Capture<org.apache.kafka.clients.producer.Callback> callback0 = EasyMock.newCapture();    storeLog.send(EasyMock.aryEq(TP0_KEY.array()), EasyMock.aryEq(TP0_VALUE.array()), EasyMock.capture(callback0));    PowerMock.expectLastCall();    Capture<org.apache.kafka.clients.producer.Callback> callback1 = EasyMock.newCapture();    storeLog.send(EasyMock.aryEq(TP1_KEY.array()), EasyMock.aryEq(TP1_VALUE.array()), EasyMock.capture(callback1));    PowerMock.expectLastCall();    // Second get() should get the produced data and return the new values    final Capture<Callback<Void>> secondGetReadToEndCallback = EasyMock.newCapture();    storeLog.readToEnd(EasyMock.capture(secondGetReadToEndCallback));    PowerMock.expectLastCall().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY.array(), TP0_VALUE.array()));            capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), TP1_VALUE.array()));            secondGetReadToEndCallback.getValue().onCompletion(null, null);            return null;        }    });    // Third get() should pick up data produced by someone else and return those values    final Capture<Callback<Void>> thirdGetReadToEndCallback = EasyMock.newCapture();    storeLog.readToEnd(EasyMock.capture(thirdGetReadToEndCallback));    PowerMock.expectLastCall().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY.array(), TP0_VALUE_NEW.array()));            capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), TP1_VALUE_NEW.array()));            thirdGetReadToEndCallback.getValue().onCompletion(null, null);            return null;        }    });    PowerMock.replayAll();    store.configure(DEFAULT_DISTRIBUTED_CONFIG);    store.start();    // Getting from empty store should return nulls    final AtomicBoolean getInvokedAndPassed = new AtomicBoolean(false);    store.get(Arrays.asList(TP0_KEY, TP1_KEY), new Callback<Map<ByteBuffer, ByteBuffer>>() {        @Override        public void onCompletion(Throwable error, Map<ByteBuffer, ByteBuffer> result) {            // Since we didn't read them yet, these will be null            assertEquals(null, result.get(TP0_KEY));            assertEquals(null, result.get(TP1_KEY));            getInvokedAndPassed.set(true);        }    }).get(10000, TimeUnit.MILLISECONDS);    assertTrue(getInvokedAndPassed.get());    // Set some offsets    Map<ByteBuffer, ByteBuffer> toSet = new HashMap<>();    toSet.put(TP0_KEY, TP0_VALUE);    toSet.put(TP1_KEY, TP1_VALUE);    final AtomicBoolean invoked = new AtomicBoolean(false);    Future<Void> setFuture = store.set(toSet, new Callback<Void>() {        @Override        public void onCompletion(Throwable error, Void result) {            invoked.set(true);        }    });    assertFalse(setFuture.isDone());    // Out of order callbacks shouldn't matter, should still require all to be invoked before invoking the callback    // for the store's set callback    callback1.getValue().onCompletion(null, null);    assertFalse(invoked.get());    callback0.getValue().onCompletion(null, null);    setFuture.get(10000, TimeUnit.MILLISECONDS);    assertTrue(invoked.get());    // Getting data should read to end of our published data and return it    final AtomicBoolean secondGetInvokedAndPassed = new AtomicBoolean(false);    store.get(Arrays.asList(TP0_KEY, TP1_KEY), new Callback<Map<ByteBuffer, ByteBuffer>>() {        @Override        public void onCompletion(Throwable error, Map<ByteBuffer, ByteBuffer> result) {            assertEquals(TP0_VALUE, result.get(TP0_KEY));            assertEquals(TP1_VALUE, result.get(TP1_KEY));            secondGetInvokedAndPassed.set(true);        }    }).get(10000, TimeUnit.MILLISECONDS);    assertTrue(secondGetInvokedAndPassed.get());    // Getting data should read to end of our published data and return it    final AtomicBoolean thirdGetInvokedAndPassed = new AtomicBoolean(false);    store.get(Arrays.asList(TP0_KEY, TP1_KEY), new Callback<Map<ByteBuffer, ByteBuffer>>() {        @Override        public void onCompletion(Throwable error, Map<ByteBuffer, ByteBuffer> result) {            assertEquals(TP0_VALUE_NEW, result.get(TP0_KEY));            assertEquals(TP1_VALUE_NEW, result.get(TP1_KEY));            thirdGetInvokedAndPassed.set(true);        }    }).get(10000, TimeUnit.MILLISECONDS);    assertTrue(thirdGetInvokedAndPassed.get());    store.stop();    PowerMock.verifyAll();}
f11622
0
answer
public Object kafkatest_f11623_0() throws Throwable
{    capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY.array(), TP0_VALUE.array()));    capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), TP1_VALUE.array()));    secondGetReadToEndCallback.getValue().onCompletion(null, null);    return null;}
f11623
0
onCompletion
public void kafkatest_f11631_0(Throwable error, Map<ByteBuffer, ByteBuffer> result)
{    assertEquals(TP0_VALUE, result.get(null));    assertNull(result.get(TP1_KEY));    secondGetInvokedAndPassed.set(true);}
f11631
0
testSetFailure
public void kafkatest_f11632_0() throws Exception
{    expectConfigure();    expectStart(Collections.emptyList());    expectStop();    // Set offsets    Capture<org.apache.kafka.clients.producer.Callback> callback0 = EasyMock.newCapture();    storeLog.send(EasyMock.aryEq(TP0_KEY.array()), EasyMock.aryEq(TP0_VALUE.array()), EasyMock.capture(callback0));    PowerMock.expectLastCall();    Capture<org.apache.kafka.clients.producer.Callback> callback1 = EasyMock.newCapture();    storeLog.send(EasyMock.aryEq(TP1_KEY.array()), EasyMock.aryEq(TP1_VALUE.array()), EasyMock.capture(callback1));    PowerMock.expectLastCall();    Capture<org.apache.kafka.clients.producer.Callback> callback2 = EasyMock.newCapture();    storeLog.send(EasyMock.aryEq(TP2_KEY.array()), EasyMock.aryEq(TP2_VALUE.array()), EasyMock.capture(callback2));    PowerMock.expectLastCall();    PowerMock.replayAll();    store.configure(DEFAULT_DISTRIBUTED_CONFIG);    store.start();    // Set some offsets    Map<ByteBuffer, ByteBuffer> toSet = new HashMap<>();    toSet.put(TP0_KEY, TP0_VALUE);    toSet.put(TP1_KEY, TP1_VALUE);    toSet.put(TP2_KEY, TP2_VALUE);    final AtomicBoolean invoked = new AtomicBoolean(false);    final AtomicBoolean invokedFailure = new AtomicBoolean(false);    Future<Void> setFuture = store.set(toSet, new Callback<Void>() {        @Override        public void onCompletion(Throwable error, Void result) {            invoked.set(true);            if (error != null)                invokedFailure.set(true);        }    });    assertFalse(setFuture.isDone());    // Out of order callbacks shouldn't matter, should still require all to be invoked before invoking the callback    // for the store's set callback    callback1.getValue().onCompletion(null, null);    assertFalse(invoked.get());    callback2.getValue().onCompletion(null, new KafkaException("bogus error"));    assertTrue(invoked.get());    assertTrue(invokedFailure.get());    callback0.getValue().onCompletion(null, null);    try {        setFuture.get(10000, TimeUnit.MILLISECONDS);        fail("Should have seen KafkaException thrown when waiting on KafkaOffsetBackingStore.set() future");    } catch (ExecutionException e) {        // expected        assertNotNull(e.getCause());        assertTrue(e.getCause() instanceof KafkaException);    }    store.stop();    PowerMock.verifyAll();}
f11632
0
onCompletion
public void kafkatest_f11633_0(Throwable error, Void result)
{    invoked.set(true);    if (error != null)        invokedFailure.set(true);}
f11633
0
putConnectorStateRetriableFailure
public void kafkatest_f11641_0()
{    KafkaBasedLog<String, byte[]> kafkaBasedLog = mock(KafkaBasedLog.class);    Converter converter = mock(Converter.class);    KafkaStatusBackingStore store = new KafkaStatusBackingStore(new MockTime(), converter, STATUS_TOPIC, kafkaBasedLog);    byte[] value = new byte[0];    expect(converter.fromConnectData(eq(STATUS_TOPIC), anyObject(Schema.class), anyObject(Struct.class))).andStubReturn(value);    final Capture<Callback> callbackCapture = newCapture();    kafkaBasedLog.send(eq("status-connector-conn"), eq(value), capture(callbackCapture));    expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callbackCapture.getValue().onCompletion(null, new TimeoutException());            return null;        }    }).andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callbackCapture.getValue().onCompletion(null, null);            return null;        }    });    replayAll();    ConnectorStatus status = new ConnectorStatus(CONNECTOR, ConnectorStatus.State.RUNNING, WORKER_ID, 0);    store.put(status);    // state is not visible until read back from the log    assertEquals(null, store.get(CONNECTOR));    verifyAll();}
f11641
0
answer
public Void kafkatest_f11642_0() throws Throwable
{    callbackCapture.getValue().onCompletion(null, new TimeoutException());    return null;}
f11642
0
answer
public Void kafkatest_f11643_0() throws Throwable
{    callbackCapture.getValue().onCompletion(null, null);    return null;}
f11643
0
answer
public Void kafkatest_f11651_0() throws Throwable
{    callbackCapture.getValue().onCompletion(null, null);    store.read(consumerRecord(1, "status-connector-conn", value));    return null;}
f11651
0
readConnectorState
public void kafkatest_f11652_0()
{    byte[] value = new byte[0];    KafkaBasedLog<String, byte[]> kafkaBasedLog = mock(KafkaBasedLog.class);    Converter converter = mock(Converter.class);    KafkaStatusBackingStore store = new KafkaStatusBackingStore(new MockTime(), converter, STATUS_TOPIC, kafkaBasedLog);    Map<String, Object> statusMap = new HashMap<>();    statusMap.put("worker_id", WORKER_ID);    statusMap.put("state", "RUNNING");    statusMap.put("generation", 0L);    expect(converter.toConnectData(STATUS_TOPIC, value)).andReturn(new SchemaAndValue(null, statusMap));    replayAll();    store.read(consumerRecord(0, "status-connector-conn", value));    ConnectorStatus status = new ConnectorStatus(CONNECTOR, ConnectorStatus.State.RUNNING, WORKER_ID, 0);    assertEquals(status, store.get(CONNECTOR));    verifyAll();}
f11652
0
putTaskState
public void kafkatest_f11653_0()
{    KafkaBasedLog<String, byte[]> kafkaBasedLog = mock(KafkaBasedLog.class);    Converter converter = mock(Converter.class);    KafkaStatusBackingStore store = new KafkaStatusBackingStore(new MockTime(), converter, STATUS_TOPIC, kafkaBasedLog);    byte[] value = new byte[0];    expect(converter.fromConnectData(eq(STATUS_TOPIC), anyObject(Schema.class), anyObject(Struct.class))).andStubReturn(value);    final Capture<Callback> callbackCapture = newCapture();    kafkaBasedLog.send(eq("status-task-conn-0"), eq(value), capture(callbackCapture));    expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callbackCapture.getValue().onCompletion(null, null);            return null;        }    });    replayAll();    TaskStatus status = new TaskStatus(TASK, TaskStatus.State.RUNNING, WORKER_ID, 0);    store.put(status);    // state is not visible until read back from the log    assertEquals(null, store.get(TASK));    verifyAll();}
f11653
0
setup
public void kafkatest_f11661_0()
{    writer = new OffsetStorageWriter(store, NAMESPACE, keyConverter, valueConverter);    service = Executors.newFixedThreadPool(1);}
f11661
0
teardown
public void kafkatest_f11662_0()
{    service.shutdownNow();}
f11662
0
testWriteFlush
public void kafkatest_f11663_0() throws Exception
{    @SuppressWarnings("unchecked")    Callback<Void> callback = PowerMock.createMock(Callback.class);    expectStore(OFFSET_KEY, OFFSET_KEY_SERIALIZED, OFFSET_VALUE, OFFSET_VALUE_SERIALIZED, callback, false, null);    PowerMock.replayAll();    writer.offset(OFFSET_KEY, OFFSET_VALUE);    assertTrue(writer.beginFlush());    writer.doFlush(callback).get(1000, TimeUnit.MILLISECONDS);    PowerMock.verifyAll();}
f11663
0
expectStore
private void kafkatest_f11671_0(Map<String, String> key, byte[] keySerialized, Map<String, Integer> value, byte[] valueSerialized, final Callback<Void> callback, final boolean fail, final CountDownLatch waitForCompletion)
{    List<Object> keyWrapped = Arrays.asList(NAMESPACE, key);    EasyMock.expect(keyConverter.fromConnectData(NAMESPACE, null, keyWrapped)).andReturn(keySerialized);    EasyMock.expect(valueConverter.fromConnectData(NAMESPACE, null, value)).andReturn(valueSerialized);    final Capture<Callback<Void>> storeCallback = Capture.newInstance();    final Map<ByteBuffer, ByteBuffer> offsetsSerialized = Collections.singletonMap(keySerialized == null ? null : ByteBuffer.wrap(keySerialized), valueSerialized == null ? null : ByteBuffer.wrap(valueSerialized));    EasyMock.expect(store.set(EasyMock.eq(offsetsSerialized), EasyMock.capture(storeCallback))).andAnswer(new IAnswer<Future<Void>>() {        @Override        public Future<Void> answer() throws Throwable {            return service.submit(new Callable<Void>() {                @Override                public Void call() throws Exception {                    if (waitForCompletion != null)                        assertTrue(waitForCompletion.await(10000, TimeUnit.MILLISECONDS));                    if (fail) {                        storeCallback.getValue().onCompletion(exception, null);                    } else {                        storeCallback.getValue().onCompletion(null, null);                    }                    return null;                }            });        }    });    if (callback != null) {        if (fail) {            callback.onCompletion(EasyMock.eq(exception), EasyMock.eq((Void) null));        } else {            callback.onCompletion(null, null);        }    }    PowerMock.expectLastCall();}
f11671
0
answer
public Future<Void> kafkatest_f11672_0() throws Throwable
{    return service.submit(new Callable<Void>() {        @Override        public Void call() throws Exception {            if (waitForCompletion != null)                assertTrue(waitForCompletion.await(10000, TimeUnit.MILLISECONDS));            if (fail) {                storeCallback.getValue().onCompletion(exception, null);            } else {                storeCallback.getValue().onCompletion(null, null);            }            return null;        }    });}
f11672
0
call
public Void kafkatest_f11673_0() throws Exception
{    if (waitForCompletion != null)        assertTrue(waitForCompletion.await(10000, TimeUnit.MILLISECONDS));    if (fail) {        storeCallback.getValue().onCompletion(exception, null);    } else {        storeCallback.getValue().onCompletion(null, null);    }    return null;}
f11673
0
removeWorker
public void kafkatest_f11681_0(WorkerHandle worker)
{    if (connectCluster.isEmpty()) {        throw new IllegalStateException("Cannot remove worker. Cluster is empty");    }    stopWorker(worker);    connectCluster.remove(worker);}
f11681
0
stopWorker
private voidf11682_1WorkerHandle worker)
{    try {                worker.stop();    } catch (UngracefulShutdownException e) {            } catch (Exception e) {                throw new RuntimeException("Could not stop worker", e);    }}
private voidf11682
1
startConnect
public voidf11683_1)
{        workerProps.put(BOOTSTRAP_SERVERS_CONFIG, kafka().bootstrapServers());    workerProps.put(REST_HOST_NAME_CONFIG, REST_HOST_NAME);    // use a random available port    workerProps.put(REST_PORT_CONFIG, "0");    String internalTopicsReplFactor = String.valueOf(numBrokers);    putIfAbsent(workerProps, GROUP_ID_CONFIG, "connect-integration-test-" + connectClusterName);    putIfAbsent(workerProps, OFFSET_STORAGE_TOPIC_CONFIG, "connect-offset-topic-" + connectClusterName);    putIfAbsent(workerProps, OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG, internalTopicsReplFactor);    putIfAbsent(workerProps, CONFIG_TOPIC_CONFIG, "connect-config-topic-" + connectClusterName);    putIfAbsent(workerProps, CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG, internalTopicsReplFactor);    putIfAbsent(workerProps, STATUS_STORAGE_TOPIC_CONFIG, "connect-storage-topic-" + connectClusterName);    putIfAbsent(workerProps, STATUS_STORAGE_REPLICATION_FACTOR_CONFIG, internalTopicsReplFactor);    putIfAbsent(workerProps, KEY_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.storage.StringConverter");    putIfAbsent(workerProps, VALUE_CONVERTER_CLASS_CONFIG, "org.apache.kafka.connect.storage.StringConverter");    for (int i = 0; i < numInitialWorkers; i++) {        addWorker();    }}
public voidf11683
1
putIfAbsent
private static void kafkatest_f11691_0(Map<String, String> props, String propertyKey, String propertyValue)
{    if (!props.containsKey(propertyKey)) {        props.put(propertyKey, propertyValue);    }}
f11691
0
kafka
public EmbeddedKafkaCluster kafkatest_f11692_0()
{    return kafkaCluster;}
f11692
0
executePut
public intf11693_1String url, String body) throws IOException
{        HttpURLConnection httpCon = (HttpURLConnection) new URL(url).openConnection();    httpCon.setDoOutput(true);    httpCon.setRequestProperty("Content-Type", "application/json");    httpCon.setRequestMethod("PUT");    try (OutputStreamWriter out = new OutputStreamWriter(httpCon.getOutputStream())) {        out.write(body);    }    if (httpCon.getResponseCode() < HttpURLConnection.HTTP_BAD_REQUEST) {        try (InputStream is = httpCon.getInputStream()) {                    }    } else {        try (InputStream is = httpCon.getErrorStream()) {                    }    }    return httpCon.getResponseCode();}
public intf11693
1
brokerProps
public Builder kafkatest_f11701_0(Properties brokerProps)
{    this.brokerProps = brokerProps;    return this;}
f11701
0
maskExitProcedures
public Builder kafkatest_f11702_0(boolean mask)
{    this.maskExitProcedures = mask;    return this;}
f11702
0
build
public EmbeddedConnectCluster kafkatest_f11703_0()
{    return new EmbeddedConnectCluster(name, workerProps, numWorkers, numBrokers, brokerProps, maskExitProcedures);}
f11703
0
address
public String kafkatest_f11711_0(KafkaServer server)
{    return server.config().hostName() + ":" + server.boundPort(listenerName);}
f11711
0
zKConnectString
public String kafkatest_f11712_0()
{    return "127.0.0.1:" + zookeeper.port();}
f11712
0
createTopic
public void kafkatest_f11713_0(String topic)
{    createTopic(topic, 1);}
f11713
0
createConsumer
public KafkaConsumer<byte[], byte[]> kafkatest_f11721_0(Map<String, Object> consumerProps)
{    Map<String, Object> props = new HashMap<>(consumerProps);    putIfAbsent(props, GROUP_ID_CONFIG, UUID.randomUUID().toString());    putIfAbsent(props, BOOTSTRAP_SERVERS_CONFIG, bootstrapServers());    putIfAbsent(props, ENABLE_AUTO_COMMIT_CONFIG, "false");    putIfAbsent(props, AUTO_OFFSET_RESET_CONFIG, "earliest");    putIfAbsent(props, KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.ByteArrayDeserializer");    putIfAbsent(props, VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.ByteArrayDeserializer");    KafkaConsumer<byte[], byte[]> consumer;    try {        consumer = new KafkaConsumer<>(props);    } catch (Throwable t) {        throw new ConnectException("Failed to create consumer", t);    }    return consumer;}
f11721
0
createConsumerAndSubscribeTo
public KafkaConsumer<byte[], byte[]> kafkatest_f11722_0(Map<String, Object> consumerProps, String... topics)
{    KafkaConsumer<byte[], byte[]> consumer = createConsumer(consumerProps);    consumer.subscribe(Arrays.asList(topics));    return consumer;}
f11722
0
putIfAbsent
private static void kafkatest_f11723_0(final Map<String, Object> props, final String propertyKey, final Object propertyValue)
{    if (!props.containsKey(propertyKey)) {        props.put(propertyKey, propertyValue);    }}
f11723
0
testLookupKafkaClusterId
public void kafkatest_f11731_0()
{    final Node broker1 = new Node(0, "dummyHost-1", 1234);    final Node broker2 = new Node(1, "dummyHost-2", 1234);    List<Node> cluster = Arrays.asList(broker1, broker2);    MockAdminClient adminClient = new MockAdminClient(cluster, broker1);    assertEquals(MockAdminClient.DEFAULT_CLUSTER_ID, ConnectUtils.lookupKafkaClusterId(adminClient));}
f11731
0
testLookupNullKafkaClusterId
public void kafkatest_f11732_0()
{    final Node broker1 = new Node(0, "dummyHost-1", 1234);    final Node broker2 = new Node(1, "dummyHost-2", 1234);    List<Node> cluster = Arrays.asList(broker1, broker2);    MockAdminClient adminClient = new MockAdminClient(cluster, broker1, null);    assertNull(ConnectUtils.lookupKafkaClusterId(adminClient));}
f11732
0
testLookupKafkaClusterIdTimeout
public void kafkatest_f11733_0()
{    final Node broker1 = new Node(0, "dummyHost-1", 1234);    final Node broker2 = new Node(1, "dummyHost-2", 1234);    List<Node> cluster = Arrays.asList(broker1, broker2);    MockAdminClient adminClient = new MockAdminClient(cluster, broker1);    adminClient.timeoutNextRequest(1);    ConnectUtils.lookupKafkaClusterId(adminClient);}
f11733
0
run
public void kafkatest_f11741_0()
{    finishedLatch.countDown();}
f11741
0
testReloadOnStartWithNoNewRecordsPresent
public void kafkatest_f11742_0() throws Exception
{    expectStart();    expectStop();    PowerMock.replayAll();    Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(TP0, 7L);    endOffsets.put(TP1, 7L);    consumer.updateEndOffsets(endOffsets);    // Better test with an advanced offset other than just 0L    consumer.updateBeginningOffsets(endOffsets);    consumer.schedulePollTask(new Runnable() {        @Override        public void run() {            // time outs (for instance via ConnectRestException)            throw new WakeupException();        }    });    store.start();    assertEquals(CONSUMER_ASSIGNMENT, consumer.assignment());    assertEquals(7L, consumer.position(TP0));    assertEquals(7L, consumer.position(TP1));    store.stop();    assertFalse(Whitebox.<Thread>getInternalState(store, "thread").isAlive());    assertTrue(consumer.closed());    PowerMock.verifyAll();}
f11742
0
run
public void kafkatest_f11743_0()
{    // time outs (for instance via ConnectRestException)    throw new WakeupException();}
f11743
0
run
public void kafkatest_f11751_0()
{    // Trigger exception    consumer.schedulePollTask(new Runnable() {        @Override        public void run() {            consumer.setPollException(Errors.COORDINATOR_NOT_AVAILABLE.exception());        }    });    // Should keep polling until it reaches current log end offset for all partitions    consumer.scheduleNopPollTask();    consumer.scheduleNopPollTask();    consumer.schedulePollTask(new Runnable() {        @Override        public void run() {            consumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE_NEW));            consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE_NEW));        }    });    consumer.schedulePollTask(new Runnable() {        @Override        public void run() {            finishedLatch.countDown();        }    });}
f11751
0
run
public void kafkatest_f11752_0()
{    consumer.setPollException(Errors.COORDINATOR_NOT_AVAILABLE.exception());}
f11752
0
run
public void kafkatest_f11753_0()
{    consumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE_NEW));    consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE_NEW));}
f11753
0
onCompletion
public void kafkatest_f11761_0(RecordMetadata metadata, Exception exception)
{    // Should only be invoked once    assertNull(setException.get());    setException.set(exception);}
f11761
0
expectStart
private void kafkatest_f11762_0() throws Exception
{    initializer.run();    EasyMock.expectLastCall().times(1);    PowerMock.expectPrivate(store, "createProducer").andReturn(producer);    PowerMock.expectPrivate(store, "createConsumer").andReturn(consumer);}
f11762
0
expectStop
private void kafkatest_f11763_0()
{    producer.close();    PowerMock.expectLastCall();// MockConsumer close is checked after test.}
f11763
0
shouldCreateConnectorLoggingContext
public voidf11771_1)
{    assertMdcExtrasUntouched();    assertMdc(null, null, null);    try (LoggingContext loggingContext = LoggingContext.forConnector(CONNECTOR_NAME)) {        assertMdc(CONNECTOR_NAME, null, Scope.WORKER);            }    assertMdcExtrasUntouched();    assertMdc(null, null, null);}
public voidf11771
1
shouldCreateTaskLoggingContext
public voidf11772_1)
{    assertMdcExtrasUntouched();    try (LoggingContext loggingContext = LoggingContext.forTask(TASK_ID1)) {        assertMdc(TASK_ID1.connector(), TASK_ID1.task(), Scope.TASK);            }    assertMdcExtrasUntouched();    assertMdc(null, null, null);}
public voidf11772
1
shouldCreateOffsetsLoggingContext
public voidf11773_1)
{    assertMdcExtrasUntouched();    try (LoggingContext loggingContext = LoggingContext.forOffsets(TASK_ID1)) {        assertMdc(TASK_ID1.connector(), TASK_ID1.task(), Scope.OFFSETS);            }    assertMdcExtrasUntouched();    assertMdc(null, null, null);}
public voidf11773
1
testForcibleShutdown
public void kafkatest_f11781_0() throws InterruptedException
{    final CountDownLatch startedLatch = new CountDownLatch(1);    ShutdownableThread thread = new ShutdownableThread("forcible") {        @Override        public void execute() {            try {                startedLatch.countDown();                Thread.sleep(100000);            } catch (InterruptedException e) {            // Ignore            }        }    };    thread.start();    startedLatch.await();    thread.forceShutdown();    // Not all threads can be forcibly stopped since interrupt() doesn't work on threads in    // certain conditions, but in this case we know the thread is interruptible so we should be    // able join() it    thread.join(1000);    assertFalse(thread.isAlive());}
f11781
0
execute
public void kafkatest_f11782_0()
{    try {        startedLatch.countDown();        Thread.sleep(100000);    } catch (InterruptedException e) {    // Ignore    }}
f11782
0
basicOperations
public void kafkatest_f11783_0()
{    Table<String, Integer, String> table = new Table<>();    table.put("foo", 5, "bar");    table.put("foo", 6, "baz");    assertEquals("bar", table.get("foo", 5));    assertEquals("baz", table.get("foo", 6));    Map<Integer, String> row = table.row("foo");    assertEquals("bar", row.get(5));    assertEquals("baz", row.get(6));    assertEquals("bar", table.remove("foo", 5));    assertNull(table.get("foo", 5));    assertEquals("baz", table.remove("foo", 6));    assertNull(table.get("foo", 6));    assertTrue(table.row("foo").isEmpty());}
f11783
0
get
public T kafkatest_f11791_0() throws InterruptedException, ExecutionException
{    getCalledLatch.countDown();    while (true) {        try {            return get(Integer.MAX_VALUE, TimeUnit.DAYS);        } catch (TimeoutException e) {        // ignore        }    }}
f11791
0
get
public T kafkatest_f11792_0(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException
{    getCalledLatch.countDown();    if (resolveOnGet) {        if (resolveOnGetException != null)            resolve(resolveOnGetException);        else            resolve(resolveOnGetResult);    }    synchronized (this) {        while (!resolved) {            this.wait(TimeUnit.MILLISECONDS.convert(timeout, unit));        }    }    if (exception != null) {        if (exception instanceof TimeoutException)            throw (TimeoutException) exception;        else if (exception instanceof InterruptedException)            throw (InterruptedException) exception;        else            throw new ExecutionException(exception);    }    return result;}
f11792
0
resolveOnGet
public void kafkatest_f11793_0(T val)
{    resolveOnGet = true;    resolveOnGetResult = val;}
f11793
0
returnNullWithClusterAuthorizationFailure
public void kafkatest_f11801_0()
{    final NewTopic newTopic = TopicAdmin.defineTopic("myTopic").partitions(1).compacted().build();    Cluster cluster = createCluster(1);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(new MockTime(), cluster)) {        env.kafkaClient().prepareResponse(createTopicResponseWithClusterAuthorizationException(newTopic));        TopicAdmin admin = new TopicAdmin(null, env.adminClient());        boolean created = admin.createTopic(newTopic);        assertFalse(created);    }}
f11801
0
returnNullWithTopicAuthorizationFailure
public void kafkatest_f11802_0()
{    final NewTopic newTopic = TopicAdmin.defineTopic("myTopic").partitions(1).compacted().build();    Cluster cluster = createCluster(1);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(new MockTime(), cluster)) {        env.kafkaClient().prepareResponse(createTopicResponseWithTopicAuthorizationException(newTopic));        TopicAdmin admin = new TopicAdmin(null, env.adminClient());        boolean created = admin.createTopic(newTopic);        assertFalse(created);    }}
f11802
0
shouldNotCreateTopicWhenItAlreadyExists
public void kafkatest_f11803_0()
{    NewTopic newTopic = TopicAdmin.defineTopic("myTopic").partitions(1).compacted().build();    Cluster cluster = createCluster(1);    try (MockAdminClient mockAdminClient = new MockAdminClient(cluster.nodes(), cluster.nodeById(0))) {        TopicPartitionInfo topicPartitionInfo = new TopicPartitionInfo(0, cluster.nodeById(0), cluster.nodes(), Collections.<Node>emptyList());        mockAdminClient.addTopic(false, "myTopic", Collections.singletonList(topicPartitionInfo), null);        TopicAdmin admin = new TopicAdmin(null, mockAdminClient);        assertFalse(admin.createTopic(newTopic));    }}
f11803
0
createTopicResponse
private CreateTopicsResponse kafkatest_f11811_0(ApiError error, NewTopic... topics)
{    if (error == null)        error = new ApiError(Errors.NONE, "");    CreateTopicsResponseData response = new CreateTopicsResponseData();    for (NewTopic topic : topics) {        response.topics().add(new CreatableTopicResult().setName(topic.name()).setErrorCode(error.error().code()).setErrorMessage(error.message()));    }    return new CreateTopicsResponse(response);}
f11811
0
ensureValid
public void kafkatest_f11812_0(String name, Object valueObject)
{    List<String> value = (List<String>) valueObject;    if (value == null || value.isEmpty()) {        throw new ConfigException("Must specify at least one field to cast.");    }    parseFieldTypes(value);}
f11812
0
toString
public String kafkatest_f11813_0()
{    return "list of colon-delimited pairs, e.g. <code>foo:bar,abc:xyz</code>";}
f11813
0
castValueToType
private static Object kafkatest_f11822_0(Schema schema, Object value, Schema.Type targetType)
{    try {        if (value == null)            return null;        Schema.Type inferredType = schema == null ? ConnectSchema.schemaType(value.getClass()) : schema.type();        if (inferredType == null) {            throw new DataException("Cast transformation was passed a value of type " + value.getClass() + " which is not supported by Connect's data API");        }        // Ensure the type we are trying to cast from is supported        validCastType(inferredType, FieldType.INPUT);        switch(targetType) {            case INT8:                return castToInt8(value);            case INT16:                return castToInt16(value);            case INT32:                return castToInt32(value);            case INT64:                return castToInt64(value);            case FLOAT32:                return castToFloat32(value);            case FLOAT64:                return castToFloat64(value);            case BOOLEAN:                return castToBoolean(value);            case STRING:                return castToString(value);            default:                throw new DataException(targetType.toString() + " is not supported in the Cast transformation.");        }    } catch (NumberFormatException e) {        throw new DataException("Value (" + value.toString() + ") was out of range for requested data type", e);    }}
f11822
0
castToInt8
private static byte kafkatest_f11823_0(Object value)
{    if (value instanceof Number)        return ((Number) value).byteValue();    else if (value instanceof Boolean)        return ((boolean) value) ? (byte) 1 : (byte) 0;    else if (value instanceof String)        return Byte.parseByte((String) value);    else        throw new DataException("Unexpected type in Cast transformation: " + value.getClass());}
f11823
0
castToInt16
private static short kafkatest_f11824_0(Object value)
{    if (value instanceof Number)        return ((Number) value).shortValue();    else if (value instanceof Boolean)        return ((boolean) value) ? (short) 1 : (short) 0;    else if (value instanceof String)        return Short.parseShort((String) value);    else        throw new DataException("Unexpected type in Cast transformation: " + value.getClass());}
f11824
0
validCastType
private static Schema.Type kafkatest_f11832_0(Schema.Type type, FieldType fieldType)
{    switch(fieldType) {        case INPUT:            if (!SUPPORTED_CAST_INPUT_TYPES.contains(type)) {                throw new DataException("Cast transformation does not support casting from " + type + "; supported types are " + SUPPORTED_CAST_INPUT_TYPES);            }            break;        case OUTPUT:            if (!SUPPORTED_CAST_OUTPUT_TYPES.contains(type)) {                throw new ConfigException("Cast transformation does not support casting to " + type + "; supported types are " + SUPPORTED_CAST_OUTPUT_TYPES);            }            break;    }    return type;}
f11832
0
operatingSchema
protected Schema kafkatest_f11833_0(R record)
{    return record.keySchema();}
f11833
0
operatingValue
protected Object kafkatest_f11834_0(R record)
{    return record.key();}
f11834
0
operatingSchema
protected Schema kafkatest_f11843_0(R record)
{    return record.keySchema();}
f11843
0
operatingValue
protected Object kafkatest_f11844_0(R record)
{    return record.key();}
f11844
0
newRecord
protected R kafkatest_f11845_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());}
f11845
0
applySchemaless
private void kafkatest_f11854_0(Map<String, Object> originalRecord, String fieldNamePrefix, Map<String, Object> newRecord)
{    for (Map.Entry<String, Object> entry : originalRecord.entrySet()) {        final String fieldName = fieldName(fieldNamePrefix, entry.getKey());        Object value = entry.getValue();        if (value == null) {            newRecord.put(fieldName(fieldNamePrefix, entry.getKey()), null);            return;        }        Schema.Type inferredType = ConnectSchema.schemaType(value.getClass());        if (inferredType == null) {            throw new DataException("Flatten transformation was passed a value of type " + value.getClass() + " which is not supported by Connect's data API");        }        switch(inferredType) {            case INT8:            case INT16:            case INT32:            case INT64:            case FLOAT32:            case FLOAT64:            case BOOLEAN:            case STRING:            case BYTES:                newRecord.put(fieldName(fieldNamePrefix, entry.getKey()), entry.getValue());                break;            case MAP:                final Map<String, Object> fieldValue = requireMap(entry.getValue(), PURPOSE);                applySchemaless(fieldValue, fieldName, newRecord);                break;            default:                throw new DataException("Flatten transformation does not support " + entry.getValue().getClass() + " for record without schemas (for field " + fieldName + ").");        }    }}
f11854
0
applyWithSchema
private R kafkatest_f11855_0(R record)
{    final Struct value = requireStructOrNull(operatingValue(record), PURPOSE);    Schema schema = operatingSchema(record);    Schema updatedSchema = schemaUpdateCache.get(schema);    if (updatedSchema == null) {        final SchemaBuilder builder = SchemaUtil.copySchemaBasics(schema, SchemaBuilder.struct());        Struct defaultValue = (Struct) schema.defaultValue();        buildUpdatedSchema(schema, "", builder, schema.isOptional(), defaultValue);        updatedSchema = builder.build();        schemaUpdateCache.put(schema, updatedSchema);    }    if (value == null) {        return newRecord(record, updatedSchema, null);    } else {        final Struct updatedValue = new Struct(updatedSchema);        buildWithSchema(value, "", updatedValue);        return newRecord(record, updatedSchema, updatedValue);    }}
f11855
0
buildUpdatedSchema
private void kafkatest_f11856_0(Schema schema, String fieldNamePrefix, SchemaBuilder newSchema, boolean optional, Struct defaultFromParent)
{    for (Field field : schema.fields()) {        final String fieldName = fieldName(fieldNamePrefix, field.name());        final boolean fieldIsOptional = optional || field.schema().isOptional();        Object fieldDefaultValue = null;        if (field.schema().defaultValue() != null) {            fieldDefaultValue = field.schema().defaultValue();        } else if (defaultFromParent != null) {            fieldDefaultValue = defaultFromParent.get(field);        }        switch(field.schema().type()) {            case INT8:            case INT16:            case INT32:            case INT64:            case FLOAT32:            case FLOAT64:            case BOOLEAN:            case STRING:            case BYTES:                newSchema.field(fieldName, convertFieldSchema(field.schema(), fieldIsOptional, fieldDefaultValue));                break;            case STRUCT:                buildUpdatedSchema(field.schema(), fieldName, newSchema, fieldIsOptional, (Struct) fieldDefaultValue);                break;            default:                throw new DataException("Flatten transformation does not support " + field.schema().type() + " for record without schemas (for field " + fieldName + ").");        }    }}
f11856
0
operatingValue
protected Object kafkatest_f11864_0(R record)
{    return record.value();}
f11864
0
newRecord
protected R kafkatest_f11865_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(), updatedSchema, updatedValue, record.timestamp());}
f11865
0
configure
public void kafkatest_f11866_0(Map<String, ?> props)
{    final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);    fieldName = config.getString("field");    schemaUpdateCache = new SynchronizedCache<>(new LRUCache<Schema, Schema>(16));}
f11866
0
operatingValue
protected Object kafkatest_f11874_0(R record)
{    return record.value();}
f11874
0
newRecord
protected R kafkatest_f11875_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(), updatedSchema, updatedValue, record.timestamp());}
f11875
0
parse
public static InsertionSpec kafkatest_f11876_0(String spec)
{    if (spec == null)        return null;    if (spec.endsWith("?")) {        return new InsertionSpec(spec.substring(0, spec.length() - 1), true);    }    if (spec.endsWith("!")) {        return new InsertionSpec(spec.substring(0, spec.length() - 1), false);    }    return new InsertionSpec(spec, true);}
f11876
0
operatingSchema
protected Schema kafkatest_f11884_0(R record)
{    return record.keySchema();}
f11884
0
operatingValue
protected Object kafkatest_f11885_0(R record)
{    return record.key();}
f11885
0
newRecord
protected R kafkatest_f11886_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());}
f11886
0
masked
private static Object kafkatest_f11894_0(Object value)
{    if (value == null)        return null;    Object maskedValue = PRIMITIVE_VALUE_MAPPING.get(value.getClass());    if (maskedValue == null) {        if (value instanceof List)            maskedValue = Collections.emptyList();        else if (value instanceof Map)            maskedValue = Collections.emptyMap();        else            throw new DataException("Cannot mask value of type: " + value.getClass());    }    return maskedValue;}
f11894
0
config
public ConfigDef kafkatest_f11895_0()
{    return CONFIG_DEF;}
f11895
0
operatingSchema
protected Schema kafkatest_f11897_0(R record)
{    return record.keySchema();}
f11897
0
config
public ConfigDef kafkatest_f11906_0()
{    return CONFIG_DEF;}
f11906
0
ensureValid
public void kafkatest_f11907_0(String name, Object value)
{    parseRenameMappings((List<String>) value);}
f11907
0
toString
public String kafkatest_f11908_0()
{    return "list of colon-delimited pairs, e.g. <code>foo:bar,abc:xyz</code>";}
f11908
0
applySchemaless
private R kafkatest_f11916_0(R record)
{    final Map<String, Object> value = requireMap(operatingValue(record), PURPOSE);    final Map<String, Object> updatedValue = new HashMap<>(value.size());    for (Map.Entry<String, Object> e : value.entrySet()) {        final String fieldName = e.getKey();        if (filter(fieldName)) {            final Object fieldValue = e.getValue();            updatedValue.put(renamed(fieldName), fieldValue);        }    }    return newRecord(record, null, updatedValue);}
f11916
0
applyWithSchema
private R kafkatest_f11917_0(R record)
{    final Struct value = requireStruct(operatingValue(record), PURPOSE);    Schema updatedSchema = schemaUpdateCache.get(value.schema());    if (updatedSchema == null) {        updatedSchema = makeUpdatedSchema(value.schema());        schemaUpdateCache.put(value.schema(), updatedSchema);    }    final Struct updatedValue = new Struct(updatedSchema);    for (Field field : updatedSchema.fields()) {        final Object fieldValue = value.get(reverseRenamed(field.name()));        updatedValue.put(field.name(), fieldValue);    }    return newRecord(record, updatedSchema, updatedValue);}
f11917
0
makeUpdatedSchema
private Schema kafkatest_f11918_0(Schema schema)
{    final SchemaBuilder builder = SchemaUtil.copySchemaBasics(schema, SchemaBuilder.struct());    for (Field field : schema.fields()) {        if (filter(field.name())) {            builder.field(renamed(field.name()), field.schema());        }    }    return builder.build();}
f11918
0
newRecord
protected R kafkatest_f11926_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(), updatedSchema, updatedValue, record.timestamp());}
f11926
0
configure
public void kafkatest_f11927_0(Map<String, ?> configs)
{    final SimpleConfig config = new SimpleConfig(CONFIG_DEF, configs);    schemaName = config.getString(ConfigName.SCHEMA_NAME);    schemaVersion = config.getInt(ConfigName.SCHEMA_VERSION);    if (schemaName == null && schemaVersion == null) {        throw new ConfigException("Neither schema name nor version configured");    }}
f11927
0
apply
public R kafkatest_f11928_0(R record)
{    final Schema schema = operatingSchema(record);    requireSchema(schema, "updating schema metadata");    final boolean isArray = schema.type() == Schema.Type.ARRAY;    final boolean isMap = schema.type() == Schema.Type.MAP;    final Schema updatedSchema = new ConnectSchema(schema.type(), schema.isOptional(), schema.defaultValue(), schemaName != null ? schemaName : schema.name(), schemaVersion != null ? schemaVersion : schema.version(), schema.doc(), schema.parameters(), schema.fields(), isMap ? schema.keySchema() : null, isMap || isArray ? schema.valueSchema() : null);    log.trace("Applying SetSchemaMetadata SMT. Original schema: {}, updated schema: {}", schema, updatedSchema);    return newRecord(record, updatedSchema);}
f11928
0
typeSchema
public Schema kafkatest_f11937_0(boolean isOptional)
{    return isOptional ? Schema.OPTIONAL_STRING_SCHEMA : Schema.STRING_SCHEMA;}
f11937
0
toType
public String kafkatest_f11938_0(Config config, Date orig)
{    synchronized (config.format) {        return config.format.format(orig);    }}
f11938
0
toRaw
public Date kafkatest_f11939_0(Config config, Object orig)
{    if (!(orig instanceof Long))        throw new DataException("Expected Unix timestamp to be a Long, but found " + orig.getClass());    return Timestamp.toLogical(Timestamp.SCHEMA, (Long) orig);}
f11939
0
toType
public Date kafkatest_f11947_0(Config config, Date orig)
{    Calendar origCalendar = Calendar.getInstance(UTC);    origCalendar.setTime(orig);    Calendar result = Calendar.getInstance(UTC);    result.setTimeInMillis(0L);    result.set(Calendar.HOUR_OF_DAY, origCalendar.get(Calendar.HOUR_OF_DAY));    result.set(Calendar.MINUTE, origCalendar.get(Calendar.MINUTE));    result.set(Calendar.SECOND, origCalendar.get(Calendar.SECOND));    result.set(Calendar.MILLISECOND, origCalendar.get(Calendar.MILLISECOND));    return result.getTime();}
f11947
0
toRaw
public Date kafkatest_f11948_0(Config config, Object orig)
{    if (!(orig instanceof Date))        throw new DataException("Expected Timestamp to be a java.util.Date, but found " + orig.getClass());    return (Date) orig;}
f11948
0
typeSchema
public Schema kafkatest_f11949_0(boolean isOptional)
{    return isOptional ? OPTIONAL_TIMESTAMP_SCHEMA : Timestamp.SCHEMA;}
f11949
0
operatingSchema
protected Schema kafkatest_f11958_0(R record)
{    return record.valueSchema();}
f11958
0
operatingValue
protected Object kafkatest_f11959_0(R record)
{    return record.value();}
f11959
0
newRecord
protected R kafkatest_f11960_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(), updatedSchema, updatedValue, record.timestamp());}
f11960
0
configure
public void kafkatest_f11968_0(Map<String, ?> props)
{    final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);    topicFormat = config.getString(ConfigName.TOPIC_FORMAT);    final String timestampFormatStr = config.getString(ConfigName.TIMESTAMP_FORMAT);    timestampFormat = new ThreadLocal<SimpleDateFormat>() {        @Override        protected SimpleDateFormat initialValue() {            final SimpleDateFormat fmt = new SimpleDateFormat(timestampFormatStr);            fmt.setTimeZone(TimeZone.getTimeZone("UTC"));            return fmt;        }    };}
f11968
0
initialValue
protected SimpleDateFormat kafkatest_f11969_0()
{    final SimpleDateFormat fmt = new SimpleDateFormat(timestampFormatStr);    fmt.setTimeZone(TimeZone.getTimeZone("UTC"));    return fmt;}
f11969
0
apply
public R kafkatest_f11970_0(R record)
{    final Long timestamp = record.timestamp();    if (timestamp == null) {        throw new DataException("Timestamp missing on record: " + record);    }    final String formattedTimestamp = timestampFormat.get().format(new Date(timestamp));    final String replace1 = TOPIC.matcher(topicFormat).replaceAll(Matcher.quoteReplacement(record.topic()));    final String updatedTopic = TIMESTAMP.matcher(replace1).replaceAll(Matcher.quoteReplacement(formattedTimestamp));    return record.newRecord(updatedTopic, record.kafkaPartition(), record.keySchema(), record.key(), record.valueSchema(), record.value(), record.timestamp());}
f11970
0
requireMap
public static Map<String, Object> kafkatest_f11978_0(Object value, String purpose)
{    if (!(value instanceof Map)) {        throw new DataException("Only Map objects supported in absence of schema for [" + purpose + "], found: " + nullSafeClassName(value));    }    return (Map<String, Object>) value;}
f11978
0
requireMapOrNull
public static Map<String, Object> kafkatest_f11979_0(Object value, String purpose)
{    if (value == null) {        return null;    }    return requireMap(value, purpose);}
f11979
0
requireStruct
public static Struct kafkatest_f11980_0(Object value, String purpose)
{    if (!(value instanceof Struct)) {        throw new DataException("Only Struct objects supported for [" + purpose + "], found: " + nullSafeClassName(value));    }    return (Struct) value;}
f11980
0
applySchemaless
private R kafkatest_f11988_0(R record)
{    final Map<String, Object> value = requireMap(record.value(), PURPOSE);    final Map<String, Object> key = new HashMap<>(fields.size());    for (String field : fields) {        key.put(field, value.get(field));    }    return record.newRecord(record.topic(), record.kafkaPartition(), null, key, record.valueSchema(), record.value(), record.timestamp());}
f11988
0
applyWithSchema
private R kafkatest_f11989_0(R record)
{    final Struct value = requireStruct(record.value(), PURPOSE);    Schema keySchema = valueToKeySchemaCache.get(value.schema());    if (keySchema == null) {        final SchemaBuilder keySchemaBuilder = SchemaBuilder.struct();        for (String field : fields) {            final Schema fieldSchema = value.schema().field(field).schema();            keySchemaBuilder.field(field, fieldSchema);        }        keySchema = keySchemaBuilder.build();        valueToKeySchemaCache.put(value.schema(), keySchema);    }    final Struct key = new Struct(keySchema);    for (String field : fields) {        key.put(field, value.get(field));    }    return record.newRecord(record.topic(), record.kafkaPartition(), keySchema, key, value.schema(), value, record.timestamp());}
f11989
0
config
public ConfigDef kafkatest_f11990_0()
{    return CONFIG_DEF;}
f11990
0
testConfigMixWholeAndFieldTransformation
public void kafkatest_f11998_0()
{    xformKey.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "foo:int8,int32"));}
f11998
0
castWholeRecordKeyWithSchema
public void kafkatest_f11999_0()
{    xformKey.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "int8"));    SourceRecord transformed = xformKey.apply(new SourceRecord(null, null, "topic", 0, Schema.INT32_SCHEMA, 42, Schema.STRING_SCHEMA, "bogus"));    assertEquals(Schema.Type.INT8, transformed.keySchema().type());    assertEquals((byte) 42, transformed.key());}
f11999
0
castWholeRecordValueWithSchemaInt8
public void kafkatest_f12000_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "int8"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, Schema.INT32_SCHEMA, 42));    assertEquals(Schema.Type.INT8, transformed.valueSchema().type());    assertEquals((byte) 42, transformed.value());}
f12000
0
castWholeRecordValueWithSchemaString
public void kafkatest_f12008_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "string"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, Schema.INT32_SCHEMA, 42));    assertEquals(Schema.Type.STRING, transformed.valueSchema().type());    assertEquals("42", transformed.value());}
f12008
0
castWholeBigDecimalRecordValueWithSchemaString
public void kafkatest_f12009_0()
{    BigDecimal bigDecimal = new BigDecimal(42);    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "string"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, Decimal.schema(bigDecimal.scale()), bigDecimal));    assertEquals(Schema.Type.STRING, transformed.valueSchema().type());    assertEquals("42", transformed.value());}
f12009
0
castWholeDateRecordValueWithSchemaString
public void kafkatest_f12010_0()
{    // day + 1msec to get a timestamp formatting.    Date timestamp = new Date(MILLIS_PER_DAY + 1);    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "string"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, Timestamp.SCHEMA, timestamp));    assertEquals(Schema.Type.STRING, transformed.valueSchema().type());    assertEquals(Values.dateFormatFor(timestamp).format(timestamp), transformed.value());}
f12010
0
castWholeRecordValueSchemalessFloat64
public void kafkatest_f12018_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "float64"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, null, 42));    assertNull(transformed.valueSchema());    assertEquals(42., transformed.value());}
f12018
0
castWholeRecordValueSchemalessBooleanTrue
public void kafkatest_f12019_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "boolean"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, null, 42));    assertNull(transformed.valueSchema());    assertEquals(true, transformed.value());}
f12019
0
castWholeRecordValueSchemalessBooleanFalse
public void kafkatest_f12020_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "boolean"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, null, 0));    assertNull(transformed.valueSchema());    assertEquals(false, transformed.value());}
f12020
0
withSchema
public void kafkatest_f12028_0()
{    xform.configure(Collections.singletonMap("field", "magic"));    final Schema keySchema = SchemaBuilder.struct().field("magic", Schema.INT32_SCHEMA).build();    final Struct key = new Struct(keySchema).put("magic", 42);    final SinkRecord record = new SinkRecord("test", 0, keySchema, key, null, null, 0);    final SinkRecord transformedRecord = xform.apply(record);    assertEquals(Schema.INT32_SCHEMA, transformedRecord.keySchema());    assertEquals(42, transformedRecord.key());}
f12028
0
testNullWithSchema
public void kafkatest_f12029_0()
{    xform.configure(Collections.singletonMap("field", "magic"));    final Schema keySchema = SchemaBuilder.struct().field("magic", Schema.INT32_SCHEMA).optional().build();    final Struct key = null;    final SinkRecord record = new SinkRecord("test", 0, keySchema, key, null, null, 0);    final SinkRecord transformedRecord = xform.apply(record);    assertEquals(Schema.INT32_SCHEMA, transformedRecord.keySchema());    assertNull(transformedRecord.key());}
f12029
0
teardown
public void kafkatest_f12030_0()
{    xformKey.close();    xformValue.close();}
f12030
0
testOptionalFieldMap
public void kafkatest_f12038_0()
{    xformValue.configure(Collections.<String, String>emptyMap());    Map<String, Object> supportedTypes = new HashMap<>();    supportedTypes.put("opt_int32", null);    Map<String, Object> oneLevelNestedMap = Collections.singletonMap("B", (Object) supportedTypes);    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, null, oneLevelNestedMap));    assertNull(transformed.valueSchema());    assertTrue(transformed.value() instanceof Map);    @SuppressWarnings("unchecked")    Map<String, Object> transformedMap = (Map<String, Object>) transformed.value();    assertNull(transformedMap.get("B.opt_int32"));}
f12038
0
testKey
public void kafkatest_f12039_0()
{    xformKey.configure(Collections.<String, String>emptyMap());    Map<String, Map<String, Integer>> key = Collections.singletonMap("A", Collections.singletonMap("B", 12));    SourceRecord src = new SourceRecord(null, null, "topic", null, key, null, null);    SourceRecord transformed = xformKey.apply(src);    assertNull(transformed.keySchema());    assertTrue(transformed.key() instanceof Map);    @SuppressWarnings("unchecked")    Map<String, Object> transformedMap = (Map<String, Object>) transformed.key();    assertEquals(12, transformedMap.get("A.B"));}
f12039
0
testUnsupportedTypeInMap
public void kafkatest_f12040_0()
{    xformValue.configure(Collections.<String, String>emptyMap());    Object value = Collections.singletonMap("foo", Arrays.asList("bar", "baz"));    xformValue.apply(new SourceRecord(null, null, "topic", 0, null, value));}
f12040
0
schemalessInsertConfiguredFields
public void kafkatest_f12048_0()
{    final Map<String, Object> props = new HashMap<>();    props.put("topic.field", "topic_field!");    props.put("partition.field", "partition_field");    props.put("timestamp.field", "timestamp_field?");    props.put("static.field", "instance_id");    props.put("static.value", "my-instance-id");    xform.configure(props);    final SourceRecord record = new SourceRecord(null, null, "test", 0, null, Collections.singletonMap("magic", 42L));    final SourceRecord transformedRecord = xform.apply(record);    assertEquals(42L, ((Map) transformedRecord.value()).get("magic"));    assertEquals("test", ((Map) transformedRecord.value()).get("topic_field"));    assertEquals(0, ((Map) transformedRecord.value()).get("partition_field"));    assertEquals(null, ((Map) transformedRecord.value()).get("timestamp_field"));    assertEquals("my-instance-id", ((Map) transformedRecord.value()).get("instance_id"));}
f12048
0
transform
private static MaskField<SinkRecord> kafkatest_f12049_0(List<String> fields)
{    final MaskField<SinkRecord> xform = new MaskField.Value<>();    xform.configure(Collections.singletonMap("fields", fields));    return xform;}
f12049
0
record
private static SinkRecord kafkatest_f12050_0(Schema schema, Object value)
{    return new SinkRecord("", 0, null, null, schema, value, 0);}
f12050
0
addSuffix
public void kafkatest_f12058_0()
{    assertEquals("orig-suffix", apply("(.*)", "$1-suffix", "orig"));}
f12058
0
slice
public void kafkatest_f12059_0()
{    assertEquals("index", apply("(.*)-(\\d\\d\\d\\d\\d\\d\\d\\d)", "$1", "index-20160117"));}
f12059
0
teardown
public void kafkatest_f12060_0()
{    xform.close();}
f12060
0
updateSchemaOfStruct
public void kafkatest_f12068_0()
{    final String fieldName1 = "f1";    final String fieldName2 = "f2";    final String fieldValue1 = "value1";    final int fieldValue2 = 1;    final Schema schema = SchemaBuilder.struct().name("my.orig.SchemaDefn").field(fieldName1, Schema.STRING_SCHEMA).field(fieldName2, Schema.INT32_SCHEMA).build();    final Struct value = new Struct(schema).put(fieldName1, fieldValue1).put(fieldName2, fieldValue2);    final Schema newSchema = SchemaBuilder.struct().name("my.updated.SchemaDefn").field(fieldName1, Schema.STRING_SCHEMA).field(fieldName2, Schema.INT32_SCHEMA).build();    Struct newValue = (Struct) SetSchemaMetadata.updateSchemaIn(value, newSchema);    assertMatchingSchema(newValue, newSchema);}
f12068
0
updateSchemaOfNonStruct
public void kafkatest_f12069_0()
{    Object value = new Integer(1);    Object updatedValue = SetSchemaMetadata.updateSchemaIn(value, Schema.INT32_SCHEMA);    assertSame(value, updatedValue);}
f12069
0
updateSchemaOfNull
public void kafkatest_f12070_0()
{    Object updatedValue = SetSchemaMetadata.updateSchemaIn(null, Schema.INT32_SCHEMA);    assertEquals(null, updatedValue);}
f12070
0
testSchemalessTimestampToDate
public void kafkatest_f12078_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Date"));    SourceRecord transformed = xformValue.apply(createRecordSchemaless(DATE_PLUS_TIME.getTime()));    assertNull(transformed.valueSchema());    assertEquals(DATE.getTime(), transformed.value());}
f12078
0
testSchemalessTimestampToTime
public void kafkatest_f12079_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Time"));    SourceRecord transformed = xformValue.apply(createRecordSchemaless(DATE_PLUS_TIME.getTime()));    assertNull(transformed.valueSchema());    assertEquals(TIME.getTime(), transformed.value());}
f12079
0
testSchemalessTimestampToUnix
public void kafkatest_f12080_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "unix"));    SourceRecord transformed = xformValue.apply(createRecordSchemaless(DATE_PLUS_TIME.getTime()));    assertNull(transformed.valueSchema());    assertEquals(DATE_PLUS_TIME_UNIX, transformed.value());}
f12080
0
testWithSchemaTimestampToTime
public void kafkatest_f12088_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Time"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Timestamp.SCHEMA, DATE_PLUS_TIME.getTime()));    assertEquals(Time.SCHEMA, transformed.valueSchema());    assertEquals(TIME.getTime(), transformed.value());}
f12088
0
testWithSchemaTimestampToUnix
public void kafkatest_f12089_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "unix"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Timestamp.SCHEMA, DATE_PLUS_TIME.getTime()));    assertEquals(Schema.INT64_SCHEMA, transformed.valueSchema());    assertEquals(DATE_PLUS_TIME_UNIX, transformed.value());}
f12089
0
testWithSchemaTimestampToString
public void kafkatest_f12090_0()
{    Map<String, String> config = new HashMap<>();    config.put(TimestampConverter.TARGET_TYPE_CONFIG, "string");    config.put(TimestampConverter.FORMAT_CONFIG, STRING_DATE_FMT);    xformValue.configure(config);    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Timestamp.SCHEMA, DATE_PLUS_TIME.getTime()));    assertEquals(Schema.STRING_SCHEMA, transformed.valueSchema());    assertEquals(DATE_PLUS_TIME_STRING, transformed.value());}
f12090
0
testWithSchemaDateToTimestamp
public void kafkatest_f12098_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Date.SCHEMA, DATE.getTime()));    assertEquals(Timestamp.SCHEMA, transformed.valueSchema());    // No change expected since the source type is coarser-grained    assertEquals(DATE.getTime(), transformed.value());}
f12098
0
testWithSchemaTimeToTimestamp
public void kafkatest_f12099_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Time.SCHEMA, TIME.getTime()));    assertEquals(Timestamp.SCHEMA, transformed.valueSchema());    // No change expected since the source type is coarser-grained    assertEquals(TIME.getTime(), transformed.value());}
f12099
0
testWithSchemaUnixToTimestamp
public void kafkatest_f12100_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Schema.INT64_SCHEMA, DATE_PLUS_TIME_UNIX));    assertEquals(Timestamp.SCHEMA, transformed.valueSchema());    assertEquals(DATE_PLUS_TIME.getTime(), transformed.value());}
f12100
0
testWithSchemaNullValueToDate
public void kafkatest_f12108_0()
{    testWithSchemaNullValueConversion("Date", Schema.OPTIONAL_INT64_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);    testWithSchemaNullValueConversion("Date", TimestampConverter.OPTIONAL_TIME_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);    testWithSchemaNullValueConversion("Date", TimestampConverter.OPTIONAL_DATE_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);    testWithSchemaNullValueConversion("Date", Schema.OPTIONAL_STRING_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);    testWithSchemaNullValueConversion("Date", TimestampConverter.OPTIONAL_TIMESTAMP_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);}
f12108
0
testWithSchemaNullFieldToDate
public void kafkatest_f12109_0()
{    testWithSchemaNullFieldConversion("Date", Schema.OPTIONAL_INT64_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);    testWithSchemaNullFieldConversion("Date", TimestampConverter.OPTIONAL_TIME_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);    testWithSchemaNullFieldConversion("Date", TimestampConverter.OPTIONAL_DATE_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);    testWithSchemaNullFieldConversion("Date", Schema.OPTIONAL_STRING_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);    testWithSchemaNullFieldConversion("Date", TimestampConverter.OPTIONAL_TIMESTAMP_SCHEMA, TimestampConverter.OPTIONAL_DATE_SCHEMA);}
f12109
0
testWithSchemaNullValueToString
public void kafkatest_f12110_0()
{    testWithSchemaNullValueConversion("string", Schema.OPTIONAL_INT64_SCHEMA, Schema.OPTIONAL_STRING_SCHEMA);    testWithSchemaNullValueConversion("string", TimestampConverter.OPTIONAL_TIME_SCHEMA, Schema.OPTIONAL_STRING_SCHEMA);    testWithSchemaNullValueConversion("string", TimestampConverter.OPTIONAL_DATE_SCHEMA, Schema.OPTIONAL_STRING_SCHEMA);    testWithSchemaNullValueConversion("string", Schema.OPTIONAL_STRING_SCHEMA, Schema.OPTIONAL_STRING_SCHEMA);    testWithSchemaNullValueConversion("string", TimestampConverter.OPTIONAL_TIMESTAMP_SCHEMA, Schema.OPTIONAL_STRING_SCHEMA);}
f12110
0
createRecordSchemaless
private SourceRecord kafkatest_f12118_0(Object value)
{    return createRecordWithSchema(null, value);}
f12118
0
teardown
public void kafkatest_f12119_0()
{    xform.close();}
f12119
0
defaultConfiguration
public void kafkatest_f12120_0()
{    // defaults    xform.configure(Collections.<String, Object>emptyMap());    final SourceRecord record = new SourceRecord(null, null, "test", 0, null, null, null, null, 1483425001864L);    assertEquals("test-20170103", xform.apply(record).topic());}
f12120
0
run
public int kafkatest_f12128_0(final String[] args, final Properties config)
{    int exitCode;    Admin adminClient = null;    try {        parseArguments(args);        final boolean dryRun = options.has(dryRunOption);        final String groupId = options.valueOf(applicationIdOption);        final Properties properties = new Properties();        if (options.has(commandConfigOption)) {            properties.putAll(Utils.loadProps(options.valueOf(commandConfigOption)));        }        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, options.valueOf(bootstrapServerOption));        adminClient = Admin.create(properties);        validateNoActiveConsumers(groupId, adminClient);        allTopics.clear();        allTopics.addAll(adminClient.listTopics().names().get(60, TimeUnit.SECONDS));        if (dryRun) {            System.out.println("----Dry run displays the actions which will be performed when running Streams Reset Tool----");        }        final HashMap<Object, Object> consumerConfig = new HashMap<>(config);        consumerConfig.putAll(properties);        exitCode = maybeResetInputAndSeekToEndIntermediateTopicOffsets(consumerConfig, dryRun);        maybeDeleteInternalTopics(adminClient, dryRun);    } catch (final Throwable e) {        exitCode = EXIT_CODE_ERROR;        System.err.println("ERROR: " + e);        e.printStackTrace(System.err);    } finally {        if (adminClient != null) {            adminClient.close(Duration.ofSeconds(60));        }    }    return exitCode;}
f12128
0
validateNoActiveConsumers
private void kafkatest_f12129_0(final String groupId, final Admin adminClient) throws ExecutionException, InterruptedException
{    final DescribeConsumerGroupsResult describeResult = adminClient.describeConsumerGroups(Collections.singleton(groupId), new DescribeConsumerGroupsOptions().timeoutMs(10 * 1000));    final List<MemberDescription> members = new ArrayList<>(describeResult.describedGroups().get(groupId).get().members());    if (!members.isEmpty()) {        throw new IllegalStateException("Consumer group '" + groupId + "' is still active " + "and has following members: " + members + ". " + "Make sure to stop all running application instances before running the reset tool.");    }}
f12129
0
parseArguments
private void kafkatest_f12130_0(final String[] args)
{    final OptionParser optionParser = new OptionParser(false);    applicationIdOption = optionParser.accepts("application-id", "The Kafka Streams application ID (application.id).").withRequiredArg().ofType(String.class).describedAs("id").required();    bootstrapServerOption = optionParser.accepts("bootstrap-servers", "Comma-separated list of broker urls with format: HOST1:PORT1,HOST2:PORT2").withRequiredArg().ofType(String.class).defaultsTo("localhost:9092").describedAs("urls");    inputTopicsOption = optionParser.accepts("input-topics", "Comma-separated list of user input topics. For these topics, the tool will reset the offset to the earliest available offset.").withRequiredArg().ofType(String.class).withValuesSeparatedBy(',').describedAs("list");    intermediateTopicsOption = optionParser.accepts("intermediate-topics", "Comma-separated list of intermediate user topics (topics used in the through() method). For these topics, the tool will skip to the end.").withRequiredArg().ofType(String.class).withValuesSeparatedBy(',').describedAs("list");    toOffsetOption = optionParser.accepts("to-offset", "Reset offsets to a specific offset.").withRequiredArg().ofType(Long.class);    toDatetimeOption = optionParser.accepts("to-datetime", "Reset offsets to offset from datetime. Format: 'YYYY-MM-DDTHH:mm:SS.sss'").withRequiredArg().ofType(String.class);    byDurationOption = optionParser.accepts("by-duration", "Reset offsets to offset by duration from current timestamp. Format: 'PnDTnHnMnS'").withRequiredArg().ofType(String.class);    toEarliestOption = optionParser.accepts("to-earliest", "Reset offsets to earliest offset.");    toLatestOption = optionParser.accepts("to-latest", "Reset offsets to latest offset.");    fromFileOption = optionParser.accepts("from-file", "Reset offsets to values defined in CSV file.").withRequiredArg().ofType(String.class);    shiftByOption = optionParser.accepts("shift-by", "Reset offsets shifting current offset by 'n', where 'n' can be positive or negative").withRequiredArg().describedAs("number-of-offsets").ofType(Long.class);    commandConfigOption = optionParser.accepts("config-file", "Property file containing configs to be passed to admin clients and embedded consumer.").withRequiredArg().ofType(String.class).describedAs("file name");    executeOption = optionParser.accepts("execute", "Execute the command.");    dryRunOption = optionParser.accepts("dry-run", "Display the actions that would be performed without executing the reset commands.");    helpOption = optionParser.accepts("help", "Print usage information.").forHelp();    versionOption = optionParser.accepts("version", "Print version information and exit.").forHelp();    // TODO: deprecated in 1.0; can be removed eventually: https://issues.apache.org/jira/browse/KAFKA-7606    optionParser.accepts("zookeeper", "Zookeeper option is deprecated by bootstrap.servers, as the reset tool would no longer access Zookeeper directly.");    try {        options = optionParser.parse(args);        if (args.length == 0 || options.has(helpOption)) {            CommandLineUtils.printUsageAndDie(optionParser, usage);        }        if (options.has(versionOption)) {            CommandLineUtils.printVersionAndDie();        }    } catch (final OptionException e) {        CommandLineUtils.printUsageAndDie(optionParser, e.getMessage());    }    if (options.has(executeOption) && options.has(dryRunOption)) {        CommandLineUtils.printUsageAndDie(optionParser, "Only one of --dry-run and --execute can be specified");    }    final Set<OptionSpec<?>> allScenarioOptions = new HashSet<>();    allScenarioOptions.add(toOffsetOption);    allScenarioOptions.add(toDatetimeOption);    allScenarioOptions.add(byDurationOption);    allScenarioOptions.add(toEarliestOption);    allScenarioOptions.add(toLatestOption);    allScenarioOptions.add(fromFileOption);    allScenarioOptions.add(shiftByOption);    checkInvalidArgs(optionParser, options, allScenarioOptions, toOffsetOption);    checkInvalidArgs(optionParser, options, allScenarioOptions, toDatetimeOption);    checkInvalidArgs(optionParser, options, allScenarioOptions, byDurationOption);    checkInvalidArgs(optionParser, options, allScenarioOptions, toEarliestOption);    checkInvalidArgs(optionParser, options, allScenarioOptions, toLatestOption);    checkInvalidArgs(optionParser, options, allScenarioOptions, fromFileOption);    checkInvalidArgs(optionParser, options, allScenarioOptions, shiftByOption);}
f12130
0
resetToDatetime
private void kafkatest_f12138_0(final Consumer<byte[], byte[]> client, final Set<TopicPartition> inputTopicPartitions, final Long timestamp)
{    final Map<TopicPartition, Long> topicPartitionsAndTimes = new HashMap<>(inputTopicPartitions.size());    for (final TopicPartition topicPartition : inputTopicPartitions) {        topicPartitionsAndTimes.put(topicPartition, timestamp);    }    final Map<TopicPartition, OffsetAndTimestamp> topicPartitionsAndOffset = client.offsetsForTimes(topicPartitionsAndTimes);    for (final TopicPartition topicPartition : inputTopicPartitions) {        client.seek(topicPartition, topicPartitionsAndOffset.get(topicPartition).offset());    }}
f12138
0
shiftOffsetsBy
public void kafkatest_f12139_0(final Consumer<byte[], byte[]> client, final Set<TopicPartition> inputTopicPartitions, final long shiftBy)
{    final Map<TopicPartition, Long> endOffsets = client.endOffsets(inputTopicPartitions);    final Map<TopicPartition, Long> beginningOffsets = client.beginningOffsets(inputTopicPartitions);    final Map<TopicPartition, Long> topicPartitionsAndOffset = new HashMap<>(inputTopicPartitions.size());    for (final TopicPartition topicPartition : inputTopicPartitions) {        final long position = client.position(topicPartition);        final long offset = position + shiftBy;        topicPartitionsAndOffset.put(topicPartition, offset);    }    final Map<TopicPartition, Long> validatedTopicPartitionsAndOffset = checkOffsetRange(topicPartitionsAndOffset, beginningOffsets, endOffsets);    for (final TopicPartition topicPartition : inputTopicPartitions) {        client.seek(topicPartition, validatedTopicPartitionsAndOffset.get(topicPartition));    }}
f12139
0
resetOffsetsTo
public void kafkatest_f12140_0(final Consumer<byte[], byte[]> client, final Set<TopicPartition> inputTopicPartitions, final Long offset)
{    final Map<TopicPartition, Long> endOffsets = client.endOffsets(inputTopicPartitions);    final Map<TopicPartition, Long> beginningOffsets = client.beginningOffsets(inputTopicPartitions);    final Map<TopicPartition, Long> topicPartitionsAndOffset = new HashMap<>(inputTopicPartitions.size());    for (final TopicPartition topicPartition : inputTopicPartitions) {        topicPartitionsAndOffset.put(topicPartition, offset);    }    final Map<TopicPartition, Long> validatedTopicPartitionsAndOffset = checkOffsetRange(topicPartitionsAndOffset, beginningOffsets, endOffsets);    for (final TopicPartition topicPartition : inputTopicPartitions) {        client.seek(topicPartition, validatedTopicPartitionsAndOffset.get(topicPartition));    }}
f12140
0
isInternalTopic
private boolean kafkatest_f12148_0(final String topicName)
{    // Cf. https://issues.apache.org/jira/browse/KAFKA-7930    return !isInputTopic(topicName) && !isIntermediateTopic(topicName) && topicName.startsWith(options.valueOf(applicationIdOption) + "-") && (topicName.endsWith("-changelog") || topicName.endsWith("-repartition"));}
f12148
0
main
public static void kafkatest_f12149_0(final String[] args)
{    Exit.exit(new StreamsResetter().run(args));}
f12149
0
doWork
public void kafkatest_f12150_0()
{    consumer.subscribe(Collections.singletonList(this.topic));    ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofSeconds(1));    for (ConsumerRecord<Integer, String> record : records) {        System.out.println("Received message: (" + record.key() + ", " + record.value() + ") at offset " + record.offset());    }}
f12150
0
responseSchema
 String kafkatest_f12158_0()
{    if (responseSpec == null) {        return "null";    } else {        return String.format("%sData.SCHEMAS", responseSpec.name());    }}
f12158
0
hasRegisteredTypes
public boolean kafkatest_f12159_0()
{    return !apis.isEmpty();}
f12159
0
registerMessageType
public void kafkatest_f12160_0(MessageSpec spec)
{    switch(spec.type()) {        case REQUEST:            {                short apiKey = spec.apiKey().get();                ApiData data = apis.get(apiKey);                if (!apis.containsKey(apiKey)) {                    data = new ApiData(apiKey);                    apis.put(apiKey, data);                }                if (data.requestSpec != null) {                    throw new RuntimeException("Found more than one request with " + "API key " + spec.apiKey().get());                }                data.requestSpec = spec;                break;            }        case RESPONSE:            {                short apiKey = spec.apiKey().get();                ApiData data = apis.get(apiKey);                if (!apis.containsKey(apiKey)) {                    data = new ApiData(apiKey);                    apis.put(apiKey, data);                }                if (data.responseSpec != null) {                    throw new RuntimeException("Found more than one response with " + "API key " + spec.apiKey().get());                }                data.responseSpec = spec;                break;            }        default:            // do nothing            break;    }}
f12160
0
generateToString
private void kafkatest_f12168_0()
{    buffer.printf("@Override%n");    buffer.printf("public String toString() {%n");    buffer.incrementIndent();    buffer.printf("return this.name();%n");    buffer.decrementIndent();    buffer.printf("}%n");}
f12168
0
write
public void kafkatest_f12169_0(BufferedWriter writer) throws IOException
{    headerGenerator.buffer().write(writer);    buffer.write(writer);}
f12169
0
incrementIndent
public void kafkatest_f12170_0()
{    indent++;}
f12170
0
verifyTypeMatches
public void kafkatest_f12178_0(String fieldName, FieldType type)
{    if (this == UNKNOWN) {        return;    }    if (type instanceof FieldType.ArrayType) {        FieldType.ArrayType arrayType = (FieldType.ArrayType) type;        verifyTypeMatches(fieldName, arrayType.elementType());    } else {        if (!type.toString().equals(baseType.toString())) {            throw new RuntimeException("Field " + fieldName + " has entity type " + name() + ", but field type " + type.toString() + ", which does " + "not match.");        }    }}
f12178
0
name
public String kafkatest_f12179_0()
{    return name;}
f12179
0
capitalizedCamelCaseName
 String kafkatest_f12180_0()
{    return MessageGenerator.capitalizeFirst(name);}
f12180
0
mapKey
public boolean kafkatest_f12188_0()
{    return mapKey;}
f12188
0
nullableVersions
public Versions kafkatest_f12189_0()
{    return nullableVersions;}
f12189
0
nullableVersionsString
public String kafkatest_f12190_0()
{    return nullableVersions.toString();}
f12190
0
toString
public String kafkatest_f12198_0()
{    return NAME;}
f12198
0
isInteger
public boolean kafkatest_f12199_0()
{    return true;}
f12199
0
fixedLength
public Optional<Integer> kafkatest_f12200_0()
{    return Optional.of(2);}
f12200
0
fixedLength
public Optional<Integer> kafkatest_f12208_0()
{    return Optional.of(16);}
f12208
0
toString
public String kafkatest_f12209_0()
{    return NAME;}
f12209
0
isString
public boolean kafkatest_f12210_0()
{    return true;}
f12210
0
isArray
public boolean kafkatest_f12218_0()
{    return true;}
f12218
0
isStructArray
public boolean kafkatest_f12219_0()
{    return elementType.isStruct();}
f12219
0
canBeNullable
public boolean kafkatest_f12220_0()
{    return true;}
f12220
0
isString
 boolean kafkatest_f12228_0()
{    return false;}
f12228
0
isBytes
 boolean kafkatest_f12229_0()
{    return false;}
f12229
0
isStruct
 boolean kafkatest_f12230_0()
{    return false;}
f12230
0
generateClass
private void kafkatest_f12238_0(Optional<MessageSpec> topLevelMessageSpec, String className, StructSpec struct, Versions parentVersions) throws Exception
{    buffer.printf("%n");    boolean isTopLevel = topLevelMessageSpec.isPresent();    // Check if the class is inside a set.    boolean isSetElement = struct.hasKeys();    if (isTopLevel && isSetElement) {        throw new RuntimeException("Cannot set mapKey on top level fields.");    }    generateClassHeader(className, isTopLevel, isSetElement);    buffer.incrementIndent();    generateFieldDeclarations(struct, isSetElement);    buffer.printf("%n");    schemaGenerator.writeSchema(className, buffer);    generateClassConstructors(className, struct);    buffer.printf("%n");    if (isTopLevel) {        generateShortAccessor("apiKey", topLevelMessageSpec.get().apiKey().orElse((short) -1));    }    buffer.printf("%n");    generateShortAccessor("lowestSupportedVersion", parentVersions.lowest());    buffer.printf("%n");    generateShortAccessor("highestSupportedVersion", parentVersions.highest());    buffer.printf("%n");    generateClassReader(className, struct, parentVersions);    buffer.printf("%n");    generateClassWriter(className, struct, parentVersions);    buffer.printf("%n");    generateClassFromStruct(className, struct, parentVersions);    buffer.printf("%n");    generateClassToStruct(className, struct, parentVersions);    buffer.printf("%n");    generateClassSize(className, struct, parentVersions);    buffer.printf("%n");    generateClassEquals(className, struct, isSetElement);    buffer.printf("%n");    generateClassHashCode(struct, isSetElement);    buffer.printf("%n");    generateClassToString(className, struct);    generateFieldAccessors(struct, isSetElement);    generateFieldMutators(struct, className, isSetElement);    if (!isTopLevel) {        buffer.decrementIndent();        buffer.printf("}%n");    }    generateSubclasses(className, struct, parentVersions, isSetElement);    if (isTopLevel) {        for (Iterator<StructSpec> iter = structRegistry.commonStructs(); iter.hasNext(); ) {            StructSpec commonStruct = iter.next();            generateClass(Optional.empty(), commonStruct.name(), commonStruct, commonStruct.versions());        }        buffer.decrementIndent();        buffer.printf("}%n");    }}
f12238
0
generateClassHeader
private void kafkatest_f12239_0(String className, boolean isTopLevel, boolean isSetElement)
{    Set<String> implementedInterfaces = new HashSet<>();    if (isTopLevel) {        implementedInterfaces.add("ApiMessage");        headerGenerator.addImport(MessageGenerator.API_MESSAGE_CLASS);    } else {        implementedInterfaces.add("Message");        headerGenerator.addImport(MessageGenerator.MESSAGE_CLASS);    }    if (isSetElement) {        headerGenerator.addImport(MessageGenerator.IMPLICIT_LINKED_HASH_MULTI_COLLECTION_CLASS);        implementedInterfaces.add("ImplicitLinkedHashMultiCollection.Element");    }    Set<String> classModifiers = new HashSet<>();    classModifiers.add("public");    if (!isTopLevel) {        classModifiers.add("static");    }    buffer.printf("%s class %s implements %s {%n", String.join(" ", classModifiers), className, String.join(", ", implementedInterfaces));}
f12239
0
generateSubclasses
private void kafkatest_f12240_0(String className, StructSpec struct, Versions parentVersions, boolean isSetElement) throws Exception
{    for (FieldSpec field : struct.fields()) {        if (field.type().isStructArray()) {            FieldType.ArrayType arrayType = (FieldType.ArrayType) field.type();            if (!structRegistry.commonStructNames().contains(arrayType.elementName())) {                generateClass(Optional.empty(), arrayType.elementType().toString(), structRegistry.findStruct(field), parentVersions.intersect(struct.versions()));            }        }    }    if (isSetElement) {        generateHashSet(className, struct);    }}
f12240
0
commaSeparatedHashSetFieldAndTypes
private String kafkatest_f12248_0(StructSpec struct)
{    return struct.fields().stream().filter(f -> f.mapKey()).map(f -> String.format("%s %s", fieldConcreteJavaType(f), f.camelCaseName())).collect(Collectors.joining(", "));}
f12248
0
generateFieldDeclarations
private void kafkatest_f12249_0(StructSpec struct, boolean isSetElement)
{    for (FieldSpec field : struct.fields()) {        generateFieldDeclaration(field);    }    if (isSetElement) {        buffer.printf("private int next;%n");        buffer.printf("private int prev;%n");    }}
f12249
0
generateFieldDeclaration
private void kafkatest_f12250_0(FieldSpec field)
{    buffer.printf("private %s %s;%n", fieldAbstractJavaType(field), field.camelCaseName());}
f12250
0
generateShortAccessor
private void kafkatest_f12258_0(String name, short val)
{    buffer.printf("@Override%n");    buffer.printf("public short %s() {%n", name);    buffer.incrementIndent();    buffer.printf("return %d;%n", val);    buffer.decrementIndent();    buffer.printf("}%n");}
f12258
0
generateClassReader
private void kafkatest_f12259_0(String className, StructSpec struct, Versions parentVersions)
{    headerGenerator.addImport(MessageGenerator.READABLE_CLASS);    buffer.printf("@Override%n");    buffer.printf("public void read(Readable readable, short version) {%n");    buffer.incrementIndent();    if (generateInverseVersionCheck(parentVersions, struct.versions())) {        buffer.incrementIndent();        headerGenerator.addImport(MessageGenerator.UNSUPPORTED_VERSION_EXCEPTION_CLASS);        buffer.printf("throw new UnsupportedVersionException(\"Can't read " + "version \" + version + \" of %s\");%n", className);        buffer.decrementIndent();        buffer.printf("}%n");    }    Versions curVersions = parentVersions.intersect(struct.versions());    if (curVersions.empty()) {        throw new RuntimeException("Version ranges " + parentVersions + " and " + struct.versions() + " have no versions in common.");    }    for (FieldSpec field : struct.fields()) {        generateFieldReader(field, curVersions);    }    buffer.decrementIndent();    buffer.printf("}%n");}
f12259
0
generateFieldReader
private void kafkatest_f12260_0(FieldSpec field, Versions curVersions)
{    if (field.type().isArray()) {        boolean maybeAbsent = generateVersionCheck(curVersions, field.versions());        if (!maybeAbsent) {            buffer.printf("{%n");            buffer.incrementIndent();        }        boolean hasKeys = structRegistry.isStructArrayWithKeys(field);        buffer.printf("int arrayLength = readable.readInt();%n");        buffer.printf("if (arrayLength < 0) {%n");        buffer.incrementIndent();        buffer.printf("this.%s = null;%n", field.camelCaseName());        buffer.decrementIndent();        buffer.printf("} else {%n");        buffer.incrementIndent();        buffer.printf("this.%s.clear(%s);%n", field.camelCaseName(), hasKeys ? "arrayLength" : "");        buffer.printf("for (int i = 0; i < arrayLength; i++) {%n");        buffer.incrementIndent();        FieldType.ArrayType arrayType = (FieldType.ArrayType) field.type();        buffer.printf("this.%s.add(%s);%n", field.camelCaseName(), readFieldFromReadable(arrayType.elementType()));        buffer.decrementIndent();        buffer.printf("}%n");        buffer.decrementIndent();        buffer.printf("}%n");        if (maybeAbsent) {            generateSetDefault(field);        } else {            buffer.decrementIndent();            buffer.printf("}%n");        }    } else {        boolean maybeAbsent = generateVersionCheck(curVersions, field.versions());        buffer.printf("this.%s = %s;%n", field.camelCaseName(), readFieldFromReadable(field.type()));        if (maybeAbsent) {            generateSetDefault(field);        }    }}
f12260
0
generateFieldWriter
private void kafkatest_f12268_0(FieldSpec field, Versions curVersions)
{    if (field.type().isArray()) {        boolean maybeAbsent = generateVersionCheck(curVersions, field.versions());        boolean maybeNull = generateNullCheck(curVersions, field);        if (maybeNull) {            buffer.printf("writable.writeInt(-1);%n");            buffer.decrementIndent();            buffer.printf("} else {%n");            buffer.incrementIndent();        }        buffer.printf("writable.writeInt(%s.size());%n", field.camelCaseName());        FieldType.ArrayType arrayType = (FieldType.ArrayType) field.type();        FieldType elementType = arrayType.elementType();        String nestedTypeName = elementType.isStruct() ? elementType.toString() : getBoxedJavaType(elementType);        buffer.printf("for (%s element : %s) {%n", nestedTypeName, field.camelCaseName());        buffer.incrementIndent();        buffer.printf("%s;%n", writeFieldToWritable(elementType, false, "element"));        buffer.decrementIndent();        buffer.printf("}%n");        if (maybeNull) {            buffer.decrementIndent();            buffer.printf("}%n");        }        if (maybeAbsent) {            buffer.decrementIndent();            buffer.printf("}%n");        }    } else {        boolean maybeAbsent = generateVersionCheck(curVersions, field.versions());        buffer.printf("%s;%n", writeFieldToWritable(field.type(), !field.nullableVersions().empty(), field.camelCaseName()));        if (maybeAbsent) {            buffer.decrementIndent();            buffer.printf("}%n");        }    }}
f12268
0
generateClassToStruct
private void kafkatest_f12269_0(String className, StructSpec struct, Versions parentVersions)
{    headerGenerator.addImport(MessageGenerator.STRUCT_CLASS);    buffer.printf("@Override%n");    buffer.printf("public Struct toStruct(short version) {%n");    buffer.incrementIndent();    if (generateInverseVersionCheck(parentVersions, struct.versions())) {        buffer.incrementIndent();        headerGenerator.addImport(MessageGenerator.UNSUPPORTED_VERSION_EXCEPTION_CLASS);        buffer.printf("throw new UnsupportedVersionException(\"Can't write " + "version \" + version + \" of %s\");%n", className);        buffer.decrementIndent();        buffer.printf("}%n");    }    Versions curVersions = parentVersions.intersect(struct.versions());    if (curVersions.empty()) {        throw new RuntimeException("Version ranges " + parentVersions + " and " + struct.versions() + " have no versions in common.");    }    buffer.printf("Struct struct = new Struct(SCHEMAS[version]);%n");    for (FieldSpec field : struct.fields()) {        generateFieldToStruct(field, curVersions);    }    buffer.printf("return struct;%n");    buffer.decrementIndent();    buffer.printf("}%n");}
f12269
0
generateFieldToStruct
private void kafkatest_f12270_0(FieldSpec field, Versions curVersions)
{    if ((!field.type().canBeNullable()) && (!field.nullableVersions().empty())) {        throw new RuntimeException("Fields of type " + field.type() + " cannot be nullable.");    }    if ((field.type() instanceof FieldType.BoolFieldType) || (field.type().isInteger()) || (field.type() instanceof FieldType.UUIDFieldType) || (field.type() instanceof FieldType.StringFieldType)) {        boolean maybeAbsent = generateVersionCheck(curVersions, field.versions());        buffer.printf("struct.set(\"%s\", this.%s);%n", field.snakeCaseName(), field.camelCaseName());        if (maybeAbsent) {            buffer.decrementIndent();            buffer.printf("}%n");        }    } else if (field.type().isBytes()) {        boolean maybeAbsent = generateVersionCheck(curVersions, field.versions());        buffer.printf("struct.setByteArray(\"%s\", this.%s);%n", field.snakeCaseName(), field.camelCaseName());        if (maybeAbsent) {            buffer.decrementIndent();            buffer.printf("}%n");        }    } else if (field.type().isArray()) {        boolean maybeAbsent = generateVersionCheck(curVersions, field.versions());        if (!maybeAbsent) {            buffer.printf("{%n");            buffer.incrementIndent();        }        boolean maybeNull = generateNullCheck(curVersions, field);        if (maybeNull) {            buffer.printf("struct.set(\"%s\", null);%n", field.snakeCaseName());            buffer.decrementIndent();            buffer.printf("} else {%n");            buffer.incrementIndent();        }        FieldType.ArrayType arrayType = (FieldType.ArrayType) field.type();        FieldType elementType = arrayType.elementType();        String boxdElementType = elementType.isStruct() ? "Struct" : getBoxedJavaType(elementType);        buffer.printf("%s[] nestedObjects = new %s[%s.size()];%n", boxdElementType, boxdElementType, field.camelCaseName());        buffer.printf("int i = 0;%n");        buffer.printf("for (%s element : this.%s) {%n", getBoxedJavaType(arrayType.elementType()), field.camelCaseName());        buffer.incrementIndent();        if (elementType.isStruct()) {            buffer.printf("nestedObjects[i++] = element.toStruct(version);%n");        } else {            buffer.printf("nestedObjects[i++] = element;%n");        }        buffer.decrementIndent();        buffer.printf("}%n");        buffer.printf("struct.set(\"%s\", (Object[]) nestedObjects);%n", field.snakeCaseName());        if (maybeNull) {            buffer.decrementIndent();            buffer.printf("}%n");        }        buffer.decrementIndent();        buffer.printf("}%n");    } else {        throw new RuntimeException("Unsupported field type " + field.type());    }}
f12270
0
generateFieldHashCode
private void kafkatest_f12278_0(FieldSpec field)
{    if (field.type() instanceof FieldType.BoolFieldType) {        buffer.printf("hashCode = 31 * hashCode + (%s ? 1231 : 1237);%n", field.camelCaseName());    } else if ((field.type() instanceof FieldType.Int8FieldType) || (field.type() instanceof FieldType.Int16FieldType) || (field.type() instanceof FieldType.Int32FieldType)) {        buffer.printf("hashCode = 31 * hashCode + %s;%n", field.camelCaseName());    } else if (field.type() instanceof FieldType.Int64FieldType) {        buffer.printf("hashCode = 31 * hashCode + ((int) (%s >> 32) ^ (int) %s);%n", field.camelCaseName(), field.camelCaseName());    } else if (field.type().isBytes()) {        headerGenerator.addImport(MessageGenerator.ARRAYS_CLASS);        buffer.printf("hashCode = 31 * hashCode + Arrays.hashCode(%s);%n", field.camelCaseName());    } else if (field.type().isStruct() || field.type().isArray() || field.type().isString() || field.type() instanceof FieldType.UUIDFieldType) {        buffer.printf("hashCode = 31 * hashCode + (%s == null ? 0 : %s.hashCode());%n", field.camelCaseName(), field.camelCaseName());    } else {        throw new RuntimeException("Unsupported field type " + field.type());    }}
f12278
0
generateClassToString
private void kafkatest_f12279_0(String className, StructSpec struct)
{    buffer.printf("@Override%n");    buffer.printf("public String toString() {%n");    buffer.incrementIndent();    buffer.printf("return \"%s(\"%n", className);    buffer.incrementIndent();    String prefix = "";    for (FieldSpec field : struct.fields()) {        generateFieldToString(prefix, field);        prefix = ", ";    }    buffer.printf("+ \")\";%n");    buffer.decrementIndent();    buffer.decrementIndent();    buffer.printf("}%n");}
f12279
0
generateFieldToString
private void kafkatest_f12280_0(String prefix, FieldSpec field)
{    if (field.type() instanceof FieldType.BoolFieldType) {        buffer.printf("+ \"%s%s=\" + (%s ? \"true\" : \"false\")%n", prefix, field.camelCaseName(), field.camelCaseName());    } else if ((field.type() instanceof FieldType.Int8FieldType) || (field.type() instanceof FieldType.Int16FieldType) || (field.type() instanceof FieldType.Int32FieldType) || (field.type() instanceof FieldType.Int64FieldType) || (field.type() instanceof FieldType.UUIDFieldType)) {        buffer.printf("+ \"%s%s=\" + %s%n", prefix, field.camelCaseName(), field.camelCaseName());    } else if (field.type().isString()) {        buffer.printf("+ \"%s%s='\" + %s + \"'\"%n", prefix, field.camelCaseName(), field.camelCaseName());    } else if (field.type().isBytes()) {        headerGenerator.addImport(MessageGenerator.ARRAYS_CLASS);        buffer.printf("+ \"%s%s=\" + Arrays.toString(%s)%n", prefix, field.camelCaseName(), field.camelCaseName());    } else if (field.type().isStruct()) {        buffer.printf("+ \"%s%s=\" + %s.toString()%n", prefix, field.camelCaseName(), field.camelCaseName());    } else if (field.type().isArray()) {        headerGenerator.addImport(MessageGenerator.MESSAGE_UTIL_CLASS);        if (field.nullableVersions().empty()) {            buffer.printf("+ \"%s%s=\" + MessageUtil.deepToString(%s.iterator())%n", prefix, field.camelCaseName(), field.camelCaseName());        } else {            buffer.printf("+ \"%s%s=\" + ((%s == null) ? \"null\" : " + "MessageUtil.deepToString(%s.iterator()))%n", prefix, field.camelCaseName(), field.camelCaseName(), field.camelCaseName());        }    } else {        throw new RuntimeException("Unsupported field type " + field.type());    }}
f12280
0
generateAccessor
private void kafkatest_f12288_0(String javaType, String functionName, String memberName)
{    buffer.printf("public %s %s() {%n", javaType, functionName);    buffer.incrementIndent();    buffer.printf("return this.%s;%n", memberName);    buffer.decrementIndent();    buffer.printf("}%n");}
f12288
0
generateFieldMutator
private void kafkatest_f12289_0(String className, FieldSpec field)
{    buffer.printf("%n");    buffer.printf("public %s set%s(%s v) {%n", className, field.capitalizedCamelCaseName(), fieldAbstractJavaType(field));    buffer.incrementIndent();    buffer.printf("this.%s = v;%n", field.camelCaseName());    buffer.printf("return this;%n");    buffer.decrementIndent();    buffer.printf("}%n");}
f12289
0
generateSetter
private void kafkatest_f12290_0(String javaType, String functionName, String memberName)
{    buffer.printf("public void %s(%s v) {%n", functionName, javaType);    buffer.incrementIndent();    buffer.printf("this.%s = v;%n", memberName);    buffer.decrementIndent();    buffer.printf("}%n");}
f12290
0
struct
public StructSpec kafkatest_f12298_0()
{    return struct;}
f12298
0
name
public String kafkatest_f12299_0()
{    return struct.name();}
f12299
0
validVersionsString
public String kafkatest_f12300_0()
{    return struct.versionsString();}
f12300
0
generateSchemaForVersion
private void kafkatest_f12308_0(StructSpec struct, short version, CodeBuffer buffer) throws Exception
{    // Find the last valid field index.    int lastValidIndex = struct.fields().size() - 1;    while (true) {        if (lastValidIndex < 0) {            break;        }        FieldSpec field = struct.fields().get(lastValidIndex);        if (field.versions().contains(version)) {            break;        }        lastValidIndex--;    }    headerGenerator.addImport(MessageGenerator.SCHEMA_CLASS);    buffer.printf("new Schema(%n");    buffer.incrementIndent();    for (int i = 0; i <= lastValidIndex; i++) {        FieldSpec field = struct.fields().get(i);        if (!field.versions().contains(version)) {            continue;        }        headerGenerator.addImport(MessageGenerator.FIELD_CLASS);        buffer.printf("new Field(\"%s\", %s, \"%s\")%s%n", field.snakeCaseName(), fieldTypeToSchemaType(field, version), field.about(), i == lastValidIndex ? "" : ",");    }    buffer.decrementIndent();    buffer.printf(");%n");}
f12308
0
fieldTypeToSchemaType
private String kafkatest_f12309_0(FieldSpec field, short version)
{    return fieldTypeToSchemaType(field.type(), field.nullableVersions().contains(version), version);}
f12309
0
fieldTypeToSchemaType
private String kafkatest_f12310_0(FieldType type, boolean nullable, short version)
{    if (type instanceof FieldType.BoolFieldType) {        headerGenerator.addImport(MessageGenerator.TYPE_CLASS);        if (nullable) {            throw new RuntimeException("Type " + type + " cannot be nullable.");        }        return "Type.BOOLEAN";    } else if (type instanceof FieldType.Int8FieldType) {        headerGenerator.addImport(MessageGenerator.TYPE_CLASS);        if (nullable) {            throw new RuntimeException("Type " + type + " cannot be nullable.");        }        return "Type.INT8";    } else if (type instanceof FieldType.Int16FieldType) {        headerGenerator.addImport(MessageGenerator.TYPE_CLASS);        if (nullable) {            throw new RuntimeException("Type " + type + " cannot be nullable.");        }        return "Type.INT16";    } else if (type instanceof FieldType.Int32FieldType) {        headerGenerator.addImport(MessageGenerator.TYPE_CLASS);        if (nullable) {            throw new RuntimeException("Type " + type + " cannot be nullable.");        }        return "Type.INT32";    } else if (type instanceof FieldType.Int64FieldType) {        headerGenerator.addImport(MessageGenerator.TYPE_CLASS);        if (nullable) {            throw new RuntimeException("Type " + type + " cannot be nullable.");        }        return "Type.INT64";    } else if (type instanceof FieldType.UUIDFieldType) {        headerGenerator.addImport(MessageGenerator.TYPE_CLASS);        if (nullable) {            throw new RuntimeException("Type " + type + " cannot be nullable.");        }        return "Type.UUID";    } else if (type instanceof FieldType.StringFieldType) {        headerGenerator.addImport(MessageGenerator.TYPE_CLASS);        return nullable ? "Type.NULLABLE_STRING" : "Type.STRING";    } else if (type instanceof FieldType.BytesFieldType) {        headerGenerator.addImport(MessageGenerator.TYPE_CLASS);        return nullable ? "Type.NULLABLE_BYTES" : "Type.BYTES";    } else if (type.isArray()) {        headerGenerator.addImport(MessageGenerator.ARRAYOF_CLASS);        FieldType.ArrayType arrayType = (FieldType.ArrayType) type;        String prefix = nullable ? "ArrayOf.nullable" : "new ArrayOf";        return String.format("%s(%s)", prefix, fieldTypeToSchemaType(arrayType.elementType(), false, version));    } else if (type.isStruct()) {        if (nullable) {            throw new RuntimeException("Type " + type + " cannot be nullable.");        }        return String.format("%s.SCHEMA_%d", type.toString(), floorVersion(type.toString(), version));    } else {        throw new RuntimeException("Unsupported type " + type);    }}
f12310
0
commonStructs
 Iterator<StructSpec> kafkatest_f12318_0()
{    return new Iterator<StructSpec>() {        private final Iterator<String> iter = commonStructNames.iterator();        @Override        public boolean hasNext() {            return iter.hasNext();        }        @Override        public StructSpec next() {            return structSpecs.get(iter.next());        }    };}
f12318
0
hasNext
public boolean kafkatest_f12319_0()
{    return iter.hasNext();}
f12319
0
next
public StructSpec kafkatest_f12320_0()
{    return structSpecs.get(iter.next());}
f12320
0
highest
public short kafkatest_f12328_0()
{    return highest;}
f12328
0
empty
public boolean kafkatest_f12329_0()
{    return lowest > highest;}
f12329
0
toString
public String kafkatest_f12330_0()
{    if (empty()) {        return NONE_STRING;    } else if (lowest == highest) {        return String.valueOf(lowest);    } else if (highest == Short.MAX_VALUE) {        return String.format("%d+", lowest);    } else {        return String.format("%d-%d", lowest, highest);    }}
f12330
0
expectException
private static void kafkatest_f12338_0(Runnable r)
{    try {        r.run();        fail("expected an exception");    } catch (RuntimeException e) {    }}
f12338
0
testVerifyTypeMismatches
public void kafkatest_f12339_0()
{    expectException(() -> EntityType.TRANSACTIONAL_ID.verifyTypeMatches("transactionalIdField", FieldType.Int32FieldType.INSTANCE));    expectException(() -> EntityType.PRODUCER_ID.verifyTypeMatches("producerIdField", FieldType.StringFieldType.INSTANCE));    expectException(() -> EntityType.GROUP_ID.verifyTypeMatches("groupIdField", FieldType.Int8FieldType.INSTANCE));    expectException(() -> EntityType.TOPIC_NAME.verifyTypeMatches("topicNameField", new FieldType.ArrayType(FieldType.Int64FieldType.INSTANCE)));    expectException(() -> EntityType.BROKER_ID.verifyTypeMatches("brokerIdField", FieldType.Int64FieldType.INSTANCE));}
f12339
0
testNullDefaults
public void kafkatest_f12340_0() throws Exception
{    MessageSpec testMessageSpec = MessageGenerator.JSON_SERDE.readValue(String.join("", Arrays.asList("{", "  \"type\": \"request\",", "  \"name\": \"FooBar\",", "  \"validVersions\": \"0-2\",", "  \"fields\": [", "    { \"name\": \"field1\", \"type\": \"int32\", \"versions\": \"0+\" },", "    { \"name\": \"field2\", \"type\": \"[]TestStruct\", \"versions\": \"1+\", ", "    \"nullableVersions\": \"1+\", \"default\": \"null\", \"fields\": [", "      { \"name\": \"field1\", \"type\": \"int32\", \"versions\": \"0+\" }", "    ]},", "    { \"name\": \"field3\", \"type\": \"bytes\", \"versions\": \"2+\", ", "      \"nullableVersions\": \"2+\", \"default\": \"null\" }", "  ]", "}")), MessageSpec.class);    new MessageDataGenerator().generate(testMessageSpec);}
f12340
0
testCommonStructs
public void kafkatest_f12348_0() throws Exception
{    MessageSpec testMessageSpec = MessageGenerator.JSON_SERDE.readValue(String.join("", Arrays.asList("{", "  \"type\": \"request\",", "  \"name\": \"LeaderAndIsrRequest\",", "  \"validVersions\": \"0-2\",", "  \"fields\": [", "    { \"name\": \"field1\", \"type\": \"int32\", \"versions\": \"0+\" },", "    { \"name\": \"field2\", \"type\": \"[]TestCommonStruct\", \"versions\": \"1+\" },", "    { \"name\": \"field3\", \"type\": \"[]TestInlineStruct\", \"versions\": \"0+\", ", "    \"fields\": [", "      { \"name\": \"inlineField1\", \"type\": \"int64\", \"versions\": \"0+\" }", "    ]}", "  ],", "  \"commonStructs\": [", "    { \"name\": \"TestCommonStruct\", \"versions\": \"0+\", \"fields\": [", "      { \"name\": \"commonField1\", \"type\": \"int64\", \"versions\": \"0+\" }", "    ]}", "  ]", "}")), MessageSpec.class);    StructRegistry structRegistry = new StructRegistry();    structRegistry.register(testMessageSpec);    assertEquals(Collections.singleton("TestCommonStruct"), structRegistry.commonStructNames());    assertFalse(structRegistry.isStructArrayWithKeys(testMessageSpec.fields().get(1)));    assertFalse(structRegistry.isStructArrayWithKeys(testMessageSpec.fields().get(2)));    assertTrue(structRegistry.commonStructs().hasNext());    assertEquals("TestCommonStruct", structRegistry.commonStructs().next().name());}
f12348
0
testReSpecifiedCommonStructError
public void kafkatest_f12349_0() throws Exception
{    MessageSpec testMessageSpec = MessageGenerator.JSON_SERDE.readValue(String.join("", Arrays.asList("{", "  \"type\": \"request\",", "  \"name\": \"LeaderAndIsrRequest\",", "  \"validVersions\": \"0-2\",", "  \"fields\": [", "    { \"name\": \"field1\", \"type\": \"int32\", \"versions\": \"0+\" },", "    { \"name\": \"field2\", \"type\": \"[]TestCommonStruct\", \"versions\": \"0+\", ", "    \"fields\": [", "      { \"name\": \"inlineField1\", \"type\": \"int64\", \"versions\": \"0+\" }", "    ]}", "  ],", "  \"commonStructs\": [", "    { \"name\": \"TestCommonStruct\", \"versions\": \"0+\", \"fields\": [", "      { \"name\": \"commonField1\", \"type\": \"int64\", \"versions\": \"0+\" }", "    ]}", "  ]", "}")), MessageSpec.class);    StructRegistry structRegistry = new StructRegistry();    try {        structRegistry.register(testMessageSpec);        fail("Expected StructRegistry#registry to fail");    } catch (RuntimeException e) {        assertTrue(e.getMessage().contains("Can't re-specify the common struct TestCommonStruct " + "as an inline struct."));    }}
f12349
0
testDuplicateCommonStructError
public void kafkatest_f12350_0() throws Exception
{    MessageSpec testMessageSpec = MessageGenerator.JSON_SERDE.readValue(String.join("", Arrays.asList("{", "  \"type\": \"request\",", "  \"name\": \"LeaderAndIsrRequest\",", "  \"validVersions\": \"0-2\",", "  \"fields\": [", "    { \"name\": \"field1\", \"type\": \"int32\", \"versions\": \"0+\" }", "  ],", "  \"commonStructs\": [", "    { \"name\": \"TestCommonStruct\", \"versions\": \"0+\", \"fields\": [", "      { \"name\": \"commonField1\", \"type\": \"int64\", \"versions\": \"0+\" }", "    ]},", "    { \"name\": \"TestCommonStruct\", \"versions\": \"0+\", \"fields\": [", "      { \"name\": \"commonField1\", \"type\": \"int64\", \"versions\": \"0+\" }", "    ]}", "  ]", "}")), MessageSpec.class);    StructRegistry structRegistry = new StructRegistry();    try {        structRegistry.register(testMessageSpec);        fail("Expected StructRegistry#registry to fail");    } catch (RuntimeException e) {        assertTrue(e.getMessage().contains("Common struct TestCommonStruct was specified twice."));    }}
f12350
0
testCachePerformance
public String kafkatest_f12358_0()
{    counter++;    int index = (int) (counter % DISTINCT_KEYS);    String hashkey = keys[index];    lruCache.put(hashkey, values[index]);    return lruCache.get(hashkey);}
f12358
0
main
public static void kafkatest_f12359_0(String[] args) throws RunnerException
{    Options opt = new OptionsBuilder().include(LRUCacheBenchmark.class.getSimpleName()).forks(2).build();    new Runner(opt).run();}
f12359
0
testValidate
public BenchState kafkatest_f12360_0(BenchState state)
{    // validate doesn't return anything, so return `state` to prevent the JVM from optimising the whole call away    Topic.validate(state.topicName);    return state;}
f12360
0
getBrokerList
public String kafkatest_f12368_0()
{    return brokerList;}
f12368
0
setBrokerList
public void kafkatest_f12369_0(String brokerList)
{    this.brokerList = brokerList;}
f12369
0
getRequiredNumAcks
public int kafkatest_f12370_0()
{    return requiredNumAcks;}
f12370
0
getTopic
public String kafkatest_f12378_0()
{    return topic;}
f12378
0
setTopic
public void kafkatest_f12379_0(String topic)
{    this.topic = topic;}
f12379
0
getIgnoreExceptions
public boolean kafkatest_f12380_0()
{    return ignoreExceptions;}
f12380
0
setSslTruststoreLocation
public void kafkatest_f12388_0(String sslTruststoreLocation)
{    this.sslTruststoreLocation = sslTruststoreLocation;}
f12388
0
setSslTruststorePassword
public void kafkatest_f12389_0(String sslTruststorePassword)
{    this.sslTruststorePassword = sslTruststorePassword;}
f12389
0
setSslKeystorePassword
public void kafkatest_f12390_0(String sslKeystorePassword)
{    this.sslKeystorePassword = sslKeystorePassword;}
f12390
0
getSslKeystorePassword
public String kafkatest_f12398_0()
{    return sslKeystorePassword;}
f12398
0
getSaslKerberosServiceName
public String kafkatest_f12399_0()
{    return saslKerberosServiceName;}
f12399
0
getClientJaasConfPath
public String kafkatest_f12400_0()
{    return clientJaasConfPath;}
f12400
0
activateOptions
public voidf12408_1)
{    // check for config parameter validity    Properties props = new Properties();    if (brokerList != null)        props.put(BOOTSTRAP_SERVERS_CONFIG, brokerList);    if (props.isEmpty())        throw new ConfigException("The bootstrap servers property should be specified");    if (topic == null)        throw new ConfigException("Topic must be specified by the Kafka log4j appender");    if (compressionType != null)        props.put(COMPRESSION_TYPE_CONFIG, compressionType);    props.put(ACKS_CONFIG, Integer.toString(requiredNumAcks));    props.put(RETRIES_CONFIG, retries);    props.put(DELIVERY_TIMEOUT_MS_CONFIG, deliveryTimeoutMs);    if (securityProtocol != null) {        props.put(SECURITY_PROTOCOL_CONFIG, securityProtocol);    }    if (securityProtocol != null && securityProtocol.contains("SSL") && sslTruststoreLocation != null && sslTruststorePassword != null) {        props.put(SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);        props.put(SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);        if (sslKeystoreType != null && sslKeystoreLocation != null && sslKeystorePassword != null) {            props.put(SSL_KEYSTORE_TYPE_CONFIG, sslKeystoreType);            props.put(SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);            props.put(SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);        }    }    if (securityProtocol != null && securityProtocol.contains("SASL") && saslKerberosServiceName != null && clientJaasConfPath != null) {        props.put(SASL_KERBEROS_SERVICE_NAME, saslKerberosServiceName);        System.setProperty("java.security.auth.login.config", clientJaasConfPath);    }    if (kerb5ConfPath != null) {        System.setProperty("java.security.krb5.conf", kerb5ConfPath);    }    if (saslMechanism != null) {        props.put(SASL_MECHANISM, saslMechanism);    }    if (clientJaasConf != null) {        props.put(SASL_JAAS_CONFIG, clientJaasConf);    }    if (maxBlockMs != null) {        props.put(MAX_BLOCK_MS_CONFIG, maxBlockMs);    }    props.put(KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    props.put(VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    this.producer = getKafkaProducer(props);    Log    Log}
public voidf12408
1
getKafkaProducer
protected Producer<byte[], byte[]> kafkatest_f12409_0(Properties props)
{    return new KafkaProducer<>(props);}
f12409
0
append
protected voidf12410_1LoggingEvent event)
{    String message = subAppend(event);    Log    Future<RecordMetadata> response = producer.send(new ProducerRecord<>(topic, message.getBytes(StandardCharsets.UTF_8)));    if (syncSend) {        try {            response.get();        } catch (InterruptedException | ExecutionException ex) {            if (!ignoreExceptions)                throw new RuntimeException(ex);            Log        }    }}
protected voidf12410
1
testSetJaasConfig
public void kafkatest_f12418_0()
{    Properties props = getLog4jConfig(false);    props.put("log4j.appender.KAFKA.ClientJaasConf", "jaas-config");    PropertyConfigurator.configure(props);    MockKafkaLog4jAppender mockKafkaLog4jAppender = getMockKafkaLog4jAppender();    assertThat(mockKafkaLog4jAppender.getProducerProperties().getProperty(SaslConfigs.SASL_JAAS_CONFIG), equalTo("jaas-config"));}
f12418
0
testJaasConfigNotSet
public void kafkatest_f12419_0()
{    testProducerPropertyNotSet(SaslConfigs.SASL_JAAS_CONFIG);}
f12419
0
testProducerPropertyNotSet
private void kafkatest_f12420_0(String name)
{    PropertyConfigurator.configure(getLog4jConfig(false));    MockKafkaLog4jAppender mockKafkaLog4jAppender = getMockKafkaLog4jAppender();    assertThat(mockKafkaLog4jAppender.getProducerProperties().stringPropertyNames(), not(hasItem(name)));}
f12420
0
getMessage
private byte[] kafkatest_f12428_0(int i)
{    return ("test_" + i).getBytes(StandardCharsets.UTF_8);}
f12428
0
getLog4jConfigWithRealProducer
private Properties kafkatest_f12429_0(boolean ignoreExceptions)
{    Properties props = new Properties();    props.put("log4j.rootLogger", "INFO, KAFKA");    props.put("log4j.appender.KAFKA", "org.apache.kafka.log4jappender.KafkaLog4jAppender");    props.put("log4j.appender.KAFKA.layout", "org.apache.log4j.PatternLayout");    props.put("log4j.appender.KAFKA.layout.ConversionPattern", "%-5p: %c - %m%n");    props.put("log4j.appender.KAFKA.BrokerList", "127.0.0.2:9093");    props.put("log4j.appender.KAFKA.Topic", "test-topic");    props.put("log4j.appender.KAFKA.RequiredNumAcks", "1");    props.put("log4j.appender.KAFKA.SyncSend", "true");    // setting producer timeout (max.block.ms) to be low    props.put("log4j.appender.KAFKA.maxBlockMs", "10");    // ignoring exceptions    props.put("log4j.appender.KAFKA.IgnoreExceptions", Boolean.toString(ignoreExceptions));    props.put("log4j.logger.kafka.log4j", "INFO, KAFKA");    return props;}
f12429
0
getLog4jConfig
private Properties kafkatest_f12430_0(boolean syncSend)
{    Properties props = new Properties();    props.put("log4j.rootLogger", "INFO, KAFKA");    props.put("log4j.appender.KAFKA", "org.apache.kafka.log4jappender.MockKafkaLog4jAppender");    props.put("log4j.appender.KAFKA.layout", "org.apache.log4j.PatternLayout");    props.put("log4j.appender.KAFKA.layout.ConversionPattern", "%-5p: %c - %m%n");    props.put("log4j.appender.KAFKA.BrokerList", "127.0.0.1:9093");    props.put("log4j.appender.KAFKA.Topic", "test-topic");    props.put("log4j.appender.KAFKA.RequiredNumAcks", "1");    props.put("log4j.appender.KAFKA.SyncSend", Boolean.toString(syncSend));    props.put("log4j.logger.kafka.log4j", "INFO, KAFKA");    return props;}
f12430
0
serialize
public byte[] kafkatest_f12439_0(final String topic, final T data)
{    if (data == null) {        return null;    }    try {        return OBJECT_MAPPER.writeValueAsBytes(data);    } catch (final Exception e) {        throw new SerializationException("Error serializing JSON message", e);    }}
f12439
0
serializer
public Serializer<T> kafkatest_f12441_0()
{    return this;}
f12441
0
deserializer
public Deserializer<T> kafkatest_f12442_0()
{    return this;}
f12442
0
main
public static void kafkatest_f12450_0(final String[] args)
{    final Properties props = new Properties();    props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-wordcount");    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    // setting offset reset to earliest so that we can re-run the demo code with the same pre-loaded data    // Note: To re-run the demo, you need to use the offset reset tool:    // https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Application+Reset+Tool    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> source = builder.stream("streams-plaintext-input");    final KTable<String, Long> counts = source.flatMapValues(value -> Arrays.asList(value.toLowerCase(Locale.getDefault()).split(" "))).groupBy((key, value) -> value).count();    // need to override value serde to Long type    counts.toStream().to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));    final KafkaStreams streams = new KafkaStreams(builder.build(), props);    final CountDownLatch latch = new CountDownLatch(1);    // attach shutdown handler to catch control-c    Runtime.getRuntime().addShutdownHook(new Thread("streams-wordcount-shutdown-hook") {        @Override        public void run() {            streams.close();            latch.countDown();        }    });    try {        streams.start();        latch.await();    } catch (final Throwable e) {        System.exit(1);    }    System.exit(0);}
f12450
0
run
public void kafkatest_f12451_0()
{    streams.close();    latch.countDown();}
f12451
0
get
public Processor<String, String> kafkatest_f12452_0()
{    return new Processor<String, String>() {        private ProcessorContext context;        private KeyValueStore<String, Integer> kvStore;        @Override        @SuppressWarnings("unchecked")        public void init(final ProcessorContext context) {            this.context = context;            this.context.schedule(Duration.ofSeconds(1), PunctuationType.STREAM_TIME, timestamp -> {                try (final KeyValueIterator<String, Integer> iter = kvStore.all()) {                    System.out.println("----------- " + timestamp + " ----------- ");                    while (iter.hasNext()) {                        final KeyValue<String, Integer> entry = iter.next();                        System.out.println("[" + entry.key + ", " + entry.value + "]");                        context.forward(entry.key, entry.value.toString());                    }                }            });            this.kvStore = (KeyValueStore<String, Integer>) context.getStateStore("Counts");        }        @Override        public void process(final String dummy, final String line) {            final String[] words = line.toLowerCase(Locale.getDefault()).split(" ");            for (final String word : words) {                final Integer oldValue = this.kvStore.get(word);                if (oldValue == null) {                    this.kvStore.put(word, 1);                } else {                    this.kvStore.put(word, oldValue + 1);                }            }            context.commit();        }        @Override        public void close() {        }    };}
f12452
0
handle
public DeserializationHandlerResponsef12461_1final ProcessorContext context, final ConsumerRecord<byte[], byte[]> record, final Exception exception)
{        return DeserializationHandlerResponse.CONTINUE;}
public DeserializationHandlerResponsef12461
1
configure
public void kafkatest_f12462_0(final Map<String, ?> configs)
{// ignore}
f12462
0
handle
public DeserializationHandlerResponsef12463_1final ProcessorContext context, final ConsumerRecord<byte[], byte[]> record, final Exception exception)
{        return DeserializationHandlerResponse.FAIL;}
public DeserializationHandlerResponsef12463
1
isRunning
public boolean kafkatest_f12471_0()
{    return equals(RUNNING) || equals(REBALANCING);}
f12471
0
isValidTransition
public boolean kafkatest_f12472_0(final State newState)
{    return validTransitions.contains(newState.ordinal());}
f12472
0
waitOnState
private booleanf12473_1final State targetState, final long waitMs)
{    final long begin = time.milliseconds();    synchronized (stateLock) {        long elapsedMs = 0L;        while (state != targetState) {            if (waitMs > elapsedMs) {                final long remainingMs = waitMs - elapsedMs;                try {                    stateLock.wait(remainingMs);                } catch (final InterruptedException e) {                // it is ok: just move on to the next iteration                }            } else {                                return false;            }            elapsedMs = time.milliseconds() - begin;        }        return true;    }}
private booleanf12473
1
metrics
public Map<MetricName, ? extends Metric> kafkatest_f12481_0()
{    final Map<MetricName, Metric> result = new LinkedHashMap<>();    // producer and consumer clients are per-thread    for (final StreamThread thread : threads) {        result.putAll(thread.producerMetrics());        result.putAll(thread.consumerMetrics());        // admin client is shared, so we can actually move it        // to result.putAll(adminClient.metrics()).        // we did it intentionally just for flexibility.        result.putAll(thread.adminClientMetrics());    }    // global thread's consumer client    if (globalStreamThread != null) {        result.putAll(globalStreamThread.consumerMetrics());    }    // self streams metrics    result.putAll(metrics.metrics());    return Collections.unmodifiableMap(result);}
f12481
0
maybeSetError
private voidf12482_1)
{    // check if we have at least one thread running    for (final StreamThread.State state : threadState.values()) {        if (state != StreamThread.State.DEAD) {            return;        }    }    if (setState(State.ERROR)) {            }}
private voidf12482
1
maybeSetRunning
private void kafkatest_f12483_0()
{    // state can be transferred to RUNNING if all threads are either RUNNING or DEAD    for (final StreamThread.State state : threadState.values()) {        if (state != StreamThread.State.RUNNING && state != StreamThread.State.DEAD) {            return;        }    }    // when we don't have a global state thread at all, e.g., when we don't have global KTables    if (globalThreadState != null && globalThreadState != GlobalStreamThread.State.RUNNING) {        return;    }    setState(State.RUNNING);}
f12483
0
close
public void kafkatest_f12491_0()
{    close(Long.MAX_VALUE);}
f12491
0
close
public synchronized booleanf12492_1final long timeout, final TimeUnit timeUnit)
{    long timeoutMs = timeUnit.toMillis(timeout);        if (timeoutMs < 0) {        timeoutMs = 0;    } else if (timeoutMs == 0) {        timeoutMs = Long.MAX_VALUE;    }    return close(timeoutMs);}
public synchronized booleanf12492
1
close
private booleanf12493_1final long timeoutMs)
{    if (!setState(State.PENDING_SHUTDOWN)) {        // if transition failed, it means it was either in PENDING_SHUTDOWN        // or NOT_RUNNING already; just check that all threads have been stopped            } else {        stateDirCleaner.shutdownNow();        // wait for all threads to join in a separate thread;        // save the current thread so that if it is a stream thread        // we don't attempt to join it and cause a deadlock        final Thread shutdownThread = new Thread(() -> {            // further state reports from the thread since we're shutting down            for (final StreamThread thread : threads) {                thread.shutdown();            }            for (final StreamThread thread : threads) {                try {                    if (!thread.isRunning()) {                        thread.join();                    }                } catch (final InterruptedException ex) {                    Thread.currentThread().interrupt();                }            }            if (globalStreamThread != null) {                globalStreamThread.shutdown();            }            if (globalStreamThread != null && !globalStreamThread.stillRunning()) {                try {                    globalStreamThread.join();                } catch (final InterruptedException e) {                    Thread.currentThread().interrupt();                }                globalStreamThread = null;            }            adminClient.close();            metrics.close();            setState(State.NOT_RUNNING);        }, "kafka-streams-close-thread");        shutdownThread.setDaemon(true);        shutdownThread.start();    }    if (waitOnState(State.NOT_RUNNING, timeoutMs)) {                return true;    } else {                return false;    }}
private booleanf12493
1
localThreadsMetadata
public Set<ThreadMetadata> kafkatest_f12501_0()
{    validateIsRunning();    final Set<ThreadMetadata> threadMetadata = new HashSet<>();    for (final StreamThread thread : threads) {        threadMetadata.add(thread.threadMetadata());    }    return threadMetadata;}
f12501
0
pair
public static KeyValue<K, V> kafkatest_f12502_0(final K key, final V value)
{    return new KeyValue<>(key, value);}
f12502
0
toString
public String kafkatest_f12503_0()
{    return "KeyValue(" + key + ", " + value + ")";}
f12503
0
withKeySerde
public Consumed<K, V> kafkatest_f12511_0(final Serde<K> keySerde)
{    this.keySerde = keySerde;    return this;}
f12511
0
withValueSerde
public Consumed<K, V> kafkatest_f12512_0(final Serde<V> valueSerde)
{    this.valueSerde = valueSerde;    return this;}
f12512
0
withTimestampExtractor
public Consumed<K, V> kafkatest_f12513_0(final TimestampExtractor timestampExtractor)
{    this.timestampExtractor = timestampExtractor;    return this;}
f12513
0
with
public static Grouped<K, V> kafkatest_f12521_0(final String name, final Serde<K> keySerde, final Serde<V> valueSerde)
{    return new Grouped<>(name, keySerde, valueSerde);}
f12521
0
with
public static Grouped<K, V> kafkatest_f12522_0(final Serde<K> keySerde, final Serde<V> valueSerde)
{    return new Grouped<>(null, keySerde, valueSerde);}
f12522
0
withName
public Grouped<K, V> kafkatest_f12523_0(final String name)
{    return new Grouped<>(name, keySerde, valueSerde);}
f12523
0
init
public void kafkatest_f12531_0(final ProcessorContext context)
{    valueTransformer.init(context);}
f12531
0
transform
public VR kafkatest_f12532_0(final K readOnlyKey, final V value)
{    return valueTransformer.transform(value);}
f12532
0
close
public void kafkatest_f12533_0()
{    valueTransformer.close();}
f12533
0
deserialize
public Change<T> kafkatest_f12541_0(final String topic, final Headers headers, final byte[] data)
{    final byte[] bytes = new byte[data.length - NEWFLAG_SIZE];    System.arraycopy(data, 0, bytes, 0, bytes.length);    if (ByteBuffer.wrap(data).get(data.length - NEWFLAG_SIZE) != 0) {        return new Change<>(inner.deserialize(topic, headers, bytes), null);    } else {        return new Change<>(null, inner.deserialize(topic, headers, bytes));    }}
f12541
0
deserialize
public Change<T> kafkatest_f12542_0(final String topic, final byte[] data)
{    return deserialize(topic, null, data);}
f12542
0
close
public void kafkatest_f12543_0()
{    inner.close();}
f12543
0
valueSerde
public Serde<V> kafkatest_f12551_0()
{    return valueSerde;}
f12551
0
valueDeserializer
public Deserializer<V> kafkatest_f12552_0()
{    return valueSerde == null ? null : valueSerde.deserializer();}
f12552
0
timestampExtractor
public TimestampExtractor kafkatest_f12553_0()
{    return timestampExtractor;}
f12553
0
decomposeLegacyFormattedArrayIntoChangeArrays
public static Change<byte[]> kafkatest_f12561_0(final byte[] data)
{    if (data == null) {        return null;    }    final ByteBuffer buffer = ByteBuffer.wrap(data);    final int oldSize = buffer.getInt();    final byte[] oldBytes = oldSize == -1 ? null : new byte[oldSize];    if (oldBytes != null) {        buffer.get(oldBytes);    }    final int newSize = buffer.getInt();    final byte[] newBytes = newSize == -1 ? null : new byte[newSize];    if (newBytes != null) {        buffer.get(newBytes);    }    return new Change<>(newBytes, oldBytes);}
f12561
0
valueGetterSupplier
 KTableValueGetterSupplier<K, V> kafkatest_f12562_0()
{    return valueGetterSupplier;}
f12562
0
queryableStoreName
public String kafkatest_f12563_0()
{    return queryableStoreName;}
f12563
0
toString
public String kafkatest_f12571_0()
{    return "BaseRepartitionNode{" + "keySerde=" + keySerde + ", valueSerde=" + valueSerde + ", sinkName='" + sinkName + '\'' + ", sourceName='" + sourceName + '\'' + ", repartitionTopic='" + repartitionTopic + '\'' + ", processorParameters=" + processorParameters + "} " + super.toString();}
f12571
0
writeToTopology
public void kafkatest_f12572_0(final InternalTopologyBuilder topologyBuilder)
{    storeBuilder.withLoggingDisabled();    topologyBuilder.addGlobalStore(storeBuilder, sourceName, consumed.timestampExtractor(), consumed.keyDeserializer(), consumed.valueDeserializer(), topic, processorName, stateUpdateSupplier);}
f12572
0
toString
public String kafkatest_f12573_0()
{    return "GlobalStoreNode{" + "sourceName='" + sourceName + '\'' + ", topic='" + topic + '\'' + ", processorName='" + processorName + '\'' + "} ";}
f12573
0
toString
public String kafkatest_f12581_0()
{    return "GroupedTableOperationRepartitionNode{} " + super.toString();}
f12581
0
writeToTopology
public void kafkatest_f12582_0(final InternalTopologyBuilder topologyBuilder)
{    final Serializer<K> keySerializer = keySerde != null ? keySerde.serializer() : null;    final Deserializer<K> keyDeserializer = keySerde != null ? keySerde.deserializer() : null;    topologyBuilder.addInternalTopic(repartitionTopic);    topologyBuilder.addSink(sinkName, repartitionTopic, keySerializer, getValueSerializer(), null, parentNodeNames());    topologyBuilder.addSource(null, sourceName, new FailOnInvalidTimestamp(), keyDeserializer, getValueDeserializer(), repartitionTopic);}
f12582
0
groupedTableOperationNodeBuilder
public static GroupedTableOperationRepartitionNodeBuilder<K1, V1> kafkatest_f12583_0()
{    return new GroupedTableOperationRepartitionNodeBuilder<>();}
f12583
0
build
public GroupedTableOperationRepartitionNode<K, V> kafkatest_f12591_0()
{    return new GroupedTableOperationRepartitionNode<>(nodeName, keySerde, valueSerde, sinkName, sourceName, repartitionTopic, processorParameters);}
f12591
0
keySerde
public Serde<K> kafkatest_f12592_0()
{    return keySerde;}
f12592
0
valueSerde
public Serde<VR> kafkatest_f12593_0()
{    return valueSerde;}
f12593
0
withNodeName
public KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12601_0(final String nodeName)
{    this.nodeName = nodeName;    return this;}
f12601
0
withJoinThisProcessorParameters
public KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12602_0(final ProcessorParameters<K, Change<V1>> joinThisProcessorParameters)
{    this.joinThisProcessorParameters = joinThisProcessorParameters;    return this;}
f12602
0
withJoinOtherProcessorParameters
public KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12603_0(final ProcessorParameters<K, Change<V2>> joinOtherProcessorParameters)
{    this.joinOtherProcessorParameters = joinOtherProcessorParameters;    return this;}
f12603
0
withStoreBuilder
public KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12611_0(final StoreBuilder<TimestampedKeyValueStore<K, VR>> storeBuilder)
{    this.storeBuilder = storeBuilder;    return this;}
f12611
0
build
public KTableKTableJoinNode<K, V1, V2, VR> kafkatest_f12612_0()
{    return new KTableKTableJoinNode<>(nodeName, joinThisProcessorParameters, joinOtherProcessorParameters, new ProcessorParameters<>(KTableKTableJoinMerger.of((KTableProcessorSupplier<K, V1, VR>) (joinThisProcessorParameters.processorSupplier()), (KTableProcessorSupplier<K, V2, VR>) (joinOtherProcessorParameters.processorSupplier()), queryableStoreName), nodeName), thisJoinSide, otherJoinSide, keySerde, valueSerde, joinThisStoreNames, joinOtherStoreNames, storeBuilder);}
f12612
0
keySerde
public Serde<K> kafkatest_f12613_0()
{    return keySerde;}
f12613
0
withProcessorParameters
public OptimizableRepartitionNodeBuilder<K, V> kafkatest_f12621_0(final ProcessorParameters processorParameters)
{    this.processorParameters = processorParameters;    return this;}
f12621
0
withKeySerde
public OptimizableRepartitionNodeBuilder<K, V> kafkatest_f12622_0(final Serde<K> keySerde)
{    this.keySerde = keySerde;    return this;}
f12622
0
withValueSerde
public OptimizableRepartitionNodeBuilder<K, V> kafkatest_f12623_0(final Serde<V> valueSerde)
{    this.valueSerde = valueSerde;    return this;}
f12623
0
writeToTopology
public void kafkatest_f12631_0(final InternalTopologyBuilder topologyBuilder)
{    topologyBuilder.addProcessor(processorParameters.processorName(), processorParameters.processorSupplier(), parentNodeNames());}
f12631
0
processorSupplier
public ProcessorSupplier<K, V> kafkatest_f12632_0()
{    return processorSupplier;}
f12632
0
processorName
public String kafkatest_f12633_0()
{    return processorName;}
f12633
0
allParentsWrittenToTopology
public boolean kafkatest_f12641_0()
{    for (final StreamsGraphNode parentNode : parentNodes) {        if (!parentNode.hasWrittenToTopology()) {            return false;        }    }    return true;}
f12641
0
children
public Collection<StreamsGraphNode> kafkatest_f12642_0()
{    return new LinkedHashSet<>(childNodes);}
f12642
0
clearChildren
public void kafkatest_f12643_0()
{    for (final StreamsGraphNode childNode : childNodes) {        childNode.parentNodes.remove(this);    }    childNodes.clear();}
f12643
0
setValueChangingOperation
public void kafkatest_f12651_0(final boolean valueChangingOperation)
{    this.valueChangingOperation = valueChangingOperation;}
f12651
0
keyChangingOperation
public void kafkatest_f12652_0(final boolean keyChangingOperation)
{    this.keyChangingOperation = keyChangingOperation;}
f12652
0
setBuildPriority
public void kafkatest_f12653_0(final int buildPriority)
{    this.buildPriority = buildPriority;}
f12653
0
topicPattern
public Pattern kafkatest_f12661_0()
{    return topicPattern;}
f12661
0
consumedInternal
public ConsumedInternal<K, V> kafkatest_f12662_0()
{    return consumedInternal;}
f12662
0
keySerde
public Serde<K> kafkatest_f12663_0()
{    return consumedInternal.keySerde();}
f12663
0
withJoinThisProcessorParameters
public StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12671_0(final ProcessorParameters<K, V1> joinThisProcessorParameters)
{    this.joinThisProcessorParameters = joinThisProcessorParameters;    return this;}
f12671
0
withNodeName
public StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12672_0(final String nodeName)
{    this.nodeName = nodeName;    return this;}
f12672
0
withJoinOtherProcessorParameters
public StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12673_0(final ProcessorParameters<K, V2> joinOtherProcessParameters)
{    this.joinOtherProcessorParameters = joinOtherProcessParameters;    return this;}
f12673
0
toString
public String kafkatest_f12681_0()
{    return "StreamTableJoinNode{" + "storeNames=" + Arrays.toString(storeNames) + ", processorParameters=" + processorParameters + ", otherJoinSideNodeName='" + otherJoinSideNodeName + '\'' + "} " + super.toString();}
f12681
0
writeToTopology
public void kafkatest_f12682_0(final InternalTopologyBuilder topologyBuilder)
{    final String processorName = processorParameters.processorName();    final ProcessorSupplier processorSupplier = processorParameters.processorSupplier();    // Stream - Table join (Global or KTable)    topologyBuilder.addProcessor(processorName, processorSupplier, parentNodeNames());    // Steam - KTable join only    if (otherJoinSideNodeName != null) {        topologyBuilder.connectProcessorAndStateStores(processorName, storeNames);    }}
f12682
0
toString
public String kafkatest_f12683_0()
{    return "TableProcessorNode{" + ", processorParameters=" + processorParameters + ", storeBuilder=" + (storeBuilder == null ? "null" : storeBuilder.name()) + ", storeNames=" + Arrays.toString(storeNames) + "} " + super.toString();}
f12683
0
withMaterializedInternal
public TableSourceNodeBuilder<K, V> kafkatest_f12691_0(final MaterializedInternal<K, V, ?> materializedInternal)
{    this.materializedInternal = materializedInternal;    return this;}
f12691
0
withConsumedInternal
public TableSourceNodeBuilder<K, V> kafkatest_f12692_0(final ConsumedInternal<K, V> consumedInternal)
{    this.consumedInternal = consumedInternal;    return this;}
f12692
0
withProcessorParameters
public TableSourceNodeBuilder<K, V> kafkatest_f12693_0(final ProcessorParameters<K, V> processorParameters)
{    this.processorParameters = processorParameters;    return this;}
f12693
0
createRepartitionSource
private String kafkatest_f12701_0(final String repartitionTopicNamePrefix, final OptimizableRepartitionNodeBuilder<K, V> optimizableRepartitionNodeBuilder)
{    return KStreamImpl.createRepartitionedSource(builder, keySerde, valueSerde, repartitionTopicNamePrefix, optimizableRepartitionNodeBuilder);}
f12701
0
writeToTopology
public void kafkatest_f12702_0(final InternalTopologyBuilder topologyBuilder)
{// no-op for root node}
f12702
0
stream
public KStream<K, V> kafkatest_f12703_0(final Collection<String> topics, final ConsumedInternal<K, V> consumed)
{    final String name = new NamedInternal(consumed.name()).orElseGenerateWithPrefix(this, KStreamImpl.SOURCE_NAME);    final StreamSourceNode<K, V> streamSourceNode = new StreamSourceNode<>(name, topics, consumed);    addGraphNode(root, streamSourceNode);    return new KStreamImpl<>(name, consumed.keySerde(), consumed.valueSerde(), Collections.singleton(name), false, streamSourceNode, this);}
f12703
0
addGlobalStore
public synchronized void kafkatest_f12711_0(final StoreBuilder<KeyValueStore> storeBuilder, final String topic, final ConsumedInternal consumed, final ProcessorSupplier stateUpdateSupplier)
{    // explicitly disable logging for global stores    storeBuilder.withLoggingDisabled();    final String sourceName = newProcessorName(KStreamImpl.SOURCE_NAME);    final String processorName = newProcessorName(KTableImpl.SOURCE_NAME);    addGlobalStore(storeBuilder, sourceName, topic, consumed, processorName, stateUpdateSupplier);}
f12711
0
addGraphNode
 void kafkatest_f12712_0(final StreamsGraphNode parent, final StreamsGraphNode child)
{    Objects.requireNonNull(parent, "parent node can't be null");    Objects.requireNonNull(child, "child node can't be null");    parent.addChild(child);    maybeAddNodeForOptimizationMetadata(child);}
f12712
0
addGraphNode
 void kafkatest_f12713_0(final Collection<StreamsGraphNode> parents, final StreamsGraphNode child)
{    Objects.requireNonNull(parents, "parent node can't be null");    Objects.requireNonNull(child, "child node can't be null");    if (parents.isEmpty()) {        throw new StreamsException("Parent node collection can't be empty");    }    for (final StreamsGraphNode parent : parents) {        addGraphNode(parent, child);    }}
f12713
0
createRepartitionNode
private OptimizableRepartitionNode kafkatest_f12721_0(final String repartitionTopicName, final Serde keySerde, final Serde valueSerde)
{    final OptimizableRepartitionNode.OptimizableRepartitionNodeBuilder repartitionNodeBuilder = OptimizableRepartitionNode.optimizableRepartitionNodeBuilder();    KStreamImpl.createRepartitionedSource(this, keySerde, valueSerde, repartitionTopicName, repartitionNodeBuilder);    // ensures setting the repartition topic to the name of the    // first repartition topic to get merged    // this may be an auto-generated name or a user specified name    repartitionNodeBuilder.withRepartitionTopic(repartitionTopicName);    return repartitionNodeBuilder.build();}
f12721
0
getKeyChangingParentNode
private StreamsGraphNode kafkatest_f12722_0(final StreamsGraphNode repartitionNode)
{    final StreamsGraphNode shouldBeKeyChangingNode = findParentNodeMatching(repartitionNode, n -> n.isKeyChangingOperation() || n.isValueChangingOperation());    final StreamsGraphNode keyChangingNode = findParentNodeMatching(repartitionNode, StreamsGraphNode::isKeyChangingOperation);    if (shouldBeKeyChangingNode != null && shouldBeKeyChangingNode.equals(keyChangingNode)) {        return keyChangingNode;    }    return null;}
f12722
0
getFirstRepartitionTopicName
private String kafkatest_f12723_0(final Collection<OptimizableRepartitionNode> repartitionNodes)
{    return repartitionNodes.iterator().next().repartitionTopic();}
f12723
0
reduce
public KTable<K, V> kafkatest_f12731_0(final Reducer<V> reducer)
{    return reduce(reducer, Materialized.with(keySerde, valSerde));}
f12731
0
reduce
public KTable<K, V> kafkatest_f12732_0(final Reducer<V> reducer, final Materialized<K, V, KeyValueStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(reducer, "reducer can't be null");    Objects.requireNonNull(materialized, "materialized can't be null");    final MaterializedInternal<K, V, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, REDUCE_NAME);    if (materializedInternal.keySerde() == null) {        materializedInternal.withKeySerde(keySerde);    }    if (materializedInternal.valueSerde() == null) {        materializedInternal.withValueSerde(valSerde);    }    return doAggregate(new KStreamReduce<>(materializedInternal.storeName(), reducer), REDUCE_NAME, materializedInternal);}
f12732
0
aggregate
public KTable<K, VR> kafkatest_f12733_0(final Initializer<VR> initializer, final Aggregator<? super K, ? super V, VR> aggregator, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(initializer, "initializer can't be null");    Objects.requireNonNull(aggregator, "aggregator can't be null");    Objects.requireNonNull(materialized, "materialized can't be null");    final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);    if (materializedInternal.keySerde() == null) {        materializedInternal.withKeySerde(keySerde);    }    return doAggregate(new KStreamAggregate<>(materializedInternal.storeName(), initializer, aggregator), AGGREGATE_NAME, materializedInternal);}
f12733
0
doAggregate
private KTable<K, T> kafkatest_f12741_0(final ProcessorSupplier<K, Change<V>> aggregateSupplier, final String functionName, final MaterializedInternal<K, T, KeyValueStore<Bytes, byte[]>> materialized)
{    final String sinkName = builder.newProcessorName(KStreamImpl.SINK_NAME);    final String sourceName = builder.newProcessorName(KStreamImpl.SOURCE_NAME);    final String funcName = builder.newProcessorName(functionName);    final String repartitionTopic = (userProvidedRepartitionTopicName != null ? userProvidedRepartitionTopicName : materialized.storeName()) + KStreamImpl.REPARTITION_TOPIC_SUFFIX;    if (repartitionGraphNode == null || userProvidedRepartitionTopicName == null) {        repartitionGraphNode = createRepartitionNode(sinkName, sourceName, repartitionTopic);    }    // the passed in StreamsGraphNode must be the parent of the repartition node    builder.addGraphNode(this.streamsGraphNode, repartitionGraphNode);    final StatefulProcessorNode statefulProcessorNode = new StatefulProcessorNode<>(funcName, new ProcessorParameters<>(aggregateSupplier, funcName), new TimestampedKeyValueStoreMaterializer<>(materialized).materialize());    // now the repartition node must be the parent of the StateProcessorNode    builder.addGraphNode(repartitionGraphNode, statefulProcessorNode);    // return the KTable representation with the intermediate topic as the sources    return new KTableImpl<>(funcName, materialized.keySerde(), materialized.valueSerde(), Collections.singleton(sourceName), materialized.queryableStoreName(), aggregateSupplier, statefulProcessorNode, builder);}
f12741
0
createRepartitionNode
private GroupedTableOperationRepartitionNode<K, V> kafkatest_f12742_0(final String sinkName, final String sourceName, final String topic)
{    return GroupedTableOperationRepartitionNode.<K, V>groupedTableOperationNodeBuilder().withRepartitionTopic(topic).withSinkName(sinkName).withSourceName(sourceName).withKeySerde(keySerde).withValueSerde(valSerde).withNodeName(sourceName).build();}
f12742
0
reduce
public KTable<K, V> kafkatest_f12743_0(final Reducer<V> adder, final Reducer<V> subtractor, final Materialized<K, V, KeyValueStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(adder, "adder can't be null");    Objects.requireNonNull(subtractor, "subtractor can't be null");    Objects.requireNonNull(materialized, "materialized can't be null");    final MaterializedInternal<K, V, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);    if (materializedInternal.keySerde() == null) {        materializedInternal.withKeySerde(keySerde);    }    if (materializedInternal.valueSerde() == null) {        materializedInternal.withValueSerde(valSerde);    }    final ProcessorSupplier<K, Change<V>> aggregateSupplier = new KTableReduce<>(materializedInternal.storeName(), adder, subtractor);    return doAggregate(aggregateSupplier, REDUCE_NAME, materializedInternal);}
f12743
0
init
public void kafkatest_f12751_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    store = (TimestampedKeyValueStore<K, T>) context.getStateStore(storeName);    tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);}
f12751
0
process
public voidf12752_1final K key, final V value)
{    // If the key or value is null we don't need to proceed    if (key == null || value == null) {                skippedRecordsSensor.record();        return;    }    final ValueAndTimestamp<T> oldAggAndTimestamp = store.get(key);    T oldAgg = getValueOrNull(oldAggAndTimestamp);    final T newAgg;    final long newTimestamp;    if (oldAgg == null) {        oldAgg = initializer.apply();        newTimestamp = context().timestamp();    } else {        oldAgg = oldAggAndTimestamp.value();        newTimestamp = Math.max(context().timestamp(), oldAggAndTimestamp.timestamp());    }    newAgg = aggregator.apply(key, value, oldAgg);    store.put(key, ValueAndTimestamp.make(newAgg, newTimestamp));    tupleForwarder.maybeForward(key, newAgg, sendOldValues ? oldAgg : null, newTimestamp);}
public voidf12752
1
view
public KTableValueGetterSupplier<K, T> kafkatest_f12753_0()
{    return new KTableValueGetterSupplier<K, T>() {        public KTableValueGetter<K, T> get() {            return new KStreamAggregateValueGetter();        }        @Override        public String[] storeNames() {            return new String[] { storeName };        }    };}
f12753
0
process
public void kafkatest_f12762_0(final K key, final V value)
{    if (filterNot ^ predicate.test(key, value)) {        context().forward(key, value);    }}
f12762
0
get
public Processor<K, V> kafkatest_f12763_0()
{    return new KStreamFlatMapProcessor();}
f12763
0
process
public void kafkatest_f12764_0(final K key, final V value)
{    for (final KeyValue<? extends K1, ? extends V1> newPair : mapper.apply(key, value)) {        context().forward(newPair.key, newPair.value);    }}
f12764
0
init
public void kafkatest_f12772_0(final ProcessorContext context)
{    valueTransformer.init(new ForwardingDisabledProcessorContext(context));    this.context = context;}
f12772
0
process
public void kafkatest_f12773_0(final KIn key, final VIn value)
{    final Iterable<VOut> transformedValues = valueTransformer.transform(key, value);    if (transformedValues != null) {        for (final VOut transformedValue : transformedValues) {            context.forward(key, transformedValue);        }    }}
f12773
0
close
public void kafkatest_f12774_0()
{    valueTransformer.close();}
f12774
0
internalSelectKey
private ProcessorGraphNode<K, V> kafkatest_f12782_0(final KeyValueMapper<? super K, ? super V, ? extends KR> mapper, final NamedInternal named)
{    final String name = named.orElseGenerateWithPrefix(builder, KEY_SELECT_NAME);    final KStreamMap<K, V, KR, V> kStreamMap = new KStreamMap<>((key, value) -> new KeyValue<>(mapper.apply(key, value), value));    final ProcessorParameters<K, V> processorParameters = new ProcessorParameters<>(kStreamMap, name);    return new ProcessorGraphNode<>(name, processorParameters);}
f12782
0
map
public KStream<KR, VR> kafkatest_f12783_0(final KeyValueMapper<? super K, ? super V, ? extends KeyValue<? extends KR, ? extends VR>> mapper)
{    return map(mapper, NamedInternal.empty());}
f12783
0
map
public KStream<KR, VR> kafkatest_f12784_0(final KeyValueMapper<? super K, ? super V, ? extends KeyValue<? extends KR, ? extends VR>> mapper, final Named named)
{    Objects.requireNonNull(mapper, "mapper can't be null");    Objects.requireNonNull(named, "named can't be null");    final String name = new NamedInternal(named).orElseGenerateWithPrefix(builder, MAP_NAME);    final ProcessorParameters<? super K, ? super V> processorParameters = new ProcessorParameters<>(new KStreamMap<>(mapper), name);    final ProcessorGraphNode<? super K, ? super V> mapProcessorNode = new ProcessorGraphNode<>(name, processorParameters);    mapProcessorNode.keyChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, mapProcessorNode);    // key and value serde cannot be preserved    return new KStreamImpl<>(name, null, null, sourceNodes, true, mapProcessorNode, builder);}
f12784
0
flatMapValues
public KStream<K, VR> kafkatest_f12792_0(final ValueMapper<? super V, ? extends Iterable<? extends VR>> mapper)
{    return flatMapValues(withKey(mapper));}
f12792
0
flatMapValues
public KStream<K, VR> kafkatest_f12793_0(final ValueMapper<? super V, ? extends Iterable<? extends VR>> mapper, final Named named)
{    return flatMapValues(withKey(mapper), named);}
f12793
0
flatMapValues
public KStream<K, VR> kafkatest_f12794_0(final ValueMapperWithKey<? super K, ? super V, ? extends Iterable<? extends VR>> mapper)
{    return flatMapValues(mapper, NamedInternal.empty());}
f12794
0
foreach
public void kafkatest_f12802_0(final ForeachAction<? super K, ? super V> action)
{    foreach(action, NamedInternal.empty());}
f12802
0
foreach
public void kafkatest_f12803_0(final ForeachAction<? super K, ? super V> action, final Named named)
{    Objects.requireNonNull(action, "action can't be null");    final String name = new NamedInternal(named).orElseGenerateWithPrefix(builder, FOREACH_NAME);    final ProcessorParameters<? super K, ? super V> processorParameters = new ProcessorParameters<>(new KStreamPeek<>(action, false), name);    final ProcessorGraphNode<? super K, ? super V> foreachNode = new ProcessorGraphNode<>(name, processorParameters);    builder.addGraphNode(this.streamsGraphNode, foreachNode);}
f12803
0
peek
public KStream<K, V> kafkatest_f12804_0(final ForeachAction<? super K, ? super V> action)
{    return peek(action, NamedInternal.empty());}
f12804
0
to
private void kafkatest_f12812_0(final TopicNameExtractor<K, V> topicExtractor, final ProducedInternal<K, V> produced)
{    final String name = new NamedInternal(produced.name()).orElseGenerateWithPrefix(builder, SINK_NAME);    final StreamSinkNode<K, V> sinkNode = new StreamSinkNode<>(name, topicExtractor, produced);    builder.addGraphNode(this.streamsGraphNode, sinkNode);}
f12812
0
transform
public KStream<KR, VR> kafkatest_f12813_0(final TransformerSupplier<? super K, ? super V, KeyValue<KR, VR>> transformerSupplier, final String... stateStoreNames)
{    Objects.requireNonNull(transformerSupplier, "transformerSupplier can't be null");    final String name = builder.newProcessorName(TRANSFORM_NAME);    return flatTransform(new TransformerSupplierAdapter<>(transformerSupplier), Named.as(name), stateStoreNames);}
f12813
0
transform
public KStream<KR, VR> kafkatest_f12814_0(final TransformerSupplier<? super K, ? super V, KeyValue<KR, VR>> transformerSupplier, final Named named, final String... stateStoreNames)
{    Objects.requireNonNull(transformerSupplier, "transformerSupplier can't be null");    return flatTransform(new TransformerSupplierAdapter<>(transformerSupplier), named, stateStoreNames);}
f12814
0
flatTransformValues
public KStream<K, VR> kafkatest_f12822_0(final ValueTransformerSupplier<? super V, Iterable<VR>> valueTransformerSupplier, final String... stateStoreNames)
{    Objects.requireNonNull(valueTransformerSupplier, "valueTransformerSupplier can't be null");    return doFlatTransformValues(toValueTransformerWithKeySupplier(valueTransformerSupplier), NamedInternal.empty(), stateStoreNames);}
f12822
0
flatTransformValues
public KStream<K, VR> kafkatest_f12823_0(final ValueTransformerSupplier<? super V, Iterable<VR>> valueTransformerSupplier, final Named named, final String... stateStoreNames)
{    Objects.requireNonNull(valueTransformerSupplier, "valueTransformerSupplier can't be null");    return doFlatTransformValues(toValueTransformerWithKeySupplier(valueTransformerSupplier), named, stateStoreNames);}
f12823
0
flatTransformValues
public KStream<K, VR> kafkatest_f12824_0(final ValueTransformerWithKeySupplier<? super K, ? super V, Iterable<VR>> valueTransformerSupplier, final String... stateStoreNames)
{    Objects.requireNonNull(valueTransformerSupplier, "valueTransformerSupplier can't be null");    return doFlatTransformValues(valueTransformerSupplier, NamedInternal.empty(), stateStoreNames);}
f12824
0
outerJoin
public KStream<K, VR> kafkatest_f12832_0(final KStream<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final JoinWindows windows, final Joined<K, V, VO> joined)
{    return doJoin(other, joiner, windows, joined, new KStreamImplJoin(true, true));}
f12832
0
doJoin
private KStream<K, VR> kafkatest_f12833_0(final KStream<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final JoinWindows windows, final Joined<K, V, VO> joined, final KStreamImplJoin join)
{    Objects.requireNonNull(other, "other KStream can't be null");    Objects.requireNonNull(joiner, "joiner can't be null");    Objects.requireNonNull(windows, "windows can't be null");    Objects.requireNonNull(joined, "joined can't be null");    KStreamImpl<K, V> joinThis = this;    KStreamImpl<K, VO> joinOther = (KStreamImpl<K, VO>) other;    final JoinedInternal<K, V, VO> joinedInternal = new JoinedInternal<>(joined);    final NamedInternal name = new NamedInternal(joinedInternal.name());    if (joinThis.repartitionRequired) {        final String joinThisName = joinThis.name;        final String leftJoinRepartitionTopicName = name.suffixWithOrElseGet("-left", joinThisName);        joinThis = joinThis.repartitionForJoin(leftJoinRepartitionTopicName, joined.keySerde(), joined.valueSerde());    }    if (joinOther.repartitionRequired) {        final String joinOtherName = joinOther.name;        final String rightJoinRepartitionTopicName = name.suffixWithOrElseGet("-right", joinOtherName);        joinOther = joinOther.repartitionForJoin(rightJoinRepartitionTopicName, joined.keySerde(), joined.otherValueSerde());    }    joinThis.ensureJoinableWith(joinOther);    return join.join(joinThis, joinOther, joiner, windows, joined);}
f12833
0
repartitionForJoin
private KStreamImpl<K, V> kafkatest_f12834_0(final String repartitionName, final Serde<K> keySerdeOverride, final Serde<V> valueSerdeOverride)
{    final Serde<K> repartitionKeySerde = keySerdeOverride != null ? keySerdeOverride : keySerde;    final Serde<V> repartitionValueSerde = valueSerdeOverride != null ? valueSerdeOverride : valSerde;    final OptimizableRepartitionNodeBuilder<K, V> optimizableRepartitionNodeBuilder = OptimizableRepartitionNode.optimizableRepartitionNodeBuilder();    final String repartitionedSourceName = createRepartitionedSource(builder, repartitionKeySerde, repartitionValueSerde, repartitionName, optimizableRepartitionNodeBuilder);    final OptimizableRepartitionNode<K, V> optimizableRepartitionNode = optimizableRepartitionNodeBuilder.build();    builder.addGraphNode(this.streamsGraphNode, optimizableRepartitionNode);    return new KStreamImpl<>(repartitionedSourceName, repartitionKeySerde, repartitionValueSerde, Collections.singleton(repartitionedSourceName), false, optimizableRepartitionNode, builder);}
f12834
0
join
public KStream<K, VR> kafkatest_f12842_0(final GlobalKTable<KG, VG> globalTable, final KeyValueMapper<? super K, ? super V, ? extends KG> keyMapper, final ValueJoiner<? super V, ? super VG, ? extends VR> joiner)
{    return globalTableJoin(globalTable, keyMapper, joiner, false, NamedInternal.empty());}
f12842
0
join
public KStream<K, VR> kafkatest_f12843_0(final GlobalKTable<KG, VG> globalTable, final KeyValueMapper<? super K, ? super V, ? extends KG> keyMapper, final ValueJoiner<? super V, ? super VG, ? extends VR> joiner, final Named named)
{    return globalTableJoin(globalTable, keyMapper, joiner, false, named);}
f12843
0
leftJoin
public KStream<K, VR> kafkatest_f12844_0(final GlobalKTable<KG, VG> globalTable, final KeyValueMapper<? super K, ? super V, ? extends KG> keyMapper, final ValueJoiner<? super V, ? super VG, ? extends VR> joiner)
{    return globalTableJoin(globalTable, keyMapper, joiner, true, NamedInternal.empty());}
f12844
0
groupByKey
public KGroupedStream<K, V> kafkatest_f12852_0(final org.apache.kafka.streams.kstream.Serialized<K, V> serialized)
{    final SerializedInternal<K, V> serializedInternal = new SerializedInternal<>(serialized);    return groupByKey(Grouped.with(serializedInternal.keySerde(), serializedInternal.valueSerde()));}
f12852
0
groupByKey
public KGroupedStream<K, V> kafkatest_f12853_0(final Grouped<K, V> grouped)
{    final GroupedInternal<K, V> groupedInternal = new GroupedInternal<>(grouped);    return new KGroupedStreamImpl<>(name, sourceNodes, groupedInternal, repartitionRequired, streamsGraphNode, builder);}
f12853
0
joinWindowStoreBuilder
private static StoreBuilder<WindowStore<K, V>> kafkatest_f12854_0(final String joinName, final JoinWindows windows, final Serde<K> keySerde, final Serde<V> valueSerde)
{    return Stores.windowStoreBuilder(Stores.persistentWindowStore(joinName + "-store", Duration.ofMillis(windows.size() + windows.gracePeriodMs()), Duration.ofMillis(windows.size()), true), keySerde, valueSerde);}
f12854
0
apply
public K kafkatest_f12862_0(final K key, final V1 value)
{    return key;}
f12862
0
get
public Processor<K, V1> kafkatest_f12863_0()
{    return new KStreamKTableJoinProcessor<>(valueGetterSupplier.get(), keyValueMapper, joiner, leftJoin);}
f12863
0
init
public void kafkatest_f12864_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    valueGetter.init(context);}
f12864
0
process
public void kafkatest_f12872_0(final K key, final V value)
{    context().forward(key, value);}
f12872
0
get
public Processor<K, V> kafkatest_f12873_0()
{    return new KStreamPeekProcessor();}
f12873
0
process
public void kafkatest_f12874_0(final K key, final V value)
{    action.apply(key, value);    if (forwardDownStream) {        context().forward(key, value);    }}
f12874
0
view
public KTableValueGetterSupplier<K, V> kafkatest_f12882_0()
{    return new KTableValueGetterSupplier<K, V>() {        public KTableValueGetter<K, V> get() {            return new KStreamReduceValueGetter();        }        @Override        public String[] storeNames() {            return new String[] { storeName };        }    };}
f12882
0
get
public KTableValueGetter<K, V> kafkatest_f12883_0()
{    return new KStreamReduceValueGetter();}
f12883
0
storeNames
public String[] kafkatest_f12884_0()
{    return new String[] { storeName };}
f12884
0
mergeSessionWindow
private SessionWindow kafkatest_f12893_0(final SessionWindow one, final SessionWindow two)
{    final long start = one.start() < two.start() ? one.start() : two.start();    final long end = one.end() > two.end() ? one.end() : two.end();    return new SessionWindow(start, end);}
f12893
0
view
public KTableValueGetterSupplier<Windowed<K>, Agg> kafkatest_f12894_0()
{    return new KTableValueGetterSupplier<Windowed<K>, Agg>() {        @Override        public KTableValueGetter<Windowed<K>, Agg> get() {            return new KTableSessionWindowValueGetter();        }        @Override        public String[] storeNames() {            return new String[] { storeName };        }    };}
f12894
0
get
public KTableValueGetter<Windowed<K>, Agg> kafkatest_f12895_0()
{    return new KTableSessionWindowValueGetter();}
f12895
0
get
public Processor<K, V> kafkatest_f12904_0()
{    return new KStreamWindowAggregateProcessor();}
f12904
0
windows
public Windows<W> kafkatest_f12905_0()
{    return windows;}
f12905
0
enableSendingOldValues
public void kafkatest_f12906_0()
{    sendOldValues = true;}
f12906
0
enableSendingOldValues
public void kafkatest_f12915_0()
{    sendOldValues = true;}
f12915
0
get
public Processor<K, Change<V>> kafkatest_f12916_0()
{    return new KTableAggregateProcessor();}
f12916
0
init
public void kafkatest_f12917_0(final ProcessorContext context)
{    super.init(context);    store = (TimestampedKeyValueStore<K, T>) context.getStateStore(storeName);    tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);}
f12917
0
process
public void kafkatest_f12925_0(final K key, final Change<V> change)
{    final V newValue = computeValue(key, change.newValue);    final V oldValue = sendOldValues ? computeValue(key, change.oldValue) : null;    if (sendOldValues && oldValue == null && newValue == null) {        // unnecessary to forward here.        return;    }    if (queryableName != null) {        store.put(key, ValueAndTimestamp.make(newValue, context().timestamp()));        tupleForwarder.maybeForward(key, newValue, oldValue);    } else {        context().forward(key, new Change<>(newValue, oldValue));    }}
f12925
0
view
public KTableValueGetterSupplier<K, V> kafkatest_f12926_0()
{    // otherwise rely on the parent getter and apply filter on-the-fly    if (queryableName != null) {        return new KTableMaterializedValueGetterSupplier<>(queryableName);    } else {        return new KTableValueGetterSupplier<K, V>() {            final KTableValueGetterSupplier<K, V> parentValueGetterSupplier = parent.valueGetterSupplier();            public KTableValueGetter<K, V> get() {                return new KTableFilterValueGetter(parentValueGetterSupplier.get());            }            @Override            public String[] storeNames() {                return parentValueGetterSupplier.storeNames();            }        };    }}
f12926
0
get
public KTableValueGetter<K, V> kafkatest_f12927_0()
{    return new KTableFilterValueGetter(parentValueGetterSupplier.get());}
f12927
0
filter
public KTable<K, V> kafkatest_f12935_0(final Predicate<? super K, ? super V> predicate, final Named named)
{    Objects.requireNonNull(predicate, "predicate can't be null");    return doFilter(predicate, named, null, false);}
f12935
0
filter
public KTable<K, V> kafkatest_f12936_0(final Predicate<? super K, ? super V> predicate, final Named named, final Materialized<K, V, KeyValueStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(predicate, "predicate can't be null");    Objects.requireNonNull(materialized, "materialized can't be null");    final MaterializedInternal<K, V, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized);    return doFilter(predicate, named, materializedInternal, false);}
f12936
0
filter
public KTable<K, V> kafkatest_f12937_0(final Predicate<? super K, ? super V> predicate, final Materialized<K, V, KeyValueStore<Bytes, byte[]>> materialized)
{    return filter(predicate, NamedInternal.empty(), materialized);}
f12937
0
mapValues
public KTable<K, VR> kafkatest_f12945_0(final ValueMapperWithKey<? super K, ? super V, ? extends VR> mapper)
{    Objects.requireNonNull(mapper, "mapper can't be null");    return doMapValues(mapper, NamedInternal.empty(), null);}
f12945
0
mapValues
public KTable<K, VR> kafkatest_f12946_0(final ValueMapperWithKey<? super K, ? super V, ? extends VR> mapper, final Named named)
{    Objects.requireNonNull(mapper, "mapper can't be null");    return doMapValues(mapper, named, null);}
f12946
0
mapValues
public KTable<K, VR> kafkatest_f12947_0(final ValueMapper<? super V, ? extends VR> mapper, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized)
{    return mapValues(mapper, NamedInternal.empty(), materialized);}
f12947
0
doTransformValues
private KTable<K, VR> kafkatest_f12955_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> transformerSupplier, final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal, final NamedInternal namedInternal, final String... stateStoreNames)
{    Objects.requireNonNull(stateStoreNames, "stateStoreNames");    final Serde<K> keySerde;    final Serde<VR> valueSerde;    final String queryableStoreName;    final StoreBuilder<TimestampedKeyValueStore<K, VR>> storeBuilder;    if (materializedInternal != null) {        // don't inherit parent value serde, since this operation may change the value type, more specifically:        // we preserve the key following the order of 1) materialized, 2) parent, 3) null        keySerde = materializedInternal.keySerde() != null ? materializedInternal.keySerde() : this.keySerde;        // we preserve the value following the order of 1) materialized, 2) null        valueSerde = materializedInternal.valueSerde();        queryableStoreName = materializedInternal.queryableStoreName();        // only materialize if materialized is specified and it has queryable name        storeBuilder = queryableStoreName != null ? (new TimestampedKeyValueStoreMaterializer<>(materializedInternal)).materialize() : null;    } else {        keySerde = this.keySerde;        valueSerde = null;        queryableStoreName = null;        storeBuilder = null;    }    final String name = namedInternal.orElseGenerateWithPrefix(builder, TRANSFORMVALUES_NAME);    final KTableProcessorSupplier<K, V, VR> processorSupplier = new KTableTransformValues<>(this, transformerSupplier, queryableStoreName);    final ProcessorParameters<K, VR> processorParameters = unsafeCastProcessorParametersToCompletelyDifferentType(new ProcessorParameters<>(processorSupplier, name));    final StreamsGraphNode tableNode = new TableProcessorNode<>(name, processorParameters, storeBuilder, stateStoreNames);    builder.addGraphNode(this.streamsGraphNode, tableNode);    return new KTableImpl<>(name, keySerde, valueSerde, sourceNodes, queryableStoreName, processorSupplier, tableNode, builder);}
f12955
0
toStream
public KStream<K, V> kafkatest_f12956_0()
{    return toStream(NamedInternal.empty());}
f12956
0
toStream
public KStream<K, V> kafkatest_f12957_0(final Named named)
{    Objects.requireNonNull(named, "named can't be null");    final String name = new NamedInternal(named).orElseGenerateWithPrefix(builder, TOSTREAM_NAME);    final ProcessorSupplier<K, Change<V>> kStreamMapValues = new KStreamMapValues<>((key, change) -> change.newValue);    final ProcessorParameters<K, V> processorParameters = unsafeCastProcessorParametersToCompletelyDifferentType(new ProcessorParameters<>(kStreamMapValues, name));    final ProcessorGraphNode<K, V> toStreamNode = new ProcessorGraphNode<>(name, processorParameters);    builder.addGraphNode(this.streamsGraphNode, toStreamNode);    // we can inherit parent key and value serde    return new KStreamImpl<>(name, keySerde, valSerde, sourceNodes, false, toStreamNode, builder);}
f12957
0
join
public KTable<K, VR> kafkatest_f12965_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Named named, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(materialized, "materialized can't be null");    final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, MERGE_NAME);    return doJoin(other, joiner, named, materializedInternal, false, false);}
f12965
0
outerJoin
public KTable<K, R> kafkatest_f12966_0(final KTable<K, V1> other, final ValueJoiner<? super V, ? super V1, ? extends R> joiner)
{    return outerJoin(other, joiner, NamedInternal.empty());}
f12966
0
outerJoin
public KTable<K, R> kafkatest_f12967_0(final KTable<K, V1> other, final ValueJoiner<? super V, ? super V1, ? extends R> joiner, final Named named)
{    return doJoin(other, joiner, named, null, true, true);}
f12967
0
groupBy
public KGroupedTable<K1, V1> kafkatest_f12975_0(final KeyValueMapper<? super K, ? super V, KeyValue<K1, V1>> selector)
{    return groupBy(selector, Grouped.with(null, null));}
f12975
0
groupBy
public KGroupedTable<K1, V1> kafkatest_f12976_0(final KeyValueMapper<? super K, ? super V, KeyValue<K1, V1>> selector, final org.apache.kafka.streams.kstream.Serialized<K1, V1> serialized)
{    Objects.requireNonNull(selector, "selector can't be null");    Objects.requireNonNull(serialized, "serialized can't be null");    final SerializedInternal<K1, V1> serializedInternal = new SerializedInternal<>(serialized);    return groupBy(selector, Grouped.with(serializedInternal.keySerde(), serializedInternal.valueSerde()));}
f12976
0
groupBy
public KGroupedTable<K1, V1> kafkatest_f12977_0(final KeyValueMapper<? super K, ? super V, KeyValue<K1, V1>> selector, final Grouped<K1, V1> grouped)
{    Objects.requireNonNull(selector, "selector can't be null");    Objects.requireNonNull(grouped, "grouped can't be null");    final GroupedInternal<K1, V1> groupedInternal = new GroupedInternal<>(grouped);    final String selectName = new NamedInternal(groupedInternal.name()).orElseGenerateWithPrefix(builder, SELECT_NAME);    final KTableProcessorSupplier<K, V, KeyValue<K1, V1>> selectSupplier = new KTableRepartitionMap<>(this, selector);    final ProcessorParameters<K, Change<V>> processorParameters = new ProcessorParameters<>(selectSupplier, selectName);    // select the aggregate key and values (old and new), it would require parent to send old values    final ProcessorGraphNode<K, Change<V>> groupByMapNode = new ProcessorGraphNode<>(selectName, processorParameters);    builder.addGraphNode(this.streamsGraphNode, groupByMapNode);    this.enableSendingOldValues();    return new KGroupedTableImpl<>(builder, selectName, sourceNodes, groupedInternal, groupByMapNode);}
f12977
0
view
public KTableValueGetterSupplier<K, R> kafkatest_f12985_0()
{    return new KTableKTableInnerJoinValueGetterSupplier(valueGetterSupplier1, valueGetterSupplier2);}
f12985
0
get
public KTableValueGetter<K, R> kafkatest_f12986_0()
{    return new KTableKTableInnerJoinValueGetter(valueGetterSupplier1.get(), valueGetterSupplier2.get());}
f12986
0
init
public void kafkatest_f12987_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    valueGetter.init(context);}
f12987
0
view
public KTableValueGetterSupplier<K, V> kafkatest_f12995_0()
{    // otherwise rely on the parent getter and apply join on-the-fly    if (queryableName != null) {        return new KTableMaterializedValueGetterSupplier<>(queryableName);    } else {        return new KTableValueGetterSupplier<K, V>() {            public KTableValueGetter<K, V> get() {                return parent1.view().get();            }            @Override            public String[] storeNames() {                final String[] storeNames1 = parent1.view().storeNames();                final String[] storeNames2 = parent2.view().storeNames();                final Set<String> stores = new HashSet<>(storeNames1.length + storeNames2.length);                Collections.addAll(stores, storeNames1);                Collections.addAll(stores, storeNames2);                return stores.toArray(new String[stores.size()]);            }        };    }}
f12995
0
get
public KTableValueGetter<K, V> kafkatest_f12996_0()
{    return parent1.view().get();}
f12996
0
storeNames
public String[] kafkatest_f12997_0()
{    final String[] storeNames1 = parent1.view().storeNames();    final String[] storeNames2 = parent2.view().storeNames();    final Set<String> stores = new HashSet<>(storeNames1.length + storeNames2.length);    Collections.addAll(stores, storeNames1);    Collections.addAll(stores, storeNames2);    return stores.toArray(new String[stores.size()]);}
f12997
0
get
public KTableValueGetter<K, R> kafkatest_f13005_0()
{    return new KTableKTableLeftJoinValueGetter(valueGetterSupplier1.get(), valueGetterSupplier2.get());}
f13005
0
init
public void kafkatest_f13006_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    valueGetter.init(context);}
f13006
0
process
public voidf13007_1final K key, final Change<V1> change)
{    // we do join iff keys are equal, thus, if key is null we cannot join and just ignore the record    if (key == null) {                skippedRecordsSensor.record();        return;    }    R newValue = null;    final long resultTimestamp;    R oldValue = null;    final ValueAndTimestamp<V2> valueAndTimestampRight = valueGetter.get(key);    final V2 value2 = getValueOrNull(valueAndTimestampRight);    final long timestampRight;    if (value2 == null) {        if (change.newValue == null && change.oldValue == null) {            return;        }        timestampRight = UNKNOWN;    } else {        timestampRight = valueAndTimestampRight.timestamp();    }    resultTimestamp = Math.max(context().timestamp(), timestampRight);    if (change.newValue != null) {        newValue = joiner.apply(change.newValue, value2);    }    if (sendOldValues && change.oldValue != null) {        oldValue = joiner.apply(change.oldValue, value2);    }    context().forward(key, new Change<>(newValue, oldValue), To.all().withTimestamp(resultTimestamp));}
public voidf13007
1
init
public void kafkatest_f13015_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    valueGetter.init(context);}
f13015
0
process
public voidf13016_1final K key, final Change<V1> change)
{    // we do join iff keys are equal, thus, if key is null we cannot join and just ignore the record    if (key == null) {                skippedRecordsSensor.record();        return;    }    R newValue = null;    final long resultTimestamp;    R oldValue = null;    final ValueAndTimestamp<V2> valueAndTimestamp2 = valueGetter.get(key);    final V2 value2 = getValueOrNull(valueAndTimestamp2);    if (value2 == null) {        if (change.newValue == null && change.oldValue == null) {            return;        }        resultTimestamp = context().timestamp();    } else {        resultTimestamp = Math.max(context().timestamp(), valueAndTimestamp2.timestamp());    }    if (value2 != null || change.newValue != null) {        newValue = joiner.apply(change.newValue, value2);    }    if (sendOldValues && (value2 != null || change.oldValue != null)) {        oldValue = joiner.apply(change.oldValue, value2);    }    context().forward(key, new Change<>(newValue, oldValue), To.all().withTimestamp(resultTimestamp));}
public voidf13016
1
close
public void kafkatest_f13017_0()
{    valueGetter.close();}
f13017
0
process
public voidf13025_1final K key, final Change<V1> change)
{    // we do join iff keys are equal, thus, if key is null we cannot join and just ignore the record    if (key == null) {                skippedRecordsSensor.record();        return;    }    final R newValue;    final long resultTimestamp;    R oldValue = null;    final ValueAndTimestamp<V2> valueAndTimestampLeft = valueGetter.get(key);    final V2 valueLeft = getValueOrNull(valueAndTimestampLeft);    if (valueLeft == null) {        return;    }    resultTimestamp = Math.max(context().timestamp(), valueAndTimestampLeft.timestamp());    // joiner == "reverse joiner"    newValue = joiner.apply(change.newValue, valueLeft);    if (sendOldValues) {        // joiner == "reverse joiner"        oldValue = joiner.apply(change.oldValue, valueLeft);    }    context().forward(key, new Change<>(newValue, oldValue), To.all().withTimestamp(resultTimestamp));}
public voidf13025
1
close
public void kafkatest_f13026_0()
{    valueGetter.close();}
f13026
0
init
public void kafkatest_f13027_0(final ProcessorContext context)
{    valueGetter1.init(context);    valueGetter2.init(context);}
f13027
0
computeValue
private V1 kafkatest_f13035_0(final K key, final V value)
{    V1 newValue = null;    if (value != null) {        newValue = mapper.apply(key, value);    }    return newValue;}
f13035
0
computeValueAndTimestamp
private ValueAndTimestamp<V1> kafkatest_f13036_0(final K key, final ValueAndTimestamp<V> valueAndTimestamp)
{    V1 newValue = null;    long timestamp = 0;    if (valueAndTimestamp != null) {        newValue = mapper.apply(key, valueAndTimestamp.value());        timestamp = valueAndTimestamp.timestamp();    }    return ValueAndTimestamp.make(newValue, timestamp);}
f13036
0
init
public void kafkatest_f13037_0(final ProcessorContext context)
{    super.init(context);    if (queryableName != null) {        store = (TimestampedKeyValueStore<K, V1>) context.getStateStore(queryableName);        tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<K, V1>(context), sendOldValues);    }}
f13037
0
get
public ValueAndTimestamp<V> kafkatest_f13045_0(final K key)
{    return store.get(key);}
f13045
0
enableSendingOldValues
public void kafkatest_f13047_0()
{    sendOldValues = true;}
f13047
0
get
public Processor<K, Change<V>> kafkatest_f13048_0()
{    return new KTableReduceProcessor();}
f13048
0
enableSendingOldValues
public void kafkatest_f13056_0()
{    // this should never be called    throw new IllegalStateException("KTableRepartitionMap should always require sending old values.");}
f13056
0
process
public void kafkatest_f13057_0(final K key, final Change<V> change)
{    // the original key should never be null    if (key == null) {        throw new StreamsException("Record key for the grouping KTable should not be null.");    }    // if the value is null, we do not need to forward its selected key-value further    final KeyValue<? extends K1, ? extends V1> newPair = change.newValue == null ? null : mapper.apply(key, change.newValue);    final KeyValue<? extends K1, ? extends V1> oldPair = change.oldValue == null ? null : mapper.apply(key, change.oldValue);    // forward oldPair first, to be consistent with reduce and aggregate    if (oldPair != null && oldPair.key != null && oldPair.value != null) {        context().forward(oldPair.key, new Change<>(null, oldPair.value));    }    if (newPair != null && newPair.key != null && newPair.value != null) {        context().forward(newPair.key, new Change<>(newPair.value, null));    }}
f13057
0
init
public void kafkatest_f13058_0(final ProcessorContext context)
{    this.context = context;    parentGetter.init(context);}
f13058
0
process
public voidf13066_1final K key, final V value)
{    // if the key is null, then ignore the record    if (key == null) {                skippedRecordsSensor.record();        return;    }    if (queryableName != null) {        final ValueAndTimestamp<V> oldValueAndTimestamp = store.get(key);        final V oldValue;        if (oldValueAndTimestamp != null) {            oldValue = oldValueAndTimestamp.value();            if (context().timestamp() < oldValueAndTimestamp.timestamp()) {                            }        } else {            oldValue = null;        }        store.put(key, ValueAndTimestamp.make(value, context().timestamp()));        tupleForwarder.maybeForward(key, value, oldValue);    } else {        context().forward(key, new Change<>(value, null));    }}
public voidf13066
1
get
public KTableValueGetter<K, V> kafkatest_f13067_0()
{    return new KTableSourceValueGetter();}
f13067
0
storeNames
public String[] kafkatest_f13068_0()
{    return new String[] { storeName };}
f13068
0
init
public void kafkatest_f13077_0(final ProcessorContext context)
{    super.init(context);    valueTransformer.init(new ForwardingDisabledProcessorContext(context));    if (queryableName != null) {        store = (TimestampedKeyValueStore<K, V1>) context.getStateStore(queryableName);        tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);    }}
f13077
0
process
public void kafkatest_f13078_0(final K key, final Change<V> change)
{    final V1 newValue = valueTransformer.transform(key, change.newValue);    if (queryableName == null) {        final V1 oldValue = sendOldValues ? valueTransformer.transform(key, change.oldValue) : null;        context().forward(key, new Change<>(newValue, oldValue));    } else {        final V1 oldValue = sendOldValues ? getValueOrNull(store.get(key)) : null;        store.put(key, ValueAndTimestamp.make(newValue, context().timestamp()));        tupleForwarder.maybeForward(key, newValue, oldValue);    }}
f13078
0
close
public void kafkatest_f13079_0()
{    valueTransformer.close();}
f13079
0
valueSerde
public Serde<V> kafkatest_f13087_0()
{    return valueSerde;}
f13087
0
loggingEnabled
public boolean kafkatest_f13088_0()
{    return loggingEnabled;}
f13088
0
logConfig
 Map<String, String> kafkatest_f13089_0()
{    return topicConfig;}
f13089
0
name
public String kafkatest_f13097_0()
{    return name;}
f13097
0
withName
public NamedInternal kafkatest_f13098_0(final String name)
{    return new NamedInternal(name);}
f13098
0
suffixWithOrElseGet
 String kafkatest_f13099_0(final String suffix, final String other)
{    if (name != null) {        return name + suffix;    } else {        return other;    }}
f13099
0
valueSerde
public Serde<V> kafkatest_f13107_0()
{    return valueSerde;}
f13107
0
streamPartitioner
public StreamPartitioner<? super K, ? super V> kafkatest_f13108_0()
{    return partitioner;}
f13108
0
name
public String kafkatest_f13109_0()
{    return processorName;}
f13109
0
doCount
private KTable<Windowed<K>, Long> kafkatest_f13117_0(final Materialized<K, Long, SessionStore<Bytes, byte[]>> materialized)
{    final MaterializedInternal<K, Long, SessionStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);    if (materializedInternal.keySerde() == null) {        materializedInternal.withKeySerde(keySerde);    }    if (materializedInternal.valueSerde() == null) {        materializedInternal.withValueSerde(Serdes.Long());    }    return aggregateBuilder.build(AGGREGATE_NAME, materialize(materializedInternal), new KStreamSessionWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator, countMerger), materializedInternal.queryableStoreName(), materializedInternal.keySerde() != null ? new WindowedSerdes.SessionWindowedSerde<>(materializedInternal.keySerde()) : null, materializedInternal.valueSerde());}
f13117
0
reduce
public KTable<Windowed<K>, V> kafkatest_f13118_0(final Reducer<V> reducer)
{    return reduce(reducer, Materialized.with(keySerde, valSerde));}
f13118
0
reduce
public KTable<Windowed<K>, V> kafkatest_f13119_0(final Reducer<V> reducer, final Materialized<K, V, SessionStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(reducer, "reducer can't be null");    Objects.requireNonNull(materialized, "materialized can't be null");    final Aggregator<K, V, V> reduceAggregator = aggregatorForReducer(reducer);    final MaterializedInternal<K, V, SessionStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, REDUCE_NAME);    if (materializedInternal.keySerde() == null) {        materializedInternal.withKeySerde(keySerde);    }    if (materializedInternal.valueSerde() == null) {        materializedInternal.withValueSerde(valSerde);    }    return aggregateBuilder.build(REDUCE_NAME, materialize(materializedInternal), new KStreamSessionWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, reduceAggregator, mergerForAggregator(reduceAggregator)), materializedInternal.queryableStoreName(), materializedInternal.keySerde() != null ? new WindowedSerdes.SessionWindowedSerde<>(materializedInternal.keySerde()) : null, materializedInternal.valueSerde());}
f13119
0
emitEarlyWhenFull
public Suppressed.EagerBufferConfig kafkatest_f13127_0()
{    return new EagerBufferConfigImpl(maxRecords(), maxBytes());}
f13127
0
withMaxRecords
public Suppressed.EagerBufferConfig kafkatest_f13128_0(final long recordLimit)
{    return new EagerBufferConfigImpl(recordLimit, maxBytes);}
f13128
0
withMaxBytes
public Suppressed.EagerBufferConfig kafkatest_f13129_0(final long byteLimit)
{    return new EagerBufferConfigImpl(maxRecords, byteLimit);}
f13129
0
withName
public Suppressed<K> kafkatest_f13137_0(final String name)
{    return new FinalResultsSuppressionBuilder<>(name, bufferConfig);}
f13137
0
equals
public boolean kafkatest_f13138_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final FinalResultsSuppressionBuilder<?> that = (FinalResultsSuppressionBuilder<?>) o;    return Objects.equals(name, that.name) && Objects.equals(bufferConfig, that.bufferConfig);}
f13138
0
name
public String kafkatest_f13139_0()
{    return name;}
f13139
0
close
public void kafkatest_f13147_0()
{    parentGetter.close();// the main processor is responsible for the buffer's lifecycle}
f13147
0
storeNames
public String[] kafkatest_f13148_0()
{    final String[] parentStores = parentValueGetterSupplier.storeNames();    final String[] stores = new String[1 + parentStores.length];    System.arraycopy(parentStores, 0, stores, 1, parentStores.length);    stores[0] = storeName;    return stores;}
f13148
0
enableSendingOldValues
public void kafkatest_f13149_0()
{    parentKTable.enableSendingOldValues();}
f13149
0
withMaxRecords
public Suppressed.StrictBufferConfig kafkatest_f13158_0(final long recordLimit)
{    return new StrictBufferConfigImpl(recordLimit, maxBytes, bufferFullStrategy);}
f13158
0
withMaxBytes
public Suppressed.StrictBufferConfig kafkatest_f13159_0(final long byteLimit)
{    return new StrictBufferConfigImpl(maxRecords, byteLimit, bufferFullStrategy);}
f13159
0
maxRecords
public long kafkatest_f13160_0()
{    return maxRecords;}
f13160
0
bufferConfig
 BufferConfigInternal kafkatest_f13168_0()
{    return bufferConfig;}
f13168
0
timeDefinition
 TimeDefinition<K> kafkatest_f13169_0()
{    return timeDefinition;}
f13169
0
timeToWaitForMoreEvents
 Duration kafkatest_f13170_0()
{    return timeToWaitForMoreEvents == null ? Duration.ZERO : timeToWaitForMoreEvents;}
f13170
0
instance
public static WindowEndTimeDefinition<K> kafkatest_f13178_0()
{    return WindowEndTimeDefinition.INSTANCE;}
f13178
0
time
public long kafkatest_f13179_0(final ProcessorContext context, final K key)
{    return key.window().end();}
f13179
0
type
public TimeDefinitionType kafkatest_f13180_0()
{    return TimeDefinitionType.WINDOW_END_TIME;}
f13180
0
doCount
private KTable<Windowed<K>, Long> kafkatest_f13188_0(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized)
{    final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);    if (materializedInternal.keySerde() == null) {        materializedInternal.withKeySerde(keySerde);    }    if (materializedInternal.valueSerde() == null) {        materializedInternal.withValueSerde(Serdes.Long());    }    return aggregateBuilder.build(AGGREGATE_NAME, materialize(materializedInternal), new KStreamWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator), materializedInternal.queryableStoreName(), materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.size()) : null, materializedInternal.valueSerde());}
f13188
0
aggregate
public KTable<Windowed<K>, VR> kafkatest_f13189_0(final Initializer<VR> initializer, final Aggregator<? super K, ? super V, VR> aggregator)
{    return aggregate(initializer, aggregator, Materialized.with(keySerde, null));}
f13189
0
aggregate
public KTable<Windowed<K>, VR> kafkatest_f13190_0(final Initializer<VR> initializer, final Aggregator<? super K, ? super V, VR> aggregator, final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(initializer, "initializer can't be null");    Objects.requireNonNull(aggregator, "aggregator can't be null");    Objects.requireNonNull(materialized, "materialized can't be null");    final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);    if (materializedInternal.keySerde() == null) {        materializedInternal.withKeySerde(keySerde);    }    return aggregateBuilder.build(AGGREGATE_NAME, materialize(materializedInternal), new KStreamWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator), materializedInternal.queryableStoreName(), materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.size()) : null, materializedInternal.valueSerde());}
f13190
0
close
public void kafkatest_f13198_0()
{    transformer.close();}
f13198
0
overlap
public boolean kafkatest_f13199_0(final Window other)
{    if (getClass() != other.getClass()) {        throw new IllegalArgumentException("Cannot compare windows of different type. Other window has type " + other.getClass() + ".");    }    return true;}
f13199
0
partition
public Integer kafkatest_f13200_0(final String topic, final Windowed<K> windowedKey, final V value, final int numPartitions)
{    final byte[] keyBytes = serializer.serializeBaseKey(topic, windowedKey);    // hash the keyBytes to choose a partition    return toPositive(Utils.murmur2(keyBytes)) % numPartitions;}
f13200
0
withKeySerde
public Joined<K, V, VO> kafkatest_f13208_0(final Serde<K> keySerde)
{    return new Joined<>(keySerde, valueSerde, otherValueSerde, name);}
f13208
0
withValueSerde
public Joined<K, V, VO> kafkatest_f13209_0(final Serde<V> valueSerde)
{    return new Joined<>(keySerde, valueSerde, otherValueSerde, name);}
f13209
0
withOtherValueSerde
public Joined<K, V, VO> kafkatest_f13210_0(final Serde<VO> otherValueSerde)
{    return new Joined<>(keySerde, valueSerde, otherValueSerde, name);}
f13210
0
before
public JoinWindows kafkatest_f13218_0(final long timeDifferenceMs) throws IllegalArgumentException
{    return new JoinWindows(timeDifferenceMs, afterMs, graceMs, maintainDurationMs, segments);}
f13218
0
before
public JoinWindows kafkatest_f13219_0(final Duration timeDifference) throws IllegalArgumentException
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeDifference, "timeDifference");    return before(ApiUtils.validateMillisecondDuration(timeDifference, msgPrefix));}
f13219
0
after
public JoinWindows kafkatest_f13220_0(final long timeDifferenceMs) throws IllegalArgumentException
{    return new JoinWindows(beforeMs, timeDifferenceMs, graceMs, maintainDurationMs, segments);}
f13220
0
equals
public boolean kafkatest_f13228_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final JoinWindows that = (JoinWindows) o;    return beforeMs == that.beforeMs && afterMs == that.afterMs && maintainDurationMs == that.maintainDurationMs && segments == that.segments && graceMs == that.graceMs;}
f13228
0
hashCode
public int kafkatest_f13229_0()
{    return Objects.hash(beforeMs, afterMs, graceMs, maintainDurationMs, segments);}
f13229
0
toString
public String kafkatest_f13230_0()
{    return "JoinWindows{" + "beforeMs=" + beforeMs + ", afterMs=" + afterMs + ", graceMs=" + graceMs + ", maintainDurationMs=" + maintainDurationMs + ", segments=" + segments + '}';}
f13230
0
withLoggingEnabled
public Materialized<K, V, S> kafkatest_f13238_0(final Map<String, String> config)
{    loggingEnabled = true;    this.topicConfig = config;    return this;}
f13238
0
withLoggingDisabled
public Materialized<K, V, S> kafkatest_f13239_0()
{    loggingEnabled = false;    this.topicConfig.clear();    return this;}
f13239
0
withCachingEnabled
public Materialized<K, V, S> kafkatest_f13240_0()
{    cachingEnabled = true;    return this;}
f13240
0
toFile
public static Printed<K, V> kafkatest_f13248_0(final String filePath)
{    Objects.requireNonNull(filePath, "filePath can't be null");    if (filePath.trim().isEmpty()) {        throw new TopologyException("filePath can't be an empty string");    }    try {        return new Printed<>(Files.newOutputStream(Paths.get(filePath)));    } catch (final IOException e) {        throw new TopologyException("Unable to write stream to file at [" + filePath + "] " + e.getMessage());    }}
f13248
0
toSysOut
public static Printed<K, V> kafkatest_f13249_0()
{    return new Printed<>(System.out);}
f13249
0
withLabel
public Printed<K, V> kafkatest_f13250_0(final String label)
{    Objects.requireNonNull(label, "label can't be null");    this.label = label;    return this;}
f13250
0
streamPartitioner
public static Produced<K, V> kafkatest_f13258_0(final StreamPartitioner<? super K, ? super V> partitioner)
{    return new Produced<>(null, null, partitioner, null);}
f13258
0
withStreamPartitioner
public Produced<K, V> kafkatest_f13259_0(final StreamPartitioner<? super K, ? super V> partitioner)
{    this.partitioner = partitioner;    return this;}
f13259
0
withValueSerde
public Produced<K, V> kafkatest_f13260_0(final Serde<V> valueSerde)
{    this.valueSerde = valueSerde;    return this;}
f13260
0
configure
public void kafkatest_f13268_0(final Map<String, ?> configs, final boolean isKey)
{    if (inner == null) {        final String propertyName = isKey ? StreamsConfig.DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS : StreamsConfig.DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS;        final String value = (String) configs.get(propertyName);        try {            inner = Serde.class.cast(Utils.newInstance(value, Serde.class)).deserializer();            inner.configure(configs, isKey);        } catch (final ClassNotFoundException e) {            throw new ConfigException(propertyName, value, "Serde class " + value + " could not be found.");        }    }}
f13268
0
deserialize
public Windowed<T> kafkatest_f13269_0(final String topic, final byte[] data)
{    WindowedSerdes.verifyInnerDeserializerNotNull(inner, this);    if (data == null || data.length == 0) {        return null;    }    // for either key or value, their schema is the same hence we will just use session key schema    return SessionKeySchema.from(data, inner, topic);}
f13269
0
close
public void kafkatest_f13270_0()
{    if (inner != null) {        inner.close();    }}
f13270
0
with
public static SessionWindows kafkatest_f13278_0(final Duration inactivityGap)
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(inactivityGap, "inactivityGap");    return with(ApiUtils.validateMillisecondDuration(inactivityGap, msgPrefix));}
f13278
0
until
public SessionWindows kafkatest_f13279_0(final long durationMs) throws IllegalArgumentException
{    if (durationMs < gapMs) {        throw new IllegalArgumentException("Window retention time (durationMs) cannot be smaller than window gap.");    }    return new SessionWindows(gapMs, durationMs, graceMs);}
f13279
0
grace
public SessionWindows kafkatest_f13280_0(final Duration afterWindowEnd) throws IllegalArgumentException
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(afterWindowEnd, "afterWindowEnd");    final long afterWindowEndMs = ApiUtils.validateMillisecondDuration(afterWindowEnd, msgPrefix);    if (afterWindowEndMs < 0) {        throw new IllegalArgumentException("Grace period must not be negative.");    }    return new SessionWindows(gapMs, maintainDurationMs, afterWindowEndMs);}
f13280
0
maxBytes
 static EagerBufferConfig kafkatest_f13288_0(final long byteLimit)
{    return new EagerBufferConfigImpl(Long.MAX_VALUE, byteLimit);}
f13288
0
unbounded
 static StrictBufferConfig kafkatest_f13289_0()
{    return new StrictBufferConfigImpl();}
f13289
0
untilWindowCloses
 static Suppressed<Windowed> kafkatest_f13290_0(final StrictBufferConfig bufferConfig)
{    return new FinalResultsSuppressionBuilder<>(null, bufferConfig);}
f13290
0
configure
public void kafkatest_f13298_0(final Map<String, ?> configs, final boolean isKey)
{    if (inner == null) {        final String propertyName = isKey ? StreamsConfig.DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS : StreamsConfig.DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS;        final String value = (String) configs.get(propertyName);        try {            inner = Utils.newInstance(value, Serde.class).serializer();            inner.configure(configs, isKey);        } catch (final ClassNotFoundException e) {            throw new ConfigException(propertyName, value, "Serde class " + value + " could not be found.");        }    }}
f13298
0
serialize
public byte[] kafkatest_f13299_0(final String topic, final Windowed<T> data)
{    WindowedSerdes.verifyInnerSerializerNotNull(inner, this);    if (data == null) {        return null;    }    return WindowKeySchema.toBinary(data, inner, topic);}
f13299
0
close
public void kafkatest_f13300_0()
{    if (inner != null) {        inner.close();    }}
f13300
0
size
public long kafkatest_f13308_0()
{    return sizeMs;}
f13308
0
grace
public TimeWindows kafkatest_f13309_0(final Duration afterWindowEnd) throws IllegalArgumentException
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(afterWindowEnd, "afterWindowEnd");    final long afterWindowEndMs = ApiUtils.validateMillisecondDuration(afterWindowEnd, msgPrefix);    if (afterWindowEndMs < 0) {        throw new IllegalArgumentException("Grace period must not be negative.");    }    return new TimeWindows(sizeMs, advanceMs, afterWindowEndMs, maintainDurationMs, segments);}
f13309
0
gracePeriodMs
public long kafkatest_f13310_0()
{    // or we can default to (24h - size) if you want to be super accurate.    return graceMs != -1 ? graceMs : maintainMs() - size();}
f13310
0
startOn
public UnlimitedWindows kafkatest_f13318_0(final Instant start) throws IllegalArgumentException
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(start, "start");    return startOn(ApiUtils.validateMillisecondInstant(start, msgPrefix));}
f13318
0
windowsFor
public Map<Long, UnlimitedWindow> kafkatest_f13319_0(final long timestamp)
{    // always return the single unlimited window    // we cannot use Collections.singleMap since it does not support remove()    final Map<Long, UnlimitedWindow> windows = new HashMap<>();    if (timestamp >= startMs) {        windows.put(startMs, new UnlimitedWindow(startMs));    }    return windows;}
f13319
0
size
public long kafkatest_f13320_0()
{    return Long.MAX_VALUE;}
f13320
0
end
public long kafkatest_f13328_0()
{    return endMs;}
f13328
0
startTime
public Instant kafkatest_f13329_0()
{    return startTime;}
f13329
0
endTime
public Instant kafkatest_f13330_0()
{    return endTime;}
f13330
0
hashCode
public int kafkatest_f13338_0()
{    final long n = ((long) window.hashCode() << 32) | key.hashCode();    return (int) (n % 0xFFFFFFFFL);}
f13338
0
forChangelog
public TimeWindowedSerde<T> kafkatest_f13339_0(final boolean isChangelogTopic)
{    final TimeWindowedDeserializer deserializer = (TimeWindowedDeserializer) this.deserializer();    deserializer.setIsChangelogTopic(isChangelogTopic);    return this;}
f13339
0
timeWindowedSerdeFrom
public static Serde<Windowed<T>> kafkatest_f13340_0(final Class<T> type)
{    return new TimeWindowedSerde<>(Serdes.serdeFrom(type));}
f13340
0
restore
public void kafkatest_f13348_0(final byte[] key, final byte[] value)
{    throw new UnsupportedOperationException("Single restore not supported");}
f13348
0
init
public void kafkatest_f13355_0(final ProcessorContext context)
{    this.context = context;}
f13355
0
close
public void kafkatest_f13356_0()
{// do nothing}
f13356
0
keySerde
public Serde<?> kafkatest_f13364_0()
{    return keySerde;}
f13364
0
valueSerde
public Serde<?> kafkatest_f13365_0()
{    return valueSerde;}
f13365
0
stateDir
public File kafkatest_f13366_0()
{    return stateManager.baseDir();}
f13366
0
appConfigs
public Map<String, Object> kafkatest_f13374_0()
{    final Map<String, Object> combined = new HashMap<>();    combined.putAll(config.originals());    combined.putAll(config.values());    return combined;}
f13374
0
appConfigsWithPrefix
public Map<String, Object> kafkatest_f13375_0(final String prefix)
{    return config.originalsWithPrefix(prefix);}
f13375
0
setRecordContext
public void kafkatest_f13376_0(final ProcessorRecordContext recordContext)
{    this.recordContext = recordContext;}
f13376
0
applicationId
public String kafkatest_f13384_0()
{    return applicationId;}
f13384
0
partitions
public Set<TopicPartition> kafkatest_f13385_0()
{    return partitions;}
f13385
0
topology
public ProcessorTopology kafkatest_f13386_0()
{    return topology;}
f13386
0
reinitializeStateStoresForPartitions
 void kafkatest_f13394_0(final Collection<TopicPartition> partitions)
{    stateMgr.reinitializeStateStoresForPartitions(partitions, processorContext);}
f13394
0
closeStateManager
 void kafkatest_f13395_0(final boolean clean) throws ProcessorStateException
{    ProcessorStateException exception = null;    log.trace("Closing state manager");    try {        stateMgr.close(clean);    } catch (final ProcessorStateException e) {        exception = e;    } finally {        try {            stateDirectory.unlock(id);        } catch (final IOException e) {            if (exception == null) {                exception = new ProcessorStateException(String.format("%sFailed to release state dir lock", logPrefix), e);            }        }    }    if (exception != null) {        throw exception;    }}
f13395
0
isClosed
public boolean kafkatest_f13396_0()
{    return taskClosed;}
f13396
0
allAssignedTaskIds
 Set<TaskId> kafkatest_f13404_0()
{    final Set<TaskId> taskIds = super.allAssignedTaskIds();    taskIds.addAll(restoring.keySet());    return taskIds;}
f13404
0
allTasksRunning
 boolean kafkatest_f13405_0()
{    return super.allTasksRunning() && restoring.isEmpty();}
f13405
0
closeAllRestoringTasks
 RuntimeExceptionf13406_1)
{    RuntimeException exception = null;    log.trace("Closing all restoring stream tasks {}", restoring.keySet());    final Iterator<StreamTask> restoringTaskIterator = restoring.values().iterator();    while (restoringTaskIterator.hasNext()) {        final StreamTask task = restoringTaskIterator.next();                try {            task.closeStateManager(true);        } catch (final RuntimeException e) {                        if (exception == null) {                exception = e;            }        } finally {            restoringTaskIterator.remove();        }    }    restoring.clear();    restoredPartitions.clear();    restoringByPartition.clear();    return exception;}
 RuntimeExceptionf13406
1
toString
public String kafkatest_f13414_0(final String indent)
{    final StringBuilder builder = new StringBuilder();    builder.append(super.toString(indent));    describe(builder, restoring.values(), indent, "Restoring:");    return builder.toString();}
f13414
0
restoringTasks
 Collection<StreamTask> kafkatest_f13415_0()
{    return Collections.unmodifiableCollection(restoring.values());}
f13415
0
addNewTask
 void kafkatest_f13416_0(final T task)
{    created.put(task.id(), task);}
f13416
0
hasRunningTasks
 boolean kafkatest_f13424_0()
{    return !running.isEmpty();}
f13424
0
maybeResumeSuspendedTask
 booleanf13425_1final TaskId taskId, final Set<TopicPartition> partitions)
{    if (suspended.containsKey(taskId)) {        final T task = suspended.get(taskId);        log.trace("Found suspended {} {}", taskTypeName, taskId);        if (task.partitions().equals(partitions)) {            suspended.remove(taskId);            task.resume();            try {                transitionToRunning(task);            } catch (final TaskMigratedException e) {                // we need to catch migration exception internally since this function                // is triggered in the rebalance callback                                final RuntimeException fatalException = closeZombieTask(task);                running.remove(task.id());                if (fatalException != null) {                    throw fatalException;                }                throw e;            }            log.trace("Resuming suspended {} {}", taskTypeName, task.id());            return true;        } else {                    }    }    return false;}
 booleanf13425
1
transitionToRunning
 voidf13426_1final T task)
{        running.put(task.id(), task);    task.initializeTopology();    for (final TopicPartition topicPartition : task.partitions()) {        runningByPartition.put(topicPartition, task);    }    for (final TopicPartition topicPartition : task.changelogPartitions()) {        runningByPartition.put(topicPartition, task);    }}
 voidf13426
1
allAssignedTaskIds
 Set<TaskId> kafkatest_f13434_0()
{    final Set<TaskId> taskIds = new HashSet<>();    taskIds.addAll(running.keySet());    taskIds.addAll(suspended.keySet());    taskIds.addAll(created.keySet());    return taskIds;}
f13434
0
clear
 void kafkatest_f13435_0()
{    runningByPartition.clear();    running.clear();    created.clear();    suspended.clear();}
f13435
0
previousTaskIds
 Set<TaskId> kafkatest_f13436_0()
{    return previousActiveTasks;}
f13436
0
activeTasks
public List<TaskId> kafkatest_f13444_0()
{    return activeTasks;}
f13444
0
standbyTasks
public Map<TaskId, Set<TopicPartition>> kafkatest_f13445_0()
{    return standbyTasks;}
f13445
0
partitionsByHost
public Map<HostInfo, Set<TopicPartition>> kafkatest_f13446_0()
{    return partitionsByHost;}
f13446
0
writeTopicPartitions
private void kafkatest_f13454_0(final DataOutputStream out, final Set<TopicPartition> partitions) throws IOException
{    out.writeInt(partitions.size());    for (final TopicPartition partition : partitions) {        out.writeUTF(partition.topic());        out.writeInt(partition.partition());    }}
f13454
0
encodeVersionThree
private void kafkatest_f13455_0(final DataOutputStream out) throws IOException
{    out.writeInt(3);    out.writeInt(LATEST_SUPPORTED_VERSION);    encodeActiveAndStandbyTaskAssignment(out);    encodePartitionsByHost(out);}
f13455
0
encodeVersionFour
private void kafkatest_f13456_0(final DataOutputStream out) throws IOException
{    out.writeInt(4);    out.writeInt(LATEST_SUPPORTED_VERSION);    encodeActiveAndStandbyTaskAssignment(out);    encodePartitionsByHost(out);    out.writeInt(errCode);}
f13456
0
readTopicPartitions
private static Set<TopicPartition> kafkatest_f13464_0(final DataInputStream in) throws IOException
{    final int numPartitions = in.readInt();    final Set<TopicPartition> partitions = new HashSet<>(numPartitions);    for (int j = 0; j < numPartitions; j++) {        partitions.add(new TopicPartition(in.readUTF(), in.readInt()));    }    return partitions;}
f13464
0
decodePartitionsByHostUsingDictionary
private static void kafkatest_f13465_0(final AssignmentInfo assignmentInfo, final DataInputStream in) throws IOException
{    assignmentInfo.partitionsByHost = new HashMap<>();    final int dictSize = in.readInt();    final Map<Integer, String> topicIndexDict = new HashMap<>(dictSize);    for (int i = 0; i < dictSize; i++) {        topicIndexDict.put(in.readInt(), in.readUTF());    }    final int numEntries = in.readInt();    for (int i = 0; i < numEntries; i++) {        final HostInfo hostInfo = new HostInfo(in.readUTF(), in.readInt());        assignmentInfo.partitionsByHost.put(hostInfo, readTopicPartitions(in, topicIndexDict));    }}
f13465
0
readTopicPartitions
private static Set<TopicPartition> kafkatest_f13466_0(final DataInputStream in, final Map<Integer, String> topicIndexDict) throws IOException
{    final int numPartitions = in.readInt();    final Set<TopicPartition> partitions = new HashSet<>(numPartitions);    for (int j = 0; j < numPartitions; j++) {        partitions.add(new TopicPartition(topicIndexDict.get(in.readInt()), in.readInt()));    }    return partitions;}
f13466
0
getTaskManager
public TaskManager kafkatest_f13474_0()
{    return taskManager;}
f13474
0
logPrefix
public String kafkatest_f13475_0()
{    return logPrefix;}
f13475
0
configuredMetadataVersion
public intf13476_1final int priorVersion)
{    final String upgradeFrom = streamsConfig.getString(StreamsConfig.UPGRADE_FROM_CONFIG);    if (upgradeFrom != null) {        switch(upgradeFrom) {            case StreamsConfig.UPGRADE_FROM_0100:                                return VERSION_ONE;            case StreamsConfig.UPGRADE_FROM_0101:            case StreamsConfig.UPGRADE_FROM_0102:            case StreamsConfig.UPGRADE_FROM_0110:            case StreamsConfig.UPGRADE_FROM_10:            case StreamsConfig.UPGRADE_FROM_11:                                return VERSION_TWO;            default:                throw new IllegalArgumentException("Unknown configuration value for parameter 'upgrade.from': " + upgradeFrom);        }    } else {        return priorVersion;    }}
public intf13476
1
assign
public void kafkatest_f13484_0(final TaskId taskId, final boolean active)
{    if (active) {        activeTasks.add(taskId);    } else {        standbyTasks.add(taskId);    }    assignedTasks.add(taskId);}
f13484
0
activeTasks
public Set<TaskId> kafkatest_f13485_0()
{    return activeTasks;}
f13485
0
standbyTasks
public Set<TaskId> kafkatest_f13486_0()
{    return standbyTasks;}
f13486
0
toString
public String kafkatest_f13494_0()
{    return "[activeTasks: (" + activeTasks + ") standbyTasks: (" + standbyTasks + ") assignedTasks: (" + assignedTasks + ") prevActiveTasks: (" + prevActiveTasks + ") prevStandbyTasks: (" + prevStandbyTasks + ") prevAssignedTasks: (" + prevAssignedTasks + ") capacity: " + capacity + "]";}
f13494
0
reachedCapacity
 boolean kafkatest_f13495_0()
{    return assignedTasks.size() >= capacity;}
f13495
0
hasMoreAvailableCapacityThan
 boolean kafkatest_f13496_0(final ClientState other)
{    if (this.capacity <= 0) {        throw new IllegalStateException("Capacity of this ClientState must be greater than 0.");    }    if (other.capacity <= 0) {        throw new IllegalStateException("Capacity of other ClientState must be greater than 0");    }    final double otherLoad = (double) other.assignedTaskCount() / other.capacity;    final double thisLoad = (double) assignedTaskCount() / capacity;    if (thisLoad < otherLoad) {        return true;    } else if (thisLoad > otherLoad) {        return false;    } else {        return capacity > other.capacity;    }}
f13496
0
enforce
public voidf13504_1final Set<String> copartitionGroup, final Map<String, InternalTopicConfig> allRepartitionTopicsNumPartitions, final Cluster metadata)
{    if (copartitionGroup.isEmpty()) {        return;    }    final Map<Object, InternalTopicConfig> repartitionTopicConfigs = copartitionGroup.stream().filter(allRepartitionTopicsNumPartitions::containsKey).collect(Collectors.toMap(topic -> topic, allRepartitionTopicsNumPartitions::get));    final Map<String, Integer> nonRepartitionTopicPartitions = copartitionGroup.stream().filter(topic -> !allRepartitionTopicsNumPartitions.containsKey(topic)).collect(Collectors.toMap(topic -> topic, topic -> {        final Integer partitions = metadata.partitionCountForTopic(topic);        if (partitions == null) {            final String str = String.format("%sTopic not found: %s", logPrefix, topic);                        throw new IllegalStateException(str);        } else {            return partitions;        }    }));    final int numPartitionsToUseForRepartitionTopics;    if (copartitionGroup.equals(repartitionTopicConfigs.keySet())) {        // If all topics for this co-partition group is repartition topics,        // then set the number of partitions to be the maximum of the number of partitions.        numPartitionsToUseForRepartitionTopics = getMaxPartitions(repartitionTopicConfigs);    } else {        // Otherwise, use the number of partitions from external topics (which must all be the same)        numPartitionsToUseForRepartitionTopics = getSamePartitions(nonRepartitionTopicPartitions);    }    // coerce all the repartition topics to use the decided number of partitions.    for (final InternalTopicConfig config : repartitionTopicConfigs.values()) {        config.setNumberOfPartitions(numPartitionsToUseForRepartitionTopics);    }}
public voidf13504
1
getSamePartitions
private int kafkatest_f13505_0(final Map<String, Integer> nonRepartitionTopicsInCopartitionGroup)
{    final int partitions = nonRepartitionTopicsInCopartitionGroup.values().iterator().next();    for (final Map.Entry<String, Integer> entry : nonRepartitionTopicsInCopartitionGroup.entrySet()) {        if (entry.getValue() != partitions) {            final TreeMap<String, Integer> sorted = new TreeMap<>(nonRepartitionTopicsInCopartitionGroup);            throw new TopologyException(String.format("%sTopics not co-partitioned: [%s]", logPrefix, sorted));        }    }    return partitions;}
f13505
0
getMaxPartitions
private int kafkatest_f13506_0(final Map<Object, InternalTopicConfig> repartitionTopicsInCopartitionGroup)
{    int maxPartitions = 0;    for (final InternalTopicConfig config : repartitionTopicsInCopartitionGroup.values()) {        final Optional<Integer> partitions = config.numberOfPartitions();        maxPartitions = Integer.max(maxPartitions, partitions.orElse(maxPartitions));    }    if (maxPartitions <= 0) {        throw new IllegalStateException(logPrefix + "Could not validate the copartitioning of topics: " + repartitionTopicsInCopartitionGroup.keySet());    }    return maxPartitions;}
f13506
0
shouldBalanceLoad
private boolean kafkatest_f13514_0(final ClientState client)
{    return client.reachedCapacity() && hasClientsWithMoreAvailableCapacity(client);}
f13514
0
hasClientsWithMoreAvailableCapacity
private boolean kafkatest_f13515_0(final ClientState client)
{    for (final ClientState clientState : clients.values()) {        if (clientState.hasMoreAvailableCapacityThan(client)) {            return true;        }    }    return false;}
f13515
0
findClientsWithPreviousAssignedTask
private ClientState kafkatest_f13516_0(final TaskId taskId, final Set<ID> clientsWithin)
{    final ID previous = previousActiveTaskAssignment.get(taskId);    if (previous != null && clientsWithin.contains(previous)) {        return clients.get(previous);    }    return findLeastLoadedClientWithPreviousStandByTask(taskId, clientsWithin);}
f13516
0
pair
 Pair kafkatest_f13524_0(final TaskId task1, final TaskId task2)
{    if (task1.compareTo(task2) < 0) {        return new Pair(task1, task2);    }    return new Pair(task2, task1);}
f13524
0
equals
public boolean kafkatest_f13525_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final Pair pair = (Pair) o;    return Objects.equals(task1, pair.task1) && Objects.equals(task2, pair.task2);}
f13525
0
hashCode
public int kafkatest_f13526_0()
{    return Objects.hash(task1, task2);}
f13526
0
encodeVersionOne
private ByteBuffer kafkatest_f13534_0()
{    final ByteBuffer buf = ByteBuffer.allocate(getVersionOneByteLength());    // version    buf.putInt(1);    encodeClientUUID(buf);    encodeTasks(buf, prevTasks);    encodeTasks(buf, standbyTasks);    return buf;}
f13534
0
getVersionOneByteLength
private int kafkatest_f13535_0()
{    return // version    4 + // client ID    16 + 4 + // length + prev tasks    prevTasks.size() * 8 + 4 + // length + standby tasks    standbyTasks.size() * 8;}
f13535
0
encodeClientUUID
protected void kafkatest_f13536_0(final ByteBuffer buf)
{    buf.putLong(processId.getMostSignificantBits());    buf.putLong(processId.getLeastSignificantBits());}
f13536
0
encodeVersionFour
private ByteBuffer kafkatest_f13544_0()
{    return encodeVersionThreeFourAndFive(4);}
f13544
0
encodeVersionFive
private ByteBuffer kafkatest_f13545_0()
{    return encodeVersionThreeFourAndFive(5);}
f13545
0
getVersionThreeFourAndFiveByteLength
protected int kafkatest_f13546_0(final byte[] endPointBytes)
{    return // used version    4 + // latest supported version version    4 + // client ID    16 + 4 + // length + prev tasks    prevTasks.size() * 8 + 4 + // length + standby tasks    standbyTasks.size() * 8 + 4 + // length + userEndPoint    endPointBytes.length;}
f13546
0
hashCode
public int kafkatest_f13554_0()
{    final int hashCode = usedVersion ^ latestSupportedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();    if (userEndPoint == null) {        return hashCode;    }    return hashCode ^ userEndPoint.hashCode();}
f13554
0
equals
public boolean kafkatest_f13555_0(final Object o)
{    if (o instanceof SubscriptionInfo) {        final SubscriptionInfo other = (SubscriptionInfo) o;        return this.usedVersion == other.usedVersion && this.latestSupportedVersion == other.latestSupportedVersion && this.processId.equals(other.processId) && this.prevTasks.equals(other.prevTasks) && this.standbyTasks.equals(other.standbyTasks) && this.userEndPoint != null ? this.userEndPoint.equals(other.userEndPoint) : other.userEndPoint == null;    } else {        return false;    }}
f13555
0
toString
public String kafkatest_f13556_0()
{    return "[version=" + usedVersion + ", supported version=" + latestSupportedVersion + ", process ID=" + processId + ", prev tasks=" + prevTasks + ", standby tasks=" + standbyTasks + ", user endpoint=" + userEndPoint + "]";}
f13556
0
getAdminClient
public AdminClient kafkatest_f13565_0(final Map<String, Object> config)
{    return (AdminClient) getAdmin(config);}
f13565
0
getAdmin
public Admin kafkatest_f13566_0(final Map<String, Object> config)
{    // create a new client upon each call; but expect this call to be only triggered once so this should be fine    return Admin.create(config);}
f13566
0
getProducer
public Producer<byte[], byte[]> kafkatest_f13567_0(final Map<String, Object> config)
{    return new KafkaProducer<>(config, new ByteArraySerializer(), new ByteArraySerializer());}
f13567
0
valueSerde
public Serde<?> kafkatest_f13575_0()
{    return delegate.valueSerde();}
f13575
0
stateDir
public File kafkatest_f13576_0()
{    return delegate.stateDir();}
f13576
0
metrics
public StreamsMetrics kafkatest_f13577_0()
{    return delegate.metrics();}
f13577
0
forward
public void kafkatest_f13585_0(final K key, final V value, final String childName)
{    throw new StreamsException("ProcessorContext#forward() not supported.");}
f13585
0
commit
public void kafkatest_f13586_0()
{    delegate.commit();}
f13586
0
topic
public String kafkatest_f13587_0()
{    return delegate.topic();}
f13587
0
forward
public void kafkatest_f13595_0(final K key, final V value)
{    final ProcessorNode previousNode = currentNode();    try {        for (final ProcessorNode child : (List<ProcessorNode<K, V>>) currentNode().children()) {            setCurrentNode(child);            child.process(key, value);        }    } finally {        setCurrentNode(previousNode);    }}
f13595
0
forward
public void kafkatest_f13596_0(final K key, final V value, final To to)
{    if (!currentNode().children().isEmpty()) {        throw new IllegalStateException("This method should only be called on 'GlobalStateStore.flush' that should not have any children.");    }}
f13596
0
forward
public void kafkatest_f13597_0(final K key, final V value, final int childIndex)
{    throw new UnsupportedOperationException("this should not happen: forward() not supported in global processor context.");}
f13597
0
getGlobalStore
public StateStore kafkatest_f13605_0(final String name)
{    return globalStores.getOrDefault(name, Optional.empty()).orElse(null);}
f13605
0
getStore
public StateStore kafkatest_f13606_0(final String name)
{    return getGlobalStore(name);}
f13606
0
baseDir
public File kafkatest_f13607_0()
{    return baseDir;}
f13607
0
initialize
public Map<TopicPartition, Long> kafkatest_f13615_0()
{    final Set<String> storeNames = stateMgr.initialize();    final Map<String, String> storeNameToTopic = topology.storeToChangelogTopic();    for (final String storeName : storeNames) {        final String sourceTopic = storeNameToTopic.get(storeName);        final SourceNode source = topology.source(sourceTopic);        deserializers.put(sourceTopic, new RecordDeserializer(source, deserializationExceptionHandler, logContext, ThreadMetrics.skipRecordSensor(processorContext.metrics())));    }    initTopology();    processorContext.initialize();    return stateMgr.checkpointed();}
f13615
0
update
public void kafkatest_f13616_0(final ConsumerRecord<byte[], byte[]> record)
{    final RecordDeserializer sourceNodeAndDeserializer = deserializers.get(record.topic());    final ConsumerRecord<Object, Object> deserialized = sourceNodeAndDeserializer.deserialize(processorContext, record);    if (deserialized != null) {        final ProcessorRecordContext recordContext = new ProcessorRecordContext(deserialized.timestamp(), deserialized.offset(), deserialized.partition(), deserialized.topic(), deserialized.headers());        processorContext.setRecordContext(recordContext);        processorContext.setCurrentNode(sourceNodeAndDeserializer.sourceNode());        sourceNodeAndDeserializer.sourceNode().process(deserialized.key(), deserialized.value());    }    offsets.put(new TopicPartition(record.topic(), record.partition()), record.offset() + 1);}
f13616
0
flushState
public void kafkatest_f13617_0()
{    stateMgr.flush();    stateMgr.checkpoint(offsets);}
f13617
0
stillRunning
public boolean kafkatest_f13625_0()
{    synchronized (stateLock) {        return state.isRunning();    }}
f13625
0
initialize
 void kafkatest_f13626_0()
{    final Map<TopicPartition, Long> partitionOffsets = stateMaintainer.initialize();    globalConsumer.assign(partitionOffsets.keySet());    for (final Map.Entry<TopicPartition, Long> entry : partitionOffsets.entrySet()) {        globalConsumer.seek(entry.getKey(), entry.getValue());    }    lastFlush = time.milliseconds();}
f13626
0
pollAndUpdate
 voidf13627_1)
{    try {        final ConsumerRecords<byte[], byte[]> received = globalConsumer.poll(pollTime);        for (final ConsumerRecord<byte[], byte[]> record : received) {            stateMaintainer.update(record);        }        final long now = time.milliseconds();        if (now >= lastFlush + flushInterval) {            stateMaintainer.flushState();            lastFlush = now;        }    } catch (final InvalidOffsetException recoverableException) {                throw new StreamsException("Updating global state failed. " + "You can restart KafkaStreams to recover from this error.", recoverableException);    }}
 voidf13627
1
numberOfPartitions
public Optional<Integer> kafkatest_f13635_0()
{    return numberOfPartitions;}
f13635
0
setNumberOfPartitions
public void kafkatest_f13636_0(final int numberOfPartitions)
{    if (numberOfPartitions < 1) {        throw new IllegalArgumentException("Number of partitions must be at least 1.");    }    this.numberOfPartitions = Optional.of(numberOfPartitions);}
f13636
0
toString
public String kafkatest_f13637_0()
{    return "InternalTopicConfig(" + "name=" + name + ", topicConfigs=" + topicConfigs + ")";}
f13637
0
name
private String kafkatest_f13645_0()
{    return builder.name();}
f13645
0
isWindowStore
private boolean kafkatest_f13646_0()
{    return builder instanceof WindowStoreBuilder || builder instanceof TimestampedWindowStoreBuilder || builder instanceof SessionStoreBuilder;}
f13646
0
logConfig
private Map<String, String> kafkatest_f13647_0()
{    return builder.logConfig();}
f13647
0
build
public ProcessorNode kafkatest_f13655_0()
{    if (topicExtractor instanceof StaticTopicNameExtractor) {        final String topic = ((StaticTopicNameExtractor) topicExtractor).topicName;        if (internalTopicNames.contains(topic)) {            // prefix the internal topic name with the application id            return new SinkNode<>(name, new StaticTopicNameExtractor<>(decorateTopic(topic)), keySerializer, valSerializer, partitioner);        } else {            return new SinkNode<>(name, topicExtractor, keySerializer, valSerializer, partitioner);        }    } else {        return new SinkNode<>(name, topicExtractor, keySerializer, valSerializer, partitioner);    }}
f13655
0
describe
 Sink kafkatest_f13656_0()
{    return new Sink(name, topicExtractor);}
f13656
0
setApplicationId
public final synchronized InternalTopologyBuilder kafkatest_f13657_0(final String applicationId)
{    Objects.requireNonNull(applicationId, "applicationId can't be null");    this.applicationId = applicationId;    return this;}
f13657
0
addStateStore
public final void kafkatest_f13665_0(final StoreBuilder<?> storeBuilder, final boolean allowOverride, final String... processorNames)
{    Objects.requireNonNull(storeBuilder, "storeBuilder can't be null");    if (!allowOverride && stateFactories.containsKey(storeBuilder.name())) {        throw new TopologyException("StateStore " + storeBuilder.name() + " is already added.");    }    stateFactories.put(storeBuilder.name(), new StateStoreFactory(storeBuilder));    if (processorNames != null) {        for (final String processorName : processorNames) {            Objects.requireNonNull(processorName, "processor name must not be null");            connectProcessorAndStateStore(processorName, storeBuilder.name());        }    }    nodeGroups = null;}
f13665
0
addGlobalStore
public final void kafkatest_f13666_0(final StoreBuilder storeBuilder, final String sourceName, final TimestampExtractor timestampExtractor, final Deserializer keyDeserializer, final Deserializer valueDeserializer, final String topic, final String processorName, final ProcessorSupplier stateUpdateSupplier)
{    Objects.requireNonNull(storeBuilder, "store builder must not be null");    validateGlobalStoreArguments(sourceName, topic, processorName, stateUpdateSupplier, storeBuilder.name(), storeBuilder.loggingEnabled());    validateTopicNotAlreadyRegistered(topic);    final String[] topics = { topic };    final String[] predecessors = { sourceName };    final ProcessorNodeFactory nodeFactory = new ProcessorNodeFactory(processorName, predecessors, stateUpdateSupplier);    globalTopics.add(topic);    nodeFactories.put(sourceName, new SourceNodeFactory(sourceName, topics, null, timestampExtractor, keyDeserializer, valueDeserializer));    nodeToSourceTopics.put(sourceName, Arrays.asList(topics));    nodeGrouper.add(sourceName);    nodeFactory.addStateStore(storeBuilder.name());    nodeFactories.put(processorName, nodeFactory);    nodeGrouper.add(processorName);    nodeGrouper.unite(processorName, predecessors);    globalStateBuilders.put(storeBuilder.name(), storeBuilder);    connectSourceStoreAndTopic(storeBuilder.name(), topic);    nodeGroups = null;}
f13666
0
validateTopicNotAlreadyRegistered
private void kafkatest_f13667_0(final String topic)
{    if (sourceTopicNames.contains(topic) || globalTopics.contains(topic)) {        throw new TopologyException("Topic " + topic + " has already been registered by another source.");    }    for (final Pattern pattern : nodeToSourcePatterns.values()) {        if (pattern.matcher(topic).matches()) {            throw new TopologyException("Topic " + topic + " matches a Pattern already registered by another source.");        }    }}
f13667
0
connectStateStoreNameToSourceTopicsOrPattern
private void kafkatest_f13675_0(final String stateStoreName, final ProcessorNodeFactory processorNodeFactory)
{    if (stateStoreNameToSourceTopics.containsKey(stateStoreName) || stateStoreNameToSourceRegex.containsKey(stateStoreName)) {        return;    }    final Set<String> sourceTopics = new HashSet<>();    final Set<Pattern> sourcePatterns = new HashSet<>();    final Set<SourceNodeFactory> sourceNodesForPredecessor = findSourcesForProcessorPredecessors(processorNodeFactory.predecessors);    for (final SourceNodeFactory sourceNodeFactory : sourceNodesForPredecessor) {        if (sourceNodeFactory.pattern != null) {            sourcePatterns.add(sourceNodeFactory.pattern);        } else {            sourceTopics.addAll(sourceNodeFactory.topics);        }    }    if (!sourceTopics.isEmpty()) {        stateStoreNameToSourceTopics.put(stateStoreName, Collections.unmodifiableSet(sourceTopics));    }    if (!sourcePatterns.isEmpty()) {        stateStoreNameToSourceRegex.put(stateStoreName, Collections.unmodifiableSet(sourcePatterns));    }}
f13675
0
maybeAddToResetList
private void kafkatest_f13676_0(final Collection<T> earliestResets, final Collection<T> latestResets, final Topology.AutoOffsetReset offsetReset, final T item)
{    if (offsetReset != null) {        switch(offsetReset) {            case EARLIEST:                earliestResets.add(item);                break;            case LATEST:                latestResets.add(item);                break;            default:                throw new TopologyException(String.format("Unrecognized reset format %s", offsetReset));        }    }}
f13676
0
nodeGroups
public synchronized Map<Integer, Set<String>> kafkatest_f13677_0()
{    if (nodeGroups == null) {        nodeGroups = makeNodeGroups();    }    return nodeGroups;}
f13677
0
buildSinkNode
private void kafkatest_f13685_0(final Map<String, ProcessorNode> processorMap, final Map<String, SinkNode> topicSinkMap, final Set<String> repartitionTopics, final SinkNodeFactory sinkNodeFactory, final SinkNode node)
{    for (final String predecessor : sinkNodeFactory.predecessors) {        processorMap.get(predecessor).addChild(node);        if (sinkNodeFactory.topicExtractor instanceof StaticTopicNameExtractor) {            final String topic = ((StaticTopicNameExtractor) sinkNodeFactory.topicExtractor).topicName;            if (internalTopicNames.contains(topic)) {                // prefix the internal topic name with the application id                final String decoratedTopic = decorateTopic(topic);                topicSinkMap.put(decoratedTopic, node);                repartitionTopics.add(decoratedTopic);            } else {                topicSinkMap.put(topic, node);            }        }    }}
f13685
0
buildSourceNode
private void kafkatest_f13686_0(final Map<String, SourceNode> topicSourceMap, final Set<String> repartitionTopics, final SourceNodeFactory sourceNodeFactory, final SourceNode node)
{    final List<String> topics = (sourceNodeFactory.pattern != null) ? sourceNodeFactory.getTopics(subscriptionUpdates.getUpdates()) : sourceNodeFactory.topics;    for (final String topic : topics) {        if (internalTopicNames.contains(topic)) {            // prefix the internal topic name with the application id            final String decoratedTopic = decorateTopic(topic);            topicSourceMap.put(decoratedTopic, node);            repartitionTopics.add(decoratedTopic);        } else {            topicSourceMap.put(topic, node);        }    }}
f13686
0
buildProcessorNode
private void kafkatest_f13687_0(final Map<String, ProcessorNode> processorMap, final Map<String, StateStore> stateStoreMap, final ProcessorNodeFactory factory, final ProcessorNode node)
{    for (final String predecessor : factory.predecessors) {        final ProcessorNode<?, ?> predecessorNode = processorMap.get(predecessor);        predecessorNode.addChild(node);    }    for (final String stateStoreName : factory.stateStoreNames) {        if (!stateStoreMap.containsKey(stateStoreName)) {            if (stateFactories.containsKey(stateStoreName)) {                final StateStoreFactory stateStoreFactory = stateFactories.get(stateStoreName);                // remember the changelog topic if this state store is change-logging enabled                if (stateStoreFactory.loggingEnabled() && !storeToChangelogTopic.containsKey(stateStoreName)) {                    final String changelogTopic = ProcessorStateManager.storeChangelogTopic(applicationId, stateStoreName);                    storeToChangelogTopic.put(stateStoreName, changelogTopic);                }                stateStoreMap.put(stateStoreName, stateStoreFactory.build());            } else {                stateStoreMap.put(stateStoreName, globalStateStores.get(stateStoreName));            }        }    }}
f13687
0
latestResetTopicsPattern
public synchronized Pattern kafkatest_f13695_0()
{    return resetTopicsPattern(latestResetTopics, latestResetPatterns);}
f13695
0
resetTopicsPattern
private Pattern kafkatest_f13696_0(final Set<String> resetTopics, final Set<Pattern> resetPatterns)
{    final List<String> topics = maybeDecorateInternalSourceTopics(resetTopics);    return buildPatternForOffsetResetTopics(topics, resetPatterns);}
f13696
0
buildPatternForOffsetResetTopics
private static Pattern kafkatest_f13697_0(final Collection<String> sourceTopics, final Collection<Pattern> sourcePatterns)
{    final StringBuilder builder = new StringBuilder();    for (final String topic : sourceTopics) {        builder.append(topic).append("|");    }    for (final Pattern sourcePattern : sourcePatterns) {        builder.append(sourcePattern.pattern()).append("|");    }    if (builder.length() > 0) {        builder.setLength(builder.length() - 1);        return Pattern.compile(builder.toString());    }    return EMPTY_ZERO_LENGTH_PATTERN;}
f13697
0
isGlobalSource
private boolean kafkatest_f13705_0(final String nodeName)
{    final NodeFactory nodeFactory = nodeFactories.get(nodeName);    if (nodeFactory instanceof SourceNodeFactory) {        final List<String> topics = ((SourceNodeFactory) nodeFactory).topics;        return topics != null && topics.size() == 1 && globalTopics.contains(topics.get(0));    }    return false;}
f13705
0
describe
public TopologyDescription kafkatest_f13706_0()
{    final TopologyDescription description = new TopologyDescription();    for (final Map.Entry<Integer, Set<String>> nodeGroup : makeNodeGroups().entrySet()) {        final Set<String> allNodesOfGroups = nodeGroup.getValue();        final boolean isNodeGroupOfGlobalStores = nodeGroupContainsGlobalSourceNode(allNodesOfGroups);        if (!isNodeGroupOfGlobalStores) {            describeSubtopology(description, nodeGroup.getKey(), allNodesOfGroups);        } else {            describeGlobalStore(description, allNodesOfGroups, nodeGroup.getKey());        }    }    return description;}
f13706
0
describeGlobalStore
private void kafkatest_f13707_0(final TopologyDescription description, final Set<String> nodes, final int id)
{    final Iterator<String> it = nodes.iterator();    while (it.hasNext()) {        final String node = it.next();        if (isGlobalSource(node)) {            // we found a GlobalStore node group; those contain exactly two node: {sourceNode,processorNode}            // remove sourceNode from group            it.remove();            // get remaining processorNode            final String processorNode = nodes.iterator().next();            description.addGlobalStore(new GlobalStore(node, processorNode, ((ProcessorNodeFactory) nodeFactories.get(processorNode)).stateStoreNames.iterator().next(), nodeToSourceTopics.get(node).get(0), id));            break;        }    }}
f13707
0
toString
public String kafkatest_f13715_0()
{    return "Sub-topology: " + id + " for global store (will not generate tasks)\n" + "    " + source.toString() + "\n" + "    " + processor.toString() + "\n";}
f13715
0
equals
public boolean kafkatest_f13716_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final GlobalStore that = (GlobalStore) o;    return source.equals(that.source) && processor.equals(that.processor);}
f13716
0
hashCode
public int kafkatest_f13717_0()
{    return Objects.hash(source, processor);}
f13717
0
topicPattern
public Pattern kafkatest_f13725_0()
{    return topicPattern;}
f13725
0
addPredecessor
public void kafkatest_f13726_0(final TopologyDescription.Node predecessor)
{    throw new UnsupportedOperationException("Sources don't have predecessors.");}
f13726
0
toString
public String kafkatest_f13727_0()
{    final String topicsString = topics == null ? topicPattern.toString() : topics.toString();    return "Source: " + name + " (topics: " + topicsString + ")\n      --> " + nodeNames(successors);}
f13727
0
topicNameExtractor
public TopicNameExtractor kafkatest_f13735_0()
{    if (topicNameExtractor instanceof StaticTopicNameExtractor) {        return null;    } else {        return topicNameExtractor;    }}
f13735
0
addSuccessor
public void kafkatest_f13736_0(final TopologyDescription.Node successor)
{    throw new UnsupportedOperationException("Sinks don't have successors.");}
f13736
0
toString
public String kafkatest_f13737_0()
{    if (topicNameExtractor instanceof StaticTopicNameExtractor) {        return "Sink: " + name + " (topic: " + topic() + ")\n      <-- " + nodeNames(predecessors);    }    return "Sink: " + name + " (extractor class: " + topicNameExtractor + ")\n      <-- " + nodeNames(predecessors);}
f13737
0
equals
public boolean kafkatest_f13745_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final Subtopology that = (Subtopology) o;    return id == that.id && nodes.equals(that.nodes);}
f13745
0
hashCode
public int kafkatest_f13746_0()
{    return Objects.hash(id, nodes);}
f13746
0
equals
public boolean kafkatest_f13747_0(final Object o)
{    if (o instanceof TopicsInfo) {        final TopicsInfo other = (TopicsInfo) o;        return other.sourceTopics.equals(sourceTopics) && other.stateChangelogTopics.equals(stateChangelogTopics);    } else {        return false;    }}
f13747
0
globalStores
public Set<TopologyDescription.GlobalStore> kafkatest_f13755_0()
{    return Collections.unmodifiableSet(globalStores);}
f13755
0
toString
public String kafkatest_f13756_0()
{    final StringBuilder sb = new StringBuilder();    sb.append("Topologies:\n ");    final TopologyDescription.Subtopology[] sortedSubtopologies = subtopologies.descendingSet().toArray(new Subtopology[0]);    final TopologyDescription.GlobalStore[] sortedGlobalStores = globalStores.descendingSet().toArray(new GlobalStore[0]);    int expectedId = 0;    int subtopologiesIndex = sortedSubtopologies.length - 1;    int globalStoresIndex = sortedGlobalStores.length - 1;    while (subtopologiesIndex != -1 && globalStoresIndex != -1) {        sb.append("  ");        final TopologyDescription.Subtopology subtopology = sortedSubtopologies[subtopologiesIndex];        final TopologyDescription.GlobalStore globalStore = sortedGlobalStores[globalStoresIndex];        if (subtopology.id() == expectedId) {            sb.append(subtopology);            subtopologiesIndex--;        } else {            sb.append(globalStore);            globalStoresIndex--;        }        expectedId++;    }    while (subtopologiesIndex != -1) {        final TopologyDescription.Subtopology subtopology = sortedSubtopologies[subtopologiesIndex];        sb.append("  ");        sb.append(subtopology);        subtopologiesIndex--;    }    while (globalStoresIndex != -1) {        final TopologyDescription.GlobalStore globalStore = sortedGlobalStores[globalStoresIndex];        sb.append("  ");        sb.append(globalStore);        globalStoresIndex--;    }    return sb.toString();}
f13756
0
equals
public boolean kafkatest_f13757_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final TopologyDescription that = (TopologyDescription) o;    return subtopologies.equals(that.subtopologies) && globalStores.equals(that.globalStores);}
f13757
0
getSourceTopicNames
public synchronized Set<String> kafkatest_f13765_0()
{    return sourceTopicNames;}
f13765
0
getStateStores
public synchronized Map<String, StateStoreFactory> kafkatest_f13766_0()
{    return stateFactories;}
f13766
0
parseBuiltInMetricsVersion
private static Version kafkatest_f13767_0(final String builtInMetricsVersion)
{    if (builtInMetricsVersion.equals(StreamsConfig.METRICS_LATEST)) {        return Version.LATEST;    } else {        return Version.FROM_100_TO_23;    }}
f13767
0
storeLevelTagMap
public Map<String, String> kafkatest_f13775_0(final String taskName, final String storeType, final String storeName)
{    final Map<String, String> tagMap = taskLevelTagMap(taskName);    tagMap.put(storeType + "-" + STORE_ID_TAG, storeName);    return tagMap;}
f13775
0
taskLevelSensor
public final Sensor kafkatest_f13776_0(final String taskName, final String sensorName, final RecordingLevel recordingLevel, final Sensor... parents)
{    final String key = taskSensorPrefix(taskName);    synchronized (taskLevelSensors) {        if (!taskLevelSensors.containsKey(key)) {            taskLevelSensors.put(key, new LinkedList<>());        }        final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;        final Sensor sensor = metrics.sensor(fullSensorName, recordingLevel, parents);        taskLevelSensors.get(key).push(fullSensorName);        return sensor;    }}
f13776
0
removeAllTaskLevelSensors
public final void kafkatest_f13777_0(final String taskName)
{    final String key = taskSensorPrefix(taskName);    synchronized (taskLevelSensors) {        final Deque<String> sensors = taskLevelSensors.remove(key);        while (sensors != null && !sensors.isEmpty()) {            metrics.removeSensor(sensors.pop());        }    }}
f13777
0
storeLevelSensor
public final Sensor kafkatest_f13785_0(final String taskName, final String storeName, final String sensorName, final Sensor.RecordingLevel recordingLevel, final Sensor... parents)
{    final String key = storeSensorPrefix(taskName, storeName);    synchronized (storeLevelSensors) {        if (!storeLevelSensors.containsKey(key)) {            storeLevelSensors.put(key, new LinkedList<>());        }        final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;        final Sensor sensor = metrics.sensor(fullSensorName, recordingLevel, parents);        storeLevelSensors.get(key).push(fullSensorName);        return sensor;    }}
f13785
0
removeAllStoreLevelSensors
public final void kafkatest_f13786_0(final String taskName, final String storeName)
{    final String key = storeSensorPrefix(taskName, storeName);    synchronized (storeLevelSensors) {        final Deque<String> sensors = storeLevelSensors.remove(key);        while (sensors != null && !sensors.isEmpty()) {            metrics.removeSensor(sensors.pop());        }    }}
f13786
0
storeSensorPrefix
private String kafkatest_f13787_0(final String taskName, final String storeName)
{    return taskSensorPrefix(taskName) + SENSOR_PREFIX_DELIMITER + "store" + SENSOR_PREFIX_DELIMITER + storeName;}
f13787
0
addLatencyAndThroughputSensor
public Sensor kafkatest_f13795_0(final String scopeName, final String entityName, final String operationName, final Sensor.RecordingLevel recordingLevel, final String... tags)
{    final String group = groupNameFromScope(scopeName);    final Map<String, String> tagMap = constructTags(scopeName, entityName, tags);    final Map<String, String> allTagMap = constructTags(scopeName, "all", tags);    // first add the global operation metrics if not yet, with the global tags only    final Sensor parent = metrics.sensor(externalParentSensorName(operationName), recordingLevel);    addAvgAndMaxLatencyToSensor(parent, group, allTagMap, operationName);    addInvocationRateAndCountToSensor(parent, group, allTagMap, operationName);    // add the operation metrics with additional tags    final Sensor sensor = metrics.sensor(externalChildSensorName(operationName, entityName), recordingLevel, parent);    addAvgAndMaxLatencyToSensor(sensor, group, tagMap, operationName);    addInvocationRateAndCountToSensor(sensor, group, tagMap, operationName);    parentSensors.put(sensor, parent);    return sensor;}
f13795
0
addThroughputSensor
public Sensor kafkatest_f13796_0(final String scopeName, final String entityName, final String operationName, final Sensor.RecordingLevel recordingLevel, final String... tags)
{    final String group = groupNameFromScope(scopeName);    final Map<String, String> tagMap = constructTags(scopeName, entityName, tags);    final Map<String, String> allTagMap = constructTags(scopeName, "all", tags);    // first add the global operation metrics if not yet, with the global tags only    final Sensor parent = metrics.sensor(externalParentSensorName(operationName), recordingLevel);    addInvocationRateAndCountToSensor(parent, group, allTagMap, operationName);    // add the operation metrics with additional tags    final Sensor sensor = metrics.sensor(externalChildSensorName(operationName, entityName), recordingLevel, parent);    addInvocationRateAndCountToSensor(sensor, group, tagMap, operationName);    parentSensors.put(sensor, parent);    return sensor;}
f13796
0
externalChildSensorName
private String kafkatest_f13797_0(final String operationName, final String entityName)
{    return "external" + SENSOR_PREFIX_DELIMITER + threadName + SENSOR_PREFIX_DELIMITER + "entity" + SENSOR_PREFIX_DELIMITER + entityName + SENSOR_NAME_DELIMITER + operationName;}
f13797
0
addSumMetricToSensor
public static void kafkatest_f13805_0(final Sensor sensor, final String group, final Map<String, String> tags, final String operation, final String description)
{    sensor.add(new MetricName(operation + TOTAL_SUFFIX, group, description, tags), new CumulativeSum());}
f13805
0
addValueMetricToSensor
public static void kafkatest_f13806_0(final Sensor sensor, final String group, final Map<String, String> tags, final String name, final String description)
{    sensor.add(new MetricName(name, group, description, tags), new Value());}
f13806
0
addAvgAndSumMetricsToSensor
public static void kafkatest_f13807_0(final Sensor sensor, final String group, final Map<String, String> tags, final String metricNamePrefix, final String descriptionOfAvg, final String descriptionOfTotal)
{    sensor.add(new MetricName(metricNamePrefix + AVG_SUFFIX, group, descriptionOfAvg, tags), new Avg());    sensor.add(new MetricName(metricNamePrefix + TOTAL_SUFFIX, group, descriptionOfTotal, tags), new CumulativeSum());}
f13807
0
processSensor
public static Sensor kafkatest_f13815_0(final StreamsMetricsImpl streamsMetrics)
{    final Sensor processSensor = streamsMetrics.threadLevelSensor(PROCESS, Sensor.RecordingLevel.INFO);    final Map<String, String> tagMap = streamsMetrics.threadLevelTagMap();    addAvgAndMaxToSensor(processSensor, THREAD_LEVEL_GROUP, tagMap, PROCESS_LATENCY);    addInvocationRateAndCountToSensor(processSensor, THREAD_LEVEL_GROUP, tagMap, PROCESS, PROCESS_TOTAL_DESCRIPTION, PROCESS_RATE_DESCRIPTION);    return processSensor;}
f13815
0
punctuateSensor
public static Sensor kafkatest_f13816_0(final StreamsMetricsImpl streamsMetrics)
{    final Sensor punctuateSensor = streamsMetrics.threadLevelSensor(PUNCTUATE, Sensor.RecordingLevel.INFO);    final Map<String, String> tagMap = streamsMetrics.threadLevelTagMap();    addAvgAndMaxToSensor(punctuateSensor, THREAD_LEVEL_GROUP, tagMap, PUNCTUATE_LATENCY);    addInvocationRateAndCountToSensor(punctuateSensor, THREAD_LEVEL_GROUP, tagMap, PUNCTUATE, PUNCTUATE_TOTAL_DESCRIPTION, PUNCTUATE_RATE_DESCRIPTION);    return punctuateSensor;}
f13816
0
skipRecordSensor
public static Sensor kafkatest_f13817_0(final StreamsMetricsImpl streamsMetrics)
{    final Sensor skippedRecordsSensor = streamsMetrics.threadLevelSensor(SKIP_RECORD, Sensor.RecordingLevel.INFO);    addInvocationRateAndCountToSensor(skippedRecordsSensor, THREAD_LEVEL_GROUP, streamsMetrics.threadLevelTagMap(), SKIP_RECORD, SKIP_RECORD_TOTAL_DESCRIPTION, SKIP_RECORD_RATE_DESCRIPTION);    return skippedRecordsSensor;}
f13817
0
streamTime
public long kafkatest_f13825_0()
{    return streamTime;}
f13825
0
numBuffered
 int kafkatest_f13826_0(final TopicPartition partition)
{    final RecordQueue recordQueue = partitionQueues.get(partition);    if (recordQueue == null) {        throw new IllegalStateException(String.format("Record's partition %s does not belong to this partition-group.", partition));    }    return recordQueue.size();}
f13826
0
numBuffered
 int kafkatest_f13827_0()
{    return totalBuffered;}
f13827
0
forward
public void kafkatest_f13835_0(final K key, final V value, final int childIndex)
{    forward(key, value, To.child(((List<ProcessorNode>) currentNode().children()).get(childIndex).name()));}
f13835
0
forward
public void kafkatest_f13836_0(final K key, final V value, final String childName)
{    forward(key, value, To.child(childName));}
f13836
0
forward
public void kafkatest_f13837_0(final K key, final V value, final To to)
{    final ProcessorNode previousNode = currentNode();    final ProcessorRecordContext previousContext = recordContext;    try {        toInternal.update(to);        if (toInternal.hasTimestamp()) {            recordContext = new ProcessorRecordContext(toInternal.timestamp(), recordContext.offset(), recordContext.partition(), recordContext.topic(), recordContext.headers());        }        final String sendTo = toInternal.child();        if (sendTo == null) {            final List<ProcessorNode<K, V>> children = (List<ProcessorNode<K, V>>) currentNode().children();            for (final ProcessorNode child : children) {                forward(child, key, value);            }        } else {            final ProcessorNode child = currentNode().getChild(sendTo);            if (child == null) {                throw new StreamsException("Unknown downstream node: " + sendTo + " either does not exist or is not connected to this processor.");            }            forward(child, key, value);        }    } finally {        recordContext = previousContext;        setCurrentNode(previousNode);    }}
f13837
0
get
public V kafkatest_f13845_0(final K key)
{    return wrapped().get(key);}
f13845
0
range
public KeyValueIterator<K, V> kafkatest_f13846_0(final K from, final K to)
{    return wrapped().range(from, to);}
f13846
0
all
public KeyValueIterator<K, V> kafkatest_f13847_0()
{    return wrapped().all();}
f13847
0
fetch
public V kafkatest_f13855_0(final K key, final long time)
{    return wrapped().fetch(key, time);}
f13855
0
fetch
public WindowStoreIterator<V> kafkatest_f13856_0(final K key, final long timeFrom, final long timeTo)
{    return wrapped().fetch(key, timeFrom, timeTo);}
f13856
0
fetch
public KeyValueIterator<Windowed<K>, V> kafkatest_f13857_0(final K from, final K to, final long timeFrom, final long timeTo)
{    return wrapped().fetch(from, to, timeFrom, timeTo);}
f13857
0
fetch
public KeyValueIterator<Windowed<K>, AGG> kafkatest_f13865_0(final K key)
{    return wrapped().fetch(key);}
f13865
0
fetch
public KeyValueIterator<Windowed<K>, AGG> kafkatest_f13866_0(final K from, final K to)
{    return wrapped().fetch(from, to);}
f13866
0
init
public void kafkatest_f13867_0(final ProcessorContext context, final StateStore root)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
f13867
0
putAll
public void kafkatest_f13875_0(final List<KeyValue<K, V>> entries)
{    wrapped().putAll(entries);}
f13875
0
delete
public V kafkatest_f13876_0(final K key)
{    return wrapped().delete(key);}
f13876
0
put
public void kafkatest_f13877_0(final K key, final V value)
{    wrapped().put(key, value);}
f13877
0
findSessions
public KeyValueIterator<Windowed<K>, AGG> kafkatest_f13885_0(final K keyFrom, final K keyTo, final long earliestSessionEndTime, final long latestSessionStartTime)
{    return wrapped().findSessions(keyFrom, keyTo, earliestSessionEndTime, latestSessionStartTime);}
f13885
0
remove
public void kafkatest_f13886_0(final Windowed<K> sessionKey)
{    wrapped().remove(sessionKey);}
f13886
0
put
public void kafkatest_f13887_0(final Windowed<K> sessionKey, final AGG aggregate)
{    wrapped().put(sessionKey, aggregate);}
f13887
0
addChild
public void kafkatest_f13895_0(final ProcessorNode<?, ?> child)
{    children.add(child);    childByName.put(child.name, child);}
f13895
0
init
public void kafkatest_f13896_0(final InternalProcessorContext context)
{    try {        nodeMetrics = new NodeMetrics(context.metrics(), name, context);        final long startNs = time.nanoseconds();        if (processor != null) {            processor.init(context);        }        nodeMetrics.nodeCreationSensor.record(time.nanoseconds() - startNs);    } catch (final Exception e) {        throw new StreamsException(String.format("failed to initialize processor %s", name), e);    }}
f13896
0
close
public void kafkatest_f13897_0()
{    try {        final long startNs = time.nanoseconds();        if (processor != null) {            processor.close();        }        nodeMetrics.nodeDestructionSensor.record(time.nanoseconds() - startNs);        nodeMetrics.removeAllSensors();    } catch (final Exception e) {        throw new StreamsException(String.format("failed to close processor %s", name), e);    }}
f13897
0
offset
public long kafkatest_f13905_0()
{    return offset;}
f13905
0
timestamp
public long kafkatest_f13906_0()
{    return timestamp;}
f13906
0
topic
public String kafkatest_f13907_0()
{    return topic;}
f13907
0
toString
public String kafkatest_f13915_0()
{    return "ProcessorRecordContext{" + "topic='" + topic + '\'' + ", partition=" + partition + ", offset=" + offset + ", timestamp=" + timestamp + ", headers=" + headers + '}';}
f13915
0
storeChangelogTopic
public static String kafkatest_f13916_0(final String applicationId, final String storeName)
{    return applicationId + "-" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;}
f13916
0
baseDir
public File kafkatest_f13917_0()
{    return baseDir;}
f13917
0
getStore
public StateStore kafkatest_f13925_0(final String name)
{    return registeredStores.getOrDefault(name, Optional.empty()).orElse(null);}
f13925
0
flush
public voidf13926_1)
{    ProcessorStateException firstException = null;    // attempting to flush the stores    if (!registeredStores.isEmpty()) {                for (final Map.Entry<String, Optional<StateStore>> entry : registeredStores.entrySet()) {            if (entry.getValue().isPresent()) {                final StateStore store = entry.getValue().get();                log.trace("Flushing store {}", store.name());                try {                    store.flush();                } catch (final RuntimeException e) {                    if (firstException == null) {                        firstException = new ProcessorStateException(String.format("%sFailed to flush state store %s", logPrefix, store.name()), e);                    }                                    }            } else {                throw new IllegalStateException("Expected " + entry.getKey() + " to have been initialized");            }        }    }    if (firstException != null) {        throw firstException;    }}
public voidf13926
1
close
public voidf13927_1final boolean clean) throws ProcessorStateException
{    ProcessorStateException firstException = null;    // are not closed by a ProcessorNode yet    if (!registeredStores.isEmpty()) {                for (final Map.Entry<String, Optional<StateStore>> entry : registeredStores.entrySet()) {            if (entry.getValue().isPresent()) {                final StateStore store = entry.getValue().get();                                try {                    store.close();                    registeredStores.put(store.name(), Optional.empty());                } catch (final RuntimeException e) {                    if (firstException == null) {                        firstException = new ProcessorStateException(String.format("%sFailed to close state store %s", logPrefix, store.name()), e);                    }                                    }            } else {                            }        }    }    if (!clean && eosEnabled) {        // delete the checkpoint file if this is an unclean close        try {            clearCheckpoints();        } catch (final IOException e) {            throw new ProcessorStateException(String.format("%sError while deleting the checkpoint file", logPrefix), e);        }    }    if (firstException != null) {        throw firstException;    }}
public voidf13927
1
validCheckpointableTopics
private Set<TopicPartition> kafkatest_f13935_0()
{    // it's only valid to record checkpoints for registered stores that are both persistent and change-logged    final Set<TopicPartition> result = new HashSet<>(storeToChangelogTopic.size());    for (final Map.Entry<String, String> storeToChangelog : storeToChangelogTopic.entrySet()) {        final String storeName = storeToChangelog.getKey();        if (registeredStores.containsKey(storeName) && registeredStores.get(storeName).isPresent() && registeredStores.get(storeName).get().persistent()) {            final String changelogTopic = storeToChangelog.getValue();            result.add(new TopicPartition(changelogTopic, getPartition(changelogTopic)));        }    }    return result;}
f13935
0
validCheckpointableOffsets
private static Map<TopicPartition, Long> kafkatest_f13936_0(final Map<TopicPartition, Long> checkpointableOffsets, final Set<TopicPartition> validCheckpointableTopics)
{    final Map<TopicPartition, Long> result = new HashMap<>(checkpointableOffsets.size());    for (final Map.Entry<TopicPartition, Long> topicToCheckpointableOffset : checkpointableOffsets.entrySet()) {        final TopicPartition topic = topicToCheckpointableOffset.getKey();        if (validCheckpointableTopics.contains(topic)) {            final Long checkpointableOffset = topicToCheckpointableOffset.getValue();            result.put(topic, checkpointableOffset);        }    }    return result;}
f13936
0
sourceTopics
public Set<String> kafkatest_f13937_0()
{    return sourcesByTopic.keySet();}
f13937
0
globalStateStores
public List<StateStore> kafkatest_f13945_0()
{    return globalStateStores;}
f13945
0
storeToChangelogTopic
public Map<String, String> kafkatest_f13946_0()
{    return storeToChangelogTopic;}
f13946
0
isRepartitionTopic
 boolean kafkatest_f13947_0(final String topic)
{    return repartitionTopics.contains(topic);}
f13947
0
close
public void kafkatest_f13955_0()
{    synchronized (pq) {        pq.clear();    }}
f13955
0
mayPunctuate
 boolean kafkatest_f13956_0(final long timestamp, final PunctuationType type, final ProcessorNodePunctuator processorNodePunctuator)
{    synchronized (pq) {        boolean punctuated = false;        PunctuationSchedule top = pq.peek();        while (top != null && top.timestamp <= timestamp) {            final PunctuationSchedule sched = top;            pq.poll();            if (!sched.isCancelled()) {                processorNodePunctuator.punctuate(sched.node(), timestamp, type, sched.punctuator());                // sched can be cancelled from within the punctuator                if (!sched.isCancelled()) {                    pq.add(sched.next(timestamp));                }                punctuated = true;            }            top = pq.peek();        }        return punctuated;    }}
f13956
0
node
public ProcessorNode kafkatest_f13957_0()
{    return value;}
f13957
0
setSchedule
 synchronized void kafkatest_f13965_0(final PunctuationSchedule schedule)
{    this.schedule = schedule;}
f13965
0
cancel
public synchronized void kafkatest_f13966_0()
{    schedule.markCancelled();}
f13966
0
add
public void kafkatest_f13967_0(final T id)
{    ids.put(id, id);}
f13967
0
send
public void kafkatest_f13975_0(final String topic, final K key, final V value, final Headers headers, final Long timestamp, final Serializer<K> keySerializer, final Serializer<V> valueSerializer, final StreamPartitioner<? super K, ? super V> partitioner)
{    Integer partition = null;    if (partitioner != null) {        final List<PartitionInfo> partitions = producer.partitionsFor(topic);        if (partitions.size() > 0) {            partition = partitioner.partition(topic, key, value, partitions.size());        } else {            throw new StreamsException("Could not get partition information for topic '" + topic + "'." + " This can happen if the topic does not exist.");        }    }    send(topic, key, value, headers, partition, timestamp, keySerializer, valueSerializer);}
f13975
0
productionExceptionIsFatal
private boolean kafkatest_f13976_0(final Exception exception)
{    final boolean securityException = exception instanceof AuthenticationException || exception instanceof AuthorizationException || exception instanceof SecurityDisabledException;    final boolean communicationException = exception instanceof InvalidTopicException || exception instanceof UnknownServerException || exception instanceof SerializationException || exception instanceof OffsetMetadataTooLarge || exception instanceof IllegalStateException;    return securityException || communicationException;}
f13976
0
recordSendError
private voidf13977_1final K key, final V value, final Long timestamp, final String topic, final Exception exception)
{    String errorLogMessage = LOG_MESSAGE;    String errorMessage = EXCEPTION_MESSAGE;    // even though it's an implementation detail (i.e. we do the best we can given what's available)    if (exception instanceof RetriableException) {        errorLogMessage += PARAMETER_HINT;        errorMessage += PARAMETER_HINT;    }        // KAFKA-7510 put message key and value in TRACE level log so we don't leak data by default    log.trace("Failed message: key {} value {} timestamp {}", key, value, timestamp);    sendException = new StreamsException(String.format(errorMessage, logPrefix, "an error caught", timestamp, topic, exception.toString()), exception);}
private voidf13977
1
deserialize
 ConsumerRecord<Object, Object>f13985_1final ProcessorContext processorContext, final ConsumerRecord<byte[], byte[]> rawRecord)
{    try {        return new ConsumerRecord<>(rawRecord.topic(), rawRecord.partition(), rawRecord.offset(), rawRecord.timestamp(), TimestampType.CREATE_TIME, rawRecord.checksum(), rawRecord.serializedKeySize(), rawRecord.serializedValueSize(), sourceNode.deserializeKey(rawRecord.topic(), rawRecord.headers(), rawRecord.key()), sourceNode.deserializeValue(rawRecord.topic(), rawRecord.headers(), rawRecord.value()), rawRecord.headers());    } catch (final Exception deserializationException) {        final DeserializationExceptionHandler.DeserializationHandlerResponse response;        try {            response = deserializationExceptionHandler.handle(processorContext, rawRecord, deserializationException);        } catch (final Exception fatalUserException) {                        throw new StreamsException("Fatal user code error in deserialization error callback", fatalUserException);        }        if (response == DeserializationExceptionHandler.DeserializationHandlerResponse.FAIL) {            throw new StreamsException("Deserialization exception handler is set to fail upon" + " a deserialization error. If you would rather have the streaming pipeline" + " continue after a deserialization error, please set the " + DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG + " appropriately.", deserializationException);        } else {                        skippedRecordSensor.record();            return null;        }    }}
 ConsumerRecord<Object, Object>f13985
1
sourceNode
 SourceNode kafkatest_f13986_0()
{    return sourceNode;}
f13986
0
source
public SourceNode kafkatest_f13987_0()
{    return source;}
f13987
0
clear
public void kafkatest_f13995_0()
{    fifoQueue.clear();    headRecord = null;    partitionTime = RecordQueue.UNKNOWN;}
f13995
0
updateHead
private voidf13996_1)
{    while (headRecord == null && !fifoQueue.isEmpty()) {        final ConsumerRecord<byte[], byte[]> raw = fifoQueue.pollFirst();        final ConsumerRecord<Object, Object> deserialized = recordDeserializer.deserialize(processorContext, raw);        if (deserialized == null) {            // this only happens if the deserializer decides to skip. It has already logged the reason.            continue;        }        final long timestamp;        try {            timestamp = timestampExtractor.extract(deserialized, partitionTime);        } catch (final StreamsException internalFatalExtractorException) {            throw internalFatalExtractorException;        } catch (final Exception fatalUserException) {            throw new StreamsException(String.format("Fatal user code error in TimestampExtractor callback for record %s.", deserialized), fatalUserException);        }        log.trace("Source node {} extracted timestamp {} for record {}", source.name(), timestamp, deserialized);        // drop message if TS is invalid, i.e., negative        if (timestamp < 0) {                        skipRecordsSensor.record();            continue;        }        headRecord = new StampedRecord(deserialized, timestamp);        partitionTime = Math.max(partitionTime, timestamp);    }}
private voidf13996
1
getProperties
public Map<String, String> kafkatest_f13997_0(final Map<String, String> defaultProperties, final long additionalRetentionMs)
{    // internal topic config overridden rule: library overrides < global config overrides < per-topic config overrides    final Map<String, String> topicConfig = new HashMap<>(REPARTITION_TOPIC_DEFAULT_OVERRIDES);    topicConfig.putAll(defaultProperties);    topicConfig.putAll(topicConfigs);    return topicConfig;}
f13997
0
toString
public String kafkatest_f14005_0(final String indent)
{    final StringBuilder sb = new StringBuilder(super.toString(indent));    sb.append(indent).append("\ttopic:\t\t");    sb.append(topicExtractor);    sb.append("\n");    return sb.toString();}
f14005
0
deserializeKey
 K kafkatest_f14006_0(final String topic, final Headers headers, final byte[] data)
{    return keyDeserializer.deserialize(topic, headers, data);}
f14006
0
deserializeValue
 V kafkatest_f14007_0(final String topic, final Headers headers, final byte[] data)
{    return valDeserializer.deserialize(topic, headers, data);}
f14007
0
hashCode
public int kafkatest_f14015_0()
{    return Objects.hash(timestamp);}
f14015
0
topic
public String kafkatest_f14016_0()
{    return value.topic();}
f14016
0
partition
public int kafkatest_f14017_0()
{    return value.partition();}
f14017
0
recordCollector
public RecordCollector kafkatest_f14030_0()
{    return NO_OP_COLLECTOR;}
f14030
0
getStateStore
public StateStore kafkatest_f14031_0(final String name)
{    throw new UnsupportedOperationException("this should not happen: getStateStore() not supported in standby tasks.");}
f14031
0
topic
public String kafkatest_f14032_0()
{    throw new UnsupportedOperationException("this should not happen: topic() not supported in standby tasks.");}
f14032
0
commit
public void kafkatest_f14040_0()
{    throw new UnsupportedOperationException("this should not happen: commit() not supported in standby tasks.");}
f14040
0
schedule
public Cancellable kafkatest_f14041_0(final long interval, final PunctuationType type, final Punctuator callback)
{    throw new UnsupportedOperationException("this should not happen: schedule() not supported in standby tasks.");}
f14041
0
schedule
public Cancellable kafkatest_f14042_0(final Duration interval, final PunctuationType type, final Punctuator callback) throws IllegalArgumentException
{    throw new UnsupportedOperationException("this should not happen: schedule() not supported in standby tasks.");}
f14042
0
commit
public void kafkatest_f14050_0()
{    log.trace("Committing");    flushAndCheckpointState();    allowUpdateOfOffsetLimit();    commitNeeded = false;}
f14050
0
suspend
public voidf14051_1)
{        flushAndCheckpointState();}
public voidf14051
1
flushAndCheckpointState
private void kafkatest_f14052_0()
{    stateMgr.flush();    stateMgr.checkpoint(Collections.emptyMap());}
f14052
0
globalStateDir
 File kafkatest_f14060_0()
{    final File dir = new File(stateDir, "global");    if (createStateDirectory && !dir.exists() && !dir.mkdir()) {        throw new ProcessorStateException(String.format("global state directory [%s] doesn't exist and couldn't be created", dir.getPath()));    }    return dir;}
f14060
0
logPrefix
private String kafkatest_f14061_0()
{    return String.format("stream-thread [%s]", Thread.currentThread().getName());}
f14061
0
lock
 synchronized booleanf14062_1final TaskId taskId) throws IOException
{    if (!createStateDirectory) {        return true;    }    final File lockFile;    // we already have the lock so bail out here    final LockAndOwner lockAndOwner = locks.get(taskId);    if (lockAndOwner != null && lockAndOwner.owningThread.equals(Thread.currentThread().getName())) {        log.trace("{} Found cached state dir lock for task {}", logPrefix(), taskId);        return true;    } else if (lockAndOwner != null) {        // another thread owns the lock        return false;    }    try {        lockFile = new File(directoryForTask(taskId), LOCK_FILE_NAME);    } catch (final ProcessorStateException e) {        // has concurrently deleted the directory        return false;    }    final FileChannel channel;    try {        channel = getOrCreateFileChannel(taskId, lockFile.toPath());    } catch (final NoSuchFileException e) {        // file, in this case we will return immediately indicating locking failed.        return false;    }    final FileLock lock = tryLock(channel);    if (lock != null) {        locks.put(taskId, new LockAndOwner(Thread.currentThread().getName(), lock));            }    return lock != null;}
 synchronized booleanf14062
1
getOrCreateFileChannel
private FileChannel kafkatest_f14070_0(final TaskId taskId, final Path lockPath) throws IOException
{    if (!channels.containsKey(taskId)) {        channels.put(taskId, FileChannel.open(lockPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE));    }    return channels.get(taskId);}
f14070
0
tryLock
private FileLock kafkatest_f14071_0(final FileChannel channel) throws IOException
{    try {        return channel.tryLock();    } catch (final OverlappingFileLockException e) {        return null;    }}
f14071
0
converterForStore
 static RecordConverter kafkatest_f14072_0(final StateStore store)
{    return isTimestamped(store) ? rawValueToTimestampedValue() : identity();}
f14072
0
restoreStarted
 void kafkatest_f14080_0()
{    compositeRestoreListener.onRestoreStart(partition, storeName, startingOffset, endingOffset);}
f14080
0
restoreDone
 void kafkatest_f14081_0()
{    compositeRestoreListener.onRestoreEnd(partition, storeName, restoredNumRecords());}
f14081
0
restoreBatchCompleted
 void kafkatest_f14082_0(final long currentRestoredOffset, final int numRestored)
{    compositeRestoreListener.onBatchRestored(partition, storeName, currentRestoredOffset, numRestored);}
f14082
0
hasCompleted
 boolean kafkatest_f14090_0(final long recordOffset, final long endOffset)
{    return endOffset == 0 || recordOffset >= readTo(endOffset);}
f14090
0
restoredOffset
 Long kafkatest_f14091_0()
{    return restoredOffset;}
f14091
0
restoredNumRecords
 long kafkatest_f14092_0()
{    return restoredOffset - startingOffset;}
f14092
0
startRestoration
private voidf14100_1final Set<TopicPartition> initialized, final RestoringTasks active)
{        final Set<TopicPartition> assignment = new HashSet<>(restoreConsumer.assignment());    assignment.addAll(initialized);    restoreConsumer.assign(assignment);    final List<StateRestorer> needsPositionUpdate = new ArrayList<>();    for (final TopicPartition partition : initialized) {        final StateRestorer restorer = stateRestorers.get(partition);        if (restorer.checkpoint() != StateRestorer.NO_CHECKPOINT) {            log.trace("Found checkpoint {} from changelog {} for store {}.", restorer.checkpoint(), partition, restorer.storeName());            restoreConsumer.seek(partition, restorer.checkpoint());            logRestoreOffsets(partition, restorer.checkpoint(), restoreToOffsets.get(partition));            restorer.setStartingOffset(restoreConsumer.position(partition));            restorer.restoreStarted();        } else {            log.trace("Did not find checkpoint from changelog {} for store {}, rewinding to beginning.", partition, restorer.storeName());            restoreConsumer.seekToBeginning(Collections.singletonList(partition));            needsPositionUpdate.add(restorer);        }    }    for (final StateRestorer restorer : needsPositionUpdate) {        final TopicPartition partition = restorer.partition();        // If checkpoint does not exist it means the task was not shutdown gracefully before;        // and in this case if EOS is turned on we should wipe out the state and re-initialize the task        final StreamTask task = active.restoringTaskFor(partition);        if (task.isEosEnabled()) {                        needsInitializing.remove(partition);            initialized.remove(partition);            restorer.setCheckpointOffset(restoreConsumer.position(partition));            task.reinitializeStateStoresForPartitions(Collections.singleton(partition));        } else {                        final long position = restoreConsumer.position(restorer.partition());            logRestoreOffsets(restorer.partition(), position, restoreToOffsets.get(restorer.partition()));            restorer.setStartingOffset(position);            restorer.restoreStarted();        }    }    needsRestoring.addAll(initialized);}
private voidf14100
1
logRestoreOffsets
private voidf14101_1final TopicPartition partition, final long startingOffset, final Long endOffset)
{    }
private voidf14101
1
completed
private Collection<TopicPartition> kafkatest_f14102_0()
{    return completedRestorers;}
f14102
0
getAllMetadata
public synchronized Collection<StreamsMetadata> kafkatest_f14110_0()
{    return allMetadata;}
f14110
0
getAllMetadataForStore
public synchronized Collection<StreamsMetadata> kafkatest_f14111_0(final String storeName)
{    Objects.requireNonNull(storeName, "storeName cannot be null");    if (!isInitialized()) {        return Collections.emptyList();    }    if (globalStores.contains(storeName)) {        return allMetadata;    }    final List<String> sourceTopics = builder.stateStoreNameToSourceTopics().get(storeName);    if (sourceTopics == null) {        return Collections.emptyList();    }    final ArrayList<StreamsMetadata> results = new ArrayList<>();    for (final StreamsMetadata metadata : allMetadata) {        if (metadata.stateStoreNames().contains(storeName)) {            results.add(metadata);        }    }    return results;}
f14111
0
getMetadataWithKey
public synchronized StreamsMetadata kafkatest_f14112_0(final String storeName, final K key, final Serializer<K> keySerializer)
{    Objects.requireNonNull(keySerializer, "keySerializer can't be null");    Objects.requireNonNull(storeName, "storeName can't be null");    Objects.requireNonNull(key, "key can't be null");    if (!isInitialized()) {        return StreamsMetadata.NOT_AVAILABLE;    }    if (globalStores.contains(storeName)) {        // for this host then just pick the first metadata        if (thisHost == UNKNOWN_HOST) {            return allMetadata.get(0);        }        return myMetadata;    }    final SourceTopicsInfo sourceTopicsInfo = getSourceTopicsInfo(storeName);    if (sourceTopicsInfo == null) {        return null;    }    return getStreamsMetadataForKey(storeName, key, new DefaultStreamPartitioner<>(keySerializer, clusterMetadata), sourceTopicsInfo);}
f14112
0
compareTo
public int kafkatest_f14120_0(final AssignedPartition that)
{    return PARTITION_COMPARATOR.compare(partition, that.partition);}
f14120
0
equals
public boolean kafkatest_f14121_0(final Object o)
{    if (!(o instanceof AssignedPartition)) {        return false;    }    final AssignedPartition other = (AssignedPartition) o;    return compareTo(other) == 0;}
f14121
0
hashCode
public int kafkatest_f14122_0()
{    // Only partition is important for compareTo, equals and hashCode.    return partition.hashCode();}
f14122
0
errorAssignment
private Map<String, Assignment>f14130_1final Map<UUID, ClientMetadata> clientsMetadata, final String topic, final int errorCode)
{        final Map<String, Assignment> assignment = new HashMap<>();    for (final ClientMetadata clientMetadata : clientsMetadata.values()) {        for (final String consumerId : clientMetadata.consumers) {            assignment.put(consumerId, new Assignment(Collections.emptyList(), new AssignmentInfo(LATEST_SUPPORTED_VERSION, Collections.emptyList(), Collections.emptyMap(), Collections.emptyMap(), errorCode).encode()));        }    }    return assignment;}
private Map<String, Assignment>f14130
1
assign
public GroupAssignmentf14131_1final Cluster metadata, final GroupSubscription groupSubscription)
{    final Map<String, Subscription> subscriptions = groupSubscription.groupSubscription();    // construct the client metadata from the decoded subscription info    final Map<UUID, ClientMetadata> clientMetadataMap = new HashMap<>();    final Set<String> futureConsumers = new HashSet<>();    int minReceivedMetadataVersion = LATEST_SUPPORTED_VERSION;    int futureMetadataVersion = UNKNOWN;    for (final Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {        final String consumerId = entry.getKey();        final Subscription subscription = entry.getValue();        final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());        final int usedVersion = info.version();        if (usedVersion > LATEST_SUPPORTED_VERSION) {            futureMetadataVersion = usedVersion;            futureConsumers.add(consumerId);            continue;        }        if (usedVersion < minReceivedMetadataVersion) {            minReceivedMetadataVersion = usedVersion;        }        // create the new client metadata if necessary        ClientMetadata clientMetadata = clientMetadataMap.get(info.processId());        if (clientMetadata == null) {            clientMetadata = new ClientMetadata(info.userEndPoint());            clientMetadataMap.put(info.processId(), clientMetadata);        }        // add the consumer to the client        clientMetadata.addConsumer(consumerId, info);    }    final boolean versionProbing;    if (futureMetadataVersion == UNKNOWN) {        versionProbing = false;    } else {        if (minReceivedMetadataVersion >= EARLIEST_PROBEABLE_VERSION) {                        versionProbing = true;        } else {            throw new IllegalStateException("Received a future (version probing) subscription (version: " + futureMetadataVersion + ") and an incompatible pre Kafka 2.0 subscription (version: " + minReceivedMetadataVersion + ") at the same time.");        }    }    if (minReceivedMetadataVersion < LATEST_SUPPORTED_VERSION) {            }        // ---------------- Step Zero ---------------- //    // parse the topology to determine the repartition source topics,    // making sure they are created with the number of partitions as    // the maximum of the depending sub-topologies source topics' number of partitions    final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = taskManager.builder().topicGroups();    final Map<String, InternalTopicConfig> repartitionTopicMetadata = new HashMap<>();    for (final InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {        for (final String topic : topicsInfo.sourceTopics) {            if (!topicsInfo.repartitionSourceTopics.keySet().contains(topic) && !metadata.topics().contains(topic)) {                                return new GroupAssignment(errorAssignment(clientMetadataMap, topic, AssignorError.INCOMPLETE_SOURCE_TOPIC_METADATA.code()));            }        }        for (final InternalTopicConfig topic : topicsInfo.repartitionSourceTopics.values()) {            repartitionTopicMetadata.put(topic.name(), topic);        }    }    boolean numPartitionsNeeded;    do {        numPartitionsNeeded = false;        for (final InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {            for (final String topicName : topicsInfo.repartitionSourceTopics.keySet()) {                final Optional<Integer> maybeNumPartitions = repartitionTopicMetadata.get(topicName).numberOfPartitions();                Integer numPartitions = null;                if (!maybeNumPartitions.isPresent()) {                    // try set the number of partitions for this repartition topic if it is not set yet                    for (final InternalTopologyBuilder.TopicsInfo otherTopicsInfo : topicGroups.values()) {                        final Set<String> otherSinkTopics = otherTopicsInfo.sinkTopics;                        if (otherSinkTopics.contains(topicName)) {                            // use the maximum of all its source topic partitions as the number of partitions                            for (final String sourceTopicName : otherTopicsInfo.sourceTopics) {                                final int numPartitionsCandidate;                                // map().join().join(map())                                if (repartitionTopicMetadata.containsKey(sourceTopicName) && repartitionTopicMetadata.get(sourceTopicName).numberOfPartitions().isPresent()) {                                    numPartitionsCandidate = repartitionTopicMetadata.get(sourceTopicName).numberOfPartitions().get();                                } else {                                    final Integer count = metadata.partitionCountForTopic(sourceTopicName);                                    if (count == null) {                                        throw new IllegalStateException("No partition count found for source topic " + sourceTopicName + ", but it should have been.");                                    }                                    numPartitionsCandidate = count;                                }                                if (numPartitions == null || numPartitionsCandidate > numPartitions) {                                    numPartitions = numPartitionsCandidate;                                }                            }                        }                    }                    // another iteration is needed                    if (numPartitions == null) {                        numPartitionsNeeded = true;                    } else {                        repartitionTopicMetadata.get(topicName).setNumberOfPartitions(numPartitions);                    }                }            }        }    } while (numPartitionsNeeded);    // ensure the co-partitioning topics within the group have the same number of partitions,    // and enforce the number of partitions for those repartition topics to be the same if they    // are co-partitioned as well.    ensureCopartitioning(taskManager.builder().copartitionGroups(), repartitionTopicMetadata, metadata);    // make sure the repartition source topics exist with the right number of partitions,    // create these topics if necessary    prepareTopic(repartitionTopicMetadata);    // augment the metadata with the newly computed number of partitions for all the    // repartition source topics    final Map<TopicPartition, PartitionInfo> allRepartitionTopicPartitions = new HashMap<>();    for (final Map.Entry<String, InternalTopicConfig> entry : repartitionTopicMetadata.entrySet()) {        final String topic = entry.getKey();        final int numPartitions = entry.getValue().numberOfPartitions().orElse(-1);        for (int partition = 0; partition < numPartitions; partition++) {            allRepartitionTopicPartitions.put(new TopicPartition(topic, partition), new PartitionInfo(topic, partition, null, new Node[0], new Node[0]));        }    }    final Cluster fullMetadata = metadata.withPartitions(allRepartitionTopicPartitions);    taskManager.setClusterMetadata(fullMetadata);        // ---------------- Step One ---------------- //    // get the tasks as partition groups from the partition grouper    final Set<String> allSourceTopics = new HashSet<>();    final Map<Integer, Set<String>> sourceTopicsByGroup = new HashMap<>();    for (final Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {        allSourceTopics.addAll(entry.getValue().sourceTopics);        sourceTopicsByGroup.put(entry.getKey(), entry.getValue().sourceTopics);    }    final Map<TaskId, Set<TopicPartition>> partitionsForTask = partitionGrouper.partitionGroups(sourceTopicsByGroup, fullMetadata);    // check if all partitions are assigned, and there are no duplicates of partitions in multiple tasks    final Set<TopicPartition> allAssignedPartitions = new HashSet<>();    final Map<Integer, Set<TaskId>> tasksByTopicGroup = new HashMap<>();    for (final Map.Entry<TaskId, Set<TopicPartition>> entry : partitionsForTask.entrySet()) {        final Set<TopicPartition> partitions = entry.getValue();        for (final TopicPartition partition : partitions) {            if (allAssignedPartitions.contains(partition)) {                            }        }        allAssignedPartitions.addAll(partitions);        final TaskId id = entry.getKey();        tasksByTopicGroup.computeIfAbsent(id.topicGroupId, k -> new HashSet<>()).add(id);    }    for (final String topic : allSourceTopics) {        final List<PartitionInfo> partitionInfoList = fullMetadata.partitionsForTopic(topic);        if (partitionInfoList.isEmpty()) {                    } else {            for (final PartitionInfo partitionInfo : partitionInfoList) {                final TopicPartition partition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());                if (!allAssignedPartitions.contains(partition)) {                                    }            }        }    }    // add tasks to state change log topic subscribers    final Map<String, InternalTopicConfig> changelogTopicMetadata = new HashMap<>();    for (final Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {        final int topicGroupId = entry.getKey();        final Map<String, InternalTopicConfig> stateChangelogTopics = entry.getValue().stateChangelogTopics;        for (final InternalTopicConfig topicConfig : stateChangelogTopics.values()) {            // the expected number of partitions is the max value of TaskId.partition + 1            int numPartitions = UNKNOWN;            if (tasksByTopicGroup.get(topicGroupId) != null) {                for (final TaskId task : tasksByTopicGroup.get(topicGroupId)) {                    if (numPartitions < task.partition + 1) {                        numPartitions = task.partition + 1;                    }                }                topicConfig.setNumberOfPartitions(numPartitions);                changelogTopicMetadata.put(topicConfig.name(), topicConfig);            } else {                            }        }    }    prepareTopic(changelogTopicMetadata);        // ---------------- Step Two ---------------- //    // assign tasks to clients    final Map<UUID, ClientState> states = new HashMap<>();    for (final Map.Entry<UUID, ClientMetadata> entry : clientMetadataMap.entrySet()) {        states.put(entry.getKey(), entry.getValue().state);    }        final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(states, partitionsForTask.keySet());    taskAssignor.assign(numStandbyReplicas);        // ---------------- Step Three ---------------- //    // construct the global partition assignment per host map    final Map<HostInfo, Set<TopicPartition>> partitionsByHostState = new HashMap<>();    if (minReceivedMetadataVersion >= 2) {        for (final Map.Entry<UUID, ClientMetadata> entry : clientMetadataMap.entrySet()) {            final HostInfo hostInfo = entry.getValue().hostInfo;            // if application server is configured, also include host state map            if (hostInfo != null) {                final Set<TopicPartition> topicPartitions = new HashSet<>();                final ClientState state = entry.getValue().state;                for (final TaskId id : state.activeTasks()) {                    topicPartitions.addAll(partitionsForTask.get(id));                }                partitionsByHostState.put(hostInfo, topicPartitions);            }        }    }    taskManager.setPartitionsByHostState(partitionsByHostState);    final Map<String, Assignment> assignment;    if (versionProbing) {        assignment = versionProbingAssignment(clientMetadataMap, partitionsForTask, partitionsByHostState, futureConsumers, minReceivedMetadataVersion);    } else {        assignment = computeNewAssignment(clientMetadataMap, partitionsForTask, partitionsByHostState, minReceivedMetadataVersion);    }    return new GroupAssignment(assignment);}
public GroupAssignmentf14131
1
computeNewAssignment
private static Map<String, Assignment> kafkatest_f14132_0(final Map<UUID, ClientMetadata> clientsMetadata, final Map<TaskId, Set<TopicPartition>> partitionsForTask, final Map<HostInfo, Set<TopicPartition>> partitionsByHostState, final int minUserMetadataVersion)
{    final Map<String, Assignment> assignment = new HashMap<>();    // within the client, distribute tasks to its owned consumers    for (final Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {        final Set<String> consumers = entry.getValue().consumers;        final ClientState state = entry.getValue().state;        final List<List<TaskId>> interleavedActive = interleaveTasksByGroupId(state.activeTasks(), consumers.size());        final List<List<TaskId>> interleavedStandby = interleaveTasksByGroupId(state.standbyTasks(), consumers.size());        int consumerTaskIndex = 0;        for (final String consumer : consumers) {            final Map<TaskId, Set<TopicPartition>> standby = new HashMap<>();            final List<AssignedPartition> assignedPartitions = new ArrayList<>();            final List<TaskId> assignedActiveList = interleavedActive.get(consumerTaskIndex);            for (final TaskId taskId : assignedActiveList) {                for (final TopicPartition partition : partitionsForTask.get(taskId)) {                    assignedPartitions.add(new AssignedPartition(taskId, partition));                }            }            if (!state.standbyTasks().isEmpty()) {                final List<TaskId> assignedStandbyList = interleavedStandby.get(consumerTaskIndex);                for (final TaskId taskId : assignedStandbyList) {                    standby.computeIfAbsent(taskId, k -> new HashSet<>()).addAll(partitionsForTask.get(taskId));                }            }            consumerTaskIndex++;            Collections.sort(assignedPartitions);            final List<TaskId> active = new ArrayList<>();            final List<TopicPartition> activePartitions = new ArrayList<>();            for (final AssignedPartition partition : assignedPartitions) {                active.add(partition.taskId);                activePartitions.add(partition.partition);            }            // finally, encode the assignment before sending back to coordinator            assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState, 0).encode()));        }    }    return assignment;}
f14132
0
ensureCopartitioning
private void kafkatest_f14140_0(final Collection<Set<String>> copartitionGroups, final Map<String, InternalTopicConfig> allRepartitionTopicsNumPartitions, final Cluster metadata)
{    for (final Set<String> copartitionGroup : copartitionGroups) {        copartitionedTopicsEnforcer.enforce(copartitionGroup, allRepartitionTopicsNumPartitions, metadata);    }}
f14140
0
setInternalTopicManager
 void kafkatest_f14141_0(final InternalTopicManager internalTopicManager)
{    this.internalTopicManager = internalTopicManager;}
f14141
0
removeAllSensors
 void kafkatest_f14142_0()
{    metrics.removeAllTaskLevelSensors(taskName);}
f14142
0
updateProcessorContext
private void kafkatest_f14150_0(final StampedRecord record, final ProcessorNode currNode)
{    processorContext.setRecordContext(new ProcessorRecordContext(record.timestamp, record.offset(), record.partition(), record.topic(), record.headers()));    processorContext.setCurrentNode(currNode);}
f14150
0
commit
public void kafkatest_f14151_0()
{    commit(true);}
f14151
0
commit
 voidf14152_1final boolean startNewTransaction)
{    final long startNs = time.nanoseconds();        flushState();    if (!eosEnabled) {        stateMgr.checkpoint(activeTaskCheckpointableOffsets());    }    final Map<TopicPartition, OffsetAndMetadata> consumedOffsetsAndMetadata = new HashMap<>(consumedOffsets.size());    for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {        final TopicPartition partition = entry.getKey();        final long offset = entry.getValue() + 1;        consumedOffsetsAndMetadata.put(partition, new OffsetAndMetadata(offset));    }    try {        if (eosEnabled) {            producer.sendOffsetsToTransaction(consumedOffsetsAndMetadata, applicationId);            producer.commitTransaction();            transactionInFlight = false;            if (startNewTransaction) {                producer.beginTransaction();                transactionInFlight = true;            }        } else {            consumer.commitSync(consumedOffsetsAndMetadata);        }    } catch (final CommitFailedException | ProducerFencedException error) {        throw new TaskMigratedException(this, error);    }    commitNeeded = false;    commitRequested = false;    taskMetrics.taskCommitTimeSensor.record(time.nanoseconds() - startNs);}
 voidf14152
1
closeTopology
private void kafkatest_f14160_0()
{    log.trace("Closing processor topology");    partitionGroup.clear();    // close the processors    // make sure close() is called for each node even when there is a RuntimeException    RuntimeException exception = null;    if (taskInitialized) {        for (final ProcessorNode node : topology.processors()) {            processorContext.setCurrentNode(node);            try {                node.close();            } catch (final RuntimeException e) {                exception = e;            } finally {                processorContext.setCurrentNode(null);            }        }    }    if (exception != null) {        throw exception;    }}
f14160
0
closeSuspended
public voidf14161_1final boolean clean, final boolean isZombie, RuntimeException firstException)
{    try {        closeStateManager(clean);    } catch (final RuntimeException e) {        if (firstException == null) {            firstException = e;        }            }    partitionGroup.close();    taskMetrics.removeAllSensors();    closeTaskSensor.record();    if (firstException != null) {        throw firstException;    }}
public voidf14161
1
close
public voidf14162_1boolean clean, final boolean isZombie)
{        RuntimeException firstException = null;    try {        suspend(clean, isZombie);    } catch (final RuntimeException e) {        clean = false;        firstException = e;            }    closeSuspended(clean, isZombie, firstException);    taskClosed = true;}
public voidf14162
1
commitRequested
 boolean kafkatest_f14170_0()
{    return commitRequested;}
f14170
0
recordCollector
 RecordCollector kafkatest_f14171_0()
{    return recordCollector;}
f14171
0
getProducer
 Producer<byte[], byte[]> kafkatest_f14172_0()
{    return producer;}
f14172
0
isRunning
public boolean kafkatest_f14180_0()
{    synchronized (stateLock) {        return state.isRunning();    }}
f14180
0
onPartitionsAssigned
public voidf14181_1final Collection<TopicPartition> assignment)
{        if (streamThread.assignmentErrorCode.get() == AssignorError.INCOMPLETE_SOURCE_TOPIC_METADATA.code()) {                streamThread.shutdown();        return;    }    final long start = time.milliseconds();    try {        if (streamThread.setState(State.PARTITIONS_ASSIGNED) == null) {                    } else if (streamThread.assignmentErrorCode.get() != AssignorError.NONE.code()) {                    } else {                        taskManager.createTasks(assignment);        }    } catch (final Throwable t) {                streamThread.setRebalanceException(t);    } finally {            }}
public voidf14181
1
onPartitionsRevoked
public voidf14182_1final Collection<TopicPartition> assignment)
{        if (streamThread.setState(State.PARTITIONS_REVOKED) != null) {        final long start = time.milliseconds();        try {            // suspend active tasks            taskManager.suspendTasksAndState();        } catch (final Throwable t) {                        streamThread.setRebalanceException(t);        } finally {            streamThread.clearStandbyRecords();                    }    }}
public voidf14182
1
create
public static StreamThreadf14191_1final InternalTopologyBuilder builder, final StreamsConfig config, final KafkaClientSupplier clientSupplier, final Admin adminClient, final UUID processId, final String clientId, final Metrics metrics, final Time time, final StreamsMetadataState streamsMetadataState, final long cacheSizeBytes, final StateDirectory stateDirectory, final StateRestoreListener userStateRestoreListener, final int threadIdx)
{    final String threadClientId = clientId + "-StreamThread-" + threadIdx;    final String logPrefix = String.format("stream-thread [%s] ", threadClientId);    final LogContext logContext = new LogContext(logPrefix);    final Logger log = logContext.logger(StreamThread.class);        final Map<String, Object> restoreConsumerConfigs = config.getRestoreConsumerConfigs(getRestoreConsumerClientId(threadClientId));    final Consumer<byte[], byte[]> restoreConsumer = clientSupplier.getRestoreConsumer(restoreConsumerConfigs);    final Duration pollTime = Duration.ofMillis(config.getLong(StreamsConfig.POLL_MS_CONFIG));    final StoreChangelogReader changelogReader = new StoreChangelogReader(restoreConsumer, pollTime, userStateRestoreListener, logContext);    Producer<byte[], byte[]> threadProducer = null;    final boolean eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));    if (!eosEnabled) {        final Map<String, Object> producerConfigs = config.getProducerConfigs(getThreadProducerClientId(threadClientId));                threadProducer = clientSupplier.getProducer(producerConfigs);    }    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, threadClientId, config.getString(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG));    final ThreadCache cache = new ThreadCache(logContext, cacheSizeBytes, streamsMetrics);    final AbstractTaskCreator<StreamTask> activeTaskCreator = new TaskCreator(builder, config, streamsMetrics, stateDirectory, changelogReader, cache, time, clientSupplier, threadProducer, threadClientId, log);    final AbstractTaskCreator<StandbyTask> standbyTaskCreator = new StandbyTaskCreator(builder, config, streamsMetrics, stateDirectory, changelogReader, time, log);    final TaskManager taskManager = new TaskManager(changelogReader, processId, logPrefix, restoreConsumer, streamsMetadataState, activeTaskCreator, standbyTaskCreator, adminClient, new AssignedStreamsTasks(logContext), new AssignedStandbyTasks(logContext));        final String applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);    final Map<String, Object> consumerConfigs = config.getMainConsumerConfigs(applicationId, getConsumerClientId(threadClientId), threadIdx);    consumerConfigs.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);    final AtomicInteger assignmentErrorCode = new AtomicInteger();    consumerConfigs.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentErrorCode);    String originalReset = null;    if (!builder.latestResetTopicsPattern().pattern().equals("") || !builder.earliestResetTopicsPattern().pattern().equals("")) {        originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);        consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "none");    }    final Consumer<byte[], byte[]> consumer = clientSupplier.getConsumer(consumerConfigs);    taskManager.setConsumer(consumer);    return new StreamThread(time, config, threadProducer, restoreConsumer, consumer, originalReset, taskManager, streamsMetrics, builder, threadClientId, logContext, assignmentErrorCode).updateThreadMetadata(getSharedAdminClientId(clientId));}
public static StreamThreadf14191
1
getTaskProducerClientId
private static String kafkatest_f14192_0(final String threadClientId, final TaskId taskId)
{    return threadClientId + "-" + taskId + "-producer";}
f14192
0
getThreadProducerClientId
private static String kafkatest_f14193_0(final String threadClientId)
{    return threadClientId + "-producer";}
f14193
0
runOnce
 voidf14201_1)
{    final ConsumerRecords<byte[], byte[]> records;    now = time.milliseconds();    if (state == State.PARTITIONS_ASSIGNED) {        // try to fetch some records with zero poll millis        // to unblock the restoration as soon as possible        records = pollRequests(Duration.ZERO);    } else if (state == State.PARTITIONS_REVOKED) {        // try to fetch some records with normal poll time        // in order to wait long enough to get the join response        records = pollRequests(pollTime);    } else if (state == State.RUNNING || state == State.STARTING) {        // try to fetch some records with normal poll time        // in order to get long polling        records = pollRequests(pollTime);    } else {        // any other state should not happen                throw new StreamsException(logPrefix + "Unexpected state " + state + " during normal iteration");    }    // could affect the task manager state beyond this point within #runOnce().    if (!isRunning()) {                return;    }    final long pollLatency = advanceNowAndComputeLatency();    if (records != null && !records.isEmpty()) {        pollSensor.record(pollLatency, now);        addRecordsToTasks(records);    }    // if the state is still in PARTITION_ASSIGNED after the poll call    if (state == State.PARTITIONS_ASSIGNED) {        if (taskManager.updateNewAndRestoringTasks()) {            setState(State.RUNNING);        }    }    advanceNowAndComputeLatency();    // tasks are still being restored.    if (taskManager.hasActiveRunningTasks()) {        /*             * Within an iteration, after N (N initialized as 1 upon start up) round of processing one-record-each on the applicable tasks, check the current time:             *  1. If it is time to commit, do it;             *  2. If it is time to punctuate, do it;             *  3. If elapsed time is close to consumer's max.poll.interval.ms, end the current iteration immediately.             *  4. If none of the the above happens, increment N.             *  5. If one of the above happens, half the value of N.             */        int processed = 0;        long timeSinceLastPoll = 0L;        do {            for (int i = 0; i < numIterations; i++) {                processed = taskManager.process(now);                if (processed > 0) {                    final long processLatency = advanceNowAndComputeLatency();                    processSensor.record(processLatency / (double) processed, now);                    // commit any tasks that have requested a commit                    final int committed = taskManager.maybeCommitActiveTasksPerUserRequested();                    if (committed > 0) {                        final long commitLatency = advanceNowAndComputeLatency();                        commitSensor.record(commitLatency / (double) committed, now);                    }                } else {                    // if there is no records to be processed, exit immediately                    break;                }            }            timeSinceLastPoll = Math.max(now - lastPollMs, 0);            if (maybePunctuate() || maybeCommit()) {                numIterations = numIterations > 1 ? numIterations / 2 : numIterations;            } else if (timeSinceLastPoll > maxPollTimeMs / 2) {                numIterations = numIterations > 1 ? numIterations / 2 : numIterations;                break;            } else if (processed > 0) {                numIterations++;            }        } while (processed > 0);    }    // update standby tasks and maybe commit the standby tasks as well    maybeUpdateStandbyTasks();    maybeCommit();}
 voidf14201
1
pollRequests
private ConsumerRecords<byte[], byte[]> kafkatest_f14202_0(final Duration pollTime)
{    ConsumerRecords<byte[], byte[]> records = null;    lastPollMs = now;    try {        records = consumer.poll(pollTime);    } catch (final InvalidOffsetException e) {        resetInvalidOffsets(e);    }    if (rebalanceException != null) {        if (rebalanceException instanceof TaskMigratedException) {            throw (TaskMigratedException) rebalanceException;        } else {            throw new StreamsException(logPrefix + "Failed to rebalance.", rebalanceException);        }    }    return records;}
f14202
0
resetInvalidOffsets
private void kafkatest_f14203_0(final InvalidOffsetException e)
{    final Set<TopicPartition> partitions = e.partitions();    final Set<String> loggedTopics = new HashSet<>();    final Set<TopicPartition> seekToBeginning = new HashSet<>();    final Set<TopicPartition> seekToEnd = new HashSet<>();    for (final TopicPartition partition : partitions) {        if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {            addToResetList(partition, seekToBeginning, "Setting topic '{}' to consume from {} offset", "earliest", loggedTopics);        } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {            addToResetList(partition, seekToEnd, "Setting topic '{}' to consume from {} offset", "latest", loggedTopics);        } else {            if (originalReset == null || (!originalReset.equals("earliest") && !originalReset.equals("latest"))) {                final String errorMessage = "No valid committed offset found for input topic %s (partition %s) and no valid reset policy configured." + " You need to set configuration parameter \"auto.offset.reset\" or specify a topic specific reset " + "policy via StreamsBuilder#stream(..., Consumed.with(Topology.AutoOffsetReset)) or StreamsBuilder#table(..., Consumed.with(Topology.AutoOffsetReset))";                throw new StreamsException(String.format(errorMessage, partition.topic(), partition.partition()), e);            }            if (originalReset.equals("earliest")) {                addToResetList(partition, seekToBeginning, "No custom setting defined for topic '{}' using original config '{}' for offset reset", "earliest", loggedTopics);            } else if (originalReset.equals("latest")) {                addToResetList(partition, seekToEnd, "No custom setting defined for topic '{}' using original config '{}' for offset reset", "latest", loggedTopics);            }        }    }    if (!seekToBeginning.isEmpty()) {        consumer.seekToBeginning(seekToBeginning);    }    if (!seekToEnd.isEmpty()) {        consumer.seekToEnd(seekToEnd);    }}
f14203
0
completeShutdown
private voidf14211_1final boolean cleanRun)
{    // set the state to pending shutdown first as it may be called due to error;    // its state may already be PENDING_SHUTDOWN so it will return false but we    // intentionally do not check the returned flag    setState(State.PENDING_SHUTDOWN);        try {        taskManager.shutdown(cleanRun);    } catch (final Throwable e) {            }    try {        consumer.close();    } catch (final Throwable e) {            }    try {        restoreConsumer.close();    } catch (final Throwable e) {            }    streamsMetrics.removeAllThreadLevelSensors();    setState(State.DEAD);    }
private voidf14211
1
clearStandbyRecords
private void kafkatest_f14212_0()
{    standbyRecords.clear();}
f14212
0
threadMetadata
public final ThreadMetadata kafkatest_f14213_0()
{    return threadMetadata;}
f14213
0
adminClientMetrics
public Map<MetricName, Metric> kafkatest_f14221_0()
{    final Map<MetricName, ? extends Metric> adminClientMetrics = taskManager.getAdminClient().metrics();    final LinkedHashMap<MetricName, Metric> result = new LinkedHashMap<>();    result.putAll(adminClientMetrics);    return result;}
f14221
0
setNow
 void kafkatest_f14222_0(final long now)
{    this.now = now;}
f14222
0
taskManager
 TaskManager kafkatest_f14223_0()
{    return taskManager;}
f14223
0
activeTaskIds
 Set<TaskId> kafkatest_f14231_0()
{    return active.allAssignedTaskIds();}
f14231
0
standbyTaskIds
 Set<TaskId> kafkatest_f14232_0()
{    return standby.allAssignedTaskIds();}
f14232
0
prevActiveTaskIds
public Set<TaskId> kafkatest_f14233_0()
{    return active.previousTaskIds();}
f14233
0
suspendedStandbyTaskIds
 Set<TaskId> kafkatest_f14241_0()
{    return standby.previousTaskIds();}
f14241
0
activeTask
 StreamTask kafkatest_f14242_0(final TopicPartition partition)
{    return active.runningTaskFor(partition);}
f14242
0
standbyTask
 StandbyTask kafkatest_f14243_0(final TopicPartition partition)
{    return standby.runningTaskFor(partition);}
f14243
0
setClusterMetadata
public void kafkatest_f14251_0(final Cluster cluster)
{    this.cluster = cluster;}
f14251
0
setPartitionsByHostState
public void kafkatest_f14252_0(final Map<HostInfo, Set<TopicPartition>> partitionsByHostState)
{    this.streamsMetadataState.onChange(partitionsByHostState, cluster);}
f14252
0
setAssignmentMetadata
public void kafkatest_f14253_0(final Map<TaskId, Set<TopicPartition>> activeTasks, final Map<TaskId, Set<TopicPartition>> standbyTasks)
{    this.assignedActiveTasks = activeTasks;    this.assignedStandbyTasks = standbyTasks;}
f14253
0
toString
public String kafkatest_f14261_0()
{    return toString("");}
f14261
0
toString
public String kafkatest_f14262_0(final String indent)
{    final StringBuilder builder = new StringBuilder();    builder.append("TaskManager\n");    builder.append(indent).append("\tMetadataState:\n");    builder.append(streamsMetadataState.toString(indent + "\t\t"));    builder.append(indent).append("\tActive tasks:\n");    builder.append(active.toString(indent + "\t\t"));    builder.append(indent).append("\tStandby tasks:\n");    builder.append(standby.toString(indent + "\t\t"));    return builder.toString();}
f14262
0
assignedActiveTasks
 Map<TaskId, Set<TopicPartition>> kafkatest_f14263_0()
{    return assignedActiveTasks;}
f14263
0
hashCode
public int kafkatest_f14271_0()
{    return Objects.hash(name, topicConfigs);}
f14271
0
toString
public String kafkatest_f14272_0()
{    return "UnwindowedChangelogTopicConfig(" + "name=" + name + ", topicConfigs=" + topicConfigs + ")";}
f14272
0
getProperties
public Map<String, String> kafkatest_f14273_0(final Map<String, String> defaultProperties, final long additionalRetentionMs)
{    // internal topic config overridden rule: library overrides < global config overrides < per-topic config overrides    final Map<String, String> topicConfig = new HashMap<>(WINDOWED_STORE_CHANGELOG_TOPIC_DEFAULT_OVERRIDES);    topicConfig.putAll(defaultProperties);    topicConfig.putAll(topicConfigs);    if (retentionMs != null) {        long retentionValue;        try {            retentionValue = Math.addExact(retentionMs, additionalRetentionMs);        } catch (final ArithmeticException swallow) {            retentionValue = Long.MAX_VALUE;        }        topicConfig.put(TopicConfig.RETENTION_MS_CONFIG, String.valueOf(retentionValue));    }    return topicConfig;}
f14273
0
writeTo
public void kafkatest_f14281_0(final DataOutputStream out) throws IOException
{    out.writeInt(topicGroupId);    out.writeInt(partition);}
f14281
0
readFrom
public static TaskId kafkatest_f14282_0(final DataInputStream in) throws IOException
{    return new TaskId(in.readInt(), in.readInt());}
f14282
0
writeTo
public void kafkatest_f14283_0(final ByteBuffer buf)
{    buf.putInt(topicGroupId);    buf.putInt(partition);}
f14283
0
hashCode
public int kafkatest_f14291_0()
{    return Objects.hash(taskId, topicPartitions);}
f14291
0
toString
public String kafkatest_f14292_0()
{    return "TaskMetadata{" + "taskId=" + taskId + ", topicPartitions=" + topicPartitions + '}';}
f14292
0
threadState
public String kafkatest_f14293_0()
{    return threadState;}
f14293
0
equals
public boolean kafkatest_f14301_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final ThreadMetadata that = (ThreadMetadata) o;    return Objects.equals(threadName, that.threadName) && Objects.equals(threadState, that.threadState) && Objects.equals(activeTasks, that.activeTasks) && Objects.equals(standbyTasks, that.standbyTasks) && mainConsumerClientId.equals(that.mainConsumerClientId) && restoreConsumerClientId.equals(that.restoreConsumerClientId) && Objects.equals(producerClientIds, that.producerClientIds) && adminClientId.equals(that.adminClientId);}
f14301
0
hashCode
public int kafkatest_f14302_0()
{    return Objects.hash(threadName, threadState, activeTasks, standbyTasks, mainConsumerClientId, restoreConsumerClientId, producerClientIds, adminClientId);}
f14302
0
toString
public String kafkatest_f14303_0()
{    return "ThreadMetadata{" + "threadName=" + threadName + ", threadState=" + threadState + ", activeTasks=" + activeTasks + ", standbyTasks=" + standbyTasks + ", consumerClientId=" + mainConsumerClientId + ", restoreConsumerClientId=" + restoreConsumerClientId + ", producerClientIds=" + producerClientIds + ", adminClientId=" + adminClientId + '}';}
f14303
0
extract
public long kafkatest_f14311_0(final ConsumerRecord<Object, Object> record, final long partitionTime)
{    return System.currentTimeMillis();}
f14311
0
equals
public boolean kafkatest_f14312_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final HostInfo hostInfo = (HostInfo) o;    return port == hostInfo.port && host.equals(hostInfo.host);}
f14312
0
hashCode
public int kafkatest_f14313_0()
{    int result = host.hashCode();    result = 31 * result + port;    return result;}
f14313
0
nextCacheValue
private KeyValue<K, V> kafkatest_f14321_0(final Bytes nextCacheKey)
{    final KeyValue<Bytes, LRUCacheEntry> next = cacheIterator.next();    if (!next.key.equals(nextCacheKey)) {        throw new IllegalStateException("Next record key is not the peeked key value; this should not happen");    }    return KeyValue.pair(deserializeCacheKey(next.key), deserializeCacheValue(next.value));}
f14321
0
peekNextKey
public K kafkatest_f14322_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    final Bytes nextCacheKey = cacheIterator.hasNext() ? cacheIterator.peekNextKey() : null;    final KS nextStoreKey = storeIterator.hasNext() ? storeIterator.peekNextKey() : null;    if (nextCacheKey == null) {        return deserializeStoreKey(nextStoreKey);    }    if (nextStoreKey == null) {        return deserializeCacheKey(nextCacheKey);    }    final int comparison = compare(nextCacheKey, nextStoreKey);    if (comparison > 0) {        return deserializeStoreKey(nextStoreKey);    } else if (comparison < 0) {        return deserializeCacheKey(nextCacheKey);    } else {        // skip the same keyed element        storeIterator.next();        return deserializeCacheKey(nextCacheKey);    }}
f14322
0
close
public void kafkatest_f14323_0()
{    cacheIterator.close();    storeIterator.close();}
f14323
0
name
public String kafkatest_f14331_0()
{    return name;}
f14331
0
init
public void kafkatest_f14332_0(final ProcessorContext context, final StateStore root)
{    this.context = (InternalProcessorContext) context;    final StreamsMetricsImpl metrics = this.context.metrics();    final String taskName = context.taskId().toString();    expiredRecordSensor = metrics.storeLevelSensor(taskName, name(), EXPIRED_WINDOW_RECORD_DROP, Sensor.RecordingLevel.INFO);    addInvocationRateAndCountToSensor(expiredRecordSensor, "stream-" + metricScope + "-metrics", metrics.tagMap("task-id", taskName, metricScope + "-id", name()), EXPIRED_WINDOW_RECORD_DROP);    segments.openExisting(this.context, observedStreamTime);    bulkLoadSegments = new HashSet<>(segments.allSegments());    // register and possibly restore the state from the logs    context.register(root, new RocksDBSegmentsBatchingRestoreCallback());    open = true;}
f14332
0
flush
public void kafkatest_f14333_0()
{    segments.flush();}
f14333
0
restoreAll
public void kafkatest_f14341_0(final Collection<KeyValue<byte[], byte[]>> records)
{    restoreAllInternal(records);}
f14341
0
onRestoreStart
public void kafkatest_f14342_0(final TopicPartition topicPartition, final String storeName, final long startingOffset, final long endingOffset)
{    toggleForBulkLoading(true);}
f14342
0
onRestoreEnd
public void kafkatest_f14343_0(final TopicPartition topicPartition, final String storeName, final long totalRestored)
{    toggleForBulkLoading(false);}
f14343
0
flush
public void kafkatest_f14351_0()
{    for (final S segment : segments.values()) {        segment.flush();    }}
f14351
0
close
public void kafkatest_f14352_0()
{    for (final S segment : segments.values()) {        segment.close();    }    segments.clear();}
f14352
0
cleanupEarlierThan
private voidf14353_1final long minLiveSegment)
{    final Iterator<Map.Entry<Long, S>> toRemove = segments.headMap(minLiveSegment, false).entrySet().iterator();    while (toRemove.hasNext()) {        final Map.Entry<Long, S> next = toRemove.next();        toRemove.remove();        final S segment = next.getValue();        segment.close();        try {            segment.destroy();        } catch (final IOException e) {                    }    }}
private voidf14353
1
loggingEnabled
public boolean kafkatest_f14361_0()
{    return enableLogging;}
f14361
0
name
public String kafkatest_f14362_0()
{    return name;}
f14362
0
time
 long kafkatest_f14363_0()
{    return time;}
f14363
0
newValue
 byte[] kafkatest_f14371_0()
{    return newValue;}
f14371
0
context
 ProcessorRecordContext kafkatest_f14372_0()
{    return recordContext;}
f14372
0
deserialize
 static BufferValue kafkatest_f14373_0(final ByteBuffer buffer)
{    final ProcessorRecordContext context = ProcessorRecordContext.deserialize(buffer);    final byte[] priorValue = extractValue(buffer);    final byte[] oldValue;    final int oldValueLength = buffer.getInt();    if (oldValueLength == NULL_VALUE_SENTINEL) {        oldValue = null;    } else if (oldValueLength == OLD_PREV_DUPLICATE_VALUE_SENTINEL) {        oldValue = priorValue;    } else {        oldValue = new byte[oldValueLength];        buffer.get(oldValue);    }    final byte[] newValue = extractValue(buffer);    return new BufferValue(priorValue, oldValue, newValue, context);}
f14373
0
init
public void kafkatest_f14381_0(final ProcessorContext context, final StateStore root)
{    initInternal(context);    super.init(context, root);    // save the stream thread as we only ever want to trigger a flush    // when the stream thread is the current thread.    streamThread = Thread.currentThread();}
f14381
0
initInternal
private void kafkatest_f14382_0(final ProcessorContext context)
{    this.context = (InternalProcessorContext) context;    this.cache = this.context.getCache();    this.cacheName = ThreadCache.nameSpaceFromTaskIdAndStore(context.taskId().toString(), name());    cache.addDirtyEntryFlushListener(cacheName, entries -> {        for (final ThreadCache.DirtyEntry entry : entries) {            putAndMaybeForward(entry, (InternalProcessorContext) context);        }    });}
f14382
0
putAndMaybeForward
private void kafkatest_f14383_0(final ThreadCache.DirtyEntry entry, final InternalProcessorContext context)
{    if (flushListener != null) {        final byte[] rawNewValue = entry.newValue();        final byte[] rawOldValue = rawNewValue == null || sendOldValues ? wrapped().get(entry.key()) : null;        // we can skip flushing to downstream as well as writing to underlying store        if (rawNewValue != null || rawOldValue != null) {            // we need to get the old values if needed, and then put to store, and then flush            wrapped().put(entry.key(), entry.newValue());            final ProcessorRecordContext current = context.recordContext();            context.setRecordContext(entry.entry().context());            try {                flushListener.apply(entry.key().get(), rawNewValue, sendOldValues ? rawOldValue : null, entry.entry().context().timestamp());            } finally {                context.setRecordContext(current);            }        }    } else {        wrapped().put(entry.key(), entry.newValue());    }}
f14383
0
get
public byte[] kafkatest_f14391_0(final Bytes key)
{    Objects.requireNonNull(key, "key cannot be null");    validateStoreOpen();    final Lock theLock;    if (Thread.currentThread().equals(streamThread)) {        theLock = lock.writeLock();    } else {        theLock = lock.readLock();    }    theLock.lock();    try {        return getInternal(key);    } finally {        theLock.unlock();    }}
f14391
0
getInternal
private byte[] kafkatest_f14392_0(final Bytes key)
{    LRUCacheEntry entry = null;    if (cache != null) {        entry = cache.get(cacheName, key);    }    if (entry == null) {        final byte[] rawValue = wrapped().get(key);        if (rawValue == null) {            return null;        }        // as we don't want other threads to trigger an eviction/flush        if (Thread.currentThread().equals(streamThread)) {            cache.put(cacheName, key, new LRUCacheEntry(rawValue));        }        return rawValue;    } else {        return entry.value();    }}
f14392
0
range
public KeyValueIterator<Bytes, byte[]>f14393_1final Bytes from, final Bytes to)
{    if (from.compareTo(to) > 0) {                return KeyValueIterators.emptyIterator();    }    validateStoreOpen();    final KeyValueIterator<Bytes, byte[]> storeIterator = wrapped().range(from, to);    final ThreadCache.MemoryLRUCacheBytesIterator cacheIterator = cache.range(cacheName, from, to);    return new MergedSortedCacheKeyValueBytesStoreIterator(cacheIterator, storeIterator);}
public KeyValueIterator<Bytes, byte[]>f14393
1
setFlushListener
public boolean kafkatest_f14401_0(final CacheFlushListener<byte[], byte[]> flushListener, final boolean sendOldValues)
{    this.flushListener = flushListener;    this.sendOldValues = sendOldValues;    return true;}
f14401
0
put
public void kafkatest_f14402_0(final Windowed<Bytes> key, final byte[] value)
{    validateStoreOpen();    final Bytes binaryKey = SessionKeySchema.toBinary(key);    final LRUCacheEntry entry = new LRUCacheEntry(value, context.headers(), true, context.offset(), context.timestamp(), context.partition(), context.topic());    cache.put(cacheName, cacheFunction.cacheKey(binaryKey), entry);    maxObservedTimestamp = Math.max(keySchema.segmentTimestamp(binaryKey), maxObservedTimestamp);}
f14402
0
remove
public void kafkatest_f14403_0(final Windowed<Bytes> sessionKey)
{    validateStoreOpen();    put(sessionKey, null);}
f14403
0
hasNext
public boolean kafkatest_f14411_0()
{    if (current == null) {        return false;    }    if (current.hasNext()) {        return true;    }    while (!current.hasNext()) {        getNextSegmentIterator();        if (current == null) {            return false;        }    }    return true;}
f14411
0
peekNextKey
public Bytes kafkatest_f14412_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return current.peekNextKey();}
f14412
0
peekNext
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f14413_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return current.peekNext();}
f14413
0
segmentUpperRangeFixedSize
private Bytes kafkatest_f14421_0(final Bytes key, final long segmentEndTime)
{    final Windowed<Bytes> sessionKey = new Windowed<>(key, new SessionWindow(Math.min(latestSessionStartTime, segmentEndTime), segmentEndTime));    return SessionKeySchema.toBinary(sessionKey);}
f14421
0
init
public void kafkatest_f14422_0(final ProcessorContext context, final StateStore root)
{    initInternal((InternalProcessorContext) context);    super.init(context, root);}
f14422
0
initInternal
private void kafkatest_f14423_0(final InternalProcessorContext context)
{    this.context = context;    final String topic = ProcessorStateManager.storeChangelogTopic(context.applicationId(), name());    bytesSerdes = new StateSerdes<>(topic, Serdes.Bytes(), Serdes.ByteArray());    name = context.taskId() + "-" + name();    cache = this.context.getCache();    cache.addDirtyEntryFlushListener(name, entries -> {        for (final ThreadCache.DirtyEntry entry : entries) {            putAndMaybeForward(entry, context);        }    });}
f14423
0
fetchAll
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14431_0(final long timeFrom, final long timeTo)
{    validateStoreOpen();    final KeyValueIterator<Windowed<Bytes>, byte[]> underlyingIterator = wrapped().fetchAll(timeFrom, timeTo);    final ThreadCache.MemoryLRUCacheBytesIterator cacheIterator = cache.all(name);    final HasNextCondition hasNextCondition = keySchema.hasNextCondition(null, null, timeFrom, timeTo);    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> filteredCacheIterator = new FilteredCacheIterator(cacheIterator, hasNextCondition, cacheFunction);    return new MergedSortedCacheWindowStoreKeyValueIterator(filteredCacheIterator, underlyingIterator, bytesSerdes, windowSize, cacheFunction);}
f14431
0
all
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14432_0()
{    validateStoreOpen();    final KeyValueIterator<Windowed<Bytes>, byte[]> underlyingIterator = wrapped().all();    final ThreadCache.MemoryLRUCacheBytesIterator cacheIterator = cache.all(name);    return new MergedSortedCacheWindowStoreKeyValueIterator(cacheIterator, underlyingIterator, bytesSerdes, windowSize, cacheFunction);}
f14432
0
flush
public synchronized void kafkatest_f14433_0()
{    cache.flush(name);    wrapped().flush();}
f14433
0
currentSegmentLastTime
private long kafkatest_f14441_0()
{    return Math.min(timeTo, currentSegmentBeginTime() + segmentInterval - 1);}
f14441
0
getNextSegmentIterator
private void kafkatest_f14442_0()
{    ++currentSegmentId;    lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp));    if (currentSegmentId > lastSegmentId) {        current = null;        return;    }    setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());    current.close();    current = cache.range(name, cacheKeyFrom, cacheKeyTo);}
f14442
0
setCacheKeyRange
private void kafkatest_f14443_0(final long lowerRangeEndTime, final long upperRangeEndTime)
{    if (cacheFunction.segmentId(lowerRangeEndTime) != cacheFunction.segmentId(upperRangeEndTime)) {        throw new IllegalStateException("Error iterating over segments: segment interval has changed");    }    if (keyFrom == keyTo) {        cacheKeyFrom = cacheFunction.cacheKey(segmentLowerRangeFixedSize(keyFrom, lowerRangeEndTime));        cacheKeyTo = cacheFunction.cacheKey(segmentUpperRangeFixedSize(keyTo, upperRangeEndTime));    } else {        cacheKeyFrom = cacheFunction.cacheKey(keySchema.lowerRange(keyFrom, lowerRangeEndTime), currentSegmentId);        cacheKeyTo = cacheFunction.cacheKey(keySchema.upperRange(keyTo, timeTo), currentSegmentId);    }}
f14443
0
delete
public byte[] kafkatest_f14451_0(final Bytes key)
{    final byte[] oldValue = wrapped().delete(key);    log(key, null);    return oldValue;}
f14451
0
get
public byte[] kafkatest_f14452_0(final Bytes key)
{    return wrapped().get(key);}
f14452
0
range
public KeyValueIterator<Bytes, byte[]> kafkatest_f14453_0(final Bytes from, final Bytes to)
{    return wrapped().range(from, to);}
f14453
0
fetchSession
public byte[] kafkatest_f14461_0(final Bytes key, final long startTime, final long endTime)
{    return wrapped().fetchSession(key, startTime, endTime);}
f14461
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14462_0(final Bytes key)
{    return wrapped().fetch(key);}
f14462
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14463_0(final Bytes from, final Bytes to)
{    return wrapped().fetch(from, to);}
f14463
0
fetchAll
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14471_0(final long timeFrom, final long timeTo)
{    return wrapped().fetchAll(timeFrom, timeTo);}
f14471
0
put
public void kafkatest_f14472_0(final Bytes key, final byte[] value)
{    // Note: It's incorrect to bypass the wrapped store here by delegating to another method,    // but we have no alternative. We must send a timestamped key to the changelog, which means    // we need to know what timestamp gets used for the record. Hopefully, we can deprecate this    // method in the future to resolve the situation.    put(key, value, context.timestamp());}
f14472
0
put
public void kafkatest_f14473_0(final Bytes key, final byte[] value, final long windowStartTimestamp)
{    wrapped().put(key, value, windowStartTimestamp);    log(WindowKeySchema.toStoreKeyBinary(key, windowStartTimestamp, maybeUpdateSeqnumForDups()), value);}
f14473
0
range
public KeyValueIterator<K, V> kafkatest_f14481_0(final K from, final K to)
{    Objects.requireNonNull(from);    Objects.requireNonNull(to);    final NextIteratorFunction<K, V, ReadOnlyKeyValueStore<K, V>> nextIteratorFunction = new NextIteratorFunction<K, V, ReadOnlyKeyValueStore<K, V>>() {        @Override        public KeyValueIterator<K, V> apply(final ReadOnlyKeyValueStore<K, V> store) {            try {                return store.range(from, to);            } catch (final InvalidStateStoreException e) {                throw new InvalidStateStoreException("State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.");            }        }    };    final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);    return new DelegatingPeekingKeyValueIterator<>(storeName, new CompositeKeyValueIterator<>(stores.iterator(), nextIteratorFunction));}
f14481
0
apply
public KeyValueIterator<K, V> kafkatest_f14482_0(final ReadOnlyKeyValueStore<K, V> store)
{    try {        return store.range(from, to);    } catch (final InvalidStateStoreException e) {        throw new InvalidStateStoreException("State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.");    }}
f14482
0
all
public KeyValueIterator<K, V> kafkatest_f14483_0()
{    final NextIteratorFunction<K, V, ReadOnlyKeyValueStore<K, V>> nextIteratorFunction = new NextIteratorFunction<K, V, ReadOnlyKeyValueStore<K, V>>() {        @Override        public KeyValueIterator<K, V> apply(final ReadOnlyKeyValueStore<K, V> store) {            try {                return store.all();            } catch (final InvalidStateStoreException e) {                throw new InvalidStateStoreException("State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.");            }        }    };    final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);    return new DelegatingPeekingKeyValueIterator<>(storeName, new CompositeKeyValueIterator<>(stores.iterator(), nextIteratorFunction));}
f14483
0
fetch
public KeyValueIterator<Windowed<K>, V> kafkatest_f14491_0(final K from, final K to, final long timeFrom, final long timeTo)
{    Objects.requireNonNull(from, "from can't be null");    Objects.requireNonNull(to, "to can't be null");    final NextIteratorFunction<Windowed<K>, V, ReadOnlyWindowStore<K, V>> nextIteratorFunction = store -> store.fetch(from, to, timeFrom, timeTo);    return new DelegatingPeekingKeyValueIterator<>(storeName, new CompositeKeyValueIterator<>(provider.stores(storeName, windowStoreType).iterator(), nextIteratorFunction));}
f14491
0
fetch
public KeyValueIterator<Windowed<K>, V> kafkatest_f14492_0(final K from, final K to, final Instant fromTime, final Instant toTime) throws IllegalArgumentException
{    return fetch(from, to, ApiUtils.validateMillisecondInstant(fromTime, prepareMillisCheckFailMsgPrefix(fromTime, "fromTime")), ApiUtils.validateMillisecondInstant(toTime, prepareMillisCheckFailMsgPrefix(toTime, "toTime")));}
f14492
0
all
public KeyValueIterator<Windowed<K>, V> kafkatest_f14493_0()
{    final NextIteratorFunction<Windowed<K>, V, ReadOnlyWindowStore<K, V>> nextIteratorFunction = ReadOnlyWindowStore::all;    return new DelegatingPeekingKeyValueIterator<>(storeName, new CompositeKeyValueIterator<>(provider.stores(storeName, windowStoreType).iterator(), nextIteratorFunction));}
f14493
0
equals
public boolean kafkatest_f14501_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final ContextualRecord that = (ContextualRecord) o;    return Arrays.equals(value, that.value) && Objects.equals(recordContext, that.recordContext);}
f14501
0
hashCode
public int kafkatest_f14502_0()
{    return Objects.hash(value, recordContext);}
f14502
0
toString
public String kafkatest_f14503_0()
{    return "ContextualRecord{" + "recordContext=" + recordContext + ", value=" + Arrays.toString(value) + '}';}
f14503
0
peekNextKey
public Bytes kafkatest_f14511_0()
{    return cacheFunction.key(cacheIterator.peekNextKey());}
f14511
0
hasNext
public boolean kafkatest_f14512_0()
{    return cacheIterator.hasNext();}
f14512
0
next
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f14513_0()
{    return cachedPair(cacheIterator.next());}
f14513
0
name
public String kafkatest_f14521_0()
{    return name;}
f14521
0
init
public void kafkatest_f14522_0(final ProcessorContext context, final StateStore root)
{    size = 0;    if (root != null) {        // register the store        context.register(root, (key, value) -> put(Bytes.wrap(key), value));    }    open = true;}
f14522
0
persistent
public boolean kafkatest_f14523_0()
{    return false;}
f14523
0
all
public synchronized KeyValueIterator<Bytes, byte[]> kafkatest_f14531_0()
{    return new DelegatingPeekingKeyValueIterator<>(name, new InMemoryKeyValueIterator(map.keySet()));}
f14531
0
approximateNumEntries
public long kafkatest_f14532_0()
{    return size;}
f14532
0
flush
public void kafkatest_f14533_0()
{// do-nothing since it is in-memory}
f14533
0
metricsScope
public String kafkatest_f14541_0()
{    return "in-memory-session";}
f14541
0
segmentIntervalMs
public long kafkatest_f14542_0()
{    return 1;}
f14542
0
retentionPeriod
public long kafkatest_f14543_0()
{    return retentionPeriod;}
f14543
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14551_0(final Bytes key)
{    Objects.requireNonNull(key, "key cannot be null");    removeExpiredSegments();    return registerNewIterator(key, key, Long.MAX_VALUE, endTimeMap.entrySet().iterator());}
f14551
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14552_0(final Bytes from, final Bytes to)
{    Objects.requireNonNull(from, "from key cannot be null");    Objects.requireNonNull(to, "to key cannot be null");    removeExpiredSegments();    return registerNewIterator(from, to, Long.MAX_VALUE, endTimeMap.entrySet().iterator());}
f14552
0
persistent
public boolean kafkatest_f14553_0()
{    return false;}
f14553
0
next
public KeyValue<Windowed<Bytes>, byte[]> kafkatest_f14561_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    final KeyValue<Windowed<Bytes>, byte[]> ret = next;    next = null;    return ret;}
f14561
0
close
public void kafkatest_f14562_0()
{    next = null;    recordIterator = null;    callback.deregisterIterator(this);}
f14562
0
minTime
 Long kafkatest_f14563_0()
{    return currentEndTime;}
f14563
0
withLoggingDisabled
public StoreBuilder<InMemoryTimeOrderedKeyValueBuffer<K, V>> kafkatest_f14571_0()
{    loggingEnabled = false;    return this;}
f14571
0
build
public InMemoryTimeOrderedKeyValueBuffer<K, V> kafkatest_f14572_0()
{    return new InMemoryTimeOrderedKeyValueBuffer<>(storeName, loggingEnabled, keySerde, valSerde);}
f14572
0
logConfig
public Map<String, String> kafkatest_f14573_0()
{    return Collections.emptyMap();}
f14573
0
close
public void kafkatest_f14581_0()
{    open = false;    index.clear();    sortedMap.clear();    dirtyKeys.clear();    memBufferSize = 0;    minTimestamp = Long.MAX_VALUE;    updateBufferMetrics();}
f14581
0
flush
public void kafkatest_f14582_0()
{    if (loggingEnabled) {        // counting on this getting called before the record collector's flush        for (final Bytes key : dirtyKeys) {            final BufferKey bufferKey = index.get(key);            if (bufferKey == null) {                // The record was evicted from the buffer. Send a tombstone.                logTombstone(key);            } else {                final BufferValue value = sortedMap.get(bufferKey);                logValue(key, bufferKey, value);            }        }        dirtyKeys.clear();    }}
f14582
0
logValue
private void kafkatest_f14583_0(final Bytes key, final BufferKey bufferKey, final BufferValue value)
{    final int sizeOfBufferTime = Long.BYTES;    final ByteBuffer buffer = value.serialize(sizeOfBufferTime);    buffer.putLong(bufferKey.time());    collector.send(changelogTopic, key, buffer.array(), V_2_CHANGELOG_HEADERS, partition, null, KEY_SERIALIZER, VALUE_SERIALIZER);}
f14583
0
cleanPut
private void kafkatest_f14591_0(final long time, final Bytes key, final BufferValue value)
{    // non-resetting semantics:    // if there was a previous version of the same record,    // then insert the new record in the same place in the priority queue    final BufferKey previousKey = index.get(key);    if (previousKey == null) {        final BufferKey nextKey = new BufferKey(time, key);        index.put(key, nextKey);        sortedMap.put(nextKey, value);        minTimestamp = Math.min(minTimestamp, time);        memBufferSize += computeRecordSize(key, value);    } else {        final BufferValue removedValue = sortedMap.put(previousKey, value);        memBufferSize = memBufferSize + computeRecordSize(key, value) - (removedValue == null ? 0 : computeRecordSize(key, removedValue));    }}
f14591
0
numRecords
public int kafkatest_f14592_0()
{    return index.size();}
f14592
0
bufferSize
public long kafkatest_f14593_0()
{    return memBufferSize;}
f14593
0
segments
public int kafkatest_f14601_0()
{    throw new IllegalStateException("Segments is deprecated and should not be called");}
f14601
0
retentionPeriod
public long kafkatest_f14602_0()
{    return retentionPeriod;}
f14602
0
windowSize
public long kafkatest_f14603_0()
{    return windowSize;}
f14603
0
fetch
public WindowStoreIterator<byte[]> kafkatest_f14611_0(final Bytes key, final long timeFrom, final long timeTo)
{    Objects.requireNonNull(key, "key cannot be null");    removeExpiredSegments();    // add one b/c records expire exactly retentionPeriod ms after created    final long minTime = Math.max(timeFrom, observedStreamTime - retentionPeriod + 1);    if (timeTo < minTime) {        return WrappedInMemoryWindowStoreIterator.emptyIterator();    }    return registerNewWindowStoreIterator(key, segmentMap.subMap(minTime, true, timeTo, true).entrySet().iterator());}
f14611
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]>f14612_1final Bytes from, final Bytes to, final long timeFrom, final long timeTo)
{    Objects.requireNonNull(from, "from key cannot be null");    Objects.requireNonNull(to, "to key cannot be null");    removeExpiredSegments();    if (from.compareTo(to) > 0) {                return KeyValueIterators.emptyIterator();    }    // add one b/c records expire exactly retentionPeriod ms after created    final long minTime = Math.max(timeFrom, observedStreamTime - retentionPeriod + 1);    if (timeTo < minTime) {        return KeyValueIterators.emptyIterator();    }    return registerNewWindowedKeyValueIterator(from, to, segmentMap.subMap(minTime, true, timeTo, true).entrySet().iterator());}
public KeyValueIterator<Windowed<Bytes>, byte[]>f14612
1
fetchAll
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14613_0(final long timeFrom, final long timeTo)
{    removeExpiredSegments();    // add one b/c records expire exactly retentionPeriod ms after created    final long minTime = Math.max(timeFrom, observedStreamTime - retentionPeriod + 1);    if (timeTo < minTime) {        return KeyValueIterators.emptyIterator();    }    return registerNewWindowedKeyValueIterator(null, null, segmentMap.subMap(minTime, true, timeTo, true).entrySet().iterator());}
f14613
0
wrapForDups
private static Bytes kafkatest_f14621_0(final Bytes key, final int seqnum)
{    final ByteBuffer buf = ByteBuffer.allocate(key.get().length + SEQNUM_SIZE);    buf.put(key.get());    buf.putInt(seqnum);    return Bytes.wrap(buf.array());}
f14621
0
getKey
private static Bytes kafkatest_f14622_0(final Bytes keyBytes)
{    final byte[] bytes = new byte[keyBytes.get().length - SEQNUM_SIZE];    System.arraycopy(keyBytes.get(), 0, bytes, 0, bytes.length);    return Bytes.wrap(bytes);}
f14622
0
registerNewWindowStoreIterator
private WrappedInMemoryWindowStoreIterator kafkatest_f14623_0(final Bytes key, final Iterator<Map.Entry<Long, ConcurrentNavigableMap<Bytes, byte[]>>> segmentIterator)
{    final Bytes keyFrom = retainDuplicates ? wrapForDups(key, 0) : key;    final Bytes keyTo = retainDuplicates ? wrapForDups(key, Integer.MAX_VALUE) : key;    final WrappedInMemoryWindowStoreIterator iterator = new WrappedInMemoryWindowStoreIterator(keyFrom, keyTo, segmentIterator, openIterators::remove, retainDuplicates);    openIterators.add(iterator);    return iterator;}
f14623
0
next
public KeyValue<Long, byte[]> kafkatest_f14631_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    final KeyValue<Long, byte[]> result = new KeyValue<>(super.currentTime, super.next.value);    super.next = null;    return result;}
f14631
0
emptyIterator
public static WrappedInMemoryWindowStoreIterator kafkatest_f14632_0()
{    return new WrappedInMemoryWindowStoreIterator(null, null, null, it -> {    }, false);}
f14632
0
peekNextKey
public Windowed<Bytes> kafkatest_f14633_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return getWindowedKey();}
f14633
0
hasNext
public boolean kafkatest_f14642_0()
{    return false;}
f14642
0
next
public KeyValue<K, V> kafkatest_f14643_0()
{    throw new NoSuchElementException();}
f14643
0
emptyIterator
 static KeyValueIterator<K, V> kafkatest_f14644_0()
{    return (KeyValueIterator<K, V>) EMPTY_ITERATOR;}
f14644
0
getOrCreateSegment
public KeyValueSegment kafkatest_f14652_0(final long segmentId, final InternalProcessorContext context)
{    if (segments.containsKey(segmentId)) {        return segments.get(segmentId);    } else {        final KeyValueSegment newSegment = new KeyValueSegment(segmentName(segmentId), name, segmentId, metricsRecorder);        if (segments.put(segmentId, newSegment) != null) {            throw new IllegalStateException("KeyValueSegment already exists. Possible concurrent access.");        }        newSegment.openDB(context);        return newSegment;    }}
f14652
0
close
public void kafkatest_f14653_0()
{    metricsRecorder.close();    super.close();}
f14653
0
build
public KeyValueStore<K, V> kafkatest_f14654_0()
{    return new MeteredKeyValueStore<>(maybeWrapCaching(maybeWrapLogging(storeSupplier.get())), storeSupplier.metricsScope(), time, keySerde, valueSerde);}
f14654
0
init
public void kafkatest_f14662_0(final ProcessorContext context, final StateStore root)
{    store.init(context, root);}
f14662
0
flush
public void kafkatest_f14663_0()
{    store.flush();}
f14663
0
close
public void kafkatest_f14664_0()
{    store.close();}
f14664
0
peekNextKey
public K kafkatest_f14672_0()
{    return innerIterator.peekNextKey();}
f14672
0
hasNext
public boolean kafkatest_f14673_0()
{    return innerIterator.hasNext();}
f14673
0
next
public KeyValue<K, byte[]> kafkatest_f14674_0()
{    final KeyValue<K, byte[]> plainKeyValue = innerIterator.next();    return KeyValue.pair(plainKeyValue.key, convertToTimestampedFormat(plainKeyValue.value));}
f14674
0
defined
public static Maybe<T> kafkatest_f14682_0(final T nullableValue)
{    return new Maybe<>(nullableValue);}
f14682
0
undefined
public static Maybe<T> kafkatest_f14683_0()
{    return new Maybe<>();}
f14683
0
getNullableValue
public T kafkatest_f14684_0()
{    if (defined) {        return nullableValue;    } else {        throw new NoSuchElementException();    }}
f14684
0
init
public void kafkatest_f14692_0(final ProcessorContext context, final StateStore root)
{    // register the store    context.register(root, (key, value) -> {        restoring = true;        put(Bytes.wrap(key), value);        restoring = false;    });}
f14692
0
persistent
public boolean kafkatest_f14693_0()
{    return false;}
f14693
0
isOpen
public boolean kafkatest_f14694_0()
{    return open;}
f14694
0
approximateNumEntries
public long kafkatest_f14702_0()
{    return this.map.size();}
f14702
0
flush
public void kafkatest_f14703_0()
{// do-nothing since it is in-memory}
f14703
0
close
public void kafkatest_f14704_0()
{    open = false;}
f14704
0
peekNextKey
public Bytes kafkatest_f14712_0()
{    throw new UnsupportedOperationException("peekNextKey not supported");}
f14712
0
deserializeStorePair
public KeyValue<Bytes, byte[]> kafkatest_f14713_0(final KeyValue<Bytes, byte[]> pair)
{    return pair;}
f14713
0
deserializeCacheKey
 Bytes kafkatest_f14714_0(final Bytes cacheKey)
{    return cacheKey;}
f14714
0
compare
public int kafkatest_f14722_0(final Bytes cacheKey, final Windowed<Bytes> storeKey)
{    final Bytes storeKeyBytes = SessionKeySchema.toBinary(storeKey);    return cacheFunction.compareSegmentedKeys(cacheKey, storeKeyBytes);}
f14722
0
deserializeStorePair
public KeyValue<Long, byte[]> kafkatest_f14723_0(final KeyValue<Long, byte[]> pair)
{    return pair;}
f14723
0
deserializeCacheKey
 Long kafkatest_f14724_0(final Bytes cacheKey)
{    final byte[] binaryKey = bytesFromCacheKey(cacheKey);    return WindowKeySchema.extractStoreTimestamp(binaryKey);}
f14724
0
compare
 int kafkatest_f14732_0(final Bytes cacheKey, final Windowed<Bytes> storeKey)
{    final Bytes storeKeyBytes = WindowKeySchema.toStoreKeyBinary(storeKey.key(), storeKey.window().start(), 0);    return cacheFunction.compareSegmentedKeys(cacheKey, storeKeyBytes);}
f14732
0
init
public void kafkatest_f14733_0(final ProcessorContext context, final StateStore root)
{    metrics = (StreamsMetricsImpl) context.metrics();    taskName = context.taskId().toString();    final String metricsGroup = "stream-" + metricsScope + "-state-metrics";    final Map<String, String> taskTags = metrics.storeLevelTagMap(taskName, metricsScope, ROLLUP_VALUE);    final Map<String, String> storeTags = metrics.storeLevelTagMap(taskName, metricsScope, name());    initStoreSerde(context);    putTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "put", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    putIfAbsentTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "put-if-absent", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    putAllTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "put-all", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    getTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "get", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    allTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "all", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    rangeTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "range", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    flushTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "flush", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    deleteTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "delete", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    final Sensor restoreTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "restore", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    // register and possibly restore the state from the logs    if (restoreTime.shouldRecord()) {        measureLatency(() -> {            super.init(context, root);            return null;        }, restoreTime);    } else {        super.init(context, root);    }}
f14733
0
initStoreSerde
 void kafkatest_f14734_0(final ProcessorContext context)
{    serdes = new StateSerdes<>(ProcessorStateManager.storeChangelogTopic(context.applicationId(), name()), keySerde == null ? (Serde<K>) context.keySerde() : keySerde, valueSerde == null ? (Serde<V>) context.valueSerde() : valueSerde);}
f14734
0
all
public KeyValueIterator<K, V> kafkatest_f14742_0()
{    return new MeteredKeyValueIterator(wrapped().all(), allTime);}
f14742
0
flush
public void kafkatest_f14743_0()
{    if (flushTime.shouldRecord()) {        measureLatency(() -> {            super.flush();            return null;        }, flushTime);    } else {        super.flush();    }}
f14743
0
approximateNumEntries
public long kafkatest_f14744_0()
{    return wrapped().approximateNumEntries();}
f14744
0
close
public void kafkatest_f14752_0()
{    try {        iter.close();    } finally {        metrics.recordLatency(sensor, startNs, time.nanoseconds());    }}
f14752
0
peekNextKey
public K kafkatest_f14753_0()
{    return serdes.keyFrom(iter.peekNextKey().get());}
f14753
0
init
public void kafkatest_f14754_0(final ProcessorContext context, final StateStore root)
{    // noinspection unchecked    serdes = new StateSerdes<>(ProcessorStateManager.storeChangelogTopic(context.applicationId(), name()), keySerde == null ? (Serde<K>) context.keySerde() : keySerde, valueSerde == null ? (Serde<V>) context.valueSerde() : valueSerde);    metrics = (StreamsMetricsImpl) context.metrics();    taskName = context.taskId().toString();    final String metricsGroup = "stream-" + metricsScope + "-state-metrics";    final Map<String, String> taskTags = metrics.storeLevelTagMap(taskName, metricsScope, ROLLUP_VALUE);    final Map<String, String> storeTags = metrics.storeLevelTagMap(taskName, metricsScope, name());    putTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "put", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    fetchTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "fetch", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    flushTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "flush", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    removeTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "remove", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    final Sensor restoreTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "restore", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    // register and possibly restore the state from the logs    final long startNs = time.nanoseconds();    try {        super.init(context, root);    } finally {        metrics.recordLatency(restoreTime, startNs, time.nanoseconds());    }}
f14754
0
findSessions
public KeyValueIterator<Windowed<K>, V> kafkatest_f14762_0(final K keyFrom, final K keyTo, final long earliestSessionEndTime, final long latestSessionStartTime)
{    Objects.requireNonNull(keyFrom, "keyFrom cannot be null");    Objects.requireNonNull(keyTo, "keyTo cannot be null");    final Bytes bytesKeyFrom = keyBytes(keyFrom);    final Bytes bytesKeyTo = keyBytes(keyTo);    return new MeteredWindowedKeyValueIterator<>(wrapped().findSessions(bytesKeyFrom, bytesKeyTo, earliestSessionEndTime, latestSessionStartTime), fetchTime, metrics, serdes, time);}
f14762
0
flush
public void kafkatest_f14763_0()
{    final long startNs = time.nanoseconds();    try {        super.flush();    } finally {        metrics.recordLatency(flushTime, startNs, time.nanoseconds());    }}
f14763
0
close
public void kafkatest_f14764_0()
{    super.close();    metrics.removeAllStoreLevelSensors(taskName, name());}
f14764
0
peekNextKey
public Windowed<K> kafkatest_f14772_0()
{    return windowedKey(iter.peekNextKey());}
f14772
0
init
public void kafkatest_f14773_0(final ProcessorContext context, final StateStore root)
{    this.context = context;    initStoreSerde(context);    metrics = (StreamsMetricsImpl) context.metrics();    taskName = context.taskId().toString();    final String metricsGroup = GROUP_PREFIX + metricsScope + STATE_LEVEL_GROUP_SUFFIX;    final Map<String, String> taskTags = metrics.storeLevelTagMap(taskName, metricsScope, ROLLUP_VALUE);    final Map<String, String> storeTags = metrics.storeLevelTagMap(taskName, metricsScope, name());    putTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "put", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    fetchTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "fetch", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    flushTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "flush", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    final Sensor restoreTime = createTaskAndStoreLatencyAndThroughputSensors(DEBUG, "restore", metrics, metricsGroup, taskName, name(), taskTags, storeTags);    // register and possibly restore the state from the logs    final long startNs = time.nanoseconds();    try {        super.init(context, root);    } finally {        metrics.recordLatency(restoreTime, startNs, time.nanoseconds());    }}
f14773
0
initStoreSerde
 void kafkatest_f14774_0(final ProcessorContext context)
{    serdes = new StateSerdes<>(ProcessorStateManager.storeChangelogTopic(context.applicationId(), name()), keySerde == null ? (Serde<K>) context.keySerde() : keySerde, valueSerde == null ? (Serde<V>) context.valueSerde() : valueSerde);}
f14774
0
all
public KeyValueIterator<Windowed<K>, V> kafkatest_f14782_0()
{    return new MeteredWindowedKeyValueIterator<>(wrapped().all(), fetchTime, metrics, serdes, time);}
f14782
0
flush
public void kafkatest_f14783_0()
{    final long startNs = time.nanoseconds();    try {        super.flush();    } finally {        metrics.recordLatency(flushTime, startNs, time.nanoseconds());    }}
f14783
0
close
public void kafkatest_f14784_0()
{    super.close();    metrics.removeAllStoreLevelSensors(taskName, name());}
f14784
0
storeName
public String kafkatest_f14792_0()
{    return storeName;}
f14792
0
equals
public boolean kafkatest_f14793_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final RocksDBMetricContext that = (RocksDBMetricContext) o;    return Objects.equals(taskName, that.taskName) && Objects.equals(metricsScope, that.metricsScope) && Objects.equals(storeName, that.storeName);}
f14793
0
hashCode
public int kafkatest_f14794_0()
{    return Objects.hash(taskName, metricsScope, storeName);}
f14794
0
writeStallDurationSensor
public static Sensor kafkatest_f14802_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, WRITE_STALL_DURATION);    addAvgAndSumMetricsToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), WRITE_STALL_DURATION, WRITE_STALL_DURATION_AVG_DESCRIPTION, WRITE_STALL_DURATION_TOTAL_DESCRIPTION);    return sensor;}
f14802
0
blockCacheDataHitRatioSensor
public static Sensor kafkatest_f14803_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, BLOCK_CACHE_DATA_HIT_RATIO);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), BLOCK_CACHE_DATA_HIT_RATIO, BLOCK_CACHE_DATA_HIT_RATIO_DESCRIPTION);    return sensor;}
f14803
0
blockCacheIndexHitRatioSensor
public static Sensor kafkatest_f14804_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, BLOCK_CACHE_INDEX_HIT_RATIO);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), BLOCK_CACHE_INDEX_HIT_RATIO, BLOCK_CACHE_INDEX_HIT_RATIO_DESCRIPTION);    return sensor;}
f14804
0
numberOfFileErrorsSensor
public static Sensor kafkatest_f14812_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, NUMBER_OF_FILE_ERRORS);    addSumMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), NUMBER_OF_FILE_ERRORS, NUMBER_OF_FILE_ERRORS_DESCRIPTION);    return sensor;}
f14812
0
createSensor
private static Sensor kafkatest_f14813_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext, final String sensorName)
{    return streamsMetrics.storeLevelSensor(metricContext.taskName(), metricContext.storeName(), sensorName, RecordingLevel.DEBUG);}
f14813
0
init
private void kafkatest_f14814_0(final StreamsMetricsImpl streamsMetrics, final TaskId taskId)
{    final RocksDBMetricContext metricContext = new RocksDBMetricContext(taskId.toString(), metricsScope, storeName);    bytesWrittenToDatabaseSensor = RocksDBMetrics.bytesWrittenToDatabaseSensor(streamsMetrics, metricContext);    bytesReadToDatabaseSensor = RocksDBMetrics.bytesReadFromDatabaseSensor(streamsMetrics, metricContext);    memtableBytesFlushedSensor = RocksDBMetrics.memtableBytesFlushedSensor(streamsMetrics, metricContext);    memtableHitRatioSensor = RocksDBMetrics.memtableHitRatioSensor(streamsMetrics, metricContext);    memtableAvgFlushTimeSensor = RocksDBMetrics.memtableAvgFlushTimeSensor(streamsMetrics, metricContext);    memtableMinFlushTimeSensor = RocksDBMetrics.memtableMinFlushTimeSensor(streamsMetrics, metricContext);    memtableMaxFlushTimeSensor = RocksDBMetrics.memtableMaxFlushTimeSensor(streamsMetrics, metricContext);    writeStallDurationSensor = RocksDBMetrics.writeStallDurationSensor(streamsMetrics, metricContext);    blockCacheDataHitRatioSensor = RocksDBMetrics.blockCacheDataHitRatioSensor(streamsMetrics, metricContext);    blockCacheIndexHitRatioSensor = RocksDBMetrics.blockCacheIndexHitRatioSensor(streamsMetrics, metricContext);    blockCacheFilterHitRatioSensor = RocksDBMetrics.blockCacheFilterHitRatioSensor(streamsMetrics, metricContext);    bytesReadDuringCompactionSensor = RocksDBMetrics.bytesReadDuringCompactionSensor(streamsMetrics, metricContext);    bytesWrittenDuringCompactionSensor = RocksDBMetrics.bytesWrittenDuringCompactionSensor(streamsMetrics, metricContext);    compactionTimeAvgSensor = RocksDBMetrics.compactionTimeAvgSensor(streamsMetrics, metricContext);    compactionTimeMinSensor = RocksDBMetrics.compactionTimeMinSensor(streamsMetrics, metricContext);    compactionTimeMaxSensor = RocksDBMetrics.compactionTimeMaxSensor(streamsMetrics, metricContext);    numberOfOpenFilesSensor = RocksDBMetrics.numberOfOpenFilesSensor(streamsMetrics, metricContext);    numberOfFileErrorsSensor = RocksDBMetrics.numberOfFileErrorsSensor(streamsMetrics, metricContext);}
f14814
0
getBufferSizeOrCountSensor
private static Sensor kafkatest_f14822_0(final StateStore store, final InternalProcessorContext context, final String property)
{    final StreamsMetricsImpl metrics = context.metrics();    final String sensorName = "suppression-buffer-" + property;    final Sensor sensor = metrics.storeLevelSensor(context.taskId().toString(), store.name(), sensorName, Sensor.RecordingLevel.DEBUG);    final String metricsGroup = "stream-buffer-metrics";    final Map<String, String> tags = metrics.tagMap("task-id", context.taskId().toString(), "buffer-id", store.name());    sensor.add(new MetricName(sensorName + "-current", metricsGroup, "The current " + property + " of buffered records.", tags), new Value());    sensor.add(new MetricName(sensorName + "-avg", metricsGroup, "The average " + property + " of buffered records.", tags), new Avg());    sensor.add(new MetricName(sensorName + "-max", metricsGroup, "The max " + property + " of buffered records.", tags), new Max());    return sensor;}
f14822
0
name
 final synchronized String kafkatest_f14823_0()
{    return name;}
f14823
0
hits
 synchronized long kafkatest_f14824_0()
{    return numReadHits;}
f14824
0
put
 synchronized void kafkatest_f14832_0(final Bytes key, final LRUCacheEntry value)
{    if (!value.isDirty() && dirtyKeys.contains(key)) {        throw new IllegalStateException(String.format("Attempting to put a clean entry for key [%s] into NamedCache [%s] when it already contains a dirty entry for the same key", key, name));    }    LRUNode node = cache.get(key);    if (node != null) {        numOverwrites++;        currentSizeBytes -= node.size();        node.update(value);        updateLRU(node);    } else {        node = new LRUNode(key, value);        // put element        putHead(node);        cache.put(key, node);    }    if (value.isDirty()) {        // first remove and then add so we can maintain ordering as the arrival order of the records.        dirtyKeys.remove(key);        dirtyKeys.add(key);    }    currentSizeBytes += node.size();}
f14832
0
sizeInBytes
 synchronized long kafkatest_f14833_0()
{    return currentSizeBytes;}
f14833
0
getInternal
private LRUNode kafkatest_f14834_0(final Bytes key)
{    final LRUNode node = cache.get(key);    if (node == null) {        numReadMisses++;        return null;    } else {        numReadHits++;        namedCacheMetrics.hitRatioSensor.record((double) numReadHits / (double) (numReadHits + numReadMisses));    }    return node;}
f14834
0
size
public long kafkatest_f14842_0()
{    return cache.size();}
f14842
0
isEmpty
public boolean kafkatest_f14843_0()
{    return cache.isEmpty();}
f14843
0
keyRange
 synchronized Iterator<Bytes> kafkatest_f14844_0(final Bytes from, final Bytes to)
{    return keySetIterator(cache.navigableKeySet().subSet(from, true, to, true));}
f14844
0
entry
 LRUCacheEntry kafkatest_f14852_0()
{    return entry;}
f14852
0
key
 Bytes kafkatest_f14853_0()
{    return key;}
f14853
0
size
 long kafkatest_f14854_0()
{    return key.get().length + // entry    8 + // previous    8 + // next    8 + entry.size();}
f14854
0
read
public Map<TopicPartition, Long> kafkatest_f14862_0() throws IOException
{    synchronized (lock) {        try (final BufferedReader reader = Files.newBufferedReader(file.toPath())) {            final int version = readInt(reader);            switch(version) {                case 0:                    final int expectedSize = readInt(reader);                    final Map<TopicPartition, Long> offsets = new HashMap<>();                    String line = reader.readLine();                    while (line != null) {                        final String[] pieces = WHITESPACE_MINIMUM_ONCE.split(line);                        if (pieces.length != 3) {                            throw new IOException(String.format("Malformed line in offset checkpoint file: '%s'.", line));                        }                        final String topic = pieces[0];                        final int partition = Integer.parseInt(pieces[1]);                        final long offset = Long.parseLong(pieces[2]);                        offsets.put(new TopicPartition(topic, partition), offset);                        line = reader.readLine();                    }                    if (offsets.size() != expectedSize) {                        throw new IOException(String.format("Expected %d entries but found only %d", expectedSize, offsets.size()));                    }                    return offsets;                default:                    throw new IllegalArgumentException("Unknown offset checkpoint version: " + version);            }        } catch (final NoSuchFileException e) {            return Collections.emptyMap();        }    }}
f14862
0
readInt
private int kafkatest_f14863_0(final BufferedReader reader) throws IOException
{    final String line = reader.readLine();    if (line == null) {        throw new EOFException("File ended prematurely.");    }    return Integer.parseInt(line);}
f14863
0
delete
public void kafkatest_f14864_0() throws IOException
{    Files.deleteIfExists(file.toPath());}
f14864
0
approximateNumEntries
public long kafkatest_f14872_0()
{    return inner.approximateNumEntries();}
f14872
0
fetch
public V kafkatest_f14873_0(final K key, final long time)
{    return getValueOrNull(inner.fetch(key, time));}
f14873
0
fetch
public WindowStoreIterator<V> kafkatest_f14874_0(final K key, final long timeFrom, final long timeTo)
{    return new WindowStoreIteratorFacade<>(inner.fetch(key, timeFrom, timeTo));}
f14874
0
peekNextKey
public Long kafkatest_f14882_0()
{    return innerIterator.peekNextKey();}
f14882
0
hasNext
public boolean kafkatest_f14883_0()
{    return innerIterator.hasNext();}
f14883
0
next
public KeyValue<Long, V> kafkatest_f14884_0()
{    final KeyValue<Long, ValueAndTimestamp<V>> innerKeyValue = innerIterator.next();    return KeyValue.pair(innerKeyValue.key, getValueOrNull(innerKeyValue.value));}
f14884
0
prepareForBulkLoad
public Options kafkatest_f14892_0()
{    /* From https://github.com/facebook/rocksdb/wiki/RocksDB-FAQ         *         * Q: What's the fastest way to load data into RocksDB?         *         * A: A fast way to direct insert data to the DB:         *         *  1. using single writer thread and insert in sorted order         *  2. batch hundreds of keys into one write batch         *  3. use vector memtable         *  4. make sure options.max_background_flushes is at least 4         *  5. before inserting the data,         *       disable automatic compaction,         *       set options.level0_file_num_compaction_trigger,         *           options.level0_slowdown_writes_trigger         *           and options.level0_stop_writes_trigger to very large.         *     After inserting all the data, issue a manual compaction.         *         * 3-5 will be automatically done if you call Options::PrepareForBulkLoad() to your option         */    // (1) not in our control    // (2) is done via bulk-loading API    // (3) skipping because, not done in actual PrepareForBulkLoad() code in https://github.com/facebook/rocksdb/blob/master/options/options.cc    // columnFamilyOptions.setMemTableConfig(new VectorMemTableConfig());    // (4-5) below:    dbOptions.setMaxBackgroundFlushes(4);    columnFamilyOptions.setDisableAutoCompactions(true);    columnFamilyOptions.setLevel0FileNumCompactionTrigger(1 << 30);    columnFamilyOptions.setLevel0SlowdownWritesTrigger(1 << 30);    columnFamilyOptions.setLevel0StopWritesTrigger(1 << 30);    return this;}
f14892
0
createIfMissing
public boolean kafkatest_f14893_0()
{    return dbOptions.createIfMissing();}
f14893
0
createMissingColumnFamilies
public boolean kafkatest_f14894_0()
{    return dbOptions.createMissingColumnFamilies();}
f14894
0
setComparator
public Options kafkatest_f14902_0(final AbstractComparator<? extends AbstractSlice<?>> comparator)
{    columnFamilyOptions.setComparator(comparator);    return this;}
f14902
0
setMergeOperatorName
public Options kafkatest_f14903_0(final String name)
{    columnFamilyOptions.setMergeOperatorName(name);    return this;}
f14903
0
setMergeOperator
public Options kafkatest_f14904_0(final MergeOperator mergeOperator)
{    columnFamilyOptions.setMergeOperator(mergeOperator);    return this;}
f14904
0
setParanoidChecks
public Options kafkatest_f14912_0(final boolean paranoidChecks)
{    columnFamilyOptions.paranoidFileChecks();    dbOptions.setParanoidChecks(paranoidChecks);    return this;}
f14912
0
maxOpenFiles
public int kafkatest_f14913_0()
{    return dbOptions.maxOpenFiles();}
f14913
0
setMaxFileOpeningThreads
public Options kafkatest_f14914_0(final int maxFileOpeningThreads)
{    dbOptions.setMaxFileOpeningThreads(maxFileOpeningThreads);    return this;}
f14914
0
dbPaths
public List<DbPath> kafkatest_f14922_0()
{    return dbOptions.dbPaths();}
f14922
0
dbLogDir
public String kafkatest_f14923_0()
{    return dbOptions.dbLogDir();}
f14923
0
setDbLogDir
public Options kafkatest_f14924_0(final String dbLogDir)
{    dbOptions.setDbLogDir(dbLogDir);    return this;}
f14924
0
setBaseBackgroundCompactions
public void kafkatest_f14932_0(final int baseBackgroundCompactions)
{    dbOptions.setBaseBackgroundCompactions(baseBackgroundCompactions);}
f14932
0
baseBackgroundCompactions
public int kafkatest_f14933_0()
{    return dbOptions.baseBackgroundCompactions();}
f14933
0
setMaxBackgroundCompactions
public Options kafkatest_f14934_0(final int maxBackgroundCompactions)
{    dbOptions.setMaxBackgroundCompactions(maxBackgroundCompactions);    return this;}
f14934
0
setMaxLogFileSize
public Options kafkatest_f14942_0(final long maxLogFileSize)
{    dbOptions.setMaxLogFileSize(maxLogFileSize);    return this;}
f14942
0
logFileTimeToRoll
public long kafkatest_f14943_0()
{    return dbOptions.logFileTimeToRoll();}
f14943
0
setLogFileTimeToRoll
public Options kafkatest_f14944_0(final long logFileTimeToRoll)
{    dbOptions.setLogFileTimeToRoll(logFileTimeToRoll);    return this;}
f14944
0
maxTableFilesSizeFIFO
public long kafkatest_f14952_0()
{    return columnFamilyOptions.maxTableFilesSizeFIFO();}
f14952
0
tableCacheNumshardbits
public int kafkatest_f14953_0()
{    return dbOptions.tableCacheNumshardbits();}
f14953
0
setTableCacheNumshardbits
public Options kafkatest_f14954_0(final int tableCacheNumshardbits)
{    dbOptions.setTableCacheNumshardbits(tableCacheNumshardbits);    return this;}
f14954
0
useDirectReads
public boolean kafkatest_f14962_0()
{    return dbOptions.useDirectReads();}
f14962
0
setUseDirectIoForFlushAndCompaction
public Options kafkatest_f14963_0(final boolean useDirectIoForFlushAndCompaction)
{    dbOptions.setUseDirectIoForFlushAndCompaction(useDirectIoForFlushAndCompaction);    return this;}
f14963
0
useDirectIoForFlushAndCompaction
public boolean kafkatest_f14964_0()
{    return dbOptions.useDirectIoForFlushAndCompaction();}
f14964
0
setIsFdCloseOnExec
public Options kafkatest_f14972_0(final boolean isFdCloseOnExec)
{    dbOptions.setIsFdCloseOnExec(isFdCloseOnExec);    return this;}
f14972
0
statsDumpPeriodSec
public int kafkatest_f14973_0()
{    return dbOptions.statsDumpPeriodSec();}
f14973
0
setStatsDumpPeriodSec
public Options kafkatest_f14974_0(final int statsDumpPeriodSec)
{    dbOptions.setStatsDumpPeriodSec(statsDumpPeriodSec);    return this;}
f14974
0
newTableReaderForCompactionInputs
public boolean kafkatest_f14982_0()
{    return dbOptions.newTableReaderForCompactionInputs();}
f14982
0
setCompactionReadaheadSize
public Options kafkatest_f14983_0(final long compactionReadaheadSize)
{    dbOptions.setCompactionReadaheadSize(compactionReadaheadSize);    return this;}
f14983
0
compactionReadaheadSize
public long kafkatest_f14984_0()
{    return dbOptions.compactionReadaheadSize();}
f14984
0
setBytesPerSync
public Options kafkatest_f14992_0(final long bytesPerSync)
{    dbOptions.setBytesPerSync(bytesPerSync);    return this;}
f14992
0
setWalBytesPerSync
public Options kafkatest_f14993_0(final long walBytesPerSync)
{    dbOptions.setWalBytesPerSync(walBytesPerSync);    return this;}
f14993
0
walBytesPerSync
public long kafkatest_f14994_0()
{    return dbOptions.walBytesPerSync();}
f14994
0
enableWriteThreadAdaptiveYield
public boolean kafkatest_f15002_0()
{    return dbOptions.enableWriteThreadAdaptiveYield();}
f15002
0
setWriteThreadMaxYieldUsec
public Options kafkatest_f15003_0(final long writeThreadMaxYieldUsec)
{    dbOptions.setWriteThreadMaxYieldUsec(writeThreadMaxYieldUsec);    return this;}
f15003
0
writeThreadMaxYieldUsec
public long kafkatest_f15004_0()
{    return dbOptions.writeThreadMaxYieldUsec();}
f15004
0
allow2pc
public boolean kafkatest_f15012_0()
{    return dbOptions.allow2pc();}
f15012
0
setRowCache
public Options kafkatest_f15013_0(final Cache rowCache)
{    dbOptions.setRowCache(rowCache);    return this;}
f15013
0
rowCache
public Cache kafkatest_f15014_0()
{    return dbOptions.rowCache();}
f15014
0
avoidFlushDuringShutdown
public boolean kafkatest_f15022_0()
{    return dbOptions.avoidFlushDuringShutdown();}
f15022
0
memTableConfig
public MemTableConfig kafkatest_f15023_0()
{    return columnFamilyOptions.memTableConfig();}
f15023
0
setMemTableConfig
public Options kafkatest_f15024_0(final MemTableConfig config)
{    columnFamilyOptions.setMemTableConfig(config);    return this;}
f15024
0
setTableFormatConfig
public Options kafkatest_f15032_0(final TableFormatConfig config)
{    columnFamilyOptions.setTableFormatConfig(config);    return this;}
f15032
0
tableFactoryName
public String kafkatest_f15033_0()
{    return columnFamilyOptions.tableFactoryName();}
f15033
0
useFixedLengthPrefixExtractor
public Options kafkatest_f15034_0(final int n)
{    columnFamilyOptions.useFixedLengthPrefixExtractor(n);    return this;}
f15034
0
setCompressionOptions
public Options kafkatest_f15042_0(final CompressionOptions compressionOptions)
{    columnFamilyOptions.setCompressionOptions(compressionOptions);    return this;}
f15042
0
compressionOptions
public CompressionOptions kafkatest_f15043_0()
{    return columnFamilyOptions.compressionOptions();}
f15043
0
compactionStyle
public CompactionStyle kafkatest_f15044_0()
{    return columnFamilyOptions.compactionStyle();}
f15044
0
levelZeroStopWritesTrigger
public int kafkatest_f15052_0()
{    return columnFamilyOptions.levelZeroStopWritesTrigger();}
f15052
0
setLevelZeroStopWritesTrigger
public Options kafkatest_f15053_0(final int numFiles)
{    columnFamilyOptions.setLevelZeroStopWritesTrigger(numFiles);    return this;}
f15053
0
targetFileSizeBase
public long kafkatest_f15054_0()
{    return columnFamilyOptions.targetFileSizeBase();}
f15054
0
maxBytesForLevelMultiplier
public double kafkatest_f15062_0()
{    return columnFamilyOptions.maxBytesForLevelMultiplier();}
f15062
0
setMaxBytesForLevelMultiplier
public Options kafkatest_f15063_0(final double multiplier)
{    columnFamilyOptions.setMaxBytesForLevelMultiplier(multiplier);    return this;}
f15063
0
maxCompactionBytes
public long kafkatest_f15064_0()
{    return columnFamilyOptions.maxCompactionBytes();}
f15064
0
inplaceUpdateSupport
public boolean kafkatest_f15072_0()
{    return columnFamilyOptions.inplaceUpdateSupport();}
f15072
0
setInplaceUpdateSupport
public Options kafkatest_f15073_0(final boolean inplaceUpdateSupport)
{    columnFamilyOptions.setInplaceUpdateSupport(inplaceUpdateSupport);    return this;}
f15073
0
inplaceUpdateNumLocks
public long kafkatest_f15074_0()
{    return columnFamilyOptions.inplaceUpdateNumLocks();}
f15074
0
minWriteBufferNumberToMerge
public int kafkatest_f15082_0()
{    return columnFamilyOptions.minWriteBufferNumberToMerge();}
f15082
0
setMinWriteBufferNumberToMerge
public Options kafkatest_f15083_0(final int minWriteBufferNumberToMerge)
{    columnFamilyOptions.setMinWriteBufferNumberToMerge(minWriteBufferNumberToMerge);    return this;}
f15083
0
setOptimizeFiltersForHits
public Options kafkatest_f15084_0(final boolean optimizeFiltersForHits)
{    columnFamilyOptions.setOptimizeFiltersForHits(optimizeFiltersForHits);    return this;}
f15084
0
setLevel0FileNumCompactionTrigger
public Options kafkatest_f15092_0(final int level0FileNumCompactionTrigger)
{    columnFamilyOptions.setLevel0FileNumCompactionTrigger(level0FileNumCompactionTrigger);    return this;}
f15092
0
level0FileNumCompactionTrigger
public int kafkatest_f15093_0()
{    return columnFamilyOptions.level0FileNumCompactionTrigger();}
f15093
0
setLevel0SlowdownWritesTrigger
public Options kafkatest_f15094_0(final int level0SlowdownWritesTrigger)
{    columnFamilyOptions.setLevel0SlowdownWritesTrigger(level0SlowdownWritesTrigger);    return this;}
f15094
0
setMaxWriteBufferNumberToMaintain
public Options kafkatest_f15102_0(final int maxWriteBufferNumberToMaintain)
{    columnFamilyOptions.setMaxWriteBufferNumberToMaintain(maxWriteBufferNumberToMaintain);    return this;}
f15102
0
maxWriteBufferNumberToMaintain
public int kafkatest_f15103_0()
{    return columnFamilyOptions.maxWriteBufferNumberToMaintain();}
f15103
0
setCompactionPriority
public Options kafkatest_f15104_0(final CompactionPriority compactionPriority)
{    columnFamilyOptions.setCompactionPriority(compactionPriority);    return this;}
f15104
0
setForceConsistencyChecks
public Options kafkatest_f15112_0(final boolean forceConsistencyChecks)
{    columnFamilyOptions.setForceConsistencyChecks(forceConsistencyChecks);    return this;}
f15112
0
forceConsistencyChecks
public boolean kafkatest_f15113_0()
{    return columnFamilyOptions.forceConsistencyChecks();}
f15113
0
setWriteBufferManager
public Options kafkatest_f15114_0(final WriteBufferManager writeBufferManager)
{    dbOptions.setWriteBufferManager(writeBufferManager);    return this;}
f15114
0
close
public synchronized void kafkatest_f15122_0()
{    openIterators.remove(this);    iter.close();    open = false;}
f15122
0
peekNextKey
public Bytes kafkatest_f15123_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return next.key;}
f15123
0
name
public String kafkatest_f15124_0()
{    return name;}
f15124
0
retentionPeriod
public long kafkatest_f15132_0()
{    return retentionPeriod;}
f15132
0
findSessions
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f15133_0(final Bytes key, final long earliestSessionEndTime, final long latestSessionStartTime)
{    final KeyValueIterator<Bytes, byte[]> bytesIterator = wrapped().fetch(key, earliestSessionEndTime, latestSessionStartTime);    return new WrappedSessionStoreIterator(bytesIterator);}
f15133
0
findSessions
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f15134_0(final Bytes keyFrom, final Bytes keyTo, final long earliestSessionEndTime, final long latestSessionStartTime)
{    final KeyValueIterator<Bytes, byte[]> bytesIterator = wrapped().fetch(keyFrom, keyTo, earliestSessionEndTime, latestSessionStartTime);    return new WrappedSessionStoreIterator(bytesIterator);}
f15134
0
openRocksDB
 void kafkatest_f15142_0(final DBOptions dbOptions, final ColumnFamilyOptions columnFamilyOptions)
{    final List<ColumnFamilyDescriptor> columnFamilyDescriptors = Collections.singletonList(new ColumnFamilyDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY, columnFamilyOptions));    final List<ColumnFamilyHandle> columnFamilies = new ArrayList<>(columnFamilyDescriptors.size());    try {        db = RocksDB.open(dbOptions, dbDir.getAbsolutePath(), columnFamilyDescriptors, columnFamilies);        dbAccessor = new SingleColumnFamilyAccessor(columnFamilies.get(0));    } catch (final RocksDBException e) {        throw new ProcessorStateException("Error opening store " + name + " at location " + dbDir.toString(), e);    }}
f15142
0
init
public void kafkatest_f15143_0(final ProcessorContext context, final StateStore root)
{    // open the DB dir    internalProcessorContext = context;    openDB(context);    batchingStateRestoreCallback = new RocksDBBatchingRestoreCallback(this);    // value getter should always read directly from rocksDB    // since it is only for values that are already flushed    context.register(root, batchingStateRestoreCallback);}
f15143
0
isPrepareForBulkload
 boolean kafkatest_f15144_0()
{    return prepareForBulkload;}
f15144
0
get
public synchronized byte[] kafkatest_f15152_0(final Bytes key)
{    validateStoreOpen();    try {        return dbAccessor.get(key.get());    } catch (final RocksDBException e) {        // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.        throw new ProcessorStateException("Error while getting value for key from store " + name, e);    }}
f15152
0
delete
public synchronized byte[] kafkatest_f15153_0(final Bytes key)
{    Objects.requireNonNull(key, "key cannot be null");    final byte[] oldValue;    try {        oldValue = dbAccessor.getOnly(key.get());    } catch (final RocksDBException e) {        // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.        throw new ProcessorStateException("Error while getting value for key from store " + name, e);    }    put(key, null);    return oldValue;}
f15153
0
range
public synchronized KeyValueIterator<Bytes, byte[]>f15154_1final Bytes from, final Bytes to)
{    Objects.requireNonNull(from, "from cannot be null");    Objects.requireNonNull(to, "to cannot be null");    if (from.compareTo(to) > 0) {                return KeyValueIterators.emptyIterator();    }    validateStoreOpen();    final KeyValueIterator<Bytes, byte[]> rocksDBRangeIterator = dbAccessor.range(from, to);    openIterators.add(rocksDBRangeIterator);    return rocksDBRangeIterator;}
public synchronized KeyValueIterator<Bytes, byte[]>f15154
1
close
public synchronized void kafkatest_f15162_0()
{    if (!open) {        return;    }    open = false;    closeOpenIterators();    if (configSetter != null) {        configSetter.close(name, userSpecifiedOptions);        configSetter = null;    }    closeOrUpdateMetricsRecorder();    // Important: do not rearrange the order in which the below objects are closed!    // Order of closing must follow: ColumnFamilyHandle > RocksDB > DBOptions > ColumnFamilyOptions    dbAccessor.close();    db.close();    userSpecifiedOptions.close();    wOptions.close();    fOptions.close();    filter.close();    cache.close();    dbAccessor = null;    userSpecifiedOptions = null;    wOptions = null;    fOptions = null;    db = null;    filter = null;    cache = null;}
f15162
0
closeOpenIterators
private voidf15163_1)
{    final HashSet<KeyValueIterator<Bytes, byte[]>> iterators;    synchronized (openIterators) {        iterators = new HashSet<>(openIterators);    }    if (iterators.size() != 0) {                for (final KeyValueIterator<Bytes, byte[]> iterator : iterators) {            iterator.close();        }    }}
private voidf15163
1
closeOrUpdateMetricsRecorder
private void kafkatest_f15164_0()
{    if (closeMetricsRecorder) {        metricsRecorder.close();    } else if (removeStatisticsFromMetricsRecorder) {        metricsRecorder.removeStatistics(name);    }}
f15164
0
flush
public void kafkatest_f15172_0() throws RocksDBException
{    db.flush(fOptions, columnFamily);}
f15172
0
prepareBatchForRestore
public void kafkatest_f15173_0(final Collection<KeyValue<byte[], byte[]>> records, final WriteBatch batch) throws RocksDBException
{    for (final KeyValue<byte[], byte[]> record : records) {        addToBatch(record.key, record.value, batch);    }}
f15173
0
addToBatch
public void kafkatest_f15174_0(final byte[] key, final byte[] value, final WriteBatch batch) throws RocksDBException
{    if (value == null) {        batch.delete(columnFamily, key);    } else {        batch.put(columnFamily, key, value);    }}
f15174
0
setDbAccessor
private voidf15182_1final ColumnFamilyHandle noTimestampColumnFamily, final ColumnFamilyHandle withTimestampColumnFamily)
{    final RocksIterator noTimestampsIter = db.newIterator(noTimestampColumnFamily);    noTimestampsIter.seekToFirst();    if (noTimestampsIter.isValid()) {                dbAccessor = new DualColumnFamilyAccessor(noTimestampColumnFamily, withTimestampColumnFamily);    } else {                dbAccessor = new SingleColumnFamilyAccessor(withTimestampColumnFamily);        noTimestampColumnFamily.close();    }    noTimestampsIter.close();}
private voidf15182
1
put
public void kafkatest_f15183_0(final byte[] key, final byte[] valueWithTimestamp)
{    if (valueWithTimestamp == null) {        try {            db.delete(oldColumnFamily, wOptions, key);        } catch (final RocksDBException e) {            // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.            throw new ProcessorStateException("Error while removing key from store " + name, e);        }        try {            db.delete(newColumnFamily, wOptions, key);        } catch (final RocksDBException e) {            // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.            throw new ProcessorStateException("Error while removing key from store " + name, e);        }    } else {        try {            db.delete(oldColumnFamily, wOptions, key);        } catch (final RocksDBException e) {            // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.            throw new ProcessorStateException("Error while removing key from store " + name, e);        }        try {            db.put(newColumnFamily, wOptions, key, valueWithTimestamp);        } catch (final RocksDBException e) {            // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.            throw new ProcessorStateException("Error while putting key/value into store " + name, e);        }    }}
f15183
0
prepareBatch
public void kafkatest_f15184_0(final List<KeyValue<Bytes, byte[]>> entries, final WriteBatch batch) throws RocksDBException
{    for (final KeyValue<Bytes, byte[]> entry : entries) {        Objects.requireNonNull(entry.key, "key cannot be null");        addToBatch(entry.key.get(), entry.value, batch);    }}
f15184
0
addToBatch
public void kafkatest_f15192_0(final byte[] key, final byte[] value, final WriteBatch batch) throws RocksDBException
{    if (value == null) {        batch.delete(oldColumnFamily, key);        batch.delete(newColumnFamily, key);    } else {        batch.delete(oldColumnFamily, key);        batch.put(newColumnFamily, key, value);    }}
f15192
0
close
public void kafkatest_f15193_0()
{    oldColumnFamily.close();    newColumnFamily.close();}
f15193
0
toggleDbForBulkLoading
public void kafkatest_f15194_0()
{    try {        db.compactRange(oldColumnFamily, true, 1, 0);    } catch (final RocksDBException e) {        throw new ProcessorStateException("Error while range compacting during restoring  store " + name, e);    }    try {        db.compactRange(newColumnFamily, true, 1, 0);    } catch (final RocksDBException e) {        throw new ProcessorStateException("Error while range compacting during restoring  store " + name, e);    }}
f15194
0
get
public WindowStore<Bytes, byte[]> kafkatest_f15202_0()
{    if (!returnTimestampedStore) {        return new RocksDBWindowStore(new RocksDBSegmentedBytesStore(name, metricsScope(), retentionPeriod, segmentInterval, new WindowKeySchema()), retainDuplicates, windowSize);    } else {        return new RocksDBTimestampedWindowStore(new RocksDBTimestampedSegmentedBytesStore(name, metricsScope(), retentionPeriod, segmentInterval, new WindowKeySchema()), retainDuplicates, windowSize);    }}
f15202
0
metricsScope
public String kafkatest_f15203_0()
{    return "rocksdb-window";}
f15203
0
segments
public int kafkatest_f15204_0()
{    return (int) (retentionPeriod / segmentInterval) + 1;}
f15204
0
fetch
public byte[] kafkatest_f15212_0(final Bytes key, final long timestamp)
{    final byte[] bytesValue = wrapped().get(WindowKeySchema.toStoreKeyBinary(key, timestamp, seqnum));    return bytesValue;}
f15212
0
fetch
public WindowStoreIterator<byte[]> kafkatest_f15213_0(final Bytes key, final long timeFrom, final long timeTo)
{    final KeyValueIterator<Bytes, byte[]> bytesIterator = wrapped().fetch(key, timeFrom, timeTo);    return new WindowStoreIteratorWrapper(bytesIterator, windowSize).valuesIterator();}
f15213
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f15214_0(final Bytes from, final Bytes to, final long timeFrom, final long timeTo)
{    final KeyValueIterator<Bytes, byte[]> bytesIterator = wrapped().fetch(from, to, timeFrom, timeTo);    return new WindowStoreIteratorWrapper(bytesIterator, windowSize).keyValueIterator();}
f15214
0
segmentId
public long kafkatest_f15222_0(final Bytes key)
{    return segmentId(keySchema.segmentTimestamp(key));}
f15222
0
segmentId
 long kafkatest_f15223_0(final long timestamp)
{    return timestamp / segmentInterval;}
f15223
0
getSegmentInterval
 long kafkatest_f15224_0()
{    return segmentInterval;}
f15224
0
lowerRangeFixedSize
public Bytes kafkatest_f15232_0(final Bytes key, final long from)
{    final Windowed<Bytes> sessionKey = new Windowed<>(key, new SessionWindow(0, Math.max(0, from)));    return SessionKeySchema.toBinary(sessionKey);}
f15232
0
upperRange
public Bytes kafkatest_f15233_0(final Bytes key, final long to)
{    final byte[] maxSuffix = ByteBuffer.allocate(SUFFIX_SIZE).putLong(Long.MAX_VALUE).putLong(to).array();    return OrderedBytes.upperRange(key, maxSuffix);}
f15233
0
lowerRange
public Bytes kafkatest_f15234_0(final Bytes key, final long from)
{    return OrderedBytes.lowerRange(key, MIN_SUFFIX);}
f15234
0
extractWindow
 static Window kafkatest_f15242_0(final byte[] binaryKey)
{    final ByteBuffer buffer = ByteBuffer.wrap(binaryKey);    final long start = buffer.getLong(binaryKey.length - TIMESTAMP_SIZE);    final long end = buffer.getLong(binaryKey.length - 2 * TIMESTAMP_SIZE);    return new SessionWindow(start, end);}
f15242
0
from
public static Windowed<K> kafkatest_f15243_0(final byte[] binaryKey, final Deserializer<K> keyDeserializer, final String topic)
{    final K key = extractKey(binaryKey, keyDeserializer, topic);    final Window window = extractWindow(binaryKey);    return new Windowed<>(key, window);}
f15243
0
from
public static Windowed<Bytes> kafkatest_f15244_0(final Bytes bytesKey)
{    final byte[] binaryKey = bytesKey.get();    final Window window = extractWindow(binaryKey);    return new Windowed<>(Bytes.wrap(extractKeyBytes(binaryKey)), window);}
f15244
0
retentionPeriod
public long kafkatest_f15252_0()
{    return storeSupplier.retentionPeriod();}
f15252
0
logChange
 void kafkatest_f15253_0(final K key, final V value)
{    logChange(key, value, context.timestamp());}
f15253
0
logChange
 void kafkatest_f15254_0(final K key, final V value, final long timestamp)
{    // Sending null headers to changelog topics (KIP-244)    collector.send(topic, key, value, null, partition, timestamp, keySerializer, valueSerializer);}
f15254
0
underlyingStoreNamefromCacheName
public static String kafkatest_f15262_0(final String cacheName)
{    final String[] tokens = cacheName.split("-", 2);    return tokens[1];}
f15262
0
addDirtyEntryFlushListener
public void kafkatest_f15263_0(final String namespace, final DirtyEntryFlushListener listener)
{    final NamedCache cache = getOrCreateCache(namespace);    cache.setListener(listener);}
f15263
0
flush
public void kafkatest_f15264_0(final String namespace)
{    numFlushes++;    final NamedCache cache = getCache(namespace);    if (cache == null) {        return;    }    cache.flush();    if (log.isTraceEnabled()) {        log.trace("Cache stats on flush: #puts={}, #gets={}, #evicts={}, #flushes={}", puts(), gets(), evicts(), flushes());    }}
f15264
0
size
public long kafkatest_f15272_0()
{    long size = 0;    for (final NamedCache cache : caches.values()) {        size += cache.size();        if (isOverflowing(size)) {            return Long.MAX_VALUE;        }    }    return size;}
f15272
0
isOverflowing
private boolean kafkatest_f15273_0(final long size)
{    return size < 0;}
f15273
0
sizeBytes
 long kafkatest_f15274_0()
{    long sizeInBytes = 0;    for (final NamedCache namedCache : caches.values()) {        sizeInBytes += namedCache.sizeInBytes();        if (isOverflowing(sizeInBytes)) {            return Long.MAX_VALUE;        }    }    return sizeInBytes;}
f15274
0
next
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f15282_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    final KeyValue<Bytes, LRUCacheEntry> result = nextEntry;    nextEntry = null;    return result;}
f15282
0
internalNext
private void kafkatest_f15283_0()
{    final Bytes cacheKey = keys.next();    final LRUCacheEntry entry = cache.get(cacheKey);    if (entry == null) {        return;    }    nextEntry = new KeyValue<>(cacheKey, entry);}
f15283
0
close
public void kafkatest_f15284_0()
{// do nothing}
f15284
0
equals
public boolean kafkatest_f15292_0(final Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    final Eviction<?, ?> eviction = (Eviction<?, ?>) o;    return Objects.equals(key, eviction.key) && Objects.equals(value, eviction.value) && Objects.equals(recordContext, eviction.recordContext);}
f15292
0
hashCode
public int kafkatest_f15293_0()
{    return Objects.hash(key, value, recordContext);}
f15293
0
build
public TimestampedKeyValueStore<K, V> kafkatest_f15294_0()
{    KeyValueStore<Bytes, byte[]> store = storeSupplier.get();    if (!(store instanceof TimestampedBytesStore)) {        if (store.persistent()) {            store = new KeyValueToTimestampedKeyValueByteStoreAdapter(store);        } else {            store = new InMemoryTimestampedKeyValueStoreMarker(store);        }    }    return new MeteredTimestampedKeyValueStore<>(maybeWrapCaching(maybeWrapLogging(store)), storeSupplier.metricsScope(), time, keySerde, valueSerde);}
f15294
0
get
public byte[] kafkatest_f15302_0(final Bytes key)
{    return wrapped.get(key);}
f15302
0
range
public KeyValueIterator<Bytes, byte[]> kafkatest_f15303_0(final Bytes from, final Bytes to)
{    return wrapped.range(from, to);}
f15303
0
all
public KeyValueIterator<Bytes, byte[]> kafkatest_f15304_0()
{    return wrapped.all();}
f15304
0
compareTo
public int kafkatest_f15312_0(final TimestampedSegment segment)
{    return Long.compare(id, segment.id);}
f15312
0
openDB
public void kafkatest_f15313_0(final ProcessorContext context)
{    super.openDB(context);    // skip the registering step    internalProcessorContext = context;}
f15313
0
toString
public String kafkatest_f15314_0()
{    return "TimestampedSegment(id=" + id + ", name=" + name() + ")";}
f15314
0
retentionPeriod
public long kafkatest_f15322_0()
{    return storeSupplier.retentionPeriod();}
f15322
0
init
public void kafkatest_f15323_0(final ProcessorContext context, final StateStore root)
{    wrapped.init(context, root);}
f15323
0
put
public void kafkatest_f15324_0(final Bytes key, final byte[] value)
{    wrapped.put(key, value);}
f15324
0
close
public void kafkatest_f15332_0()
{    wrapped.close();}
f15332
0
isOpen
public boolean kafkatest_f15333_0()
{    return wrapped.isOpen();}
f15333
0
name
public String kafkatest_f15334_0()
{    return wrapped.name();}
f15334
0
configure
public void kafkatest_f15342_0(final Map<String, ?> configs, final boolean isKey)
{    valueAndTimestampSerializer.configure(configs, isKey);    valueAndTimestampDeserializer.configure(configs, isKey);}
f15342
0
close
public void kafkatest_f15343_0()
{    valueAndTimestampSerializer.close();    valueAndTimestampDeserializer.close();}
f15343
0
serializer
public Serializer<ValueAndTimestamp<V>> kafkatest_f15344_0()
{    return valueAndTimestampSerializer;}
f15344
0
lowerRangeFixedSize
public Bytes kafkatest_f15352_0(final Bytes key, final long from)
{    return WindowKeySchema.toStoreKeyBinary(key, Math.max(0, from), 0);}
f15352
0
upperRangeFixedSize
public Bytes kafkatest_f15353_0(final Bytes key, final long to)
{    return WindowKeySchema.toStoreKeyBinary(key, to, Integer.MAX_VALUE);}
f15353
0
segmentTimestamp
public long kafkatest_f15354_0(final Bytes key)
{    return WindowKeySchema.extractStoreTimestamp(key.get());}
f15354
0
toStoreKeyBinary
public static Bytes kafkatest_f15362_0(final K key, final long timestamp, final int seqnum, final StateSerdes<K, ?> serdes)
{    final byte[] serializedKey = serdes.rawKey(key);    return toStoreKeyBinary(serializedKey, timestamp, seqnum);}
f15362
0
toStoreKeyBinary
public static Bytes kafkatest_f15363_0(final Windowed<Bytes> timeKey, final int seqnum)
{    final byte[] bytes = timeKey.key().get();    return toStoreKeyBinary(bytes, timeKey.window().start(), seqnum);}
f15363
0
toStoreKeyBinary
public static Bytes kafkatest_f15364_0(final Windowed<K> timeKey, final int seqnum, final StateSerdes<K, ?> serdes)
{    final byte[] serializedKey = serdes.rawKey(timeKey.key());    return toStoreKeyBinary(serializedKey, timeKey.window().start(), seqnum);}
f15364
0
fromStoreBytesKey
public static Windowed<Bytes> kafkatest_f15372_0(final byte[] binaryKey, final long windowSize)
{    final Bytes key = Bytes.wrap(extractStoreKeyBytes(binaryKey));    final Window window = extractStoreWindow(binaryKey, windowSize);    return new Windowed<>(key, window);}
f15372
0
extractStoreWindow
 static Window kafkatest_f15373_0(final byte[] binaryKey, final long windowSize)
{    final ByteBuffer buffer = ByteBuffer.wrap(binaryKey);    final long start = buffer.getLong(binaryKey.length - TIMESTAMP_SIZE - SEQNUM_SIZE);    return timeWindowForSize(start, windowSize);}
f15373
0
build
public WindowStore<K, V> kafkatest_f15374_0()
{    return new MeteredWindowStore<>(maybeWrapCaching(maybeWrapLogging(storeSupplier.get())), storeSupplier.windowSize(), storeSupplier.metricsScope(), time, keySerde, valueSerde);}
f15374
0
next
public KeyValue<Long, byte[]> kafkatest_f15382_0()
{    final KeyValue<Bytes, byte[]> next = bytesIterator.next();    final long timestamp = WindowKeySchema.extractStoreTimestamp(next.key.get());    return KeyValue.pair(timestamp, next.value);}
f15382
0
close
public void kafkatest_f15383_0()
{    bytesIterator.close();}
f15383
0
peekNextKey
public Windowed<Bytes> kafkatest_f15384_0()
{    final byte[] nextKey = bytesIterator.peekNextKey().get();    return WindowKeySchema.fromStoreBytesKey(nextKey, windowSize);}
f15384
0
fetch
public WindowStoreIterator<byte[]> kafkatest_f15392_0(final Bytes key, final Instant from, final Instant to)
{    return new WindowToTimestampedWindowIteratorAdapter(store.fetch(key, from, to));}
f15392
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f15393_0(final Bytes from, final Bytes to, final long timeFrom, final long timeTo)
{    return new KeyValueToTimestampedKeyValueIteratorAdapter<>(store.fetch(from, to, timeFrom, timeTo));}
f15393
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f15394_0(final Bytes from, final Bytes to, final Instant fromTime, final Instant toTime)
{    return new KeyValueToTimestampedKeyValueIteratorAdapter<>(store.fetch(from, to, fromTime, toTime));}
f15394
0
persistent
public boolean kafkatest_f15402_0()
{    return true;}
f15402
0
isOpen
public boolean kafkatest_f15403_0()
{    return store.isOpen();}
f15403
0
close
public void kafkatest_f15404_0()
{    bytesIterator.close();}
f15404
0
persistent
public boolean kafkatest_f15412_0()
{    return wrapped.persistent();}
f15412
0
isOpen
public boolean kafkatest_f15413_0()
{    return wrapped.isOpen();}
f15413
0
validateStoreOpen
 void kafkatest_f15414_0()
{    if (!wrapped.isOpen()) {        throw new InvalidStateStoreException("Store " + wrapped.name() + " is currently closed.");    }}
f15414
0
timestampedWindowStore
public static QueryableStoreType<ReadOnlyWindowStore<K, ValueAndTimestamp<V>>> kafkatest_f15422_0()
{    return new TimestampedWindowStoreType<>();}
f15422
0
sessionStore
public static QueryableStoreType<ReadOnlySessionStore<K, V>> kafkatest_f15423_0()
{    return new SessionStoreType<>();}
f15423
0
accepts
public boolean kafkatest_f15424_0(final StateStore stateStore)
{    for (final Class matchToClass : matchTo) {        if (!matchToClass.isAssignableFrom(stateStore.getClass())) {            return false;        }    }    return true;}
f15424
0
keySerde
public Serde<K> kafkatest_f15432_0()
{    return keySerde;}
f15432
0
valueSerde
public Serde<V> kafkatest_f15433_0()
{    return valueSerde;}
f15433
0
keyDeserializer
public Deserializer<K> kafkatest_f15434_0()
{    return keySerde.deserializer();}
f15434
0
rawValue
public byte[] kafkatest_f15442_0(final V value)
{    try {        return valueSerde.serializer().serialize(topic, value);    } catch (final ClassCastException e) {        final String valueClass;        final Class<? extends Serializer> serializerClass;        if (valueSerializer() instanceof ValueAndTimestampSerializer) {            serializerClass = ((ValueAndTimestampSerializer) valueSerializer()).valueSerializer.getClass();            valueClass = value == null ? "unknown because value is null" : ((ValueAndTimestamp) value).value().getClass().getName();        } else {            serializerClass = valueSerializer().getClass();            valueClass = value == null ? "unknown because value is null" : value.getClass().getName();        }        throw new StreamsException(String.format("A serializer (%s) is not compatible to the actual value type " + "(value type: %s). Change the default Serdes in StreamConfig or " + "provide correct Serdes via method parameters.", serializerClass.getName(), valueClass), e);    }}
f15442
0
persistentKeyValueStore
public static KeyValueBytesStoreSupplier kafkatest_f15443_0(final String name)
{    Objects.requireNonNull(name, "name cannot be null");    return new RocksDbKeyValueBytesStoreSupplier(name, false);}
f15443
0
persistentTimestampedKeyValueStore
public static KeyValueBytesStoreSupplier kafkatest_f15444_0(final String name)
{    Objects.requireNonNull(name, "name cannot be null");    return new RocksDbKeyValueBytesStoreSupplier(name, true);}
f15444
0
metricsScope
public String kafkatest_f15452_0()
{    return "in-memory-lru";}
f15452
0
persistentWindowStore
public static WindowBytesStoreSupplier kafkatest_f15453_0(final String name, final long retentionPeriod, final int numSegments, final long windowSize, final boolean retainDuplicates)
{    if (numSegments < 2) {        throw new IllegalArgumentException("numSegments cannot be smaller than 2");    }    final long legacySegmentInterval = Math.max(retentionPeriod / (numSegments - 1), 60_000L);    return persistentWindowStore(name, retentionPeriod, windowSize, retainDuplicates, legacySegmentInterval, false);}
f15453
0
persistentWindowStore
public static WindowBytesStoreSupplier kafkatest_f15454_0(final String name, final Duration retentionPeriod, final Duration windowSize, final boolean retainDuplicates) throws IllegalArgumentException
{    return persistentWindowStore(name, retentionPeriod, windowSize, retainDuplicates, false);}
f15454
0
keyValueStoreBuilder
public static StoreBuilder<KeyValueStore<K, V>> kafkatest_f15462_0(final KeyValueBytesStoreSupplier supplier, final Serde<K> keySerde, final Serde<V> valueSerde)
{    Objects.requireNonNull(supplier, "supplier cannot be null");    return new KeyValueStoreBuilder<>(supplier, keySerde, valueSerde, Time.SYSTEM);}
f15462
0
timestampedKeyValueStoreBuilder
public static StoreBuilder<TimestampedKeyValueStore<K, V>> kafkatest_f15463_0(final KeyValueBytesStoreSupplier supplier, final Serde<K> keySerde, final Serde<V> valueSerde)
{    Objects.requireNonNull(supplier, "supplier cannot be null");    return new TimestampedKeyValueStoreBuilder<>(supplier, keySerde, valueSerde, Time.SYSTEM);}
f15463
0
windowStoreBuilder
public static StoreBuilder<WindowStore<K, V>> kafkatest_f15464_0(final WindowBytesStoreSupplier supplier, final Serde<K> keySerde, final Serde<V> valueSerde)
{    Objects.requireNonNull(supplier, "supplier cannot be null");    return new WindowStoreBuilder<>(supplier, keySerde, valueSerde, Time.SYSTEM);}
f15464
0
equals
public boolean kafkatest_f15472_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final StreamsMetadata that = (StreamsMetadata) o;    if (!hostInfo.equals(that.hostInfo)) {        return false;    }    if (!stateStoreNames.equals(that.stateStoreNames)) {        return false;    }    return topicPartitions.equals(that.topicPartitions);}
f15472
0
hashCode
public int kafkatest_f15473_0()
{    int result = hostInfo.hashCode();    result = 31 * result + stateStoreNames.hashCode();    result = 31 * result + topicPartitions.hashCode();    return result;}
f15473
0
toString
public String kafkatest_f15474_0()
{    return "StreamsMetadata{" + "hostInfo=" + hostInfo + ", stateStoreNames=" + stateStoreNames + ", topicPartitions=" + topicPartitions + '}';}
f15474
0
hashCode
public int kafkatest_f15482_0()
{    return Objects.hash(value, timestamp);}
f15482
0
fetch
 WindowStoreIterator<V> kafkatest_f15483_0(final K key, final Instant from, final Instant to)
{    return fetch(key, ApiUtils.validateMillisecondInstant(from, prepareMillisCheckFailMsgPrefix(from, "from")), ApiUtils.validateMillisecondInstant(to, prepareMillisCheckFailMsgPrefix(to, "to")));}
f15483
0
fetch
 KeyValueIterator<Windowed<K>, V> kafkatest_f15484_0(final K from, final K to, final Instant fromTime, final Instant toTime)
{    return fetch(from, to, ApiUtils.validateMillisecondInstant(fromTime, prepareMillisCheckFailMsgPrefix(fromTime, "fromTime")), ApiUtils.validateMillisecondInstant(toTime, prepareMillisCheckFailMsgPrefix(toTime, "toTime")));}
f15484
0
table
public synchronized KTable<K, V> kafkatest_f15492_0(final String topic, final Consumed<K, V> consumed, final Materialized<K, V, KeyValueStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(topic, "topic can't be null");    Objects.requireNonNull(consumed, "consumed can't be null");    Objects.requireNonNull(materialized, "materialized can't be null");    final ConsumedInternal<K, V> consumedInternal = new ConsumedInternal<>(consumed);    materialized.withKeySerde(consumedInternal.keySerde()).withValueSerde(consumedInternal.valueSerde());    final MaterializedInternal<K, V, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, internalStreamsBuilder, topic + "-");    return internalStreamsBuilder.table(topic, consumedInternal, materializedInternal);}
f15492
0
table
public synchronized KTable<K, V> kafkatest_f15493_0(final String topic)
{    return table(topic, new ConsumedInternal<>());}
f15493
0
table
public synchronized KTable<K, V> kafkatest_f15494_0(final String topic, final Consumed<K, V> consumed)
{    Objects.requireNonNull(topic, "topic can't be null");    Objects.requireNonNull(consumed, "consumed can't be null");    final ConsumedInternal<K, V> consumedInternal = new ConsumedInternal<>(consumed);    final MaterializedInternal<K, V, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(Materialized.with(consumedInternal.keySerde(), consumedInternal.valueSerde()), internalStreamsBuilder, topic + "-");    return internalStreamsBuilder.table(topic, consumedInternal, materializedInternal);}
f15494
0
addGlobalStore
public synchronized StreamsBuilder kafkatest_f15502_0(final StoreBuilder storeBuilder, final String topic, final Consumed consumed, final ProcessorSupplier stateUpdateSupplier)
{    Objects.requireNonNull(storeBuilder, "storeBuilder can't be null");    Objects.requireNonNull(consumed, "consumed can't be null");    internalStreamsBuilder.addGlobalStore(storeBuilder, topic, new ConsumedInternal<>(consumed), stateUpdateSupplier);    return this;}
f15502
0
build
public synchronized Topology kafkatest_f15503_0()
{    return build(null);}
f15503
0
build
public synchronized Topology kafkatest_f15504_0(final Properties props)
{    internalStreamsBuilder.buildAndOptimizeTopology(props);    return topology;}
f15504
0
configDef
public static ConfigDef kafkatest_f15512_0()
{    return new ConfigDef(CONFIG);}
f15512
0
postProcessParsedConfig
protected Map<String, Object>f15513_1final Map<String, Object> parsedValues)
{    final Map<String, Object> configUpdates = CommonClientConfigs.postProcessReconnectBackoffConfigs(this, parsedValues);    final boolean eosEnabled = EXACTLY_ONCE.equals(parsedValues.get(PROCESSING_GUARANTEE_CONFIG));    if (eosEnabled && !originals().containsKey(COMMIT_INTERVAL_MS_CONFIG)) {                configUpdates.put(COMMIT_INTERVAL_MS_CONFIG, EOS_DEFAULT_COMMIT_INTERVAL_MS);    }    return configUpdates;}
protected Map<String, Object>f15513
1
getCommonConsumerConfigs
private Map<String, Object> kafkatest_f15514_0()
{    final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(CONSUMER_PREFIX, ConsumerConfig.configNames());    checkIfUnexpectedUserSpecifiedConsumerConfig(clientProvidedProps, NON_CONFIGURABLE_CONSUMER_DEFAULT_CONFIGS);    checkIfUnexpectedUserSpecifiedConsumerConfig(clientProvidedProps, NON_CONFIGURABLE_CONSUMER_EOS_CONFIGS);    final Map<String, Object> consumerProps = new HashMap<>(eosEnabled ? CONSUMER_EOS_OVERRIDES : CONSUMER_DEFAULT_OVERRIDES);    consumerProps.putAll(getClientCustomProps());    consumerProps.putAll(clientProvidedProps);    // bootstrap.servers should be from StreamsConfig    consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, originals().get(BOOTSTRAP_SERVERS_CONFIG));    return consumerProps;}
f15514
0
getClientPropsWithPrefix
private Map<String, Object> kafkatest_f15522_0(final String prefix, final Set<String> configNames)
{    final Map<String, Object> props = clientProps(configNames, originals());    props.putAll(originalsWithPrefix(prefix));    return props;}
f15522
0
getClientCustomProps
private Map<String, Object> kafkatest_f15523_0()
{    final Map<String, Object> props = originals();    props.keySet().removeAll(CONFIG.names());    props.keySet().removeAll(ConsumerConfig.configNames());    props.keySet().removeAll(ProducerConfig.configNames());    props.keySet().removeAll(AdminClientConfig.configNames());    props.keySet().removeAll(originalsWithPrefix(CONSUMER_PREFIX, false).keySet());    props.keySet().removeAll(originalsWithPrefix(PRODUCER_PREFIX, false).keySet());    props.keySet().removeAll(originalsWithPrefix(ADMIN_CLIENT_PREFIX, false).keySet());    return props;}
f15523
0
defaultKeySerde
public Serde kafkatest_f15524_0()
{    final Object keySerdeConfigSetting = get(DEFAULT_KEY_SERDE_CLASS_CONFIG);    try {        final Serde<?> serde = getConfiguredInstance(DEFAULT_KEY_SERDE_CLASS_CONFIG, Serde.class);        serde.configure(originals(), true);        return serde;    } catch (final Exception e) {        throw new StreamsException(String.format("Failed to configure key serde %s", keySerdeConfigSetting), e);    }}
f15524
0
addSource
public synchronized Topology kafkatest_f15532_0(final String name, final Pattern topicPattern)
{    internalTopologyBuilder.addSource(null, name, null, null, null, topicPattern);    return this;}
f15532
0
addSource
public synchronized Topology kafkatest_f15533_0(final AutoOffsetReset offsetReset, final String name, final String... topics)
{    internalTopologyBuilder.addSource(offsetReset, name, null, null, null, topics);    return this;}
f15533
0
addSource
public synchronized Topology kafkatest_f15534_0(final AutoOffsetReset offsetReset, final String name, final Pattern topicPattern)
{    internalTopologyBuilder.addSource(offsetReset, name, null, null, null, topicPattern);    return this;}
f15534
0
addSource
public synchronized Topology kafkatest_f15542_0(final AutoOffsetReset offsetReset, final String name, final Deserializer keyDeserializer, final Deserializer valueDeserializer, final Pattern topicPattern)
{    internalTopologyBuilder.addSource(offsetReset, name, null, keyDeserializer, valueDeserializer, topicPattern);    return this;}
f15542
0
addSource
public synchronized Topology kafkatest_f15543_0(final AutoOffsetReset offsetReset, final String name, final TimestampExtractor timestampExtractor, final Deserializer keyDeserializer, final Deserializer valueDeserializer, final String... topics)
{    internalTopologyBuilder.addSource(offsetReset, name, timestampExtractor, keyDeserializer, valueDeserializer, topics);    return this;}
f15543
0
addSource
public synchronized Topology kafkatest_f15544_0(final AutoOffsetReset offsetReset, final String name, final TimestampExtractor timestampExtractor, final Deserializer keyDeserializer, final Deserializer valueDeserializer, final Pattern topicPattern)
{    internalTopologyBuilder.addSource(offsetReset, name, timestampExtractor, keyDeserializer, valueDeserializer, topicPattern);    return this;}
f15544
0
addSink
public synchronized Topology kafkatest_f15552_0(final String name, final TopicNameExtractor<K, V> topicExtractor, final Serializer<K> keySerializer, final Serializer<V> valueSerializer, final StreamPartitioner<? super K, ? super V> partitioner, final String... parentNames)
{    internalTopologyBuilder.addSink(name, topicExtractor, keySerializer, valueSerializer, partitioner, parentNames);    return this;}
f15552
0
addProcessor
public synchronized Topology kafkatest_f15553_0(final String name, final ProcessorSupplier supplier, final String... parentNames)
{    internalTopologyBuilder.addProcessor(name, supplier, parentNames);    return this;}
f15553
0
addStateStore
public synchronized Topology kafkatest_f15554_0(final StoreBuilder storeBuilder, final String... processorNames)
{    internalTopologyBuilder.addStateStore(storeBuilder, processorNames);    return this;}
f15554
0
verifyHashCodeConsistency
private static void kafkatest_f15562_0(final T o1, final T o2)
{    {        final int first = o1.hashCode();        final int second = o1.hashCode();        if (first != second) {            throw new AssertionError(String.format("o1[%s]'s hashcode was not consistent: [%d]!=[%d].", o1, first, second));        }    }    {        final int first = o2.hashCode();        final int second = o2.hashCode();        if (first != second) {            throw new AssertionError(String.format("o2[%s]'s hashcode was not consistent: [%d]!=[%d].", o2, first, second));        }    }}
f15562
0
handle
public ProductionExceptionHandlerResponse kafkatest_f15563_0(final ProducerRecord<byte[], byte[]> record, final Exception exception)
{    return ProductionExceptionHandlerResponse.CONTINUE;}
f15563
0
configure
public void kafkatest_f15564_0(final Map<String, ?> configs)
{// ignore}
f15564
0
runTest
 void kafkatest_f15572_0(final List<List<KeyValueTimestamp<Long, String>>> expectedResult, final String storeName) throws Exception
{    assert expectedResult.size() == input.size();    IntegrationTestUtils.purgeLocalStreamsState(STREAMS_CONFIG);    streams = new KafkaStreams(builder.build(), STREAMS_CONFIG);    KeyValueTimestamp<Long, String> expectedFinalResult = null;    try {        streams.start();        final long firstTimestamp = System.currentTimeMillis();        long ts = firstTimestamp;        final Iterator<List<KeyValueTimestamp<Long, String>>> resultIterator = expectedResult.iterator();        for (final Input<String> singleInput : input) {            producer.send(new ProducerRecord<>(singleInput.topic, null, ++ts, singleInput.record.key, singleInput.record.value)).get();            final List<KeyValueTimestamp<Long, String>> expected = resultIterator.next();            if (expected != null) {                final List<KeyValueTimestamp<Long, String>> updatedExpected = new LinkedList<>();                for (final KeyValueTimestamp<Long, String> record : expected) {                    updatedExpected.add(new KeyValueTimestamp<>(record.key(), record.value(), firstTimestamp + record.timestamp()));                }                checkResult(OUTPUT_TOPIC, updatedExpected);                expectedFinalResult = updatedExpected.get(expected.size() - 1);            }        }        if (storeName != null) {            checkQueryableStore(storeName, expectedFinalResult);        }    } finally {        streams.close();    }}
f15572
0
runTest
 void kafkatest_f15573_0(final KeyValueTimestamp<Long, String> expectedFinalResult) throws Exception
{    runTest(expectedFinalResult, null);}
f15573
0
runTest
 void kafkatest_f15574_0(final KeyValueTimestamp<Long, String> expectedFinalResult, final String storeName) throws Exception
{    IntegrationTestUtils.purgeLocalStreamsState(STREAMS_CONFIG);    streams = new KafkaStreams(builder.build(), STREAMS_CONFIG);    try {        streams.start();        final long firstTimestamp = System.currentTimeMillis();        long ts = firstTimestamp;        for (final Input<String> singleInput : input) {            producer.send(new ProducerRecord<>(singleInput.topic, null, ++ts, singleInput.record.key, singleInput.record.value)).get();        }        TestUtils.waitForCondition(() -> finalResultReached.get(), "Never received expected final result.");        final KeyValueTimestamp<Long, String> updatedExpectedFinalResult = new KeyValueTimestamp<>(expectedFinalResult.key(), expectedFinalResult.value(), firstTimestamp + expectedFinalResult.timestamp());        checkResult(OUTPUT_TOPIC, updatedExpectedFinalResult, numRecordsExpected);        if (storeName != null) {            checkQueryableStore(storeName, updatedExpectedFinalResult);        }    } finally {        streams.close();    }}
f15574
0
cleanupTest
 void kafkatest_f15582_0() throws Exception
{    if (streams != null) {        streams.close(Duration.ofSeconds(30));    }    IntegrationTestUtils.purgeLocalStreamsState(streamsConfig);}
f15582
0
add10InputElements
private void kafkatest_f15583_0() throws java.util.concurrent.ExecutionException, InterruptedException
{    final List<KeyValue<Long, String>> records = Arrays.asList(KeyValue.pair(0L, "aaa"), KeyValue.pair(1L, "bbb"), KeyValue.pair(0L, "ccc"), KeyValue.pair(1L, "ddd"), KeyValue.pair(0L, "eee"), KeyValue.pair(1L, "fff"), KeyValue.pair(0L, "ggg"), KeyValue.pair(1L, "hhh"), KeyValue.pair(0L, "iii"), KeyValue.pair(1L, "jjj"));    for (final KeyValue<Long, String> record : records) {        mockTime.sleep(10);        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(INPUT_TOPIC, Collections.singleton(record), producerConfig, mockTime.milliseconds());    }}
f15583
0
shouldNotAllowToResetWhileStreamsIsRunning
 void kafkatest_f15584_0()
{    appID = testId + "-not-reset-during-runtime";    final String[] parameters = new String[] { "--application-id", appID, "--bootstrap-servers", cluster.bootstrapServers(), "--input-topics", NON_EXISTING_TOPIC, "--execute" };    final Properties cleanUpConfig = new Properties();    cleanUpConfig.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 100);    cleanUpConfig.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "" + CLEANUP_CONSUMER_TIMEOUT);    streamsConfig.put(StreamsConfig.APPLICATION_ID_CONFIG, appID);    // RUN    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.start();    final int exitCode = new StreamsResetter().run(parameters, cleanUpConfig);    Assert.assertEquals(1, exitCode);    streams.close();}
f15584
0
setupTopologyWithIntermediateUserTopic
private Topology kafkatest_f15592_0(final String outputTopic2)
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Long, String> input = builder.stream(INPUT_TOPIC);    // use map to trigger internal re-partitioning before groupByKey    input.map(KeyValue::new).groupByKey().count().toStream().to(OUTPUT_TOPIC, Produced.with(Serdes.Long(), Serdes.Long()));    input.through(INTERMEDIATE_USER_TOPIC).groupByKey().windowedBy(TimeWindows.of(ofMillis(35)).advanceBy(ofMillis(10))).count().toStream().map((key, value) -> new KeyValue<>(key.window().start() + key.window().end(), value)).to(outputTopic2, Produced.with(Serdes.Long(), Serdes.Long()));    return builder.build();}
f15592
0
setupTopologyWithoutIntermediateUserTopic
private Topology kafkatest_f15593_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Long, String> input = builder.stream(INPUT_TOPIC);    // use map to trigger internal re-partitioning before groupByKey    input.map((key, value) -> new KeyValue<>(key, key)).to(OUTPUT_TOPIC, Produced.with(Serdes.Long(), Serdes.Long()));    return builder.build();}
f15593
0
cleanGlobal
private void kafkatest_f15594_0(final boolean withIntermediateTopics, final String resetScenario, final String resetScenarioArg) throws Exception
{    // leaving --zookeeper arg here to ensure tool works if users add it    final List<String> parameterList = new ArrayList<>(Arrays.asList("--application-id", appID, "--bootstrap-servers", cluster.bootstrapServers(), "--input-topics", INPUT_TOPIC, "--execute"));    if (withIntermediateTopics) {        parameterList.add("--intermediate-topics");        parameterList.add(INTERMEDIATE_USER_TOPIC);    }    final Map<String, Object> sslConfig = getClientSslConfig();    if (sslConfig != null) {        final File configFile = TestUtils.tempFile();        final BufferedWriter writer = new BufferedWriter(new FileWriter(configFile));        writer.write(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG + "=SSL\n");        writer.write(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG + "=" + sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG) + "\n");        writer.write(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG + "=" + ((Password) sslConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value() + "\n");        writer.close();        parameterList.add("--config-file");        parameterList.add(configFile.getAbsolutePath());    }    if (resetScenario != null) {        parameterList.add(resetScenario);    }    if (resetScenarioArg != null) {        parameterList.add(resetScenarioArg);    }    final String[] parameters = parameterList.toArray(new String[0]);    final Properties cleanUpConfig = new Properties();    cleanUpConfig.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 100);    cleanUpConfig.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "" + CLEANUP_CONSUMER_TIMEOUT);    final int exitCode = new StreamsResetter().run(parameters, cleanUpConfig);    Assert.assertEquals(0, exitCode);}
f15594
0
shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions
public void kafkatest_f15602_0() throws Exception
{    runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC);}
f15602
0
runSimpleCopyTest
private void kafkatest_f15603_0(final int numberOfRestarts, final String inputTopic, final String throughTopic, final String outputTopic) throws Exception
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Long, Long> input = builder.stream(inputTopic);    KStream<Long, Long> output = input;    if (throughTopic != null) {        output = input.through(throughTopic);    }    output.to(outputTopic);    final Properties properties = new Properties();    properties.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);    properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), 1);    properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.METADATA_MAX_AGE_CONFIG), "1000");    properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), "earliest");    for (int i = 0; i < numberOfRestarts; ++i) {        final Properties config = StreamsTestUtils.getStreamsConfig(applicationId, CLUSTER.bootstrapServers(), Serdes.LongSerde.class.getName(), Serdes.LongSerde.class.getName(), properties);        try (final KafkaStreams streams = new KafkaStreams(builder.build(), config)) {            streams.start();            final List<KeyValue<Long, Long>> inputData = prepareData(i * 100, i * 100 + 10L, 0L, 1L);            IntegrationTestUtils.produceKeyValuesSynchronously(inputTopic, inputData, TestUtils.producerConfig(CLUSTER.bootstrapServers(), LongSerializer.class, LongSerializer.class), CLUSTER.time);            final List<KeyValue<Long, Long>> committedRecords = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(TestUtils.consumerConfig(CLUSTER.bootstrapServers(), CONSUMER_GROUP_ID, LongDeserializer.class, LongDeserializer.class, Utils.mkProperties(Collections.singletonMap(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.name().toLowerCase(Locale.ROOT)))), outputTopic, inputData.size());            checkResultPerKey(committedRecords, inputData);        }    }}
f15603
0
checkResultPerKey
private void kafkatest_f15604_0(final List<KeyValue<Long, Long>> result, final List<KeyValue<Long, Long>> expectedResult)
{    final Set<Long> allKeys = new HashSet<>();    addAllKeys(allKeys, result);    addAllKeys(allKeys, expectedResult);    for (final Long key : allKeys) {        assertThat(getAllRecordPerKey(key, result), equalTo(getAllRecordPerKey(key, expectedResult)));    }}
f15604
0
getKafkaStreams
private KafkaStreams kafkatest_f15612_0(final boolean withState, final String appDir, final int numberOfStreamsThreads)
{    commitRequested = new AtomicInteger(0);    errorInjected = new AtomicBoolean(false);    gcInjected = new AtomicBoolean(false);    final StreamsBuilder builder = new StreamsBuilder();    String[] storeNames = null;    if (withState) {        storeNames = new String[] { storeName };        final StoreBuilder<KeyValueStore<Long, Long>> storeBuilder = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(storeName), Serdes.Long(), Serdes.Long()).withCachingEnabled();        builder.addStateStore(storeBuilder);    }    final KStream<Long, Long> input = builder.stream(MULTI_PARTITION_INPUT_TOPIC);    input.transform(new TransformerSupplier<Long, Long, KeyValue<Long, Long>>() {        @SuppressWarnings("unchecked")        @Override        public Transformer<Long, Long, KeyValue<Long, Long>> get() {            return new Transformer<Long, Long, KeyValue<Long, Long>>() {                ProcessorContext context;                KeyValueStore<Long, Long> state = null;                @Override                public void init(final ProcessorContext context) {                    this.context = context;                    if (withState) {                        state = (KeyValueStore<Long, Long>) context.getStateStore(storeName);                    }                }                @Override                public KeyValue<Long, Long> transform(final Long key, final Long value) {                    if (gcInjected.compareAndSet(true, false)) {                        while (doGC) {                            try {                                Thread.sleep(100);                            } catch (final InterruptedException e) {                                throw new RuntimeException(e);                            }                        }                    }                    if ((value + 1) % 10 == 0) {                        context.commit();                        commitRequested.incrementAndGet();                    }                    if (state != null) {                        Long sum = state.get(key);                        if (sum == null) {                            sum = value;                        } else {                            sum += value;                        }                        state.put(key, sum);                        state.flush();                    }                    if (errorInjected.compareAndSet(true, false)) {                        // only tries to fail once on one of the task                        throw new RuntimeException("Injected test exception.");                    }                    if (state != null) {                        return new KeyValue<>(key, state.get(key));                    } else {                        return new KeyValue<>(key, value);                    }                }                @Override                public void close() {                }            };        }    }, storeNames).to(SINGLE_PARTITION_OUTPUT_TOPIC);    final Properties properties = new Properties();    properties.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);    properties.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, numberOfStreamsThreads);    properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Long.MAX_VALUE);    properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.METADATA_MAX_AGE_CONFIG), "1000");    properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), "earliest");    properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG), 5 * 1000);    properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG), 5 * 1000 - 1);    properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG), MAX_POLL_INTERVAL_MS);    properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    properties.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath() + File.separator + appDir);    properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, "dummy:2142");    final Properties config = StreamsTestUtils.getStreamsConfig(applicationId, CLUSTER.bootstrapServers(), Serdes.LongSerde.class.getName(), Serdes.LongSerde.class.getName(), properties);    final KafkaStreams streams = new KafkaStreams(builder.build(), config);    streams.setUncaughtExceptionHandler((t, e) -> {        if (uncaughtException != null) {            e.printStackTrace(System.err);            fail("Should only get one uncaught exception from Streams.");        }        uncaughtException = e;    });    return streams;}
f15612
0
get
public Transformer<Long, Long, KeyValue<Long, Long>> kafkatest_f15613_0()
{    return new Transformer<Long, Long, KeyValue<Long, Long>>() {        ProcessorContext context;        KeyValueStore<Long, Long> state = null;        @Override        public void init(final ProcessorContext context) {            this.context = context;            if (withState) {                state = (KeyValueStore<Long, Long>) context.getStateStore(storeName);            }        }        @Override        public KeyValue<Long, Long> transform(final Long key, final Long value) {            if (gcInjected.compareAndSet(true, false)) {                while (doGC) {                    try {                        Thread.sleep(100);                    } catch (final InterruptedException e) {                        throw new RuntimeException(e);                    }                }            }            if ((value + 1) % 10 == 0) {                context.commit();                commitRequested.incrementAndGet();            }            if (state != null) {                Long sum = state.get(key);                if (sum == null) {                    sum = value;                } else {                    sum += value;                }                state.put(key, sum);                state.flush();            }            if (errorInjected.compareAndSet(true, false)) {                // only tries to fail once on one of the task                throw new RuntimeException("Injected test exception.");            }            if (state != null) {                return new KeyValue<>(key, state.get(key));            } else {                return new KeyValue<>(key, value);            }        }        @Override        public void close() {        }    };}
f15613
0
init
public void kafkatest_f15614_0(final ProcessorContext context)
{    this.context = context;    if (withState) {        state = (KeyValueStore<Long, Long>) context.getStateStore(storeName);    }}
f15614
0
setUp
public void kafkatest_f15623_0() throws IOException
{    final Properties props = new Properties();    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    props.put(ConsumerConfig.METADATA_MAX_AGE_CONFIG, "1000");    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsConfiguration = StreamsTestUtils.getStreamsConfig("testAutoOffsetId", CLUSTER.bootstrapServers(), STRING_SERDE_CLASSNAME, STRING_SERDE_CLASSNAME, props);    // Remove any state from previous test runs    IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);}
f15623
0
shouldOnlyReadRecordsWhereEarliestSpecifiedWithNoCommittedOffsetsWithGlobalAutoOffsetResetLatest
public void kafkatest_f15624_0() throws Exception
{    streamsConfiguration.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), "latest");    final List<String> expectedReceivedValues = Arrays.asList(topic1TestMessage, topic2TestMessage);    shouldOnlyReadForEarliest("_0", TOPIC_1_0, TOPIC_2_0, TOPIC_A_0, TOPIC_C_0, TOPIC_Y_0, TOPIC_Z_0, OUTPUT_TOPIC_0, expectedReceivedValues);}
f15624
0
shouldOnlyReadRecordsWhereEarliestSpecifiedWithNoCommittedOffsetsWithDefaultGlobalAutoOffsetResetEarliest
public void kafkatest_f15625_0() throws Exception
{    final List<String> expectedReceivedValues = Arrays.asList(topic1TestMessage, topic2TestMessage, topicYTestMessage, topicZTestMessage);    shouldOnlyReadForEarliest("_1", TOPIC_1_1, TOPIC_2_1, TOPIC_A_1, TOPIC_C_1, TOPIC_Y_1, TOPIC_Z_1, OUTPUT_TOPIC_1, expectedReceivedValues);}
f15625
0
before
public void kafkatest_f15633_0() throws Exception
{    builder = new StreamsBuilder();    createTopics();    streamsConfiguration = new Properties();    final String applicationId = "globalTableTopic-table-eos-test-" + testNo.incrementAndGet();    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());    streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    streamsConfiguration.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, "exactly_once");    globalTable = builder.globalTable(globalTableTopic, Consumed.with(Serdes.Long(), Serdes.String()), Materialized.<Long, String, KeyValueStore<Bytes, byte[]>>as(globalStore).withKeySerde(Serdes.Long()).withValueSerde(Serdes.String()));    final Consumed<String, Long> stringLongConsumed = Consumed.with(Serdes.String(), Serdes.Long());    stream = builder.stream(streamTopic, stringLongConsumed);    foreachAction = results::put;}
f15633
0
whenShuttingDown
public void kafkatest_f15634_0() throws Exception
{    if (kafkaStreams != null) {        kafkaStreams.close();    }    IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);}
f15634
0
shouldKStreamGlobalKTableLeftJoin
public void kafkatest_f15635_0() throws Exception
{    final KStream<String, String> streamTableJoin = stream.leftJoin(globalTable, keyMapper, joiner);    streamTableJoin.foreach(foreachAction);    produceInitialGlobalTableValues();    startStreams();    produceTopicValues(streamTopic);    final Map<String, String> expected = new HashMap<>();    expected.put("a", "1+A");    expected.put("b", "2+B");    expected.put("c", "3+C");    expected.put("d", "4+D");    expected.put("e", "5+null");    TestUtils.waitForCondition(() -> results.equals(expected), 30000L, "waiting for initial values");    produceGlobalTableValues();    final ReadOnlyKeyValueStore<Long, String> replicatedStore = kafkaStreams.store(globalStore, QueryableStoreTypes.keyValueStore());    TestUtils.waitForCondition(() -> "J".equals(replicatedStore.get(5L)), 30000, "waiting for data in replicated store");    produceTopicValues(streamTopic);    expected.put("a", "1+F");    expected.put("b", "2+G");    expected.put("c", "3+H");    expected.put("d", "4+I");    expected.put("e", "5+J");    TestUtils.waitForCondition(() -> results.equals(expected), 30000L, "waiting for final values");}
f15635
0
produceInitialGlobalTableValues
private void kafkatest_f15643_0() throws Exception
{    produceInitialGlobalTableValues(true);}
f15643
0
produceInitialGlobalTableValues
private void kafkatest_f15644_0(final boolean enableTransactions) throws Exception
{    final Properties properties = new Properties();    if (enableTransactions) {        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "someid");        properties.put(ProducerConfig.RETRIES_CONFIG, 1);    }    IntegrationTestUtils.produceKeyValuesSynchronously(globalTableTopic, Arrays.asList(new KeyValue<>(1L, "A"), new KeyValue<>(2L, "B"), new KeyValue<>(3L, "C"), new KeyValue<>(4L, "D")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), LongSerializer.class, StringSerializer.class, properties), mockTime, enableTransactions);}
f15644
0
produceGlobalTableValues
private void kafkatest_f15645_0() throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronously(globalTableTopic, Arrays.asList(new KeyValue<>(1L, "F"), new KeyValue<>(2L, "G"), new KeyValue<>(3L, "H"), new KeyValue<>(4L, "I"), new KeyValue<>(5L, "J")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), LongSerializer.class, StringSerializer.class, new Properties()), mockTime);}
f15645
0
produceTopicValues
private void kafkatest_f15653_0(final String topic) throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronously(topic, Arrays.asList(new KeyValue<>("a", 1L), new KeyValue<>("b", 2L), new KeyValue<>("c", 3L), new KeyValue<>("d", 4L), new KeyValue<>("e", 5L)), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, LongSerializer.class, new Properties()), mockTime);}
f15653
0
produceInitialGlobalTableValues
private void kafkatest_f15654_0() throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronously(globalTableTopic, Arrays.asList(new KeyValue<>(1L, "A"), new KeyValue<>(2L, "B"), new KeyValue<>(3L, "C"), new KeyValue<>(4L, "D")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), LongSerializer.class, StringSerializer.class), mockTime);}
f15654
0
produceGlobalTableValues
private void kafkatest_f15655_0() throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronously(globalTableTopic, Arrays.asList(new KeyValue<>(1L, "F"), new KeyValue<>(2L, "G"), new KeyValue<>(3L, "H"), new KeyValue<>(4L, "I"), new KeyValue<>(5L, "J")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), LongSerializer.class, StringSerializer.class, new Properties()), mockTime);}
f15655
0
close
public void kafkatest_f15663_0()
{    final List<String> keys = Arrays.asList("A", "B", "C", "D");    for (final String key : keys) {        // need to simulate thread slow in closing        Utils.sleep(1000);        retrievedValuesList.add(store.get(key));    }}
f15663
0
startKafkaCluster
public static void kafkatest_f15664_0() throws InterruptedException
{    CLUSTER.createTopics(DEFAULT_INPUT_TOPIC);}
f15664
0
before
public void kafkatest_f15665_0()
{    streamsProp = new Properties();    streamsProp.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    streamsProp.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    streamsProp.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    streamsProp.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());    streamsProp.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    streamsProp.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    streamsProp.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");}
f15665
0
whenShuttingDown
public void kafkatest_f15673_0() throws IOException
{    if (kafkaStreams != null) {        kafkaStreams.close();    }    IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);}
f15673
0
shouldReduce
public void kafkatest_f15674_0() throws Exception
{    produceMessages(System.currentTimeMillis());    groupedStream.reduce(reducer, Materialized.as("reduce-by-key")).toStream().to(outputTopic, Produced.with(Serdes.String(), Serdes.String()));    startStreams();    final long timestamp = System.currentTimeMillis();    produceMessages(timestamp);    validateReceivedMessages(new StringDeserializer(), new StringDeserializer(), Arrays.asList(new KeyValueTimestamp<>("A", "A:A", timestamp), new KeyValueTimestamp<>("B", "B:B", timestamp), new KeyValueTimestamp<>("C", "C:C", timestamp), new KeyValueTimestamp<>("D", "D:D", timestamp), new KeyValueTimestamp<>("E", "E:E", timestamp)));}
f15674
0
shouldReduceWindowed
public void kafkatest_f15675_0() throws Exception
{    final long firstBatchTimestamp = System.currentTimeMillis() - 1000;    produceMessages(firstBatchTimestamp);    final long secondBatchTimestamp = System.currentTimeMillis();    produceMessages(secondBatchTimestamp);    produceMessages(secondBatchTimestamp);    groupedStream.windowedBy(TimeWindows.of(ofMillis(500L))).reduce(reducer, Materialized.as("reduce-time-windows")).toStream((windowedKey, value) -> windowedKey.key() + "@" + windowedKey.window().start()).to(outputTopic, Produced.with(Serdes.String(), Serdes.String()));    startStreams();    final long firstBatchWindow = firstBatchTimestamp / 500 * 500;    final long secondBatchWindow = secondBatchTimestamp / 500 * 500;    validateReceivedMessages(new StringDeserializer(), new StringDeserializer(), Arrays.asList(new KeyValueTimestamp<>("A@" + firstBatchWindow, "A", firstBatchTimestamp), new KeyValueTimestamp<>("A@" + secondBatchWindow, "A:A", secondBatchTimestamp), new KeyValueTimestamp<>("B@" + firstBatchWindow, "B", firstBatchTimestamp), new KeyValueTimestamp<>("B@" + secondBatchWindow, "B:B", secondBatchTimestamp), new KeyValueTimestamp<>("C@" + firstBatchWindow, "C", firstBatchTimestamp), new KeyValueTimestamp<>("C@" + secondBatchWindow, "C:C", secondBatchTimestamp), new KeyValueTimestamp<>("D@" + firstBatchWindow, "D", firstBatchTimestamp), new KeyValueTimestamp<>("D@" + secondBatchWindow, "D:D", secondBatchTimestamp), new KeyValueTimestamp<>("E@" + firstBatchWindow, "E", firstBatchTimestamp), new KeyValueTimestamp<>("E@" + secondBatchWindow, "E:E", secondBatchTimestamp)));}
f15675
0
shouldReduce
public void kafkatest_f15683_0() throws Exception
{    produceMessages(mockTime.milliseconds());    groupedStream.reduce(reducer, Materialized.as("reduce-by-key")).toStream().to(outputTopic, Produced.with(Serdes.String(), Serdes.String()));    startStreams();    produceMessages(mockTime.milliseconds());    final List<KeyValueTimestamp<String, String>> results = receiveMessages(new StringDeserializer(), new StringDeserializer(), 10);    results.sort(KStreamAggregationIntegrationTest::compare);    assertThat(results, is(Arrays.asList(new KeyValueTimestamp("A", "A", mockTime.milliseconds()), new KeyValueTimestamp("A", "A:A", mockTime.milliseconds()), new KeyValueTimestamp("B", "B", mockTime.milliseconds()), new KeyValueTimestamp("B", "B:B", mockTime.milliseconds()), new KeyValueTimestamp("C", "C", mockTime.milliseconds()), new KeyValueTimestamp("C", "C:C", mockTime.milliseconds()), new KeyValueTimestamp("D", "D", mockTime.milliseconds()), new KeyValueTimestamp("D", "D:D", mockTime.milliseconds()), new KeyValueTimestamp("E", "E", mockTime.milliseconds()), new KeyValueTimestamp("E", "E:E", mockTime.milliseconds()))));}
f15683
0
compare
private static int kafkatest_f15684_0(final KeyValueTimestamp<K, V> o1, final KeyValueTimestamp<K, V> o2)
{    final int keyComparison = o1.key().compareTo(o2.key());    if (keyComparison == 0) {        final int valueComparison = o1.value().compareTo(o2.value());        if (valueComparison == 0) {            return Long.compare(o1.timestamp(), o2.timestamp());        }        return valueComparison;    }    return keyComparison;}
f15684
0
shouldReduceWindowed
public void kafkatest_f15685_0() throws Exception
{    final long firstBatchTimestamp = mockTime.milliseconds();    mockTime.sleep(1000);    produceMessages(firstBatchTimestamp);    final long secondBatchTimestamp = mockTime.milliseconds();    produceMessages(secondBatchTimestamp);    produceMessages(secondBatchTimestamp);    final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);    groupedStream.windowedBy(TimeWindows.of(ofMillis(500L))).reduce(reducer).toStream().to(outputTopic, Produced.with(windowedSerde, Serdes.String()));    startStreams();    final List<KeyValueTimestamp<Windowed<String>, String>> windowedOutput = receiveMessages(new TimeWindowedDeserializer<>(), new StringDeserializer(), String.class, 15);    // read from ConsoleConsumer    final String resultFromConsoleConsumer = readWindowedKeyedMessagesViaConsoleConsumer(new TimeWindowedDeserializer<String>(), new StringDeserializer(), String.class, 15, true);    final Comparator<KeyValueTimestamp<Windowed<String>, String>> comparator = Comparator.comparing((KeyValueTimestamp<Windowed<String>, String> o) -> o.key().key()).thenComparing(KeyValueTimestamp::value);    windowedOutput.sort(comparator);    final long firstBatchWindow = firstBatchTimestamp / 500 * 500;    final long secondBatchWindow = secondBatchTimestamp / 500 * 500;    final List<KeyValueTimestamp<Windowed<String>, String>> expectResult = Arrays.asList(new KeyValueTimestamp<>(new Windowed<>("A", new TimeWindow(firstBatchWindow, Long.MAX_VALUE)), "A", firstBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("A", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "A", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("A", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "A:A", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("B", new TimeWindow(firstBatchWindow, Long.MAX_VALUE)), "B", firstBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("B", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "B", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("B", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "B:B", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("C", new TimeWindow(firstBatchWindow, Long.MAX_VALUE)), "C", firstBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("C", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "C", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("C", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "C:C", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("D", new TimeWindow(firstBatchWindow, Long.MAX_VALUE)), "D", firstBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("D", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "D", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("D", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "D:D", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("E", new TimeWindow(firstBatchWindow, Long.MAX_VALUE)), "E", firstBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("E", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "E", secondBatchTimestamp), new KeyValueTimestamp<>(new Windowed<>("E", new TimeWindow(secondBatchWindow, Long.MAX_VALUE)), "E:E", secondBatchTimestamp));    assertThat(windowedOutput, is(expectResult));    final Set<String> expectResultString = new HashSet<>(expectResult.size());    for (final KeyValueTimestamp<Windowed<String>, String> eachRecord : expectResult) {        expectResultString.add("CreateTime:" + eachRecord.timestamp() + ", " + eachRecord.key() + ", " + eachRecord.value());    }    // check every message is contained in the expect result    final String[] allRecords = resultFromConsoleConsumer.split("\n");    for (final String record : allRecords) {        assertTrue(expectResultString.contains(record));    }}
f15685
0
init
public void kafkatest_f15693_0(final ProcessorContext context)
{    this.context = context;}
f15693
0
transform
public KeyValue<Object, Object> kafkatest_f15694_0(final Windowed<String> key, final Long value)
{    results.put(key, KeyValue.pair(value, context.timestamp()));    latch.countDown();    return null;}
f15694
0
shouldReduceSessionWindows
public void kafkatest_f15696_0() throws Exception
{    // something to do with time    final long sessionGap = 1000L;    final List<KeyValue<String, String>> t1Messages = Arrays.asList(new KeyValue<>("bob", "start"), new KeyValue<>("penny", "start"), new KeyValue<>("jo", "pause"), new KeyValue<>("emily", "pause"));    final long t1 = mockTime.milliseconds();    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, t1Messages, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t1);    final long t2 = t1 + (sessionGap / 2);    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Collections.singletonList(new KeyValue<>("emily", "resume")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t2);    final long t3 = t1 + sessionGap + 1;    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Arrays.asList(new KeyValue<>("bob", "pause"), new KeyValue<>("penny", "stop")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t3);    final long t4 = t3 + (sessionGap / 2);    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Arrays.asList(// bobs session continues    new KeyValue<>("bob", "resume"), // jo's starts new session    new KeyValue<>("jo", "resume")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t4);    final long t5 = t4 - 1;    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Collections.singletonList(// jo has late arrival    new KeyValue<>("jo", "late")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t5);    final Map<Windowed<String>, KeyValue<String, Long>> results = new HashMap<>();    final CountDownLatch latch = new CountDownLatch(13);    final String userSessionsStore = "UserSessionsStore";    builder.stream(userSessionsStream, Consumed.with(Serdes.String(), Serdes.String())).groupByKey(Grouped.with(Serdes.String(), Serdes.String())).windowedBy(SessionWindows.with(ofMillis(sessionGap))).reduce((value1, value2) -> value1 + ":" + value2, Materialized.as(userSessionsStore)).toStream().transform(() -> new Transformer<Windowed<String>, String, KeyValue<Object, Object>>() {        private ProcessorContext context;        @Override        public void init(final ProcessorContext context) {            this.context = context;        }        @Override        public KeyValue<Object, Object> transform(final Windowed<String> key, final String value) {            results.put(key, KeyValue.pair(value, context.timestamp()));            latch.countDown();            return null;        }        @Override        public void close() {        }    });    startStreams();    latch.await(30, TimeUnit.SECONDS);    // verify correct data received    assertThat(results.get(new Windowed<>("bob", new SessionWindow(t1, t1))), equalTo(KeyValue.pair("start", t1)));    assertThat(results.get(new Windowed<>("penny", new SessionWindow(t1, t1))), equalTo(KeyValue.pair("start", t1)));    assertThat(results.get(new Windowed<>("jo", new SessionWindow(t1, t1))), equalTo(KeyValue.pair("pause", t1)));    assertThat(results.get(new Windowed<>("jo", new SessionWindow(t5, t4))), equalTo(KeyValue.pair("resume:late", t4)));    assertThat(results.get(new Windowed<>("emily", new SessionWindow(t1, t2))), equalTo(KeyValue.pair("pause:resume", t2)));    assertThat(results.get(new Windowed<>("bob", new SessionWindow(t3, t4))), equalTo(KeyValue.pair("pause:resume", t4)));    assertThat(results.get(new Windowed<>("penny", new SessionWindow(t3, t3))), equalTo(KeyValue.pair("stop", t3)));    // verify can query data via IQ    final ReadOnlySessionStore<String, String> sessionStore = kafkaStreams.store(userSessionsStore, QueryableStoreTypes.sessionStore());    final KeyValueIterator<Windowed<String>, String> bob = sessionStore.fetch("bob");    assertThat(bob.next(), equalTo(KeyValue.pair(new Windowed<>("bob", new SessionWindow(t1, t1)), "start")));    assertThat(bob.next(), equalTo(KeyValue.pair(new Windowed<>("bob", new SessionWindow(t3, t4)), "pause:resume")));    assertFalse(bob.hasNext());}
f15696
0
startStreams
private void kafkatest_f15706_0()
{    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();}
f15706
0
receiveMessages
private List<KeyValueTimestamp<K, V>> kafkatest_f15707_0(final Deserializer<K> keyDeserializer, final Deserializer<V> valueDeserializer, final int numMessages) throws InterruptedException
{    return receiveMessages(keyDeserializer, valueDeserializer, null, numMessages);}
f15707
0
receiveMessages
private List<KeyValueTimestamp<K, V>> kafkatest_f15708_0(final Deserializer<K> keyDeserializer, final Deserializer<V> valueDeserializer, final Class innerClass, final int numMessages) throws InterruptedException
{    final Properties consumerProperties = new Properties();    consumerProperties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    consumerProperties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "kgroupedstream-test-" + testNo);    consumerProperties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    consumerProperties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializer.getClass().getName());    consumerProperties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializer.getClass().getName());    if (keyDeserializer instanceof TimeWindowedDeserializer || keyDeserializer instanceof SessionWindowedDeserializer) {        consumerProperties.setProperty(StreamsConfig.DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS, Serdes.serdeFrom(innerClass).getClass().getName());    }    return IntegrationTestUtils.waitUntilMinKeyValueWithTimestampRecordsReceived(consumerProperties, outputTopic, numMessages, 60 * 1000);}
f15708
0
shouldFlatTransform
public void kafkatest_f15717_0()
{    stream.flatTransform(() -> new Transformer<Integer, Integer, Iterable<KeyValue<Integer, Integer>>>() {        private KeyValueStore<Integer, Integer> state;        @SuppressWarnings("unchecked")        @Override        public void init(final ProcessorContext context) {            state = (KeyValueStore<Integer, Integer>) context.getStateStore(stateStoreName);        }        @Override        public Iterable<KeyValue<Integer, Integer>> transform(final Integer key, final Integer value) {            final List<KeyValue<Integer, Integer>> result = new ArrayList<>();            state.putIfAbsent(key, 0);            Integer storedValue = state.get(key);            for (int i = 0; i < 3; i++) {                result.add(new KeyValue<>(key + i, value + storedValue++));            }            state.put(key, storedValue);            return result;        }        @Override        public void close() {        }    }, "myTransformState").foreach(action);    final List<KeyValue<Integer, Integer>> expected = Arrays.asList(KeyValue.pair(1, 1), KeyValue.pair(2, 2), KeyValue.pair(3, 3), KeyValue.pair(2, 2), KeyValue.pair(3, 3), KeyValue.pair(4, 4), KeyValue.pair(3, 3), KeyValue.pair(4, 4), KeyValue.pair(5, 5), KeyValue.pair(2, 4), KeyValue.pair(3, 5), KeyValue.pair(4, 6), KeyValue.pair(2, 9), KeyValue.pair(3, 10), KeyValue.pair(4, 11), KeyValue.pair(1, 6), KeyValue.pair(2, 7), KeyValue.pair(3, 8));    verifyResult(expected);}
f15717
0
init
public void kafkatest_f15718_0(final ProcessorContext context)
{    state = (KeyValueStore<Integer, Integer>) context.getStateStore(stateStoreName);}
f15718
0
transform
public Iterable<KeyValue<Integer, Integer>> kafkatest_f15719_0(final Integer key, final Integer value)
{    final List<KeyValue<Integer, Integer>> result = new ArrayList<>();    state.putIfAbsent(key, 0);    Integer storedValue = state.get(key);    for (int i = 0; i < 3; i++) {        result.add(new KeyValue<>(key + i, value + storedValue++));    }    state.put(key, storedValue);    return result;}
f15719
0
init
public void kafkatest_f15730_0(final ProcessorContext context)
{    state = (KeyValueStore<Integer, Integer>) context.getStateStore("myTransformState");}
f15730
0
transform
public Iterable<Integer> kafkatest_f15731_0(final Integer key, final Integer value)
{    final List<Integer> result = new ArrayList<>();    state.putIfAbsent(key, 0);    Integer storedValue = state.get(key);    for (int i = 0; i < 3; i++) {        result.add(value + storedValue++);    }    state.put(key, storedValue);    return result;}
f15731
0
shouldFlatTransformValuesWithValueTransformerWithoutKey
public void kafkatest_f15733_0()
{    stream.flatTransformValues(() -> new ValueTransformer<Integer, Iterable<Integer>>() {        private KeyValueStore<Integer, Integer> state;        @Override        public void init(final ProcessorContext context) {            state = (KeyValueStore<Integer, Integer>) context.getStateStore("myTransformState");        }        @Override        public Iterable<Integer> transform(final Integer value) {            final List<Integer> result = new ArrayList<>();            state.putIfAbsent(value, 0);            Integer counter = state.get(value);            for (int i = 0; i < 3; i++) {                result.add(++counter);            }            state.put(value, counter);            return result;        }        @Override        public void close() {        }    }, "myTransformState").foreach(action);    final List<KeyValue<Integer, Integer>> expected = Arrays.asList(KeyValue.pair(1, 1), KeyValue.pair(1, 2), KeyValue.pair(1, 3), KeyValue.pair(2, 1), KeyValue.pair(2, 2), KeyValue.pair(2, 3), KeyValue.pair(3, 1), KeyValue.pair(3, 2), KeyValue.pair(3, 3), KeyValue.pair(2, 4), KeyValue.pair(2, 5), KeyValue.pair(2, 6), KeyValue.pair(2, 4), KeyValue.pair(2, 5), KeyValue.pair(2, 6), KeyValue.pair(1, 7), KeyValue.pair(1, 8), KeyValue.pair(1, 9));    verifyResult(expected);}
f15733
0
shouldRestoreAndProgressWhenTopicNotWrittenToDuringRestoration
public void kafkatest_f15742_0() throws Exception
{    try {        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        streamsOne.start();        produceKeyValues("a", "b", "c");        assertNumberValuesRead(readKeyValues, expectedInitialResultsMap, "Table did not read all values");        streamsOne.close();        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        streamsOne.start();        produceKeyValues("f", "g", "h");        final Map<String, String> expectedValues = createExpectedResultsMap("a", "b", "c", "f", "g", "h");        assertNumberValuesRead(readKeyValues, expectedValues, "Table did not get all values after restart");    } finally {        streamsOne.close(Duration.ofSeconds(5));    }}
f15742
0
assertNumberValuesRead
private void kafkatest_f15743_0(final Map<String, String> valueMap, final Map<String, String> expectedMap, final String errorMessage) throws InterruptedException
{    TestUtils.waitForCondition(() -> valueMap.equals(expectedMap), 30 * 1000L, errorMessage);}
f15743
0
produceKeyValues
private void kafkatest_f15744_0(final String... keys) throws ExecutionException, InterruptedException
{    final List<KeyValue<String, String>> keyValueList = new ArrayList<>();    for (final String key : keys) {        keyValueList.add(new KeyValue<>(key, key + "1"));    }    IntegrationTestUtils.produceKeyValuesSynchronously(SOURCE_TOPIC, keyValueList, PRODUCER_CONFIG, time);}
f15744
0
closeApplication
private void kafkatest_f15754_0() throws Exception
{    kafkaStreams.close();    kafkaStreams.cleanUp();    IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);    final long timeout = 60000;    TestUtils.waitForCondition(() -> kafkaStreams.state() == State.NOT_RUNNING, timeout, () -> "Kafka Streams application did not reach state NOT_RUNNING in " + timeout + " ms");}
f15754
0
shouldAddMetricsOnAllLevels
public void kafkatest_f15755_0() throws Exception
{    builder.stream(STREAM_INPUT, Consumed.with(Serdes.Integer(), Serdes.String())).to(STREAM_OUTPUT_1, Produced.with(Serdes.Integer(), Serdes.String()));    builder.table(STREAM_OUTPUT_1, Materialized.as(Stores.inMemoryKeyValueStore(MY_STORE_IN_MEMORY)).withCachingEnabled()).toStream().to(STREAM_OUTPUT_2);    builder.table(STREAM_OUTPUT_2, Materialized.as(Stores.persistentKeyValueStore(MY_STORE_PERSISTENT_KEY_VALUE)).withCachingEnabled()).toStream().to(STREAM_OUTPUT_3);    builder.table(STREAM_OUTPUT_3, Materialized.as(Stores.lruMap(MY_STORE_LRU_MAP, 10000)).withCachingEnabled()).toStream().to(STREAM_OUTPUT_4);    startApplication();    checkThreadLevelMetrics();    checkTaskLevelMetrics();    checkProcessorLevelMetrics();    checkKeyValueStoreMetricsByGroup(STREAM_STORE_IN_MEMORY_STATE_METRICS);    checkKeyValueStoreMetricsByGroup(STREAM_STORE_ROCKSDB_STATE_METRICS);    checkKeyValueStoreMetricsByGroup(STREAM_STORE_IN_MEMORY_LRU_STATE_METRICS);    checkRocksDBMetricsByTag("rocksdb-state-id");    checkCacheMetrics();    closeApplication();    checkMetricsDeregistration();}
f15755
0
shouldAddMetricsForWindowStore
public void kafkatest_f15756_0() throws Exception
{    final Duration windowSize = Duration.ofMillis(50);    builder.stream(STREAM_INPUT, Consumed.with(Serdes.Integer(), Serdes.String())).groupByKey().windowedBy(TimeWindows.of(windowSize).grace(Duration.ZERO)).aggregate(() -> 0L, (aggKey, newValue, aggValue) -> aggValue, Materialized.<Integer, Long, WindowStore<Bytes, byte[]>>as(TIME_WINDOWED_AGGREGATED_STREAM_STORE).withValueSerde(Serdes.Long()).withRetention(windowSize)).toStream().map((key, value) -> KeyValue.pair(value, value)).to(STREAM_OUTPUT_1, Produced.with(Serdes.Long(), Serdes.Long()));    produceRecordsForTwoSegments(windowSize);    startApplication();    waitUntilAllRecordsAreConsumed();    checkWindowStoreMetrics();    checkRocksDBMetricsByTag("rocksdb-window-state-id");    closeApplication();    checkMetricsDeregistration();}
f15756
0
checkMetricsDeregistration
private void kafkatest_f15764_0()
{    final List<Metric> listMetricAfterClosingApp = new ArrayList<Metric>(kafkaStreams.metrics().values()).stream().filter(m -> m.metricName().group().contains(STREAM_STRING)).collect(Collectors.toList());    assertThat(listMetricAfterClosingApp.size(), is(0));}
f15764
0
checkCacheMetrics
private void kafkatest_f15765_0()
{    final List<Metric> listMetricCache = new ArrayList<Metric>(kafkaStreams.metrics().values()).stream().filter(m -> m.metricName().group().equals(STREAM_CACHE_NODE_METRICS)).collect(Collectors.toList());    checkMetricByName(listMetricCache, HIT_RATIO_AVG, 6);    checkMetricByName(listMetricCache, HIT_RATIO_MIN, 6);    checkMetricByName(listMetricCache, HIT_RATIO_MAX, 6);}
f15765
0
checkWindowStoreMetrics
private void kafkatest_f15766_0()
{    final List<Metric> listMetricStore = new ArrayList<Metric>(kafkaStreams.metrics().values()).stream().filter(m -> m.metricName().group().equals(STREAM_STORE_WINDOW_ROCKSDB_STATE_METRICS)).collect(Collectors.toList());    checkMetricByName(listMetricStore, PUT_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, PUT_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_LATENCY_AVG, 0);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_LATENCY_MAX, 0);    checkMetricByName(listMetricStore, GET_LATENCY_AVG, 0);    checkMetricByName(listMetricStore, GET_LATENCY_MAX, 0);    checkMetricByName(listMetricStore, DELETE_LATENCY_AVG, 0);    checkMetricByName(listMetricStore, DELETE_LATENCY_MAX, 0);    checkMetricByName(listMetricStore, PUT_ALL_LATENCY_AVG, 0);    checkMetricByName(listMetricStore, PUT_ALL_LATENCY_MAX, 0);    checkMetricByName(listMetricStore, ALL_LATENCY_AVG, 0);    checkMetricByName(listMetricStore, ALL_LATENCY_MAX, 0);    checkMetricByName(listMetricStore, RANGE_LATENCY_AVG, 0);    checkMetricByName(listMetricStore, RANGE_LATENCY_MAX, 0);    checkMetricByName(listMetricStore, FLUSH_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, FLUSH_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, RESTORE_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, RESTORE_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, PUT_RATE, 2);    checkMetricByName(listMetricStore, PUT_TOTAL, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_RATE, 0);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_TOTAL, 0);    checkMetricByName(listMetricStore, GET_RATE, 0);    checkMetricByName(listMetricStore, DELETE_RATE, 0);    checkMetricByName(listMetricStore, DELETE_TOTAL, 0);    checkMetricByName(listMetricStore, PUT_ALL_RATE, 0);    checkMetricByName(listMetricStore, PUT_ALL_TOTAL, 0);    checkMetricByName(listMetricStore, ALL_RATE, 0);    checkMetricByName(listMetricStore, ALL_TOTAL, 0);    checkMetricByName(listMetricStore, RANGE_RATE, 0);    checkMetricByName(listMetricStore, RANGE_TOTAL, 0);    checkMetricByName(listMetricStore, FLUSH_RATE, 2);    checkMetricByName(listMetricStore, FLUSH_TOTAL, 2);    checkMetricByName(listMetricStore, RESTORE_RATE, 2);    checkMetricByName(listMetricStore, RESTORE_TOTAL, 2);}
f15766
0
retryOnExceptionWithTimeout
private void kafkatest_f15774_0(final long pollInterval, final long timeout, final TimeUnit timeUnit, final Runnable runnable) throws InterruptedException
{    final long expectedEnd = System.currentTimeMillis() + timeUnit.toMillis(timeout);    while (true) {        try {            runnable.run();            return;        } catch (final Throwable t) {            if (expectedEnd <= System.currentTimeMillis()) {                throw new AssertionError(t);            }            Thread.sleep(timeUnit.toMillis(pollInterval));        }    }}
f15774
0
waitForKafkaStreamssToEnterRunningState
private void kafkatest_f15775_0(final Collection<KafkaStreams> kafkaStreamss, final long time, final TimeUnit timeUnit) throws InterruptedException
{    final long expectedEnd = System.currentTimeMillis() + timeUnit.toMillis(time);    kafkaStreamsStatesLock.lock();    try {        while (!kafkaStreamss.stream().allMatch(kafkaStreams -> kafkaStreamsStates.get(kafkaStreams) == State.RUNNING)) {            if (expectedEnd <= System.currentTimeMillis()) {                fail("one or more kafkaStreamss did not enter RUNNING in a timely manner");            }            final long millisRemaining = Math.max(1, expectedEnd - System.currentTimeMillis());            kafkaStreamsStateUpdate.await(millisRemaining, TimeUnit.MILLISECONDS);        }    } finally {        kafkaStreamsStatesLock.unlock();    }}
f15775
0
createKafkaStreams
private KafkaStreams kafkatest_f15776_0(final StreamsBuilder builder, final Properties config)
{    final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(config), config);    kafkaStreamsStatesLock.lock();    try {        kafkaStreamsStates.put(kafkaStreams, kafkaStreams.state());    } finally {        kafkaStreamsStatesLock.unlock();    }    kafkaStreams.setStateListener((newState, oldState) -> {        kafkaStreamsStatesLock.lock();        try {            kafkaStreamsStates.put(kafkaStreams, newState);            if (newState == State.RUNNING) {                if (kafkaStreamsStates.values().stream().allMatch(state -> state == State.RUNNING)) {                    kafkaStreamsStateUpdate.signalAll();                }            }        } finally {            kafkaStreamsStatesLock.unlock();        }    });    return kafkaStreams;}
f15776
0
shutdown
public void kafkatest_f15786_0()
{    if (kafkaStreams != null) {        kafkaStreams.close(Duration.ofSeconds(30));    }}
f15786
0
shouldRestoreState
public void kafkatest_f15787_0() throws Exception
{    // produce some data to input topic    final List<KeyValue<Integer, Integer>> messages = new ArrayList<>();    for (int i = 0; i < 1000; i++) {        messages.add(new KeyValue<>(i, i));    }    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(INPUT_TOPIC, messages, TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, IntegerSerializer.class), time.milliseconds());    kafkaStreams.start();    TestUtils.waitForCondition(new RepartitionTopicCreatedWithExpectedConfigs(), 60000, "Repartition topic " + REPARTITION_TOPIC + " not created with the expected configs after 60000 ms.");    TestUtils.waitForCondition(new RepartitionTopicVerified(currentSize -> currentSize > 0), 60000, "Repartition topic " + REPARTITION_TOPIC + " not received data after 60000 ms.");    // we need long enough timeout to by-pass the log manager's InitialTaskDelayMs, which is hard-coded on server side    TestUtils.waitForCondition(new RepartitionTopicVerified(currentSize -> currentSize <= PURGE_SEGMENT_BYTES), 60000, "Repartition topic " + REPARTITION_TOPIC + " not purged data after 60000 ms.");}
f15787
0
createTopics
private void kafkatest_f15788_0() throws Exception
{    streamOne = streamOne + "-" + testNo;    streamConcurrent = streamConcurrent + "-" + testNo;    streamThree = streamThree + "-" + testNo;    outputTopic = outputTopic + "-" + testNo;    outputTopicConcurrent = outputTopicConcurrent + "-" + testNo;    outputTopicConcurrentWindowed = outputTopicConcurrentWindowed + "-" + testNo;    outputTopicThree = outputTopicThree + "-" + testNo;    streamTwo = streamTwo + "-" + testNo;    CLUSTER.createTopics(streamOne, streamConcurrent);    CLUSTER.createTopic(streamTwo, STREAM_TWO_PARTITIONS, NUM_REPLICAS);    CLUSTER.createTopic(streamThree, STREAM_THREE_PARTITIONS, 1);    CLUSTER.createTopics(outputTopic, outputTopicConcurrent, outputTopicConcurrentWindowed, outputTopicThree);}
f15788
0
getStream
public final KafkaStreams kafkatest_f15796_0()
{    return myStream;}
f15796
0
getStateListener
 final KafkaStreamsTest.StateListenerStub kafkatest_f15797_0()
{    return stateListener;}
f15797
0
verifyAllKVKeys
private void kafkatest_f15798_0(final StreamRunnable[] streamRunnables, final KafkaStreams streams, final KafkaStreamsTest.StateListenerStub stateListenerStub, final Set<String> keys, final String storeName) throws Exception
{    for (final String key : keys) {        TestUtils.waitForCondition(() -> {            try {                final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());                if (metadata == null || metadata.equals(StreamsMetadata.NOT_AVAILABLE)) {                    return false;                }                final int index = metadata.hostInfo().port();                final KafkaStreams streamsWithKey = streamRunnables[index].getStream();                final ReadOnlyKeyValueStore<String, Long> store = streamsWithKey.store(storeName, QueryableStoreTypes.keyValueStore());                return store != null && store.get(key) != null;            } catch (final IllegalStateException e) {                // Kafka Streams instance may have closed but rebalance hasn't happened                return false;            } catch (final InvalidStateStoreException e) {                // there must have been at least one rebalance state                assertTrue(stateListenerStub.mapStates.get(KafkaStreams.State.REBALANCING) >= 1);                return false;            }        }, 120000, "waiting for metadata, store and value to be non null");    }}
f15798
0
shouldBeAbleToQueryMapValuesAfterFilterState
public void kafkatest_f15806_0() throws Exception
{    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());    final StreamsBuilder builder = new StreamsBuilder();    final String[] keys = { "hello", "goodbye", "welcome", "go", "kafka" };    final Set<KeyValue<String, String>> batch1 = new HashSet<>(Arrays.asList(new KeyValue<>(keys[0], "1"), new KeyValue<>(keys[1], "1"), new KeyValue<>(keys[2], "3"), new KeyValue<>(keys[3], "5"), new KeyValue<>(keys[4], "2")));    final Set<KeyValue<String, Long>> expectedBatch1 = new HashSet<>(Collections.singleton(new KeyValue<>(keys[4], 2L)));    IntegrationTestUtils.produceKeyValuesSynchronously(streamOne, batch1, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), mockTime);    final Predicate<String, String> filterPredicate = (key, value) -> key.contains("kafka");    final KTable<String, String> t1 = builder.table(streamOne);    final KTable<String, String> t2 = t1.filter(filterPredicate, Materialized.as("queryFilter"));    final KTable<String, Long> t3 = t2.mapValues((ValueMapper<String, Long>) Long::valueOf, Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as("queryMapValues").withValueSerde(Serdes.Long()));    t3.toStream().to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();    waitUntilAtLeastNumRecordProcessed(outputTopic, 1);    final ReadOnlyKeyValueStore<String, Long> myMapStore = kafkaStreams.store("queryMapValues", QueryableStoreTypes.keyValueStore());    for (final KeyValue<String, Long> expectedEntry : expectedBatch1) {        assertEquals(myMapStore.get(expectedEntry.key), expectedEntry.value);    }    for (final KeyValue<String, String> batchEntry : batch1) {        final KeyValue<String, Long> batchEntryMapValue = new KeyValue<>(batchEntry.key, Long.valueOf(batchEntry.value));        if (!expectedBatch1.contains(batchEntryMapValue)) {            assertNull(myMapStore.get(batchEntry.key));        }    }}
f15806
0
verifyCanQueryState
private void kafkatest_f15807_0(final int cacheSizeBytes) throws Exception
{    streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, cacheSizeBytes);    final StreamsBuilder builder = new StreamsBuilder();    final String[] keys = { "hello", "goodbye", "welcome", "go", "kafka" };    final Set<KeyValue<String, String>> batch1 = new TreeSet<>(stringComparator);    batch1.addAll(Arrays.asList(new KeyValue<>(keys[0], "hello"), new KeyValue<>(keys[1], "goodbye"), new KeyValue<>(keys[2], "welcome"), new KeyValue<>(keys[3], "go"), new KeyValue<>(keys[4], "kafka")));    final Set<KeyValue<String, Long>> expectedCount = new TreeSet<>(stringLongComparator);    for (final String key : keys) {        expectedCount.add(new KeyValue<>(key, 1L));    }    IntegrationTestUtils.produceKeyValuesSynchronously(streamOne, batch1, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), mockTime);    final KStream<String, String> s1 = builder.stream(streamOne);    // Non Windowed    final String storeName = "my-count";    s1.groupByKey().count(Materialized.as(storeName)).toStream().to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));    final String windowStoreName = "windowed-count";    s1.groupByKey().windowedBy(TimeWindows.of(ofMillis(WINDOW_SIZE))).count(Materialized.as(windowStoreName));    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();    waitUntilAtLeastNumRecordProcessed(outputTopic, 1);    final ReadOnlyKeyValueStore<String, Long> myCount = kafkaStreams.store(storeName, QueryableStoreTypes.keyValueStore());    final ReadOnlyWindowStore<String, Long> windowStore = kafkaStreams.store(windowStoreName, QueryableStoreTypes.windowStore());    verifyCanGetByKey(keys, expectedCount, expectedCount, windowStore, myCount);    verifyRangeAndAll(expectedCount, myCount);}
f15807
0
shouldNotMakeStoreAvailableUntilAllStoresAvailable
public void kafkatest_f15808_0() throws Exception
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> stream = builder.stream(streamThree);    final String storeName = "count-by-key";    stream.groupByKey().count(Materialized.as(storeName));    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();    final KeyValue<String, String> hello = KeyValue.pair("hello", "hello");    IntegrationTestUtils.produceKeyValuesSynchronously(streamThree, Arrays.asList(hello, hello, hello, hello, hello, hello, hello, hello), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), mockTime);    final int maxWaitMs = 30000;    TestUtils.waitForCondition(new WaitForStore(storeName), maxWaitMs, "waiting for store " + storeName);    final ReadOnlyKeyValueStore<String, Long> store = kafkaStreams.store(storeName, QueryableStoreTypes.keyValueStore());    TestUtils.waitForCondition(() -> new Long(8).equals(store.get("hello")), maxWaitMs, "wait for count to be 8");    // close stream    kafkaStreams.close();    // start again    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();    // make sure we never get any value other than 8 for hello    TestUtils.waitForCondition(() -> {        try {            assertEquals(Long.valueOf(8L), kafkaStreams.store(storeName, QueryableStoreTypes.<String, Long>keyValueStore()).get("hello"));            return true;        } catch (final InvalidStateStoreException ise) {            return false;        }    }, maxWaitMs, "waiting for store " + storeName);}
f15808
0
fetchMap
private Map<String, Long> kafkatest_f15816_0(final ReadOnlyWindowStore<String, Long> store, final String key)
{    final WindowStoreIterator<Long> fetch = store.fetch(key, ofEpochMilli(0), ofEpochMilli(System.currentTimeMillis()));    if (fetch.hasNext()) {        final KeyValue<Long, Long> next = fetch.next();        return Collections.singletonMap(key, next.value);    }    return Collections.emptyMap();}
f15816
0
incrementIteration
private synchronized void kafkatest_f15817_0()
{    currIteration++;}
f15817
0
getCurrIteration
 synchronized int kafkatest_f15818_0()
{    return currIteration;}
f15818
0
subscribe
public void kafkatest_f15826_0(final Pattern topics, final ConsumerRebalanceListener listener)
{    super.subscribe(topics, new TheConsumerRebalanceListener(assignedTopics, listener));}
f15826
0
createTopic
private String kafkatest_f15827_0(final int suffix) throws InterruptedException
{    final String outputTopic = "outputTopic_" + suffix;    CLUSTER.createTopic(outputTopic);    return outputTopic;}
f15827
0
testRegexMatchesTopicsAWhenDeleted
public void kafkatest_f15828_0() throws Exception
{    final Serde<String> stringSerde = Serdes.String();    final List<String> expectedFirstAssignment = Arrays.asList("TEST-TOPIC-A", "TEST-TOPIC-B");    final List<String> expectedSecondAssignment = Collections.singletonList("TEST-TOPIC-B");    CLUSTER.createTopics("TEST-TOPIC-A", "TEST-TOPIC-B");    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> pattern1Stream = builder.stream(Pattern.compile("TEST-TOPIC-[A-Z]"));    pattern1Stream.to(outputTopic, Produced.with(stringSerde, stringSerde));    final List<String> assignedTopics = new CopyOnWriteArrayList<>();    streams = new KafkaStreams(builder.build(), streamsConfiguration, new DefaultKafkaClientSupplier() {        @Override        public Consumer<byte[], byte[]> getConsumer(final Map<String, Object> config) {            return new KafkaConsumer<byte[], byte[]>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer()) {                @Override                public void subscribe(final Pattern topics, final ConsumerRebalanceListener listener) {                    super.subscribe(topics, new TheConsumerRebalanceListener(assignedTopics, listener));                }            };        }    });    streams.start();    TestUtils.waitForCondition(() -> assignedTopics.equals(expectedFirstAssignment), STREAM_TASKS_NOT_UPDATED);    CLUSTER.deleteTopic("TEST-TOPIC-A");    TestUtils.waitForCondition(() -> assignedTopics.equals(expectedSecondAssignment), STREAM_TASKS_NOT_UPDATED);}
f15828
0
getConsumer
public Consumer<byte[], byte[]> kafkatest_f15836_0(final Map<String, Object> config)
{    return new KafkaConsumer<byte[], byte[]>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer()) {        @Override        public void subscribe(final Pattern topics, final ConsumerRebalanceListener listener) {            super.subscribe(topics, new TheConsumerRebalanceListener(followerAssignment, listener));        }    };}
f15836
0
subscribe
public void kafkatest_f15837_0(final Pattern topics, final ConsumerRebalanceListener listener)
{    super.subscribe(topics, new TheConsumerRebalanceListener(followerAssignment, listener));}
f15837
0
testNoMessagesSentExceptionFromOverlappingPatterns
public void kafkatest_f15838_0() throws Exception
{    final String fMessage = "fMessage";    final String fooMessage = "fooMessage";    final Serde<String> stringSerde = Serdes.String();    final StreamsBuilder builder = new StreamsBuilder();    // overlapping patterns here, no messages should be sent as TopologyException    // will be thrown when the processor topology is built.    final KStream<String, String> pattern1Stream = builder.stream(Pattern.compile("foo.*"));    final KStream<String, String> pattern2Stream = builder.stream(Pattern.compile("f.*"));    pattern1Stream.to(outputTopic, Produced.with(stringSerde, stringSerde));    pattern2Stream.to(outputTopic, Produced.with(stringSerde, stringSerde));    final AtomicBoolean expectError = new AtomicBoolean(false);    streams = new KafkaStreams(builder.build(), streamsConfiguration);    streams.setStateListener((newState, oldState) -> {        if (newState == KafkaStreams.State.ERROR) {            expectError.set(true);        }    });    streams.start();    final Properties producerConfig = TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class);    IntegrationTestUtils.produceValuesSynchronously(FA_TOPIC, Collections.singleton(fMessage), producerConfig, mockTime);    IntegrationTestUtils.produceValuesSynchronously(FOO_TOPIC, Collections.singleton(fooMessage), producerConfig, mockTime);    final Properties consumerConfig = TestUtils.consumerConfig(CLUSTER.bootstrapServers(), StringDeserializer.class, StringDeserializer.class);    try {        IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerConfig, outputTopic, 2, 5000);        throw new IllegalStateException("This should not happen: an assertion error should have been thrown before this.");    } catch (final AssertionError e) {    // this is fine    }    assertThat(expectError.get(), is(true));}
f15838
0
getCountOfRepartitionTopicsFound
private int kafkatest_f15846_0(final String topologyString)
{    final Matcher matcher = repartitionTopicPattern.matcher(topologyString);    final List<String> repartitionTopicsFound = new ArrayList<>();    while (matcher.find()) {        repartitionTopicsFound.add(matcher.group());    }    return repartitionTopicsFound.size();}
f15846
0
getKeyValues
private List<KeyValue<String, String>> kafkatest_f15847_0()
{    final List<KeyValue<String, String>> keyValueList = new ArrayList<>();    final String[] keys = new String[] { "a", "b", "c" };    final String[] values = new String[] { "foo", "bar", "baz" };    for (final String key : keys) {        for (final String value : values) {            keyValueList.add(KeyValue.pair(key, value));        }    }    return keyValueList;}
f15847
0
process
public void kafkatest_f15848_0(final String key, final String value)
{    valueList.add(value);}
f15848
0
getClientSslConfig
 Map<String, Object> kafkatest_f15856_0()
{    return null;}
f15856
0
before
public void kafkatest_f15857_0() throws Exception
{    testId = TEST_ID;    cluster = CLUSTER;    prepareTest();}
f15857
0
after
public void kafkatest_f15858_0() throws Exception
{    cleanupTest();}
f15858
0
shouldNotAllowToResetWhenIntermediateTopicAbsent
public void kafkatest_f15866_0() throws Exception
{    super.shouldNotAllowToResetWhenIntermediateTopicAbsent();}
f15866
0
getClientSslConfig
 Map<String, Object> kafkatest_f15867_0()
{    return sslConfig;}
f15867
0
before
public void kafkatest_f15868_0() throws Exception
{    testId = TEST_ID;    cluster = CLUSTER;    prepareTest();}
f15868
0
onRestoreEnd
public void kafkatest_f15878_0(final TopicPartition topicPartition, final String storeName, final long totalRestored)
{    restored.addAndGet(totalRestored);}
f15878
0
shouldRestoreStateFromChangelogTopic
public void kafkatest_f15879_0() throws Exception
{    final AtomicInteger numReceived = new AtomicInteger(0);    final StreamsBuilder builder = new StreamsBuilder();    final Properties props = props(APPID);    // restoring from 1000 to 5000, and then process from 5000 to 10000 on each of the two partitions    final int offsetCheckpointed = 1000;    createStateForRestoration(APPID + "-store-changelog");    createStateForRestoration(INPUT_STREAM);    final StateDirectory stateDirectory = new StateDirectory(new StreamsConfig(props), new MockTime(), true);    new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 0)), ".checkpoint")).write(Collections.singletonMap(new TopicPartition(APPID + "-store-changelog", 0), (long) offsetCheckpointed));    new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 1)), ".checkpoint")).write(Collections.singletonMap(new TopicPartition(APPID + "-store-changelog", 1), (long) offsetCheckpointed));    final CountDownLatch startupLatch = new CountDownLatch(1);    final CountDownLatch shutdownLatch = new CountDownLatch(1);    builder.table(INPUT_STREAM, Consumed.with(Serdes.Integer(), Serdes.Integer()), Materialized.as("store")).toStream().foreach((key, value) -> {        if (numReceived.incrementAndGet() == numberOfKeys) {            shutdownLatch.countDown();        }    });    kafkaStreams = new KafkaStreams(builder.build(), props);    kafkaStreams.setStateListener((newState, oldState) -> {        if (newState == KafkaStreams.State.RUNNING && oldState == KafkaStreams.State.REBALANCING) {            startupLatch.countDown();        }    });    final AtomicLong restored = new AtomicLong(0);    kafkaStreams.setGlobalStateRestoreListener(new StateRestoreListener() {        @Override        public void onRestoreStart(final TopicPartition topicPartition, final String storeName, final long startingOffset, final long endingOffset) {        }        @Override        public void onBatchRestored(final TopicPartition topicPartition, final String storeName, final long batchEndOffset, final long numRestored) {        }        @Override        public void onRestoreEnd(final TopicPartition topicPartition, final String storeName, final long totalRestored) {            restored.addAndGet(totalRestored);        }    });    kafkaStreams.start();    assertTrue(startupLatch.await(30, TimeUnit.SECONDS));    assertThat(restored.get(), equalTo((long) numberOfKeys - 2 * offsetCheckpointed));    assertTrue(shutdownLatch.await(30, TimeUnit.SECONDS));    assertThat(numReceived.get(), equalTo(numberOfKeys));}
f15879
0
onRestoreEnd
public void kafkatest_f15882_0(final TopicPartition topicPartition, final String storeName, final long totalRestored)
{    restored.addAndGet(totalRestored);}
f15882
0
exception
public Exception kafkatest_f15891_0()
{    return exception;}
f15891
0
result
 SmokeTestDriver.VerificationResult kafkatest_f15892_0()
{    return result;}
f15892
0
shouldWorkWithRebalance
public void kafkatest_f15893_0() throws InterruptedException
{    int numClientsCreated = 0;    final ArrayList<SmokeTestClient> clients = new ArrayList<>();    IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, SmokeTestDriver.topics());    final String bootstrapServers = CLUSTER.bootstrapServers();    final Driver driver = new Driver(bootstrapServers, 10, 1000);    driver.start();    System.out.println("started driver");    final Properties props = new Properties();    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);    // cycle out Streams instances as long as the test is running.    while (driver.isAlive()) {        // take a nap        Thread.sleep(1000);        // add a new client        final SmokeTestClient smokeTestClient = new SmokeTestClient("streams-" + numClientsCreated++);        clients.add(smokeTestClient);        smokeTestClient.start(props);        while (!clients.get(clients.size() - 1).started()) {            Thread.sleep(100);        }        // let the oldest client die of "natural causes"        if (clients.size() >= 3) {            clients.remove(0).closeAsync();        }    }    try {        // wait for verification to finish        driver.join();    } finally {        // whether or not the assertions failed, tell all the streams instances to stop        for (final SmokeTestClient client : clients) {            client.closeAsync();        }        // then, wait for them to stop        for (final SmokeTestClient client : clients) {            client.close();        }    }    // check to make sure that it actually succeeded    if (driver.exception() != null) {        driver.exception().printStackTrace();        throw new AssertionError(driver.exception());    }    Assert.assertTrue(driver.result().result(), driver.result().passed());}
f15893
0
setStateListenersForVerification
private void kafkatest_f15903_0(final Predicate<ThreadMetadata> taskCondition)
{    client1.setStateListener((newState, oldState) -> {        if (newState == State.RUNNING && client1.localThreadsMetadata().stream().allMatch(taskCondition)) {            client1IsOk = true;        }    });    client2.setStateListener((newState, oldState) -> {        if (newState == State.RUNNING && client2.localThreadsMetadata().stream().allMatch(taskCondition)) {            client2IsOk = true;        }    });}
f15903
0
startClients
private void kafkatest_f15904_0()
{    client1.start();    client2.start();}
f15904
0
waitUntilBothClientAreOK
private void kafkatest_f15905_0(final String message) throws Exception
{    TestUtils.waitForCondition(() -> client1IsOk && client2IsOk, 30 * 1000, message + ": " + "Client 1 is " + (!client1IsOk ? "NOT " : "") + "OK, " + "client 2 is " + (!client2IsOk ? "NOT " : "") + "OK.");}
f15905
0
shouldMigrateKeyValueStoreToTimestampedKeyValueStoreUsingPapi
private void kafkatest_f15913_0(final boolean persistentStore) throws Exception
{    final StreamsBuilder streamsBuilderForOldStore = new StreamsBuilder();    streamsBuilderForOldStore.addStateStore(Stores.keyValueStoreBuilder(persistentStore ? Stores.persistentKeyValueStore(STORE_NAME) : Stores.inMemoryKeyValueStore(STORE_NAME), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(KeyValueProcessor::new, STORE_NAME);    final Properties props = props();    kafkaStreams = new KafkaStreams(streamsBuilderForOldStore.build(), props);    kafkaStreams.start();    processKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(1, 1L)));    processKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(1, 2L)));    final long lastUpdateKeyOne = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processKeyValueAndVerifyPlainCount(2, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L)));    final long lastUpdateKeyTwo = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processKeyValueAndVerifyPlainCount(3, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L), KeyValue.pair(3, 1L)));    final long lastUpdateKeyThree = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L), KeyValue.pair(3, 1L), KeyValue.pair(4, 1L)));    processKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L), KeyValue.pair(3, 1L), KeyValue.pair(4, 2L)));    processKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L), KeyValue.pair(3, 1L), KeyValue.pair(4, 3L)));    final long lastUpdateKeyFour = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    kafkaStreams.close();    kafkaStreams = null;    final StreamsBuilder streamsBuilderForNewStore = new StreamsBuilder();    streamsBuilderForNewStore.addStateStore(Stores.timestampedKeyValueStoreBuilder(persistentStore ? Stores.persistentTimestampedKeyValueStore(STORE_NAME) : Stores.inMemoryKeyValueStore(STORE_NAME), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(TimestampedKeyValueProcessor::new, STORE_NAME);    kafkaStreams = new KafkaStreams(streamsBuilderForNewStore.build(), props);    kafkaStreams.start();    verifyCountWithTimestamp(1, 2L, lastUpdateKeyOne);    verifyCountWithTimestamp(2, 1L, lastUpdateKeyTwo);    verifyCountWithTimestamp(3, 1L, lastUpdateKeyThree);    verifyCountWithTimestamp(4, 3L, lastUpdateKeyFour);    final long currentTime = CLUSTER.time.milliseconds();    processKeyValueAndVerifyCountWithTimestamp(1, currentTime + 42L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(2, ValueAndTimestamp.make(1L, lastUpdateKeyTwo)), KeyValue.pair(3, ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(4, ValueAndTimestamp.make(3L, lastUpdateKeyFour))));    processKeyValueAndVerifyCountWithTimestamp(2, currentTime + 45L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(2, ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(4, ValueAndTimestamp.make(3L, lastUpdateKeyFour))));    // can process "out of order" record for different key    processKeyValueAndVerifyCountWithTimestamp(4, currentTime + 21L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(2, ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(4, ValueAndTimestamp.make(4L, currentTime + 21L))));    processKeyValueAndVerifyCountWithTimestamp(4, currentTime + 42L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(2, ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(4, ValueAndTimestamp.make(5L, currentTime + 42L))));    // out of order (same key) record should not reduce result timestamp    processKeyValueAndVerifyCountWithTimestamp(4, currentTime + 10L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(2, ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(4, ValueAndTimestamp.make(6L, currentTime + 42L))));    kafkaStreams.close();}
f15913
0
shouldProxyKeyValueStoreToTimestampedKeyValueStoreUsingPapi
public void kafkatest_f15914_0() throws Exception
{    final StreamsBuilder streamsBuilderForOldStore = new StreamsBuilder();    streamsBuilderForOldStore.addStateStore(Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(STORE_NAME), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(KeyValueProcessor::new, STORE_NAME);    final Properties props = props();    kafkaStreams = new KafkaStreams(streamsBuilderForOldStore.build(), props);    kafkaStreams.start();    processKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(1, 1L)));    processKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(1, 2L)));    processKeyValueAndVerifyPlainCount(2, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L)));    processKeyValueAndVerifyPlainCount(3, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L), KeyValue.pair(3, 1L)));    processKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L), KeyValue.pair(3, 1L), KeyValue.pair(4, 1L)));    processKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L), KeyValue.pair(3, 1L), KeyValue.pair(4, 2L)));    processKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(1, 2L), KeyValue.pair(2, 1L), KeyValue.pair(3, 1L), KeyValue.pair(4, 3L)));    kafkaStreams.close();    kafkaStreams = null;    final StreamsBuilder streamsBuilderForNewStore = new StreamsBuilder();    streamsBuilderForNewStore.addStateStore(Stores.timestampedKeyValueStoreBuilder(Stores.persistentKeyValueStore(STORE_NAME), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(TimestampedKeyValueProcessor::new, STORE_NAME);    kafkaStreams = new KafkaStreams(streamsBuilderForNewStore.build(), props);    kafkaStreams.start();    verifyCountWithSurrogateTimestamp(1, 2L);    verifyCountWithSurrogateTimestamp(2, 1L);    verifyCountWithSurrogateTimestamp(3, 1L);    verifyCountWithSurrogateTimestamp(4, 3L);    processKeyValueAndVerifyCount(1, 42L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(2, ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(4, ValueAndTimestamp.make(3L, -1L))));    processKeyValueAndVerifyCount(2, 45L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(2, ValueAndTimestamp.make(2L, -1L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(4, ValueAndTimestamp.make(3L, -1L))));    // can process "out of order" record for different key    processKeyValueAndVerifyCount(4, 21L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(2, ValueAndTimestamp.make(2L, -1L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(4, ValueAndTimestamp.make(4L, -1L))));    processKeyValueAndVerifyCount(4, 42L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(2, ValueAndTimestamp.make(2L, -1L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(4, ValueAndTimestamp.make(5L, -1L))));    // out of order (same key) record should not reduce result timestamp    processKeyValueAndVerifyCount(4, 10L, asList(KeyValue.pair(1, ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(2, ValueAndTimestamp.make(2L, -1L)), KeyValue.pair(3, ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(4, ValueAndTimestamp.make(6L, -1L))));    kafkaStreams.close();}
f15914
0
processKeyValueAndVerifyPlainCount
private void kafkatest_f15915_0(final K key, final List<KeyValue<Integer, Object>> expectedStoreContent) throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronously(inputStream, singletonList(KeyValue.pair(key, 0)), TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, IntegerSerializer.class), CLUSTER.time);    TestUtils.waitForCondition(() -> {        try {            final ReadOnlyKeyValueStore<K, V> store = kafkaStreams.store(STORE_NAME, QueryableStoreTypes.keyValueStore());            try (final KeyValueIterator<K, V> all = store.all()) {                final List<KeyValue<K, V>> storeContent = new LinkedList<>();                while (all.hasNext()) {                    storeContent.add(all.next());                }                return storeContent.equals(expectedStoreContent);            }        } catch (final Exception swallow) {            swallow.printStackTrace();            System.err.println(swallow.getMessage());            return false;        }    }, 60_000L, "Could not get expected result in time.");}
f15915
0
shouldProxyWindowStoreToTimestampedWindowStoreUsingPapi
public void kafkatest_f15923_0() throws Exception
{    final StreamsBuilder streamsBuilderForOldStore = new StreamsBuilder();    streamsBuilderForOldStore.addStateStore(Stores.windowStoreBuilder(Stores.persistentWindowStore(STORE_NAME, Duration.ofMillis(1000L), Duration.ofMillis(1000L), false), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(WindowedProcessor::new, STORE_NAME);    final Properties props = props();    kafkaStreams = new KafkaStreams(streamsBuilderForOldStore.build(), props);    kafkaStreams.start();    processWindowedKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 1L)));    processWindowedKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L)));    processWindowedKeyValueAndVerifyPlainCount(2, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L)));    processWindowedKeyValueAndVerifyPlainCount(3, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L)));    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 1L)));    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 2L)));    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 3L)));    kafkaStreams.close();    kafkaStreams = null;    final StreamsBuilder streamsBuilderForNewStore = new StreamsBuilder();    streamsBuilderForNewStore.addStateStore(Stores.timestampedWindowStoreBuilder(Stores.persistentWindowStore(STORE_NAME, Duration.ofMillis(1000L), Duration.ofMillis(1000L), false), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(TimestampedWindowedProcessor::new, STORE_NAME);    kafkaStreams = new KafkaStreams(streamsBuilderForNewStore.build(), props);    kafkaStreams.start();    verifyWindowedCountWithSurrogateTimestamp(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L);    verifyWindowedCountWithSurrogateTimestamp(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L);    verifyWindowedCountWithSurrogateTimestamp(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L);    verifyWindowedCountWithSurrogateTimestamp(new Windowed<>(4, new TimeWindow(0L, 1000L)), 3L);    processKeyValueAndVerifyWindowedCountWithTimestamp(1, 42L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, -1L))));    processKeyValueAndVerifyWindowedCountWithTimestamp(2, 45L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, -1L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, -1L))));    // can process "out of order" record for different key    processKeyValueAndVerifyWindowedCountWithTimestamp(4, 21L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, -1L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(4L, -1L))));    processKeyValueAndVerifyWindowedCountWithTimestamp(4, 42L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, -1L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(5L, -1L))));    // out of order (same key) record should not reduce result timestamp    processKeyValueAndVerifyWindowedCountWithTimestamp(4, 10L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, -1L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, -1L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, -1L)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(6L, -1L))));    // test new segment    processKeyValueAndVerifyWindowedCountWithTimestamp(10, 100001L, singletonList(KeyValue.pair(new Windowed<>(10, new TimeWindow(100000L, 101000L)), ValueAndTimestamp.make(1L, -1L))));    kafkaStreams.close();}
f15923
0
processWindowedKeyValueAndVerifyPlainCount
private void kafkatest_f15924_0(final K key, final List<KeyValue<Windowed<Integer>, Object>> expectedStoreContent) throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronously(inputStream, singletonList(KeyValue.pair(key, 0)), TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, IntegerSerializer.class), CLUSTER.time);    TestUtils.waitForCondition(() -> {        try {            final ReadOnlyWindowStore<K, V> store = kafkaStreams.store(STORE_NAME, QueryableStoreTypes.windowStore());            try (final KeyValueIterator<Windowed<K>, V> all = store.all()) {                final List<KeyValue<Windowed<K>, V>> storeContent = new LinkedList<>();                while (all.hasNext()) {                    storeContent.add(all.next());                }                return storeContent.equals(expectedStoreContent);            }        } catch (final Exception swallow) {            swallow.printStackTrace();            System.err.println(swallow.getMessage());            return false;        }    }, 60_000L, "Could not get expected result in time.");}
f15924
0
verifyWindowedCountWithSurrogateTimestamp
private void kafkatest_f15925_0(final Windowed<K> key, final long value) throws Exception
{    TestUtils.waitForCondition(() -> {        try {            final ReadOnlyWindowStore<K, ValueAndTimestamp<Long>> store = kafkaStreams.store(STORE_NAME, QueryableStoreTypes.timestampedWindowStore());            final ValueAndTimestamp<Long> count = store.fetch(key.key(), key.window().start());            return count.value() == value && count.timestamp() == -1L;        } catch (final Exception swallow) {            swallow.printStackTrace();            System.err.println(swallow.getMessage());            return false;        }    }, 60_000L, "Could not get expected result in time.");}
f15925
0
process
public void kafkatest_f15935_0(final Integer key, final Integer value)
{    final long newCount;    final Long oldCount = store.fetch(key, key < 10 ? 0L : 100000L);    if (oldCount != null) {        newCount = oldCount + 1L;    } else {        newCount = 1L;    }    store.put(key, newCount, key < 10 ? 0L : 100000L);}
f15935
0
init
public void kafkatest_f15937_0(final ProcessorContext context)
{    this.context = context;    store = (TimestampedWindowStore<Integer, Long>) context.getStateStore(STORE_NAME);}
f15937
0
process
public void kafkatest_f15938_0(final Integer key, final Integer value)
{    final long newCount;    final ValueAndTimestamp<Long> oldCountWithTimestamp = store.fetch(key, key < 10 ? 0L : 100000L);    final long newTimestamp;    if (oldCountWithTimestamp == null) {        newCount = 1L;        newTimestamp = context.timestamp();    } else {        newCount = oldCountWithTimestamp.value() + 1L;        newTimestamp = Math.max(oldCountWithTimestamp.timestamp(), context.timestamp());    }    store.put(key, ValueAndTimestamp.make(newCount, newTimestamp), key < 10 ? 0L : 100000L);}
f15938
0
testMultiInner
public void kafkatest_f15947_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-multi-inner");    final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-b-a", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-a", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-b-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-a-a", 9L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-a-b", 9L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-b-a", 9L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-b-b", 9L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-c-a", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-c-b", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-c-a", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-c-b", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-a", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-b", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-b-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-a-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-b-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-c-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-c-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L)), null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-d-a", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-d-b", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-d-c", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-d-a", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-d-b", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-d-c", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-d-a", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-d-b", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-d-c", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-b-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-a-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-b-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-c-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-c-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-d-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-d-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-d-d", 14L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-a-a", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-a-b", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-a-c", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-a-d", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-b-a", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-b-b", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-b-c", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-b-d", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-c-a", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-c-b", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-c-c", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-c-d", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-a", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-b", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-c", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-d", 15L)));    leftStream.join(rightStream, valueJoiner, JoinWindows.of(ofSeconds(10))).join(rightStream, valueJoiner, JoinWindows.of(ofSeconds(10))).to(OUTPUT_TOPIC);    runTest(expectedResult);}
f15947
0
prepareTopology
public void kafkatest_f15948_0() throws InterruptedException
{    super.prepareEnvironment();    appID = "stream-table-join-integration-test";    builder = new StreamsBuilder();    rightTable = builder.table(INPUT_TOPIC_RIGHT);    leftStream = builder.stream(INPUT_TOPIC_LEFT);}
f15948
0
testShouldAutoShutdownOnIncompleteMetadata
public void kafkatest_f15949_0() throws InterruptedException
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-incomplete");    final KStream<Long, String> notExistStream = builder.stream(INPUT_TOPIC_LEFT + "-not-existed");    final KTable<Long, String> aggregatedTable = notExistStream.leftJoin(rightTable, valueJoiner).groupBy((key, value) -> key).reduce((value1, value2) -> value1 + value2);    // Write the (continuously updating) results to the output topic.    aggregatedTable.toStream().to(OUTPUT_TOPIC);    final KafkaStreamsWrapper streams = new KafkaStreamsWrapper(builder.build(), STREAMS_CONFIG);    final IntegrationTestUtils.StateListenerStub listener = new IntegrationTestUtils.StateListenerStub();    streams.setStreamThreadStateListener(listener);    streams.start();    TestUtils.waitForCondition(listener::transitToPendingShutdownSeen, "Did not seen thread state transited to PENDING_SHUTDOWN");    streams.close();    assertTrue(listener.transitToPendingShutdownSeen());}
f15949
0
raiseExceptionIfAny
 void kafkatest_f15958_0()
{    final Throwable exception = firstException.get();    if (exception != null) {        throw new AssertionError("Got an exception during run", exception);    }}
f15958
0
verifyOutput
private void kafkatest_f15959_0(final String topic, final List<KeyValueTimestamp<String, Long>> keyValueTimestamps)
{    final Properties properties = mkProperties(mkMap(mkEntry(ConsumerConfig.GROUP_ID_CONFIG, "test-group"), mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ((Deserializer<String>) STRING_DESERIALIZER).getClass().getName()), mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ((Deserializer<Long>) LONG_DESERIALIZER).getClass().getName())));    IntegrationTestUtils.verifyKeyValueTimestamps(properties, topic, keyValueTimestamps);}
f15959
0
verifyOutput
private void kafkatest_f15960_0(final String topic, final Set<KeyValueTimestamp<String, Long>> keyValueTimestamps)
{    final Properties properties = mkProperties(mkMap(mkEntry(ConsumerConfig.GROUP_ID_CONFIG, "test-group"), mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ((Deserializer<String>) STRING_DESERIALIZER).getClass().getName()), mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ((Deserializer<Long>) LONG_DESERIALIZER).getClass().getName())));    IntegrationTestUtils.verifyKeyValueTimestamps(properties, topic, keyValueTimestamps);}
f15960
0
shouldShutdownWhenBytesConstraintIsViolated
public void kafkatest_f15968_0() throws InterruptedException
{    final String testId = "-shouldShutdownWhenBytesConstraintIsViolated";    final String appId = getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;    final String input = "input" + testId;    final String outputSuppressed = "output-suppressed" + testId;    final String outputRaw = "output-raw" + testId;    cleanStateBeforeTest(CLUSTER, input, outputRaw, outputSuppressed);    final StreamsBuilder builder = new StreamsBuilder();    final KTable<String, Long> valueCounts = buildCountsTable(input, builder);    valueCounts.suppress(untilTimeLimit(ofMillis(MAX_VALUE), maxBytes(200L).shutDownWhenFull())).toStream().to(outputSuppressed, Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().to(outputRaw, Produced.with(STRING_SERDE, Serdes.Long()));    final Properties streamsConfig = getStreamsConfig(appId);    final KafkaStreams driver = IntegrationTestUtils.getStartedStreams(streamsConfig, builder, true);    try {        produceSynchronously(input, asList(new KeyValueTimestamp<>("k1", "v1", scaledTime(0L)), new KeyValueTimestamp<>("k1", "v2", scaledTime(1L)), new KeyValueTimestamp<>("k2", "v1", scaledTime(2L)), new KeyValueTimestamp<>("x", "x", scaledTime(3L))));        verifyErrorShutdown(driver);    } finally {        driver.close();        cleanStateAfterTest(CLUSTER, driver);    }}
f15968
0
getStreamsConfig
private static Properties kafkatest_f15969_0(final String appId)
{    return mkProperties(mkMap(mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId), mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()), mkEntry(StreamsConfig.POLL_MS_CONFIG, Integer.toString(COMMIT_INTERVAL)), mkEntry(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Integer.toString(COMMIT_INTERVAL)), mkEntry(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, AT_LEAST_ONCE), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath())));}
f15969
0
scaledTime
private static long kafkatest_f15970_0(final long unscaledTime)
{    return COMMIT_INTERVAL * 2 * unscaledTime;}
f15970
0
testInnerInner
public void kafkatest_f15978_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-inner-inner");    if (cacheEnabled) {        leftTable.join(rightTable, valueJoiner).join(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(true)).to(OUTPUT_TOPIC);        runTest(expectedFinalMultiJoinResult, storeName);    } else {        // FIXME: the duplicate below for all the multi-joins        // are due to KAFKA-6443, should be updated once it is fixed.        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 7L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L)), // correct would be -> new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 11L)        null, // we don't get correct value, because of self-join of `rightTable`        null, null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-d", 15L)));        leftTable.join(rightTable, valueJoiner).join(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
f15978
0
testInnerLeft
public void kafkatest_f15979_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-inner-left");    if (cacheEnabled) {        leftTable.join(rightTable, valueJoiner).leftJoin(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(true)).to(OUTPUT_TOPIC);        runTest(expectedFinalMultiJoinResult, storeName);    } else {        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 7L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 11L)), null, null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-d", 15L)));        leftTable.join(rightTable, valueJoiner).leftJoin(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
f15979
0
testInnerOuter
public void kafkatest_f15980_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-inner-outer");    if (cacheEnabled) {        leftTable.join(rightTable, valueJoiner).outerJoin(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(true)).to(OUTPUT_TOPIC);        runTest(expectedFinalMultiJoinResult, storeName);    } else {        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-b", 7L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 8L)), null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 11L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 11L)), null, null, null, Arrays.asList(// incorrect result `null-d` is caused by self-join of `rightTable`        new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-d", 15L)));        leftTable.join(rightTable, valueJoiner).outerJoin(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
f15980
0
putIfAbsent
private void kafkatest_f15988_0(final Properties props, final String propertyKey, final Object propertyValue)
{    if (!props.containsKey(propertyKey)) {        brokerConfig.put(propertyKey, propertyValue);    }}
f15988
0
stop
private void kafkatest_f15989_0()
{    for (final KafkaEmbedded broker : brokers) {        broker.stop();    }    zookeeper.shutdown();}
f15989
0
zKConnectString
public String kafkatest_f15990_0()
{    return "127.0.0.1:" + zookeeper.port();}
f15990
0
deleteTopic
public void kafkatest_f15998_0(final String topic) throws InterruptedException
{    deleteTopicsAndWait(-1L, topic);}
f15998
0
deleteTopicAndWait
public void kafkatest_f15999_0(final String topic) throws InterruptedException
{    deleteTopicsAndWait(TOPIC_DELETION_TIMEOUT, topic);}
f15999
0
deleteTopicAndWait
public void kafkatest_f16000_0(final long timeoutMs, final String topic) throws InterruptedException
{    deleteTopicsAndWait(timeoutMs, topic);}
f16000
0
conditionMet
public boolean kafkatest_f16008_0()
{    final Set<String> allTopics = new HashSet<>(JavaConverters.setAsJavaSetConverter(brokers[0].kafkaServer().zkClient().getAllTopicsInCluster()).asJava());    return !allTopics.removeAll(deletedTopics);}
f16008
0
conditionMet
public boolean kafkatest_f16009_0()
{    final Set<String> allTopics = JavaConverters.setAsJavaSetConverter(brokers[0].kafkaServer().zkClient().getAllTopicsInCluster()).asJava();    return allTopics.equals(remainingTopics);}
f16009
0
brokers
private List<KafkaServer> kafkatest_f16010_0()
{    final List<KafkaServer> servers = new ArrayList<>();    for (final KafkaEmbedded broker : brokers) {        servers.add(broker.kafkaServer());    }    return servers;}
f16010
0
produceKeyValuesSynchronously
public static void kafkatest_f16018_0(final String topic, final Collection<KeyValue<K, V>> records, final Properties producerConfig, final Headers headers, final Time time) throws ExecutionException, InterruptedException
{    produceKeyValuesSynchronously(topic, records, producerConfig, headers, time, false);}
f16018
0
produceKeyValuesSynchronously
public static void kafkatest_f16019_0(final String topic, final Collection<KeyValue<K, V>> records, final Properties producerConfig, final Time time, final boolean enableTransactions) throws ExecutionException, InterruptedException
{    produceKeyValuesSynchronously(topic, records, producerConfig, null, time, enableTransactions);}
f16019
0
produceKeyValuesSynchronously
public static void kafkatest_f16020_0(final String topic, final Collection<KeyValue<K, V>> records, final Properties producerConfig, final Headers headers, final Time time, final boolean enableTransactions) throws ExecutionException, InterruptedException
{    for (final KeyValue<K, V> record : records) {        produceKeyValuesSynchronouslyWithTimestamp(topic, Collections.singleton(record), producerConfig, headers, time.milliseconds(), enableTransactions);        time.sleep(1L);    }}
f16020
0
waitForCompletion
public static void kafkatest_f16028_0(final KafkaStreams streams, final int expectedPartitions, final int timeoutMilliseconds)
{    final long start = System.currentTimeMillis();    while (true) {        int lagMetrics = 0;        double totalLag = 0.0;        for (final Metric metric : streams.metrics().values()) {            if (metric.metricName().name().equals("records-lag")) {                lagMetrics++;                totalLag += ((Number) metric.metricValue()).doubleValue();            }        }        if (lagMetrics >= expectedPartitions && totalLag == 0.0) {            return;        }        if (System.currentTimeMillis() - start >= timeoutMilliseconds) {            throw new RuntimeException(String.format("Timed out waiting for completion. lagMetrics=[%s/%s] totalLag=[%s]", lagMetrics, expectedPartitions, totalLag));        }    }}
f16028
0
waitUntilMinRecordsReceived
public static List<ConsumerRecord<K, V>> kafkatest_f16029_0(final Properties consumerConfig, final String topic, final int expectedNumRecords) throws InterruptedException
{    return waitUntilMinRecordsReceived(consumerConfig, topic, expectedNumRecords, DEFAULT_TIMEOUT);}
f16029
0
waitUntilMinRecordsReceived
public static List<ConsumerRecord<K, V>> kafkatest_f16030_0(final Properties consumerConfig, final String topic, final int expectedNumRecords, final long waitTime) throws InterruptedException
{    final List<ConsumerRecord<K, V>> accumData = new ArrayList<>();    try (final Consumer<K, V> consumer = createConsumer(consumerConfig)) {        final TestCondition valuesRead = () -> {            final List<ConsumerRecord<K, V>> readData = readRecords(topic, consumer, waitTime, expectedNumRecords);            accumData.addAll(readData);            return accumData.size() >= expectedNumRecords;        };        final String conditionDetails = "Did not receive all " + expectedNumRecords + " records from topic " + topic;        TestUtils.waitForCondition(valuesRead, waitTime, conditionDetails);    }    return accumData;}
f16030
0
waitUntilFinalKeyValueRecordsReceived
private static List<T> kafkatest_f16038_0(final Properties consumerConfig, final String topic, final List<T> expectedRecords, final long waitTime, final boolean withTimestamp) throws InterruptedException
{    final List<T> accumData = new ArrayList<>();    try (final Consumer<K, V> consumer = createConsumer(consumerConfig)) {        final TestCondition valuesRead = () -> {            final List<T> readData;            if (withTimestamp) {                readData = (List<T>) readKeyValuesWithTimestamp(topic, consumer, waitTime, expectedRecords.size());            } else {                readData = (List<T>) readKeyValues(topic, consumer, waitTime, expectedRecords.size());            }            accumData.addAll(readData);            // filter out all intermediate records we don't want            final List<T> accumulatedActual = accumData.stream().filter(expectedRecords::contains).collect(Collectors.toList());            // still need to check that for each key, the ordering is expected            final Map<K, List<T>> finalAccumData = new HashMap<>();            for (final T kv : accumulatedActual) {                finalAccumData.computeIfAbsent((K) (withTimestamp ? ((KeyValueTimestamp) kv).key() : ((KeyValue) kv).key), key -> new ArrayList<>()).add(kv);            }            final Map<K, List<T>> finalExpected = new HashMap<>();            for (final T kv : expectedRecords) {                finalExpected.computeIfAbsent((K) (withTimestamp ? ((KeyValueTimestamp) kv).key() : ((KeyValue) kv).key), key -> new ArrayList<>()).add(kv);            }            // and the last record received matches the last expected record            return finalAccumData.equals(finalExpected);        };        final String conditionDetails = "Did not receive all " + expectedRecords + " records from topic " + topic;        TestUtils.waitForCondition(valuesRead, waitTime, conditionDetails);    }    return accumData;}
f16038
0
waitUntilMinValuesRecordsReceived
public static List<V> kafkatest_f16039_0(final Properties consumerConfig, final String topic, final int expectedNumRecords) throws InterruptedException
{    return waitUntilMinValuesRecordsReceived(consumerConfig, topic, expectedNumRecords, DEFAULT_TIMEOUT);}
f16039
0
waitUntilMinValuesRecordsReceived
public static List<V> kafkatest_f16040_0(final Properties consumerConfig, final String topic, final int expectedNumRecords, final long waitTime) throws InterruptedException
{    final List<V> accumData = new ArrayList<>();    try (final Consumer<Object, V> consumer = createConsumer(consumerConfig)) {        final TestCondition valuesRead = () -> {            final List<V> readData = readValues(topic, consumer, waitTime, expectedNumRecords);            accumData.addAll(readData);            return accumData.size() >= expectedNumRecords;        };        final String conditionDetails = "Did not receive all " + expectedNumRecords + " records from topic " + topic;        TestUtils.waitForCondition(valuesRead, waitTime, conditionDetails);    }    return accumData;}
f16040
0
readKeyValues
public static List<KeyValue<K, V>> kafkatest_f16048_0(final String topic, final Properties consumerConfig, final long waitTime, final int maxMessages)
{    final List<KeyValue<K, V>> consumedValues;    try (final Consumer<K, V> consumer = createConsumer(consumerConfig)) {        consumedValues = readKeyValues(topic, consumer, waitTime, maxMessages);    }    return consumedValues;}
f16048
0
getStartedStreams
public static KafkaStreams kafkatest_f16049_0(final Properties streamsConfig, final StreamsBuilder builder, final boolean clean)
{    final KafkaStreams driver = new KafkaStreams(builder.build(), streamsConfig);    if (clean) {        driver.cleanUp();    }    driver.start();    return driver;}
f16049
0
readValues
private static List<V> kafkatest_f16050_0(final String topic, final Consumer<Object, V> consumer, final long waitTime, final int maxMessages)
{    final List<V> returnList = new ArrayList<>();    final List<KeyValue<Object, V>> kvs = readKeyValues(topic, consumer, waitTime, maxMessages);    for (final KeyValue<?, V> kv : kvs) {        returnList.add(kv.value);    }    return returnList;}
f16050
0
zookeeperConnect
public String kafkatest_f16058_0()
{    return effectiveConfig.getProperty("zookeeper.connect", DEFAULT_ZK_CONNECT);}
f16058
0
stop
public voidf16059_1)
{        kafka.shutdown();    kafka.awaitShutdown();        try {        Utils.delete(logDir);    } catch (final IOException e) {        throw new RuntimeException(e);    }    tmpFolder.delete();    }
public voidf16059
1
createTopic
public void kafkatest_f16060_0(final String topic)
{    createTopic(topic, 1, 1, Collections.emptyMap());}
f16060
0
shouldThrowNullPointerExceptionForNullInstant
public void kafkatest_f16068_0()
{    final String nullInstantPrefix = prepareMillisCheckFailMsgPrefix(null, "nullInstant");    try {        validateMillisecondInstant(null, nullInstantPrefix);        fail("Expected exception when null value passed for instant.");    } catch (final IllegalArgumentException e) {        assertThat(e.getMessage(), containsString(nullInstantPrefix));    }}
f16068
0
shouldThrowArithmeticExceptionForMaxInstant
public void kafkatest_f16069_0()
{    final String maxInstantPrefix = prepareMillisCheckFailMsgPrefix(Instant.MAX, "maxInstant");    try {        validateMillisecondInstant(Instant.MAX, maxInstantPrefix);        fail("Expected exception when maximum value passed for instant, because of long overflow.");    } catch (final IllegalArgumentException e) {        assertThat(e.getMessage(), containsString(maxInstantPrefix));    }}
f16069
0
shouldReturnMillisecondsOnValidDuration
public void kafkatest_f16070_0()
{    final Duration sampleDuration = Duration.ofDays(MAX_ACCEPTABLE_DAYS_FOR_DURATION_TO_MILLIS);    assertEquals(sampleDuration.toMillis(), validateMillisecondDuration(sampleDuration, "sampleDuration"));}
f16070
0
stateShouldTransitToNotRunningIfCloseRightAfterCreated
public void kafkatest_f16078_0()
{    globalStreams.close();    Assert.assertEquals(KafkaStreams.State.NOT_RUNNING, globalStreams.state());}
f16078
0
stateShouldTransitToRunningIfNonDeadThreadsBackToRunning
public void kafkatest_f16079_0() throws InterruptedException
{    final StateListenerStub stateListener = new StateListenerStub();    globalStreams.setStateListener(stateListener);    Assert.assertEquals(0, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.CREATED, globalStreams.state());    globalStreams.start();    TestUtils.waitForCondition(() -> stateListener.numChanges == 2, "Streams never started.");    Assert.assertEquals(KafkaStreams.State.RUNNING, globalStreams.state());    for (final StreamThread thread : globalStreams.threads) {        thread.stateListener().onChange(thread, StreamThread.State.PARTITIONS_REVOKED, StreamThread.State.RUNNING);    }    Assert.assertEquals(3, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.REBALANCING, globalStreams.state());    for (final StreamThread thread : globalStreams.threads) {        thread.stateListener().onChange(thread, StreamThread.State.PARTITIONS_ASSIGNED, StreamThread.State.PARTITIONS_REVOKED);    }    Assert.assertEquals(3, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.REBALANCING, globalStreams.state());    globalStreams.threads[NUM_THREADS - 1].stateListener().onChange(globalStreams.threads[NUM_THREADS - 1], StreamThread.State.PENDING_SHUTDOWN, StreamThread.State.PARTITIONS_ASSIGNED);    globalStreams.threads[NUM_THREADS - 1].stateListener().onChange(globalStreams.threads[NUM_THREADS - 1], StreamThread.State.DEAD, StreamThread.State.PENDING_SHUTDOWN);    Assert.assertEquals(3, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.REBALANCING, globalStreams.state());    for (final StreamThread thread : globalStreams.threads) {        if (thread != globalStreams.threads[NUM_THREADS - 1]) {            thread.stateListener().onChange(thread, StreamThread.State.RUNNING, StreamThread.State.PARTITIONS_ASSIGNED);        }    }    Assert.assertEquals(4, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.RUNNING, globalStreams.state());    globalStreams.close();    TestUtils.waitForCondition(() -> stateListener.numChanges == 6, "Streams never closed.");    Assert.assertEquals(KafkaStreams.State.NOT_RUNNING, globalStreams.state());}
f16079
0
stateShouldTransitToErrorIfAllThreadsDead
public void kafkatest_f16080_0() throws InterruptedException
{    final StateListenerStub stateListener = new StateListenerStub();    globalStreams.setStateListener(stateListener);    Assert.assertEquals(0, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.CREATED, globalStreams.state());    globalStreams.start();    TestUtils.waitForCondition(() -> stateListener.numChanges == 2, "Streams never started.");    Assert.assertEquals(KafkaStreams.State.RUNNING, globalStreams.state());    for (final StreamThread thread : globalStreams.threads) {        thread.stateListener().onChange(thread, StreamThread.State.PARTITIONS_REVOKED, StreamThread.State.RUNNING);    }    Assert.assertEquals(3, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.REBALANCING, globalStreams.state());    globalStreams.threads[NUM_THREADS - 1].stateListener().onChange(globalStreams.threads[NUM_THREADS - 1], StreamThread.State.PENDING_SHUTDOWN, StreamThread.State.PARTITIONS_REVOKED);    globalStreams.threads[NUM_THREADS - 1].stateListener().onChange(globalStreams.threads[NUM_THREADS - 1], StreamThread.State.DEAD, StreamThread.State.PENDING_SHUTDOWN);    Assert.assertEquals(3, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.REBALANCING, globalStreams.state());    for (final StreamThread thread : globalStreams.threads) {        if (thread != globalStreams.threads[NUM_THREADS - 1]) {            thread.stateListener().onChange(thread, StreamThread.State.PENDING_SHUTDOWN, StreamThread.State.PARTITIONS_REVOKED);            thread.stateListener().onChange(thread, StreamThread.State.DEAD, StreamThread.State.PENDING_SHUTDOWN);        }    }    Assert.assertEquals(4, stateListener.numChanges);    Assert.assertEquals(KafkaStreams.State.ERROR, globalStreams.state());    globalStreams.close();    // the state should not stuck with ERROR, but transit to NOT_RUNNING in the end    TestUtils.waitForCondition(() -> stateListener.numChanges == 6, "Streams never closed.");    Assert.assertEquals(KafkaStreams.State.NOT_RUNNING, globalStreams.state());}
f16080
0
testCannotStartOnceClosed
public void kafkatest_f16088_0()
{    globalStreams.start();    globalStreams.close();    try {        globalStreams.start();        fail("Should have throw IllegalStateException");    } catch (final IllegalStateException expected) {    // this is ok    } finally {        globalStreams.close();    }}
f16088
0
testCannotStartTwice
public void kafkatest_f16089_0()
{    globalStreams.start();    try {        globalStreams.start();        fail("Should throw an IllegalStateException");    } catch (final IllegalStateException e) {    // this is ok    } finally {        globalStreams.close();    }}
f16089
0
shouldNotSetGlobalRestoreListenerAfterStarting
public void kafkatest_f16090_0()
{    globalStreams.start();    try {        globalStreams.setGlobalStateRestoreListener(new MockStateRestoreListener());        fail("Should throw an IllegalStateException");    } catch (final IllegalStateException e) {    // expected    } finally {        globalStreams.close();    }}
f16090
0
shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning
public void kafkatest_f16098_0()
{    globalStreams.metadataForKey("store", "key", (topic, key, value, numPartitions) -> 0);}
f16098
0
shouldReturnFalseOnCloseWhenThreadsHaventTerminated
public void kafkatest_f16099_0() throws Exception
{    final AtomicBoolean keepRunning = new AtomicBoolean(true);    KafkaStreams streams = null;    try {        final StreamsBuilder builder = new StreamsBuilder();        final CountDownLatch latch = new CountDownLatch(1);        final String topic = "input";        CLUSTER.createTopics(topic);        builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String())).foreach((key, value) -> {            try {                latch.countDown();                while (keepRunning.get()) {                    Thread.sleep(10);                }            } catch (final InterruptedException e) {            // no-op            }        });        streams = new KafkaStreams(builder.build(), props);        streams.start();        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(topic, Collections.singletonList(new KeyValue<>("A", "A")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), System.currentTimeMillis());        assertTrue("Timed out waiting to receive single message", latch.await(30, TimeUnit.SECONDS));        assertFalse(streams.close(Duration.ofMillis(10)));    } finally {        // stop the thread so we don't interfere with other tests etc        keepRunning.set(false);        if (streams != null) {            streams.close();        }    }}
f16099
0
shouldReturnThreadMetadata
public void kafkatest_f16100_0()
{    globalStreams.start();    final Set<ThreadMetadata> threadMetadata = globalStreams.localThreadsMetadata();    assertNotNull(threadMetadata);    assertEquals(2, threadMetadata.size());    for (final ThreadMetadata metadata : threadMetadata) {        assertTrue("#threadState() was: " + metadata.threadState() + "; expected either RUNNING, STARTING, PARTITIONS_REVOKED, PARTITIONS_ASSIGNED, or CREATED", asList("RUNNING", "STARTING", "PARTITIONS_REVOKED", "PARTITIONS_ASSIGNED", "CREATED").contains(metadata.threadState()));        assertEquals(0, metadata.standbyTasks().size());        assertEquals(0, metadata.activeTasks().size());        final String threadName = metadata.threadName();        assertTrue(threadName.startsWith("clientId-StreamThread-"));        assertEquals(threadName + "-consumer", metadata.consumerClientId());        assertEquals(threadName + "-restore-consumer", metadata.restoreConsumerClientId());        assertEquals(Collections.singleton(threadName + "-producer"), metadata.producerClientIds());        assertEquals("clientId-admin", metadata.adminClientId());    }}
f16100
0
inMemoryStatefulTopologyShouldNotCreateStateDirectory
public void kafkatest_f16108_0() throws Exception
{    final String inputTopic = testName.getMethodName() + "-input";    final String outputTopic = testName.getMethodName() + "-output";    final String globalTopicName = testName.getMethodName() + "-global";    final String storeName = testName.getMethodName() + "-counts";    final String globalStoreName = testName.getMethodName() + "-globalStore";    final Topology topology = getStatefulTopology(inputTopic, outputTopic, globalTopicName, storeName, globalStoreName, false);    startStreamsAndCheckDirExists(topology, asList(inputTopic, globalTopicName), outputTopic, false);}
f16108
0
statefulTopologyShouldCreateStateDirectory
public void kafkatest_f16109_0() throws Exception
{    final String inputTopic = testName.getMethodName() + "-input";    final String outputTopic = testName.getMethodName() + "-output";    final String globalTopicName = testName.getMethodName() + "-global";    final String storeName = testName.getMethodName() + "-counts";    final String globalStoreName = testName.getMethodName() + "-globalStore";    final Topology topology = getStatefulTopology(inputTopic, outputTopic, globalTopicName, storeName, globalStoreName, true);    startStreamsAndCheckDirExists(topology, asList(inputTopic, globalTopicName), outputTopic, true);}
f16109
0
getStatefulTopology
private Topology kafkatest_f16110_0(final String inputTopic, final String outputTopic, final String globalTopicName, final String storeName, final String globalStoreName, final boolean isPersistentStore) throws Exception
{    CLUSTER.createTopics(inputTopic, outputTopic, globalTopicName);    final StoreBuilder<KeyValueStore<String, Long>> storeBuilder = Stores.keyValueStoreBuilder(isPersistentStore ? Stores.persistentKeyValueStore(storeName) : Stores.inMemoryKeyValueStore(storeName), Serdes.String(), Serdes.Long());    final Topology topology = new Topology();    topology.addSource("source", Serdes.String().deserializer(), Serdes.String().deserializer(), inputTopic).addProcessor("process", () -> new AbstractProcessor<String, String>() {        @Override        public void process(final String key, final String value) {            final KeyValueStore<String, Long> kvStore = (KeyValueStore<String, Long>) context().getStateStore(storeName);            kvStore.put(key, 5L);            context().forward(key, "5");            context().commit();        }    }, "source").addStateStore(storeBuilder, "process").addSink("sink", outputTopic, new StringSerializer(), new StringSerializer(), "process");    final StoreBuilder<KeyValueStore<String, String>> globalStoreBuilder = Stores.keyValueStoreBuilder(isPersistentStore ? Stores.persistentKeyValueStore(globalStoreName) : Stores.inMemoryKeyValueStore(globalStoreName), Serdes.String(), Serdes.String()).withLoggingDisabled();    topology.addGlobalStore(globalStoreBuilder, "global", Serdes.String().deserializer(), Serdes.String().deserializer(), globalTopicName, globalTopicName + "-processor", new MockProcessorSupplier());    return topology;}
f16110
0
value
public V kafkatest_f16118_0()
{    return value;}
f16118
0
timestamp
public long kafkatest_f16119_0()
{    return timestamp;}
f16119
0
toString
public String kafkatest_f16120_0()
{    return "KeyValueTimestamp{key=" + key + ", value=" + value + ", timestamp=" + timestamp + '}';}
f16120
0
process
public void kafkatest_f16128_0(final K key, final V value)
{    // flip a coin and filter    if (rand.nextBoolean()) {        context().forward(key, value);    }}
f16128
0
shouldRoundTripNull
public void kafkatest_f16129_0()
{    assertThat(serde.serializeParts(null, null), nullValue());    assertThat(FullChangeSerde.mergeChangeArraysIntoSingleLegacyFormattedArray(null), nullValue());    assertThat(FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(null), nullValue());    assertThat(serde.deserializeParts(null, null), nullValue());}
f16129
0
shouldRoundTripNullChange
public void kafkatest_f16130_0()
{    assertThat(serde.serializeParts(null, new Change<>(null, null)), is(new Change<byte[]>(null, null)));    assertThat(serde.deserializeParts(null, new Change<>(null, null)), is(new Change<String>(null, null)));    final byte[] legacyFormat = FullChangeSerde.mergeChangeArraysIntoSingleLegacyFormattedArray(new Change<>(null, null));    assertThat(FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(legacyFormat), is(new Change<byte[]>(null, null)));}
f16130
0
shouldThrowOnNull
public void kafkatest_f16138_0()
{    try {        GraphGraceSearchUtil.findAndVerifyWindowGrace(null);        fail("Should have thrown.");    } catch (final TopologyException e) {        assertThat(e.getMessage(), is("Invalid topology: Window close time is only defined for windowed computations. Got []."));    }}
f16138
0
shouldFailIfThereIsNoGraceAncestor
public void kafkatest_f16139_0()
{    // doesn't matter if this ancestor is stateless or stateful. The important thing it that there is    // no grace period defined on any ancestor of the node    final StatefulProcessorNode<String, Long> gracelessAncestor = new StatefulProcessorNode<>("stateful", new ProcessorParameters<>(() -> new Processor<String, Long>() {        @Override        public void init(final ProcessorContext context) {        }        @Override        public void process(final String key, final Long value) {        }        @Override        public void close() {        }    }, "dummy"), (StoreBuilder<? extends StateStore>) null);    final ProcessorGraphNode<String, Long> node = new ProcessorGraphNode<>("stateless", null);    gracelessAncestor.addChild(node);    try {        GraphGraceSearchUtil.findAndVerifyWindowGrace(node);        fail("should have thrown.");    } catch (final TopologyException e) {        assertThat(e.getMessage(), is("Invalid topology: Window close time is only defined for windowed computations. Got [stateful->stateless]."));    }}
f16139
0
shouldExtractGraceFromKStreamWindowAggregateNode
public void kafkatest_f16143_0()
{    final TimeWindows windows = TimeWindows.of(ofMillis(10L)).grace(ofMillis(1234L));    final StatefulProcessorNode<String, Long> node = new StatefulProcessorNode<>("asdf", new ProcessorParameters<>(new KStreamWindowAggregate<String, Long, Integer, TimeWindow>(windows, "asdf", null, null), "asdf"), (StoreBuilder<? extends StateStore>) null);    final long extracted = GraphGraceSearchUtil.findAndVerifyWindowGrace(node);    assertThat(extracted, is(windows.gracePeriodMs()));}
f16143
0
shouldNotOptimizeWhenAThroughOperationIsDone
public void kafkatest_f16154_0()
{    final Topology attemptedOptimize = getTopologyWithThroughOperation(StreamsConfig.OPTIMIZE);    final Topology noOptimziation = getTopologyWithThroughOperation(StreamsConfig.NO_OPTIMIZATION);    assertEquals(attemptedOptimize.describe().toString(), noOptimziation.describe().toString());    assertEquals(0, getCountOfRepartitionTopicsFound(attemptedOptimize.describe().toString()));    assertEquals(0, getCountOfRepartitionTopicsFound(noOptimziation.describe().toString()));}
f16154
0
getTopologyWithChangingValuesAfterChangingKey
private Topology kafkatest_f16155_0(final String optimizeConfig)
{    final StreamsBuilder builder = new StreamsBuilder();    final Properties properties = new Properties();    properties.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizeConfig);    final KStream<String, String> inputStream = builder.stream("input");    final KStream<String, String> mappedKeyStream = inputStream.selectKey((k, v) -> k + v);    mappedKeyStream.mapValues(v -> v.toUpperCase(Locale.getDefault())).groupByKey().count().toStream().to("output");    mappedKeyStream.flatMapValues(v -> Arrays.asList(v.split("\\s"))).groupByKey().windowedBy(TimeWindows.of(ofMillis(5000))).count().toStream().to("windowed-output");    return builder.build(properties);}
f16155
0
getTopologyWithThroughOperation
private Topology kafkatest_f16156_0(final String optimizeConfig)
{    final StreamsBuilder builder = new StreamsBuilder();    final Properties properties = new Properties();    properties.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizeConfig);    final KStream<String, String> inputStream = builder.stream("input");    final KStream<String, String> mappedKeyStream = inputStream.selectKey((k, v) -> k + v).through("through-topic");    mappedKeyStream.groupByKey().count().toStream().to("output");    mappedKeyStream.groupByKey().windowedBy(TimeWindows.of(ofMillis(5000))).count().toStream().to("windowed-output");    return builder.build(properties);}
f16156
0
test
public boolean kafkatest_f16167_0(final String key, final String value)
{    return true;}
f16167
0
shouldNotMaterializeSourceKTableIfNotRequired
public void kafkatest_f16168_0()
{    final MaterializedInternal<String, String, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(Materialized.with(null, null), builder, storePrefix);    final KTable table1 = builder.table("topic2", consumed, materializedInternal);    builder.buildAndOptimizeTopology();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(APP_ID))).build(null);    assertEquals(0, topology.stateStores().size());    assertEquals(0, topology.storeToChangelogTopic().size());    assertNull(table1.queryableStoreName());}
f16168
0
shouldBuildGlobalTableWithNonQueryableStoreName
public void kafkatest_f16169_0()
{    final MaterializedInternal<String, String, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(Materialized.with(null, null), builder, storePrefix);    final GlobalKTable<String, String> table1 = builder.globalTable("topic2", consumed, materializedInternal);    assertNull(table1.queryableStoreName());}
f16169
0
shouldAddTopicToLatestAutoOffsetResetList
public void kafkatest_f16177_0()
{    final String topicName = "topic-1";    final ConsumedInternal consumed = new ConsumedInternal<>(Consumed.with(AutoOffsetReset.LATEST));    builder.stream(Collections.singleton(topicName), consumed);    builder.buildAndOptimizeTopology();    assertTrue(builder.internalTopologyBuilder.latestResetTopicsPattern().matcher(topicName).matches());    assertFalse(builder.internalTopologyBuilder.earliestResetTopicsPattern().matcher(topicName).matches());}
f16177
0
shouldAddTableToEarliestAutoOffsetResetList
public void kafkatest_f16178_0()
{    final String topicName = "topic-1";    builder.table(topicName, new ConsumedInternal<>(Consumed.<String, String>with(AutoOffsetReset.EARLIEST)), materialized);    builder.buildAndOptimizeTopology();    assertTrue(builder.internalTopologyBuilder.earliestResetTopicsPattern().matcher(topicName).matches());    assertFalse(builder.internalTopologyBuilder.latestResetTopicsPattern().matcher(topicName).matches());}
f16178
0
shouldAddTableToLatestAutoOffsetResetList
public void kafkatest_f16179_0()
{    final String topicName = "topic-1";    builder.table(topicName, new ConsumedInternal<>(Consumed.<String, String>with(AutoOffsetReset.LATEST)), materialized);    builder.buildAndOptimizeTopology();    assertTrue(builder.internalTopologyBuilder.latestResetTopicsPattern().matcher(topicName).matches());    assertFalse(builder.internalTopologyBuilder.earliestResetTopicsPattern().matcher(topicName).matches());}
f16179
0
ktableShouldUseProvidedTimestampExtractor
public void kafkatest_f16187_0()
{    final ConsumedInternal<String, String> consumed = new ConsumedInternal<>(Consumed.<String, String>with(new MockTimestampExtractor()));    builder.table("topic", consumed, materialized);    builder.buildAndOptimizeTopology();    final ProcessorTopology processorTopology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(APP_ID))).build(null);    assertThat(processorTopology.source("topic").getTimestampExtractor(), instanceOf(MockTimestampExtractor.class));}
f16187
0
internalTopologyBuilder
public static InternalTopologyBuilder kafkatest_f16188_0(final InternalStreamsBuilder internalStreamsBuilder)
{    return internalStreamsBuilder.internalTopologyBuilder;}
f16188
0
before
public void kafkatest_f16189_0()
{    final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));    groupedStream = stream.groupByKey(Grouped.with(Serdes.String(), Serdes.String()));}
f16189
0
shouldNotHaveInvalidStoreNameOnAggregate
public void kafkatest_f16197_0()
{    groupedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, Materialized.as(INVALID_STORE_NAME));}
f16197
0
shouldNotHaveNullInitializerOnWindowedAggregate
public void kafkatest_f16198_0()
{    groupedStream.windowedBy(TimeWindows.of(ofMillis(10))).aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as("store"));}
f16198
0
shouldNotHaveNullAdderOnWindowedAggregate
public void kafkatest_f16199_0()
{    groupedStream.windowedBy(TimeWindows.of(ofMillis(10))).aggregate(MockInitializer.STRING_INIT, null, Materialized.as("store"));}
f16199
0
shouldCountSessionWindowsWithInternalStoreName
public void kafkatest_f16207_0()
{    final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();    final KTable<Windowed<String>, Long> table = groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).count();    table.toStream().process(supplier);    doCountSessionWindows(supplier);    assertNull(table.queryableStoreName());}
f16207
0
doReduceSessionWindows
private void kafkatest_f16208_0(final MockProcessorSupplier<Windowed<String>, String> supplier)
{    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(TOPIC, "1", "A", 10));        driver.pipeInput(recordFactory.create(TOPIC, "2", "Z", 15));        driver.pipeInput(recordFactory.create(TOPIC, "1", "B", 30));        driver.pipeInput(recordFactory.create(TOPIC, "1", "A", 70));        driver.pipeInput(recordFactory.create(TOPIC, "1", "B", 100));        driver.pipeInput(recordFactory.create(TOPIC, "1", "C", 90));    }    final Map<Windowed<String>, ValueAndTimestamp<String>> result = supplier.theCapturedProcessor().lastValueAndTimestampPerKey;    assertEquals(ValueAndTimestamp.make("A:B", 30L), result.get(new Windowed<>("1", new SessionWindow(10L, 30L))));    assertEquals(ValueAndTimestamp.make("Z", 15L), result.get(new Windowed<>("2", new SessionWindow(15L, 15L))));    assertEquals(ValueAndTimestamp.make("A:B:C", 100L), result.get(new Windowed<>("1", new SessionWindow(70L, 100L))));}
f16208
0
shouldReduceSessionWindows
public void kafkatest_f16209_0()
{    final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();    final KTable<Windowed<String>, String> table = groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).reduce((value1, value2) -> value1 + ":" + value2, Materialized.as("session-store"));    table.toStream().process(supplier);    doReduceSessionWindows(supplier);    assertEquals(table.queryableStoreName(), "session-store");}
f16209
0
shouldNotAcceptNullSessionMergerWhenAggregatingSessionWindows
public void kafkatest_f16217_0()
{    groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, null, Materialized.as("storeName"));}
f16217
0
shouldNotAcceptNullSessionWindowsWhenAggregatingSessionWindows
public void kafkatest_f16218_0()
{    groupedStream.windowedBy((SessionWindows) null);}
f16218
0
shouldAcceptNullStoreNameWhenAggregatingSessionWindows
public void kafkatest_f16219_0()
{    groupedStream.windowedBy(SessionWindows.with(ofMillis(10))).aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (aggKey, aggOne, aggTwo) -> null, Materialized.with(Serdes.String(), Serdes.String()));}
f16219
0
shouldLogAndMeasureSkipsInReduce
public void kafkatest_f16227_0()
{    groupedStream.reduce(MockReducer.STRING_ADDER, Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as("reduce").withKeySerde(Serdes.String()).withValueSerde(Serdes.String()));    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);        LogCaptureAppender.unregister(appender);        final Map<MetricName, ? extends Metric> metrics = driver.metrics();        assertEquals(1.0, getMetricByName(metrics, "skipped-records-total", "stream-metrics").metricValue());        assertNotEquals(0.0, getMetricByName(metrics, "skipped-records-rate", "stream-metrics").metricValue());        assertThat(appender.getMessages(), hasItem("Skipping record due to null key or value. key=[3] value=[null] topic=[topic] partition=[0] offset=[6]"));    }}
f16227
0
shouldAggregateAndMaterializeResults
public void kafkatest_f16228_0()
{    groupedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as("aggregate").withKeySerde(Serdes.String()).withValueSerde(Serdes.String()));    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);        {            final KeyValueStore<String, String> aggregate = driver.getKeyValueStore("aggregate");            assertThat(aggregate.get("1"), equalTo("0+A+C+D"));            assertThat(aggregate.get("2"), equalTo("0+B"));            assertThat(aggregate.get("3"), equalTo("0+E+F"));        }        {            final KeyValueStore<String, ValueAndTimestamp<String>> aggregate = driver.getTimestampedKeyValueStore("aggregate");            assertThat(aggregate.get("1"), equalTo(ValueAndTimestamp.make("0+A+C+D", 10L)));            assertThat(aggregate.get("2"), equalTo(ValueAndTimestamp.make("0+B", 1L)));            assertThat(aggregate.get("3"), equalTo(ValueAndTimestamp.make("0+E+F", 9L)));        }    }}
f16228
0
shouldAggregateWithDefaultSerdes
public void kafkatest_f16229_0()
{    final MockProcessorSupplier<String, String> supplier = new MockProcessorSupplier<>();    groupedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER).toStream().process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);        assertThat(supplier.theCapturedProcessor().lastValueAndTimestampPerKey.get("1"), equalTo(ValueAndTimestamp.make("0+A+C+D", 10L)));        assertThat(supplier.theCapturedProcessor().lastValueAndTimestampPerKey.get("2"), equalTo(ValueAndTimestamp.make("0+B", 1L)));        assertThat(supplier.theCapturedProcessor().lastValueAndTimestampPerKey.get("3"), equalTo(ValueAndTimestamp.make("0+E+F", 9L)));    }}
f16229
0
shouldNotAllowNullAdderOnAggregate
public void kafkatest_f16237_0()
{    groupedTable.aggregate(MockInitializer.STRING_INIT, null, MockAggregator.TOSTRING_REMOVER, Materialized.as("store"));}
f16237
0
shouldNotAllowNullSubtractorOnAggregate
public void kafkatest_f16238_0()
{    groupedTable.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, null, Materialized.as("store"));}
f16238
0
shouldNotAllowNullAdderOnReduce
public void kafkatest_f16239_0()
{    groupedTable.reduce(null, MockReducer.STRING_REMOVER, Materialized.as("store"));}
f16239
0
shouldCountAndMaterializeResults
public void kafkatest_f16247_0()
{    builder.table(topic, Consumed.with(Serdes.String(), Serdes.String())).groupBy(MockMapper.selectValueKeyValueMapper(), Grouped.with(Serdes.String(), Serdes.String())).count(Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as("count").withKeySerde(Serdes.String()).withValueSerde(Serdes.Long()));    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(topic, driver);        {            final KeyValueStore<String, Long> counts = driver.getKeyValueStore("count");            assertThat(counts.get("1"), equalTo(3L));            assertThat(counts.get("2"), equalTo(2L));        }        {            final KeyValueStore<String, ValueAndTimestamp<Long>> counts = driver.getTimestampedKeyValueStore("count");            assertThat(counts.get("1"), equalTo(ValueAndTimestamp.make(3L, 50L)));            assertThat(counts.get("2"), equalTo(ValueAndTimestamp.make(2L, 60L)));        }    }}
f16247
0
shouldAggregateAndMaterializeResults
public void kafkatest_f16248_0()
{    builder.table(topic, Consumed.with(Serdes.String(), Serdes.String())).groupBy(MockMapper.selectValueKeyValueMapper(), Grouped.with(Serdes.String(), Serdes.String())).aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as("aggregate").withValueSerde(Serdes.String()).withKeySerde(Serdes.String()));    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(topic, driver);        {            {                final KeyValueStore<String, String> aggregate = driver.getKeyValueStore("aggregate");                assertThat(aggregate.get("1"), equalTo("0+1+1+1"));                assertThat(aggregate.get("2"), equalTo("0+2+2"));            }            {                final KeyValueStore<String, ValueAndTimestamp<String>> aggregate = driver.getTimestampedKeyValueStore("aggregate");                assertThat(aggregate.get("1"), equalTo(ValueAndTimestamp.make("0+1+1+1", 50L)));                assertThat(aggregate.get("2"), equalTo(ValueAndTimestamp.make("0+2+2", 60L)));            }        }    }}
f16248
0
shouldThrowNullPointOnCountWhenMaterializedIsNull
public void kafkatest_f16249_0()
{    groupedTable.count((Materialized) null);}
f16249
0
processData
private void kafkatest_f16257_0(final String topic, final TopologyTestDriver driver)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());    driver.pipeInput(recordFactory.create(topic, "A", "1", 10L));    driver.pipeInput(recordFactory.create(topic, "B", "1", 50L));    driver.pipeInput(recordFactory.create(topic, "C", "1", 30L));    driver.pipeInput(recordFactory.create(topic, "D", "2", 40L));    driver.pipeInput(recordFactory.create(topic, "E", "2", 60L));}
f16257
0
testKStreamBranch
public void kafkatest_f16258_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final Predicate<Integer, String> isEven = (key, value) -> (key % 2) == 0;    final Predicate<Integer, String> isMultipleOfThree = (key, value) -> (key % 3) == 0;    final Predicate<Integer, String> isOdd = (key, value) -> (key % 2) != 0;    final int[] expectedKeys = new int[] { 1, 2, 3, 4, 5, 6 };    final KStream<Integer, String> stream;    final KStream<Integer, String>[] branches;    stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.String()));    branches = stream.branch(isEven, isMultipleOfThree, isOdd);    assertEquals(3, branches.length);    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    for (int i = 0; i < branches.length; i++) {        branches[i].process(supplier);    }    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topicName, expectedKey, "V" + expectedKey));        }    }    final List<MockProcessor<Integer, String>> processors = supplier.capturedProcessors(3);    assertEquals(3, processors.get(0).processed.size());    assertEquals(1, processors.get(1).processed.size());    assertEquals(2, processors.get(2).processed.size());}
f16258
0
testTypeVariance
public void kafkatest_f16259_0()
{    final Predicate<Number, Object> positive = (key, value) -> key.doubleValue() > 0;    final Predicate<Number, Object> negative = (key, value) -> key.doubleValue() < 0;    new StreamsBuilder().<Integer, String>stream("empty").branch(positive, negative);}
f16259
0
shouldInitialiseFlatTransformProcessor
public void kafkatest_f16267_0()
{    transformer.init(context);    replayAll();    processor.init(context);    verifyAll();}
f16267
0
shouldTransformInputRecordToMultipleOutputRecords
public void kafkatest_f16268_0()
{    final Iterable<KeyValue<Integer, Integer>> outputRecords = Arrays.asList(KeyValue.pair(2, 20), KeyValue.pair(3, 30), KeyValue.pair(4, 40));    processor.init(context);    EasyMock.reset(transformer);    EasyMock.expect(transformer.transform(inputKey, inputValue)).andReturn(outputRecords);    for (final KeyValue<Integer, Integer> outputRecord : outputRecords) {        context.forward(outputRecord.key, outputRecord.value);    }    replayAll();    processor.process(inputKey, inputValue);    verifyAll();}
f16268
0
shouldAllowEmptyListAsResultOfTransform
public void kafkatest_f16269_0()
{    processor.init(context);    EasyMock.reset(transformer);    EasyMock.expect(transformer.transform(inputKey, inputValue)).andReturn(Collections.<KeyValue<Integer, Integer>>emptyList());    replayAll();    processor.process(inputKey, inputValue);    verifyAll();}
f16269
0
shouldEmitNoRecordIfTransformReturnsNull
public void kafkatest_f16277_0()
{    processor.init(context);    EasyMock.reset(valueTransformer);    EasyMock.expect(valueTransformer.transform(inputKey, inputValue)).andReturn(null);    replayAll();    processor.process(inputKey, inputValue);    verifyAll();}
f16277
0
shouldCloseFlatTransformValuesProcessor
public void kafkatest_f16278_0()
{    valueTransformer.close();    replayAll();    processor.close();    verifyAll();}
f16278
0
shouldGetFlatTransformValuesProcessor
public void kafkatest_f16279_0()
{    final ValueTransformerWithKeySupplier<Integer, Integer, Iterable<String>> valueTransformerSupplier = mock(ValueTransformerWithKeySupplier.class);    final KStreamFlatTransformValues<Integer, Integer, String> processorSupplier = new KStreamFlatTransformValues<>(valueTransformerSupplier);    EasyMock.expect(valueTransformerSupplier.get()).andReturn(valueTransformer);    replayAll();    final Processor<Integer, Integer> processor = processorSupplier.get();    verifyAll();    assertTrue(processor instanceof KStreamFlatTransformValuesProcessor);}
f16279
0
shouldNotRequireCopartitioning
public void kafkatest_f16287_0()
{    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals("KStream-GlobalKTable joins do not need to be co-partitioned", 0, copartitionGroups.size());}
f16287
0
shouldNotJoinWithEmptyGlobalTableOnStreamUpdates
public void kafkatest_f16288_0()
{    // push two items to the primary stream. the globalTable is empty    pushToStream(2, "X", true);    processor.checkAndClearProcessResult(EMPTY);}
f16288
0
shouldNotJoinOnGlobalTableUpdates
public void kafkatest_f16289_0()
{    // push two items to the primary stream. the globalTable is empty    pushToStream(2, "X", true);    processor.checkAndClearProcessResult(EMPTY);    // push two items to the globalTable. this should not produce any item.    pushToGlobalTable(2, "Y");    processor.checkAndClearProcessResult(EMPTY);    // push all four items to the primary stream. this should produce two items.    pushToStream(4, "X", true);    processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "X0,FKey0+Y0", 0), new KeyValueTimestamp<>(1, "X1,FKey1+Y1", 1));    // push all items to the globalTable. this should not produce any item    pushToGlobalTable(4, "YY");    processor.checkAndClearProcessResult(EMPTY);    // push all four items to the primary stream. this should produce four items.    pushToStream(4, "X", true);    processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "X0,FKey0+YY0", 0), new KeyValueTimestamp<>(1, "X1,FKey1+YY1", 1), new KeyValueTimestamp<>(2, "X2,FKey2+YY2", 2), new KeyValueTimestamp<>(3, "X3,FKey3+YY3", 3));    // push all items to the globalTable. this should not produce any item    pushToGlobalTable(4, "YYY");    processor.checkAndClearProcessResult(EMPTY);}
f16289
0
pushNullValueToGlobalTable
private void kafkatest_f16297_0(final int messageCount)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer(), 0L, 1L);    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(globalTableTopic, "FKey" + expectedKeys[i], (String) null));    }}
f16297
0
shouldNotRequireCopartitioning
public void kafkatest_f16298_0()
{    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals("KStream-GlobalKTable joins do not need to be co-partitioned", 0, copartitionGroups.size());}
f16298
0
shouldNotJoinWithEmptyGlobalTableOnStreamUpdates
public void kafkatest_f16299_0()
{    // push two items to the primary stream. the globalTable is empty    pushToStream(2, "X", true);    processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "X0,FKey0+null", 0), new KeyValueTimestamp<>(1, "X1,FKey1+null", 1));}
f16299
0
transform
public KeyValue<String, String> kafkatest_f16308_0(final String key, final String value)
{    return new KeyValue<>(key, value);}
f16308
0
transform
public String kafkatest_f16311_0(final String value)
{    return value;}
f16311
0
shouldUseRecordMetadataTimestampExtractorWithThrough
public void kafkatest_f16313_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> stream1 = builder.stream(Arrays.asList("topic-1", "topic-2"), stringConsumed);    final KStream<String, String> stream2 = builder.stream(Arrays.asList("topic-3", "topic-4"), stringConsumed);    stream1.to("topic-5");    stream2.through("topic-6");    final ProcessorTopology processorTopology = TopologyWrapper.getInternalTopologyBuilder(builder.build()).setApplicationId("X").build(null);    assertThat(processorTopology.source("topic-6").getTimestampExtractor(), instanceOf(FailOnInvalidTimestamp.class));    assertNull(processorTopology.source("topic-4").getTimestampExtractor());    assertNull(processorTopology.source("topic-3").getTimestampExtractor());    assertNull(processorTopology.source("topic-2").getTimestampExtractor());    assertNull(processorTopology.source("topic-1").getTimestampExtractor());}
f16313
0
shouldNotAllowNullPredicateOnFilter
public void kafkatest_f16321_0()
{    testStream.filter(null);}
f16321
0
shouldNotAllowNullPredicateOnFilterNot
public void kafkatest_f16322_0()
{    testStream.filterNot(null);}
f16322
0
shouldNotAllowNullMapperOnSelectKey
public void kafkatest_f16323_0()
{    testStream.selectKey(null);}
f16323
0
shouldCantHaveNullPredicate
public void kafkatest_f16331_0()
{    testStream.branch((Predicate) null);}
f16331
0
shouldNotAllowNullTopicOnThrough
public void kafkatest_f16332_0()
{    testStream.through(null);}
f16332
0
shouldNotAllowNullTopicOnTo
public void kafkatest_f16333_0()
{    testStream.to((String) null);}
f16333
0
shouldNotAllowNullProcessSupplier
public void kafkatest_f16341_0()
{    testStream.process(null);}
f16341
0
shouldNotAllowNullOtherStreamOnJoin
public void kafkatest_f16342_0()
{    testStream.join(null, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(10)));}
f16342
0
shouldNotAllowNullValueJoinerOnJoin
public void kafkatest_f16343_0()
{    testStream.join(testStream, null, JoinWindows.of(ofMillis(10)));}
f16343
0
shouldNotAllowNullJoinerOnJoinWithGlobalTable
public void kafkatest_f16351_0()
{    testStream.join(builder.globalTable("global", stringConsumed), MockMapper.selectValueMapper(), null);}
f16351
0
shouldNotAllowNullTableOnJLeftJoinWithGlobalTable
public void kafkatest_f16352_0()
{    testStream.leftJoin((GlobalKTable) null, MockMapper.selectValueMapper(), MockValueJoiner.TOSTRING_JOINER);}
f16352
0
shouldNotAllowNullMapperOnLeftJoinWithGlobalTable
public void kafkatest_f16353_0()
{    testStream.leftJoin(builder.globalTable("global", stringConsumed), null, MockValueJoiner.TOSTRING_JOINER);}
f16353
0
shouldThrowNullPointerOnOuterJoinJoinedIsNull
public void kafkatest_f16361_0()
{    testStream.outerJoin(testStream, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(10)), null);}
f16361
0
shouldMergeTwoStreams
public void kafkatest_f16362_0()
{    final String topic1 = "topic-1";    final String topic2 = "topic-2";    final KStream<String, String> source1 = builder.stream(topic1);    final KStream<String, String> source2 = builder.stream(topic2);    final KStream<String, String> merged = source1.merge(source2);    merged.process(processorSupplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic1, "A", "aa"));        driver.pipeInput(recordFactory.create(topic2, "B", "bb"));        driver.pipeInput(recordFactory.create(topic2, "C", "cc"));        driver.pipeInput(recordFactory.create(topic1, "D", "dd"));    }    assertEquals(asList(new KeyValueTimestamp<>("A", "aa", 0), new KeyValueTimestamp<>("B", "bb", 0), new KeyValueTimestamp<>("C", "cc", 0), new KeyValueTimestamp<>("D", "dd", 0)), processorSupplier.theCapturedProcessor().processed);}
f16362
0
shouldMergeMultipleStreams
public void kafkatest_f16363_0()
{    final String topic1 = "topic-1";    final String topic2 = "topic-2";    final String topic3 = "topic-3";    final String topic4 = "topic-4";    final KStream<String, String> source1 = builder.stream(topic1);    final KStream<String, String> source2 = builder.stream(topic2);    final KStream<String, String> source3 = builder.stream(topic3);    final KStream<String, String> source4 = builder.stream(topic4);    final KStream<String, String> merged = source1.merge(source2).merge(source3).merge(source4);    merged.process(processorSupplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic1, "A", "aa", 1L));        driver.pipeInput(recordFactory.create(topic2, "B", "bb", 9L));        driver.pipeInput(recordFactory.create(topic3, "C", "cc", 2L));        driver.pipeInput(recordFactory.create(topic4, "D", "dd", 8L));        driver.pipeInput(recordFactory.create(topic4, "E", "ee", 3L));        driver.pipeInput(recordFactory.create(topic3, "F", "ff", 7L));        driver.pipeInput(recordFactory.create(topic2, "G", "gg", 4L));        driver.pipeInput(recordFactory.create(topic1, "H", "hh", 6L));    }    assertEquals(asList(new KeyValueTimestamp<>("A", "aa", 1), new KeyValueTimestamp<>("B", "bb", 9), new KeyValueTimestamp<>("C", "cc", 2), new KeyValueTimestamp<>("D", "dd", 8), new KeyValueTimestamp<>("E", "ee", 3), new KeyValueTimestamp<>("F", "ff", 7), new KeyValueTimestamp<>("G", "gg", 4), new KeyValueTimestamp<>("H", "hh", 6)), processorSupplier.theCapturedProcessor().processed);}
f16363
0
testAsymmetricWindowingBefore
public void kafkatest_f16371_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KStream<Integer, String> stream1;    final KStream<Integer, String> stream2;    final KStream<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream1 = builder.stream(topic1, consumed);    stream2 = builder.stream(topic2, consumed);    joined = stream1.join(stream2, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(0)).before(ofMillis(100)), Joined.with(Serdes.Integer(), Serdes.String(), Serdes.String()));    joined.process(supplier);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> processor = supplier.theCapturedProcessor();        long time = 1000L;        // w2 = {}        for (int i = 0; i < expectedKeys.length; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "A" + expectedKeys[i], time + i));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with smaller timestamps (before the window) to the other stream; this should produce no items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = {}        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899) }        time = 1000L - 100L - 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "a" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with increased timestamp to the other stream; this should produce one item        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "b" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+b0", 1000));        // push four items with increased timestamp to the other stream; this should produce two items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "c" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+c0", 1000), new KeyValueTimestamp<>(1, "A1+c1", 1001));        // push four items with increased timestamp to the other stream; this should produce three items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "d" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+d0", 1000), new KeyValueTimestamp<>(1, "A1+d1", 1001), new KeyValueTimestamp<>(2, "A2+d2", 1002));        // push four items with increased timestamp to the other stream; this should produce four items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "e" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+e0", 1000), new KeyValueTimestamp<>(1, "A1+e1", 1001), new KeyValueTimestamp<>(2, "A2+e2", 1002), new KeyValueTimestamp<>(3, "A3+e3", 1003));        // push four items with larger timestamp to the other stream; this should produce four items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000) }        time = 1000L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "f" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+f0", 1000), new KeyValueTimestamp<>(1, "A1+f1", 1001), new KeyValueTimestamp<>(2, "A2+f2", 1002), new KeyValueTimestamp<>(3, "A3+f3", 1003));        // push four items with increase timestamp to the other stream; this should produce three items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000),        // 0:g0 (ts: 1001), 1:g1 (ts: 1001), 2:g2 (ts: 1001), 3:g3 (ts: 1001) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "g" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(1, "A1+g1", 1001), new KeyValueTimestamp<>(2, "A2+g2", 1002), new KeyValueTimestamp<>(3, "A3+g3", 1003));        // push four items with increase timestamp to the other stream; this should produce two items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000),        // 0:g0 (ts: 1001), 1:g1 (ts: 1001), 2:g2 (ts: 1001), 3:g3 (ts: 1001) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000),        // 0:g0 (ts: 1001), 1:g1 (ts: 1001), 2:g2 (ts: 1001), 3:g3 (ts: 1001),        // 0:h0 (ts: 1002), 1:h1 (ts: 1002), 2:h2 (ts: 1002), 3:h3 (ts: 1002) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "h" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(2, "A2+h2", 1002), new KeyValueTimestamp<>(3, "A3+h3", 1003));        // push four items with increase timestamp to the other stream; this should produce one item        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000),        // 0:g0 (ts: 1001), 1:g1 (ts: 1001), 2:g2 (ts: 1001), 3:g3 (ts: 1001),        // 0:h0 (ts: 1002), 1:h1 (ts: 1002), 2:h2 (ts: 1002), 3:h3 (ts: 1002) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000),        // 0:g0 (ts: 1001), 1:g1 (ts: 1001), 2:g2 (ts: 1001), 3:g3 (ts: 1001),        // 0:h0 (ts: 1002), 1:h1 (ts: 1002), 2:h2 (ts: 1002), 3:h3 (ts: 1002),        // 0:i0 (ts: 1003), 1:i1 (ts: 1003), 2:i2 (ts: 1003), 3:i3 (ts: 1003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "i" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(3, "A3+i3", 1003));        // push four items with increase timestamp (no out of window) to the other stream; this should produce no items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000),        // 0:g0 (ts: 1001), 1:g1 (ts: 1001), 2:g2 (ts: 1001), 3:g3 (ts: 1001),        // 0:h0 (ts: 1002), 1:h1 (ts: 1002), 2:h2 (ts: 1002), 3:h3 (ts: 1002),        // 0:i0 (ts: 1003), 1:i1 (ts: 1003), 2:i2 (ts: 1003), 3:i3 (ts: 1003) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 899), 1:a1 (ts: 899), 2:a2 (ts: 899), 3:a3 (ts: 899),        // 0:b0 (ts: 900), 1:b1 (ts: 900), 2:b2 (ts: 900), 3:b3 (ts: 900),        // 0:c0 (ts: 901), 1:c1 (ts: 901), 2:c2 (ts: 901), 3:c3 (ts: 901),        // 0:d0 (ts: 902), 1:d1 (ts: 902), 2:d2 (ts: 902), 3:d3 (ts: 902),        // 0:e0 (ts: 903), 1:e1 (ts: 903), 2:e2 (ts: 903), 3:e3 (ts: 903),        // 0:f0 (ts: 1000), 1:f1 (ts: 1000), 2:f2 (ts: 1000), 3:f3 (ts: 1000),        // 0:g0 (ts: 1001), 1:g1 (ts: 1001), 2:g2 (ts: 1001), 3:g3 (ts: 1001),        // 0:h0 (ts: 1002), 1:h1 (ts: 1002), 2:h2 (ts: 1002), 3:h3 (ts: 1002),        // 0:i0 (ts: 1003), 1:i1 (ts: 1003), 2:i2 (ts: 1003), 3:i3 (ts: 1003),        // 0:j0 (ts: 1004), 1:j1 (ts: 1004), 2:j2 (ts: 1004), 3:j3 (ts: 1004) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "j" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);    }}
f16371
0
testLeftJoin
public void kafkatest_f16372_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KStream<Integer, String> stream1;    final KStream<Integer, String> stream2;    final KStream<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream1 = builder.stream(topic1, consumed);    stream2 = builder.stream(topic2, consumed);    joined = stream1.leftJoin(stream2, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(100)), Joined.with(Serdes.Integer(), Serdes.String(), Serdes.String()));    joined.process(supplier);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> processor = supplier.theCapturedProcessor();        // --> w2 = {}        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "A" + expectedKeys[i]));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+null", 0), new KeyValueTimestamp<>(1, "A1+null", 0));        // --> w2 = { 0:a0, 1:a1 }        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "a" + expectedKeys[i]));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+a0", 0), new KeyValueTimestamp<>(1, "A1+a1", 0));        // --> w2 = { 0:a0, 1:a1 }        for (int i = 0; i < 3; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "B" + expectedKeys[i]));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+a0", 0), new KeyValueTimestamp<>(1, "B1+a1", 0), new KeyValueTimestamp<>(2, "B2+null", 0));        // --> w2 = { 0:a0, 1:a1, 0:b0, 1:b1, 2:b2, 3:b3 }        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "b" + expectedKey));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+b0", 0), new KeyValueTimestamp<>(0, "B0+b0", 0), new KeyValueTimestamp<>(1, "A1+b1", 0), new KeyValueTimestamp<>(1, "B1+b1", 0), new KeyValueTimestamp<>(2, "B2+b2", 0));        // --> w2 = { 0:a0, 1:a1, 0:b0, 1:b1, 2:b2, 3:b3 }        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "C" + expectedKey));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "C0+a0", 0), new KeyValueTimestamp<>(0, "C0+b0", 0), new KeyValueTimestamp<>(1, "C1+a1", 0), new KeyValueTimestamp<>(1, "C1+b1", 0), new KeyValueTimestamp<>(2, "C2+b2", 0), new KeyValueTimestamp<>(3, "C3+b3", 0));    }}
f16372
0
testWindowing
public void kafkatest_f16373_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KStream<Integer, String> stream1;    final KStream<Integer, String> stream2;    final KStream<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream1 = builder.stream(topic1, consumed);    stream2 = builder.stream(topic2, consumed);    joined = stream1.leftJoin(stream2, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(100)), Joined.with(Serdes.Integer(), Serdes.String(), Serdes.String()));    joined.process(supplier);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> processor = supplier.theCapturedProcessor();        final long time = 0L;        // --> w2 = {}        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "A" + expectedKeys[i], time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+null", 0), new KeyValueTimestamp<>(1, "A1+null", 0));        // --> w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0), 2:a2 (ts: 0), 3:a3 (ts: 0) }        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "a" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+a0", 0), new KeyValueTimestamp<>(1, "A1+a1", 0));        testUpperWindowBound(expectedKeys, driver, processor);        testLowerWindowBound(expectedKeys, driver, processor);    }}
f16373
0
shouldRequireCopartitionedStreams
public void kafkatest_f16381_0()
{    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(streamTopic, tableTopic)), copartitionGroups.iterator().next());}
f16381
0
shouldNotJoinWithEmptyTableOnStreamUpdates
public void kafkatest_f16382_0()
{    // push two items to the primary stream. the table is empty    pushToStream(2, "X");    processor.checkAndClearProcessResult(EMPTY);}
f16382
0
shouldNotJoinOnTableUpdates
public void kafkatest_f16383_0()
{    // push two items to the primary stream. the table is empty    pushToStream(2, "X");    processor.checkAndClearProcessResult(EMPTY);    // push two items to the table. this should not produce any item.    pushToTable(2, "Y");    processor.checkAndClearProcessResult(EMPTY);    // push all four items to the primary stream. this should produce two items.    pushToStream(4, "X");    processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "X0+Y0", 0), new KeyValueTimestamp<>(1, "X1+Y1", 1));    // push all items to the table. this should not produce any item    pushToTable(4, "YY");    processor.checkAndClearProcessResult(EMPTY);    // push all four items to the primary stream. this should produce four items.    pushToStream(4, "X");    processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "X0+YY0", 0), new KeyValueTimestamp<>(1, "X1+YY1", 1), new KeyValueTimestamp<>(2, "X2+YY2", 2), new KeyValueTimestamp<>(3, "X3+YY3", 3));    // push all items to the table. this should not produce any item    pushToTable(4, "YYY");    processor.checkAndClearProcessResult(EMPTY);}
f16383
0
pushToTable
private void kafkatest_f16391_0(final int messageCount, final String valuePrefix)
{    final Random r = new Random(System.currentTimeMillis());    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(tableTopic, expectedKeys[i], valuePrefix + expectedKeys[i], r.nextInt(Integer.MAX_VALUE)));    }}
f16391
0
pushNullValueToTable
private void kafkatest_f16392_0(final int messageCount)
{    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(tableTopic, expectedKeys[i], null));    }}
f16392
0
shouldRequireCopartitionedStreams
public void kafkatest_f16393_0()
{    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(streamTopic, tableTopic)), copartitionGroups.iterator().next());}
f16393
0
testMapValuesWithKeys
public void kafkatest_f16401_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final ValueMapperWithKey<Integer, CharSequence, Integer> mapper = (readOnlyKey, value) -> value.length() + readOnlyKey;    final int[] expectedKeys = { 1, 10, 100, 1000 };    final KStream<Integer, String> stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.String()));    stream.mapValues(mapper).process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topicName, expectedKey, Integer.toString(expectedKey), expectedKey / 2L));        }    }    final KeyValueTimestamp[] expected = { new KeyValueTimestamp<>(1, 2, 0), new KeyValueTimestamp<>(10, 12, 5), new KeyValueTimestamp<>(100, 103, 50), new KeyValueTimestamp<>(1000, 1004, 500) };    assertArrayEquals(expected, supplier.theCapturedProcessor().processed.toArray());}
f16401
0
shouldObserveStreamElements
public void kafkatest_f16402_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Integer, String> stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.String()));    final List<KeyValue<Integer, String>> peekObserved = new ArrayList<>(), streamObserved = new ArrayList<>();    stream.peek(collect(peekObserved)).foreach(collect(streamObserved));    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final List<KeyValue<Integer, String>> expected = new ArrayList<>();        for (int key = 0; key < 32; key++) {            final String value = "V" + key;            driver.pipeInput(recordFactory.create(topicName, key, value));            expected.add(new KeyValue<>(key, value));        }        assertEquals(expected, peekObserved);        assertEquals(expected, streamObserved);    }}
f16402
0
shouldNotAllowNullAction
public void kafkatest_f16403_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Integer, String> stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.String()));    try {        stream.peek(null);        fail("expected null action to throw NPE");    } catch (final NullPointerException expected) {    // do nothing    }}
f16403
0
initStore
private void kafkatest_f16411_0(final boolean enableCaching)
{    final StoreBuilder<SessionStore<String, Long>> storeBuilder = Stores.sessionStoreBuilder(Stores.persistentSessionStore(STORE_NAME, ofMillis(GAP_MS * 3)), Serdes.String(), Serdes.Long()).withLoggingDisabled();    if (enableCaching) {        storeBuilder.withCachingEnabled();    }    sessionStore = storeBuilder.build();    sessionStore.init(context, sessionStore);}
f16411
0
closeStore
public void kafkatest_f16412_0()
{    sessionStore.close();}
f16412
0
shouldCreateSingleSessionWhenWithinGap
public void kafkatest_f16413_0()
{    context.setTime(0);    processor.process("john", "first");    context.setTime(500);    processor.process("john", "second");    final KeyValueIterator<Windowed<String>, Long> values = sessionStore.findSessions("john", 0, 2000);    assertTrue(values.hasNext());    assertEquals(Long.valueOf(2), values.next().value);}
f16413
0
shouldImmediatelyForwardRemovedSessionsWhenMerging
public void kafkatest_f16421_0()
{    initStore(false);    processor.init(context);    context.setTime(0);    processor.process("a", "1");    context.setTime(5);    processor.process("a", "1");    assertEquals(Arrays.asList(new KeyValueTimestamp<>(new Windowed<>("a", new SessionWindow(0, 0)), new Change<>(1L, null), 0L), new KeyValueTimestamp<>(new Windowed<>("a", new SessionWindow(0, 0)), new Change<>(null, null), 0L), new KeyValueTimestamp<>(new Windowed<>("a", new SessionWindow(0, 5)), new Change<>(2L, null), 5L)), results);}
f16421
0
shouldLogAndMeterWhenSkippingNullKey
public void kafkatest_f16422_0()
{    initStore(false);    processor.init(context);    context.setRecordContext(new ProcessorRecordContext(-1, -2, -3, "topic", null));    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    processor.process(null, "1");    LogCaptureAppender.unregister(appender);    assertEquals(1.0, getMetricByName(context.metrics().metrics(), "skipped-records-total", "stream-metrics").metricValue());    assertThat(appender.getMessages(), hasItem("Skipping record due to null key. value=[1] topic=[topic] partition=[-3] offset=[-2]"));}
f16422
0
shouldLogAndMeterWhenSkippingLateRecordWithZeroGrace
public void kafkatest_f16423_0()
{    LogCaptureAppender.setClassLoggerToDebug(KStreamSessionWindowAggregate.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    final Processor<String, String> processor = new KStreamSessionWindowAggregate<>(SessionWindows.with(ofMillis(10L)).grace(ofMillis(0L)), STORE_NAME, initializer, aggregator, sessionMerger).get();    initStore(false);    processor.init(context);    // dummy record to establish stream time = 0    context.setRecordContext(new ProcessorRecordContext(0, -2, -3, "topic", null));    processor.process("dummy", "dummy");    // record arrives on time, should not be skipped    context.setRecordContext(new ProcessorRecordContext(0, -2, -3, "topic", null));    processor.process("OnTime1", "1");    // dummy record to advance stream time = 1    context.setRecordContext(new ProcessorRecordContext(1, -2, -3, "topic", null));    processor.process("dummy", "dummy");    // record is late    context.setRecordContext(new ProcessorRecordContext(0, -2, -3, "topic", null));    processor.process("Late1", "1");    LogCaptureAppender.unregister(appender);    final MetricName dropMetric = new MetricName("late-record-drop-total", "stream-processor-node-metrics", "The total number of occurrence of late-record-drop operations.", mkMap(mkEntry("client-id", "test"), mkEntry("task-id", "0_0"), mkEntry("processor-node-id", "TESTING_NODE")));    assertThat(metrics.metrics().get(dropMetric).metricValue(), is(1.0));    final MetricName dropRate = new MetricName("late-record-drop-rate", "stream-processor-node-metrics", "The average number of occurrence of late-record-drop operations.", mkMap(mkEntry("client-id", "test"), mkEntry("task-id", "0_0"), mkEntry("processor-node-id", "TESTING_NODE")));    assertThat((Double) metrics.metrics().get(dropRate).metricValue(), greaterThan(0.0));    assertThat(appender.getMessages(), hasItem("Skipping record for expired window. key=[Late1] topic=[topic] partition=[-3] offset=[-2] timestamp=[0] window=[0,0] expiration=[1] streamTime=[1]"));}
f16423
0
testTransform
public void kafkatest_f16433_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final ValueTransformerSupplier<Number, Integer> valueTransformerSupplier = () -> new ValueTransformer<Number, Integer>() {        private int total = 0;        @Override        public void init(final ProcessorContext context) {        }        @Override        public Integer transform(final Number value) {            total += value.intValue();            return total;        }        @Override        public void close() {        }    };    final int[] expectedKeys = { 1, 10, 100, 1000 };    final KStream<Integer, Integer> stream;    stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.Integer()));    stream.transformValues(valueTransformerSupplier).process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topicName, expectedKey, expectedKey * 10, expectedKey / 2L));        }    }    final KeyValueTimestamp[] expected = { new KeyValueTimestamp<>(1, 10, 0), new KeyValueTimestamp<>(10, 110, 5), new KeyValueTimestamp<>(100, 1110, 50), new KeyValueTimestamp<>(1000, 11110, 500) };    assertArrayEquals(expected, supplier.theCapturedProcessor().processed.toArray());}
f16433
0
transform
public Integer kafkatest_f16435_0(final Number value)
{    total += value.intValue();    return total;}
f16435
0
testTransformWithKey
public void kafkatest_f16437_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final ValueTransformerWithKeySupplier<Integer, Number, Integer> valueTransformerSupplier = () -> new ValueTransformerWithKey<Integer, Number, Integer>() {        private int total = 0;        @Override        public void init(final ProcessorContext context) {        }        @Override        public Integer transform(final Integer readOnlyKey, final Number value) {            total += value.intValue() + readOnlyKey;            return total;        }        @Override        public void close() {        }    };    final int[] expectedKeys = { 1, 10, 100, 1000 };    final KStream<Integer, Integer> stream;    stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.Integer()));    stream.transformValues(valueTransformerSupplier).process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topicName, expectedKey, expectedKey * 10, expectedKey / 2L));        }    }    final KeyValueTimestamp[] expected = { new KeyValueTimestamp<>(1, 11, 0), new KeyValueTimestamp<>(10, 121, 5), new KeyValueTimestamp<>(100, 1221, 50), new KeyValueTimestamp<>(1000, 12221, 500) };    assertArrayEquals(expected, supplier.theCapturedProcessor().processed.toArray());}
f16437
0
assertLatenessMetrics
private void kafkatest_f16447_0(final TopologyTestDriver driver, final Matcher<Object> dropTotal, final Matcher<Object> maxLateness, final Matcher<Object> avgLateness)
{    final MetricName dropMetric = new MetricName("late-record-drop-total", "stream-processor-node-metrics", "The total number of occurrence of late-record-drop operations.", mkMap(mkEntry("client-id", "topology-test-driver-virtual-thread"), mkEntry("task-id", "0_0"), mkEntry("processor-node-id", "KSTREAM-AGGREGATE-0000000001")));    assertThat(driver.metrics().get(dropMetric).metricValue(), dropTotal);    final MetricName dropRate = new MetricName("late-record-drop-rate", "stream-processor-node-metrics", "The average number of occurrence of late-record-drop operations.", mkMap(mkEntry("client-id", "topology-test-driver-virtual-thread"), mkEntry("task-id", "0_0"), mkEntry("processor-node-id", "KSTREAM-AGGREGATE-0000000001")));    assertThat(driver.metrics().get(dropRate).metricValue(), not(0.0));    final MetricName latenessMaxMetric = new MetricName("record-lateness-max", "stream-task-metrics", "The max observed lateness of records.", mkMap(mkEntry("client-id", "topology-test-driver-virtual-thread"), mkEntry("task-id", "0_0")));    assertThat(driver.metrics().get(latenessMaxMetric).metricValue(), maxLateness);    final MetricName latenessAvgMetric = new MetricName("record-lateness-avg", "stream-task-metrics", "The average observed lateness of records.", mkMap(mkEntry("client-id", "topology-test-driver-virtual-thread"), mkEntry("task-id", "0_0")));    assertThat(driver.metrics().get(latenessAvgMetric).metricValue(), avgLateness);}
f16447
0
getOutput
private ProducerRecord<String, String> kafkatest_f16448_0(final TopologyTestDriver driver)
{    return driver.readOutput("output", new StringDeserializer(), new StringDeserializer());}
f16448
0
testAggBasic
public void kafkatest_f16449_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTable<String, String> table1 = builder.table(topic1, consumed);    final KTable<String, String> table2 = table1.groupBy(MockMapper.noOpKeyValueMapper(), stringSerialized).aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as("topic1-Canonized").withValueSerde(stringSerde));    table2.toStream().process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), mkProperties(mkMap(mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummy"), mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, "test"), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory("kafka-test").getAbsolutePath()))), 0L)) {        final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer(), 0L, 0L);        driver.pipeInput(recordFactory.create(topic1, "A", "1", 10L));        driver.pipeInput(recordFactory.create(topic1, "B", "2", 15L));        driver.pipeInput(recordFactory.create(topic1, "A", "3", 20L));        driver.pipeInput(recordFactory.create(topic1, "B", "4", 18L));        driver.pipeInput(recordFactory.create(topic1, "C", "5", 5L));        driver.pipeInput(recordFactory.create(topic1, "D", "6", 25L));        driver.pipeInput(recordFactory.create(topic1, "B", "7", 15L));        driver.pipeInput(recordFactory.create(topic1, "C", "8", 10L));        assertEquals(asList(new KeyValueTimestamp<>("A", "0+1", 10L), new KeyValueTimestamp<>("B", "0+2", 15L), new KeyValueTimestamp<>("A", "0+1-1", 20L), new KeyValueTimestamp<>("A", "0+1-1+3", 20L), new KeyValueTimestamp<>("B", "0+2-2", 18L), new KeyValueTimestamp<>("B", "0+2-2+4", 18L), new KeyValueTimestamp<>("C", "0+5", 5L), new KeyValueTimestamp<>("D", "0+6", 25L), new KeyValueTimestamp<>("B", "0+2-2+4-4", 18L), new KeyValueTimestamp<>("B", "0+2-2+4-4+7", 18L), new KeyValueTimestamp<>("C", "0+5-5", 10L), new KeyValueTimestamp<>("C", "0+5-5+8", 10L)), supplier.theCapturedProcessor().processed);    }}
f16449
0
shouldPassThroughWithoutMaterialization
public void kafkatest_f16457_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTable<String, Integer> table1 = builder.table(topic1, consumed);    final KTable<String, Integer> table2 = table1.filter(predicate);    final KTable<String, Integer> table3 = table1.filterNot(predicate);    assertNull(table1.queryableStoreName());    assertNull(table2.queryableStoreName());    assertNull(table3.queryableStoreName());    doTestKTable(builder, table2, table3, topic1);}
f16457
0
shouldPassThroughOnMaterialization
public void kafkatest_f16458_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTable<String, Integer> table1 = builder.table(topic1, consumed);    final KTable<String, Integer> table2 = table1.filter(predicate, Materialized.as("store2"));    final KTable<String, Integer> table3 = table1.filterNot(predicate);    assertNull(table1.queryableStoreName());    assertEquals("store2", table2.queryableStoreName());    assertNull(table3.queryableStoreName());    doTestKTable(builder, table2, table3, topic1);}
f16458
0
doTestValueGetter
private void kafkatest_f16459_0(final StreamsBuilder builder, final KTableImpl<String, Integer, Integer> table2, final KTableImpl<String, Integer, Integer> table3, final String topic1)
{    final Topology topology = builder.build();    final KTableValueGetterSupplier<String, Integer> getterSupplier2 = table2.valueGetterSupplier();    final KTableValueGetterSupplier<String, Integer> getterSupplier3 = table3.valueGetterSupplier();    final InternalTopologyBuilder topologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology);    topologyBuilder.connectProcessorAndStateStores(table2.name, getterSupplier2.storeNames());    topologyBuilder.connectProcessorAndStateStores(table3.name, getterSupplier3.storeNames());    try (final TopologyTestDriverWrapper driver = new TopologyTestDriverWrapper(topology, props)) {        final KTableValueGetter<String, Integer> getter2 = getterSupplier2.get();        final KTableValueGetter<String, Integer> getter3 = getterSupplier3.get();        getter2.init(driver.setCurrentNodeForProcessorContext(table2.name));        getter3.init(driver.setCurrentNodeForProcessorContext(table3.name));        driver.pipeInput(recordFactory.create(topic1, "A", 1, 5L));        driver.pipeInput(recordFactory.create(topic1, "B", 1, 10L));        driver.pipeInput(recordFactory.create(topic1, "C", 1, 15L));        assertNull(getter2.get("A"));        assertNull(getter2.get("B"));        assertNull(getter2.get("C"));        assertEquals(ValueAndTimestamp.make(1, 5L), getter3.get("A"));        assertEquals(ValueAndTimestamp.make(1, 10L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(1, 15L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", 2, 10L));        driver.pipeInput(recordFactory.create(topic1, "B", 2, 5L));        assertEquals(ValueAndTimestamp.make(2, 10L), getter2.get("A"));        assertEquals(ValueAndTimestamp.make(2, 5L), getter2.get("B"));        assertNull(getter2.get("C"));        assertNull(getter3.get("A"));        assertNull(getter3.get("B"));        assertEquals(ValueAndTimestamp.make(1, 15L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", 3, 15L));        assertNull(getter2.get("A"));        assertEquals(ValueAndTimestamp.make(2, 5L), getter2.get("B"));        assertNull(getter2.get("C"));        assertEquals(ValueAndTimestamp.make(3, 15L), getter3.get("A"));        assertNull(getter3.get("B"));        assertEquals(ValueAndTimestamp.make(1, 15L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", null, 10L));        driver.pipeInput(recordFactory.create(topic1, "B", null, 20L));        assertNull(getter2.get("A"));        assertNull(getter2.get("B"));        assertNull(getter2.get("C"));        assertNull(getter3.get("A"));        assertNull(getter3.get("B"));        assertEquals(ValueAndTimestamp.make(1, 15L), getter3.get("C"));    }}
f16459
0
doTestSkipNullOnMaterialization
private void kafkatest_f16467_0(final StreamsBuilder builder, final KTableImpl<String, String, String> table1, final KTableImpl<String, String, String> table2, final String topic1)
{    final MockProcessorSupplier<String, String> supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build();    topology.addProcessor("proc1", supplier, table1.name);    topology.addProcessor("proc2", supplier, table2.name);    final ConsumerRecordFactory<String, String> stringRecordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer(), 0L);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        driver.pipeInput(stringRecordFactory.create(topic1, "A", "reject", 5L));        driver.pipeInput(stringRecordFactory.create(topic1, "B", "reject", 10L));        driver.pipeInput(stringRecordFactory.create(topic1, "C", "reject", 20L));    }    final List<MockProcessor<String, String>> processors = supplier.capturedProcessors(2);    processors.get(0).checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("reject", null), 5), new KeyValueTimestamp<>("B", new Change<>("reject", null), 10), new KeyValueTimestamp<>("C", new Change<>("reject", null), 20));    processors.get(1).checkEmptyAndClearProcessResult();}
f16467
0
shouldSkipNullToRepartitionWithoutMaterialization
public void kafkatest_f16468_0()
{    // Do not explicitly set enableSendingOldValues. Let a further downstream stateful operator trigger it instead.    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final Consumed<String, String> consumed = Consumed.with(Serdes.String(), Serdes.String());    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    final KTableImpl<String, String, String> table2 = (KTableImpl<String, String, String>) table1.filter((key, value) -> value.equalsIgnoreCase("accept")).groupBy(MockMapper.noOpKeyValueMapper()).reduce(MockReducer.STRING_ADDER, MockReducer.STRING_REMOVER);    doTestSkipNullOnMaterialization(builder, table1, table2, topic1);}
f16468
0
shouldSkipNullToRepartitionOnMaterialization
public void kafkatest_f16469_0()
{    // Do not explicitly set enableSendingOldValues. Let a further downstream stateful operator trigger it instead.    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final Consumed<String, String> consumed = Consumed.with(Serdes.String(), Serdes.String());    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    final KTableImpl<String, String, String> table2 = (KTableImpl<String, String, String>) table1.filter((key, value) -> value.equalsIgnoreCase("accept"), Materialized.as("store2")).groupBy(MockMapper.noOpKeyValueMapper()).reduce(MockReducer.STRING_ADDER, MockReducer.STRING_REMOVER, Materialized.as("mock-result"));    doTestSkipNullOnMaterialization(builder, table1, table2, topic1);}
f16469
0
assertTopologyContainsProcessor
private void kafkatest_f16479_0(final Topology topology, final String processorName)
{    for (final TopologyDescription.Subtopology subtopology : topology.describe().subtopologies()) {        for (final TopologyDescription.Node node : subtopology.nodes()) {            if (node.name().equals(processorName)) {                return;            }        }    }    throw new AssertionError("No processor named '" + processorName + "'" + "found in the provided Topology:\n" + topology.describe());}
f16479
0
shouldCreateSourceAndSinkNodesForRepartitioningTopic
public void kafkatest_f16480_0() throws Exception
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final String storeName1 = "storeName1";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed, Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as(storeName1).withKeySerde(Serdes.String()).withValueSerde(Serdes.String()));    table1.groupBy(MockMapper.noOpKeyValueMapper()).aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, Materialized.as("mock-result1"));    table1.groupBy(MockMapper.noOpKeyValueMapper()).reduce(MockReducer.STRING_ADDER, MockReducer.STRING_REMOVER, Materialized.as("mock-result2"));    final Topology topology = builder.build();    try (final TopologyTestDriverWrapper driver = new TopologyTestDriverWrapper(topology, props)) {        assertEquals(3, driver.getAllStateStores().size());        assertTopologyContainsProcessor(topology, "KSTREAM-SINK-0000000003");        assertTopologyContainsProcessor(topology, "KSTREAM-SOURCE-0000000004");        assertTopologyContainsProcessor(topology, "KSTREAM-SINK-0000000007");        assertTopologyContainsProcessor(topology, "KSTREAM-SOURCE-0000000008");        final Field valSerializerField = ((SinkNode) driver.getProcessor("KSTREAM-SINK-0000000003")).getClass().getDeclaredField("valSerializer");        final Field valDeserializerField = ((SourceNode) driver.getProcessor("KSTREAM-SOURCE-0000000004")).getClass().getDeclaredField("valDeserializer");        valSerializerField.setAccessible(true);        valDeserializerField.setAccessible(true);        assertNotNull(((ChangedSerializer) valSerializerField.get(driver.getProcessor("KSTREAM-SINK-0000000003"))).inner());        assertNotNull(((ChangedDeserializer) valDeserializerField.get(driver.getProcessor("KSTREAM-SOURCE-0000000004"))).inner());        assertNotNull(((ChangedSerializer) valSerializerField.get(driver.getProcessor("KSTREAM-SINK-0000000007"))).inner());        assertNotNull(((ChangedDeserializer) valDeserializerField.get(driver.getProcessor("KSTREAM-SOURCE-0000000008"))).inner());    }}
f16480
0
shouldNotAllowNullSelectorOnToStream
public void kafkatest_f16481_0()
{    table.toStream((KeyValueMapper) null);}
f16481
0
shouldNotAllowNullJoinerJoin
public void kafkatest_f16489_0()
{    table.join(table, null);}
f16489
0
shouldNotAllowNullOtherTableOnOuterJoin
public void kafkatest_f16490_0()
{    table.outerJoin(null, MockValueJoiner.TOSTRING_JOINER);}
f16490
0
shouldNotAllowNullJoinerOnOuterJoin
public void kafkatest_f16491_0()
{    table.outerJoin(table, null);}
f16491
0
shouldThrowNullPointerOnTransformValuesWithKeyWhenTransformerSupplierIsNull
public void kafkatest_f16499_0()
{    table.transformValues((ValueTransformerWithKeySupplier) null);}
f16499
0
shouldThrowNullPointerOnTransformValuesWithKeyWhenMaterializedIsNull
public void kafkatest_f16500_0()
{    final ValueTransformerWithKeySupplier<String, String, ?> valueTransformerSupplier = mock(ValueTransformerWithKeySupplier.class);    table.transformValues(valueTransformerSupplier, (Materialized) null);}
f16500
0
shouldThrowNullPointerOnTransformValuesWithKeyWhenStoreNamesNull
public void kafkatest_f16501_0()
{    final ValueTransformerWithKeySupplier<String, String, ?> valueTransformerSupplier = mock(ValueTransformerWithKeySupplier.class);    table.transformValues(valueTransformerSupplier, (String[]) null);}
f16501
0
doTestJoin
private void kafkatest_f16509_0(final StreamsBuilder builder, final int[] expectedKeys)
{    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        assertNull(driver.readOutput(output));        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        assertOutputKeyValueTimestamp(driver, 0, "X0+Y0", 5L);        assertOutputKeyValueTimestamp(driver, 1, "X1+Y1", 10L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce two items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        assertOutputKeyValueTimestamp(driver, 0, "XX0+Y0", 7L);        assertOutputKeyValueTimestamp(driver, 1, "XX1+Y1", 10L);        assertNull(driver.readOutput(output));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XX0+YY0", 7L);        assertOutputKeyValueTimestamp(driver, 1, "XX1+YY1", 7L);        assertOutputKeyValueTimestamp(driver, 2, "XX2+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "XX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXX0+YY0", 6L);        assertOutputKeyValueTimestamp(driver, 1, "XXX1+YY1", 6L);        assertOutputKeyValueTimestamp(driver, 2, "XXX2+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "XXX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, null, 6L);        assertOutputKeyValueTimestamp(driver, 1, null, 7L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce two items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 2, "XXXX2+YY2", 13L);        assertOutputKeyValueTimestamp(driver, 3, "XXXX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push fourt items to the primary stream with null. this should produce two items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 2, null, 10L);        assertOutputKeyValueTimestamp(driver, 3, null, 20L);        assertNull(driver.readOutput(output));    }}
f16509
0
assertOutputKeyValueTimestamp
private void kafkatest_f16510_0(final TopologyTestDriver driver, final Integer expectedKey, final String expectedValue, final long expectedTimestamp)
{    OutputVerifier.compareKeyValueTimestamp(driver.readOutput(output, Serdes.Integer().deserializer(), Serdes.String().deserializer()), expectedKey, expectedValue, expectedTimestamp);}
f16510
0
testJoin
public void kafkatest_f16511_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KTable<Integer, String> table1 = builder.table(topic1, consumed);    final KTable<Integer, String> table2 = builder.table(topic2, consumed);    final KTable<Integer, String> joined = table1.leftJoin(table2, MockValueJoiner.TOSTRING_JOINER);    joined.toStream().to(output);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        assertOutputKeyValueTimestamp(driver, 0, "X0+null", 5L);        assertOutputKeyValueTimestamp(driver, 1, "X1+null", 6L);        assertNull(driver.readOutput(output));        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        assertOutputKeyValueTimestamp(driver, 0, "X0+Y0", 5L);        assertOutputKeyValueTimestamp(driver, 1, "X1+Y1", 10L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        assertOutputKeyValueTimestamp(driver, 0, "XX0+Y0", 7L);        assertOutputKeyValueTimestamp(driver, 1, "XX1+Y1", 10L);        assertOutputKeyValueTimestamp(driver, 2, "XX2+null", 7L);        assertOutputKeyValueTimestamp(driver, 3, "XX3+null", 7L);        assertNull(driver.readOutput(output));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XX0+YY0", 7L);        assertOutputKeyValueTimestamp(driver, 1, "XX1+YY1", 7L);        assertOutputKeyValueTimestamp(driver, 2, "XX2+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "XX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXX0+YY0", 6L);        assertOutputKeyValueTimestamp(driver, 1, "XXX1+YY1", 6L);        assertOutputKeyValueTimestamp(driver, 2, "XXX2+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "XXX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXX0+null", 6L);        assertOutputKeyValueTimestamp(driver, 1, "XXX1+null", 7L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXXX0+null", 13L);        assertOutputKeyValueTimestamp(driver, 1, "XXXX1+null", 13L);        assertOutputKeyValueTimestamp(driver, 2, "XXXX2+YY2", 13L);        assertOutputKeyValueTimestamp(driver, 3, "XXXX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push three items to the primary stream with null. this should produce four items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, null, 0L);        assertOutputKeyValueTimestamp(driver, 1, null, 42L);        assertOutputKeyValueTimestamp(driver, 2, null, 10L);        assertOutputKeyValueTimestamp(driver, 3, null, 20L);        assertNull(driver.readOutput(output));    }}
f16511
0
testSendingOldValue
public void kafkatest_f16519_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KTable<Integer, String> table1;    final KTable<Integer, String> table2;    final KTable<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier;    table1 = builder.table(topic1, consumed);    table2 = builder.table(topic2, consumed);    joined = table1.outerJoin(table2, MockValueJoiner.TOSTRING_JOINER);    ((KTableImpl<?, ?, ?>) joined).enableSendingOldValues();    supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build().addProcessor("proc", supplier, ((KTableImpl<?, ?, ?>) joined).name);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        final MockProcessor<Integer, String> proc = supplier.theCapturedProcessor();        assertTrue(((KTableImpl<?, ?, ?>) table1).sendingOldValueEnabled());        assertTrue(((KTableImpl<?, ?, ?>) table2).sendingOldValueEnabled());        assertTrue(((KTableImpl<?, ?, ?>) joined).sendingOldValueEnabled());        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("X0+null", null), 5), new KeyValueTimestamp<>(1, new Change<>("X1+null", null), 6));        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("X0+Y0", "X0+null"), 5), new KeyValueTimestamp<>(1, new Change<>("X1+Y1", "X1+null"), 10));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+Y0", "X0+Y0"), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+Y1", "X1+Y1"), 10), new KeyValueTimestamp<>(2, new Change<>("XX2+null", null), 7), new KeyValueTimestamp<>(3, new Change<>("XX3+null", null), 7));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+YY0", "XX0+Y0"), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+YY1", "XX1+Y1"), 7), new KeyValueTimestamp<>(2, new Change<>("XX2+YY2", "XX2+null"), 10), new KeyValueTimestamp<>(3, new Change<>("XX3+YY3", "XX3+null"), 15));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXX0+YY0", "XX0+YY0"), 6), new KeyValueTimestamp<>(1, new Change<>("XXX1+YY1", "XX1+YY1"), 6), new KeyValueTimestamp<>(2, new Change<>("XXX2+YY2", "XX2+YY2"), 10), new KeyValueTimestamp<>(3, new Change<>("XXX3+YY3", "XX3+YY3"), 15));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXX0+null", "XXX0+YY0"), 6), new KeyValueTimestamp<>(1, new Change<>("XXX1+null", "XXX1+YY1"), 7));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXXX0+null", "XXX0+null"), 13), new KeyValueTimestamp<>(1, new Change<>("XXXX1+null", "XXX1+null"), 13), new KeyValueTimestamp<>(2, new Change<>("XXXX2+YY2", "XXX2+YY2"), 13), new KeyValueTimestamp<>(3, new Change<>("XXXX3+YY3", "XXX3+YY3"), 15));        // push four items to the primary stream with null. this should produce four items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>(null, "XXXX0+null"), 0), new KeyValueTimestamp<>(1, new Change<>(null, "XXXX1+null"), 42), new KeyValueTimestamp<>(2, new Change<>("null+YY2", "XXXX2+YY2"), 10), new KeyValueTimestamp<>(3, new Change<>("null+YY3", "XXXX3+YY3"), 20));    }}
f16519
0
shouldLogAndMeterSkippedRecordsDueToNullLeftKey
public void kafkatest_f16520_0()
{    final StreamsBuilder builder = new StreamsBuilder();    @SuppressWarnings("unchecked")    final Processor<String, Change<String>> join = new KTableKTableOuterJoin<>((KTableImpl<String, String, String>) builder.table("left", Consumed.with(Serdes.String(), Serdes.String())), (KTableImpl<String, String, String>) builder.table("right", Consumed.with(Serdes.String(), Serdes.String())), null).get();    final MockProcessorContext context = new MockProcessorContext();    context.setRecordMetadata("left", -1, -2, null, -3);    join.init(context);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    join.process(null, new Change<>("new", "old"));    LogCaptureAppender.unregister(appender);    assertEquals(1.0, getMetricByName(context.metrics().metrics(), "skipped-records-total", "stream-metrics").metricValue());    assertThat(appender.getMessages(), hasItem("Skipping record due to null key. change=[(new<-old)] topic=[left] partition=[-1] offset=[-2]"));}
f16520
0
assertOutputKeyValueTimestamp
private void kafkatest_f16521_0(final TopologyTestDriver driver, final Integer expectedKey, final String expectedValue, final long expectedTimestamp)
{    OutputVerifier.compareKeyValueTimestamp(driver.readOutput(output, Serdes.Integer().deserializer(), Serdes.String().deserializer()), expectedKey, expectedValue, expectedTimestamp);}
f16521
0
testNotSendingOldValue
public void kafkatest_f16529_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    final KTableImpl<String, String, Integer> table2 = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new);    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build().addProcessor("proc", supplier, table2.name);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        final MockProcessor<String, Integer> proc = supplier.theCapturedProcessor();        assertFalse(table1.sendingOldValueEnabled());        assertFalse(table2.sendingOldValueEnabled());        driver.pipeInput(recordFactory.create(topic1, "A", "01", 5L));        driver.pipeInput(recordFactory.create(topic1, "B", "01", 10L));        driver.pipeInput(recordFactory.create(topic1, "C", "01", 15L));        proc.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(1, null), 5), new KeyValueTimestamp<>("B", new Change<>(1, null), 10), new KeyValueTimestamp<>("C", new Change<>(1, null), 15));        driver.pipeInput(recordFactory.create(topic1, "A", "02", 10L));        driver.pipeInput(recordFactory.create(topic1, "B", "02", 8L));        proc.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(2, null), 10), new KeyValueTimestamp<>("B", new Change<>(2, null), 8));        driver.pipeInput(recordFactory.create(topic1, "A", "03", 20L));        proc.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(3, null), 20));        driver.pipeInput(recordFactory.create(topic1, "A", (String) null, 30L));        proc.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(null, null), 30));    }}
f16529
0
testSendingOldValue
public void kafkatest_f16530_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    final KTableImpl<String, String, Integer> table2 = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new);    table2.enableSendingOldValues();    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();    builder.build().addProcessor("proc", supplier, table2.name);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<String, Integer> proc = supplier.theCapturedProcessor();        assertTrue(table1.sendingOldValueEnabled());        assertTrue(table2.sendingOldValueEnabled());        driver.pipeInput(recordFactory.create(topic1, "A", "01", 5L));        driver.pipeInput(recordFactory.create(topic1, "B", "01", 10L));        driver.pipeInput(recordFactory.create(topic1, "C", "01", 15L));        proc.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(1, null), 5), new KeyValueTimestamp<>("B", new Change<>(1, null), 10), new KeyValueTimestamp<>("C", new Change<>(1, null), 15));        driver.pipeInput(recordFactory.create(topic1, "A", "02", 10L));        driver.pipeInput(recordFactory.create(topic1, "B", "02", 8L));        proc.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(2, 1), 10), new KeyValueTimestamp<>("B", new Change<>(2, 1), 8));        driver.pipeInput(recordFactory.create(topic1, "A", "03", 20L));        proc.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(3, 2), 20));        driver.pipeInput(recordFactory.create(topic1, "A", (String) null, 30L));        proc.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(null, 3), 30));    }}
f16530
0
shouldAddAndSubtract
public void kafkatest_f16531_0()
{    final InternalMockProcessorContext context = new InternalMockProcessorContext();    final Processor<String, Change<Set<String>>> reduceProcessor = new KTableReduce<String, Set<String>>("myStore", this::unionNotNullArgs, this::differenceNotNullArgs).get();    final TimestampedKeyValueStore<String, Set<String>> myStore = new GenericInMemoryTimestampedKeyValueStore<>("myStore");    context.register(myStore, null);    reduceProcessor.init(context);    context.setCurrentNode(new ProcessorNode<>("reduce", reduceProcessor, singleton("myStore")));    context.setTime(10L);    reduceProcessor.process("A", new Change<>(singleton("a"), null));    assertEquals(ValueAndTimestamp.make(singleton("a"), 10L), myStore.get("A"));    context.setTime(15L);    reduceProcessor.process("A", new Change<>(singleton("b"), singleton("a")));    assertEquals(ValueAndTimestamp.make(singleton("b"), 15L), myStore.get("A"));    context.setTime(12L);    reduceProcessor.process("A", new Change<>(null, singleton("b")));    assertEquals(ValueAndTimestamp.make(emptySet(), 15L), myStore.get("A"));}
f16531
0
cleanup
public void kafkatest_f16539_0()
{    if (driver != null) {        driver.close();        driver = null;    }}
f16539
0
setUp
public void kafkatest_f16540_0()
{    capture = new MockProcessorSupplier<>();    builder = new StreamsBuilder();}
f16540
0
shouldThrowOnGetIfSupplierReturnsNull
public void kafkatest_f16541_0()
{    final KTableTransformValues<String, String, String> transformer = new KTableTransformValues<>(parent, new NullSupplier(), QUERYABLE_NAME);    try {        transformer.get();        fail("NPE expected");    } catch (final NullPointerException expected) {    // expected    }}
f16541
0
shouldGetStoreNamesFromParentIfNotMaterialized
public void kafkatest_f16549_0()
{    final KTableTransformValues<String, String, String> transformValues = new KTableTransformValues<>(parent, new ExclamationValueTransformerSupplier(), null);    expect(parent.valueGetterSupplier()).andReturn(parentGetterSupplier);    expect(parentGetterSupplier.storeNames()).andReturn(new String[] { "store1", "store2" });    replay(parent, parentGetterSupplier);    final String[] storeNames = transformValues.view().storeNames();    assertThat(storeNames, is(new String[] { "store1", "store2" }));}
f16549
0
shouldGetQueryableStoreNameIfMaterialized
public void kafkatest_f16550_0()
{    final KTableTransformValues<String, String, String> transformValues = new KTableTransformValues<>(parent, new ExclamationValueTransformerSupplier(), QUERYABLE_NAME);    final String[] storeNames = transformValues.view().storeNames();    assertThat(storeNames, is(new String[] { QUERYABLE_NAME }));}
f16550
0
shouldCloseTransformerOnProcessorClose
public void kafkatest_f16551_0()
{    final KTableTransformValues<String, String, String> transformValues = new KTableTransformValues<>(parent, mockSupplier, null);    expect(mockSupplier.get()).andReturn(transformer);    transformer.close();    expectLastCall();    replay(mockSupplier, transformer);    final Processor<String, Change<String>> processor = transformValues.get();    processor.close();    verify(transformer);}
f16551
0
toForceSendingOfOldValues
private static KeyValueMapper<String, Integer, KeyValue<String, Integer>> kafkatest_f16559_0()
{    return KeyValue::new;}
f16559
0
mapBackToStrings
private static ValueMapper<Integer, String> kafkatest_f16560_0()
{    return Object::toString;}
f16560
0
storeBuilder
private static StoreBuilder<KeyValueStore<Long, Long>> kafkatest_f16561_0(final String storeName)
{    return Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(storeName), Serdes.Long(), Serdes.Long());}
f16561
0
transform
public Integer kafkatest_f16571_0(final String readOnlyKey, final String value)
{    return ++counter;}
f16571
0
get
public ValueTransformerWithKey<String, String, Integer> kafkatest_f16573_0()
{    return new StatelessTransformer();}
f16573
0
transform
public Integer kafkatest_f16575_0(final String readOnlyKey, final String value)
{    return value == null ? null : value.length();}
f16575
0
shouldNotGenerateWithPrefixGivenValidName
public void kafkatest_f16584_0()
{    final String validName = "validName";    assertEquals(validName, NamedInternal.with(validName).orElseGenerateWithPrefix(new TestNameProvider(), "KSTREAM-MAP-"));}
f16584
0
shouldForwardKeyNewValueOldValueAndTimestamp
public void kafkatest_f16585_0()
{    final InternalProcessorContext context = mock(InternalProcessorContext.class);    expect(context.currentNode()).andReturn(null).anyTimes();    context.setCurrentNode(null);    context.setCurrentNode(null);    context.forward(new Windowed<>("key", new SessionWindow(21L, 73L)), new Change<>("newValue", "oldValue"), To.all().withTimestamp(73L));    expectLastCall();    replay(context);    new SessionCacheFlushListener<>(context).apply(new Windowed<>("key", new SessionWindow(21L, 73L)), "newValue", "oldValue", 42L);    verify(context);}
f16585
0
shouldSetFlushListenerOnWrappedStateStore
public void kafkatest_f16586_0()
{    setFlushListener(true);    setFlushListener(false);}
f16586
0
shouldCountSessionWindowed
private void kafkatest_f16594_0()
{    final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();    stream.count().toStream().process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);    }    final Map<Windowed<String>, ValueAndTimestamp<Long>> result = supplier.theCapturedProcessor().lastValueAndTimestampPerKey;    assertThat(result.size(), equalTo(3));    assertThat(result.get(new Windowed<>("1", new SessionWindow(10L, 15L))), equalTo(ValueAndTimestamp.make(2L, 15L)));    assertThat(result.get(new Windowed<>("2", new SessionWindow(599L, 600L))), equalTo(ValueAndTimestamp.make(2L, 600L)));    assertThat(result.get(new Windowed<>("1", new SessionWindow(600L, 600L))), equalTo(ValueAndTimestamp.make(1L, 600L)));}
f16594
0
shouldReduceWindowed
public void kafkatest_f16595_0()
{    final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();    stream.reduce(MockReducer.STRING_ADDER).toStream().process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);    }    final Map<Windowed<String>, ValueAndTimestamp<String>> result = supplier.theCapturedProcessor().lastValueAndTimestampPerKey;    assertThat(result.size(), equalTo(3));    assertThat(result.get(new Windowed<>("1", new SessionWindow(10, 15))), equalTo(ValueAndTimestamp.make("1+2", 15L)));    assertThat(result.get(new Windowed<>("2", new SessionWindow(599L, 600))), equalTo(ValueAndTimestamp.make("1+2", 600L)));    assertThat(result.get(new Windowed<>("1", new SessionWindow(600, 600))), equalTo(ValueAndTimestamp.make("3", 600L)));}
f16595
0
shouldAggregateSessionWindowed
public void kafkatest_f16596_0()
{    final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();    stream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, sessionMerger, Materialized.with(Serdes.String(), Serdes.String())).toStream().process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);    }    final Map<Windowed<String>, ValueAndTimestamp<String>> result = supplier.theCapturedProcessor().lastValueAndTimestampPerKey;    assertThat(result.size(), equalTo(3));    assertThat(result.get(new Windowed<>("1", new SessionWindow(10, 15))), equalTo(ValueAndTimestamp.make("0+0+1+2", 15L)));    assertThat(result.get(new Windowed<>("2", new SessionWindow(599, 600))), equalTo(ValueAndTimestamp.make("0+0+1+2", 600L)));    assertThat(result.get(new Windowed<>("1", new SessionWindow(600, 600))), equalTo(ValueAndTimestamp.make("0+3", 600L)));}
f16596
0
shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull
public void kafkatest_f16604_0()
{    stream.aggregate(null, MockAggregator.TOSTRING_ADDER, sessionMerger, Materialized.as("store"));}
f16604
0
shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull
public void kafkatest_f16605_0()
{    stream.aggregate(MockInitializer.STRING_INIT, null, sessionMerger, Materialized.as("store"));}
f16605
0
shouldThrowNullPointerOnMaterializedAggregateIfMergerIsNull
public void kafkatest_f16606_0()
{    stream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, null, Materialized.as("store"));}
f16606
0
shouldOverlapIfOtherWindowContainsThisWindow
public void kafkatest_f16614_0()
{    /*         * This:        [-------]         * Other: [------------------]         */    assertTrue(window.overlap(new SessionWindow(0, end)));    assertTrue(window.overlap(new SessionWindow(0, end + 1)));    assertTrue(window.overlap(new SessionWindow(0, 150)));    assertTrue(window.overlap(new SessionWindow(start - 1, end)));    assertTrue(window.overlap(new SessionWindow(start - 1, end + 1)));    assertTrue(window.overlap(new SessionWindow(start - 1, 150)));    assertTrue(window.overlap(new SessionWindow(start, end)));    assertTrue(window.overlap(new SessionWindow(start, end + 1)));    assertTrue(window.overlap(new SessionWindow(start, 150)));}
f16614
0
shouldOverlapIfOtherWindowIsWithinThisWindow
public void kafkatest_f16615_0()
{    /*         * This:        [-------]         * Other:         [---]         */    assertTrue(window.overlap(new SessionWindow(start, start)));    assertTrue(window.overlap(new SessionWindow(start, 75)));    assertTrue(window.overlap(new SessionWindow(start, end)));    assertTrue(window.overlap(new SessionWindow(75, end)));    assertTrue(window.overlap(new SessionWindow(end, end)));}
f16615
0
shouldOverlapIfOtherWindowStartIsWithinThisWindow
public void kafkatest_f16616_0()
{    /*         * This:        [-------]         * Other:           [-------]         */    assertTrue(window.overlap(new SessionWindow(start, end + 1)));    assertTrue(window.overlap(new SessionWindow(start, 150)));    assertTrue(window.overlap(new SessionWindow(75, end + 1)));    assertTrue(window.overlap(new SessionWindow(75, 150)));    assertTrue(window.overlap(new SessionWindow(end, end + 1)));    assertTrue(window.overlap(new SessionWindow(end, 150)));}
f16616
0
finalResultsSuppressionShouldBufferAndEmitAtGraceExpiration
public void kafkatest_f16624_0()
{    final Harness<Windowed<String>, Long> harness = new Harness<>(finalResults(ofMillis(1L)), timeWindowedSerdeFrom(String.class, 1L), Long());    final MockInternalProcessorContext context = harness.context;    final long windowStart = 99L;    final long recordTime = 99L;    final long windowEnd = 100L;    context.setRecordMetadata("topic", 0, 0, null, recordTime);    final Windowed<String> key = new Windowed<>("hey", new TimeWindow(windowStart, windowEnd));    final Change<Long> value = ARBITRARY_CHANGE;    harness.processor.process(key, value);    assertThat(context.forwarded(), hasSize(0));    // although the stream time is now 100, we have to wait 1 ms after the window *end* before we    // emit "hey", so we don't emit yet.    final long windowStart2 = 100L;    final long recordTime2 = 100L;    final long windowEnd2 = 101L;    context.setRecordMetadata("topic", 0, 1, null, recordTime2);    harness.processor.process(new Windowed<>("dummyKey1", new TimeWindow(windowStart2, windowEnd2)), ARBITRARY_CHANGE);    assertThat(context.forwarded(), hasSize(0));    // ok, now it's time to emit "hey"    final long windowStart3 = 101L;    final long recordTime3 = 101L;    final long windowEnd3 = 102L;    context.setRecordMetadata("topic", 0, 1, null, recordTime3);    harness.processor.process(new Windowed<>("dummyKey2", new TimeWindow(windowStart3, windowEnd3)), ARBITRARY_CHANGE);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(recordTime));}
f16624
0
finalResultsWithZeroGraceShouldStillBufferUntilTheWindowEnd
public void kafkatest_f16625_0()
{    final Harness<Windowed<String>, Long> harness = new Harness<>(finalResults(ofMillis(0L)), timeWindowedSerdeFrom(String.class, 100L), Long());    final MockInternalProcessorContext context = harness.context;    // note the record is in the past, but the window end is in the future, so we still have to buffer,    // even though the grace period is 0.    final long timestamp = 5L;    final long windowEnd = 100L;    context.setRecordMetadata("", 0, 0L, null, timestamp);    final Windowed<String> key = new Windowed<>("hey", new TimeWindow(0, windowEnd));    final Change<Long> value = ARBITRARY_CHANGE;    harness.processor.process(key, value);    assertThat(context.forwarded(), hasSize(0));    context.setRecordMetadata("", 0, 1L, null, windowEnd);    harness.processor.process(new Windowed<>("dummyKey", new TimeWindow(windowEnd, windowEnd + 100L)), ARBITRARY_CHANGE);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
f16625
0
finalResultsWithZeroGraceAtWindowEndShouldImmediatelyEmit
public void kafkatest_f16626_0()
{    final Harness<Windowed<String>, Long> harness = new Harness<>(finalResults(ofMillis(0L)), timeWindowedSerdeFrom(String.class, 100L), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 100L;    context.setRecordMetadata("", 0, 0L, null, timestamp);    final Windowed<String> key = new Windowed<>("hey", new TimeWindow(0, 100L));    final Change<Long> value = ARBITRARY_CHANGE;    harness.processor.process(key, value);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
f16626
0
suppressShouldShutDownWhenOverRecordCapacity
public void kafkatest_f16634_0()
{    final Harness<String, Long> harness = new Harness<>(untilTimeLimit(Duration.ofDays(100), maxRecords(1).shutDownWhenFull()), String(), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 100L;    context.setRecordMetadata("", 0, 0L, null, timestamp);    context.setCurrentNode(new ProcessorNode("testNode"));    final String key = "hey";    final Change<Long> value = new Change<>(null, ARBITRARY_LONG);    harness.processor.process(key, value);    context.setRecordMetadata("", 0, 1L, null, timestamp);    try {        harness.processor.process("dummyKey", value);        fail("expected an exception");    } catch (final StreamsException e) {        assertThat(e.getMessage(), containsString("buffer exceeded its max capacity"));    }}
f16634
0
suppressShouldShutDownWhenOverByteCapacity
public void kafkatest_f16635_0()
{    final Harness<String, Long> harness = new Harness<>(untilTimeLimit(Duration.ofDays(100), maxBytes(60L).shutDownWhenFull()), String(), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 100L;    context.setRecordMetadata("", 0, 0L, null, timestamp);    context.setCurrentNode(new ProcessorNode("testNode"));    final String key = "hey";    final Change<Long> value = new Change<>(null, ARBITRARY_LONG);    harness.processor.process(key, value);    context.setRecordMetadata("", 0, 1L, null, timestamp);    try {        harness.processor.process("dummyKey", value);        fail("expected an exception");    } catch (final StreamsException e) {        assertThat(e.getMessage(), containsString("buffer exceeded its max capacity"));    }}
f16635
0
finalResults
private static SuppressedInternal<K> kafkatest_f16636_0(final Duration grace)
{    return ((FinalResultsSuppressionBuilder) untilWindowCloses(unbounded())).buildFinalResultsSuppression(grace);}
f16636
0
shouldSuppressIntermediateEventsWithBytesLimit
public void kafkatest_f16644_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<String, Long> valueCounts = builder.table("input", Consumed.with(STRING_SERDE, STRING_SERDE), Materialized.<String, String, KeyValueStore<Bytes, byte[]>>with(STRING_SERDE, STRING_SERDE).withCachingDisabled().withLoggingDisabled()).groupBy((k, v) -> new KeyValue<>(v, k), Grouped.with(STRING_SERDE, STRING_SERDE)).count();    valueCounts.suppress(untilTimeLimit(ofMillis(Long.MAX_VALUE), maxBytes(200L).emitEarlyWhenFull())).toStream().to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().to("output-raw", Produced.with(STRING_SERDE, Serdes.Long()));    final Topology topology = builder.build();    System.out.println(topology.describe());    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(STRING_SERIALIZER, STRING_SERIALIZER);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, config)) {        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v2", 1L));        driver.pipeInput(recordFactory.create("input", "k2", "v1", 2L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("v1", 1L, 0L), new KeyValueTimestamp<>("v1", 0L, 1L), new KeyValueTimestamp<>("v2", 1L, 1L), new KeyValueTimestamp<>("v1", 1L, 2L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(// consecutive updates to v1 get suppressed into only the latter.        new KeyValueTimestamp<>("v1", 0L, 1L), new KeyValueTimestamp<>("v2", 1L, 1L)));        driver.pipeInput(recordFactory.create("input", "x", "x", 3L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("x", 1L, 3L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(// now we see that last update to v1, but we won't see the update to x until it gets evicted        new KeyValueTimestamp<>("v1", 1L, 2L)));    }}
f16644
0
shouldSupportFinalResultsForTimeWindows
public void kafkatest_f16645_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<Windowed<String>, Long> valueCounts = builder.stream("input", Consumed.with(STRING_SERDE, STRING_SERDE)).groupBy((String k, String v) -> k, Grouped.with(STRING_SERDE, STRING_SERDE)).windowedBy(TimeWindows.of(ofMillis(2L)).grace(ofMillis(1L))).count(Materialized.<String, Long, WindowStore<Bytes, byte[]>>as("counts").withCachingDisabled());    valueCounts.suppress(untilWindowCloses(unbounded())).toStream().map((final Windowed<String> k, final Long v) -> new KeyValue<>(k.toString(), v)).to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().map((final Windowed<String> k, final Long v) -> new KeyValue<>(k.toString(), v)).to("output-raw", Produced.with(STRING_SERDE, Serdes.Long()));    final Topology topology = builder.build();    System.out.println(topology.describe());    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(STRING_SERIALIZER, STRING_SERIALIZER);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, config)) {        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 1L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 2L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 1L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 5L));        // note this last record gets dropped because it is out of the grace period        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("[k1@0/2]", 1L, 0L), new KeyValueTimestamp<>("[k1@0/2]", 2L, 1L), new KeyValueTimestamp<>("[k1@2/4]", 1L, 2L), new KeyValueTimestamp<>("[k1@0/2]", 3L, 1L), new KeyValueTimestamp<>("[k1@0/2]", 4L, 1L), new KeyValueTimestamp<>("[k1@4/6]", 1L, 5L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("[k1@0/2]", 4L, 1L), new KeyValueTimestamp<>("[k1@2/4]", 1L, 2L)));    }}
f16645
0
shouldSupportFinalResultsForTimeWindowsWithLargeJump
public void kafkatest_f16646_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<Windowed<String>, Long> valueCounts = builder.stream("input", Consumed.with(STRING_SERDE, STRING_SERDE)).groupBy((String k, String v) -> k, Grouped.with(STRING_SERDE, STRING_SERDE)).windowedBy(TimeWindows.of(ofMillis(2L)).grace(ofMillis(2L))).count(Materialized.<String, Long, WindowStore<Bytes, byte[]>>as("counts").withCachingDisabled().withKeySerde(STRING_SERDE));    valueCounts.suppress(untilWindowCloses(unbounded())).toStream().map((final Windowed<String> k, final Long v) -> new KeyValue<>(k.toString(), v)).to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().map((final Windowed<String> k, final Long v) -> new KeyValue<>(k.toString(), v)).to("output-raw", Produced.with(STRING_SERDE, Serdes.Long()));    final Topology topology = builder.build();    System.out.println(topology.describe());    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(STRING_SERIALIZER, STRING_SERIALIZER);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, config)) {        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 1L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 2L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 3L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 4L));        // this update should get dropped, since the previous event advanced the stream time and closed the window.        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v1", 30L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("[k1@0/2]", 1L, 0L), new KeyValueTimestamp<>("[k1@0/2]", 2L, 1L), new KeyValueTimestamp<>("[k1@2/4]", 1L, 2L), new KeyValueTimestamp<>("[k1@0/2]", 3L, 1L), new KeyValueTimestamp<>("[k1@2/4]", 2L, 3L), new KeyValueTimestamp<>("[k1@0/2]", 4L, 1L), new KeyValueTimestamp<>("[k1@4/6]", 1L, 4L), new KeyValueTimestamp<>("[k1@30/32]", 1L, 30L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("[k1@0/2]", 4L, 1L), new KeyValueTimestamp<>("[k1@2/4]", 2L, 3L), new KeyValueTimestamp<>("[k1@4/6]", 1L, 4L)));    }}
f16646
0
shouldUseNumberingForAnonymousFinalSuppressionNode
public void kafkatest_f16654_0()
{    final StreamsBuilder anonymousNodeBuilder = new StreamsBuilder();    anonymousNodeBuilder.stream("input", Consumed.with(STRING_SERDE, STRING_SERDE)).groupBy((String k, String v) -> k, Grouped.with(STRING_SERDE, STRING_SERDE)).windowedBy(SessionWindows.with(ofMillis(5L)).grace(ofMillis(5L))).count(Materialized.<String, Long, SessionStore<Bytes, byte[]>>as("counts").withCachingDisabled()).suppress(untilWindowCloses(unbounded())).toStream().map((final Windowed<String> k, final Long v) -> new KeyValue<>(k.toString(), v)).to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    final String anonymousNodeTopology = anonymousNodeBuilder.build().describe().toString();    // without the name, the suppression node increments the topology index    assertThat(anonymousNodeTopology, is(ANONYMOUS_FINAL_TOPOLOGY));}
f16654
0
shouldApplyNameToFinalSuppressionNode
public void kafkatest_f16655_0()
{    final StreamsBuilder namedNodeBuilder = new StreamsBuilder();    namedNodeBuilder.stream("input", Consumed.with(STRING_SERDE, STRING_SERDE)).groupBy((String k, String v) -> k, Grouped.with(STRING_SERDE, STRING_SERDE)).windowedBy(SessionWindows.with(ofMillis(5L)).grace(ofMillis(5L))).count(Materialized.<String, Long, SessionStore<Bytes, byte[]>>as("counts").withCachingDisabled()).suppress(untilWindowCloses(unbounded()).withName("myname")).toStream().map((final Windowed<String> k, final Long v) -> new KeyValue<>(k.toString(), v)).to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    final String namedNodeTopology = namedNodeBuilder.build().describe().toString();    // without the name, the suppression node does not increment the topology index    assertThat(namedNodeTopology, is(NAMED_FINAL_TOPOLOGY));}
f16655
0
shouldUseNumberingForAnonymousSuppressionNode
public void kafkatest_f16656_0()
{    final StreamsBuilder anonymousNodeBuilder = new StreamsBuilder();    anonymousNodeBuilder.stream("input", Consumed.with(STRING_SERDE, STRING_SERDE)).groupByKey().count().suppress(untilTimeLimit(Duration.ofSeconds(1), unbounded())).toStream().to("output", Produced.with(STRING_SERDE, Serdes.Long()));    final String anonymousNodeTopology = anonymousNodeBuilder.build().describe().toString();    // without the name, the suppression node increments the topology index    assertThat(anonymousNodeTopology, is(ANONYMOUS_INTERMEDIATE_TOPOLOGY));}
f16656
0
shouldNotForwardRecordsIfWrappedStateStoreDoesCache
public void kafkatest_f16664_0()
{    final WrappedStateStore<StateStore, String, String> store = mock(WrappedStateStore.class);    final ProcessorContext context = mock(ProcessorContext.class);    expect(store.setFlushListener(null, false)).andReturn(true);    replay(store, context);    final TimestampedTupleForwarder<String, String> forwarder = new TimestampedTupleForwarder<>(store, context, null, false);    forwarder.maybeForward("key", "newValue", "oldValue");    forwarder.maybeForward("key", "newValue", "oldValue", 42L);    verify(store, context);}
f16664
0
before
public void kafkatest_f16665_0()
{    final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));    windowedStream = stream.groupByKey(Grouped.with(Serdes.String(), Serdes.String())).windowedBy(TimeWindows.of(ofMillis(500L)));}
f16665
0
shouldCountWindowed
public void kafkatest_f16666_0()
{    final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();    windowedStream.count().toStream().process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);    }    assertThat(supplier.theCapturedProcessor().lastValueAndTimestampPerKey.get(new Windowed<>("1", new TimeWindow(0L, 500L))), equalTo(ValueAndTimestamp.make(2L, 15L)));    assertThat(supplier.theCapturedProcessor().lastValueAndTimestampPerKey.get(new Windowed<>("2", new TimeWindow(500L, 1000L))), equalTo(ValueAndTimestamp.make(2L, 550L)));    assertThat(supplier.theCapturedProcessor().lastValueAndTimestampPerKey.get(new Windowed<>("1", new TimeWindow(500L, 1000L))), equalTo(ValueAndTimestamp.make(1L, 500L)));}
f16666
0
shouldThrowNullPointerOnReduceIfReducerIsNull
public void kafkatest_f16674_0()
{    windowedStream.reduce(null);}
f16674
0
shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull
public void kafkatest_f16675_0()
{    windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as("store"));}
f16675
0
shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull
public void kafkatest_f16676_0()
{    windowedStream.aggregate(MockInitializer.STRING_INIT, null, Materialized.as("store"));}
f16676
0
shouldOverlapIfOtherWindowEndIsWithinThisWindow
public void kafkatest_f16684_0()
{    /*         * This:        [-------)         * Other: [---------)         */    assertTrue(window.overlap(new TimeWindow(0, start + 1)));    assertTrue(window.overlap(new TimeWindow(0, 75)));    assertTrue(window.overlap(new TimeWindow(0, end - 1)));    assertTrue(window.overlap(new TimeWindow(start - 1, start + 1)));    assertTrue(window.overlap(new TimeWindow(start - 1, 75)));    assertTrue(window.overlap(new TimeWindow(start - 1, end - 1)));}
f16684
0
shouldOverlapIfOtherWindowContainsThisWindow
public void kafkatest_f16685_0()
{    /*         * This:        [-------)         * Other: [------------------)         */    assertTrue(window.overlap(new TimeWindow(0, end)));    assertTrue(window.overlap(new TimeWindow(0, end + 1)));    assertTrue(window.overlap(new TimeWindow(0, 150)));    assertTrue(window.overlap(new TimeWindow(start - 1, end)));    assertTrue(window.overlap(new TimeWindow(start - 1, end + 1)));    assertTrue(window.overlap(new TimeWindow(start - 1, 150)));    assertTrue(window.overlap(new TimeWindow(start, end)));    assertTrue(window.overlap(new TimeWindow(start, end + 1)));    assertTrue(window.overlap(new TimeWindow(start, 150)));}
f16685
0
shouldOverlapIfOtherWindowIsWithinThisWindow
public void kafkatest_f16686_0()
{    /*         * This:        [-------)         * Other:         [---)         */    assertTrue(window.overlap(new TimeWindow(start, 75)));    assertTrue(window.overlap(new TimeWindow(start, end)));    assertTrue(window.overlap(new TimeWindow(75, end)));}
f16686
0
shouldCallTransformOfAdapteeTransformerAndReturnSingletonIterable
public void kafkatest_f16694_0()
{    EasyMock.expect(transformerSupplier.get()).andReturn(transformer);    EasyMock.expect(transformer.transform(key, value)).andReturn(KeyValue.pair(0, 1));    replayAll();    final TransformerSupplierAdapter<String, String, Integer, Integer> adapter = new TransformerSupplierAdapter<>(transformerSupplier);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adaptedTransformer = adapter.get();    final Iterator<KeyValue<Integer, Integer>> iterator = adaptedTransformer.transform(key, value).iterator();    verifyAll();    assertThat(iterator.hasNext(), equalTo(true));    iterator.next();    assertThat(iterator.hasNext(), equalTo(false));}
f16694
0
shouldCallTransformOfAdapteeTransformerAndReturnEmptyIterable
public void kafkatest_f16695_0()
{    EasyMock.expect(transformerSupplier.get()).andReturn(transformer);    EasyMock.expect(transformer.transform(key, value)).andReturn(null);    replayAll();    final TransformerSupplierAdapter<String, String, Integer, Integer> adapter = new TransformerSupplierAdapter<>(transformerSupplier);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adaptedTransformer = adapter.get();    final Iterator<KeyValue<Integer, Integer>> iterator = adaptedTransformer.transform(key, value).iterator();    verifyAll();    assertThat(iterator.hasNext(), equalTo(false));}
f16695
0
shouldAlwaysGetNewAdapterTransformer
public void kafkatest_f16696_0()
{    final Transformer<String, String, KeyValue<Integer, Integer>> transformer1 = mock(Transformer.class);    final Transformer<String, String, KeyValue<Integer, Integer>> transformer2 = mock(Transformer.class);    final Transformer<String, String, KeyValue<Integer, Integer>> transformer3 = mock(Transformer.class);    EasyMock.expect(transformerSupplier.get()).andReturn(transformer1);    transformer1.init(context);    EasyMock.expect(transformerSupplier.get()).andReturn(transformer2);    transformer2.init(context);    EasyMock.expect(transformerSupplier.get()).andReturn(transformer3);    transformer3.init(context);    replayAll();    final TransformerSupplierAdapter<String, String, Integer, Integer> adapter = new TransformerSupplierAdapter<>(transformerSupplier);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adapterTransformer1 = adapter.get();    adapterTransformer1.init(context);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adapterTransformer2 = adapter.get();    adapterTransformer2.init(context);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adapterTransformer3 = adapter.get();    adapterTransformer3.init(context);    verifyAll();    assertThat(adapterTransformer1, not(sameInstance(adapterTransformer2)));    assertThat(adapterTransformer2, not(sameInstance(adapterTransformer3)));    assertThat(adapterTransformer3, not(sameInstance(adapterTransformer1)));}
f16696
0
untilShouldSetGraceDuration
public void kafkatest_f16704_0()
{    final JoinWindows windowSpec = JoinWindows.of(ofMillis(ANY_SIZE));    final long windowSize = windowSpec.size();    assertEquals(windowSize, windowSpec.grace(ofMillis(windowSize)).gracePeriodMs());}
f16704
0
retentionTimeMustNoBeSmallerThanWindowSize
public void kafkatest_f16705_0()
{    final JoinWindows windowSpec = JoinWindows.of(ofMillis(ANY_SIZE));    final long windowSize = windowSpec.size();    try {        windowSpec.until(windowSize - 1);        fail("should not accept retention time smaller than window size");    } catch (final IllegalArgumentException e) {    // expected    }}
f16705
0
gracePeriodShouldEnforceBoundaries
public void kafkatest_f16706_0()
{    JoinWindows.of(ofMillis(3L)).grace(ofMillis(0L));    try {        JoinWindows.of(ofMillis(3L)).grace(ofMillis(-1L));        fail("should not accept negatives");    } catch (final IllegalArgumentException e) {    // expected    }}
f16706
0
shouldThrowExceptionGivenNullName
public void kafkatest_f16714_0()
{    Named.as(null);}
f16714
0
shouldThrowExceptionOnInvalidTopicNames
public void kafkatest_f16715_0()
{    final char[] longString = new char[250];    Arrays.fill(longString, 'a');    final String[] invalidNames = { "", "foo bar", "..", "foo:bar", "foo=bar", ".", new String(longString) };    for (final String name : invalidNames) {        try {            Named.validate(name);            fail("No exception was thrown for named with invalid name: " + name);        } catch (final TopologyException e) {        // success        }    }}
f16715
0
before
public void kafkatest_f16716_0()
{    System.setOut(new PrintStream(sysOut));    sysOutPrinter = Printed.toSysOut();}
f16716
0
shouldThrowNullPointerExceptionIfMapperIsNull
public void kafkatest_f16724_0()
{    sysOutPrinter.withKeyValueMapper(null);}
f16724
0
shouldThrowNullPointerExceptionIfLabelIsNull
public void kafkatest_f16725_0()
{    sysOutPrinter.withLabel(null);}
f16725
0
shouldThrowTopologyExceptionIfFilePathIsEmpty
public void kafkatest_f16726_0()
{    Printed.toFile("");}
f16726
0
shouldNotReuseRepartitionNodeWithUnamedRepartitionTopics
public void kafkatest_f16734_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KGroupedStream<String, String> kGroupedStream = builder.<String, String>stream("topic").selectKey((k, v) -> k).groupByKey();    kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(10L))).count().toStream().to("output-one");    kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(30L))).count().toStream().to("output-two");    final String topologyString = builder.build().describe().toString();    assertThat(2, is(getCountOfRepartitionTopicsFound(topologyString, repartitionTopicPattern)));}
f16734
0
shouldNotReuseRepartitionNodeWithUnamedRepartitionTopicsKGroupedTable
public void kafkatest_f16735_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KGroupedTable<String, String> kGroupedTable = builder.<String, String>table("topic").groupBy(KeyValue::pair);    kGroupedTable.count().toStream().to("output-count");    kGroupedTable.reduce((v, v2) -> v2, (v, v2) -> v2).toStream().to("output-reduce");    final String topologyString = builder.build().describe().toString();    assertThat(2, is(getCountOfRepartitionTopicsFound(topologyString, repartitionTopicPattern)));}
f16735
0
shouldNotFailWithSameRepartitionTopicNameUsingSameKGroupedStreamOptimizationsOn
public void kafkatest_f16736_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KGroupedStream<String, String> kGroupedStream = builder.<String, String>stream("topic").selectKey((k, v) -> k).groupByKey(Grouped.as("grouping"));    kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(10L))).count();    kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(30L))).count();    final Properties properties = new Properties();    properties.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    final Topology topology = builder.build(properties);    assertThat(getCountOfRepartitionTopicsFound(topology.describe().toString(), repartitionTopicPattern), is(1));}
f16736
0
shouldKeepRepartitionTopicNameForGroupByKeySessionWindows
public void kafkatest_f16744_0()
{    final String expectedSessionWindowRepartitionTopic = "(topic: session-window-grouping-repartition)";    final String sessionWindowGroupingRepartitionTopology = buildStreamGroupByKeySessionWindows(false, true);    assertTrue(sessionWindowGroupingRepartitionTopology.contains(expectedSessionWindowRepartitionTopic));    final String sessionWindowGroupingUpdatedTopology = buildStreamGroupByKeySessionWindows(true, true);    assertTrue(sessionWindowGroupingUpdatedTopology.contains(expectedSessionWindowRepartitionTopic));}
f16744
0
shouldKeepRepartitionTopicNameForGroupBySessionWindows
public void kafkatest_f16745_0()
{    final String expectedSessionWindowRepartitionTopic = "(topic: session-window-grouping-repartition)";    final String sessionWindowGroupingRepartitionTopology = buildStreamGroupByKeySessionWindows(false, false);    assertTrue(sessionWindowGroupingRepartitionTopology.contains(expectedSessionWindowRepartitionTopic));    final String sessionWindowGroupingUpdatedTopology = buildStreamGroupByKeySessionWindows(true, false);    assertTrue(sessionWindowGroupingUpdatedTopology.contains(expectedSessionWindowRepartitionTopic));}
f16745
0
shouldKeepRepartitionNameForGroupByKTable
public void kafkatest_f16746_0()
{    final String expectedKTableGroupByRepartitionTopic = "(topic: ktable-group-by-repartition)";    final String ktableGroupByTopology = buildKTableGroupBy(false);    assertTrue(ktableGroupByTopology.contains(expectedKTableGroupByRepartitionTopic));    final String ktableUpdatedGroupByTopology = buildKTableGroupBy(true);    assertTrue(ktableUpdatedGroupByTopology.contains(expectedKTableGroupByRepartitionTopic));}
f16746
0
process
public void kafkatest_f16754_0(final String key, final String value)
{    valueList.add(value);}
f16754
0
setUp
public void kafkatest_f16755_0()
{    props.put(StreamsConfig.DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS, Serdes.StringSerde.class.getName());    props.put(StreamsConfig.DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS, Serdes.ByteArraySerde.class.getName());}
f16755
0
testWindowedKeyDeserializerNoArgConstructors
public void kafkatest_f16756_0()
{    sessionWindowedDeserializer.configure(props, true);    final Deserializer<?> inner = sessionWindowedDeserializer.innerDeserializer();    assertNotNull("Inner deserializer should be not null", inner);    assertTrue("Inner deserializer type should be StringDeserializer", inner instanceof StringDeserializer);}
f16756
0
windowSizeMustNotBeNegative
public void kafkatest_f16764_0()
{    SessionWindows.with(ofMillis(-1));}
f16764
0
windowSizeMustNotBeZero
public void kafkatest_f16765_0()
{    SessionWindows.with(ofMillis(0));}
f16765
0
retentionTimeShouldBeGapIfGapIsLargerThanDefaultRetentionTime
public void kafkatest_f16766_0()
{    final long windowGap = 2 * SessionWindows.with(ofMillis(1)).maintainMs();    assertEquals(windowGap, SessionWindows.with(ofMillis(windowGap)).maintainMs());}
f16766
0
testWindowedKeyDeserializerNoArgConstructors
public void kafkatest_f16774_0()
{    timeWindowedDeserializer.configure(props, true);    final Deserializer<?> inner = timeWindowedDeserializer.innerDeserializer();    assertNotNull("Inner deserializer should be not null", inner);    assertTrue("Inner deserializer type should be StringDeserializer", inner instanceof StringDeserializer);}
f16774
0
testWindowedValueDeserializerNoArgConstructors
public void kafkatest_f16775_0()
{    timeWindowedDeserializer.configure(props, false);    final Deserializer<?> inner = timeWindowedDeserializer.innerDeserializer();    assertNotNull("Inner deserializer should be not null", inner);    assertTrue("Inner deserializer type should be ByteArrayDeserializer", inner instanceof ByteArrayDeserializer);}
f16775
0
setUp
public void kafkatest_f16776_0()
{    props.put(StreamsConfig.DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS, Serdes.StringSerde.class.getName());    props.put(StreamsConfig.DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS, Serdes.ByteArraySerde.class.getName());}
f16776
0
windowSizeMustNotBeNegative
public void kafkatest_f16784_0()
{    TimeWindows.of(ofMillis(-1));}
f16784
0
advanceIntervalMustNotBeZero
public void kafkatest_f16785_0()
{    final TimeWindows windowSpec = TimeWindows.of(ofMillis(ANY_SIZE));    try {        windowSpec.advanceBy(ofMillis(0));        fail("should not accept zero advance parameter");    } catch (final IllegalArgumentException e) {    // expected    }}
f16785
0
advanceIntervalMustNotBeNegative
public void kafkatest_f16786_0()
{    final TimeWindows windowSpec = TimeWindows.of(ofMillis(ANY_SIZE));    try {        windowSpec.advanceBy(ofMillis(-1));        fail("should not accept negative advance parameter");    } catch (final IllegalArgumentException e) {    // expected    }}
f16786
0
equalsAndHashcodeShouldBeValidForNegativeCases
public void kafkatest_f16794_0()
{    verifyInEquality(TimeWindows.of(ofMillis(9)), TimeWindows.of(ofMillis(3)));    verifyInEquality(TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(2)), TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)));    verifyInEquality(TimeWindows.of(ofMillis(3)).grace(ofMillis(2)), TimeWindows.of(ofMillis(3)).grace(ofMillis(1)));    verifyInEquality(TimeWindows.of(ofMillis(3)).grace(ofMillis(9)), TimeWindows.of(ofMillis(3)).grace(ofMillis(4)));    verifyInEquality(TimeWindows.of(ofMillis(4)).advanceBy(ofMillis(2)).grace(ofMillis(2)), TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(2)).grace(ofMillis(2)));    verifyInEquality(TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)).grace(ofMillis(2)), TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(2)).grace(ofMillis(2)));    assertNotEquals(TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(2)).grace(ofMillis(1)), TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(2)).grace(ofMillis(2)));}
f16794
0
shouldSetWindowStartTime
public void kafkatest_f16795_0()
{    assertEquals(anyStartTime, UnlimitedWindows.of().startOn(ofEpochMilli(anyStartTime)).startMs);}
f16795
0
startTimeMustNotBeNegative
public void kafkatest_f16796_0()
{    UnlimitedWindows.of().startOn(ofEpochMilli(-1));}
f16796
0
shouldWrapForSessionWindowedSerde
public void kafkatest_f16804_0()
{    final Serde<Windowed<String>> serde = WindowedSerdes.sessionWindowedSerdeFrom(String.class);    assertTrue(serde.serializer() instanceof SessionWindowedSerializer);    assertTrue(serde.deserializer() instanceof SessionWindowedDeserializer);    assertTrue(((SessionWindowedSerializer) serde.serializer()).innerSerializer() instanceof StringSerializer);    assertTrue(((SessionWindowedDeserializer) serde.deserializer()).innerDeserializer() instanceof StringDeserializer);}
f16804
0
testTimeWindowSerdeFrom
public void kafkatest_f16805_0()
{    final Windowed<Integer> timeWindowed = new Windowed<>(10, new TimeWindow(0, Long.MAX_VALUE));    final Serde<Windowed<Integer>> timeWindowedSerde = WindowedSerdes.timeWindowedSerdeFrom(Integer.class);    final byte[] bytes = timeWindowedSerde.serializer().serialize(topic, timeWindowed);    final Windowed<Integer> windowed = timeWindowedSerde.deserializer().deserialize(topic, bytes);    Assert.assertEquals(timeWindowed, windowed);}
f16805
0
testSessionWindowedSerdeFrom
public void kafkatest_f16806_0()
{    final Windowed<Integer> sessionWindowed = new Windowed<>(10, new SessionWindow(0, 1));    final Serde<Windowed<Integer>> sessionWindowedSerde = WindowedSerdes.sessionWindowedSerdeFrom(Integer.class);    final byte[] bytes = sessionWindowedSerde.serializer().serialize(topic, sessionWindowed);    final Windowed<Integer> windowed = sessionWindowedSerde.deserializer().deserialize(topic, bytes);    Assert.assertEquals(sessionWindowed, windowed);}
f16806
0
timeWindowedDeserializerShouldNotThrowOnCloseIfNotInitializedProperly
public void kafkatest_f16814_0()
{    new TimeWindowedDeserializer<>().close();}
f16814
0
sessionWindowedSerializerShouldNotThrowOnCloseIfNotInitializedProperly
public void kafkatest_f16815_0()
{    new SessionWindowedSerializer<>().close();}
f16815
0
sessionWindowedDeserializerShouldNotThrowOnCloseIfNotInitializedProperly
public void kafkatest_f16816_0()
{    new SessionWindowedDeserializer<>().close();}
f16816
0
overlap
public boolean kafkatest_f16824_0(final Window other)
{    return false;}
f16824
0
shouldThrowIfStartIsNegative
public void kafkatest_f16825_0()
{    new TestWindow(-1, 0);}
f16825
0
shouldThrowIfEndIsSmallerThanStart
public void kafkatest_f16826_0()
{    new TestWindow(1, 0);}
f16826
0
setStreamProperties
public void kafkatest_f16834_0(final String applicationId)
{    props.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);    props.put(StreamsConfig.CLIENT_ID_CONFIG, "simple-benchmark");    props.put(StreamsConfig.POLL_MS_CONFIG, POLL_MS);    props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL_MS);    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.ByteArray().getClass());    // the socket buffer needs to be large, especially when running in AWS with    // high latency. if running locally the default is fine.    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    props.put(ConsumerConfig.RECEIVE_BUFFER_CONFIG, SOCKET_SIZE_BYTES);    props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, MAX_POLL_RECORDS);    // improve producer throughput    props.put(ProducerConfig.LINGER_MS_CONFIG, 5000);    props.put(ProducerConfig.BATCH_SIZE_CONFIG, 128 * 1024);}
f16834
0
setProduceConsumeProperties
private Properties kafkatest_f16835_0(final String clientId)
{    final Properties clientProps = new Properties();    clientProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, props.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG));    clientProps.put(ProducerConfig.CLIENT_ID_CONFIG, clientId);    // the socket buffer needs to be large, especially when running in AWS with    // high latency. if running locally the default is fine.    clientProps.put(ProducerConfig.LINGER_MS_CONFIG, 5000);    clientProps.put(ProducerConfig.BATCH_SIZE_CONFIG, 128 * 1024);    clientProps.put(ProducerConfig.SEND_BUFFER_CONFIG, SOCKET_SIZE_BYTES);    clientProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);    clientProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);    clientProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);    clientProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);    clientProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");    // the socket buffer needs to be large, especially when running in AWS with    // high latency. if running locally the default is fine.    clientProps.put(ConsumerConfig.RECEIVE_BUFFER_CONFIG, SOCKET_SIZE_BYTES);    clientProps.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, MAX_POLL_RECORDS);    return clientProps;}
f16835
0
resetStats
 void kafkatest_f16836_0()
{    processedRecords = 0;    processedBytes = 0L;}
f16836
0
init
public void kafkatest_f16844_0(final ProcessorContext context)
{    super.init(context);    store = (KeyValueStore<Integer, byte[]>) context.getStateStore("store");}
f16844
0
process
public void kafkatest_f16845_0(final Integer key, final byte[] value)
{    store.get(key);    store.put(key, value);}
f16845
0
processStreamWithWindowStore
private void kafkatest_f16846_0(final String topic)
{    final CountDownLatch latch = new CountDownLatch(1);    setStreamProperties("simple-benchmark-streams-with-store");    final StreamsBuilder builder = new StreamsBuilder();    final StoreBuilder<WindowStore<Integer, byte[]>> storeBuilder = Stores.windowStoreBuilder(Stores.persistentWindowStore("store", ofMillis(AGGREGATE_WINDOW_SIZE * 3), ofMillis(AGGREGATE_WINDOW_SIZE), false), INTEGER_SERDE, BYTE_SERDE);    builder.addStateStore(storeBuilder.withCachingEnabled());    final KStream<Integer, byte[]> source = builder.stream(topic);    source.peek(new CountDownAction(latch)).process(new ProcessorSupplier<Integer, byte[]>() {        @Override        public Processor<Integer, byte[]> get() {            return new AbstractProcessor<Integer, byte[]>() {                WindowStore<Integer, byte[]> store;                @SuppressWarnings("unchecked")                @Override                public void init(final ProcessorContext context) {                    super.init(context);                    store = (WindowStore<Integer, byte[]>) context.getStateStore("store");                }                @Override                public void process(final Integer key, final byte[] value) {                    final long timestamp = context().timestamp();                    final KeyValueIterator<Windowed<Integer>, byte[]> iter = store.fetch(key - 10, key + 10, ofEpochMilli(timestamp - 1000L), ofEpochMilli(timestamp));                    while (iter.hasNext()) {                        iter.next();                    }                    iter.close();                    store.put(key, value);                }            };        }    }, "store");    final KafkaStreams streams = createKafkaStreamsWithExceptionHandler(builder, props);    runGenericBenchmark(streams, "Streams Stateful Performance [records/latency/rec-sec/MB-sec joined]: ", latch);}
f16846
0
tableTableJoin
private void kafkatest_f16854_0(final String kTableTopic1, final String kTableTopic2)
{    final CountDownLatch latch = new CountDownLatch(1);    // setup join    setStreamProperties("simple-benchmark-table-table-join");    final StreamsBuilder builder = new StreamsBuilder();    final KTable<Integer, byte[]> input1 = builder.table(kTableTopic1);    final KTable<Integer, byte[]> input2 = builder.table(kTableTopic2);    input1.leftJoin(input2, VALUE_JOINER).toStream().foreach(new CountDownAction(latch));    final KafkaStreams streams = createKafkaStreamsWithExceptionHandler(builder, props);    // run benchmark    runGenericBenchmark(streams, "Streams KTableKTable LeftJoin Performance [records/latency/rec-sec/MB-sec joined]: ", latch);}
f16854
0
printResults
 void kafkatest_f16855_0(final String nameOfBenchmark, final long latency)
{    System.out.println(nameOfBenchmark + processedRecords + "/" + latency + "/" + recordsPerSec(latency, processedRecords) + "/" + megabytesPerSec(latency, processedBytes));}
f16855
0
runGenericBenchmark
 void kafkatest_f16856_0(final KafkaStreams streams, final String nameOfBenchmark, final CountDownLatch latch)
{    streams.start();    final long startTime = System.currentTimeMillis();    long endTime = startTime;    while (latch.getCount() > 0 && (endTime - startTime < MAX_WAIT_MS)) {        try {            latch.await(1000, TimeUnit.MILLISECONDS);        } catch (final InterruptedException ex) {            Thread.interrupted();        }        endTime = System.currentTimeMillis();    }    streams.close();    printResults(nameOfBenchmark, endTime - startTime);}
f16856
0
next
 int kafkatest_f16864_0()
{    if (skew == 0.0d) {        return rand.nextInt(size);    } else {        int rank;        double dice;        double frequency;        rank = rand.nextInt(size);        frequency = (1.0d / Math.pow(rank, this.skew)) / this.bottom;        dice = rand.nextDouble();        while (!(dice < frequency)) {            rank = rand.nextInt(size);            frequency = (1.0d / Math.pow(rank, this.skew)) / this.bottom;            dice = rand.nextDouble();        }        return rank;    }}
f16864
0
maybeSetupPhaseCampaigns
private boolean kafkatest_f16865_0(final String topic, final String clientId, final boolean skipIfAllTests, final int numCampaigns, final int adsPerCampaign, final List<String> ads)
{    parent.resetStats();    // initialize topics    System.out.println("Initializing topic " + topic);    final Properties props = new Properties();    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, parent.props.get(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG));    props.put(ProducerConfig.CLIENT_ID_CONFIG, clientId);    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    try (final KafkaProducer<String, String> producer = new KafkaProducer<>(props)) {        for (int c = 0; c < numCampaigns; c++) {            final String campaignID = UUID.randomUUID().toString();            for (int a = 0; a < adsPerCampaign; a++) {                final String adId = UUID.randomUUID().toString();                final String concat = adId + ":" + campaignID;                producer.send(new ProducerRecord<>(topic, adId, concat));                ads.add(adId);                parent.processedRecords++;                parent.processedBytes += concat.length() + adId.length();            }        }    }    return true;}
f16865
0
maybeSetupPhaseEvents
private void kafkatest_f16866_0(final String topic, final String clientId, final int numRecords, final List<String> ads)
{    parent.resetStats();    final String[] eventTypes = new String[] { "view", "click", "purchase" };    final Random rand = new Random(System.currentTimeMillis());    System.out.println("Initializing topic " + topic);    final Properties props = new Properties();    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, parent.props.get(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG));    props.put(ProducerConfig.CLIENT_ID_CONFIG, clientId);    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);    final long startTime = System.currentTimeMillis();    try (final KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props)) {        final ProjectedEvent event = new ProjectedEvent();        final Map<String, Object> serdeProps = new HashMap<>();        final Serializer<ProjectedEvent> projectedEventSerializer = new JsonPOJOSerializer<>();        serdeProps.put("JsonPOJOClass", ProjectedEvent.class);        projectedEventSerializer.configure(serdeProps, false);        for (int i = 0; i < numRecords; i++) {            event.eventType = eventTypes[rand.nextInt(eventTypes.length - 1)];            event.adID = ads.get(rand.nextInt(ads.size() - 1));            event.eventTime = System.currentTimeMillis();            final byte[] value = projectedEventSerializer.serialize(topic, event);            producer.send(new ProducerRecord<>(topic, event.adID, value));            parent.processedRecords++;            parent.processedBytes += value.length + event.adID.length();        }    }    final long endTime = System.currentTimeMillis();    parent.printResults("Producer Performance [records/latency/rec-sec/MB-sec write]: ", endTime - startTime);}
f16866
0
shouldNotCreateAnyTasksBecauseOneTopicHasUnknownPartitions
public void kafkatest_f16874_0()
{    final PartitionGrouper grouper = new DefaultPartitionGrouper();    final Map<Integer, Set<String>> topicGroups = new HashMap<>();    final int topicGroupId = 0;    topicGroups.put(topicGroupId, mkSet("topic1", "unknownTopic", "topic2"));    grouper.partitionGroups(topicGroups, metadata);}
f16874
0
extractMetadataTimestamp
public void kafkatest_f16875_0()
{    testExtractMetadataTimestamp(new FailOnInvalidTimestamp());}
f16875
0
failOnInvalidTimestamp
public void kafkatest_f16876_0()
{    final TimestampExtractor extractor = new FailOnInvalidTimestamp();    extractor.extract(new ConsumerRecord<>("anyTopic", 0, 0, null, null), 42);}
f16876
0
shouldThrowIllegalStateExceptionOnPartitionIfNoRecordContext
public void kafkatest_f16884_0()
{    context.setRecordContext(null);    try {        context.partition();        fail("should throw illegal state exception when record context is null");    } catch (final IllegalStateException e) {    // pass    }}
f16884
0
shouldReturnPartitionFromRecordContext
public void kafkatest_f16885_0()
{    assertThat(context.partition(), equalTo(recordContext.partition()));}
f16885
0
shouldThrowIllegalStateExceptionOnOffsetIfNoRecordContext
public void kafkatest_f16886_0()
{    context.setRecordContext(null);    try {        context.offset();    } catch (final IllegalStateException e) {    // pass    }}
f16886
0
appConfigsShouldReturnUnrecognizedValues
public void kafkatest_f16894_0()
{    assertThat(context.appConfigs().get("user.supplied.config"), equalTo("user-suppplied-value"));}
f16894
0
getStateStore
public StateStore kafkatest_f16895_0(final String name)
{    return null;}
f16895
0
schedule
public Cancellable kafkatest_f16896_0(final long interval, final PunctuationType type, final Punctuator callback)
{    return null;}
f16896
0
initializeStateStores
public boolean kafkatest_f16914_0()
{    return false;}
f16914
0
before
public void kafkatest_f16916_0()
{    assignedTasks = new AssignedStreamsTasks(new LogContext("log "));    EasyMock.expect(t1.id()).andReturn(taskId1).anyTimes();    EasyMock.expect(t2.id()).andReturn(taskId2).anyTimes();}
f16916
0
shouldInitializeNewTasks
public void kafkatest_f16917_0()
{    EasyMock.expect(t1.initializeStateStores()).andReturn(false);    EasyMock.expect(t1.partitions()).andReturn(Collections.singleton(tp1));    EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.emptySet());    EasyMock.replay(t1);    addAndInitTask();    EasyMock.verify(t1);}
f16917
0
shouldCloseTaskOnSuspendWhenRuntimeException
public void kafkatest_f16925_0()
{    mockTaskInitialization();    t1.suspend();    EasyMock.expectLastCall().andThrow(new RuntimeException("KABOOM!"));    t1.close(false, false);    EasyMock.expectLastCall();    EasyMock.replay(t1);    assertThat(suspendTask(), not(nullValue()));    assertThat(assignedTasks.previousTaskIds(), equalTo(Collections.singleton(taskId1)));    EasyMock.verify(t1);}
f16925
0
shouldCloseTaskOnSuspendIfTaskMigratedException
public void kafkatest_f16926_0()
{    mockTaskInitialization();    t1.suspend();    EasyMock.expectLastCall().andThrow(new TaskMigratedException());    t1.close(false, true);    EasyMock.expectLastCall();    EasyMock.replay(t1);    assertThat(suspendTask(), nullValue());    assertTrue(assignedTasks.previousTaskIds().isEmpty());    EasyMock.verify(t1);}
f16926
0
shouldResumeMatchingSuspendedTasks
public void kafkatest_f16927_0()
{    mockRunningTaskSuspension();    t1.resume();    EasyMock.expectLastCall();    t1.initializeTopology();    EasyMock.expectLastCall().once();    EasyMock.replay(t1);    assertThat(suspendTask(), nullValue());    assertTrue(assignedTasks.maybeResumeSuspendedTask(taskId1, Collections.singleton(tp1)));    assertThat(assignedTasks.runningTaskIds(), equalTo(Collections.singleton(taskId1)));    EasyMock.verify(t1);}
f16927
0
shouldCloseTaskOnProcessesIfTaskMigratedException
public void kafkatest_f16935_0()
{    mockTaskInitialization();    EasyMock.expect(t1.isProcessable(0L)).andReturn(true);    t1.process();    EasyMock.expectLastCall().andThrow(new TaskMigratedException());    t1.close(false, true);    EasyMock.expectLastCall();    EasyMock.replay(t1);    addAndInitTask();    try {        assignedTasks.process(0L);        fail("Should have thrown TaskMigratedException.");    } catch (final TaskMigratedException expected) {    /* ignore */    }    assertThat(assignedTasks.runningTaskIds(), equalTo(Collections.EMPTY_SET));    EasyMock.verify(t1);}
f16935
0
shouldNotProcessUnprocessableTasks
public void kafkatest_f16936_0()
{    mockTaskInitialization();    EasyMock.expect(t1.isProcessable(0L)).andReturn(false);    EasyMock.replay(t1);    addAndInitTask();    assertThat(assignedTasks.process(0L), equalTo(0));    EasyMock.verify(t1);}
f16936
0
shouldAlwaysProcessProcessableTasks
public void kafkatest_f16937_0()
{    mockTaskInitialization();    EasyMock.expect(t1.isProcessable(0L)).andReturn(true);    EasyMock.expect(t1.process()).andReturn(true).once();    EasyMock.replay(t1);    addAndInitTask();    assertThat(assignedTasks.process(0L), equalTo(1));    EasyMock.verify(t1);}
f16937
0
mockRunningTaskSuspension
private void kafkatest_f16945_0()
{    EasyMock.expect(t1.initializeStateStores()).andReturn(true);    t1.initializeTopology();    EasyMock.expectLastCall().once();    EasyMock.expect(t1.hasStateStores()).andReturn(false).anyTimes();    EasyMock.expect(t1.partitions()).andReturn(Collections.singleton(tp1)).anyTimes();    EasyMock.expect(t1.changelogPartitions()).andReturn(Collections.emptyList()).anyTimes();    t1.suspend();    EasyMock.expectLastCall();}
f16945
0
shouldUseLatestSupportedVersionByDefault
public void kafkatest_f16946_0()
{    final AssignmentInfo info = new AssignmentInfo(activeTasks, standbyTasks, globalAssignment);    assertEquals(LATEST_SUPPORTED_VERSION, info.version());}
f16946
0
shouldThrowForUnknownVersion1
public void kafkatest_f16947_0()
{    new AssignmentInfo(0, activeTasks, standbyTasks, globalAssignment, 0);}
f16947
0
shouldHaveReachedCapacityWhenAssignedTasksGreaterThanOrEqualToCapacity
public void kafkatest_f16955_0()
{    client.assign(new TaskId(0, 1), true);    assertTrue(client.reachedCapacity());}
f16955
0
shouldAddActiveTasksToBothAssignedAndActive
public void kafkatest_f16956_0()
{    final TaskId tid = new TaskId(0, 1);    client.assign(tid, true);    assertThat(client.activeTasks(), equalTo(Collections.singleton(tid)));    assertThat(client.assignedTasks(), equalTo(Collections.singleton(tid)));    assertThat(client.assignedTaskCount(), equalTo(1));    assertThat(client.standbyTasks().size(), equalTo(0));}
f16956
0
shouldAddStandbyTasksToBothStandbyAndActive
public void kafkatest_f16957_0()
{    final TaskId tid = new TaskId(0, 1);    client.assign(tid, false);    assertThat(client.assignedTasks(), equalTo(Collections.singleton(tid)));    assertThat(client.standbyTasks(), equalTo(Collections.singleton(tid)));    assertThat(client.assignedTaskCount(), equalTo(1));    assertThat(client.activeTasks().size(), equalTo(0));}
f16957
0
shouldUseMultiplesOfCapacityToDetermineClientWithMoreAvailableCapacity
public void kafkatest_f16965_0()
{    final ClientState c2 = new ClientState(2);    for (int i = 0; i < 7; i++) {        c2.assign(new TaskId(0, i), true);    }    for (int i = 7; i < 11; i++) {        client.assign(new TaskId(0, i), true);    }    assertTrue(c2.hasMoreAvailableCapacityThan(client));}
f16965
0
shouldHaveMoreAvailableCapacityWhenCapacityIsTheSameButAssignedTasksIsLess
public void kafkatest_f16966_0()
{    final ClientState c1 = new ClientState(3);    final ClientState c2 = new ClientState(3);    for (int i = 0; i < 4; i++) {        c1.assign(new TaskId(0, i), true);        c2.assign(new TaskId(0, i), true);    }    c2.assign(new TaskId(0, 5), true);    assertTrue(c1.hasMoreAvailableCapacityThan(c2));}
f16966
0
shouldThrowIllegalStateExceptionIfCapacityOfThisClientStateIsZero
public void kafkatest_f16967_0()
{    final ClientState c1 = new ClientState(0);    c1.hasMoreAvailableCapacityThan(new ClientState(1));}
f16967
0
shouldMigrateActiveTasksToNewProcessWithoutChangingAllAssignments
public void kafkatest_f16975_0()
{    createClientWithPreviousActiveTasks(p1, 1, task00, task02);    createClientWithPreviousActiveTasks(p2, 1, task01);    createClient(p3, 1);    final StickyTaskAssignor taskAssignor = createTaskAssignor(task00, task01, task02);    taskAssignor.assign(0);    assertThat(clients.get(p2).activeTasks(), equalTo(Collections.singleton(task01)));    assertThat(clients.get(p1).activeTasks().size(), equalTo(1));    assertThat(clients.get(p3).activeTasks().size(), equalTo(1));    assertThat(allActiveTasks(), equalTo(Arrays.asList(task00, task01, task02)));}
f16975
0
shouldAssignBasedOnCapacity
public void kafkatest_f16976_0()
{    createClient(p1, 1);    createClient(p2, 2);    final StickyTaskAssignor taskAssignor = createTaskAssignor(task00, task01, task02);    taskAssignor.assign(0);    assertThat(clients.get(p1).activeTasks().size(), equalTo(1));    assertThat(clients.get(p2).activeTasks().size(), equalTo(2));}
f16976
0
shouldAssignTasksEvenlyWithUnequalTopicGroupSizes
public void kafkatest_f16977_0()
{    createClientWithPreviousActiveTasks(p1, 1, task00, task01, task02, task03, task04, task05, task10);    createClient(p2, 1);    final StickyTaskAssignor taskAssignor = createTaskAssignor(task10, task00, task01, task02, task03, task04, task05);    final Set<TaskId> expectedClientITasks = new HashSet<>(Arrays.asList(task00, task01, task10, task05));    final Set<TaskId> expectedClientIITasks = new HashSet<>(Arrays.asList(task02, task03, task04));    taskAssignor.assign(0);    assertThat(clients.get(p1).activeTasks(), equalTo(expectedClientITasks));    assertThat(clients.get(p2).activeTasks(), equalTo(expectedClientIITasks));}
f16977
0
shouldAssignAtLeastOneTaskToEachClientIfPossible
public void kafkatest_f16985_0()
{    createClient(p1, 3);    createClient(p2, 1);    createClient(p3, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task01, task02);    taskAssignor.assign(0);    assertThat(clients.get(p1).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p2).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p3).assignedTaskCount(), equalTo(1));}
f16985
0
shouldAssignEachActiveTaskToOneClientWhenMoreClientsThanTasks
public void kafkatest_f16986_0()
{    createClient(p1, 1);    createClient(p2, 1);    createClient(p3, 1);    createClient(p4, 1);    createClient(5, 1);    createClient(6, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task01, task02);    taskAssignor.assign(0);    assertThat(allActiveTasks(), equalTo(Arrays.asList(task00, task01, task02)));}
f16986
0
shouldBalanceActiveAndStandbyTasksAcrossAvailableClients
public void kafkatest_f16987_0()
{    createClient(p1, 1);    createClient(p2, 1);    createClient(p3, 1);    createClient(p4, 1);    createClient(5, 1);    createClient(6, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task01, task02);    taskAssignor.assign(1);    for (final ClientState clientState : clients.values()) {        assertThat(clientState.assignedTaskCount(), equalTo(1));    }}
f16987
0
shouldRebalanceTasksToClientsBasedOnCapacity
public void kafkatest_f16995_0()
{    createClientWithPreviousActiveTasks(p2, 1, task00, task03, task02);    createClient(p3, 2);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task02, task03);    taskAssignor.assign(0);    assertThat(clients.get(p2).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p3).assignedTaskCount(), equalTo(2));}
f16995
0
shouldMoveMinimalNumberOfTasksWhenPreviouslyAboveCapacityAndNewClientAdded
public void kafkatest_f16996_0()
{    final Set<TaskId> p1PrevTasks = Utils.mkSet(task00, task02);    final Set<TaskId> p2PrevTasks = Utils.mkSet(task01, task03);    createClientWithPreviousActiveTasks(p1, 1, task00, task02);    createClientWithPreviousActiveTasks(p2, 1, task01, task03);    createClientWithPreviousActiveTasks(p3, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task02, task01, task03);    taskAssignor.assign(0);    final Set<TaskId> p3ActiveTasks = clients.get(p3).activeTasks();    assertThat(p3ActiveTasks.size(), equalTo(1));    if (p1PrevTasks.removeAll(p3ActiveTasks)) {        assertThat(clients.get(p2).activeTasks(), equalTo(p2PrevTasks));    } else {        assertThat(clients.get(p1).activeTasks(), equalTo(p1PrevTasks));    }}
f16996
0
shouldNotMoveAnyTasksWhenNewTasksAdded
public void kafkatest_f16997_0()
{    createClientWithPreviousActiveTasks(p1, 1, task00, task01);    createClientWithPreviousActiveTasks(p2, 1, task02, task03);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task03, task01, task04, task02, task00, task05);    taskAssignor.assign(0);    assertThat(clients.get(p1).activeTasks(), hasItems(task00, task01));    assertThat(clients.get(p2).activeTasks(), hasItems(task02, task03));}
f16997
0
allActiveTasks
private List<TaskId> kafkatest_f17005_0()
{    final List<TaskId> allActive = new ArrayList<>();    for (final ClientState client : clients.values()) {        allActive.addAll(client.activeTasks());    }    Collections.sort(allActive);    return allActive;}
f17005
0
allStandbyTasks
private List<TaskId> kafkatest_f17006_0()
{    final List<TaskId> tasks = new ArrayList<>();    for (final ClientState client : clients.values()) {        tasks.addAll(client.standbyTasks());    }    Collections.sort(tasks);    return tasks;}
f17006
0
createClient
private ClientState kafkatest_f17007_0(final Integer processId, final int capacity)
{    return createClientWithPreviousActiveTasks(processId, capacity);}
f17007
0
shouldEncodeAndDecodeVersion1
public void kafkatest_f17015_0()
{    final SubscriptionInfo info = new SubscriptionInfo(1, processId, activeTasks, standbyTasks, IGNORED_USER_ENDPOINT);    final SubscriptionInfo expectedInfo = new SubscriptionInfo(1, UNKNOWN, processId, activeTasks, standbyTasks, null);    assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));}
f17015
0
shouldEncodeAndDecodeVersion2
public void kafkatest_f17016_0()
{    final SubscriptionInfo info = new SubscriptionInfo(2, processId, activeTasks, standbyTasks, "localhost:80");    final SubscriptionInfo expectedInfo = new SubscriptionInfo(2, UNKNOWN, processId, activeTasks, standbyTasks, "localhost:80");    assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));}
f17016
0
shouldEncodeAndDecodeVersion3
public void kafkatest_f17017_0()
{    final SubscriptionInfo info = new SubscriptionInfo(3, processId, activeTasks, standbyTasks, "localhost:80");    final SubscriptionInfo expectedInfo = new SubscriptionInfo(3, LATEST_SUPPORTED_VERSION, processId, activeTasks, standbyTasks, "localhost:80");    assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));}
f17017
0
shouldNotifyRestoreProgressNonBatchMode
public void kafkatest_f17025_0()
{    setUpCompositeRestoreListener(stateRestoreCallback);    compositeRestoreListener.onBatchRestored(topicPartition, storeName, endOffset, numberRestored);    assertStateRestoreListenerOnBatchCompleteNotification(stateRestoreCallback);    assertStateRestoreListenerOnBatchCompleteNotification(reportingStoreListener);}
f17025
0
shouldNotifyRestoreProgressBatchMode
public void kafkatest_f17026_0()
{    setUpCompositeRestoreListener(batchingStateRestoreCallback);    compositeRestoreListener.onBatchRestored(topicPartition, storeName, endOffset, numberRestored);    assertStateRestoreListenerOnBatchCompleteNotification(batchingStateRestoreCallback);    assertStateRestoreListenerOnBatchCompleteNotification(reportingStoreListener);}
f17026
0
shouldNotifyRestoreEndInNonBatchMode
public void kafkatest_f17027_0()
{    setUpCompositeRestoreListener(stateRestoreCallback);    compositeRestoreListener.onRestoreEnd(topicPartition, storeName, numberRestored);    assertStateRestoreOnEndNotification(stateRestoreCallback);    assertStateRestoreOnEndNotification(reportingStoreListener);}
f17027
0
assertStateRestoreOnEndNotification
private void kafkatest_f17035_0(final MockStateRestoreListener restoreListener)
{    assertTrue(restoreListener.storeNameCalledStates.containsKey(RESTORE_END));    assertThat(restoreListener.restoreTopicPartition, is(topicPartition));    assertThat(restoreListener.totalNumRestored, is(numberRestored));}
f17035
0
setUpCompositeRestoreListener
private void kafkatest_f17036_0(final StateRestoreCallback stateRestoreCallback)
{    compositeRestoreListener = new CompositeRestoreListener(stateRestoreCallback);    compositeRestoreListener.setUserRestoreListener(reportingStoreListener);}
f17036
0
restore
public void kafkatest_f17037_0(final byte[] key, final byte[] value)
{    restoredKey = key;    restoredValue = value;}
f17037
0
createTopicConfig
private InternalTopicConfig kafkatest_f17045_0(final String repartitionTopic, final int partitions)
{    final InternalTopicConfig repartitionTopicConfig = new RepartitionTopicConfig(repartitionTopic, Collections.emptyMap());    repartitionTopicConfig.setNumberOfPartitions(partitions);    return repartitionTopicConfig;}
f17045
0
setUp
public void kafkatest_f17046_0()
{    context = new ForwardingDisabledProcessorContext(delegate);}
f17046
0
shouldThrowOnForward
public void kafkatest_f17047_0()
{    context.forward("key", "value");}
f17047
0
shouldNotSupportForwardingViaChildIndex
public void kafkatest_f17055_0()
{    globalContext.forward(null, null, 0);}
f17055
0
shouldNotSupportForwardingViaChildName
public void kafkatest_f17056_0()
{    globalContext.forward(null, null, "processorName");}
f17056
0
shouldNotFailOnNoOpCommit
public void kafkatest_f17057_0()
{    globalContext.commit();}
f17057
0
shouldNotAllowCloseForKeyValueStore
public void kafkatest_f17065_0()
{    final StateStore store = globalContext.getStateStore(GLOBAL_KEY_VALUE_STORE_NAME);    try {        store.close();        fail("Should have thrown UnsupportedOperationException.");    } catch (final UnsupportedOperationException expected) {    }}
f17065
0
shouldNotAllowCloseForTimestampedKeyValueStore
public void kafkatest_f17066_0()
{    final StateStore store = globalContext.getStateStore(GLOBAL_TIMESTAMPED_KEY_VALUE_STORE_NAME);    try {        store.close();        fail("Should have thrown UnsupportedOperationException.");    } catch (final UnsupportedOperationException expected) {    }}
f17066
0
shouldNotAllowCloseForWindowStore
public void kafkatest_f17067_0()
{    final StateStore store = globalContext.getStateStore(GLOBAL_WINDOW_STORE_NAME);    try {        store.close();        fail("Should have thrown UnsupportedOperationException.");    } catch (final UnsupportedOperationException expected) {    }}
f17067
0
shouldReadCheckpointOffsets
public void kafkatest_f17075_0() throws IOException
{    final Map<TopicPartition, Long> expected = writeCheckpoint();    stateManager.initialize();    final Map<TopicPartition, Long> offsets = stateManager.checkpointed();    assertEquals(expected, offsets);}
f17075
0
shouldNotDeleteCheckpointFileAfterLoaded
public void kafkatest_f17076_0() throws IOException
{    writeCheckpoint();    stateManager.initialize();    assertTrue(checkpointFile.exists());}
f17076
0
shouldThrowStreamsExceptionIfFailedToReadCheckpointedOffsets
public void kafkatest_f17077_0() throws IOException
{    writeCorruptCheckpoint();    stateManager.initialize();}
f17077
0
shouldConvertValuesIfStoreImplementsTimestampedBytesStore
public void kafkatest_f17085_0()
{    initializeConsumer(1, 0, t2);    stateManager.initialize();    stateManager.register(store2, stateRestoreCallback);    final KeyValue<byte[], byte[]> restoredRecord = stateRestoreCallback.restored.get(0);    assertEquals(3, restoredRecord.key.length);    assertEquals(13, restoredRecord.value.length);}
f17085
0
shouldConvertValuesIfInnerStoreImplementsTimestampedBytesStore
public void kafkatest_f17086_0()
{    initializeConsumer(1, 0, t2);    stateManager.initialize();    stateManager.register(new WrappedStateStore<NoOpReadOnlyStore<Object, Object>, Object, Object>(store2) {    }, stateRestoreCallback);    final KeyValue<byte[], byte[]> restoredRecord = stateRestoreCallback.restored.get(0);    assertEquals(3, restoredRecord.key.length);    assertEquals(13, restoredRecord.value.length);}
f17086
0
shouldRestoreRecordsUpToHighwatermark
public void kafkatest_f17087_0()
{    initializeConsumer(2, 0, t1);    stateManager.initialize();    stateManager.register(store1, stateRestoreCallback);    assertEquals(2, stateRestoreCallback.restored.size());}
f17087
0
shouldCloseStateStores
public void kafkatest_f17095_0() throws IOException
{    stateManager.initialize();    // register the stores    initializeConsumer(1, 0, t1);    stateManager.register(store1, stateRestoreCallback);    initializeConsumer(1, 0, t2);    stateManager.register(store2, stateRestoreCallback);    stateManager.close(true);    assertFalse(store1.isOpen());    assertFalse(store2.isOpen());}
f17095
0
shouldThrowProcessorStateStoreExceptionIfStoreCloseFailed
public void kafkatest_f17096_0() throws IOException
{    stateManager.initialize();    initializeConsumer(1, 0, t1);    stateManager.register(new NoOpReadOnlyStore(store1.name()) {        @Override        public void close() {            throw new RuntimeException("KABOOM!");        }    }, stateRestoreCallback);    stateManager.close(true);}
f17096
0
close
public void kafkatest_f17097_0()
{    throw new RuntimeException("KABOOM!");}
f17097
0
shouldCheckpointOffsets
public void kafkatest_f17105_0() throws IOException
{    final Map<TopicPartition, Long> offsets = Collections.singletonMap(t1, 25L);    stateManager.initialize();    stateManager.checkpoint(offsets);    final Map<TopicPartition, Long> result = readOffsetsCheckpoint();    assertThat(result, equalTo(offsets));    assertThat(stateManager.checkpointed(), equalTo(offsets));}
f17105
0
shouldNotRemoveOffsetsOfUnUpdatedTablesDuringCheckpoint
public void kafkatest_f17106_0()
{    stateManager.initialize();    initializeConsumer(10, 0, t1);    stateManager.register(store1, stateRestoreCallback);    initializeConsumer(20, 0, t2);    stateManager.register(store2, stateRestoreCallback);    final Map<TopicPartition, Long> initialCheckpoint = stateManager.checkpointed();    stateManager.checkpoint(Collections.singletonMap(t1, 101L));    final Map<TopicPartition, Long> updatedCheckpoint = stateManager.checkpointed();    assertThat(updatedCheckpoint.get(t2), equalTo(initialCheckpoint.get(t2)));    assertThat(updatedCheckpoint.get(t1), equalTo(101L));}
f17106
0
shouldSkipNullKeysWhenRestoring
public void kafkatest_f17107_0()
{    final HashMap<TopicPartition, Long> startOffsets = new HashMap<>();    startOffsets.put(t1, 1L);    final HashMap<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(t1, 3L);    consumer.updatePartitions(t1.topic(), Collections.singletonList(new PartitionInfo(t1.topic(), t1.partition(), null, null, null)));    consumer.assign(Collections.singletonList(t1));    consumer.updateEndOffsets(endOffsets);    consumer.updateBeginningOffsets(startOffsets);    consumer.addRecord(new ConsumerRecord<>(t1.topic(), t1.partition(), 1, null, "null".getBytes()));    final byte[] expectedKey = "key".getBytes();    final byte[] expectedValue = "value".getBytes();    consumer.addRecord(new ConsumerRecord<>(t1.topic(), t1.partition(), 2, expectedKey, expectedValue));    stateManager.initialize();    stateManager.register(store1, stateRestoreCallback);    final KeyValue<byte[], byte[]> restoredKv = stateRestoreCallback.restored.get(0);    assertThat(stateRestoreCallback.restored, equalTo(Collections.singletonList(KeyValue.pair(restoredKv.key, restoredKv.value))));}
f17107
0
shouldRetryWhenPartitionsForThrowsTimeoutException
public void kafkatest_f17115_0()
{    final int retries = 2;    final AtomicInteger numberOfCalls = new AtomicInteger(0);    consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {        @Override        public synchronized List<PartitionInfo> partitionsFor(final String topic) {            numberOfCalls.incrementAndGet();            throw new TimeoutException();        }    };    streamsConfig = new StreamsConfig(new Properties() {        {            put(StreamsConfig.APPLICATION_ID_CONFIG, "appId");            put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummy:1234");            put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());            put(StreamsConfig.RETRIES_CONFIG, retries);        }    });    try {        new GlobalStateManagerImpl(new LogContext("mock"), topology, consumer, stateDirectory, stateRestoreListener, streamsConfig);    } catch (final StreamsException expected) {        assertEquals(numberOfCalls.get(), retries);    }}
f17115
0
partitionsFor
public synchronized List<PartitionInfo> kafkatest_f17116_0(final String topic)
{    numberOfCalls.incrementAndGet();    throw new TimeoutException();}
f17116
0
shouldDeleteAndRecreateStoreDirectoryOnReinitialize
public void kafkatest_f17117_0() throws IOException
{    final File storeDirectory1 = new File(stateDirectory.globalStateDir().getAbsolutePath() + File.separator + "rocksdb" + File.separator + storeName1);    final File storeDirectory2 = new File(stateDirectory.globalStateDir().getAbsolutePath() + File.separator + "rocksdb" + File.separator + storeName2);    final File storeDirectory3 = new File(stateDirectory.globalStateDir().getAbsolutePath() + File.separator + storeName3);    final File storeDirectory4 = new File(stateDirectory.globalStateDir().getAbsolutePath() + File.separator + storeName4);    final File testFile1 = new File(storeDirectory1.getAbsolutePath() + File.separator + "testFile");    final File testFile2 = new File(storeDirectory2.getAbsolutePath() + File.separator + "testFile");    final File testFile3 = new File(storeDirectory3.getAbsolutePath() + File.separator + "testFile");    final File testFile4 = new File(storeDirectory4.getAbsolutePath() + File.separator + "testFile");    consumer.updatePartitions(t1.topic(), Collections.singletonList(new PartitionInfo(t1.topic(), t1.partition(), null, null, null)));    consumer.updatePartitions(t2.topic(), Collections.singletonList(new PartitionInfo(t2.topic(), t2.partition(), null, null, null)));    consumer.updatePartitions(t3.topic(), Collections.singletonList(new PartitionInfo(t3.topic(), t3.partition(), null, null, null)));    consumer.updatePartitions(t4.topic(), Collections.singletonList(new PartitionInfo(t4.topic(), t4.partition(), null, null, null)));    consumer.updateBeginningOffsets(new HashMap<TopicPartition, Long>() {        {            put(t1, 0L);            put(t2, 0L);            put(t3, 0L);            put(t4, 0L);        }    });    consumer.updateEndOffsets(new HashMap<TopicPartition, Long>() {        {            put(t1, 0L);            put(t2, 0L);            put(t3, 0L);            put(t4, 0L);        }    });    stateManager.initialize();    stateManager.register(store1, stateRestoreCallback);    stateManager.register(store2, stateRestoreCallback);    stateManager.register(store3, stateRestoreCallback);    stateManager.register(store4, stateRestoreCallback);    testFile1.createNewFile();    assertTrue(testFile1.exists());    testFile2.createNewFile();    assertTrue(testFile2.exists());    testFile3.createNewFile();    assertTrue(testFile3.exists());    testFile4.createNewFile();    assertTrue(testFile4.exists());    // only delete and recreate store 1 and 3 -- 2 and 4 must be untouched    stateManager.reinitializeStateStoresForPartitions(asList(t1, t3), processorContext);    assertFalse(testFile1.exists());    assertTrue(testFile2.exists());    assertFalse(testFile3.exists());    assertTrue(testFile4.exists());}
f17117
0
shouldInitializeProcessorTopology
public void kafkatest_f17125_0()
{    globalStateTask.initialize();    assertTrue(sourceOne.initialized);    assertTrue(sourceTwo.initialized);    assertTrue(processorOne.initialized);    assertTrue(processorTwo.initialized);}
f17125
0
shouldProcessRecordsForTopic
public void kafkatest_f17126_0()
{    globalStateTask.initialize();    globalStateTask.update(new ConsumerRecord<>(topic1, 1, 1, "foo".getBytes(), "bar".getBytes()));    assertEquals(1, sourceOne.numReceived);    assertEquals(0, sourceTwo.numReceived);}
f17126
0
shouldProcessRecordsForOtherTopic
public void kafkatest_f17127_0()
{    final byte[] integerBytes = new IntegerSerializer().serialize("foo", 1);    globalStateTask.initialize();    globalStateTask.update(new ConsumerRecord<>(topic2, 1, 1, integerBytes, integerBytes));    assertEquals(1, sourceTwo.numReceived);    assertEquals(0, sourceOne.numReceived);}
f17127
0
before
public void kafkatest_f17135_0()
{    final MaterializedInternal<Object, Object, KeyValueStore<Bytes, byte[]>> materialized = new MaterializedInternal<>(Materialized.with(null, null), new InternalNameProvider() {        @Override        public String newProcessorName(final String prefix) {            return "processorName";        }        @Override        public String newStoreName(final String prefix) {            return GLOBAL_STORE_NAME;        }    }, "store-");    builder.addGlobalStore(new TimestampedKeyValueStoreMaterializer<>(materialized).materialize().withLoggingDisabled(), "sourceName", null, null, null, GLOBAL_STORE_TOPIC_NAME, "processorName", new KTableSource<>(GLOBAL_STORE_NAME, GLOBAL_STORE_NAME));    final HashMap<String, Object> properties = new HashMap<>();    properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "blah");    properties.put(StreamsConfig.APPLICATION_ID_CONFIG, "blah");    properties.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());    config = new StreamsConfig(properties);    globalStreamThread = new GlobalStreamThread(builder.rewriteTopology(config).buildGlobalStateTopology(), config, mockConsumer, new StateDirectory(config, time, true), 0, new Metrics(), new MockTime(), "clientId", stateRestoreListener);}
f17135
0
newProcessorName
public String kafkatest_f17136_0(final String prefix)
{    return "processorName";}
f17136
0
newStoreName
public String kafkatest_f17137_0(final String prefix)
{    return GLOBAL_STORE_NAME;}
f17137
0
shouldStayDeadAfterTwoCloses
public void kafkatest_f17145_0() throws Exception
{    initializeConsumer();    globalStreamThread.start();    globalStreamThread.shutdown();    globalStreamThread.join();    globalStreamThread.shutdown();    assertEquals(GlobalStreamThread.State.DEAD, globalStreamThread.state());}
f17145
0
shouldTransitionToRunningOnStart
public void kafkatest_f17146_0() throws Exception
{    initializeConsumer();    globalStreamThread.start();    TestUtils.waitForCondition(() -> globalStreamThread.state() == RUNNING, 10 * 1000, "Thread never started.");    globalStreamThread.shutdown();}
f17146
0
shouldDieOnInvalidOffsetException
public void kafkatest_f17147_0() throws Exception
{    initializeConsumer();    globalStreamThread.start();    TestUtils.waitForCondition(() -> globalStreamThread.state() == RUNNING, 10 * 1000, "Thread never started.");    mockConsumer.updateEndOffsets(Collections.singletonMap(topicPartition, 1L));    mockConsumer.addRecord(new ConsumerRecord<>(GLOBAL_STORE_TOPIC_NAME, 0, 0L, "K1".getBytes(), "V1".getBytes()));    TestUtils.waitForCondition(() -> mockConsumer.position(topicPartition) == 1L, 10 * 1000, "Input record never consumed");    mockConsumer.setPollException(new InvalidOffsetException("Try Again!") {        @Override        public Set<TopicPartition> partitions() {            return Collections.singleton(topicPartition);        }    });    // feed first record for recovery    mockConsumer.addRecord(new ConsumerRecord<>(GLOBAL_STORE_TOPIC_NAME, 0, 0L, "K1".getBytes(), "V1".getBytes()));    TestUtils.waitForCondition(() -> globalStreamThread.state() == DEAD, 10 * 1000, "GlobalStreamThread should have died.");}
f17147
0
init
public void kafkatest_f17155_0()
{    mockAdminClient = new MockAdminClient(cluster, broker1);    internalTopicManager = new InternalTopicManager(mockAdminClient, new StreamsConfig(config));}
f17155
0
shutdown
public void kafkatest_f17156_0()
{    mockAdminClient.close();}
f17156
0
shouldReturnCorrectPartitionCounts
public void kafkatest_f17157_0()
{    mockAdminClient.addTopic(false, topic, Collections.singletonList(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList())), null);    assertEquals(Collections.singletonMap(topic, 1), internalTopicManager.getNumPartitions(Collections.singleton(topic)));}
f17157
0
shouldAddSourceWithOffsetReset
public void kafkatest_f17165_0()
{    final String earliestTopic = "earliestTopic";    final String latestTopic = "latestTopic";    builder.addSource(Topology.AutoOffsetReset.EARLIEST, "source", null, null, null, earliestTopic);    builder.addSource(Topology.AutoOffsetReset.LATEST, "source2", null, null, null, latestTopic);    assertTrue(builder.earliestResetTopicsPattern().matcher(earliestTopic).matches());    assertTrue(builder.latestResetTopicsPattern().matcher(latestTopic).matches());}
f17165
0
shouldAddSourcePatternWithOffsetReset
public void kafkatest_f17166_0()
{    final String earliestTopicPattern = "earliest.*Topic";    final String latestTopicPattern = "latest.*Topic";    builder.addSource(Topology.AutoOffsetReset.EARLIEST, "source", null, null, null, Pattern.compile(earliestTopicPattern));    builder.addSource(Topology.AutoOffsetReset.LATEST, "source2", null, null, null, Pattern.compile(latestTopicPattern));    assertTrue(builder.earliestResetTopicsPattern().matcher("earliestTestTopic").matches());    assertTrue(builder.latestResetTopicsPattern().matcher("latestTestTopic").matches());}
f17166
0
shouldAddSourceWithoutOffsetReset
public void kafkatest_f17167_0()
{    final Pattern expectedPattern = Pattern.compile("test-topic");    builder.addSource(null, "source", null, stringSerde.deserializer(), stringSerde.deserializer(), "test-topic");    assertEquals(expectedPattern.pattern(), builder.sourceTopicPattern().pattern());    assertEquals(builder.earliestResetTopicsPattern().pattern(), "");    assertEquals(builder.latestResetTopicsPattern().pattern(), "");}
f17167
0
testAddProcessorWithSelfParent
public void kafkatest_f17175_0()
{    builder.addProcessor("processor", new MockProcessorSupplier(), "processor");}
f17175
0
testAddProcessorWithEmptyParents
public void kafkatest_f17176_0()
{    builder.addProcessor("processor", new MockProcessorSupplier());}
f17176
0
testAddProcessorWithNullParents
public void kafkatest_f17177_0()
{    builder.addProcessor("processor", new MockProcessorSupplier(), (String) null);}
f17177
0
testSourceTopics
public void kafkatest_f17185_0()
{    builder.setApplicationId("X");    builder.addSource(null, "source-1", null, null, null, "topic-1");    builder.addSource(null, "source-2", null, null, null, "topic-2");    builder.addSource(null, "source-3", null, null, null, "topic-3");    builder.addInternalTopic("topic-3");    final Pattern expectedPattern = Pattern.compile("X-topic-3|topic-1|topic-2");    assertEquals(expectedPattern.pattern(), builder.sourceTopicPattern().pattern());}
f17185
0
testSourceTopicsWithGlobalTopics
public void kafkatest_f17186_0()
{    builder.setApplicationId("X");    builder.addSource(null, "source-1", null, null, null, "topic-1");    builder.addSource(null, "source-2", null, null, null, "topic-2");    builder.addGlobalStore(new MockKeyValueStoreBuilder("global-store", false).withLoggingDisabled(), "globalSource", null, null, null, "globalTopic", "global-processor", new MockProcessorSupplier());    final Pattern expectedPattern = Pattern.compile("topic-1|topic-2");    assertThat(builder.sourceTopicPattern().pattern(), equalTo(expectedPattern.pattern()));}
f17186
0
testPatternSourceTopic
public void kafkatest_f17187_0()
{    final Pattern expectedPattern = Pattern.compile("topic-\\d");    builder.addSource(null, "source-1", null, null, null, expectedPattern);    assertEquals(expectedPattern.pattern(), builder.sourceTopicPattern().pattern());}
f17187
0
testAddStateStoreWithDuplicates
public void kafkatest_f17195_0()
{    builder.addStateStore(storeBuilder);    try {        builder.addStateStore(storeBuilder);        fail("Should throw TopologyException with store name conflict");    } catch (final TopologyException expected) {    /* ok */    }}
f17195
0
testAddStateStore
public void kafkatest_f17196_0()
{    builder.addStateStore(storeBuilder);    builder.setApplicationId("X");    builder.addSource(null, "source-1", null, null, null, "topic-1");    builder.addProcessor("processor-1", new MockProcessorSupplier(), "source-1");    assertEquals(0, builder.build(null).stateStores().size());    builder.connectProcessorAndStateStores("processor-1", storeBuilder.name());    final List<StateStore> suppliers = builder.build(null).stateStores();    assertEquals(1, suppliers.size());    assertEquals(storeBuilder.name(), suppliers.get(0).name());}
f17196
0
testTopicGroups
public void kafkatest_f17197_0()
{    builder.setApplicationId("X");    builder.addInternalTopic("topic-1x");    builder.addSource(null, "source-1", null, null, null, "topic-1", "topic-1x");    builder.addSource(null, "source-2", null, null, null, "topic-2");    builder.addSource(null, "source-3", null, null, null, "topic-3");    builder.addSource(null, "source-4", null, null, null, "topic-4");    builder.addSource(null, "source-5", null, null, null, "topic-5");    builder.addProcessor("processor-1", new MockProcessorSupplier(), "source-1");    builder.addProcessor("processor-2", new MockProcessorSupplier(), "source-2", "processor-1");    builder.copartitionSources(asList("source-1", "source-2"));    builder.addProcessor("processor-3", new MockProcessorSupplier(), "source-3", "source-4");    final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();    final Map<Integer, InternalTopologyBuilder.TopicsInfo> expectedTopicGroups = new HashMap<>();    expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet("topic-1", "X-topic-1x", "topic-2"), Collections.emptyMap(), Collections.emptyMap()));    expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet("topic-3", "topic-4"), Collections.emptyMap(), Collections.emptyMap()));    expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet("topic-5"), Collections.emptyMap(), Collections.emptyMap()));    assertEquals(3, topicGroups.size());    assertEquals(expectedTopicGroups, topicGroups);    final Collection<Set<String>> copartitionGroups = builder.copartitionGroups();    assertEquals(mkSet(mkSet("topic-1", "X-topic-1x", "topic-2")), new HashSet<>(copartitionGroups));}
f17197
0
shouldNotAllowNullProcessorSupplier
public void kafkatest_f17205_0()
{    builder.addProcessor("name", null);}
f17205
0
shouldNotAllowNullNameWhenAddingSource
public void kafkatest_f17206_0()
{    builder.addSource(null, null, null, null, null, Pattern.compile(".*"));}
f17206
0
shouldNotAllowNullProcessorNameWhenConnectingProcessorAndStateStores
public void kafkatest_f17207_0()
{    builder.connectProcessorAndStateStores(null, "store");}
f17207
0
shouldCorrectlyMapStateStoreToInternalTopics
public void kafkatest_f17215_0()
{    builder.setApplicationId("appId");    builder.addInternalTopic("internal-topic");    builder.addSource(null, "source", null, null, null, "internal-topic");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    builder.addStateStore(storeBuilder, "processor");    final Map<String, List<String>> stateStoreNameToSourceTopic = builder.stateStoreNameToSourceTopics();    assertEquals(1, stateStoreNameToSourceTopic.size());    assertEquals(Collections.singletonList("appId-internal-topic"), stateStoreNameToSourceTopic.get("store"));}
f17215
0
shouldAddInternalTopicConfigForWindowStores
public void kafkatest_f17216_0()
{    builder.setApplicationId("appId");    builder.addSource(null, "source", null, null, null, "topic");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    builder.addStateStore(Stores.windowStoreBuilder(Stores.persistentWindowStore("store1", ofSeconds(30L), ofSeconds(10L), false), Serdes.String(), Serdes.String()), "processor");    builder.addStateStore(Stores.sessionStoreBuilder(Stores.persistentSessionStore("store2", ofSeconds(30)), Serdes.String(), Serdes.String()), "processor");    final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();    final InternalTopologyBuilder.TopicsInfo topicsInfo = topicGroups.values().iterator().next();    final InternalTopicConfig topicConfig1 = topicsInfo.stateChangelogTopics.get("appId-store1-changelog");    final Map<String, String> properties1 = topicConfig1.getProperties(Collections.emptyMap(), 10000);    assertEquals(2, properties1.size());    assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT + "," + TopicConfig.CLEANUP_POLICY_DELETE, properties1.get(TopicConfig.CLEANUP_POLICY_CONFIG));    assertEquals("40000", properties1.get(TopicConfig.RETENTION_MS_CONFIG));    assertEquals("appId-store1-changelog", topicConfig1.name());    assertTrue(topicConfig1 instanceof WindowedChangelogTopicConfig);    final InternalTopicConfig topicConfig2 = topicsInfo.stateChangelogTopics.get("appId-store2-changelog");    final Map<String, String> properties2 = topicConfig2.getProperties(Collections.emptyMap(), 10000);    assertEquals(2, properties2.size());    assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT + "," + TopicConfig.CLEANUP_POLICY_DELETE, properties2.get(TopicConfig.CLEANUP_POLICY_CONFIG));    assertEquals("40000", properties2.get(TopicConfig.RETENTION_MS_CONFIG));    assertEquals("appId-store2-changelog", topicConfig2.name());    assertTrue(topicConfig2 instanceof WindowedChangelogTopicConfig);}
f17216
0
shouldAddInternalTopicConfigForNonWindowStores
public void kafkatest_f17217_0()
{    builder.setApplicationId("appId");    builder.addSource(null, "source", null, null, null, "topic");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    builder.addStateStore(storeBuilder, "processor");    final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();    final InternalTopologyBuilder.TopicsInfo topicsInfo = topicGroups.values().iterator().next();    final InternalTopicConfig topicConfig = topicsInfo.stateChangelogTopics.get("appId-store-changelog");    final Map<String, String> properties = topicConfig.getProperties(Collections.emptyMap(), 10000);    assertEquals(1, properties.size());    assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT, properties.get(TopicConfig.CLEANUP_POLICY_CONFIG));    assertEquals("appId-store-changelog", topicConfig.name());    assertTrue(topicConfig instanceof UnwindowedChangelogTopicConfig);}
f17217
0
shouldThrowIfNameIsNull
public void kafkatest_f17225_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> new InternalTopologyBuilder.Source(null, Collections.emptySet(), null));    assertEquals("name cannot be null", e.getMessage());}
f17225
0
shouldThrowIfTopicAndPatternAreNull
public void kafkatest_f17226_0()
{    final Exception e = assertThrows(IllegalArgumentException.class, () -> new InternalTopologyBuilder.Source("name", null, null));    assertEquals("Either topics or pattern must be not-null, but both are null.", e.getMessage());}
f17226
0
shouldThrowIfBothTopicAndPatternAreNotNull
public void kafkatest_f17227_0()
{    final Exception e = assertThrows(IllegalArgumentException.class, () -> new InternalTopologyBuilder.Source("name", Collections.emptySet(), Pattern.compile("")));    assertEquals("Either topics or pattern must be null, but both are not null.", e.getMessage());}
f17227
0
testNullMetrics
public void kafkatest_f17235_0()
{    new StreamsMetricsImpl(null, "", VERSION);}
f17235
0
testRemoveNullSensor
public void kafkatest_f17236_0()
{    streamsMetrics.removeSensor(null);}
f17236
0
testRemoveSensor
public void kafkatest_f17237_0()
{    final String sensorName = "sensor1";    final String scope = "scope";    final String entity = "entity";    final String operation = "put";    final Sensor sensor1 = streamsMetrics.addSensor(sensorName, Sensor.RecordingLevel.DEBUG);    streamsMetrics.removeSensor(sensor1);    final Sensor sensor1a = streamsMetrics.addSensor(sensorName, Sensor.RecordingLevel.DEBUG, sensor1);    streamsMetrics.removeSensor(sensor1a);    final Sensor sensor2 = streamsMetrics.addLatencyAndThroughputSensor(scope, entity, operation, Sensor.RecordingLevel.DEBUG);    streamsMetrics.removeSensor(sensor2);    final Sensor sensor3 = streamsMetrics.addThroughputSensor(scope, entity, operation, Sensor.RecordingLevel.DEBUG);    streamsMetrics.removeSensor(sensor3);    assertEquals(Collections.emptyMap(), streamsMetrics.parentSensors());}
f17237
0
shouldAddAmountRate
public void kafkatest_f17245_0()
{    StreamsMetricsImpl.addRateOfSumMetricToSensor(sensor, group, tags, metricNamePrefix, description1);    final double valueToRecord1 = 18.0;    final double valueToRecord2 = 72.0;    final long defaultWindowSizeInSeconds = Duration.ofMillis(new MetricConfig().timeWindowMs()).getSeconds();    final double expectedRateMetricValue = (valueToRecord1 + valueToRecord2) / defaultWindowSizeInSeconds;    verifyMetric(metricNamePrefix + "-rate", description1, valueToRecord1, valueToRecord2, expectedRateMetricValue);    // one metric is added automatically in the constructor of Metrics    assertThat(metrics.metrics().size(), equalTo(1 + 1));}
f17245
0
shouldAddValue
public void kafkatest_f17246_0()
{    StreamsMetricsImpl.addValueMetricToSensor(sensor, group, tags, metricNamePrefix, description1);    final KafkaMetric ratioMetric = metrics.metric(new MetricName(metricNamePrefix, group, description1, tags));    assertThat(ratioMetric, is(notNullValue()));    final MetricConfig metricConfig = new MetricConfig();    final double value1 = 42.0;    sensor.record(value1);    assertThat(ratioMetric.measurable().measure(metricConfig, time.milliseconds()), equalTo(42.0));    final double value2 = 18.0;    sensor.record(value2);    assertThat(ratioMetric.measurable().measure(metricConfig, time.milliseconds()), equalTo(18.0));    // one metric is added automatically in the constructor of Metrics    assertThat(metrics.metrics().size(), equalTo(1 + 1));}
f17246
0
shouldAddAvgAndTotalMetricsToSensor
public void kafkatest_f17247_0()
{    StreamsMetricsImpl.addAvgAndSumMetricsToSensor(sensor, group, tags, metricNamePrefix, description1, description2);    final double valueToRecord1 = 18.0;    final double valueToRecord2 = 42.0;    final double expectedAvgMetricValue = (valueToRecord1 + valueToRecord2) / 2;    verifyMetric(metricNamePrefix + "-avg", description1, valueToRecord1, valueToRecord2, expectedAvgMetricValue);    // values are recorded once for each metric verification    final double expectedSumMetricValue = 2 * valueToRecord1 + 2 * valueToRecord2;    verifyMetric(metricNamePrefix + "-total", description2, valueToRecord1, valueToRecord2, expectedSumMetricValue);    // one metric is added automatically in the constructor of Metrics    assertThat(metrics.metrics().size(), equalTo(2 + 1));}
f17247
0
shouldGetProcessSensor
public void kafkatest_f17255_0()
{    final String operation = "process";    final String operationLatency = operation + StreamsMetricsImpl.LATENCY_SUFFIX;    final String totalDescription = "The total number of process calls";    final String rateDescription = "The average per-second number of process calls";    mockStatic(StreamsMetricsImpl.class);    expect(streamsMetrics.threadLevelSensor(operation, RecordingLevel.INFO)).andReturn(dummySensor);    expect(streamsMetrics.threadLevelTagMap()).andReturn(dummyTagMap);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operation, totalDescription, rateDescription);    StreamsMetricsImpl.addAvgAndMaxToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operationLatency);    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = ThreadMetrics.processSensor(streamsMetrics);    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(dummySensor));}
f17255
0
shouldGetPunctuateSensor
public void kafkatest_f17256_0()
{    final String operation = "punctuate";    final String operationLatency = operation + StreamsMetricsImpl.LATENCY_SUFFIX;    final String totalDescription = "The total number of punctuate calls";    final String rateDescription = "The average per-second number of punctuate calls";    mockStatic(StreamsMetricsImpl.class);    expect(streamsMetrics.threadLevelSensor(operation, RecordingLevel.INFO)).andReturn(dummySensor);    expect(streamsMetrics.threadLevelTagMap()).andReturn(dummyTagMap);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operation, totalDescription, rateDescription);    StreamsMetricsImpl.addAvgAndMaxToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operationLatency);    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = ThreadMetrics.punctuateSensor(streamsMetrics);    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(dummySensor));}
f17256
0
shouldGetSkipRecordSensor
public void kafkatest_f17257_0()
{    final String operation = "skipped-records";    final String totalDescription = "The total number of skipped records";    final String rateDescription = "The average per-second number of skipped records";    mockStatic(StreamsMetricsImpl.class);    expect(streamsMetrics.threadLevelSensor(operation, RecordingLevel.INFO)).andReturn(dummySensor);    expect(streamsMetrics.threadLevelTagMap()).andReturn(dummyTagMap);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operation, totalDescription, rateDescription);    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = ThreadMetrics.skipRecordSensor(streamsMetrics);    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(dummySensor));}
f17257
0
getValueSensor
private static Sensor kafkatest_f17265_0(final Metrics metrics, final MetricName metricName)
{    final Sensor lastRecordedValue = metrics.sensor(metricName.name());    lastRecordedValue.add(metricName, new Value());    return lastRecordedValue;}
f17265
0
testTimeTracking
public void kafkatest_f17266_0()
{    assertEquals(0, group.numBuffered());    // add three 3 records with timestamp 1, 3, 5 to partition-1    final List<ConsumerRecord<byte[], byte[]>> list1 = Arrays.asList(new ConsumerRecord<>("topic", 1, 1L, recordKey, recordValue), new ConsumerRecord<>("topic", 1, 3L, recordKey, recordValue), new ConsumerRecord<>("topic", 1, 5L, recordKey, recordValue));    group.addRawRecords(partition1, list1);    // add three 3 records with timestamp 2, 4, 6 to partition-2    final List<ConsumerRecord<byte[], byte[]>> list2 = Arrays.asList(new ConsumerRecord<>("topic", 2, 2L, recordKey, recordValue), new ConsumerRecord<>("topic", 2, 4L, recordKey, recordValue), new ConsumerRecord<>("topic", 2, 6L, recordKey, recordValue));    group.addRawRecords(partition2, list2);    // 1:[1, 3, 5]    // 2:[2, 4, 6]    // st: -1 since no records was being processed yet    verifyBuffered(6, 3, 3);    assertEquals(-1L, group.streamTime());    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());    StampedRecord record;    final PartitionGroup.RecordInfo info = new PartitionGroup.RecordInfo();    // get one record, now the time should be advanced    record = group.nextRecord(info);    // 1:[3, 5]    // 2:[2, 4, 6]    // st: 1    assertEquals(partition1, info.partition());    verifyTimes(record, 1L, 1L);    verifyBuffered(5, 2, 3);    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());    // get one record, now the time should be advanced    record = group.nextRecord(info);    // 1:[3, 5]    // 2:[4, 6]    // st: 2    assertEquals(partition2, info.partition());    verifyTimes(record, 2L, 2L);    verifyBuffered(4, 2, 2);    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());    // add 2 more records with timestamp 2, 4 to partition-1    final List<ConsumerRecord<byte[], byte[]>> list3 = Arrays.asList(new ConsumerRecord<>("topic", 1, 2L, recordKey, recordValue), new ConsumerRecord<>("topic", 1, 4L, recordKey, recordValue));    group.addRawRecords(partition1, list3);    // 1:[3, 5, 2, 4]    // 2:[4, 6]    // st: 2 (just adding records shouldn't change it)    verifyBuffered(6, 4, 2);    assertEquals(2L, group.streamTime());    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());    // get one record, time should be advanced    record = group.nextRecord(info);    // 1:[5, 2, 4]    // 2:[4, 6]    // st: 3    assertEquals(partition1, info.partition());    verifyTimes(record, 3L, 3L);    verifyBuffered(5, 3, 2);    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());    // get one record, time should be advanced    record = group.nextRecord(info);    // 1:[5, 2, 4]    // 2:[6]    // st: 4    assertEquals(partition2, info.partition());    verifyTimes(record, 4L, 4L);    verifyBuffered(4, 3, 1);    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());    // get one more record, time should be advanced    record = group.nextRecord(info);    // 1:[2, 4]    // 2:[6]    // st: 5    assertEquals(partition1, info.partition());    verifyTimes(record, 5L, 5L);    verifyBuffered(3, 2, 1);    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());    // get one more record, time should not be advanced    record = group.nextRecord(info);    // 1:[4]    // 2:[6]    // st: 5    assertEquals(partition1, info.partition());    verifyTimes(record, 2L, 5L);    verifyBuffered(2, 1, 1);    assertEquals(3.0, metrics.metric(lastLatenessValue).metricValue());    // get one more record, time should not be advanced    record = group.nextRecord(info);    // 1:[]    // 2:[6]    // st: 5    assertEquals(partition1, info.partition());    verifyTimes(record, 4L, 5L);    verifyBuffered(1, 0, 1);    assertEquals(1.0, metrics.metric(lastLatenessValue).metricValue());    // get one more record, time should be advanced    record = group.nextRecord(info);    // 1:[]    // 2:[]    // st: 6    assertEquals(partition2, info.partition());    verifyTimes(record, 6L, 6L);    verifyBuffered(0, 0, 0);    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());}
f17266
0
shouldChooseNextRecordBasedOnHeadTimestamp
public void kafkatest_f17267_0()
{    assertEquals(0, group.numBuffered());    // add three 3 records with timestamp 1, 5, 3 to partition-1    final List<ConsumerRecord<byte[], byte[]>> list1 = Arrays.asList(new ConsumerRecord<>("topic", 1, 1L, recordKey, recordValue), new ConsumerRecord<>("topic", 1, 5L, recordKey, recordValue), new ConsumerRecord<>("topic", 1, 3L, recordKey, recordValue));    group.addRawRecords(partition1, list1);    verifyBuffered(3, 3, 0);    assertEquals(-1L, group.streamTime());    assertEquals(0.0, metrics.metric(lastLatenessValue).metricValue());    StampedRecord record;    final PartitionGroup.RecordInfo info = new PartitionGroup.RecordInfo();    // get first two records from partition 1    record = group.nextRecord(info);    assertEquals(record.timestamp, 1L);    record = group.nextRecord(info);    assertEquals(record.timestamp, 5L);    // add three 3 records with timestamp 2, 4, 6 to partition-2    final List<ConsumerRecord<byte[], byte[]>> list2 = Arrays.asList(new ConsumerRecord<>("topic", 2, 2L, recordKey, recordValue), new ConsumerRecord<>("topic", 2, 4L, recordKey, recordValue), new ConsumerRecord<>("topic", 2, 6L, recordKey, recordValue));    group.addRawRecords(partition2, list2);    // 1:[3]    // 2:[2, 4, 6]    // get one record, next record should be ts=2 from partition 2    record = group.nextRecord(info);    // 1:[3]    // 2:[4, 6]    assertEquals(record.timestamp, 2L);    // get one record, next up should have ts=3 from partition 1 (even though it has seen a larger max timestamp =5)    record = group.nextRecord(info);    // 1:[]    // 2:[4, 6]    assertEquals(record.timestamp, 3L);}
f17267
0
globalSessionStoreShouldBeReadOnly
public void kafkatest_f17275_0()
{    doTest("GlobalSessionStore", (Consumer<SessionStore<String, Long>>) store -> {        verifyStoreCannotBeInitializedOrClosed(store);        checkThrowsUnsupportedOperation(store::flush, "flush()");        checkThrowsUnsupportedOperation(() -> store.remove(null), "remove()");        checkThrowsUnsupportedOperation(() -> store.put(null, null), "put()");        assertEquals(iters.get(3), store.findSessions(KEY, 1L, 2L));        assertEquals(iters.get(4), store.findSessions(KEY, KEY, 1L, 2L));        assertEquals(iters.get(5), store.fetch(KEY));        assertEquals(iters.get(6), store.fetch(KEY, KEY));    });}
f17275
0
localKeyValueStoreShouldNotAllowInitOrClose
public void kafkatest_f17276_0()
{    doTest("LocalKeyValueStore", (Consumer<KeyValueStore<String, Long>>) store -> {        verifyStoreCannotBeInitializedOrClosed(store);        store.flush();        assertTrue(flushExecuted);        store.put("1", 1L);        assertTrue(putExecuted);        store.putIfAbsent("1", 1L);        assertTrue(putIfAbsentExecuted);        store.putAll(Collections.emptyList());        assertTrue(putAllExecuted);        store.delete("1");        assertTrue(deleteExecuted);        assertEquals((Long) VALUE, store.get(KEY));        assertEquals(rangeIter, store.range("one", "two"));        assertEquals(allIter, store.all());        assertEquals(VALUE, store.approximateNumEntries());    });}
f17276
0
localTimestampedKeyValueStoreShouldNotAllowInitOrClose
public void kafkatest_f17277_0()
{    doTest("LocalTimestampedKeyValueStore", (Consumer<TimestampedKeyValueStore<String, Long>>) store -> {        verifyStoreCannotBeInitializedOrClosed(store);        store.flush();        assertTrue(flushExecuted);        store.put("1", ValueAndTimestamp.make(1L, 2L));        assertTrue(putExecuted);        store.putIfAbsent("1", ValueAndTimestamp.make(1L, 2L));        assertTrue(putIfAbsentExecuted);        store.putAll(Collections.emptyList());        assertTrue(putAllExecuted);        store.delete("1");        assertTrue(deleteExecuted);        assertEquals(VALUE_AND_TIMESTAMP, store.get(KEY));        assertEquals(timestampedRangeIter, store.range("one", "two"));        assertEquals(timestampedAllIter, store.all());        assertEquals(VALUE, store.approximateNumEntries());    });}
f17277
0
sessionStoreMock
private SessionStore<String, Long> kafkatest_f17285_0()
{    final SessionStore<String, Long> sessionStore = mock(SessionStore.class);    initStateStoreMock(sessionStore);    expect(sessionStore.findSessions(anyString(), anyLong(), anyLong())).andReturn(iters.get(3));    expect(sessionStore.findSessions(anyString(), anyString(), anyLong(), anyLong())).andReturn(iters.get(4));    expect(sessionStore.fetch(anyString())).andReturn(iters.get(5));    expect(sessionStore.fetch(anyString(), anyString())).andReturn(iters.get(6));    sessionStore.put(anyObject(Windowed.class), anyLong());    expectLastCall().andAnswer(() -> {        putExecuted = true;        return null;    });    sessionStore.remove(anyObject(Windowed.class));    expectLastCall().andAnswer(() -> {        removeExecuted = true;        return null;    });    replay(sessionStore);    return sessionStore;}
f17285
0
initStateStoreMock
private void kafkatest_f17286_0(final StateStore stateStore)
{    expect(stateStore.name()).andReturn(STORE_NAME);    expect(stateStore.persistent()).andReturn(true);    expect(stateStore.isOpen()).andReturn(true);    stateStore.flush();    expectLastCall().andAnswer(() -> {        flushExecuted = true;        return null;    });}
f17286
0
doTest
private void kafkatest_f17287_0(final String name, final Consumer<T> checker)
{    final Processor processor = new Processor<String, Long>() {        @Override        @SuppressWarnings("unchecked")        public void init(final ProcessorContext context) {            final T store = (T) context.getStateStore(name);            checker.accept(store);        }        @Override        public void process(final String k, final Long v) {        // No-op.        }        @Override        public void close() {        // No-op.        }    };    processor.init(context);}
f17287
0
shouldNotAllowToScheduleSubMillisecondPunctuation
public void kafkatest_f17295_0()
{    try {        context.schedule(Duration.ofNanos(999_999L), null, null);        fail("Should have thrown IllegalArgumentException");    } catch (final IllegalArgumentException expected) {        assertThat(expected.getMessage(), equalTo("The minimum supported scheduling interval is 1 millisecond."));    }}
f17295
0
shouldThrowStreamsExceptionIfExceptionCaughtDuringInit
public void kafkatest_f17296_0()
{    final ProcessorNode node = new ProcessorNode("name", new ExceptionalProcessor(), Collections.emptySet());    node.init(null);}
f17296
0
shouldThrowStreamsExceptionIfExceptionCaughtDuringClose
public void kafkatest_f17297_0()
{    final ProcessorNode node = new ProcessorNode("name", new ExceptionalProcessor(), Collections.emptySet());    node.close();}
f17297
0
shouldEstimateHeadersLength
public void kafkatest_f17308_0()
{    final Headers headers = new RecordHeaders();    headers.add("header-key", "header-value".getBytes());    final ProcessorRecordContext context = new ProcessorRecordContext(42L, 73L, 0, null, headers);    assertEquals(MIN_SIZE + 10L + 12L, context.residentMemorySizeEstimate());}
f17308
0
shouldEstimateNullValueInHeaderAsZero
public void kafkatest_f17309_0()
{    final Headers headers = new RecordHeaders();    headers.add("header-key", null);    final ProcessorRecordContext context = new ProcessorRecordContext(42L, 73L, 0, null, headers);    assertEquals(MIN_SIZE + 10L, context.residentMemorySizeEstimate());}
f17309
0
setup
public void kafkatest_f17310_0()
{    baseDir = TestUtils.tempDirectory();    stateDirectory = new StateDirectory(new StreamsConfig(new Properties() {        {            put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);            put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummy:1234");            put(StreamsConfig.STATE_DIR_CONFIG, baseDir.getPath());        }    }), new MockTime(), true);    checkpointFile = new File(stateDirectory.directoryForTask(taskId), StateManagerUtil.CHECKPOINT_FILE_NAME);    checkpoint = new OffsetCheckpoint(checkpointFile);}
f17310
0
testGetStore
public void kafkatest_f17318_0() throws IOException
{    final MockKeyValueStore mockKeyValueStore = new MockKeyValueStore(nonPersistentStoreName, false);    final ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 1), noPartitions, false, stateDirectory, emptyMap(), changelogReader, false, logContext);    try {        stateMgr.register(mockKeyValueStore, mockKeyValueStore.stateRestoreCallback);        assertNull(stateMgr.getStore("noSuchStore"));        assertEquals(mockKeyValueStore, stateMgr.getStore(nonPersistentStoreName));    } finally {        stateMgr.close(true);    }}
f17318
0
testFlushAndClose
public void kafkatest_f17319_0() throws IOException
{    checkpoint.write(emptyMap());    // set up ack'ed offsets    final HashMap<TopicPartition, Long> ackedOffsets = new HashMap<>();    ackedOffsets.put(new TopicPartition(persistentStoreTopicName, 1), 123L);    ackedOffsets.put(new TopicPartition(nonPersistentStoreTopicName, 1), 456L);    ackedOffsets.put(new TopicPartition(ProcessorStateManager.storeChangelogTopic(applicationId, "otherTopic"), 1), 789L);    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, mkMap(mkEntry(persistentStoreName, persistentStoreTopicName), mkEntry(nonPersistentStoreName, nonPersistentStoreTopicName)), changelogReader, false, logContext);    try {        // make sure the checkpoint file is not written yet        assertFalse(checkpointFile.exists());        stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);        stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);    } finally {        // close the state manager with the ack'ed offsets        stateMgr.flush();        stateMgr.checkpoint(ackedOffsets);        stateMgr.close(true);    }    // make sure all stores are closed, and the checkpoint file is written.    assertTrue(persistentStore.flushed);    assertTrue(persistentStore.closed);    assertTrue(nonPersistentStore.flushed);    assertTrue(nonPersistentStore.closed);    assertTrue(checkpointFile.exists());    // make sure that flush is called in the proper order    assertThat(persistentStore.getLastFlushCount(), Matchers.lessThan(nonPersistentStore.getLastFlushCount()));    // the checkpoint file should contain an offset from the persistent store only.    final Map<TopicPartition, Long> checkpointedOffsets = checkpoint.read();    assertThat(checkpointedOffsets, is(singletonMap(new TopicPartition(persistentStoreTopicName, 1), 124L)));}
f17319
0
shouldMaintainRegistrationOrderWhenReregistered
public void kafkatest_f17320_0() throws IOException
{    checkpoint.write(emptyMap());    // set up ack'ed offsets    final TopicPartition persistentTopicPartition = new TopicPartition(persistentStoreTopicName, 1);    final TopicPartition nonPersistentTopicPartition = new TopicPartition(nonPersistentStoreTopicName, 1);    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, mkMap(mkEntry(persistentStoreName, persistentStoreTopicName), mkEntry(nonPersistentStoreName, nonPersistentStoreTopicName)), changelogReader, false, logContext);    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);    // de-registers the stores, but doesn't re-register them because    // the context isn't connected to our state manager    stateMgr.reinitializeStateStoresForPartitions(asList(nonPersistentTopicPartition, persistentTopicPartition), new MockInternalProcessorContext());    // register them in backward order    stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    stateMgr.flush();    // make sure that flush is called in the proper order    assertTrue(persistentStore.flushed);    assertTrue(nonPersistentStore.flushed);    assertThat(persistentStore.getLastFlushCount(), Matchers.lessThan(nonPersistentStore.getLastFlushCount()));}
f17320
0
shouldWriteCheckpointForStandbyReplica
public void kafkatest_f17328_0() throws IOException
{    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, // standby    true, stateDirectory, singletonMap(persistentStore.name(), persistentStoreTopicName), changelogReader, false, logContext);    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    final byte[] bytes = Serdes.Integer().serializer().serialize("", 10);    stateMgr.updateStandbyStates(persistentStorePartition, singletonList(new ConsumerRecord<>("", 0, 0L, bytes, bytes)), 888L);    stateMgr.checkpoint(emptyMap());    final Map<TopicPartition, Long> read = checkpoint.read();    assertThat(read, equalTo(singletonMap(persistentStorePartition, 889L)));}
f17328
0
shouldNotWriteCheckpointForNonPersistent
public void kafkatest_f17329_0() throws IOException
{    final TopicPartition topicPartition = new TopicPartition(nonPersistentStoreTopicName, 1);    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, // standby    true, stateDirectory, singletonMap(nonPersistentStoreName, nonPersistentStoreTopicName), changelogReader, false, logContext);    stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);    stateMgr.checkpoint(singletonMap(topicPartition, 876L));    final Map<TopicPartition, Long> read = checkpoint.read();    assertThat(read, equalTo(emptyMap()));}
f17329
0
shouldNotWriteCheckpointForStoresWithoutChangelogTopic
public void kafkatest_f17330_0() throws IOException
{    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, // standby    true, stateDirectory, emptyMap(), changelogReader, false, logContext);    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    stateMgr.checkpoint(singletonMap(persistentStorePartition, 987L));    final Map<TopicPartition, Long> read = checkpoint.read();    assertThat(read, equalTo(emptyMap()));}
f17330
0
shouldFlushAllStoresEvenIfStoreThrowsException
public void kafkatest_f17338_0() throws IOException
{    final AtomicBoolean flushedStore = new AtomicBoolean(false);    final MockKeyValueStore stateStore1 = new MockKeyValueStore(storeName, true) {        @Override        public void flush() {            throw new RuntimeException("KABOOM!");        }    };    final MockKeyValueStore stateStore2 = new MockKeyValueStore(storeName + "2", true) {        @Override        public void flush() {            flushedStore.set(true);        }    };    final ProcessorStateManager stateManager = new ProcessorStateManager(taskId, Collections.singleton(changelogTopicPartition), false, stateDirectory, singletonMap(storeName, changelogTopic), changelogReader, false, logContext);    stateManager.register(stateStore1, stateStore1.stateRestoreCallback);    stateManager.register(stateStore2, stateStore2.stateRestoreCallback);    try {        stateManager.flush();    } catch (final ProcessorStateException expected) {    /* ignode */    }    Assert.assertTrue(flushedStore.get());}
f17338
0
flush
public void kafkatest_f17339_0()
{    throw new RuntimeException("KABOOM!");}
f17339
0
flush
public void kafkatest_f17340_0()
{    flushedStore.set(true);}
f17340
0
register
public void kafkatest_f17348_0(final StateStore store, final StateRestoreCallback stateRestoreCallback)
{    stateManager.register(store, stateRestoreCallback);}
f17348
0
getStandByStateManager
private ProcessorStateManager kafkatest_f17349_0(final TaskId taskId) throws IOException
{    return new ProcessorStateManager(taskId, noPartitions, true, stateDirectory, singletonMap(persistentStoreName, persistentStoreTopicName), changelogReader, false, logContext);}
f17349
0
getPersistentStore
private MockKeyValueStore kafkatest_f17350_0()
{    return new MockKeyValueStore("persistentStore", true);}
f17350
0
testDrivingMultiplexingTopology
public void kafkatest_f17358_0()
{    driver = new TopologyTestDriver(createMultiplexingTopology(), props);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key1", "value1"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key1", "value1(1)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key1", "value1(2)");    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key2", "value2"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key2", "value2(1)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key2", "value2(2)");    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key3", "value3"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key4", "value4"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key5", "value5"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key3", "value3(1)");    assertNextOutputRecord(OUTPUT_TOPIC_1, "key4", "value4(1)");    assertNextOutputRecord(OUTPUT_TOPIC_1, "key5", "value5(1)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key3", "value3(2)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key4", "value4(2)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key5", "value5(2)");}
f17358
0
testDrivingMultiplexByNameTopology
public void kafkatest_f17359_0()
{    driver = new TopologyTestDriver(createMultiplexByNameTopology(), props);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key1", "value1"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key1", "value1(1)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key1", "value1(2)");    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key2", "value2"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key2", "value2(1)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key2", "value2(2)");    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key3", "value3"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key4", "value4"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key5", "value5"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key3", "value3(1)");    assertNextOutputRecord(OUTPUT_TOPIC_1, "key4", "value4(1)");    assertNextOutputRecord(OUTPUT_TOPIC_1, "key5", "value5(1)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key3", "value3(2)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key4", "value4(2)");    assertNextOutputRecord(OUTPUT_TOPIC_2, "key5", "value5(2)");}
f17359
0
testDrivingStatefulTopology
public void kafkatest_f17360_0()
{    final String storeName = "entries";    driver = new TopologyTestDriver(createStatefulTopology(storeName), props);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key1", "value1"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key2", "value2"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key3", "value3"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key1", "value4"));    assertNoOutputRecord(OUTPUT_TOPIC_1);    final KeyValueStore<String, String> store = driver.getKeyValueStore(storeName);    assertEquals("value4", store.get("key1"));    assertEquals("value2", store.get("key2"));    assertEquals("value3", store.get("key3"));    assertNull(store.get("key4"));}
f17360
0
shouldCreateStringWithProcessors
public void kafkatest_f17368_0()
{    topology.addSource("source", "t").addProcessor("processor", mockProcessorSupplier, "source").addProcessor("other", mockProcessorSupplier, "source");    final ProcessorTopology processorTopology = topology.getInternalBuilder().build();    final String result = processorTopology.toString();    assertThat(result, containsString("\t\tchildren:\t[processor, other]"));    assertThat(result, containsString("processor:\n"));    assertThat(result, containsString("other:\n"));}
f17368
0
shouldRecursivelyPrintChildren
public void kafkatest_f17369_0()
{    topology.addSource("source", "t").addProcessor("processor", mockProcessorSupplier, "source").addProcessor("child-one", mockProcessorSupplier, "processor").addProcessor("child-one-one", mockProcessorSupplier, "child-one").addProcessor("child-two", mockProcessorSupplier, "processor").addProcessor("child-two-one", mockProcessorSupplier, "child-two");    final String result = topology.getInternalBuilder().build().toString();    assertThat(result, containsString("child-one:\n\t\tchildren:\t[child-one-one]"));    assertThat(result, containsString("child-two:\n\t\tchildren:\t[child-two-one]"));}
f17369
0
shouldConsiderTimeStamps
public void kafkatest_f17370_0()
{    final int partition = 10;    driver = new TopologyTestDriver(createSimpleTopology(partition), props);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key1", "value1", 10L));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key2", "value2", 20L));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key3", "value3", 30L));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key1", "value1", partition, 10L);    assertNextOutputRecord(OUTPUT_TOPIC_1, "key2", "value2", partition, 20L);    assertNextOutputRecord(OUTPUT_TOPIC_1, "key3", "value3", partition, 30L);}
f17370
0
inMemoryStoreShouldNotResultInPersistentGlobalStore
public void kafkatest_f17378_0()
{    final ProcessorTopology processorTopology = createGlobalStoreTopology(Stores.inMemoryKeyValueStore("my-store"));    assertFalse(processorTopology.hasPersistentGlobalStore());}
f17378
0
persistentGlobalStoreShouldBeDetected
public void kafkatest_f17379_0()
{    final ProcessorTopology processorTopology = createGlobalStoreTopology(Stores.persistentKeyValueStore("my-store"));    assertTrue(processorTopology.hasPersistentGlobalStore());}
f17379
0
createLocalStoreTopology
private ProcessorTopology kafkatest_f17380_0(final KeyValueBytesStoreSupplier storeSupplier)
{    final TopologyWrapper topology = new TopologyWrapper();    final String processor = "processor";    final StoreBuilder<KeyValueStore<String, String>> storeBuilder = Stores.keyValueStoreBuilder(storeSupplier, Serdes.String(), Serdes.String());    topology.addSource("source", STRING_DESERIALIZER, STRING_DESERIALIZER, "topic").addProcessor(processor, () -> new StatefulProcessor(storeSupplier.name()), "source").addStateStore(storeBuilder, processor);    return topology.getInternalBuilder("anyAppId").build();}
f17380
0
constantPartitioner
private StreamPartitioner<Object, Object> kafkatest_f17388_0(final Integer partition)
{    return (topic, key, value, numPartitions) -> partition;}
f17388
0
createSimpleTopology
private Topology kafkatest_f17389_0(final int partition)
{    return topology.addSource("source", STRING_DESERIALIZER, STRING_DESERIALIZER, INPUT_TOPIC_1).addProcessor("processor", define(new ForwardingProcessor()), "source").addSink("sink", OUTPUT_TOPIC_1, constantPartitioner(partition), "processor");}
f17389
0
createTimestampTopology
private Topology kafkatest_f17390_0(final int partition)
{    return topology.addSource("source", STRING_DESERIALIZER, STRING_DESERIALIZER, INPUT_TOPIC_1).addProcessor("processor", define(new TimestampProcessor()), "source").addSink("sink", OUTPUT_TOPIC_1, constantPartitioner(partition), "processor");}
f17390
0
createSimpleMultiSourceTopology
private Topology kafkatest_f17398_0(final int partition)
{    return topology.addSource("source-1", STRING_DESERIALIZER, STRING_DESERIALIZER, INPUT_TOPIC_1).addProcessor("processor-1", define(new ForwardingProcessor()), "source-1").addSink("sink-1", OUTPUT_TOPIC_1, constantPartitioner(partition), "processor-1").addSource("source-2", STRING_DESERIALIZER, STRING_DESERIALIZER, INPUT_TOPIC_2).addProcessor("processor-2", define(new ForwardingProcessor()), "source-2").addSink("sink-2", OUTPUT_TOPIC_2, constantPartitioner(partition), "processor-2");}
f17398
0
createAddHeaderTopology
private Topology kafkatest_f17399_0()
{    return topology.addSource("source-1", STRING_DESERIALIZER, STRING_DESERIALIZER, INPUT_TOPIC_1).addProcessor("processor-1", define(new AddHeaderProcessor()), "source-1").addSink("sink-1", OUTPUT_TOPIC_1, "processor-1");}
f17399
0
process
public void kafkatest_f17400_0(final String key, final String value)
{    context().forward(key, value);}
f17400
0
process
public void kafkatest_f17408_0(final String key, final String value)
{    store.put(key, value);}
f17408
0
define
private ProcessorSupplier<K, V> kafkatest_f17409_0(final Processor<K, V> processor)
{    return () -> processor;}
f17409
0
extract
public long kafkatest_f17410_0(final ConsumerRecord<Object, Object> record, final long partitionTime)
{    if (record.value().toString().matches(".*@[0-9]+")) {        return Long.parseLong(record.value().toString().split("@")[1]);    }    if (record.timestamp() >= 0L) {        return record.timestamp();    }    return DEFAULT_TIMESTAMP;}
f17410
0
testUnite
public void kafkatest_f17421_0()
{    final QuickUnion<Long> qu = new QuickUnion<>();    final long[] ids = { 1L, 2L, 3L, 4L, 5L };    for (final long id : ids) {        qu.add(id);    }    assertEquals(5, roots(qu, ids).size());    qu.unite(1L, 2L);    assertEquals(4, roots(qu, ids).size());    assertEquals(qu.root(1L), qu.root(2L));    qu.unite(3L, 4L);    assertEquals(3, roots(qu, ids).size());    assertEquals(qu.root(1L), qu.root(2L));    assertEquals(qu.root(3L), qu.root(4L));    qu.unite(1L, 5L);    assertEquals(2, roots(qu, ids).size());    assertEquals(qu.root(1L), qu.root(2L));    assertEquals(qu.root(2L), qu.root(5L));    assertEquals(qu.root(3L), qu.root(4L));    qu.unite(3L, 5L);    assertEquals(1, roots(qu, ids).size());    assertEquals(qu.root(1L), qu.root(2L));    assertEquals(qu.root(2L), qu.root(3L));    assertEquals(qu.root(3L), qu.root(4L));    assertEquals(qu.root(4L), qu.root(5L));}
f17421
0
testUniteMany
public void kafkatest_f17422_0()
{    final QuickUnion<Long> qu = new QuickUnion<>();    final long[] ids = { 1L, 2L, 3L, 4L, 5L };    for (final long id : ids) {        qu.add(id);    }    assertEquals(5, roots(qu, ids).size());    qu.unite(1L, 2L, 3L, 4L);    assertEquals(2, roots(qu, ids).size());    assertEquals(qu.root(1L), qu.root(2L));    assertEquals(qu.root(2L), qu.root(3L));    assertEquals(qu.root(3L), qu.root(4L));    assertNotEquals(qu.root(1L), qu.root(5L));}
f17422
0
roots
private Set<Long> kafkatest_f17423_0(final QuickUnion<Long> qu, final long... ids)
{    final HashSet<Long> roots = new HashSet<>();    for (final long id : ids) {        roots.add(qu.root(id));    }    return roots;}
f17423
0
shouldNotThrowStreamsExceptionOnSubsequentCallIfASendFailsWithContinueExceptionHandler
public void kafkatest_f17431_0()
{    final RecordCollector collector = new RecordCollectorImpl("test", logContext, new AlwaysContinueProductionExceptionHandler(), new Metrics().sensor("skipped-records"));    collector.init(new MockProducer(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {        @Override        public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {            callback.onCompletion(null, new Exception());            return null;        }    });    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);}
f17431
0
send
public synchronized Future<RecordMetadata> kafkatest_f17432_0(final ProducerRecord record, final Callback callback)
{    callback.onCompletion(null, new Exception());    return null;}
f17432
0
shouldRecordSkippedMetricAndLogWarningIfSendFailsWithContinueExceptionHandler
public void kafkatest_f17433_0()
{    final Metrics metrics = new Metrics();    final Sensor sensor = metrics.sensor("skipped-records");    final LogCaptureAppender logCaptureAppender = LogCaptureAppender.createAndRegister();    final MetricName metricName = new MetricName("name", "group", "description", Collections.emptyMap());    sensor.add(metricName, new WindowedSum());    final RecordCollector collector = new RecordCollectorImpl("test", logContext, new AlwaysContinueProductionExceptionHandler(), sensor);    collector.init(new MockProducer(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {        @Override        public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {            callback.onCompletion(null, new Exception());            return null;        }    });    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);    assertEquals(1.0, metrics.metrics().get(metricName).metricValue());    assertTrue(logCaptureAppender.getMessages().contains("test Error sending records topic=[topic1] and partition=[0]; The exception handler chose to CONTINUE processing in spite of this error. Enable TRACE logging to view failed messages key and value."));    LogCaptureAppender.unregister(logCaptureAppender);}
f17433
0
shouldNotThrowStreamsExceptionOnCloseIfASendFailedWithContinueExceptionHandler
public void kafkatest_f17441_0()
{    final RecordCollector collector = new RecordCollectorImpl("test", logContext, new AlwaysContinueProductionExceptionHandler(), new Metrics().sensor("skipped-records"));    collector.init(new MockProducer(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {        @Override        public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {            callback.onCompletion(null, new Exception());            return null;        }    });    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);    collector.close();}
f17441
0
send
public synchronized Future<RecordMetadata> kafkatest_f17442_0(final ProducerRecord record, final Callback callback)
{    callback.onCompletion(null, new Exception());    return null;}
f17442
0
shouldThrowIfTopicIsUnknownWithDefaultExceptionHandler
public void kafkatest_f17443_0()
{    final RecordCollector collector = new RecordCollectorImpl("test", logContext, new DefaultProductionExceptionHandler(), new Metrics().sensor("skipped-records"));    collector.init(new MockProducer(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {        @Override        public List<PartitionInfo> partitionsFor(final String topic) {            return Collections.emptyList();        }    });    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);}
f17443
0
shouldReturnConsumerRecordWithDeserializedValueWhenNoExceptions
public void kafkatest_f17451_0()
{    final RecordDeserializer recordDeserializer = new RecordDeserializer(new TheSourceNode(false, false, "key", "value"), null, new LogContext(), new Metrics().sensor("skipped-records"));    final ConsumerRecord<Object, Object> record = recordDeserializer.deserialize(null, rawRecord);    assertEquals(rawRecord.topic(), record.topic());    assertEquals(rawRecord.partition(), record.partition());    assertEquals(rawRecord.offset(), record.offset());    assertEquals(rawRecord.checksum(), record.checksum());    assertEquals("key", record.key());    assertEquals("value", record.value());    assertEquals(rawRecord.timestamp(), record.timestamp());    assertEquals(TimestampType.CREATE_TIME, record.timestampType());    assertEquals(rawRecord.headers(), record.headers());}
f17451
0
deserializeKey
public Object kafkatest_f17452_0(final String topic, final Headers headers, final byte[] data)
{    if (keyThrowsException) {        throw new RuntimeException();    }    return key;}
f17452
0
deserializeValue
public Object kafkatest_f17453_0(final String topic, final Headers headers, final byte[] data)
{    if (valueThrowsException) {        throw new RuntimeException();    }    return value;}
f17453
0
shouldNotThrowStreamsExceptionWhenValueDeserializationFailsWithSkipHandler
public void kafkatest_f17461_0()
{    final byte[] value = Serdes.Long().serializer().serialize("foo", 1L);    final List<ConsumerRecord<byte[], byte[]>> records = Collections.singletonList(new ConsumerRecord<>("topic", 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, value));    queueThatSkipsDeserializeErrors.addRawRecords(records);    assertEquals(0, queueThatSkipsDeserializeErrors.size());}
f17461
0
shouldThrowOnNegativeTimestamp
public void kafkatest_f17462_0()
{    final List<ConsumerRecord<byte[], byte[]>> records = Collections.singletonList(new ConsumerRecord<>("topic", 1, 1, -1L, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, recordValue));    final RecordQueue queue = new RecordQueue(new TopicPartition(topics[0], 1), new MockSourceNode<>(topics, intDeserializer, intDeserializer), new FailOnInvalidTimestamp(), new LogAndContinueExceptionHandler(), new InternalMockProcessorContext(), new LogContext());    queue.addRawRecords(records);}
f17462
0
shouldDropOnNegativeTimestamp
public void kafkatest_f17463_0()
{    final List<ConsumerRecord<byte[], byte[]>> records = Collections.singletonList(new ConsumerRecord<>("topic", 1, 1, -1L, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, recordValue));    final RecordQueue queue = new RecordQueue(new TopicPartition(topics[0], 1), new MockSourceNode<>(topics, intDeserializer, intDeserializer), new LogAndSkipOnInvalidTimestamp(), new LogAndContinueExceptionHandler(), new InternalMockProcessorContext(), new LogContext());    queue.addRawRecords(records);    assertEquals(0, queue.size());}
f17463
0
shouldHandleNullValuesWhenThrowingStreamsExceptionOnKeyValueTypeSerializerMismatch
public void kafkatest_f17471_0()
{    final String invalidKeyToTriggerSerializerMismatch = "";    // When/Then    context.setTime(1);    try {        illTypedSink.process(invalidKeyToTriggerSerializerMismatch, null);        fail("Should have thrown StreamsException");    } catch (final StreamsException e) {        assertThat(e.getCause(), instanceOf(ClassCastException.class));        assertThat(e.getMessage(), containsString("unknown because value is null"));    }}
f17471
0
shouldProvideTopicHeadersAndDataToKeyDeserializer
public void kafkatest_f17472_0()
{    final SourceNode<String, String> sourceNode = new MockSourceNode<>(new String[] { "" }, new TheDeserializer(), new TheDeserializer());    final RecordHeaders headers = new RecordHeaders();    final String deserializeKey = sourceNode.deserializeKey("topic", headers, "data".getBytes(StandardCharsets.UTF_8));    assertThat(deserializeKey, is("topic" + headers + "data"));}
f17472
0
shouldProvideTopicHeadersAndDataToValueDeserializer
public void kafkatest_f17473_0()
{    final SourceNode<String, String> sourceNode = new MockSourceNode<>(new String[] { "" }, new TheDeserializer(), new TheDeserializer());    final RecordHeaders headers = new RecordHeaders();    final String deserializedValue = sourceNode.deserializeValue("topic", headers, "data".getBytes(StandardCharsets.UTF_8));    assertThat(deserializedValue, is("topic" + headers + "data"));}
f17473
0
testUpdate
public void kafkatest_f17481_0() throws IOException
{    final StreamsConfig config = createConfig(baseDir);    task = new StandbyTask(taskId, topicPartitions, topology, consumer, changelogReader, config, streamsMetrics, stateDirectory);    task.initializeStateStores();    final Set<TopicPartition> partition = Collections.singleton(partition2);    restoreStateConsumer.assign(partition);    for (final ConsumerRecord<Integer, Integer> record : asList(new ConsumerRecord<>(partition2.topic(), partition2.partition(), 10, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, 1, 100), new ConsumerRecord<>(partition2.topic(), partition2.partition(), 20, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, 2, 100), new ConsumerRecord<>(partition2.topic(), partition2.partition(), 30, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, 3, 100))) {        restoreStateConsumer.bufferRecord(record);    }    restoreStateConsumer.seekToBeginning(partition);    task.update(partition2, restoreStateConsumer.poll(ofMillis(100)).records(partition2));    final StandbyContextImpl context = (StandbyContextImpl) task.context();    final MockKeyValueStore store1 = (MockKeyValueStore) context.getStateMgr().getStore(storeName1);    final MockKeyValueStore store2 = (MockKeyValueStore) context.getStateMgr().getStore(storeName2);    assertEquals(Collections.emptyList(), store1.keys);    assertEquals(asList(1, 2, 3), store2.keys);}
f17481
0
shouldRestoreToWindowedStores
public void kafkatest_f17482_0() throws IOException
{    final String storeName = "windowed-store";    final String changelogName = applicationId + "-" + storeName + "-changelog";    final TopicPartition topicPartition = new TopicPartition(changelogName, 1);    final List<TopicPartition> partitions = Collections.singletonList(topicPartition);    consumer.assign(partitions);    final InternalTopologyBuilder internalTopologyBuilder = new InternalTopologyBuilder().setApplicationId(applicationId);    final InternalStreamsBuilder builder = new InternalStreamsBuilder(internalTopologyBuilder);    builder.stream(Collections.singleton("topic"), new ConsumedInternal<>()).groupByKey().windowedBy(TimeWindows.of(ofMillis(60_000)).grace(ofMillis(0L))).count(Materialized.<Object, Long, WindowStore<Bytes, byte[]>>as(storeName).withRetention(ofMillis(120_000L)));    builder.buildAndOptimizeTopology();    task = new StandbyTask(taskId, partitions, internalTopologyBuilder.build(0), consumer, new StoreChangelogReader(restoreStateConsumer, Duration.ZERO, stateRestoreListener, new LogContext("standby-task-test ")), createConfig(baseDir), new MockStreamsMetrics(new Metrics()), stateDirectory);    task.initializeStateStores();    consumer.commitSync(mkMap(mkEntry(topicPartition, new OffsetAndMetadata(35L))));    task.commit();    final List<ConsumerRecord<byte[], byte[]>> remaining1 = task.update(topicPartition, asList(makeWindowedConsumerRecord(changelogName, 10, 1, 0L, 60_000L), makeWindowedConsumerRecord(changelogName, 20, 2, 60_000L, 120_000), makeWindowedConsumerRecord(changelogName, 30, 3, 120_000L, 180_000), makeWindowedConsumerRecord(changelogName, 40, 4, 180_000L, 240_000)));    assertEquals(asList(new KeyValue<>(new Windowed<>(1, new TimeWindow(0, 60_000)), ValueAndTimestamp.make(100L, 60_000L)), new KeyValue<>(new Windowed<>(2, new TimeWindow(60_000, 120_000)), ValueAndTimestamp.make(100L, 120_000L)), new KeyValue<>(new Windowed<>(3, new TimeWindow(120_000, 180_000)), ValueAndTimestamp.make(100L, 180_000L))), getWindowedStoreContents(storeName, task));    consumer.commitSync(mkMap(mkEntry(topicPartition, new OffsetAndMetadata(45L))));    task.commit();    final List<ConsumerRecord<byte[], byte[]>> remaining2 = task.update(topicPartition, remaining1);    assertEquals(emptyList(), remaining2);    // the first record's window should have expired.    assertEquals(asList(new KeyValue<>(new Windowed<>(2, new TimeWindow(60_000, 120_000)), ValueAndTimestamp.make(100L, 120_000L)), new KeyValue<>(new Windowed<>(3, new TimeWindow(120_000, 180_000)), ValueAndTimestamp.make(100L, 180_000L)), new KeyValue<>(new Windowed<>(4, new TimeWindow(180_000, 240_000)), ValueAndTimestamp.make(100L, 240_000L))), getWindowedStoreContents(storeName, task));}
f17482
0
makeWindowedConsumerRecord
private ConsumerRecord<byte[], byte[]> kafkatest_f17483_0(final String changelogName, final int offset, final int key, final long start, final long end)
{    final Windowed<Integer> data = new Windowed<>(key, new TimeWindow(start, end));    final Bytes wrap = Bytes.wrap(new IntegerSerializer().serialize(null, data.key()));    final byte[] keyBytes = WindowKeySchema.toStoreKeyBinary(new Windowed<>(wrap, data.window()), 1).get();    return new ConsumerRecord<>(changelogName, 1, offset, end, TimestampType.CREATE_TIME, 0L, 0, 0, keyBytes, new LongSerializer().serialize(null, 100L));}
f17483
0
committed
public synchronized OffsetAndMetadata kafkatest_f17491_0(final TopicPartition partition)
{    committedCallCount.getAndIncrement();    return super.committed(partition);}
f17491
0
shouldInitializeStateStoreWithoutException
public void kafkatest_f17492_0() throws IOException
{    final InternalStreamsBuilder builder = new InternalStreamsBuilder(new InternalTopologyBuilder());    builder.stream(Collections.singleton("topic"), new ConsumedInternal<>()).groupByKey().count();    initializeStandbyStores(builder);}
f17492
0
shouldInitializeWindowStoreWithoutException
public void kafkatest_f17493_0() throws IOException
{    final InternalStreamsBuilder builder = new InternalStreamsBuilder(new InternalTopologyBuilder());    builder.stream(Collections.singleton("topic"), new ConsumedInternal<>()).groupByKey().windowedBy(TimeWindows.of(ofMillis(100))).count();    initializeStandbyStores(builder);}
f17493
0
shouldRecordTaskClosedMetricOnClose
public void kafkatest_f17501_0() throws IOException
{    final MetricName metricName = setupCloseTaskMetric();    final StandbyTask task = new StandbyTask(taskId, ktablePartitions, ktableTopology, consumer, changelogReader, createConfig(baseDir), streamsMetrics, stateDirectory);    final boolean clean = true;    final boolean isZombie = false;    task.close(clean, isZombie);    final double expectedCloseTaskMetric = 1.0;    verifyCloseTaskMetric(expectedCloseTaskMetric, streamsMetrics, metricName);}
f17501
0
shouldRecordTaskClosedMetricOnCloseSuspended
public void kafkatest_f17502_0() throws IOException
{    final MetricName metricName = setupCloseTaskMetric();    final StandbyTask task = new StandbyTask(taskId, ktablePartitions, ktableTopology, consumer, changelogReader, createConfig(baseDir), streamsMetrics, stateDirectory);    final boolean clean = true;    final boolean isZombie = false;    task.closeSuspended(clean, isZombie, new RuntimeException());    final double expectedCloseTaskMetric = 1.0;    verifyCloseTaskMetric(expectedCloseTaskMetric, streamsMetrics, metricName);}
f17502
0
setUp
public void kafkatest_f17503_0()
{    partitionOffsets.put(topicOne, 20L);    partitionOffsets.put(topicTwo, 30L);    stateMaintainer = new StateMaintainerStub(partitionOffsets);    stateConsumer = new GlobalStreamThread.StateConsumer(logContext, consumer, stateMaintainer, time, Duration.ofMillis(10L), FLUSH_INTERVAL);}
f17503
0
shouldCloseStateMaintainer
public void kafkatest_f17511_0() throws IOException
{    stateConsumer.close();    assertTrue(stateMaintainer.closed);}
f17511
0
initialize
public Map<TopicPartition, Long> kafkatest_f17512_0()
{    return partitionOffsets;}
f17512
0
flushState
public void kafkatest_f17513_0()
{    flushed = true;}
f17513
0
shouldLockTaskStateDirectory
public void kafkatest_f17521_0() throws Exception
{    final TaskId taskId = new TaskId(0, 0);    final File taskDirectory = directory.directoryForTask(taskId);    directory.lock(taskId);    try (final FileChannel channel = FileChannel.open(new File(taskDirectory, StateDirectory.LOCK_FILE_NAME).toPath(), StandardOpenOption.CREATE, StandardOpenOption.WRITE)) {        channel.tryLock();        fail("shouldn't be able to lock already locked directory");    } catch (final OverlappingFileLockException e) {    // swallow    } finally {        directory.unlock(taskId);    }}
f17521
0
shouldBeTrueIfAlreadyHoldsLock
public void kafkatest_f17522_0() throws Exception
{    final TaskId taskId = new TaskId(0, 0);    directory.directoryForTask(taskId);    directory.lock(taskId);    try {        assertTrue(directory.lock(taskId));    } finally {        directory.unlock(taskId);    }}
f17522
0
shouldThrowProcessorStateException
public void kafkatest_f17523_0() throws Exception
{    final TaskId taskId = new TaskId(0, 0);    Utils.delete(stateDir);    try {        directory.directoryForTask(taskId);        fail("Should have thrown ProcessorStateException");    } catch (final ProcessorStateException expected) {    // swallow    }}
f17523
0
shouldCreateDirectoriesIfParentDoesntExist
public void kafkatest_f17531_0()
{    final File tempDir = TestUtils.tempDirectory();    final File stateDir = new File(new File(tempDir, "foo"), "state-dir");    final StateDirectory stateDirectory = new StateDirectory(new StreamsConfig(new Properties() {        {            put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);            put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummy:1234");            put(StreamsConfig.STATE_DIR_CONFIG, stateDir.getPath());        }    }), time, true);    final File taskDir = stateDirectory.directoryForTask(new TaskId(0, 0));    assertTrue(stateDir.exists());    assertTrue(taskDir.exists());}
f17531
0
shouldLockGlobalStateDirectory
public void kafkatest_f17532_0() throws Exception
{    directory.lockGlobalState();    try (final FileChannel channel = FileChannel.open(new File(directory.globalStateDir(), StateDirectory.LOCK_FILE_NAME).toPath(), StandardOpenOption.CREATE, StandardOpenOption.WRITE)) {        channel.lock();        fail("Should have thrown OverlappingFileLockException");    } catch (final OverlappingFileLockException expcted) {    // swallow    } finally {        directory.unlockGlobalState();    }}
f17532
0
shouldUnlockGlobalStateDirectory
public void kafkatest_f17533_0() throws Exception
{    directory.lockGlobalState();    directory.unlockGlobalState();    try (final FileChannel channel = FileChannel.open(new File(directory.globalStateDir(), StateDirectory.LOCK_FILE_NAME).toPath(), StandardOpenOption.CREATE, StandardOpenOption.WRITE)) {        // should lock without any exceptions        channel.lock();    }}
f17533
0
shouldLockGlobalStateDirectoryWhenDirectoryCreationDisabled
public void kafkatest_f17541_0() throws Exception
{    initializeStateDirectory(false);    assertTrue(directory.lockGlobalState());}
f17541
0
baseDir
public File kafkatest_f17542_0()
{    return null;}
f17542
0
getGlobalStore
public StateStore kafkatest_f17547_0(final String name)
{    return null;}
f17547
0
restore
public void kafkatest_f17556_0(final byte[] key, final byte[] value)
{// unreachable}
f17556
0
shouldConvertToKeyValue
public void kafkatest_f17557_0()
{    final ArrayList<KeyValue<byte[], byte[]>> actual = new ArrayList<>();    final StateRestoreCallback callback = (key, value) -> actual.add(new KeyValue<>(key, value));    final RecordBatchingStateRestoreCallback adapted = adapt(callback);    final byte[] key1 = { 1 };    final byte[] value1 = { 2 };    final byte[] key2 = { 3 };    final byte[] value2 = { 4 };    adapted.restoreBatch(asList(new ConsumerRecord<>("topic1", 0, 0L, key1, value1), new ConsumerRecord<>("topic2", 1, 1L, key2, value2)));    assertThat(actual, is(asList(new KeyValue<>(key1, value1), new KeyValue<>(key2, value2))));}
f17557
0
validate
private void kafkatest_f17558_0(final List<ConsumerRecord<byte[], byte[]>> actual, final List<ConsumerRecord<byte[], byte[]>> expected)
{    assertThat(actual.size(), is(expected.size()));    for (int i = 0; i < actual.size(); i++) {        final ConsumerRecord<byte[], byte[]> actual1 = actual.get(i);        final ConsumerRecord<byte[], byte[]> expected1 = expected.get(i);        assertThat(actual1.topic(), is(expected1.topic()));        assertThat(actual1.partition(), is(expected1.partition()));        assertThat(actual1.offset(), is(expected1.offset()));        assertThat(actual1.key(), is(expected1.key()));        assertThat(actual1.value(), is(expected1.value()));        assertThat(actual1.timestamp(), is(expected1.timestamp()));        assertThat(actual1.headers(), is(expected1.headers()));    }}
f17558
0
shouldSetStartingOffsetToMinOfLimitAndOffset
public void kafkatest_f17566_0()
{    restorer.setStartingOffset(20);    assertThat(restorer.startingOffset(), equalTo(20L));    restorer.setRestoredOffset(100);    assertThat(restorer.restoredOffset(), equalTo(OFFSET_LIMIT));}
f17566
0
shouldReturnCorrectNumRestoredRecords
public void kafkatest_f17567_0()
{    restorer.setStartingOffset(20);    restorer.setRestoredOffset(40);    assertThat(restorer.restoredNumRecords(), equalTo(20L));    restorer.setRestoredOffset(100);    assertThat(restorer.restoredNumRecords(), equalTo(OFFSET_LIMIT - 20));}
f17567
0
setUp
public void kafkatest_f17568_0()
{    restoreListener.setUserRestoreListener(stateRestoreListener);}
f17568
0
shouldRestoreMessagesFromCheckpoint
public void kafkatest_f17576_0()
{    final int messages = 10;    setupConsumer(messages, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, 5L, Long.MAX_VALUE, true, "storeName", identity()));    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(5));}
f17576
0
shouldClearAssignmentAtEndOfRestore
public void kafkatest_f17577_0()
{    final int messages = 1;    setupConsumer(messages, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);    replay(active, task);    changelogReader.restore(active);    assertThat(consumer.assignment(), equalTo(Collections.<TopicPartition>emptySet()));}
f17577
0
shouldRestoreToLimitWhenSupplied
public void kafkatest_f17578_0()
{    setupConsumer(10, topicPartition);    final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, null, 3, true, "storeName", identity());    changelogReader.register(restorer);    expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);    replay(active, task);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(3));    assertThat(restorer.restoredOffset(), equalTo(3L));}
f17578
0
shouldReturnRestoredOffsetsForPersistentStores
public void kafkatest_f17586_0()
{    setupConsumer(10, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);    replay(active, task);    changelogReader.restore(active);    final Map<TopicPartition, Long> restoredOffsets = changelogReader.restoredOffsets();    assertThat(restoredOffsets, equalTo(Collections.singletonMap(topicPartition, 10L)));}
f17586
0
shouldNotReturnRestoredOffsetsForNonPersistentStore
public void kafkatest_f17587_0()
{    setupConsumer(10, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);    replay(active, task);    changelogReader.restore(active);    final Map<TopicPartition, Long> restoredOffsets = changelogReader.restoredOffsets();    assertThat(restoredOffsets, equalTo(Collections.emptyMap()));}
f17587
0
shouldIgnoreNullKeysWhenRestoring
public void kafkatest_f17588_0()
{    assignPartition(3, topicPartition);    final byte[] bytes = new byte[0];    consumer.addRecord(new ConsumerRecord<>(topicPartition.topic(), topicPartition.partition(), 0, bytes, bytes));    consumer.addRecord(new ConsumerRecord<>(topicPartition.topic(), topicPartition.partition(), 1, null, bytes));    consumer.addRecord(new ConsumerRecord<>(topicPartition.topic(), topicPartition.partition(), 2, bytes, bytes));    consumer.assign(Collections.singletonList(topicPartition));    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, false, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);    replay(active, task);    changelogReader.restore(active);    assertThat(callback.restored, CoreMatchers.equalTo(asList(KeyValue.pair(bytes, bytes), KeyValue.pair(bytes, bytes))));}
f17588
0
shouldNotThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForSourceTopicEOSEnabled
public void kafkatest_f17596_0()
{    final int totalMessages = 10;    assignPartition(totalMessages, topicPartition);    // records 0..4 last offset before commit is 4    addRecords(5, topicPartition, 0);    // EOS enabled so commit marker at offset 5 so records start at 6    addRecords(5, topicPartition, 6);    consumer.assign(Collections.emptyList());    // commit marker is 5 so ending offset is 12    consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 12L));    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 6, true, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andReturn(task);    replay(active);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(5));}
f17596
0
shouldNotThrowTaskMigratedExceptionIfEndOffsetNotExceededDuringRestoreForSourceTopicEOSEnabled
public void kafkatest_f17597_0()
{    final int totalMessages = 10;    setupConsumer(totalMessages, topicPartition);    // records have offsets 0..9 10 is commit marker so 11 is ending offset    consumer.updateEndOffsets(Collections.singletonMap(topicPartition, 11L));    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 11, true, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andReturn(task);    replay(active);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(10));}
f17597
0
shouldRestoreUpToOffsetLimit
public void kafkatest_f17598_0()
{    setupConsumer(10, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, 2L, 5, true, "storeName1", identity()));    expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);    replay(active, task);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(3));    assertAllCallbackStatesExecuted(callback, "storeName1");    assertCorrectOffsetsReportedByListener(callback, 2L, 4L, 3L);}
f17598
0
partition
public Integer kafkatest_f17606_0(final String topic, final String key, final Object value, final int numPartitions)
{    return 1;}
f17606
0
shouldNotThrowNPEWhenOnChangeNotCalled
public void kafkatest_f17607_0()
{    new StreamsMetadataState(TopologyWrapper.getInternalTopologyBuilder(builder.build()), hostOne).getAllMetadataForStore("store");}
f17607
0
shouldGetAllStreamInstances
public void kafkatest_f17608_0()
{    final StreamsMetadata one = new StreamsMetadata(hostOne, Utils.mkSet(globalTable, "table-one", "table-two", "merged-table"), Utils.mkSet(topic1P0, topic2P1, topic4P0));    final StreamsMetadata two = new StreamsMetadata(hostTwo, Utils.mkSet(globalTable, "table-two", "table-one", "merged-table"), Utils.mkSet(topic2P0, topic1P1));    final StreamsMetadata three = new StreamsMetadata(hostThree, Utils.mkSet(globalTable, "table-three"), Collections.singleton(topic3P0));    final Collection<StreamsMetadata> actual = metadataState.getAllMetadata();    assertEquals(3, actual.size());    assertTrue("expected " + actual + " to contain " + one, actual.contains(one));    assertTrue("expected " + actual + " to contain " + two, actual.contains(two));    assertTrue("expected " + actual + " to contain " + three, actual.contains(three));}
f17608
0
shouldReturnNotAvailableWhenClusterIsEmpty
public void kafkatest_f17616_0()
{    metadataState.onChange(Collections.<HostInfo, Set<TopicPartition>>emptyMap(), Cluster.empty());    final StreamsMetadata result = metadataState.getMetadataWithKey("table-one", "a", Serdes.String().serializer());    assertEquals(StreamsMetadata.NOT_AVAILABLE, result);}
f17616
0
shouldGetInstanceWithKeyWithMergedStreams
public void kafkatest_f17617_0()
{    final TopicPartition topic2P2 = new TopicPartition("topic-two", 2);    hostToPartitions.put(hostTwo, Utils.mkSet(topic2P0, topic1P1, topic2P2));    metadataState.onChange(hostToPartitions, cluster.withPartitions(Collections.singletonMap(topic2P2, new PartitionInfo("topic-two", 2, null, null, null))));    final StreamsMetadata expected = new StreamsMetadata(hostTwo, Utils.mkSet("global-table", "table-two", "table-one", "merged-table"), Utils.mkSet(topic2P0, topic1P1, topic2P2));    final StreamsMetadata actual = metadataState.getMetadataWithKey("merged-table", "123", new StreamPartitioner<String, Object>() {        @Override        public Integer partition(final String topic, final String key, final Object value, final int numPartitions) {            return 2;        }    });    assertEquals(expected, actual);}
f17617
0
partition
public Integer kafkatest_f17618_0(final String topic, final String key, final Object value, final int numPartitions)
{    return 2;}
f17618
0
shouldGetAnyHostForGlobalStoreByKeyIfMyHostUnknown
public void kafkatest_f17626_0()
{    final StreamsMetadataState streamsMetadataState = new StreamsMetadataState(TopologyWrapper.getInternalTopologyBuilder(builder.build()), StreamsMetadataState.UNKNOWN_HOST);    streamsMetadataState.onChange(hostToPartitions, cluster);    assertNotNull(streamsMetadataState.getMetadataWithKey(globalTable, "key", Serdes.String().serializer()));}
f17626
0
shouldGetMyMetadataForGlobalStoreWithKeyAndPartitioner
public void kafkatest_f17627_0()
{    final StreamsMetadata metadata = metadataState.getMetadataWithKey(globalTable, "key", partitioner);    assertEquals(hostOne, metadata.hostInfo());}
f17627
0
shouldGetAnyHostForGlobalStoreByKeyAndPartitionerIfMyHostUnknown
public void kafkatest_f17628_0()
{    final StreamsMetadataState streamsMetadataState = new StreamsMetadataState(TopologyWrapper.getInternalTopologyBuilder(builder.build()), StreamsMetadataState.UNKNOWN_HOST);    streamsMetadataState.onChange(hostToPartitions, cluster);    assertNotNull(streamsMetadataState.getMetadataWithKey(globalTable, "key", partitioner));}
f17628
0
testAssignBasic
public void kafkatest_f17636_0()
{    builder.addSource(null, "source1", null, null, null, "topic1");    builder.addSource(null, "source2", null, null, null, "topic2");    builder.addProcessor("processor", new MockProcessorSupplier(), "source1", "source2");    final List<String> topics = asList("topic1", "topic2");    final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);    final Set<TaskId> prevTasks10 = Utils.mkSet(task0);    final Set<TaskId> prevTasks11 = Utils.mkSet(task1);    final Set<TaskId> prevTasks20 = Utils.mkSet(task2);    final Set<TaskId> standbyTasks10 = Utils.mkSet(task1);    final Set<TaskId> standbyTasks11 = Utils.mkSet(task2);    final Set<TaskId> standbyTasks20 = Utils.mkSet(task0);    final UUID uuid1 = UUID.randomUUID();    final UUID uuid2 = UUID.randomUUID();    createMockTaskManager(prevTasks10, standbyTasks10, uuid1, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));    subscriptions.put("consumer10", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks10, standbyTasks10, userEndPoint).encode()));    subscriptions.put("consumer11", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks11, standbyTasks11, userEndPoint).encode()));    subscriptions.put("consumer20", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, prevTasks20, standbyTasks20, userEndPoint).encode()));    final Map<String, ConsumerPartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();    // check assigned partitions    assertEquals(Utils.mkSet(Utils.mkSet(t1p0, t2p0), Utils.mkSet(t1p1, t2p1)), Utils.mkSet(new HashSet<>(assignments.get("consumer10").partitions()), new HashSet<>(assignments.get("consumer11").partitions())));    assertEquals(Utils.mkSet(t1p2, t2p2), new HashSet<>(assignments.get("consumer20").partitions()));    // check assignment info    // the first consumer    final AssignmentInfo info10 = checkAssignment(allTopics, assignments.get("consumer10"));    final Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());    // the second consumer    final AssignmentInfo info11 = checkAssignment(allTopics, assignments.get("consumer11"));    allActiveTasks.addAll(info11.activeTasks());    assertEquals(Utils.mkSet(task0, task1), allActiveTasks);    // the third consumer    final AssignmentInfo info20 = checkAssignment(allTopics, assignments.get("consumer20"));    allActiveTasks.addAll(info20.activeTasks());    assertEquals(3, allActiveTasks.size());    assertEquals(allTasks, new HashSet<>(allActiveTasks));    assertEquals(3, allActiveTasks.size());    assertEquals(allTasks, allActiveTasks);}
f17636
0
shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads
public void kafkatest_f17637_0()
{    builder.addSource(null, "source1", null, null, null, "topic1");    builder.addSource(null, "source2", null, null, null, "topic2");    builder.addProcessor("processor", new MockProcessorSupplier(), "source1");    builder.addProcessor("processorII", new MockProcessorSupplier(), "source2");    final List<PartitionInfo> localInfos = asList(new PartitionInfo("topic1", 0, Node.noNode(), new Node[0], new Node[0]), new PartitionInfo("topic1", 1, Node.noNode(), new Node[0], new Node[0]), new PartitionInfo("topic1", 2, Node.noNode(), new Node[0], new Node[0]), new PartitionInfo("topic1", 3, Node.noNode(), new Node[0], new Node[0]), new PartitionInfo("topic2", 0, Node.noNode(), new Node[0], new Node[0]), new PartitionInfo("topic2", 1, Node.noNode(), new Node[0], new Node[0]), new PartitionInfo("topic2", 2, Node.noNode(), new Node[0], new Node[0]), new PartitionInfo("topic2", 3, Node.noNode(), new Node[0], new Node[0]));    final Cluster localMetadata = new Cluster("cluster", Collections.singletonList(Node.noNode()), localInfos, Collections.emptySet(), Collections.emptySet());    final List<String> topics = asList("topic1", "topic2");    final TaskId taskIdA0 = new TaskId(0, 0);    final TaskId taskIdA1 = new TaskId(0, 1);    final TaskId taskIdA2 = new TaskId(0, 2);    final TaskId taskIdA3 = new TaskId(0, 3);    final TaskId taskIdB0 = new TaskId(1, 0);    final TaskId taskIdB1 = new TaskId(1, 1);    final TaskId taskIdB2 = new TaskId(1, 2);    final TaskId taskIdB3 = new TaskId(1, 3);    final UUID uuid1 = UUID.randomUUID();    createMockTaskManager(new HashSet<>(), new HashSet<>(), uuid1, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));    subscriptions.put("consumer10", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, new HashSet<>(), new HashSet<>(), userEndPoint).encode()));    subscriptions.put("consumer11", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, new HashSet<>(), new HashSet<>(), userEndPoint).encode()));    final Map<String, ConsumerPartitionAssignor.Assignment> assignments = partitionAssignor.assign(localMetadata, new GroupSubscription(subscriptions)).groupAssignment();    // check assigned partitions    assertEquals(Utils.mkSet(Utils.mkSet(t2p2, t1p0, t1p2, t2p0), Utils.mkSet(t1p1, t2p1, t1p3, t2p3)), Utils.mkSet(new HashSet<>(assignments.get("consumer10").partitions()), new HashSet<>(assignments.get("consumer11").partitions())));    // the first consumer    final AssignmentInfo info10 = AssignmentInfo.decode(assignments.get("consumer10").userData());    final List<TaskId> expectedInfo10TaskIds = asList(taskIdA1, taskIdA3, taskIdB1, taskIdB3);    assertEquals(expectedInfo10TaskIds, info10.activeTasks());    // the second consumer    final AssignmentInfo info11 = AssignmentInfo.decode(assignments.get("consumer11").userData());    final List<TaskId> expectedInfo11TaskIds = asList(taskIdA0, taskIdA2, taskIdB0, taskIdB2);    assertEquals(expectedInfo11TaskIds, info11.activeTasks());}
f17637
0
testAssignWithPartialTopology
public void kafkatest_f17638_0()
{    builder.addSource(null, "source1", null, null, null, "topic1");    builder.addProcessor("processor1", new MockProcessorSupplier(), "source1");    builder.addStateStore(new MockKeyValueStoreBuilder("store1", false), "processor1");    builder.addSource(null, "source2", null, null, null, "topic2");    builder.addProcessor("processor2", new MockProcessorSupplier(), "source2");    builder.addStateStore(new MockKeyValueStoreBuilder("store2", false), "processor2");    final List<String> topics = asList("topic1", "topic2");    final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);    final UUID uuid1 = UUID.randomUUID();    createMockTaskManager(emptyTasks, emptyTasks, uuid1, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.singletonMap(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, SingleGroupPartitionGrouperStub.class));    partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));    // will throw exception if it fails    subscriptions.put("consumer10", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));    final Map<String, ConsumerPartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();    // check assignment info    final AssignmentInfo info10 = checkAssignment(Utils.mkSet("topic1"), assignments.get("consumer10"));    final Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());    assertEquals(3, allActiveTasks.size());    assertEquals(allTasks, new HashSet<>(allActiveTasks));}
f17638
0
testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic
public void kafkatest_f17646_0()
{    final String applicationId = "test";    builder.setApplicationId(applicationId);    builder.addInternalTopic("topicX");    builder.addSource(null, "source1", null, null, null, "topic1");    builder.addProcessor("processor1", new MockProcessorSupplier(), "source1");    builder.addSink("sink1", "topicX", null, null, null, "processor1");    builder.addSource(null, "source2", null, null, null, "topicX");    builder.addInternalTopic("topicZ");    builder.addProcessor("processor2", new MockProcessorSupplier(), "source2");    builder.addSink("sink2", "topicZ", null, null, null, "processor2");    builder.addSource(null, "source3", null, null, null, "topicZ");    final List<String> topics = asList("topic1", "test-topicX", "test-topicZ");    final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);    final UUID uuid1 = UUID.randomUUID();    createMockTaskManager(emptyTasks, emptyTasks, uuid1, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    final MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);    partitionAssignor.setInternalTopicManager(internalTopicManager);    subscriptions.put("consumer10", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));    partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();    // check prepared internal topics    assertEquals(2, internalTopicManager.readyTopics.size());    assertEquals(allTasks.size(), (long) internalTopicManager.readyTopics.get("test-topicZ"));}
f17646
0
shouldGenerateTasksForAllCreatedPartitions
public void kafkatest_f17647_0()
{    final StreamsBuilder builder = new StreamsBuilder();    // KStream with 3 partitions    final KStream<Object, Object> stream1 = builder.stream("topic1").map((KeyValueMapper<Object, Object, KeyValue<Object, Object>>) KeyValue::new);    // KTable with 4 partitions    final KTable<Object, Long> table1 = builder.table("topic3").groupBy(KeyValue::new).count();    // joining the stream and the table    // this triggers the enforceCopartitioning() routine in the StreamsPartitionAssignor,    // forcing the stream.map to get repartitioned to a topic with four partitions.    stream1.join(table1, (ValueJoiner) (value1, value2) -> null);    final UUID uuid = UUID.randomUUID();    final String client = "client1";    final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(builder.build());    internalTopologyBuilder.setApplicationId(applicationId);    createMockTaskManager(emptyTasks, emptyTasks, UUID.randomUUID(), internalTopologyBuilder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    final MockInternalTopicManager mockInternalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);    partitionAssignor.setInternalTopicManager(mockInternalTopicManager);    subscriptions.put(client, new ConsumerPartitionAssignor.Subscription(asList("topic1", "topic3"), new SubscriptionInfo(uuid, emptyTasks, emptyTasks, userEndPoint).encode()));    final Map<String, ConsumerPartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();    final Map<String, Integer> expectedCreatedInternalTopics = new HashMap<>();    expectedCreatedInternalTopics.put(applicationId + "-KTABLE-AGGREGATE-STATE-STORE-0000000006-repartition", 4);    expectedCreatedInternalTopics.put(applicationId + "-KTABLE-AGGREGATE-STATE-STORE-0000000006-changelog", 4);    expectedCreatedInternalTopics.put(applicationId + "-topic3-STATE-STORE-0000000002-changelog", 4);    expectedCreatedInternalTopics.put(applicationId + "-KSTREAM-MAP-0000000001-repartition", 4);    // check if all internal topics were created as expected    assertThat(mockInternalTopicManager.readyTopics, equalTo(expectedCreatedInternalTopics));    final List<TopicPartition> expectedAssignment = asList(new TopicPartition("topic1", 0), new TopicPartition("topic1", 1), new TopicPartition("topic1", 2), new TopicPartition("topic3", 0), new TopicPartition("topic3", 1), new TopicPartition("topic3", 2), new TopicPartition("topic3", 3), new TopicPartition(applicationId + "-KTABLE-AGGREGATE-STATE-STORE-0000000006-repartition", 0), new TopicPartition(applicationId + "-KTABLE-AGGREGATE-STATE-STORE-0000000006-repartition", 1), new TopicPartition(applicationId + "-KTABLE-AGGREGATE-STATE-STORE-0000000006-repartition", 2), new TopicPartition(applicationId + "-KTABLE-AGGREGATE-STATE-STORE-0000000006-repartition", 3), new TopicPartition(applicationId + "-KSTREAM-MAP-0000000001-repartition", 0), new TopicPartition(applicationId + "-KSTREAM-MAP-0000000001-repartition", 1), new TopicPartition(applicationId + "-KSTREAM-MAP-0000000001-repartition", 2), new TopicPartition(applicationId + "-KSTREAM-MAP-0000000001-repartition", 3));    // check if we created a task for all expected topicPartitions.    assertThat(new HashSet<>(assignment.get(client).partitions()), equalTo(new HashSet<>(expectedAssignment)));}
f17647
0
shouldAddUserDefinedEndPointToSubscription
public void kafkatest_f17648_0()
{    builder.setApplicationId(applicationId);    builder.addSource(null, "source", null, null, null, "input");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    builder.addSink("sink", "output", null, null, null, "processor");    final UUID uuid1 = UUID.randomUUID();    createMockTaskManager(emptyTasks, emptyTasks, uuid1, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint));    final Set<String> topics = Utils.mkSet("input");    final ConsumerPartitionAssignor.Subscription subscription = new ConsumerPartitionAssignor.Subscription(new ArrayList<>(topics), partitionAssignor.subscriptionUserData(topics));    final SubscriptionInfo subscriptionInfo = SubscriptionInfo.decode(subscription.userData());    assertEquals("localhost:8080", subscriptionInfo.userEndPoint());}
f17648
0
shouldThrowKafkaExceptionIfTaskMangerConfigIsNotTaskManagerInstance
public void kafkatest_f17656_0()
{    final Map<String, Object> config = configProps();    config.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, "i am not a task manager");    try {        partitionAssignor.configure(config);        fail("Should have thrown KafkaException");    } catch (final KafkaException expected) {        assertThat(expected.getMessage(), equalTo("java.lang.String is not an instance of org.apache.kafka.streams.processor.internals.TaskManager"));    }}
f17656
0
shouldThrowKafkaExceptionAssignmentErrorCodeNotConfigured
public void kafkatest_f17657_0()
{    createMockTaskManager();    final Map<String, Object> config = configProps();    config.remove(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE);    try {        partitionAssignor.configure(config);        fail("Should have thrown KafkaException");    } catch (final KafkaException expected) {        assertThat(expected.getMessage(), equalTo("assignmentErrorCode is not specified"));    }}
f17657
0
shouldThrowKafkaExceptionIfVersionProbingFlagConfigIsNotAtomicInteger
public void kafkatest_f17658_0()
{    createMockTaskManager();    final Map<String, Object> config = configProps();    config.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, "i am not an AtomicInteger");    try {        partitionAssignor.configure(config);        fail("Should have thrown KafkaException");    } catch (final KafkaException expected) {        assertThat(expected.getMessage(), equalTo("java.lang.String is not an instance of java.util.concurrent.atomic.AtomicInteger"));    }}
f17658
0
shouldDownGradeSubscriptionToVersion2For0110
public void kafkatest_f17666_0()
{    shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0110);}
f17666
0
shouldDownGradeSubscriptionToVersion2For10
public void kafkatest_f17667_0()
{    shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_10);}
f17667
0
shouldDownGradeSubscriptionToVersion2For11
public void kafkatest_f17668_0()
{    shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_11);}
f17668
0
checkAssignment
private AssignmentInfo kafkatest_f17676_0(final Set<String> expectedTopics, final ConsumerPartitionAssignor.Assignment assignment)
{    // This assumed 1) DefaultPartitionGrouper is used, and 2) there is an only one topic group.    final AssignmentInfo info = AssignmentInfo.decode(assignment.userData());    // check if the number of assigned partitions == the size of active task id list    assertEquals(assignment.partitions().size(), info.activeTasks().size());    // check if active tasks are consistent    final List<TaskId> activeTasks = new ArrayList<>();    final Set<String> activeTopics = new HashSet<>();    for (final TopicPartition partition : assignment.partitions()) {        // since default grouper, taskid.partition == partition.partition()        activeTasks.add(new TaskId(0, partition.partition()));        activeTopics.add(partition.topic());    }    assertEquals(activeTasks, info.activeTasks());    // check if active partitions cover all topics    assertEquals(expectedTopics, activeTopics);    // check if standby tasks are consistent    final Set<String> standbyTopics = new HashSet<>();    for (final Map.Entry<TaskId, Set<TopicPartition>> entry : info.standbyTasks().entrySet()) {        final TaskId id = entry.getKey();        final Set<TopicPartition> partitions = entry.getValue();        for (final TopicPartition partition : partitions) {            // since default grouper, taskid.partition == partition.partition()            assertEquals(id.partition, partition.partition());            standbyTopics.add(partition.topic());        }    }    if (info.standbyTasks().size() > 0) {        // check if standby partitions cover all topics        assertEquals(expectedTopics, standbyTopics);    }    return info;}
f17676
0
process
public void kafkatest_f17677_0(final Integer key, final Integer value)
{    throw new RuntimeException("KABOOM!");}
f17677
0
close
public void kafkatest_f17678_0()
{    throw new RuntimeException("KABOOM!");}
f17678
0
shouldHandleInitTransactionsTimeoutExceptionOnCreation
public void kafkatest_f17686_0()
{    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    final ProcessorTopology topology = withSources(asList(source1, source2, processorStreamTime, processorSystemTime), mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source2)));    source1.addChild(processorStreamTime);    source2.addChild(processorStreamTime);    source1.addChild(processorSystemTime);    source2.addChild(processorSystemTime);    try {        new StreamTask(taskId00, partitions, topology, consumer, changelogReader, createConfig(true), streamsMetrics, stateDirectory, null, time, () -> producer = new MockProducer<byte[], byte[]>(false, bytesSerializer, bytesSerializer) {            @Override            public void initTransactions() {                throw new TimeoutException("test");            }        }, null);        fail("Expected an exception");    } catch (final StreamsException expected) {        // make sure we log the explanation as an ERROR        assertTimeoutErrorLog(appender);        // make sure we report the correct message        assertThat(expected.getMessage(), is("task [0_0] Failed to initialize task 0_0 due to timeout."));        // make sure we preserve the cause        assertEquals(expected.getCause().getClass(), TimeoutException.class);        assertThat(expected.getCause().getMessage(), is("test"));    }    LogCaptureAppender.unregister(appender);}
f17686
0
initTransactions
public void kafkatest_f17687_0()
{    throw new TimeoutException("test");}
f17687
0
shouldHandleInitTransactionsTimeoutExceptionOnResume
public void kafkatest_f17688_0()
{    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    final ProcessorTopology topology = withSources(asList(source1, source2, processorStreamTime, processorSystemTime), mkMap(mkEntry(topic1, (SourceNode) source1), mkEntry(topic2, (SourceNode) source2)));    source1.addChild(processorStreamTime);    source2.addChild(processorStreamTime);    source1.addChild(processorSystemTime);    source2.addChild(processorSystemTime);    final AtomicBoolean timeOut = new AtomicBoolean(false);    final StreamTask testTask = new StreamTask(taskId00, partitions, topology, consumer, changelogReader, createConfig(true), streamsMetrics, stateDirectory, null, time, () -> producer = new MockProducer<byte[], byte[]>(false, bytesSerializer, bytesSerializer) {        @Override        public void initTransactions() {            if (timeOut.get()) {                throw new TimeoutException("test");            } else {                super.initTransactions();            }        }    }, null);    testTask.initializeTopology();    testTask.suspend();    timeOut.set(true);    try {        testTask.resume();        fail("Expected an exception");    } catch (final StreamsException expected) {        // make sure we log the explanation as an ERROR        assertTimeoutErrorLog(appender);        // make sure we report the correct message        assertThat(expected.getMessage(), is("task [0_0] Failed to initialize task 0_0 due to timeout."));        // make sure we preserve the cause        assertEquals(expected.getCause().getClass(), TimeoutException.class);        assertThat(expected.getCause().getMessage(), is("test"));    }    LogCaptureAppender.unregister(appender);}
f17688
0
shouldRespectPunctuateCancellationStreamTime
public void kafkatest_f17696_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    task.addRecords(partition1, asList(getConsumerRecord(partition1, 20), getConsumerRecord(partition1, 30), getConsumerRecord(partition1, 40)));    task.addRecords(partition2, asList(getConsumerRecord(partition2, 25), getConsumerRecord(partition2, 35), getConsumerRecord(partition2, 45)));    assertFalse(task.maybePunctuateStreamTime());    // st is now 20    assertTrue(task.process());    assertTrue(task.maybePunctuateStreamTime());    // st is now 25    assertTrue(task.process());    assertFalse(task.maybePunctuateStreamTime());    // st is now 30    assertTrue(task.process());    processorStreamTime.mockProcessor.scheduleCancellable.cancel();    assertFalse(task.maybePunctuateStreamTime());    processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 20L);}
f17696
0
shouldRespectPunctuateCancellationSystemTime
public void kafkatest_f17697_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    final long now = time.milliseconds();    time.sleep(10);    assertTrue(task.maybePunctuateSystemTime());    processorSystemTime.mockProcessor.scheduleCancellable.cancel();    time.sleep(10);    assertFalse(task.maybePunctuateSystemTime());    processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 10);}
f17697
0
shouldRespectCommitNeeded
public void kafkatest_f17698_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    assertFalse(task.commitNeeded());    task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0)));    assertTrue(task.process());    assertTrue(task.commitNeeded());    task.commit();    assertFalse(task.commitNeeded());    assertTrue(task.maybePunctuateStreamTime());    assertTrue(task.commitNeeded());    task.commit();    assertFalse(task.commitNeeded());    time.sleep(10);    assertTrue(task.maybePunctuateSystemTime());    assertTrue(task.commitNeeded());    task.commit();    assertFalse(task.commitNeeded());}
f17698
0
shouldWrapKafkaExceptionsWithStreamsExceptionAndAddContextWhenPunctuatingStreamTime
public void kafkatest_f17706_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    try {        task.punctuate(processorStreamTime, 1, PunctuationType.STREAM_TIME, new Punctuator() {            @Override            public void punctuate(final long timestamp) {                throw new KafkaException("KABOOM!");            }        });        fail("Should've thrown StreamsException");    } catch (final StreamsException e) {        final String message = e.getMessage();        assertTrue("message=" + message + " should contain processor", message.contains("processor '" + processorStreamTime.name() + "'"));        assertThat(task.processorContext.currentNode(), nullValue());    }}
f17706
0
punctuate
public void kafkatest_f17707_0(final long timestamp)
{    throw new KafkaException("KABOOM!");}
f17707
0
shouldWrapKafkaExceptionsWithStreamsExceptionAndAddContextWhenPunctuatingWallClockTimeTime
public void kafkatest_f17708_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    try {        task.punctuate(processorSystemTime, 1, PunctuationType.WALL_CLOCK_TIME, new Punctuator() {            @Override            public void punctuate(final long timestamp) {                throw new KafkaException("KABOOM!");            }        });        fail("Should've thrown StreamsException");    } catch (final StreamsException e) {        final String message = e.getMessage();        assertTrue("message=" + message + " should contain processor", message.contains("processor '" + processorSystemTime.name() + "'"));        assertThat(task.processorContext.currentNode(), nullValue());    }}
f17708
0
shouldSetProcessorNodeOnContextBackToNullAfterSuccessfulPunctuate
public void kafkatest_f17716_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    task.punctuate(processorStreamTime, 5, PunctuationType.STREAM_TIME, punctuator);    assertThat(((ProcessorContextImpl) task.context()).currentNode(), nullValue());}
f17716
0
shouldThrowIllegalStateExceptionOnScheduleIfCurrentNodeIsNull
public void kafkatest_f17717_0()
{    task = createStatelessTask(createConfig(false));    task.schedule(1, PunctuationType.STREAM_TIME, new Punctuator() {        @Override        public void punctuate(final long timestamp) {        // no-op        }    });}
f17717
0
punctuate
public void kafkatest_f17718_0(final long timestamp)
{// no-op}
f17718
0
shouldNotAbortTransactionAndNotCloseProducerOnErrorDuringCleanCloseWithEosEnabled
public void kafkatest_f17726_0()
{    task = createTaskThatThrowsException(true);    task.initializeTopology();    try {        task.close(true, false);        fail("should have thrown runtime exception");    } catch (final RuntimeException expected) {        task = null;    }    assertTrue(producer.transactionInFlight());    assertFalse(producer.closed());}
f17726
0
shouldOnlyCloseProducerIfFencedOnCommitDuringCleanCloseWithEosEnabled
public void kafkatest_f17727_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    producer.fenceProducer();    try {        task.close(true, false);        fail("should have thrown TaskMigratedException");    } catch (final TaskMigratedException expected) {        task = null;        assertTrue(expected.getCause() instanceof ProducerFencedException);    }    assertFalse(producer.transactionCommitted());    assertTrue(producer.transactionInFlight());    assertFalse(producer.transactionAborted());    assertFalse(producer.transactionCommitted());    assertTrue(producer.closed());}
f17727
0
shouldNotCloseProducerIfFencedOnCloseDuringCleanCloseWithEosEnabled
public void kafkatest_f17728_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    producer.fenceProducerOnClose();    try {        task.close(true, false);        fail("should have thrown TaskMigratedException");    } catch (final TaskMigratedException expected) {        task = null;        assertTrue(expected.getCause() instanceof ProducerFencedException);    }    assertTrue(producer.transactionCommitted());    assertFalse(producer.transactionInFlight());    assertFalse(producer.closed());}
f17728
0
shouldWrapProducerFencedExceptionWithTaskMigratedExceptionForBeginTransaction
public void kafkatest_f17736_0()
{    task = createStatelessTask(createConfig(true));    producer.fenceProducer();    try {        task.initializeTopology();        fail("Should have throws TaskMigratedException");    } catch (final TaskMigratedException expected) {        assertTrue(expected.getCause() instanceof ProducerFencedException);    }}
f17736
0
shouldNotThrowOnCloseIfTaskWasNotInitializedWithEosEnabled
public void kafkatest_f17737_0()
{    task = createStatelessTask(createConfig(true));    assertFalse(producer.transactionInFlight());    task.close(false, false);}
f17737
0
shouldNotInitOrBeginTransactionOnCreateIfEosDisabled
public void kafkatest_f17738_0()
{    task = createStatelessTask(createConfig(false));    assertFalse(producer.transactionInitialized());    assertFalse(producer.transactionInFlight());}
f17738
0
shouldStartNewTransactionOnCommitIfEosEnabled
public void kafkatest_f17746_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0)));    task.process();    task.commit();    assertTrue(producer.transactionInFlight());}
f17746
0
shouldNotStartNewTransactionOnCommitIfEosDisabled
public void kafkatest_f17747_0()
{    task = createStatelessTask(createConfig(false));    task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0)));    task.process();    task.commit();    assertFalse(producer.transactionInFlight());}
f17747
0
shouldNotAbortTransactionOnZombieClosedIfEosEnabled
public void kafkatest_f17748_0()
{    task = createStatelessTask(createConfig(true));    task.close(false, true);    task = null;    assertFalse(producer.transactionAborted());}
f17748
0
shouldNotCloseTopologyProcessorNodesIfNotInitialized
public void kafkatest_f17756_0()
{    final StreamTask task = createTaskThatThrowsException(false);    try {        task.close(false, false);    } catch (final Exception e) {        fail("should have not closed non-initialized topology");    }}
f17756
0
shouldBeInitializedIfChangelogPartitionsIsEmpty
public void kafkatest_f17757_0()
{    final StreamTask task = createStatefulTask(createConfig(false), false);    assertTrue(task.initializeStateStores());}
f17757
0
shouldNotBeInitializedIfChangelogPartitionsIsNonEmpty
public void kafkatest_f17758_0()
{    final StreamTask task = createStatefulTask(createConfig(false), true);    assertFalse(task.initializeStateStores());}
f17758
0
mockConsumerWithCommittedException
private Consumer<byte[], byte[]> kafkatest_f17766_0(final RuntimeException toThrow)
{    return new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {        @Override        public OffsetAndMetadata committed(final TopicPartition partition) {            throw toThrow;        }    };}
f17766
0
committed
public OffsetAndMetadata kafkatest_f17767_0(final TopicPartition partition)
{    throw toThrow;}
f17767
0
createOptimizedStatefulTask
private StreamTask kafkatest_f17768_0(final StreamsConfig config, final Consumer<byte[], byte[]> consumer)
{    final StateStore stateStore = new MockKeyValueStore(storeName, true);    final ProcessorTopology topology = ProcessorTopologyFactories.with(asList(source1), mkMap(mkEntry(topic1, source1)), singletonList(stateStore), Collections.singletonMap(storeName, topic1));    return new StreamTask(taskId00, mkSet(partition1), topology, consumer, changelogReader, config, streamsMetrics, stateDirectory, null, time, () -> producer = new MockProducer<>(false, bytesSerializer, bytesSerializer));}
f17768
0
configProps
private Properties kafkatest_f17776_0(final boolean enableEoS)
{    return mkProperties(mkMap(mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, applicationId), mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:2171"), mkEntry(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, "3"), mkEntry(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName()), mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath()), mkEntry(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, enableEoS ? StreamsConfig.EXACTLY_ONCE : StreamsConfig.AT_LEAST_ONCE)));}
f17776
0
testPartitionAssignmentChangeForSingleGroup
public void kafkatest_f17777_0()
{    internalTopologyBuilder.addSource(null, "source1", null, null, null, topic1);    final StreamThread thread = createStreamThread(clientId, config, false);    final StateListenerStub stateListener = new StateListenerStub();    thread.setStateListener(stateListener);    assertEquals(thread.state(), StreamThread.State.CREATED);    final ConsumerRebalanceListener rebalanceListener = thread.rebalanceListener;    final List<TopicPartition> revokedPartitions;    final List<TopicPartition> assignedPartitions;    // revoke nothing    thread.setState(StreamThread.State.STARTING);    revokedPartitions = Collections.emptyList();    rebalanceListener.onPartitionsRevoked(revokedPartitions);    assertEquals(thread.state(), StreamThread.State.PARTITIONS_REVOKED);    // assign single partition    assignedPartitions = singletonList(t1p1);    thread.taskManager().setAssignmentMetadata(Collections.emptyMap(), Collections.emptyMap());    final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;    mockConsumer.assign(assignedPartitions);    mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));    rebalanceListener.onPartitionsAssigned(assignedPartitions);    thread.runOnce();    assertEquals(thread.state(), StreamThread.State.RUNNING);    Assert.assertEquals(4, stateListener.numChanges);    Assert.assertEquals(StreamThread.State.PARTITIONS_ASSIGNED, stateListener.oldState);    thread.shutdown();    assertSame(StreamThread.State.PENDING_SHUTDOWN, thread.state());}
f17777
0
testStateChangeStartClose
public void kafkatest_f17778_0() throws Exception
{    final StreamThread thread = createStreamThread(clientId, config, false);    final StateListenerStub stateListener = new StateListenerStub();    thread.setStateListener(stateListener);    thread.start();    TestUtils.waitForCondition(() -> thread.state() == StreamThread.State.STARTING, 10 * 1000, "Thread never started.");    thread.shutdown();    TestUtils.waitForCondition(() -> thread.state() == StreamThread.State.DEAD, 10 * 1000, "Thread never shut down.");    thread.shutdown();    assertEquals(thread.state(), StreamThread.State.DEAD);}
f17778
0
mockTaskManagerCommit
private TaskManager kafkatest_f17786_0(final Consumer<byte[], byte[]> consumer, final int numberOfCommits, final int commits)
{    final TaskManager taskManager = EasyMock.createNiceMock(TaskManager.class);    EasyMock.expect(taskManager.commitAll()).andReturn(commits).times(numberOfCommits);    EasyMock.replay(taskManager, consumer);    return taskManager;}
f17786
0
shouldInjectSharedProducerForAllTasksUsingClientSupplierOnCreateIfEosDisabled
public void kafkatest_f17787_0()
{    internalTopologyBuilder.addSource(null, "source1", null, null, null, topic1);    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread(clientId, config, false);    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(Collections.emptyList());    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    final List<TopicPartition> assignedPartitions = new ArrayList<>();    // assign single partition    assignedPartitions.add(t1p1);    assignedPartitions.add(t1p2);    activeTasks.put(task1, Collections.singleton(t1p1));    activeTasks.put(task2, Collections.singleton(t1p2));    thread.taskManager().setAssignmentMetadata(activeTasks, Collections.emptyMap());    final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;    mockConsumer.assign(assignedPartitions);    final Map<TopicPartition, Long> beginOffsets = new HashMap<>();    beginOffsets.put(t1p1, 0L);    beginOffsets.put(t1p2, 0L);    mockConsumer.updateBeginningOffsets(beginOffsets);    thread.rebalanceListener.onPartitionsAssigned(new HashSet<>(assignedPartitions));    assertEquals(1, clientSupplier.producers.size());    final Producer globalProducer = clientSupplier.producers.get(0);    for (final Task task : thread.tasks().values()) {        assertSame(globalProducer, ((RecordCollectorImpl) ((StreamTask) task).recordCollector()).producer());    }    assertSame(clientSupplier.consumer, thread.consumer);    assertSame(clientSupplier.restoreConsumer, thread.restoreConsumer);}
f17787
0
shouldInjectProducerPerTaskUsingClientSupplierOnCreateIfEosEnable
public void kafkatest_f17788_0()
{    internalTopologyBuilder.addSource(null, "source1", null, null, null, topic1);    final StreamThread thread = createStreamThread(clientId, new StreamsConfig(configProps(true)), true);    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(Collections.emptyList());    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    final List<TopicPartition> assignedPartitions = new ArrayList<>();    // assign single partition    assignedPartitions.add(t1p1);    assignedPartitions.add(t1p2);    activeTasks.put(task1, Collections.singleton(t1p1));    activeTasks.put(task2, Collections.singleton(t1p2));    thread.taskManager().setAssignmentMetadata(activeTasks, Collections.emptyMap());    final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;    mockConsumer.assign(assignedPartitions);    final Map<TopicPartition, Long> beginOffsets = new HashMap<>();    beginOffsets.put(t1p1, 0L);    beginOffsets.put(t1p2, 0L);    mockConsumer.updateBeginningOffsets(beginOffsets);    thread.rebalanceListener.onPartitionsAssigned(new HashSet<>(assignedPartitions));    thread.runOnce();    assertEquals(thread.tasks().size(), clientSupplier.producers.size());    assertSame(clientSupplier.consumer, thread.consumer);    assertSame(clientSupplier.restoreConsumer, thread.restoreConsumer);}
f17788
0
setStreamThread
private void kafkatest_f17796_0(final StreamThread streamThread)
{    this.streamThread = streamThread;}
f17796
0
shouldOnlyShutdownOnce
public void kafkatest_f17797_0()
{    final Consumer<byte[], byte[]> consumer = EasyMock.createNiceMock(Consumer.class);    final TaskManager taskManager = EasyMock.createNiceMock(TaskManager.class);    taskManager.shutdown(true);    EasyMock.expectLastCall();    EasyMock.replay(taskManager, consumer);    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId, StreamsConfig.METRICS_LATEST);    final StreamThread thread = new StreamThread(mockTime, config, null, consumer, consumer, null, taskManager, streamsMetrics, internalTopologyBuilder, clientId, new LogContext(""), new AtomicInteger()).updateThreadMetadata(getSharedAdminClientId(clientId));    thread.shutdown();    // Execute the run method. Verification of the mock will check that shutdown was only done once    thread.run();    EasyMock.verify(taskManager);}
f17797
0
shouldNotNullPointerWhenStandbyTasksAssignedAndNoStateStoresForTopology
public void kafkatest_f17798_0()
{    internalTopologyBuilder.addSource(null, "name", null, null, null, "topic");    internalTopologyBuilder.addSink("out", "output", null, null, null, "name");    final StreamThread thread = createStreamThread(clientId, config, false);    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(Collections.emptyList());    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    // assign single partition    standbyTasks.put(task1, Collections.singleton(t1p1));    thread.taskManager().setAssignmentMetadata(Collections.emptyMap(), standbyTasks);    thread.taskManager().createTasks(Collections.emptyList());    thread.rebalanceListener.onPartitionsAssigned(Collections.emptyList());}
f17798
0
shouldCreateStandbyTask
public void kafkatest_f17806_0()
{    setupInternalTopologyWithoutState();    internalTopologyBuilder.addStateStore(new MockKeyValueStoreBuilder("myStore", true), "processor1");    final StandbyTask standbyTask = createStandbyTask();    assertThat(standbyTask, not(nullValue()));}
f17806
0
shouldNotCreateStandbyTaskWithoutStateStores
public void kafkatest_f17807_0()
{    setupInternalTopologyWithoutState();    final StandbyTask standbyTask = createStandbyTask();    assertThat(standbyTask, nullValue());}
f17807
0
shouldNotCreateStandbyTaskIfStateStoresHaveLoggingDisabled
public void kafkatest_f17808_0()
{    setupInternalTopologyWithoutState();    final StoreBuilder storeBuilder = new MockKeyValueStoreBuilder("myStore", true);    storeBuilder.withLoggingDisabled();    internalTopologyBuilder.addStateStore(storeBuilder, "processor1");    final StandbyTask standbyTask = createStandbyTask();    assertThat(standbyTask, nullValue());}
f17808
0
partitions
public Set<TopicPartition> kafkatest_f17818_0()
{    return changelogPartitionSet;}
f17818
0
shouldRecordSkippedMetricForDeserializationException
public void kafkatest_f17819_0()
{    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    internalTopologyBuilder.addSource(null, "source1", null, null, null, topic1);    final Properties config = configProps(false);    config.setProperty(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, LogAndContinueExceptionHandler.class.getName());    config.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass().getName());    final StreamThread thread = createStreamThread(clientId, new StreamsConfig(config), false);    thread.setState(StreamThread.State.STARTING);    thread.setState(StreamThread.State.PARTITIONS_REVOKED);    final Set<TopicPartition> assignedPartitions = Collections.singleton(t1p1);    thread.taskManager().setAssignmentMetadata(Collections.singletonMap(new TaskId(0, t1p1.partition()), assignedPartitions), Collections.emptyMap());    final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;    mockConsumer.assign(Collections.singleton(t1p1));    mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));    thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);    thread.runOnce();    final MetricName skippedTotalMetric = metrics.metricName("skipped-records-total", "stream-metrics", Collections.singletonMap("client-id", thread.getName()));    final MetricName skippedRateMetric = metrics.metricName("skipped-records-rate", "stream-metrics", Collections.singletonMap("client-id", thread.getName()));    assertEquals(0.0, metrics.metric(skippedTotalMetric).metricValue());    assertEquals(0.0, metrics.metric(skippedRateMetric).metricValue());    long offset = -1;    mockConsumer.addRecord(new ConsumerRecord<>(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, ConsumerRecord.NULL_CHECKSUM, -1, -1, new byte[0], "I am not an integer.".getBytes()));    mockConsumer.addRecord(new ConsumerRecord<>(t1p1.topic(), t1p1.partition(), ++offset, -1, TimestampType.CREATE_TIME, ConsumerRecord.NULL_CHECKSUM, -1, -1, new byte[0], "I am not an integer.".getBytes()));    thread.runOnce();    assertEquals(2.0, metrics.metric(skippedTotalMetric).metricValue());    assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());    LogCaptureAppender.unregister(appender);    final List<String> strings = appender.getMessages();    assertTrue(strings.contains("task [0_1] Skipping record due to deserialization error. topic=[topic1] partition=[1] offset=[0]"));    assertTrue(strings.contains("task [0_1] Skipping record due to deserialization error. topic=[topic1] partition=[1] offset=[1]"));}
f17819
0
shouldReportSkippedRecordsForInvalidTimestamps
public void kafkatest_f17820_0()
{    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    internalTopologyBuilder.addSource(null, "source1", null, null, null, topic1);    final Properties config = configProps(false);    config.setProperty(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, LogAndSkipOnInvalidTimestamp.class.getName());    final StreamThread thread = createStreamThread(clientId, new StreamsConfig(config), false);    thread.setState(StreamThread.State.STARTING);    thread.setState(StreamThread.State.PARTITIONS_REVOKED);    final Set<TopicPartition> assignedPartitions = Collections.singleton(t1p1);    thread.taskManager().setAssignmentMetadata(Collections.singletonMap(new TaskId(0, t1p1.partition()), assignedPartitions), Collections.emptyMap());    final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;    mockConsumer.assign(Collections.singleton(t1p1));    mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));    thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);    thread.runOnce();    final MetricName skippedTotalMetric = metrics.metricName("skipped-records-total", "stream-metrics", Collections.singletonMap("client-id", thread.getName()));    final MetricName skippedRateMetric = metrics.metricName("skipped-records-rate", "stream-metrics", Collections.singletonMap("client-id", thread.getName()));    assertEquals(0.0, metrics.metric(skippedTotalMetric).metricValue());    assertEquals(0.0, metrics.metric(skippedRateMetric).metricValue());    long offset = -1;    addRecord(mockConsumer, ++offset);    addRecord(mockConsumer, ++offset);    thread.runOnce();    assertEquals(2.0, metrics.metric(skippedTotalMetric).metricValue());    assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());    addRecord(mockConsumer, ++offset);    addRecord(mockConsumer, ++offset);    addRecord(mockConsumer, ++offset);    addRecord(mockConsumer, ++offset);    thread.runOnce();    assertEquals(6.0, metrics.metric(skippedTotalMetric).metricValue());    assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());    addRecord(mockConsumer, ++offset, 1L);    addRecord(mockConsumer, ++offset, 1L);    thread.runOnce();    assertEquals(6.0, metrics.metric(skippedTotalMetric).metricValue());    assertNotEquals(0.0, metrics.metric(skippedRateMetric).metricValue());    LogCaptureAppender.unregister(appender);    final List<String> strings = appender.getMessages();    assertTrue(strings.contains("task [0_1] Skipping record due to negative extracted timestamp. " + "topic=[topic1] partition=[1] offset=[0] extractedTimestamp=[-1] " + "extractor=[org.apache.kafka.streams.processor.LogAndSkipOnInvalidTimestamp]"));    assertTrue(strings.contains("task [0_1] Skipping record due to negative extracted timestamp. " + "topic=[topic1] partition=[1] offset=[1] extractedTimestamp=[-1] " + "extractor=[org.apache.kafka.streams.processor.LogAndSkipOnInvalidTimestamp]"));    assertTrue(strings.contains("task [0_1] Skipping record due to negative extracted timestamp. " + "topic=[topic1] partition=[1] offset=[2] extractedTimestamp=[-1] " + "extractor=[org.apache.kafka.streams.processor.LogAndSkipOnInvalidTimestamp]"));    assertTrue(strings.contains("task [0_1] Skipping record due to negative extracted timestamp. " + "topic=[topic1] partition=[1] offset=[3] extractedTimestamp=[-1] " + "extractor=[org.apache.kafka.streams.processor.LogAndSkipOnInvalidTimestamp]"));    assertTrue(strings.contains("task [0_1] Skipping record due to negative extracted timestamp. " + "topic=[topic1] partition=[1] offset=[4] extractedTimestamp=[-1] " + "extractor=[org.apache.kafka.streams.processor.LogAndSkipOnInvalidTimestamp]"));    assertTrue(strings.contains("task [0_1] Skipping record due to negative extracted timestamp. " + "topic=[topic1] partition=[1] offset=[5] extractedTimestamp=[-1] " + "extractor=[org.apache.kafka.streams.processor.LogAndSkipOnInvalidTimestamp]"));}
f17820
0
shouldUpdateSubscriptionFromAssignment
public void kafkatest_f17828_0()
{    mockTopologyBuilder();    expect(subscriptionUpdates.getUpdates()).andReturn(Utils.mkSet(topic1));    topologyBuilder.updateSubscribedTopics(EasyMock.eq(Utils.mkSet(topic1, topic2)), EasyMock.anyString());    expectLastCall().once();    EasyMock.replay(activeTaskCreator, topologyBuilder, subscriptionUpdates);    taskManager.updateSubscriptionsFromAssignment(asList(t1p1, t2p1));    EasyMock.verify(activeTaskCreator, topologyBuilder, subscriptionUpdates);}
f17828
0
shouldNotUpdateSubscriptionFromAssignment
public void kafkatest_f17829_0()
{    mockTopologyBuilder();    expect(subscriptionUpdates.getUpdates()).andReturn(Utils.mkSet(topic1, topic2));    EasyMock.replay(activeTaskCreator, topologyBuilder, subscriptionUpdates);    taskManager.updateSubscriptionsFromAssignment(asList(t1p1));    EasyMock.verify(activeTaskCreator, topologyBuilder, subscriptionUpdates);}
f17829
0
shouldUpdateSubscriptionFromMetadata
public void kafkatest_f17830_0()
{    mockTopologyBuilder();    expect(subscriptionUpdates.getUpdates()).andReturn(Utils.mkSet(topic1));    topologyBuilder.updateSubscribedTopics(EasyMock.eq(Utils.mkSet(topic1, topic2)), EasyMock.anyString());    expectLastCall().once();    EasyMock.replay(activeTaskCreator, topologyBuilder, subscriptionUpdates);    taskManager.updateSubscriptionsFromMetadata(Utils.mkSet(topic1, topic2));    EasyMock.verify(activeTaskCreator, topologyBuilder, subscriptionUpdates);}
f17830
0
shouldNotAddResumedStandbyTasks
public void kafkatest_f17838_0()
{    checkOrder(active, true);    expect(standby.maybeResumeSuspendedTask(taskId0, taskId0Partitions)).andReturn(true);    replay();    taskManager.setAssignmentMetadata(Collections.<TaskId, Set<TopicPartition>>emptyMap(), taskId0Assignment);    taskManager.createTasks(taskId0Partitions);    // should be no calls to standbyTaskCreator and no calls to standby.addNewTasks(..)    verify(standby, standbyTaskCreator);}
f17838
0
shouldPauseActivePartitions
public void kafkatest_f17839_0()
{    mockSingleActiveTask();    consumer.pause(taskId0Partitions);    expectLastCall();    replay();    taskManager.setAssignmentMetadata(taskId0Assignment, Collections.<TaskId, Set<TopicPartition>>emptyMap());    taskManager.createTasks(taskId0Partitions);    verify(consumer);}
f17839
0
shouldSuspendActiveTasks
public void kafkatest_f17840_0()
{    expect(active.suspend()).andReturn(null);    replay();    taskManager.suspendTasksAndState();    verify(active);}
f17840
0
shouldInitializeNewStandbyTasks
public void kafkatest_f17848_0()
{    active.updateRestored(EasyMock.<Collection<TopicPartition>>anyObject());    expectLastCall();    replay();    taskManager.updateNewAndRestoringTasks();    verify(standby);}
f17848
0
shouldRestoreStateFromChangeLogReader
public void kafkatest_f17849_0()
{    expect(changeLogReader.restore(active)).andReturn(taskId0Partitions);    active.updateRestored(taskId0Partitions);    expectLastCall();    replay();    taskManager.updateNewAndRestoringTasks();    verify(changeLogReader, active);}
f17849
0
shouldResumeRestoredPartitions
public void kafkatest_f17850_0()
{    expect(changeLogReader.restore(active)).andReturn(taskId0Partitions);    expect(active.allTasksRunning()).andReturn(true);    expect(consumer.assignment()).andReturn(taskId0Partitions);    expect(standby.running()).andReturn(Collections.<StandbyTask>emptySet());    consumer.resume(taskId0Partitions);    expectLastCall();    replay();    taskManager.updateNewAndRestoringTasks();    verify(consumer);}
f17850
0
shouldPropagateExceptionFromActiveCommit
public void kafkatest_f17858_0()
{    // upgrade to strict mock to ensure no calls    checkOrder(standby, true);    active.commit();    expectLastCall().andThrow(new RuntimeException(""));    replay();    try {        taskManager.commitAll();        fail("should have thrown first exception");    } catch (final Exception e) {    // ok    }    verify(active, standby);}
f17858
0
shouldPropagateExceptionFromStandbyCommit
public void kafkatest_f17859_0()
{    expect(standby.commit()).andThrow(new RuntimeException(""));    replay();    try {        taskManager.commitAll();        fail("should have thrown exception");    } catch (final Exception e) {    // ok    }    verify(standby);}
f17859
0
shouldSendPurgeData
public void kafkatest_f17860_0()
{    final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();    final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));    final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, (KafkaFuture<DeletedRecords>) futureDeletedRecords));    futureDeletedRecords.complete(null);    expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).times(2);    expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).times(2);    replay();    taskManager.maybePurgeCommitedRecords();    taskManager.maybePurgeCommitedRecords();    verify(active, adminClient);}
f17860
0
mockAssignStandbyPartitions
private void kafkatest_f17868_0(final long offset)
{    final StandbyTask task = EasyMock.createNiceMock(StandbyTask.class);    expect(active.allTasksRunning()).andReturn(true);    expect(standby.running()).andReturn(Collections.singletonList(task));    expect(task.checkpointedOffsets()).andReturn(Collections.singletonMap(t1p0, offset));    restoreConsumer.assign(taskId0Partitions);    expectLastCall();    EasyMock.replay(task);}
f17868
0
mockStandbyTaskExpectations
private void kafkatest_f17869_0()
{    expect(standbyTaskCreator.createTasks(EasyMock.<Consumer<byte[], byte[]>>anyObject(), EasyMock.eq(taskId0Assignment))).andReturn(Collections.singletonList(standbyTask));}
f17869
0
mockSingleActiveTask
private void kafkatest_f17870_0()
{    expect(activeTaskCreator.createTasks(EasyMock.<Consumer<byte[], byte[]>>anyObject(), EasyMock.eq(taskId0Assignment))).andReturn(Collections.singletonList(streamTask));}
f17870
0
append
protected void kafkatest_f17878_0(final LoggingEvent event)
{    synchronized (events) {        events.add(event);    }}
f17878
0
getMessages
public List<String> kafkatest_f17879_0()
{    final LinkedList<String> result = new LinkedList<>();    synchronized (events) {        for (final LoggingEvent event : events) {            result.add(event.getRenderedMessage());        }    }    return result;}
f17879
0
getEvents
public List<Event> kafkatest_f17880_0()
{    final LinkedList<Event> result = new LinkedList<>();    synchronized (events) {        for (final LoggingEvent event : events) {            final String[] throwableStrRep = event.getThrowableStrRep();            final Optional<String> throwableString;            if (throwableStrRep == null) {                throwableString = Optional.empty();            } else {                final StringBuilder throwableStringBuilder = new StringBuilder();                for (final String s : throwableStrRep) {                    throwableStringBuilder.append(s);                }                throwableString = Optional.of(throwableStringBuilder.toString());            }            result.add(new Event(event.getLevel().toString(), event.getRenderedMessage(), throwableString));        }    }    return result;}
f17880
0
logAndSkipOnInvalidTimestamp
public void kafkatest_f17889_0()
{    final long invalidMetadataTimestamp = -42;    final TimestampExtractor extractor = new LogAndSkipOnInvalidTimestamp();    final long timestamp = extractor.extract(new ConsumerRecord<>("anyTopic", 0, 0, invalidMetadataTimestamp, TimestampType.NO_TIMESTAMP_TYPE, 0, 0, 0, null, null), 0);    assertThat(timestamp, is(invalidMetadataTimestamp));}
f17889
0
testExtractMetadataTimestamp
 void kafkatest_f17890_0(final TimestampExtractor extractor)
{    final long metadataTimestamp = 42;    final long timestamp = extractor.extract(new ConsumerRecord<>("anyTopic", 0, 0, metadataTimestamp, TimestampType.NO_TIMESTAMP_TYPE, 0, 0, 0, null, null), 0);    assertThat(timestamp, is(metadataTimestamp));}
f17890
0
extractMetadataTimestamp
public void kafkatest_f17891_0()
{    testExtractMetadataTimestamp(new UsePreviousTimeOnInvalidTimestamp());}
f17891
0
shouldNotIncludeDeletedFromRangeResult
public void kafkatest_f17901_0()
{    store.close();    final Serializer<String> serializer = new StringSerializer() {        private int numCalls = 0;        @Override        public byte[] serialize(final String topic, final String data) {            if (++numCalls > 3) {                fail("Value serializer is called; it should never happen");            }            return super.serialize(topic, data);        }    };    context.setValueSerde(Serdes.serdeFrom(serializer, new StringDeserializer()));    store = createKeyValueStore(driver.context());    store.put(0, "zero");    store.put(1, "one");    store.put(2, "two");    store.delete(0);    store.delete(1);    // should not include deleted records in iterator    final Map<Integer, String> expectedContents = Collections.singletonMap(2, "two");    assertEquals(expectedContents, getContents(store.all()));}
f17901
0
serialize
public byte[] kafkatest_f17902_0(final String topic, final String data)
{    if (++numCalls > 3) {        fail("Value serializer is called; it should never happen");    }    return super.serialize(topic, data);}
f17902
0
shouldDeleteIfSerializedValueIsNull
public void kafkatest_f17903_0()
{    store.close();    final Serializer<String> serializer = new StringSerializer() {        @Override        public byte[] serialize(final String topic, final String data) {            if (data.equals("null")) {                // will be serialized to null bytes, indicating deletes                return null;            }            return super.serialize(topic, data);        }    };    context.setValueSerde(Serdes.serdeFrom(serializer, new StringDeserializer()));    store = createKeyValueStore(driver.context());    store.put(0, "zero");    store.put(1, "one");    store.put(2, "two");    store.put(0, "null");    store.put(1, "null");    // should not include deleted records in iterator    final Map<Integer, String> expectedContents = Collections.singletonMap(2, "two");    assertEquals(expectedContents, getContents(store.all()));}
f17903
0
shouldNotThrowNullPointerExceptionOnPutNullValue
public void kafkatest_f17911_0()
{    store.put(1, null);}
f17911
0
shouldThrowNullPointerExceptionOnPutIfAbsentNullKey
public void kafkatest_f17912_0()
{    store.putIfAbsent(null, "anyValue");}
f17912
0
shouldNotThrowNullPointerExceptionOnPutIfAbsentNullValue
public void kafkatest_f17913_0()
{    store.putIfAbsent(1, null);}
f17913
0
shouldPutAll
public void kafkatest_f17921_0()
{    final List<KeyValue<Integer, String>> entries = new ArrayList<>();    entries.add(new KeyValue<>(1, "one"));    entries.add(new KeyValue<>(2, "two"));    store.putAll(entries);    final List<KeyValue<Integer, String>> allReturned = new ArrayList<>();    final List<KeyValue<Integer, String>> expectedReturned = Arrays.asList(KeyValue.pair(1, "one"), KeyValue.pair(2, "two"));    final Iterator<KeyValue<Integer, String>> iterator = store.all();    while (iterator.hasNext()) {        allReturned.add(iterator.next());    }    assertThat(allReturned, equalTo(expectedReturned));}
f17921
0
shouldDeleteFromStore
public void kafkatest_f17922_0()
{    store.put(1, "one");    store.put(2, "two");    store.delete(2);    assertNull(store.get(2));}
f17922
0
shouldReturnSameResultsForGetAndRangeWithEqualKeys
public void kafkatest_f17923_0()
{    final List<KeyValue<Integer, String>> entries = new ArrayList<>();    entries.add(new KeyValue<>(1, "one"));    entries.add(new KeyValue<>(2, "two"));    entries.add(new KeyValue<>(3, "three"));    store.putAll(entries);    final Iterator<KeyValue<Integer, String>> iterator = store.range(2, 2);    assertEquals(iterator.next().value, store.get(2));    assertFalse(iterator.hasNext());}
f17923
0
shouldRemove
public void kafkatest_f17931_0()
{    bytesStore.put(serializeKey(new Windowed<>("a", windows[0])), serializeValue(30));    bytesStore.put(serializeKey(new Windowed<>("a", windows[1])), serializeValue(50));    bytesStore.remove(serializeKey(new Windowed<>("a", windows[0])));    final KeyValueIterator<Bytes, byte[]> value = bytesStore.fetch(Bytes.wrap("a".getBytes()), 0, 100);    assertFalse(value.hasNext());}
f17931
0
shouldRollSegments
public void kafkatest_f17932_0()
{    // just to validate directories    final AbstractSegments<S> segments = newSegments();    final String key = "a";    bytesStore.put(serializeKey(new Windowed<>(key, windows[0])), serializeValue(50));    bytesStore.put(serializeKey(new Windowed<>(key, windows[1])), serializeValue(100));    bytesStore.put(serializeKey(new Windowed<>(key, windows[2])), serializeValue(500));    assertEquals(Collections.singleton(segments.segmentName(0)), segmentDirs());    bytesStore.put(serializeKey(new Windowed<>(key, windows[3])), serializeValue(1000));    assertEquals(Utils.mkSet(segments.segmentName(0), segments.segmentName(1)), segmentDirs());    final List<KeyValue<Windowed<String>, Long>> results = toList(bytesStore.fetch(Bytes.wrap(key.getBytes()), 0, 1500));    assertEquals(Arrays.asList(KeyValue.pair(new Windowed<>(key, windows[0]), 50L), KeyValue.pair(new Windowed<>(key, windows[1]), 100L), KeyValue.pair(new Windowed<>(key, windows[2]), 500L)), results);}
f17932
0
shouldGetAllSegments
public void kafkatest_f17933_0()
{    // just to validate directories    final AbstractSegments<S> segments = newSegments();    final String key = "a";    bytesStore.put(serializeKey(new Windowed<>(key, windows[0])), serializeValue(50L));    assertEquals(Collections.singleton(segments.segmentName(0)), segmentDirs());    bytesStore.put(serializeKey(new Windowed<>(key, windows[3])), serializeValue(100L));    assertEquals(Utils.mkSet(segments.segmentName(0), segments.segmentName(1)), segmentDirs());    final List<KeyValue<Windowed<String>, Long>> results = toList(bytesStore.all());    assertEquals(Arrays.asList(KeyValue.pair(new Windowed<>(key, windows[0]), 50L), KeyValue.pair(new Windowed<>(key, windows[3]), 100L)), results);}
f17933
0
shouldLogAndMeasureExpiredRecords
public void kafkatest_f17941_0()
{    LogCaptureAppender.setClassLoggerToDebug(AbstractRocksDBSegmentedBytesStore.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    // write a record to advance stream time, with a high enough timestamp    // that the subsequent record in windows[0] will already be expired.    bytesStore.put(serializeKey(new Windowed<>("dummy", nextSegmentWindow)), serializeValue(0));    final Bytes key = serializeKey(new Windowed<>("a", windows[0]));    final byte[] value = serializeValue(5);    bytesStore.put(key, value);    LogCaptureAppender.unregister(appender);    final Map<MetricName, ? extends Metric> metrics = context.metrics().metrics();    final Metric dropTotal = metrics.get(new MetricName("expired-window-record-drop-total", "stream-metrics-scope-metrics", "The total number of occurrence of expired-window-record-drop operations.", mkMap(mkEntry("client-id", "mock"), mkEntry("task-id", "0_0"), mkEntry("metrics-scope-id", "bytes-store"))));    final Metric dropRate = metrics.get(new MetricName("expired-window-record-drop-rate", "stream-metrics-scope-metrics", "The average number of occurrence of expired-window-record-drop operation per second.", mkMap(mkEntry("client-id", "mock"), mkEntry("task-id", "0_0"), mkEntry("metrics-scope-id", "bytes-store"))));    assertEquals(1.0, dropTotal.metricValue());    assertNotEquals(0.0, dropRate.metricValue());    final List<String> messages = appender.getMessages();    assertThat(messages, hasItem("Skipping record for expired segment."));}
f17941
0
segmentDirs
private Set<String> kafkatest_f17942_0()
{    final File windowDir = new File(stateDir, storeName);    return Utils.mkSet(Objects.requireNonNull(windowDir.list()));}
f17942
0
serializeKey
private Bytes kafkatest_f17943_0(final Windowed<String> key)
{    final StateSerdes<String, Long> stateSerdes = StateSerdes.withBuiltinTypes("dummy", String.class, Long.class);    if (schema instanceof SessionKeySchema) {        return Bytes.wrap(SessionKeySchema.toBinary(key, stateSerdes.keySerializer(), "dummy"));    } else {        return WindowKeySchema.toStoreKeyBinary(key, 0, stateSerdes);    }}
f17943
0
shouldStoreDifferentValuesWithOldNull
public void kafkatest_f17951_0()
{    final byte[] priorValue = { (byte) 0 };    final byte[] oldValue = null;    final BufferValue bufferValue = new BufferValue(priorValue, oldValue, null, null);    assertSame(priorValue, bufferValue.priorValue());    assertNull(bufferValue.oldValue());    assertNotEquals(bufferValue.priorValue(), bufferValue.oldValue());}
f17951
0
shouldAccountForDeduplicationInSizeEstimate
public void kafkatest_f17952_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(0L, 0L, 0, "topic", null);    assertEquals(25L, new BufferValue(null, null, null, context).residentMemorySizeEstimate());    assertEquals(26L, new BufferValue(new byte[] { (byte) 0 }, null, null, context).residentMemorySizeEstimate());    assertEquals(26L, new BufferValue(null, new byte[] { (byte) 0 }, null, context).residentMemorySizeEstimate());    assertEquals(26L, new BufferValue(new byte[] { (byte) 0 }, new byte[] { (byte) 0 }, null, context).residentMemorySizeEstimate());    assertEquals(27L, new BufferValue(new byte[] { (byte) 0 }, new byte[] { (byte) 1 }, null, context).residentMemorySizeEstimate());    // new value should get counted, but doesn't get deduplicated    assertEquals(28L, new BufferValue(new byte[] { (byte) 0 }, new byte[] { (byte) 1 }, new byte[] { (byte) 0 }, context).residentMemorySizeEstimate());}
f17952
0
shouldSerializeNulls
public void kafkatest_f17953_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(0L, 0L, 0, "topic", null);    final byte[] serializedContext = context.serialize();    final byte[] bytes = new BufferValue(null, null, null, context).serialize(0).array();    final byte[] withoutContext = Arrays.copyOfRange(bytes, serializedContext.length, bytes.length);    assertThat(withoutContext, is(ByteBuffer.allocate(Integer.BYTES * 3).putInt(-1).putInt(-1).putInt(-1).array()));}
f17953
0
shouldDeserializeCompactedDuplicates
public void kafkatest_f17961_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(0L, 0L, 0, "topic", null);    final byte[] serializedContext = context.serialize();    final byte[] duplicate = { (byte) 5 };    final ByteBuffer serialValue = ByteBuffer.allocate(serializedContext.length + Integer.BYTES * 3 + duplicate.length).put(serializedContext).putInt(1).put(duplicate).putInt(-2).putInt(-1);    serialValue.position(0);    final BufferValue bufferValue = BufferValue.deserialize(serialValue);    assertThat(bufferValue, is(new BufferValue(duplicate, duplicate, null, context)));    assertSame(bufferValue.priorValue(), bufferValue.oldValue());}
f17961
0
setUp
public void kafkatest_f17962_0()
{    final String storeName = "store";    underlyingStore = new InMemoryKeyValueStore(storeName);    cacheFlushListener = new CacheFlushListenerStub<>(new StringDeserializer(), new StringDeserializer());    store = new CachingKeyValueStore(underlyingStore);    store.setFlushListener(cacheFlushListener, false);    cache = new ThreadCache(new LogContext("testCache "), maxCacheSizeBytes, new MockStreamsMetrics(new Metrics()));    context = new InternalMockProcessorContext(null, null, null, null, cache);    topic = "topic";    context.setRecordContext(new ProcessorRecordContext(10, 0, 0, topic, null));    store.init(context, null);}
f17962
0
after
public void kafkatest_f17963_0()
{    super.after();}
f17963
0
shouldFlushEvictedItemsIntoUnderlyingStore
public void kafkatest_f17971_0()
{    final int added = addItemsToCache();    // all dirty entries should have been flushed    assertEquals(added, underlyingStore.approximateNumEntries());    assertEquals(added, store.approximateNumEntries());    assertNotNull(underlyingStore.get(Bytes.wrap("0".getBytes())));}
f17971
0
shouldForwardDirtyItemToListenerWhenEvicted
public void kafkatest_f17972_0()
{    final int numRecords = addItemsToCache();    assertEquals(numRecords, cacheFlushListener.forwarded.size());}
f17972
0
shouldForwardDirtyItemsWhenFlushCalled
public void kafkatest_f17973_0()
{    store.put(bytesKey("1"), bytesValue("a"));    store.flush();    assertEquals("a", cacheFlushListener.forwarded.get("1").newValue);    assertNull(cacheFlushListener.forwarded.get("1").oldValue);}
f17973
0
shouldThrowIfTryingToGetFromClosedCachingStore
public void kafkatest_f17981_0()
{    store.close();    store.get(bytesKey("a"));}
f17981
0
shouldThrowIfTryingToWriteToClosedCachingStore
public void kafkatest_f17982_0()
{    store.close();    store.put(bytesKey("a"), bytesValue("a"));}
f17982
0
shouldThrowIfTryingToDoRangeQueryOnClosedCachingStore
public void kafkatest_f17983_0()
{    store.close();    store.range(bytesKey("a"), bytesKey("b"));}
f17983
0
shouldPutIfAbsent
public void kafkatest_f17991_0()
{    store.putIfAbsent(bytesKey("b"), bytesValue("2"));    assertThat(store.get(bytesKey("b")), equalTo(bytesValue("2")));    store.putIfAbsent(bytesKey("b"), bytesValue("3"));    assertThat(store.get(bytesKey("b")), equalTo(bytesValue("2")));}
f17991
0
shouldPutAll
public void kafkatest_f17992_0()
{    final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();    entries.add(new KeyValue<>(bytesKey("a"), bytesValue("1")));    entries.add(new KeyValue<>(bytesKey("b"), bytesValue("2")));    store.putAll(entries);    assertThat(store.get(bytesKey("a")), equalTo(bytesValue("1")));    assertThat(store.get(bytesKey("b")), equalTo(bytesValue("2")));}
f17992
0
shouldReturnUnderlying
public void kafkatest_f17993_0()
{    assertEquals(underlyingStore, store.wrapped());}
f17993
0
shouldFetchAllSessionsWithSameRecordKey
public void kafkatest_f18001_0()
{    final List<KeyValue<Windowed<Bytes>, byte[]>> expected = asList(KeyValue.pair(new Windowed<>(keyA, new SessionWindow(0, 0)), "1".getBytes()), KeyValue.pair(new Windowed<>(keyA, new SessionWindow(10, 10)), "2".getBytes()), KeyValue.pair(new Windowed<>(keyA, new SessionWindow(100, 100)), "3".getBytes()), KeyValue.pair(new Windowed<>(keyA, new SessionWindow(1000, 1000)), "4".getBytes()));    for (final KeyValue<Windowed<Bytes>, byte[]> kv : expected) {        cachingStore.put(kv.key, kv.value);    }    // add one that shouldn't appear in the results    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(0, 0)), "5".getBytes());    final List<KeyValue<Windowed<Bytes>, byte[]>> results = toList(cachingStore.fetch(keyA));    verifyKeyValueList(expected, results);}
f18001
0
shouldFlushItemsToStoreOnEviction
public void kafkatest_f18002_0()
{    final List<KeyValue<Windowed<Bytes>, byte[]>> added = addSessionsUntilOverflow("a", "b", "c", "d");    assertEquals(added.size() - 1, cache.size());    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.findSessions(added.get(0).key.key(), 0, 0);    final KeyValue<Windowed<Bytes>, byte[]> next = iterator.next();    assertEquals(added.get(0).key, next.key);    assertArrayEquals(added.get(0).value, next.value);}
f18002
0
shouldQueryItemsInCacheAndStore
public void kafkatest_f18003_0()
{    final List<KeyValue<Windowed<Bytes>, byte[]>> added = addSessionsUntilOverflow("a");    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.findSessions(Bytes.wrap("a".getBytes(StandardCharsets.UTF_8)), 0, added.size() * 10);    final List<KeyValue<Windowed<Bytes>, byte[]>> actual = toList(iterator);    verifyKeyValueList(added, actual);}
f18003
0
shouldClearNamespaceCacheOnClose
public void kafkatest_f18011_0()
{    final Windowed<Bytes> a1 = new Windowed<>(keyA, new SessionWindow(0, 0));    cachingStore.put(a1, "1".getBytes());    assertEquals(1, cache.size());    cachingStore.close();    assertEquals(0, cache.size());}
f18011
0
shouldThrowIfTryingToFetchFromClosedCachingStore
public void kafkatest_f18012_0()
{    cachingStore.close();    cachingStore.fetch(keyA);}
f18012
0
shouldThrowIfTryingToFindMergeSessionFromClosedCachingStore
public void kafkatest_f18013_0()
{    cachingStore.close();    cachingStore.findSessions(keyA, 0, Long.MAX_VALUE);}
f18013
0
shouldThrowNullPointerExceptionOnFetchNullKey
public void kafkatest_f18021_0()
{    cachingStore.fetch(null);}
f18021
0
shouldThrowNullPointerExceptionOnRemoveNullKey
public void kafkatest_f18022_0()
{    cachingStore.remove(null);}
f18022
0
shouldThrowNullPointerExceptionOnPutNullKey
public void kafkatest_f18023_0()
{    cachingStore.put(null, "1".getBytes());}
f18023
0
init
public void kafkatest_f18031_0(final ProcessorContext processorContext)
{    this.store = (WindowStore<String, String>) processorContext.getStateStore("store-name");    int count = 0;    final KeyValueIterator<Windowed<String>, String> all = store.all();    while (all.hasNext()) {        count++;        all.next();    }    assertThat(count, equalTo(0));}
f18031
0
transform
public KeyValue<String, String> kafkatest_f18032_0(final String key, final String value)
{    int count = 0;    final KeyValueIterator<Windowed<String>, String> all = store.all();    while (all.hasNext()) {        count++;        all.next();    }    assertThat(count, equalTo(numRecordsProcessed));    store.put(value, value);    numRecordsProcessed++;    return new KeyValue<>(key, value);}
f18032
0
shouldPutFetchFromCache
public void kafkatest_f18034_0()
{    cachingStore.put(bytesKey("a"), bytesValue("a"));    cachingStore.put(bytesKey("b"), bytesValue("b"));    assertThat(cachingStore.fetch(bytesKey("a"), 10), equalTo(bytesValue("a")));    assertThat(cachingStore.fetch(bytesKey("b"), 10), equalTo(bytesValue("b")));    assertThat(cachingStore.fetch(bytesKey("c"), 10), equalTo(null));    assertThat(cachingStore.fetch(bytesKey("a"), 0), equalTo(null));    final WindowStoreIterator<byte[]> a = cachingStore.fetch(bytesKey("a"), ofEpochMilli(10), ofEpochMilli(10));    final WindowStoreIterator<byte[]> b = cachingStore.fetch(bytesKey("b"), ofEpochMilli(10), ofEpochMilli(10));    verifyKeyValue(a.next(), DEFAULT_TIMESTAMP, "a");    verifyKeyValue(b.next(), DEFAULT_TIMESTAMP, "b");    assertFalse(a.hasNext());    assertFalse(b.hasNext());    assertEquals(2, cache.size());}
f18034
0
shouldFlushEvictedItemsIntoUnderlyingStore
public void kafkatest_f18042_0()
{    final int added = addItemsToCache();    // all dirty entries should have been flushed    final KeyValueIterator<Bytes, byte[]> iter = underlying.fetch(Bytes.wrap("0".getBytes(StandardCharsets.UTF_8)), DEFAULT_TIMESTAMP, DEFAULT_TIMESTAMP);    final KeyValue<Bytes, byte[]> next = iter.next();    assertEquals(DEFAULT_TIMESTAMP, keySchema.segmentTimestamp(next.key));    assertArrayEquals("0".getBytes(), next.value);    assertFalse(iter.hasNext());    assertEquals(added - 1, cache.size());}
f18042
0
shouldForwardDirtyItemsWhenFlushCalled
public void kafkatest_f18043_0()
{    final Windowed<String> windowedKey = new Windowed<>("1", new TimeWindow(DEFAULT_TIMESTAMP, DEFAULT_TIMESTAMP + WINDOW_SIZE));    cachingStore.put(bytesKey("1"), bytesValue("a"));    cachingStore.flush();    assertEquals("a", cacheListener.forwarded.get(windowedKey).newValue);    assertNull(cacheListener.forwarded.get(windowedKey).oldValue);}
f18043
0
shouldSetFlushListener
public void kafkatest_f18044_0()
{    assertTrue(cachingStore.setFlushListener(null, true));    assertTrue(cachingStore.setFlushListener(null, false));}
f18044
0
shouldClearNamespaceCacheOnClose
public void kafkatest_f18052_0()
{    cachingStore.put(bytesKey("a"), bytesValue("a"));    assertEquals(1, cache.size());    cachingStore.close();    assertEquals(0, cache.size());}
f18052
0
shouldThrowIfTryingToFetchFromClosedCachingStore
public void kafkatest_f18053_0()
{    cachingStore.close();    cachingStore.fetch(bytesKey("a"), ofEpochMilli(0), ofEpochMilli(10));}
f18053
0
shouldThrowIfTryingToFetchRangeFromClosedCachingStore
public void kafkatest_f18054_0()
{    cachingStore.close();    cachingStore.fetch(bytesKey("a"), bytesKey("b"), ofEpochMilli(0), ofEpochMilli(10));}
f18054
0
shouldThrowNullPointerExceptionOnRangeNullFromKey
public void kafkatest_f18062_0()
{    cachingStore.fetch(null, bytesKey("anyTo"), ofEpochMilli(1L), ofEpochMilli(2L));}
f18062
0
shouldThrowNullPointerExceptionOnRangeNullToKey
public void kafkatest_f18063_0()
{    cachingStore.fetch(bytesKey("anyFrom"), null, ofEpochMilli(1L), ofEpochMilli(2L));}
f18063
0
shouldNotThrowInvalidRangeExceptionWithNegativeFromKey
public void kafkatest_f18064_0()
{    LogCaptureAppender.setClassLoggerToDebug(InMemoryWindowStore.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    final Bytes keyFrom = Bytes.wrap(Serdes.Integer().serializer().serialize("", -1));    final Bytes keyTo = Bytes.wrap(Serdes.Integer().serializer().serialize("", 1));    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.fetch(keyFrom, keyTo, 0L, 10L);    assertFalse(iterator.hasNext());    final List<String> messages = appender.getMessages();    assertThat(messages, hasItem("Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"));}
f18064
0
shouldWriteAllKeyValueToInnerStoreOnPutAll
public void kafkatest_f18072_0()
{    store.putAll(Arrays.asList(KeyValue.pair(hi, there), KeyValue.pair(hello, world)));    assertThat(inner.get(hi), equalTo(there));    assertThat(inner.get(hello), equalTo(world));}
f18072
0
shouldLogChangesOnPutAll
public void kafkatest_f18073_0()
{    store.putAll(Arrays.asList(KeyValue.pair(hi, there), KeyValue.pair(hello, world)));    assertThat(sent.get(hi), equalTo(there));    assertThat(sent.get(hello), equalTo(world));}
f18073
0
shouldPropagateDelete
public void kafkatest_f18074_0()
{    store.put(hi, there);    store.delete(hi);    assertThat(inner.approximateNumEntries(), equalTo(0L));    assertThat(inner.get(hi), nullValue());}
f18074
0
shouldReturnNullOnPutIfAbsentWhenNoPreviousValue
public void kafkatest_f18082_0()
{    assertThat(store.putIfAbsent(hi, there), is(nullValue()));}
f18082
0
shouldReturnValueOnGetWhenExists
public void kafkatest_f18083_0()
{    store.put(hello, world);    assertThat(store.get(hello), equalTo(world));}
f18083
0
shouldReturnNullOnGetWhenDoesntExist
public void kafkatest_f18084_0()
{    assertThat(store.get(hello), is(nullValue()));}
f18084
0
shouldDelegateToUnderlyingStoreWhenFindingSessions
public void kafkatest_f18092_0()
{    EasyMock.expect(inner.findSessions(bytesKey, 0, 1)).andReturn(KeyValueIterators.<Windowed<Bytes>, byte[]>emptyIterator());    init();    store.findSessions(bytesKey, 0, 1);    EasyMock.verify(inner);}
f18092
0
shouldDelegateToUnderlyingStoreWhenFindingSessionRange
public void kafkatest_f18093_0()
{    EasyMock.expect(inner.findSessions(bytesKey, bytesKey, 0, 1)).andReturn(KeyValueIterators.<Windowed<Bytes>, byte[]>emptyIterator());    init();    store.findSessions(bytesKey, bytesKey, 0, 1);    EasyMock.verify(inner);}
f18093
0
shouldFlushUnderlyingStore
public void kafkatest_f18094_0()
{    inner.flush();    EasyMock.expectLastCall();    init();    store.flush();    EasyMock.verify(inner);}
f18094
0
shouldLogChangesOnPutAll
public void kafkatest_f18102_0()
{    store.putAll(Arrays.asList(KeyValue.pair(hi, rawThere), KeyValue.pair(hello, rawWorld)));    final ValueAndTimestamp<byte[]> logged = sent.get(hi);    assertThat(logged.value(), equalTo(there.value()));    assertThat(logged.timestamp(), equalTo(there.timestamp()));    final ValueAndTimestamp<byte[]> logged2 = sent.get(hello);    assertThat(logged2.value(), equalTo(world.value()));    assertThat(logged2.timestamp(), equalTo(world.timestamp()));}
f18102
0
shouldPropagateDelete
public void kafkatest_f18103_0()
{    store.put(hi, rawThere);    store.delete(hi);    assertThat(root.approximateNumEntries(), equalTo(0L));    assertThat(root.get(hi), nullValue());}
f18103
0
shouldReturnOldValueOnDelete
public void kafkatest_f18104_0()
{    store.put(hi, rawThere);    assertThat(store.delete(hi), equalTo(rawThere));}
f18104
0
shouldReturnValueOnGetWhenExists
public void kafkatest_f18112_0()
{    store.put(hello, rawWorld);    assertThat(store.get(hello), equalTo(rawWorld));}
f18112
0
shouldReturnNullOnGetWhenDoesntExist
public void kafkatest_f18113_0()
{    assertThat(store.get(hello), is(nullValue()));}
f18113
0
send
public void kafkatest_f18114_0(final String topic, final K key, final V value, final Headers headers, final Integer partition, final Long timestamp, final Serializer<K> keySerializer, final Serializer<V> valueSerializer)
{    sent.put(key, ValueAndTimestamp.make(value, timestamp));}
f18114
0
setUp
public void kafkatest_f18122_0()
{    store = new ChangeLoggingWindowBytesStore(inner, false);}
f18122
0
init
private void kafkatest_f18123_0()
{    EasyMock.expect(context.taskId()).andReturn(taskId);    EasyMock.expect(context.recordCollector()).andReturn(collector);    inner.init(context, store);    EasyMock.expectLastCall();    EasyMock.replay(inner, context);    store.init(context, store);}
f18123
0
shouldLogPuts
public void kafkatest_f18124_0()
{    inner.put(bytesKey, value, 0);    EasyMock.expectLastCall();    init();    store.put(bytesKey, value);    assertArrayEquals(value, (byte[]) sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 0)));    EasyMock.verify(inner);}
f18124
0
shouldThrowNullPointerExceptionOnRangeNullFromKey
public void kafkatest_f18132_0()
{    theStore.range(null, "to");}
f18132
0
shouldThrowNullPointerExceptionOnRangeNullToKey
public void kafkatest_f18133_0()
{    theStore.range("from", null);}
f18133
0
shouldReturnValueIfExists
public void kafkatest_f18134_0()
{    stubOneUnderlying.put("key", "value");    assertEquals("value", theStore.get("key"));}
f18134
0
shouldSupportRangeAcrossMultipleKVStores
public void kafkatest_f18142_0()
{    final KeyValueStore<String, String> cache = newStoreInstance();    stubProviderTwo.addStore(storeName, cache);    stubOneUnderlying.put("a", "a");    stubOneUnderlying.put("b", "b");    stubOneUnderlying.put("z", "z");    cache.put("c", "c");    cache.put("d", "d");    cache.put("x", "x");    final List<KeyValue<String, String>> results = toList(theStore.range("a", "e"));    assertTrue(results.contains(new KeyValue<>("a", "a")));    assertTrue(results.contains(new KeyValue<>("b", "b")));    assertTrue(results.contains(new KeyValue<>("c", "c")));    assertTrue(results.contains(new KeyValue<>("d", "d")));    assertEquals(4, results.size());}
f18142
0
shouldSupportAllAcrossMultipleStores
public void kafkatest_f18143_0()
{    final KeyValueStore<String, String> cache = newStoreInstance();    stubProviderTwo.addStore(storeName, cache);    stubOneUnderlying.put("a", "a");    stubOneUnderlying.put("b", "b");    stubOneUnderlying.put("z", "z");    cache.put("c", "c");    cache.put("d", "d");    cache.put("x", "x");    final List<KeyValue<String, String>> results = toList(theStore.all());    assertTrue(results.contains(new KeyValue<>("a", "a")));    assertTrue(results.contains(new KeyValue<>("b", "b")));    assertTrue(results.contains(new KeyValue<>("c", "c")));    assertTrue(results.contains(new KeyValue<>("d", "d")));    assertTrue(results.contains(new KeyValue<>("x", "x")));    assertTrue(results.contains(new KeyValue<>("z", "z")));    assertEquals(6, results.size());}
f18143
0
shouldThrowInvalidStoreExceptionDuringRebalance
public void kafkatest_f18144_0()
{    rebalancing().get("anything");}
f18144
0
approximateNumEntries
public long kafkatest_f18152_0()
{    return Long.MAX_VALUE;}
f18152
0
approximateNumEntries
public long kafkatest_f18153_0()
{    return Long.MAX_VALUE;}
f18153
0
rebalancing
private CompositeReadOnlyKeyValueStore<Object, Object> kafkatest_f18154_0()
{    return new CompositeReadOnlyKeyValueStore<>(new WrappingStoreProvider(Collections.<StateStoreProvider>singletonList(new StateStoreProviderStub(true))), QueryableStoreTypes.keyValueStore(), storeName);}
f18154
0
shouldThrowNullPointerExceptionIfFetchingNullKey
public void kafkatest_f18162_0()
{    sessionStore.fetch(null);}
f18162
0
shouldFetchKeyRangeAcrossStores
public void kafkatest_f18163_0()
{    final ReadOnlySessionStoreStub<String, Long> secondUnderlying = new ReadOnlySessionStoreStub<>();    stubProviderTwo.addStore(storeName, secondUnderlying);    underlyingSessionStore.put(new Windowed<>("a", new SessionWindow(0, 0)), 0L);    secondUnderlying.put(new Windowed<>("b", new SessionWindow(0, 0)), 10L);    final List<KeyValue<Windowed<String>, Long>> results = StreamsTestUtils.toList(sessionStore.fetch("a", "b"));    assertThat(results.size(), equalTo(2));}
f18163
0
shouldThrowNPEIfKeyIsNull
public void kafkatest_f18164_0()
{    underlyingSessionStore.fetch(null);}
f18164
0
shouldThrowInvalidStateStoreExceptionOnRebalance
public void kafkatest_f18172_0()
{    final CompositeReadOnlyWindowStore<Object, Object> store = new CompositeReadOnlyWindowStore<>(new StateStoreProviderStub(true), QueryableStoreTypes.windowStore(), "foo");    store.fetch("key", ofEpochMilli(1), ofEpochMilli(10));}
f18172
0
shouldThrowInvalidStateStoreExceptionIfFetchThrows
public void kafkatest_f18173_0()
{    underlyingWindowStore.setOpen(false);    final CompositeReadOnlyWindowStore<Object, Object> store = new CompositeReadOnlyWindowStore<>(stubProviderOne, QueryableStoreTypes.windowStore(), "window-store");    try {        store.fetch("key", ofEpochMilli(1), ofEpochMilli(10));        Assert.fail("InvalidStateStoreException was expected");    } catch (final InvalidStateStoreException e) {        Assert.assertEquals("State store is not available anymore and may have been migrated to another instance; " + "please re-discover its location from the state metadata.", e.getMessage());    }}
f18173
0
emptyIteratorAlwaysReturnsFalse
public void kafkatest_f18174_0()
{    final CompositeReadOnlyWindowStore<Object, Object> store = new CompositeReadOnlyWindowStore<>(new StateStoreProviderStub(false), QueryableStoreTypes.windowStore(), "foo");    final WindowStoreIterator<Object> windowStoreIterator = store.fetch("key", ofEpochMilli(1), ofEpochMilli(10));    Assert.assertFalse(windowStoreIterator.hasNext());}
f18174
0
shouldThrowNPEIfFromKeyIsNull
public void kafkatest_f18182_0()
{    windowStore.fetch(null, "a", ofEpochMilli(0), ofEpochMilli(0));}
f18182
0
shouldThrowNPEIfToKeyIsNull
public void kafkatest_f18183_0()
{    windowStore.fetch("a", null, ofEpochMilli(0), ofEpochMilli(0));}
f18183
0
setUp
public void kafkatest_f18184_0()
{    store = new GenericInMemoryKeyValueStore<>(name);}
f18184
0
before
public void kafkatest_f18192_0()
{    store.putAll(entries);    final HasNextCondition allCondition = new HasNextCondition() {        @Override        public boolean hasNext(final KeyValueIterator<Bytes, ?> iterator) {            return iterator.hasNext();        }    };    allIterator = new FilteredCacheIterator(new DelegatingPeekingKeyValueIterator<>("", store.all()), allCondition, IDENTITY_FUNCTION);    final HasNextCondition firstEntryCondition = new HasNextCondition() {        @Override        public boolean hasNext(final KeyValueIterator<Bytes, ?> iterator) {            return iterator.hasNext() && iterator.peekNextKey().equals(firstEntry.key);        }    };    firstEntryIterator = new FilteredCacheIterator(new DelegatingPeekingKeyValueIterator<>("", store.all()), firstEntryCondition, IDENTITY_FUNCTION);}
f18192
0
hasNext
public boolean kafkatest_f18193_0(final KeyValueIterator<Bytes, ?> iterator)
{    return iterator.hasNext();}
f18193
0
hasNext
public boolean kafkatest_f18194_0(final KeyValueIterator<Bytes, ?> iterator)
{    return iterator.hasNext() && iterator.peekNextKey().equals(firstEntry.key);}
f18194
0
shouldReturnSingleItemListIfStoreExists
public void kafkatest_f18202_0()
{    final GlobalStateStoreProvider provider = new GlobalStateStoreProvider(Collections.singletonMap("global", new NoOpReadOnlyStore<>()));    final List<ReadOnlyKeyValueStore<Object, Object>> stores = provider.stores("global", QueryableStoreTypes.keyValueStore());    assertEquals(stores.size(), 1);}
f18202
0
shouldReturnEmptyItemListIfStoreDoesntExist
public void kafkatest_f18203_0()
{    final GlobalStateStoreProvider provider = new GlobalStateStoreProvider(Collections.emptyMap());    final List<ReadOnlyKeyValueStore<Object, Object>> stores = provider.stores("global", QueryableStoreTypes.keyValueStore());    assertTrue(stores.isEmpty());}
f18203
0
shouldThrowExceptionIfStoreIsntOpen
public void kafkatest_f18204_0()
{    final NoOpReadOnlyStore<Object, Object> store = new NoOpReadOnlyStore<>();    store.close();    final GlobalStateStoreProvider provider = new GlobalStateStoreProvider(Collections.singletonMap("global", store));    provider.stores("global", QueryableStoreTypes.keyValueStore());}
f18204
0
shouldRemoveKeysWithNullValues
public void kafkatest_f18212_0()
{    store.close();    // Add any entries that will be restored to any store    // that uses the driver's context ...    driver.addEntryToRestoreLog(0, "zero");    driver.addEntryToRestoreLog(1, "one");    driver.addEntryToRestoreLog(2, "two");    driver.addEntryToRestoreLog(3, "three");    driver.addEntryToRestoreLog(0, null);    store = createKeyValueStore(driver.context());    context.restore(store.name(), driver.restoredEntries());    assertEquals(3, driver.sizeOf(store));    assertThat(store.get(0), nullValue());}
f18212
0
createKeyValueStore
protected KeyValueStore<K, V> kafkatest_f18213_0(final ProcessorContext context)
{    final StoreBuilder storeBuilder = Stores.keyValueStoreBuilder(Stores.lruMap("my-store", 10), (Serde<K>) context.keySerde(), (Serde<V>) context.valueSerde());    final StateStore store = storeBuilder.build();    store.init(context, store);    return (KeyValueStore<K, V>) store;}
f18213
0
shouldPutAllKeyValuePairs
public void kafkatest_f18214_0()
{    final List<KeyValue<Integer, String>> kvPairs = Arrays.asList(KeyValue.pair(1, "1"), KeyValue.pair(2, "2"), KeyValue.pair(3, "3"));    store.putAll(kvPairs);    assertThat(store.approximateNumEntries(), equalTo(3L));    for (final KeyValue<Integer, String> kvPair : kvPairs) {        assertThat(store.get(kvPair.key), equalTo(kvPair.value));    }}
f18214
0
shouldNotExpireFromOpenIterator
public void kafkatest_f18222_0()
{    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 0)), 1L);    sessionStore.put(new Windowed<>("aa", new SessionWindow(0, 10)), 2L);    sessionStore.put(new Windowed<>("a", new SessionWindow(10, 20)), 3L);    final KeyValueIterator<Windowed<String>, Long> iterator = sessionStore.findSessions("a", "b", 0L, RETENTION_PERIOD);    // Advance stream time to expire the first three record    sessionStore.put(new Windowed<>("aa", new SessionWindow(100, 2 * RETENTION_PERIOD)), 4L);    assertEquals(valuesToSet(iterator), new HashSet<>(Arrays.asList(1L, 2L, 3L, 4L)));    assertFalse(iterator.hasNext());    iterator.close();    assertFalse(sessionStore.findSessions("a", "b", 0L, 20L).hasNext());}
f18222
0
bufferShouldAllowCacheEnablement
public void kafkatest_f18223_0()
{    new InMemoryTimeOrderedKeyValueBuffer.Builder<>(null, null, null).withCachingEnabled();}
f18223
0
bufferShouldAllowCacheDisablement
public void kafkatest_f18224_0()
{    new InMemoryTimeOrderedKeyValueBuffer.Builder<>(null, null, null).withCachingDisabled();}
f18224
0
shouldForwardHasNext
public void kafkatest_f18232_0()
{    expect(mockedKeyValueIterator.hasNext()).andReturn(true).andReturn(false);    replay(mockedKeyValueIterator);    assertTrue(keyValueIteratorFacade.hasNext());    assertFalse(keyValueIteratorFacade.hasNext());    verify(mockedKeyValueIterator);}
f18232
0
shouldForwardPeekNextKey
public void kafkatest_f18233_0()
{    expect(mockedKeyValueIterator.peekNextKey()).andReturn("key");    replay(mockedKeyValueIterator);    assertThat(keyValueIteratorFacade.peekNextKey(), is("key"));    verify(mockedKeyValueIterator);}
f18233
0
shouldReturnPlainKeyValuePairOnGet
public void kafkatest_f18234_0()
{    expect(mockedKeyValueIterator.next()).andReturn(new KeyValue<>("key", ValueAndTimestamp.make("value", 42L)));    replay(mockedKeyValueIterator);    assertThat(keyValueIteratorFacade.next(), is(KeyValue.pair("key", "value")));    verify(mockedKeyValueIterator);}
f18234
0
shouldNotCreateSegmentThatIsAlreadyExpired
public void kafkatest_f18242_0()
{    final long streamTime = updateStreamTimeAndCreateSegment(7);    assertNull(segments.getOrCreateSegmentIfLive(0, context, streamTime));    assertFalse(new File(context.stateDir(), "test/test.0").exists());}
f18242
0
shouldCleanupSegmentsThatHaveExpired
public void kafkatest_f18243_0()
{    final KeyValueSegment segment1 = segments.getOrCreateSegmentIfLive(0, context, -1L);    final KeyValueSegment segment2 = segments.getOrCreateSegmentIfLive(1, context, -1L);    final KeyValueSegment segment3 = segments.getOrCreateSegmentIfLive(7, context, SEGMENT_INTERVAL * 7L);    assertFalse(segment1.isOpen());    assertFalse(segment2.isOpen());    assertTrue(segment3.isOpen());    assertFalse(new File(context.stateDir(), "test/test.0").exists());    assertFalse(new File(context.stateDir(), "test/test." + SEGMENT_INTERVAL).exists());    assertTrue(new File(context.stateDir(), "test/test." + 7 * SEGMENT_INTERVAL).exists());}
f18243
0
shouldGetSegmentForTimestamp
public void kafkatest_f18244_0()
{    final KeyValueSegment segment = segments.getOrCreateSegmentIfLive(0, context, -1L);    segments.getOrCreateSegmentIfLive(1, context, -1L);    assertEquals(segment, segments.getSegmentForTimestamp(0L));}
f18244
0
updateStreamTimeAndCreateSegment
private long kafkatest_f18252_0(final int segment)
{    final long streamTime = SEGMENT_INTERVAL * segment;    segments.getOrCreateSegmentIfLive(segment, context, streamTime);    return streamTime;}
f18252
0
shouldUpdateSegmentFileNameFromOldDateFormatToNewFormat
public void kafkatest_f18253_0() throws Exception
{    // the old segment file's naming system maxes out at 1 minute granularity.    final long segmentInterval = 60_000L;    segments = new KeyValueSegments(storeName, METRICS_SCOPE, NUM_SEGMENTS * segmentInterval, segmentInterval);    final String storeDirectoryPath = stateDirectory.getAbsolutePath() + File.separator + storeName;    final File storeDirectory = new File(storeDirectoryPath);    // noinspection ResultOfMethodCallIgnored    storeDirectory.mkdirs();    final SimpleDateFormat formatter = new SimpleDateFormat("yyyyMMddHHmm");    formatter.setTimeZone(new SimpleTimeZone(0, "UTC"));    for (int segmentId = 0; segmentId < NUM_SEGMENTS; ++segmentId) {        final File oldSegment = new File(storeDirectoryPath + File.separator + storeName + "-" + formatter.format(new Date(segmentId * segmentInterval)));        // noinspection ResultOfMethodCallIgnored        oldSegment.createNewFile();    }    segments.openExisting(context, -1L);    for (int segmentId = 0; segmentId < NUM_SEGMENTS; ++segmentId) {        final String segmentName = storeName + "." + (long) segmentId * segmentInterval;        final File newSegment = new File(storeDirectoryPath + File.separator + segmentName);        assertTrue(newSegment.exists());    }}
f18253
0
shouldUpdateSegmentFileNameFromOldColonFormatToNewFormat
public void kafkatest_f18254_0() throws Exception
{    final String storeDirectoryPath = stateDirectory.getAbsolutePath() + File.separator + storeName;    final File storeDirectory = new File(storeDirectoryPath);    // noinspection ResultOfMethodCallIgnored    storeDirectory.mkdirs();    for (int segmentId = 0; segmentId < NUM_SEGMENTS; ++segmentId) {        final File oldSegment = new File(storeDirectoryPath + File.separator + storeName + ":" + segmentId * (RETENTION_PERIOD / (NUM_SEGMENTS - 1)));        // noinspection ResultOfMethodCallIgnored        oldSegment.createNewFile();    }    segments.openExisting(context, -1L);    for (int segmentId = 0; segmentId < NUM_SEGMENTS; ++segmentId) {        final File newSegment = new File(storeDirectoryPath + File.separator + storeName + "." + segmentId * (RETENTION_PERIOD / (NUM_SEGMENTS - 1)));        assertTrue(newSegment.exists());    }}
f18254
0
shouldHaveMeteredStoreAsOuterStore
public void kafkatest_f18262_0()
{    final KeyValueStore<String, String> store = builder.build();    assertThat(store, instanceOf(MeteredKeyValueStore.class));}
f18262
0
shouldHaveChangeLoggingStoreByDefault
public void kafkatest_f18263_0()
{    final KeyValueStore<String, String> store = builder.build();    assertThat(store, instanceOf(MeteredKeyValueStore.class));    final StateStore next = ((WrappedStateStore) store).wrapped();    assertThat(next, instanceOf(ChangeLoggingKeyValueBytesStore.class));}
f18263
0
shouldNotHaveChangeLoggingStoreWhenDisabled
public void kafkatest_f18264_0()
{    final KeyValueStore<String, String> store = builder.withLoggingDisabled().build();    final StateStore next = ((WrappedStateStore) store).wrapped();    assertThat(next, CoreMatchers.equalTo(inner));}
f18264
0
shouldThrowNullPointerIfMetricsScopeIsNull
public void kafkatest_f18272_0()
{    new KeyValueStoreBuilder<>(supplier, Serdes.String(), Serdes.String(), new MockTime());}
f18272
0
shouldReturnDefinedValue
public void kafkatest_f18273_0()
{    assertThat(Maybe.defined(null).getNullableValue(), nullValue());    assertThat(Maybe.defined("ASDF").getNullableValue(), is("ASDF"));}
f18273
0
shouldAnswerIsDefined
public void kafkatest_f18274_0()
{    assertThat(Maybe.defined(null).isDefined(), is(true));    assertThat(Maybe.defined("ASDF").isDefined(), is(true));    assertThat(Maybe.undefined().isDefined(), is(false));}
f18274
0
shouldIgnoreIfDeletedInCacheButExistsInStore
public void kafkatest_f18282_0() throws Exception
{    final byte[][] bytes = { { 0 } };    cache.put(namespace, Bytes.wrap(bytes[0]), new LRUCacheEntry(null));    store.put(Bytes.wrap(bytes[0]), bytes[0]);    final MergedSortedCacheKeyValueBytesStoreIterator iterator = createIterator();    assertFalse(iterator.hasNext());}
f18282
0
shouldNotHaveNextIfAllCachedItemsDeleted
public void kafkatest_f18283_0() throws Exception
{    final byte[][] bytes = { { 0 }, { 1 }, { 2 } };    for (final byte[] aByte : bytes) {        final Bytes aBytes = Bytes.wrap(aByte);        store.put(aBytes, aByte);        cache.put(namespace, aBytes, new LRUCacheEntry(null));    }    assertFalse(createIterator().hasNext());}
f18283
0
shouldNotHaveNextIfOnlyCacheItemsAndAllDeleted
public void kafkatest_f18284_0() throws Exception
{    final byte[][] bytes = { { 0 }, { 1 }, { 2 } };    for (final byte[] aByte : bytes) {        cache.put(namespace, Bytes.wrap(aByte), new LRUCacheEntry(null));    }    assertFalse(createIterator().hasNext());}
f18284
0
shouldHaveNextFromCache
public void kafkatest_f18292_0()
{    final MergedSortedCacheSessionStoreIterator mergeIterator = createIterator(Collections.emptyIterator(), cacheKvs);    assertTrue(mergeIterator.hasNext());}
f18292
0
shouldGetNextFromCache
public void kafkatest_f18293_0()
{    final MergedSortedCacheSessionStoreIterator mergeIterator = createIterator(Collections.emptyIterator(), cacheKvs);    assertThat(mergeIterator.next(), equalTo(KeyValue.pair(new Windowed<>(cacheKey, cacheWindow), cacheKey.get())));}
f18293
0
shouldPeekNextKeyFromCache
public void kafkatest_f18294_0()
{    final MergedSortedCacheSessionStoreIterator mergeIterator = createIterator(Collections.emptyIterator(), cacheKvs);    assertThat(mergeIterator.peekNextKey(), equalTo(new Windowed<>(cacheKey, cacheWindow)));}
f18294
0
shouldHaveNextFromStore
public void kafkatest_f18302_0()
{    final MergedSortedCacheWindowStoreKeyValueIterator mergeIterator = createIterator(storeKvs, Collections.emptyIterator());    assertTrue(mergeIterator.hasNext());}
f18302
0
shouldGetNextFromStore
public void kafkatest_f18303_0()
{    final MergedSortedCacheWindowStoreKeyValueIterator mergeIterator = createIterator(storeKvs, Collections.emptyIterator());    assertThat(convertKeyValuePair(mergeIterator.next()), equalTo(KeyValue.pair(new Windowed<>(storeKey, storeWindow), storeKey)));}
f18303
0
shouldPeekNextKeyFromStore
public void kafkatest_f18304_0()
{    final MergedSortedCacheWindowStoreKeyValueIterator mergeIterator = createIterator(storeKvs, Collections.emptyIterator());    assertThat(convertWindowedKey(mergeIterator.peekNextKey()), equalTo(new Windowed<>(storeKey, storeWindow)));}
f18304
0
before
public void kafkatest_f18312_0()
{    metered = new MeteredKeyValueStore<>(inner, "scope", new MockTime(), Serdes.String(), Serdes.String());    metrics.config().recordLevel(Sensor.RecordingLevel.DEBUG);    expect(context.metrics()).andReturn(new MockStreamsMetrics(metrics));    expect(context.taskId()).andReturn(taskId);    expect(inner.name()).andReturn("metered").anyTimes();}
f18312
0
init
private void kafkatest_f18313_0()
{    replay(inner, context);    metered.init(context, metered);}
f18313
0
testMetrics
public void kafkatest_f18314_0()
{    init();    final JmxReporter reporter = new JmxReporter("kafka.streams");    metrics.addReporter(reporter);    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", taskId.toString(), "scope", "metered")));    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", taskId.toString(), "scope", "all")));}
f18314
0
shouldGetAllFromInnerStoreAndRecordAllMetric
public void kafkatest_f18322_0()
{    expect(inner.all()).andReturn(new KeyValueIteratorStub<>(Collections.singletonList(byteKeyValuePair).iterator()));    init();    final KeyValueIterator<String, String> iterator = metered.all();    assertThat(iterator.next().value, equalTo(value));    assertFalse(iterator.hasNext());    iterator.close();    final KafkaMetric metric = metric(new MetricName("all-rate", "stream-scope-state-metrics", "", tags));    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18322
0
shouldFlushInnerWhenFlushTimeRecords
public void kafkatest_f18323_0()
{    inner.flush();    expectLastCall().once();    init();    metered.flush();    final KafkaMetric metric = metric("flush-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18323
0
shouldSetFlushListenerOnWrappedCachingStore
public void kafkatest_f18324_0()
{    final CachedKeyValueStore cachedKeyValueStore = mock(CachedKeyValueStore.class);    expect(cachedKeyValueStore.setFlushListener(anyObject(CacheFlushListener.class), eq(false))).andReturn(true);    replay(cachedKeyValueStore);    metered = new MeteredKeyValueStore<>(cachedKeyValueStore, "scope", new MockTime(), Serdes.String(), Serdes.String());    assertTrue(metered.setFlushListener(null, false));    verify(cachedKeyValueStore);}
f18324
0
shouldFindSessionsFromStoreAndRecordFetchMetric
public void kafkatest_f18332_0()
{    expect(inner.findSessions(Bytes.wrap(keyBytes), 0, 0)).andReturn(new KeyValueIteratorStub<>(Collections.singleton(KeyValue.pair(windowedKeyBytes, keyBytes)).iterator()));    init();    final KeyValueIterator<Windowed<String>, String> iterator = metered.findSessions(key, 0, 0);    assertThat(iterator.next().value, equalTo(key));    assertFalse(iterator.hasNext());    iterator.close();    final KafkaMetric metric = metric("fetch-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18332
0
shouldFindSessionRangeFromStoreAndRecordFetchMetric
public void kafkatest_f18333_0()
{    expect(inner.findSessions(Bytes.wrap(keyBytes), Bytes.wrap(keyBytes), 0, 0)).andReturn(new KeyValueIteratorStub<>(Collections.singleton(KeyValue.pair(windowedKeyBytes, keyBytes)).iterator()));    init();    final KeyValueIterator<Windowed<String>, String> iterator = metered.findSessions(key, key, 0, 0);    assertThat(iterator.next().value, equalTo(key));    assertFalse(iterator.hasNext());    iterator.close();    final KafkaMetric metric = metric("fetch-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18333
0
shouldRemoveFromStoreAndRecordRemoveMetric
public void kafkatest_f18334_0()
{    inner.remove(windowedKeyBytes);    expectLastCall();    init();    metered.remove(new Windowed<>(key, new SessionWindow(0, 0)));    final KafkaMetric metric = metric("remove-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18334
0
shouldThrowNullPointerOnFetchRangeIfFromIsNull
public void kafkatest_f18342_0()
{    metered.fetch(null, "to");}
f18342
0
shouldThrowNullPointerOnFetchRangeIfToIsNull
public void kafkatest_f18343_0()
{    metered.fetch("from", null);}
f18343
0
shouldThrowNullPointerOnFindSessionsIfKeyIsNull
public void kafkatest_f18344_0()
{    metered.findSessions(null, 0, 0);}
f18344
0
testMetrics
public void kafkatest_f18352_0()
{    init();    final JmxReporter reporter = new JmxReporter("kafka.streams");    metrics.addReporter(reporter);    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", taskId.toString(), "scope", "metered")));    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", taskId.toString(), "scope", "all")));}
f18352
0
shouldWriteBytesToInnerStoreAndRecordPutMetric
public void kafkatest_f18353_0()
{    inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));    expectLastCall();    init();    metered.put(key, valueAndTimestamp);    final KafkaMetric metric = metric("put-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18353
0
shouldGetBytesFromInnerStoreAndReturnGetMetric
public void kafkatest_f18354_0()
{    expect(inner.get(keyBytes)).andReturn(valueAndTimestampBytes);    init();    assertThat(metered.get(key), equalTo(valueAndTimestamp));    final KafkaMetric metric = metric("get-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18354
0
shouldSetFlushListenerOnWrappedCachingStore
public void kafkatest_f18362_0()
{    final CachedKeyValueStore cachedKeyValueStore = mock(CachedKeyValueStore.class);    expect(cachedKeyValueStore.setFlushListener(anyObject(CacheFlushListener.class), eq(false))).andReturn(true);    replay(cachedKeyValueStore);    metered = new MeteredTimestampedKeyValueStore<>(cachedKeyValueStore, "scope", new MockTime(), Serdes.String(), new ValueAndTimestampSerde<>(Serdes.String()));    assertTrue(metered.setFlushListener(null, false));    verify(cachedKeyValueStore);}
f18362
0
shouldNotSetFlushListenerOnWrappedNoneCachingStore
public void kafkatest_f18363_0()
{    assertFalse(metered.setFlushListener(null, false));}
f18363
0
metric
private KafkaMetric kafkatest_f18364_0(final MetricName metricName)
{    return this.metrics.metric(metricName);}
f18364
0
setUp
public void kafkatest_f18372_0()
{    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, "test", StreamsConfig.METRICS_LATEST);    context = new InternalMockProcessorContext(TestUtils.tempDirectory(), Serdes.String(), Serdes.Long(), streamsMetrics, new StreamsConfig(StreamsTestUtils.getStreamsConfig()), NoOpRecordCollector::new, new ThreadCache(new LogContext("testCache "), 0, streamsMetrics));}
f18372
0
testMetrics
public void kafkatest_f18373_0()
{    replay(innerStoreMock);    store.init(context, store);    final JmxReporter reporter = new JmxReporter("kafka.streams");    metrics.addReporter(reporter);    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", context.taskId().toString(), "scope", "mocked-store")));    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", context.taskId().toString(), "scope", "all")));}
f18373
0
shouldRecordRestoreLatencyOnInit
public void kafkatest_f18374_0()
{    innerStoreMock.init(context, store);    expectLastCall();    replay(innerStoreMock);    store.init(context, store);    final Map<MetricName, ? extends Metric> metrics = context.metrics().metrics();    assertEquals(1.0, getMetricByNameFilterByTags(metrics, "restore-total", "stream-scope-state-metrics", singletonMap("scope-state-id", "all")).metricValue());    assertEquals(1.0, getMetricByNameFilterByTags(metrics, "restore-total", "stream-scope-state-metrics", singletonMap("scope-state-id", "mocked-store")).metricValue());}
f18374
0
shouldNotSetFlushListenerOnWrappedNoneCachingStore
public void kafkatest_f18382_0()
{    assertFalse(store.setFlushListener(null, false));}
f18382
0
shouldSetStatsLevelToExceptDetailedTimers
public void kafkatest_f18383_0()
{    mockStaticNice(RocksDBMetrics.class);    replay(RocksDBMetrics.class);    statisticsToAdd1.setStatsLevel(StatsLevel.EXCEPT_DETAILED_TIMERS);    replay(statisticsToAdd1);    recorder.addStatistics(SEGMENT_STORE_NAME_1, statisticsToAdd1, streamsMetrics, taskId);    verify(statisticsToAdd1);}
f18383
0
shouldInitMetricsOnlyWhenFirstStatisticsIsAdded
public void kafkatest_f18384_0()
{    replayMetricsInitialization();    statisticsToAdd1.setStatsLevel(StatsLevel.EXCEPT_DETAILED_TIMERS);    replay(statisticsToAdd1);    recorder.addStatistics(SEGMENT_STORE_NAME_1, statisticsToAdd1, streamsMetrics, taskId);    verify(RocksDBMetrics.class);    mockStatic(RocksDBMetrics.class);    replay(RocksDBMetrics.class);    recorder.addStatistics(SEGMENT_STORE_NAME_2, statisticsToAdd2, streamsMetrics, taskId);    verify(RocksDBMetrics.class);}
f18384
0
shouldGetMemtableAvgFlushTimeSensor
public void kafkatest_f18392_0()
{    final String metricNamePrefix = "memtable-flush-time-avg";    final String description = "Average time spent on flushing the memtable to disk in ms";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::memtableAvgFlushTimeSensor);}
f18392
0
shouldGetMemtableMinFlushTimeSensor
public void kafkatest_f18393_0()
{    final String metricNamePrefix = "memtable-flush-time-min";    final String description = "Minimum time spent on flushing the memtable to disk in ms";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::memtableMinFlushTimeSensor);}
f18393
0
shouldGetMemtableMaxFlushTimeSensor
public void kafkatest_f18394_0()
{    final String metricNamePrefix = "memtable-flush-time-max";    final String description = "Maximum time spent on flushing the memtable to disk in ms";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::memtableMaxFlushTimeSensor);}
f18394
0
shouldGetCompactionTimeMinSensor
public void kafkatest_f18402_0()
{    final String metricNamePrefix = "compaction-time-min";    final String description = "Minimum time spent on compaction in ms";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::compactionTimeMinSensor);}
f18402
0
shouldGetCompactionTimeMaxSensor
public void kafkatest_f18403_0()
{    final String metricNamePrefix = "compaction-time-max";    final String description = "Maximum time spent on compaction in ms";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::compactionTimeMaxSensor);}
f18403
0
shouldGetNumberOfOpenFilesSensor
public void kafkatest_f18404_0()
{    final String metricNamePrefix = "number-open-files";    final String description = "Number of currently open files";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::numberOfOpenFilesSensor);}
f18404
0
shouldKeepTrackOfMostRecentlyAndLeastRecentlyUsed
public void kafkatest_f18412_0() throws IOException
{    final List<KeyValue<String, String>> toInsert = Arrays.asList(new KeyValue<>("K1", "V1"), new KeyValue<>("K2", "V2"), new KeyValue<>("K3", "V3"), new KeyValue<>("K4", "V4"), new KeyValue<>("K5", "V5"));    for (int i = 0; i < toInsert.size(); i++) {        final byte[] key = toInsert.get(i).key.getBytes();        final byte[] value = toInsert.get(i).value.getBytes();        cache.put(Bytes.wrap(key), new LRUCacheEntry(value, null, true, 1, 1, 1, ""));        final LRUCacheEntry head = cache.first();        final LRUCacheEntry tail = cache.last();        assertEquals(new String(head.value()), toInsert.get(i).value);        assertEquals(new String(tail.value()), toInsert.get(0).value);        assertEquals(cache.flushes(), 0);        assertEquals(cache.hits(), 0);        assertEquals(cache.misses(), 0);        assertEquals(cache.overwrites(), 0);    }}
f18412
0
testMetrics
public void kafkatest_f18413_0()
{    final Map<String, String> metricTags = new LinkedHashMap<>();    metricTags.put("record-cache-id", underlyingStoreName);    metricTags.put("task-id", taskIDString);    metricTags.put("client-id", "test");    getMetricByNameFilterByTags(metrics.metrics(), "hitRatio-avg", "stream-record-cache-metrics", metricTags);    getMetricByNameFilterByTags(metrics.metrics(), "hitRatio-min", "stream-record-cache-metrics", metricTags);    getMetricByNameFilterByTags(metrics.metrics(), "hitRatio-max", "stream-record-cache-metrics", metricTags);    // test "all"    metricTags.put("record-cache-id", "all");    getMetricByNameFilterByTags(metrics.metrics(), "hitRatio-avg", "stream-record-cache-metrics", metricTags);    getMetricByNameFilterByTags(metrics.metrics(), "hitRatio-min", "stream-record-cache-metrics", metricTags);    getMetricByNameFilterByTags(metrics.metrics(), "hitRatio-max", "stream-record-cache-metrics", metricTags);    final JmxReporter reporter = new JmxReporter("kafka.streams");    innerMetrics.addReporter(reporter);    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-record-cache-metrics,client-id=test,task-id=%s,record-cache-id=%s", taskIDString, underlyingStoreName)));    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-record-cache-metrics,client-id=test,task-id=%s,record-cache-id=%s", taskIDString, "all")));}
f18413
0
shouldKeepTrackOfSize
public void kafkatest_f18414_0()
{    final LRUCacheEntry value = new LRUCacheEntry(new byte[] { 0 });    cache.put(Bytes.wrap(new byte[] { 0 }), value);    cache.put(Bytes.wrap(new byte[] { 1 }), value);    cache.put(Bytes.wrap(new byte[] { 2 }), value);    final long size = cache.sizeInBytes();    // 1 byte key + 24 bytes overhead    assertEquals((value.size() + 25) * 3, size);}
f18414
0
apply
public void kafkatest_f18422_0(final List<ThreadCache.DirtyEntry> dirty)
{    flushed.addAll(dirty);}
f18422
0
shouldNotThrowNullPointerWhenCacheIsEmptyAndEvictionCalled
public void kafkatest_f18423_0()
{    cache.evict();}
f18423
0
shouldThrowIllegalStateExceptionWhenTryingToOverwriteDirtyEntryWithCleanEntry
public void kafkatest_f18424_0()
{    cache.put(Bytes.wrap(new byte[] { 0 }), new LRUCacheEntry(new byte[] { 10 }, headers, true, 0, 0, 0, ""));    cache.put(Bytes.wrap(new byte[] { 0 }), new LRUCacheEntry(new byte[] { 10 }, null, false, 0, 0, 0, ""));}
f18424
0
testReadWrite
public void kafkatest_f18432_0() throws IOException
{    final File f = TestUtils.tempFile();    final OffsetCheckpoint checkpoint = new OffsetCheckpoint(f);    try {        final Map<TopicPartition, Long> offsets = new HashMap<>();        offsets.put(new TopicPartition(topic, 0), 0L);        offsets.put(new TopicPartition(topic, 1), 1L);        offsets.put(new TopicPartition(topic, 2), 2L);        checkpoint.write(offsets);        assertEquals(offsets, checkpoint.read());        checkpoint.delete();        assertFalse(f.exists());        offsets.put(new TopicPartition(topic, 3), 3L);        checkpoint.write(offsets);        assertEquals(offsets, checkpoint.read());    } finally {        checkpoint.delete();    }}
f18432
0
shouldNotWriteCheckpointWhenNoOffsets
public void kafkatest_f18433_0() throws IOException
{    // we do not need to worry about file name uniqueness since this file should not be created    final File f = new File(TestUtils.tempDirectory().getAbsolutePath(), "kafka.tmp");    final OffsetCheckpoint checkpoint = new OffsetCheckpoint(f);    checkpoint.write(Collections.<TopicPartition, Long>emptyMap());    assertFalse(f.exists());    assertEquals(Collections.<TopicPartition, Long>emptyMap(), checkpoint.read());    // deleting a non-exist checkpoint file should be fine    checkpoint.delete();}
f18433
0
before
public void kafkatest_f18434_0()
{    final StateStoreProviderStub theStoreProvider = new StateStoreProviderStub(false);    theStoreProvider.addStore(keyValueStore, new NoOpReadOnlyStore<>());    theStoreProvider.addStore(windowStore, new NoOpWindowStore());    globalStateStores = new HashMap<>();    storeProvider = new QueryableStoreProvider(Collections.<StateStoreProvider>singletonList(theStoreProvider), new GlobalStateStoreProvider(globalStateStores));}
f18434
0
setup
public void kafkatest_f18442_0()
{    readOnlyKeyValueStoreFacade = new ReadOnlyKeyValueStoreFacade<>(mockedKeyValueTimestampStore);}
f18442
0
shouldReturnPlainValueOnGet
public void kafkatest_f18443_0()
{    expect(mockedKeyValueTimestampStore.get("key")).andReturn(ValueAndTimestamp.make("value", 42L));    expect(mockedKeyValueTimestampStore.get("unknownKey")).andReturn(null);    replay(mockedKeyValueTimestampStore);    assertThat(readOnlyKeyValueStoreFacade.get("key"), is("value"));    assertNull(readOnlyKeyValueStoreFacade.get("unknownKey"));    verify(mockedKeyValueTimestampStore);}
f18443
0
shouldReturnPlainKeyValuePairsForRangeIterator
public void kafkatest_f18444_0()
{    expect(mockedKeyValueTimestampIterator.next()).andReturn(KeyValue.pair("key1", ValueAndTimestamp.make("value1", 21L))).andReturn(KeyValue.pair("key2", ValueAndTimestamp.make("value2", 42L)));    expect(mockedKeyValueTimestampStore.range("key1", "key2")).andReturn(mockedKeyValueTimestampIterator);    replay(mockedKeyValueTimestampIterator, mockedKeyValueTimestampStore);    final KeyValueIterator<String, String> iterator = readOnlyKeyValueStoreFacade.range("key1", "key2");    assertThat(iterator.next(), is(KeyValue.pair("key1", "value1")));    assertThat(iterator.next(), is(KeyValue.pair("key2", "value2")));    verify(mockedKeyValueTimestampIterator, mockedKeyValueTimestampStore);}
f18444
0
shouldReturnPlainKeyValuePairsOnRangeFetchInstantParameters
public void kafkatest_f18452_0()
{    expect(mockedKeyValueWindowTimestampIterator.next()).andReturn(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), ValueAndTimestamp.make("value1", 22L))).andReturn(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), ValueAndTimestamp.make("value2", 100L)));    expect(mockedWindowTimestampStore.fetch("key1", "key2", Instant.ofEpochMilli(21L), Instant.ofEpochMilli(42L))).andReturn(mockedKeyValueWindowTimestampIterator);    replay(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);    final KeyValueIterator<Windowed<String>, String> iterator = readOnlyWindowStoreFacade.fetch("key1", "key2", Instant.ofEpochMilli(21L), Instant.ofEpochMilli(42L));    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), "value1")));    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), "value2")));    verify(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);}
f18452
0
shouldReturnPlainKeyValuePairsOnFetchAllLongParameters
public void kafkatest_f18453_0()
{    expect(mockedKeyValueWindowTimestampIterator.next()).andReturn(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), ValueAndTimestamp.make("value1", 22L))).andReturn(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), ValueAndTimestamp.make("value2", 100L)));    expect(mockedWindowTimestampStore.fetchAll(21L, 42L)).andReturn(mockedKeyValueWindowTimestampIterator);    replay(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);    final KeyValueIterator<Windowed<String>, String> iterator = readOnlyWindowStoreFacade.fetchAll(21L, 42L);    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), "value1")));    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), "value2")));    verify(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);}
f18453
0
shouldReturnPlainKeyValuePairsOnFetchAllInstantParameters
public void kafkatest_f18454_0()
{    expect(mockedKeyValueWindowTimestampIterator.next()).andReturn(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), ValueAndTimestamp.make("value1", 22L))).andReturn(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), ValueAndTimestamp.make("value2", 100L)));    expect(mockedWindowTimestampStore.fetchAll(Instant.ofEpochMilli(21L), Instant.ofEpochMilli(42L))).andReturn(mockedKeyValueWindowTimestampIterator);    replay(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);    final KeyValueIterator<Windowed<String>, String> iterator = readOnlyWindowStoreFacade.fetchAll(Instant.ofEpochMilli(21L), Instant.ofEpochMilli(42L));    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), "value1")));    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), "value2")));    verify(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);}
f18454
0
next
public KeyValue<Windowed<K>, V> kafkatest_f18463_0()
{    return iterator.next();}
f18463
0
fetchAll
public KeyValueIterator<Windowed<K>, V> kafkatest_f18464_0(final long timeFrom, final long timeTo)
{    if (!open) {        throw new InvalidStateStoreException("Store is not open");    }    final List<KeyValue<Windowed<K>, V>> results = new ArrayList<>();    for (final long now : data.keySet()) {        if (!(now >= timeFrom && now <= timeTo)) {            continue;        }        final NavigableMap<K, V> kvMap = data.get(now);        if (kvMap != null) {            for (final Entry<K, V> entry : kvMap.entrySet()) {                results.add(new KeyValue<>(new Windowed<>(entry.getKey(), new TimeWindow(now, now + windowSize)), entry.getValue()));            }        }    }    final Iterator<KeyValue<Windowed<K>, V>> iterator = results.iterator();    return new KeyValueIterator<Windowed<K>, V>() {        @Override        public void close() {        }        @Override        public Windowed<K> peekNextKey() {            throw new UnsupportedOperationException("peekNextKey() not supported in " + getClass().getName());        }        @Override        public boolean hasNext() {            return iterator.hasNext();        }        @Override        public KeyValue<Windowed<K>, V> next() {            return iterator.next();        }    };}
f18464
0
peekNextKey
public Windowed<K> kafkatest_f18466_0()
{    throw new UnsupportedOperationException("peekNextKey() not supported in " + getClass().getName());}
f18466
0
fetch
public KeyValueIterator<Windowed<K>, V> kafkatest_f18475_0(final K from, final K to, final Instant fromTime, final Instant toTime) throws IllegalArgumentException
{    return fetch(from, to, ApiUtils.validateMillisecondInstant(fromTime, prepareMillisCheckFailMsgPrefix(fromTime, "fromTime")), ApiUtils.validateMillisecondInstant(toTime, prepareMillisCheckFailMsgPrefix(toTime, "toTime")));}
f18475
0
put
public void kafkatest_f18476_0(final K key, final V value, final long timestamp)
{    if (!data.containsKey(timestamp)) {        data.put(timestamp, new TreeMap<>());    }    data.get(timestamp).put(key, value);}
f18476
0
name
public String kafkatest_f18477_0()
{    return null;}
f18477
0
shouldAddTimestampToValueOnConversionWhenValueIsNotNull
public void kafkatest_f18489_0()
{    final long timestamp = 10L;    final byte[] value = new byte[1];    final ConsumerRecord<byte[], byte[]> inputRecord = new ConsumerRecord<>("topic", 1, 0, timestamp, TimestampType.CREATE_TIME, 0L, 0, 0, new byte[0], value);    final byte[] expectedValue = ByteBuffer.allocate(9).putLong(timestamp).put(value).array();    final byte[] actualValue = timestampedValueConverter.convert(inputRecord).value();    assertArrayEquals(expectedValue, actualValue);}
f18489
0
shouldOverwriteAllOptionsMethods
public void kafkatest_f18490_0() throws Exception
{    for (final Method method : Options.class.getMethods()) {        if (!ignoreMethods.contains(method.getName())) {            RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.class.getDeclaredMethod(method.getName(), method.getParameterTypes());        }    }}
f18490
0
shouldForwardAllDbOptionsCalls
public void kafkatest_f18491_0() throws Exception
{    for (final Method method : Options.class.getMethods()) {        if (!ignoreMethods.contains(method.getName())) {            try {                DBOptions.class.getMethod(method.getName(), method.getParameterTypes());                verifyDBOptionsMethodCall(method);            } catch (final NoSuchMethodException expectedAndSwallow) {            }        }    }}
f18491
0
createKeyValueStore
protected KeyValueStore<K, V> kafkatest_f18500_0(final ProcessorContext context)
{    final StoreBuilder storeBuilder = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore("my-store"), (Serde<K>) context.keySerde(), (Serde<V>) context.valueSerde());    final StateStore store = storeBuilder.build();    store.init(context, store);    return (KeyValueStore<K, V>) store;}
f18500
0
setConfig
public void kafkatest_f18501_0(final String storeName, final Options options, final Map<String, Object> configs)
{    called = true;}
f18501
0
shouldUseCustomRocksDbConfigSetter
public void kafkatest_f18502_0()
{    assertTrue(TheRocksDbConfigSetter.called);}
f18502
0
getMetricsScope
 String kafkatest_f18510_0()
{    return new RocksDbSessionBytesStoreSupplier(null, 0).metricsScope();}
f18510
0
setClassLoggerToDebug
 void kafkatest_f18511_0()
{    LogCaptureAppender.setClassLoggerToDebug(AbstractRocksDBSegmentedBytesStore.class);}
f18511
0
shouldRemoveExpired
public void kafkatest_f18512_0()
{    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 0)), 1L);    sessionStore.put(new Windowed<>("aa", new SessionWindow(0, SEGMENT_INTERVAL)), 2L);    sessionStore.put(new Windowed<>("a", new SessionWindow(10, SEGMENT_INTERVAL)), 3L);    // Advance stream time to expire the first record    sessionStore.put(new Windowed<>("aa", new SessionWindow(10, 2 * SEGMENT_INTERVAL)), 4L);    try (final KeyValueIterator<Windowed<String>, Long> iterator = sessionStore.findSessions("a", "b", 0L, Long.MAX_VALUE)) {        assertEquals(valuesToSet(iterator), new HashSet<>(Arrays.asList(2L, 3L, 4L)));    }}
f18512
0
shouldNotRemoveStatisticsFromInjectedMetricsRecorderOnCloseWhenUserProvidsStatistics
public void kafkatest_f18520_0()
{    final RocksDBMetricsRecorder metricsRecorder = mock(RocksDBMetricsRecorder.class);    replay(metricsRecorder);    final RocksDBStore store = new RocksDBStore(DB_NAME, METRICS_SCOPE, metricsRecorder);    final ProcessorContext mockContext = mock(ProcessorContext.class);    expect(mockContext.appConfigs()).andReturn(mkMap(mkEntry(METRICS_RECORDING_LEVEL_CONFIG, "DEBUG"), mkEntry(ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, TestRocksDBConfigSetter.class)));    final String directoryPath = TestUtils.tempDirectory().getAbsolutePath();    final File directory = new File(directoryPath);    expect(mockContext.stateDir()).andReturn(directory);    replay(mockContext);    store.openDB(mockContext);    store.close();    verify(metricsRecorder);}
f18520
0
shouldRespectBulkloadOptionsDuringInit
public void kafkatest_f18521_0()
{    rocksDBStore.init(context, rocksDBStore);    final StateRestoreListener restoreListener = context.getRestoreListener(rocksDBStore.name());    restoreListener.onRestoreStart(null, rocksDBStore.name(), 0L, 0L);    assertThat(rocksDBStore.getOptions().level0FileNumCompactionTrigger(), equalTo(1 << 30));    assertThat(rocksDBStore.getOptions().level0SlowdownWritesTrigger(), equalTo(1 << 30));    assertThat(rocksDBStore.getOptions().level0StopWritesTrigger(), equalTo(1 << 30));    restoreListener.onRestoreEnd(null, rocksDBStore.name(), 0L);    assertThat(rocksDBStore.getOptions().level0FileNumCompactionTrigger(), equalTo(10));    assertThat(rocksDBStore.getOptions().level0SlowdownWritesTrigger(), equalTo(20));    assertThat(rocksDBStore.getOptions().level0StopWritesTrigger(), equalTo(36));}
f18521
0
shouldNotThrowExceptionOnRestoreWhenThereIsPreExistingRocksDbFiles
public void kafkatest_f18522_0()
{    rocksDBStore.init(context, rocksDBStore);    final String message = "how can a 4 ounce bird carry a 2lb coconut";    int intKey = 1;    for (int i = 0; i < 2000000; i++) {        rocksDBStore.put(new Bytes(stringSerializer.serialize(null, "theKeyIs" + intKey++)), stringSerializer.serialize(null, message));    }    final List<KeyValue<byte[], byte[]>> restoreBytes = new ArrayList<>();    final byte[] restoredKey = "restoredKey".getBytes(UTF_8);    final byte[] restoredValue = "restoredValue".getBytes(UTF_8);    restoreBytes.add(KeyValue.pair(restoredKey, restoredValue));    context.restore(DB_NAME, restoreBytes);    assertThat(stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "restoredKey")))), equalTo("restoredValue"));}
f18522
0
shouldHandleDeletesOnRestoreAll
public void kafkatest_f18530_0()
{    final List<KeyValue<byte[], byte[]>> entries = getKeyValueEntries();    entries.add(new KeyValue<>("1".getBytes(UTF_8), null));    rocksDBStore.init(context, rocksDBStore);    context.restore(rocksDBStore.name(), entries);    final KeyValueIterator<Bytes, byte[]> iterator = rocksDBStore.all();    final Set<String> keys = new HashSet<>();    while (iterator.hasNext()) {        keys.add(stringDeserializer.deserialize(null, iterator.next().key.get()));    }    assertThat(keys, equalTo(Utils.mkSet("2", "3")));}
f18530
0
shouldHandleDeletesAndPutbackOnRestoreAll
public void kafkatest_f18531_0()
{    final List<KeyValue<byte[], byte[]>> entries = new ArrayList<>();    entries.add(new KeyValue<>("1".getBytes(UTF_8), "a".getBytes(UTF_8)));    entries.add(new KeyValue<>("2".getBytes(UTF_8), "b".getBytes(UTF_8)));    // this will be deleted    entries.add(new KeyValue<>("1".getBytes(UTF_8), null));    entries.add(new KeyValue<>("3".getBytes(UTF_8), "c".getBytes(UTF_8)));    // this will restore key "1" as WriteBatch applies updates in order    entries.add(new KeyValue<>("1".getBytes(UTF_8), "restored".getBytes(UTF_8)));    rocksDBStore.init(context, rocksDBStore);    context.restore(rocksDBStore.name(), entries);    final KeyValueIterator<Bytes, byte[]> iterator = rocksDBStore.all();    final Set<String> keys = new HashSet<>();    while (iterator.hasNext()) {        keys.add(stringDeserializer.deserialize(null, iterator.next().key.get()));    }    assertThat(keys, equalTo(Utils.mkSet("1", "2", "3")));    assertEquals("restored", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "1")))));    assertEquals("b", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "2")))));    assertEquals("c", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "3")))));}
f18531
0
shouldRestoreThenDeleteOnRestoreAll
public void kafkatest_f18532_0()
{    final List<KeyValue<byte[], byte[]>> entries = getKeyValueEntries();    rocksDBStore.init(context, rocksDBStore);    context.restore(rocksDBStore.name(), entries);    assertEquals("a", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "1")))));    assertEquals("b", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "2")))));    assertEquals("c", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "3")))));    entries.clear();    entries.add(new KeyValue<>("2".getBytes(UTF_8), "b".getBytes(UTF_8)));    entries.add(new KeyValue<>("3".getBytes(UTF_8), "c".getBytes(UTF_8)));    entries.add(new KeyValue<>("1".getBytes(UTF_8), null));    context.restore(rocksDBStore.name(), entries);    final KeyValueIterator<Bytes, byte[]> iterator = rocksDBStore.all();    final Set<String> keys = new HashSet<>();    while (iterator.hasNext()) {        keys.add(stringDeserializer.deserialize(null, iterator.next().key.get()));    }    assertThat(keys, equalTo(Utils.mkSet("2", "3")));}
f18532
0
setConfig
public void kafkatest_f18540_0(final String storeName, final Options options, final Map<String, Object> configs)
{    called = true;    options.setLevel0FileNumCompactionTrigger(10);}
f18540
0
setConfig
public void kafkatest_f18541_0(final String storeName, final Options options, final Map<String, Object> configs)
{    final BlockBasedTableConfig tableConfig = new BlockBasedTableConfig();    cache = new LRUCache(50 * 1024 * 1024L);    tableConfig.setBlockCache(cache);    tableConfig.setBlockSize(4096L);    if (enableBloomFilters) {        filter = new BloomFilter();        tableConfig.setFilter(filter);        options.optimizeFiltersForHits();        bloomFiltersSet = true;    } else {        options.setOptimizeFiltersForHits(false);        bloomFiltersSet = false;    }    options.setTableFormatConfig(tableConfig);}
f18541
0
close
public void kafkatest_f18542_0(final String storeName, final Options options)
{    if (filter != null) {        filter.close();    }    cache.close();}
f18542
0
shouldMigrateDataFromDefaultToTimestampColumnFamily
public void kafkatest_f18550_0() throws Exception
{    prepareOldStore();    LogCaptureAppender.setClassLoggerToDebug(RocksDBTimestampedStore.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    rocksDBStore.init(context, rocksDBStore);    assertThat(appender.getMessages(), hasItem("Opening store " + DB_NAME + " in upgrade mode"));    LogCaptureAppender.unregister(appender);    // approx: 7 entries on old CF, 0 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(7L));    // get()    // should be no-op on both CF    assertThat(rocksDBStore.get(new Bytes("unknown".getBytes())), new IsNull<>());    // approx: 7 entries on old CF, 0 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(7L));    // should migrate key1 from old to new CF    // must return timestamp plus value, ie, it's not 1 byte but 9 bytes    assertThat(rocksDBStore.get(new Bytes("key1".getBytes())).length, is(8 + 1));    // one delete on old CF, one put on new CF    // approx: 6 entries on old CF, 1 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(7L));    // put()    // should migrate key2 from old to new CF with new value    rocksDBStore.put(new Bytes("key2".getBytes()), "timestamp+22".getBytes());    // one delete on old CF, one put on new CF    // approx: 5 entries on old CF, 2 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(7L));    // should delete key3 from old and new CF    rocksDBStore.put(new Bytes("key3".getBytes()), null);    // count is off by one, due to two delete operations (even if one does not delete anything)    // approx: 4 entries on old CF, 1 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(5L));    // should add new key8 to new CF    rocksDBStore.put(new Bytes("key8".getBytes()), "timestamp+88888888".getBytes());    // one delete on old CF, one put on new CF    // approx: 3 entries on old CF, 2 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(5L));    // putIfAbsent()    // should migrate key4 from old to new CF with old value    assertThat(rocksDBStore.putIfAbsent(new Bytes("key4".getBytes()), "timestamp+4444".getBytes()).length, is(8 + 4));    // one delete on old CF, one put on new CF    // approx: 2 entries on old CF, 3 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(5L));    // should add new key11 to new CF    assertThat(rocksDBStore.putIfAbsent(new Bytes("key11".getBytes()), "timestamp+11111111111".getBytes()), new IsNull<>());    // one delete on old CF, one put on new CF    // approx: 1 entries on old CF, 4 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(5L));    // should not delete key5 but migrate to new CF    assertThat(rocksDBStore.putIfAbsent(new Bytes("key5".getBytes()), null).length, is(8 + 5));    // one delete on old CF, one put on new CF    // approx: 0 entries on old CF, 5 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(5L));    // should be no-op on both CF    assertThat(rocksDBStore.putIfAbsent(new Bytes("key12".getBytes()), null), new IsNull<>());    // two delete operation, however, only one is counted because old CF count was zero before already    // approx: 0 entries on old CF, 4 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(4L));    // delete()    // should delete key6 from old and new CF    assertThat(rocksDBStore.delete(new Bytes("key6".getBytes())).length, is(8 + 6));    // two delete operation, however, only one is counted because old CF count was zero before already    // approx: 0 entries on old CF, 3 in new CF    assertThat(rocksDBStore.approximateNumEntries(), is(3L));    iteratorsShouldNotMigrateData();    assertThat(rocksDBStore.approximateNumEntries(), is(3L));    rocksDBStore.close();    verifyOldAndNewColumnFamily();}
f18550
0
iteratorsShouldNotMigrateData
private void kafkatest_f18551_0()
{    // iterating should not migrate any data, but return all key over both CF (plus surrogate timestamps for old CF)    try (final KeyValueIterator<Bytes, byte[]> itAll = rocksDBStore.all()) {        {            final KeyValue<Bytes, byte[]> keyValue = itAll.next();            assertArrayEquals("key1".getBytes(), keyValue.key.get());            // unknown timestamp == -1 plus value == 1            assertArrayEquals(new byte[] { -1, -1, -1, -1, -1, -1, -1, -1, '1' }, keyValue.value);        }        {            final KeyValue<Bytes, byte[]> keyValue = itAll.next();            assertArrayEquals("key11".getBytes(), keyValue.key.get());            assertArrayEquals(new byte[] { 't', 'i', 'm', 'e', 's', 't', 'a', 'm', 'p', '+', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1' }, keyValue.value);        }        {            final KeyValue<Bytes, byte[]> keyValue = itAll.next();            assertArrayEquals("key2".getBytes(), keyValue.key.get());            assertArrayEquals(new byte[] { 't', 'i', 'm', 'e', 's', 't', 'a', 'm', 'p', '+', '2', '2' }, keyValue.value);        }        {            final KeyValue<Bytes, byte[]> keyValue = itAll.next();            assertArrayEquals("key4".getBytes(), keyValue.key.get());            // unknown timestamp == -1 plus value == 4444            assertArrayEquals(new byte[] { -1, -1, -1, -1, -1, -1, -1, -1, '4', '4', '4', '4' }, keyValue.value);        }        {            final KeyValue<Bytes, byte[]> keyValue = itAll.next();            assertArrayEquals("key5".getBytes(), keyValue.key.get());            // unknown timestamp == -1 plus value == 55555            assertArrayEquals(new byte[] { -1, -1, -1, -1, -1, -1, -1, -1, '5', '5', '5', '5', '5' }, keyValue.value);        }        {            final KeyValue<Bytes, byte[]> keyValue = itAll.next();            assertArrayEquals("key7".getBytes(), keyValue.key.get());            // unknown timestamp == -1 plus value == 7777777            assertArrayEquals(new byte[] { -1, -1, -1, -1, -1, -1, -1, -1, '7', '7', '7', '7', '7', '7', '7' }, keyValue.value);        }        {            final KeyValue<Bytes, byte[]> keyValue = itAll.next();            assertArrayEquals("key8".getBytes(), keyValue.key.get());            assertArrayEquals(new byte[] { 't', 'i', 'm', 'e', 's', 't', 'a', 'm', 'p', '+', '8', '8', '8', '8', '8', '8', '8', '8' }, keyValue.value);        }        assertFalse(itAll.hasNext());    }    try (final KeyValueIterator<Bytes, byte[]> it = rocksDBStore.range(new Bytes("key2".getBytes()), new Bytes("key5".getBytes()))) {        {            final KeyValue<Bytes, byte[]> keyValue = it.next();            assertArrayEquals("key2".getBytes(), keyValue.key.get());            assertArrayEquals(new byte[] { 't', 'i', 'm', 'e', 's', 't', 'a', 'm', 'p', '+', '2', '2' }, keyValue.value);        }        {            final KeyValue<Bytes, byte[]> keyValue = it.next();            assertArrayEquals("key4".getBytes(), keyValue.key.get());            // unknown timestamp == -1 plus value == 4444            assertArrayEquals(new byte[] { -1, -1, -1, -1, -1, -1, -1, -1, '4', '4', '4', '4' }, keyValue.value);        }        {            final KeyValue<Bytes, byte[]> keyValue = it.next();            assertArrayEquals("key5".getBytes(), keyValue.key.get());            // unknown timestamp == -1 plus value == 55555            assertArrayEquals(new byte[] { -1, -1, -1, -1, -1, -1, -1, -1, '5', '5', '5', '5', '5' }, keyValue.value);        }        assertFalse(it.hasNext());    }}
f18551
0
verifyOldAndNewColumnFamily
private void kafkatest_f18552_0() throws Exception
{    final DBOptions dbOptions = new DBOptions();    final ColumnFamilyOptions columnFamilyOptions = new ColumnFamilyOptions();    final List<ColumnFamilyDescriptor> columnFamilyDescriptors = asList(new ColumnFamilyDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY, columnFamilyOptions), new ColumnFamilyDescriptor("keyValueWithTimestamp".getBytes(StandardCharsets.UTF_8), columnFamilyOptions));    final List<ColumnFamilyHandle> columnFamilies = new ArrayList<>(columnFamilyDescriptors.size());    RocksDB db = null;    ColumnFamilyHandle noTimestampColumnFamily = null, withTimestampColumnFamily = null;    boolean errorOccurred = false;    try {        db = RocksDB.open(dbOptions, new File(new File(context.stateDir(), "rocksdb"), DB_NAME).getAbsolutePath(), columnFamilyDescriptors, columnFamilies);        noTimestampColumnFamily = columnFamilies.get(0);        withTimestampColumnFamily = columnFamilies.get(1);        assertThat(db.get(noTimestampColumnFamily, "unknown".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key1".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key2".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key3".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key4".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key5".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key6".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key7".getBytes()).length, is(7));        assertThat(db.get(noTimestampColumnFamily, "key8".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key11".getBytes()), new IsNull<>());        assertThat(db.get(noTimestampColumnFamily, "key12".getBytes()), new IsNull<>());        assertThat(db.get(withTimestampColumnFamily, "unknown".getBytes()), new IsNull<>());        assertThat(db.get(withTimestampColumnFamily, "key1".getBytes()).length, is(8 + 1));        assertThat(db.get(withTimestampColumnFamily, "key2".getBytes()).length, is(12));        assertThat(db.get(withTimestampColumnFamily, "key3".getBytes()), new IsNull<>());        assertThat(db.get(withTimestampColumnFamily, "key4".getBytes()).length, is(8 + 4));        assertThat(db.get(withTimestampColumnFamily, "key5".getBytes()).length, is(8 + 5));        assertThat(db.get(withTimestampColumnFamily, "key6".getBytes()), new IsNull<>());        assertThat(db.get(withTimestampColumnFamily, "key7".getBytes()), new IsNull<>());        assertThat(db.get(withTimestampColumnFamily, "key8".getBytes()).length, is(18));        assertThat(db.get(withTimestampColumnFamily, "key11".getBytes()).length, is(21));        assertThat(db.get(withTimestampColumnFamily, "key12".getBytes()), new IsNull<>());    } catch (final RuntimeException fatal) {        errorOccurred = true;    } finally {        // Order of closing must follow: ColumnFamilyHandle > RocksDB > DBOptions > ColumnFamilyOptions        if (noTimestampColumnFamily != null) {            noTimestampColumnFamily.close();        }        if (withTimestampColumnFamily != null) {            withTimestampColumnFamily.close();        }        if (db != null) {            db.close();        }        if (errorOccurred) {            dbOptions.close();            columnFamilyOptions.close();        }    }    // check that still in upgrade mode    LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    rocksDBStore.init(context, rocksDBStore);    assertThat(appender.getMessages(), hasItem("Opening store " + DB_NAME + " in upgrade mode"));    LogCaptureAppender.unregister(appender);    rocksDBStore.close();    // clear old CF    columnFamilies.clear();    db = null;    noTimestampColumnFamily = null;    try {        db = RocksDB.open(dbOptions, new File(new File(context.stateDir(), "rocksdb"), DB_NAME).getAbsolutePath(), columnFamilyDescriptors, columnFamilies);        noTimestampColumnFamily = columnFamilies.get(0);        db.delete(noTimestampColumnFamily, "key7".getBytes());    } finally {        // Order of closing must follow: ColumnFamilyHandle > RocksDB > DBOptions > ColumnFamilyOptions        if (noTimestampColumnFamily != null) {            noTimestampColumnFamily.close();        }        if (db != null) {            db.close();        }        dbOptions.close();        columnFamilyOptions.close();    }    // check that still in regular mode    appender = LogCaptureAppender.createAndRegister();    rocksDBStore.init(context, rocksDBStore);    assertThat(appender.getMessages(), hasItem("Opening store " + DB_NAME + " in regular mode"));    LogCaptureAppender.unregister(appender);}
f18552
0
testInitialLoading
public void kafkatest_f18560_0()
{    final File storeDir = new File(baseDir, STORE_NAME);    new File(storeDir, segments.segmentName(0L)).mkdir();    new File(storeDir, segments.segmentName(1L)).mkdir();    new File(storeDir, segments.segmentName(2L)).mkdir();    new File(storeDir, segments.segmentName(3L)).mkdir();    new File(storeDir, segments.segmentName(4L)).mkdir();    new File(storeDir, segments.segmentName(5L)).mkdir();    new File(storeDir, segments.segmentName(6L)).mkdir();    windowStore.close();    windowStore = buildWindowStore(RETENTION_PERIOD, WINDOW_SIZE, false, Serdes.Integer(), Serdes.String());    windowStore.init(context, windowStore);    // put something in the store to advance its stream time and expire the old segments    windowStore.put(1, "v", 6L * SEGMENT_INTERVAL);    final List<String> expected = asList(segments.segmentName(4L), segments.segmentName(5L), segments.segmentName(6L));    expected.sort(String::compareTo);    final List<String> actual = Utils.toList(segmentDirs(baseDir).iterator());    actual.sort(String::compareTo);    assertEquals(expected, actual);    try (final WindowStoreIterator iter = windowStore.fetch(0, ofEpochMilli(0L), ofEpochMilli(1000000L))) {        while (iter.hasNext()) {            iter.next();        }    }    assertEquals(Utils.mkSet(segments.segmentName(4L), segments.segmentName(5L), segments.segmentName(6L)), segmentDirs(baseDir));}
f18560
0
testRestore
public void kafkatest_f18561_0() throws Exception
{    final long startTime = SEGMENT_INTERVAL * 2;    final long increment = SEGMENT_INTERVAL / 2;    setCurrentTime(startTime);    windowStore.put(0, "zero");    setCurrentTime(startTime + increment);    windowStore.put(1, "one");    setCurrentTime(startTime + increment * 2);    windowStore.put(2, "two");    setCurrentTime(startTime + increment * 3);    windowStore.put(3, "three");    setCurrentTime(startTime + increment * 4);    windowStore.put(4, "four");    setCurrentTime(startTime + increment * 5);    windowStore.put(5, "five");    setCurrentTime(startTime + increment * 6);    windowStore.put(6, "six");    setCurrentTime(startTime + increment * 7);    windowStore.put(7, "seven");    setCurrentTime(startTime + increment * 8);    windowStore.put(8, "eight");    windowStore.flush();    windowStore.close();    // remove local store image    Utils.delete(baseDir);    windowStore = buildWindowStore(RETENTION_PERIOD, WINDOW_SIZE, false, Serdes.Integer(), Serdes.String());    windowStore.init(context, windowStore);    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(6, ofEpochMilli(startTime + increment * 6 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 6 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(7, ofEpochMilli(startTime + increment * 7 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 7 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(8, ofEpochMilli(startTime + increment * 8 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 8 + WINDOW_SIZE))));    context.restore(STORE_NAME, changeLog);    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("six")), toSet(windowStore.fetch(6, ofEpochMilli(startTime + increment * 6 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 6 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("seven")), toSet(windowStore.fetch(7, ofEpochMilli(startTime + increment * 7 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 7 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("eight")), toSet(windowStore.fetch(8, ofEpochMilli(startTime + increment * 8 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 8 + WINDOW_SIZE))));    // check segment directories    windowStore.flush();    assertEquals(Utils.mkSet(segments.segmentName(4L), segments.segmentName(5L), segments.segmentName(6L)), segmentDirs(baseDir));}
f18561
0
segmentDirs
private Set<String> kafkatest_f18562_0(final File baseDir)
{    final File windowDir = new File(baseDir, windowStore.name());    return new HashSet<>(asList(requireNonNull(windowDir.list())));}
f18562
0
shouldNotThrowExceptionOnHasNextWhenStoreClosed
public void kafkatest_f18570_0()
{    iterator = new SegmentIterator<>(Collections.singletonList(segmentOne).iterator(), hasNextCondition, Bytes.wrap("a".getBytes()), Bytes.wrap("z".getBytes()));    iterator.currentIterator = segmentOne.all();    segmentOne.close();    assertFalse(iterator.hasNext());}
f18570
0
shouldOnlyIterateOverSegmentsInRange
public void kafkatest_f18571_0()
{    iterator = new SegmentIterator<>(Arrays.asList(segmentOne, segmentTwo).iterator(), hasNextCondition, Bytes.wrap("a".getBytes()), Bytes.wrap("b".getBytes()));    assertTrue(iterator.hasNext());    assertEquals("a", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("a", "1"), toStringKeyValue(iterator.next()));    assertTrue(iterator.hasNext());    assertEquals("b", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("b", "2"), toStringKeyValue(iterator.next()));    assertFalse(iterator.hasNext());}
f18571
0
shouldThrowNoSuchElementOnPeekNextKeyIfNoNext
public void kafkatest_f18572_0()
{    iterator = new SegmentIterator<>(Arrays.asList(segmentOne, segmentTwo).iterator(), hasNextCondition, Bytes.wrap("f".getBytes()), Bytes.wrap("h".getBytes()));    iterator.peekNextKey();}
f18572
0
setUp
public void kafkatest_f18580_0()
{    sessionStore = buildSessionStore(RETENTION_PERIOD, Serdes.String(), Serdes.Long());    final RecordCollector recordCollector = createRecordCollector(sessionStore.name());    recordCollector.init(producer);    context = new InternalMockProcessorContext(TestUtils.tempDirectory(), Serdes.String(), Serdes.Long(), recordCollector, new ThreadCache(new LogContext("testCache"), 0, new MockStreamsMetrics(new Metrics())));    sessionStore.init(context, sessionStore);}
f18580
0
after
public void kafkatest_f18581_0()
{    sessionStore.close();}
f18581
0
shouldPutAndFindSessionsInRange
public void kafkatest_f18582_0()
{    final String key = "a";    final Windowed<String> a1 = new Windowed<>(key, new SessionWindow(10, 10L));    final Windowed<String> a2 = new Windowed<>(key, new SessionWindow(500L, 1000L));    sessionStore.put(a1, 1L);    sessionStore.put(a2, 2L);    sessionStore.put(new Windowed<>(key, new SessionWindow(1500L, 2000L)), 1L);    sessionStore.put(new Windowed<>(key, new SessionWindow(2500L, 3000L)), 2L);    final List<KeyValue<Windowed<String>, Long>> expected = Arrays.asList(KeyValue.pair(a1, 1L), KeyValue.pair(a2, 2L));    try (final KeyValueIterator<Windowed<String>, Long> values = sessionStore.findSessions(key, 0, 1000L)) {        assertEquals(new HashSet<>(expected), toSet(values));    }    final List<KeyValue<Windowed<String>, Long>> expected2 = Collections.singletonList(KeyValue.pair(a2, 2L));    try (final KeyValueIterator<Windowed<String>, Long> values2 = sessionStore.findSessions(key, 400L, 600L)) {        assertEquals(new HashSet<>(expected2), toSet(values2));    }}
f18582
0
shouldFindSessionsToMerge
public void kafkatest_f18590_0()
{    final Windowed<String> session1 = new Windowed<>("a", new SessionWindow(0, 100));    final Windowed<String> session2 = new Windowed<>("a", new SessionWindow(101, 200));    final Windowed<String> session3 = new Windowed<>("a", new SessionWindow(201, 300));    final Windowed<String> session4 = new Windowed<>("a", new SessionWindow(301, 400));    final Windowed<String> session5 = new Windowed<>("a", new SessionWindow(401, 500));    sessionStore.put(session1, 1L);    sessionStore.put(session2, 2L);    sessionStore.put(session3, 3L);    sessionStore.put(session4, 4L);    sessionStore.put(session5, 5L);    final List<KeyValue<Windowed<String>, Long>> expected = Arrays.asList(KeyValue.pair(session2, 2L), KeyValue.pair(session3, 3L));    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 150, 300)) {        assertEquals(new HashSet<>(expected), toSet(results));    }}
f18590
0
shouldFetchExactKeys
public void kafkatest_f18591_0()
{    sessionStore = buildSessionStore(0x7a00000000000000L, Serdes.String(), Serdes.Long());    sessionStore.init(context, sessionStore);    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 0)), 1L);    sessionStore.put(new Windowed<>("aa", new SessionWindow(0, 10)), 2L);    sessionStore.put(new Windowed<>("a", new SessionWindow(10, 20)), 3L);    sessionStore.put(new Windowed<>("aa", new SessionWindow(10, 20)), 4L);    sessionStore.put(new Windowed<>("a", new SessionWindow(0x7a00000000000000L - 2, 0x7a00000000000000L - 1)), 5L);    try (final KeyValueIterator<Windowed<String>, Long> iterator = sessionStore.findSessions("a", 0, Long.MAX_VALUE)) {        assertThat(valuesToSet(iterator), equalTo(new HashSet<>(asList(1L, 3L, 5L))));    }    try (final KeyValueIterator<Windowed<String>, Long> iterator = sessionStore.findSessions("aa", 0, Long.MAX_VALUE)) {        assertThat(valuesToSet(iterator), equalTo(new HashSet<>(asList(2L, 4L))));    }    try (final KeyValueIterator<Windowed<String>, Long> iterator = sessionStore.findSessions("a", "aa", 0, Long.MAX_VALUE)) {        assertThat(valuesToSet(iterator), equalTo(new HashSet<>(asList(1L, 2L, 3L, 4L, 5L))));    }    try (final KeyValueIterator<Windowed<String>, Long> iterator = sessionStore.findSessions("a", "aa", 10, 0)) {        assertThat(valuesToSet(iterator), equalTo(new HashSet<>(Collections.singletonList(2L))));    }}
f18591
0
shouldFetchAndIterateOverExactBinaryKeys
public void kafkatest_f18592_0()
{    final SessionStore<Bytes, String> sessionStore = buildSessionStore(RETENTION_PERIOD, Serdes.Bytes(), Serdes.String());    sessionStore.init(context, sessionStore);    final Bytes key1 = Bytes.wrap(new byte[] { 0 });    final Bytes key2 = Bytes.wrap(new byte[] { 0, 0 });    final Bytes key3 = Bytes.wrap(new byte[] { 0, 0, 0 });    sessionStore.put(new Windowed<>(key1, new SessionWindow(1, 100)), "1");    sessionStore.put(new Windowed<>(key2, new SessionWindow(2, 100)), "2");    sessionStore.put(new Windowed<>(key3, new SessionWindow(3, 100)), "3");    sessionStore.put(new Windowed<>(key1, new SessionWindow(4, 100)), "4");    sessionStore.put(new Windowed<>(key2, new SessionWindow(5, 100)), "5");    sessionStore.put(new Windowed<>(key3, new SessionWindow(6, 100)), "6");    sessionStore.put(new Windowed<>(key1, new SessionWindow(7, 100)), "7");    sessionStore.put(new Windowed<>(key2, new SessionWindow(8, 100)), "8");    sessionStore.put(new Windowed<>(key3, new SessionWindow(9, 100)), "9");    final Set<String> expectedKey1 = new HashSet<>(asList("1", "4", "7"));    assertThat(valuesToSet(sessionStore.findSessions(key1, 0L, Long.MAX_VALUE)), equalTo(expectedKey1));    final Set<String> expectedKey2 = new HashSet<>(asList("2", "5", "8"));    assertThat(valuesToSet(sessionStore.findSessions(key2, 0L, Long.MAX_VALUE)), equalTo(expectedKey2));    final Set<String> expectedKey3 = new HashSet<>(asList("3", "6", "9"));    assertThat(valuesToSet(sessionStore.findSessions(key3, 0L, Long.MAX_VALUE)), equalTo(expectedKey3));}
f18592
0
shouldThrowNullPointerExceptionOnFindSessionsNullFromKey
public void kafkatest_f18600_0()
{    sessionStore.findSessions(null, "anyKeyTo", 1L, 2L);}
f18600
0
shouldThrowNullPointerExceptionOnFindSessionsNullToKey
public void kafkatest_f18601_0()
{    sessionStore.findSessions("anyKeyFrom", null, 1L, 2L);}
f18601
0
shouldThrowNullPointerExceptionOnFetchNullFromKey
public void kafkatest_f18602_0()
{    sessionStore.fetch(null, "anyToKey");}
f18602
0
before
public void kafkatest_f18610_0()
{    final List<KeyValue<Bytes, Integer>> keys = Arrays.asList(KeyValue.pair(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0, 0 }), new SessionWindow(0, 0))), 1), KeyValue.pair(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0 }), new SessionWindow(0, 0))), 2), KeyValue.pair(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0, 0, 0 }), new SessionWindow(0, 0))), 3), KeyValue.pair(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0 }), new SessionWindow(10, 20))), 4), KeyValue.pair(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0, 0 }), new SessionWindow(10, 20))), 5), KeyValue.pair(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0, 0, 0 }), new SessionWindow(10, 20))), 6));    iterator = new DelegatingPeekingKeyValueIterator<>("foo", new KeyValueIteratorStub<>(keys.iterator()));}
f18610
0
shouldFetchExactKeysSkippingLongerKeys
public void kafkatest_f18611_0()
{    final Bytes key = Bytes.wrap(new byte[] { 0 });    final List<Integer> result = getValues(sessionKeySchema.hasNextCondition(key, key, 0, Long.MAX_VALUE));    assertThat(result, equalTo(Arrays.asList(2, 4)));}
f18611
0
shouldFetchExactKeySkippingShorterKeys
public void kafkatest_f18612_0()
{    final Bytes key = Bytes.wrap(new byte[] { 0, 0 });    final HasNextCondition hasNextCondition = sessionKeySchema.hasNextCondition(key, key, 0, Long.MAX_VALUE);    final List<Integer> results = getValues(hasNextCondition);    assertThat(results, equalTo(Arrays.asList(1, 5)));}
f18612
0
shouldSerializeNullToNull
public void kafkatest_f18620_0()
{    assertNull(keySerde.serializer().serialize(topic, null));}
f18620
0
shouldDeSerializeEmtpyByteArrayToNull
public void kafkatest_f18621_0()
{    assertNull(keySerde.deserializer().deserialize(topic, new byte[0]));}
f18621
0
shouldDeSerializeNullToNull
public void kafkatest_f18622_0()
{    assertNull(keySerde.deserializer().deserialize(topic, null));}
f18622
0
getValues
private List<Integer> kafkatest_f18630_0(final HasNextCondition hasNextCondition)
{    final List<Integer> results = new ArrayList<>();    while (hasNextCondition.hasNext(iterator)) {        results.add(iterator.next().value);    }    return results;}
f18630
0
setUp
public void kafkatest_f18631_0() throws Exception
{    expect(supplier.get()).andReturn(inner);    expect(supplier.name()).andReturn("name");    replay(supplier);    builder = new SessionStoreBuilder<>(supplier, Serdes.String(), Serdes.String(), new MockTime());}
f18631
0
shouldHaveMeteredStoreAsOuterStore
public void kafkatest_f18632_0()
{    final SessionStore<String, String> store = builder.build();    assertThat(store, instanceOf(MeteredSessionStore.class));}
f18632
0
shouldThrowNullPointerIfValueSerdeIsNull
public void kafkatest_f18640_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> new SessionStoreBuilder<>(supplier, Serdes.String(), null, new MockTime()));    assertThat(e.getMessage(), equalTo("name cannot be null"));}
f18640
0
shouldThrowNullPointerIfTimeIsNull
public void kafkatest_f18641_0()
{    reset(supplier);    expect(supplier.name()).andReturn("name");    replay(supplier);    final Exception e = assertThrows(NullPointerException.class, () -> new SessionStoreBuilder<>(supplier, Serdes.String(), Serdes.String(), null));    assertThat(e.getMessage(), equalTo("time cannot be null"));}
f18641
0
shouldThrowNullPointerIfMetricsScopeIsNull
public void kafkatest_f18642_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> new SessionStoreBuilder<>(supplier, Serdes.String(), Serdes.String(), new MockTime()));    assertThat(e.getMessage(), equalTo("name cannot be null"));}
f18642
0
shouldFindTimestampedKeyValueStores
public void kafkatest_f18650_0()
{    mockThread(true);    final List<ReadOnlyKeyValueStore<String, ValueAndTimestamp<String>>> tkvStores = provider.stores("timestamped-kv-store", QueryableStoreTypes.timestampedKeyValueStore());    assertEquals(2, tkvStores.size());    for (final ReadOnlyKeyValueStore<String, ValueAndTimestamp<String>> store : tkvStores) {        assertThat(store, instanceOf(ReadOnlyKeyValueStore.class));        assertThat(store, instanceOf(TimestampedKeyValueStore.class));    }}
f18650
0
shouldNotFindKeyValueStoresAsTimestampedStore
public void kafkatest_f18651_0()
{    mockThread(true);    final List<ReadOnlyKeyValueStore<String, ValueAndTimestamp<String>>> tkvStores = provider.stores("kv-store", QueryableStoreTypes.timestampedKeyValueStore());    assertEquals(0, tkvStores.size());}
f18651
0
shouldFindTimestampedKeyValueStoresAsKeyValueStores
public void kafkatest_f18652_0()
{    mockThread(true);    final List<ReadOnlyKeyValueStore<String, ValueAndTimestamp<String>>> tkvStores = provider.stores("timestamped-kv-store", QueryableStoreTypes.keyValueStore());    assertEquals(2, tkvStores.size());    for (final ReadOnlyKeyValueStore<String, ValueAndTimestamp<String>> store : tkvStores) {        assertThat(store, instanceOf(ReadOnlyKeyValueStore.class));        assertThat(store, not(instanceOf(TimestampedKeyValueStore.class)));    }}
f18652
0
shouldThrowInvalidStoreExceptionIfTsWindowStoreClosed
public void kafkatest_f18660_0()
{    mockThread(true);    taskOne.getStore("timestamped-window-store").close();    provider.stores("timestamped-window-store", QueryableStoreTypes.timestampedWindowStore());}
f18660
0
shouldReturnEmptyListIfNoStoresFoundWithName
public void kafkatest_f18661_0()
{    mockThread(true);    assertEquals(Collections.emptyList(), provider.stores("not-a-store", QueryableStoreTypes.keyValueStore()));}
f18661
0
shouldReturnEmptyListIfStoreExistsButIsNotOfTypeValueStore
public void kafkatest_f18662_0()
{    mockThread(true);    assertEquals(Collections.emptyList(), provider.stores("window-store", QueryableStoreTypes.keyValueStore()));}
f18662
0
cacheOverheadsLargeValues
public void kafkatest_f18670_0()
{    final Runtime runtime = Runtime.getRuntime();    final double factor = 0.05;    // if I ask for a cache size of 10 MB, accept an overhead of 2x, i.e., 20 MBs might be allocated    final double systemFactor = 2;    final long desiredCacheSize = Math.min(100 * 1024 * 1024L, runtime.maxMemory());    final int keySizeBytes = 8;    final int valueSizeBytes = 1000;    checkOverheads(factor, systemFactor, desiredCacheSize, keySizeBytes, valueSizeBytes);}
f18670
0
memoryCacheEntrySize
 static int kafkatest_f18671_0(final byte[] key, final byte[] value, final String topic)
{    return key.length + value.length + // isDirty    1 + // timestamp    8 + // offset    8 + 4 + topic.length() + // LRU Node entries    key.length + // entry    8 + // previous    8 + // next    8;}
f18671
0
evict
public void kafkatest_f18672_0()
{    final List<KeyValue<String, String>> received = new ArrayList<>();    final List<KeyValue<String, String>> expected = Collections.singletonList(new KeyValue<>("K1", "V1"));    final List<KeyValue<String, String>> toInsert = Arrays.asList(new KeyValue<>("K1", "V1"), new KeyValue<>("K2", "V2"), new KeyValue<>("K3", "V3"), new KeyValue<>("K4", "V4"), new KeyValue<>("K5", "V5"));    final KeyValue<String, String> kv = toInsert.get(0);    final ThreadCache cache = new ThreadCache(logContext, memoryCacheEntrySize(kv.key.getBytes(), kv.value.getBytes(), ""), new MockStreamsMetrics(new Metrics()));    cache.addDirtyEntryFlushListener(namespace, dirty -> {        for (final ThreadCache.DirtyEntry dirtyEntry : dirty) {            received.add(new KeyValue<>(dirtyEntry.key().toString(), new String(dirtyEntry.newValue())));        }    });    for (final KeyValue<String, String> kvToInsert : toInsert) {        final Bytes key = Bytes.wrap(kvToInsert.key.getBytes());        final byte[] value = kvToInsert.value.getBytes();        cache.put(namespace, key, new LRUCacheEntry(value, null, true, 1, 1, 1, ""));    }    for (int i = 0; i < expected.size(); i++) {        final KeyValue<String, String> expectedRecord = expected.get(i);        final KeyValue<String, String> actualRecord = received.get(i);        assertEquals(expectedRecord, actualRecord);    }    assertEquals(cache.evicts(), 4);}
f18672
0
shouldThrowIfNoPeekNextKey
public void kafkatest_f18680_0()
{    final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics()));    final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.range(namespace, Bytes.wrap(new byte[] { 0 }), Bytes.wrap(new byte[] { 1 }));    iterator.peekNextKey();}
f18680
0
shouldReturnFalseIfNoNextKey
public void kafkatest_f18681_0()
{    final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics()));    final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.range(namespace, Bytes.wrap(new byte[] { 0 }), Bytes.wrap(new byte[] { 1 }));    assertFalse(iterator.hasNext());}
f18681
0
shouldPeekAndIterateOverRange
public void kafkatest_f18682_0()
{    final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics()));    final byte[][] bytes = { { 0 }, { 1 }, { 2 }, { 3 }, { 4 }, { 5 }, { 6 }, { 7 }, { 8 }, { 9 }, { 10 } };    for (final byte[] aByte : bytes) {        cache.put(namespace, Bytes.wrap(aByte), dirtyEntry(aByte));    }    final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.range(namespace, Bytes.wrap(new byte[] { 1 }), Bytes.wrap(new byte[] { 4 }));    int bytesIndex = 1;    while (iterator.hasNext()) {        final Bytes peekedKey = iterator.peekNextKey();        final KeyValue<Bytes, LRUCacheEntry> next = iterator.next();        assertArrayEquals(bytes[bytesIndex], peekedKey.get());        assertArrayEquals(bytes[bytesIndex], next.key.get());        bytesIndex++;    }    assertEquals(5, bytesIndex);}
f18682
0
shouldPutAll
public void kafkatest_f18690_0()
{    final ThreadCache cache = new ThreadCache(logContext, 100000, new MockStreamsMetrics(new Metrics()));    cache.putAll(namespace, Arrays.asList(KeyValue.pair(Bytes.wrap(new byte[] { 0 }), dirtyEntry(new byte[] { 5 })), KeyValue.pair(Bytes.wrap(new byte[] { 1 }), dirtyEntry(new byte[] { 6 }))));    assertArrayEquals(new byte[] { 5 }, cache.get(namespace, Bytes.wrap(new byte[] { 0 })).value());    assertArrayEquals(new byte[] { 6 }, cache.get(namespace, Bytes.wrap(new byte[] { 1 })).value());}
f18690
0
shouldNotForwardCleanEntryOnEviction
public void kafkatest_f18691_0()
{    final ThreadCache cache = new ThreadCache(logContext, 0, new MockStreamsMetrics(new Metrics()));    final List<ThreadCache.DirtyEntry> received = new ArrayList<>();    cache.addDirtyEntryFlushListener(namespace, received::addAll);    cache.put(namespace, Bytes.wrap(new byte[] { 1 }), cleanEntry(new byte[] { 0 }));    assertEquals(0, received.size());}
f18691
0
shouldPutIfAbsent
public void kafkatest_f18692_0()
{    final ThreadCache cache = new ThreadCache(logContext, 100000, new MockStreamsMetrics(new Metrics()));    final Bytes key = Bytes.wrap(new byte[] { 10 });    final byte[] value = { 30 };    assertNull(cache.putIfAbsent(namespace, key, dirtyEntry(value)));    assertArrayEquals(value, cache.putIfAbsent(namespace, key, dirtyEntry(new byte[] { 8 })).value());    assertArrayEquals(value, cache.get(namespace, key).value());}
f18692
0
serialize
public byte[] kafkatest_f18700_0(final String topic, final String data)
{    if (data == null) {        throw new IllegalArgumentException();    }    return super.serialize(topic, data);}
f18700
0
parameters
public static Collection<Object[]> kafkatest_f18701_0()
{    return singletonList(new Object[] { "in-memory buffer", (Function<String, InMemoryTimeOrderedKeyValueBuffer<String, String>>) name -> new InMemoryTimeOrderedKeyValueBuffer.Builder<>(name, Serdes.String(), Serdes.serdeFrom(new NullRejectingStringSerializer(), new StringDeserializer())).build() });}
f18701
0
makeContext
private static MockInternalProcessorContext kafkatest_f18702_0()
{    final Properties properties = new Properties();    properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, APP_ID);    properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "");    final TaskId taskId = new TaskId(0, 0);    final MockInternalProcessorContext context = new MockInternalProcessorContext(properties, taskId, TestUtils.tempDirectory());    context.setRecordCollector(new MockRecordCollector());    return context;}
f18702
0
shouldTrackSize
public void kafkatest_f18710_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    putRecord(buffer, context, 0L, 0L, "asdf", "23roni");    assertThat(buffer.bufferSize(), is(43L));    putRecord(buffer, context, 1L, 0L, "asdf", "3l");    assertThat(buffer.bufferSize(), is(39L));    putRecord(buffer, context, 0L, 0L, "zxcv", "qfowin");    assertThat(buffer.bufferSize(), is(82L));    cleanup(context, buffer);}
f18710
0
shouldTrackMinTimestamp
public void kafkatest_f18711_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    putRecord(buffer, context, 1L, 0L, "asdf", "2093j");    assertThat(buffer.minTimestamp(), is(1L));    putRecord(buffer, context, 0L, 0L, "zxcv", "3gon4i");    assertThat(buffer.minTimestamp(), is(0L));    cleanup(context, buffer);}
f18711
0
shouldEvictOldestAndUpdateSizeAndCountAndMinTimestamp
public void kafkatest_f18712_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    putRecord(buffer, context, 1L, 0L, "zxcv", "o23i4");    assertThat(buffer.numRecords(), is(1));    assertThat(buffer.bufferSize(), is(42L));    assertThat(buffer.minTimestamp(), is(1L));    putRecord(buffer, context, 0L, 0L, "asdf", "3ng");    assertThat(buffer.numRecords(), is(2));    assertThat(buffer.bufferSize(), is(82L));    assertThat(buffer.minTimestamp(), is(0L));    final AtomicInteger callbackCount = new AtomicInteger(0);    buffer.evictWhile(() -> true, kv -> {        switch(callbackCount.incrementAndGet()) {            case 1:                {                    assertThat(kv.key(), is("asdf"));                    assertThat(buffer.numRecords(), is(2));                    assertThat(buffer.bufferSize(), is(82L));                    assertThat(buffer.minTimestamp(), is(0L));                    break;                }            case 2:                {                    assertThat(kv.key(), is("zxcv"));                    assertThat(buffer.numRecords(), is(1));                    assertThat(buffer.bufferSize(), is(42L));                    assertThat(buffer.minTimestamp(), is(1L));                    break;                }            default:                {                    fail("too many invocations");                    break;                }        }    });    assertThat(callbackCount.get(), is(2));    assertThat(buffer.numRecords(), is(0));    assertThat(buffer.bufferSize(), is(0L));    assertThat(buffer.minTimestamp(), is(Long.MAX_VALUE));    cleanup(context, buffer);}
f18712
0
putRecord
private static void kafkatest_f18720_0(final TimeOrderedKeyValueBuffer<String, String> buffer, final MockInternalProcessorContext context, final long streamTime, final long recordTimestamp, final String key, final String value)
{    final ProcessorRecordContext recordContext = getContext(recordTimestamp);    context.setRecordContext(recordContext);    buffer.put(streamTime, key, new Change<>(value, null), recordContext);}
f18720
0
getBufferValue
private static BufferValue kafkatest_f18721_0(final String value, final long timestamp)
{    return new BufferValue(null, null, Serdes.String().serializer().serialize(null, value), getContext(timestamp));}
f18721
0
getContextualRecord
private static ContextualRecord kafkatest_f18722_0(final String value, final long timestamp)
{    final FullChangeSerde<String> fullChangeSerde = FullChangeSerde.wrap(Serdes.String());    return new ContextualRecord(FullChangeSerde.mergeChangeArraysIntoSingleLegacyFormattedArray(fullChangeSerde.serializeParts(null, new Change<>(value, null))), getContext(timestamp));}
f18722
0
shouldHaveCachingAndChangeLoggingWhenBothEnabled
public void kafkatest_f18730_0()
{    final TimestampedKeyValueStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).withCachingEnabled().build();    final WrappedStateStore caching = (WrappedStateStore) ((WrappedStateStore) store).wrapped();    final WrappedStateStore changeLogging = (WrappedStateStore) caching.wrapped();    assertThat(store, instanceOf(MeteredTimestampedKeyValueStore.class));    assertThat(caching, instanceOf(CachingKeyValueStore.class));    assertThat(changeLogging, instanceOf(ChangeLoggingTimestampedKeyValueBytesStore.class));    assertThat(changeLogging.wrapped(), CoreMatchers.equalTo(inner));}
f18730
0
shouldNotWrapTimestampedByteStore
public void kafkatest_f18731_0()
{    reset(supplier);    expect(supplier.get()).andReturn(new RocksDBTimestampedStore("name", "metrics-scope"));    expect(supplier.name()).andReturn("name");    replay(supplier);    final TimestampedKeyValueStore<String, String> store = builder.withLoggingDisabled().withCachingDisabled().build();    assertThat(((WrappedStateStore) store).wrapped(), instanceOf(RocksDBTimestampedStore.class));}
f18731
0
shouldWrapPlainKeyValueStoreAsTimestampStore
public void kafkatest_f18732_0()
{    reset(supplier);    expect(supplier.get()).andReturn(new RocksDBStore("name", "metrics-scope"));    expect(supplier.name()).andReturn("name");    replay(supplier);    final TimestampedKeyValueStore<String, String> store = builder.withLoggingDisabled().withCachingDisabled().build();    assertThat(((WrappedStateStore) store).wrapped(), instanceOf(KeyValueToTimestampedKeyValueByteStoreAdapter.class));}
f18732
0
shouldGetSegmentIdsFromTimestamp
public void kafkatest_f18740_0()
{    assertEquals(0, segments.segmentId(0));    assertEquals(1, segments.segmentId(SEGMENT_INTERVAL));    assertEquals(2, segments.segmentId(2 * SEGMENT_INTERVAL));    assertEquals(3, segments.segmentId(3 * SEGMENT_INTERVAL));}
f18740
0
shouldBaseSegmentIntervalOnRetentionAndNumSegments
public void kafkatest_f18741_0()
{    final TimestampedSegments segments = new TimestampedSegments("test", METRICS_SCOPE, 8 * SEGMENT_INTERVAL, 2 * SEGMENT_INTERVAL);    assertEquals(0, segments.segmentId(0));    assertEquals(0, segments.segmentId(SEGMENT_INTERVAL));    assertEquals(1, segments.segmentId(2 * SEGMENT_INTERVAL));}
f18741
0
shouldGetSegmentNameFromId
public void kafkatest_f18742_0()
{    assertEquals("test.0", segments.segmentName(0));    assertEquals("test." + SEGMENT_INTERVAL, segments.segmentName(1));    assertEquals("test." + 2 * SEGMENT_INTERVAL, segments.segmentName(2));}
f18742
0
shouldGetSegmentsWithinTimeRange
public void kafkatest_f18750_0()
{    updateStreamTimeAndCreateSegment(0);    updateStreamTimeAndCreateSegment(1);    updateStreamTimeAndCreateSegment(2);    updateStreamTimeAndCreateSegment(3);    final long streamTime = updateStreamTimeAndCreateSegment(4);    segments.getOrCreateSegmentIfLive(0, context, streamTime);    segments.getOrCreateSegmentIfLive(1, context, streamTime);    segments.getOrCreateSegmentIfLive(2, context, streamTime);    segments.getOrCreateSegmentIfLive(3, context, streamTime);    segments.getOrCreateSegmentIfLive(4, context, streamTime);    final List<TimestampedSegment> segments = this.segments.segments(0, 2 * SEGMENT_INTERVAL);    assertEquals(3, segments.size());    assertEquals(0, segments.get(0).id);    assertEquals(1, segments.get(1).id);    assertEquals(2, segments.get(2).id);}
f18750
0
shouldGetSegmentsWithinTimeRangeOutOfOrder
public void kafkatest_f18751_0()
{    updateStreamTimeAndCreateSegment(4);    updateStreamTimeAndCreateSegment(2);    updateStreamTimeAndCreateSegment(0);    updateStreamTimeAndCreateSegment(1);    updateStreamTimeAndCreateSegment(3);    final List<TimestampedSegment> segments = this.segments.segments(0, 2 * SEGMENT_INTERVAL);    assertEquals(3, segments.size());    assertEquals(0, segments.get(0).id);    assertEquals(1, segments.get(1).id);    assertEquals(2, segments.get(2).id);}
f18751
0
shouldRollSegments
public void kafkatest_f18752_0()
{    updateStreamTimeAndCreateSegment(0);    verifyCorrectSegments(0, 1);    updateStreamTimeAndCreateSegment(1);    verifyCorrectSegments(0, 2);    updateStreamTimeAndCreateSegment(2);    verifyCorrectSegments(0, 3);    updateStreamTimeAndCreateSegment(3);    verifyCorrectSegments(0, 4);    updateStreamTimeAndCreateSegment(4);    verifyCorrectSegments(0, 5);    updateStreamTimeAndCreateSegment(5);    verifyCorrectSegments(1, 5);    updateStreamTimeAndCreateSegment(6);    verifyCorrectSegments(2, 5);}
f18752
0
shouldBeEqualIfIdIsEqual
public void kafkatest_f18760_0()
{    final TimestampedSegment segment = new TimestampedSegment("anyName", "anyName", 0L, metricsRecorder);    final TimestampedSegment segmentSameId = new TimestampedSegment("someOtherName", "someOtherName", 0L, metricsRecorder);    final TimestampedSegment segmentDifferentId = new TimestampedSegment("anyName", "anyName", 1L, metricsRecorder);    assertThat(segment, equalTo(segment));    assertThat(segment, equalTo(segmentSameId));    assertThat(segment, not(equalTo(segmentDifferentId)));    assertThat(segment, not(equalTo(null)));    assertThat(segment, not(equalTo("anyName")));}
f18760
0
shouldHashOnSegmentIdOnly
public void kafkatest_f18761_0()
{    final TimestampedSegment segment = new TimestampedSegment("anyName", "anyName", 0L, metricsRecorder);    final TimestampedSegment segmentSameId = new TimestampedSegment("someOtherName", "someOtherName", 0L, metricsRecorder);    final TimestampedSegment segmentDifferentId = new TimestampedSegment("anyName", "anyName", 1L, metricsRecorder);    final Set<TimestampedSegment> set = new HashSet<>();    assertTrue(set.add(segment));    assertFalse(set.add(segmentSameId));    assertTrue(set.add(segmentDifferentId));}
f18761
0
shouldCompareSegmentIdOnly
public void kafkatest_f18762_0()
{    final TimestampedSegment segment1 = new TimestampedSegment("a", "C", 50L, metricsRecorder);    final TimestampedSegment segment2 = new TimestampedSegment("b", "B", 100L, metricsRecorder);    final TimestampedSegment segment3 = new TimestampedSegment("c", "A", 0L, metricsRecorder);    assertThat(segment1.compareTo(segment1), equalTo(0));    assertThat(segment1.compareTo(segment2), equalTo(-1));    assertThat(segment2.compareTo(segment1), equalTo(1));    assertThat(segment1.compareTo(segment3), equalTo(1));    assertThat(segment3.compareTo(segment1), equalTo(-1));    assertThat(segment2.compareTo(segment3), equalTo(1));    assertThat(segment3.compareTo(segment2), equalTo(-1));}
f18762
0
shouldNotWrapTimestampedByteStore
public void kafkatest_f18770_0()
{    reset(supplier);    expect(supplier.get()).andReturn(new RocksDBTimestampedWindowStore(new RocksDBTimestampedSegmentedBytesStore("name", "metric-scope", 10L, 5L, new WindowKeySchema()), false, 1L));    expect(supplier.name()).andReturn("name");    replay(supplier);    final TimestampedWindowStore<String, String> store = builder.withLoggingDisabled().withCachingDisabled().build();    assertThat(((WrappedStateStore) store).wrapped(), instanceOf(RocksDBTimestampedWindowStore.class));}
f18770
0
shouldWrapPlainKeyValueStoreAsTimestampStore
public void kafkatest_f18771_0()
{    reset(supplier);    expect(supplier.get()).andReturn(new RocksDBWindowStore(new RocksDBSegmentedBytesStore("name", "metric-scope", 10L, 5L, new WindowKeySchema()), false, 1L));    expect(supplier.name()).andReturn("name");    replay(supplier);    final TimestampedWindowStore<String, String> store = builder.withLoggingDisabled().withCachingDisabled().build();    assertThat(((WrappedStateStore) store).wrapped(), instanceOf(WindowToTimestampedWindowByteStoreAdapter.class));}
f18771
0
shouldThrowNullPointerIfInnerIsNull
public void kafkatest_f18772_0()
{    new TimestampedWindowStoreBuilder<>(null, Serdes.String(), Serdes.String(), new MockTime());}
f18772
0
testRangeAndSinglePointFetch
public void kafkatest_f18780_0()
{    final long startTime = SEGMENT_INTERVAL - 4L;    putFirstBatch(windowStore, startTime, context);    assertEquals("zero", windowStore.fetch(0, startTime));    assertEquals("one", windowStore.fetch(1, startTime + 1L));    assertEquals("two", windowStore.fetch(2, startTime + 2L));    assertEquals("four", windowStore.fetch(4, startTime + 4L));    assertEquals("five", windowStore.fetch(5, startTime + 5L));    assertEquals(new HashSet<>(Collections.singletonList("zero")), toSet(windowStore.fetch(0, ofEpochMilli(startTime + 0 - WINDOW_SIZE), ofEpochMilli(startTime + 0 + WINDOW_SIZE))));    putSecondBatch(windowStore, startTime, context);    assertEquals("two+1", windowStore.fetch(2, startTime + 3L));    assertEquals("two+2", windowStore.fetch(2, startTime + 4L));    assertEquals("two+3", windowStore.fetch(2, startTime + 5L));    assertEquals("two+4", windowStore.fetch(2, startTime + 6L));    assertEquals("two+5", windowStore.fetch(2, startTime + 7L));    assertEquals("two+6", windowStore.fetch(2, startTime + 8L));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(2, ofEpochMilli(startTime - 2L - WINDOW_SIZE), ofEpochMilli(startTime - 2L + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("two")), toSet(windowStore.fetch(2, ofEpochMilli(startTime - 1L - WINDOW_SIZE), ofEpochMilli(startTime - 1L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two", "two+1")), toSet(windowStore.fetch(2, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two", "two+1", "two+2")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 1L - WINDOW_SIZE), ofEpochMilli(startTime + 1L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two", "two+1", "two+2", "two+3")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 2L - WINDOW_SIZE), ofEpochMilli(startTime + 2L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two", "two+1", "two+2", "two+3", "two+4")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 3L - WINDOW_SIZE), ofEpochMilli(startTime + 3L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two", "two+1", "two+2", "two+3", "two+4", "two+5")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 4L - WINDOW_SIZE), ofEpochMilli(startTime + 4L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two", "two+1", "two+2", "two+3", "two+4", "two+5", "two+6")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 5L - WINDOW_SIZE), ofEpochMilli(startTime + 5L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two+1", "two+2", "two+3", "two+4", "two+5", "two+6")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 6L - WINDOW_SIZE), ofEpochMilli(startTime + 6L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two+2", "two+3", "two+4", "two+5", "two+6")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 7L - WINDOW_SIZE), ofEpochMilli(startTime + 7L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two+3", "two+4", "two+5", "two+6")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 8L - WINDOW_SIZE), ofEpochMilli(startTime + 8L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two+4", "two+5", "two+6")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 9L - WINDOW_SIZE), ofEpochMilli(startTime + 9L + WINDOW_SIZE))));    assertEquals(new HashSet<>(asList("two+5", "two+6")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 10L - WINDOW_SIZE), ofEpochMilli(startTime + 10L + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("two+6")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 11L - WINDOW_SIZE), ofEpochMilli(startTime + 11L + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(2, ofEpochMilli(startTime + 12L - WINDOW_SIZE), ofEpochMilli(startTime + 12L + WINDOW_SIZE))));    // Flush the store and verify all current entries were properly flushed ...    windowStore.flush();    final Map<Integer, Set<String>> entriesByKey = entriesByKey(changeLog, startTime);    assertEquals(Utils.mkSet("zero@0"), entriesByKey.get(0));    assertEquals(Utils.mkSet("one@1"), entriesByKey.get(1));    assertEquals(Utils.mkSet("two@2", "two+1@3", "two+2@4", "two+3@5", "two+4@6", "two+5@7", "two+6@8"), entriesByKey.get(2));    assertNull(entriesByKey.get(3));    assertEquals(Utils.mkSet("four@4"), entriesByKey.get(4));    assertEquals(Utils.mkSet("five@5"), entriesByKey.get(5));    assertNull(entriesByKey.get(6));}
f18780
0
shouldGetAll
public void kafkatest_f18781_0()
{    final long startTime = SEGMENT_INTERVAL - 4L;    putFirstBatch(windowStore, startTime, context);    final KeyValue<Windowed<Integer>, String> zero = windowedPair(0, "zero", startTime + 0);    final KeyValue<Windowed<Integer>, String> one = windowedPair(1, "one", startTime + 1);    final KeyValue<Windowed<Integer>, String> two = windowedPair(2, "two", startTime + 2);    final KeyValue<Windowed<Integer>, String> four = windowedPair(4, "four", startTime + 4);    final KeyValue<Windowed<Integer>, String> five = windowedPair(5, "five", startTime + 5);    assertEquals(new HashSet<>(asList(zero, one, two, four, five)), toSet(windowStore.all()));}
f18781
0
shouldFetchAllInTimeRange
public void kafkatest_f18782_0()
{    final long startTime = SEGMENT_INTERVAL - 4L;    putFirstBatch(windowStore, startTime, context);    final KeyValue<Windowed<Integer>, String> zero = windowedPair(0, "zero", startTime + 0);    final KeyValue<Windowed<Integer>, String> one = windowedPair(1, "one", startTime + 1);    final KeyValue<Windowed<Integer>, String> two = windowedPair(2, "two", startTime + 2);    final KeyValue<Windowed<Integer>, String> four = windowedPair(4, "four", startTime + 4);    final KeyValue<Windowed<Integer>, String> five = windowedPair(5, "five", startTime + 5);    assertEquals(new HashSet<>(asList(one, two, four)), toSet(windowStore.fetchAll(ofEpochMilli(startTime + 1), ofEpochMilli(startTime + 4))));    assertEquals(new HashSet<>(asList(zero, one, two)), toSet(windowStore.fetchAll(ofEpochMilli(startTime + 0), ofEpochMilli(startTime + 3))));    assertEquals(new HashSet<>(asList(one, two, four, five)), toSet(windowStore.fetchAll(ofEpochMilli(startTime + 1), ofEpochMilli(startTime + 5))));}
f18782
0
shouldReturnNullOnWindowNotFound
public void kafkatest_f18790_0()
{    assertNull(windowStore.fetch(1, 0L));}
f18790
0
shouldThrowNullPointerExceptionOnPutNullKey
public void kafkatest_f18791_0()
{    windowStore.put(null, "anyValue");}
f18791
0
shouldThrowNullPointerExceptionOnGetNullKey
public void kafkatest_f18792_0()
{    windowStore.fetch(null, ofEpochMilli(1L), ofEpochMilli(2L));}
f18792
0
testWindowIteratorPeek
public void kafkatest_f18800_0()
{    final long currentTime = 0;    setCurrentTime(currentTime);    windowStore.put(1, "one");    final KeyValueIterator<Windowed<Integer>, String> iterator = windowStore.fetchAll(0L, currentTime);    assertTrue(iterator.hasNext());    final Windowed<Integer> nextKey = iterator.peekNextKey();    assertEquals(iterator.peekNextKey(), nextKey);    assertEquals(iterator.peekNextKey(), iterator.next().key);    assertFalse(iterator.hasNext());}
f18800
0
testValueIteratorPeek
public void kafkatest_f18801_0()
{    windowStore.put(1, "one", 0L);    final WindowStoreIterator<String> iterator = windowStore.fetch(1, 0L, 10L);    assertTrue(iterator.hasNext());    final Long nextKey = iterator.peekNextKey();    assertEquals(iterator.peekNextKey(), nextKey);    assertEquals(iterator.peekNextKey(), iterator.next().key);    assertFalse(iterator.hasNext());}
f18801
0
shouldNotThrowConcurrentModificationException
public void kafkatest_f18802_0()
{    long currentTime = 0;    setCurrentTime(currentTime);    windowStore.put(1, "one");    currentTime += WINDOW_SIZE * 10;    setCurrentTime(currentTime);    windowStore.put(1, "two");    final KeyValueIterator<Windowed<Integer>, String> iterator = windowStore.all();    currentTime += WINDOW_SIZE * 10;    setCurrentTime(currentTime);    windowStore.put(1, "three");    currentTime += WINDOW_SIZE * 10;    setCurrentTime(currentTime);    windowStore.put(2, "four");    // Iterator should return all records in store and not throw exception b/c some were added after fetch    assertEquals(windowedPair(1, "one", 0), iterator.next());    assertEquals(windowedPair(1, "two", WINDOW_SIZE * 10), iterator.next());    assertEquals(windowedPair(1, "three", WINDOW_SIZE * 20), iterator.next());    assertEquals(windowedPair(2, "four", WINDOW_SIZE * 30), iterator.next());    assertFalse(iterator.hasNext());}
f18802
0
windowedPair
private static KeyValue<Windowed<K>, V> kafkatest_f18810_0(final K key, final V value, final long timestamp, final long windowSize)
{    return KeyValue.pair(new Windowed<>(key, WindowKeySchema.timeWindowForSize(timestamp, windowSize)), value);}
f18810
0
setCurrentTime
protected void kafkatest_f18811_0(final long currentTime)
{    context.setRecordContext(createRecordContext(currentTime));}
f18811
0
createRecordContext
private ProcessorRecordContext kafkatest_f18812_0(final long time)
{    return new ProcessorRecordContext(time, 0, 0, "topic", null);}
f18812
0
testLowerBoundMatchesTrailingZeros
public void kafkatest_f18820_0()
{    final Bytes lower = windowKeySchema.lowerRange(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), Long.MAX_VALUE - 1);    assertThat("appending zeros to key should still be in range", lower.compareTo(WindowKeySchema.toStoreKeyBinary(new byte[] { 0xA, 0xB, 0xC, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }, Long.MAX_VALUE - 1, 0)) < 0);    assertThat(lower, equalTo(WindowKeySchema.toStoreKeyBinary(new byte[] { 0xA, 0xB, 0xC }, 0, 0)));}
f18820
0
shouldSerializeDeserialize
public void kafkatest_f18821_0()
{    final byte[] bytes = keySerde.serializer().serialize(topic, windowedKey);    final Windowed<String> result = keySerde.deserializer().deserialize(topic, bytes);    // TODO: fix this part as last bits of KAFKA-4468    assertEquals(new Windowed<>(key, new TimeWindow(startTime, Long.MAX_VALUE)), result);}
f18821
0
testSerializeDeserializeOverflowWindowSize
public void kafkatest_f18822_0()
{    final byte[] bytes = keySerde.serializer().serialize(topic, windowedKey);    final Windowed<String> result = new TimeWindowedDeserializer<>(serde.deserializer(), Long.MAX_VALUE - 1).deserialize(topic, bytes);    assertEquals(new Windowed<>(key, new TimeWindow(startTime, Long.MAX_VALUE)), result);}
f18822
0
shouldExtractStartTimeFromBinary
public void kafkatest_f18830_0()
{    final Bytes serialized = WindowKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);    assertEquals(startTime, WindowKeySchema.extractStoreTimestamp(serialized.get()));}
f18830
0
shouldExtractWindowFromBinary
public void kafkatest_f18831_0()
{    final Bytes serialized = WindowKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);    assertEquals(window, WindowKeySchema.extractStoreWindow(serialized.get(), endTime - startTime));}
f18831
0
shouldExtractKeyBytesFromBinary
public void kafkatest_f18832_0()
{    final Bytes serialized = WindowKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);    assertArrayEquals(key.getBytes(), WindowKeySchema.extractStoreKeyBytes(serialized.get()));}
f18832
0
shouldHaveChangeLoggingStoreWhenLoggingEnabled
public void kafkatest_f18840_0()
{    final WindowStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredWindowStore.class));    assertThat(wrapped, instanceOf(ChangeLoggingWindowBytesStore.class));    assertThat(((WrappedStateStore) wrapped).wrapped(), CoreMatchers.equalTo(inner));}
f18840
0
shouldHaveCachingAndChangeLoggingWhenBothEnabled
public void kafkatest_f18841_0()
{    final WindowStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).withCachingEnabled().build();    final WrappedStateStore caching = (WrappedStateStore) ((WrappedStateStore) store).wrapped();    final WrappedStateStore changeLogging = (WrappedStateStore) caching.wrapped();    assertThat(store, instanceOf(MeteredWindowStore.class));    assertThat(caching, instanceOf(CachingWindowStore.class));    assertThat(changeLogging, instanceOf(ChangeLoggingWindowBytesStore.class));    assertThat(changeLogging.wrapped(), CoreMatchers.equalTo(inner));}
f18841
0
shouldThrowNullPointerIfInnerIsNull
public void kafkatest_f18842_0()
{    new WindowStoreBuilder<>(null, Serdes.String(), Serdes.String(), new MockTime());}
f18842
0
create
public static KeyValueStoreTestDriver<K, V> kafkatest_f18850_0(final Class<K> keyClass, final Class<V> valueClass)
{    final StateSerdes<K, V> serdes = StateSerdes.withBuiltinTypes("unexpected", keyClass, valueClass);    return new KeyValueStoreTestDriver<>(serdes);}
f18850
0
create
public static KeyValueStoreTestDriver<K, V> kafkatest_f18851_0(final Serializer<K> keySerializer, final Deserializer<K> keyDeserializer, final Serializer<V> valueSerializer, final Deserializer<V> valueDeserializer)
{    final StateSerdes<K, V> serdes = new StateSerdes<>("unexpected", Serdes.serdeFrom(keySerializer, keyDeserializer), Serdes.serdeFrom(valueSerializer, valueDeserializer));    return new KeyValueStoreTestDriver<>(serdes);}
f18851
0
send
public void kafkatest_f18852_0(final String topic, final K1 key, final V1 value, final Headers headers, final Integer partition, final Long timestamp, final Serializer<K1> keySerializer, final Serializer<V1> valueSerializer)
{    // for byte arrays we need to wrap it for comparison    final K keyTest = serdes.keyFrom(keySerializer.serialize(topic, headers, key));    final V valueTest = serdes.valueFrom(valueSerializer.serialize(topic, headers, value));    recordFlushed(keyTest, valueTest);}
f18852
0
context
public ProcessorContext kafkatest_f18860_0()
{    return context;}
f18860
0
checkForRestoredEntries
public int kafkatest_f18861_0(final KeyValueStore<K, V> store)
{    int missing = 0;    for (final KeyValue<byte[], byte[]> kv : restorableEntries) {        if (kv != null) {            final V value = store.get(stateSerdes.keyFrom(kv.key));            if (!Objects.equals(value, stateSerdes.valueFrom(kv.value))) {                ++missing;            }        }    }    return missing;}
f18861
0
sizeOf
public int kafkatest_f18862_0(final KeyValueStore<K, V> store)
{    int size = 0;    try (final KeyValueIterator<K, V> iterator = store.all()) {        while (iterator.hasNext()) {            iterator.next();            ++size;        }    }    return size;}
f18862
0
next
public KeyValue<Long, KeyValue> kafkatest_f18871_0()
{    throw new NoSuchElementException();}
f18871
0
name
public String kafkatest_f18872_0()
{    return "";}
f18872
0
persistent
public boolean kafkatest_f18876_0()
{    return false;}
f18876
0
fetchAll
public WindowStoreIterator<KeyValue> kafkatest_f18884_0(final long timeFrom, final long timeTo)
{    return EMPTY_WINDOW_STORE_ITERATOR;}
f18884
0
fetchAll
public KeyValueIterator kafkatest_f18885_0(final Instant from, final Instant to)
{    return EMPTY_WINDOW_STORE_ITERATOR;}
f18885
0
shouldThrowIfTopicNameIsNullForBuiltinTypes
public void kafkatest_f18886_0()
{    StateSerdes.withBuiltinTypes(null, byte[].class, byte[].class);}
f18886
0
shouldThrowIfValueClassIsNull
public void kafkatest_f18894_0()
{    new StateSerdes<>("anyName", Serdes.ByteArray(), null);}
f18894
0
shouldThrowIfIncompatibleSerdeForValue
public void kafkatest_f18895_0() throws ClassNotFoundException
{    final Class myClass = Class.forName("java.lang.String");    final StateSerdes<Object, Object> stateSerdes = new StateSerdes<Object, Object>("anyName", Serdes.serdeFrom(myClass), Serdes.serdeFrom(myClass));    final Integer myInt = 123;    final Exception e = assertThrows(StreamsException.class, () -> stateSerdes.rawValue(myInt));    assertThat(e.getMessage(), equalTo("A serializer (org.apache.kafka.common.serialization.StringSerializer) " + "is not compatible to the actual value type (value type: java.lang.Integer). " + "Change the default Serdes in StreamConfig or provide correct Serdes via method parameters."));}
f18895
0
shouldSkipValueAndTimestampeInformationForErrorOnTimestampAndValueSerialization
public void kafkatest_f18896_0() throws ClassNotFoundException
{    final Class myClass = Class.forName("java.lang.String");    final StateSerdes<Object, Object> stateSerdes = new StateSerdes<Object, Object>("anyName", Serdes.serdeFrom(myClass), new ValueAndTimestampSerde(Serdes.serdeFrom(myClass)));    final Integer myInt = 123;    final Exception e = assertThrows(StreamsException.class, () -> stateSerdes.rawValue(ValueAndTimestamp.make(myInt, 0L)));    assertThat(e.getMessage(), equalTo("A serializer (org.apache.kafka.common.serialization.StringSerializer) " + "is not compatible to the actual value type (value type: java.lang.Integer). " + "Change the default Serdes in StreamConfig or provide correct Serdes via method parameters."));}
f18896
0
shouldThrowIfIPersistentTimestampedWindowStoreStoreNameIsNull
public void kafkatest_f18904_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> Stores.persistentTimestampedWindowStore(null, ZERO, ZERO, false));    assertEquals("name cannot be null", e.getMessage());}
f18904
0
shouldThrowIfIPersistentWindowStoreRetentionPeriodIsNegative
public void kafkatest_f18905_0()
{    final Exception e = assertThrows(IllegalArgumentException.class, () -> Stores.persistentWindowStore("anyName", ofMillis(-1L), ZERO, false));    assertEquals("retentionPeriod cannot be negative", e.getMessage());}
f18905
0
shouldThrowIfIPersistentTimestampedWindowStoreRetentionPeriodIsNegative
public void kafkatest_f18906_0()
{    final Exception e = assertThrows(IllegalArgumentException.class, () -> Stores.persistentTimestampedWindowStore("anyName", ofMillis(-1L), ZERO, false));    assertEquals("retentionPeriod cannot be negative", e.getMessage());}
f18906
0
shouldThrowIfSupplierIsNullForSessionStoreBuilder
public void kafkatest_f18914_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> Stores.sessionStoreBuilder(null, Serdes.ByteArray(), Serdes.ByteArray()));    assertEquals("supplier cannot be null", e.getMessage());}
f18914
0
shouldCreateInMemoryKeyValueStore
public void kafkatest_f18915_0()
{    assertThat(Stores.inMemoryKeyValueStore("memory").get(), instanceOf(InMemoryKeyValueStore.class));}
f18915
0
shouldCreateMemoryNavigableCache
public void kafkatest_f18916_0()
{    assertThat(Stores.lruMap("map", 10).get(), instanceOf(MemoryNavigableLRUCache.class));}
f18916
0
shouldBuildTimestampedKeyValueStoreThatWrapsKeyValueStore
public void kafkatest_f18924_0()
{    final TimestampedKeyValueStore<String, String> store = Stores.timestampedKeyValueStoreBuilder(Stores.persistentKeyValueStore("name"), Serdes.String(), Serdes.String()).build();    assertThat(store, not(nullValue()));}
f18924
0
shouldBuildTimestampedKeyValueStoreThatWrapsInMemoryKeyValueStore
public void kafkatest_f18925_0()
{    final TimestampedKeyValueStore<String, String> store = Stores.timestampedKeyValueStoreBuilder(Stores.inMemoryKeyValueStore("name"), Serdes.String(), Serdes.String()).withLoggingDisabled().withCachingDisabled().build();    assertThat(store, not(nullValue()));    assertThat(((WrappedStateStore) store).wrapped(), instanceOf(TimestampedBytesStore.class));}
f18925
0
shouldBuildWindowStore
public void kafkatest_f18926_0()
{    final WindowStore<String, String> store = Stores.windowStoreBuilder(Stores.persistentWindowStore("store", ofMillis(3L), ofMillis(3L), true), Serdes.String(), Serdes.String()).build();    assertThat(store, not(nullValue()));}
f18926
0
shouldAllowJoinUnmaterializedMapValuedKTable
public void kafkatest_f18934_0()
{    final KTable<Bytes, String> mappedKTable = builder.<Bytes, String>table(TABLE_TOPIC).mapValues(MockMapper.noOpValueMapper());    builder.<Bytes, String>stream(STREAM_TOPIC).join(mappedKTable, MockValueJoiner.TOSTRING_JOINER);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertThat(topology.stateStores().size(), equalTo(1));    assertThat(topology.processorConnectedStateStores("KSTREAM-JOIN-0000000005"), equalTo(Collections.singleton(topology.stateStores().get(0).name())));    assertTrue(topology.processorConnectedStateStores("KTABLE-MAPVALUES-0000000003").isEmpty());}
f18934
0
shouldAllowJoinMaterializedMapValuedKTable
public void kafkatest_f18935_0()
{    final KTable<Bytes, String> mappedKTable = builder.<Bytes, String>table(TABLE_TOPIC).mapValues(MockMapper.noOpValueMapper(), Materialized.as("store"));    builder.<Bytes, String>stream(STREAM_TOPIC).join(mappedKTable, MockValueJoiner.TOSTRING_JOINER);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertThat(topology.stateStores().size(), equalTo(1));    assertThat(topology.processorConnectedStateStores("KSTREAM-JOIN-0000000005"), equalTo(Collections.singleton("store")));    assertThat(topology.processorConnectedStateStores("KTABLE-MAPVALUES-0000000003"), equalTo(Collections.singleton("store")));}
f18935
0
shouldAllowJoinUnmaterializedJoinedKTable
public void kafkatest_f18936_0()
{    final KTable<Bytes, String> table1 = builder.table("table-topic1");    final KTable<Bytes, String> table2 = builder.table("table-topic2");    builder.<Bytes, String>stream(STREAM_TOPIC).join(table1.join(table2, MockValueJoiner.TOSTRING_JOINER), MockValueJoiner.TOSTRING_JOINER);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertThat(topology.stateStores().size(), equalTo(2));    assertThat(topology.processorConnectedStateStores("KSTREAM-JOIN-0000000010"), equalTo(Utils.mkSet(topology.stateStores().get(0).name(), topology.stateStores().get(1).name())));    assertTrue(topology.processorConnectedStateStores("KTABLE-MERGE-0000000007").isEmpty());}
f18936
0
shouldNotMaterializeStoresIfNotRequired
public void kafkatest_f18944_0()
{    final String topic = "topic";    builder.table(topic, Materialized.with(Serdes.Long(), Serdes.String()));    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertThat(topology.stateStores().size(), equalTo(0));}
f18944
0
shouldReuseSourceTopicAsChangelogsWithOptimization20
public void kafkatest_f18945_0()
{    final String topic = "topic";    builder.table(topic, Materialized.<Long, String, KeyValueStore<Bytes, byte[]>>as("store"));    final Properties props = StreamsTestUtils.getStreamsConfig();    props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    final Topology topology = builder.build(props);    final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology);    internalTopologyBuilder.rewriteTopology(new StreamsConfig(props));    assertThat(internalTopologyBuilder.build().storeToChangelogTopic(), equalTo(Collections.singletonMap("store", "topic")));    assertThat(internalTopologyBuilder.getStateStores().keySet(), equalTo(Collections.singleton("store")));    assertThat(internalTopologyBuilder.getStateStores().get("store").loggingEnabled(), equalTo(false));    assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.isEmpty(), equalTo(true));}
f18945
0
shouldNotReuseSourceTopicAsChangelogsByDefault
public void kafkatest_f18946_0()
{    final String topic = "topic";    builder.table(topic, Materialized.<Long, String, KeyValueStore<Bytes, byte[]>>as("store"));    final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(builder.build());    internalTopologyBuilder.setApplicationId("appId");    assertThat(internalTopologyBuilder.build().storeToChangelogTopic(), equalTo(Collections.singletonMap("store", "appId-store-changelog")));    assertThat(internalTopologyBuilder.getStateStores().keySet(), equalTo(Collections.singleton("store")));    assertThat(internalTopologyBuilder.getStateStores().get("store").loggingEnabled(), equalTo(true));    assertThat(internalTopologyBuilder.topicGroups().get(0).stateChangelogTopics.keySet(), equalTo(Collections.singleton("appId-store-changelog")));}
f18946
0
shouldUseSpecifiedNameForMapValuesOperation
public void kafkatest_f18954_0()
{    builder.stream(STREAM_TOPIC).mapValues(v -> v, Named.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", STREAM_OPERATION_NAME);}
f18954
0
shouldUseSpecifiedNameForMapValuesWithKeyOperation
public void kafkatest_f18955_0()
{    builder.stream(STREAM_TOPIC).mapValues((k, v) -> v, Named.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", STREAM_OPERATION_NAME);}
f18955
0
shouldUseSpecifiedNameForFilterOperation
public void kafkatest_f18956_0()
{    builder.stream(STREAM_TOPIC).filter((k, v) -> true, Named.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", STREAM_OPERATION_NAME);}
f18956
0
shouldUseSpecifiedNameForLeftJoinOperationBetweenKStreamAndKStream
public void kafkatest_f18964_0()
{    final KStream<String, String> streamOne = builder.stream(STREAM_TOPIC);    final KStream<String, String> streamTwo = builder.stream(STREAM_TOPIC_TWO);    streamOne.leftJoin(streamTwo, (value1, value2) -> value1, JoinWindows.of(Duration.ofHours(1)), Joined.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForStateStore(topology.stateStores(), STREAM_OPERATION_NAME + "-this-join-store", STREAM_OPERATION_NAME + "-outer-other-join-store");    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", "KSTREAM-SOURCE-0000000001", STREAM_OPERATION_NAME + "-this-windowed", STREAM_OPERATION_NAME + "-other-windowed", STREAM_OPERATION_NAME + "-this-join", STREAM_OPERATION_NAME + "-outer-other-join", STREAM_OPERATION_NAME + "-merge");}
f18964
0
shouldUseSpecifiedNameForJoinOperationBetweenKStreamAndKStream
public void kafkatest_f18965_0()
{    final KStream<String, String> streamOne = builder.stream(STREAM_TOPIC);    final KStream<String, String> streamTwo = builder.stream(STREAM_TOPIC_TWO);    streamOne.join(streamTwo, (value1, value2) -> value1, JoinWindows.of(Duration.ofHours(1)), Joined.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForStateStore(topology.stateStores(), STREAM_OPERATION_NAME + "-this-join-store", STREAM_OPERATION_NAME + "-other-join-store");    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", "KSTREAM-SOURCE-0000000001", STREAM_OPERATION_NAME + "-this-windowed", STREAM_OPERATION_NAME + "-other-windowed", STREAM_OPERATION_NAME + "-this-join", STREAM_OPERATION_NAME + "-other-join", STREAM_OPERATION_NAME + "-merge");}
f18965
0
shouldUseSpecifiedNameForOuterJoinOperationBetweenKStreamAndKStream
public void kafkatest_f18966_0()
{    final KStream<String, String> streamOne = builder.stream(STREAM_TOPIC);    final KStream<String, String> streamTwo = builder.stream(STREAM_TOPIC_TWO);    streamOne.outerJoin(streamTwo, (value1, value2) -> value1, JoinWindows.of(Duration.ofHours(1)), Joined.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForStateStore(topology.stateStores(), STREAM_OPERATION_NAME + "-outer-this-join-store", STREAM_OPERATION_NAME + "-outer-other-join-store");    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", "KSTREAM-SOURCE-0000000001", STREAM_OPERATION_NAME + "-this-windowed", STREAM_OPERATION_NAME + "-other-windowed", STREAM_OPERATION_NAME + "-outer-this-join", STREAM_OPERATION_NAME + "-outer-other-join", STREAM_OPERATION_NAME + "-merge");}
f18966
0
assertSpecifiedNameForOperation
private static void kafkatest_f18974_0(final ProcessorTopology topology, final String... expected)
{    final List<ProcessorNode> processors = topology.processors();    assertEquals("Invalid number of expected processors", expected.length, processors.size());    for (int i = 0; i < expected.length; i++) {        assertEquals(expected[i], processors.get(i).name());    }}
f18974
0
assertSpecifiedNameForStateStore
private static void kafkatest_f18975_0(final List<StateStore> stores, final String... expected)
{    assertEquals("Invalid number of expected state stores", expected.length, stores.size());    for (int i = 0; i < expected.length; i++) {        assertEquals(expected[i], stores.get(i).name());    }}
f18975
0
setUp
public void kafkatest_f18976_0()
{    props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-config-test");    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    props.put("key.deserializer.encoding", "UTF8");    props.put("value.deserializer.encoding", "UTF-16");    streamsConfig = new StreamsConfig(props);}
f18976
0
testGetMainConsumerConfigsWithMainConsumerOverridenPrefix
public void kafkatest_f18984_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "5");    props.put(StreamsConfig.mainConsumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "50");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> returnedProps = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertEquals("50", returnedProps.get(ConsumerConfig.MAX_POLL_RECORDS_CONFIG));}
f18984
0
testGetRestoreConsumerConfigs
public void kafkatest_f18985_0()
{    final Map<String, Object> returnedProps = streamsConfig.getRestoreConsumerConfigs(clientId);    assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), clientId);    assertNull(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG));}
f18985
0
defaultSerdeShouldBeConfigured
public void kafkatest_f18986_0()
{    final Map<String, Object> serializerConfigs = new HashMap<>();    serializerConfigs.put("key.serializer.encoding", "UTF8");    serializerConfigs.put("value.serializer.encoding", "UTF-16");    final Serializer<String> serializer = Serdes.String().serializer();    final String str = "my string for testing";    final String topic = "my topic";    serializer.configure(serializerConfigs, true);    assertEquals("Should get the original string after serialization and deserialization with the configured encoding", str, streamsConfig.defaultKeySerde().deserializer().deserialize(topic, serializer.serialize(topic, str)));    serializer.configure(serializerConfigs, false);    assertEquals("Should get the original string after serialization and deserialization with the configured encoding", str, streamsConfig.defaultValueSerde().deserializer().deserialize(topic, serializer.serialize(topic, str)));}
f18986
0
shouldBeSupportNonPrefixedConsumerConfigs
public void kafkatest_f18994_0()
{    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertEquals("earliest", consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));    assertEquals(1, consumerConfigs.get(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG));}
f18994
0
shouldBeSupportNonPrefixedRestoreConsumerConfigs
public void kafkatest_f18995_0()
{    props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getRestoreConsumerConfigs(groupId);    assertEquals(1, consumerConfigs.get(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG));}
f18995
0
shouldSupportNonPrefixedProducerConfigs
public void kafkatest_f18996_0()
{    props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10);    props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> configs = streamsConfig.getProducerConfigs(clientId);    assertEquals(10, configs.get(ProducerConfig.BUFFER_MEMORY_CONFIG));    assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));}
f18996
0
shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer
public void kafkatest_f19004_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "10");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getRestoreConsumerConfigs(clientId);    assertEquals("10", consumerConfigs.get(ConsumerConfig.MAX_POLL_RECORDS_CONFIG));}
f19004
0
shouldResetToDefaultIfConsumerAutoCommitIsOverridden
public void kafkatest_f19005_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), "true");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs("a", "b", threadIdx);    assertEquals("false", consumerConfigs.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));}
f19005
0
shouldResetToDefaultIfRestoreConsumerAutoCommitIsOverridden
public void kafkatest_f19006_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), "true");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getRestoreConsumerConfigs(clientId);    assertEquals("false", consumerConfigs.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));}
f19006
0
shouldSetInternalLeaveGroupOnCloseConfigToFalseInConsumer
public void kafkatest_f19014_0()
{    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertThat(consumerConfigs.get("internal.leave.group.on.close"), CoreMatchers.equalTo(false));}
f19014
0
shouldAcceptAtLeastOnce
public void kafkatest_f19015_0()
{    // don't use `StreamsConfig.AT_LEAST_ONCE` to actually do a useful test    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, "at_least_once");    new StreamsConfig(props);}
f19015
0
shouldAcceptExactlyOnce
public void kafkatest_f19016_0()
{    // don't use `StreamsConfig.EXACLTY_ONCE` to actually do a useful test    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, "exactly_once");    new StreamsConfig(props);}
f19016
0
shouldResetToDefaultIfProducerEnableIdempotenceIsOverriddenIfEosEnabled
public void kafkatest_f19024_0()
{    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);    props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "anyValue");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(clientId);    assertTrue((Boolean) producerConfigs.get(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG));}
f19024
0
shouldAllowSettingProducerEnableIdempotenceIfEosDisabled
public void kafkatest_f19025_0()
{    props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, false);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(clientId);    assertThat(producerConfigs.get(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG), equalTo(false));}
f19025
0
shouldSetDifferentDefaultsIfEosEnabled
public void kafkatest_f19026_0()
{    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(clientId);    assertThat(consumerConfigs.get(ConsumerConfig.ISOLATION_LEVEL_CONFIG), equalTo(READ_COMMITTED.name().toLowerCase(Locale.ROOT)));    assertTrue((Boolean) producerConfigs.get(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG));    assertThat(producerConfigs.get(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG), equalTo(Integer.MAX_VALUE));    assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(100L));}
f19026
0
shouldThrowExceptionIfMaxInFlightRequestsGreaterThanFiveIfEosEnabled
public void kafkatest_f19034_0()
{    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);    props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 7);    final StreamsConfig streamsConfig = new StreamsConfig(props);    try {        streamsConfig.getProducerConfigs(clientId);        fail("Should throw ConfigException when ESO is enabled and maxInFlight requests exceeds 5");    } catch (final ConfigException e) {        assertEquals("Invalid value 7 for configuration max.in.flight.requests.per.connection: Can't exceed 5 when exactly-once processing is enabled", e.getMessage());    }}
f19034
0
shouldAllowToSpecifyMaxInFlightRequestsPerConnectionAsStringIfEosEnabled
public void kafkatest_f19035_0()
{    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);    props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "3");    new StreamsConfig(props).getProducerConfigs(clientId);}
f19035
0
shouldThrowConfigExceptionIfMaxInFlightRequestsPerConnectionIsInvalidStringIfEosEnabled
public void kafkatest_f19036_0()
{    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);    props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "not-a-number");    try {        new StreamsConfig(props).getProducerConfigs(clientId);        fail("Should throw ConfigException when EOS is enabled and maxInFlight cannot be paresed into an integer");    } catch (final ConfigException e) {        assertEquals("Invalid value not-a-number for configuration max.in.flight.requests.per.connection: String value could not be parsed as 32-bit integer", e.getMessage());    }}
f19036
0
main
public static void kafkatest_f19044_0(final String[] args) throws IOException
{    if (args.length < 2) {        System.err.println("BrokerCompatibilityTest are expecting two parameters: propFile, eosEnabled; but only see " + args.length + " parameter");        System.exit(1);    }    System.out.println("StreamsTest instance started");    final String propFileName = args[0];    final boolean eosEnabled = Boolean.parseBoolean(args[1]);    final Properties streamsProperties = Utils.loadProps(propFileName);    final String kafka = streamsProperties.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);    if (kafka == null) {        System.err.println("No bootstrap kafka servers specified in " + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);        System.exit(1);    }    streamsProperties.put(StreamsConfig.APPLICATION_ID_CONFIG, "kafka-streams-system-test-broker-compatibility");    streamsProperties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsProperties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsProperties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsProperties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    streamsProperties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    if (eosEnabled) {        streamsProperties.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);    }    final int timeout = 6000;    streamsProperties.put(StreamsConfig.consumerPrefix(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG), timeout);    streamsProperties.put(StreamsConfig.consumerPrefix(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG), timeout);    streamsProperties.put(StreamsConfig.REQUEST_TIMEOUT_MS_CONFIG, timeout + 1);    final Serde<String> stringSerde = Serdes.String();    final StreamsBuilder builder = new StreamsBuilder();    builder.<String, String>stream(SOURCE_TOPIC).groupByKey(Grouped.with(stringSerde, stringSerde)).count().toStream().mapValues(new ValueMapper<Long, String>() {        @Override        public String apply(final Long value) {            return value.toString();        }    }).to(SINK_TOPIC);    final KafkaStreams streams = new KafkaStreams(builder.build(), streamsProperties);    streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {        @Override        public void uncaughtException(final Thread t, final Throwable e) {            Throwable cause = e;            if (cause instanceof StreamsException) {                while (cause.getCause() != null) {                    cause = cause.getCause();                }            }            System.err.println("FATAL: An unexpected exception " + cause);            e.printStackTrace(System.err);            System.err.flush();            streams.close(Duration.ofSeconds(30));        }    });    System.out.println("start Kafka Streams");    streams.start();    System.out.println("send data");    final Properties producerProperties = new Properties();    producerProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    producerProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    producerProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    try {        try (final KafkaProducer<String, String> producer = new KafkaProducer<>(producerProperties)) {            producer.send(new ProducerRecord<>(SOURCE_TOPIC, "key", "value"));            System.out.println("wait for result");            loopUntilRecordReceived(kafka, eosEnabled);            System.out.println("close Kafka Streams");            streams.close();        }    } catch (final RuntimeException e) {        System.err.println("Non-Streams exception occurred: ");        e.printStackTrace(System.err);        System.err.flush();    }}
f19044
0
apply
public String kafkatest_f19045_0(final Long value)
{    return value.toString();}
f19045
0
uncaughtException
public void kafkatest_f19046_0(final Thread t, final Throwable e)
{    Throwable cause = e;    if (cause instanceof StreamsException) {        while (cause.getCause() != null) {            cause = cause.getCause();        }    }    System.err.println("FATAL: An unexpected exception " + cause);    e.printStackTrace(System.err);    System.err.flush();    streams.close(Duration.ofSeconds(30));}
f19046
0
apply
public Integer kafkatest_f19054_0(final String aggKey, final Integer value, final Integer aggregate)
{    return (value < aggregate) ? value : aggregate;}
f19054
0
apply
public Long kafkatest_f19055_0()
{    return 0L;}
f19055
0
apply
public Long kafkatest_f19056_0(final String aggKey, final Integer value, final Long aggregate)
{    return (long) value + aggregate;}
f19056
0
getCommittedOffsets
private static Map<TopicPartition, Long> kafkatest_f19064_0(final Admin adminClient, final boolean withRepartitioning)
{    final Map<TopicPartition, OffsetAndMetadata> topicPartitionOffsetAndMetadataMap;    try {        final ListConsumerGroupOffsetsResult listConsumerGroupOffsetsResult = adminClient.listConsumerGroupOffsets(EosTestClient.APP_ID);        topicPartitionOffsetAndMetadataMap = listConsumerGroupOffsetsResult.partitionsToOffsetAndMetadata().get(10, TimeUnit.SECONDS);    } catch (final InterruptedException | ExecutionException | java.util.concurrent.TimeoutException e) {        e.printStackTrace();        throw new RuntimeException(e);    }    final Map<TopicPartition, Long> committedOffsets = new HashMap<>();    for (final Map.Entry<TopicPartition, OffsetAndMetadata> entry : topicPartitionOffsetAndMetadataMap.entrySet()) {        final String topic = entry.getKey().topic();        if (topic.equals("data") || withRepartitioning && topic.equals("repartition")) {            committedOffsets.put(entry.getKey(), entry.getValue().offset());        }    }    return committedOffsets;}
f19064
0
getRecords
private static Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> kafkatest_f19065_0(final KafkaConsumer<byte[], byte[]> consumer, final Map<TopicPartition, Long> readEndOffsets, final boolean withRepartitioning, final boolean isInputTopic)
{    System.err.println("read end offset: " + readEndOffsets);    final Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> recordPerTopicPerPartition = new HashMap<>();    final Map<TopicPartition, Long> maxReceivedOffsetPerPartition = new HashMap<>();    final Map<TopicPartition, Long> maxConsumerPositionPerPartition = new HashMap<>();    long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;    boolean allRecordsReceived = false;    while (!allRecordsReceived && System.currentTimeMillis() < maxWaitTime) {        final ConsumerRecords<byte[], byte[]> receivedRecords = consumer.poll(Duration.ofMillis(100));        for (final ConsumerRecord<byte[], byte[]> record : receivedRecords) {            maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;            final TopicPartition tp = new TopicPartition(record.topic(), record.partition());            maxReceivedOffsetPerPartition.put(tp, record.offset());            final long readEndOffset = readEndOffsets.get(tp);            if (record.offset() < readEndOffset) {                addRecord(record, recordPerTopicPerPartition, withRepartitioning);            } else if (!isInputTopic) {                throw new RuntimeException("FAIL: did receive more records than expected for " + tp + " (expected EOL offset: " + readEndOffset + "; current offset: " + record.offset());            }        }        for (final TopicPartition tp : readEndOffsets.keySet()) {            maxConsumerPositionPerPartition.put(tp, consumer.position(tp));            if (consumer.position(tp) >= readEndOffsets.get(tp)) {                consumer.pause(Collections.singletonList(tp));            }        }        allRecordsReceived = consumer.paused().size() == readEndOffsets.keySet().size();    }    if (!allRecordsReceived) {        System.err.println("Pause partitions (ie, received all data): " + consumer.paused());        System.err.println("Max received offset per partition: " + maxReceivedOffsetPerPartition);        System.err.println("Max consumer position per partition: " + maxConsumerPositionPerPartition);        throw new RuntimeException("FAIL: did not receive all records after " + (MAX_IDLE_TIME_MS / 1000) + " sec idle time.");    }    return recordPerTopicPerPartition;}
f19065
0
addRecord
private static void kafkatest_f19066_0(final ConsumerRecord<byte[], byte[]> record, final Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> recordPerTopicPerPartition, final boolean withRepartitioning)
{    final String topic = record.topic();    final TopicPartition partition = new TopicPartition(topic, record.partition());    if (verifyTopic(topic, withRepartitioning)) {        final Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> topicRecordsPerPartition = recordPerTopicPerPartition.computeIfAbsent(topic, k -> new HashMap<>());        final List<ConsumerRecord<byte[], byte[]>> records = topicRecordsPerPartition.computeIfAbsent(partition, k -> new ArrayList<>());        records.add(record);    } else {        throw new RuntimeException("FAIL: received data from unexpected topic: " + record);    }}
f19066
0
getAllPartitions
private static List<TopicPartition> kafkatest_f19074_0(final KafkaConsumer<?, ?> consumer, final String... topics)
{    final ArrayList<TopicPartition> partitions = new ArrayList<>();    for (final String topic : topics) {        for (final PartitionInfo info : consumer.partitionsFor(topic)) {            partitions.add(new TopicPartition(info.topic(), info.partition()));        }    }    return partitions;}
f19074
0
getConsumerGroupDescription
private static ConsumerGroupDescription kafkatest_f19075_0(final Admin adminClient)
{    final ConsumerGroupDescription description;    try {        description = adminClient.describeConsumerGroups(Collections.singleton(EosTestClient.APP_ID)).describedGroups().get(EosTestClient.APP_ID).get(10, TimeUnit.SECONDS);    } catch (final InterruptedException | ExecutionException | java.util.concurrent.TimeoutException e) {        e.printStackTrace();        throw new RuntimeException("Unexpected Exception getting group description", e);    }    return description;}
f19075
0
start
public void kafkatest_f19076_0()
{    final String topic = "source";    final Properties props = new Properties();    props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "shouldNotDeadlock");    props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> source = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));    source.foreach(new ForeachAction<String, String>() {        @Override        public void apply(final String key, final String value) {            throw new RuntimeException("KABOOM!");        }    });    final KafkaStreams streams = new KafkaStreams(builder.build(), props);    streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {        @Override        public void uncaughtException(final Thread t, final Throwable e) {            Exit.exit(1);        }    });    Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {        @Override        public void run() {            streams.close(Duration.ofSeconds(5));        }    }));    final Properties producerProps = new Properties();    producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, "SmokeTest");    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    final KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);    producer.send(new ProducerRecord<>(topic, "a", "a"));    producer.flush();    streams.start();    synchronized (this) {        try {            wait();        } catch (final InterruptedException e) {        // ignored        }    }}
f19076
0
getStreamsConfig
private Properties kafkatest_f19084_0(final Properties props)
{    final Properties fullProps = new Properties(props);    fullProps.put(StreamsConfig.APPLICATION_ID_CONFIG, "SmokeTest");    fullProps.put(StreamsConfig.CLIENT_ID_CONFIG, "SmokeTest-" + name);    fullProps.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);    fullProps.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);    fullProps.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);    fullProps.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    fullProps.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);    fullProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    fullProps.put(ProducerConfig.ACKS_CONFIG, "all");    fullProps.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());    fullProps.putAll(props);    return fullProps;}
f19084
0
createKafkaStreams
private KafkaStreams kafkatest_f19085_0(final Properties props)
{    final Topology build = getTopology();    final KafkaStreams streamsClient = new KafkaStreams(build, getStreamsConfig(props));    streamsClient.setStateListener((newState, oldState) -> {        System.out.printf("%s %s: %s -> %s%n", name, Instant.now(), oldState, newState);        if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {            started = true;        }    });    streamsClient.setUncaughtExceptionHandler((t, e) -> {        System.out.println(name + ": FATAL: An unexpected exception is encountered on thread " + t + ": " + e);        streamsClient.close(Duration.ofSeconds(30));    });    return streamsClient;}
f19085
0
getTopology
public Topology kafkatest_f19086_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final Consumed<String, Integer> stringIntConsumed = Consumed.with(stringSerde, intSerde);    final KStream<String, Integer> source = builder.stream("data", stringIntConsumed);    source.filterNot((k, v) -> k.equals("flush")).to("echo", Produced.with(stringSerde, intSerde));    final KStream<String, Integer> data = source.filter((key, value) -> value == null || value != END);    data.process(SmokeTestUtil.printProcessorSupplier("data", name));    // min    final KGroupedStream<String, Integer> groupedData = data.groupByKey(Grouped.with(stringSerde, intSerde));    final KTable<Windowed<String>, Integer> minAggregation = groupedData.windowedBy(TimeWindows.of(Duration.ofDays(1)).grace(Duration.ofMinutes(1))).aggregate(() -> Integer.MAX_VALUE, (aggKey, value, aggregate) -> (value < aggregate) ? value : aggregate, Materialized.<String, Integer, WindowStore<Bytes, byte[]>>as("uwin-min").withValueSerde(intSerde).withRetention(Duration.ofHours(25)));    streamify(minAggregation, "min-raw");    streamify(minAggregation.suppress(untilWindowCloses(BufferConfig.unbounded())), "min-suppressed");    minAggregation.toStream(new Unwindow<>()).filterNot((k, v) -> k.equals("flush")).to("min", Produced.with(stringSerde, intSerde));    final KTable<Windowed<String>, Integer> smallWindowSum = groupedData.windowedBy(TimeWindows.of(Duration.ofSeconds(2)).advanceBy(Duration.ofSeconds(1)).grace(Duration.ofSeconds(30))).reduce((l, r) -> l + r);    streamify(smallWindowSum, "sws-raw");    streamify(smallWindowSum.suppress(untilWindowCloses(BufferConfig.unbounded())), "sws-suppressed");    final KTable<String, Integer> minTable = builder.table("min", Consumed.with(stringSerde, intSerde), Materialized.as("minStoreName"));    minTable.toStream().process(SmokeTestUtil.printProcessorSupplier("min", name));    // max    groupedData.windowedBy(TimeWindows.of(Duration.ofDays(2))).aggregate(() -> Integer.MIN_VALUE, (aggKey, value, aggregate) -> (value > aggregate) ? value : aggregate, Materialized.<String, Integer, WindowStore<Bytes, byte[]>>as("uwin-max").withValueSerde(intSerde)).toStream(new Unwindow<>()).filterNot((k, v) -> k.equals("flush")).to("max", Produced.with(stringSerde, intSerde));    final KTable<String, Integer> maxTable = builder.table("max", Consumed.with(stringSerde, intSerde), Materialized.as("maxStoreName"));    maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier("max", name));    // sum    groupedData.windowedBy(TimeWindows.of(Duration.ofDays(2))).aggregate(() -> 0L, (aggKey, value, aggregate) -> (long) value + aggregate, Materialized.<String, Long, WindowStore<Bytes, byte[]>>as("win-sum").withValueSerde(longSerde)).toStream(new Unwindow<>()).filterNot((k, v) -> k.equals("flush")).to("sum", Produced.with(stringSerde, longSerde));    final Consumed<String, Long> stringLongConsumed = Consumed.with(stringSerde, longSerde);    final KTable<String, Long> sumTable = builder.table("sum", stringLongConsumed);    sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier("sum", name));    // cnt    groupedData.windowedBy(TimeWindows.of(Duration.ofDays(2))).count(Materialized.as("uwin-cnt")).toStream(new Unwindow<>()).filterNot((k, v) -> k.equals("flush")).to("cnt", Produced.with(stringSerde, longSerde));    final KTable<String, Long> cntTable = builder.table("cnt", Consumed.with(stringSerde, longSerde), Materialized.as("cntStoreName"));    cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier("cnt", name));    // dif    maxTable.join(minTable, (value1, value2) -> value1 - value2).toStream().filterNot((k, v) -> k.equals("flush")).to("dif", Produced.with(stringSerde, intSerde));    // avg    sumTable.join(cntTable, (value1, value2) -> (double) value1 / (double) value2).toStream().filterNot((k, v) -> k.equals("flush")).to("avg", Produced.with(stringSerde, doubleSerde));    // test repartition    final Agg agg = new Agg();    cntTable.groupBy(agg.selector(), Grouped.with(stringSerde, longSerde)).aggregate(agg.init(), agg.adder(), agg.remover(), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore("cntByCnt")).withKeySerde(Serdes.String()).withValueSerde(Serdes.Long())).toStream().to("tagg", Produced.with(stringSerde, longSerde));    return builder.build();}
f19086
0
shuffle
private static void kafkatest_f19094_0(final int[] data, kafkatest_f19094_0("SameParameterValue") final int windowSize)
{    final Random rand = new Random();    for (int i = 0; i < data.length; i++) {        // we shuffle data within windowSize        final int j = rand.nextInt(Math.min(data.length - i, windowSize)) + i;        // swap        final int tmp = data[i];        data[i] = data[j];        data[j] = tmp;    }}
f19094
0
deserialize
public Number kafkatest_f19095_0(final String topic, final byte[] data)
{    final Number value;    switch(topic) {        case "data":        case "echo":        case "min":        case "min-raw":        case "min-suppressed":        case "sws-raw":        case "sws-suppressed":        case "max":        case "dif":            value = intSerde.deserializer().deserialize(topic, data);            break;        case "sum":        case "cnt":        case "tagg":            value = longSerde.deserializer().deserialize(topic, data);            break;        case "avg":            value = doubleSerde.deserializer().deserialize(topic, data);            break;        default:            throw new RuntimeException("unknown topic: " + topic);    }    return value;}
f19095
0
verify
public static VerificationResult kafkatest_f19096_0(final String kafka, final Map<String, Set<Integer>> inputs, final int maxRecordsPerKey)
{    final Properties props = new Properties();    props.put(ConsumerConfig.CLIENT_ID_CONFIG, "verifier");    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, NumberDeserializer.class);    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");    final KafkaConsumer<String, Number> consumer = new KafkaConsumer<>(props);    final List<TopicPartition> partitions = getAllPartitions(consumer, TOPICS);    consumer.assign(partitions);    consumer.seekToBeginning(partitions);    final int recordsGenerated = inputs.size() * maxRecordsPerKey;    int recordsProcessed = 0;    final Map<String, AtomicInteger> processed = Stream.of(TOPICS).collect(Collectors.toMap(t -> t, t -> new AtomicInteger(0)));    final Map<String, Map<String, LinkedList<ConsumerRecord<String, Number>>>> events = new HashMap<>();    VerificationResult verificationResult = new VerificationResult(false, "no results yet");    int retry = 0;    final long start = System.currentTimeMillis();    while (System.currentTimeMillis() - start < TimeUnit.MINUTES.toMillis(6)) {        final ConsumerRecords<String, Number> records = consumer.poll(Duration.ofSeconds(1));        if (records.isEmpty() && recordsProcessed >= recordsGenerated) {            verificationResult = verifyAll(inputs, events);            if (verificationResult.passed()) {                break;            } else if (retry++ > MAX_RECORD_EMPTY_RETRIES) {                System.out.println(Instant.now() + " Didn't get any more results, verification hasn't passed, and out of retries.");                break;            } else {                System.out.println(Instant.now() + " Didn't get any more results, but verification hasn't passed (yet). Retrying...");            }        } else {            retry = 0;            for (final ConsumerRecord<String, Number> record : records) {                final String key = record.key();                final String topic = record.topic();                processed.get(topic).incrementAndGet();                if (topic.equals("echo")) {                    recordsProcessed++;                    if (recordsProcessed % 100 == 0) {                        System.out.println("Echo records processed = " + recordsProcessed);                    }                }                events.computeIfAbsent(topic, t -> new HashMap<>()).computeIfAbsent(key, k -> new LinkedList<>()).add(record);            }            System.out.println(processed);        }    }    consumer.close();    final long finished = System.currentTimeMillis() - start;    System.out.println("Verification time=" + finished);    System.out.println("-------------------");    System.out.println("Result Verification");    System.out.println("-------------------");    System.out.println("recordGenerated=" + recordsGenerated);    System.out.println("recordProcessed=" + recordsProcessed);    if (recordsProcessed > recordsGenerated) {        System.out.println("PROCESSED-MORE-THAN-GENERATED");    } else if (recordsProcessed < recordsGenerated) {        System.out.println("PROCESSED-LESS-THAN-GENERATED");    }    boolean success;    final Map<String, Set<Number>> received = events.get("echo").entrySet().stream().map(entry -> mkEntry(entry.getKey(), entry.getValue().stream().map(ConsumerRecord::value).collect(Collectors.toSet()))).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));    success = inputs.equals(received);    if (success) {        System.out.println("ALL-RECORDS-DELIVERED");    } else {        int missedCount = 0;        for (final Map.Entry<String, Set<Integer>> entry : inputs.entrySet()) {            missedCount += received.get(entry.getKey()).size();        }        System.out.println("missedRecords=" + missedCount);    }    // give it one more try if it's not already passing.    if (!verificationResult.passed()) {        verificationResult = verifyAll(inputs, events);    }    success &= verificationResult.passed();    System.out.println(verificationResult.result());    System.out.println(success ? "SUCCESS" : "FAILURE");    return verificationResult;}
f19096
0
getAvg
private static Double kafkatest_f19104_0(final String key)
{    final int min = getMin(key).intValue();    final int max = getMax(key).intValue();    return ((long) min + max) / 2.0;}
f19104
0
verifyTAgg
private static boolean kafkatest_f19105_0(final PrintStream resultStream, final Map<String, Set<Integer>> allData, final Map<String, LinkedList<ConsumerRecord<String, Number>>> taggEvents)
{    if (taggEvents == null) {        resultStream.println("tagg is missing");        return false;    } else if (taggEvents.isEmpty()) {        resultStream.println("tagg is empty");        return false;    } else {        resultStream.println("verifying tagg");        // generate expected answer        final Map<String, Long> expected = new HashMap<>();        for (final String key : allData.keySet()) {            final int min = getMin(key).intValue();            final int max = getMax(key).intValue();            final String cnt = Long.toString(max - min + 1L);            expected.put(cnt, expected.getOrDefault(cnt, 0L) + 1);        }        // check the result        for (final Map.Entry<String, LinkedList<ConsumerRecord<String, Number>>> entry : taggEvents.entrySet()) {            final String key = entry.getKey();            Long expectedCount = expected.remove(key);            if (expectedCount == null) {                expectedCount = 0L;            }            if (entry.getValue().getLast().value().longValue() != expectedCount) {                resultStream.println("fail: key=" + key + " tagg=" + entry.getValue() + " expected=" + expected.get(key));                resultStream.println("\t outputEvents: " + entry.getValue());                return false;            }        }    }    return true;}
f19105
0
getMin
private static Number kafkatest_f19106_0(final String key)
{    return Integer.parseInt(key.split("-")[0]);}
f19106
0
apply
public K kafkatest_f19114_0(final Windowed<K> winKey, final V value)
{    return winKey.key();}
f19114
0
selector
 KeyValueMapper<String, Long, KeyValue<String, Long>> kafkatest_f19115_0()
{    return new KeyValueMapper<String, Long, KeyValue<String, Long>>() {        @Override        public KeyValue<String, Long> apply(final String key, final Long value) {            return new KeyValue<>(value == null ? null : Long.toString(value), 1L);        }    };}
f19115
0
apply
public KeyValue<String, Long> kafkatest_f19116_0(final String key, final Long value)
{    return new KeyValue<>(value == null ? null : Long.toString(value), 1L);}
f19116
0
sleep
public static void kafkatest_f19124_0(final long duration)
{    try {        Thread.sleep(duration);    } catch (final Exception ignore) {    }}
f19124
0
main
public static void kafkatest_f19125_0(final String[] args) throws Exception
{    if (args.length < 1) {        System.err.println(testName + " requires one argument (properties-file) but none provided: ");    }    System.out.println("StreamsTest instance started");    final String propFileName = args[0];    final Properties streamsProperties = Utils.loadProps(propFileName);    final String groupInstanceId = Objects.requireNonNull(streamsProperties.getProperty(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG));    System.out.println(testName + " instance started with group.instance.id " + groupInstanceId);    System.out.println("props=" + streamsProperties);    System.out.flush();    final StreamsBuilder builder = new StreamsBuilder();    final String inputTopic = (String) (Objects.requireNonNull(streamsProperties.remove("input.topic")));    final KStream dataStream = builder.stream(inputTopic);    dataStream.peek((k, v) -> System.out.println(String.format("PROCESSED key=%s value=%s", k, v)));    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, testName);    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder.build(), config);    streams.setStateListener((newState, oldState) -> {        if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {            System.out.println("REBALANCING -> RUNNING");            System.out.flush();        }    });    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread() {        @Override        public void run() {            System.out.println("closing Kafka Streams instance");            System.out.flush();            streams.close();            System.out.println("Static membership test closed");            System.out.flush();        }    });}
f19125
0
run
public void kafkatest_f19126_0()
{    System.out.println("closing Kafka Streams instance");    System.out.flush();    streams.close();    System.out.println("Static membership test closed");    System.out.flush();}
f19126
0
main
public static void kafkatest_f19134_0(final String[] args) throws Exception
{    if (args.length < 1) {        System.err.println("StreamsNamedRepartitionTest requires one argument (properties-file) but none provided: ");    }    final String propFileName = args[0];    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started NAMED_REPARTITION_TEST");    System.out.println("props=" + streamsProperties);    final String inputTopic = (String) (Objects.requireNonNull(streamsProperties.remove("input.topic")));    final String aggregationTopic = (String) (Objects.requireNonNull(streamsProperties.remove("aggregation.topic")));    final boolean addOperators = Boolean.valueOf(Objects.requireNonNull((String) streamsProperties.remove("add.operations")));    final Initializer<Integer> initializer = () -> 0;    final Aggregator<String, String, Integer> aggregator = (k, v, agg) -> agg + Integer.parseInt(v);    final Function<String, String> keyFunction = s -> Integer.toString(Integer.parseInt(s) % 9);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> sourceStream = builder.stream(inputTopic, Consumed.with(Serdes.String(), Serdes.String()));    sourceStream.peek((k, v) -> System.out.println(String.format("input data key=%s, value=%s", k, v)));    final KStream<String, String> mappedStream = sourceStream.selectKey((k, v) -> keyFunction.apply(v));    final KStream<String, String> maybeUpdatedStream;    if (addOperators) {        maybeUpdatedStream = mappedStream.filter((k, v) -> true).mapValues(v -> Integer.toString(Integer.parseInt(v) + 1));    } else {        maybeUpdatedStream = mappedStream;    }    maybeUpdatedStream.groupByKey(Grouped.with("grouped-stream", Serdes.String(), Serdes.String())).aggregate(initializer, aggregator, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>as("count-store").withKeySerde(Serdes.String()).withValueSerde(Serdes.Integer())).toStream().peek((k, v) -> System.out.println(String.format("AGGREGATED key=%s value=%s", k, v))).to(aggregationTopic, Produced.with(Serdes.String(), Serdes.Integer()));    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsNamedRepartitionTest");    config.setProperty(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, "0");    config.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    config.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    config.putAll(streamsProperties);    final Topology topology = builder.build(config);    final KafkaStreams streams = new KafkaStreams(topology, config);    streams.setStateListener((newState, oldState) -> {        if (oldState == State.REBALANCING && newState == State.RUNNING) {            if (addOperators) {                System.out.println("UPDATED Topology");            } else {                System.out.println("REBALANCING -> RUNNING");            }            System.out.flush();        }    });    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread(() -> {        System.out.println("closing Kafka Streams instance");        System.out.flush();        streams.close(Duration.ofMillis(5000));        System.out.println("NAMED_REPARTITION_TEST Streams Stopped");        System.out.flush();    }));}
f19134
0
main
public static void kafkatest_f19135_0(final String[] args) throws Exception
{    if (args.length < 1) {        System.err.println("StreamsOptimizedTest requires one argument (properties-file) but no provided: ");    }    final String propFileName = args[0];    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started StreamsOptimizedTest");    System.out.println("props=" + streamsProperties);    final String inputTopic = (String) Objects.requireNonNull(streamsProperties.remove("input.topic"));    final String aggregationTopic = (String) Objects.requireNonNull(streamsProperties.remove("aggregation.topic"));    final String reduceTopic = (String) Objects.requireNonNull(streamsProperties.remove("reduce.topic"));    final String joinTopic = (String) Objects.requireNonNull(streamsProperties.remove("join.topic"));    final Pattern repartitionTopicPattern = Pattern.compile("Sink: .*-repartition");    final Initializer<Integer> initializer = () -> 0;    final Aggregator<String, String, Integer> aggregator = (k, v, agg) -> agg + v.length();    final Reducer<String> reducer = (v1, v2) -> Integer.toString(Integer.parseInt(v1) + Integer.parseInt(v2));    final Function<String, String> keyFunction = s -> Integer.toString(Integer.parseInt(s) % 9);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> sourceStream = builder.stream(inputTopic, Consumed.with(Serdes.String(), Serdes.String()));    final KStream<String, String> mappedStream = sourceStream.selectKey((k, v) -> keyFunction.apply(v));    final KStream<String, Long> countStream = mappedStream.groupByKey().count(Materialized.with(Serdes.String(), Serdes.Long())).toStream();    mappedStream.groupByKey().aggregate(initializer, aggregator, Materialized.with(Serdes.String(), Serdes.Integer())).toStream().peek((k, v) -> System.out.println(String.format("AGGREGATED key=%s value=%s", k, v))).to(aggregationTopic, Produced.with(Serdes.String(), Serdes.Integer()));    mappedStream.groupByKey().reduce(reducer, Materialized.with(Serdes.String(), Serdes.String())).toStream().peek((k, v) -> System.out.println(String.format("REDUCED key=%s value=%s", k, v))).to(reduceTopic, Produced.with(Serdes.String(), Serdes.String()));    mappedStream.join(countStream, (v1, v2) -> v1 + ":" + v2.toString(), JoinWindows.of(ofMillis(500)), Joined.with(Serdes.String(), Serdes.String(), Serdes.Long())).peek((k, v) -> System.out.println(String.format("JOINED key=%s value=%s", k, v))).to(joinTopic, Produced.with(Serdes.String(), Serdes.String()));    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsOptimizedTest");    config.setProperty(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, "0");    config.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    config.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    config.setProperty(StreamsConfig.adminClientPrefix(AdminClientConfig.RETRIES_CONFIG), "100");    config.putAll(streamsProperties);    final Topology topology = builder.build(config);    final KafkaStreams streams = new KafkaStreams(topology, config);    streams.setStateListener((newState, oldState) -> {        if (oldState == State.REBALANCING && newState == State.RUNNING) {            final int repartitionTopicCount = getCountOfRepartitionTopicsFound(topology.describe().toString(), repartitionTopicPattern);            System.out.println(String.format("REBALANCING -> RUNNING with REPARTITION TOPIC COUNT=%d", repartitionTopicCount));            System.out.flush();        }    });    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread() {        @Override        public void run() {            System.out.println("closing Kafka Streams instance");            System.out.flush();            streams.close(Duration.ofMillis(5000));            System.out.println("OPTIMIZE_TEST Streams Stopped");            System.out.flush();        }    });}
f19135
0
run
public void kafkatest_f19136_0()
{    System.out.println("closing Kafka Streams instance");    System.out.flush();    streams.close(Duration.ofMillis(5000));    System.out.println("OPTIMIZE_TEST Streams Stopped");    System.out.flush();}
f19136
0
subscriptionUserData
public ByteBuffer kafkatest_f19144_0(final Set<String> topics)
{    // Adds the following information to subscription    // 1. Client UUID (a unique id assigned to an instance of KafkaStreams)    // 2. Task ids of previously running tasks    // 3. Task ids of valid local states on the client's state directory.    final TaskManager taskManager = taskManger();    final Set<TaskId> previousActiveTasks = taskManager.prevActiveTaskIds();    final Set<TaskId> standbyTasks = taskManager.cachedTasksIds();    standbyTasks.removeAll(previousActiveTasks);    final FutureSubscriptionInfo data = new FutureSubscriptionInfo(usedSubscriptionMetadataVersion, LATEST_SUPPORTED_VERSION + 1, taskManager.processId(), previousActiveTasks, standbyTasks, userEndPoint());    taskManager.updateSubscriptionsFromMetadata(topics);    return data.encode();}
f19144
0
onAssignment
public void kafkatest_f19145_0(final ConsumerPartitionAssignor.Assignment assignment, final ConsumerGroupMetadata metadata)
{    try {        super.onAssignment(assignment, metadata);        return;    } catch (final TaskAssignmentException cannotProcessFutureVersion) {    // continue    }    final ByteBuffer data = assignment.userData();    data.rewind();    final int usedVersion;    try (final DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) {        usedVersion = in.readInt();    } catch (final IOException ex) {        throw new TaskAssignmentException("Failed to decode AssignmentInfo", ex);    }    if (usedVersion > LATEST_SUPPORTED_VERSION + 1) {        throw new IllegalStateException("Unknown metadata version: " + usedVersion + "; latest supported version: " + LATEST_SUPPORTED_VERSION + 1);    }    final AssignmentInfo info = AssignmentInfo.decode(assignment.userData().putInt(0, LATEST_SUPPORTED_VERSION));    final List<TopicPartition> partitions = new ArrayList<>(assignment.partitions());    partitions.sort(PARTITION_COMPARATOR);    // version 1 field    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    // version 2 fields    final Map<TopicPartition, PartitionInfo> topicToPartitionInfo = new HashMap<>();    final Map<HostInfo, Set<TopicPartition>> partitionsByHost;    processVersionTwoAssignment("test ", info, partitions, activeTasks, topicToPartitionInfo);    partitionsByHost = info.partitionsByHost();    final TaskManager taskManager = taskManger();    taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));    taskManager.setPartitionsByHostState(partitionsByHost);    taskManager.setAssignmentMetadata(activeTasks, info.standbyTasks());    taskManager.updateSubscriptionsFromAssignment(partitions);}
f19145
0
assign
public GroupAssignment kafkatest_f19146_0(final Cluster metadata, final GroupSubscription groupSubscription)
{    final Map<String, Subscription> subscriptions = groupSubscription.groupSubscription();    final Set<Integer> supportedVersions = new HashSet<>();    for (final Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {        final Subscription subscription = entry.getValue();        final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());        supportedVersions.add(info.latestSupportedVersion());    }    Map<String, Assignment> assignment = null;    final Map<String, Subscription> downgradedSubscriptions = new HashMap<>();    for (final Subscription subscription : subscriptions.values()) {        final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());        if (info.version() < LATEST_SUPPORTED_VERSION + 1) {            assignment = super.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();            break;        }    }    boolean bumpUsedVersion = false;    final boolean bumpSupportedVersion;    if (assignment != null) {        bumpSupportedVersion = supportedVersions.size() == 1 && supportedVersions.iterator().next() == LATEST_SUPPORTED_VERSION + 1;    } else {        for (final Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {            final Subscription subscription = entry.getValue();            final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData().putInt(0, LATEST_SUPPORTED_VERSION).putInt(4, LATEST_SUPPORTED_VERSION));            downgradedSubscriptions.put(entry.getKey(), new Subscription(subscription.topics(), new SubscriptionInfo(info.processId(), info.prevTasks(), info.standbyTasks(), info.userEndPoint()).encode()));        }        assignment = super.assign(metadata, new GroupSubscription(downgradedSubscriptions)).groupAssignment();        bumpUsedVersion = true;        bumpSupportedVersion = true;    }    final Map<String, Assignment> newAssignment = new HashMap<>();    for (final Map.Entry<String, Assignment> entry : assignment.entrySet()) {        final Assignment singleAssignment = entry.getValue();        newAssignment.put(entry.getKey(), new Assignment(singleAssignment.partitions(), new FutureAssignmentInfo(bumpUsedVersion, bumpSupportedVersion, singleAssignment.userData()).encode()));    }    return new GroupAssignment(newAssignment);}
f19146
0
shouldThrowExceptionIfNotCorrectKeyValueSeparator
public void kafkatest_f19154_0()
{    final String badString = "foo:bar,baz:boo";    SystemTestUtil.parseConfigs(badString);}
f19154
0
shouldThrowExceptionIfNotCorrectKeyValuePairSeparator
public void kafkatest_f19155_0()
{    final String badString = "foo=bar;baz=boo";    SystemTestUtil.parseConfigs(badString);}
f19155
0
shouldParseSingleKeyValuePairString
public void kafkatest_f19156_0()
{    final Map<String, String> expectedSinglePairMap = new HashMap<>();    expectedSinglePairMap.put("foo", "bar");    final String singleValueString = "foo=bar";    final Map<String, String> parsedMap = SystemTestUtil.parseConfigs(singleValueString);    assertEquals(expectedSinglePairMap, parsedMap);}
f19156
0
testResetUsingPlanWhenBetweenBeginningAndEndOffset
public void kafkatest_f19164_0()
{    final Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(topicPartition, 4L);    consumer.updateEndOffsets(endOffsets);    final Map<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(topicPartition, 0L);    consumer.updateBeginningOffsets(beginningOffsets);    final Map<TopicPartition, Long> topicPartitionsAndOffset = new HashMap<>();    topicPartitionsAndOffset.put(topicPartition, 3L);    streamsResetter.resetOffsetsFromResetPlan(consumer, inputTopicPartitions, topicPartitionsAndOffset);    final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(500));    assertEquals(2, records.count());}
f19164
0
testResetUsingPlanWhenBeforeBeginningOffset
public void kafkatest_f19165_0()
{    final Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(topicPartition, 4L);    consumer.updateEndOffsets(endOffsets);    final Map<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(topicPartition, 3L);    consumer.updateBeginningOffsets(beginningOffsets);    final Map<TopicPartition, Long> topicPartitionsAndOffset = new HashMap<>();    topicPartitionsAndOffset.put(topicPartition, 1L);    streamsResetter.resetOffsetsFromResetPlan(consumer, inputTopicPartitions, topicPartitionsAndOffset);    final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(500));    assertEquals(2, records.count());}
f19165
0
testResetUsingPlanWhenAfterEndOffset
public void kafkatest_f19166_0()
{    final Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(topicPartition, 3L);    consumer.updateEndOffsets(endOffsets);    final Map<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(topicPartition, 0L);    consumer.updateBeginningOffsets(beginningOffsets);    final Map<TopicPartition, Long> topicPartitionsAndOffset = new HashMap<>();    topicPartitionsAndOffset.put(topicPartition, 5L);    streamsResetter.resetOffsetsFromResetPlan(consumer, inputTopicPartitions, topicPartitionsAndOffset);    final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(500));    assertEquals(2, records.count());}
f19166
0
shouldNotAllowNullNameWhenAddingSourceWithPattern
public void kafkatest_f19174_0()
{    topology.addSource(null, Pattern.compile(".*"));}
f19174
0
shouldNotAllowNullTopicsWhenAddingSoureWithTopic
public void kafkatest_f19175_0()
{    topology.addSource("source", (String[]) null);}
f19175
0
shouldNotAllowNullTopicsWhenAddingSourceWithPattern
public void kafkatest_f19176_0()
{    topology.addSource("source", (Pattern) null);}
f19176
0
shouldNotAllowNullStoreNameWhenConnectingProcessorAndStateStores
public void kafkatest_f19184_0()
{    topology.connectProcessorAndStateStores("processor", (String[]) null);}
f19184
0
shouldNotAllowZeroStoreNameWhenConnectingProcessorAndStateStores
public void kafkatest_f19185_0()
{    topology.connectProcessorAndStateStores("processor");}
f19185
0
shouldNotAddNullStateStoreSupplier
public void kafkatest_f19186_0()
{    topology.addStateStore(null);}
f19186
0
shouldFailOnUnknownSource
public void kafkatest_f19194_0()
{    topology.addProcessor("processor", new MockProcessorSupplier(), "source");}
f19194
0
shouldFailIfNodeIsItsOwnParent
public void kafkatest_f19195_0()
{    topology.addProcessor("processor", new MockProcessorSupplier(), "processor");}
f19195
0
shouldNotAllowToAddSinkWithSameName
public void kafkatest_f19196_0()
{    topology.addSource("source", "topic-1");    topology.addSink("sink", "topic-2", "source");    try {        topology.addSink("sink", "topic-3", "source");        fail("Should throw TopologyException for duplicate sink name");    } catch (final TopologyException expected) {    }}
f19196
0
shouldNotAllowToAddStateStoreToSink
public void kafkatest_f19204_0()
{    mockStoreBuilder();    EasyMock.replay(storeBuilder);    topology.addSource("source-1", "topic-1");    topology.addSink("sink-1", "topic-1", "source-1");    try {        topology.addStateStore(storeBuilder, "sink-1");        fail("Should have thrown TopologyException for adding store to sink node");    } catch (final TopologyException expected) {    }}
f19204
0
mockStoreBuilder
private void kafkatest_f19205_0()
{    EasyMock.expect(storeBuilder.name()).andReturn("store").anyTimes();    EasyMock.expect(storeBuilder.logConfig()).andReturn(Collections.emptyMap());    EasyMock.expect(storeBuilder.loggingEnabled()).andReturn(false);}
f19205
0
shouldNotAllowToAddStoreWithSameName
public void kafkatest_f19206_0()
{    mockStoreBuilder();    EasyMock.replay(storeBuilder);    topology.addStateStore(storeBuilder);    try {        topology.addStateStore(storeBuilder);        fail("Should have thrown TopologyException for duplicate store name");    } catch (final TopologyException expected) {    }}
f19206
0
singleSourceShouldHaveSingleSubtopology
public void kafkatest_f19216_0()
{    final TopologyDescription.Source expectedSourceNode = addSource("source", "topic");    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, Collections.singleton(expectedSourceNode)));    assertThat(topology.describe(), equalTo(expectedDescription));}
f19216
0
singleSourceWithListOfTopicsShouldHaveSingleSubtopology
public void kafkatest_f19217_0()
{    final TopologyDescription.Source expectedSourceNode = addSource("source", "topic1", "topic2", "topic3");    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, Collections.singleton(expectedSourceNode)));    assertThat(topology.describe(), equalTo(expectedDescription));}
f19217
0
singleSourcePatternShouldHaveSingleSubtopology
public void kafkatest_f19218_0()
{    final TopologyDescription.Source expectedSourceNode = addSource("source", Pattern.compile("topic[0-9]"));    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, Collections.singleton(expectedSourceNode)));    assertThat(topology.describe(), equalTo(expectedDescription));}
f19218
0
multipleSourcesWithSinksShouldHaveDistinctSubtopologies
public void kafkatest_f19226_0()
{    final TopologyDescription.Source expectedSourceNode1 = addSource("source1", "topic1");    final TopologyDescription.Sink expectedSinkNode1 = addSink("sink1", "sinkTopic1", expectedSourceNode1);    final TopologyDescription.Source expectedSourceNode2 = addSource("source2", "topic2");    final TopologyDescription.Sink expectedSinkNode2 = addSink("sink2", "sinkTopic2", expectedSourceNode2);    final TopologyDescription.Source expectedSourceNode3 = addSource("source3", "topic3");    final TopologyDescription.Sink expectedSinkNode3 = addSink("sink3", "sinkTopic3", expectedSourceNode3);    final Set<TopologyDescription.Node> allNodes1 = new HashSet<>();    allNodes1.add(expectedSourceNode1);    allNodes1.add(expectedSinkNode1);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, allNodes1));    final Set<TopologyDescription.Node> allNodes2 = new HashSet<>();    allNodes2.add(expectedSourceNode2);    allNodes2.add(expectedSinkNode2);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(1, allNodes2));    final Set<TopologyDescription.Node> allNodes3 = new HashSet<>();    allNodes3.add(expectedSourceNode3);    allNodes3.add(expectedSinkNode3);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(2, allNodes3));    assertThat(topology.describe(), equalTo(expectedDescription));}
f19226
0
processorsWithSameSinkShouldHaveSameSubtopology
public void kafkatest_f19227_0()
{    final TopologyDescription.Source expectedSourceNode1 = addSource("source", "topic");    final TopologyDescription.Processor expectedProcessorNode1 = addProcessor("processor1", expectedSourceNode1);    final TopologyDescription.Source expectedSourceNode2 = addSource("source2", "topic2");    final TopologyDescription.Processor expectedProcessorNode2 = addProcessor("processor2", expectedSourceNode2);    final TopologyDescription.Source expectedSourceNode3 = addSource("source3", "topic3");    final TopologyDescription.Processor expectedProcessorNode3 = addProcessor("processor3", expectedSourceNode3);    final TopologyDescription.Sink expectedSinkNode = addSink("sink", "sinkTopic", expectedProcessorNode1, expectedProcessorNode2, expectedProcessorNode3);    final Set<TopologyDescription.Node> allNodes = new HashSet<>();    allNodes.add(expectedSourceNode1);    allNodes.add(expectedProcessorNode1);    allNodes.add(expectedSourceNode2);    allNodes.add(expectedProcessorNode2);    allNodes.add(expectedSourceNode3);    allNodes.add(expectedProcessorNode3);    allNodes.add(expectedSinkNode);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, allNodes));    assertThat(topology.describe(), equalTo(expectedDescription));}
f19227
0
processorsWithSharedStateShouldHaveSameSubtopology
public void kafkatest_f19228_0()
{    final String[] store1 = new String[] { "store1" };    final String[] store2 = new String[] { "store2" };    final String[] bothStores = new String[] { store1[0], store2[0] };    final TopologyDescription.Source expectedSourceNode1 = addSource("source", "topic");    final TopologyDescription.Processor expectedProcessorNode1 = addProcessorWithNewStore("processor1", store1, expectedSourceNode1);    final TopologyDescription.Source expectedSourceNode2 = addSource("source2", "topic2");    final TopologyDescription.Processor expectedProcessorNode2 = addProcessorWithNewStore("processor2", store2, expectedSourceNode2);    final TopologyDescription.Source expectedSourceNode3 = addSource("source3", "topic3");    final TopologyDescription.Processor expectedProcessorNode3 = addProcessorWithExistingStore("processor3", bothStores, expectedSourceNode3);    final Set<TopologyDescription.Node> allNodes = new HashSet<>();    allNodes.add(expectedSourceNode1);    allNodes.add(expectedProcessorNode1);    allNodes.add(expectedSourceNode2);    allNodes.add(expectedProcessorNode2);    allNodes.add(expectedSourceNode3);    allNodes.add(expectedProcessorNode3);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, allNodes));    assertThat(topology.describe(), equalTo(expectedDescription));}
f19228
0
kGroupedStreamAnonymousMaterializedCountShouldPreserveTopologyStructure
public void kafkatest_f19236_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("input-topic").groupByKey().count(Materialized.with(null, Serdes.Long()));    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])\n" + "      --> KSTREAM-AGGREGATE-0000000003\n" + "    Processor: KSTREAM-AGGREGATE-0000000003 (stores: [KSTREAM-AGGREGATE-STATE-STORE-0000000002])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000000\n\n", describe.toString());}
f19236
0
timeWindowZeroArgCountShouldPreserveTopologyStructure
public void kafkatest_f19237_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("input-topic").groupByKey().windowedBy(TimeWindows.of(ofMillis(1))).count();    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])\n" + "      --> KSTREAM-AGGREGATE-0000000002\n" + "    Processor: KSTREAM-AGGREGATE-0000000002 (stores: [KSTREAM-AGGREGATE-STATE-STORE-0000000001])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000000\n\n", describe.toString());}
f19237
0
timeWindowNamedMaterializedCountShouldPreserveTopologyStructure
public void kafkatest_f19238_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("input-topic").groupByKey().windowedBy(TimeWindows.of(ofMillis(1))).count(Materialized.as("count-store"));    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])\n" + "      --> KSTREAM-AGGREGATE-0000000001\n" + "    Processor: KSTREAM-AGGREGATE-0000000001 (stores: [count-store])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000000\n\n", describe.toString());}
f19238
0
kTableNonMaterializedMapValuesShouldPreserveTopologyStructure
public void kafkatest_f19246_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<Object, Object> table = builder.table("input-topic");    table.mapValues((readOnlyKey, value) -> null);    final TopologyDescription describe = builder.build().describe();    Assert.assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000001 (topics: [input-topic])\n" + "      --> KTABLE-SOURCE-0000000002\n" + "    Processor: KTABLE-SOURCE-0000000002 (stores: [])\n" + "      --> KTABLE-MAPVALUES-0000000003\n" + "      <-- KSTREAM-SOURCE-0000000001\n" + "    Processor: KTABLE-MAPVALUES-0000000003 (stores: [])\n" + "      --> none\n" + "      <-- KTABLE-SOURCE-0000000002\n\n", describe.toString());}
f19246
0
kTableAnonymousMaterializedMapValuesShouldPreserveTopologyStructure
public void kafkatest_f19247_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<Object, Object> table = builder.table("input-topic");    table.mapValues((readOnlyKey, value) -> null, Materialized.with(null, null));    final TopologyDescription describe = builder.build().describe();    Assert.assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000001 (topics: [input-topic])\n" + "      --> KTABLE-SOURCE-0000000002\n" + "    Processor: KTABLE-SOURCE-0000000002 (stores: [])\n" + "      --> KTABLE-MAPVALUES-0000000004\n" + "      <-- KSTREAM-SOURCE-0000000001\n" + // but we added a change not to materialize non-queriable stores. This change shouldn't break compatibility.    "    Processor: KTABLE-MAPVALUES-0000000004 (stores: [])\n" + "      --> none\n" + "      <-- KTABLE-SOURCE-0000000002\n" + "\n", describe.toString());}
f19247
0
kTableNamedMaterializedMapValuesShouldPreserveTopologyStructure
public void kafkatest_f19248_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<Object, Object> table = builder.table("input-topic");    table.mapValues((readOnlyKey, value) -> null, Materialized.<Object, Object, KeyValueStore<Bytes, byte[]>>as("store-name").withKeySerde(null).withValueSerde(null));    final TopologyDescription describe = builder.build().describe();    Assert.assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000001 (topics: [input-topic])\n" + "      --> KTABLE-SOURCE-0000000002\n" + "    Processor: KTABLE-SOURCE-0000000002 (stores: [])\n" + "      --> KTABLE-MAPVALUES-0000000003\n" + "      <-- KSTREAM-SOURCE-0000000001\n" + "    Processor: KTABLE-MAPVALUES-0000000003 (stores: [store-name])\n" + "      --> none\n" + "      <-- KTABLE-SOURCE-0000000002\n" + "\n", describe.toString());}
f19248
0
addProcessorWithExistingStore
private TopologyDescription.Processor kafkatest_f19256_0(final String processorName, final String[] storeNames, final TopologyDescription.Node... parents)
{    return addProcessorWithStore(processorName, storeNames, false, parents);}
f19256
0
addProcessorWithStore
private TopologyDescription.Processor kafkatest_f19257_0(final String processorName, final String[] storeNames, final boolean newStores, final TopologyDescription.Node... parents)
{    final String[] parentNames = new String[parents.length];    for (int i = 0; i < parents.length; ++i) {        parentNames[i] = parents[i].name();    }    topology.addProcessor(processorName, new MockProcessorSupplier(), parentNames);    if (newStores) {        for (final String store : storeNames) {            final StoreBuilder storeBuilder = EasyMock.createNiceMock(StoreBuilder.class);            EasyMock.expect(storeBuilder.name()).andReturn(store).anyTimes();            EasyMock.replay(storeBuilder);            topology.addStateStore(storeBuilder, processorName);        }    } else {        topology.connectProcessorAndStateStores(processorName, storeNames);    }    final TopologyDescription.Processor expectedProcessorNode = new InternalTopologyBuilder.Processor(processorName, new HashSet<>(Arrays.asList(storeNames)));    for (final TopologyDescription.Node parent : parents) {        ((InternalTopologyBuilder.AbstractNode) parent).addSuccessor(expectedProcessorNode);        ((InternalTopologyBuilder.AbstractNode) expectedProcessorNode).addPredecessor(parent);    }    return expectedProcessorNode;}
f19257
0
addSink
private TopologyDescription.Sink kafkatest_f19258_0(final String sinkName, final String sinkTopic, final TopologyDescription.Node... parents)
{    final String[] parentNames = new String[parents.length];    for (int i = 0; i < parents.length; ++i) {        parentNames[i] = parents[i].name();    }    topology.addSink(sinkName, sinkTopic, null, null, null, parentNames);    final TopologyDescription.Sink expectedSinkNode = new InternalTopologyBuilder.Sink(sinkName, sinkTopic);    for (final TopologyDescription.Node parent : parents) {        ((InternalTopologyBuilder.AbstractNode) parent).addSuccessor(expectedSinkNode);        ((InternalTopologyBuilder.AbstractNode) expectedSinkNode).addPredecessor(parent);    }    return expectedSinkNode;}
f19258
0
init
public void kafkatest_f19266_0(final ProcessorContext context, final StateStore root)
{    if (root != null) {        context.register(root, null);    }    this.open = true;}
f19266
0
setFlushListener
public boolean kafkatest_f19267_0(final CacheFlushListener<K, V> listener, final boolean sendOldValues)
{    return false;}
f19267
0
persistent
public boolean kafkatest_f19268_0()
{    return false;}
f19268
0
all
public synchronized KeyValueIterator<K, V> kafkatest_f19276_0()
{    final TreeMap<K, V> copy = new TreeMap<>(this.map);    return new DelegatingPeekingKeyValueIterator<>(name, new GenericInMemoryKeyValueIterator<>(copy.entrySet().iterator()));}
f19276
0
approximateNumEntries
public long kafkatest_f19277_0()
{    return this.map.size();}
f19277
0
flush
public void kafkatest_f19278_0()
{// do-nothing since it is in-memory}
f19278
0
setFlushListener
public boolean kafkatest_f19286_0(final CacheFlushListener<K, ValueAndTimestamp<V>> listener, final boolean sendOldValues)
{    return false;}
f19286
0
persistent
public boolean kafkatest_f19287_0()
{    return false;}
f19287
0
isOpen
public boolean kafkatest_f19288_0()
{    return this.open;}
f19288
0
approximateNumEntries
public long kafkatest_f19296_0()
{    return this.map.size();}
f19296
0
flush
public void kafkatest_f19297_0()
{// do-nothing since it is in-memory}
f19297
0
close
public void kafkatest_f19298_0()
{    this.map.clear();    this.open = false;}
f19298
0
checkpoint
public void kafkatest_f19310_0(final Map<TopicPartition, Long> offsets)
{    this.offsets.putAll(offsets);}
f19310
0
getGlobalStore
public StateStore kafkatest_f19311_0(final String name)
{    return null;}
f19311
0
getStore
public StateStore kafkatest_f19312_0(final String name)
{    return null;}
f19312
0
register
public void kafkatest_f19321_0(final StateStore store, final StateRestoreCallback func)
{    storeMap.put(store.name(), store);    restoreFuncs.put(store.name(), func);}
f19321
0
getStateStore
public StateStore kafkatest_f19322_0(final String name)
{    return storeMap.get(name);}
f19322
0
schedule
public Cancellable kafkatest_f19323_0(final long interval, final PunctuationType type, final Punctuator callback)
{    throw new UnsupportedOperationException("schedule() not supported.");}
f19323
0
topic
public String kafkatest_f19332_0()
{    if (recordContext == null) {        return null;    }    return recordContext.topic();}
f19332
0
partition
public int kafkatest_f19333_0()
{    if (recordContext == null) {        return -1;    }    return recordContext.partition();}
f19333
0
offset
public long kafkatest_f19334_0()
{    if (recordContext == null) {        return -1L;    }    return recordContext.offset();}
f19334
0
hasNext
public boolean kafkatest_f19342_0()
{    return iterator.hasNext();}
f19342
0
next
public KeyValue<K, V> kafkatest_f19343_0()
{    return iterator.next();}
f19343
0
toStringInstance
public static Aggregator<K, V, String> kafkatest_f19344_0(final String sep)
{    return (aggKey, value, aggregate) -> aggregate + sep + value;}
f19344
0
getProducer
public Producer<byte[], byte[]> kafkatest_f19352_0(final Map<String, Object> config)
{    if (applicationId != null) {        assertThat((String) config.get(ProducerConfig.TRANSACTIONAL_ID_CONFIG), startsWith(applicationId + "-"));    } else {        assertFalse(config.containsKey(ProducerConfig.TRANSACTIONAL_ID_CONFIG));    }    final MockProducer<byte[], byte[]> producer = new MockProducer<>(true, BYTE_ARRAY_SERIALIZER, BYTE_ARRAY_SERIALIZER);    producers.add(producer);    return producer;}
f19352
0
getConsumer
public Consumer<byte[], byte[]> kafkatest_f19353_0(final Map<String, Object> config)
{    return consumer;}
f19353
0
getRestoreConsumer
public Consumer<byte[], byte[]> kafkatest_f19354_0(final Map<String, Object> config)
{    return restoreConsumer;}
f19354
0
recordContext
public ProcessorRecordContext kafkatest_f19365_0()
{    return new ProcessorRecordContext(timestamp(), offset(), partition(), topic(), headers());}
f19365
0
setRecordContext
public void kafkatest_f19366_0(final ProcessorRecordContext recordContext)
{    setRecordMetadata(recordContext.topic(), recordContext.partition(), recordContext.offset(), recordContext.headers(), recordContext.timestamp());}
f19366
0
setCurrentNode
public void kafkatest_f19367_0(final ProcessorNode currentNode)
{    this.currentNode = currentNode;}
f19367
0
getNumPartitions
protected Map<String, Integer> kafkatest_f19377_0(final Set<String> topics)
{    final Map<String, Integer> partitions = new HashMap<>();    for (final String topic : topics) {        partitions.put(topic, restoreConsumer.partitionsFor(topic) == null ? null : restoreConsumer.partitionsFor(topic).size());    }    return partitions;}
f19377
0
name
public String kafkatest_f19378_0()
{    return name;}
f19378
0
init
public void kafkatest_f19379_0(final ProcessorContext context, final StateStore root)
{    context.register(root, stateRestoreCallback);    initialized = true;    closed = false;}
f19379
0
delete
public Object kafkatest_f19388_0(final Object key)
{    return null;}
f19388
0
get
public Object kafkatest_f19390_0(final Object key)
{    return null;}
f19390
0
range
public KeyValueIterator kafkatest_f19391_0(final Object from, final Object to)
{    return null;}
f19391
0
apply
public K kafkatest_f19399_0(final K key, final V value)
{    return key;}
f19399
0
apply
public V kafkatest_f19400_0(final V value)
{    return value;}
f19400
0
selectKeyKeyValueMapper
public static KeyValueMapper<K, V, K> kafkatest_f19401_0()
{    return new SelectKeyMapper<>();}
f19401
0
init
public void kafkatest_f19409_0(final ProcessorContext context)
{    super.init(context);    if (scheduleInterval > 0L) {        scheduleCancellable = context.schedule(Duration.ofMillis(scheduleInterval), punctuationType, timestamp -> {            if (punctuationType == PunctuationType.STREAM_TIME) {                assertEquals(timestamp, context().timestamp());            }            assertEquals(-1, context().partition());            assertEquals(-1L, context().offset());            (punctuationType == PunctuationType.STREAM_TIME ? punctuatedStreamTime : punctuatedSystemTime).add(timestamp);        });    }}
f19409
0
process
public void kafkatest_f19410_0(final K key, final V value)
{    KeyValueTimestamp<Object, Object> keyValueTimestamp = new KeyValueTimestamp<>(key, value, context().timestamp());    if (value != null) {        lastValueAndTimestampPerKey.put(key, ValueAndTimestamp.make(value, context().timestamp()));    } else {        lastValueAndTimestampPerKey.remove(key);    }    processed.add(keyValueTimestamp);    if (commitRequested) {        context().commit();        commitRequested = false;    }}
f19410
0
checkAndClearProcessResult
public void kafkatest_f19411_0(final KeyValueTimestamp... expected)
{    assertEquals("the number of outputs:" + processed, expected.length, processed.size());    for (int i = 0; i < expected.length; i++) {        assertEquals("output[" + i + "]:", expected[i], processed.get(i));    }    processed.clear();}
f19411
0
theCapturedProcessor
public MockProcessor<K, V> kafkatest_f19419_0()
{    return capturedProcessors(1).get(0);}
f19419
0
capturedProcessorsCount
public int kafkatest_f19420_0()
{    return processors.size();}
f19420
0
capturedProcessors
public List<MockProcessor<K, V>> kafkatest_f19421_0(final int expectedNumberOfProcessors)
{    assertEquals(expectedNumberOfProcessors, processors.size());    return processors;}
f19421
0
assign
public synchronized void kafkatest_f19429_0(final Collection<TopicPartition> partitions)
{    final int numPartitions = partitions.size();    if (numPartitions > 1)        throw new IllegalArgumentException("RestoreConsumer: more than one partition specified");    if (numPartitions == 1) {        if (assignedPartition != null)            throw new IllegalStateException("RestoreConsumer: partition already assigned");        assignedPartition = partitions.iterator().next();        // set the beginning offset to 0        // NOTE: this is users responsible to set the initial lEO.        super.updateBeginningOffsets(Collections.singletonMap(assignedPartition, 0L));    }    super.assign(partitions);}
f19429
0
poll
public ConsumerRecords<byte[], byte[]> kafkatest_f19430_0(final Duration timeout)
{    // add buffered records to MockConsumer    for (final ConsumerRecord<byte[], byte[]> record : recordBuffer) {        super.addRecord(record);    }    recordBuffer.clear();    final ConsumerRecords<byte[], byte[]> records = super.poll(timeout);    // set the current offset    final Iterable<ConsumerRecord<byte[], byte[]>> partitionRecords = records.records(assignedPartition);    for (final ConsumerRecord<byte[], byte[]> record : partitionRecords) {        currentOffset = record.offset();    }    return records;}
f19430
0
position
public synchronized long kafkatest_f19431_0(final TopicPartition partition)
{    if (!partition.equals(assignedPartition))        throw new IllegalStateException("RestoreConsumer: unassigned partition");    return currentOffset;}
f19431
0
onRestoreStart
public void kafkatest_f19439_0(final TopicPartition topicPartition, final String storeName, final long startingOffset, final long endingOffset)
{    restoreTopicPartition = topicPartition;    storeNameCalledStates.put(RESTORE_START, storeName);    restoreStartOffset = startingOffset;    restoreEndOffset = endingOffset;}
f19439
0
onBatchRestored
public void kafkatest_f19440_0(final TopicPartition topicPartition, final String storeName, final long batchEndOffset, final long numRestored)
{    restoreTopicPartition = topicPartition;    storeNameCalledStates.put(RESTORE_BATCH, storeName);    restoredBatchOffset = batchEndOffset;    numBatchRestored = numRestored;}
f19440
0
onRestoreEnd
public void kafkatest_f19441_0(final TopicPartition topicPartition, final String storeName, final long totalRestored)
{    restoreTopicPartition = topicPartition;    storeNameCalledStates.put(RESTORE_END, storeName);    totalNumRestored = totalRestored;}
f19441
0
schedule
public Cancellable kafkatest_f19449_0(final Duration interval, final PunctuationType type, final Punctuator callback) throws IllegalArgumentException
{    return null;}
f19449
0
forward
public void kafkatest_f19450_0(final K key, final V value)
{    forwardedValues.put(key, value);}
f19450
0
forward
public void kafkatest_f19451_0(final K key, final V value, final To to)
{    forwardedValues.put(key, value);}
f19451
0
name
public String kafkatest_f19461_0()
{    return name;}
f19461
0
init
public void kafkatest_f19462_0(final ProcessorContext context, final StateStore root)
{    if (rocksdbStore) {        // cf. RocksDBStore        new File(context.stateDir() + File.separator + "rocksdb" + File.separator + name).mkdirs();    } else {        new File(context.stateDir() + File.separator + name).mkdir();    }    this.initialized = true;}
f19462
0
flush
public void kafkatest_f19463_0()
{    flushed = true;}
f19463
0
hasNext
public boolean kafkatest_f19476_0()
{    while (it == null || !it.hasNext()) {        if (!keysIterator.hasNext()) {            return false;        }        it = keysIterator.next().iterator();    }    return true;}
f19476
0
next
public KeyValue<Windowed<K>, V> kafkatest_f19477_0()
{    return it.next();}
f19477
0
name
public String kafkatest_f19478_0()
{    return "";}
f19478
0
all
public KeyValueIterator<Bytes, byte[]> kafkatest_f19489_0()
{    fetchCalled = true;    return new KeyValueIteratorStub<>(Collections.<KeyValue<Bytes, byte[]>>emptyIterator());}
f19489
0
fetchAll
public KeyValueIterator<Bytes, byte[]> kafkatest_f19490_0(final long timeFrom, final long timeTo)
{    fetchCalled = true;    return new KeyValueIteratorStub<>(Collections.<KeyValue<Bytes, byte[]>>emptyIterator());}
f19490
0
remove
public void kafkatest_f19491_0(final Bytes key)
{    store.put(key, null);    removeCalled = true;}
f19491
0
transform
public V kafkatest_f19499_0(final K readOnlyKey, final V value)
{    return value;}
f19499
0
get
public ValueTransformerWithKey<K, V, V> kafkatest_f19501_0()
{    return transformer;}
f19501
0
stores
public List<T> kafkatest_f19502_0(final String storeName, final QueryableStoreType<T> queryableStoreType)
{    if (throwException) {        throw new InvalidStateStoreException("store is unavailable");    }    if (stores.containsKey(storeName) && queryableStoreType.accepts(stores.get(storeName))) {        return (List<T>) Collections.singletonList(stores.get(storeName));    }    return Collections.emptyList();}
f19502
0
verifyKeyValueList
public static void kafkatest_f19510_0(final List<KeyValue<K, byte[]>> expected, final List<KeyValue<K, byte[]>> actual)
{    assertThat(actual.size(), equalTo(expected.size()));    for (int i = 0; i < actual.size(); i++) {        final KeyValue<K, byte[]> expectedKv = expected.get(i);        final KeyValue<K, byte[]> actualKv = actual.get(i);        assertThat(actualKv.key, equalTo(expectedKv.key));        assertThat(actualKv.value, equalTo(expectedKv.value));    }}
f19510
0
verifyWindowedKeyValue
public static void kafkatest_f19511_0(final KeyValue<Windowed<Bytes>, byte[]> actual, final Windowed<Bytes> expectedKey, final String expectedValue)
{    assertThat(actual.key.window(), equalTo(expectedKey.window()));    assertThat(actual.key.key(), equalTo(expectedKey.key()));    assertThat(actual.value, equalTo(expectedValue.getBytes()));}
f19511
0
getMetricByName
public static Metric kafkatest_f19512_0(final Map<MetricName, ? extends Metric> metrics, final String name, final String group)
{    Metric metric = null;    for (final Map.Entry<MetricName, ? extends Metric> entry : metrics.entrySet()) {        if (entry.getKey().name().equals(name) && entry.getKey().group().equals(group)) {            if (metric == null) {                metric = entry.getValue();            } else {                throw new IllegalStateException("Found two metrics with name=[" + name + "]: \n" + metric.metricName().toString() + " AND \n" + entry.getKey().toString());            }        }    }    if (metric == null) {        throw new IllegalStateException("Didn't find metric with name=[" + name + "]");    } else {        return metric;    }}
f19512
0
close
public void kafkatest_f19520_0()
{    inner.close();}
f19520
0
name
public String kafkatest_f19521_0()
{    return inner.name();}
f19521
0
persistent
public boolean kafkatest_f19522_0()
{    return inner.persistent();}
f19522
0
persistent
public boolean kafkatest_f19530_0()
{    return inner.persistent();}
f19530
0
isOpen
public boolean kafkatest_f19531_0()
{    return inner.isOpen();}
f19531
0
getIntervalMs
public long kafkatest_f19532_0()
{    return intervalMs;}
f19532
0
applicationId
public String kafkatest_f19540_0()
{    return config.getString(StreamsConfig.APPLICATION_ID_CONFIG);}
f19540
0
taskId
public TaskId kafkatest_f19541_0()
{    return taskId;}
f19541
0
appConfigs
public Map<String, Object> kafkatest_f19542_0()
{    final Map<String, Object> combined = new HashMap<>();    combined.putAll(config.originals());    combined.putAll(config.values());    return combined;}
f19542
0
setPartition
public void kafkatest_f19550_0(final int partition)
{    this.partition = partition;}
f19550
0
setOffset
public void kafkatest_f19551_0(final long offset)
{    this.offset = offset;}
f19551
0
setHeaders
public void kafkatest_f19552_0(final Headers headers)
{    this.headers = headers;}
f19552
0
getStateStore
public StateStore kafkatest_f19560_0(final String name)
{    return stateStores.get(name);}
f19560
0
schedule
public Cancellable kafkatest_f19561_0(final long intervalMs, final PunctuationType type, final Punctuator callback)
{    final CapturedPunctuator capturedPunctuator = new CapturedPunctuator(intervalMs, type, callback);    punctuators.add(capturedPunctuator);    return capturedPunctuator::cancel;}
f19561
0
schedule
public Cancellable kafkatest_f19562_0(final Duration interval, final PunctuationType type, final Punctuator callback) throws IllegalArgumentException
{    return schedule(ApiUtils.validateMillisecondDuration(interval, "interval"), type, callback);}
f19562
0
resetForwards
public void kafkatest_f19570_0()
{    capturedForwards.clear();}
f19570
0
commit
public void kafkatest_f19571_0()
{    committed = true;}
f19571
0
committed
public boolean kafkatest_f19572_0()
{    return committed;}
f19572
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19580_0(final String topicName, final K key, final V value)
{    final long timestamp = timeMs;    timeMs += advanceMs;    return create(topicName, key, value, new RecordHeaders(), timestamp);}
f19580
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19581_0(final String topicName, final K key, final V value, final Headers headers)
{    final long timestamp = timeMs;    timeMs += advanceMs;    return create(topicName, key, value, headers, timestamp);}
f19581
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19582_0(final K key, final V value)
{    return create(key, value, new RecordHeaders());}
f19582
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19590_0(final V value)
{    return create(value, new RecordHeaders());}
f19590
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19591_0(final V value, final Headers headers)
{    if (topicName == null) {        throw new IllegalStateException("ConsumerRecordFactory was created without defaultTopicName. " + "Use #create(String topicName, V value, long timestampMs) instead.");    }    return create(topicName, value, headers);}
f19591
0
create
public List<ConsumerRecord<byte[], byte[]>> kafkatest_f19592_0(final String topicName, final List<KeyValue<K, V>> keyValues)
{    final List<ConsumerRecord<byte[], byte[]>> records = new ArrayList<>(keyValues.size());    for (final KeyValue<K, V> keyValue : keyValues) {        records.add(create(topicName, keyValue.key, keyValue.value));    }    return records;}
f19592
0
compareKeyValue
public static void kafkatest_f19600_0(final ProducerRecord<K, V> record, final K expectedKey, final V expectedValue) throws AssertionError
{    Objects.requireNonNull(record);    final K recordKey = record.key();    final V recordValue = record.value();    final AssertionError error = new AssertionError("Expected <" + expectedKey + ", " + expectedValue + "> " + "but was <" + recordKey + ", " + recordValue + ">");    if (recordKey != null) {        if (!recordKey.equals(expectedKey)) {            throw error;        }    } else if (expectedKey != null) {        throw error;    }    if (recordValue != null) {        if (!recordValue.equals(expectedValue)) {            throw error;        }    } else if (expectedValue != null) {        throw error;    }}
f19600
0
compareKeyValue
public static void kafkatest_f19601_0(final ProducerRecord<K, V> record, final ProducerRecord<K, V> expectedRecord) throws AssertionError
{    Objects.requireNonNull(expectedRecord);    compareKeyValue(record, expectedRecord.key(), expectedRecord.value());}
f19601
0
compareValueTimestamp
public static void kafkatest_f19602_0(final ProducerRecord<K, V> record, final V expectedValue, final long expectedTimestamp) throws AssertionError
{    Objects.requireNonNull(record);    final V recordValue = record.value();    final long recordTimestamp = record.timestamp();    final AssertionError error = new AssertionError("Expected value=" + expectedValue + " with timestamp=" + expectedTimestamp + " but was value=" + recordValue + " with timestamp=" + recordTimestamp);    if (recordValue != null) {        if (!recordValue.equals(expectedValue)) {            throw error;        }    } else if (expectedValue != null) {        throw error;    }    if (recordTimestamp != expectedTimestamp) {        throw error;    }}
f19602
0
compareKeyValueHeadersTimestamp
public static void kafkatest_f19610_0(final ProducerRecord<K, V> record, final K expectedKey, final V expectedValue, final Headers expectedHeaders, final long expectedTimestamp) throws AssertionError
{    Objects.requireNonNull(record);    final K recordKey = record.key();    final V recordValue = record.value();    final Headers recordHeaders = record.headers();    final long recordTimestamp = record.timestamp();    final AssertionError error = new AssertionError("Expected <" + expectedKey + ", " + expectedValue + ">" + " with timestamp=" + expectedTimestamp + " and headers=" + expectedHeaders + " but was <" + recordKey + ", " + recordValue + ">" + " with timestamp=" + recordTimestamp + " and headers=" + recordHeaders);    if (recordKey != null) {        if (!recordKey.equals(expectedKey)) {            throw error;        }    } else if (expectedKey != null) {        throw error;    }    if (recordValue != null) {        if (!recordValue.equals(expectedValue)) {            throw error;        }    } else if (expectedValue != null) {        throw error;    }    if (recordHeaders != null) {        if (!recordHeaders.equals(expectedHeaders)) {            throw error;        }    } else if (expectedHeaders != null) {        throw error;    }    if (recordTimestamp != expectedTimestamp) {        throw error;    }}
f19610
0
compareKeyValueHeadersTimestamp
public static void kafkatest_f19611_0(final ProducerRecord<K, V> record, final ProducerRecord<K, V> expectedRecord) throws AssertionError
{    Objects.requireNonNull(expectedRecord);    compareKeyValueHeadersTimestamp(record, expectedRecord.key(), expectedRecord.value(), expectedRecord.headers(), expectedRecord.timestamp());}
f19611
0
partitionsFor
public List<PartitionInfo> kafkatest_f19612_0(final String topic)
{    return Collections.singletonList(new PartitionInfo(topic, PARTITION_ID, null, null, null));}
f19612
0
readOutput
public ProducerRecord<byte[], byte[]> kafkatest_f19623_0(final String topic)
{    final Queue<ProducerRecord<byte[], byte[]>> outputRecords = outputRecordsByTopic.get(topic);    if (outputRecords == null) {        return null;    }    return outputRecords.poll();}
f19623
0
readOutput
public ProducerRecord<K, V> kafkatest_f19624_0(final String topic, final Deserializer<K> keyDeserializer, final Deserializer<V> valueDeserializer)
{    final ProducerRecord<byte[], byte[]> record = readOutput(topic);    if (record == null) {        return null;    }    final K key = keyDeserializer.deserialize(record.topic(), record.key());    final V value = valueDeserializer.deserialize(record.topic(), record.value());    return new ProducerRecord<>(record.topic(), record.partition(), record.timestamp(), key, value, record.headers());}
f19624
0
getAllStateStores
public Map<String, StateStore> kafkatest_f19625_0()
{    final Map<String, StateStore> allStores = new HashMap<>();    for (final String storeName : internalTopologyBuilder.allStateStoreName()) {        allStores.put(storeName, getStateStore(storeName, false));    }    return allStores;}
f19625
0
getSessionStore
public SessionStore<K, V> kafkatest_f19633_0(final String name)
{    final StateStore store = getStateStore(name, false);    return store instanceof SessionStore ? (SessionStore<K, V>) store : null;}
f19633
0
close
public void kafkatest_f19634_0()
{    if (task != null) {        task.close(true, false);    }    if (globalStateTask != null) {        try {            globalStateTask.close();        } catch (final IOException e) {        // ignore        }    }    captureOutputRecords();    if (!eosEnabled) {        producer.close();    }    stateDirectory.clean();}
f19634
0
milliseconds
public long kafkatest_f19635_0()
{    return timeMs.get();}
f19635
0
shouldForwardInit
public void kafkatest_f19645_0()
{    final ProcessorContext context = mock(ProcessorContext.class);    final StateStore store = mock(StateStore.class);    mockedKeyValueTimestampStore.init(context, store);    expectLastCall();    replay(mockedKeyValueTimestampStore);    keyValueStoreFacade.init(context, store);    verify(mockedKeyValueTimestampStore);}
f19645
0
shouldPutWithUnknownTimestamp
public void kafkatest_f19646_0()
{    mockedKeyValueTimestampStore.put("key", ValueAndTimestamp.make("value", ConsumerRecord.NO_TIMESTAMP));    expectLastCall();    replay(mockedKeyValueTimestampStore);    keyValueStoreFacade.put("key", "value");    verify(mockedKeyValueTimestampStore);}
f19646
0
shouldPutIfAbsentWithUnknownTimestamp
public void kafkatest_f19647_0()
{    expect(mockedKeyValueTimestampStore.putIfAbsent("key", ValueAndTimestamp.make("value", ConsumerRecord.NO_TIMESTAMP))).andReturn(null).andReturn(ValueAndTimestamp.make("oldValue", 42L));    replay(mockedKeyValueTimestampStore);    assertNull(keyValueStoreFacade.putIfAbsent("key", "value"));    assertThat(keyValueStoreFacade.putIfAbsent("key", "value"), is("oldValue"));    verify(mockedKeyValueTimestampStore);}
f19647
0
setup
public void kafkatest_f19655_0()
{    windowStoreFacade = new WindowStoreFacade<>(mockedWindowTimestampStore);}
f19655
0
shouldForwardInit
public void kafkatest_f19656_0()
{    final ProcessorContext context = mock(ProcessorContext.class);    final StateStore store = mock(StateStore.class);    mockedWindowTimestampStore.init(context, store);    expectLastCall();    replay(mockedWindowTimestampStore);    windowStoreFacade.init(context, store);    verify(mockedWindowTimestampStore);}
f19656
0
shouldPutWithUnknownTimestamp
public void kafkatest_f19657_0()
{    mockedWindowTimestampStore.put("key", ValueAndTimestamp.make("value", ConsumerRecord.NO_TIMESTAMP));    expectLastCall();    replay(mockedWindowTimestampStore);    windowStoreFacade.put("key", "value");    verify(mockedWindowTimestampStore);}
f19657
0
process
public void kafkatest_f19665_0(final String key, final Long value)
{    context().forward(key + value, key.length() + value);}
f19665
0
shouldCaptureOutputRecordsUsingTo
public void kafkatest_f19666_0()
{    final AbstractProcessor<String, Long> processor = new AbstractProcessor<String, Long>() {        @Override        public void process(final String key, final Long value) {            context().forward(key + value, key.length() + value, To.all());        }    };    final MockProcessorContext context = new MockProcessorContext();    processor.init(context);    processor.process("foo", 5L);    processor.process("barbaz", 50L);    final Iterator<CapturedForward> forwarded = context.forwarded().iterator();    assertEquals(new KeyValue<>("foo5", 8L), forwarded.next().keyValue());    assertEquals(new KeyValue<>("barbaz50", 56L), forwarded.next().keyValue());    assertFalse(forwarded.hasNext());    context.resetForwards();    assertEquals(0, context.forwarded().size());}
f19666
0
process
public void kafkatest_f19667_0(final String key, final Long value)
{    context().forward(key + value, key.length() + value, To.all());}
f19667
0
process
public void kafkatest_f19675_0(final String key, final Long value)
{    if (++count > 2) {        context().commit();    }}
f19675
0
shouldStoreAndReturnStateStores
public void kafkatest_f19676_0()
{    final AbstractProcessor<String, Long> processor = new AbstractProcessor<String, Long>() {        @Override        public void process(final String key, final Long value) {            @SuppressWarnings("unchecked")            final KeyValueStore<String, Long> stateStore = (KeyValueStore<String, Long>) context().getStateStore("my-state");            stateStore.put(key, (stateStore.get(key) == null ? 0 : stateStore.get(key)) + value);            stateStore.put("all", (stateStore.get("all") == null ? 0 : stateStore.get("all")) + value);        }    };    final MockProcessorContext context = new MockProcessorContext();    final StoreBuilder storeBuilder = Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore("my-state"), Serdes.String(), Serdes.Long()).withLoggingDisabled();    final KeyValueStore<String, Long> store = (KeyValueStore<String, Long>) storeBuilder.build();    store.init(context, store);    processor.init(context);    processor.process("foo", 5L);    processor.process("bar", 50L);    assertEquals(5L, (long) store.get("foo"));    assertEquals(50L, (long) store.get("bar"));    assertEquals(55L, (long) store.get("all"));}
f19676
0
process
public void kafkatest_f19677_0(final String key, final Long value)
{    @SuppressWarnings("unchecked")    final KeyValueStore<String, Long> stateStore = (KeyValueStore<String, Long>) context().getStateStore("my-state");    stateStore.put(key, (stateStore.get(key) == null ? 0 : stateStore.get(key)) + value);    stateStore.put("all", (stateStore.get("all") == null ? 0 : stateStore.get("all")) + value);}
f19677
0
shouldNotAllowNegativeSleep
public void kafkatest_f19687_0()
{    new TopologyTestDriver.MockTime(42).sleep(-1L);}
f19687
0
shouldAdvanceTimeOnSleep
public void kafkatest_f19688_0()
{    final TopologyTestDriver.MockTime time = new TopologyTestDriver.MockTime(42L);    assertEquals(42L, time.milliseconds());    time.sleep(1L);    assertEquals(43L, time.milliseconds());    time.sleep(0L);    assertEquals(43L, time.milliseconds());    time.sleep(3L);    assertEquals(46L, time.milliseconds());}
f19688
0
shouldAdvanceTime
public void kafkatest_f19689_0()
{    factory.advanceTimeMs(3L);    verifyRecord(topicName, rawKey, rawValue, 3L, factory.create(topicName, rawKey, value));    factory.advanceTimeMs(2L);    verifyRecord(topicName, rawKey, rawValue, 5L, factory.create(topicName, rawKey, value));}
f19689
0
shouldRequireCustomTopicNameIfNotDefaultFactoryTopicName
public void kafkatest_f19697_0()
{    defaultFactory.create(rawKey, value, timestamp);}
f19697
0
shouldRequireCustomTopicNameIfNotDefaultFactoryTopicNameWithDefaultTimestamp
public void kafkatest_f19698_0()
{    defaultFactory.create(rawKey, value);}
f19698
0
shouldRequireCustomTopicNameIfNotDefaultFactoryTopicNameWithNullKey
public void kafkatest_f19699_0()
{    defaultFactory.create(value, timestamp);}
f19699
0
shouldCreateNullKeyConsumerRecordWithOtherTopicNameAndTimestampWithTimetamp
public void kafkatest_f19707_0()
{    verifyRecord(topicName, null, rawValue, timestamp, factory.create(value, timestamp));}
f19707
0
shouldCreateNullKeyConsumerRecordWithTimestampWithTimestamp
public void kafkatest_f19708_0()
{    verifyRecord(topicName, null, rawValue, timestamp, factory.create(value, timestamp));}
f19708
0
shouldCreateNullKeyConsumerRecord
public void kafkatest_f19709_0()
{    verifyRecord(topicName, null, rawValue, 0L, factory.create(value));    factory.advanceTimeMs(3L);    verifyRecord(topicName, null, rawValue, 3L, factory.create(value));}
f19709
0
shouldNotAllowNullProducerRecordForCompareKeyValue
public void kafkatest_f19717_0()
{    OutputVerifier.compareKeyValue(null, key, value);}
f19717
0
shouldNotAllowNullProducerRecordWithExpectedRecordForCompareKeyValue
public void kafkatest_f19718_0()
{    OutputVerifier.compareKeyValue(null, producerRecord);}
f19718
0
shouldNotAllowNullExpectedRecordForCompareKeyValue
public void kafkatest_f19719_0()
{    OutputVerifier.compareKeyValue(producerRecord, null);}
f19719
0
shouldPassIfValueIsEqualWithNullForCompareValue
public void kafkatest_f19727_0()
{    OutputVerifier.compareValue(nullKeyValueRecord, (byte[]) null);}
f19727
0
shouldFailIfValueIsDifferentForCompareValue
public void kafkatest_f19728_0()
{    OutputVerifier.compareValue(producerRecord, key);}
f19728
0
shouldFailIfValueIsDifferentWithNullForCompareValue
public void kafkatest_f19729_0()
{    OutputVerifier.compareValue(producerRecord, (byte[]) null);}
f19729
0
shouldPassIfKeyAndValueIsEqualWithNullForCompareKeyValue
public void kafkatest_f19737_0()
{    OutputVerifier.compareKeyValue(nullKeyValueRecord, null, null);}
f19737
0
shouldFailIfKeyIsDifferentForCompareKeyValue
public void kafkatest_f19738_0()
{    OutputVerifier.compareKeyValue(producerRecord, value, value);}
f19738
0
shouldFailIfKeyIsDifferentWithNullForCompareKeyValue
public void kafkatest_f19739_0()
{    OutputVerifier.compareKeyValue(producerRecord, null, value);}
f19739
0
shouldFailIfKeyIsDifferentWithNullForCompareKeyValueWithProducerRecord
public void kafkatest_f19747_0()
{    OutputVerifier.compareKeyValue(producerRecord, new ProducerRecord<byte[], byte[]>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, null, value));}
f19747
0
shouldFailIfKeyIsDifferentWithNullReversForCompareKeyValueWithProducerRecord
public void kafkatest_f19748_0()
{    OutputVerifier.compareKeyValue(new ProducerRecord<byte[], byte[]>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, null, value), producerRecord);}
f19748
0
shouldFailIfValueIsDifferentForCompareKeyValueWithProducerRecord
public void kafkatest_f19749_0()
{    OutputVerifier.compareKeyValue(producerRecord, new ProducerRecord<>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, key, key));}
f19749
0
shouldPassIfValueAndTimestampIsEqualForCompareValueTimestampWithProducerRecord
public void kafkatest_f19757_0()
{    OutputVerifier.compareValueTimestamp(producerRecord, new ProducerRecord<>("otherTopic", 0, Long.MAX_VALUE, value, value));}
f19757
0
shouldPassIfValueAndTimestampIsEqualWithNullForCompareValueTimestampWithProducerRecord
public void kafkatest_f19758_0()
{    OutputVerifier.compareValueTimestamp(nullKeyValueRecord, new ProducerRecord<byte[], byte[]>("otherTopic", 0, Long.MAX_VALUE, value, null));}
f19758
0
shouldFailIfValueIsDifferentForCompareValueTimestampWithProducerRecord
public void kafkatest_f19759_0()
{    OutputVerifier.compareValueTimestamp(producerRecord, new ProducerRecord<>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, key, key));}
f19759
0
shouldFailIfKeyIsDifferentForCompareKeyValueTimestamp
public void kafkatest_f19767_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, value, value, Long.MAX_VALUE);}
f19767
0
shouldFailIfKeyIsDifferentWithNullForCompareKeyValueTimestamp
public void kafkatest_f19768_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, null, value, Long.MAX_VALUE);}
f19768
0
shouldFailIfKeyIsDifferentWithNullReversForCompareKeyValueTimestamp
public void kafkatest_f19769_0()
{    OutputVerifier.compareKeyValueTimestamp(new ProducerRecord<byte[], byte[]>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, null, value), key, value, Long.MAX_VALUE);}
f19769
0
shouldFailIfKeyIsDifferentWithNullReversForCompareKeyValueTimestampWithProducerRecord
public void kafkatest_f19777_0()
{    OutputVerifier.compareKeyValueTimestamp(new ProducerRecord<byte[], byte[]>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, null, value), producerRecord);}
f19777
0
shouldFailIfValueIsDifferentForCompareKeyValueTimestampWithProducerRecord
public void kafkatest_f19778_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, new ProducerRecord<>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, key, key));}
f19778
0
shouldFailIfValueIsDifferentWithNullForCompareKeyValueTimestampWithProducerRecord
public void kafkatest_f19779_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, new ProducerRecord<byte[], byte[]>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, key, null));}
f19779
0
process
public void kafkatest_f19787_0(final Object key, final Object value)
{    processedRecords.add(new Record(key, value, context.headers(), context.timestamp(), context.offset(), context.topic()));    context.forward(key, value);}
f19787
0
close
public void kafkatest_f19788_0()
{    closed = true;}
f19788
0
get
public Processor kafkatest_f19789_0()
{    final MockProcessor mockProcessor = new MockProcessor(punctuations);    mockProcessors.add(mockProcessor);    return mockProcessor;}
f19789
0
shouldInitProcessor
public void kafkatest_f19797_0()
{    testDriver = new TopologyTestDriver(setupSingleProcessorTopology(), config);    assertTrue(mockProcessors.get(0).initialized);}
f19797
0
shouldCloseProcessor
public void kafkatest_f19798_0()
{    testDriver = new TopologyTestDriver(setupSingleProcessorTopology(), config);    testDriver.close();    assertTrue(mockProcessors.get(0).closed);    // As testDriver is already closed, bypassing @After tearDown testDriver.close().    testDriver = null;}
f19798
0
shouldThrowForUnknownTopic
public void kafkatest_f19799_0()
{    final String unknownTopic = "unknownTopic";    final ConsumerRecordFactory<byte[], byte[]> consumerRecordFactory = new ConsumerRecordFactory<>("unknownTopic", new ByteArraySerializer(), new ByteArraySerializer());    testDriver = new TopologyTestDriver(new Topology(), config);    try {        testDriver.pipeInput(consumerRecordFactory.create((byte[]) null));        fail("Should have throw IllegalArgumentException");    } catch (final IllegalArgumentException exception) {        assertEquals("Unknown topic: " + unknownTopic, exception.getMessage());    }}
f19799
0
shouldPopulateGlobalStore
public void kafkatest_f19807_0()
{    testDriver = new TopologyTestDriver(setupGlobalStoreTopology(SOURCE_TOPIC_1), config);    final KeyValueStore<byte[], byte[]> globalStore = testDriver.getKeyValueStore(SOURCE_TOPIC_1 + "-globalStore");    Assert.assertNotNull(globalStore);    Assert.assertNotNull(testDriver.getAllStateStores().get(SOURCE_TOPIC_1 + "-globalStore"));    testDriver.pipeInput(consumerRecord1);    final List<Record> processedRecords = mockProcessors.get(0).processedRecords;    assertEquals(1, processedRecords.size());    final Record record = processedRecords.get(0);    final Record expectedResult = new Record(consumerRecord1, 0L);    assertThat(record, equalTo(expectedResult));}
f19807
0
shouldPunctuateOnStreamsTime
public void kafkatest_f19808_0()
{    final MockPunctuator mockPunctuator = new MockPunctuator();    testDriver = new TopologyTestDriver(setupSingleProcessorTopology(10L, PunctuationType.STREAM_TIME, mockPunctuator), config);    final List<Long> expectedPunctuations = new LinkedList<>();    expectedPunctuations.add(42L);    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 42L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 42L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    expectedPunctuations.add(51L);    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 51L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 52L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    expectedPunctuations.add(61L);    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 61L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 65L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    expectedPunctuations.add(71L);    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 71L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 72L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    expectedPunctuations.add(95L);    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 95L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    expectedPunctuations.add(101L);    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 101L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    testDriver.pipeInput(consumerRecordFactory.create(SOURCE_TOPIC_1, key1, value1, 102L));    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));}
f19808
0
shouldPunctuateOnWallClockTime
public void kafkatest_f19809_0()
{    final MockPunctuator mockPunctuator = new MockPunctuator();    testDriver = new TopologyTestDriver(setupSingleProcessorTopology(10L, PunctuationType.WALL_CLOCK_TIME, mockPunctuator), config, 0);    final List<Long> expectedPunctuations = new LinkedList<>();    testDriver.advanceWallClockTime(5L);    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    expectedPunctuations.add(14L);    testDriver.advanceWallClockTime(9L);    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    testDriver.advanceWallClockTime(1L);    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    expectedPunctuations.add(35L);    testDriver.advanceWallClockTime(20L);    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));    expectedPunctuations.add(40L);    testDriver.advanceWallClockTime(5L);    assertThat(mockPunctuator.punctuatedAt, equalTo(expectedPunctuations));}
f19809
0
addStoresToTopology
private void kafkatest_f19817_0(final Topology topology, final boolean persistent, final String keyValueStoreName, final String timestampedKeyValueStoreName, final String windowStoreName, final String timestampedWindowStoreName, final String sessionStoreName, final String globalKeyValueStoreName, final String globalTimestampedKeyValueStoreName)
{    // add state stores    topology.addStateStore(Stores.keyValueStoreBuilder(persistent ? Stores.persistentKeyValueStore(keyValueStoreName) : Stores.inMemoryKeyValueStore(keyValueStoreName), Serdes.ByteArray(), Serdes.ByteArray()), "processor");    topology.addStateStore(Stores.timestampedKeyValueStoreBuilder(persistent ? Stores.persistentTimestampedKeyValueStore(timestampedKeyValueStoreName) : Stores.inMemoryKeyValueStore(timestampedKeyValueStoreName), Serdes.ByteArray(), Serdes.ByteArray()), "processor");    topology.addStateStore(Stores.windowStoreBuilder(persistent ? Stores.persistentWindowStore(windowStoreName, Duration.ofMillis(1000L), Duration.ofMillis(100L), false) : Stores.inMemoryWindowStore(windowStoreName, Duration.ofMillis(1000L), Duration.ofMillis(100L), false), Serdes.ByteArray(), Serdes.ByteArray()), "processor");    topology.addStateStore(Stores.timestampedWindowStoreBuilder(persistent ? Stores.persistentTimestampedWindowStore(timestampedWindowStoreName, Duration.ofMillis(1000L), Duration.ofMillis(100L), false) : Stores.inMemoryWindowStore(timestampedWindowStoreName, Duration.ofMillis(1000L), Duration.ofMillis(100L), false), Serdes.ByteArray(), Serdes.ByteArray()), "processor");    topology.addStateStore(persistent ? Stores.sessionStoreBuilder(Stores.persistentSessionStore(sessionStoreName, Duration.ofMillis(1000L)), Serdes.ByteArray(), Serdes.ByteArray()) : Stores.sessionStoreBuilder(Stores.inMemorySessionStore(sessionStoreName, Duration.ofMillis(1000L)), Serdes.ByteArray(), Serdes.ByteArray()), "processor");    // add global stores    topology.addGlobalStore(persistent ? Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(globalKeyValueStoreName), Serdes.ByteArray(), Serdes.ByteArray()).withLoggingDisabled() : Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore(globalKeyValueStoreName), Serdes.ByteArray(), Serdes.ByteArray()).withLoggingDisabled(), "sourceDummy1", Serdes.ByteArray().deserializer(), Serdes.ByteArray().deserializer(), "topicDummy1", "processorDummy1", () -> null);    topology.addGlobalStore(persistent ? Stores.timestampedKeyValueStoreBuilder(Stores.persistentTimestampedKeyValueStore(globalTimestampedKeyValueStoreName), Serdes.ByteArray(), Serdes.ByteArray()).withLoggingDisabled() : Stores.timestampedKeyValueStoreBuilder(Stores.inMemoryKeyValueStore(globalTimestampedKeyValueStoreName), Serdes.ByteArray(), Serdes.ByteArray()).withLoggingDisabled(), "sourceDummy2", Serdes.ByteArray().deserializer(), Serdes.ByteArray().deserializer(), "topicDummy2", "processorDummy2", () -> null);}
f19817
0
shouldReturnAllStoresNames
public void kafkatest_f19818_0()
{    final Topology topology = setupSourceSinkTopology();    topology.addStateStore(new KeyValueStoreBuilder<>(Stores.inMemoryKeyValueStore("store"), Serdes.ByteArray(), Serdes.ByteArray(), new SystemTime()));    topology.addGlobalStore(new KeyValueStoreBuilder<>(Stores.inMemoryKeyValueStore("globalStore"), Serdes.ByteArray(), Serdes.ByteArray(), new SystemTime()).withLoggingDisabled(), "sourceProcessorName", Serdes.ByteArray().deserializer(), Serdes.ByteArray().deserializer(), "globalTopicName", "globalProcessorName", () -> null);    testDriver = new TopologyTestDriver(topology, config);    final Set<String> expectedStoreNames = new HashSet<>();    expectedStoreNames.add("store");    expectedStoreNames.add("globalStore");    assertThat(testDriver.getAllStateStores().keySet(), equalTo(expectedStoreNames));}
f19818
0
setup
private void kafkatest_f19819_0()
{    setup(Stores.inMemoryKeyValueStore("aggStore"));}
f19819
0
get
public Processor<String, Long> kafkatest_f19827_0()
{    return new CustomMaxAggregator();}
f19827
0
init
public void kafkatest_f19828_0(final ProcessorContext context)
{    this.context = context;    context.schedule(Duration.ofMinutes(1), PunctuationType.WALL_CLOCK_TIME, timestamp -> flushStore());    context.schedule(Duration.ofSeconds(10), PunctuationType.STREAM_TIME, timestamp -> flushStore());    store = (KeyValueStore<String, Long>) context.getStateStore("aggStore");}
f19828
0
process
public void kafkatest_f19829_0(final String key, final Long value)
{    final Long oldValue = store.get(key);    if (oldValue == null || value > oldValue) {        store.put(key, value);    }}
f19829
0
setupMultipleSourcesPatternTopology
private Topology kafkatest_f19839_0(final Pattern... sourceTopicPatternNames)
{    final Topology topology = new Topology();    final String[] processorNames = new String[sourceTopicPatternNames.length];    int i = 0;    for (final Pattern sourceTopicPatternName : sourceTopicPatternNames) {        final String sourceName = sourceTopicPatternName + "-source";        final String processorName = sourceTopicPatternName + "-processor";        topology.addSource(sourceName, sourceTopicPatternName);        processorNames[i++] = processorName;        topology.addProcessor(processorName, new MockProcessorSupplier(), sourceName);    }    topology.addSink("sink-topic", SINK_TOPIC_1, processorNames);    return topology;}
f19839
0
shouldProcessFromSourcesThatMatchMultiplePattern
public void kafkatest_f19840_0()
{    final Pattern pattern2Source1 = Pattern.compile("source-topic-\\d");    final Pattern pattern2Source2 = Pattern.compile("source-topic-[A-Z]");    final String consumerTopic2 = "source-topic-Z";    final ConsumerRecord<byte[], byte[]> consumerRecord2 = consumerRecordFactory.create(consumerTopic2, key2, value2, timestamp2);    testDriver = new TopologyTestDriver(setupMultipleSourcesPatternTopology(pattern2Source1, pattern2Source2), config);    final List<Record> processedRecords1 = mockProcessors.get(0).processedRecords;    final List<Record> processedRecords2 = mockProcessors.get(1).processedRecords;    testDriver.pipeInput(consumerRecord1);    assertEquals(1, processedRecords1.size());    assertEquals(0, processedRecords2.size());    final Record record1 = processedRecords1.get(0);    final Record expectedResult1 = new Record(consumerRecord1, 0L);    assertThat(record1, equalTo(expectedResult1));    testDriver.pipeInput(consumerRecord2);    assertEquals(1, processedRecords1.size());    assertEquals(1, processedRecords2.size());    final Record record2 = processedRecords2.get(0);    final Record expectedResult2 = new Record(consumerRecord2, 0L);    assertThat(record2, equalTo(expectedResult2));}
f19840
0
shouldProcessFromSourceThatMatchPattern
public void kafkatest_f19841_0()
{    final String sourceName = "source";    final Pattern pattern2Source1 = Pattern.compile("source-topic-\\d");    final Topology topology = new Topology();    topology.addSource(sourceName, pattern2Source1);    topology.addSink("sink", SINK_TOPIC_1, sourceName);    testDriver = new TopologyTestDriver(topology, config);    testDriver.pipeInput(consumerRecord1);    final ProducerRecord outputRecord = testDriver.readOutput(SINK_TOPIC_1);    assertEquals(key1, outputRecord.key());    assertEquals(value1, outputRecord.value());    assertEquals(SINK_TOPIC_1, outputRecord.topic());}
f19841
0
init
public void kafkatest_f19849_0(final ProcessorContext context)
{    System.out.println("[0.10.0] initializing processor: topic=data taskId=" + context.taskId());    numRecordsProcessed = 0;}
f19849
0
process
public void kafkatest_f19850_0(final K key, final V value)
{    numRecordsProcessed++;    if (numRecordsProcessed % 100 == 0) {        System.out.println("processed " + numRecordsProcessed + " records from topic=data");    }}
f19850
0
main
public static void kafkatest_f19853_0(final String[] args) throws Exception
{    if (args.length < 3) {        System.err.println("StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, properties-file) but only " + args.length + " provided: " + (args.length > 0 ? args[0] + " " : "") + (args.length > 1 ? args[1] : ""));    }    final String kafka = args[0];    final String zookeeper = args[1];    final String propFileName = args.length > 2 ? args[2] : null;    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started (StreamsUpgradeTest v0.10.1)");    System.out.println("kafka=" + kafka);    System.out.println("zookeeper=" + zookeeper);    System.out.println("props=" + streamsProperties);    final KStreamBuilder builder = new KStreamBuilder();    final KStream dataStream = builder.stream("data");    dataStream.process(printProcessorSupplier());    dataStream.to("echo");    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsUpgradeTest");    config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder, config);    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread() {        @Override        public void run() {            System.out.println("closing Kafka Streams instance");            System.out.flush();            streams.close();            System.out.println("UPGRADE-TEST-CLIENT-CLOSED");            System.out.flush();        }    });}
f19853
0
printProcessorSupplier
private static ProcessorSupplier<K, V> kafkatest_f19863_0()
{    return new ProcessorSupplier<K, V>() {        public Processor<K, V> get() {            return new AbstractProcessor<K, V>() {                private int numRecordsProcessed = 0;                @Override                public void init(final ProcessorContext context) {                    System.out.println("[0.10.2] initializing processor: topic=data taskId=" + context.taskId());                    numRecordsProcessed = 0;                }                @Override                public void process(final K key, final V value) {                    numRecordsProcessed++;                    if (numRecordsProcessed % 100 == 0) {                        System.out.println("processed " + numRecordsProcessed + " records from topic=data");                    }                }                @Override                public void punctuate(final long timestamp) {                }                @Override                public void close() {                }            };        }    };}
f19863
0
get
public Processor<K, V> kafkatest_f19864_0()
{    return new AbstractProcessor<K, V>() {        private int numRecordsProcessed = 0;        @Override        public void init(final ProcessorContext context) {            System.out.println("[0.10.2] initializing processor: topic=data taskId=" + context.taskId());            numRecordsProcessed = 0;        }        @Override        public void process(final K key, final V value) {            numRecordsProcessed++;            if (numRecordsProcessed % 100 == 0) {                System.out.println("processed " + numRecordsProcessed + " records from topic=data");            }        }        @Override        public void punctuate(final long timestamp) {        }        @Override        public void close() {        }    };}
f19864
0
init
public void kafkatest_f19865_0(final ProcessorContext context)
{    System.out.println("[0.10.2] initializing processor: topic=data taskId=" + context.taskId());    numRecordsProcessed = 0;}
f19865
0
main
public static void kafkatest_f19877_0(final String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only " + args.length + " provided: " + (args.length > 0 ? args[0] : ""));    }    final String kafka = args[0];    final String propFileName = args.length > 1 ? args[1] : null;    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started (StreamsUpgradeTest v1.0)");    System.out.println("kafka=" + kafka);    System.out.println("props=" + streamsProperties);    final StreamsBuilder builder = new StreamsBuilder();    final KStream dataStream = builder.stream("data");    dataStream.process(printProcessorSupplier());    dataStream.to("echo");    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsUpgradeTest");    config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder.build(), config);    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread() {        @Override        public void run() {            streams.close();            System.out.println("UPGRADE-TEST-CLIENT-CLOSED");            System.out.flush();        }    });}
f19877
0
run
public void kafkatest_f19878_0()
{    streams.close();    System.out.println("UPGRADE-TEST-CLIENT-CLOSED");    System.out.flush();}
f19878
0
printProcessorSupplier
private static ProcessorSupplier<K, V> kafkatest_f19879_0()
{    return new ProcessorSupplier<K, V>() {        public Processor<K, V> get() {            return new AbstractProcessor<K, V>() {                private int numRecordsProcessed = 0;                @Override                public void init(final ProcessorContext context) {                    System.out.println("[1.0] initializing processor: topic=data taskId=" + context.taskId());                    numRecordsProcessed = 0;                }                @Override                public void process(final K key, final V value) {                    numRecordsProcessed++;                    if (numRecordsProcessed % 100 == 0) {                        System.out.println("processed " + numRecordsProcessed + " records from topic=data");                    }                }                @Override                public void punctuate(final long timestamp) {                }                @Override                public void close() {                }            };        }    };}
f19879
0
init
public void kafkatest_f19889_0(final ProcessorContext context)
{    System.out.println("[1.1] initializing processor: topic=data taskId=" + context.taskId());    numRecordsProcessed = 0;}
f19889
0
process
public void kafkatest_f19890_0(final K key, final V value)
{    numRecordsProcessed++;    if (numRecordsProcessed % 100 == 0) {        System.out.println("processed " + numRecordsProcessed + " records from topic=data");    }}
f19890
0
main
public static void kafkatest_f19893_0(final String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only " + args.length + " provided: " + (args.length > 0 ? args[0] : ""));    }    final String kafka = args[0];    final String propFileName = args.length > 1 ? args[1] : null;    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started (StreamsUpgradeTest v2.0)");    System.out.println("kafka=" + kafka);    System.out.println("props=" + streamsProperties);    final StreamsBuilder builder = new StreamsBuilder();    final KStream dataStream = builder.stream("data");    dataStream.process(printProcessorSupplier());    dataStream.to("echo");    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsUpgradeTest");    config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder.build(), config);    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread(() -> {        streams.close();        System.out.println("UPGRADE-TEST-CLIENT-CLOSED");        System.out.flush();    }));}
f19893
0
main
public static void kafkatest_f19903_0(final String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only " + args.length + " provided: " + (args.length > 0 ? args[0] : ""));    }    final String kafka = args[0];    final String propFileName = args.length > 1 ? args[1] : null;    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started (StreamsUpgradeTest v2.2)");    System.out.println("kafka=" + kafka);    System.out.println("props=" + streamsProperties);    final StreamsBuilder builder = new StreamsBuilder();    final KStream dataStream = builder.stream("data");    dataStream.process(printProcessorSupplier());    dataStream.to("echo");    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsUpgradeTest");    config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder.build(), config);    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread(() -> {        streams.close();        System.out.println("UPGRADE-TEST-CLIENT-CLOSED");        System.out.flush();    }));}
f19903
0
printProcessorSupplier
private static ProcessorSupplier<K, V> kafkatest_f19904_0()
{    return () -> new AbstractProcessor<K, V>() {        private int numRecordsProcessed = 0;        @Override        public void init(final ProcessorContext context) {            System.out.println("[2.2] initializing processor: topic=data taskId=" + context.taskId());            numRecordsProcessed = 0;        }        @Override        public void process(final K key, final V value) {            numRecordsProcessed++;            if (numRecordsProcessed % 100 == 0) {                System.out.println("processed " + numRecordsProcessed + " records from topic=data");            }        }        @Override        public void close() {        }    };}
f19904
0
init
public void kafkatest_f19905_0(final ProcessorContext context)
{    System.out.println("[2.2] initializing processor: topic=data taskId=" + context.taskId());    numRecordsProcessed = 0;}
f19905
0
compareArrays
private static void kafkatest_f19915_0(byte[] a, byte[] b)
{    if (!Arrays.equals(a, b)) {        throw new RuntimeException("Arrays did not match: expected " + toHexString(a) + ", got " + toHexString(b));    }}
f19915
0
run
 void kafkatest_f19916_0() throws Throwable
{    long prodTimeMs = Time.SYSTEM.milliseconds();    testAdminClient();    testProduce();    testConsume(prodTimeMs);}
f19916
0
testProduce
public void kafkatest_f19917_0() throws Exception
{    Properties producerProps = new Properties();    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, testConfig.bootstrapServer);    ByteArraySerializer serializer = new ByteArraySerializer();    KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps, serializer, serializer);    ProducerRecord<byte[], byte[]> record1 = new ProducerRecord<>(testConfig.topic, message1);    Future<RecordMetadata> future1 = producer.send(record1);    ProducerRecord<byte[], byte[]> record2 = new ProducerRecord<>(testConfig.topic, message2);    Future<RecordMetadata> future2 = producer.send(record2);    producer.flush();    future1.get();    future2.get();    producer.close();}
f19917
0
fetchNext
private byte[] kafkatest_f19925_0()
{    while (true) {        long curTime = Time.SYSTEM.milliseconds();        if (curTime - prodTimeMs > TIMEOUT_MS)            throw new RuntimeException("Timed out after " + TIMEOUT_MS + " ms.");        if (recordIter == null) {            ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));            recordIter = records.iterator();        }        if (recordIter.hasNext())            return recordIter.next().value();        recordIter = null;    }}
f19925
0
hasNext
public boolean kafkatest_f19926_0()
{    if (next != null)        return true;    next = fetchNext();    return next != null;}
f19926
0
next
public byte[] kafkatest_f19927_0()
{    if (!hasNext())        throw new NoSuchElementException();    byte[] cur = next;    next = null;    return cur;}
f19927
0
printWindow
public void kafkatest_f19935_0()
{    long ellapsed = System.currentTimeMillis() - windowStart;    double recsPerSec = 1000.0 * windowCount / (double) ellapsed;    double mbPerSec = 1000.0 * this.windowBytes / (double) ellapsed / (1024.0 * 1024.0);    System.out.printf("%d records sent, %.1f records/sec (%.2f MB/sec), %.1f ms avg latency, %.1f ms max latency.%n", windowCount, recsPerSec, mbPerSec, windowTotalLatency / (double) windowCount, (double) windowMaxLatency);}
f19935
0
newWindow
public void kafkatest_f19936_0()
{    this.windowStart = System.currentTimeMillis();    this.windowCount = 0;    this.windowMaxLatency = 0;    this.windowTotalLatency = 0;    this.windowBytes = 0;}
f19936
0
printTotal
public void kafkatest_f19937_0()
{    long elapsed = System.currentTimeMillis() - start;    double recsPerSec = 1000.0 * count / (double) elapsed;    double mbPerSec = 1000.0 * this.bytes / (double) elapsed / (1024.0 * 1024.0);    int[] percs = percentiles(this.latencies, index, 0.5, 0.95, 0.99, 0.999);    System.out.printf("%d records sent, %f records/sec (%.2f MB/sec), %.2f ms avg latency, %.2f ms max latency, %d ms 50th, %d ms 95th, %d ms 99th, %d ms 99.9th.%n", count, recsPerSec, mbPerSec, totalLatency / (double) count, (double) maxLatency, percs[0], percs[1], percs[2], percs[3]);}
f19937
0
run
public voidf19945_1)
{    long now = time.milliseconds();    final List<MetricValue> samples;    synchronized (lock) {        samples = new ArrayList<>(metrics.size());        for (KafkaMetric metric : metrics.values()) {            MetricName name = metric.metricName();            samples.add(new MetricValue(name.name(), name.group(), name.tags(), metric.metricValue()));        }    }    MetricsReport report = new MetricsReport(new MetricClientInfo(host, clientId, now), samples);    log.trace("Reporting {} metrics to {}", samples.size(), url);    HttpURLConnection connection = null;    try {        connection = newHttpConnection(url);        connection.setRequestMethod("POST");        // connection.getResponseCode() implicitly calls getInputStream, so always set to true.        // On the other hand, leaving this out breaks nothing.        connection.setDoInput(true);        connection.setRequestProperty("Content-Type", "application/json");        byte[] data = json.writeValueAsBytes(report);        connection.setRequestProperty("Content-Length", Integer.toString(data.length));        connection.setRequestProperty("Accept", "*/*");        connection.setUseCaches(false);        connection.setDoOutput(true);        try (OutputStream os = connection.getOutputStream()) {            os.write(data);            os.flush();        }        int responseCode = connection.getResponseCode();        if (responseCode >= 400) {            InputStream is = connection.getErrorStream();            String msg = readResponse(is);                    } else if (responseCode >= 300) {                    } else {                    }    } catch (Throwable t) {                throw new KafkaException("Failed to report current metrics", t);    } finally {        if (connection != null) {            connection.disconnect();        }    }}
public voidf19945
1
newHttpConnection
 static HttpURLConnection kafkatest_f19946_0(URL url) throws IOException
{    return (HttpURLConnection) url.openConnection();}
f19946
0
readResponse
 static String kafkatest_f19947_0(InputStream is)
{    try (Scanner s = new Scanner(is, StandardCharsets.UTF_8.name()).useDelimiter("\\A")) {        return s.hasNext() ? s.next() : "";    }}
f19947
0
tags
public Map<String, String> kafkatest_f19955_0()
{    return tags;}
f19955
0
value
public Object kafkatest_f19956_0()
{    return value;}
f19956
0
getInteger
public Integer kafkatest_f19957_0(String key)
{    return (Integer) get(key);}
f19957
0
producerRecordFromConsumerRecord
private static ProducerRecord<String, String> kafkatest_f19965_0(String topic, ConsumerRecord<String, String> record)
{    return new ProducerRecord<>(topic, record.key(), record.value());}
f19965
0
consumerPositions
private static Map<TopicPartition, OffsetAndMetadata> kafkatest_f19966_0(KafkaConsumer<String, String> consumer)
{    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();    for (TopicPartition topicPartition : consumer.assignment()) {        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));    }    return positions;}
f19966
0
resetToLastCommittedPositions
private static void kafkatest_f19967_0(KafkaConsumer<String, String> consumer)
{    for (TopicPartition topicPartition : consumer.assignment()) {        OffsetAndMetadata offsetAndMetadata = consumer.committed(topicPartition);        if (offsetAndMetadata != null)            consumer.seek(topicPartition, offsetAndMetadata.offset());        else            consumer.seekToBeginning(singleton(topicPartition));    }}
f19967
0
hasMessageLimit
private boolean kafkatest_f19975_0()
{    return maxMessages >= 0;}
f19975
0
isFinished
private boolean kafkatest_f19976_0()
{    return hasMessageLimit() && consumedMessages >= maxMessages;}
f19976
0
onRecordsReceived
private Map<TopicPartition, OffsetAndMetadata> kafkatest_f19977_0(ConsumerRecords<String, String> records)
{    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    List<RecordSetSummary> summaries = new ArrayList<>();    for (TopicPartition tp : records.partitions()) {        List<ConsumerRecord<String, String>> partitionRecords = records.records(tp);        if (hasMessageLimit() && consumedMessages + partitionRecords.size() > maxMessages)            partitionRecords = partitionRecords.subList(0, maxMessages - consumedMessages);        if (partitionRecords.isEmpty())            continue;        long minOffset = partitionRecords.get(0).offset();        long maxOffset = partitionRecords.get(partitionRecords.size() - 1).offset();        offsets.put(tp, new OffsetAndMetadata(maxOffset + 1));        summaries.add(new RecordSetSummary(tp.topic(), tp.partition(), partitionRecords.size(), minOffset, maxOffset));        if (verbose) {            for (ConsumerRecord<String, String> record : partitionRecords) printJson(new RecordData(record));        }        consumedMessages += partitionRecords.size();        if (isFinished())            break;    }    printJson(new RecordsConsumed(records.count(), summaries));    return offsets;}
f19977
0
timestamp
public long kafkatest_f19985_0()
{    return timestamp;}
f19985
0
name
public String kafkatest_f19986_0()
{    return "startup_complete";}
f19986
0
name
public String kafkatest_f19987_0()
{    return "shutdown_complete";}
f19987
0
name
public String kafkatest_f19995_0()
{    return "record_data";}
f19995
0
topic
public String kafkatest_f19996_0()
{    return record.topic();}
f19996
0
partition
public int kafkatest_f19997_0()
{    return record.partition();}
f19997
0
error
public String kafkatest_f20005_0()
{    return error;}
f20005
0
success
public boolean kafkatest_f20006_0()
{    return success;}
f20006
0
offset
public long kafkatest_f20007_0()
{    return offset;}
f20007
0
loadProps
public static Properties kafkatest_f20015_0(String filename) throws IOException
{    Properties props = new Properties();    try (InputStream propStream = Files.newInputStream(Paths.get(filename))) {        props.load(propStream);    }    return props;}
f20015
0
createFromArgs
public static VerifiableLog4jAppender kafkatest_f20016_0(String[] args)
{    ArgumentParser parser = argParser();    VerifiableLog4jAppender producer = null;    try {        Namespace res = parser.parseArgs(args);        int maxMessages = res.getInt("maxMessages");        String topic = res.getString("topic");        String configFile = res.getString("appender.config");        Properties props = new Properties();        props.setProperty("log4j.rootLogger", "INFO, KAFKA");        props.setProperty("log4j.appender.KAFKA", "org.apache.kafka.log4jappender.KafkaLog4jAppender");        props.setProperty("log4j.appender.KAFKA.layout", "org.apache.log4j.PatternLayout");        props.setProperty("log4j.appender.KAFKA.layout.ConversionPattern", "%-5p: %c - %m%n");        props.setProperty("log4j.appender.KAFKA.BrokerList", res.getString("brokerList"));        props.setProperty("log4j.appender.KAFKA.Topic", topic);        props.setProperty("log4j.appender.KAFKA.RequiredNumAcks", res.getString("acks"));        props.setProperty("log4j.appender.KAFKA.SyncSend", "true");        final String securityProtocol = res.getString("securityProtocol");        if (securityProtocol != null && !securityProtocol.equals(SecurityProtocol.PLAINTEXT.toString())) {            props.setProperty("log4j.appender.KAFKA.SecurityProtocol", securityProtocol);        }        if (securityProtocol != null && securityProtocol.contains("SSL")) {            props.setProperty("log4j.appender.KAFKA.SslTruststoreLocation", res.getString("sslTruststoreLocation"));            props.setProperty("log4j.appender.KAFKA.SslTruststorePassword", res.getString("sslTruststorePassword"));        }        if (securityProtocol != null && securityProtocol.contains("SASL")) {            props.setProperty("log4j.appender.KAFKA.SaslKerberosServiceName", res.getString("saslKerberosServiceName"));            props.setProperty("log4j.appender.KAFKA.clientJaasConfPath", res.getString("clientJaasConfPath"));            props.setProperty("log4j.appender.KAFKA.kerb5ConfPath", res.getString("kerb5ConfPath"));        }        props.setProperty("log4j.logger.kafka.log4j", "INFO, KAFKA");        // Changing log level from INFO to WARN as a temporary workaround for KAFKA-6415. This is to        // avoid deadlock in system tests when producer network thread appends to log while updating metadata.        props.setProperty("log4j.logger.org.apache.kafka.clients.Metadata", "WARN, KAFKA");        if (configFile != null) {            try {                props.putAll(loadProps(configFile));            } catch (IOException e) {                throw new ArgumentParserException(e.getMessage(), parser);            }        }        producer = new VerifiableLog4jAppender(props, maxMessages);    } catch (ArgumentParserException e) {        if (args.length == 0) {            parser.printHelp();            Exit.exit(0);        } else {            parser.handleError(e);            Exit.exit(1);        }    }    return producer;}
f20016
0
main
public static void kafkatest_f20017_0(String[] args) throws IOException
{    final VerifiableLog4jAppender appender = createFromArgs(args);    boolean infinite = appender.maxMessages < 0;    Runtime.getRuntime().addShutdownHook(new Thread(() -> {        // Trigger main thread to stop producing messages        appender.stopLogging = true;    }));    long maxMessages = infinite ? Long.MAX_VALUE : appender.maxMessages;    for (long i = 0; i < maxMessages; i++) {        if (appender.stopLogging) {            break;        }        appender.append(String.format("%d", i));    }}
f20017
0
close
public void kafkatest_f20025_0()
{    producer.close();    printJson(new ShutdownComplete());}
f20025
0
timestamp
public long kafkatest_f20026_0()
{    return timestamp;}
f20026
0
name
public String kafkatest_f20027_0()
{    return "startup_complete";}
f20027
0
name
public String kafkatest_f20035_0()
{    return "producer_send_error";}
f20035
0
key
public String kafkatest_f20036_0()
{    return key;}
f20036
0
value
public String kafkatest_f20037_0()
{    return value;}
f20037
0
avgThroughput
public double kafkatest_f20045_0()
{    return this.avgThroughput;}
f20045
0
printJson
private void kafkatest_f20046_0(Object data)
{    try {        System.out.println(mapper.writeValueAsString(data));    } catch (JsonProcessingException e) {        System.out.println("Bad data can't be written as json: " + e.getMessage());    }}
f20046
0
onCompletion
public void kafkatest_f20047_0(RecordMetadata recordMetadata, Exception e)
{    synchronized (System.out) {        if (e == null) {            VerifiableProducer.this.numAcked++;            printJson(new SuccessfulSend(this.key, this.value, recordMetadata));        } else {            printJson(new FailedSend(this.key, this.value, topic, e));        }    }}
f20047
0
createWorker
public void kafkatest_f20055_0(CreateWorkerRequest req) throws Throwable
{    workerManager.createWorker(req.workerId(), req.taskId(), req.spec());}
f20055
0
stopWorker
public void kafkatest_f20056_0(StopWorkerRequest req) throws Throwable
{    workerManager.stopWorker(req.workerId(), false);}
f20056
0
destroyWorker
public void kafkatest_f20057_0(DestroyWorkerRequest req) throws Throwable
{    workerManager.stopWorker(req.workerId(), true);}
f20057
0
build
public AgentClient kafkatest_f20065_0()
{    if (target == null) {        throw new RuntimeException("You must specify a target.");    }    return new AgentClient(log, maxTries, target);}
f20065
0
target
public String kafkatest_f20066_0()
{    return target;}
f20066
0
maxTries
public int kafkatest_f20067_0()
{    return maxTries;}
f20067
0
addTargetArgument
private static void kafkatest_f20075_0(ArgumentParser parser)
{    parser.addArgument("--target", "-t").action(store()).required(true).type(String.class).dest("target").metavar("TARGET").help("A colon-separated host and port pair.  For example, example.com:8888");}
f20075
0
addJsonArgument
private static void kafkatest_f20076_0(ArgumentParser parser)
{    parser.addArgument("--json").action(storeTrue()).dest("json").metavar("JSON").help("Show the full response as JSON.");}
f20076
0
addWorkerIdArgument
private static void kafkatest_f20077_0(ArgumentParser parser, String help)
{    parser.addArgument("--workerId").action(storeTrue()).type(Long.class).dest("workerId").metavar("WORKER_ID").help(help);}
f20077
0
shutdown
public Empty kafkatest_f20085_0() throws Throwable
{    agent().beginShutdown();    return Empty.INSTANCE;}
f20085
0
agent
private Agent kafkatest_f20086_0()
{    Agent myAgent = agent.get();    if (myAgent == null) {        throw new RuntimeException("AgentRestResource has not been initialized yet.");    }    return myAgent;}
f20086
0
close
public void kafkatest_f20087_0()
{    if (closed.compareAndSet(false, true)) {        synchronized (ShutdownManager.this) {            refCount--;            if (shutdown && (refCount == 0)) {                ShutdownManager.this.notifyAll();            }        }    }}
f20087
0
transitionToRunning
 void kafkatest_f20095_0()
{    state = State.RUNNING;    timeoutFuture = scheduler.schedule(stateChangeExecutor, new StopWorker(workerId, false), Math.max(0, spec.endMs() - time.milliseconds()));}
f20095
0
transitionToStopping
 Future<Void> kafkatest_f20096_0()
{    state = State.STOPPING;    if (timeoutFuture != null) {        timeoutFuture.cancel(false);        timeoutFuture = null;    }    return workerCleanupExecutor.submit(new HaltWorker(this));}
f20096
0
transitionToDone
 void kafkatest_f20097_0()
{    state = State.DONE;    doneMs = time.milliseconds();    if (reference != null) {        reference.close();        reference = null;    }    doneFuture.complete(error);}
f20097
0
call
public Voidf20105_1) throws Exception
{    Worker worker = workers.get(workerId);    if (worker == null) {                return null;    }    if (mustDestroy) {        worker.mustDestroy = true;    }    switch(worker.state) {        case STARTING:                        worker.state = State.CANCELLING;            break;        case CANCELLING:                        break;        case RUNNING:                        worker.transitionToStopping();            break;        case STOPPING:                        break;        case DONE:            if (worker.mustDestroy) {                                workers.remove(worker.workerId);            } else {                            }            break;    }    return null;}
public Voidf20105
1
call
public Voidf20106_1) throws Exception
{    String failure = "";    try {        worker.taskWorker.stop(platform);    } catch (Exception exception) {                failure = exception.getMessage();    }    stateChangeExecutor.submit(new CompleteWorker(worker, failure));    return null;}
public Voidf20106
1
workerStates
public TreeMap<Long, WorkerState> kafkatest_f20107_0() throws Exception
{    try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {        return stateChangeExecutor.submit(new GetWorkerStates()).get();    }}
f20107
0
getConfig
public String kafkatest_f20115_0(String key)
{    return config.get(key);}
f20115
0
tags
public Set<String> kafkatest_f20116_0()
{    return tags;}
f20116
0
hashCode
public int kafkatest_f20117_0()
{    return Objects.hash(name, hostname, config, tags);}
f20117
0
node
public Node kafkatest_f20125_0(String id)
{    return nodes.get(id);}
f20125
0
nodes
public NavigableMap<String, Node> kafkatest_f20126_0()
{    return nodes;}
f20126
0
toJsonString
public static String kafkatest_f20127_0(Object object)
{    try {        return JSON_SERDE.writeValueAsString(object);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
f20127
0
expand
public static HashSet<String> kafkatest_f20135_0(String val)
{    HashSet<String> set = new HashSet<>();    Matcher matcher = NUMERIC_RANGE_PATTERN.matcher(val);    if (!matcher.matches()) {        set.add(val);        return set;    }    String prequel = matcher.group(1);    String rangeStart = matcher.group(2);    String rangeEnd = matcher.group(3);    String epilog = matcher.group(4);    int rangeStartInt = Integer.parseInt(rangeStart);    int rangeEndInt = Integer.parseInt(rangeEnd);    if (rangeEndInt < rangeStartInt) {        throw new RuntimeException("Invalid range: start " + rangeStartInt + " is higher than end " + rangeEndInt);    }    for (int i = rangeStartInt; i <= rangeEndInt; i++) {        set.add(String.format("%s%d%s", prequel, i, epilog));    }    return set;}
f20135
0
dateString
public static String kafkatest_f20136_0(long timeMs, ZoneOffset zoneOffset)
{    return new Date(timeMs).toInstant().atOffset(zoneOffset).format(DateTimeFormatter.ISO_OFFSET_DATE_TIME);}
f20136
0
durationString
public static String kafkatest_f20137_0(long periodMs)
{    StringBuilder bld = new StringBuilder();    Duration duration = Duration.ofMillis(periodMs);    long hours = duration.toHours();    if (hours > 0) {        bld.append(hours).append("h");        duration = duration.minusHours(hours);    }    long minutes = duration.toMinutes();    if (minutes > 0) {        bld.append(minutes).append("m");        duration = duration.minusMinutes(minutes);    }    long seconds = duration.getSeconds();    if ((seconds != 0) || bld.toString().isEmpty()) {        bld.append(seconds).append("s");    }    return bld.toString();}
f20137
0
createTopics
public static voidf20145_1Logger log, String bootstrapServers, Map<String, String> commonClientConf, Map<String, String> adminClientConf, Map<String, NewTopic> topics, boolean failOnExisting) throws Throwable
{    // re-thrown so that admin client is closed when the method returns.    try (Admin adminClient = createAdminClient(bootstrapServers, commonClientConf, adminClientConf)) {        createTopics(log, adminClient, topics, failOnExisting);    } catch (Exception e) {                throw e;    }}
public static voidf20145
1
createTopics
 static voidf20146_1Logger log, Admin adminClient, Map<String, NewTopic> topics, boolean failOnExisting) throws Throwable
{    if (topics.isEmpty()) {                return;    }    Collection<String> topicsExists = createTopics(log, adminClient, topics.values());    if (!topicsExists.isEmpty()) {        if (failOnExisting) {                        throw new TopicExistsException("One or more topics already exist.");        } else {            verifyTopics(log, adminClient, topicsExists, topics, 3, 2500);        }    }}
 static voidf20146
1
createTopics
private static Collection<String>f20147_1Logger log, Admin adminClient, Collection<NewTopic> topics) throws Throwable
{    long startMs = Time.SYSTEM.milliseconds();    int tries = 0;    List<String> existingTopics = new ArrayList<>();    Map<String, NewTopic> newTopics = new HashMap<>();    for (NewTopic newTopic : topics) {        newTopics.put(newTopic.name(), newTopic);    }    List<String> topicsToCreate = new ArrayList<>(newTopics.keySet());    while (true) {                Map<String, Future<Void>> creations = new HashMap<>();        while (!topicsToCreate.isEmpty()) {            List<NewTopic> newTopicsBatch = new ArrayList<>();            for (int i = 0; (i < MAX_CREATE_TOPICS_BATCH_SIZE) && !topicsToCreate.isEmpty(); i++) {                String topicName = topicsToCreate.remove(0);                newTopicsBatch.add(newTopics.get(topicName));            }            creations.putAll(adminClient.createTopics(newTopicsBatch).values());        }        // timeout.  This is a workaround for KAFKA-6368.        for (Map.Entry<String, Future<Void>> entry : creations.entrySet()) {            String topicName = entry.getKey();            Future<Void> future = entry.getValue();            try {                future.get();                            } catch (Exception e) {                if ((e.getCause() instanceof TimeoutException) || (e.getCause() instanceof NotEnoughReplicasException)) {                                        topicsToCreate.add(topicName);                } else if (e.getCause() instanceof TopicExistsException) {                                        existingTopics.add(topicName);                } else {                                        throw e.getCause();                }            }        }        if (topicsToCreate.isEmpty()) {            break;        }        if (Time.SYSTEM.milliseconds() > startMs + CREATE_TOPICS_CALL_TIMEOUT) {            String str = "Unable to create topic(s): " + Utils.join(topicsToCreate, ", ") + "after " + tries + " attempt(s)";                        throw new TimeoutException(str);        }    }    return existingTopics;}
private static Collection<String>f20147
1
createTask
public void kafkatest_f20155_0(CreateTaskRequest request) throws Throwable
{    taskManager.createTask(request.id(), request.spec());}
f20155
0
stopTask
public void kafkatest_f20156_0(StopTaskRequest request) throws Throwable
{    taskManager.stopTask(request.id());}
f20156
0
destroyTask
public void kafkatest_f20157_0(DestroyTaskRequest request) throws Throwable
{    taskManager.destroyTask(request.id());}
f20157
0
target
public Builder kafkatest_f20165_0(String target)
{    this.target = target;    return this;}
f20165
0
target
public Builder kafkatest_f20166_0(String host, int port)
{    this.target = String.format("%s:%d", host, port);    return this;}
f20166
0
build
public CoordinatorClient kafkatest_f20167_0()
{    if (target == null) {        throw new RuntimeException("You must specify a target.");    }    return new CoordinatorClient(log, maxTries, target);}
f20167
0
tasks
public TasksResponse kafkatest_f20175_0(TasksRequest request) throws Exception
{    UriBuilder uriBuilder = UriBuilder.fromPath(url("/coordinator/tasks"));    uriBuilder.queryParam("taskId", (Object[]) request.taskIds().toArray(new String[0]));    uriBuilder.queryParam("firstStartMs", request.firstStartMs());    uriBuilder.queryParam("lastStartMs", request.lastStartMs());    uriBuilder.queryParam("firstEndMs", request.firstEndMs());    uriBuilder.queryParam("lastEndMs", request.lastEndMs());    if (request.state().isPresent()) {        uriBuilder.queryParam("state", request.state().get().toString());    }    HttpResponse<TasksResponse> resp = JsonRestServer.httpRequest(log, uriBuilder.build().toString(), "GET", null, new TypeReference<TasksResponse>() {    }, maxTries);    return resp.body();}
f20175
0
task
public TaskState kafkatest_f20176_0(TaskRequest request) throws Exception
{    String uri = UriBuilder.fromPath(url("/coordinator/tasks/{taskId}")).build(request.taskId()).toString();    HttpResponse<TaskState> resp = JsonRestServer.httpRequest(log, uri, "GET", null, new TypeReference<TaskState>() {    }, maxTries);    return resp.body();}
f20176
0
shutdown
public void kafkatest_f20177_0() throws Exception
{    HttpResponse<Empty> resp = JsonRestServer.httpRequest(log, url("/coordinator/shutdown"), "PUT", null, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
f20177
0
uptime
public UptimeResponse kafkatest_f20185_0()
{    return coordinator().uptime();}
f20185
0
createTask
public Empty kafkatest_f20186_0(CreateTaskRequest request) throws Throwable
{    coordinator().createTask(request);    return Empty.INSTANCE;}
f20186
0
stopTask
public Empty kafkatest_f20187_0(StopTaskRequest request) throws Throwable
{    coordinator().stopTask(request);    return Empty.INSTANCE;}
f20187
0
toString
public String kafkatest_f20195_0()
{    return String.format("%s_%d", taskId, workerId);}
f20195
0
rescheduleNextHeartbeat
 void kafkatest_f20196_0(long initialDelayMs)
{    if (this.heartbeatFuture != null) {        this.heartbeatFuture.cancel(false);    }    this.heartbeatFuture = this.executor.scheduleAtFixedRate(heartbeat, initialDelayMs, HEARTBEAT_DELAY_MS, TimeUnit.MILLISECONDS);}
f20196
0
run
public voidf20197_1)
{    rescheduleNextHeartbeat(HEARTBEAT_DELAY_MS);    try {        AgentStatusResponse agentStatus = null;        try {            agentStatus = client.status();        } catch (ConnectException e) {                        return;        } catch (Exception e) {                        // agents going down?            return;        }        if (log.isTraceEnabled()) {            log.trace("{}: got heartbeat status {}", node.name(), agentStatus);        }        handleMissingWorkers(agentStatus);        handlePresentWorkers(agentStatus);    } catch (Throwable e) {            }}
public voidf20197
1
call
public Voidf20205_1) throws Exception
{    ManagedWorker worker = workers.remove(workerId);    if (worker == null) {                return null;    }    rescheduleNextHeartbeat(0);    return null;}
public Voidf20205
1
beginShutdown
public voidf20206_1boolean stopNode)
{    executor.shutdownNow();    if (stopNode) {        try {            client.invokeShutdown();        } catch (Exception e) {                    }    }}
public voidf20206
1
waitForShutdown
public void kafkatest_f20207_0() throws InterruptedException
{    executor.awaitTermination(1, TimeUnit.DAYS);}
f20207
0
createTask
public voidf20215_1final String id, TaskSpec spec) throws Throwable
{    try {        executor.submit(new CreateTask(id, spec)).get();    } catch (ExecutionException | JsonProcessingException e) {                throw e.getCause();    }}
public voidf20215
1
call
public Voidf20216_1) throws Exception
{    if (id.isEmpty()) {        throw new InvalidRequestException("Invalid empty ID in createTask request.");    }    ManagedTask task = tasks.get(id);    if (task != null) {        if (!task.originalSpec.equals(originalSpec)) {            throw new RequestConflictException("Task ID " + id + " already " + "exists, and has a different spec " + task.originalSpec);        }                return null;    }    TaskController controller = null;    String failure = null;    try {        controller = spec.newController(id);    } catch (Throwable t) {        failure = "Failed to create TaskController: " + t.getMessage();    }    if (failure != null) {                task = new ManagedTask(id, originalSpec, spec, null, TaskStateType.DONE);        task.doneMs = time.milliseconds();        task.maybeSetError(failure);        tasks.put(id, task);        return null;    }    task = new ManagedTask(id, originalSpec, spec, controller, TaskStateType.PENDING);    tasks.put(id, task);    long delayMs = task.startDelayMs(time.milliseconds());    task.startFuture = scheduler.schedule(executor, new RunTask(task), delayMs);        return null;}
public Voidf20216
1
call
public Voidf20217_1) throws Exception
{    task.clearStartFuture();    if (task.state != TaskStateType.PENDING) {                return null;    }    TreeSet<String> nodeNames;    try {        nodeNames = task.findNodeNames();    } catch (Exception e) {                task.doneMs = time.milliseconds();        task.state = TaskStateType.DONE;        task.maybeSetError("Unable to find nodes for task: " + e.getMessage());        return null;    }        task.state = TaskStateType.RUNNING;    task.startedMs = time.milliseconds();    for (String workerName : nodeNames) {        long workerId = nextWorkerId++;        task.workerIds.put(workerName, workerId);        workerStates.put(workerId, new WorkerReceiving(task.id, task.spec));        nodeManagers.get(workerName).createWorker(workerId, task.id, task.spec);    }    return null;}
public Voidf20217
1
tasks
public TasksResponse kafkatest_f20225_0(TasksRequest request) throws ExecutionException, InterruptedException
{    return executor.submit(new GetTasksResponse(request)).get();}
f20225
0
call
public TasksResponse kafkatest_f20226_0() throws Exception
{    TreeMap<String, TaskState> states = new TreeMap<>();    for (ManagedTask task : tasks.values()) {        if (request.matches(task.id, task.startedMs, task.doneMs, task.state)) {            states.put(task.id, task.taskState());        }    }    return new TasksResponse(states);}
f20226
0
task
public TaskState kafkatest_f20227_0(TaskRequest request) throws ExecutionException, InterruptedException
{    return executor.submit(new GetTaskState(request)).get();}
f20227
0
newTaskWorker
public TaskWorker kafkatest_f20235_0(String id)
{    return new DegradedNetworkFaultWorker(id, nodeSpecs);}
f20235
0
nodeSpecs
public Map<String, NodeDegradeSpec> kafkatest_f20236_0()
{    return nodeSpecs;}
f20236
0
start
public voidf20237_1Platform platform, WorkerStatusTracker status, KafkaFutureImpl<String> haltFuture) throws Exception
{        this.status = status;    this.status.update(new TextNode("enabling traffic control " + id));    Node curNode = platform.curNode();    DegradedNetworkFaultSpec.NodeDegradeSpec nodeSpec = nodeSpecs.get(curNode.name());    if (nodeSpec != null) {        for (String device : devicesForSpec(nodeSpec)) {            if (nodeSpec.latencyMs() < 0) {                throw new RuntimeException("Expected a positive value for latencyMs, but got " + nodeSpec.latencyMs());            } else {                enableTrafficControl(platform, device, nodeSpec.latencyMs());            }        }    }    this.status.update(new TextNode("enabled traffic control " + id));}
public voidf20237
1
errorCode
public int kafkatest_f20245_0()
{    return errorCode;}
f20245
0
newController
public TaskController kafkatest_f20246_0(String id)
{    return new KiboshFaultController(nodeNames);}
f20246
0
newTaskWorker
public TaskWorker kafkatest_f20247_0(String id)
{    return new KiboshFaultWorker(id, new KiboshFilesUnreadableFaultSpec(prefix, errorCode), mountPath);}
f20247
0
read
public static KiboshControlFile kafkatest_f20255_0(Path controlPath) throws IOException
{    byte[] controlFileBytes = Files.readAllBytes(controlPath);    return JsonUtil.JSON_SERDE.readValue(controlFileBytes, KiboshControlFile.class);}
f20255
0
faults
public List<KiboshFaultSpec> kafkatest_f20256_0()
{    return faults;}
f20256
0
write
public void kafkatest_f20257_0(Path controlPath) throws IOException
{    Files.write(controlPath, JsonUtil.JSON_SERDE.writeValueAsBytes(this));}
f20257
0
start
public voidf20265_1Platform platform, WorkerStatusTracker status, KafkaFutureImpl<String> errorFuture) throws Exception
{        this.status = status;    this.status.update(new TextNode("Adding fault " + id));    Kibosh.INSTANCE.addFault(mountPath, spec);    this.status.update(new TextNode("Added fault " + id));}
public voidf20265
1
stop
public voidf20266_1Platform platform) throws Exception
{        this.status.update(new TextNode("Removing fault " + id));    Kibosh.INSTANCE.removeFault(mountPath, spec);    this.status.update(new TextNode("Removed fault " + id));}
public voidf20266
1
targetNodes
public Set<String> kafkatest_f20267_0(Topology topology)
{    Set<String> targetNodes = new HashSet<>();    for (Set<String> partitionSet : partitionSets) {        targetNodes.addAll(partitionSet);    }    return targetNodes;}
f20267
0
targetNodes
public Set<String> kafkatest_f20275_0(Topology topology)
{    return nodeNames;}
f20275
0
nodeNames
public Set<String> kafkatest_f20276_0()
{    return nodeNames;}
f20276
0
javaProcessName
public String kafkatest_f20277_0()
{    return javaProcessName;}
f20277
0
stopAgents
public boolean kafkatest_f20285_0()
{    return stopAgents;}
f20285
0
serverStartMs
public long kafkatest_f20286_0()
{    return serverStartMs;}
f20286
0
id
public String kafkatest_f20287_0()
{    return id;}
f20287
0
hashCode
public int kafkatest_f20295_0()
{    return 1;}
f20295
0
toString
public String kafkatest_f20296_0()
{    return JsonUtil.toJsonString(this);}
f20296
0
code
public int kafkatest_f20297_0()
{    return code;}
f20297
0
waitForShutdown
public void kafkatest_f20305_0() throws InterruptedException
{    while (!shutdownExecutor.isShutdown()) {        shutdownExecutor.awaitTermination(1, TimeUnit.DAYS);    }}
f20305
0
httpRequest
public static HttpResponse<T>f20306_1Logger logger, String url, String method, Object requestBodyData, TypeReference<T> responseFormat) throws IOException
{    HttpURLConnection connection = null;    try {        String serializedBody = requestBodyData == null ? null : JsonUtil.JSON_SERDE.writeValueAsString(requestBodyData);                connection = (HttpURLConnection) new URL(url).openConnection();        connection.setRequestMethod(method);        connection.setRequestProperty("User-Agent", "kafka");        connection.setRequestProperty("Accept", "application/json");        // connection.getResponseCode() implicitly calls getInputStream, so always set        // this to true.        connection.setDoInput(true);        connection.setUseCaches(false);        if (requestBodyData != null) {            connection.setRequestProperty("Content-Type", "application/json");            connection.setDoOutput(true);            OutputStream os = connection.getOutputStream();            os.write(serializedBody.getBytes(StandardCharsets.UTF_8));            os.flush();            os.close();        }        int responseCode = connection.getResponseCode();        if (responseCode == HttpURLConnection.HTTP_NO_CONTENT) {            return new HttpResponse<>(null, new ErrorResponse(responseCode, connection.getResponseMessage()));        } else if ((responseCode >= 200) && (responseCode < 300)) {            InputStream is = connection.getInputStream();            T result = JsonUtil.JSON_SERDE.readValue(is, responseFormat);            is.close();            return new HttpResponse<>(result, null);        } else {            // If the resposne code was not in the 200s, we assume that this is an error            // response.            InputStream es = connection.getErrorStream();            if (es == null) {                // Handle the case where HttpURLConnection#getErrorStream returns null.                return new HttpResponse<>(null, new ErrorResponse(responseCode, ""));            }            // Try to read the error response JSON.            ErrorResponse error = JsonUtil.JSON_SERDE.readValue(es, ErrorResponse.class);            es.close();            return new HttpResponse<>(null, error);        }    } finally {        if (connection != null) {            connection.disconnect();        }    }}
public static HttpResponse<T>f20306
1
httpRequest
public static HttpResponse<T> kafkatest_f20307_0(String url, String method, Object requestBodyData, TypeReference<T> responseFormat, int maxTries) throws IOException, InterruptedException
{    return httpRequest(log, url, method, requestBodyData, responseFormat, maxTries);}
f20307
0
toException
public static Exception kafkatest_f20315_0(int code, String msg) throws Exception
{    if (code == Response.Status.NOT_FOUND.getStatusCode()) {        throw new NotFoundException(msg);    } else if (code == Response.Status.NOT_IMPLEMENTED.getStatusCode()) {        throw new ClassNotFoundException(msg);    } else if (code == Response.Status.BAD_REQUEST.getStatusCode()) {        throw new InvalidRequestException(msg);    } else if (code == Response.Status.CONFLICT.getStatusCode()) {        throw new RequestConflictException(msg);    } else {        throw new RuntimeException(msg);    }}
f20315
0
buildResponse
private Response kafkatest_f20316_0(Response.Status code, Throwable e)
{    return Response.status(code).entity(new ErrorResponse(code.getStatusCode(), e.getMessage())).build();}
f20316
0
id
public String kafkatest_f20317_0()
{    return id;}
f20317
0
taskId
public String kafkatest_f20325_0()
{    return taskId;}
f20325
0
startedMs
public long kafkatest_f20326_0()
{    return startedMs;}
f20326
0
stateType
public TaskStateType kafkatest_f20327_0()
{    return TaskStateType.RUNNING;}
f20327
0
tasks
public Map<String, TaskState> kafkatest_f20335_0()
{    return tasks;}
f20335
0
spec
public TaskSpec kafkatest_f20336_0()
{    return spec;}
f20336
0
status
public JsonNode kafkatest_f20337_0()
{    return status;}
f20337
0
error
public String kafkatest_f20345_0()
{    return error;}
f20345
0
done
public boolean kafkatest_f20346_0()
{    return true;}
f20346
0
status
public JsonNode kafkatest_f20347_0()
{    return new TextNode("receiving");}
f20347
0
done
public boolean kafkatest_f20355_0()
{    return false;}
f20355
0
startedMs
public long kafkatest_f20356_0()
{    throw new KafkaException("invalid state");}
f20356
0
running
public boolean kafkatest_f20357_0()
{    return false;}
f20357
0
newController
public TaskController kafkatest_f20365_0(String id)
{    return new NoOpTaskController();}
f20365
0
newTaskWorker
public TaskWorker kafkatest_f20366_0(String id)
{    return new NoOpTaskWorker(id);}
f20366
0
start
public voidf20367_1Platform platform, WorkerStatusTracker status, KafkaFutureImpl<String> errorFuture) throws Exception
{        this.status = status;    this.status.update(new TextNode("active"));}
public voidf20367
1
configOrEmptyMap
protected Map<String, String> kafkatest_f20375_0(Map<String, String> config)
{    return (config == null) ? Collections.<String, String>emptyMap() : config;}
f20375
0
clientNode
public List<String> kafkatest_f20376_0()
{    return clientNodes;}
f20376
0
bootstrapServers
public String kafkatest_f20377_0()
{    return bootstrapServers;}
f20377
0
fromSpec
 static Stressor kafkatest_f20385_0(ConnectionStressSpec spec)
{    switch(spec.action()) {        case CONNECT:            return new ConnectStressor(spec);        case FETCH_METADATA:            return new FetchMetadataStressor(spec);    }    throw new RuntimeException("invalid spec.action " + spec.action());}
f20385
0
tryConnect
public boolean kafkatest_f20386_0()
{    try {        List<Node> nodes = updater.fetchNodes();        Node targetNode = nodes.get(ThreadLocalRandom.current().nextInt(nodes.size()));        try (Metrics metrics = new Metrics()) {            LogContext logContext = new LogContext();            try (Selector selector = new Selector(conf.getLong(AdminClientConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), metrics, TIME, "", channelBuilder, logContext)) {                try (NetworkClient client = new NetworkClient(selector, updater, "ConnectionStressWorker", 1, 1000, 1000, 4096, 4096, 1000, ClientDnsLookup.forConfig(conf.getString(AdminClientConfig.CLIENT_DNS_LOOKUP_CONFIG)), TIME, false, new ApiVersions(), logContext)) {                    NetworkClientUtils.awaitReady(client, targetNode, TIME, 100);                }            }        }        return true;    } catch (IOException e) {        return false;    }}
f20386
0
close
public void kafkatest_f20387_0() throws Exception
{    Utils.closeQuietly(updater, "ManualMetadataUpdater");    Utils.closeQuietly(channelBuilder, "ChannelBuilder");}
f20387
0
size
public int kafkatest_f20396_0()
{    return size;}
f20396
0
value
public byte[] kafkatest_f20397_0()
{    return value;}
f20397
0
generate
public byte[] kafkatest_f20398_0(long position)
{    byte[] next = new byte[size];    for (int i = 0; i < next.length; i += value.length) {        System.arraycopy(value, 0, next, i, Math.min(next.length - i, value.length));    }    return next;}
f20398
0
commonClientConf
public Map<String, String> kafkatest_f20406_0()
{    return commonClientConf;}
f20406
0
adminClientConf
public Map<String, String> kafkatest_f20407_0()
{    return adminClientConf;}
f20407
0
activeTopics
public List<String> kafkatest_f20408_0()
{    return activeTopics;}
f20408
0
clientId
private String kafkatest_f20416_0(int idx)
{    return String.format("consumer.%s-%d", id, idx);}
f20416
0
consumer
private ThreadSafeConsumer kafkatest_f20417_0(String consumerGroup, String clientId)
{    Properties props = new Properties();    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, spec.bootstrapServers());    props.put(ConsumerConfig.CLIENT_ID_CONFIG, clientId);    props.put(ConsumerConfig.GROUP_ID_CONFIG, consumerGroup);    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 100000);    // these defaults maybe over-written by the user-specified commonClientConf or consumerConf    WorkerUtils.addConfigsToProperties(props, spec.commonClientConf(), spec.consumerConf());    return new ThreadSafeConsumer(new KafkaConsumer<>(props, new ByteArrayDeserializer(), new ByteArrayDeserializer()), clientId);}
f20417
0
consumerGroup
private String kafkatest_f20418_0()
{    return toUseRandomConsumeGroup() ? "consume-bench-" + UUID.randomUUID().toString() : spec.consumerGroup();}
f20418
0
run
public void kafkatest_f20426_0()
{    try {        update();    } catch (Exception e) {        WorkerUtils.abort(log, "ConsumeStatusUpdater", e, doneFuture);    }}
f20426
0
update
 StatusDataf20427_1)
{    Histogram.Summary latSummary = latencyHistogram.summarize(StatusData.PERCENTILES);    Histogram.Summary msgSummary = messageSizeHistogram.summarize(StatusData.PERCENTILES);    StatusData statusData = new StatusData(consumer.assignedPartitions(), latSummary.numSamples(), (long) (msgSummary.numSamples() * msgSummary.average()), (long) msgSummary.average(), latSummary.average(), latSummary.percentiles().get(0).value(), latSummary.percentiles().get(1).value(), latSummary.percentiles().get(2).value());    statusUpdater.updateConsumeStatus(consumer.clientId(), statusData);        return statusData;}
 StatusDataf20427
1
assignedPartitions
public List<String> kafkatest_f20428_0()
{    return assignedPartitions;}
f20428
0
stop
public voidf20436_1Platform platform) throws Exception
{    if (!running.compareAndSet(true, false)) {        throw new IllegalStateException("ConsumeBenchWorker is not running.");    }        doneFuture.complete("");    executor.shutdownNow();    executor.awaitTermination(1, TimeUnit.DAYS);    consumer.close();    this.consumer = null;    this.executor = null;    this.statusUpdater = null;    this.statusUpdaterFuture = null;    this.workerStatus = null;    this.doneFuture = null;}
public voidf20436
1
poll
 ConsumerRecords<byte[], byte[]> kafkatest_f20437_0()
{    this.consumerLock.lock();    try {        return consumer.poll(Duration.ofMillis(50));    } finally {        this.consumerLock.unlock();    }}
f20437
0
close
 void kafkatest_f20438_0()
{    if (closed)        return;    this.consumerLock.lock();    try {        consumer.unsubscribe();        Utils.closeQuietly(consumer, "consumer");        closed = true;    } finally {        this.consumerLock.unlock();    }}
f20438
0
workload
public JsonNode kafkatest_f20446_0()
{    return workload;}
f20446
0
shutdownGracePeriodMs
public Optional<Integer> kafkatest_f20447_0()
{    return shutdownGracePeriodMs;}
f20447
0
newController
public TaskController kafkatest_f20448_0(String id)
{    return topology -> Collections.singleton(commandNode);}
f20448
0
run
public voidf20456_1)
{    try {        int exitStatus = process.waitFor();                // Wait for the stdout and stderr monitors to exit.  It's particularly important        // to wait for the stdout monitor to exit since there may be an error or status        // there that we haven't seen yet.        stdoutFuture.get();        stderrFuture.get();        // that if doneFuture was already completed previously, this will have no effect.        if (exitStatus == 0) {            doneFuture.complete("");        } else {            doneFuture.complete("exited with return code " + exitStatus);        }        // Tell the StdinWriter thread to exit.        stdinQueue.add(Optional.empty());        // Tell the shutdown manager thread to exit.        terminatorActionQueue.add(TerminatorAction.CLOSE);        terminatorFuture.get();        executor.shutdown();    } catch (Throwable e) {                doneFuture.complete("ExitMonitor error: " + e.getMessage());    }}
public voidf20456
1
run
public voidf20457_1)
{    try {        while (true) {            switch(terminatorActionQueue.take()) {                case DESTROY:                                        process.getInputStream().close();                    process.getErrorStream().close();                    process.destroy();                    break;                case DESTROY_FORCIBLY:                                        process.getInputStream().close();                    process.getErrorStream().close();                    process.destroyForcibly();                    break;                case CLOSE:                    log.trace("{}: closing Terminator thread.", id);                    return;            }        }    } catch (Throwable e) {                doneFuture.complete("Terminator error: " + e.getMessage());    }}
public voidf20457
1
stop
public voidf20458_1Platform platform) throws Exception
{    if (!running.compareAndSet(true, false)) {        throw new IllegalStateException("ExternalCommandWorker is not running.");    }        terminatorActionQueue.add(TerminatorAction.DESTROY);    int shutdownGracePeriodMs = spec.shutdownGracePeriodMs().isPresent() ? spec.shutdownGracePeriodMs().get() : DEFAULT_SHUTDOWN_GRACE_PERIOD_MS;    if (!executor.awaitTermination(shutdownGracePeriodMs, TimeUnit.MILLISECONDS)) {        terminatorActionQueue.add(TerminatorAction.DESTROY_FORCIBLY);        executor.awaitTermination(1, TimeUnit.DAYS);    }    this.status = null;    this.doneFuture = null;    this.executor = null;}
public voidf20458
1
summarize
public Summary kafkatest_f20466_0()
{    return summarize(new float[0]);}
f20466
0
summarize
public Summary kafkatest_f20467_0(float[] percentiles)
{    int[] countsCopy = new int[counts.length];    synchronized (this) {        System.arraycopy(counts, 0, countsCopy, 0, counts.length);    }    // Verify that the percentiles array is sorted and positive.    float prev = 0f;    for (int i = 0; i < percentiles.length; i++) {        if (percentiles[i] < prev) {            throw new RuntimeException("Invalid percentiles fraction array.  Bad element " + percentiles[i] + ".  The array must be sorted and non-negative.");        }        if (percentiles[i] > 1.0f) {            throw new RuntimeException("Invalid percentiles fraction array.  Bad element " + percentiles[i] + ".  Elements must be less than or equal to 1.");        }    }    // Find out how many total samples we have, and what the average is.    long numSamples = 0;    float total = 0f;    for (int i = 0; i < countsCopy.length; i++) {        long count = countsCopy[i];        numSamples = numSamples + count;        total = total + (i * count);    }    float average = (numSamples == 0) ? 0.0f : (total / numSamples);    List<PercentileSummary> percentileSummaries = summarizePercentiles(countsCopy, percentiles, numSamples);    return new Summary(numSamples, average, percentileSummaries);}
f20467
0
summarizePercentiles
private List<PercentileSummary> kafkatest_f20468_0(int[] countsCopy, float[] percentiles, long numSamples)
{    if (percentiles.length == 0) {        return Collections.emptyList();    }    List<PercentileSummary> summaries = new ArrayList<>(percentiles.length);    int i = 0, j = 0;    long seen = 0, next = (long) (numSamples * percentiles[0]);    while (true) {        if (i == countsCopy.length - 1) {            for (; j < percentiles.length; j++) {                summaries.add(new PercentileSummary(percentiles[j], i));            }            return summaries;        }        seen += countsCopy[i];        while (seen >= next) {            summaries.add(new PercentileSummary(percentiles[j], i));            j++;            if (j == percentiles.length) {                return summaries;            }            next = (long) (numSamples * percentiles[j]);        }        i++;    }}
f20468
0
hasNext
public boolean kafkatest_f20476_0()
{    return true;}
f20476
0
next
public synchronized byte[] kafkatest_f20477_0()
{    return generator.generate(position++);}
f20477
0
remove
public void kafkatest_f20478_0()
{    throw new UnsupportedOperationException();}
f20478
0
keyGenerator
public PayloadGenerator kafkatest_f20486_0()
{    return keyGenerator;}
f20486
0
valueGenerator
public PayloadGenerator kafkatest_f20487_0()
{    return valueGenerator;}
f20487
0
transactionGenerator
public Optional<TransactionGenerator> kafkatest_f20488_0()
{    return transactionGenerator;}
f20488
0
newController
public TaskController kafkatest_f20496_0(String id)
{    return topology -> Collections.singleton(producerNode);}
f20496
0
newTaskWorker
public TaskWorker kafkatest_f20497_0(String id)
{    return new ProduceBenchWorker(id, this);}
f20497
0
start
public voidf20498_1Platform platform, WorkerStatusTracker status, KafkaFutureImpl<String> doneFuture) throws Exception
{    if (!running.compareAndSet(false, true)) {        throw new IllegalStateException("ProducerBenchWorker is already running.");    }        // Create an executor with 2 threads.  We need the second thread so    // that the StatusUpdater can run in parallel with SendRecords.    this.executor = Executors.newScheduledThreadPool(2, ThreadUtils.createThreadFactory("ProduceBenchWorkerThread%d", false));    this.status = status;    this.doneFuture = doneFuture;    executor.submit(new Prepare());}
public voidf20498
1
run
public void kafkatest_f20506_0()
{    try {        update();    } catch (Exception e) {        WorkerUtils.abort(log, "StatusUpdater", e, doneFuture);    }}
f20506
0
update
 StatusData kafkatest_f20507_0()
{    Histogram.Summary summary = histogram.summarize(StatusData.PERCENTILES);    StatusData statusData = new StatusData(summary.numSamples(), summary.average(), summary.percentiles().get(0).value(), summary.percentiles().get(1).value(), summary.percentiles().get(2).value(), transactionsCommitted.get());    status.update(JsonUtil.JSON_SERDE.valueToTree(statusData));    return statusData;}
f20507
0
totalSent
public long kafkatest_f20508_0()
{    return totalSent;}
f20508
0
component
public PayloadGenerator kafkatest_f20516_0()
{    return component;}
f20516
0
seed
public long kafkatest_f20517_0()
{    return seed;}
f20517
0
components
public List<RandomComponent> kafkatest_f20518_0()
{    return components;}
f20518
0
addPending
 synchronized void kafkatest_f20526_0(long messageIndex)
{    pending.add(messageIndex);}
f20526
0
removePending
 synchronized boolean kafkatest_f20527_0(long messageIndex)
{    if (pending.remove(messageIndex)) {        totalReceived++;        return true;    } else {        return false;    }}
f20527
0
totalReceived
 synchronized long kafkatest_f20528_0()
{    return totalReceived;}
f20528
0
clientNode
public String kafkatest_f20536_0()
{    return clientNode;}
f20536
0
bootstrapServers
public String kafkatest_f20537_0()
{    return bootstrapServers;}
f20537
0
targetMessagesPerSec
public int kafkatest_f20538_0()
{    return targetMessagesPerSec;}
f20538
0
newController
public TaskController kafkatest_f20546_0(String id)
{    return topology -> Collections.singleton(clientNode);}
f20546
0
newTaskWorker
public TaskWorker kafkatest_f20547_0(String id)
{    return new RoundTripWorker(id, this);}
f20547
0
size
public int kafkatest_f20548_0()
{    return size;}
f20548
0
commonClientConf
public Map<String, String> kafkatest_f20556_0()
{    return commonClientConf;}
f20556
0
keyGenerator
public PayloadGenerator kafkatest_f20557_0()
{    return keyGenerator;}
f20557
0
valueGenerator
public PayloadGenerator kafkatest_f20558_0()
{    return valueGenerator;}
f20558
0
newTaskWorker
public TaskWorker kafkatest_f20566_0(String id)
{    return new SustainedConnectionWorker(id, this);}
f20566
0
start
public voidf20567_1Platform platform, WorkerStatusTracker status, KafkaFutureImpl<String> doneFuture) throws Exception
{    if (!running.compareAndSet(false, true)) {        throw new IllegalStateException("SustainedConnectionWorker is already running.");    }        this.doneFuture = doneFuture;    this.status = status;    this.connections = new ArrayList<>();    // Initialize all status reporting metrics to 0.    this.totalProducerConnections = new AtomicLong(0);    this.totalProducerFailedConnections = new AtomicLong(0);    this.totalConsumerConnections = new AtomicLong(0);    this.totalConsumerFailedConnections = new AtomicLong(0);    this.totalMetadataConnections = new AtomicLong(0);    this.totalMetadataFailedConnections = new AtomicLong(0);    this.totalAbortedThreads = new AtomicLong(0);    // Create the worker classes and add them to the list of items to act on.    for (int i = 0; i < this.spec.producerConnectionCount(); i++) {        this.connections.add(new ProducerSustainedConnection());    }    for (int i = 0; i < this.spec.consumerConnectionCount(); i++) {        this.connections.add(new ConsumerSustainedConnection());    }    for (int i = 0; i < this.spec.metadataConnectionCount(); i++) {        this.connections.add(new MetadataSustainedConnection());    }    // Create the status reporter thread and schedule it.    this.statusUpdaterExecutor = Executors.newScheduledThreadPool(1, ThreadUtils.createThreadFactory("StatusUpdaterWorkerThread%d", false));    this.statusUpdaterFuture = this.statusUpdaterExecutor.scheduleAtFixedRate(new StatusUpdater(), 0, REPORT_INTERVAL_MS, TimeUnit.MILLISECONDS);    // Create the maintainer pool, add all the maintainer threads, then start it.    this.workerExecutor = Executors.newFixedThreadPool(spec.numThreads(), ThreadUtils.createThreadFactory("SustainedConnectionWorkerThread%d", false));    for (int i = 0; i < this.spec.numThreads(); i++) {        this.workerExecutor.submit(new MaintainLoop());    }}
public voidf20567
1
needsRefresh
public boolean kafkatest_f20568_0(long milliseconds)
{    return !this.inUse && (milliseconds > this.nextUpdate);}
f20568
0
refresh
public voidf20576_1)
{    try {        if (this.consumer == null) {            // Housekeeping to track the number of opened connections.            SustainedConnectionWorker.this.totalConsumerConnections.incrementAndGet();            // Create the consumer and fetch the partitions for the specified topic.            this.consumer = new KafkaConsumer<>(this.props, new ByteArrayDeserializer(), new ByteArrayDeserializer());            List<TopicPartition> partitions = this.consumer.partitionsFor(this.topicName).stream().map(partitionInfo -> new TopicPartition(partitionInfo.topic(), partitionInfo.partition())).collect(Collectors.toList());            // Select a random partition and assign it.            this.activePartition = partitions.get(this.rand.nextInt(partitions.size()));            this.consumer.assign(Collections.singletonList(this.activePartition));        }        // The behavior when passing in an empty list is to seek to the end of all subscribed partitions.        this.consumer.seekToEnd(Collections.emptyList());        // Poll to keep the connection alive, ignoring any records returned.        this.consumer.poll(Duration.ofMillis(50));    } catch (Throwable e) {        // Set the consumer to be recreated on the next cycle.        this.closeQuietly();        // Housekeeping to track the number of opened connections and failed connection attempts.        SustainedConnectionWorker.this.totalConsumerConnections.decrementAndGet();        SustainedConnectionWorker.this.totalConsumerFailedConnections.incrementAndGet();        SustainedConnectionWorker.    }    // Schedule this again and set to not in use.    this.completeRefresh();}
public voidf20576
1
closeQuietly
protected void kafkatest_f20577_0()
{    Utils.closeQuietly(this.consumer, "KafkaConsumer");    this.consumer = null;    this.activePartition = null;}
f20577
0
run
public voidf20578_1)
{    try {        while (!doneFuture.isDone()) {            Optional<SustainedConnection> currentConnection = SustainedConnectionWorker.this.findConnectionToMaintain();            if (currentConnection.isPresent()) {                currentConnection.get().refresh();            } else {                SustainedConnectionWorker.SYSTEM_TIME.sleep(SustainedConnectionWorker.BACKOFF_PERIOD_MS);            }        }    } catch (Exception e) {        SustainedConnectionWorker.this.totalAbortedThreads.incrementAndGet();        SustainedConnectionWorker.    }}
public voidf20578
1
totalMetadataFailedConnections
public long kafkatest_f20586_0()
{    return totalMetadataFailedConnections;}
f20586
0
totalAbortedThreads
public long kafkatest_f20587_0()
{    return totalAbortedThreads;}
f20587
0
updatedMs
public long kafkatest_f20588_0()
{    return updatedMs;}
f20588
0
get
public Map<String, PartitionsSpec> kafkatest_f20596_0()
{    return map;}
f20596
0
set
public void kafkatest_f20597_0(String name, PartitionsSpec value)
{    map.put(name, value);}
f20597
0
immutableCopy
public TopicsSpec kafkatest_f20598_0()
{    HashMap<String, PartitionsSpec> mapCopy = new HashMap<>();    mapCopy.putAll(map);    return new TopicsSpec(Collections.unmodifiableMap(mapCopy));}
f20598
0
setUp
public void kafkatest_f20606_0()
{    reporter = new PushHttpMetricsReporter(time, executor);    PowerMock.mockStatic(PushHttpMetricsReporter.class);}
f20606
0
testConfigureClose
public void kafkatest_f20607_0() throws Exception
{    expectConfigure();    expectClose();    replayAll();    configure();    reporter.close();    verifyAll();}
f20607
0
testConfigureBadUrl
public void kafkatest_f20608_0() throws Exception
{    Map<String, String> config = new HashMap<>();    config.put(PushHttpMetricsReporter.METRICS_URL_CONFIG, "malformed;url");    config.put(PushHttpMetricsReporter.METRICS_PERIOD_CONFIG, "5");    reporter.configure(config);}
f20608
0
expectRequest
private void kafkatest_f20616_0(int returnStatus) throws Exception
{    expectRequest(returnStatus, false);}
f20616
0
expectRequest
private void kafkatest_f20617_0(int returnStatus, boolean readResponse) throws Exception
{    EasyMock.expect(PushHttpMetricsReporter.newHttpConnection(URL)).andReturn(httpReq);    httpReq.setRequestMethod("POST");    EasyMock.expectLastCall();    httpReq.setDoInput(true);    EasyMock.expectLastCall();    httpReq.setRequestProperty("Content-Type", "application/json");    EasyMock.expectLastCall();    httpReq.setRequestProperty(EasyMock.eq("Content-Length"), EasyMock.anyString());    EasyMock.expectLastCall();    httpReq.setRequestProperty("Accept", "*/*");    EasyMock.expectLastCall();    httpReq.setUseCaches(false);    EasyMock.expectLastCall();    httpReq.setDoOutput(true);    EasyMock.expectLastCall();    EasyMock.expect(httpReq.getOutputStream()).andReturn(httpOut);    httpOut.write(EasyMock.capture(httpPayload));    EasyMock.expectLastCall();    httpOut.flush();    EasyMock.expectLastCall();    httpOut.close();    EasyMock.expectLastCall();    EasyMock.expect(httpReq.getResponseCode()).andReturn(returnStatus);    if (readResponse)        expectReadResponse();    httpReq.disconnect();    EasyMock.expectLastCall();}
f20617
0
assertPayloadHasClientInfo
private void kafkatest_f20618_0(JsonNode payload) throws UnknownHostException
{    // Should contain client info...    JsonNode client = payload.get("client");    assertTrue(client.isObject());    assertEquals(InetAddress.getLocalHost().getCanonicalHostName(), client.get("host").textValue());    assertEquals("", client.get("client_id").textValue());    assertEquals(time.milliseconds(), client.get("time").longValue());}
f20618
0
testAgentGetStatus
public void kafkatest_f20626_0() throws Exception
{    Agent agent = createAgent(Scheduler.SYSTEM);    AgentClient client = new AgentClient.Builder().maxTries(10).target("localhost", agent.port()).build();    AgentStatusResponse status = client.status();    assertEquals(agent.status(), status);    agent.beginShutdown();    agent.waitForShutdown();}
f20626
0
testCreateExpiredWorkerIsNotScheduled
public void kafkatest_f20627_0() throws Exception
{    long initialTimeMs = 100;    long tickMs = 15;    final boolean[] toSleep = { true };    MockTime time = new MockTime(tickMs, initialTimeMs, 0) {        /**         * Modify sleep() to call super.sleep() every second call         * in order to avoid the endless loop in the tick() calls to the MockScheduler listener         */        @Override        public void sleep(long ms) {            toSleep[0] = !toSleep[0];            if (toSleep[0])                super.sleep(ms);        }    };    MockScheduler scheduler = new MockScheduler(time);    Agent agent = createAgent(scheduler);    AgentClient client = new AgentClient.Builder().maxTries(10).target("localhost", agent.port()).build();    AgentStatusResponse status = client.status();    assertEquals(Collections.emptyMap(), status.workers());    new ExpectedTasks().waitFor(client);    final NoOpTaskSpec fooSpec = new NoOpTaskSpec(10, 10);    client.createWorker(new CreateWorkerRequest(0, "foo", fooSpec));    long actualStartTimeMs = initialTimeMs + tickMs;    long doneMs = actualStartTimeMs + 2 * tickMs;    new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerDone("foo", fooSpec, actualStartTimeMs, doneMs, null, "worker expired")).taskState(new TaskDone(fooSpec, actualStartTimeMs, doneMs, "worker expired", false, null)).build()).waitFor(client);}
f20627
0
sleep
public void kafkatest_f20628_0(long ms)
{    toSleep[0] = !toSleep[0];    if (toSleep[0])        super.sleep(ms);}
f20628
0
testDestroyWorkers
public void kafkatest_f20636_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    MockScheduler scheduler = new MockScheduler(time);    Agent agent = createAgent(scheduler);    AgentClient client = new AgentClient.Builder().maxTries(10).target("localhost", agent.port()).build();    new ExpectedTasks().waitFor(client);    final NoOpTaskSpec fooSpec = new NoOpTaskSpec(0, 5);    client.createWorker(new CreateWorkerRequest(0, "foo", fooSpec));    new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerRunning("foo", fooSpec, 0, new TextNode("active"))).build()).waitFor(client);    time.sleep(1);    client.destroyWorker(new DestroyWorkerRequest(0));    client.destroyWorker(new DestroyWorkerRequest(0));    client.destroyWorker(new DestroyWorkerRequest(1));    new ExpectedTasks().waitFor(client);    time.sleep(1);    final NoOpTaskSpec fooSpec2 = new NoOpTaskSpec(2, 1);    client.createWorker(new CreateWorkerRequest(1, "foo", fooSpec2));    new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerRunning("foo", fooSpec2, 2, new TextNode("active"))).build()).waitFor(client);    time.sleep(2);    new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerDone("foo", fooSpec2, 2, 4, new TextNode("done"), "")).build()).waitFor(client);    time.sleep(1);    client.destroyWorker(new DestroyWorkerRequest(1));    new ExpectedTasks().waitFor(client);    agent.beginShutdown();    agent.waitForShutdown();}
f20636
0
testExec
 static void kafkatest_f20637_0(Agent agent, String expected, boolean expectedReturn, TaskSpec spec) throws Exception
{    ByteArrayOutputStream b = new ByteArrayOutputStream();    PrintStream p = new PrintStream(b, true, StandardCharsets.UTF_8.toString());    boolean actualReturn = agent.exec(spec, p);    assertEquals(expected, b.toString());    assertEquals(expectedReturn, actualReturn);}
f20637
0
testAgentExecWithTimeout
public void kafkatest_f20638_0() throws Exception
{    Agent agent = createAgent(Scheduler.SYSTEM);    NoOpTaskSpec spec = new NoOpTaskSpec(0, 1);    TaskSpec rebasedSpec = agent.rebaseTaskSpecTime(spec);    testExec(agent, String.format("Waiting for completion of task:%s%n", JsonUtil.toPrettyJsonString(rebasedSpec)) + String.format("Task failed with status null and error worker expired%n"), false, rebasedSpec);    agent.beginShutdown();    agent.waitForShutdown();}
f20638
0
workerState
public ExpectedTaskBuilder kafkatest_f20646_0(WorkerState workerState)
{    this.workerState = workerState;    return this;}
f20646
0
build
public ExpectedTask kafkatest_f20647_0()
{    return new ExpectedTask(id, taskSpec, taskState, workerState);}
f20647
0
compare
 String kafkatest_f20648_0(TaskState actual)
{    if (actual == null) {        return "Did not find task " + id + "\n";    }    if ((taskSpec != null) && (!actual.spec().equals(taskSpec))) {        return "Invalid spec for task " + id + ": expected " + taskSpec + ", got " + actual.spec();    }    if ((taskState != null) && (!actual.equals(taskState))) {        return "Invalid state for task " + id + ": expected " + taskState + ", got " + actual;    }    return null;}
f20648
0
waitFor
public ExpectedTasksf20656_1final AgentClient client) throws InterruptedException
{    TestUtils.waitForCondition(() -> {        AgentStatusResponse status = null;        try {            status = client.status();        } catch (Exception e) {                        throw new RuntimeException(e);        }        StringBuilder errors = new StringBuilder();        HashMap<String, WorkerState> taskIdToWorkerState = new HashMap<>();        for (WorkerState state : status.workers().values()) {            taskIdToWorkerState.put(state.taskId(), state);        }        for (Map.Entry<String, ExpectedTask> entry : expected.entrySet()) {            String id = entry.getKey();            ExpectedTask worker = entry.getValue();            String differences = worker.compare(taskIdToWorkerState.get(id));            if (differences != null) {                errors.append(differences);            }        }        String errorString = errors.toString();        if (!errorString.isEmpty()) {                                                return false;        }        return true;    }, "Timed out waiting for expected workers " + JsonUtil.toJsonString(expected));    return this;}
public ExpectedTasksf20656
1
testDeserializationDoesNotProduceNulls
public void kafkatest_f20657_0() throws Exception
{    verify(new FilesUnreadableFaultSpec(0, 0, null, null, null, 0));    verify(new Kibosh.KiboshControlFile(null));    verify(new NetworkPartitionFaultSpec(0, 0, null));    verify(new ProcessStopFaultSpec(0, 0, null, null));    verify(new AgentStatusResponse(0, null));    verify(new TasksResponse(null));    verify(new WorkerDone(null, null, 0, 0, null, null));    verify(new WorkerRunning(null, null, 0, null));    verify(new WorkerStopping(null, null, 0, null));    verify(new ProduceBenchSpec(0, 0, null, null, 0, 0, null, null, Optional.empty(), null, null, null, null, null, false, false));    verify(new RoundTripWorkloadSpec(0, 0, null, null, null, null, null, null, 0, null, null, 0));    verify(new TopicsSpec());    verify(new PartitionsSpec(0, (short) 0, null, null));    Map<Integer, List<Integer>> partitionAssignments = new HashMap<Integer, List<Integer>>();    partitionAssignments.put(0, Arrays.asList(1, 2, 3));    partitionAssignments.put(1, Arrays.asList(1, 2, 3));    verify(new PartitionsSpec(0, (short) 0, partitionAssignments, null));    verify(new PartitionsSpec(0, (short) 0, null, null));}
f20657
0
verify
private void kafkatest_f20658_0(T val1) throws Exception
{    byte[] bytes = JsonUtil.JSON_SERDE.writeValueAsBytes(val1);    @SuppressWarnings("unchecked")    Class<T> clazz = (Class<T>) val1.getClass();    T val2 = JsonUtil.JSON_SERDE.readValue(bytes, clazz);    for (Field field : clazz.getDeclaredFields()) {        boolean wasAccessible = field.isAccessible();        field.setAccessible(true);        assertNotNull("Field " + field + " was null.", field.get(val2));        field.setAccessible(wasAccessible);    }}
f20658
0
build
public MiniTrogdorClusterf20666_1) throws Exception
{        TreeMap<String, NodeData> nodes = new TreeMap<>();    for (String agentName : agentNames) {        NodeData node = getOrCreate(agentName, nodes);        node.agentRestResource = new AgentRestResource();        node.agentRestServer = new JsonRestServer(0);        node.agentRestServer.start(node.agentRestResource);        node.agentPort = node.agentRestServer.port();    }    if (coordinatorName != null) {        NodeData node = getOrCreate(coordinatorName, nodes);        node.coordinatorRestResource = new CoordinatorRestResource();        node.coordinatorRestServer = new JsonRestServer(0);        node.coordinatorRestServer.start(node.coordinatorRestResource);        node.coordinatorPort = node.coordinatorRestServer.port();    }    for (Map.Entry<String, NodeData> entry : nodes.entrySet()) {        NodeData node = entry.getValue();        HashMap<String, String> config = new HashMap<>();        if (node.agentPort != 0) {            config.put(Platform.Config.TROGDOR_AGENT_PORT, Integer.toString(node.agentPort));        }        if (node.coordinatorPort != 0) {            config.put(Platform.Config.TROGDOR_COORDINATOR_PORT, Integer.toString(node.coordinatorPort));        }        node.node = new BasicNode(entry.getKey(), node.hostname, config, Collections.<String>emptySet());    }    TreeMap<String, Node> topologyNodes = new TreeMap<>();    for (Map.Entry<String, NodeData> entry : nodes.entrySet()) {        topologyNodes.put(entry.getKey(), entry.getValue().node);    }    final BasicTopology topology = new BasicTopology(topologyNodes);    ScheduledExecutorService executor = Executors.newScheduledThreadPool(1, ThreadUtils.createThreadFactory("MiniTrogdorClusterStartupThread%d", false));    final AtomicReference<Exception> failure = new AtomicReference<Exception>(null);    for (final Map.Entry<String, NodeData> entry : nodes.entrySet()) {        executor.submit((Callable<Void>) () -> {            String nodeName = entry.getKey();            try {                NodeData node = entry.getValue();                node.platform = new BasicPlatform(nodeName, topology, scheduler, commandRunner);                if (node.agentRestResource != null) {                    node.agent = new Agent(node.platform, scheduler, node.agentRestServer, node.agentRestResource);                }                if (node.coordinatorRestResource != null) {                    node.coordinator = new Coordinator(node.platform, scheduler, node.coordinatorRestServer, node.coordinatorRestResource, 0);                }            } catch (Exception e) {                                failure.compareAndSet(null, e);            }            return null;        });    }    executor.shutdown();    executor.awaitTermination(1, TimeUnit.DAYS);    Exception failureException = failure.get();    if (failureException != null) {        throw failureException;    }    TreeMap<String, Agent> agents = new TreeMap<>();    Coordinator coordinator = null;    for (Map.Entry<String, NodeData> entry : nodes.entrySet()) {        NodeData node = entry.getValue();        if (node.agent != null) {            agents.put(entry.getKey(), node.agent);        }        if (node.coordinator != null) {            coordinator = node.coordinator;        }    }    return new MiniTrogdorCluster(scheduler, agents, nodes, coordinator);}
public MiniTrogdorClusterf20666
1
agents
public TreeMap<String, Agent> kafkatest_f20667_0()
{    return agents;}
f20667
0
coordinator
public Coordinator kafkatest_f20668_0()
{    return coordinator;}
f20668
0
testDurationString
public void kafkatest_f20676_0()
{    assertEquals("1m", durationString(60000));    assertEquals("1m1s", durationString(61000));    assertEquals("1m1s", durationString(61200));    assertEquals("5s", durationString(5000));    assertEquals("2h", durationString(7200000));    assertEquals("2h1s", durationString(7201000));    assertEquals("2h5m3s", durationString(7503000));}
f20676
0
testPrettyPrintGrid
public void kafkatest_f20677_0()
{    assertEquals(String.format("ANIMAL  NUMBER INDEX %n" + "lion    1      12345 %n" + "manatee 50     1     %n"), StringFormatter.prettyPrintGrid(Arrays.asList(Arrays.asList("ANIMAL", "NUMBER", "INDEX"), Arrays.asList("lion", "1", "12345"), Arrays.asList("manatee", "50", "1"))));}
f20677
0
testAgentNodeNames
public void kafkatest_f20678_0() throws Exception
{    TreeMap<String, Node> nodes = new TreeMap<>();    final int numNodes = 5;    for (int i = 0; i < numNodes; i++) {        HashMap<String, String> conf = new HashMap<>();        if (i == 0) {            conf.put(Platform.Config.TROGDOR_COORDINATOR_PORT, String.valueOf(Coordinator.DEFAULT_PORT));        } else {            conf.put(Platform.Config.TROGDOR_AGENT_PORT, String.valueOf(Agent.DEFAULT_PORT));        }        BasicNode node = new BasicNode(String.format("node%02d", i), String.format("node%d.example.com", i), conf, new HashSet<String>());        nodes.put(node.name(), node);    }    Topology topology = new BasicTopology(nodes);    Set<String> names = Topology.Util.agentNodeNames(topology);    assertEquals(4, names.size());    for (int i = 1; i < numNodes - 1; i++) {        assertTrue(names.contains(String.format("node%02d", i)));    }}
f20678
0
testCreatesNotExistingTopics
public void kafkatest_f20686_0() throws Throwable
{    // should be no topics before the call    assertEquals(0, adminClient.listTopics().names().get().size());    WorkerUtils.createTopics(log, adminClient, Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC), false);    assertEquals(Collections.singleton(TEST_TOPIC), adminClient.listTopics().names().get());    assertEquals(new TopicDescription(TEST_TOPIC, false, Collections.singletonList(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList()))), adminClient.describeTopics(Collections.singleton(TEST_TOPIC)).values().get(TEST_TOPIC).get());}
f20686
0
testCreatesOneTopicVerifiesOneTopic
public void kafkatest_f20687_0() throws Throwable
{    final String existingTopic = "existing-topic";    List<TopicPartitionInfo> tpInfo = new ArrayList<>();    tpInfo.add(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList()));    tpInfo.add(new TopicPartitionInfo(1, broker2, singleReplica, Collections.<Node>emptyList()));    adminClient.addTopic(false, existingTopic, tpInfo, null);    Map<String, NewTopic> topics = new HashMap<>();    topics.put(existingTopic, new NewTopic(existingTopic, tpInfo.size(), TEST_REPLICATION_FACTOR));    topics.put(TEST_TOPIC, NEW_TEST_TOPIC);    WorkerUtils.createTopics(log, adminClient, topics, false);    assertEquals(Utils.mkSet(existingTopic, TEST_TOPIC), adminClient.listTopics().names().get());}
f20687
0
testCreateNonExistingTopicsWithZeroTopicsDoesNothing
public void kafkatest_f20688_0() throws Throwable
{    WorkerUtils.createTopics(log, adminClient, Collections.<String, NewTopic>emptyMap(), false);    assertEquals(0, adminClient.listTopics().names().get().size());}
f20688
0
testPrettyPrintTaskInfo
public void kafkatest_f20696_0()
{    assertEquals("Will start at 2019-01-08T07:05:59.85Z", CoordinatorClient.prettyPrintTaskInfo(new TaskPending(new NoOpTaskSpec(1546931159850L, 9000)), ZoneOffset.UTC));    assertEquals("Started 2009-07-07T01:45:59.85Z; will stop after 9s", CoordinatorClient.prettyPrintTaskInfo(new TaskRunning(new NoOpTaskSpec(1146931159850L, 9000), 1246931159850L, JsonNodeFactory.instance.objectNode()), ZoneOffset.UTC));    assertEquals("Started 2009-07-07T01:45:59.85Z", CoordinatorClient.prettyPrintTaskInfo(new TaskStopping(new NoOpTaskSpec(1146931159850L, 9000), 1246931159850L, JsonNodeFactory.instance.objectNode()), ZoneOffset.UTC));    assertEquals("FINISHED at 2019-01-08T20:59:29.85Z after 10s", CoordinatorClient.prettyPrintTaskInfo(new TaskDone(new NoOpTaskSpec(0, 1000), 1546981159850L, 1546981169850L, "", false, JsonNodeFactory.instance.objectNode()), ZoneOffset.UTC));    assertEquals("CANCELLED at 2019-01-08T20:59:29.85Z after 10s", CoordinatorClient.prettyPrintTaskInfo(new TaskDone(new NoOpTaskSpec(0, 1000), 1546981159850L, 1546981169850L, "", true, JsonNodeFactory.instance.objectNode()), ZoneOffset.UTC));    assertEquals("FAILED at 2019-01-08T20:59:29.85Z after 10s", CoordinatorClient.prettyPrintTaskInfo(new TaskDone(new NoOpTaskSpec(0, 1000), 1546981159850L, 1546981169850L, "foobar", true, JsonNodeFactory.instance.objectNode()), ZoneOffset.UTC));}
f20696
0
testCoordinatorStatus
public void kafkatest_f20697_0() throws Exception
{    try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().addCoordinator("node01").build()) {        CoordinatorStatusResponse status = cluster.coordinatorClient().status();        assertEquals(cluster.coordinator().status(), status);    }}
f20697
0
testCoordinatorUptime
public void kafkatest_f20698_0() throws Exception
{    MockTime time = new MockTime(0, 200, 0);    Scheduler scheduler = new MockScheduler(time);    try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().addCoordinator("node01").scheduler(scheduler).build()) {        UptimeResponse uptime = cluster.coordinatorClient().uptime();        assertEquals(cluster.coordinator().uptime(), uptime);        time.setCurrentTimeMs(250);        assertNotEquals(cluster.coordinator().uptime(), uptime);    }}
f20698
0
toString
public String kafkatest_f20706_0()
{    return Utils.join(expectedLines, ", ");}
f20706
0
createPartitionLists
private static List<List<String>> kafkatest_f20707_0(String[][] array)
{    List<List<String>> list = new ArrayList<>();    for (String[] a : array) {        list.add(Arrays.asList(a));    }    return list;}
f20707
0
testNetworkPartitionFault
public void kafkatest_f20708_0() throws Exception
{    CapturingCommandRunner runner = new CapturingCommandRunner();    MockTime time = new MockTime(0, 0, 0);    Scheduler scheduler = new MockScheduler(time);    try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().addCoordinator("node01").addAgent("node01").addAgent("node02").addAgent("node03").commandRunner(runner).scheduler(scheduler).build()) {        CoordinatorClient coordinatorClient = cluster.coordinatorClient();        NetworkPartitionFaultSpec spec = new NetworkPartitionFaultSpec(0, Long.MAX_VALUE, createPartitionLists(new String[][] { new String[] { "node01", "node02" }, new String[] { "node03" } }));        coordinatorClient.createTask(new CreateTaskRequest("netpart", spec));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("netpart").taskSpec(spec).build()).waitFor(coordinatorClient);        checkLines("-A", runner);    }    checkLines("-D", runner);}
f20708
0
testWorkersExitingAtDifferentTimes
public void kafkatest_f20716_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    Scheduler scheduler = new MockScheduler(time);    try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().addCoordinator("node01").addAgent("node02").addAgent("node03").scheduler(scheduler).build()) {        CoordinatorClient coordinatorClient = cluster.coordinatorClient();        new ExpectedTasks().waitFor(coordinatorClient);        HashMap<String, Long> nodeToExitMs = new HashMap<>();        nodeToExitMs.put("node02", 10L);        nodeToExitMs.put("node03", 20L);        SampleTaskSpec fooSpec = new SampleTaskSpec(2, 100, nodeToExitMs, "");        coordinatorClient.createTask(new CreateTaskRequest("foo", fooSpec));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").taskState(new TaskPending(fooSpec)).build()).waitFor(coordinatorClient);        time.sleep(2);        ObjectNode status1 = new ObjectNode(JsonNodeFactory.instance);        status1.set("node02", new TextNode("active"));        status1.set("node03", new TextNode("active"));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").taskState(new TaskRunning(fooSpec, 2, status1)).workerState(new WorkerRunning("foo", fooSpec, 2, new TextNode("active"))).build()).waitFor(coordinatorClient).waitFor(cluster.agentClient("node02")).waitFor(cluster.agentClient("node03"));        time.sleep(10);        ObjectNode status2 = new ObjectNode(JsonNodeFactory.instance);        status2.set("node02", new TextNode("halted"));        status2.set("node03", new TextNode("active"));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").taskState(new TaskRunning(fooSpec, 2, status2)).workerState(new WorkerRunning("foo", fooSpec, 2, new TextNode("active"))).build()).waitFor(coordinatorClient).waitFor(cluster.agentClient("node03"));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").taskState(new TaskRunning(fooSpec, 2, status2)).workerState(new WorkerDone("foo", fooSpec, 2, 12, new TextNode("halted"), "")).build()).waitFor(cluster.agentClient("node02"));        time.sleep(10);        ObjectNode status3 = new ObjectNode(JsonNodeFactory.instance);        status3.set("node02", new TextNode("halted"));        status3.set("node03", new TextNode("halted"));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").taskState(new TaskDone(fooSpec, 2, 22, "", false, status3)).build()).waitFor(coordinatorClient);    }}
f20716
0
testToResponseNotFound
public void kafkatest_f20717_0()
{    RestExceptionMapper mapper = new RestExceptionMapper();    Response resp = mapper.toResponse(new NotFoundException());    assertEquals(resp.getStatus(), Response.Status.NOT_FOUND.getStatusCode());}
f20717
0
testToResponseInvalidTypeIdException
public void kafkatest_f20718_0()
{    RestExceptionMapper mapper = new RestExceptionMapper();    JsonParser parser = null;    JavaType type = null;    Response resp = mapper.toResponse(InvalidTypeIdException.from(parser, "dummy msg", type, "dummy typeId"));    assertEquals(resp.getStatus(), Response.Status.NOT_IMPLEMENTED.getStatusCode());}
f20718
0
testToExceptionSerializationException
public void kafkatest_f20726_0() throws Exception
{    RestExceptionMapper.toException(Response.Status.BAD_REQUEST.getStatusCode(), "Bad Request");}
f20726
0
testToExceptionRuntimeException
public void kafkatest_f20727_0() throws Exception
{    RestExceptionMapper.toException(-1, "Unkown status code");}
f20727
0
targetNodes
public Set<String> kafkatest_f20728_0(Topology topology)
{    return Topology.Util.agentNodeNames(topology);}
f20728
0
testMaterializeTopicsWithNoPartitions
public void kafkatest_f20736_0()
{    Map<String, List<TopicPartition>> materializedTopics = consumeBenchSpec(Arrays.asList("topic[1-3]", "secondTopic")).materializeTopics();    Map<String, List<TopicPartition>> expected = new HashMap<>();    expected.put("topic1", new ArrayList<>());    expected.put("topic2", new ArrayList<>());    expected.put("topic3", new ArrayList<>());    expected.put("secondTopic", new ArrayList<>());    assertEquals(expected, materializedTopics);}
f20736
0
testMaterializeTopicsWithSomePartitions
public void kafkatest_f20737_0()
{    Map<String, List<TopicPartition>> materializedTopics = consumeBenchSpec(Arrays.asList("topic[1-3]:[1-5]", "secondTopic", "thirdTopic:1")).materializeTopics();    Map<String, List<TopicPartition>> expected = new HashMap<>();    expected.put("topic1", IntStream.range(1, 6).asLongStream().mapToObj(i -> new TopicPartition("topic1", (int) i)).collect(Collectors.toList()));    expected.put("topic2", IntStream.range(1, 6).asLongStream().mapToObj(i -> new TopicPartition("topic2", (int) i)).collect(Collectors.toList()));    expected.put("topic3", IntStream.range(1, 6).asLongStream().mapToObj(i -> new TopicPartition("topic3", (int) i)).collect(Collectors.toList()));    expected.put("secondTopic", new ArrayList<>());    expected.put("thirdTopic", Collections.singletonList(new TopicPartition("thirdTopic", 1)));    assertEquals(expected, materializedTopics);}
f20737
0
testInvalidTopicNameRaisesExceptionInMaterialize
public void kafkatest_f20738_0()
{    for (String invalidName : Arrays.asList("In:valid", "invalid:", ":invalid", "in:valid:1", "invalid:2:2", "invalid::1", "invalid[1-3]:")) {        try {            consumeBenchSpec(Collections.singletonList(invalidName)).materializeTopics();            fail(String.format("Invalid topic name (%s) should have raised an exception.", invalidName));        } catch (IllegalArgumentException ignored) {        }    }}
f20738
0
testProcessStop
public void kafkatest_f20746_0() throws Exception
{    if (OperatingSystem.IS_WINDOWS)        return;    ExternalCommandWorker worker = new ExternalCommandWorkerBuilder("testStopTask").command("sleep", "3600000").build();    KafkaFutureImpl<String> doneFuture = new KafkaFutureImpl<>();    worker.start(null, new AgentWorkerStatusTracker(), doneFuture);    worker.stop(null);    // We don't check the numeric return code, since that will vary based on    // platform.    assertTrue(doneFuture.get().startsWith("exited with return code "));}
f20746
0
testProcessForceKillTimeout
public void kafkatest_f20747_0() throws Exception
{    if (OperatingSystem.IS_WINDOWS)        return;    File tempFile = null;    try {        tempFile = TestUtils.tempFile();        try (OutputStream stream = Files.newOutputStream(tempFile.toPath())) {            for (String line : new String[] { "echo hello world\n", "# Test that the initial message is sent correctly.\n", "read -r line\n", "[[ $line == '{\"id\":\"testForceKillTask\",\"workload\":{\"foo\":\"value1\",\"bar\":123}}' ]] || exit 0\n", "\n", "# Ignore SIGTERM signals.  This ensures that we test SIGKILL delivery.\n", "trap 'echo SIGTERM' SIGTERM\n", "\n", "# Update the process status.  This will also unblock the junit test.\n", "# It is important that we do this after we disabled SIGTERM, to ensure\n", "# that we are testing SIGKILL.\n", "echo '{\"status\": \"green\", \"log\": \"my log message.\"}'\n", "\n", "# Wait for the SIGKILL.\n", "while true; do sleep 0.01; done\n" }) {                stream.write(line.getBytes(StandardCharsets.UTF_8));            }        }        CompletableFuture<String> statusFuture = new CompletableFuture<>();        final WorkerStatusTracker statusTracker = status -> statusFuture.complete(status.textValue());        ExternalCommandWorker worker = new ExternalCommandWorkerBuilder("testForceKillTask").shutdownGracePeriodMs(1).command("bash", tempFile.getAbsolutePath()).build();        KafkaFutureImpl<String> doneFuture = new KafkaFutureImpl<>();        worker.start(null, statusTracker, doneFuture);        assertEquals("green", statusFuture.get());        worker.stop(null);        assertTrue(doneFuture.get().startsWith("exited with return code "));    } finally {        if (tempFile != null) {            Files.delete(tempFile.toPath());        }    }}
f20747
0
createHistogram
private static Histogram kafkatest_f20748_0(int maxValue, int... values)
{    Histogram histogram = new Histogram(maxValue);    for (int value : values) {        histogram.add(value);    }    return histogram;}
f20748
0
testUniformRandomPayloadGenerator
public void kafkatest_f20756_0()
{    PayloadIterator iter = new PayloadIterator(new UniformRandomPayloadGenerator(1234, 456, 0));    byte[] prev = iter.next();    for (int uniques = 0; uniques < 1000; ) {        byte[] cur = iter.next();        assertEquals(prev.length, cur.length);        if (!Arrays.equals(prev, cur)) {            uniques++;        }    }    testReproducible(new UniformRandomPayloadGenerator(1234, 456, 0));    testReproducible(new UniformRandomPayloadGenerator(1, 0, 0));    testReproducible(new UniformRandomPayloadGenerator(10, 6, 5));    testReproducible(new UniformRandomPayloadGenerator(512, 123, 100));}
f20756
0
testReproducible
private static void kafkatest_f20757_0(PayloadGenerator generator)
{    byte[] val = generator.generate(123);    generator.generate(456);    byte[] val2 = generator.generate(123);    if (val == null) {        assertNull(val2);    } else {        assertArrayEquals(val, val2);    }}
f20757
0
testUniformRandomPayloadGeneratorPaddingBytes
public void kafkatest_f20758_0()
{    UniformRandomPayloadGenerator generator = new UniformRandomPayloadGenerator(1000, 456, 100);    byte[] val1 = generator.generate(0);    byte[] val1End = new byte[100];    System.arraycopy(val1, 900, val1End, 0, 100);    byte[] val2 = generator.generate(100);    byte[] val2End = new byte[100];    System.arraycopy(val2, 900, val2End, 0, 100);    byte[] val3 = generator.generate(200);    byte[] val3End = new byte[100];    System.arraycopy(val3, 900, val3End, 0, 100);    assertArrayEquals(val1End, val2End);    assertArrayEquals(val1End, val3End);}
f20758
0
testCommitsTransactionAfterIntervalPasses
public void kafkatest_f20766_0()
{    MockTime time = new MockTime();    TimeIntervalTransactionsGenerator generator = new TimeIntervalTransactionsGenerator(100, time);    assertEquals(100, generator.transactionIntervalMs());    assertEquals(TransactionGenerator.TransactionAction.BEGIN_TRANSACTION, generator.nextAction());    assertEquals(TransactionGenerator.TransactionAction.NO_OP, generator.nextAction());    time.sleep(50);    assertEquals(TransactionGenerator.TransactionAction.NO_OP, generator.nextAction());    time.sleep(49);    assertEquals(TransactionGenerator.TransactionAction.NO_OP, generator.nextAction());    time.sleep(1);    assertEquals(TransactionGenerator.TransactionAction.COMMIT_TRANSACTION, generator.nextAction());    assertEquals(TransactionGenerator.TransactionAction.BEGIN_TRANSACTION, generator.nextAction());}
f20766
0
testMaterialize
public void kafkatest_f20767_0()
{    Map<String, PartitionsSpec> parts = FOO.materialize();    assertTrue(parts.containsKey("topicA0"));    assertTrue(parts.containsKey("topicA1"));    assertTrue(parts.containsKey("topicA2"));    assertTrue(parts.containsKey("topicB"));    assertEquals(4, parts.keySet().size());    assertEquals(PARTSA, parts.get("topicA0"));    assertEquals(PARTSA, parts.get("topicA1"));    assertEquals(PARTSA, parts.get("topicA2"));    assertEquals(PARTSB, parts.get("topicB"));}
f20767
0
testPartitionNumbers
public void kafkatest_f20768_0()
{    List<Integer> partsANumbers = PARTSA.partitionNumbers();    assertEquals(Integer.valueOf(0), partsANumbers.get(0));    assertEquals(Integer.valueOf(1), partsANumbers.get(1));    assertEquals(Integer.valueOf(2), partsANumbers.get(2));    assertEquals(3, partsANumbers.size());    List<Integer> partsBNumbers = PARTSB.partitionNumbers();    assertEquals(Integer.valueOf(0), partsBNumbers.get(0));    assertEquals(Integer.valueOf(1), partsBNumbers.get(1));    assertEquals(2, partsBNumbers.size());}
f20768
0
