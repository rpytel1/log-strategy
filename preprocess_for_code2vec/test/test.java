 void kafkatest_f5_0(long duration, TimeUnit unit)
{    close(Duration.ofMillis(unit.toMillis(duration)));}
 CreateTopicsResult kafkatest_f6_0(Collection<NewTopic> newTopics)
{    return createTopics(newTopics, new CreateTopicsOptions());}
 AlterConfigsResult kafkatest_f15_0(Map<ConfigResource, Config> configs)
{    return alterConfigs(configs, new AlterConfigsOptions());}
 AlterConfigsResult kafkatest_f16_0(Map<ConfigResource, Collection<AlterConfigOp>> configs)
{    return incrementalAlterConfigs(configs, new AlterConfigsOptions());}
 DescribeDelegationTokenResult kafkatest_f25_0()
{    return describeDelegationToken(new DescribeDelegationTokenOptions());}
 DescribeConsumerGroupsResult kafkatest_f26_0(Collection<String> groupIds)
{    return describeConsumerGroups(groupIds, new DescribeConsumerGroupsOptions());}
 ListPartitionReassignmentsResult kafkatest_f35_0()
{    return listPartitionReassignments(new ListPartitionReassignmentsOptions());}
 ListPartitionReassignmentsResult kafkatest_f36_0(Set<TopicPartition> partitions)
{    return listPartitionReassignments(partitions, new ListPartitionReassignmentsOptions());}
public byte kafkatest_f45_0()
{    return id;}
public static OpType kafkatest_f46_0(final byte id)
{    return OP_TYPES.get(id);}
public Map<ConfigResource, KafkaFuture<Void>> kafkatest_f55_0()
{    return futures;}
public KafkaFuture<Void> kafkatest_f56_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0]));}
public String kafkatest_f65_0()
{    return "Config(entries=" + entries.values() + ")";}
public String kafkatest_f66_0()
{    return name;}
public String kafkatest_f75_0()
{    return "ConfigEntry(" + "name=" + name + ", value=" + value + ", source=" + source + ", isSensitive=" + isSensitive + ", isReadOnly=" + isReadOnly + ", synonyms=" + synonyms + ")";}
public String kafkatest_f76_0()
{    return name;}
public boolean kafkatest_f85_0()
{    return isSimpleConsumerGroup;}
public Collection<MemberDescription> kafkatest_f86_0()
{    return members;}
public CreateAclsOptions kafkatest_f95_0(Integer timeoutMs)
{    this.timeoutMs = timeoutMs;    return this;}
public Map<AclBinding, KafkaFuture<Void>> kafkatest_f96_0()
{    return futures;}
public Map<String, KafkaFuture<Void>> kafkatest_f105_0()
{    return values;}
public KafkaFuture<Void> kafkatest_f106_0()
{    return KafkaFuture.allOf(values.values().toArray(new KafkaFuture[0]));}
public List<FilterResult> kafkatest_f115_0()
{    return values;}
public Map<AclBindingFilter, KafkaFuture<FilterResults>> kafkatest_f116_0()
{    return futures;}
public long kafkatest_f125_0()
{    return lowWatermark;}
public Map<TopicPartition, KafkaFuture<DeletedRecords>> kafkatest_f126_0()
{    return futures;}
public boolean kafkatest_f135_0()
{    return includeAuthorizedOperations;}
public KafkaFuture<Collection<Node>> kafkatest_f136_0()
{    return nodes;}
public Map<ConfigResource, Config> kafkatest_f145_0(Void v)
{    Map<ConfigResource, Config> configs = new HashMap<>(futures.size());    for (Map.Entry<ConfigResource, KafkaFuture<Config>> entry : futures.entrySet()) {        try {            configs.put(entry.getKey(), entry.getValue().get());        } catch (InterruptedException | ExecutionException e) {            // completed successfully.            throw new RuntimeException(e);        }    }    return configs;}
public DescribeConsumerGroupsOptions kafkatest_f146_0(boolean includeAuthorizedOperations)
{    this.includeAuthorizedOperations = includeAuthorizedOperations;    return this;}
public KafkaFuture<Map<Integer, Map<String, LogDirInfo>>> kafkatest_f155_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0])).thenApply(new KafkaFuture.BaseFunction<Void, Map<Integer, Map<String, LogDirInfo>>>() {        @Override        public Map<Integer, Map<String, LogDirInfo>> apply(Void v) {            Map<Integer, Map<String, LogDirInfo>> descriptions = new HashMap<>(futures.size());            for (Map.Entry<Integer, KafkaFuture<Map<String, LogDirInfo>>> entry : futures.entrySet()) {                try {                    descriptions.put(entry.getKey(), entry.getValue().get());                } catch (InterruptedException | ExecutionException e) {                    // This should be unreachable, because allOf ensured that all the futures completed successfully.                    throw new RuntimeException(e);                }            }            return descriptions;        }    });}
public Map<Integer, Map<String, LogDirInfo>> kafkatest_f156_0(Void v)
{    Map<Integer, Map<String, LogDirInfo>> descriptions = new HashMap<>(futures.size());    for (Map.Entry<Integer, KafkaFuture<Map<String, LogDirInfo>>> entry : futures.entrySet()) {        try {            descriptions.put(entry.getKey(), entry.getValue().get());        } catch (InterruptedException | ExecutionException e) {            // This should be unreachable, because allOf ensured that all the futures completed successfully.            throw new RuntimeException(e);        }    }    return descriptions;}
public DescribeTopicsOptions kafkatest_f165_0(Integer timeoutMs)
{    this.timeoutMs = timeoutMs;    return this;}
public DescribeTopicsOptions kafkatest_f166_0(boolean includeAuthorizedOperations)
{    this.includeAuthorizedOperations = includeAuthorizedOperations;    return this;}
public void kafkatest_f175_0(Map<TopicPartition, Optional<Throwable>> topicPartitions, Throwable throwable)
{    if (throwable != null) {        result.completeExceptionally(throwable);    } else if (!topicPartitions.containsKey(partition)) {        result.completeExceptionally(new UnknownTopicOrPartitionException("Preferred leader election for partition \"" + partition + "\" was not attempted"));    } else {        Optional<Throwable> exception = topicPartitions.get(partition);        if (exception.isPresent()) {            result.completeExceptionally(exception.get());        } else {            result.complete(null);        }    }}
public KafkaFuture<Set<TopicPartition>> kafkatest_f176_0()
{    final KafkaFutureImpl<Set<TopicPartition>> result = new KafkaFutureImpl<>();    electionResult.partitions().whenComplete(new KafkaFuture.BiConsumer<Map<TopicPartition, Optional<Throwable>>, Throwable>() {        @Override        public void accept(Map<TopicPartition, Optional<Throwable>> topicPartitions, Throwable throwable) {            if (throwable != null) {                result.completeExceptionally(throwable);            } else {                result.complete(topicPartitions.keySet());            }        }    });    return result;}
public void kafkatest_f185_0(String destination)
{// Do nothing}
public void kafkatest_f186_0(KafkaException e)
{    updateFailed(e);}
public long kafkatest_f196_0(long now)
{    switch(state) {        case QUIESCENT:            // so there is a metadata refresh backoff period.            return Math.max(delayBeforeNextAttemptMs(now), delayBeforeNextExpireMs(now));        case UPDATE_REQUESTED:            // Respect the backoff, even if an update has been requested            return delayBeforeNextAttemptMs(now);        default:            // An update is already pending, so we don't need to initiate another one.            return Long.MAX_VALUE;    }}
private long kafkatest_f197_0(long now)
{    long timeSinceUpdate = now - lastMetadataUpdateMs;    return Math.max(0, metadataExpireMs - timeSinceUpdate);}
private long kafkatest_f206_0(long now, Integer optionTimeoutMs)
{    if (optionTimeoutMs != null)        return now + Math.max(0, optionTimeoutMs);    return now + defaultTimeoutMs;}
 static String kafkatest_f207_0(Throwable throwable)
{    if (throwable == null)        return "Null exception.";    if (throwable.getMessage() != null) {        return throwable.getClass().getSimpleName() + ": " + throwable.getMessage();    }    return throwable.getClass().getSimpleName();}
public Node kafkatest_f216_0()
{    if (metadataManager.isReady()) {        // In that case, we will postpone node assignment.        return client.leastLoadedNode(time.milliseconds());    }    metadataManager.requestUpdate();    return null;}
protected Node kafkatest_f217_0()
{    return curNode;}
private voidf226_1TimeoutProcessor processor)
{    int numTimedOut = processor.handleTimeouts(pendingCalls, "Timed out waiting for a node assignment.");    if (numTimedOut > 0)        }
private intf227_1TimeoutProcessor processor)
{    int numTimedOut = 0;    for (List<Call> callList : callsToSend.values()) {        numTimedOut += processor.handleTimeouts(callList, "Timed out waiting to send the call.");    }    if (numTimedOut > 0)            return numTimedOut;}
private boolean kafkatest_f236_0()
{    if (hasActiveExternalCalls(pendingCalls)) {        return true;    }    for (List<Call> callList : callsToSend.values()) {        if (hasActiveExternalCalls(callList)) {            return true;        }    }    return hasActiveExternalCalls(correlationIdToCalls.values());}
private booleanf237_1long now, long curHardShutdownTimeMs)
{    if (!hasActiveExternalCalls()) {        log.trace("All work has been completed, and the I/O thread is now exiting.");        return true;    }    if (now >= curHardShutdownTimeMs) {                return true;    }        return false;}
private static boolean kafkatest_f246_0(String groupId)
{    return groupId == null;}
 int kafkatest_f247_0()
{    return runnable.pendingCalls.size();}
public ListTopicsResult kafkatest_f256_0(final ListTopicsOptions options)
{    final KafkaFutureImpl<Map<String, TopicListing>> topicListingFuture = new KafkaFutureImpl<>();    final long now = time.milliseconds();    runnable.call(new Call("listTopics", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return MetadataRequest.Builder.allTopics();        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            MetadataResponse response = (MetadataResponse) abstractResponse;            Map<String, TopicListing> topicListing = new HashMap<>();            for (MetadataResponse.TopicMetadata topicMetadata : response.topicMetadata()) {                String topicName = topicMetadata.topic();                boolean isInternal = topicMetadata.isInternal();                if (!topicMetadata.isInternal() || options.shouldListInternal())                    topicListing.put(topicName, new TopicListing(topicName, isInternal));            }            topicListingFuture.complete(topicListing);        }        @Override        void handleFailure(Throwable throwable) {            topicListingFuture.completeExceptionally(throwable);        }    }, now);    return new ListTopicsResult(topicListingFuture);}
 AbstractRequest.Builder kafkatest_f257_0(int timeoutMs)
{    return MetadataRequest.Builder.allTopics();}
public DescribeClusterResult kafkatest_f266_0(DescribeClusterOptions options)
{    final KafkaFutureImpl<Collection<Node>> describeClusterFuture = new KafkaFutureImpl<>();    final KafkaFutureImpl<Node> controllerFuture = new KafkaFutureImpl<>();    final KafkaFutureImpl<String> clusterIdFuture = new KafkaFutureImpl<>();    final KafkaFutureImpl<Set<AclOperation>> authorizedOperationsFuture = new KafkaFutureImpl<>();    final long now = time.milliseconds();    runnable.call(new Call("listNodes", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            // simplifies communication with older brokers)            return new MetadataRequest.Builder(new MetadataRequestData().setTopics(Collections.emptyList()).setAllowAutoTopicCreation(true).setIncludeClusterAuthorizedOperations(options.includeAuthorizedOperations()));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            MetadataResponse response = (MetadataResponse) abstractResponse;            describeClusterFuture.complete(response.brokers());            controllerFuture.complete(controller(response));            clusterIdFuture.complete(response.clusterId());            authorizedOperationsFuture.complete(validAclOperations(response.clusterAuthorizedOperations()));        }        private Node controller(MetadataResponse response) {            if (response.controller() == null || response.controller().id() == MetadataResponse.NO_CONTROLLER_ID)                return null;            return response.controller();        }        @Override        void handleFailure(Throwable throwable) {            describeClusterFuture.completeExceptionally(throwable);            controllerFuture.completeExceptionally(throwable);            clusterIdFuture.completeExceptionally(throwable);            authorizedOperationsFuture.completeExceptionally(throwable);        }    }, now);    return new DescribeClusterResult(describeClusterFuture, controllerFuture, clusterIdFuture, authorizedOperationsFuture);}
 AbstractRequest.Builder kafkatest_f267_0(int timeoutMs)
{    // simplifies communication with older brokers)    return new MetadataRequest.Builder(new MetadataRequestData().setTopics(Collections.emptyList()).setAllowAutoTopicCreation(true).setIncludeClusterAuthorizedOperations(options.includeAuthorizedOperations()));}
 AbstractRequest.Builder kafkatest_f276_0(int timeoutMs)
{    return new CreateAclsRequest.Builder(aclCreations);}
 void kafkatest_f277_0(AbstractResponse abstractResponse)
{    CreateAclsResponse response = (CreateAclsResponse) abstractResponse;    List<AclCreationResponse> responses = response.aclCreationResponses();    Iterator<AclCreationResponse> iter = responses.iterator();    for (AclCreation aclCreation : aclCreations) {        KafkaFutureImpl<Void> future = futures.get(aclCreation.acl());        if (!iter.hasNext()) {            future.completeExceptionally(new UnknownServerException("The broker reported no creation result for the given ACL."));        } else {            AclCreationResponse creation = iter.next();            if (creation.error().isFailure()) {                future.completeExceptionally(creation.error().exception());            } else {                future.complete(null);            }        }    }}
 void kafkatest_f286_0(Throwable throwable)
{    completeAllExceptionally(unifiedRequestFutures.values(), throwable);}
 AbstractRequest.Builder kafkatest_f287_0(int timeoutMs)
{    return new DescribeConfigsRequest.Builder(Collections.singleton(resource)).includeSynonyms(options.includeSynonyms());}
 void kafkatest_f296_0(Throwable throwable)
{    completeAllExceptionally(futures.values(), throwable);}
public AlterConfigsResult kafkatest_f297_0(Map<ConfigResource, Collection<AlterConfigOp>> configs, final AlterConfigsOptions options)
{    final Map<ConfigResource, KafkaFutureImpl<Void>> allFutures = new HashMap<>();    // We must make a separate AlterConfigs request for every BROKER resource we want to alter    // and send the request to that specific broker. Other resources are grouped together into    // a single request that may be sent to any broker.    final Collection<ConfigResource> unifiedRequestResources = new ArrayList<>();    for (ConfigResource resource : configs.keySet()) {        if (dependsOnSpecificNode(resource)) {            NodeProvider nodeProvider = new ConstantNodeIdProvider(Integer.parseInt(resource.name()));            allFutures.putAll(incrementalAlterConfigs(configs, options, Collections.singleton(resource), nodeProvider));        } else            unifiedRequestResources.add(resource);    }    if (!unifiedRequestResources.isEmpty())        allFutures.putAll(incrementalAlterConfigs(configs, options, unifiedRequestResources, new LeastLoadedNodeProvider()));    return new AlterConfigsResult(new HashMap<>(allFutures));}
 void kafkatest_f306_0(Throwable throwable)
{    completeAllExceptionally(futures.values(), throwable);}
public DescribeLogDirsResult kafkatest_f307_0(Collection<Integer> brokers, DescribeLogDirsOptions options)
{    final Map<Integer, KafkaFutureImpl<Map<String, DescribeLogDirsResponse.LogDirInfo>>> futures = new HashMap<>(brokers.size());    for (Integer brokerId : brokers) {        futures.put(brokerId, new KafkaFutureImpl<>());    }    final long now = time.milliseconds();    for (final Integer brokerId : brokers) {        runnable.call(new Call("describeLogDirs", calcDeadlineMs(now, options.timeoutMs()), new ConstantNodeIdProvider(brokerId)) {            @Override            public AbstractRequest.Builder createRequest(int timeoutMs) {                // Query selected partitions in all log directories                return new DescribeLogDirsRequest.Builder(null);            }            @Override            public void handleResponse(AbstractResponse abstractResponse) {                DescribeLogDirsResponse response = (DescribeLogDirsResponse) abstractResponse;                KafkaFutureImpl<Map<String, DescribeLogDirsResponse.LogDirInfo>> future = futures.get(brokerId);                if (response.logDirInfos().size() > 0) {                    future.complete(response.logDirInfos());                } else {                    // response.logDirInfos() will be empty if and only if the user is not authorized to describe clsuter resource.                    future.completeExceptionally(Errors.CLUSTER_AUTHORIZATION_FAILED.exception());                }            }            @Override            void handleFailure(Throwable throwable) {                completeAllExceptionally(futures.values(), throwable);            }        }, now);    }    return new DescribeLogDirsResult(new HashMap<>(futures));}
public AbstractRequest.Builder kafkatest_f316_0(int timeoutMs)
{    return new CreatePartitionsRequest.Builder(requestMap, timeoutMs, options.validateOnly());}
public void kafkatest_f317_0(AbstractResponse abstractResponse)
{    CreatePartitionsResponse response = (CreatePartitionsResponse) abstractResponse;    // Check for controller change    for (ApiError error : response.errors().values()) {        if (error.error() == Errors.NOT_CONTROLLER) {            metadataManager.clearController();            metadataManager.requestUpdate();            throw error.exception();        }    }    for (Map.Entry<String, ApiError> result : response.errors().entrySet()) {        KafkaFutureImpl<Void> future = futures.get(result.getKey());        if (result.getValue().isSuccess()) {            future.complete(null);        } else {            future.completeExceptionally(result.getValue().exception());        }    }}
public CreateDelegationTokenResult kafkatest_f326_0(final CreateDelegationTokenOptions options)
{    final KafkaFutureImpl<DelegationToken> delegationTokenFuture = new KafkaFutureImpl<>();    final long now = time.milliseconds();    List<CreatableRenewers> renewers = new ArrayList<>();    for (KafkaPrincipal principal : options.renewers()) {        renewers.add(new CreatableRenewers().setPrincipalName(principal.getName()).setPrincipalType(principal.getPrincipalType()));    }    runnable.call(new Call("createDelegationToken", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder<CreateDelegationTokenRequest> createRequest(int timeoutMs) {            return new CreateDelegationTokenRequest.Builder(new CreateDelegationTokenRequestData().setRenewers(renewers).setMaxLifetimeMs(options.maxlifeTimeMs()));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            CreateDelegationTokenResponse response = (CreateDelegationTokenResponse) abstractResponse;            if (response.hasError()) {                delegationTokenFuture.completeExceptionally(response.error().exception());            } else {                CreateDelegationTokenResponseData data = response.data();                TokenInformation tokenInfo = new TokenInformation(data.tokenId(), new KafkaPrincipal(data.principalType(), data.principalName()), options.renewers(), data.issueTimestampMs(), data.maxTimestampMs(), data.expiryTimestampMs());                DelegationToken token = new DelegationToken(tokenInfo, data.hmac());                delegationTokenFuture.complete(token);            }        }        @Override        void handleFailure(Throwable throwable) {            delegationTokenFuture.completeExceptionally(throwable);        }    }, now);    return new CreateDelegationTokenResult(delegationTokenFuture);}
 AbstractRequest.Builder<CreateDelegationTokenRequest> kafkatest_f327_0(int timeoutMs)
{    return new CreateDelegationTokenRequest.Builder(new CreateDelegationTokenRequestData().setRenewers(renewers).setMaxLifetimeMs(options.maxlifeTimeMs()));}
 void kafkatest_f336_0(AbstractResponse abstractResponse)
{    ExpireDelegationTokenResponse response = (ExpireDelegationTokenResponse) abstractResponse;    if (response.hasError()) {        expiryTimeFuture.completeExceptionally(response.error().exception());    } else {        expiryTimeFuture.complete(response.expiryTimestamp());    }}
 void kafkatest_f337_0(Throwable throwable)
{    expiryTimeFuture.completeExceptionally(throwable);}
public Optional<Node> kafkatest_f346_0()
{    return node;}
public void kafkatest_f347_0(Node node)
{    this.node = Optional.ofNullable(node);}
private Call kafkatest_f356_0(ConsumerGroupOperationContext<ConsumerGroupDescription, DescribeConsumerGroupsOptions> context)
{    return new Call("describeConsumerGroups", context.getDeadline(), new ConstantNodeIdProvider(context.getNode().get().id())) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new DescribeGroupsRequest.Builder(new DescribeGroupsRequestData().setGroups(Collections.singletonList(context.getGroupId())).setIncludeAuthorizedOperations(context.getOptions().includeAuthorizedOperations()));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            final DescribeGroupsResponse response = (DescribeGroupsResponse) abstractResponse;            List<DescribedGroup> describedGroups = response.data().groups();            if (describedGroups.isEmpty()) {                context.getFuture().completeExceptionally(new InvalidGroupIdException("No consumer group found for GroupId: " + context.getGroupId()));                return;            }            if (describedGroups.size() > 1 || !describedGroups.get(0).groupId().equals(context.getGroupId())) {                String ids = Arrays.toString(describedGroups.stream().map(DescribedGroup::groupId).toArray());                context.getFuture().completeExceptionally(new InvalidGroupIdException("DescribeConsumerGroup request for GroupId: " + context.getGroupId() + " returned " + ids));                return;            }            final DescribedGroup describedGroup = describedGroups.get(0);            // If coordinator changed since we fetched it, retry            if (context.hasCoordinatorMoved(response)) {                rescheduleTask(context, () -> getDescribeConsumerGroupsCall(context));                return;            }            final Errors groupError = Errors.forCode(describedGroup.errorCode());            if (handleGroupRequestError(groupError, context.getFuture()))                return;            final String protocolType = describedGroup.protocolType();            if (protocolType.equals(ConsumerProtocol.PROTOCOL_TYPE) || protocolType.isEmpty()) {                final List<DescribedGroupMember> members = describedGroup.members();                final List<MemberDescription> memberDescriptions = new ArrayList<>(members.size());                final Set<AclOperation> authorizedOperations = validAclOperations(describedGroup.authorizedOperations());                for (DescribedGroupMember groupMember : members) {                    Set<TopicPartition> partitions = Collections.emptySet();                    if (groupMember.memberAssignment().length > 0) {                        final Assignment assignment = ConsumerProtocol.deserializeAssignment(ByteBuffer.wrap(groupMember.memberAssignment()));                        partitions = new HashSet<>(assignment.partitions());                    }                    final MemberDescription memberDescription = new MemberDescription(groupMember.memberId(), Optional.ofNullable(groupMember.groupInstanceId()), groupMember.clientId(), groupMember.clientHost(), new MemberAssignment(partitions));                    memberDescriptions.add(memberDescription);                }                final ConsumerGroupDescription consumerGroupDescription = new ConsumerGroupDescription(context.getGroupId(), protocolType.isEmpty(), memberDescriptions, describedGroup.protocolData(), ConsumerGroupState.parse(describedGroup.groupState()), context.getNode().get(), authorizedOperations);                context.getFuture().complete(consumerGroupDescription);            }        }        @Override        void handleFailure(Throwable throwable) {            context.getFuture().completeExceptionally(throwable);        }    };}
 AbstractRequest.Builder kafkatest_f357_0(int timeoutMs)
{    return new DescribeGroupsRequest.Builder(new DescribeGroupsRequestData().setGroups(Collections.singletonList(context.getGroupId())).setIncludeAuthorizedOperations(context.getOptions().includeAuthorizedOperations()));}
private synchronized void kafkatest_f366_0()
{    if (remaining.isEmpty()) {        ArrayList<Object> results = new ArrayList<>(listings.values());        results.addAll(errors);        future.complete(results);    }}
public ListConsumerGroupsResult kafkatest_f367_0(ListConsumerGroupsOptions options)
{    final KafkaFutureImpl<Collection<Object>> all = new KafkaFutureImpl<>();    final long nowMetadata = time.milliseconds();    final long deadline = calcDeadlineMs(nowMetadata, options.timeoutMs());    runnable.call(new Call("findAllBrokers", deadline, new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new MetadataRequest.Builder(new MetadataRequestData().setTopics(Collections.emptyList()).setAllowAutoTopicCreation(true));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            MetadataResponse metadataResponse = (MetadataResponse) abstractResponse;            Collection<Node> nodes = metadataResponse.brokers();            if (nodes.isEmpty())                throw new StaleMetadataException("Metadata fetch failed due to missing broker list");            HashSet<Node> allNodes = new HashSet<>(nodes);            final ListConsumerGroupsResults results = new ListConsumerGroupsResults(allNodes, all);            for (final Node node : allNodes) {                final long nowList = time.milliseconds();                runnable.call(new Call("listConsumerGroups", deadline, new ConstantNodeIdProvider(node.id())) {                    @Override                    AbstractRequest.Builder createRequest(int timeoutMs) {                        return new ListGroupsRequest.Builder(new ListGroupsRequestData());                    }                    private void maybeAddConsumerGroup(ListGroupsResponseData.ListedGroup group) {                        String protocolType = group.protocolType();                        if (protocolType.equals(ConsumerProtocol.PROTOCOL_TYPE) || protocolType.isEmpty()) {                            final String groupId = group.groupId();                            final ConsumerGroupListing groupListing = new ConsumerGroupListing(groupId, protocolType.isEmpty());                            results.addListing(groupListing);                        }                    }                    @Override                    void handleResponse(AbstractResponse abstractResponse) {                        final ListGroupsResponse response = (ListGroupsResponse) abstractResponse;                        synchronized (results) {                            Errors error = Errors.forCode(response.data().errorCode());                            if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.COORDINATOR_NOT_AVAILABLE) {                                throw error.exception();                            } else if (error != Errors.NONE) {                                results.addError(error.exception(), node);                            } else {                                for (ListGroupsResponseData.ListedGroup group : response.data().groups()) {                                    maybeAddConsumerGroup(group);                                }                            }                            results.tryComplete(node);                        }                    }                    @Override                    void handleFailure(Throwable throwable) {                        synchronized (results) {                            results.addError(throwable, node);                            results.tryComplete(node);                        }                    }                }, nowList);            }        }        @Override        void handleFailure(Throwable throwable) {            KafkaException exception = new KafkaException("Failed to find brokers to send ListGroups", throwable);            all.complete(Collections.singletonList(exception));        }    }, nowMetadata);    return new ListConsumerGroupsResult(all);}
private Callf376_1ConsumerGroupOperationContext<Map<TopicPartition, OffsetAndMetadata>, ListConsumerGroupOffsetsOptions> context)
{    return new Call("listConsumerGroupOffsets", context.getDeadline(), new ConstantNodeIdProvider(context.getNode().get().id())) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new OffsetFetchRequest.Builder(context.getGroupId(), context.getOptions().topicPartitions());        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            final OffsetFetchResponse response = (OffsetFetchResponse) abstractResponse;            final Map<TopicPartition, OffsetAndMetadata> groupOffsetsListing = new HashMap<>();            // If coordinator changed since we fetched it, retry            if (context.hasCoordinatorMoved(response)) {                rescheduleTask(context, () -> getListConsumerGroupOffsetsCall(context));                return;            }            if (handleGroupRequestError(response.error(), context.getFuture()))                return;            for (Map.Entry<TopicPartition, OffsetFetchResponse.PartitionData> entry : response.responseData().entrySet()) {                final TopicPartition topicPartition = entry.getKey();                OffsetFetchResponse.PartitionData partitionData = entry.getValue();                final Errors error = partitionData.error;                if (error == Errors.NONE) {                    final Long offset = partitionData.offset;                    final String metadata = partitionData.metadata;                    final Optional<Integer> leaderEpoch = partitionData.leaderEpoch;                    groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, leaderEpoch, metadata));                } else {                                    }            }            context.getFuture().complete(groupOffsetsListing);        }        @Override        void handleFailure(Throwable throwable) {            context.getFuture().completeExceptionally(throwable);        }    };}
 AbstractRequest.Builder kafkatest_f377_0(int timeoutMs)
{    return new OffsetFetchRequest.Builder(context.getGroupId(), context.getOptions().topicPartitions());}
private Call kafkatest_f386_0(ConsumerGroupOperationContext<Map<TopicPartition, Errors>, DeleteConsumerGroupOffsetsOptions> context, Set<TopicPartition> partitions)
{    return new Call("deleteConsumerGroupOffsets", context.getDeadline(), new ConstantNodeIdProvider(context.getNode().get().id())) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            final OffsetDeleteRequestTopicCollection topics = new OffsetDeleteRequestTopicCollection();            partitions.stream().collect(Collectors.groupingBy(TopicPartition::topic)).forEach((topic, topicPartitions) -> {                topics.add(new OffsetDeleteRequestTopic().setName(topic).setPartitions(topicPartitions.stream().map(tp -> new OffsetDeleteRequestPartition().setPartitionIndex(tp.partition())).collect(Collectors.toList())));            });            return new OffsetDeleteRequest.Builder(new OffsetDeleteRequestData().setGroupId(context.groupId).setTopics(topics));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            final OffsetDeleteResponse response = (OffsetDeleteResponse) abstractResponse;            // If coordinator changed since we fetched it, retry            if (context.hasCoordinatorMoved(response)) {                rescheduleTask(context, () -> getDeleteConsumerGroupOffsetsCall(context, partitions));                return;            }            // If the error is an error at the group level, the future is failed with it            final Errors groupError = Errors.forCode(response.data.errorCode());            if (handleGroupRequestError(groupError, context.getFuture()))                return;            final Map<TopicPartition, Errors> partitions = new HashMap<>();            response.data.topics().forEach(topic -> {                topic.partitions().forEach(partition -> {                    partitions.put(new TopicPartition(topic.name(), partition.partitionIndex()), Errors.forCode(partition.errorCode()));                });            });            context.getFuture().complete(partitions);        }        @Override        void handleFailure(Throwable throwable) {            context.getFuture().completeExceptionally(throwable);        }    };}
 AbstractRequest.Builder kafkatest_f387_0(int timeoutMs)
{    final OffsetDeleteRequestTopicCollection topics = new OffsetDeleteRequestTopicCollection();    partitions.stream().collect(Collectors.groupingBy(TopicPartition::topic)).forEach((topic, topicPartitions) -> {        topics.add(new OffsetDeleteRequestTopic().setName(topic).setPartitions(topicPartitions.stream().map(tp -> new OffsetDeleteRequestPartition().setPartitionIndex(tp.partition())).collect(Collectors.toList())));    });    return new OffsetDeleteRequest.Builder(new OffsetDeleteRequestData().setGroupId(context.groupId).setTopics(topics));}
public AbstractRequest.Builder kafkatest_f396_0(int timeoutMs)
{    AlterPartitionReassignmentsRequestData data = new AlterPartitionReassignmentsRequestData();    for (Map.Entry<String, Map<Integer, Optional<NewPartitionReassignment>>> entry : topicsToReassignments.entrySet()) {        String topicName = entry.getKey();        Map<Integer, Optional<NewPartitionReassignment>> partitionsToReassignments = entry.getValue();        List<ReassignablePartition> reassignablePartitions = new ArrayList<>();        for (Map.Entry<Integer, Optional<NewPartitionReassignment>> partitionEntry : partitionsToReassignments.entrySet()) {            int partitionIndex = partitionEntry.getKey();            Optional<NewPartitionReassignment> reassignment = partitionEntry.getValue();            ReassignablePartition reassignablePartition = new ReassignablePartition().setPartitionIndex(partitionIndex).setReplicas(reassignment.map(NewPartitionReassignment::targetBrokers).orElse(null));            reassignablePartitions.add(reassignablePartition);        }        ReassignableTopic reassignableTopic = new ReassignableTopic().setName(topicName).setPartitions(reassignablePartitions);        data.topics().add(reassignableTopic);    }    data.setTimeoutMs(timeoutMs);    return new AlterPartitionReassignmentsRequest.Builder(data);}
public void kafkatest_f397_0(AbstractResponse abstractResponse)
{    AlterPartitionReassignmentsResponse response = (AlterPartitionReassignmentsResponse) abstractResponse;    Map<TopicPartition, ApiException> errors = new HashMap<>();    int receivedResponsesCount = 0;    Errors topLevelError = Errors.forCode(response.data().errorCode());    switch(topLevelError) {        case NONE:            receivedResponsesCount += validateTopicResponses(response.data().responses(), errors);            break;        case NOT_CONTROLLER:            handleNotControllerError(topLevelError);            break;        default:            for (ReassignableTopicResponse topicResponse : response.data().responses()) {                String topicName = topicResponse.name();                for (ReassignablePartitionResponse partition : topicResponse.partitions()) {                    errors.put(new TopicPartition(topicName, partition.partitionIndex()), new ApiError(topLevelError, topLevelError.message()).exception());                    receivedResponsesCount += 1;                }            }            break;    }    assertResponseCountMatch(errors, receivedResponsesCount);    for (Map.Entry<TopicPartition, ApiException> entry : errors.entrySet()) {        ApiException exception = entry.getValue();        if (exception == null)            futures.get(entry.getKey()).complete(null);        else            futures.get(entry.getKey()).completeExceptionally(exception);    }}
private boolean kafkatest_f406_0(ConfigResource resource)
{    return (resource.type() == ConfigResource.Type.BROKER && !resource.isDefault()) || resource.type() == ConfigResource.Type.BROKER_LOGGER;}
public MembershipChangeResult kafkatest_f407_0(String groupId, RemoveMemberFromConsumerGroupOptions options)
{    final long startFindCoordinatorMs = time.milliseconds();    final long deadline = calcDeadlineMs(startFindCoordinatorMs, options.timeoutMs());    KafkaFutureImpl<RemoveMemberFromGroupResult> future = new KafkaFutureImpl<>();    ConsumerGroupOperationContext<RemoveMemberFromGroupResult, RemoveMemberFromConsumerGroupOptions> context = new ConsumerGroupOperationContext<>(groupId, options, deadline, future);    Call findCoordinatorCall = getFindCoordinatorCall(context, () -> KafkaAdminClient.this.getRemoveMembersFromGroupCall(context));    runnable.call(findCoordinatorCall, startFindCoordinatorMs);    return new MembershipChangeResult(future);}
public KafkaFuture<Collection<ConsumerGroupListing>> kafkatest_f416_0()
{    return all;}
public KafkaFuture<Collection<ConsumerGroupListing>> kafkatest_f417_0()
{    return valid;}
public KafkaFuture<Set<String>> kafkatest_f426_0()
{    return future.thenApply(new KafkaFuture.BaseFunction<Map<String, TopicListing>, Set<String>>() {        @Override        public Set<String> apply(Map<String, TopicListing> namesToListings) {            return namesToListings.keySet();        }    });}
public Set<String> kafkatest_f427_0(Map<String, TopicListing> namesToListings)
{    return namesToListings.keySet();}
public String kafkatest_f436_0()
{    return clientId;}
public String kafkatest_f437_0()
{    return host;}
public int kafkatest_f446_0()
{    return totalCount;}
public List<List<Integer>> kafkatest_f447_0()
{    return newAssignments;}
public String kafkatest_f456_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(name=").append(name).append(", numPartitions=").append(numPartitions.map(String::valueOf).orElse("default")).append(", replicationFactor=").append(replicationFactor.map(String::valueOf).orElse("default")).append(", replicasAssignments=").append(replicasAssignments).append(", configs=").append(configs).append(")");    return bld.toString();}
public List<Integer> kafkatest_f457_0()
{    return replicas;}
public Errors kafkatest_f466_0()
{    return topLevelError;}
public boolean kafkatest_f467_0()
{    return hasError;}
public List<TopicPartitionInfo> kafkatest_f476_0()
{    return partitions;}
public Set<AclOperation> kafkatest_f477_0()
{    return authorizedOperations;}
public synchronized byte kafkatest_f486_0()
{    return maxUsableProduceMagic;}
public String kafkatest_f487_0()
{    return clientDnsLookup;}
public long kafkatest_f496_0()
{    return createdTimeMs;}
public int kafkatest_f497_0()
{    return correlationId;}
public boolean kafkatest_f506_0()
{    return responseBody != null;}
public long kafkatest_f507_0()
{    return latencyMs;}
public boolean kafkatest_f516_0(String id, long now)
{    NodeConnectionState state = nodeState.get(id);    return state != null && state.state.isDisconnected() && now - state.lastConnectAttemptMs < state.reconnectBackoffMs;}
public long kafkatest_f517_0(String id, long now)
{    NodeConnectionState state = nodeState.get(id);    if (state == null)        return 0;    if (state.state.isDisconnected()) {        long timeWaited = now - state.lastConnectAttemptMs;        return Math.max(state.reconnectBackoffMs - timeWaited, 0);    } else {        // data acked) will cause a wakeup once data can be sent.        return Long.MAX_VALUE;    }}
public void kafkatest_f526_0(String id)
{    NodeConnectionState nodeState = nodeState(id);    nodeState.state = ConnectionState.CHECKING_API_VERSIONS;}
public void kafkatest_f527_0(String id)
{    NodeConnectionState nodeState = nodeState(id);    nodeState.state = ConnectionState.READY;    nodeState.authenticationException = null;    resetReconnectBackoff(nodeState);}
private void kafkatest_f536_0(NodeConnectionState nodeState)
{    if (this.reconnectBackoffMaxMs > this.reconnectBackoffInitMs) {        nodeState.failedAttempts += 1;        double backoffExp = Math.min(nodeState.failedAttempts - 1, this.reconnectBackoffMaxExp);        double backoffFactor = Math.pow(RECONNECT_BACKOFF_EXP_BASE, backoffExp);        long reconnectBackoffMs = (long) (this.reconnectBackoffInitMs * backoffFactor);        // Actual backoff is randomized to avoid connection storms.        double randomFactor = ThreadLocalRandom.current().nextDouble(0.8, 1.2);        nodeState.reconnectBackoffMs = (long) (randomFactor * reconnectBackoffMs);    }}
public void kafkatest_f537_0(String id)
{    nodeState.remove(id);}
public boolean kafkatest_f546_0()
{    return this == CHECKING_API_VERSIONS || this == READY;}
protected Map<String, Object> kafkatest_f547_0(final Map<String, Object> parsedValues)
{    return CommonClientConfigs.postProcessReconnectBackoffConfigs(this, parsedValues);}
public Optional<String> kafkatest_f556_0()
{    return groupInstanceId;}
 ByteBuffer kafkatest_f557_0(Set<String> topics)
{    return null;}
public ByteBuffer kafkatest_f567_0()
{    return userData;}
public Map<String, Subscription> kafkatest_f568_0()
{    return subscriptions;}
public V kafkatest_f577_0()
{    return value;}
public long kafkatest_f578_0()
{    return offset;}
public Iterable<ConsumerRecord<K, V>> kafkatest_f587_0(String topic)
{    if (topic == null)        throw new IllegalArgumentException("Topic must be non-null.");    List<List<ConsumerRecord<K, V>>> recs = new ArrayList<>();    for (Map.Entry<TopicPartition, List<ConsumerRecord<K, V>>> entry : records.entrySet()) {        if (entry.getKey().topic().equals(topic))            recs.add(entry.getValue());    }    return new ConcatenatedIterable<>(recs);}
public Set<TopicPartition> kafkatest_f588_0()
{    return Collections.unmodifiableSet(records.keySet());}
protected MemberData kafkatest_f597_0(Subscription subscription)
{    return new MemberData(subscription.ownedPartitions(), Optional.empty());}
public Map<String, List<TopicPartition>> kafkatest_f598_0(Map<String, Integer> partitionsPerTopic, Map<String, Subscription> subscriptions)
{    final Map<String, List<TopicPartition>> assignments = super.assign(partitionsPerTopic, subscriptions);    adjustAssignment(subscriptions, assignments);    return assignments;}
 boolean kafkatest_f608_0(final Timer timer)
{    // when sending heartbeats and does not necessarily require us to rejoin the group.    if (!ensureCoordinatorReady(timer)) {        return false;    }    startHeartbeatThreadIfNeeded();    return joinGroupIfNeeded(timer);}
private synchronized void kafkatest_f609_0()
{    if (heartbeatThread == null) {        heartbeatThread = new HeartbeatThread();        heartbeatThread.start();    }}
public voidf618_1JoinGroupResponse joinResponse, RequestFuture<ByteBuffer> future)
{    Errors error = joinResponse.error();    if (error == Errors.NONE) {                sensors.joinLatency.record(response.requestLatencyMs());        synchronized (AbstractCoordinator.this) {            if (state != MemberState.REBALANCING) {                // if the consumer was woken up before a rebalance completes, we may have already left                // the group. In this case, we do not want to continue with the sync group.                future.raise(new UnjoinedGroupException());            } else {                AbstractCoordinator.this.generation = new Generation(joinResponse.data().generationId(), joinResponse.data().memberId(), joinResponse.data().protocolName());                if (joinResponse.isLeader()) {                    onJoinLeader(joinResponse).chain(future);                } else {                    onJoinFollower().chain(future);                }            }        }    } else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS) {                // backoff and retry        future.raise(error);    } else if (error == Errors.UNKNOWN_MEMBER_ID) {        // reset the member id and retry immediately        resetGenerationOnResponseError(ApiKeys.JOIN_GROUP, error);                future.raise(error);    } else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) {        // re-discover the coordinator and retry with backoff        markCoordinatorUnknown();                future.raise(error);    } else if (error == Errors.FENCED_INSTANCE_ID) {                future.raise(error);    } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL || error == Errors.INVALID_SESSION_TIMEOUT || error == Errors.INVALID_GROUP_ID || error == Errors.GROUP_AUTHORIZATION_FAILED || error == Errors.GROUP_MAX_SIZE_REACHED) {        // log the error and re-throw the exception                if (error == Errors.GROUP_MAX_SIZE_REACHED) {            future.raise(new GroupMaxSizeReachedException("Consumer group " + rebalanceConfig.groupId + " already has the configured maximum number of members."));        } else if (error == Errors.GROUP_AUTHORIZATION_FAILED) {            future.raise(GroupAuthorizationException.forGroupId(rebalanceConfig.groupId));        } else {            future.raise(error);        }    } else if (error == Errors.UNSUPPORTED_VERSION) {                future.raise(error);    } else if (error == Errors.MEMBER_ID_REQUIRED) {        // and send another join group request in next cycle.        synchronized (AbstractCoordinator.this) {            AbstractCoordinator.this.generation = new Generation(OffsetCommitRequest.DEFAULT_GENERATION_ID, joinResponse.data().memberId(), null);            AbstractCoordinator.this.rejoinNeeded = true;            AbstractCoordinator.this.state = MemberState.UNJOINED;        }        future.raise(error);    } else {        // unexpected error, throw the exception                future.raise(new KafkaException("Unexpected error in join group response: " + error.message()));    }}
private RequestFuture<ByteBuffer>f619_1)
{    // send follower's sync group with an empty assignment    SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder(new SyncGroupRequestData().setGroupId(rebalanceConfig.groupId).setMemberId(generation.memberId).setGroupInstanceId(this.rebalanceConfig.groupInstanceId.orElse(null)).setGenerationId(generation.generationId).setAssignments(Collections.emptyList()));        return sendSyncGroupRequest(requestBuilder);}
private synchronized Node kafkatest_f628_0()
{    return this.coordinator;}
protected synchronized void kafkatest_f629_0()
{    markCoordinatorUnknown(false);}
protected synchronized void kafkatest_f638_0()
{    this.rejoinNeeded = true;}
public final void kafkatest_f639_0()
{    close(time.timer(0));}
protected Meter kafkatest_f648_0(Metrics metrics, String groupName, String baseName, String descriptiveName)
{    return new Meter(new WindowedCount(), metrics.metricName(baseName + "-rate", groupName, String.format("The number of %s per second", descriptiveName)), metrics.metricName(baseName + "-total", groupName, String.format("The total number of %s", descriptiveName)));}
public double kafkatest_f649_0(MetricConfig config, long now)
{    return TimeUnit.SECONDS.convert(now - heartbeat.lastHeartbeatSend(), TimeUnit.MILLISECONDS);}
public boolean kafkatest_f658_0()
{    return !memberId.isEmpty();}
public boolean kafkatest_f659_0(final Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    final Generation that = (Generation) o;    return generationId == that.generationId && Objects.equals(memberId, that.memberId) && Objects.equals(protocol, that.protocol);}
public int kafkatest_f668_0()
{    return memberId.hashCode();}
public String kafkatest_f669_0()
{    return "MemberInfo [member.id: " + memberId + ", group.instance.id: " + groupInstanceId.orElse("{}") + "]";}
private booleanf678_1String consumer, Map<String, List<TopicPartition>> currentAssignment, Map<String, List<TopicPartition>> consumer2AllPotentialPartitions, Map<TopicPartition, List<String>> partition2AllPotentialConsumers)
{    List<TopicPartition> currentPartitions = currentAssignment.get(consumer);    int currentAssignmentSize = currentPartitions.size();    int maxAssignmentSize = consumer2AllPotentialPartitions.get(consumer).size();    if (currentAssignmentSize > maxAssignmentSize)            if (currentAssignmentSize < maxAssignmentSize)        // if a consumer is not assigned all its potential partitions it is subject to reassignment        return true;    for (TopicPartition partition : currentPartitions) // is subject to reassignment    if (canParticipateInReassignment(partition, partition2AllPotentialConsumers))        return true;    return false;}
private void kafkatest_f679_0(Map<String, List<TopicPartition>> currentAssignment, Map<TopicPartition, ConsumerGenerationPair> prevAssignment, List<TopicPartition> sortedPartitions, List<TopicPartition> unassignedPartitions, TreeSet<String> sortedCurrentSubscriptions, Map<String, List<TopicPartition>> consumer2AllPotentialPartitions, Map<TopicPartition, List<String>> partition2AllPotentialConsumers, Map<TopicPartition, String> currentPartitionConsumer, boolean revocationRequired)
{    boolean initializing = currentAssignment.get(sortedCurrentSubscriptions.last()).isEmpty();    boolean reassignmentPerformed = false;    // assign all unassigned partitions    for (TopicPartition partition : unassignedPartitions) {        // skip if there is no potential consumer for the partition        if (partition2AllPotentialConsumers.get(partition).isEmpty())            continue;        assignPartition(partition, sortedCurrentSubscriptions, currentAssignment, consumer2AllPotentialPartitions, currentPartitionConsumer);    }    // narrow down the reassignment scope to only those partitions that can actually be reassigned    Set<TopicPartition> fixedPartitions = new HashSet<>();    for (TopicPartition partition : partition2AllPotentialConsumers.keySet()) if (!canParticipateInReassignment(partition, partition2AllPotentialConsumers))        fixedPartitions.add(partition);    sortedPartitions.removeAll(fixedPartitions);    unassignedPartitions.removeAll(fixedPartitions);    // narrow down the reassignment scope to only those consumers that are subject to reassignment    Map<String, List<TopicPartition>> fixedAssignments = new HashMap<>();    for (String consumer : consumer2AllPotentialPartitions.keySet()) if (!canParticipateInReassignment(consumer, currentAssignment, consumer2AllPotentialPartitions, partition2AllPotentialConsumers)) {        sortedCurrentSubscriptions.remove(consumer);        fixedAssignments.put(consumer, currentAssignment.remove(consumer));    }    // create a deep copy of the current assignment so we can revert to it if we do not get a more balanced assignment later    Map<String, List<TopicPartition>> preBalanceAssignment = deepCopy(currentAssignment);    Map<TopicPartition, String> preBalancePartitionConsumers = new HashMap<>(currentPartitionConsumer);    // if we don't already need to revoke something due to subscription changes, first try to balance by only moving newly added partitions    if (!revocationRequired) {        performReassignments(unassignedPartitions, currentAssignment, prevAssignment, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer);    }    reassignmentPerformed = performReassignments(sortedPartitions, currentAssignment, prevAssignment, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer);    // make sure we are getting a more balanced assignment; otherwise, revert to previous assignment    if (!initializing && reassignmentPerformed && getBalanceScore(currentAssignment) >= getBalanceScore(preBalanceAssignment)) {        deepCopy(preBalanceAssignment, currentAssignment);        currentPartitionConsumer.clear();        currentPartitionConsumer.putAll(preBalancePartitionConsumers);    }    // add the fixed assignments (those that could not change) back    for (Entry<String, List<TopicPartition>> entry : fixedAssignments.entrySet()) {        String consumer = entry.getKey();        currentAssignment.put(consumer, entry.getValue());        sortedCurrentSubscriptions.add(consumer);    }    fixedAssignments.clear();}
public int kafkatest_f688_0(TopicPartition o1, TopicPartition o2)
{    int ret = map.get(o1).size() - map.get(o2).size();    if (ret == 0) {        ret = o1.topic().compareTo(o2.topic());        if (ret == 0)            ret = o1.partition() - o2.partition();    }    return ret;}
public int kafkatest_f689_0(String o1, String o2)
{    int ret = map.get(o1).size() - map.get(o2).size();    if (ret == 0)        ret = o1.compareTo(o2);    return ret;}
public String kafkatest_f698_0()
{    return this.srcMemberId + "->" + this.dstMemberId;}
public int kafkatest_f699_0()
{    final int prime = 31;    int result = 1;    result = prime * result + ((this.srcMemberId == null) ? 0 : this.srcMemberId.hashCode());    result = prime * result + ((this.dstMemberId == null) ? 0 : this.dstMemberId.hashCode());    return result;}
protected JoinGroupRequestData.JoinGroupRequestProtocolCollectionf708_1)
{        this.joinedSubscription = subscriptions.subscription();    JoinGroupRequestData.JoinGroupRequestProtocolCollection protocolSet = new JoinGroupRequestData.JoinGroupRequestProtocolCollection();    List<String> topics = new ArrayList<>(joinedSubscription);    for (ConsumerPartitionAssignor assignor : assignors) {        Subscription subscription = new Subscription(topics, assignor.subscriptionUserData(joinedSubscription), subscriptions.assignedPartitionsList());        ByteBuffer metadata = ConsumerProtocol.serializeSubscription(subscription);        protocolSet.add(new JoinGroupRequestData.JoinGroupRequestProtocol().setName(assignor.name()).setMetadata(Utils.toArray(metadata)));    }    return protocolSet;}
public void kafkatest_f709_0(Cluster cluster)
{    final Set<String> topicsToSubscribe = cluster.topics().stream().filter(subscriptions::matchesSubscribedPattern).collect(Collectors.toSet());    if (subscriptions.subscribeFromPattern(topicsToSubscribe))        metadata.requestUpdateForNewTopics();}
public long kafkatest_f718_0(long now)
{    if (!autoCommitEnabled)        return timeToNextHeartbeat(now);    return Math.min(nextAutoCommitTimer.remainingMs(), timeToNextHeartbeat(now));}
private void kafkatest_f719_0(Set<String> topics)
{    // which ensures that all metadata changes will eventually be seen    if (this.subscriptions.groupSubscribe(topics))        metadata.requestUpdateForNewTopics();    // we can check after rebalance completion whether anything has changed    if (!client.ensureFreshMetadata(time.timer(Long.MAX_VALUE)))        throw new TimeoutException();    maybeUpdateSubscriptionMetadata();}
 void kafkatest_f728_0()
{    if (asyncCommitFenced.get()) {        throw new FencedInstanceIdException("Get fenced exception for group.instance.id " + rebalanceConfig.groupInstanceId.orElse("unset_instance_id") + ", current member.id is " + memberId());    }    while (true) {        OffsetCommitCompletion completion = completedOffsetCommits.poll();        if (completion == null) {            break;        }        completion.invoke();    }}
public void kafkatest_f729_0(final Map<TopicPartition, OffsetAndMetadata> offsets, final OffsetCommitCallback callback)
{    invokeCompletedOffsetCommitCallbacks();    if (!coordinatorUnknown()) {        doCommitOffsetsAsync(offsets, callback);    } else {        // we don't know the current coordinator, so try to find it and then send the commit        // or fail (we don't want recursive retries which can cause offset commits to arrive        // out of order). Note that there may be multiple offset commits chained to the same        // coordinator lookup request. This is fine because the listeners will be invoked in        // the same order that they were added. Note also that AbstractCoordinator prevents        // multiple concurrent coordinator lookup requests.        pendingAsyncCommits.incrementAndGet();        lookupCoordinator().addListener(new RequestFutureListener<Void>() {            @Override            public void onSuccess(Void value) {                pendingAsyncCommits.decrementAndGet();                doCommitOffsetsAsync(offsets, callback);                client.pollNoWakeup();            }            @Override            public void onFailure(RuntimeException e) {                pendingAsyncCommits.decrementAndGet();                completedOffsetCommits.add(new OffsetCommitCompletion(callback, offsets, new RetriableCommitFailedException(e)));            }        });    }    // ensure the commit has a chance to be transmitted (without blocking on its completion).    // Note that commits are treated as heartbeats by the coordinator, so there is no need to    // explicitly allow heartbeats through delayed task execution.    client.pollNoWakeup();}
private voidf738_1Timer timer)
{    if (autoCommitEnabled) {        Map<TopicPartition, OffsetAndMetadata> allConsumedOffsets = subscriptions.allConsumed();        try {                        if (!commitOffsetsSync(allConsumedOffsets, timer))                        } catch (WakeupException | InterruptException e) {                        // rethrow wakeups since they are triggered by the user            throw e;        } catch (Exception e) {            // consistent with async auto-commit failures, we do not propagate the exception                    }    }}
public voidf739_1Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception)
{    if (exception != null)        }
public ConsumerRecords<K, V>f748_1ConsumerRecords<K, V> records)
{    ConsumerRecords<K, V> interceptRecords = records;    for (ConsumerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptRecords = interceptor.onConsume(interceptRecords);        } catch (Exception e) {            // do not propagate interceptor exception, log and continue calling other interceptors                    }    }    return interceptRecords;}
public voidf749_1Map<TopicPartition, OffsetAndMetadata> offsets)
{    for (ConsumerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptor.onCommit(offsets);        } catch (Exception e) {            // do not propagate interceptor exception, just log                    }    }}
public RequestFuture<ClientResponse> kafkatest_f758_0(Node node, AbstractRequest.Builder<?> requestBuilder)
{    return send(node, requestBuilder, requestTimeoutMs);}
public RequestFuture<ClientResponse> kafkatest_f759_0(Node node, AbstractRequest.Builder<?> requestBuilder, int requestTimeoutMs)
{    long now = time.milliseconds();    RequestFutureCompletionHandler completionHandler = new RequestFutureCompletionHandler();    ClientRequest clientRequest = client.newClientRequest(node.idString(), requestBuilder, now, true, requestTimeoutMs, completionHandler);    unsent.put(node, clientRequest);    // wakeup the client in case it is blocking in poll so that we can send the queued request    client.wakeup();    return completionHandler.future;}
public void kafkatest_f768_0(Timer timer, PollCondition pollCondition)
{    poll(timer, pollCondition, false);}
public void kafkatest_f769_0(Timer timer, PollCondition pollCondition, boolean disableWakeup)
{    // there may be handlers which need to be invoked if we woke up the previous call to poll    firePendingCompletedRequests();    lock.lock();    try {        // Handle async disconnects prior to attempting any sends        handlePendingDisconnects();        // send all the requests we can send now        long pollDelayMs = trySend(timer.currentTimeMs());        // handler), the client will be woken up.        if (pendingCompletion.isEmpty() && (pollCondition == null || pollCondition.shouldBlock())) {            // if there are no requests in flight, do not block longer than the retry backoff            long pollTimeout = Math.min(timer.remainingMs(), pollDelayMs);            if (client.inFlightRequestCount() == 0)                pollTimeout = Math.min(pollTimeout, retryBackoffMs);            client.poll(pollTimeout, timer.currentTimeMs());        } else {            client.poll(0, timer.currentTimeMs());        }        timer.update();        // handle any disconnects by failing the active requests. note that disconnects must        // be checked immediately following poll since any subsequent call to client.ready()        // will reset the disconnect status        checkDisconnects(timer.currentTimeMs());        if (!disableWakeup) {            // trigger wakeups after checking for disconnects so that the callbacks will be ready            // to be fired on the next call to poll()            maybeTriggerWakeup();        }        // throw InterruptException if this thread is interrupted        maybeThrowInterruptException();        // try again to send requests since buffer space may have been        // cleared or a connect finished in the poll        trySend(timer.currentTimeMs());        // fail requests that couldn't be sent if they have expired        failExpiredRequests(timer.currentTimeMs());        // clean unsent requests collection to keep the map from growing indefinitely        unsent.clean();    } finally {        lock.unlock();    }    // called without the lock to avoid deadlock potential if handlers need to acquire locks    firePendingCompletedRequests();    metadata.maybeThrowAnyException();}
private void kafkatest_f778_0()
{    lock.lock();    try {        while (true) {            Node node = pendingDisconnects.poll();            if (node == null)                break;            failUnsentRequests(node, DisconnectException.INSTANCE);            client.disconnect(node.idString());        }    } finally {        lock.unlock();    }}
public void kafkatest_f779_0(Node node)
{    pendingDisconnects.offer(node);    client.wakeup();}
public void kafkatest_f788_0(Node node)
{    lock.lock();    try {        AuthenticationException exception = client.authenticationException(node);        if (exception != null)            throw exception;    } finally {        lock.unlock();    }}
public void kafkatest_f789_0(Node node)
{    lock.lock();    try {        client.ready(node, time.milliseconds());    } finally {        lock.unlock();    }}
private Collection<ClientRequest> kafkatest_f798_0(long now)
{    List<ClientRequest> expiredRequests = new ArrayList<>();    for (ConcurrentLinkedQueue<ClientRequest> requests : unsent.values()) {        Iterator<ClientRequest> requestIterator = requests.iterator();        while (requestIterator.hasNext()) {            ClientRequest request = requestIterator.next();            long elapsedMs = Math.max(0, now - request.createdTimeMs());            if (elapsedMs > request.requestTimeoutMs()) {                expiredRequests.add(request);                requestIterator.remove();            } else                break;        }    }    return expiredRequests;}
public void kafkatest_f799_0()
{    // queue after it has been removed from the map    synchronized (unsent) {        Iterator<ConcurrentLinkedQueue<ClientRequest>> iterator = unsent.values().iterator();        while (iterator.hasNext()) {            ConcurrentLinkedQueue<ClientRequest> requests = iterator.next();            if (requests.isEmpty())                iterator.remove();        }    }}
public static Subscription kafkatest_f808_0(ByteBuffer buffer)
{    Struct struct = SUBSCRIPTION_V0.read(buffer);    ByteBuffer userData = struct.getBytes(USER_DATA_KEY_NAME);    List<String> topics = new ArrayList<>();    for (Object topicObj : struct.getArray(TOPICS_KEY_NAME)) topics.add((String) topicObj);    return new Subscription(topics, userData, Collections.emptyList());}
public static Subscription kafkatest_f809_0(ByteBuffer buffer)
{    Struct struct = SUBSCRIPTION_V1.read(buffer);    ByteBuffer userData = struct.getBytes(USER_DATA_KEY_NAME);    List<String> topics = new ArrayList<>();    for (Object topicObj : struct.getArray(TOPICS_KEY_NAME)) topics.add((String) topicObj);    List<TopicPartition> ownedPartitions = new ArrayList<>();    for (Object structObj : struct.getArray(OWNED_PARTITIONS_KEY_NAME)) {        Struct assignment = (Struct) structObj;        String topic = assignment.getString(TOPIC_KEY_NAME);        for (Object partitionObj : assignment.getArray(PARTITIONS_KEY_NAME)) {            ownedPartitions.add(new TopicPartition(topic, (Integer) partitionObj));        }    }    return new Subscription(topics, userData, ownedPartitions);}
protected boolean kafkatest_f818_0()
{    return !completedFetches.isEmpty();}
public boolean kafkatest_f819_0()
{    return completedFetches.stream().anyMatch(fetch -> subscriptions.isFetchable(fetch.partition));}
public void kafkatest_f828_0()
{    // Raise exception from previous offset fetch if there is one    RuntimeException exception = cachedListOffsetsException.getAndSet(null);    if (exception != null)        throw exception;    Set<TopicPartition> partitions = subscriptions.partitionsNeedingReset(time.milliseconds());    if (partitions.isEmpty())        return;    final Map<TopicPartition, Long> offsetResetTimestamps = new HashMap<>();    for (final TopicPartition partition : partitions) {        Long timestamp = offsetResetStrategyTimestamp(partition);        if (timestamp != null)            offsetResetTimestamps.put(partition, timestamp);    }    resetOffsetsAsync(offsetResetTimestamps);}
public void kafkatest_f829_0()
{    RuntimeException exception = cachedOffsetForLeaderException.getAndSet(null);    if (exception != null)        throw exception;    // Validate each partition against the current leader and epoch    subscriptions.assignedPartitions().forEach(topicPartition -> {        ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.leaderAndEpoch(topicPartition);        subscriptions.maybeValidatePositionForCurrentLeader(topicPartition, leaderAndEpoch);    });    // Collect positions needing validation, with backoff    Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate = subscriptions.partitionsNeedingValidation(time.milliseconds()).stream().collect(Collectors.toMap(Function.identity(), subscriptions::position));    validateOffsetsAsync(partitionsToValidate);}
private voidf838_1Map<TopicPartition, Long> partitionResetTimestamps)
{    Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> timestampsToSearchByNode = groupListOffsetRequests(partitionResetTimestamps, new HashSet<>());    for (Map.Entry<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> entry : timestampsToSearchByNode.entrySet()) {        Node node = entry.getKey();        final Map<TopicPartition, ListOffsetRequest.PartitionData> resetTimestamps = entry.getValue();        subscriptions.setNextAllowedRetry(resetTimestamps.keySet(), time.milliseconds() + requestTimeoutMs);        RequestFuture<ListOffsetResult> future = sendListOffsetRequest(node, resetTimestamps, false);        future.addListener(new RequestFutureListener<ListOffsetResult>() {            @Override            public void onSuccess(ListOffsetResult result) {                if (!result.partitionsToRetry.isEmpty()) {                    subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs);                    metadata.requestUpdate();                }                for (Map.Entry<TopicPartition, ListOffsetData> fetchedOffset : result.fetchedOffsets.entrySet()) {                    TopicPartition partition = fetchedOffset.getKey();                    ListOffsetData offsetData = fetchedOffset.getValue();                    ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition);                    resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData);                }            }            @Override            public void onFailure(RuntimeException e) {                subscriptions.requestFailed(resetTimestamps.keySet(), time.milliseconds() + retryBackoffMs);                metadata.requestUpdate();                if (!(e instanceof RetriableException) && !cachedListOffsetsException.compareAndSet(null, e))                                }        });    }}
public void kafkatest_f839_0(ListOffsetResult result)
{    if (!result.partitionsToRetry.isEmpty()) {        subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs);        metadata.requestUpdate();    }    for (Map.Entry<TopicPartition, ListOffsetData> fetchedOffset : result.fetchedOffsets.entrySet()) {        TopicPartition partition = fetchedOffset.getKey();        ListOffsetData offsetData = fetchedOffset.getValue();        ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition);        resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData);    }}
private Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>>f848_1Map<TopicPartition, Long> timestampsToSearch, Set<TopicPartition> partitionsToRetry)
{    final Map<TopicPartition, ListOffsetRequest.PartitionData> partitionDataMap = new HashMap<>();    for (Map.Entry<TopicPartition, Long> entry : timestampsToSearch.entrySet()) {        TopicPartition tp = entry.getKey();        Long offset = entry.getValue();        Optional<MetadataCache.PartitionInfoAndEpoch> currentInfo = metadata.partitionInfoIfCurrent(tp);        if (!currentInfo.isPresent()) {                        metadata.requestUpdate();            partitionsToRetry.add(tp);        } else if (currentInfo.get().partitionInfo().leader() == null) {                        metadata.requestUpdate();            partitionsToRetry.add(tp);        } else if (client.isUnavailable(currentInfo.get().partitionInfo().leader())) {            client.maybeThrowAuthFailure(currentInfo.get().partitionInfo().leader());            // The connection has failed and we need to await the blackout period before we can            // try again. No need to request a metadata update since the disconnect will have            // done so already.                        partitionsToRetry.add(tp);        } else {            partitionDataMap.put(tp, new ListOffsetRequest.PartitionData(offset, Optional.of(currentInfo.get().epoch())));        }    }    return regroupPartitionMapByNode(partitionDataMap);}
private RequestFuture<ListOffsetResult>f849_1final Node node, final Map<TopicPartition, ListOffsetRequest.PartitionData> timestampsToSearch, boolean requireTimestamp)
{    ListOffsetRequest.Builder builder = ListOffsetRequest.Builder.forConsumer(requireTimestamp, isolationLevel).setTargetTimes(timestampsToSearch);        return client.send(node, builder).compose(new RequestFutureAdapter<ClientResponse, ListOffsetResult>() {        @Override        public void onSuccess(ClientResponse response, RequestFuture<ListOffsetResult> future) {            ListOffsetResponse lor = (ListOffsetResponse) response.responseBody();            log.trace("Received ListOffsetResponse {} from broker {}", lor, node);            handleListOffsetResponse(timestampsToSearch, lor, future);        }    });}
private ConsumerRecord<K, V> kafkatest_f858_0(TopicPartition partition, RecordBatch batch, Record record)
{    try {        long offset = record.offset();        long timestamp = record.timestamp();        Optional<Integer> leaderEpoch = maybeLeaderEpoch(batch.partitionLeaderEpoch());        TimestampType timestampType = batch.timestampType();        Headers headers = new RecordHeaders(record.headers());        ByteBuffer keyBytes = record.key();        byte[] keyByteArray = keyBytes == null ? null : Utils.toArray(keyBytes);        K key = keyBytes == null ? null : this.keyDeserializer.deserialize(partition.topic(), headers, keyByteArray);        ByteBuffer valueBytes = record.value();        byte[] valueByteArray = valueBytes == null ? null : Utils.toArray(valueBytes);        V value = valueBytes == null ? null : this.valueDeserializer.deserialize(partition.topic(), headers, valueByteArray);        return new ConsumerRecord<>(partition.topic(), partition.partition(), offset, timestamp, timestampType, record.checksumOrNull(), keyByteArray == null ? ConsumerRecord.NULL_SIZE : keyByteArray.length, valueByteArray == null ? ConsumerRecord.NULL_SIZE : valueByteArray.length, key, value, headers, leaderEpoch);    } catch (RuntimeException e) {        throw new SerializationException("Error deserializing key/value for partition " + partition + " at offset " + record.offset() + ". If needed, please seek past the record to continue consumption.", e);    }}
private Optional<Integer> kafkatest_f859_0(int leaderEpoch)
{    return leaderEpoch == RecordBatch.NO_PARTITION_LEADER_EPOCH ? Optional.empty() : Optional.of(leaderEpoch);}
private Recordf868_1)
{    while (true) {        if (records == null || !records.hasNext()) {            maybeCloseRecordStream();            if (!batches.hasNext()) {                // fetching the same batch repeatedly).                if (currentBatch != null)                    nextFetchOffset = currentBatch.nextOffset();                drain();                return null;            }            currentBatch = batches.next();            lastEpoch = currentBatch.partitionLeaderEpoch() == RecordBatch.NO_PARTITION_LEADER_EPOCH ? Optional.empty() : Optional.of(currentBatch.partitionLeaderEpoch());            maybeEnsureValid(currentBatch);            if (isolationLevel == IsolationLevel.READ_COMMITTED && currentBatch.hasProducerId()) {                // remove from the aborted transaction queue all aborted transactions which have begun                // before the current batch's last offset and add the associated producerIds to the                // aborted producer set                consumeAbortedTransactionsUpTo(currentBatch.lastOffset());                long producerId = currentBatch.producerId();                if (containsAbortMarker(currentBatch)) {                    abortedProducerIds.remove(producerId);                } else if (isBatchAborted(currentBatch)) {                                        nextFetchOffset = currentBatch.nextOffset();                    continue;                }            }            records = currentBatch.streamingIterator(decompressionBufferSupplier);        } else {            Record record = records.next();            // skip any records out of range            if (record.offset() >= nextFetchOffset) {                // we only do validation when the message should not be skipped.                maybeEnsureValid(record);                // control records are not returned to the user                if (!currentBatch.isControlBatch()) {                    return record;                } else {                    // Increment the next fetch offset when we skip a control batch.                    nextFetchOffset = record.offset() + 1;                }            }        }    }}
private List<ConsumerRecord<K, V>> kafkatest_f869_0(int maxRecords)
{    // Error when fetching the next record before deserialization.    if (corruptLastRecord)        throw new KafkaException("Received exception when fetching the next record from " + partition + ". If needed, please seek past the record to " + "continue consumption.", cachedRecordException);    if (isConsumed)        return Collections.emptyList();    List<ConsumerRecord<K, V>> records = new ArrayList<>();    try {        for (int i = 0; i < maxRecords; i++) {            // use the last record to do deserialization again.            if (cachedRecordException == null) {                corruptLastRecord = true;                lastRecord = nextFetchedRecord();                corruptLastRecord = false;            }            if (lastRecord == null)                break;            records.add(parseRecord(partition, currentBatch, lastRecord));            recordsRead++;            bytesRead += lastRecord.sizeInBytes();            nextFetchOffset = lastRecord.offset() + 1;            // In some cases, the deserialization may have thrown an exception and the retry may succeed,            // we allow user to move forward in this case.            cachedRecordException = null;        }    } catch (SerializationException se) {        cachedRecordException = se;        if (records.isEmpty())            throw se;    } catch (KafkaException e) {        cachedRecordException = e;        if (records.isEmpty())            throw new KafkaException("Received exception when fetching the next record from " + partition + ". If needed, please seek past the record to " + "continue consumption.", e);    }    return records;}
private void kafkatest_f878_0(SubscriptionState subscription)
{    int newAssignmentId = subscription.assignmentId();    if (this.assignmentId != newAssignmentId) {        Set<TopicPartition> newAssignedPartitions = subscription.assignedPartitions();        for (TopicPartition tp : this.assignedPartitions) {            if (!newAssignedPartitions.contains(tp)) {                metrics.removeSensor(partitionLagMetricName(tp));                metrics.removeSensor(partitionLeadMetricName(tp));                metrics.removeMetric(partitionPreferredReadReplicaMetricName(tp));            }        }        for (TopicPartition tp : newAssignedPartitions) {            if (!this.assignedPartitions.contains(tp)) {                MetricName metricName = partitionPreferredReadReplicaMetricName(tp);                if (metrics.metric(metricName) == null) {                    metrics.addMetric(metricName, (Gauge<Integer>) (config, now) -> subscription.preferredReadReplica(tp, 0L).orElse(-1));                }            }        }        this.assignedPartitions = newAssignedPartitions;        this.assignmentId = newAssignmentId;    }}
private void kafkatest_f879_0(TopicPartition tp, long lead)
{    this.recordsFetchLead.record(lead);    String name = partitionLeadMetricName(tp);    Sensor recordsLead = this.metrics.getSensor(name);    if (recordsLead == null) {        Map<String, String> metricTags = topicPartitionTags(tp);        recordsLead = this.metrics.sensor(name);        recordsLead.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLead, metricTags), new Value());        recordsLead.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLeadMin, metricTags), new Min());        recordsLead.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLeadAvg, metricTags), new Avg());    }    recordsLead.record(lead);}
private void kafkatest_f888_0(long now)
{    heartbeatTimer.update(now);    sessionTimer.update(now);    pollTimer.update(now);}
public void kafkatest_f889_0(long now)
{    update(now);    pollTimer.reset(maxPollIntervalMs);}
public void kafkatest_f898_0()
{    update(time.milliseconds());    sessionTimer.reset(rebalanceConfig.sessionTimeoutMs);}
public boolean kafkatest_f899_0(long now)
{    update(now);    return pollTimer.isExpired();}
public String kafkatest_f910_0()
{    return "Subscription(" + "topics=" + topics + ')';}
public List<TopicPartition> kafkatest_f911_0()
{    return partitions;}
private static GroupAssignment kafkatest_f920_0(Map<String, PartitionAssignor.Assignment> oldAssignments)
{    Map<String, Assignment> newAssignments = new HashMap<>();    for (Map.Entry<String, PartitionAssignor.Assignment> entry : oldAssignments.entrySet()) {        String member = entry.getKey();        PartitionAssignor.Assignment oldAssignment = entry.getValue();        newAssignments.put(member, new Assignment(oldAssignment.partitions(), oldAssignment.userData()));    }    return new GroupAssignment(newAssignments);}
public static List<ConsumerPartitionAssignor>f921_1List<String> assignorClasses, Map<String, Object> configs)
{    List<ConsumerPartitionAssignor> assignors = new ArrayList<>();    if (assignorClasses == null)        return assignors;    for (Object klass : assignorClasses) {        // first try to get the class if passed in as a string        if (klass instanceof String) {            try {                klass = Class.forName((String) klass, true, Utils.getContextOrKafkaClassLoader());            } catch (ClassNotFoundException classNotFound) {                throw new KafkaException(klass + " ClassNotFoundException exception occurred", classNotFound);            }        }        if (klass instanceof Class<?>) {            Object assignor = Utils.newInstance((Class<?>) klass);            if (assignor instanceof Configurable)                ((Configurable) assignor).configure(configs);            if (assignor instanceof ConsumerPartitionAssignor) {                assignors.add((ConsumerPartitionAssignor) assignor);            } else if (assignor instanceof PartitionAssignor) {                assignors.add(new PartitionAssignorAdapter((PartitionAssignor) assignor));                            } else {                throw new KafkaException(klass + " is not an instance of " + PartitionAssignor.class.getName() + " or an instance of " + ConsumerPartitionAssignor.class.getName());            }        } else {            throw new KafkaException("List contains element of type " + klass.getClass().getName() + ", expected String or Class");        }    }    return assignors;}
public void kafkatest_f930_0(RuntimeException e)
{    try {        if (e == null)            throw new IllegalArgumentException("The exception passed to raise must not be null");        if (!result.compareAndSet(INCOMPLETE_SENTINEL, e))            throw new IllegalStateException("Invalid attempt to complete a request future which is already complete");        fireFailure();    } finally {        completedLatch.countDown();    }}
public void kafkatest_f931_0(Errors error)
{    raise(error.exception());}
public void kafkatest_f940_0(RuntimeException e)
{    future.raise(e);}
public static RequestFuture<T> kafkatest_f941_0(RuntimeException e)
{    RequestFuture<T> future = new RequestFuture<>();    future.raise(e);    return future;}
private void kafkatest_f950_0(SubscriptionType type)
{    if (this.subscriptionType == SubscriptionType.NONE)        this.subscriptionType = type;    else if (this.subscriptionType != type)        throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);}
public synchronized boolean kafkatest_f951_0(Set<String> topics, ConsumerRebalanceListener listener)
{    registerRebalanceListener(listener);    setSubscriptionType(SubscriptionType.AUTO_TOPICS);    return changeSubscription(topics);}
private void kafkatest_f960_0(ConsumerRebalanceListener listener)
{    if (listener == null)        throw new IllegalArgumentException("RebalanceListener cannot be null");    this.rebalanceListener = listener;}
 synchronized boolean kafkatest_f961_0()
{    return this.subscriptionType == SubscriptionType.AUTO_PATTERN;}
private TopicPartitionState kafkatest_f970_0(TopicPartition tp)
{    return this.assignment.stateValue(tp);}
public synchronized void kafkatest_f971_0(TopicPartition tp, FetchPosition position)
{    assignedState(tp).seekValidated(position);}
public synchronized void kafkatest_f980_0(TopicPartition tp, FetchPosition position)
{    assignedState(tp).position(position);}
public synchronized boolean kafkatest_f981_0(TopicPartition tp, Metadata.LeaderAndEpoch leaderAndEpoch)
{    return assignedState(tp).maybeValidatePosition(leaderAndEpoch);}
 synchronized void kafkatest_f990_0(TopicPartition tp, long logStartOffset)
{    assignedState(tp).logStartOffset(logStartOffset);}
 synchronized void kafkatest_f991_0(TopicPartition tp, long lastStableOffset)
{    assignedState(tp).lastStableOffset(lastStableOffset);}
 boolean kafkatest_f1000_0()
{    return defaultResetStrategy != OffsetResetStrategy.NONE;}
public synchronized boolean kafkatest_f1001_0(TopicPartition partition)
{    return assignedState(partition).awaitingReset();}
public synchronized boolean kafkatest_f1010_0(TopicPartition tp)
{    TopicPartitionState assignedOrNull = assignedStateOrNull(tp);    return assignedOrNull != null && assignedOrNull.isPaused();}
 synchronized boolean kafkatest_f1011_0(TopicPartition tp)
{    TopicPartitionState assignedOrNull = assignedStateOrNull(tp);    return assignedOrNull != null && assignedOrNull.isFetchable();}
private Optional<Integer> kafkatest_f1020_0(long timeMs)
{    if (preferredReadReplicaExpireTimeMs != null && timeMs > preferredReadReplicaExpireTimeMs) {        preferredReadReplica = null;        return Optional.empty();    } else {        return Optional.ofNullable(preferredReadReplica);    }}
private void kafkatest_f1021_0(int preferredReadReplica, Supplier<Long> timeMs)
{    if (this.preferredReadReplica == null || preferredReadReplica != this.preferredReadReplica) {        this.preferredReadReplica = preferredReadReplica;        this.preferredReadReplicaExpireTimeMs = timeMs.get();    }}
private void kafkatest_f1030_0(long nextAllowedRetryTimeMs)
{    this.nextRetryTimeMs = nextAllowedRetryTimeMs;}
private void kafkatest_f1031_0(long nextAllowedRetryTimeMs)
{    this.nextRetryTimeMs = nextAllowedRetryTimeMs;}
private void kafkatest_f1040_0()
{    this.paused = false;}
private boolean kafkatest_f1041_0()
{    return !paused && hasValidPosition();}
public Collection<FetchState> kafkatest_f1050_0()
{    return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);}
public boolean kafkatest_f1051_0()
{    return true;}
public int kafkatest_f1060_0()
{    return Objects.hash(offset, offsetEpoch, currentLeader);}
public String kafkatest_f1061_0()
{    return "FetchPosition{" + "offset=" + offset + ", offsetEpoch=" + offsetEpoch + ", currentLeader=" + currentLeader + '}';}
public voidf1070_1Collection<TopicPartition> partitions)
{    acquireAndEnsureOpen();    try {        if (partitions == null) {            throw new IllegalArgumentException("Topic partition collection to assign to cannot be null");        } else if (partitions.isEmpty()) {            this.unsubscribe();        } else {            for (TopicPartition tp : partitions) {                String topic = (tp != null) ? tp.topic() : null;                if (topic == null || topic.trim().isEmpty())                    throw new IllegalArgumentException("Topic partitions to assign to cannot have null or empty topic");            }            fetcher.clearBufferedDataForUnassignedPartitions(partitions);            // are committed since there will be no following rebalance            if (coordinator != null)                this.coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds());                        if (this.subscriptions.assignFromUser(new HashSet<>(partitions)))                metadata.requestUpdateForNewTopics();        }    } finally {        release();    }}
public ConsumerRecords<K, V> kafkatest_f1071_0(final long timeoutMs)
{    return poll(time.timer(timeoutMs), false);}
public void kafkatest_f1080_0()
{    commitAsync(null);}
public void kafkatest_f1081_0(OffsetCommitCallback callback)
{    commitAsync(subscriptions.allConsumed(), callback);}
public OffsetAndMetadata kafkatest_f1090_0(TopicPartition partition, final Duration timeout)
{    acquireAndEnsureOpen();    try {        maybeThrowInvalidGroupIdException();        Map<TopicPartition, OffsetAndMetadata> offsets = coordinator.fetchCommittedOffsets(Collections.singleton(partition), time.timer(timeout));        if (offsets == null) {            throw new TimeoutException("Timeout of " + timeout.toMillis() + "ms expired before the last " + "committed offset for partition " + partition + " could be determined. Try tuning default.api.timeout.ms " + "larger to relax the threshold.");        } else {            offsets.forEach(this::updateLastSeenEpochIfNewer);            return offsets.get(partition);        }    } finally {        release();    }}
public Map<MetricName, ? extends Metric> kafkatest_f1091_0()
{    return Collections.unmodifiableMap(this.metrics.metrics());}
public Map<TopicPartition, OffsetAndTimestamp> kafkatest_f1100_0(Map<TopicPartition, Long> timestampsToSearch, Duration timeout)
{    acquireAndEnsureOpen();    try {        for (Map.Entry<TopicPartition, Long> entry : timestampsToSearch.entrySet()) {            // OffsetAndTimestamp is always positive.            if (entry.getValue() < 0)                throw new IllegalArgumentException("The target time for partition " + entry.getKey() + " is " + entry.getValue() + ". The target time cannot be negative.");        }        return fetcher.offsetsForTimes(timestampsToSearch, time.timer(timeout));    } finally {        release();    }}
public Map<TopicPartition, Long> kafkatest_f1101_0(Collection<TopicPartition> partitions)
{    return beginningOffsets(partitions, Duration.ofMillis(defaultApiTimeoutMs));}
private voidf1110_1long timeoutMs, boolean swallowException)
{    log.trace("Closing the Kafka consumer");    AtomicReference<Throwable> firstException = new AtomicReference<>();    try {        if (coordinator != null)            coordinator.close(time.timer(Math.min(timeoutMs, requestTimeoutMs)));    } catch (Throwable t) {        firstException.compareAndSet(null, t);            }    Utils.closeQuietly(fetcher, "fetcher", firstException);    Utils.closeQuietly(interceptors, "consumer interceptors", firstException);    Utils.closeQuietly(metrics, "consumer metrics", firstException);    Utils.closeQuietly(client, "consumer network client", firstException);    Utils.closeQuietly(keyDeserializer, "consumer key deserializer", firstException);    Utils.closeQuietly(valueDeserializer, "consumer value deserializer", firstException);    AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics);        Throwable exception = firstException.get();    if (exception != null && !swallowException) {        if (exception instanceof InterruptException) {            throw (InterruptException) exception;        }        throw new KafkaException("Failed to close kafka consumer", exception);    }}
private boolean kafkatest_f1111_0(final Timer timer)
{    // If any partitions have been truncated due to a leader change, we need to validate the offsets    fetcher.validateOffsetsIfNeeded();    cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions();    if (cachedSubscriptionHashAllFetchPositions)        return true;    // by always ensuring that assigned partitions have an initial position.    if (coordinator != null && !coordinator.refreshCommittedOffsetsIfNeeded(timer))        return false;    // If there are partitions still needing a position and a reset policy is defined,    // request reset using the default policy. If no reset strategy is defined and there    // are partitions with a missing position, then we will raise an exception.    subscriptions.resetMissingPositions();    // Finally send an asynchronous request to lookup and update the positions of any    // partitions which are awaiting reset.    fetcher.resetOffsetsIfNeeded();    return true;}
public synchronized Set<TopicPartition> kafkatest_f1120_0()
{    return this.subscriptions.assignedPartitions();}
public synchronized void kafkatest_f1121_0(Collection<TopicPartition> newAssignment)
{    // TODO: Rebalance callbacks    this.records.clear();    this.subscriptions.assignFromSubscribed(newAssignment);}
public synchronized ConsumerRecords<K, V> kafkatest_f1130_0(final Duration timeout)
{    ensureNotClosed();    // the callback    synchronized (pollTasks) {        Runnable task = pollTasks.poll();        if (task != null)            task.run();    }    if (wakeup.get()) {        wakeup.set(false);        throw new WakeupException();    }    if (pollException != null) {        RuntimeException exception = this.pollException;        this.pollException = null;        throw exception;    }    // Handle seeks that need to wait for a poll() call to be processed    for (TopicPartition tp : subscriptions.assignedPartitions()) if (!subscriptions.hasValidPosition(tp))        updateFetchPosition(tp);    // update the consumed offset    final Map<TopicPartition, List<ConsumerRecord<K, V>>> results = new HashMap<>();    for (Map.Entry<TopicPartition, List<ConsumerRecord<K, V>>> entry : this.records.entrySet()) {        if (!subscriptions.isPaused(entry.getKey())) {            final List<ConsumerRecord<K, V>> recs = entry.getValue();            for (final ConsumerRecord<K, V> rec : recs) {                long position = subscriptions.position(entry.getKey()).offset;                if (beginningOffsets.get(entry.getKey()) != null && beginningOffsets.get(entry.getKey()) > position) {                    throw new OffsetOutOfRangeException(Collections.singletonMap(entry.getKey(), position));                }                if (assignment().contains(entry.getKey()) && rec.offset() >= position) {                    results.computeIfAbsent(entry.getKey(), partition -> new ArrayList<>()).add(rec);                    SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition(rec.offset() + 1, rec.leaderEpoch(), new Metadata.LeaderAndEpoch(Node.noNode(), rec.leaderEpoch()));                    subscriptions.position(entry.getKey(), newPosition);                }            }        }    }    this.records.clear();    return new ConsumerRecords<>(results);}
public synchronized void kafkatest_f1131_0(ConsumerRecord<K, V> record)
{    ensureNotClosed();    TopicPartition tp = new TopicPartition(record.topic(), record.partition());    Set<TopicPartition> currentAssigned = this.subscriptions.assignedPartitions();    if (!currentAssigned.contains(tp))        throw new IllegalStateException("Cannot add records for a partition that is not assigned to the consumer");    List<ConsumerRecord<K, V>> recs = this.records.computeIfAbsent(tp, k -> new ArrayList<>());    recs.add(record);}
public synchronized void kafkatest_f1140_0(Duration timeout)
{    commitSync(this.subscriptions.allConsumed());}
public void kafkatest_f1141_0(Map<TopicPartition, OffsetAndMetadata> offsets, final Duration timeout)
{    commitSync(offsets);}
public synchronized void kafkatest_f1150_0(Collection<TopicPartition> partitions)
{    ensureNotClosed();    subscriptions.requestOffsetReset(partitions, OffsetResetStrategy.LATEST);}
public synchronized void kafkatest_f1151_0(final Map<TopicPartition, Long> newOffsets)
{    innerUpdateEndOffsets(newOffsets, false);}
public synchronized Map<TopicPartition, OffsetAndTimestamp> kafkatest_f1160_0(Map<TopicPartition, Long> timestampsToSearch)
{    throw new UnsupportedOperationException("Not implemented yet.");}
public synchronized Map<TopicPartition, Long> kafkatest_f1161_0(Collection<TopicPartition> partitions)
{    if (offsetsException != null) {        RuntimeException exception = this.offsetsException;        this.offsetsException = null;        throw exception;    }    Map<TopicPartition, Long> result = new HashMap<>();    for (TopicPartition tp : partitions) {        Long beginningOffset = beginningOffsets.get(tp);        if (beginningOffset == null)            throw new IllegalStateException("The partition " + tp + " does not have a beginning offset.");        result.put(tp, beginningOffset);    }    return result;}
private void kafkatest_f1170_0()
{    if (this.closed)        throw new IllegalStateException("This consumer has already been closed.");}
private void kafkatest_f1171_0(TopicPartition tp)
{    if (subscriptions.isOffsetResetNeeded(tp)) {        resetOffsetPosition(tp);    } else if (!committed.containsKey(tp)) {        subscriptions.requestOffsetReset(tp);        resetOffsetPosition(tp);    } else {        subscriptions.seek(tp, committed.get(tp).offset());    }}
public TopicPartition kafkatest_f1180_0()
{    return partitions.isEmpty() ? null : partitions.iterator().next();}
public Set<TopicPartition> kafkatest_f1181_0()
{    return partitions;}
public Optional<Integer> kafkatest_f1190_0()
{    return leaderEpoch;}
public String kafkatest_f1191_0()
{    return "(timestamp=" + timestamp + ", leaderEpoch=" + leaderEpoch.orElse(null) + ", offset=" + offset + ")";}
public Map<String, List<TopicPartition>> kafkatest_f1200_0(Map<String, Integer> partitionsPerTopic, Map<String, Subscription> subscriptions)
{    Map<String, List<TopicPartition>> assignment = new HashMap<>();    List<MemberInfo> memberInfoList = new ArrayList<>();    for (Map.Entry<String, Subscription> memberSubscription : subscriptions.entrySet()) {        assignment.put(memberSubscription.getKey(), new ArrayList<>());        memberInfoList.add(new MemberInfo(memberSubscription.getKey(), memberSubscription.getValue().groupInstanceId()));    }    CircularIterator<MemberInfo> assigner = new CircularIterator<>(Utils.sorted(memberInfoList));    for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) {        final String topic = partition.topic();        while (!subscriptions.get(assigner.peek().memberId).topics().contains(topic)) assigner.next();        assignment.get(assigner.next().memberId).add(partition);    }    return assignment;}
private List<TopicPartition> kafkatest_f1201_0(Map<String, Integer> partitionsPerTopic, Map<String, Subscription> subscriptions)
{    SortedSet<String> topics = new TreeSet<>();    for (Subscription subscription : subscriptions.values()) topics.addAll(subscription.topics());    List<TopicPartition> allPartitions = new ArrayList<>();    for (String topic : topics) {        Integer numPartitionsForTopic = partitionsPerTopic.get(topic);        if (numPartitionsForTopic != null)            allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));    }    return allPartitions;}
public List<TopicPartition> kafkatest_f1210_0()
{    return toForget;}
public Map<TopicPartition, PartitionData> kafkatest_f1211_0()
{    return sessionPartitions;}
private String kafkatest_f1220_0(FetchResponse<?> response)
{    Set<TopicPartition> extra = findMissing(response.responseData().keySet(), sessionPartitions.keySet());    if (!extra.isEmpty()) {        StringBuilder bld = new StringBuilder();        bld.append("extra=(").append(Utils.join(extra, ", ")).append("), ");        bld.append("response=(").append(Utils.join(response.responseData().keySet(), ", ")).append("), ");        return bld.toString();    }    return null;}
private String kafkatest_f1221_0(FetchResponse<?> response)
{    if (!log.isTraceEnabled()) {        int implied = sessionPartitions.size() - response.responseData().size();        if (implied > 0) {            return String.format(" with %d response partition(s), %d implied partition(s)", response.responseData().size(), implied);        } else {            return String.format(" with %d response partition(s)", response.responseData().size());        }    }    StringBuilder bld = new StringBuilder();    bld.append(" with response=(").append(Utils.join(response.responseData().keySet(), ", ")).append(")");    String prefix = ", implied=(";    String suffix = "";    for (TopicPartition partition : sessionPartitions.keySet()) {        if (!response.responseData().containsKey(partition)) {            bld.append(prefix);            bld.append(partition);            prefix = ", ";            suffix = ")";        }    }    bld.append(suffix);    return bld.toString();}
public boolean kafkatest_f1230_0(String node)
{    Deque<NetworkClient.InFlightRequest> queue = requests.get(node);    return queue == null || queue.isEmpty() || (queue.peekFirst().send.completed() && queue.size() < this.maxInFlightRequestsPerConnection);}
public int kafkatest_f1231_0(String node)
{    Deque<NetworkClient.InFlightRequest> queue = requests.get(node);    return queue == null ? 0 : queue.size();}
public boolean kafkatest_f1240_0(long now)
{    return false;}
public long kafkatest_f1241_0(long now)
{    return Long.MAX_VALUE;}
public synchronized int kafkatest_f1251_0()
{    this.needUpdate = true;    return this.updateVersion;}
public synchronized boolean kafkatest_f1252_0(TopicPartition topicPartition, int leaderEpoch)
{    Objects.requireNonNull(topicPartition, "TopicPartition cannot be null");    return updateLastSeenEpoch(topicPartition, leaderEpoch, oldEpoch -> leaderEpoch > oldEpoch, true);}
private voidf1261_1Cluster cluster)
{    if (!cluster.invalidTopics().isEmpty()) {                invalidTopics = new HashSet<>(cluster.invalidTopics());    }}
private voidf1262_1Cluster cluster)
{    if (!cluster.unauthorizedTopics().isEmpty()) {                unauthorizedTopics = new HashSet<>(cluster.unauthorizedTopics());    }}
private void kafkatest_f1271_0()
{    invalidTopics = Collections.emptySet();    unauthorizedTopics = Collections.emptySet();}
public synchronized void kafkatest_f1272_0(long now, KafkaException fatalException)
{    this.lastRefreshMs = now;    this.fatalException = fatalException;}
public synchronized LeaderAndEpoch kafkatest_f1281_0(TopicPartition tp)
{    return partitionInfoIfCurrent(tp).map(infoAndEpoch -> {        Node leader = infoAndEpoch.partitionInfo().leader();        return new LeaderAndEpoch(leader == null ? Node.noNode() : leader, Optional.of(infoAndEpoch.epoch()));    }).orElse(new LeaderAndEpoch(Node.noNode(), lastSeenLeaderEpoch(tp)));}
public static LeaderAndEpoch kafkatest_f1282_0()
{    return NO_LEADER_OR_EPOCH;}
 static MetadataCache kafkatest_f1291_0()
{    return new MetadataCache(null, Collections.emptyList(), Collections.emptyList(), Collections.emptySet(), Collections.emptySet(), Collections.emptySet(), null, Cluster.empty());}
public String kafkatest_f1292_0()
{    return "MetadataCache{" + "clusterId='" + clusterId + '\'' + ", nodes=" + nodes + ", partitions=" + metadataByPartition.values() + ", controller=" + controller + '}';}
public void kafkatest_f1301_0(String nodeId)
{    selector.close(nodeId);    for (InFlightRequest request : inFlightRequests.clearAll(nodeId)) if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA)        metadataUpdater.handleDisconnection(request.destination);    connectionStates.remove(nodeId);}
public long kafkatest_f1302_0(Node node, long now)
{    return connectionStates.connectionDelay(node.idString(), now);}
private voidf1311_1ClientRequest clientRequest, boolean isInternalRequest, long now)
{    ensureActive();    String nodeId = clientRequest.destination();    if (!isInternalRequest) {        // READY state.)        if (!canSendRequest(nodeId, now))            throw new IllegalStateException("Attempt to send a request to node " + nodeId + " which is not ready.");    }    AbstractRequest.Builder<?> builder = clientRequest.requestBuilder();    try {        NodeApiVersions versionInfo = apiVersions.get(nodeId);        short version;        // information itself.  It is also the case when discoverBrokerVersions is set to false.        if (versionInfo == null) {            version = builder.latestAllowedVersion();            if (discoverBrokerVersions && log.isTraceEnabled())                log.trace("No version information found when sending {} with correlation id {} to node {}. " + "Assuming version {}.", clientRequest.apiKey(), clientRequest.correlationId(), nodeId, version);        } else {            version = versionInfo.latestUsableVersion(clientRequest.apiKey(), builder.oldestAllowedVersion(), builder.latestAllowedVersion());        }        // The call to build may also throw UnsupportedVersionException, if there are essential        // fields that cannot be represented in the chosen version.        doSend(clientRequest, isInternalRequest, now, builder.build(version));    } catch (UnsupportedVersionException unsupportedVersionException) {        // If the version is not supported, skip sending the request over the wire.        // Instead, simply add it to the local queue of aborted requests.                ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(builder.latestAllowedVersion()), clientRequest.callback(), clientRequest.destination(), now, now, false, unsupportedVersionException, null, null);        abortedSends.add(clientResponse);        if (isInternalRequest && clientRequest.apiKey() == ApiKeys.METADATA)            metadataUpdater.handleFatalException(unsupportedVersionException);    }}
private voidf1312_1ClientRequest clientRequest, boolean isInternalRequest, long now, AbstractRequest request)
{    String destination = clientRequest.destination();    RequestHeader header = clientRequest.makeHeader(request.version());    if (log.isDebugEnabled()) {        int latestClientVersion = clientRequest.apiKey().latestVersion();        if (header.apiVersion() == latestClientVersion) {            log.trace("Sending {} {} with correlation id {} to node {}", clientRequest.apiKey(), request, clientRequest.correlationId(), destination);        } else {                    }    }    Send send = request.toSend(destination, header);    InFlightRequest inFlightRequest = new InFlightRequest(clientRequest, header, isInternalRequest, request, send, now);    this.inFlightRequests.add(inFlightRequest);    selector.send(send);}
public void kafkatest_f1321_0()
{    if (state.compareAndSet(State.ACTIVE, State.CLOSING)) {        wakeup();    }}
public boolean kafkatest_f1322_0()
{    return state.get() == State.ACTIVE;}
private void kafkatest_f1331_0(List<ClientResponse> responses, long now)
{    // if no response is expected then when the send is completed, return it    for (Send send : this.selector.completedSends()) {        InFlightRequest request = this.inFlightRequests.lastSent(send.destination());        if (!request.expectResponse) {            this.inFlightRequests.completeLastSent(send.destination());            responses.add(request.completed(null, now));        }    }}
private void kafkatest_f1332_0(AbstractResponse response, short apiVersion, String nodeId, long now)
{    int throttleTimeMs = response.throttleTimeMs();    if (throttleTimeMs > 0 && response.shouldClientThrottle(apiVersion)) {        connectionStates.throttle(nodeId, now + throttleTimeMs);        log.trace("Connection to node {} is throttled for {} ms until timestamp {}", nodeId, throttleTimeMs, now + throttleTimeMs);    }}
public boolean kafkatest_f1341_0(long now)
{    return !hasFetchInProgress() && this.metadata.timeToNextUpdate(now) == 0;}
private boolean kafkatest_f1342_0()
{    return inProgressRequestVersion != null;}
public ClientRequest kafkatest_f1351_0(String nodeId, AbstractRequest.Builder<?> requestBuilder, long createdTimeMs, boolean expectResponse)
{    return newClientRequest(nodeId, requestBuilder, createdTimeMs, expectResponse, defaultRequestTimeoutMs, null);}
public ClientRequest kafkatest_f1352_0(String nodeId, AbstractRequest.Builder<?> requestBuilder, long createdTimeMs, boolean expectResponse, int requestTimeoutMs, RequestCompletionHandler callback)
{    return new ClientRequest(nodeId, requestBuilder, correlation++, clientId, createdTimeMs, expectResponse, requestTimeoutMs, callback);}
public static NodeApiVersions kafkatest_f1361_0(Collection<ApiVersion> overrides)
{    List<ApiVersion> apiVersions = new LinkedList<>(overrides);    for (ApiKeys apiKey : ApiKeys.values()) {        boolean exists = false;        for (ApiVersion apiVersion : apiVersions) {            if (apiVersion.apiKey == apiKey.id) {                exists = true;                break;            }        }        if (!exists) {            apiVersions.add(new ApiVersion(apiKey));        }    }    return new NodeApiVersions(apiVersions);}
public short kafkatest_f1362_0(ApiKeys apiKey)
{    return latestUsableVersion(apiKey, apiKey.oldestVersion(), apiKey.latestVersion());}
private ByteBuffer kafkatest_f1371_0(int size)
{    boolean error = true;    try {        ByteBuffer buffer = allocateByteBuffer(size);        error = false;        return buffer;    } finally {        if (error) {            this.lock.lock();            try {                this.nonPooledAvailableMemory += size;                if (!this.waiters.isEmpty())                    this.waiters.peekFirst().signal();            } finally {                this.lock.unlock();            }        }    }}
protected ByteBuffer kafkatest_f1372_0(int size)
{    return ByteBuffer.allocate(size);}
public long kafkatest_f1381_0()
{    return this.totalMemory;}
 Deque<Condition> kafkatest_f1382_0()
{    return this.waiters;}
 RecordMetadata kafkatest_f1393_0() throws ExecutionException
{    if (this.result.error() != null)        throw new ExecutionException(this.result.error());    else        return value();}
 Long kafkatest_f1394_0()
{    return this.checksum;}
private boolean kafkatest_f1403_0(long timestamp, ByteBuffer key, ByteBuffer value, Header[] headers, Thunk thunk)
{    if (!recordsBuilder.hasRoomFor(timestamp, key, value, headers)) {        return false;    } else {        // No need to get the CRC.        this.recordsBuilder.append(timestamp, key, value, headers);        this.maxRecordSize = Math.max(this.maxRecordSize, AbstractRecords.estimateSizeInBytesUpperBound(magic(), recordsBuilder.compressionType(), key, value, headers));        FutureRecordMetadata future = new FutureRecordMetadata(this.produceFuture, this.recordCount, timestamp, thunk.future.checksumOrNull(), key == null ? -1 : key.remaining(), value == null ? -1 : value.remaining(), Time.SYSTEM);        // Chain the future to the original thunk.        thunk.future.chain(future);        this.thunks.add(thunk);        this.recordCount++;        return true;    }}
public void kafkatest_f1404_0(RuntimeException exception)
{    if (!finalState.compareAndSet(null, FinalState.ABORTED))        throw new IllegalStateException("Batch has already been completed in final state " + finalState.get());    log.trace("Aborting batch for partition {}", topicPartition, exception);    completeFutureAndFireCallbacks(ProduceResponse.INVALID_OFFSET, RecordBatch.NO_TIMESTAMP, exception);}
public FinalState kafkatest_f1413_0()
{    return this.finalState.get();}
 int kafkatest_f1414_0()
{    return attempts.get();}
public double kafkatest_f1423_0()
{    return recordsBuilder.compressionRatio();}
public boolean kafkatest_f1424_0()
{    return recordsBuilder.isFull();}
public boolean kafkatest_f1433_0()
{    return !recordsBuilder.isClosed();}
public byte kafkatest_f1434_0()
{    return recordsBuilder.magic();}
public void kafkatest_f1443_0() throws InterruptedException
{    latch.await();}
public boolean kafkatest_f1444_0(long timeout, TimeUnit unit) throws InterruptedException
{    return latch.await(timeout, unit);}
public ProducerRecord<K, V>f1453_1ProducerRecord<K, V> record)
{    ProducerRecord<K, V> interceptRecord = record;    for (ProducerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptRecord = interceptor.onSend(interceptRecord);        } catch (Exception e) {            // be careful not to throw exception from here            if (record != null)                            else                        }    }    return interceptRecord;}
public voidf1454_1RecordMetadata metadata, Exception exception)
{    for (ProducerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptor.onAcknowledgement(metadata, exception);        } catch (Exception e) {            // do not propagate interceptor exceptions, just log                    }    }}
public synchronized void kafkatest_f1463_0(int requestVersion, MetadataResponse response, long now)
{    super.update(requestVersion, response, now);    notifyAll();}
public synchronized void kafkatest_f1464_0(long now, KafkaException fatalException)
{    super.failedUpdate(now, fatalException);    if (fatalException != null)        notifyAll();}
private MemoryRecordsBuilder kafkatest_f1473_0(ByteBuffer buffer, byte maxUsableMagic)
{    if (transactionManager != null && maxUsableMagic < RecordBatch.MAGIC_VALUE_V2) {        throw new UnsupportedVersionException("Attempting to use idempotence with a broker which does not " + "support the required message format (v2). The broker must be version 0.11 or later.");    }    return MemoryRecords.builder(buffer, maxUsableMagic, compression, TimestampType.CREATE_TIME, 0L);}
private RecordAppendResult kafkatest_f1474_0(long timestamp, byte[] key, byte[] value, Header[] headers, Callback callback, Deque<ProducerBatch> deque)
{    ProducerBatch last = deque.peekLast();    if (last != null) {        FutureRecordMetadata future = last.tryAppend(timestamp, key, value, headers, callback, time.milliseconds());        if (future == null)            last.closeForRecordAppends();        else            return new RecordAppendResult(future, deque.size() > 1 || last.isFull(), false, false);    }    return null;}
public ReadyCheckResult kafkatest_f1483_0(Cluster cluster, long nowMs)
{    Set<Node> readyNodes = new HashSet<>();    long nextReadyCheckDelayMs = Long.MAX_VALUE;    Set<String> unknownLeaderTopics = new HashSet<>();    boolean exhausted = this.free.queued() > 0;    for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {        Deque<ProducerBatch> deque = entry.getValue();        synchronized (deque) {            // When producing to a large number of partitions, this path is hot and deques are often empty.            // We check whether a batch exists first to avoid the more expensive checks whenever possible.            ProducerBatch batch = deque.peekFirst();            if (batch != null) {                TopicPartition part = entry.getKey();                Node leader = cluster.leaderFor(part);                if (leader == null) {                    // This is a partition for which leader is not known, but messages are available to send.                    // Note that entries are currently not removed from batches when deque is empty.                    unknownLeaderTopics.add(part.topic());                } else if (!readyNodes.contains(leader) && !isMuted(part, nowMs)) {                    long waitedTimeMs = batch.waitedTimeMs(nowMs);                    boolean backingOff = batch.attempts() > 0 && waitedTimeMs < retryBackoffMs;                    long timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;                    boolean full = deque.size() > 1 || batch.isFull();                    boolean expired = waitedTimeMs >= timeToWaitMs;                    boolean sendable = full || expired || exhausted || closed || flushInProgress();                    if (sendable && !backingOff) {                        readyNodes.add(leader);                    } else {                        long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);                        // Note that this results in a conservative estimate since an un-sendable partition may have                        // a leader that will later be found to have sendable data. However, this is good enough                        // since we'll just wake up and then sleep again for the remaining time.                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);                    }                }            }        }    }    return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);}
public boolean kafkatest_f1484_0()
{    for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {        Deque<ProducerBatch> deque = entry.getValue();        synchronized (deque) {            if (!deque.isEmpty())                return true;        }    }    return false;}
 boolean kafkatest_f1493_0()
{    return flushesInProgress.get() > 0;}
 Map<TopicPartition, Deque<ProducerBatch>> kafkatest_f1494_0()
{    return Collections.unmodifiableMap(batches);}
public void kafkatest_f1503_0(TopicPartition tp)
{    muted.put(tp, Long.MAX_VALUE);}
public void kafkatest_f1504_0(TopicPartition tp, long throttleUntilTimeMs)
{    muted.put(tp, throttleUntilTimeMs);}
public voidf1513_1)
{        // main loop, runs until close is called    while (running) {        try {            runOnce();        } catch (Exception e) {                    }    }        // wait until these are completed.    while (!forceClose && ((this.accumulator.hasUndrained() || this.client.inFlightRequestCount() > 0) || hasPendingTransactionalRequests())) {        try {            runOnce();        } catch (Exception e) {                    }    }    // Abort the transaction if any commit or abort didn't go through the transaction manager's queue    while (!forceClose && transactionManager != null && transactionManager.hasOngoingTransaction()) {        if (!transactionManager.isCompleting()) {                        transactionManager.beginAbort();        }        try {            runOnce();        } catch (Exception e) {                    }    }    if (forceClose) {        // the futures.        if (transactionManager != null) {                        transactionManager.close();        }                this.accumulator.abortIncompleteBatches();    }    try {        this.client.close();    } catch (Exception e) {            }    }
 void kafkatest_f1514_0()
{    if (transactionManager != null) {        try {            transactionManager.resetProducerIdIfNeeded();            if (!transactionManager.isTransactional()) {                // this is an idempotent producer, so make sure we have a producer id                maybeWaitForProducerId();            } else if (transactionManager.hasUnresolvedSequences() && !transactionManager.hasFatalError()) {                transactionManager.transitionToFatalError(new KafkaException("The client hasn't received acknowledgment for " + "some previously sent messages and can no longer retry them. It isn't safe to continue."));            } else if (maybeSendAndPollTransactionalRequest()) {                return;            }            // is no producer id (for the idempotent case).            if (transactionManager.hasFatalError() || !transactionManager.hasProducerId()) {                RuntimeException lastError = transactionManager.lastError();                if (lastError != null)                    maybeAbortBatches(lastError);                client.poll(retryBackoffMs, time.milliseconds());                return;            } else if (transactionManager.hasAbortableError()) {                accumulator.abortUndrainedBatches(transactionManager.lastError());            }        } catch (AuthenticationException e) {            // This is already logged as error, but propagated here to perform any clean ups.            log.trace("Authentication exception while processing transactional request: {}", e);            transactionManager.authenticationFailed(e);        }    }    long currentTimeMs = time.milliseconds();    long pollTimeout = sendProducerData(currentTimeMs);    client.poll(pollTimeout, currentTimeMs);}
private Node kafkatest_f1523_0(FindCoordinatorRequest.CoordinatorType coordinatorType) throws IOException
{    Node node = coordinatorType != null ? transactionManager.coordinator(coordinatorType) : client.leastLoadedNode(time.milliseconds());    if (node != null && NetworkClientUtils.awaitReady(client, node, time, requestTimeoutMs)) {        return node;    }    return null;}
private voidf1524_1)
{    while (!forceClose && !transactionManager.hasProducerId() && !transactionManager.hasError()) {        Node node = null;        try {            node = awaitNodeReady(null);            if (node != null) {                ClientResponse response = sendAndAwaitInitProducerIdRequest(node);                InitProducerIdResponse initProducerIdResponse = (InitProducerIdResponse) response.responseBody();                Errors error = initProducerIdResponse.error();                if (error == Errors.NONE) {                    ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(initProducerIdResponse.data.producerId(), initProducerIdResponse.data.producerEpoch());                    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);                    return;                } else if (error.exception() instanceof RetriableException) {                                    } else {                    transactionManager.transitionToFatalError(error.exception());                    break;                }            } else {                            }        } catch (UnsupportedVersionException e) {            transactionManager.transitionToFatalError(e);            break;        } catch (IOException e) {                    }        log.trace("Retry InitProducerIdRequest in {}ms.", retryBackoffMs);        time.sleep(retryBackoffMs);        metadata.requestUpdate();    }}
private void kafkatest_f1533_0(long now, int destination, short acks, int timeout, List<ProducerBatch> batches)
{    if (batches.isEmpty())        return;    Map<TopicPartition, MemoryRecords> produceRecordsByPartition = new HashMap<>(batches.size());    final Map<TopicPartition, ProducerBatch> recordsByPartition = new HashMap<>(batches.size());    // find the minimum magic version used when creating the record sets    byte minUsedMagic = apiVersions.maxUsableProduceMagic();    for (ProducerBatch batch : batches) {        if (batch.magic() < minUsedMagic)            minUsedMagic = batch.magic();    }    for (ProducerBatch batch : batches) {        TopicPartition tp = batch.topicPartition;        MemoryRecords records = batch.records();        // which is supporting the new magic version to one which doesn't, then we will need to convert.        if (!records.hasMatchingMagic(minUsedMagic))            records = batch.records().downConvert(minUsedMagic, 0, time).records();        produceRecordsByPartition.put(tp, records);        recordsByPartition.put(tp, batch);    }    String transactionalId = null;    if (transactionManager != null && transactionManager.isTransactional()) {        transactionalId = transactionManager.transactionalId();    }    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout, produceRecordsByPartition, transactionalId);    RequestCompletionHandler callback = new RequestCompletionHandler() {        public void onComplete(ClientResponse response) {            handleProduceResponse(response, recordsByPartition, time.milliseconds());        }    };    String nodeId = Integer.toString(destination);    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, requestTimeoutMs, callback);    client.send(clientRequest, now);    log.trace("Sent produce request to {}: {}", nodeId, requestBuilder);}
public void kafkatest_f1534_0(ClientResponse response)
{    handleProduceResponse(response, recordsByPartition, time.milliseconds());}
private MetricName kafkatest_f1543_0(String name, String description)
{    return this.metrics.metricInstance(createTemplate(name, METRIC_GROUP_NAME, description, this.tags));}
private MetricNameTemplate kafkatest_f1544_0(String name, String description)
{    return createTemplate(name, TOPIC_METRIC_GROUP_NAME, description, this.topicTags);}
public MetricName kafkatest_f1553_0(Map<String, String> tags)
{    return this.metrics.metricInstance(this.topicRecordErrorTotal, tags);}
public List<MetricNameTemplate> kafkatest_f1554_0()
{    return allTemplates;}
public void kafkatest_f1563_0()
{    boolean completed = false;    while (!completed) {        try {            latch.await();            completed = true;        } catch (InterruptedException e) {        // Keep waiting until done, we have no other option for these transactional requests.        }    }    if (!isSuccessful())        throw error();}
public void kafkatest_f1564_0(long timeout, TimeUnit unit)
{    try {        boolean success = latch.await(timeout, unit);        if (!isSuccessful()) {            throw error();        }        if (!success) {            throw new TimeoutException("Timeout expired after " + timeout + unit.name().toLowerCase(Locale.ROOT) + " while awaiting " + operation);        }    } catch (InterruptedException e) {        throw new InterruptException("Received interrupt while awaiting " + operation, e);    }}
 OptionalInt kafkatest_f1573_0(TopicPartition partition)
{    TopicPartitionEntry entry = topicPartitionBookkeeping.get(partition);    if (entry != null && entry.lastAckedSequence != NO_LAST_ACKED_SEQUENCE_NUMBER)        return OptionalInt.of(entry.lastAckedSequence);    else        return OptionalInt.empty();}
public void kafkatest_f1574_0(Consumer<ProducerBatch> resetSequence)
{    TreeSet<ProducerBatch> newInflights = new TreeSet<>(Comparator.comparingInt(ProducerBatch::baseSequence));    for (ProducerBatch inflightBatch : inflightBatchesBySequence) {        resetSequence.accept(inflightBatch);        newInflights.add(inflightBatch);    }    inflightBatchesBySequence = newInflights;}
 RuntimeException kafkatest_f1583_0()
{    return lastError;}
public synchronized void kafkatest_f1584_0()
{    if (hasError())        throw new KafkaException("Cannot perform send because at least one previous transactional or " + "idempotent request has failed with errors.", lastError);    if (isTransactional()) {        if (!hasProducerId())            throw new IllegalStateException("Cannot perform a 'send' before completing a call to initTransactions " + "when transactions are enabled.");        if (currentState != State.IN_TRANSACTION)            throw new IllegalStateException("Cannot call send in state " + currentState);    }}
 synchronized voidf1593_1RuntimeException exception)
{    if (currentState == State.ABORTING_TRANSACTION) {                return;    }    transitionTo(State.ABORTABLE_ERROR, exception);}
 synchronized void kafkatest_f1594_0(RuntimeException exception)
{    transitionTo(State.FATAL_ERROR, exception);}
 synchronized Integer kafkatest_f1603_0(TopicPartition topicPartition)
{    if (!isTransactional())        topicPartitionBookkeeper.addPartition(topicPartition);    return topicPartitionBookkeeper.getPartition(topicPartition).nextSequence;}
 synchronized void kafkatest_f1604_0(TopicPartition topicPartition, int increment)
{    Integer currentSequence = sequenceNumber(topicPartition);    currentSequence = DefaultRecordBatch.incrementSequence(currentSequence, increment);    topicPartitionBookkeeper.getPartition(topicPartition).nextSequence = currentSequence;}
public synchronized voidf1613_1ProducerBatch batch, ProduceResponse.PartitionResponse response)
{    if (!hasProducerIdAndEpoch(batch.producerId(), batch.producerEpoch())) {                return;    }    maybeUpdateLastAckedSequence(batch.topicPartition, batch.baseSequence() + batch.recordCount - 1);     Set last ack'd sequence number for topic-partition {} to {}", batch.producerId(), batch.topicPartition, lastAckedSequence(batch.topicPartition).orElse(-1));    updateLastAckedOffset(response, batch);    removeInFlightBatch(batch);}
private void kafkatest_f1614_0(RuntimeException exception)
{    if (exception instanceof ClusterAuthorizationException || exception instanceof TransactionalIdAuthorizationException || exception instanceof ProducerFencedException || exception instanceof UnsupportedVersionException) {        transitionToFatalError(exception);    } else if (isTransactional()) {        transitionToAbortableError(exception);    }}
private boolean kafkatest_f1623_0(TopicPartition topicPartition, int sequence)
{    return sequence - lastAckedSequence(topicPartition).orElse(NO_LAST_ACKED_SEQUENCE_NUMBER) == 1;}
private void kafkatest_f1624_0(TopicPartition topicPartition, int sequence)
{    topicPartitionBookkeeper.getPartition(topicPartition).nextSequence = sequence;}
 boolean kafkatest_f1633_0()
{    return inFlightRequestCorrelationId != NO_INFLIGHT_REQUEST_CORRELATION_ID;}
 boolean kafkatest_f1634_0()
{    return currentState == State.FATAL_ERROR;}
private voidf1643_1State target, RuntimeException error)
{    if (!currentState.isTransitionValid(currentState, target)) {        String idString = transactionalId == null ? "" : "TransactionalId " + transactionalId + ": ";        throw new KafkaException(idString + "Invalid transition attempted from state " + currentState.name() + " to state " + target.name());    }    if (target == State.FATAL_ERROR || target == State.ABORTABLE_ERROR) {        if (error == null)            throw new IllegalArgumentException("Cannot transition to " + target + " with a null exception");        lastError = error;    } else {        lastError = null;    }    if (lastError != null)            else            currentState = target;}
private void kafkatest_f1644_0()
{    if (!isTransactional())        throw new IllegalStateException("Transactional method invoked on a non-transactional producer.");}
 void kafkatest_f1653_0(RuntimeException e)
{    result.setError(e);    transitionToFatalError(e);    result.done();}
 void kafkatest_f1654_0(RuntimeException e)
{    result.setError(e);    transitionToAbortableError(e);    result.done();}
 boolean kafkatest_f1663_0()
{    return isRetry;}
 boolean kafkatest_f1664_0()
{    return false;}
 FindCoordinatorRequest.Builder kafkatest_f1673_0()
{    return builder;}
 Priority kafkatest_f1674_0()
{    return Priority.FIND_COORDINATOR;}
 Priority kafkatest_f1683_0()
{    return Priority.ADD_PARTITIONS_OR_OFFSETS;}
public voidf1684_1AbstractResponse response)
{    AddOffsetsToTxnResponse addOffsetsToTxnResponse = (AddOffsetsToTxnResponse) response;    Errors error = addOffsetsToTxnResponse.error();    if (error == Errors.NONE) {                // note the result is not completed until the TxnOffsetCommit returns        pendingRequests.add(txnOffsetCommitHandler(result, offsets, builder.consumerGroupId()));        transactionStarted = true;    } else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) {        lookupCoordinator(FindCoordinatorRequest.CoordinatorType.TRANSACTION, transactionalId);        reenqueue();    } else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.CONCURRENT_TRANSACTIONS) {        reenqueue();    } else if (error == Errors.INVALID_PRODUCER_EPOCH) {        fatalError(error.exception());    } else if (error == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED) {        fatalError(error.exception());    } else if (error == Errors.GROUP_AUTHORIZATION_FAILED) {        abortableError(GroupAuthorizationException.forGroupId(builder.consumerGroupId()));    } else {        fatalError(new KafkaException("Unexpected error in AddOffsetsToTxnResponse: " + error.message()));    }}
private static TransactionManagerf1693_1ProducerConfig config, LogContext logContext, Logger log)
{    TransactionManager transactionManager = null;    boolean userConfiguredIdempotence = false;    if (config.originals().containsKey(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG))        userConfiguredIdempotence = true;    boolean userConfiguredTransactions = false;    if (config.originals().containsKey(ProducerConfig.TRANSACTIONAL_ID_CONFIG))        userConfiguredTransactions = true;    boolean idempotenceEnabled = config.getBoolean(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG);    if (!idempotenceEnabled && userConfiguredIdempotence && userConfiguredTransactions)        throw new ConfigException("Cannot set a " + ProducerConfig.TRANSACTIONAL_ID_CONFIG + " without also enabling idempotence.");    if (userConfiguredTransactions)        idempotenceEnabled = true;    if (idempotenceEnabled) {        String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG);        int transactionTimeoutMs = config.getInt(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG);        long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);        transactionManager = new TransactionManager(logContext, transactionalId, transactionTimeoutMs, retryBackoffMs);        if (transactionManager.isTransactional())                    else                }    return transactionManager;}
private static intf1694_1ProducerConfig config, boolean idempotenceEnabled, Logger log)
{    boolean userConfiguredRetries = false;    if (config.originals().containsKey(ProducerConfig.RETRIES_CONFIG)) {        userConfiguredRetries = true;    }    if (idempotenceEnabled && !userConfiguredRetries) {        // We recommend setting infinite retries when the idempotent producer is enabled, so it makes sense to make        // this the default.                return Integer.MAX_VALUE;    }    if (idempotenceEnabled && config.getInt(ProducerConfig.RETRIES_CONFIG) == 0) {        throw new ConfigException("Must set " + ProducerConfig.RETRIES_CONFIG + " to non-zero when using the idempotent producer.");    }    return config.getInt(ProducerConfig.RETRIES_CONFIG);}
public Future<RecordMetadata> kafkatest_f1703_0(ProducerRecord<K, V> record)
{    return send(record, null);}
public Future<RecordMetadata> kafkatest_f1704_0(ProducerRecord<K, V> record, Callback callback)
{    // intercept the record, which can be potentially modified; this method does not throw exceptions    ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);    return doSend(interceptedRecord, callback);}
public void kafkatest_f1713_0()
{    close(Duration.ofMillis(Long.MAX_VALUE));}
public void kafkatest_f1714_0(Duration timeout)
{    close(timeout, false);}
public RecordMetadata kafkatest_f1723_0(long timeout, TimeUnit unit) throws ExecutionException
{    throw this.exception;}
public boolean kafkatest_f1724_0()
{    return false;}
private void kafkatest_f1733_0()
{    if (!this.transactionInitialized) {        throw new IllegalStateException("MockProducer hasn't been initialized for transactions.");    }}
private void kafkatest_f1734_0()
{    if (!this.transactionInFlight) {        throw new IllegalStateException("There is no open transaction.");    }}
public void kafkatest_f1743_0(Duration timeout)
{    if (producerFencedOnClose) {        throw new ProducerFencedException("MockProducer is fenced.");    }    this.closed = true;}
public boolean kafkatest_f1744_0()
{    return this.closed;}
public long kafkatest_f1753_0()
{    return this.commitCount;}
public synchronized List<ProducerRecord<K, V>> kafkatest_f1754_0()
{    return new ArrayList<>(this.sent);}
public static Map<String, Object> kafkatest_f1764_0(Map<String, Object> configs, Serializer<?> keySerializer, Serializer<?> valueSerializer)
{    Map<String, Object> newConfigs = new HashMap<>(configs);    if (keySerializer != null)        newConfigs.put(KEY_SERIALIZER_CLASS_CONFIG, keySerializer.getClass());    if (valueSerializer != null)        newConfigs.put(VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer.getClass());    return newConfigs;}
public static Properties kafkatest_f1765_0(Properties properties, Serializer<?> keySerializer, Serializer<?> valueSerializer)
{    Properties newProperties = new Properties();    newProperties.putAll(properties);    if (keySerializer != null)        newProperties.put(KEY_SERIALIZER_CLASS_CONFIG, keySerializer.getClass().getName());    if (valueSerializer != null)        newProperties.put(VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer.getClass().getName());    return newProperties;}
public Integer kafkatest_f1774_0()
{    return partition;}
public String kafkatest_f1775_0()
{    String headers = this.headers == null ? "null" : this.headers.toString();    String key = this.key == null ? "null" : this.key.toString();    String value = this.value == null ? "null" : this.value.toString();    String timestamp = this.timestamp == null ? "null" : this.timestamp.toString();    return "ProducerRecord(topic=" + topic + ", partition=" + partition + ", headers=" + headers + ", key=" + key + ", value=" + value + ", timestamp=" + timestamp + ")";}
public int kafkatest_f1784_0()
{    return this.serializedValueSize;}
public String kafkatest_f1785_0()
{    return this.topicPartition.topic();}
public AclOperation kafkatest_f1798_0()
{    return data.operation();}
public AclPermissionType kafkatest_f1799_0()
{    return data.permissionType();}
 AclPermissionType kafkatest_f1808_0()
{    return permissionType;}
public String kafkatest_f1809_0()
{    if (principal() == null)        return "Principal is NULL";    if (host() == null)        return "Host is NULL";    if (operation() == AclOperation.ANY)        return "Operation is ANY";    if (operation() == AclOperation.UNKNOWN)        return "Operation is UNKNOWN";    if (permissionType() == AclPermissionType.ANY)        return "Permission type is ANY";    if (permissionType() == AclPermissionType.UNKNOWN)        return "Permission type is UNKNOWN";    return null;}
public String kafkatest_f1818_0()
{    return data.toString();}
public boolean kafkatest_f1819_0()
{    return data.isUnknown();}
public AclBindingFilter kafkatest_f1828_0()
{    return new AclBindingFilter(pattern.toFilter(), entry.toFilter());}
public String kafkatest_f1829_0()
{    return "(pattern=" + pattern + ", entry=" + entry + ")";}
public String kafkatest_f1838_0()
{    String indefinite = patternFilter.findIndefiniteField();    if (indefinite != null)        return indefinite;    return entryFilter.findIndefiniteField();}
public boolean kafkatest_f1839_0(AclBinding binding)
{    return patternFilter.matches(binding.pattern()) && entryFilter.matches(binding.entry());}
public boolean kafkatest_f1848_0()
{    return this == UNKNOWN;}
protected boolean kafkatest_f1849_0(Map.Entry<K, V> eldest)
{    // require this. prefix to make lgtm.com happy    return this.size() > maxSize;}
public static Cluster kafkatest_f1858_0()
{    return new Cluster(null, new ArrayList<>(0), new ArrayList<>(0), Collections.emptySet(), Collections.emptySet(), null);}
public static Cluster kafkatest_f1859_0(List<InetSocketAddress> addresses)
{    List<Node> nodes = new ArrayList<>();    int nodeId = -1;    for (InetSocketAddress address : addresses) nodes.add(new Node(nodeId--, address.getHostString(), address.getPort()));    return new Cluster(null, true, nodes, new ArrayList<>(0), Collections.emptySet(), Collections.emptySet(), Collections.emptySet(), null);}
public List<PartitionInfo> kafkatest_f1868_0(String topic)
{    return availablePartitionsByTopic.getOrDefault(topic, Collections.emptyList());}
public List<PartitionInfo> kafkatest_f1869_0(int nodeId)
{    return partitionsByNode.getOrDefault(nodeId, Collections.emptyList());}
public boolean kafkatest_f1878_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Cluster cluster = (Cluster) o;    return isBootstrapConfigured == cluster.isBootstrapConfigured && Objects.equals(nodes, cluster.nodes) && Objects.equals(unauthorizedTopics, cluster.unauthorizedTopics) && Objects.equals(invalidTopics, cluster.invalidTopics) && Objects.equals(internalTopics, cluster.internalTopics) && Objects.equals(controller, cluster.controller) && Objects.equals(partitionsByTopicPartition, cluster.partitionsByTopicPartition) && Objects.equals(clusterResource, cluster.clusterResource);}
public int kafkatest_f1879_0()
{    return Objects.hash(isBootstrapConfigured, nodes, unauthorizedTopics, invalidTopics, internalTopics, controller, partitionsByTopicPartition, clusterResource);}
public Integer kafkatest_f1888_0(String key)
{    return (Integer) get(key);}
public Long kafkatest_f1889_0(String key)
{    return (Long) get(key);}
public Map<String, Object> kafkatest_f1898_0()
{    Map<String, Object> copy = new RecordingMap<>();    copy.putAll(originals);    return copy;}
public Map<String, String> kafkatest_f1899_0()
{    Map<String, String> copy = new RecordingMap<>();    for (Map.Entry<String, ?> entry : originals.entrySet()) {        if (!(entry.getValue() instanceof String))            throw new ClassCastException("Non-string value found in original settings for key " + entry.getKey() + ": " + (entry.getValue() == null ? null : entry.getValue().getClass().getName()));        copy.put(entry.getKey(), (String) entry.getValue());    }    return copy;}
public T kafkatest_f1908_0(String key, Class<T> t)
{    Class<?> c = getClass(key);    return getConfiguredInstance(c, t, originals());}
public List<T> kafkatest_f1909_0(String key, Class<T> t)
{    return getConfiguredInstances(key, t, Collections.emptyMap());}
public V kafkatest_f1918_0(Object key)
{    if (key instanceof String) {        String stringKey = (String) key;        String keyWithPrefix;        if (prefix.isEmpty()) {            keyWithPrefix = stringKey;        } else {            keyWithPrefix = prefix + stringKey;        }        ignore(keyWithPrefix);        if (withIgnoreFallback)            ignore(stringKey);    }    return super.get(key);}
public V kafkatest_f1919_0(Object key)
{    if (key instanceof String && originals.containsKey(key)) {        // Intentionally ignore the result; call just to mark the original entry as used        originals.get(key);    }    // But always use the resolved entry    return super.get(key);}
public ConfigDef kafkatest_f1928_0(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation, String group, int orderInGroup, Width width, String displayName, Recommender recommender)
{    return define(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList(), recommender);}
public ConfigDef kafkatest_f1929_0(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation, String group, int orderInGroup, Width width, String displayName)
{    return define(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, Collections.<String>emptyList());}
public ConfigDef kafkatest_f1938_0(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation)
{    return define(name, type, defaultValue, validator, importance, documentation, null, -1, Width.NONE, name);}
public ConfigDef kafkatest_f1939_0(String name, Type type, Object defaultValue, Importance importance, String documentation)
{    return define(name, type, defaultValue, null, importance, documentation);}
public List<ConfigValue> kafkatest_f1948_0(Map<String, String> props)
{    return new ArrayList<>(validateAll(props).values());}
public Map<String, ConfigValue> kafkatest_f1949_0(Map<String, String> props)
{    Map<String, ConfigValue> configValues = new HashMap<>();    for (String name : configKeys.keySet()) {        configValues.put(name, new ConfigValue(name));    }    List<String> undefinedConfigKeys = undefinedDependentConfigs();    for (String undefinedConfigKey : undefinedConfigKeys) {        ConfigValue undefinedConfigValue = new ConfigValue(undefinedConfigKey);        undefinedConfigValue.addErrorMessage(undefinedConfigKey + " is referred in the dependents, but not defined.");        undefinedConfigValue.visible(false);        configValues.put(undefinedConfigKey, undefinedConfigValue);    }    Map<String, Object> parsed = parseForValidate(props, configValues);    return validate(parsed, configValues);}
public static Map<String, String> kafkatest_f1958_0(Map<String, ?> configs)
{    Map<String, String> result = new HashMap<>();    for (Map.Entry<String, ?> entry : configs.entrySet()) {        Object value = entry.getValue();        String strValue;        if (value instanceof Password)            strValue = ((Password) value).value();        else if (value instanceof List)            strValue = convertToString(value, Type.LIST);        else if (value instanceof Class)            strValue = convertToString(value, Type.CLASS);        else            strValue = convertToString(value, null);        if (strValue != null)            result.put(entry.getKey(), strValue);    }    return result;}
public static Range kafkatest_f1959_0(Number min)
{    return new Range(min, null);}
public String kafkatest_f1968_0()
{    return "[" + Utils.join(validStrings, ", ") + "]";}
public void kafkatest_f1969_0(String name, Object value)
{    if (value == null) {        // Pass in the string null to avoid the spotbugs warning        throw new ConfigException(name, "null", "entry must be non null");    }}
public String kafkatest_f1978_0()
{    return "non-empty string";}
public static NonEmptyStringWithoutControlChars kafkatest_f1979_0()
{    return new NonEmptyStringWithoutControlChars();}
private void kafkatest_f1988_0(StringBuilder builder, String value)
{    builder.append("<td>");    builder.append(value);    builder.append("</td>");}
public String kafkatest_f1989_0(Map<String, String> dynamicUpdateModes)
{    boolean hasUpdateModes = !dynamicUpdateModes.isEmpty();    List<ConfigKey> configs = sortedConfigs();    StringBuilder b = new StringBuilder();    b.append("<table class=\"data-table\"><tbody>\n");    b.append("<tr>\n");    // print column headers    for (String headerName : headers()) {        addHeader(b, headerName);    }    if (hasUpdateModes)        addHeader(b, "Dynamic Update Mode");    b.append("</tr>\n");    for (ConfigKey key : configs) {        if (key.internalConfig) {            continue;        }        b.append("<tr>\n");        // print column values        for (String headerName : headers()) {            addColumnValue(b, getConfigValue(key, headerName));            b.append("</td>");        }        if (hasUpdateModes) {            String updateMode = dynamicUpdateModes.get(key.name);            if (updateMode == null)                updateMode = "read-only";            addColumnValue(b, updateMode);        }        b.append("</tr>\n");    }    b.append("</tbody></table>");    return b.toString();}
public String kafkatest_f1998_0()
{    return base.toString();}
private static List<String> kafkatest_f1999_0(final String keyPrefix, final List<String> dependents)
{    if (dependents == null)        return null;    final List<String> updatedDependents = new ArrayList<>(dependents.size());    for (String dependent : dependents) {        updatedDependents.add(keyPrefix + dependent);    }    return updatedDependents;}
public byte kafkatest_f2008_0()
{    return id;}
public static Type kafkatest_f2009_0(final byte id)
{    return TYPES.getOrDefault(id, UNKNOWN);}
private static String kafkatest_f2018_0(Map<String, Map<String, Map<String, String>>> lookupsByProvider, String value, Pattern pattern)
{    if (value == null) {        return null;    }    Matcher matcher = pattern.matcher(value);    StringBuilder builder = new StringBuilder();    int i = 0;    while (matcher.find()) {        ConfigVariable configVar = new ConfigVariable(matcher);        Map<String, Map<String, String>> lookupsByPath = lookupsByProvider.get(configVar.providerName);        if (lookupsByPath != null) {            Map<String, String> keyValues = lookupsByPath.get(configVar.path);            String replacement = keyValues.get(configVar.variable);            builder.append(value, i, matcher.start());            if (replacement == null) {                // No replacements will be performed; just return the original value                builder.append(matcher.group(0));            } else {                builder.append(replacement);            }            i = matcher.end();        }    }    builder.append(value, i, value.length());    return builder.toString();}
public String kafkatest_f2019_0()
{    return "(" + providerName + ":" + (path != null ? path + ":" : "") + variable + ")";}
public void kafkatest_f2028_0(List<Object> recommendedValues)
{    this.recommendedValues = recommendedValues;}
public void kafkatest_f2029_0(String errorMessage)
{    this.errorMessages.add(errorMessage);}
public ConfigData kafkatest_f2039_0(String path, Set<String> keys)
{    Map<String, String> data = new HashMap<>();    if (path == null || path.isEmpty()) {        return new ConfigData(data);    }    try (Reader reader = reader(path)) {        Properties properties = new Properties();        properties.load(reader);        for (String key : keys) {            String value = properties.getProperty(key);            if (value != null) {                data.put(key, value);            }        }        return new ConfigData(data);    } catch (IOException e) {        throw new ConfigException("Could not read properties from file " + path);    }}
protected Reader kafkatest_f2040_0(String path) throws IOException
{    return Files.newBufferedReader(Paths.get(path));}
public String kafkatest_f2050_0()
{    return name;}
public static ElectionType kafkatest_f2051_0(byte value)
{    if (value == PREFERRED.value) {        return PREFERRED;    } else if (value == UNCLEAN.value) {        return UNCLEAN;    } else {        throw new IllegalArgumentException(String.format("Value %s must be one of %s", value, Arrays.asList(ElectionType.values())));    }}
public String kafkatest_f2060_0()
{    return groupId;}
public static GroupAuthorizationException kafkatest_f2061_0(String groupId)
{    return new GroupAuthorizationException("Not authorized to access group: " + groupId, groupId);}
public String kafkatest_f2070_0()
{    return "RecordHeader(key = " + key + ", value = " + Arrays.toString(value()) + ")";}
public Headers kafkatest_f2071_0(Header header) throws IllegalStateException
{    Objects.requireNonNull(header, "Header cannot be null.");    canWrite();    headers.add(header);    return this;}
private void kafkatest_f2080_0()
{    if (isReadOnly)        throw new IllegalStateException("RecordHeaders has been closed.");}
private Iterator<Header> kafkatest_f2081_0(final Iterator<Header> original)
{    return new Iterator<Header>() {        @Override        public boolean hasNext() {            return original.hasNext();        }        public Header next() {            return original.next();        }        @Override        public void remove() {            canWrite();            original.remove();        }    };}
public void kafkatest_f2090_0(List<?> candidateList)
{    for (Object candidate : candidateList) {        this.maybeAdd(candidate);    }}
public void kafkatest_f2091_0(ClusterResource cluster)
{    for (ClusterResourceListener clusterResourceListener : clusterResourceListeners) {        clusterResourceListener.onUpdate(cluster);    }}
public KafkaFuture<R> kafkatest_f2100_0(Function<T, R> function)
{    return thenApply((BaseFunction<T, R>) function);}
public void kafkatest_f2101_0(T val, Throwable exception)
{    try {        if (exception != null) {            biConsumer.accept(null, exception);        } else {            biConsumer.accept(val, null);        }    } catch (Throwable e) {        if (exception == null) {            exception = e;        }    }    if (exception != null) {        future.completeExceptionally(exception);    } else {        future.complete(val);    }}
public synchronized boolean kafkatest_f2110_0()
{    return exception instanceof CancellationException;}
public synchronized boolean kafkatest_f2111_0()
{    return exception != null;}
public List<PartitionState<S>> kafkatest_f2120_0()
{    List<PartitionState<S>> result = new ArrayList<>(map.size());    for (Map.Entry<TopicPartition, S> entry : map.entrySet()) {        result.add(new PartitionState<>(entry.getKey(), entry.getValue()));    }    return result;}
public Stream<PartitionState<S>> kafkatest_f2121_0()
{    return map.entrySet().stream().map(entry -> new PartitionState<>(entry.getKey(), entry.getValue()));}
public boolean kafkatest_f2130_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    PartitionState<?> that = (PartitionState<?>) o;    return topicPartition.equals(that.topicPartition) && value.equals(that.value);}
public int kafkatest_f2131_0()
{    int result = topicPartition.hashCode();    result = 31 * result + value.hashCode();    return result;}
private void kafkatest_f2140_0()
{    if (remainingResponses <= 0)        future.complete(null);}
public static KafkaFuture<U> kafkatest_f2141_0(U value)
{    KafkaFuture<U> future = new KafkaFutureImpl<U>();    future.complete(value);    return future;}
public ByteBuffer kafkatest_f2150_0(int sizeBytes)
{    return ByteBuffer.allocate(sizeBytes);}
public void kafkatest_f2151_0(ByteBuffer previouslyAllocated)
{// nop}
public boolean kafkatest_f2160_0()
{    return availableMemory.get() <= 0;}
protected void kafkatest_f2161_0(ByteBuffer justAllocated)
{    log.trace("allocated buffer of size {} ", justAllocated.capacity());}
public boolean kafkatest_f2170_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    MetricName other = (MetricName) obj;    return group.equals(other.group) && name.equals(other.name) && tags.equals(other.tags);}
public String kafkatest_f2171_0()
{    return "MetricName [name=" + name + ", group=" + group + ", description=" + description + ", tags=" + tags + "]";}
public MetricName kafkatest_f2180_0()
{    return name;}
public Measurable kafkatest_f2181_0()
{    return stat;}
private void kafkatest_f2191_0(KafkaMbean mbean)
{    MBeanServer server = ManagementFactory.getPlatformMBeanServer();    try {        if (server.isRegistered(mbean.name()))            server.unregisterMBean(mbean.name());    } catch (JMException e) {        throw new KafkaException("Error unregistering mbean", e);    }}
private void kafkatest_f2192_0(KafkaMbean mbean)
{    unregister(mbean);    try {        ManagementFactory.getPlatformMBeanServer().registerMBean(mbean, mbean.name());    } catch (JMException e) {        throw new KafkaException("Error registering mbean " + mbean.name(), e);    }}
public AttributeList kafkatest_f2201_0(AttributeList list)
{    throw new UnsupportedOperationException("Set not allowed.");}
public MetricConfig kafkatest_f2202_0()
{    return this.config;}
public long kafkatest_f2211_0()
{    return eventWindow;}
public MetricConfig kafkatest_f2212_0(long window)
{    this.eventWindow = window;    return this;}
public Thread kafkatest_f2221_0(Runnable runnable)
{    return KafkaThread.daemon("SensorExpiryThread", runnable);}
public double kafkatest_f2222_0(MetricConfig config, long now)
{    return metrics.size();}
public Sensor kafkatest_f2231_0(String name)
{    return this.sensors.get(Objects.requireNonNull(name));}
public Sensor kafkatest_f2232_0(String name)
{    return this.sensor(name, Sensor.RecordingLevel.INFO);}
public void kafkatest_f2241_0(MetricName metricName, Measurable measurable)
{    addMetric(metricName, null, measurable);}
public void kafkatest_f2242_0(MetricName metricName, MetricConfig config, Measurable measurable)
{    addMetric(metricName, config, (MetricValueProvider<?>) measurable);}
public KafkaMetric kafkatest_f2251_0(MetricName metricName)
{    return this.metrics.get(metricName);}
public voidf2252_1)
{    for (Map.Entry<String, Sensor> sensorEntry : sensors.entrySet()) {        // and thus not necessary to optimize        synchronized (sensorEntry.getValue()) {            if (sensorEntry.getValue().hasExpired()) {                                removeSensor(sensorEntry.getKey());            }        }    }}
public boolean kafkatest_f2261_0(double value)
{    return (upper && value <= bound) || (!upper && value >= bound);}
public int kafkatest_f2262_0()
{    final int prime = 31;    int result = 1;    result = prime * result + (int) this.bound;    result = prime * result + (this.upper ? 1 : 0);    return result;}
private void kafkatest_f2271_0(Set<Sensor> sensors)
{    if (!sensors.add(this))        throw new IllegalArgumentException("Circular dependency in sensors: " + name() + " is its own parent.");    for (Sensor parent : parents) parent.checkForest(sensors);}
public String kafkatest_f2272_0()
{    return this.name;}
public boolean kafkatest_f2281_0(CompoundStat stat)
{    return add(stat, null);}
public synchronized boolean kafkatest_f2282_0(CompoundStat stat, MetricConfig config)
{    if (hasExpired())        return false;    this.stats.add(Objects.requireNonNull(stat));    Object lock = metricLock();    for (NamedMeasurable m : stat.stats()) {        final KafkaMetric metric = new KafkaMetric(lock, m.name(), m.stat(), config == null ? this.config : config, time);        if (!metrics.containsKey(metric.metricName())) {            registry.registerMetric(metric);            metrics.put(metric.metricName(), metric);        }    }    return true;}
public void kafkatest_f2291_0(MetricConfig config, double value, long now)
{    total += value;}
public double kafkatest_f2292_0(MetricConfig config, long now)
{    return total;}
public void kafkatest_f2301_0(long now)
{    super.reset(now);    histogram.clear();}
public MetricName kafkatest_f2302_0()
{    return this.name;}
public int kafkatest_f2311_0(double x)
{    int binNumber = (int) ((x - min) / bucketWidth);    if (binNumber < MIN_BIN_NUMBER) {        return MIN_BIN_NUMBER;    }    if (binNumber > maxBinNumber) {        return maxBinNumber;    }    return binNumber;}
public int kafkatest_f2312_0()
{    return this.bins;}
public MetricName kafkatest_f2321_0()
{    return this.name;}
public double kafkatest_f2322_0()
{    return this.percentile;}
public void kafkatest_f2331_0(MetricConfig config, double value, long timeMs)
{    this.stat.record(config, value, timeMs);}
public double kafkatest_f2332_0(MetricConfig config, long now)
{    double value = stat.measure(config, now);    return value / convert(windowSize(config, now));}
protected void kafkatest_f2341_0(MetricConfig config, long now)
{    long expireAge = config.samples() * config.timeWindowMs();    for (Sample sample : samples) {        if (now - sample.lastWindowMs >= expireAge)            sample.reset(now);    }}
public void kafkatest_f2342_0(long now)
{    this.eventCount = 0;    this.lastWindowMs = now;    this.value = initialValue;}
 Long kafkatest_f2352_0()
{    return null;}
 Long kafkatest_f2353_0()
{    return null;}
public static ChannelBuilder kafkatest_f2362_0(ListenerName listenerName, boolean isInterBrokerListener, SecurityProtocol securityProtocol, AbstractConfig config, CredentialCache credentialCache, DelegationTokenCache tokenCache, Time time)
{    return create(securityProtocol, Mode.SERVER, JaasContext.Type.SERVER, config, listenerName, isInterBrokerListener, null, true, credentialCache, tokenCache, time);}
private static ChannelBuilder kafkatest_f2363_0(SecurityProtocol securityProtocol, Mode mode, JaasContext.Type contextType, AbstractConfig config, ListenerName listenerName, boolean isInterBrokerListener, String clientSaslMechanism, boolean saslHandshakeRequestEnable, CredentialCache credentialCache, DelegationTokenCache tokenCache, Time time)
{    Map<String, ?> configs;    if (listenerName == null)        configs = config.values();    else        configs = config.valuesWithPrefixOverride(listenerName.configPrefix());    ChannelBuilder channelBuilder;    switch(securityProtocol) {        case SSL:            requireNonNullMode(mode, securityProtocol);            channelBuilder = new SslChannelBuilder(mode, listenerName, isInterBrokerListener);            break;        case SASL_SSL:        case SASL_PLAINTEXT:            requireNonNullMode(mode, securityProtocol);            Map<String, JaasContext> jaasContexts;            if (mode == Mode.SERVER) {                @SuppressWarnings("unchecked")                List<String> enabledMechanisms = (List<String>) configs.get(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG);                jaasContexts = new HashMap<>(enabledMechanisms.size());                for (String mechanism : enabledMechanisms) jaasContexts.put(mechanism, JaasContext.loadServerContext(listenerName, mechanism, configs));            } else {                // Use server context for inter-broker client connections and client context for other clients                JaasContext jaasContext = contextType == JaasContext.Type.CLIENT ? JaasContext.loadClientContext(configs) : JaasContext.loadServerContext(listenerName, clientSaslMechanism, configs);                jaasContexts = Collections.singletonMap(clientSaslMechanism, jaasContext);            }            channelBuilder = new SaslChannelBuilder(mode, jaasContexts, securityProtocol, listenerName, isInterBrokerListener, clientSaslMechanism, saslHandshakeRequestEnable, credentialCache, tokenCache, time);            break;        case PLAINTEXT:            channelBuilder = new PlaintextChannelBuilder(listenerName);            break;        default:            throw new IllegalArgumentException("Unexpected securityProtocol " + securityProtocol);    }    channelBuilder.configure(configs);    return channelBuilder;}
public void kafkatest_f2372_0() throws AuthenticationException, IOException
{    boolean authenticating = false;    try {        if (!transportLayer.ready())            transportLayer.handshake();        if (transportLayer.ready() && !authenticator.complete()) {            authenticating = true;            authenticator.authenticate();        }    } catch (AuthenticationException e) {        // Clients are notified of authentication exceptions to enable operations to be terminated        // without retries. Other errors are handled as network exceptions in Selector.        String remoteDesc = remoteAddress != null ? remoteAddress.toString() : null;        state = new ChannelState(ChannelState.State.AUTHENTICATION_FAILED, e, remoteDesc);        if (authenticating) {            delayCloseOnAuthenticationFailure();            throw new DelayedResponseAuthenticationException(e);        }        throw e;    }    if (ready()) {        ++successfulAuthentications;        state = ChannelState.READY;    }}
public void kafkatest_f2373_0()
{    disconnected = true;    if (state == ChannelState.NOT_CONNECTED && remoteAddress != null) {        // if we captured the remote address we can provide more information        state = new ChannelState(ChannelState.State.NOT_CONNECTED, remoteAddress.toString());    }    transportLayer.disconnect();}
public void kafkatest_f2382_0(ChannelMuteEvent event)
{    boolean stateChanged = false;    switch(event) {        case REQUEST_RECEIVED:            if (muteState == ChannelMuteState.MUTED) {                muteState = ChannelMuteState.MUTED_AND_RESPONSE_PENDING;                stateChanged = true;            }            break;        case RESPONSE_SENT:            if (muteState == ChannelMuteState.MUTED_AND_RESPONSE_PENDING) {                muteState = ChannelMuteState.MUTED;                stateChanged = true;            }            if (muteState == ChannelMuteState.MUTED_AND_THROTTLED_AND_RESPONSE_PENDING) {                muteState = ChannelMuteState.MUTED_AND_THROTTLED;                stateChanged = true;            }            break;        case THROTTLE_STARTED:            if (muteState == ChannelMuteState.MUTED_AND_RESPONSE_PENDING) {                muteState = ChannelMuteState.MUTED_AND_THROTTLED_AND_RESPONSE_PENDING;                stateChanged = true;            }            break;        case THROTTLE_ENDED:            if (muteState == ChannelMuteState.MUTED_AND_THROTTLED) {                muteState = ChannelMuteState.MUTED;                stateChanged = true;            }            if (muteState == ChannelMuteState.MUTED_AND_THROTTLED_AND_RESPONSE_PENDING) {                muteState = ChannelMuteState.MUTED_AND_RESPONSE_PENDING;                stateChanged = true;            }    }    if (!stateChanged) {        throw new IllegalStateException("Cannot transition from " + muteState.name() + " for " + event.name());    }}
public ChannelMuteState kafkatest_f2383_0()
{    return muteState;}
public void kafkatest_f2392_0(Send send)
{    if (this.send != null)        throw new IllegalStateException("Attempt to begin a send operation with prior send operation still in progress, connection id is " + id);    this.send = send;    this.transportLayer.addInterestOps(SelectionKey.OP_WRITE);}
public NetworkReceive kafkatest_f2393_0() throws IOException
{    NetworkReceive result = null;    if (receive == null) {        receive = new NetworkReceive(maxReceiveSize, id, memoryPool);    }    receive(receive);    if (receive.complete()) {        receive.payload().rewind();        result = receive;        receive = null;    } else if (receive.requiredMemoryAmountKnown() && !receive.memoryAllocated() && isInMutableState()) {        // pool must be out of memory, mute ourselves.        mute();    }    return result;}
public String kafkatest_f2402_0()
{    return super.toString() + " id=" + id;}
public int kafkatest_f2403_0()
{    return successfulAuthentications;}
public static ListenerName kafkatest_f2412_0(String value)
{    return new ListenerName(value.toUpperCase(Locale.ROOT));}
public String kafkatest_f2413_0()
{    return value;}
public long kafkatest_f2422_0(ScatteringByteChannel channel) throws IOException
{    int read = 0;    if (size.hasRemaining()) {        int bytesRead = channel.read(size);        if (bytesRead < 0)            throw new EOFException();        read += bytesRead;        if (!size.hasRemaining()) {            size.rewind();            int receiveSize = size.getInt();            if (receiveSize < 0)                throw new InvalidReceiveException("Invalid receive (size = " + receiveSize + ")");            if (maxSize != UNLIMITED && receiveSize > maxSize)                throw new InvalidReceiveException("Invalid receive (size = " + receiveSize + " larger than " + maxSize + ")");            // may be 0 for some payloads (SASL)            requestedBufferSize = receiveSize;            if (receiveSize == 0) {                buffer = EMPTY_BUFFER;            }        }    }    if (buffer == null && requestedBufferSize != -1) {        // we know the size we want but havent been able to allocate it yet        buffer = memoryPool.tryAllocate(requestedBufferSize);        if (buffer == null)            log.trace("Broker low on memory - could not allocate buffer of size {} for source {}", requestedBufferSize, source);    }    if (buffer != null) {        int bytesRead = channel.read(buffer);        if (bytesRead < 0)            throw new EOFException();        read += bytesRead;    }    return read;}
public boolean kafkatest_f2423_0()
{    return requestedBufferSize != -1;}
public boolean kafkatest_f2434_0()
{    return true;}
public void kafkatest_f2435_0()
{    if (principalBuilder instanceof Closeable)        Utils.closeQuietly((Closeable) principalBuilder, "principal builder");}
public int kafkatest_f2445_0(ByteBuffer dst) throws IOException
{    return socketChannel.read(dst);}
public long kafkatest_f2446_0(ByteBuffer[] dsts) throws IOException
{    return socketChannel.read(dsts);}
public boolean kafkatest_f2455_0()
{    return key.isValid() && (key.interestOps() & SelectionKey.OP_READ) == 0;}
public boolean kafkatest_f2456_0()
{    return false;}
public ListenerName kafkatest_f2465_0()
{    return listenerName;}
public KafkaChannelf2466_1String id, SelectionKey key, int maxReceiveSize, MemoryPool memoryPool) throws KafkaException
{    try {        SocketChannel socketChannel = (SocketChannel) key.channel();        Socket socket = socketChannel.socket();        TransportLayer transportLayer = buildTransportLayer(id, key, socketChannel);        Supplier<Authenticator> authenticatorCreator;        if (mode == Mode.SERVER) {            authenticatorCreator = () -> buildServerAuthenticator(configs, Collections.unmodifiableMap(saslCallbackHandlers), id, transportLayer, Collections.unmodifiableMap(subjects), Collections.unmodifiableMap(connectionsMaxReauthMsByMechanism));        } else {            LoginManager loginManager = loginManagers.get(clientSaslMechanism);            authenticatorCreator = () -> buildClientAuthenticator(configs, saslCallbackHandlers.get(clientSaslMechanism), id, socket.getInetAddress().getHostName(), loginManager.serviceName(), transportLayer, subjects.get(clientSaslMechanism));        }        return new KafkaChannel(id, transportLayer, authenticatorCreator, maxReceiveSize, memoryPool != null ? memoryPool : MemoryPool.NONE);    } catch (Exception e) {                throw new KafkaException(e);    }}
private void kafkatest_f2475_0(Map<String, ?> configs)
{    for (String mechanism : jaasContexts.keySet()) {        String prefix = ListenerName.saslMechanismPrefix(mechanism);        Long connectionsMaxReauthMs = (Long) configs.get(prefix + BrokerSecurityConfigs.CONNECTIONS_MAX_REAUTH_MS);        if (connectionsMaxReauthMs == null)            connectionsMaxReauthMs = (Long) configs.get(BrokerSecurityConfigs.CONNECTIONS_MAX_REAUTH_MS);        if (connectionsMaxReauthMs != null)            connectionsMaxReauthMsByMechanism.put(mechanism, connectionsMaxReauthMs);    }}
private Class<? extends Login> kafkatest_f2476_0(Map<String, ?> configs)
{    if (jaasContexts.containsKey(SaslConfigs.GSSAPI_MECHANISM))        return KerberosLogin.class;    if (OAuthBearerLoginModule.OAUTHBEARER_MECHANISM.equals(clientSaslMechanism))        return OAuthBearerRefreshingLogin.class;    return DefaultLogin.class;}
public void kafkatest_f2485_0()
{    this.nioSelector.wakeup();}
public void kafkatest_f2486_0()
{    List<String> connections = new ArrayList<>(channels.keySet());    try {        for (String id : connections) close(id);    } finally {        // If there is any exception thrown in close(id), we should still be able        // to close the remaining objects, especially the sensors because keeping        // the sensors may lead to failure to start up the ReplicaFetcherThread if        // the old sensors with the same names has not yet been cleaned up.        AtomicReference<Throwable> firstException = new AtomicReference<>();        Utils.closeQuietly(nioSelector, "nioSelector", firstException);        Utils.closeQuietly(sensors, "sensors", firstException);        Utils.closeQuietly(channelBuilder, "channelBuilder", firstException);        Throwable exception = firstException.get();        if (exception instanceof RuntimeException && !(exception instanceof SecurityException)) {            throw (RuntimeException) exception;        }    }}
public Map<String, ChannelState> kafkatest_f2495_0()
{    return this.disconnected;}
public List<String> kafkatest_f2496_0()
{    return this.connected;}
private void kafkatest_f2505_0()
{    this.completedSends.clear();    this.completedReceives.clear();    this.connected.clear();    this.disconnected.clear();    // Remove closed channels after all their staged receives have been processed or if a send was requested    for (Iterator<Map.Entry<String, KafkaChannel>> it = closingChannels.entrySet().iterator(); it.hasNext(); ) {        KafkaChannel channel = it.next().getValue();        Deque<NetworkReceive> deque = this.stagedReceives.get(channel);        boolean sendFailed = failedSends.remove(channel.id());        if (deque == null || deque.isEmpty() || sendFailed) {            doClose(channel, true);            it.remove();        }    }    for (String channel : this.failedSends) this.disconnected.put(channel, ChannelState.FAILED_SEND);    this.failedSends.clear();    this.madeReadProgressLastPoll = false;}
private int kafkatest_f2506_0(long timeoutMs) throws IOException
{    if (timeoutMs < 0L)        throw new IllegalArgumentException("timeout should be >= 0");    if (timeoutMs == 0L)        return this.nioSelector.selectNow();    else        return this.nioSelector.select(timeoutMs);}
public KafkaChannel kafkatest_f2515_0(String id)
{    return this.channels.get(id);}
public KafkaChannel kafkatest_f2516_0(String id)
{    return closingChannels.get(id);}
public int kafkatest_f2525_0(KafkaChannel channel)
{    Deque<NetworkReceive> deque = stagedReceives.get(channel);    return deque == null ? 0 : deque.size();}
private Meter kafkatest_f2526_0(Metrics metrics, String groupName, Map<String, String> metricTags, SampledStat stat, String baseName, String descriptiveName)
{    MetricName rateMetricName = metrics.metricName(baseName + "-rate", groupName, String.format("The number of %s per second", descriptiveName), metricTags);    MetricName totalMetricName = metrics.metricName(baseName + "-total", groupName, String.format("The total number of %s", descriptiveName), metricTags);    if (stat == null)        return new Meter(rateMetricName, totalMetricName);    else        return new Meter(stat, rateMetricName, totalMetricName);}
public final void kafkatest_f2535_0()
{    if (closed)        throw new IllegalStateException("Attempt to close a channel that has already been closed");    handleCloseOnAuthenticationFailure(channel);    closed = true;}
public void kafkatest_f2536_0(String connectionId, long currentTimeNanos)
{    lruConnections.put(connectionId, currentTimeNanos);}
public void kafkatest_f2545_0(Map<String, ?> configs)
{    sslFactory.reconfigure(configs);}
public ListenerName kafkatest_f2546_0()
{    return listenerName;}
public boolean kafkatest_f2557_0()
{    return state == State.READY;}
public boolean kafkatest_f2558_0() throws IOException
{    boolean connected = socketChannel.finishConnect();    if (connected)        key.interestOps(key.interestOps() & ~SelectionKey.OP_CONNECT | SelectionKey.OP_READ);    return connected;}
protected boolean kafkatest_f2567_0(ByteBuffer buf) throws IOException
{    int remaining = buf.remaining();    if (remaining > 0) {        int written = socketChannel.write(buf);        return written >= remaining;    }    return true;}
public void kafkatest_f2568_0() throws IOException
{    if (state == State.NOT_INITALIZED)        startHandshake();    if (state == State.READY)        throw renegotiationException();    if (state == State.CLOSING)        throw closingException();    int read = 0;    boolean readable = key.isReadable();    try {        // if handshake fails)        if (readable)            read = readFromSocketChannel();        doHandshake();    } catch (SSLException e) {        maybeProcessHandshakeFailure(e, true, null);    } catch (IOException e) {        maybeThrowSslAuthenticationException();        // in the socket channel to read and unwrap, process the data so that any SSL handshake exceptions are reported.        try {            do {                handshakeUnwrap(false, true);            } while (readable && readFromSocketChannel() > 0);        } catch (SSLException e1) {            maybeProcessHandshakeFailure(e1, false, e);        }        // If we get here, this is not a handshake failure, throw the original IOException        throw e;    }    // Read from socket failed, so throw any pending handshake exception or EOF exception.    if (read == -1) {        maybeThrowSslAuthenticationException();        throw new EOFException("EOF during handshake, handshake status is " + handshakeStatus);    }}
public long kafkatest_f2577_0(ByteBuffer[] dsts) throws IOException
{    return read(dsts, 0, dsts.length);}
public long kafkatest_f2578_0(ByteBuffer[] dsts, int offset, int length) throws IOException
{    if ((offset < 0) || (length < 0) || (offset > dsts.length - length))        throw new IndexOutOfBoundsException();    int totalRead = 0;    int i = offset;    while (i < length) {        if (dsts[i].hasRemaining()) {            int read = read(dsts[i]);            if (read > 0)                totalRead += read;            else                break;        }        if (!dsts[i].hasRemaining()) {            i++;        }    }    return totalRead;}
private int kafkatest_f2587_0(ByteBuffer dst)
{    appReadBuffer.flip();    int remaining = Math.min(appReadBuffer.remaining(), dst.remaining());    if (remaining > 0) {        int limit = appReadBuffer.limit();        appReadBuffer.limit(appReadBuffer.position() + remaining);        dst.put(appReadBuffer);        appReadBuffer.limit(limit);    }    appReadBuffer.compact();    return remaining;}
protected int kafkatest_f2588_0()
{    return sslEngine.getSession().getPacketBufferSize();}
public boolean kafkatest_f2597_0()
{    return hasBytesBuffered;}
private void kafkatest_f2598_0(boolean madeProgress)
{    if (madeProgress)        hasBytesBuffered = netReadBuffer.position() != 0 || appReadBuffer.position() != 0;    else        hasBytesBuffered = false;}
public boolean kafkatest_f2607_0()
{    return rack != null;}
public String kafkatest_f2608_0()
{    return rack;}
public Node[] kafkatest_f2617_0()
{    return offlineReplicas;}
public String kafkatest_f2618_0()
{    return String.format("Partition(topic = %s, partition = %d, leader = %s, replicas = %s, isr = %s, offlineReplicas = %s)", topic, partition, leader == null ? "none" : leader.idString(), formatNodeIds(replicas), formatNodeIds(inSyncReplicas), formatNodeIds(offlineReplicas));}
public Struct kafkatest_f2627_0(short version, ByteBuffer buffer)
{    return responseSchema(version).read(buffer);}
protected Struct kafkatest_f2628_0(short version, ByteBuffer buffer, short fallbackVersion)
{    int bufferPosition = buffer.position();    try {        return responseSchema(version).read(buffer);    } catch (SchemaException e) {        if (version != fallbackVersion) {            buffer.position(bufferPosition);            return responseSchema(fallbackVersion).read(buffer);        } else            throw e;    }}
public short kafkatest_f2637_0()
{    return buf.getShort();}
public int kafkatest_f2638_0()
{    return buf.getInt();}
public ApiException kafkatest_f2647_0(String message)
{    if (message == null) {        // If no error message was specified, return an exception with the default error message.        return exception;    }    // Return an exception with the given error message.    return builder.apply(message);}
public String kafkatest_f2648_0()
{    return exception == null ? null : exception.getClass().getName();}
public static String kafkatest_f2657_0(Iterator<?> iter)
{    StringBuilder bld = new StringBuilder("[");    String prefix = "";    while (iter.hasNext()) {        Object object = iter.next();        bld.append(prefix);        bld.append(object.toString());        prefix = ", ";    }    bld.append("]");    return bld.toString();}
private static String kafkatest_f2658_0(int size)
{    StringBuilder b = new StringBuilder(size);    for (int i = 0; i < size; i++) b.append(" ");    return b.toString();}
public static ArrayOf kafkatest_f2667_0(Type type)
{    return new ArrayOf(type, true);}
public boolean kafkatest_f2668_0()
{    return nullable;}
public String kafkatest_f2677_0()
{    return def.name + ":" + def.type;}
public Field kafkatest_f2678_0(Field... fields)
{    Schema elementType = new Schema(fields);    return new Field(name, new ArrayOf(elementType), docString, false, null);}
public BoundField[] kafkatest_f2687_0()
{    return this.fields;}
public String kafkatest_f2688_0()
{    StringBuilder b = new StringBuilder();    b.append('{');    for (int i = 0; i < this.fields.length; i++) {        b.append(this.fields[i].toString());        if (i < this.fields.length - 1)            b.append(',');    }    b.append("}");    return b.toString();}
public Long kafkatest_f2700_0(Field.Int64 field)
{    return getLong(field.name);}
public UUID kafkatest_f2701_0(Field.UUID field)
{    return getUUID(field.name);}
public Short kafkatest_f2710_0(Field.Int16 field, short alternative)
{    if (hasField(field.name))        return getShort(field.name);    return alternative;}
public Byte kafkatest_f2711_0(Field.Int8 field, byte alternative)
{    if (hasField(field.name))        return getByte(field.name);    return alternative;}
public boolean kafkatest_f2720_0(Field def)
{    return schema.get(def.name) != null;}
public boolean kafkatest_f2721_0(Field.ComplexArray def)
{    return schema.get(def.name) != null;}
public Integer kafkatest_f2730_0(String name)
{    return (Integer) get(name);}
public Long kafkatest_f2731_0(String name)
{    return (Long) get(name);}
public Boolean kafkatest_f2740_0(BoundField field)
{    return (Boolean) get(field);}
public Boolean kafkatest_f2741_0(String name)
{    return (Boolean) get(name);}
public Struct kafkatest_f2750_0(Field.Int32 def, int value)
{    return set(def.name, value);}
public Struct kafkatest_f2751_0(Field.Int64 def, long value)
{    return set(def.name, value);}
public Struct kafkatest_f2760_0(Field def, Object value)
{    return setIfExists(def.name, value);}
public Struct kafkatest_f2761_0(String fieldName, Object value)
{    BoundField field = this.schema.get(fieldName);    if (field != null)        this.values[field.index] = value;    return this;}
public void kafkatest_f2770_0()
{    this.schema.validate(this);}
public String kafkatest_f2771_0()
{    StringBuilder b = new StringBuilder();    b.append('{');    for (int i = 0; i < this.values.length; i++) {        BoundField f = this.schema.get(i);        b.append(f.def.name);        b.append('=');        if (f.def.type instanceof ArrayOf && this.values[i] != null) {            Object[] arrayValue = (Object[]) this.values[i];            b.append('[');            for (int j = 0; j < arrayValue.length; j++) {                b.append(arrayValue[j]);                if (j < arrayValue.length - 1)                    b.append(',');            }            b.append(']');        } else            b.append(this.values[i]);        if (i < this.values.length - 1)            b.append(',');    }    b.append('}');    return b.toString();}
public Boolean kafkatest_f2780_0(Object item)
{    if (item instanceof Boolean)        return (Boolean) item;    else        throw new SchemaException(item + " is not a Boolean.");}
public String kafkatest_f2781_0()
{    return "Represents a boolean value in a byte. " + "Values 0 and 1 are used to represent false and true respectively. " + "When reading a boolean value, any non-zero value is considered true.";}
public int kafkatest_f2790_0(Object o)
{    return 2;}
public String kafkatest_f2791_0()
{    return "INT16";}
public void kafkatest_f2800_0(ByteBuffer buffer, Object o)
{    ByteUtils.writeUnsignedInt(buffer, (long) o);}
public Object kafkatest_f2801_0(ByteBuffer buffer)
{    return ByteUtils.readUnsignedInt(buffer);}
public Long kafkatest_f2810_0(Object item)
{    if (item instanceof Long)        return (Long) item;    else        throw new SchemaException(item + " is not a Long.");}
public String kafkatest_f2811_0()
{    return "Represents an integer between -2<sup>63</sup> and 2<sup>63</sup>-1 inclusive. " + "The values are encoded using eight bytes in network byte order (big-endian).";}
public int kafkatest_f2820_0(Object o)
{    return 2 + Utils.utf8Length((String) o);}
public String kafkatest_f2821_0()
{    return "STRING";}
public String kafkatest_f2830_0()
{    return "Represents a sequence of characters or null. For non-null strings, first the length N is given as an " + INT16 + ". Then N bytes follow which are the UTF-8 encoding of the character sequence. " + "A null value is encoded with length of -1 and there are no following bytes.";}
public void kafkatest_f2831_0(ByteBuffer buffer, Object o)
{    ByteBuffer arg = (ByteBuffer) o;    int pos = arg.position();    buffer.putInt(arg.remaining());    buffer.put(arg);    arg.position(pos);}
public int kafkatest_f2840_0(Object o)
{    if (o == null)        return 4;    ByteBuffer buffer = (ByteBuffer) o;    return 4 + buffer.remaining();}
public String kafkatest_f2841_0()
{    return "NULLABLE_BYTES";}
public String kafkatest_f2850_0()
{    return "Represents a sequence of Kafka records as " + NULLABLE_BYTES + ". " + "For a detailed description of records see " + "<a href=\"/documentation/#messageformat\">Message Sets</a>.";}
public void kafkatest_f2851_0(ByteBuffer buffer, Object o)
{    ByteUtils.writeVarint((Integer) o, buffer);}
public String kafkatest_f2860_0()
{    return "VARLONG";}
public int kafkatest_f2861_0(Object o)
{    return ByteUtils.sizeOfVarlong((Long) o);}
public long kafkatest_f2870_0()
{    return offset();}
public boolean kafkatest_f2871_0()
{    return outerRecord().isValid();}
public boolean kafkatest_f2880_0(byte magic)
{    return magic == outerRecord().magic();}
public boolean kafkatest_f2881_0(TimestampType timestampType)
{    return outerRecord().timestampType() == timestampType;}
public int kafkatest_f2890_0()
{    return outerRecord().sizeInBytes() + LOG_OVERHEAD;}
public Integer kafkatest_f2891_0()
{    return null;}
public boolean kafkatest_f2900_0()
{    return false;}
public int kafkatest_f2901_0()
{    return RecordBatch.NO_PARTITION_LEADER_EPOCH;}
 static void kafkatest_f2911_0(DataOutputStream out, long offset, int size) throws IOException
{    out.writeLong(offset);    out.writeInt(size);}
public AbstractLegacyRecordBatch kafkatest_f2912_0() throws IOException
{    offsetAndSizeBuffer.clear();    Utils.readFully(stream, offsetAndSizeBuffer);    if (offsetAndSizeBuffer.hasRemaining())        return null;    long offset = offsetAndSizeBuffer.getLong(Records.OFFSET_OFFSET);    int size = offsetAndSizeBuffer.getInt(Records.SIZE_OFFSET);    if (size < LegacyRecord.RECORD_OVERHEAD_V0)        throw new CorruptRecordException(String.format("Record size is less than the minimum record overhead (%d)", LegacyRecord.RECORD_OVERHEAD_V0));    if (size > maxMessageSize)        throw new CorruptRecordException(String.format("Record size exceeds the largest allowable message size (%d).", maxMessageSize));    ByteBuffer batchBuffer = ByteBuffer.allocate(size);    Utils.readFully(stream, batchBuffer);    if (batchBuffer.hasRemaining())        return null;    batchBuffer.flip();    return new BasicLegacyRecordBatch(offset, new LegacyRecord(batchBuffer));}
public void kafkatest_f2922_0(TimestampType timestampType, long timestamp)
{    if (record.magic() == RecordBatch.MAGIC_VALUE_V0)        throw new UnsupportedOperationException("Cannot set timestamp for a record with magic = 0");    long currentTimestamp = record.timestamp();    // We don't need to recompute crc if the timestamp is not updated.    if (record.timestampType() == timestampType && currentTimestamp == timestamp)        return;    setTimestampAndUpdateCrc(timestampType, timestamp);}
public void kafkatest_f2923_0(int epoch)
{    throw new UnsupportedOperationException("Magic versions prior to 2 do not support partition leader epoch");}
public long kafkatest_f2932_0()
{    return RecordBatch.NO_PRODUCER_ID;}
public short kafkatest_f2933_0()
{    return RecordBatch.NO_PRODUCER_EPOCH;}
public long kafkatest_f2942_0()
{    return lastOffset() + 1;}
public boolean kafkatest_f2943_0()
{    return compressionType() != CompressionType.NONE;}
public static int kafkatest_f2952_0(byte magic, CompressionType compressionType, Iterable<SimpleRecord> records)
{    int size = 0;    if (magic <= RecordBatch.MAGIC_VALUE_V1) {        for (SimpleRecord record : records) size += Records.LOG_OVERHEAD + LegacyRecord.recordSize(magic, record.key(), record.value());    } else {        size = DefaultRecordBatch.sizeInBytes(records);    }    return estimateCompressedSizeInBytes(size, compressionType);}
private static int kafkatest_f2953_0(int size, CompressionType compressionType)
{    return compressionType == CompressionType.NONE ? size : Math.min(Math.max(size / 2, 1024), 1 << 16);}
public ByteBuffer kafkatest_f2964_0(int minCapacity)
{    if (cachedBuffer != null && cachedBuffer.capacity() >= minCapacity) {        ByteBuffer res = cachedBuffer;        cachedBuffer = null;        return res;    } else {        cachedBuffer = null;        return ByteBuffer.allocate(minCapacity);    }}
public void kafkatest_f2965_0(ByteBuffer buffer)
{    buffer.clear();    cachedBuffer = buffer;}
private static float[] kafkatest_f2974_0(String topic)
{    float[] compressionRatioForTopic = COMPRESSION_RATIO.get(topic);    if (compressionRatioForTopic == null) {        compressionRatioForTopic = initialCompressionRatio();        float[] existingCompressionRatio = COMPRESSION_RATIO.putIfAbsent(topic, compressionRatioForTopic);        // Someone created the compression ratio array before us, use it.        if (existingCompressionRatio != null)            return existingCompressionRatio;    }    return compressionRatioForTopic;}
private static float[] kafkatest_f2975_0()
{    float[] compressionRatio = new float[CompressionType.values().length];    for (CompressionType type : CompressionType.values()) {        compressionRatio[type.id] = type.rate;    }    return compressionRatio;}
public InputStream kafkatest_f2984_0(ByteBuffer buffer, byte messageVersion, BufferSupplier decompressionBufferSupplier)
{    try {        return (InputStream) SnappyConstructors.INPUT.invoke(new ByteBufferInputStream(buffer));    } catch (Throwable e) {        throw new KafkaException(e);    }}
public OutputStream kafkatest_f2985_0(ByteBufferOutputStream buffer, byte messageVersion)
{    try {        return new KafkaLZ4BlockOutputStream(buffer, messageVersion == RecordBatch.MAGIC_VALUE_V0);    } catch (Throwable e) {        throw new KafkaException(e);    }}
public RecordConversionStats kafkatest_f2994_0()
{    return recordConversionStats;}
public long kafkatest_f2995_0()
{    return offset;}
public boolean kafkatest_f3005_0()
{    return key != null;}
public ByteBuffer kafkatest_f3006_0()
{    return key == null ? null : key.duplicate();}
public boolean kafkatest_f3015_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    DefaultRecord that = (DefaultRecord) o;    return sizeInBytes == that.sizeInBytes && attributes == that.attributes && offset == that.offset && timestamp == that.timestamp && sequence == that.sequence && Objects.equals(key, that.key) && Objects.equals(value, that.value) && Arrays.equals(headers, that.headers);}
public int kafkatest_f3016_0()
{    int result = sizeInBytes;    result = 31 * result + (int) attributes;    result = 31 * result + Long.hashCode(offset);    result = 31 * result + Long.hashCode(timestamp);    result = 31 * result + sequence;    result = 31 * result + (key != null ? key.hashCode() : 0);    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + Arrays.hashCode(headers);    return result;}
private static int kafkatest_f3025_0(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException
{    boolean needMore = false;    int sizeInBytes = -1;    int bytesToSkip = -1;    while (true) {        if (needMore) {            readMore(buffer, input, bytesRemaining);            needMore = false;        }        if (bytesToSkip < 0) {            if (buffer.remaining() < 5 && bytesRemaining.value > 0) {                needMore = true;            } else {                sizeInBytes = ByteUtils.readVarint(buffer);                if (sizeInBytes <= 0)                    return sizeInBytes;                else                    bytesToSkip = sizeInBytes;            }        } else {            if (bytesToSkip > buffer.remaining()) {                bytesToSkip -= buffer.remaining();                buffer.position(buffer.limit());                needMore = true;            } else {                buffer.position(buffer.position() + bytesToSkip);                return sizeInBytes;            }        }    }}
private static void kafkatest_f3026_0(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException
{    if (bytesRemaining.value > 0) {        byte[] array = buffer.array();        // first copy the remaining bytes to the beginning of the array;        // at most 4 bytes would be shifted here        int stepsToLeftShift = buffer.position();        int bytesToLeftShift = buffer.remaining();        for (int i = 0; i < bytesToLeftShift; i++) {            array[i] = array[i + stepsToLeftShift];        }        // then try to read more bytes to the remaining of the array        int bytesRead = Math.min(bytesRemaining.value, array.length - bytesToLeftShift);        input.readFully(array, bytesToLeftShift, bytesRead);        buffer.rewind();        // only those many bytes are readable        buffer.limit(bytesToLeftShift + bytesRead);        bytesRemaining.value -= bytesRead;    } else {        throw new InvalidRecordException("Invalid record size: expected to read more bytes in record payload");    }}
public byte kafkatest_f3035_0()
{    return buffer.get(MAGIC_OFFSET);}
public void kafkatest_f3036_0()
{    if (sizeInBytes() < RECORD_BATCH_OVERHEAD)        throw new InvalidRecordException("Record batch is corrupt (the size " + sizeInBytes() + " is smaller than the minimum allowed overhead " + RECORD_BATCH_OVERHEAD + ")");    if (!isValid())        throw new InvalidRecordException("Record is corrupt (stored crc = " + checksum() + ", computed crc = " + computeChecksum() + ")");}
private int kafkatest_f3045_0()
{    return buffer.getInt(LAST_OFFSET_DELTA_OFFSET);}
public int kafkatest_f3046_0()
{    int baseSequence = baseSequence();    if (baseSequence == RecordBatch.NO_SEQUENCE)        return RecordBatch.NO_SEQUENCE;    return incrementSequence(baseSequence, lastOffsetDelta());}
public int kafkatest_f3055_0()
{    return buffer.getInt(PARTITION_LEADER_EPOCH_OFFSET);}
private CloseableIterator<Record> kafkatest_f3056_0(BufferSupplier bufferSupplier, boolean skipKeyValue)
{    final ByteBuffer buffer = this.buffer.duplicate();    buffer.position(RECORDS_OFFSET);    final DataInputStream inputStream = new DataInputStream(compressionType().wrapForInput(buffer, magic(), bufferSupplier));    if (skipKeyValue) {        // this buffer is used to skip length delimited fields like key, value, headers        byte[] skipArray = new byte[MAX_SKIP_BUFFER_SIZE];        return new StreamRecordIterator(inputStream) {            @Override            protected Record doReadRecord(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime) throws IOException {                return DefaultRecord.readPartiallyFrom(inputStream, skipArray, baseOffset, firstTimestamp, baseSequence, logAppendTime);            }        };    } else {        return new StreamRecordIterator(inputStream) {            @Override            protected Record doReadRecord(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime) throws IOException {                return DefaultRecord.readFrom(inputStream, baseOffset, firstTimestamp, baseSequence, logAppendTime);            }        };    }}
public void kafkatest_f3066_0(long offset)
{    buffer.putLong(BASE_OFFSET_OFFSET, offset - lastOffsetDelta());}
public void kafkatest_f3067_0(TimestampType timestampType, long maxTimestamp)
{    long currentMaxTimestamp = maxTimestamp();    // We don't need to recompute crc if the timestamp is not updated.    if (timestampType() == timestampType && currentMaxTimestamp == maxTimestamp)        return;    byte attributes = computeAttributes(compressionType(), timestampType, isTransactional(), isControlBatch());    buffer.putShort(ATTRIBUTES_OFFSET, attributes);    buffer.putLong(MAX_TIMESTAMP_OFFSET, maxTimestamp);    long crc = computeChecksum();    ByteUtils.writeUnsignedInt(buffer, CRC_OFFSET, crc);}
public static void kafkatest_f3076_0(ByteBuffer buffer, byte magic, long producerId, short producerEpoch, int baseSequence, long baseOffset, long lastOffset, int partitionLeaderEpoch, TimestampType timestampType, long timestamp, boolean isTransactional, boolean isControlRecord)
{    int offsetDelta = (int) (lastOffset - baseOffset);    writeHeader(buffer, baseOffset, offsetDelta, DefaultRecordBatch.RECORD_BATCH_OVERHEAD, magic, CompressionType.NONE, timestampType, RecordBatch.NO_TIMESTAMP, timestamp, producerId, producerEpoch, baseSequence, isTransactional, isControlRecord, partitionLeaderEpoch, 0);}
 static void kafkatest_f3077_0(ByteBuffer buffer, long baseOffset, int lastOffsetDelta, int sizeInBytes, byte magic, CompressionType compressionType, TimestampType timestampType, long firstTimestamp, long maxTimestamp, long producerId, short epoch, int sequence, boolean isTransactional, boolean isControlBatch, int partitionLeaderEpoch, int numRecords)
{    if (magic < RecordBatch.CURRENT_MAGIC_VALUE)        throw new IllegalArgumentException("Invalid magic value " + magic);    if (firstTimestamp < 0 && firstTimestamp != NO_TIMESTAMP)        throw new IllegalArgumentException("Invalid message timestamp " + firstTimestamp);    short attributes = computeAttributes(compressionType, timestampType, isTransactional, isControlBatch);    int position = buffer.position();    buffer.putLong(position + BASE_OFFSET_OFFSET, baseOffset);    buffer.putInt(position + LENGTH_OFFSET, sizeInBytes - LOG_OVERHEAD);    buffer.putInt(position + PARTITION_LEADER_EPOCH_OFFSET, partitionLeaderEpoch);    buffer.put(position + MAGIC_OFFSET, magic);    buffer.putShort(position + ATTRIBUTES_OFFSET, attributes);    buffer.putLong(position + FIRST_TIMESTAMP_OFFSET, firstTimestamp);    buffer.putLong(position + MAX_TIMESTAMP_OFFSET, maxTimestamp);    buffer.putInt(position + LAST_OFFSET_DELTA_OFFSET, lastOffsetDelta);    buffer.putLong(position + PRODUCER_ID_OFFSET, producerId);    buffer.putShort(position + PRODUCER_EPOCH_OFFSET, epoch);    buffer.putInt(position + BASE_SEQUENCE_OFFSET, sequence);    buffer.putInt(position + RECORDS_COUNT_OFFSET, numRecords);    long crc = Crc32C.compute(buffer, ATTRIBUTES_OFFSET, sizeInBytes - ATTRIBUTES_OFFSET);    buffer.putInt(position + CRC_OFFSET, (int) crc);    buffer.position(position + RECORD_BATCH_OVERHEAD);}
public void kafkatest_f3086_0()
{    throw new UnsupportedOperationException();}
protected Record kafkatest_f3087_0(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime)
{    try {        return doReadRecord(baseOffset, firstTimestamp, baseSequence, logAppendTime);    } catch (EOFException e) {        throw new InvalidRecordException("Incorrect declared batch size, premature EOF reached");    } catch (IOException e) {        throw new KafkaException("Failed to decompress record stream", e);    }}
public int kafkatest_f3096_0()
{    return loadBatchHeader().lastSequence();}
public long kafkatest_f3097_0()
{    return loadBatchHeader().checksum();}
private Struct kafkatest_f3106_0()
{    Struct struct = new Struct(END_TXN_MARKER_SCHEMA_VERSION_V0);    struct.set("version", CURRENT_END_TXN_MARKER_VERSION);    struct.set("coordinator_epoch", coordinatorEpoch);    return struct;}
public ByteBuffer kafkatest_f3107_0()
{    Struct valueStruct = buildRecordValue();    ByteBuffer value = ByteBuffer.allocate(valueStruct.sizeOf());    valueStruct.writeTo(value);    value.flip();    return value;}
public long kafkatest_f3116_0()
{    return loadBatchHeader().checksum();}
public long kafkatest_f3117_0()
{    return loadBatchHeader().maxTimestamp();}
protected RecordBatch kafkatest_f3126_0()
{    if (fullBatch == null) {        batchHeader = null;        fullBatch = loadBatchWithSize(sizeInBytes(), "full record batch");    }    return fullBatch;}
protected RecordBatch kafkatest_f3127_0()
{    if (fullBatch != null)        return fullBatch;    if (batchHeader == null)        batchHeader = loadBatchWithSize(headerSize(), "record batch header");    return batchHeader;}
public FileRecords kafkatest_f3136_0(int position, int size) throws IOException
{    if (position < 0)        throw new IllegalArgumentException("Invalid position: " + position + " in read from " + this);    if (position > sizeInBytes() - start)        throw new IllegalArgumentException("Slice from position " + position + " exceeds end position of " + this);    if (size < 0)        throw new IllegalArgumentException("Invalid size: " + size + " in read from " + this);    int end = this.start + position + size;    // handle integer overflow or if end is beyond the end of the file    if (end < 0 || end >= start + sizeInBytes())        end = start + sizeInBytes();    return new FileRecords(file, channel, this.start + position, end, true);}
public int kafkatest_f3137_0(MemoryRecords records) throws IOException
{    if (records.sizeInBytes() > Integer.MAX_VALUE - size.get())        throw new IllegalArgumentException("Append of size " + records.sizeInBytes() + " bytes is too large for segment with current file position at " + size.get());    int written = records.writeFullyTo(channel);    size.getAndAdd(written);    return written;}
public ConvertedRecords<? extends Records> kafkatest_f3146_0(byte toMagic, long firstOffset, Time time)
{    ConvertedRecords<MemoryRecords> convertedRecords = RecordsUtil.downConvert(batches, toMagic, firstOffset, time);    if (convertedRecords.recordConversionStats().numRecordsConverted() == 0) {        // one full record batch, even if it requires exceeding the max fetch size requested by the client.        return new ConvertedRecords<>(this, RecordConversionStats.EMPTY);    } else {        return convertedRecords;    }}
public long kafkatest_f3147_0(GatheringByteChannel destChannel, long offset, int length) throws IOException
{    long newSize = Math.min(channel.size(), end) - start;    int oldSize = sizeInBytes();    if (newSize < oldSize)        throw new KafkaException(String.format("Size of FileRecords %s has been truncated during write: old size %d, new size %d", file.getAbsolutePath(), oldSize, newSize));    long position = start + offset;    int count = Math.min(length, oldSize);    final long bytesTransferred;    if (destChannel instanceof TransportLayer) {        TransportLayer tl = (TransportLayer) destChannel;        bytesTransferred = tl.transferFrom(channel, position, count);    } else {        bytesTransferred = channel.transferTo(position, count, destChannel);    }    return bytesTransferred;}
private AbstractIterator<FileChannelRecordBatch> kafkatest_f3156_0(int start)
{    final int end;    if (isSlice)        end = this.end;    else        end = this.sizeInBytes();    FileLogInputStream inputStream = new FileLogInputStream(this, start, end);    return new RecordBatchIterator<>(inputStream);}
public static FileRecords kafkatest_f3157_0(File file, boolean mutable, boolean fileAlreadyExists, int initFileSize, boolean preallocate) throws IOException
{    FileChannel channel = openChannel(file, mutable, fileAlreadyExists, initFileSize, preallocate);    int end = (!fileAlreadyExists && preallocate) ? 0 : Integer.MAX_VALUE;    return new FileRecords(file, channel, 0, end, false);}
public int kafkatest_f3166_0()
{    return Objects.hash(timestamp, offset, leaderEpoch);}
public String kafkatest_f3167_0()
{    return "TimestampAndOffset(" + "timestamp=" + timestamp + ", offset=" + offset + ", leaderEpoch=" + leaderEpoch + ')';}
public void kafkatest_f3176_0(int readlimit)
{    throw new RuntimeException("mark not supported");}
public void kafkatest_f3177_0()
{    throw new RuntimeException("reset not supported");}
private void kafkatest_f3186_0()
{    if (finished) {        throw new IllegalStateException(CLOSED_STREAM);    }}
public void kafkatest_f3187_0() throws IOException
{    try {        if (!finished) {            // basically flush the buffer writing the last block            writeBlock();            // write the end block            writeEndMark();        }    } finally {        try {            if (out != null) {                try (OutputStream outStream = out) {                    outStream.flush();                }            }        } finally {            out = null;            buffer = null;            compressedBuffer = null;            finished = true;        }    }}
public static BD kafkatest_f3196_0(byte bd)
{    int reserved2 = (bd >>> 0) & 15;    int blockMaximumSize = (bd >>> 4) & 7;    int reserved3 = (bd >>> 7) & 1;    return new BD(reserved2, blockMaximumSize, reserved3);}
private void kafkatest_f3197_0()
{    if (reserved2 != 0) {        throw new RuntimeException("Reserved2 field must be 0");    }    if (blockSizeValue < 4 || blockSizeValue > 7) {        throw new RuntimeException("Block size value must be between 4 and 7");    }    if (reserved3 != 0) {        throw new RuntimeException("Reserved3 field must be 0");    }}
public java.util.Iterator<ConvertedRecords<?>> kafkatest_f3206_0(long maximumReadSize)
{    // We typically expect only one iterator instance to be created, so null out the first converted batch after    // first use to make it available for GC.    ConvertedRecords firstBatch = firstConvertedBatch;    firstConvertedBatch = null;    return new Iterator(records, maximumReadSize, firstBatch);}
protected ConvertedRecords kafkatest_f3207_0()
{    // If we have cached the first down-converted batch, return that now    if (firstConvertedBatch != null) {        ConvertedRecords convertedBatch = firstConvertedBatch;        firstConvertedBatch = null;        return convertedBatch;    }    while (batchIterator.hasNext()) {        final List<RecordBatch> batches = new ArrayList<>();        boolean isFirstBatch = true;        long sizeSoFar = 0;        // Figure out batches we should down-convert based on the size constraints        while (batchIterator.hasNext() && (isFirstBatch || (batchIterator.peek().sizeInBytes() + sizeSoFar) <= maximumReadSize)) {            RecordBatch currentBatch = batchIterator.next();            batches.add(currentBatch);            sizeSoFar += currentBatch.sizeInBytes();            isFirstBatch = false;        }        ConvertedRecords convertedRecords = RecordsUtil.downConvert(batches, toMagic, firstOffset, time);        // We return converted records only when we have at least one valid batch of messages after conversion.        if (convertedRecords.records().sizeInBytes() > 0)            return convertedRecords;    }    return allDone();}
public TimestampType kafkatest_f3216_0()
{    return wrapperRecordTimestampType;}
public void kafkatest_f3217_0()
{    if (sizeInBytes() < RECORD_OVERHEAD_V0)        throw new InvalidRecordException("Record is corrupt (crc could not be retrieved as the record is too " + "small, size = " + sizeInBytes() + ")");    if (!isValid())        throw new InvalidRecordException("Record is corrupt (stored crc = " + checksum() + ", computed crc = " + computeChecksum() + ")");}
public long kafkatest_f3226_0()
{    if (magic() == RecordBatch.MAGIC_VALUE_V0)        return RecordBatch.NO_TIMESTAMP;    else {        // case 2        if (wrapperRecordTimestampType == TimestampType.LOG_APPEND_TIME && wrapperRecordTimestamp != null)            return wrapperRecordTimestamp;        else            return buffer.getLong(TIMESTAMP_OFFSET);    }}
public TimestampType kafkatest_f3227_0()
{    return timestampType(magic(), wrapperRecordTimestampType, attributes());}
public static LegacyRecord kafkatest_f3236_0(byte magic, long timestamp, byte[] key, byte[] value)
{    return create(magic, timestamp, key, value, CompressionType.NONE, TimestampType.CREATE_TIME);}
public static void kafkatest_f3237_0(ByteBuffer buffer, byte magic, int recordSize, long timestamp, CompressionType compressionType, TimestampType timestampType)
{    int recordPosition = buffer.position();    int valueSize = recordSize - recordOverhead(magic);    // write the record header with a null value (the key is always null for the wrapper)    write(buffer, magic, timestamp, null, null, compressionType, timestampType);    buffer.position(recordPosition);    // now fill in the value size    buffer.putInt(recordPosition + keyOffset(magic), valueSize);    // compute and fill the crc from the beginning of the message    long crc = Crc32.crc32(buffer, MAGIC_OFFSET, recordSize - MAGIC_OFFSET);    ByteUtils.writeUnsignedInt(buffer, recordPosition + CRC_OFFSET, crc);}
public static long kafkatest_f3246_0(byte magic, byte attributes, long timestamp, byte[] key, byte[] value)
{    return computeChecksum(magic, attributes, timestamp, wrapNullable(key), wrapNullable(value));}
private static long kafkatest_f3247_0(byte magic, byte attributes, long timestamp, ByteBuffer key, ByteBuffer value)
{    Crc32 crc = new Crc32();    crc.update(magic);    crc.update(attributes);    if (magic > RecordBatch.MAGIC_VALUE_V0)        Checksums.updateLong(crc, timestamp);    // update for the key    if (key == null) {        Checksums.updateInt(crc, -1);    } else {        int size = key.remaining();        Checksums.updateInt(crc, size);        Checksums.update(crc, key, size);    }    // update for the value    if (value == null) {        Checksums.updateInt(crc, -1);    } else {        int size = value.remaining();        Checksums.updateInt(crc, size);        Checksums.update(crc, value, size);    }    return crc.getValue();}
public ConvertedRecords<MemoryRecords> kafkatest_f3256_0(byte toMagic, long firstOffset, Time time)
{    return RecordsUtil.downConvert(batches(), toMagic, firstOffset, time);}
public AbstractIterator<MutableRecordBatch> kafkatest_f3257_0()
{    return new RecordBatchIterator<>(new ByteBufferLogInputStream(buffer.duplicate(), Integer.MAX_VALUE));}
public int kafkatest_f3266_0()
{    return buffer.hashCode();}
private void kafkatest_f3267_0(MutableRecordBatch retainedBatch, int numMessagesInBatch, boolean headerOnly)
{    int bytesRetained = headerOnly ? DefaultRecordBatch.RECORD_BATCH_OVERHEAD : retainedBatch.sizeInBytes();    updateRetainedBatchMetadata(retainedBatch.maxTimestamp(), retainedBatch.lastOffset(), retainedBatch.lastOffset(), numMessagesInBatch, bytesRetained);}
public long kafkatest_f3276_0()
{    return maxTimestamp;}
public long kafkatest_f3277_0()
{    return shallowOffsetOfMaxTimestamp;}
public static MemoryRecordsBuilder kafkatest_f3286_0(ByteBuffer buffer, byte magic, CompressionType compressionType, TimestampType timestampType, long baseOffset, long logAppendTime, long producerId, short producerEpoch, int baseSequence, boolean isTransactional, int partitionLeaderEpoch)
{    return builder(buffer, magic, compressionType, timestampType, baseOffset, logAppendTime, producerId, producerEpoch, baseSequence, isTransactional, false, partitionLeaderEpoch);}
public static MemoryRecordsBuilder kafkatest_f3287_0(ByteBuffer buffer, byte magic, CompressionType compressionType, TimestampType timestampType, long baseOffset, long logAppendTime, long producerId, short producerEpoch, int baseSequence, boolean isTransactional, boolean isControlBatch, int partitionLeaderEpoch)
{    return new MemoryRecordsBuilder(buffer, magic, compressionType, timestampType, baseOffset, logAppendTime, producerId, producerEpoch, baseSequence, isTransactional, isControlBatch, partitionLeaderEpoch, buffer.remaining());}
public static MemoryRecords kafkatest_f3296_0(long initialOffset, CompressionType compressionType, long producerId, short producerEpoch, int baseSequence, int partitionLeaderEpoch, SimpleRecord... records)
{    return withRecords(RecordBatch.CURRENT_MAGIC_VALUE, initialOffset, compressionType, TimestampType.CREATE_TIME, producerId, producerEpoch, baseSequence, partitionLeaderEpoch, false, records);}
public static MemoryRecords kafkatest_f3297_0(CompressionType compressionType, long producerId, short producerEpoch, int baseSequence, SimpleRecord... records)
{    return withRecords(RecordBatch.CURRENT_MAGIC_VALUE, 0L, compressionType, TimestampType.CREATE_TIME, producerId, producerEpoch, baseSequence, RecordBatch.NO_PARTITION_LEADER_EPOCH, true, records);}
public void kafkatest_f3306_0(int b)
{    throw new IllegalStateException("MemoryRecordsBuilder is closed for record appends");}
public ByteBuffer kafkatest_f3307_0()
{    return bufferStream.buffer();}
public int kafkatest_f3316_0()
{    return uncompressedRecordsSizeInBytes + batchHeaderSizeInBytes;}
public void kafkatest_f3317_0(long producerId, short producerEpoch, int baseSequence, boolean isTransactional)
{    if (isClosed()) {        // once a batch has been sent to the broker risks introducing duplicates.        throw new IllegalStateException("Trying to set producer state of an already closed batch. This indicates a bug on the client.");    }    this.producerId = producerId;    this.producerEpoch = producerEpoch;    this.baseSequence = baseSequence;    this.isTransactional = isTransactional;}
private Long kafkatest_f3326_0(long offset, boolean isControlRecord, long timestamp, ByteBuffer key, ByteBuffer value, Header[] headers)
{    try {        if (isControlRecord != isControlBatch)            throw new IllegalArgumentException("Control records can only be appended to control batches");        if (lastOffset != null && offset <= lastOffset)            throw new IllegalArgumentException(String.format("Illegal offset %s following previous offset %s " + "(Offsets must increase monotonically).", offset, lastOffset));        if (timestamp < 0 && timestamp != RecordBatch.NO_TIMESTAMP)            throw new IllegalArgumentException("Invalid negative timestamp " + timestamp);        if (magic < RecordBatch.MAGIC_VALUE_V2 && headers != null && headers.length > 0)            throw new IllegalArgumentException("Magic v" + magic + " does not support record headers");        if (firstTimestamp == null)            firstTimestamp = timestamp;        if (magic > RecordBatch.MAGIC_VALUE_V1) {            appendDefaultRecord(offset, timestamp, key, value, headers);            return null;        } else {            return appendLegacyRecord(offset, timestamp, key, value, magic);        }    } catch (IOException e) {        throw new KafkaException("I/O exception when writing to the append stream, closing", e);    }}
public Long kafkatest_f3327_0(long offset, long timestamp, byte[] key, byte[] value, Header[] headers)
{    return appendWithOffset(offset, false, timestamp, wrapNullable(key), wrapNullable(value), headers);}
public Long kafkatest_f3336_0(SimpleRecord record)
{    return appendWithOffset(nextSequentialOffset(), record);}
private Long kafkatest_f3337_0(long timestamp, ControlRecordType type, ByteBuffer value)
{    Struct keyStruct = type.recordKey();    ByteBuffer key = ByteBuffer.allocate(keyStruct.sizeOf());    keyStruct.writeTo(key);    key.flip();    return appendWithOffset(nextSequentialOffset(), true, timestamp, key, value, Record.EMPTY_HEADERS);}
private long kafkatest_f3346_0(long offset)
{    // use relative offsets for compressed messages with magic v1    if (magic > 0 && compressionType != CompressionType.NONE)        return offset - baseOffset;    return offset;}
private void kafkatest_f3347_0(long offset, long timestamp, int size)
{    if (numRecords == Integer.MAX_VALUE)        throw new IllegalArgumentException("Maximum number of records per batch exceeded, max records: " + Integer.MAX_VALUE);    if (offset - baseOffset > Integer.MAX_VALUE)        throw new IllegalArgumentException("Maximum offset delta exceeded, base offset: " + baseOffset + ", last offset: " + offset);    numRecords += 1;    uncompressedRecordsSizeInBytes += size;    lastOffset = offset;    if (magic > RecordBatch.MAGIC_VALUE_V0 && timestamp > maxTimestamp) {        maxTimestamp = timestamp;        offsetOfMaxTimestamp = offset;    }}
public int kafkatest_f3356_0()
{    return builtRecords != null ? builtRecords.sizeInBytes() : estimatedBytesWritten();}
public byte kafkatest_f3357_0()
{    return magic;}
public longf3366_1GatheringByteChannel channel) throws IOException
{    if (completed())        throw new KafkaException("This operation cannot be invoked on a complete request.");    int totalWrittenPerCall = 0;    boolean sendComplete;    do {        long written = current.writeTo(channel);        totalWrittenPerCall += written;        sendComplete = current.completed();        if (sendComplete) {            updateRecordConversionStats(current);            current = sendQueue.poll();        }    } while (!completed() && sendComplete);    totalWritten += totalWrittenPerCall;    if (completed() && totalWritten != size)         expected: {} actual: {}", size, totalWritten);    log.trace("Bytes written as part of multi-send call: {}, total bytes written so far: {}, expected bytes to write: {}", totalWrittenPerCall, totalWritten, size);    return totalWrittenPerCall;}
public Map<TopicPartition, RecordConversionStats> kafkatest_f3367_0()
{    return recordConversionStats;}
public boolean kafkatest_f3376_0()
{    return valueSize >= 0;}
public ByteBuffer kafkatest_f3377_0()
{    throw new UnsupportedOperationException("value is skipped in PartialDefaultRecord");}
public boolean kafkatest_f3386_0()
{    return remaining <= 0 && !pending;}
public final long kafkatest_f3387_0(GatheringByteChannel channel) throws IOException
{    long written = 0;    if (remaining > 0) {        written = writeTo(channel, size() - remaining, remaining);        if (written < 0)            throw new EOFException("Wrote negative bytes to channel. This shouldn't happen.");        remaining -= written;    }    pending = TransportLayers.hasPendingWrites(channel);    if (remaining <= 0 && pending)        channel.write(EMPTY_BYTE_BUFFER);    return written;}
public ByteBuffer kafkatest_f3396_0()
{    return value;}
public long kafkatest_f3397_0()
{    return timestamp;}
public InetAddress kafkatest_f3406_0()
{    return clientAddress;}
public KafkaPrincipal kafkatest_f3407_0()
{    return principal;}
public String kafkatest_f3416_0()
{    return "DefaultPartitionView{" + "replicas=" + replicas + ", leader=" + leader + '}';}
public Optional<ReplicaView> kafkatest_f3417_0(TopicPartition topicPartition, ClientMetadata clientMetadata, PartitionView partitionView)
{    if (clientMetadata.rackId() != null && !clientMetadata.rackId().isEmpty()) {        Set<ReplicaView> sameRackReplicas = partitionView.replicas().stream().filter(replicaInfo -> clientMetadata.rackId().equals(replicaInfo.endpoint().rack())).collect(Collectors.toSet());        if (sameRackReplicas.isEmpty()) {            return Optional.of(partitionView.leader());        } else {            if (sameRackReplicas.contains(partitionView.leader())) {                // Use the leader if it's in this rack                return Optional.of(partitionView.leader());            } else {                // Otherwise, get the most caught-up replica                return sameRackReplicas.stream().max(ReplicaView.comparator());            }        }    } else {        return Optional.of(partitionView.leader());    }}
public String kafkatest_f3426_0()
{    return "DefaultReplicaView{" + "endpoint=" + endpoint + ", logEndOffset=" + logEndOffset + ", timeSinceLastCaughtUpMs=" + timeSinceLastCaughtUpMs + '}';}
public int kafkatest_f3427_0()
{    return controllerId;}
public Send kafkatest_f3436_0(String destination, RequestHeader header)
{    return new NetworkSend(destination, serialize(header));}
public ByteBuffer kafkatest_f3437_0(RequestHeader header)
{    return serialize(header.toStruct(), toStruct());}
protected Map<Errors, Integer> kafkatest_f3446_0(Errors error)
{    return Collections.singletonMap(error, 1);}
protected Map<Errors, Integer> kafkatest_f3447_0(Map<?, Errors> errors)
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (Errors error : errors.values()) updateErrorCounts(errorCounts, error);    return errorCounts;}
public AddOffsetsToTxnRequest kafkatest_f3456_0(short version)
{    return new AddOffsetsToTxnRequest(version, transactionalId, producerId, producerEpoch, consumerGroupId);}
public String kafkatest_f3457_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=AddOffsetsToTxnRequest").append(", transactionalId=").append(transactionalId).append(", producerId=").append(producerId).append(", producerEpoch=").append(producerEpoch).append(", consumerGroupId=").append(consumerGroupId).append(")");    return bld.toString();}
public int kafkatest_f3466_0()
{    return throttleTimeMs;}
public Errors kafkatest_f3467_0()
{    return error;}
public String kafkatest_f3476_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=AddPartitionsToTxnRequest").append(", transactionalId=").append(transactionalId).append(", producerId=").append(producerId).append(", producerEpoch=").append(producerEpoch).append(", partitions=").append(partitions).append(")");    return bld.toString();}
public String kafkatest_f3477_0()
{    return transactionalId;}
public Map<TopicPartition, Errors> kafkatest_f3486_0()
{    return errors;}
public Map<Errors, Integer> kafkatest_f3487_0()
{    return errorCounts(errors);}
public AlterConfigsRequest kafkatest_f3496_0(short version)
{    return new AlterConfigsRequest(version, configs, validateOnly);}
public Map<ConfigResource, Config> kafkatest_f3497_0()
{    return configs;}
protected Struct kafkatest_f3506_0(short version)
{    Struct struct = new Struct(ApiKeys.ALTER_CONFIGS.responseSchema(version));    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    List<Struct> resourceStructs = new ArrayList<>(errors.size());    for (Map.Entry<ConfigResource, ApiError> entry : errors.entrySet()) {        Struct resourceStruct = struct.instance(RESOURCES_KEY_NAME);        ConfigResource resource = entry.getKey();        entry.getValue().write(resourceStruct);        resourceStruct.set(RESOURCE_TYPE_KEY_NAME, resource.type().id());        resourceStruct.set(RESOURCE_NAME_KEY_NAME, resource.name());        resourceStructs.add(resourceStruct);    }    struct.set(RESOURCES_KEY_NAME, resourceStructs.toArray(new Struct[0]));    return struct;}
public static AlterConfigsResponse kafkatest_f3507_0(ByteBuffer buffer, short version)
{    return new AlterConfigsResponse(ApiKeys.ALTER_CONFIGS.parseResponse(version, buffer));}
public AlterPartitionReassignmentsResponseData kafkatest_f3516_0()
{    return data;}
public boolean kafkatest_f3517_0(short version)
{    return true;}
public Map<TopicPartition, String> kafkatest_f3526_0()
{    return partitionDirs;}
public static AlterReplicaLogDirsRequest kafkatest_f3527_0(ByteBuffer buffer, short version)
{    return new AlterReplicaLogDirsRequest(ApiKeys.ALTER_REPLICA_LOG_DIRS.parseRequest(version, buffer), version);}
public void kafkatest_f3536_0(Struct struct)
{    struct.set(ERROR_CODE, error.code());    if (error != Errors.NONE)        struct.setIfExists(ERROR_MESSAGE, message);}
public boolean kafkatest_f3537_0(Errors error)
{    return this.error == error;}
public ApiVersionsRequest kafkatest_f3546_0(short version)
{    return new ApiVersionsRequest(version);}
public String kafkatest_f3547_0()
{    return "(type=ApiVersionsRequest)";}
public int kafkatest_f3556_0()
{    return throttleTimeMs;}
public Collection<ApiVersion> kafkatest_f3557_0()
{    return apiKeyToApiVersion.values();}
public ControlledShutdownRequest kafkatest_f3566_0(short version)
{    return new ControlledShutdownRequest(data, version);}
public String kafkatest_f3567_0()
{    return data.toString();}
public ControlledShutdownResponseData kafkatest_f3576_0()
{    return data;}
public static ControlledShutdownResponse kafkatest_f3577_0(Errors error, Set<TopicPartition> tps)
{    ControlledShutdownResponseData data = new ControlledShutdownResponseData();    data.setErrorCode(error.code());    ControlledShutdownResponseData.RemainingPartitionCollection pSet = new ControlledShutdownResponseData.RemainingPartitionCollection();    tps.forEach(tp -> {        pSet.add(new RemainingPartition().setTopicName(tp.topic()).setPartitionIndex(tp.partition()));    });    data.setRemainingPartitions(pSet);    return new ControlledShutdownResponse(data);}
public List<AclCreation> kafkatest_f3586_0()
{    return aclCreations;}
public AbstractResponse kafkatest_f3587_0(int throttleTimeMs, Throwable throwable)
{    short versionId = version();    switch(versionId) {        case 0:        case 1:            List<CreateAclsResponse.AclCreationResponse> responses = new ArrayList<>();            for (int i = 0; i < aclCreations.size(); i++) responses.add(new CreateAclsResponse.AclCreationResponse(ApiError.fromThrowable(throwable)));            return new CreateAclsResponse(throttleTimeMs, responses);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.CREATE_ACLS.latestVersion()));    }}
public Map<Errors, Integer> kafkatest_f3596_0()
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (AclCreationResponse response : aclCreationResponses) updateErrorCounts(errorCounts, response.error.error());    return errorCounts;}
public static CreateAclsResponse kafkatest_f3597_0(ByteBuffer buffer, short version)
{    return new CreateAclsResponse(ApiKeys.CREATE_ACLS.responseSchema(version).read(buffer));}
public static CreateDelegationTokenResponse kafkatest_f3606_0(int throttleTimeMs, Errors error, KafkaPrincipal owner, long issueTimestamp, long expiryTimestamp, long maxTimestamp, String tokenId, ByteBuffer hmac)
{    CreateDelegationTokenResponseData data = new CreateDelegationTokenResponseData().setThrottleTimeMs(throttleTimeMs).setErrorCode(error.code()).setPrincipalType(owner.getPrincipalType()).setPrincipalName(owner.getName()).setIssueTimestampMs(issueTimestamp).setExpiryTimestampMs(expiryTimestamp).setMaxTimestampMs(maxTimestamp).setTokenId(tokenId).setHmac(hmac.array());    return new CreateDelegationTokenResponse(data);}
public static CreateDelegationTokenResponse kafkatest_f3607_0(int throttleTimeMs, Errors error, KafkaPrincipal owner)
{    return prepareResponse(throttleTimeMs, error, owner, -1, -1, -1, "", ByteBuffer.wrap(new byte[] {}));}
public int kafkatest_f3616_0()
{    return totalCount;}
public List<List<Integer>> kafkatest_f3617_0()
{    return newAssignments;}
public AbstractResponse kafkatest_f3626_0(int throttleTimeMs, Throwable e)
{    Map<String, ApiError> topicErrors = new HashMap<>();    for (String topic : newPartitions.keySet()) {        topicErrors.put(topic, ApiError.fromThrowable(e));    }    short versionId = version();    switch(versionId) {        case 0:        case 1:            return new CreatePartitionsResponse(throttleTimeMs, topicErrors);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.CREATE_PARTITIONS.latestVersion()));    }}
public static CreatePartitionsRequest kafkatest_f3627_0(ByteBuffer buffer, short version)
{    return new CreatePartitionsRequest(ApiKeys.CREATE_PARTITIONS.parseRequest(version, buffer), version);}
public String kafkatest_f3636_0()
{    return data.toString();}
public CreateTopicsRequestData kafkatest_f3637_0()
{    return data;}
public boolean kafkatest_f3646_0(short version)
{    return version >= 3;}
public static Schema[] kafkatest_f3647_0()
{    return new Schema[] { DELETE_ACLS_REQUEST_V0, DELETE_ACLS_REQUEST_V1 };}
public ApiError kafkatest_f3656_0()
{    return error;}
public AclBinding kafkatest_f3657_0()
{    return acl;}
public static DeleteAclsResponse kafkatest_f3666_0(ByteBuffer buffer, short version)
{    return new DeleteAclsResponse(ApiKeys.DELETE_ACLS.responseSchema(version).read(buffer));}
public String kafkatest_f3667_0()
{    return "(responses=" + Utils.join(responses, ",") + ")";}
public Map<String, Errors> kafkatest_f3676_0()
{    Map<String, Errors> errorMap = new HashMap<>();    for (DeletableGroupResult result : data.results()) {        errorMap.put(result.groupId(), Errors.forCode(result.errorCode()));    }    return errorMap;}
public Errors kafkatest_f3677_0(String group) throws IllegalArgumentException
{    DeletableGroupResult result = data.results().find(group);    if (result == null) {        throw new IllegalArgumentException("could not find group " + group + " in the delete group response");    }    return Errors.forCode(result.errorCode());}
public AbstractResponse kafkatest_f3686_0(int throttleTimeMs, Throwable e)
{    Map<TopicPartition, DeleteRecordsResponse.PartitionResponse> responseMap = new HashMap<>();    for (Map.Entry<TopicPartition, Long> entry : partitionOffsets.entrySet()) {        responseMap.put(entry.getKey(), new DeleteRecordsResponse.PartitionResponse(DeleteRecordsResponse.INVALID_LOW_WATERMARK, Errors.forException(e)));    }    short versionId = version();    switch(versionId) {        case 0:        case 1:            return new DeleteRecordsResponse(throttleTimeMs, responseMap);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.DELETE_RECORDS.latestVersion()));    }}
public int kafkatest_f3687_0()
{    return timeout;}
public static DeleteRecordsResponse kafkatest_f3696_0(ByteBuffer buffer, short version)
{    return new DeleteRecordsResponse(ApiKeys.DELETE_RECORDS.responseSchema(version).read(buffer));}
public boolean kafkatest_f3697_0(short version)
{    return version >= 1;}
public DeleteTopicsResponseData kafkatest_f3706_0()
{    return data;}
public Map<Errors, Integer> kafkatest_f3707_0()
{    HashMap<Errors, Integer> counts = new HashMap<>();    for (DeletableTopicResult result : data.responses()) {        Errors error = Errors.forCode(result.errorCode());        counts.put(error, counts.getOrDefault(error, 0) + 1);    }    return counts;}
public AclBindingFilter kafkatest_f3716_0()
{    return filter;}
private void kafkatest_f3717_0(AclBindingFilter filter, short version)
{    if (version == 0 && filter.patternFilter().patternType() != PatternType.LITERAL && filter.patternFilter().patternType() != PatternType.ANY) {        throw new UnsupportedVersionException("Version 0 only supports literal resource pattern types");    }    if (filter.isUnknown()) {        throw new IllegalArgumentException("Filter contain UNKNOWN elements");    }}
private void kafkatest_f3726_0(short version)
{    if (version == 0) {        final boolean unsupported = acls.stream().map(AclBinding::pattern).map(ResourcePattern::patternType).anyMatch(patternType -> patternType != PatternType.LITERAL);        if (unsupported) {            throw new UnsupportedVersionException("Version 0 only supports literal resource pattern types");        }    }    final boolean unknown = acls.stream().anyMatch(AclBinding::isUnknown);    if (unknown) {        throw new IllegalArgumentException("Contain UNKNOWN elements");    }}
public static Schema[] kafkatest_f3727_0()
{    return new Schema[] { DESCRIBE_CONFIGS_REQUEST_V0, DESCRIBE_CONFIGS_REQUEST_V1, DESCRIBE_CONFIGS_REQUEST_V2 };}
public static DescribeConfigsRequest kafkatest_f3736_0(ByteBuffer buffer, short version)
{    return new DescribeConfigsRequest(ApiKeys.DESCRIBE_CONFIGS.parseRequest(version, buffer), version);}
public static Schema[] kafkatest_f3737_0()
{    return new Schema[] { DESCRIBE_CONFIGS_RESPONSE_V0, DESCRIBE_CONFIGS_RESPONSE_V1, DESCRIBE_CONFIGS_RESPONSE_V2 };}
public static ConfigSource kafkatest_f3746_0(byte id)
{    if (id < 0)        throw new IllegalArgumentException("id should be positive, id: " + id);    if (id >= VALUES.length)        return UNKNOWN_CONFIG;    return VALUES[id];}
public String kafkatest_f3747_0()
{    return name;}
public boolean kafkatest_f3756_0(short version)
{    return version >= 2;}
public DescribeDelegationTokenRequest kafkatest_f3757_0(short version)
{    return new DescribeDelegationTokenRequest(data, version);}
public int kafkatest_f3766_0()
{    return data.throttleTimeMs();}
public Errors kafkatest_f3767_0()
{    return Errors.forCode(data.errorCode());}
public static DescribeGroupsRequest kafkatest_f3776_0(ByteBuffer buffer, short version)
{    return new DescribeGroupsRequest(ApiKeys.DESCRIBE_GROUPS.parseRequest(version, buffer), version);}
public static DescribedGroupMember kafkatest_f3777_0(final String memberId, final String groupInstanceId, final String clientId, final String clientHost, final byte[] assignment, final byte[] metadata)
{    return new DescribedGroupMember().setMemberId(memberId).setGroupInstanceId(groupInstanceId).setClientId(clientId).setClientHost(clientHost).setMemberAssignment(assignment).setMemberMetadata(metadata);}
public static DescribeGroupsResponse kafkatest_f3786_0(ByteBuffer buffer, short version)
{    return new DescribeGroupsResponse(ApiKeys.DESCRIBE_GROUPS.responseSchema(version).read(buffer), version);}
public boolean kafkatest_f3787_0(short version)
{    return version >= 2;}
public static Schema[] kafkatest_f3796_0()
{    return new Schema[] { DESCRIBE_LOG_DIRS_RESPONSE_V0, DESCRIBE_LOG_DIRS_RESPONSE_V1 };}
protected Struct kafkatest_f3797_0(short version)
{    Struct struct = new Struct(ApiKeys.DESCRIBE_LOG_DIRS.responseSchema(version));    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    List<Struct> logDirStructArray = new ArrayList<>();    for (Map.Entry<String, LogDirInfo> logDirInfosEntry : logDirInfos.entrySet()) {        LogDirInfo logDirInfo = logDirInfosEntry.getValue();        Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);        logDirStruct.set(ERROR_CODE, logDirInfo.error.code());        logDirStruct.set(LOG_DIR_KEY_NAME, logDirInfosEntry.getKey());        Map<String, Map<Integer, ReplicaInfo>> replicaInfosByTopic = CollectionUtils.groupPartitionDataByTopic(logDirInfo.replicaInfos);        List<Struct> topicStructArray = new ArrayList<>();        for (Map.Entry<String, Map<Integer, ReplicaInfo>> replicaInfosByTopicEntry : replicaInfosByTopic.entrySet()) {            Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);            topicStruct.set(TOPIC_NAME, replicaInfosByTopicEntry.getKey());            List<Struct> partitionStructArray = new ArrayList<>();            for (Map.Entry<Integer, ReplicaInfo> replicaInfosByPartitionEntry : replicaInfosByTopicEntry.getValue().entrySet()) {                Struct partitionStruct = topicStruct.instance(PARTITIONS_KEY_NAME);                ReplicaInfo replicaInfo = replicaInfosByPartitionEntry.getValue();                partitionStruct.set(PARTITION_ID, replicaInfosByPartitionEntry.getKey());                partitionStruct.set(SIZE_KEY_NAME, replicaInfo.size);                partitionStruct.set(OFFSET_LAG_KEY_NAME, replicaInfo.offsetLag);                partitionStruct.set(IS_FUTURE_KEY_NAME, replicaInfo.isFuture);                partitionStructArray.add(partitionStruct);            }            topicStruct.set(PARTITIONS_KEY_NAME, partitionStructArray.toArray());            topicStructArray.add(topicStruct);        }        logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());        logDirStructArray.add(logDirStruct);    }    struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());    return struct;}
public String kafkatest_f3806_0()
{    return "ElectLeadersRequest(" + "electionType=" + electionType + ", topicPartitions=" + ((topicPartitions == null) ? "null" : MessageUtil.deepToString(topicPartitions.iterator())) + ", timeoutMs=" + timeoutMs + ")";}
private ElectLeadersRequestData kafkatest_f3807_0(short version)
{    if (electionType != ElectionType.PREFERRED && version == 0) {        throw new UnsupportedVersionException("API Version 0 only supports PREFERRED election type");    }    ElectLeadersRequestData data = new ElectLeadersRequestData().setTimeoutMs(timeoutMs);    if (topicPartitions != null) {        for (Map.Entry<String, List<Integer>> tp : CollectionUtils.groupPartitionsByTopic(topicPartitions).entrySet()) {            data.topicPartitions().add(new ElectLeadersRequestData.TopicPartitions().setTopic(tp.getKey()).setPartitionId(tp.getValue()));        }    } else {        data.setTopicPartitions(null);    }    data.setElectionType(electionType.value);    return data;}
public Map<Errors, Integer> kafkatest_f3816_0()
{    HashMap<Errors, Integer> counts = new HashMap<>();    for (ReplicaElectionResult result : data.replicaElectionResults()) {        for (PartitionResult partitionResult : result.partitionResult()) {            Errors error = Errors.forCode(partitionResult.errorCode());            counts.put(error, counts.getOrDefault(error, 0) + 1);        }    }    return counts;}
public static ElectLeadersResponse kafkatest_f3817_0(ByteBuffer buffer, short version)
{    return new ElectLeadersResponse(ApiKeys.ELECT_LEADERS.responseSchema(version).read(buffer), version);}
public short kafkatest_f3826_0()
{    return producerEpoch;}
public TransactionResult kafkatest_f3827_0()
{    return result;}
public static EndTxnResponse kafkatest_f3836_0(ByteBuffer buffer, short version)
{    return new EndTxnResponse(ApiKeys.END_TXN.parseResponse(version, buffer));}
public String kafkatest_f3837_0()
{    return "EndTxnResponse(" + "error=" + error + ", throttleTimeMs=" + throttleTimeMs + ')';}
public static ExpireDelegationTokenRequest kafkatest_f3846_0(ByteBuffer buffer, short version)
{    return new ExpireDelegationTokenRequest(ApiKeys.EXPIRE_DELEGATION_TOKEN.parseRequest(version, buffer), version);}
protected Struct kafkatest_f3847_0()
{    return data.toStruct(version());}
public Map<Errors, Integer> kafkatest_f3856_0()
{    return Collections.singletonMap(error(), 1);}
protected Struct kafkatest_f3857_0(short version)
{    return data.toStruct(version);}
public boolean kafkatest_f3866_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    FetchMetadata that = (FetchMetadata) o;    return sessionId == that.sessionId && epoch == that.epoch;}
public FetchMetadata kafkatest_f3867_0()
{    return new FetchMetadata(sessionId, INITIAL_EPOCH);}
public static Builder kafkatest_f3876_0(int maxWait, int minBytes, Map<TopicPartition, PartitionData> fetchData)
{    return new Builder(ApiKeys.FETCH.oldestVersion(), ApiKeys.FETCH.latestVersion(), CONSUMER_REPLICA_ID, maxWait, minBytes, fetchData);}
public static Builder kafkatest_f3877_0(short allowedVersion, int replicaId, int maxWait, int minBytes, Map<TopicPartition, PartitionData> fetchData)
{    return new Builder(allowedVersion, allowedVersion, replicaId, maxWait, minBytes, fetchData);}
public String kafkatest_f3886_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=FetchRequest").append(", replicaId=").append(replicaId).append(", maxWait=").append(maxWait).append(", minBytes=").append(minBytes).append(", maxBytes=").append(maxBytes).append(", fetchData=").append(fetchData).append(", isolationLevel=").append(isolationLevel).append(", toForget=").append(Utils.join(toForget, ", ")).append(", metadata=").append(metadata).append(", rackId=").append(rackId).append(")");    return bld.toString();}
public AbstractResponse kafkatest_f3887_0(int throttleTimeMs, Throwable e)
{    // The error is indicated in two ways: by setting the same error code in all partitions, and by    // setting the top-level error code.  The form where we set the same error code in all partitions    // is needed in order to maintain backwards compatibility with older versions of the protocol    // in which there was no top-level error code. Note that for incremental fetch responses, there    // may not be any partitions at all in the response.  For this reason, the top-level error code    // is essential for them.    Errors error = Errors.forException(e);    LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();    for (Map.Entry<TopicPartition, PartitionData> entry : fetchData.entrySet()) {        FetchResponse.PartitionData<MemoryRecords> partitionResponse = new FetchResponse.PartitionData<>(error, FetchResponse.INVALID_HIGHWATERMARK, FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), null, MemoryRecords.EMPTY);        responseData.put(entry.getKey(), partitionResponse);    }    return new FetchResponse<>(error, responseData, throttleTimeMs, metadata.sessionId());}
public FetchMetadata kafkatest_f3896_0()
{    return metadata;}
public String kafkatest_f3897_0()
{    return rackId;}
public String kafkatest_f3906_0()
{    return "(error=" + error + ", highWaterMark=" + highWatermark + ", lastStableOffset = " + lastStableOffset + ", logStartOffset = " + logStartOffset + ", preferredReadReplica = " + preferredReadReplica.map(Object::toString).orElse("absent") + ", abortedTransactions = " + abortedTransactions + ", recordsSizeInBytes=" + records.sizeInBytes() + ")";}
public static FetchResponse<MemoryRecords> kafkatest_f3907_0(Struct struct)
{    LinkedHashMap<TopicPartition, PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();    for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {        Struct topicResponse = (Struct) topicResponseObj;        String topic = topicResponse.get(TOPIC_NAME);        for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {            Struct partitionResponse = (Struct) partitionResponseObj;            Struct partitionResponseHeader = partitionResponse.getStruct(PARTITION_HEADER_KEY_NAME);            int partition = partitionResponseHeader.get(PARTITION_ID);            Errors error = Errors.forCode(partitionResponseHeader.get(ERROR_CODE));            long highWatermark = partitionResponseHeader.get(HIGH_WATERMARK);            long lastStableOffset = partitionResponseHeader.getOrElse(LAST_STABLE_OFFSET, INVALID_LAST_STABLE_OFFSET);            long logStartOffset = partitionResponseHeader.getOrElse(LOG_START_OFFSET, INVALID_LOG_START_OFFSET);            Optional<Integer> preferredReadReplica = Optional.of(partitionResponseHeader.getOrElse(PREFERRED_READ_REPLICA, INVALID_PREFERRED_REPLICA_ID)).filter(Predicate.isEqual(INVALID_PREFERRED_REPLICA_ID).negate());            BaseRecords baseRecords = partitionResponse.getRecords(RECORD_SET_KEY_NAME);            if (!(baseRecords instanceof MemoryRecords))                throw new IllegalStateException("Unknown records type found: " + baseRecords.getClass());            MemoryRecords records = (MemoryRecords) baseRecords;            List<AbortedTransaction> abortedTransactions = null;            if (partitionResponseHeader.hasField(ABORTED_TRANSACTIONS_KEY_NAME)) {                Object[] abortedTransactionsArray = partitionResponseHeader.getArray(ABORTED_TRANSACTIONS_KEY_NAME);                if (abortedTransactionsArray != null) {                    abortedTransactions = new ArrayList<>(abortedTransactionsArray.length);                    for (Object abortedTransactionObj : abortedTransactionsArray) {                        Struct abortedTransactionStruct = (Struct) abortedTransactionObj;                        long producerId = abortedTransactionStruct.get(PRODUCER_ID);                        long firstOffset = abortedTransactionStruct.get(FIRST_OFFSET);                        abortedTransactions.add(new AbortedTransaction(producerId, firstOffset));                    }                }            }            PartitionData<MemoryRecords> partitionData = new PartitionData<>(error, highWatermark, lastStableOffset, logStartOffset, preferredReadReplica, abortedTransactions, records);            responseData.put(new TopicPartition(topic, partition), partitionData);        }    }    return new FetchResponse<>(Errors.forCode(struct.getOrElse(ERROR_CODE, (short) 0)), responseData, struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME), struct.getOrElse(SESSION_ID, INVALID_SESSION_ID));}
private static void kafkatest_f3916_0(Struct struct, int throttleTimeMs, String dest, Queue<Send> sends)
{    Object[] allTopicData = struct.getArray(RESPONSES_KEY_NAME);    if (struct.hasField(ERROR_CODE)) {        ByteBuffer buffer = ByteBuffer.allocate(14);        buffer.putInt(throttleTimeMs);        buffer.putShort(struct.get(ERROR_CODE));        buffer.putInt(struct.get(SESSION_ID));        buffer.putInt(allTopicData.length);        buffer.rewind();        sends.add(new ByteBufferSend(dest, buffer));    } else if (struct.hasField(THROTTLE_TIME_MS)) {        ByteBuffer buffer = ByteBuffer.allocate(8);        buffer.putInt(throttleTimeMs);        buffer.putInt(allTopicData.length);        buffer.rewind();        sends.add(new ByteBufferSend(dest, buffer));    } else {        ByteBuffer buffer = ByteBuffer.allocate(4);        buffer.putInt(allTopicData.length);        buffer.rewind();        sends.add(new ByteBufferSend(dest, buffer));    }    for (Object topicData : allTopicData) addTopicData(dest, sends, (Struct) topicData);}
private static void kafkatest_f3917_0(String dest, Queue<Send> sends, Struct topicData)
{    String topic = topicData.get(TOPIC_NAME);    Object[] allPartitionData = topicData.getArray(PARTITIONS_KEY_NAME);    // include the topic header and the count for the number of partitions    ByteBuffer buffer = ByteBuffer.allocate(STRING.sizeOf(topic) + 4);    STRING.write(buffer, topic);    buffer.putInt(allPartitionData.length);    buffer.rewind();    sends.add(new ByteBufferSend(dest, buffer));    for (Object partitionData : allPartitionData) addPartitionData(dest, sends, (Struct) partitionData);}
public static FindCoordinatorRequest kafkatest_f3926_0(ByteBuffer buffer, short version)
{    return new FindCoordinatorRequest(ApiKeys.FIND_COORDINATOR.parseRequest(version, buffer), version);}
protected Struct kafkatest_f3927_0()
{    return data.toStruct(version());}
public Map<Errors, Integer> kafkatest_f3936_0()
{    return Collections.singletonMap(error(), 1);}
protected Struct kafkatest_f3937_0(short version)
{    return data.toStruct(version);}
protected Struct kafkatest_f3946_0()
{    return data.toStruct(version());}
public int kafkatest_f3947_0()
{    return data.throttleTimeMs();}
public IncrementalAlterConfigsRequestData kafkatest_f3956_0()
{    return data;}
protected Struct kafkatest_f3957_0()
{    return data.toStruct(version);}
public static IncrementalAlterConfigsResponse kafkatest_f3966_0(ByteBuffer buffer, short version)
{    return new IncrementalAlterConfigsResponse(ApiKeys.INCREMENTAL_ALTER_CONFIGS.responseSchema(version).read(buffer), version);}
public InitProducerIdRequest kafkatest_f3967_0(short version)
{    if (data.transactionTimeoutMs() <= 0)        throw new IllegalArgumentException("transaction timeout value is not positive: " + data.transactionTimeoutMs());    if (data.transactionalId() != null && data.transactionalId().isEmpty())        throw new IllegalArgumentException("Must set either a null or a non-empty transactional id.");    return new InitProducerIdRequest(data, version);}
public String kafkatest_f3976_0()
{    return data.toString();}
public Errors kafkatest_f3977_0()
{    return Errors.forCode(data.errorCode());}
public JoinGroupRequestData kafkatest_f3986_0()
{    return data;}
public AbstractResponse kafkatest_f3987_0(int throttleTimeMs, Throwable e)
{    short versionId = version();    switch(versionId) {        case 0:        case 1:            return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(Errors.forException(e).code()).setGenerationId(JoinGroupResponse.UNKNOWN_GENERATION_ID).setProtocolName(JoinGroupResponse.UNKNOWN_PROTOCOL).setLeader(JoinGroupResponse.UNKNOWN_MEMBER_ID).setMemberId(JoinGroupResponse.UNKNOWN_MEMBER_ID).setMembers(Collections.emptyList()));        case 2:        case 3:        case 4:        case 5:            return new JoinGroupResponse(new JoinGroupResponseData().setThrottleTimeMs(throttleTimeMs).setErrorCode(Errors.forException(e).code()).setGenerationId(JoinGroupResponse.UNKNOWN_GENERATION_ID).setProtocolName(JoinGroupResponse.UNKNOWN_PROTOCOL).setLeader(JoinGroupResponse.UNKNOWN_MEMBER_ID).setMemberId(JoinGroupResponse.UNKNOWN_MEMBER_ID).setMembers(Collections.emptyList()));        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.JOIN_GROUP.latestVersion()));    }}
protected Struct kafkatest_f3996_0(short version)
{    return data.toStruct(version);}
public String kafkatest_f3997_0()
{    return data.toString();}
public Map<TopicPartition, PartitionState> kafkatest_f4006_0()
{    return partitionStates;}
public Set<Node> kafkatest_f4007_0()
{    return liveLeaders;}
protected Struct kafkatest_f4016_0(short version)
{    Struct struct = new Struct(ApiKeys.LEADER_AND_ISR.responseSchema(version));    List<Struct> responseDatas = new ArrayList<>(responses.size());    for (Map.Entry<TopicPartition, Errors> response : responses.entrySet()) {        Struct partitionData = struct.instance(PARTITIONS);        TopicPartition partition = response.getKey();        partitionData.set(TOPIC_NAME, partition.topic());        partitionData.set(PARTITION_ID, partition.partition());        partitionData.set(ERROR_CODE, response.getValue().code());        responseDatas.add(partitionData);    }    struct.set(PARTITIONS, responseDatas.toArray());    struct.set(ERROR_CODE, error.code());    return struct;}
public String kafkatest_f4017_0()
{    return "LeaderAndIsrResponse(" + "responses=" + responses + ", error=" + error + ")";}
public List<MemberResponse> kafkatest_f4026_0()
{    return data.members();}
public Errors kafkatest_f4027_0()
{    return getError(Errors.forCode(data.errorCode()), data.members());}
public ListGroupsRequest kafkatest_f4036_0(short version)
{    return new ListGroupsRequest(data, version);}
public String kafkatest_f4037_0()
{    return data.toString();}
public boolean kafkatest_f4046_0(short version)
{    return version >= 2;}
public static Schema[] kafkatest_f4047_0()
{    return new Schema[] { LIST_OFFSET_REQUEST_V0, LIST_OFFSET_REQUEST_V1, LIST_OFFSET_REQUEST_V2, LIST_OFFSET_REQUEST_V3, LIST_OFFSET_REQUEST_V4, LIST_OFFSET_REQUEST_V5 };}
public IsolationLevel kafkatest_f4056_0()
{    return isolationLevel;}
public Map<TopicPartition, PartitionData> kafkatest_f4057_0()
{    return partitionTimestamps;}
public static ListOffsetResponse kafkatest_f4066_0(ByteBuffer buffer, short version)
{    return new ListOffsetResponse(ApiKeys.LIST_OFFSETS.parseResponse(version, buffer));}
protected Struct kafkatest_f4067_0(short version)
{    Struct struct = new Struct(ApiKeys.LIST_OFFSETS.responseSchema(version));    struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);    Map<String, Map<Integer, PartitionData>> topicsData = CollectionUtils.groupPartitionDataByTopic(responseData);    List<Struct> topicArray = new ArrayList<>();    for (Map.Entry<String, Map<Integer, PartitionData>> topicEntry : topicsData.entrySet()) {        Struct topicData = struct.instance(TOPICS);        topicData.set(TOPIC_NAME, topicEntry.getKey());        List<Struct> partitionArray = new ArrayList<>();        for (Map.Entry<Integer, PartitionData> partitionEntry : topicEntry.getValue().entrySet()) {            PartitionData offsetPartitionData = partitionEntry.getValue();            Struct partitionData = topicData.instance(PARTITIONS);            partitionData.set(PARTITION_ID, partitionEntry.getKey());            partitionData.set(ERROR_CODE, offsetPartitionData.error.code());            if (version == 0) {                partitionData.set(OFFSETS, offsetPartitionData.offsets.toArray());            } else {                partitionData.set(TIMESTAMP, offsetPartitionData.timestamp);                partitionData.set(OFFSET, offsetPartitionData.offset);                RequestUtils.setLeaderEpochIfExists(partitionData, LEADER_EPOCH, offsetPartitionData.leaderEpoch);            }            partitionArray.add(partitionData);        }        topicData.set(PARTITIONS, partitionArray.toArray());        topicArray.add(topicData);    }    struct.set(TOPICS, topicArray.toArray());    return struct;}
public static ListPartitionReassignmentsResponse kafkatest_f4076_0(ByteBuffer buffer, short version)
{    return new ListPartitionReassignmentsResponse(ApiKeys.LIST_PARTITION_REASSIGNMENTS.responseSchema(version).read(buffer), version);}
public ListPartitionReassignmentsResponseData kafkatest_f4077_0()
{    return data;}
public MetadataRequest kafkatest_f4086_0(short version)
{    if (version < 1)        throw new UnsupportedVersionException("MetadataRequest versions older than 1 are not supported.");    if (!data.allowAutoTopicCreation() && version < 4)        throw new UnsupportedVersionException("MetadataRequest versions older than 4 don't support the " + "allowAutoTopicCreation field");    return new MetadataRequest(data, version);}
public String kafkatest_f4087_0()
{    return data.toString();}
protected Struct kafkatest_f4096_0(short version)
{    return data.toStruct(version);}
public int kafkatest_f4097_0()
{    return data.throttleTimeMs();}
public Collection<Node> kafkatest_f4106_0()
{    return holder().brokers;}
public Collection<TopicMetadata> kafkatest_f4107_0()
{    return holder().topicMetadata;}
public int kafkatest_f4116_0()
{    return authorizedOperations;}
public boolean kafkatest_f4117_0(final Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    final TopicMetadata that = (TopicMetadata) o;    return isInternal == that.isInternal && error == that.error && Objects.equals(topic, that.topic) && Objects.equals(partitionMetadata, that.partitionMetadata) && Objects.equals(authorizedOperations, that.authorizedOperations);}
public List<Node> kafkatest_f4126_0()
{    return isr;}
public List<Node> kafkatest_f4127_0()
{    return offlineReplicas;}
public OffsetCommitRequest kafkatest_f4136_0(short version)
{    if (data.groupInstanceId() != null && version < 7) {        throw new UnsupportedVersionException("The broker offset commit protocol version " + version + " does not support usage of config group.instance.id.");    }    return new OffsetCommitRequest(data, version);}
public String kafkatest_f4137_0()
{    return data.toString();}
public static OffsetCommitResponse kafkatest_f4146_0(ByteBuffer buffer, short version)
{    return new OffsetCommitResponse(ApiKeys.OFFSET_COMMIT.parseResponse(version, buffer), version);}
public Struct kafkatest_f4147_0(short version)
{    return data.toStruct(version);}
protected Struct kafkatest_f4156_0()
{    return data.toStruct(version());}
protected Struct kafkatest_f4157_0(short version)
{    return data.toStruct(version);}
public List<TopicPartition> kafkatest_f4166_0()
{    if (isAllPartitions()) {        return null;    }    List<TopicPartition> partitions = new ArrayList<>();    for (OffsetFetchRequestTopic topic : data.topics()) {        for (Integer partitionIndex : topic.partitionIndexes()) {            partitions.add(new TopicPartition(topic.name(), partitionIndex));        }    }    return partitions;}
public String kafkatest_f4167_0()
{    return data.groupId();}
public String kafkatest_f4176_0()
{    return "PartitionData(" + "offset=" + offset + ", leaderEpoch=" + leaderEpoch.orElse(NO_PARTITION_LEADER_EPOCH) + ", metadata=" + metadata + ", error='" + error.toString() + ")";}
public int kafkatest_f4177_0()
{    return Objects.hash(offset, leaderEpoch, metadata, error);}
public static Schema[] kafkatest_f4186_0()
{    return new Schema[] { OFFSET_FOR_LEADER_EPOCH_REQUEST_V0, OFFSET_FOR_LEADER_EPOCH_REQUEST_V1, OFFSET_FOR_LEADER_EPOCH_REQUEST_V2, OFFSET_FOR_LEADER_EPOCH_REQUEST_V3 };}
public Map<TopicPartition, PartitionData> kafkatest_f4187_0()
{    return epochsByPartition;}
public AbstractResponse kafkatest_f4196_0(int throttleTimeMs, Throwable e)
{    Errors error = Errors.forException(e);    Map<TopicPartition, EpochEndOffset> errorResponse = new HashMap<>();    for (TopicPartition tp : epochsByPartition.keySet()) {        errorResponse.put(tp, new EpochEndOffset(error, EpochEndOffset.UNDEFINED_EPOCH, EpochEndOffset.UNDEFINED_EPOCH_OFFSET));    }    return new OffsetsForLeaderEpochResponse(throttleTimeMs, errorResponse);}
public String kafkatest_f4197_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(currentLeaderEpoch=").append(currentLeaderEpoch).append(", leaderEpoch=").append(leaderEpoch).append(")");    return bld.toString();}
public static Schema[] kafkatest_f4206_0()
{    return new Schema[] { PRODUCE_REQUEST_V0, PRODUCE_REQUEST_V1, PRODUCE_REQUEST_V2, PRODUCE_REQUEST_V3, PRODUCE_REQUEST_V4, PRODUCE_REQUEST_V5, PRODUCE_REQUEST_V6, PRODUCE_REQUEST_V7 };}
public static Builder kafkatest_f4207_0(short acks, int timeout, Map<TopicPartition, MemoryRecords> partitionRecords)
{    return forMagic(RecordBatch.CURRENT_MAGIC_VALUE, acks, timeout, partitionRecords, null);}
public String kafkatest_f4216_0(boolean verbose)
{    // Use the same format as `Struct.toString()`    StringBuilder bld = new StringBuilder();    bld.append("{acks=").append(acks).append(",timeout=").append(timeout);    if (verbose)        bld.append(",partitionSizes=").append(Utils.mkString(partitionSizes, "[", "]", "=", ","));    else        bld.append(",numPartitions=").append(partitionSizes.size());    bld.append("}");    return bld.toString();}
public ProduceResponse kafkatest_f4217_0(int throttleTimeMs, Throwable e)
{    /* In case the producer doesn't actually want any response */    if (acks == 0)        return null;    Errors error = Errors.forException(e);    Map<TopicPartition, ProduceResponse.PartitionResponse> responseMap = new HashMap<>();    ProduceResponse.PartitionResponse partitionResponse = new ProduceResponse.PartitionResponse(error);    for (TopicPartition tp : partitions()) responseMap.put(tp, partitionResponse);    short versionId = version();    switch(versionId) {        case 0:        case 1:        case 2:        case 3:        case 4:        case 5:        case 6:        case 7:            return new ProduceResponse(responseMap, throttleTimeMs);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.PRODUCE.latestVersion()));    }}
public void kafkatest_f4226_0()
{    partitionRecords = null;}
public static void kafkatest_f4227_0(short version, MemoryRecords records)
{    if (version >= 3) {        Iterator<MutableRecordBatch> iterator = records.batches().iterator();        if (!iterator.hasNext())            throw new InvalidRecordException("Produce requests with version " + version + " must have at least " + "one record batch");        MutableRecordBatch entry = iterator.next();        if (entry.magic() != RecordBatch.MAGIC_VALUE_V2)            throw new InvalidRecordException("Produce requests with version " + version + " are only allowed to " + "contain record batches with magic version 2");        if (version < 7 && entry.compressionType() == CompressionType.ZSTD) {            throw new UnsupportedCompressionTypeException("Produce requests with version " + version + " are not allowed to " + "use ZStandard compression");        }        if (iterator.hasNext())            throw new InvalidRecordException("Produce requests with version " + version + " are only allowed to " + "contain exactly one record batch");    }// Note that we do not do similar validation for older versions to ensure compatibility with// clients which send the wrong magic version in the wrong version of the produce request. The broker// did not do this validation before, so we maintain that behavior here.}
public static ProduceResponse kafkatest_f4236_0(ByteBuffer buffer, short version)
{    return new ProduceResponse(ApiKeys.PRODUCE.responseSchema(version).read(buffer));}
public boolean kafkatest_f4237_0(short version)
{    return version >= 6;}
protected Struct kafkatest_f4246_0(short version)
{    return data.toStruct(version);}
public int kafkatest_f4247_0()
{    return data.throttleTimeMs();}
public String kafkatest_f4256_0()
{    return listenerName.value();}
public SecurityProtocol kafkatest_f4257_0()
{    return securityProtocol;}
public short kafkatest_f4266_0()
{    return apiVersion;}
public String kafkatest_f4267_0()
{    return clientId;}
 static void kafkatest_f4276_0(ResourcePattern pattern, Struct struct)
{    struct.set(RESOURCE_TYPE, pattern.resourceType().code());    struct.set(RESOURCE_NAME, pattern.name());    struct.setIfExists(RESOURCE_PATTERN_TYPE, pattern.patternType().code());}
 static ResourcePatternFilter kafkatest_f4277_0(Struct struct)
{    byte resourceType = struct.get(RESOURCE_TYPE);    String name = struct.get(RESOURCE_NAME_FILTER);    PatternType patternType = PatternType.fromCode(struct.getOrElse(RESOURCE_PATTERN_TYPE_FILTER, PatternType.LITERAL.code()));    return new ResourcePatternFilter(ResourceType.fromCode(resourceType), name, patternType);}
public int kafkatest_f4286_0()
{    return toStruct().sizeOf();}
public Struct kafkatest_f4287_0()
{    Struct struct = new Struct(SCHEMA);    struct.set(CORRELATION_KEY_FIELD, correlationId);    return struct;}
public Errors kafkatest_f4296_0()
{    return Errors.forCode(data.errorCode());}
public Map<Errors, Integer> kafkatest_f4297_0()
{    return Collections.singletonMap(Errors.forCode(data.errorCode()), 1);}
public AbstractResponse kafkatest_f4306_0(int throttleTimeMs, Throwable e)
{    SaslHandshakeResponseData response = new SaslHandshakeResponseData();    response.setErrorCode(ApiError.fromThrowable(e).error().code());    return new SaslHandshakeResponse(response);}
public static SaslHandshakeRequest kafkatest_f4307_0(ByteBuffer buffer, short version)
{    return new SaslHandshakeRequest(ApiKeys.SASL_HANDSHAKE.parseRequest(version, buffer), version);}
public String kafkatest_f4316_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=StopReplicaRequest").append(", controllerId=").append(controllerId).append(", controllerEpoch=").append(controllerEpoch).append(", deletePartitions=").append(deletePartitions).append(", brokerEpoch=").append(brokerEpoch).append(", partitions=").append(Utils.join(partitions, ",")).append(")");    return bld.toString();}
public StopReplicaResponse kafkatest_f4317_0(int throttleTimeMs, Throwable e)
{    Errors error = Errors.forException(e);    Map<TopicPartition, Errors> responses = new HashMap<>(partitions.size());    for (TopicPartition partition : partitions) {        responses.put(partition, error);    }    short versionId = version();    switch(versionId) {        case 0:        case 1:            return new StopReplicaResponse(error, responses);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.STOP_REPLICA.latestVersion()));    }}
public static StopReplicaResponse kafkatest_f4326_0(ByteBuffer buffer, short version)
{    return new StopReplicaResponse(ApiKeys.STOP_REPLICA.parseResponse(version, buffer));}
protected Struct kafkatest_f4327_0(short version)
{    Struct struct = new Struct(ApiKeys.STOP_REPLICA.responseSchema(version));    List<Struct> responseDatas = new ArrayList<>(responses.size());    for (Map.Entry<TopicPartition, Errors> response : responses.entrySet()) {        Struct partitionData = struct.instance(PARTITIONS);        TopicPartition partition = response.getKey();        partitionData.set(TOPIC_NAME, partition.topic());        partitionData.set(PARTITION_ID, partition.partition());        partitionData.set(ERROR_CODE, response.getValue().code());        responseDatas.add(partitionData);    }    struct.set(PARTITIONS, responseDatas.toArray());    struct.set(ERROR_CODE, error.code());    return struct;}
public Errors kafkatest_f4336_0()
{    return Errors.forCode(data.errorCode());}
public Map<Errors, Integer> kafkatest_f4337_0()
{    return Collections.singletonMap(Errors.forCode(data.errorCode()), 1);}
protected Struct kafkatest_f4346_0()
{    return data.toStruct(version());}
 static List<TxnOffsetCommitResponseTopic> kafkatest_f4347_0(List<TxnOffsetCommitRequestTopic> requestTopics, Errors e)
{    List<TxnOffsetCommitResponseTopic> responseTopicData = new ArrayList<>();    for (TxnOffsetCommitRequestTopic entry : requestTopics) {        List<TxnOffsetCommitResponsePartition> responsePartitions = new ArrayList<>();        for (TxnOffsetCommitRequestPartition requestPartition : entry.partitions()) {            responsePartitions.add(new TxnOffsetCommitResponsePartition().setPartitionIndex(requestPartition.partitionIndex()).setErrorCode(e.code()));        }        responseTopicData.add(new TxnOffsetCommitResponseTopic().setName(entry.name()).setPartitions(responsePartitions));    }    return responseTopicData;}
public Map<TopicPartition, Errors> kafkatest_f4356_0()
{    Map<TopicPartition, Errors> errorMap = new HashMap<>();    for (TxnOffsetCommitResponseTopic topic : data.topics()) {        for (TxnOffsetCommitResponsePartition partition : topic.partitions()) {            errorMap.put(new TopicPartition(topic.name(), partition.partitionIndex()), Errors.forCode(partition.errorCode()));        }    }    return errorMap;}
public static TxnOffsetCommitResponse kafkatest_f4357_0(ByteBuffer buffer, short version)
{    return new TxnOffsetCommitResponse(ApiKeys.TXN_OFFSET_COMMIT.parseResponse(version, buffer), version);}
public String kafkatest_f4366_0()
{    return "(host=" + host + ", port=" + port + ", listenerName=" + listenerName + ", securityProtocol=" + securityProtocol + ")";}
protected Struct kafkatest_f4367_0()
{    short version = version();    Struct struct = new Struct(ApiKeys.UPDATE_METADATA.requestSchema(version));    struct.set(CONTROLLER_ID, controllerId);    struct.set(CONTROLLER_EPOCH, controllerEpoch);    struct.setIfExists(BROKER_EPOCH, brokerEpoch);    if (struct.hasField(TOPIC_STATES)) {        Map<String, Map<Integer, PartitionState>> topicStates = CollectionUtils.groupPartitionDataByTopic(partitionStates);        List<Struct> topicStatesData = new ArrayList<>(topicStates.size());        for (Map.Entry<String, Map<Integer, PartitionState>> entry : topicStates.entrySet()) {            Struct topicStateData = struct.instance(TOPIC_STATES);            topicStateData.set(TOPIC_NAME, entry.getKey());            Map<Integer, PartitionState> partitionMap = entry.getValue();            List<Struct> partitionStatesData = new ArrayList<>(partitionMap.size());            for (Map.Entry<Integer, PartitionState> partitionEntry : partitionMap.entrySet()) {                Struct partitionStateData = topicStateData.instance(PARTITION_STATES);                partitionStateData.set(PARTITION_ID, partitionEntry.getKey());                partitionEntry.getValue().setStruct(partitionStateData);                partitionStatesData.add(partitionStateData);            }            topicStateData.set(PARTITION_STATES, partitionStatesData.toArray());            topicStatesData.add(topicStateData);        }        struct.set(TOPIC_STATES, topicStatesData.toArray());    } else {        List<Struct> partitionStatesData = new ArrayList<>(partitionStates.size());        for (Map.Entry<TopicPartition, PartitionState> entry : partitionStates.entrySet()) {            Struct partitionStateData = struct.instance(PARTITION_STATES);            TopicPartition topicPartition = entry.getKey();            partitionStateData.set(TOPIC_NAME, topicPartition.topic());            partitionStateData.set(PARTITION_ID, topicPartition.partition());            entry.getValue().setStruct(partitionStateData);            partitionStatesData.add(partitionStateData);        }        struct.set(PARTITION_STATES, partitionStatesData.toArray());    }    List<Struct> brokersData = new ArrayList<>(liveBrokers.size());    for (Broker broker : liveBrokers) {        Struct brokerData = struct.instance(LIVE_BROKERS);        brokerData.set(BROKER_ID, broker.id);        if (version == 0) {            EndPoint endPoint = broker.endPoints.get(0);            brokerData.set(HOST, endPoint.host);            brokerData.set(PORT, endPoint.port);        } else {            List<Struct> endPointsData = new ArrayList<>(broker.endPoints.size());            for (EndPoint endPoint : broker.endPoints) {                Struct endPointData = brokerData.instance(ENDPOINTS);                endPointData.set(PORT, endPoint.port);                endPointData.set(HOST, endPoint.host);                endPointData.set(SECURITY_PROTOCOL_TYPE, endPoint.securityProtocol.id);                if (version >= 3)                    endPointData.set(LISTENER_NAME, endPoint.listenerName.value());                endPointsData.add(endPointData);            }            brokerData.set(ENDPOINTS, endPointsData.toArray());            if (version >= 2) {                brokerData.set(RACK, broker.rack);            }        }        brokersData.add(brokerData);    }    struct.set(LIVE_BROKERS, brokersData.toArray());    return struct;}
protected Struct kafkatest_f4376_0(short version)
{    Struct struct = new Struct(ApiKeys.UPDATE_METADATA.responseSchema(version));    struct.set(ERROR_CODE, error.code());    return struct;}
public static Schema[] kafkatest_f4377_0()
{    return new Schema[] { WRITE_TXN_MARKERS_REQUEST_V0 };}
public WriteTxnMarkersRequest kafkatest_f4386_0(short version)
{    return new WriteTxnMarkersRequest(version, markers);}
public List<TxnMarkerEntry> kafkatest_f4387_0()
{    return markers;}
public Map<Errors, Integer> kafkatest_f4396_0()
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (Map<TopicPartition, Errors> allErrors : errors.values()) {        for (Errors error : allErrors.values()) updateErrorCounts(errorCounts, error);    }    return errorCounts;}
public static WriteTxnMarkersResponse kafkatest_f4397_0(ByteBuffer buffer, short version)
{    return new WriteTxnMarkersResponse(ApiKeys.WRITE_TXN_MARKERS.parseResponse(version, buffer));}
public String kafkatest_f4406_0()
{    return "(resourceType=" + resourceType + ", name=" + ((name == null) ? "<any>" : name) + ")";}
public boolean kafkatest_f4407_0()
{    return resourceType.isUnknown();}
public boolean kafkatest_f4416_0(Resource other)
{    if ((name != null) && (!name.equals(other.name())))        return false;    return (resourceType == ResourceType.ANY) || (resourceType.equals(other.resourceType()));}
public boolean kafkatest_f4417_0()
{    return findIndefiniteField() == null;}
public int kafkatest_f4426_0()
{    return Objects.hash(resourceType, name, patternType);}
public boolean kafkatest_f4427_0()
{    return resourceType.isUnknown() || patternType.isUnknown();}
public int kafkatest_f4436_0()
{    return Objects.hash(resourceType, name, patternType);}
public static ResourceType kafkatest_f4437_0(String str) throws IllegalArgumentException
{    try {        return ResourceType.valueOf(str.toUpperCase(Locale.ROOT));    } catch (IllegalArgumentException e) {        return UNKNOWN;    }}
public String kafkatest_f4448_0()
{    return name;}
public String kafkatest_f4449_0()
{    return principalType;}
public String kafkatest_f4458_0()
{    return listenerName;}
public Map<String, String> kafkatest_f4459_0()
{    return extensionsMap;}
public SSLSession kafkatest_f4468_0()
{    return session;}
public SecurityProtocol kafkatest_f4469_0()
{    return SecurityProtocol.SSL;}
public Cache<C> kafkatest_f4480_0(String mechanism, Class<C> credentialClass)
{    Cache<C> cache = new Cache<>(credentialClass);    @SuppressWarnings("unchecked")    Cache<C> oldCache = (Cache<C>) cacheMap.putIfAbsent(mechanism, cache);    return oldCache == null ? cache : oldCache;}
public Cache<C> kafkatest_f4481_0(String mechanism, Class<C> credentialClass)
{    Cache<?> cache = cacheMap.get(mechanism);    if (cache != null) {        if (cache.credentialClass() != credentialClass)            throw new IllegalArgumentException("Invalid credential class " + credentialClass + ", expected " + cache.credentialClass());        return (Cache<C>) cache;    } else        return null;}
private KafkaPrincipal kafkatest_f4490_0(Principal principal)
{    return new KafkaPrincipal(KafkaPrincipal.USER_TYPE, principal.getName());}
public void kafkatest_f4491_0()
{    if (oldPrincipalBuilder != null)        oldPrincipalBuilder.close();}
public static void kafkatest_f4501_0()
{    synchronized (LoginManager.class) {        for (LoginMetadata<String> key : new ArrayList<>(STATIC_INSTANCES.keySet())) STATIC_INSTANCES.remove(key).login.close();        for (LoginMetadata<Password> key : new ArrayList<>(DYNAMIC_INSTANCES.keySet())) DYNAMIC_INSTANCES.remove(key).login.close();    }}
private static Class<? extends T> kafkatest_f4502_0(Map<String, ?> configs, JaasContext jaasContext, String saslMechanism, String configName, Class<? extends T> defaultClass)
{    String prefix = jaasContext.type() == JaasContext.Type.SERVER ? ListenerName.saslMechanismPrefix(saslMechanism) : "";    @SuppressWarnings("unchecked")    Class<? extends T> clazz = (Class<? extends T>) configs.get(prefix + configName);    if (clazz != null && jaasContext.configurationEntries().size() != 1) {        String errorMessage = configName + " cannot be specified with multiple login modules in the JAAS context. " + SaslConfigs.SASL_JAAS_CONFIG + " must be configured to override mechanism-specific configs.";        throw new ConfigException(errorMessage);    }    if (clazz == null)        clazz = defaultClass;    return clazz;}
public Long kafkatest_f4511_0()
{    return reauthInfo.clientSessionReauthenticationTimeNanos;}
public Long kafkatest_f4512_0()
{    return reauthInfo.reauthenticationLatencyMs();}
public KafkaPrincipal kafkatest_f4521_0()
{    return new KafkaPrincipal(KafkaPrincipal.USER_TYPE, clientPrincipalName);}
public boolean kafkatest_f4522_0()
{    return saslState == SaslState.COMPLETE;}
public boolean kafkatest_f4531_0()
{    return apiVersionsResponseFromOriginalAuthentication != null;}
public ApiVersionsResponse kafkatest_f4532_0()
{    return reauthenticating() ? apiVersionsResponseFromOriginalAuthentication : apiVersionsResponseReceivedFromBroker;}
public voidf4542_1) throws IOException
{    if (saslState != SaslState.REAUTH_PROCESS_HANDSHAKE) {        if (netOutBuffer != null && !flushNetOutBufferAndUpdateInterestOps())            return;        if (saslServer != null && saslServer.isComplete()) {            setSaslState(SaslState.COMPLETE);            return;        }        // allocate on heap (as opposed to any socket server memory pool)        if (netInBuffer == null)            netInBuffer = new NetworkReceive(MAX_RECEIVE_SIZE, connectionId);        netInBuffer.readFrom(transportLayer);        if (!netInBuffer.complete())            return;        netInBuffer.payload().rewind();    }    byte[] clientToken = new byte[netInBuffer.payload().remaining()];    netInBuffer.payload().get(clientToken, 0, clientToken.length);    // reset the networkReceive as we read all the data.    netInBuffer = null;    try {        switch(saslState) {            case REAUTH_PROCESS_HANDSHAKE:            case HANDSHAKE_OR_VERSIONS_REQUEST:            case HANDSHAKE_REQUEST:                handleKafkaRequest(clientToken);                break;            case REAUTH_BAD_MECHANISM:                throw new SaslAuthenticationException(reauthInfo.badMechanismErrorMessage);            case INITIAL_REQUEST:                if (handleKafkaRequest(clientToken))                    break;            // This is required for interoperability with 0.9.0.x clients which do not send handshake request            case AUTHENTICATE:                handleSaslToken(clientToken);                // update SASL state. Current SASL state will be updated when outgoing writes to the client complete.                if (saslServer.isComplete())                    setSaslState(SaslState.COMPLETE);                break;            default:                break;        }    } catch (AuthenticationException e) {        // Exception will be propagated after response is sent to client        setSaslState(SaslState.FAILED, e);    } catch (Exception e) {        // In the case of IOExceptions and other unexpected exceptions, fail immediately        saslState = SaslState.FAILED;                throw e;    }}
public KafkaPrincipal kafkatest_f4543_0()
{    SaslAuthenticationContext context = new SaslAuthenticationContext(saslServer, securityProtocol, clientAddress(), listenerName.value());    KafkaPrincipal principal = principalBuilder.build(context);    if (ScramMechanism.isScram(saslMechanism) && Boolean.parseBoolean((String) saslServer.getNegotiatedProperty(ScramLoginModule.TOKEN_AUTH_CONFIG))) {        principal.tokenAuthenticated(true);    }    return principal;}
private voidf4552_1SaslState saslState, AuthenticationException exception)
{    if (netOutBuffer != null && !netOutBuffer.completed()) {        pendingSaslState = saslState;        pendingException = exception;    } else {        this.saslState = saslState;                this.pendingSaslState = null;        this.pendingException = null;        if (exception != null)            throw exception;    }}
private boolean kafkatest_f4553_0() throws IOException
{    boolean flushedCompletely = flushNetOutBuffer();    if (flushedCompletely) {        transportLayer.removeInterestOps(SelectionKey.OP_WRITE);        if (pendingSaslState != null)            setSaslState(pendingSaslState, pendingException);    } else        transportLayer.addInterestOps(SelectionKey.OP_WRITE);    return flushedCompletely;}
private void kafkatest_f4562_0(RequestContext context, ApiVersionsRequest apiVersionsRequest) throws IOException
{    if (saslState != SaslState.HANDSHAKE_OR_VERSIONS_REQUEST)        throw new IllegalStateException("Unexpected ApiVersions request received during SASL authentication state " + saslState);    if (apiVersionsRequest.hasUnsupportedRequestVersion())        sendKafkaResponse(context, apiVersionsRequest.getErrorResponse(0, Errors.UNSUPPORTED_VERSION.exception()));    else {        sendKafkaResponse(context, apiVersionsResponse());        setSaslState(SaslState.HANDSHAKE_REQUEST);    }}
private void kafkatest_f4563_0(RequestContext context, AbstractResponse response)
{    authenticationFailureSend = context.buildResponse(response);}
private longf4572_1)
{    long retvalSessionLifetimeMs = 0L;    long authenticationEndMs = time.milliseconds();    authenticationEndNanos = time.nanoseconds();    Long credentialExpirationMs = (Long) saslServer.getNegotiatedProperty(SaslInternalConfigs.CREDENTIAL_LIFETIME_MS_SASL_NEGOTIATED_PROPERTY_KEY);    Long connectionsMaxReauthMs = connectionsMaxReauthMsByMechanism.get(saslMechanism);    if (credentialExpirationMs != null || connectionsMaxReauthMs != null) {        if (credentialExpirationMs == null)            retvalSessionLifetimeMs = zeroIfNegative(connectionsMaxReauthMs.longValue());        else if (connectionsMaxReauthMs == null)            retvalSessionLifetimeMs = zeroIfNegative(credentialExpirationMs.longValue() - authenticationEndMs);        else            retvalSessionLifetimeMs = zeroIfNegative(Math.min(credentialExpirationMs.longValue() - authenticationEndMs, connectionsMaxReauthMs.longValue()));        if (retvalSessionLifetimeMs > 0L)            sessionExpirationTimeNanos = Long.valueOf(authenticationEndNanos + 1000 * 1000 * retvalSessionLifetimeMs);    }    if (credentialExpirationMs != null) {        if (sessionExpirationTimeNanos != null)             session max lifetime from broker config={} ms, credential expiration={} ({} ms); session expiration = {} ({} ms), sending {} ms to client", connectionsMaxReauthMs, new Date(credentialExpirationMs), Long.valueOf(credentialExpirationMs.longValue() - authenticationEndMs), new Date(authenticationEndMs + retvalSessionLifetimeMs), retvalSessionLifetimeMs, retvalSessionLifetimeMs);        else             session max lifetime from broker config={} ms, credential expiration={} ({} ms); no session expiration, sending 0 ms to client", connectionsMaxReauthMs, new Date(credentialExpirationMs), Long.valueOf(credentialExpirationMs.longValue() - authenticationEndMs));    } else {        if (sessionExpirationTimeNanos != null)             session max lifetime from broker config={} ms, no credential expiration; session expiration = {} ({} ms), sending {} ms to client", connectionsMaxReauthMs, new Date(authenticationEndMs + retvalSessionLifetimeMs), retvalSessionLifetimeMs, retvalSessionLifetimeMs);        else             session max lifetime from broker config={} ms, no credential expiration; no session expiration, sending 0 ms to client", connectionsMaxReauthMs);    }    return retvalSessionLifetimeMs;}
public Long kafkatest_f4573_0()
{    if (!reauthenticating())        return null;    // record at least 1 ms if there is some latency    long latencyNanos = authenticationEndNanos - reauthenticationBeginNanos;    return latencyNanos == 0L ? 0L : Math.max(1L, Long.valueOf(Math.round(latencyNanos / 1000.0 / 1000.0)));}
public static JaasContextf4583_1ListenerName listenerName, String mechanism, Map<String, ?> configs)
{    if (listenerName == null)        throw new IllegalArgumentException("listenerName should not be null for SERVER");    if (mechanism == null)        throw new IllegalArgumentException("mechanism should not be null for SERVER");    String globalContextName = GLOBAL_CONTEXT_NAME_SERVER;    String listenerContextName = listenerName.value().toLowerCase(Locale.ROOT) + "." + GLOBAL_CONTEXT_NAME_SERVER;    Password dynamicJaasConfig = (Password) configs.get(mechanism.toLowerCase(Locale.ROOT) + "." + SaslConfigs.SASL_JAAS_CONFIG);    if (dynamicJaasConfig == null && configs.get(SaslConfigs.SASL_JAAS_CONFIG) != null)            return load(Type.SERVER, listenerContextName, globalContextName, dynamicJaasConfig);}
public static JaasContext kafkatest_f4584_0(Map<String, ?> configs)
{    String globalContextName = GLOBAL_CONTEXT_NAME_CLIENT;    Password dynamicJaasConfig = (Password) configs.get(SaslConfigs.SASL_JAAS_CONFIG);    return load(JaasContext.Type.CLIENT, null, globalContextName, dynamicJaasConfig);}
public static String kafkatest_f4593_0()
{    String loginConfig = System.getProperty(JAVA_LOGIN_CONFIG_PARAM);    String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT);    String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME);    return "[" + JAVA_LOGIN_CONFIG_PARAM + "=" + loginConfig + ", " + ZK_SASL_CLIENT + "=" + clientEnabled + ", " + ZK_LOGIN_CONTEXT_NAME_KEY + "=" + contextName + "]";}
public static booleanf4594_1)
{    boolean zkSaslEnabled = Boolean.parseBoolean(System.getProperty(ZK_SASL_CLIENT, DEFAULT_ZK_SASL_CLIENT));    String zkLoginContextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, DEFAULT_ZK_LOGIN_CONTEXT_NAME);        boolean foundLoginConfigEntry;    try {        Configuration loginConf = Configuration.getConfiguration();        foundLoginConfigEntry = loginConf.getAppConfigurationEntry(zkLoginContextName) != null;    } catch (Exception e) {        throw new KafkaException("Exception while loading Zookeeper JAAS login context " + zkSecuritySysConfigString(), e);    }    if (foundLoginConfigEntry && !zkSaslEnabled) {                throw new KafkaException("Exception while determining if ZooKeeper is secure " + zkSecuritySysConfigString());    }    return foundLoginConfigEntry;}
public Subject kafkatest_f4604_0()
{    return subject;}
public String kafkatest_f4605_0()
{    return serviceName;}
public String kafkatest_f4614_0()
{    StringBuilder result = new StringBuilder();    result.append(serviceName);    if (hostName != null) {        result.append('/');        result.append(hostName);    }    if (realm != null) {        result.append('@');        result.append(realm);    }    return result.toString();}
public String kafkatest_f4615_0()
{    return serviceName;}
public String kafkatest_f4624_0(KerberosName kerberosName) throws IOException
{    String[] params;    if (kerberosName.hostName() == null) {        // if it is already simple, just return it        if (kerberosName.realm() == null)            return kerberosName.serviceName();        params = new String[] { kerberosName.realm(), kerberosName.serviceName() };    } else {        params = new String[] { kerberosName.realm(), kerberosName.serviceName(), kerberosName.hostName() };    }    for (KerberosRule r : principalToLocalRules) {        String result = r.apply(params);        if (result != null)            return result;    }    throw new NoMatchingRule("No rules apply to " + kerberosName + ", rules " + principalToLocalRules);}
public String kafkatest_f4625_0()
{    return "KerberosShortNamer(principalToLocalRules = " + principalToLocalRules + ")";}
public voidf4634_1)
{        while (true) {        /*                 * Refresh thread's main loop. Each expiring credential lives for one iteration                 * of the loop. Thread will exit if the loop exits from here.                 */        long nowMs = currentMs();        Long nextRefreshMs = refreshMs(nowMs);        if (nextRefreshMs == null) {            loginContextFactory.refresherThreadDone();            return;        }        // should generally never happen except due to a bug        if (nextRefreshMs.longValue() < nowMs) {                        // refresh in 10 seconds            nextRefreshMs = Long.valueOf(nowMs + 10 * 1000);        }                time.sleep(nextRefreshMs - nowMs);        if (Thread.currentThread().isInterrupted()) {                        loginContextFactory.refresherThreadDone();            return;        }        while (true) {            /*                     * Perform a re-login over and over again with some intervening delay                     * unless/until either the refresh succeeds or we are interrupted.                     */            try {                reLogin();                // success                break;            } catch (ExitRefresherThreadDueToIllegalStateException e) {                                loginContextFactory.refresherThreadDone();                return;            } catch (LoginException loginException) {                 will sleep %d seconds before trying again.", principalLogText(), DELAY_SECONDS_BEFORE_NEXT_RETRY_WHEN_RELOGIN_FAILS), loginException);                // Sleep and allow loop to run/try again unless interrupted                time.sleep(DELAY_SECONDS_BEFORE_NEXT_RETRY_WHEN_RELOGIN_FAILS * 1000);                if (Thread.currentThread().isInterrupted()) {                                        loginContextFactory.refresherThreadDone();                    return;                }            }        }    }}
public Subject kafkatest_f4635_0()
{    // field requires volatile keyword    return subject;}
private String kafkatest_f4644_0()
{    return expiringCredential == null ? principalName : expiringCredential.getClass().getSimpleName() + ":" + principalName;}
private long kafkatest_f4645_0()
{    return time.milliseconds();}
public ExpiringCredentialf4654_1)
{    Set<OAuthBearerToken> privateCredentialTokens = expiringCredentialRefreshingLogin.subject().getPrivateCredentials(OAuthBearerToken.class);    if (privateCredentialTokens.isEmpty())        return null;    final OAuthBearerToken token = privateCredentialTokens.iterator().next();    if (log.isDebugEnabled())            return new ExpiringCredential() {        @Override        public String principalName() {            return token.principalName();        }        @Override        public Long startTimeMs() {            return token.startTimeMs();        }        @Override        public long expireTimeMs() {            return token.lifetimeMs();        }        @Override        public Long absoluteLastRefreshTimeMs() {            return null;        }    };}
public String kafkatest_f4655_0()
{    return token.principalName();}
public String kafkatest_f4664_0()
{    return OAuthBearerLoginModule.OAUTHBEARER_MECHANISM;}
public boolean kafkatest_f4665_0()
{    return true;}
public String[] kafkatest_f4675_0(Map<String, ?> props)
{    return OAuthBearerSaslServer.mechanismNamesCompatibleWithPolicy(props);}
public boolean kafkatest_f4676_0()
{    return configured;}
public String kafkatest_f4685_0()
{    if (!complete)        throw new IllegalStateException("Authentication exchange has not completed");    return tokenForNegotiatedProperty.principalName();}
public String kafkatest_f4686_0()
{    return OAuthBearerLoginModule.OAUTHBEARER_MECHANISM;}
private voidf4695_1Exception e) throws SaslException
{    String msg = String.format("%s: %s", INTERNAL_ERROR_ON_SERVER, e.getMessage());        throw new SaslException(msg);}
public static String[] kafkatest_f4696_0(Map<String, ?> props)
{    return props != null && "true".equals(String.valueOf(props.get(Sasl.POLICY_NOPLAINTEXT))) ? new String[] {} : new String[] { OAuthBearerLoginModule.OAUTHBEARER_MECHANISM };}
public Map<String, Object> kafkatest_f4705_0()
{    return header;}
public String kafkatest_f4706_0()
{    return principalName;}
public Object kafkatest_f4715_0(String claimName)
{    return claims().get(Objects.requireNonNull(claimName));}
public Number kafkatest_f4716_0() throws OAuthBearerIllegalTokenException
{    return claim("exp", Number.class);}
 void kafkatest_f4725_0(Time time)
{    this.time = Objects.requireNonNull(time);}
public boolean kafkatest_f4726_0()
{    return configured;}
private String kafkatest_f4735_0(String key)
{    return optionValue(key, null);}
private String kafkatest_f4736_0(String key, String defaultValue)
{    String explicitValue = option(key);    return explicitValue != null ? explicitValue : defaultValue;}
public void kafkatest_f4745_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    if (!OAuthBearerLoginModule.OAUTHBEARER_MECHANISM.equals(saslMechanism))        throw new IllegalArgumentException(String.format("Unexpected SASL mechanism: %s", saslMechanism));    if (Objects.requireNonNull(jaasConfigEntries).size() != 1 || jaasConfigEntries.get(0) == null)        throw new IllegalArgumentException(String.format("Must supply exactly 1 non-null JAAS mechanism configuration (size was %d)", jaasConfigEntries.size()));    final Map<String, String> unmodifiableModuleOptions = Collections.unmodifiableMap((Map<String, String>) jaasConfigEntries.get(0).getOptions());    this.moduleOptions = unmodifiableModuleOptions;    configured = true;}
public void kafkatest_f4746_0(Callback[] callbacks) throws UnsupportedCallbackException
{    if (!configured())        throw new IllegalStateException("Callback handler not configured");    for (Callback callback : callbacks) {        if (callback instanceof OAuthBearerValidatorCallback) {            OAuthBearerValidatorCallback validationCallback = (OAuthBearerValidatorCallback) callback;            try {                handleCallback(validationCallback);            } catch (OAuthBearerIllegalTokenException e) {                OAuthBearerValidationResult failureReason = e.reason();                String failureScope = failureReason.failureScope();                validationCallback.error(failureScope != null ? "insufficient_scope" : "invalid_token", failureScope, failureReason.failureOpenIdConfig());            }        } else if (callback instanceof OAuthBearerExtensionsValidatorCallback) {            OAuthBearerExtensionsValidatorCallback extensionsCallback = (OAuthBearerExtensionsValidatorCallback) callback;            extensionsCallback.inputExtensions().map().forEach((extensionName, v) -> extensionsCallback.valid(extensionName));        } else            throw new UnsupportedCallbackException(callback);    }}
public static OAuthBearerValidationResult kafkatest_f4755_0(String failureDescription)
{    return newFailure(failureDescription, null, null);}
public static OAuthBearerValidationResult kafkatest_f4756_0(String failureDescription, String failureScope, String failureOpenIdConfig)
{    return new OAuthBearerValidationResult(false, failureDescription, failureScope, failureOpenIdConfig);}
public static OAuthBearerValidationResult kafkatest_f4765_0(OAuthBearerUnsecuredJws jwt)
{    Number issuedAt;    Number expirationTime;    try {        issuedAt = Objects.requireNonNull(jwt).issuedAt();        expirationTime = jwt.expirationTime();    } catch (OAuthBearerIllegalTokenException e) {        return e.reason();    }    if (expirationTime != null && issuedAt != null && expirationTime.doubleValue() <= issuedAt.doubleValue())        return OAuthBearerValidationResult.newFailure(String.format("The Expiration Time time (%f seconds) was not after the Issued At time (%f seconds)", expirationTime.doubleValue(), issuedAt.doubleValue()));    return OAuthBearerValidationResult.newSuccess();}
public static OAuthBearerValidationResult kafkatest_f4766_0(OAuthBearerToken token, List<String> requiredScope)
{    final Set<String> tokenScope = token.scope();    if (requiredScope == null || requiredScope.isEmpty())        return OAuthBearerValidationResult.newSuccess();    for (String requiredScopeElement : requiredScope) {        if (!tokenScope.contains(requiredScopeElement))            return OAuthBearerValidationResult.newFailure(String.format("The provided scope (%s) was mising a required scope (%s).  All required scope elements: %s", String.valueOf(tokenScope), requiredScopeElement, requiredScope.toString()), requiredScope.toString(), null);    }    return OAuthBearerValidationResult.newSuccess();}
public void kafkatest_f4775_0(String invalidExtensionName, String errorMessage)
{    if (Objects.requireNonNull(invalidExtensionName).isEmpty())        throw new IllegalArgumentException("extension name must not be empty");    this.invalidExtensions.put(invalidExtensionName, errorMessage);}
public void kafkatest_f4776_0(Subject subject, CallbackHandler callbackHandler, Map<String, ?> sharedState, Map<String, ?> options)
{    this.subject = Objects.requireNonNull(subject);    if (!(Objects.requireNonNull(callbackHandler) instanceof AuthenticateCallbackHandler))        throw new IllegalArgumentException(String.format("Callback handler must be castable to %s: %s", AuthenticateCallbackHandler.class.getName(), callbackHandler.getClass().getName()));    this.callbackHandler = (AuthenticateCallbackHandler) callbackHandler;}
public String kafkatest_f4785_0()
{    return errorCode;}
public String kafkatest_f4786_0()
{    return errorDescription;}
public void kafkatest_f4795_0(OAuthBearerToken token)
{    this.token = Objects.requireNonNull(token);    this.errorStatus = null;    this.errorScope = null;    this.errorOpenIDConfiguration = null;}
public void kafkatest_f4796_0(String errorStatus, String errorScope, String errorOpenIDConfiguration)
{    if (Objects.requireNonNull(errorStatus).isEmpty())        throw new IllegalArgumentException("error status must not be empty");    this.errorStatus = errorStatus;    this.errorScope = errorScope;    this.errorOpenIDConfiguration = errorOpenIDConfiguration;    this.token = null;}
public SaslServer kafkatest_f4806_0(String mechanism, String protocol, String serverName, Map<String, ?> props, CallbackHandler cbh) throws SaslException
{    if (!PLAIN_MECHANISM.equals(mechanism))        throw new SaslException(String.format("Mechanism \'%s\' is not supported. Only PLAIN is supported.", mechanism));    return new PlainSaslServer(cbh);}
public String[] kafkatest_f4807_0(Map<String, ?> props)
{    if (props == null)        return new String[] { PLAIN_MECHANISM };    String noPlainText = (String) props.get(Sasl.POLICY_NOPLAINTEXT);    if ("true".equals(noPlainText))        return new String[] {};    else        return new String[] { PLAIN_MECHANISM };}
public boolean kafkatest_f4817_0()
{    return true;}
public boolean kafkatest_f4818_0()
{    return true;}
public byte[] kafkatest_f4827_0(byte[] str)
{    return messageDigest.digest(str);}
public byte[] kafkatest_f4828_0(byte[] first, byte[] second)
{    if (first.length != second.length)        throw new IllegalArgumentException("Argument arrays must be of the same length");    byte[] result = new byte[first.length];    for (int i = 0; i < result.length; i++) result[i] = (byte) (first[i] ^ second[i]);    return result;}
public byte[] kafkatest_f4837_0(byte[] storedKey, ClientFirstMessage clientFirstMessage, ServerFirstMessage serverFirstMessage, ClientFinalMessage clientFinalMessage) throws InvalidKeyException
{    byte[] authMessage = authMessage(clientFirstMessage, serverFirstMessage, clientFinalMessage);    return hmac(storedKey, authMessage);}
public byte[] kafkatest_f4838_0(byte[] saltedPassword, ClientFirstMessage clientFirstMessage, ServerFirstMessage serverFirstMessage, ClientFinalMessage clientFinalMessage) throws InvalidKeyException
{    byte[] clientKey = clientKey(saltedPassword);    byte[] storedKey = hash(clientKey);    byte[] clientSignature = hmac(storedKey, authMessage(clientFirstMessage, serverFirstMessage, clientFinalMessage));    return xor(clientKey, clientSignature);}
public final String kafkatest_f4847_0()
{    return mechanismName;}
public String kafkatest_f4848_0()
{    return hashAlgorithm;}
public String kafkatest_f4857_0()
{    return nonce;}
public String kafkatest_f4858_0()
{    return authorizationId;}
public byte[] kafkatest_f4867_0()
{    return channelBinding;}
public String kafkatest_f4868_0()
{    return nonce;}
public boolean kafkatest_f4877_0()
{    return true;}
public byte[]f4878_1byte[] challenge) throws SaslException
{    try {        switch(state) {            case SEND_CLIENT_FIRST_MESSAGE:                if (challenge != null && challenge.length != 0)                    throw new SaslException("Expected empty challenge");                clientNonce = formatter.secureRandomString();                NameCallback nameCallback = new NameCallback("Name:");                ScramExtensionsCallback extensionsCallback = new ScramExtensionsCallback();                try {                    callbackHandler.handle(new Callback[] { nameCallback });                    try {                        callbackHandler.handle(new Callback[] { extensionsCallback });                    } catch (UnsupportedCallbackException e) {                                            }                } catch (Throwable e) {                    throw new SaslException("User name or extensions could not be obtained", e);                }                String username = nameCallback.getName();                String saslName = formatter.saslName(username);                Map<String, String> extensions = extensionsCallback.extensions();                this.clientFirstMessage = new ScramMessages.ClientFirstMessage(saslName, clientNonce, extensions);                setState(State.RECEIVE_SERVER_FIRST_MESSAGE);                return clientFirstMessage.toBytes();            case RECEIVE_SERVER_FIRST_MESSAGE:                this.serverFirstMessage = new ServerFirstMessage(challenge);                if (!serverFirstMessage.nonce().startsWith(clientNonce))                    throw new SaslException("Invalid server nonce: does not start with client nonce");                if (serverFirstMessage.iterations() < mechanism.minIterations())                    throw new SaslException("Requested iterations " + serverFirstMessage.iterations() + " is less than the minimum " + mechanism.minIterations() + " for " + mechanism);                PasswordCallback passwordCallback = new PasswordCallback("Password:", false);                try {                    callbackHandler.handle(new Callback[] { passwordCallback });                } catch (Throwable e) {                    throw new SaslException("User name could not be obtained", e);                }                this.clientFinalMessage = handleServerFirstMessage(passwordCallback.getPassword());                setState(State.RECEIVE_SERVER_FINAL_MESSAGE);                return clientFinalMessage.toBytes();            case RECEIVE_SERVER_FINAL_MESSAGE:                ServerFinalMessage serverFinalMessage = new ServerFinalMessage(challenge);                if (serverFinalMessage.error() != null)                    throw new SaslException("Sasl authentication using " + mechanism + " failed with error: " + serverFinalMessage.error());                handleServerFinalMessage(serverFinalMessage.serverSignature());                setState(State.COMPLETE);                return null;            default:                throw new IllegalSaslStateException("Unexpected challenge in Sasl client state " + state);        }    } catch (SaslException e) {        setState(State.FAILED);        throw e;    }}
public String[] kafkatest_f4888_0(Map<String, ?> props)
{    Collection<String> mechanisms = ScramMechanism.mechanismNames();    return mechanisms.toArray(new String[mechanisms.size()]);}
public static void kafkatest_f4889_0()
{    Security.addProvider(new ScramSaslClientProvider());}
private void kafkatest_f4899_0(ClientFinalMessage clientFinalMessage) throws SaslException
{    try {        byte[] expectedStoredKey = scramCredential.storedKey();        byte[] clientSignature = formatter.clientSignature(expectedStoredKey, clientFirstMessage, serverFirstMessage, clientFinalMessage);        byte[] computedStoredKey = formatter.storedKey(clientSignature, clientFinalMessage.proof());        if (!Arrays.equals(computedStoredKey, expectedStoredKey))            throw new SaslException("Invalid client credentials");    } catch (InvalidKeyException e) {        throw new SaslException("Sasl client verification failed", e);    }}
private void kafkatest_f4900_0()
{    scramCredential = null;    clientFirstMessage = null;    serverFirstMessage = null;}
public int kafkatest_f4910_0()
{    return iterations;}
public void kafkatest_f4911_0(ScramCredential scramCredential)
{    this.scramCredential = scramCredential;}
private static SslClientAuthf4921_1String key)
{    SslClientAuth auth = SslClientAuth.forConfig(key);    if (auth != null) {        return auth;    }        return SslClientAuth.NONE;}
private static SecureRandom kafkatest_f4922_0(String key)
{    if (key == null) {        return null;    }    try {        return SecureRandom.getInstance(key);    } catch (GeneralSecurityException e) {        throw new KafkaException(e);    }}
public boolean kafkatest_f4931_0(Map<String, Object> nextConfigs)
{    if (!nextConfigs.equals(configs)) {        return true;    }    if (truststore != null && truststore.modified()) {        return true;    }    if (keystore != null && keystore.modified()) {        return true;    }    return false;}
 KeyStore kafkatest_f4932_0()
{    try (InputStream in = Files.newInputStream(Paths.get(path))) {        KeyStore ks = KeyStore.getInstance(type);        // If a password is not set access to the truststore is still available, but integrity checking is disabled.        char[] passwordChars = password != null ? password.value().toCharArray() : null;        ks.load(in, passwordChars);        return ks;    } catch (GeneralSecurityException | IOException e) {        throw new KafkaException("Failed to load SSL keystore " + path + " of type " + type, e);    }}
public SSLEngine kafkatest_f4941_0(String peerHost, int peerPort)
{    if (sslEngineBuilder == null) {        throw new IllegalStateException("SslFactory has not been configured.");    }    return sslEngineBuilder.createSslEngine(mode, peerHost, peerPort, endpointIdentification);}
public SSLContext kafkatest_f4942_0()
{    return sslEngineBuilder.sslContext();}
private static SSLEngine kafkatest_f4951_0(SslEngineBuilder sslEngineBuilder, Mode mode)
{    // Use empty hostname, disable hostname verification    return sslEngineBuilder.createSslEngine(mode, "", 0, "");}
 static void kafkatest_f4952_0(SSLEngine clientEngine, SSLEngine serverEngine) throws SSLException
{    SslEngineValidator clientValidator = new SslEngineValidator(clientEngine);    SslEngineValidator serverValidator = new SslEngineValidator(serverEngine);    try {        clientValidator.beginHandshake();        serverValidator.beginHandshake();        while (!serverValidator.complete() || !clientValidator.complete()) {            clientValidator.handshake(serverValidator);            serverValidator.handshake(clientValidator);        }    } finally {        clientValidator.close();        serverValidator.close();    }}
public String kafkatest_f4961_0()
{    return "SslPrincipalMapper(rules = " + rules + ")";}
 String kafkatest_f4962_0(String distinguishedName)
{    if (isDefault) {        return distinguishedName;    }    String result = null;    final Matcher m = pattern.matcher(distinguishedName);    if (m.matches()) {        result = distinguishedName.replaceAll(pattern.pattern(), escapeLiteralBackReferences(replacement, m.groupCount()));    }    if (toLowerCase && result != null) {        result = result.toLowerCase(Locale.ENGLISH);    } else if (toUpperCase & result != null) {        result = result.toUpperCase(Locale.ENGLISH);    }    return result;}
public ScramCredential kafkatest_f4971_0(String mechanism, String tokenId)
{    CredentialCache.Cache<ScramCredential> cache = credentialCache.cache(mechanism, ScramCredential.class);    return cache == null ? null : cache.get(tokenId);}
public String kafkatest_f4972_0(String tokenId)
{    TokenInformation tokenInfo = tokenCache.get(tokenId);    return tokenInfo == null ? null : tokenInfo.owner().getName();}
public CredentialCache.Cache<ScramCredential> kafkatest_f4981_0(String mechanism)
{    return credentialCache.cache(mechanism, ScramCredential.class);}
private void kafkatest_f4982_0(String tokenId, Map<String, ScramCredential> scramCredentialMap)
{    for (String mechanism : ScramMechanism.mechanismNames()) {        CredentialCache.Cache<ScramCredential> cache = credentialCache.cache(mechanism, ScramCredential.class);        if (cache != null) {            ScramCredential credential = scramCredentialMap.get(mechanism);            if (credential == null) {                cache.remove(tokenId);            } else {                cache.put(tokenId, credential);            }        }    }}
public long kafkatest_f4991_0()
{    return issueTimestamp;}
public long kafkatest_f4992_0()
{    return expiryTimestamp;}
public byte[] kafkatest_f5001_0(String topic, byte[] data)
{    return data;}
public ByteBuffer kafkatest_f5002_0(String topic, byte[] data)
{    if (data == null)        return null;    return ByteBuffer.wrap(data);}
public T kafkatest_f5011_0(String topic, Headers headers, byte[] data)
{    return deserialize(topic, data);}
public void kafkatest_f5012_0(Map<String, ?> configs, boolean isKey)
{    deserializer.configure(configs, isKey);}
public Float kafkatest_f5021_0(final String topic, final byte[] data)
{    if (data == null)        return null;    if (data.length != 4) {        throw new SerializationException("Size of data received by Deserializer is not 4");    }    int value = 0;    for (byte b : data) {        value <<= 8;        value |= b & 0xFF;    }    return Float.intBitsToFloat(value);}
public byte[] kafkatest_f5022_0(final String topic, final Float data)
{    if (data == null)        return null;    long bits = Float.floatToRawIntBits(data);    return new byte[] { (byte) (bits >>> 24), (byte) (bits >>> 16), (byte) (bits >>> 8), (byte) bits };}
public Serializer<T> kafkatest_f5031_0()
{    return serializer;}
public Deserializer<T> kafkatest_f5032_0()
{    return deserializer;}
public static Serde<ByteBuffer> kafkatest_f5041_0()
{    return new ByteBufferSerde();}
public static Serde<Bytes> kafkatest_f5042_0()
{    return new BytesSerde();}
public String kafkatest_f5051_0(String topic, byte[] data)
{    try {        if (data == null)            return null;        else            return new String(data, encoding);    } catch (UnsupportedEncodingException e) {        throw new SerializationException("Error when deserializing byte[] to string due to unsupported encoding " + encoding);    }}
public void kafkatest_f5052_0(Map<String, ?> configs, boolean isKey)
{    String propertyName = isKey ? "key.serializer.encoding" : "value.serializer.encoding";    Object encodingValue = configs.get(propertyName);    if (encodingValue == null)        encodingValue = configs.get("serializer.encoding");    if (encodingValue instanceof String)        encoding = (String) encodingValue;}
public boolean kafkatest_f5061_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    TopicPartition other = (TopicPartition) obj;    return partition == other.partition && Objects.equals(topic, other.topic);}
public String kafkatest_f5062_0()
{    return topic + "-" + partition;}
public int kafkatest_f5071_0()
{    return partition;}
public int kafkatest_f5072_0()
{    return brokerId;}
private Boolean kafkatest_f5081_0()
{    state = State.FAILED;    next = makeNext();    if (state == State.DONE) {        return false;    } else {        state = State.READY;        return true;    }}
public static String kafkatest_f5082_0()
{    return VERSION;}
public Long kafkatest_f5091_0()
{    return startTimeMs;}
public T kafkatest_f5092_0(MetricConfig config, long now)
{    return value;}
public int kafkatest_f5101_0()
{    return buffer.limit();}
public void kafkatest_f5102_0(int position)
{    ensureRemaining(position - buffer.position());    buffer.position(position);}
public String kafkatest_f5111_0()
{    return Bytes.toString(bytes, 0, bytes.length);}
private static String kafkatest_f5112_0(final byte[] b, int off, int len)
{    StringBuilder result = new StringBuilder();    if (b == null)        return result.toString();    // just in case we are passed a 'len' that is > buffer length...    if (off >= b.length)        return result.toString();    if (off + len > b.length)        len = b.length - off;    for (int i = off; i < off + len; ++i) {        int ch = b[i] & 0xFF;        if (ch >= ' ' && ch <= '~' && ch != '\\') {            result.append((char) ch);        } else {            result.append("\\x");            result.append(HEX_CHARS_UPPER[ch / 0x10]);            result.append(HEX_CHARS_UPPER[ch % 0x10]);        }    }    return result.toString();}
public static void kafkatest_f5121_0(OutputStream out, int value) throws IOException
{    out.write(value);    out.write(value >>> 8);    out.write(value >>> 16);    out.write(value >>> 24);}
public static void kafkatest_f5122_0(byte[] buffer, int offset, int value)
{    buffer[offset] = (byte) value;    buffer[offset + 1] = (byte) (value >>> 8);    buffer[offset + 2] = (byte) (value >>> 16);    buffer[offset + 3] = (byte) (value >>> 24);}
public static void kafkatest_f5131_0(int value, DataOutput out) throws IOException
{    writeUnsignedVarint((value << 1) ^ (value >> 31), out);}
public static void kafkatest_f5132_0(int value, ByteBuffer buffer)
{    writeUnsignedVarint((value << 1) ^ (value >> 31), buffer);}
public static void kafkatest_f5141_0(Checksum checksum, ByteBuffer buffer, int offset, int length)
{    if (buffer.hasArray()) {        checksum.update(buffer.array(), buffer.position() + buffer.arrayOffset() + offset, length);    } else {        int start = buffer.position() + offset;        for (int i = start; i < start + length; i++) checksum.update(buffer.get(i));    }}
public static void kafkatest_f5142_0(Checksum checksum, int input)
{    checksum.update((byte) (input >> 24));    checksum.update((byte) (input >> 16));    checksum.update((byte) (input >> 8));    checksum.update((byte) input);}
public void kafkatest_f5152_0()
{    inner.remove();}
public static Map<K, V> kafkatest_f5153_0(Map<? extends K, ? extends V> minuend, Map<? extends K, ? extends V> subtrahend)
{    return minuend.entrySet().stream().filter(entry -> !subtrahend.containsKey(entry.getKey())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));}
public int kafkatest_f5162_0()
{    return map.size();}
public Collection<V> kafkatest_f5163_0()
{    return map.values();}
public static long kafkatest_f5172_0(byte[] bytes)
{    return crc32(bytes, 0, bytes.length);}
public static long kafkatest_f5173_0(byte[] bytes, int offset, int size)
{    Crc32 crc = new Crc32();    crc.update(bytes, offset, size);    return crc.getValue();}
public Checksum kafkatest_f5182_0()
{    try {        return (Checksum) CONSTRUCTOR.invoke();    } catch (Throwable throwable) {        // Should never happen        throw new RuntimeException(throwable);    }}
public Checksum kafkatest_f5183_0()
{    return new PureJavaCrc32C();}
public static void kafkatest_f5192_0()
{    exitProcedure = DEFAULT_EXIT_PROCEDURE;}
public static void kafkatest_f5193_0()
{    haltProcedure = DEFAULT_HALT_PROCEDURE;}
public void kafkatest_f5202_0(int next)
{    this.next = next;}
private static Element kafkatest_f5203_0(Element head, Element[] elements, int index)
{    if (index == HEAD_INDEX) {        return head;    }    return elements[index];}
public void kafkatest_f5212_0()
{    if (lastReturnedSlot == INVALID_INDEX) {        throw new IllegalStateException();    }    if (cur == indexToElement(head, elements, lastReturnedSlot)) {        cursor--;        cur = indexToElement(head, elements, cur.prev());    }    ImplicitLinkedHashCollection.this.removeElementAtSlot(lastReturnedSlot);    lastReturnedSlot = INVALID_INDEX;}
public void kafkatest_f5213_0(E e)
{    throw new UnsupportedOperationException();}
public void kafkatest_f5222_0()
{    ImplicitLinkedHashCollection.this.clear();}
public final Iterator<E> kafkatest_f5223_0()
{    return listIterator(0);}
public final void kafkatest_f5232_0(E newElement)
{    if (!add(newElement)) {        throw new RuntimeException("Unable to add " + newElement);    }}
 int kafkatest_f5233_0(Element newElement, Element[] addElements)
{    int slot = slot(addElements, newElement);    for (int seen = 0; seen < addElements.length; seen++) {        Element element = addElements[slot];        if (element == null) {            addElements[slot] = newElement;            return slot;        }        if (element.equals(newElement)) {            return INVALID_INDEX;        }        slot = (slot + 1) % addElements.length;    }    throw new RuntimeException("Not enough hash table slots to add a new element.");}
public int kafkatest_f5242_0()
{    return this.valuesList().hashCode();}
 final int kafkatest_f5243_0()
{    return elements.length;}
 boolean kafkatest_f5252_0()
{    return majorVersion >= 9;}
public static KafkaThread kafkatest_f5253_0(final String name, Runnable runnable)
{    return new KafkaThread(name, runnable, true);}
public boolean kafkatest_f5262_0(Marker marker)
{    return logger.isTraceEnabled(marker);}
public boolean kafkatest_f5263_0()
{    return logger.isDebugEnabled();}
public void kafkatest_f5272_0(String format, Object arg)
{    if (logger.isTraceEnabled()) {        writeLog(null, LocationAwareLogger.TRACE_INT, format, new Object[] { arg }, null);    }}
public void kafkatest_f5273_0(String format, Object arg1, Object arg2)
{    if (logger.isTraceEnabled()) {        writeLog(null, LocationAwareLogger.TRACE_INT, format, new Object[] { arg1, arg2 }, null);    }}
public voidf5282_1String format, Object arg)
{    if (logger.isDebugEnabled()) {        writeLog(null, LocationAware    }}
public voidf5283_1String format, Object arg1, Object arg2)
{    if (logger.isDebugEnabled()) {        writeLog(null, LocationAware    }}
public voidf5292_1String format, Object arg)
{    writeLog(null, LocationAware}
public voidf5293_1String message, Object arg1, Object arg2)
{    writeLog(null, LocationAware}
public voidf5302_1String format, Object arg)
{    writeLog(null, LocationAware}
public voidf5303_1String format, Object arg1, Object arg2)
{    writeLog(null, LocationAware}
public voidf5312_1String format, Object arg)
{    writeLog(null, LocationAware}
public voidf5313_1String format, Object arg1, Object arg2)
{    writeLog(null, LocationAware}
public String kafkatest_f5322_0()
{    return logger.getName();}
public boolean kafkatest_f5323_0()
{    return logger.isTraceEnabled();}
public boolean kafkatest_f5332_0(Marker marker)
{    return logger.isErrorEnabled(marker);}
public void kafkatest_f5333_0(String message)
{    if (logger.isTraceEnabled()) {        logger.trace(addPrefix(message));    }}
public void kafkatest_f5342_0(Marker marker, String msg, Throwable t)
{    if (logger.isTraceEnabled()) {        logger.trace(marker, addPrefix(msg), t);    }}
public voidf5343_1String message)
{    if (logger.isDebugEnabled()) {            }}
public voidf5352_1Marker marker, String msg, Throwable t)
{    if (logger.isDebugEnabled()) {            }}
public voidf5353_1String message)
{    }
public voidf5362_1Marker marker, String msg, Throwable t)
{    }
public voidf5363_1String message)
{    }
public voidf5372_1Marker marker, String msg, Throwable t)
{    }
public voidf5373_1String message)
{    }
public voidf5382_1Marker marker, String msg, Throwable t)
{    }
public voidf5383_1) throws ReflectiveOperationException
{    Map<String, Object> jvmSignalHandlers = new ConcurrentHashMap<>();    for (String signal : SIGNALS) {        register(signal, jvmSignalHandlers);    }    }
private static MethodHandle kafkatest_f5392_0(MethodHandles.Lookup lookup) throws ReflectiveOperationException
{    Class<?> unsafeClass = Class.forName("sun.misc.Unsafe");    MethodHandle unmapper = lookup.findVirtual(unsafeClass, "invokeCleaner", methodType(void.class, ByteBuffer.class));    Field f = unsafeClass.getDeclaredField("theUnsafe");    f.setAccessible(true);    Object theUnsafe = f.get(null);    return unmapper.bindTo(theUnsafe);}
private static boolean kafkatest_f5393_0(Object o)
{    return o != null;}
public static KafkaPrincipal kafkatest_f5402_0(String str)
{    if (str == null || str.isEmpty()) {        throw new IllegalArgumentException("expected a string in format principalType:principalName but got " + str);    }    String[] split = str.split(":", 2);    if (split.length != 2) {        throw new IllegalArgumentException("expected a string in format principalType:principalName but got " + str);    }    return new KafkaPrincipal(split[0], split[1]);}
public static voidf5403_1Map<String, ?> configs)
{    String securityProviderClassesStr = (String) configs.get(SecurityConfig.SECURITY_PROVIDERS_CONFIG);    if (securityProviderClassesStr == null || securityProviderClassesStr.equals("")) {        return;    }    try {        String[] securityProviderClasses = securityProviderClassesStr.replaceAll("\\s+", "").split(",");        for (int index = 0; index < securityProviderClasses.length; index++) {            SecurityProviderCreator securityProviderCreator = (SecurityProviderCreator) Class.forName(securityProviderClasses[index]).newInstance();            securityProviderCreator.configure(configs);            Security.insertProviderAt(securityProviderCreator.getProvider(), index + 1);        }    } catch (ClassCastException e) {            } catch (ClassNotFoundException cnfe) {            } catch (IllegalAccessException | InstantiationException e) {            }}
public int kafkatest_f5412_0()
{    return exitCode;}
public void kafkatest_f5413_0() throws IOException
{    this.run();}
public Future<T> kafkatest_f5422_0(final ScheduledExecutorService executor, final Callable<T> callable, long delayMs)
{    return executor.schedule(callable, delayMs, TimeUnit.MILLISECONDS);}
public long kafkatest_f5423_0()
{    return System.currentTimeMillis();}
public void kafkatest_f5432_0(long timeoutMs)
{    update();    reset(timeoutMs);}
public void kafkatest_f5433_0(long timeoutMs)
{    if (timeoutMs < 0)        throw new IllegalArgumentException("Invalid negative timeout " + timeoutMs);    this.startMs = this.currentTimeMs;    if (currentTimeMs > Long.MAX_VALUE - timeoutMs)        this.deadlineMs = Long.MAX_VALUE;    else        this.deadlineMs = currentTimeMs + timeoutMs;}
public static String kafkatest_f5442_0(ByteBuffer buffer, int length)
{    return utf8(buffer, 0, length);}
public static String kafkatest_f5443_0(ByteBuffer buffer)
{    return utf8(buffer, buffer.remaining());}
public static byte[] kafkatest_f5452_0(ByteBuffer buffer, int size)
{    return toArray(buffer, 0, size);}
public static byte[] kafkatest_f5453_0(ByteBuffer buffer)
{    return buffer == null ? null : toArray(buffer);}
public static int kafkatest_f5462_0(final byte[] data)
{    int length = data.length;    int seed = 0x9747b28c;    // 'm' and 'r' are mixing constants generated offline.    // They're not really 'magic', they just happen to work well.    final int m = 0x5bd1e995;    final int r = 24;    // Initialize the hash to a random value    int h = seed ^ length;    int length4 = length / 4;    for (int i = 0; i < length4; i++) {        final int i4 = i * 4;        int k = (data[i4 + 0] & 0xff) + ((data[i4 + 1] & 0xff) << 8) + ((data[i4 + 2] & 0xff) << 16) + ((data[i4 + 3] & 0xff) << 24);        k *= m;        k ^= k >>> r;        k *= m;        h *= m;        h ^= k;    }    // Handle the last few bytes of the input array    switch(length % 4) {        case 3:            h ^= (data[(length & ~3) + 2] & 0xff) << 16;        case 2:            h ^= (data[(length & ~3) + 1] & 0xff) << 8;        case 1:            h ^= data[length & ~3] & 0xff;            h *= m;    }    h ^= h >>> 13;    h *= m;    h ^= h >>> 15;    return h;}
public static String kafkatest_f5463_0(String address)
{    Matcher matcher = HOST_PORT_PATTERN.matcher(address);    return matcher.matches() ? matcher.group(1) : null;}
public static Properties kafkatest_f5472_0(String filename) throws IOException
{    Properties props = new Properties();    if (filename != null) {        try (InputStream propStream = Files.newInputStream(Paths.get(filename))) {            props.load(propStream);        }    } else {        System.out.println("Did not load any properties since the property file is not specified");    }    return props;}
public static Map<String, String> kafkatest_f5473_0(Properties props)
{    Map<String, String> result = new HashMap<>();    for (Map.Entry<Object, Object> entry : props.entrySet()) result.put(entry.getKey().toString(), entry.getValue().toString());    return result;}
public K kafkatest_f5482_0()
{    return k;}
public V kafkatest_f5483_0()
{    return v;}
public static ClassLoader kafkatest_f5492_0()
{    return Utils.class.getClassLoader();}
public static ClassLoader kafkatest_f5493_0()
{    ClassLoader cl = Thread.currentThread().getContextClassLoader();    if (cl == null)        return getKafkaClassLoader();    else        return cl;}
public static final void kafkatest_f5502_0(InputStream inputStream, ByteBuffer destinationBuffer) throws IOException
{    if (!destinationBuffer.hasArray())        throw new IllegalArgumentException("destinationBuffer must be backed by an array");    int initialOffset = destinationBuffer.arrayOffset() + destinationBuffer.position();    byte[] array = destinationBuffer.array();    int length = destinationBuffer.remaining();    int totalBytesRead = 0;    do {        int bytesRead = inputStream.read(array, initialOffset + totalBytesRead, length - totalBytesRead);        if (bytesRead == -1)            break;        totalBytesRead += bytesRead;    } while (length > totalBytesRead);    destinationBuffer.position(destinationBuffer.position() + totalBytesRead);}
public static void kafkatest_f5503_0(FileChannel channel, ByteBuffer sourceBuffer) throws IOException
{    while (sourceBuffer.hasRemaining()) channel.write(sourceBuffer);}
public Optional<ApiException> kafkatest_f5512_0()
{    return exception == null ? Optional.empty() : Optional.of(exception);}
public Optional<ApiException> kafkatest_f5513_0()
{    return exception == null ? Optional.empty() : Optional.of(exception);}
public boolean kafkatest_f5522_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof Action)) {        return false;    }    Action that = (Action) o;    return Objects.equals(this.resourcePattern, that.resourcePattern) && Objects.equals(this.operation, that.operation) && this.resourceReferenceCount == that.resourceReferenceCount && this.logIfAllowed == that.logIfAllowed && this.logIfDenied == that.logIfDenied;}
public int kafkatest_f5523_0()
{    return Objects.hash(resourcePattern, operation, resourceReferenceCount, logIfAllowed, logIfDenied);}
public Map<String, String> kafkatest_f5532_0()
{    return configs;}
public String kafkatest_f5533_0()
{    return "CreateTopicPolicy.RequestMetadata(topic=" + topic + ", numPartitions=" + numPartitions + ", replicationFactor=" + replicationFactor + ", replicasAssignments=" + replicasAssignments + ", configs=" + configs + ")";}
 static Map<String, Object> kafkatest_f5542_0(String... overrides)
{    Map<String, Object> map = new HashMap<>();    map.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:8121");    map.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, "1000");    if (overrides.length % 2 != 0) {        throw new IllegalStateException();    }    for (int i = 0; i < overrides.length; i += 2) {        map.put(overrides[i], overrides[i + 1]);    }    return map;}
public static String kafkatest_f5543_0()
{    return KafkaAdminClient.NETWORK_THREAD_PREFIX;}
public void kafkatest_f5552_0()
{    mgr.transitionToUpdatePending(time.milliseconds());    assertEquals(Long.MAX_VALUE, mgr.metadataFetchDelayMs(time.milliseconds()));    mgr.updateFailed(new RuntimeException());    assertEquals(refreshBackoffMs, mgr.metadataFetchDelayMs(time.milliseconds()));    // Even if we explicitly request an update, the backoff should be respected    mgr.requestUpdate();    assertEquals(refreshBackoffMs, mgr.metadataFetchDelayMs(time.milliseconds()));    time.sleep(refreshBackoffMs);    assertEquals(0, mgr.metadataFetchDelayMs(time.milliseconds()));}
public void kafkatest_f5553_0()
{    mgr.transitionToUpdatePending(time.milliseconds());    mgr.updateFailed(new AuthenticationException("Authentication failed"));    assertEquals(refreshBackoffMs, mgr.metadataFetchDelayMs(time.milliseconds()));    try {        mgr.isReady();        fail("Expected AuthenticationException to be thrown");    } catch (AuthenticationException e) {    // Expected    }    mgr.update(mockCluster(), time.milliseconds());    assertTrue(mgr.isReady());}
private static Cluster kafkatest_f5562_0()
{    return Cluster.bootstrap(ClientUtils.parseAndValidateAddresses(singletonList("localhost:8121"), ClientDnsLookup.DEFAULT));}
private static AdminClientUnitTestEnv kafkatest_f5563_0(String... configVals)
{    return new AdminClientUnitTestEnv(mockCluster(0), configVals);}
public void kafkatest_f5572_0() throws Exception
{    // This tests the scenario in which the bootstrap server is unreachable for a short while,    // which prevents AdminClient from being able to send the initial metadata request    Cluster cluster = Cluster.bootstrap(singletonList(new InetSocketAddress("localhost", 8121)));    Map<Node, Long> unreachableNodes = Collections.singletonMap(cluster.nodes().get(0), 200L);    try (final AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(Time.SYSTEM, cluster, AdminClientUnitTestEnv.clientConfigs(), unreachableNodes)) {        Cluster discoveredCluster = mockCluster(0);        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(body -> body instanceof MetadataRequest, MetadataResponse.prepareResponse(discoveredCluster.nodes(), discoveredCluster.clusterResource().clusterId(), 1, Collections.emptyList()));        env.kafkaClient().prepareResponse(body -> body instanceof CreateTopicsRequest, prepareCreateTopicsResponse("myTopic", Errors.NONE));        KafkaFuture<Void> future = env.adminClient().createTopics(Collections.singleton(new NewTopic("myTopic", Collections.singletonMap(0, asList(0, 1, 2)))), new CreateTopicsOptions().timeoutMs(10000)).all();        future.get();    }}
public void kafkatest_f5573_0() throws Exception
{    Cluster cluster = mockCluster(0);    try (final AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(Time.SYSTEM, cluster, newStrMap(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:8121", AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, "10"))) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().createPendingAuthenticationError(cluster.nodeById(0), TimeUnit.DAYS.toMillis(1));        env.kafkaClient().prepareResponse(prepareCreateTopicsResponse("myTopic", Errors.NONE));        KafkaFuture<Void> future = env.adminClient().createTopics(Collections.singleton(new NewTopic("myTopic", Collections.singletonMap(0, asList(0, 1, 2)))), new CreateTopicsOptions().timeoutMs(1000)).all();        TestUtils.assertFutureError(future, SaslAuthenticationException.class);    }}
public void kafkatest_f5582_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Test a call where we get back ACL1 and ACL2.        env.kafkaClient().prepareResponse(new DescribeAclsResponse(0, ApiError.NONE, asList(ACL1, ACL2)));        assertCollectionIs(env.adminClient().describeAcls(FILTER1).values().get(), ACL1, ACL2);        // Test a call where we get back no results.        env.kafkaClient().prepareResponse(new DescribeAclsResponse(0, ApiError.NONE, Collections.<AclBinding>emptySet()));        assertTrue(env.adminClient().describeAcls(FILTER2).values().get().isEmpty());        // Test a call where we get back an error.        env.kafkaClient().prepareResponse(new DescribeAclsResponse(0, new ApiError(Errors.SECURITY_DISABLED, "Security is disabled"), Collections.<AclBinding>emptySet()));        TestUtils.assertFutureError(env.adminClient().describeAcls(FILTER2).values(), SecurityDisabledException.class);        // Test a call where we supply an invalid filter.        TestUtils.assertFutureError(env.adminClient().describeAcls(UNKNOWN_FILTER).values(), InvalidRequestException.class);    }}
public void kafkatest_f5583_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Test a call where we successfully create two ACLs.        env.kafkaClient().prepareResponse(new CreateAclsResponse(0, asList(new AclCreationResponse(ApiError.NONE), new AclCreationResponse(ApiError.NONE))));        CreateAclsResult results = env.adminClient().createAcls(asList(ACL1, ACL2));        assertCollectionIs(results.values().keySet(), ACL1, ACL2);        for (KafkaFuture<Void> future : results.values().values()) future.get();        results.all().get();        // Test a call where we fail to create one ACL.        env.kafkaClient().prepareResponse(new CreateAclsResponse(0, asList(new AclCreationResponse(new ApiError(Errors.SECURITY_DISABLED, "Security is disabled")), new AclCreationResponse(ApiError.NONE))));        results = env.adminClient().createAcls(asList(ACL1, ACL2));        assertCollectionIs(results.values().keySet(), ACL1, ACL2);        TestUtils.assertFutureError(results.values().get(ACL1), SecurityDisabledException.class);        results.values().get(ACL2).get();        TestUtils.assertFutureError(results.all(), SecurityDisabledException.class);    }}
public void kafkatest_f5592_0() throws Exception
{    final HashMap<Integer, Node> nodes = new HashMap<>();    Node node0 = new Node(0, "localhost", 8121);    Node node1 = new Node(1, "localhost", 8122);    Node node2 = new Node(2, "localhost", 8123);    Node node3 = new Node(3, "localhost", 8124);    nodes.put(0, node0);    nodes.put(1, node1);    nodes.put(2, node2);    nodes.put(3, node3);    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.emptyList(), Collections.emptySet(), Collections.emptySet(), nodes.get(0));    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster, AdminClientConfig.RETRIES_CONFIG, "2")) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Prepare the metadata response used for the first describe cluster        MetadataResponse response = MetadataResponse.prepareResponse(0, new ArrayList<>(nodes.values()), env.cluster().clusterResource().clusterId(), 2, Collections.emptyList(), MetadataResponse.AUTHORIZED_OPERATIONS_OMITTED);        env.kafkaClient().prepareResponse(response);        // Prepare the metadata response used for the second describe cluster        MetadataResponse response2 = MetadataResponse.prepareResponse(0, new ArrayList<>(nodes.values()), env.cluster().clusterResource().clusterId(), 3, Collections.emptyList(), 1 << AclOperation.DESCRIBE.code() | 1 << AclOperation.ALTER.code());        env.kafkaClient().prepareResponse(response2);        // Test DescribeCluster with the authorized operations omitted.        final DescribeClusterResult result = env.adminClient().describeCluster();        assertEquals(env.cluster().clusterResource().clusterId(), result.clusterId().get());        assertEquals(2, result.controller().get().id());        assertEquals(null, result.authorizedOperations().get());        // Test DescribeCluster with the authorized operations included.        final DescribeClusterResult result2 = env.adminClient().describeCluster();        assertEquals(env.cluster().clusterResource().clusterId(), result2.clusterId().get());        assertEquals(3, result2.controller().get().id());        assertEquals(new HashSet<>(Arrays.asList(AclOperation.DESCRIBE, AclOperation.ALTER)), result2.authorizedOperations().get());    }}
public void kafkatest_f5593_0() throws Exception
{    final HashMap<Integer, Node> nodes = new HashMap<>();    Node node0 = new Node(0, "localhost", 8121);    Node node1 = new Node(1, "localhost", 8122);    Node node2 = new Node(2, "localhost", 8123);    Node node3 = new Node(3, "localhost", 8124);    nodes.put(0, node0);    nodes.put(1, node1);    nodes.put(2, node2);    nodes.put(3, node3);    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.emptyList(), Collections.emptySet(), Collections.emptySet(), nodes.get(0));    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster, AdminClientConfig.RETRIES_CONFIG, "2")) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Empty metadata response should be retried        env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(Collections.emptyList(), env.cluster().clusterResource().clusterId(), -1, Collections.emptyList()));        env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(), env.cluster().clusterResource().clusterId(), env.cluster().controller().id(), Collections.emptyList()));        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.NONE.code()).setGroups(Arrays.asList(new ListGroupsResponseData.ListedGroup().setGroupId("group-1").setProtocolType(ConsumerProtocol.PROTOCOL_TYPE), new ListGroupsResponseData.ListedGroup().setGroupId("group-connect-1").setProtocolType("connector")))), node0);        // handle retriable errors        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.COORDINATOR_NOT_AVAILABLE.code()).setGroups(Collections.emptyList())), node1);        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.COORDINATOR_LOAD_IN_PROGRESS.code()).setGroups(Collections.emptyList())), node1);        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.NONE.code()).setGroups(Arrays.asList(new ListGroupsResponseData.ListedGroup().setGroupId("group-2").setProtocolType(ConsumerProtocol.PROTOCOL_TYPE), new ListGroupsResponseData.ListedGroup().setGroupId("group-connect-2").setProtocolType("connector")))), node1);        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.NONE.code()).setGroups(Arrays.asList(new ListGroupsResponseData.ListedGroup().setGroupId("group-3").setProtocolType(ConsumerProtocol.PROTOCOL_TYPE), new ListGroupsResponseData.ListedGroup().setGroupId("group-connect-3").setProtocolType("connector")))), node2);        // fatal error        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.UNKNOWN_SERVER_ERROR.code()).setGroups(Collections.emptyList())), node3);        final ListConsumerGroupsResult result = env.adminClient().listConsumerGroups();        TestUtils.assertFutureError(result.all(), UnknownServerException.class);        Collection<ConsumerGroupListing> listings = result.valid().get();        assertEquals(3, listings.size());        Set<String> groupIds = new HashSet<>();        for (ConsumerGroupListing listing : listings) {            groupIds.add(listing.groupId());        }        assertEquals(Utils.mkSet("group-1", "group-2", "group-3"), groupIds);        assertEquals(1, result.errors().get().size());    }}
public void kafkatest_f5602_0() throws Exception
{    // Non-retriable errors throw an exception    final Map<Integer, Node> nodes = new HashMap<>();    nodes.put(0, new Node(0, "localhost", 8121));    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptyList(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));    final String groupId = "group-0";    final TopicPartition tp1 = new TopicPartition("foo", 0);    final List<Errors> retriableErrors = Arrays.asList(Errors.GROUP_AUTHORIZATION_FAILED, Errors.INVALID_GROUP_ID, Errors.GROUP_ID_NOT_FOUND);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        for (Errors error : retriableErrors) {            env.kafkaClient().prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, env.cluster().controller()));            env.kafkaClient().prepareResponse(prepareOffsetDeleteResponse(error));            DeleteConsumerGroupOffsetsResult errorResult = env.adminClient().deleteConsumerGroupOffsets(groupId, Stream.of(tp1).collect(Collectors.toSet()));            TestUtils.assertFutureError(errorResult.all(), error.exception().getClass());            TestUtils.assertFutureError(errorResult.partitionResult(tp1), error.exception().getClass());        }    }}
public void kafkatest_f5603_0() throws Exception
{    // Retriable FindCoordinatorResponse errors should be retried    final Map<Integer, Node> nodes = new HashMap<>();    nodes.put(0, new Node(0, "localhost", 8121));    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptyList(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));    final String groupId = "group-0";    final TopicPartition tp1 = new TopicPartition("foo", 0);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.COORDINATOR_NOT_AVAILABLE, Node.noNode()));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.COORDINATOR_LOAD_IN_PROGRESS, Node.noNode()));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.NONE, env.cluster().controller()));        env.kafkaClient().prepareResponse(prepareOffsetDeleteResponse("foo", 0, Errors.NONE));        final DeleteConsumerGroupOffsetsResult result = env.adminClient().deleteConsumerGroupOffsets(groupId, Stream.of(tp1).collect(Collectors.toSet()));        assertNull(result.all().get());        assertNull(result.partitionResult(tp1).get());    }}
public KafkaAdminClient.TimeoutProcessor kafkatest_f5612_0(long now)
{    return new FailureInjectingTimeoutProcessor(now);}
 synchronized boolean kafkatest_f5613_0()
{    numTries++;    if (numTries == 1) {        failuresInjected++;        return true;    }    return false;}
public void kafkatest_f5622_0(final String name)
{    if (!allTopics.containsKey(name)) {        throw new IllegalArgumentException(String.format("Topic %s did not exist.", name));    }    allTopics.get(name).markedForDeletion = true;}
public void kafkatest_f5623_0(int numberOfRequest)
{    timeoutNextRequests = numberOfRequest;}
public RenewDelegationTokenResult kafkatest_f5632_0(byte[] hmac, RenewDelegationTokenOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
public ExpireDelegationTokenResult kafkatest_f5633_0(byte[] hmac, ExpireDelegationTokenOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
public MembershipChangeResult kafkatest_f5642_0(String groupId, RemoveMemberFromConsumerGroupOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
public CreateAclsResult kafkatest_f5643_0(Collection<AclBinding> acls, CreateAclsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
public AlterPartitionReassignmentsResult kafkatest_f5652_0(Map<TopicPartition, Optional<NewPartitionReassignment>> reassignments, AlterPartitionReassignmentsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
public ListPartitionReassignmentsResult kafkatest_f5653_0(Optional<Set<TopicPartition>> partitions, ListPartitionReassignmentsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
public void kafkatest_f5663_0()
{    ApiVersions apiVersions = new ApiVersions();    assertEquals(RecordBatch.CURRENT_MAGIC_VALUE, apiVersions.maxUsableProduceMagic());    apiVersions.update("0", NodeApiVersions.create());    assertEquals(RecordBatch.CURRENT_MAGIC_VALUE, apiVersions.maxUsableProduceMagic());    apiVersions.update("1", NodeApiVersions.create(Collections.singleton(new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE, (short) 0, (short) 2))));    assertEquals(RecordBatch.MAGIC_VALUE_V1, apiVersions.maxUsableProduceMagic());    apiVersions.remove("1");    assertEquals(RecordBatch.CURRENT_MAGIC_VALUE, apiVersions.maxUsableProduceMagic());}
public void kafkatest_f5664_0() throws UnknownHostException
{    checkWithoutLookup("127.0.0.1:8000");    checkWithoutLookup("localhost:8080");    checkWithoutLookup("[::1]:8000");    checkWithoutLookup("[2001:db8:85a3:8d3:1319:8a2e:370:7348]:1234", "localhost:10000");    List<InetSocketAddress> validatedAddresses = checkWithoutLookup("localhost:10000");    assertEquals(1, validatedAddresses.size());    InetSocketAddress onlyAddress = validatedAddresses.get(0);    assertEquals("localhost", onlyAddress.getHostName());    assertEquals(10000, onlyAddress.getPort());}
private List<InetSocketAddress> kafkatest_f5673_0(String... url)
{    return ClientUtils.parseAndValidateAddresses(Arrays.asList(url), ClientDnsLookup.DEFAULT);}
private List<InetSocketAddress> kafkatest_f5674_0(List<String> url)
{    return ClientUtils.parseAndValidateAddresses(url, ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY);}
public void kafkatest_f5683_0() throws UnknownHostException
{    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.DEFAULT);    InetAddress currAddress = connectionStates.currentAddress(nodeId1);    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.DEFAULT);    assertSame(currAddress, connectionStates.currentAddress(nodeId1));}
public void kafkatest_f5684_0() throws UnknownHostException
{    assertEquals(1, ClientUtils.resolve("localhost", ClientDnsLookup.USE_ALL_DNS_IPS).size());    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.USE_ALL_DNS_IPS);    InetAddress currAddress = connectionStates.currentAddress(nodeId1);    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.USE_ALL_DNS_IPS);    assertSame(currAddress, connectionStates.currentAddress(nodeId1));}
public void kafkatest_f5693_0()
{    Map<String, Object> configs = new HashMap<>();    configs.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializerClass);    configs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializerClass);    Map<String, Object> newConfigs = ConsumerConfig.addDeserializerToConfig(configs, null, null);    assertEquals(newConfigs.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG), keyDeserializerClass);    assertEquals(newConfigs.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG), valueDeserializerClass);    configs.clear();    configs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializerClass);    newConfigs = ConsumerConfig.addDeserializerToConfig(configs, keyDeserializer, null);    assertEquals(newConfigs.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG), keyDeserializerClass);    assertEquals(newConfigs.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG), valueDeserializerClass);    configs.clear();    configs.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializerClass);    newConfigs = ConsumerConfig.addDeserializerToConfig(configs, null, valueDeserializer);    assertEquals(newConfigs.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG), keyDeserializerClass);    assertEquals(newConfigs.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG), valueDeserializerClass);    configs.clear();    newConfigs = ConsumerConfig.addDeserializerToConfig(configs, keyDeserializer, valueDeserializer);    assertEquals(newConfigs.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG), keyDeserializerClass);    assertEquals(newConfigs.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG), valueDeserializerClass);}
public void kafkatest_f5694_0() throws Exception
{    Map<TopicPartition, List<ConsumerRecord<Integer, String>>> records = new LinkedHashMap<>();    String topic = "topic";    records.put(new TopicPartition(topic, 0), new ArrayList<ConsumerRecord<Integer, String>>());    ConsumerRecord<Integer, String> record1 = new ConsumerRecord<>(topic, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, 1, "value1");    ConsumerRecord<Integer, String> record2 = new ConsumerRecord<>(topic, 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, 2, "value2");    records.put(new TopicPartition(topic, 1), Arrays.asList(record1, record2));    records.put(new TopicPartition(topic, 2), new ArrayList<ConsumerRecord<Integer, String>>());    ConsumerRecords<Integer, String> consumerRecords = new ConsumerRecords<>(records);    Iterator<ConsumerRecord<Integer, String>> iter = consumerRecords.iterator();    int c = 0;    for (; iter.hasNext(); c++) {        ConsumerRecord<Integer, String> record = iter.next();        assertEquals(1, record.partition());        assertEquals(topic, record.topic());        assertEquals(c, record.offset());    }    assertEquals(2, c);}
private void kafkatest_f5703_0(int retryBackoffMs, int rebalanceTimeoutMs, Optional<String> groupInstanceId)
{    LogContext logContext = new LogContext();    this.mockTime = new MockTime();    ConsumerMetadata metadata = new ConsumerMetadata(retryBackoffMs, 60 * 60 * 1000L, false, false, new SubscriptionState(logContext, OffsetResetStrategy.EARLIEST), logContext, new ClusterResourceListeners());    this.mockClient = new MockClient(mockTime, metadata);    this.consumerClient = new ConsumerNetworkClient(logContext, mockClient, metadata, mockTime, retryBackoffMs, REQUEST_TIMEOUT_MS, HEARTBEAT_INTERVAL_MS);    Metrics metrics = new Metrics();    mockClient.updateMetadata(TestUtils.metadataUpdateWith(1, emptyMap()));    this.node = metadata.fetch().nodes().get(0);    this.coordinatorNode = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    GroupRebalanceConfig rebalanceConfig = new GroupRebalanceConfig(SESSION_TIMEOUT_MS, rebalanceTimeoutMs, HEARTBEAT_INTERVAL_MS, GROUP_ID, groupInstanceId, retryBackoffMs, !groupInstanceId.isPresent());    this.coordinator = new DummyCoordinator(rebalanceConfig, consumerClient, metrics, mockTime);}
public void kafkatest_f5704_0()
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    // blackout the coordinator for 10 milliseconds to simulate a disconnect.    // after backing off, we should be able to connect.    mockClient.blackout(coordinatorNode, 10L);    long initialTime = mockTime.milliseconds();    coordinator.ensureCoordinatorReady(mockTime.timer(Long.MAX_VALUE));    long endTime = mockTime.milliseconds();    assertTrue(endTime - initialTime >= RETRY_BACKOFF_MS);}
public void kafkatest_f5713_0()
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(mockTime.timer(0));    mockClient.prepareResponse(joinGroupFollowerResponse(defaultGeneration, memberId, JoinGroupResponse.UNKNOWN_MEMBER_ID, Errors.UNKNOWN_MEMBER_ID));    RequestFuture<ByteBuffer> future = coordinator.sendJoinGroupRequest();    assertTrue(consumerClient.poll(future, mockTime.timer(REQUEST_TIMEOUT_MS)));    assertEquals(Errors.UNKNOWN_MEMBER_ID.message(), future.exception().getMessage());    assertTrue(coordinator.rejoinNeededOrPending());    assertTrue(coordinator.hasMatchingGenerationId(defaultGeneration));}
public void kafkatest_f5714_0()
{    checkLeaveGroupRequestSent(Optional.empty());    checkLeaveGroupRequestSent(Optional.of("groupInstanceId"));}
public void kafkatest_f5723_0() throws Exception
{    final int longRetryBackoffMs = 10000;    setupCoordinator(longRetryBackoffMs);    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(syncGroupResponse(Errors.NONE));    coordinator.ensureActiveGroup();    final CountDownLatch heartbeatDone = new CountDownLatch(1);    mockClient.prepareResponse(body -> {        heartbeatDone.countDown();        return body instanceof HeartbeatRequest;    }, heartbeatResponse(Errors.NONE));    mockTime.sleep(HEARTBEAT_INTERVAL_MS);    coordinator.pollHeartbeat(mockTime.milliseconds());    if (!heartbeatDone.await(1, TimeUnit.SECONDS)) {        fail("Should have received a heartbeat request after calling pollHeartbeat");    }}
public void kafkatest_f5724_0()
{    setupCoordinator();    mockClient.blackout(node, 50);    RequestFuture<Void> noBrokersAvailableFuture = coordinator.lookupCoordinator();    assertTrue("Failed future expected", noBrokersAvailableFuture.failed());    mockTime.sleep(50);    RequestFuture<Void> future = coordinator.lookupCoordinator();    assertFalse("Request not sent", future.isDone());    assertSame("New request sent while one is in progress", future, coordinator.lookupCoordinator());    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(mockTime.timer(Long.MAX_VALUE));    assertNotSame("New request not sent after previous completed", future, coordinator.lookupCoordinator());}
public void kafkatest_f5733_0() throws Exception
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(new MockClient.RequestMatcher() {        private int invocations = 0;        @Override        public boolean matches(AbstractRequest body) {            invocations++;            boolean isSyncGroupRequest = body instanceof SyncGroupRequest;            if (isSyncGroupRequest && invocations == 1)                // simulate wakeup after the request sent                throw new WakeupException();            return isSyncGroupRequest;        }    }, syncGroupResponse(Errors.NONE));    AtomicBoolean heartbeatReceived = prepareFirstHeartbeat();    try {        coordinator.ensureActiveGroup();        fail("Should have woken up from ensureActiveGroup()");    } catch (WakeupException e) {    }    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(0, coordinator.onJoinCompleteInvokes);    assertFalse(heartbeatReceived.get());    // the join group completes in this poll()    consumerClient.poll(mockTime.timer(0));    coordinator.ensureActiveGroup();    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(1, coordinator.onJoinCompleteInvokes);    awaitFirstHeartbeat(heartbeatReceived);}
public boolean kafkatest_f5734_0(AbstractRequest body)
{    invocations++;    boolean isSyncGroupRequest = body instanceof SyncGroupRequest;    if (isSyncGroupRequest && invocations == 1)        // simulate wakeup after the request sent        throw new WakeupException();    return isSyncGroupRequest;}
private HeartbeatResponse kafkatest_f5743_0(Errors error)
{    return new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(error.code()));}
private JoinGroupResponse kafkatest_f5744_0(int generationId, String memberId, String leaderId, Errors error)
{    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(error.code()).setGenerationId(generationId).setProtocolName("dummy-subprotocol").setMemberId(memberId).setLeader(leaderId).setMembers(Collections.emptyList()));}
public void kafkatest_f5753_0()
{    MemberInfo m1 = new MemberInfo("a", Optional.empty());    MemberInfo m2 = new MemberInfo("b", Optional.empty());    MemberInfo m3 = new MemberInfo("c", Optional.empty());    List<MemberInfo> memberInfoList = Arrays.asList(m1, m2, m3);    assertEquals(memberInfoList, Utils.sorted(memberInfoList));}
public void kafkatest_f5754_0()
{    MemberInfo m1 = new MemberInfo("a", Optional.of("y"));    MemberInfo m2 = new MemberInfo("b", Optional.of("z"));    MemberInfo m3 = new MemberInfo("c", Optional.of("x"));    List<MemberInfo> memberInfoList = Arrays.asList(m1, m2, m3);    assertEquals(Arrays.asList(m3, m1, m2), Utils.sorted(memberInfoList));}
public void kafkatest_f5763_0()
{    String consumer1 = "consumer1";    String consumer2 = "consumer2";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 1);    subscriptions.put(consumer1, new Subscription(topics(topic)));    subscriptions.put(consumer2, new Subscription(topics(topic)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(partitions(tp(topic, 0)), assignment.get(consumer1));    assertEquals(Collections.<TopicPartition>emptyList(), assignment.get(consumer2));    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(isFullyBalanced(assignment));}
public void kafkatest_f5764_0()
{    String consumer1 = "consumer1";    String consumer2 = "consumer2";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 2);    subscriptions.put(consumer1, new Subscription(topics(topic)));    subscriptions.put(consumer2, new Subscription(topics(topic)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(partitions(tp(topic, 0)), assignment.get(consumer1));    assertEquals(partitions(tp(topic, 1)), assignment.get(consumer2));    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(isFullyBalanced(assignment));}
public void kafkatest_f5773_0()
{    Random rand = new Random();    int topicCount = 40;    int consumerCount = 200;    Map<String, Integer> partitionsPerTopic = new HashMap<>();    for (int i = 0; i < topicCount; i++) partitionsPerTopic.put(getTopicName(i, topicCount), rand.nextInt(10) + 1);    for (int i = 0; i < consumerCount; i++) {        List<String> topics = new ArrayList<>();        for (int j = 0; j < rand.nextInt(20); j++) topics.add(getTopicName(rand.nextInt(topicCount), topicCount));        subscriptions.put(getConsumerName(i, consumerCount), new Subscription(topics));    }    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    for (int i = 1; i < consumerCount; i++) {        String consumer = getConsumerName(i, consumerCount);        subscriptions.put(consumer, buildSubscription(subscriptions.get(consumer).topics(), assignment.get(consumer)));    }    for (int i = 0; i < 50; ++i) {        String c = getConsumerName(rand.nextInt(consumerCount), consumerCount);        subscriptions.remove(c);    }    assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(assignor.isSticky());}
public void kafkatest_f5774_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    for (int i = 1; i < 5; i++) partitionsPerTopic.put(getTopicName(i, 5), 1);    for (int i = 0; i < 3; i++) {        List<String> topics = new ArrayList<>();        for (int j = i; j <= 3 * i - 2; j++) topics.add(getTopicName(j, 5));        subscriptions.put(getConsumerName(i, 3), new Subscription(topics));    }    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    subscriptions.get(getConsumerName(0, 3)).topics().add(getTopicName(1, 5));    assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(assignor.isSticky());}
private String kafkatest_f5783_0(String str, int i, int maxNum)
{    return str + pad(i, Integer.toString(maxNum).length());}
private String kafkatest_f5784_0(int num, int digits)
{    StringBuilder sb = new StringBuilder();    int iDigits = Integer.toString(num).length();    for (int i = 1; i <= digits - iDigits; ++i) sb.append("0");    sb.append(num);    return sb.toString();}
private GroupRebalanceConfig kafkatest_f5793_0(Optional<String> groupInstanceId)
{    return new GroupRebalanceConfig(sessionTimeoutMs, rebalanceTimeoutMs, heartbeatIntervalMs, groupId, groupInstanceId, retryBackoffMs, !groupInstanceId.isPresent());}
public void kafkatest_f5794_0()
{    this.metrics.close();    this.coordinator.close(time.timer(0));}
public void kafkatest_f5804_0(RuntimeException e, RequestFuture<Object> future)
{    assertTrue("Unexpected exception type: " + e.getClass(), e instanceof DisconnectException);    assertTrue(coordinator.coordinatorUnknown());    asyncCallbackInvoked.set(true);}
public void kafkatest_f5805_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // not_coordinator will mark coordinator as unknown    time.sleep(sessionTimeoutMs);    // should send out the heartbeat    RequestFuture<Void> future = coordinator.sendHeartbeatRequest();    assertEquals(1, consumerClient.pendingRequestCount());    assertFalse(future.isDone());    client.prepareResponse(heartbeatResponse(Errors.NOT_COORDINATOR));    time.sleep(sessionTimeoutMs);    consumerClient.poll(time.timer(0));    assertTrue(future.isDone());    assertTrue(future.failed());    assertEquals(Errors.NOT_COORDINATOR.exception(), future.exception());    assertTrue(coordinator.coordinatorUnknown());}
public boolean kafkatest_f5814_0(AbstractRequest body)
{    client.updateMetadata(TestUtils.metadataUpdateWith(1, singletonMap(topic1, 1)));    return true;}
public boolean kafkatest_f5815_0(AbstractRequest body)
{    JoinGroupRequest join = (JoinGroupRequest) body;    Iterator<JoinGroupRequestData.JoinGroupRequestProtocol> protocolIterator = join.data().protocols().iterator();    assertTrue(protocolIterator.hasNext());    JoinGroupRequestData.JoinGroupRequestProtocol protocolMetadata = protocolIterator.next();    ByteBuffer metadata = ByteBuffer.wrap(protocolMetadata.metadata());    ConsumerPartitionAssignor.Subscription subscription = ConsumerProtocol.deserializeSubscription(metadata);    metadata.rewind();    return subscription.topics().contains(topic1);}
public void kafkatest_f5824_0()
{    final String consumerId = "consumer-id";    subscriptions.subscribe(singleton(topic1), rebalanceListener);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // here we return a DEFAULT_GENERATION_ID, but valid member id and leader id.    client.prepareResponse(joinGroupFollowerResponse(-1, consumerId, "leader-id", Errors.MEMBER_ID_REQUIRED));    // execute join group    coordinator.joinGroupIfNeeded(time.timer(0));    final AtomicBoolean received = new AtomicBoolean(false);    client.prepareResponse(body -> {        received.set(true);        LeaveGroupRequest leaveRequest = (LeaveGroupRequest) body;        return validateLeaveGroup(groupId, consumerId, leaveRequest);    }, new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.NONE.code())));    coordinator.maybeLeaveGroup("pending member leaves");    assertTrue(received.get());}
public void kafkatest_f5825_0()
{    final String consumerId = "consumer";    subscriptions.subscribe(singleton(topic1), rebalanceListener);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // join initially, but let coordinator rebalance on sync    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(Collections.emptyList(), Errors.UNKNOWN_SERVER_ERROR));    coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));}
public void kafkatest_f5834_0()
{    unavailableTopicTest(true, Collections.emptySet());}
public void kafkatest_f5835_0()
{    unavailableTopicTest(true, Collections.singleton("notmatching"));}
public void kafkatest_f5844_0()
{    testInFlightRequestsFailedAfterCoordinatorMarkedDead(Errors.NOT_COORDINATOR);}
public void kafkatest_f5845_0()
{    testInFlightRequestsFailedAfterCoordinatorMarkedDead(Errors.COORDINATOR_NOT_AVAILABLE);}
public void kafkatest_f5854_0()
{    int invokedBeforeTest = mockOffsetCommitCallback.invoked;    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.NONE);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), mockOffsetCommitCallback);    coordinator.invokeCompletedOffsetCommitCallbacks();    assertEquals(invokedBeforeTest + 1, mockOffsetCommitCallback.invoked);    assertNull(mockOffsetCommitCallback.exception);}
public void kafkatest_f5855_0()
{    // enable auto-assignment    subscriptions.subscribe(singleton(topic1), rebalanceListener);    joinAsFollowerAndReceiveAssignment("consumer", coordinator, singletonList(t1p));    // now switch to manual assignment    client.prepareResponse(new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.NONE.code())));    subscriptions.unsubscribe();    coordinator.maybeLeaveGroup("test commit after leave");    subscriptions.assignFromUser(singleton(t1p));    // the client should not reuse generation/memberId from auto-subscribed generation    client.prepareResponse(body -> {        OffsetCommitRequest commitRequest = (OffsetCommitRequest) body;        return commitRequest.data().memberId().equals(OffsetCommitRequest.DEFAULT_MEMBER_ID) && commitRequest.data().generationId() == OffsetCommitRequest.DEFAULT_GENERATION_ID;    }, offsetCommitResponse(singletonMap(t1p, Errors.NONE)));    AtomicBoolean success = new AtomicBoolean(false);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), callback(success));    coordinator.invokeCompletedOffsetCommitCallbacks();    assertTrue(success.get());}
public void kafkatest_f5864_0(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception)
{    committedOffsets.add(firstOffset);}
public void kafkatest_f5865_0()
{    coordinator.commitOffsetsSync(singletonMap(t1p, secondOffset), time.timer(10000));    committedOffsets.add(secondOffset);}
public void kafkatest_f5874_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    subscriptions.assignFromUser(singleton(t1p));    // Initial leader epoch of 4    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("kafka-cluster", 1, Collections.emptyMap(), singletonMap(topic1, 1), tp -> 4);    client.updateMetadata(metadataResponse);    // Load offsets from previous epoch    client.prepareResponse(offsetFetchResponse(t1p, Errors.NONE, "", 100L, 3));    coordinator.refreshCommittedOffsetsIfNeeded(time.timer(Long.MAX_VALUE));    // Offset gets loaded, but requires validation    assertEquals(Collections.emptySet(), subscriptions.missingFetchPositions());    assertFalse(subscriptions.hasAllFetchPositions());    assertTrue(subscriptions.awaitingValidation(t1p));    assertEquals(subscriptions.position(t1p).offset, 100L);    assertNull(subscriptions.validPosition(t1p));}
public void kafkatest_f5875_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    long offset = 500L;    String metadata = "blahblah";    Optional<Integer> leaderEpoch = Optional.of(15);    OffsetFetchResponse.PartitionData data = new OffsetFetchResponse.PartitionData(offset, leaderEpoch, metadata, Errors.NONE);    client.prepareResponse(new OffsetFetchResponse(Errors.NONE, singletonMap(t1p, data)));    Map<TopicPartition, OffsetAndMetadata> fetchedOffsets = coordinator.fetchCommittedOffsets(singleton(t1p), time.timer(Long.MAX_VALUE));    assertNotNull(fetchedOffsets);    assertEquals(new OffsetAndMetadata(offset, leaderEpoch, metadata), fetchedOffsets.get(t1p));}
public void kafkatest_f5884_0()
{    client.createPendingAuthenticationError(node, 300);    try {        coordinator.ensureActiveGroup();        fail("Expected an authentication error.");    } catch (AuthenticationException e) {    // OK    }}
public void kafkatest_f5885_0() throws Exception
{    // Get the assigned-partitions metric    final Metric metric = metrics.metric(new MetricName("assigned-partitions", "consumer" + groupId + "-coordinator-metrics", "", Collections.emptyMap()));    // Start polling the metric in the background    final AtomicBoolean doStop = new AtomicBoolean();    final AtomicReference<Exception> exceptionHolder = new AtomicReference<>();    final AtomicInteger observedSize = new AtomicInteger();    Thread poller = new Thread() {        @Override        public void run() {            // Poll as fast as possible to reproduce ConcurrentModificationException            while (!doStop.get()) {                try {                    int size = ((Double) metric.metricValue()).intValue();                    observedSize.set(size);                } catch (Exception e) {                    exceptionHolder.set(e);                    return;                }            }        }    };    poller.start();    // Assign two partitions to trigger a metric change that can lead to ConcurrentModificationException    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // Change the assignment several times to increase likelihood of concurrent updates    Set<TopicPartition> partitions = new HashSet<>();    int totalPartitions = 10;    for (int partition = 0; partition < totalPartitions; partition++) {        partitions.add(new TopicPartition(topic1, partition));        subscriptions.assignFromUser(partitions);    }    // Wait for the metric poller to observe the final assignment change or raise an error    TestUtils.waitForCondition(new TestCondition() {        @Override        public boolean conditionMet() {            return observedSize.get() == totalPartitions || exceptionHolder.get() != null;        }    }, "Failed to observe expected assignment change");    doStop.set(true);    poller.join();    assertNull("Failed fetching the metric at least once", exceptionHolder.get());}
public void kafkatest_f5894_0() throws Exception
{    try (ConsumerCoordinator coordinator = prepareCoordinatorForCloseTest(true, true, groupInstanceId)) {        makeCoordinatorUnknown(coordinator, Errors.COORDINATOR_NOT_AVAILABLE);        time.sleep(autoCommitIntervalMs);        closeVerifyTimeout(coordinator, 1000, 1000, 1000);    }}
public void kafkatest_f5895_0() throws Exception
{    try (ConsumerCoordinator coordinator = prepareCoordinatorForCloseTest(true, true, groupInstanceId)) {        makeCoordinatorUnknown(coordinator, Errors.COORDINATOR_NOT_AVAILABLE);        time.sleep(autoCommitIntervalMs);        closeVerifyTimeout(coordinator, Long.MAX_VALUE, requestTimeoutMs, requestTimeoutMs);    }}
private void kafkatest_f5904_0()
{    subscriptions.assignFromUser(singleton(t1p));    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.FENCED_INSTANCE_ID);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), new MockCommitCallback());    coordinator.invokeCompletedOffsetCommitCallbacks();}
private ConsumerCoordinator kafkatest_f5905_0(final boolean useGroupManagement, final boolean autoCommit, final Optional<String> groupInstanceId)
{    final String consumerId = "consumer";    rebalanceConfig = buildRebalanceConfig(groupInstanceId);    ConsumerCoordinator coordinator = buildCoordinator(rebalanceConfig, new Metrics(), assignors, autoCommit);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    if (useGroupManagement) {        subscriptions.subscribe(singleton(topic1), rebalanceListener);        client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));        client.prepareResponse(syncGroupResponse(singletonList(t1p), Errors.NONE));        coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));    } else {        subscriptions.assignFromUser(singleton(t1p));    }    subscriptions.seek(t1p, 100);    coordinator.poll(time.timer(Long.MAX_VALUE));    return coordinator;}
private HeartbeatResponse kafkatest_f5914_0(Errors error)
{    return new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(error.code()));}
private JoinGroupResponse kafkatest_f5915_0(int generationId, String memberId, Map<String, List<String>> subscriptions, Errors error)
{    List<JoinGroupResponseData.JoinGroupResponseMember> metadata = new ArrayList<>();    for (Map.Entry<String, List<String>> subscriptionEntry : subscriptions.entrySet()) {        ConsumerPartitionAssignor.Subscription subscription = new ConsumerPartitionAssignor.Subscription(subscriptionEntry.getValue());        ByteBuffer buf = ConsumerProtocol.serializeSubscription(subscription);        metadata.add(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId(subscriptionEntry.getKey()).setMetadata(buf.array()));    }    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(error.code()).setGenerationId(generationId).setProtocolName(partitionAssignor.name()).setLeader(memberId).setMemberId(memberId).setMembers(metadata));}
private void kafkatest_f5924_0(String consumerId, ConsumerCoordinator coordinator, List<TopicPartition> assignment)
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(assignment, Errors.NONE));    coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));}
private void kafkatest_f5925_0(Map<TopicPartition, Long> expectedOffsets, Errors error)
{    prepareOffsetCommitRequest(expectedOffsets, error, false);}
public void kafkatest_f5934_0(Collection<TopicPartition> partitions)
{    this.revoked = partitions;    revokedCount++;}
public void kafkatest_f5935_0(Collection<TopicPartition> partitions)
{    this.lost = partitions;    lostCount++;}
private void kafkatest_f5946_0(boolean includeInternalTopics)
{    subscription.subscribe(Pattern.compile("__.*"), new NoOpConsumerRebalanceListener());    ConsumerMetadata metadata = newConsumerMetadata(includeInternalTopics);    MetadataRequest.Builder builder = metadata.newMetadataRequestBuilder();    assertTrue(builder.isAllTopics());    List<MetadataResponse.TopicMetadata> topics = new ArrayList<>();    topics.add(topicMetadata("__consumer_offsets", true));    topics.add(topicMetadata("__matching_topic", false));    topics.add(topicMetadata("non_matching_topic", false));    MetadataResponse response = MetadataResponse.prepareResponse(singletonList(node), "clusterId", node.id(), topics);    metadata.update(response, time.milliseconds());    if (includeInternalTopics)        assertEquals(Utils.mkSet("__matching_topic", "__consumer_offsets"), metadata.fetch().topics());    else        assertEquals(Collections.singleton("__matching_topic"), metadata.fetch().topics());}
public void kafkatest_f5947_0()
{    subscription.assignFromUser(Utils.mkSet(new TopicPartition("foo", 0), new TopicPartition("bar", 0), new TopicPartition("__consumer_offsets", 0)));    testBasicSubscription(Utils.mkSet("foo", "bar"), Utils.mkSet("__consumer_offsets"));    subscription.assignFromUser(Utils.mkSet(new TopicPartition("baz", 0), new TopicPartition("__consumer_offsets", 0)));    testBasicSubscription(Utils.mkSet("baz"), Utils.mkSet("__consumer_offsets"));}
public void kafkatest_f5956_0()
{    RequestFuture<ClientResponse> future = consumerClient.send(node, heartbeat());    assertTrue(consumerClient.hasPendingRequests(node));    assertFalse(client.hasInFlightRequests(node.idString()));    consumerClient.disconnectAsync(node);    consumerClient.pollNoWakeup();    assertTrue(future.failed());    assertTrue(future.exception() instanceof DisconnectException);}
public void kafkatest_f5957_0()
{    RequestFuture<ClientResponse> future = consumerClient.send(node, heartbeat());    consumerClient.pollNoWakeup();    assertTrue(consumerClient.hasPendingRequests(node));    assertTrue(client.hasInFlightRequests(node.idString()));    consumerClient.disconnectAsync(node);    consumerClient.pollNoWakeup();    assertTrue(future.failed());    assertTrue(future.exception() instanceof DisconnectException);}
public void kafkatest_f5966_0()
{    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("clusterId", 1, Collections.singletonMap("topic", Errors.INVALID_TOPIC_EXCEPTION), Collections.emptyMap());    metadata.update(metadataResponse, time.milliseconds());    consumerClient.poll(time.timer(Duration.ZERO));}
public void kafkatest_f5967_0()
{    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("clusterId", 1, Collections.singletonMap("topic", Errors.TOPIC_AUTHORIZATION_FAILED), Collections.emptyMap());    metadata.update(metadataResponse, time.milliseconds());    consumerClient.poll(time.timer(Duration.ZERO));}
public void kafkatest_f5976_0()
{    final AtomicBoolean isReady = new AtomicBoolean();    final AtomicInteger checkCount = new AtomicInteger();    client = new MockClient(time, metadata) {        @Override        public boolean ready(Node node, long now) {            checkCount.incrementAndGet();            if (isReady.get())                return super.ready(node, now);            else                return false;        }    };    consumerClient = new ConsumerNetworkClient(new LogContext(), client, metadata, time, 100, 10, Integer.MAX_VALUE);    consumerClient.send(node, heartbeat());    consumerClient.send(node, heartbeat());    assertEquals(2, consumerClient.pendingRequestCount(node));    assertEquals(0, client.inFlightRequestCount(node.idString()));    consumerClient.trySend(time.milliseconds());    // only check one time when the node doesn't ready    assertEquals(1, checkCount.getAndSet(0));    assertEquals(2, consumerClient.pendingRequestCount(node));    assertEquals(0, client.inFlightRequestCount(node.idString()));    isReady.set(true);    consumerClient.trySend(time.milliseconds());    // check node ready or not for every request    assertEquals(2, checkCount.getAndSet(0));    assertEquals(2, consumerClient.pendingRequestCount(node));    assertEquals(2, client.inFlightRequestCount(node.idString()));}
public boolean kafkatest_f5977_0(Node node, long now)
{    checkCount.incrementAndGet();    if (isReady.get())        return super.ready(node, now);    else        return false;}
public void kafkatest_f5986_0()
{    List<TopicPartition> partitions = Arrays.asList(tp1, tp2);    ByteBuffer buffer = ConsumerProtocol.serializeAssignment(new Assignment(partitions, ByteBuffer.wrap(new byte[0])));    Assignment parsedAssignment = ConsumerProtocol.deserializeAssignment(buffer);    assertEquals(toSet(partitions), toSet(parsedAssignment.partitions()));    assertEquals(0, parsedAssignment.userData().limit());}
public void kafkatest_f5987_0()
{    List<TopicPartition> partitions = Arrays.asList(tp1, tp2);    ByteBuffer buffer = ConsumerProtocol.serializeAssignment(new Assignment(partitions, null));    Assignment parsedAssignment = ConsumerProtocol.deserializeAssignment(buffer);    assertEquals(toSet(partitions), toSet(parsedAssignment.partitions()));    assertNull(parsedAssignment.userData());}
public void kafkatest_f5996_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    client.updateMetadata(initialUpdateResponse);    Node node = initialUpdateResponse.brokers().iterator().next();    client.blackout(node, 500);    assertEquals(0, fetcher.sendFetches());    time.sleep(500);    assertEquals(1, fetcher.sendFetches());}
public void kafkatest_f5997_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    long producerId = 1;    short producerEpoch = 0;    int baseSequence = 0;    int partitionLeaderEpoch = 0;    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.idempotentBuilder(buffer, CompressionType.NONE, 0L, producerId, producerEpoch, baseSequence);    builder.append(0L, "key".getBytes(), null);    builder.close();    MemoryRecords.writeEndTransactionalMarker(buffer, 1L, time.milliseconds(), partitionLeaderEpoch, producerId, producerEpoch, new EndTransactionMarker(ControlRecordType.ABORT, 0));    buffer.flip();    client.prepareResponse(fullFetchResponse(tp0, MemoryRecords.readableRecords(buffer), Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> partitionRecords = fetchedRecords();    assertTrue(partitionRecords.containsKey(tp0));    List<ConsumerRecord<byte[], byte[]>> records = partitionRecords.get(tp0);    assertEquals(1, records.size());    assertEquals(2L, subscriptions.position(tp0).offset);    ConsumerRecord<byte[], byte[]> record = records.get(0);    assertArrayEquals("key".getBytes(), record.key());}
public void kafkatest_f6006_0()
{    buildFetcher();    ByteBuffer buffer = ByteBuffer.allocate(1024);    ByteBufferOutputStream out = new ByteBufferOutputStream(buffer);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(out, DefaultRecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.CREATE_TIME, 0L, 10L, 0L, (short) 0, 0, false, false, 0, 1024);    builder.append(10L, "key".getBytes(), "value".getBytes());    builder.close();    buffer.flip();    // Garble the CRC    buffer.position(17);    buffer.put("beef".getBytes());    buffer.position(0);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, MemoryRecords.readableRecords(buffer), Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    // the fetchedRecords() should always throw exception due to the bad batch.    for (int i = 0; i < 2; i++) {        try {            fetcher.fetchedRecords();            fail("fetchedRecords should have raised KafkaException");        } catch (KafkaException e) {            assertEquals(0, subscriptions.position(tp0).offset);        }    }}
public void kafkatest_f6007_0()
{    buildFetcher();    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V2, 0L, CompressionType.NONE, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    ByteBuffer buffer = records.buffer();    // flip some bits to fail the crc    buffer.putInt(32, buffer.get(32) ^ 87238423);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, MemoryRecords.readableRecords(buffer), Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    try {        fetcher.fetchedRecords();        fail("fetchedRecords should have raised");    } catch (KafkaException e) {        // the position should not advance since no data has been returned        assertEquals(0, subscriptions.position(tp0).offset);    }}
public void kafkatest_f6016_0()
{    buildFetcher();    subscriptions.subscribe(singleton(topicName), listener);    subscriptions.assignFromSubscribed(singleton(tp0));    subscriptions.seek(tp0, 0);    client.updateMetadata(initialUpdateResponse);    assertEquals(1, fetcher.sendFetches());    // Now the rebalance happens and fetch positions are cleared    subscriptions.assignFromSubscribed(singleton(tp0));    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    // The active fetch should be ignored since its position is no longer valid    assertTrue(fetcher.fetchedRecords().isEmpty());}
public void kafkatest_f6017_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    subscriptions.pause(tp0);    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    assertNull(fetcher.fetchedRecords().get(tp0));}
public void kafkatest_f6026_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.FENCED_LEADER_EPOCH, 100L, 0));    consumerClient.poll(time.timer(0));    assertEquals("Should not return any records", 0, fetcher.fetchedRecords().size());    assertEquals("Should have requested metadata update", 0L, metadata.timeToNextUpdate(time.milliseconds()));}
public void kafkatest_f6027_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.UNKNOWN_LEADER_EPOCH, 100L, 0));    consumerClient.poll(time.timer(0));    assertEquals("Should not return any records", 0, fetcher.fetchedRecords().size());    assertNotEquals("Should not have requested metadata update", 0L, metadata.timeToNextUpdate(time.milliseconds()));}
public void kafkatest_f6036_0()
{    buildFetcher(OffsetResetStrategy.NONE, new ByteArrayDeserializer(), new ByteArrayDeserializer(), 2, IsolationLevel.READ_UNCOMMITTED);    assignFromUser(Utils.mkSet(tp0));    subscriptions.seek(tp0, 1);    assertEquals(1, fetcher.sendFetches());    Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new HashMap<>();    partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100, FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), null, records));    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    assertEquals(2, fetcher.fetchedRecords().get(tp0).size());    subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));    subscriptions.seek(tp1, 1);    assertEquals(1, fetcher.sendFetches());    partitions = new HashMap<>();    partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.OFFSET_OUT_OF_RANGE, 100, FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), null, MemoryRecords.EMPTY));    client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions), 0, INVALID_SESSION_ID));    consumerClient.poll(time.timer(0));    assertEquals(1, fetcher.fetchedRecords().get(tp0).size());    subscriptions.seek(tp1, 10);    // Should not throw OffsetOutOfRangeException after the seek    assertEquals(0, fetcher.fetchedRecords().size());}
public void kafkatest_f6037_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0), true);    consumerClient.poll(time.timer(0));    assertEquals(0, fetcher.fetchedRecords().size());    // disconnects should have no affect on subscription state    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(0, subscriptions.position(tp0).offset);}
public void kafkatest_f6046_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.EARLIEST);    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.EARLIEST_TIMESTAMP), listOffsetResponse(Errors.NONE, 1L, 5L));    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(5, subscriptions.position(tp0).offset);}
public void kafkatest_f6047_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.LATEST);    // First fetch fails with stale metadata    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP), listOffsetResponse(Errors.NOT_LEADER_FOR_PARTITION, 1L, 5L), false);    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.hasValidPosition(tp0));    // Expect a metadata refresh    client.prepareMetadataUpdate(initialUpdateResponse);    consumerClient.pollNoWakeup();    assertFalse(client.hasPendingMetadataUpdates());    // Next fetch succeeds    time.sleep(retryBackoffMs);    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP), listOffsetResponse(Errors.NONE, 1L, 5L));    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(5, subscriptions.position(tp0).offset);}
public void kafkatest_f6056_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0);    // paused partition does not have a valid position    subscriptions.pause(tp0);    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertTrue(subscriptions.isOffsetResetNeeded(tp0));    // because tp is paused    assertFalse(subscriptions.isFetchable(tp0));    assertFalse(subscriptions.hasValidPosition(tp0));}
public void kafkatest_f6057_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 10);    // paused partition already has a valid position    subscriptions.pause(tp0);    fetcher.resetOffsetsIfNeeded();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    // because tp is paused    assertFalse(subscriptions.isFetchable(tp0));    assertTrue(subscriptions.hasValidPosition(tp0));    assertEquals(10, subscriptions.position(tp0).offset);}
public void kafkatest_f6066_0()
{    buildFetcher();    MockSelector selector = new MockSelector(time);    Sensor throttleTimeSensor = Fetcher.throttleTimeSensor(metrics, metricsRegistry);    Cluster cluster = TestUtils.singletonCluster("test", 1);    Node node = cluster.nodes().get(0);    NetworkClient client = new NetworkClient(selector, metadata, "mock", Integer.MAX_VALUE, 1000, 1000, 64 * 1024, 64 * 1024, 1000, ClientDnsLookup.DEFAULT, time, true, new ApiVersions(), throttleTimeSensor, new LogContext());    short apiVersionsResponseVersion = ApiKeys.API_VERSIONS.latestVersion();    ByteBuffer buffer = ApiVersionsResponse.createApiVersionsResponse(400, RecordBatch.CURRENT_MAGIC_VALUE).serialize(apiVersionsResponseVersion, new ResponseHeader(0));    selector.delayedReceive(new DelayedReceive(node.idString(), new NetworkReceive(node.idString(), buffer)));    while (!client.ready(node, time.milliseconds())) {        client.poll(1, time.milliseconds());        // If a throttled response is received, advance the time to ensure progress.        time.sleep(client.throttleDelayMs(node, time.milliseconds()));    }    selector.clear();    for (int i = 1; i <= 3; i++) {        int throttleTimeMs = 100 * i;        FetchRequest.Builder builder = FetchRequest.Builder.forConsumer(100, 100, new LinkedHashMap<>());        builder.rackId("");        ClientRequest request = client.newClientRequest(node.idString(), builder, time.milliseconds(), true);        client.send(request, time.milliseconds());        client.poll(1, time.milliseconds());        FetchResponse response = fullFetchResponse(tp0, nextRecords, Errors.NONE, i, throttleTimeMs);        buffer = response.serialize(ApiKeys.FETCH.latestVersion(), new ResponseHeader(request.correlationId()));        selector.completeReceive(new NetworkReceive(node.idString(), buffer));        client.poll(1, time.milliseconds());        // If a throttled response is received, advance the time to ensure progress.        time.sleep(client.throttleDelayMs(node, time.milliseconds()));        selector.clear();    }    Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();    KafkaMetric avgMetric = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchThrottleTimeAvg));    KafkaMetric maxMetric = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchThrottleTimeMax));    // Throttle times are ApiVersions=400, Fetch=(100, 200, 300)    assertEquals(250, (Double) avgMetric.metricValue(), EPSILON);    assertEquals(400, (Double) maxMetric.metricValue(), EPSILON);    client.close();}
public void kafkatest_f6067_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    MetricName maxLagMetric = metrics.metricInstance(metricsRegistry.recordsLagMax);    Map<String, String> tags = new HashMap<>();    tags.put("topic", tp0.topic());    tags.put("partition", String.valueOf(tp0.partition()));    MetricName partitionLagMetric = metrics.metricName("records-lag", metricGroup, tags);    Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();    KafkaMetric recordsFetchLagMax = allMetrics.get(maxLagMetric);    // recordsFetchLagMax should be initialized to NaN    assertEquals(Double.NaN, (Double) recordsFetchLagMax.metricValue(), EPSILON);    // recordsFetchLagMax should be hw - fetchOffset after receiving an empty FetchResponse    fetchRecords(tp0, MemoryRecords.EMPTY, Errors.NONE, 100L, 0);    assertEquals(100, (Double) recordsFetchLagMax.metricValue(), EPSILON);    KafkaMetric partitionLag = allMetrics.get(partitionLagMetric);    assertEquals(100, (Double) partitionLag.metricValue(), EPSILON);    // recordsFetchLagMax should be hw - offset of the last message after receiving a non-empty FetchResponse    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    for (int v = 0; v < 3; v++) builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, "key".getBytes(), ("value-" + v).getBytes());    fetchRecords(tp0, builder.build(), Errors.NONE, 200L, 0);    assertEquals(197, (Double) recordsFetchLagMax.metricValue(), EPSILON);    assertEquals(197, (Double) partitionLag.metricValue(), EPSILON);    // verify de-registration of partition lag    subscriptions.unsubscribe();    fetcher.sendFetches();    assertFalse(allMetrics.containsKey(partitionLagMetric));}
private Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> kafkatest_f6076_0(TopicPartition tp, MemoryRecords records, Errors error, long hw, long lastStableOffset, int throttleTime)
{    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp, records, error, hw, lastStableOffset, throttleTime));    consumerClient.poll(time.timer(0));    return fetchedRecords();}
private Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> kafkatest_f6077_0(TopicPartition tp, MemoryRecords records, Errors error, long hw, long lastStableOffset, long logStartOffset, int throttleTime)
{    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fetchResponse(tp, records, error, hw, lastStableOffset, logStartOffset, throttleTime));    consumerClient.poll(time.timer(0));    return fetchedRecords();}
public void kafkatest_f6086_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    int currentOffset = 0;    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()));    currentOffset += commitTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            FetchRequest request = (FetchRequest) body;            assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());            return true;        }    }, fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.containsKey(tp0));    assertEquals(fetchedRecords.get(tp0).size(), 2);}
public boolean kafkatest_f6087_0(AbstractRequest body)
{    FetchRequest request = (FetchRequest) body;    assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());    return true;}
public void kafkatest_f6096_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_UNCOMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    int currentOffset = 0;    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()));    abortTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    abortedTransactions.add(new FetchResponse.AbortedTransaction(1, 0));    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.containsKey(tp0));}
public void kafkatest_f6097_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    long currentOffset = 0;    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "abort1-1".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "abort1-2".getBytes(), "value".getBytes()));    currentOffset += abortTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    abortedTransactions.add(new FetchResponse.AbortedTransaction(1, 0));    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    // Ensure that we don't return any of the aborted records, but yet advance the consumer position.    assertFalse(fetchedRecords.containsKey(tp0));    assertEquals(currentOffset, subscriptions.position(tp0).offset);}
public void kafkatest_f6106_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    int currentOffset = 1;    // Empty control batch should not cause an exception    DefaultRecordBatch.writeEmptyHeader(buffer, RecordBatch.MAGIC_VALUE_V2, 1L, (short) 0, -1, 0, 0, RecordBatch.NO_PARTITION_LEADER_EPOCH, TimestampType.CREATE_TIME, time.milliseconds(), true, true);    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()));    commitTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            FetchRequest request = (FetchRequest) body;            assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());            return true;        }    }, fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.containsKey(tp0));    assertEquals(fetchedRecords.get(tp0).size(), 2);}
public boolean kafkatest_f6107_0(AbstractRequest body)
{    FetchRequest request = (FetchRequest) body;    assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());    return true;}
public void kafkatest_f6116_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(tp0.topic(), 4);    final int epochOne = 1;    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochOne), 0L);    Node node = metadata.fetch().nodes().get(0);    assertFalse(client.isConnected(node.idString()));    // Seek with a position and leader+epoch    Metadata.LeaderAndEpoch leaderAndEpoch = new Metadata.LeaderAndEpoch(metadata.leaderAndEpoch(tp0).leader, Optional.of(epochOne));    subscriptions.seekUnvalidated(tp0, new SubscriptionState.FetchPosition(20L, Optional.of(epochOne), leaderAndEpoch));    assertFalse(client.isConnected(node.idString()));    assertTrue(subscriptions.awaitingValidation(tp0));    // No version information is initially available, but the node is now connected    fetcher.validateOffsetsIfNeeded();    assertTrue(subscriptions.awaitingValidation(tp0));    assertTrue(client.isConnected(node.idString()));    apiVersions.update(node.idString(), NodeApiVersions.create());    // On the next call, the OffsetForLeaderEpoch request is sent and validation completes    Map<TopicPartition, EpochEndOffset> endOffsetMap = new HashMap<>();    endOffsetMap.put(tp0, new EpochEndOffset(Errors.NONE, epochOne, 30L));    OffsetsForLeaderEpochResponse resp = new OffsetsForLeaderEpochResponse(endOffsetMap);    client.prepareResponseFrom(resp, node);    fetcher.validateOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.awaitingValidation(tp0));    assertEquals(20L, subscriptions.position(tp0).offset);}
public void kafkatest_f6117_0()
{    // Old brokers may require CLUSTER permission to use the OffsetForLeaderEpoch API,    // so we should skip offset validation and not send the request.    buildFetcher();    assignFromUser(singleton(tp0));    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(tp0.topic(), 4);    final int epochOne = 1;    final int epochTwo = 2;    // Start with metadata, epoch=1    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochOne), 0L);    // Offset validation requires OffsetForLeaderEpoch request v3 or higher    Node node = metadata.fetch().nodes().get(0);    apiVersions.update(node.idString(), NodeApiVersions.create(singleton(new ApiVersionsResponse.ApiVersion(ApiKeys.OFFSET_FOR_LEADER_EPOCH, (short) 0, (short) 2))));    // Seek with a position and leader+epoch    Metadata.LeaderAndEpoch leaderAndEpoch = new Metadata.LeaderAndEpoch(metadata.leaderAndEpoch(tp0).leader, Optional.of(epochOne));    subscriptions.seekUnvalidated(tp0, new SubscriptionState.FetchPosition(0, Optional.of(epochOne), leaderAndEpoch));    // Update metadata to epoch=2, enter validation    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochTwo), 0L);    fetcher.validateOffsetsIfNeeded();    // Offset validation is skipped    assertFalse(subscriptions.awaitingValidation(tp0));}
private ListOffsetResponse kafkatest_f6126_0(TopicPartition tp, Errors error, long timestamp, long offset)
{    ListOffsetResponse.PartitionData partitionData = new ListOffsetResponse.PartitionData(error, timestamp, offset, Optional.empty());    Map<TopicPartition, ListOffsetResponse.PartitionData> allPartitionData = new HashMap<>();    allPartitionData.put(tp, partitionData);    return new ListOffsetResponse(allPartitionData);}
private FetchResponse<MemoryRecords> kafkatest_f6127_0(MemoryRecords records, List<FetchResponse.AbortedTransaction> abortedTransactions, Errors error, long lastStableOffset, long hw, int throttleTime)
{    Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = Collections.singletonMap(tp0, new FetchResponse.PartitionData<>(error, hw, lastStableOffset, 0L, abortedTransactions, records));    return new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions), throttleTime, INVALID_SESSION_ID);}
private void kafkatest_f6136_0(Deserializer<?> keyDeserializer, Deserializer<?> valueDeserializer)
{    buildFetcher(OffsetResetStrategy.EARLIEST, keyDeserializer, valueDeserializer, Integer.MAX_VALUE, IsolationLevel.READ_UNCOMMITTED);}
private void kafkatest_f6137_0(OffsetResetStrategy offsetResetStrategy, Deserializer<K> keyDeserializer, Deserializer<V> valueDeserializer, int maxPollRecords, IsolationLevel isolationLevel)
{    buildFetcher(new MetricConfig(), offsetResetStrategy, keyDeserializer, valueDeserializer, maxPollRecords, isolationLevel);}
public void kafkatest_f6146_0()
{    heartbeat.sentHeartbeat(time.milliseconds());    time.sleep(heartbeatIntervalMs / 2);    assertFalse(heartbeat.shouldHeartbeat(time.milliseconds()));}
public void kafkatest_f6147_0()
{    heartbeat.sentHeartbeat(time.milliseconds());    assertEquals(heartbeatIntervalMs, heartbeat.timeToNextHeartbeat(time.milliseconds()));    time.sleep(heartbeatIntervalMs);    assertEquals(0, heartbeat.timeToNextHeartbeat(time.milliseconds()));    time.sleep(heartbeatIntervalMs);    assertEquals(0, heartbeat.timeToNextHeartbeat(time.milliseconds()));}
public void kafkatest_f6156_0(Map<String, List<TopicPartition>> result)
{    this.result = result;}
public void kafkatest_f6157_0()
{    OffsetsForLeaderEpochClient offsetClient = newOffsetClient();    RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future = offsetClient.sendAsyncRequest(Node.noNode(), Collections.emptyMap());    OffsetsForLeaderEpochResponse resp = new OffsetsForLeaderEpochResponse(Collections.emptyMap());    client.prepareResponse(resp);    consumerClient.pollNoWakeup();    OffsetsForLeaderEpochClient.OffsetForEpochResult result = future.value();    assertTrue(result.partitionsToRetry().isEmpty());    assertTrue(result.endOffsets().isEmpty());}
public void kafkatest_f6166_0()
{    classNames = Arrays.asList(String.class.getName());    assertThrows(KafkaException.class, () -> getAssignorInstances(classNames, Collections.emptyMap()));}
public void kafkatest_f6167_0()
{    classNames = Arrays.asList("Non-existent assignor");    assertThrows(KafkaException.class, () -> getAssignorInstances(classNames, Collections.emptyMap()));}
public void kafkatest_f6176_0()
{    RequestFuture<String> future = new RequestFuture<>();    String value = "foo";    future.complete(value);    assertTrue(future.isDone());    assertEquals(value, future.value());}
public void kafkatest_f6177_0()
{    RequestFuture<String> future = new RequestFuture<>();    RuntimeException exception = new RuntimeException();    future.raise(exception);    assertTrue(future.isDone());    assertEquals(exception, future.exception());}
public void kafkatest_f6186_0()
{    RequestFuture<Void> future = new RequestFuture<>();    MockRequestFutureListener<Void> listener = new MockRequestFutureListener<>();    future.addListener(listener);    future.complete(null);    assertOnSuccessInvoked(listener);}
public void kafkatest_f6187_0()
{    RequestFuture<Void> future = new RequestFuture<>();    MockRequestFutureListener<Void> listener = new MockRequestFutureListener<>();    future.addListener(listener);    future.raise(new RuntimeException());    assertOnFailureInvoked(listener);}
private static void kafkatest_f6196_0(MockRequestFutureListener<T> listener)
{    assertEquals(1, listener.numOnSuccessCalls.get());    assertEquals(0, listener.numOnFailureCalls.get());}
private static void kafkatest_f6197_0(MockRequestFutureListener<T> listener)
{    assertEquals(0, listener.numOnSuccessCalls.get());    assertEquals(1, listener.numOnFailureCalls.get());}
public void kafkatest_f6206_0()
{    state.assignFromUser(singleton(tp0));    state.seek(tp0, 100);    assertTrue(state.isFetchable(tp0));    state.pause(tp0);    assertFalse(state.isFetchable(tp0));    state.resume(tp0);    assertTrue(state.isFetchable(tp0));}
public void kafkatest_f6207_0()
{    state.subscribe(singleton(topic), rebalanceListener);    assertTrue(state.checkAssignmentMatchedSubscription(singleton(tp0)));    state.assignFromSubscribed(singleton(tp0));    state.position(tp0, new SubscriptionState.FetchPosition(0, Optional.empty(), leaderAndEpoch));}
public void kafkatest_f6216_0()
{    state.assignFromUser(new HashSet<>(Arrays.asList(tp0, tp1)));    state.unsubscribe();    state.subscribe(singleton(topic), rebalanceListener);    assertEquals(singleton(topic), state.subscription());}
public void kafkatest_f6217_0()
{    state.subscribe(singleton(topic), rebalanceListener);    state.unsubscribe();    state.assignFromUser(singleton(tp0));    assertEquals(singleton(tp0), state.assignedPartitions());    assertEquals(1, state.numAssignedPartitions());}
public void kafkatest_f6226_0()
{    Node broker1 = new Node(1, "localhost", 9092);    state.assignFromUser(Collections.singleton(tp0));    int currentEpoch = 10;    long initialOffset = 10L;    int initialOffsetEpoch = 5;    SubscriptionState.FetchPosition initialPosition = new SubscriptionState.FetchPosition(initialOffset, Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, initialPosition);    assertTrue(state.awaitingValidation(tp0));    Optional<OffsetAndMetadata> divergentOffsetMetadataOpt = state.maybeCompleteValidation(tp0, initialPosition, new EpochEndOffset(initialOffsetEpoch, initialOffset + 5));    assertEquals(Optional.empty(), divergentOffsetMetadataOpt);    assertFalse(state.awaitingValidation(tp0));    assertEquals(initialPosition, state.position(tp0));}
public void kafkatest_f6227_0()
{    Node broker1 = new Node(1, "localhost", 9092);    state.assignFromUser(Collections.singleton(tp0));    int currentEpoch = 10;    long initialOffset = 10L;    int initialOffsetEpoch = 5;    long updateOffset = 20L;    int updateOffsetEpoch = 8;    SubscriptionState.FetchPosition initialPosition = new SubscriptionState.FetchPosition(initialOffset, Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, initialPosition);    assertTrue(state.awaitingValidation(tp0));    SubscriptionState.FetchPosition updatePosition = new SubscriptionState.FetchPosition(updateOffset, Optional.of(updateOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, updatePosition);    Optional<OffsetAndMetadata> divergentOffsetMetadataOpt = state.maybeCompleteValidation(tp0, initialPosition, new EpochEndOffset(initialOffsetEpoch, initialOffset + 5));    assertEquals(Optional.empty(), divergentOffsetMetadataOpt);    assertTrue(state.awaitingValidation(tp0));    assertEquals(updatePosition, state.position(tp0));}
public void kafkatest_f6236_0()
{    Map<String, Object> config = new HashMap<>();    config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    config.put(ConsumerConfig.SEND_BUFFER_CONFIG, -2);    new KafkaConsumer<>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());}
public void kafkatest_f6237_0()
{    Map<String, Object> config = new HashMap<>();    config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    config.put(ConsumerConfig.RECEIVE_BUFFER_CONFIG, -2);    new KafkaConsumer<>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());}
public void kafkatest_f6246_0()
{    try (KafkaConsumer<byte[], byte[]> consumer = newConsumer((String) null)) {        consumer.assign(null);    }}
public void kafkatest_f6247_0()
{    try (KafkaConsumer<byte[], byte[]> consumer = newConsumer(groupId)) {        consumer.assign(Collections.<TopicPartition>emptyList());        assertTrue(consumer.subscription().isEmpty());        assertTrue(consumer.assignment().isEmpty());    }}
public void kafkatest_f6256_0() throws Exception
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    Node coordinator = prepareRebalance(client, node, assignor, singletonList(tp0), null);    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    consumer.poll(Duration.ZERO);    // respond to the outstanding fetch so that we have data available on the next poll    client.respondFrom(fetchResponse(tp0, 0, 5), node);    client.poll(0, time.milliseconds());    client.prepareResponseFrom(fetchResponse(tp0, 5, 0), node);    AtomicBoolean heartbeatReceived = prepareHeartbeatResponse(client, coordinator);    time.sleep(heartbeatIntervalMs);    Thread.sleep(heartbeatIntervalMs);    consumer.poll(Duration.ZERO);    assertTrue(heartbeatReceived.get());    consumer.close(Duration.ofMillis(0));}
public void kafkatest_f6257_0()
{    final Time time = new MockTime();    final SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    final ConsumerMetadata metadata = createMetadata(subscription);    final MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    final ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    final KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    prepareRebalance(client, node, assignor, singletonList(tp0), null);    consumer.poll(Duration.ZERO);    // The underlying client should NOT get a fetch request    final Queue<ClientRequest> requests = client.requests();    Assert.assertEquals(0, requests.size());}
public void kafkatest_f6266_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.LATEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupId, groupInstanceId);    consumer.assign(singletonList(tp0));    client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);    Node coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    client.prepareResponseFrom(offsetResponse(Collections.singletonMap(tp0, -1L), Errors.NONE), coordinator);    client.prepareResponse(listOffsetsResponse(Collections.singletonMap(tp0, 50L)));    consumer.poll(Duration.ZERO);    assertEquals(50L, consumer.position(tp0));}
public void kafkatest_f6267_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.LATEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupId, Optional.empty());    consumer.assign(singletonList(tp0));    consumer.seek(tp0, 20L);    consumer.poll(Duration.ZERO);    assertEquals(subscription.validPosition(tp0).offset, 20L);}
public void kafkatest_f6276_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    Map<String, Integer> tpCounts = new HashMap<>();    tpCounts.put(topic, 1);    tpCounts.put(topic2, 1);    tpCounts.put(topic3, 1);    initMetadata(client, tpCounts);    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    // initial subscription    consumer.subscribe(Arrays.asList(topic, topic2), getConsumerRebalanceListener(consumer));    // verify that subscription has changed but assignment is still unchanged    assertTrue(consumer.subscription().size() == 2);    assertTrue(consumer.subscription().contains(topic) && consumer.subscription().contains(topic2));    assertTrue(consumer.assignment().isEmpty());    // mock rebalance responses    Node coordinator = prepareRebalance(client, node, assignor, Arrays.asList(tp0, t2p0), null);    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    consumer.poll(Duration.ZERO);    // verify that subscription is still the same, and now assignment has caught up    assertEquals(2, consumer.subscription().size());    assertTrue(consumer.subscription().contains(topic) && consumer.subscription().contains(topic2));    assertEquals(2, consumer.assignment().size());    assertTrue(consumer.assignment().contains(tp0) && consumer.assignment().contains(t2p0));    // mock a response to the outstanding fetch so that we have data available on the next poll    Map<TopicPartition, FetchInfo> fetches1 = new HashMap<>();    fetches1.put(tp0, new FetchInfo(0, 1));    fetches1.put(t2p0, new FetchInfo(0, 10));    client.respondFrom(fetchResponse(fetches1), node);    client.poll(0, time.milliseconds());    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));    // clear out the prefetch so it doesn't interfere with the rest of the test    fetches1.put(tp0, new FetchInfo(1, 0));    fetches1.put(t2p0, new FetchInfo(10, 0));    client.respondFrom(fetchResponse(fetches1), node);    client.poll(0, time.milliseconds());    // verify that the fetch occurred as expected    assertEquals(11, records.count());    assertEquals(1L, consumer.position(tp0));    assertEquals(10L, consumer.position(t2p0));    // subscription change    consumer.subscribe(Arrays.asList(topic, topic3), getConsumerRebalanceListener(consumer));    // verify that subscription has changed but assignment is still unchanged    assertTrue(consumer.subscription().size() == 2);    assertTrue(consumer.subscription().contains(topic) && consumer.subscription().contains(topic3));    assertTrue(consumer.assignment().size() == 2);    assertTrue(consumer.assignment().contains(tp0) && consumer.assignment().contains(t2p0));    // mock the offset commit response for to be revoked partitions    Map<TopicPartition, Long> partitionOffsets1 = new HashMap<>();    partitionOffsets1.put(tp0, 1L);    partitionOffsets1.put(t2p0, 10L);    AtomicBoolean commitReceived = prepareOffsetCommitResponse(client, coordinator, partitionOffsets1);    // mock rebalance responses    prepareRebalance(client, node, assignor, Arrays.asList(tp0, t3p0), coordinator);    // mock a response to the next fetch from the new assignment    Map<TopicPartition, FetchInfo> fetches2 = new HashMap<>();    fetches2.put(tp0, new FetchInfo(1, 1));    fetches2.put(t3p0, new FetchInfo(0, 100));    client.prepareResponse(fetchResponse(fetches2));    records = consumer.poll(Duration.ofMillis(1));    // verify that the fetch occurred as expected    assertEquals(101, records.count());    assertEquals(2L, consumer.position(tp0));    assertEquals(100L, consumer.position(t3p0));    // verify that the offset commits occurred as expected    assertTrue(commitReceived.get());    // verify that subscription is still the same, and now assignment has caught up    assertTrue(consumer.subscription().size() == 2);    assertTrue(consumer.subscription().contains(topic) && consumer.subscription().contains(topic3));    assertTrue(consumer.assignment().size() == 2);    assertTrue(consumer.assignment().contains(tp0) && consumer.assignment().contains(t3p0));    consumer.unsubscribe();    // verify that subscription and assignment are both cleared    assertTrue(consumer.subscription().isEmpty());    assertTrue(consumer.assignment().isEmpty());    client.requests().clear();    consumer.close();}
public void kafkatest_f6277_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    Map<String, Integer> tpCounts = new HashMap<>();    tpCounts.put(topic, 1);    tpCounts.put(topic2, 1);    initMetadata(client, tpCounts);    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, false, groupInstanceId);    // initial subscription    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    // verify that subscription has changed but assignment is still unchanged    assertEquals(singleton(topic), consumer.subscription());    assertEquals(Collections.emptySet(), consumer.assignment());    // mock rebalance responses    prepareRebalance(client, node, assignor, singletonList(tp0), null);    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    consumer.poll(Duration.ZERO);    // verify that subscription is still the same, and now assignment has caught up    assertEquals(singleton(topic), consumer.subscription());    assertEquals(singleton(tp0), consumer.assignment());    consumer.poll(Duration.ZERO);    // subscription change    consumer.subscribe(singleton(topic2), getConsumerRebalanceListener(consumer));    // verify that subscription has changed but assignment is still unchanged    assertEquals(singleton(topic2), consumer.subscription());    assertEquals(singleton(tp0), consumer.assignment());    // the auto commit is disabled, so no offset commit request should be sent    for (ClientRequest req : client.requests()) assertNotSame(ApiKeys.OFFSET_COMMIT, req.requestBuilder().apiKey());    // subscription change    consumer.unsubscribe();    // verify that subscription and assignment are both updated    assertEquals(Collections.emptySet(), consumer.subscription());    assertEquals(Collections.emptySet(), consumer.assignment());    // the auto commit is disabled, so no offset commit request should be sent    for (ClientRequest req : client.requests()) assertNotSame(ApiKeys.OFFSET_COMMIT, req.requestBuilder().apiKey());    client.requests().clear();    consumer.close();}
public void kafkatest_f6286_0() throws Exception
{    Map<TopicPartition, Errors> response = new HashMap<>();    response.put(tp0, Errors.NONE);    OffsetCommitResponse commitResponse = offsetCommitResponse(response);    consumerCloseTest(5000, singletonList(commitResponse), 5000, false);}
public void kafkatest_f6287_0() throws Exception
{    consumerCloseTest(0, Collections.<AbstractResponse>emptyList(), 0, false);}
private void kafkatest_f6296_0(final long closeTimeoutMs, List<? extends AbstractResponse> responses, long waitMs, boolean interrupt) throws Exception
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    final KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, false, Optional.empty());    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    Node coordinator = prepareRebalance(client, node, assignor, singletonList(tp0), null);    client.prepareMetadataUpdate(TestUtils.metadataUpdateWith(1, Collections.singletonMap(topic, 1)));    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    // Poll with responses    client.prepareResponseFrom(fetchResponse(tp0, 0, 1), node);    client.prepareResponseFrom(fetchResponse(tp0, 1, 0), node);    consumer.poll(Duration.ZERO);    // Initiate close() after a commit request on another thread.    // Kafka consumer is single-threaded, but the implementation allows calls on a    // different thread as long as the calls are not executed concurrently. So this is safe.    ExecutorService executor = Executors.newSingleThreadExecutor();    final AtomicReference<Exception> closeException = new AtomicReference<>();    try {        Future<?> future = executor.submit(new Runnable() {            @Override            public void run() {                consumer.commitAsync();                try {                    consumer.close(Duration.ofMillis(closeTimeoutMs));                } catch (Exception e) {                    closeException.set(e);                }            }        });        // if close timeout is not zero.        try {            future.get(100, TimeUnit.MILLISECONDS);            if (closeTimeoutMs != 0)                fail("Close completed without waiting for commit or leave response");        } catch (TimeoutException e) {        // Expected exception        }        // Ensure close has started and queued at least one more request after commitAsync        client.waitForRequests(2, 1000);        // In non-graceful mode, close() times out without an exception even though commit response is pending        for (int i = 0; i < responses.size(); i++) {            client.waitForRequests(1, 1000);            client.respondFrom(responses.get(i), coordinator);            if (i != responses.size() - 1) {                try {                    future.get(100, TimeUnit.MILLISECONDS);                    fail("Close completed without waiting for response");                } catch (TimeoutException e) {                // Expected exception                }            }        }        if (waitMs > 0)            time.sleep(waitMs);        if (interrupt) {            assertTrue("Close terminated prematurely", future.cancel(true));            TestUtils.waitForCondition(new TestCondition() {                @Override                public boolean conditionMet() {                    return closeException.get() != null;                }            }, "InterruptException did not occur within timeout.");            assertTrue("Expected exception not thrown " + closeException, closeException.get() instanceof InterruptException);        } else {            // Should succeed without TimeoutException or ExecutionException            future.get(500, TimeUnit.MILLISECONDS);            assertNull("Unexpected exception during close", closeException.get());        }    } finally {        executor.shutdownNow();    }}
public void kafkatest_f6297_0()
{    consumer.commitAsync();    try {        consumer.close(Duration.ofMillis(closeTimeoutMs));    } catch (Exception e) {        closeException.set(e);    }}
public void kafkatest_f6306_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.subscribe(singleton(topic), getExceptionConsumerRebalanceListener());    Node coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);    client.prepareResponseFrom(joinGroupFollowerResponse(assignor, 1, "memberId", "leaderId", Errors.NONE), coordinator);    client.prepareResponseFrom(syncGroupResponse(singletonList(tp0), Errors.NONE), coordinator);    // assign throws    try {        consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));        fail("Should throw exception");    } catch (Throwable e) {        assertEquals("boom!", e.getCause().getMessage());    }    // the assignment is still updated regardless of the exception    assertEquals(singleton(tp0), subscription.assignedPartitions());    // close's revoke throws    try {        consumer.close(Duration.ofMillis(0));        fail("Should throw exception");    } catch (Throwable e) {        assertEquals("boom!", e.getCause().getCause().getMessage());    }    consumer.close(Duration.ofMillis(0));    // the assignment is still updated regardless of the exception    assertTrue(subscription.assignedPartitions().isEmpty());}
private KafkaConsumer<String, String> kafkatest_f6307_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    client.createPendingAuthenticationError(node, 0);    return newConsumer(time, client, subscription, metadata, assignor, false, groupInstanceId);}
private Node kafkatest_f6317_0(MockClient client, Node node, ConsumerPartitionAssignor assignor, List<TopicPartition> partitions, Node coordinator)
{    if (coordinator == null) {        // lookup coordinator        client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);        coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    }    // join group    client.prepareResponseFrom(joinGroupFollowerResponse(assignor, 1, "memberId", "leaderId", Errors.NONE), coordinator);    // sync group    client.prepareResponseFrom(syncGroupResponse(partitions, Errors.NONE), coordinator);    return coordinator;}
private AtomicBoolean kafkatest_f6318_0(MockClient client, Node coordinator)
{    final AtomicBoolean heartbeatReceived = new AtomicBoolean(false);    client.prepareResponseFrom(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            heartbeatReceived.set(true);            return true;        }    }, new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(Errors.NONE.code())), coordinator);    return heartbeatReceived;}
private ListOffsetResponse kafkatest_f6327_0(Map<TopicPartition, Long> offsets)
{    return listOffsetsResponse(offsets, Collections.emptyMap());}
private ListOffsetResponse kafkatest_f6328_0(Map<TopicPartition, Long> partitionOffsets, Map<TopicPartition, Errors> partitionErrors)
{    Map<TopicPartition, ListOffsetResponse.PartitionData> partitionData = new HashMap<>();    for (Map.Entry<TopicPartition, Long> partitionOffset : partitionOffsets.entrySet()) {        partitionData.put(partitionOffset.getKey(), new ListOffsetResponse.PartitionData(Errors.NONE, ListOffsetResponse.UNKNOWN_TIMESTAMP, partitionOffset.getValue(), Optional.empty()));    }    for (Map.Entry<TopicPartition, Errors> partitionError : partitionErrors.entrySet()) {        partitionData.put(partitionError.getKey(), new ListOffsetResponse.PartitionData(partitionError.getValue(), ListOffsetResponse.UNKNOWN_TIMESTAMP, ListOffsetResponse.UNKNOWN_OFFSET, Optional.empty()));    }    return new ListOffsetResponse(partitionData);}
public void kafkatest_f6337_0()
{    consumer.subscribe(Collections.singleton("test"));    assertEquals(0, consumer.poll(1000).count());    consumer.rebalance(Arrays.asList(new TopicPartition("test", 0), new TopicPartition("test", 1)));    // Mock consumers need to seek manually since they cannot automatically reset offsets    HashMap<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(new TopicPartition("test", 0), 0L);    beginningOffsets.put(new TopicPartition("test", 1), 0L);    consumer.updateBeginningOffsets(beginningOffsets);    consumer.seek(new TopicPartition("test", 0), 0);    ConsumerRecord<String, String> rec1 = new ConsumerRecord<>("test", 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, "key1", "value1");    ConsumerRecord<String, String> rec2 = new ConsumerRecord<>("test", 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, "key2", "value2");    consumer.addRecord(rec1);    consumer.addRecord(rec2);    ConsumerRecords<String, String> recs = consumer.poll(1);    Iterator<ConsumerRecord<String, String>> iter = recs.iterator();    assertEquals(rec1, iter.next());    assertEquals(rec2, iter.next());    assertFalse(iter.hasNext());    assertEquals(2L, consumer.position(new TopicPartition("test", 0)));    consumer.commitSync();    assertEquals(2L, consumer.committed(new TopicPartition("test", 0)).offset());}
public void kafkatest_f6338_0()
{    TopicPartition partition = new TopicPartition("test", 0);    consumer.assign(Collections.singleton(partition));    consumer.addRecord(new ConsumerRecord<String, String>("test", 0, 0, null, null));    consumer.updateEndOffsets(Collections.singletonMap(partition, 1L));    consumer.seekToEnd(Collections.singleton(partition));    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));    assertThat(records.count(), is(0));    assertThat(records.isEmpty(), is(true));}
public void kafkatest_f6347_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, 3);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumer1, new Subscription(topics(topic1))));    assertEquals(Collections.singleton(consumer1), assignment.keySet());    assertAssignment(partitions(tp(topic1, 0), tp(topic1, 1), tp(topic1, 2)), assignment.get(consumer1));}
public void kafkatest_f6348_0()
{    String otherTopic = "other";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, 3);    partitionsPerTopic.put(otherTopic, 3);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumer1, new Subscription(topics(topic1))));    assertEquals(Collections.singleton(consumer1), assignment.keySet());    assertAssignment(partitions(tp(topic1, 0), tp(topic1, 1), tp(topic1, 2)), assignment.get(consumer1));}
public void kafkatest_f6357_0()
{    Map<String, Integer> partitionsPerTopic = setupPartitionsPerTopicWithTwoTopics(5, 5);    Map<String, Subscription> consumers = new HashMap<>();    for (MemberInfo m : staticMemberInfos) {        Subscription subscription = new Subscription(topics(topic1, topic2), null, Collections.emptyList());        subscription.setGroupInstanceId(m.groupInstanceId);        consumers.put(m.memberId, subscription);    }    Map<String, List<TopicPartition>> expectedInstanceAssignment = new HashMap<>();    expectedInstanceAssignment.put(instance1, partitions(tp(topic1, 0), tp(topic1, 1), tp(topic2, 0), tp(topic2, 1)));    expectedInstanceAssignment.put(instance2, partitions(tp(topic1, 2), tp(topic1, 3), tp(topic2, 2), tp(topic2, 3)));    expectedInstanceAssignment.put(instance3, partitions(tp(topic1, 4), tp(topic2, 4)));    Map<String, List<TopicPartition>> staticAssignment = checkStaticAssignment(assignor, partitionsPerTopic, consumers);    assertEquals(expectedInstanceAssignment, staticAssignment);    // Now switch the member.id fields for each member info, the assignment should    // stay the same as last time.    String consumer4 = "consumer4";    String consumer5 = "consumer5";    consumers.put(consumer4, consumers.get(consumer3));    consumers.remove(consumer3);    consumers.put(consumer5, consumers.get(consumer2));    consumers.remove(consumer2);    Map<String, List<TopicPartition>> newStaticAssignment = checkStaticAssignment(assignor, partitionsPerTopic, consumers);    assertEquals(staticAssignment, newStaticAssignment);}
 static Map<String, List<TopicPartition>> kafkatest_f6358_0(AbstractPartitionAssignor assignor, Map<String, Integer> partitionsPerTopic, Map<String, Subscription> consumers)
{    Map<String, List<TopicPartition>> assignmentByMemberId = assignor.assign(partitionsPerTopic, consumers);    Map<String, List<TopicPartition>> assignmentByInstanceId = new HashMap<>();    for (Map.Entry<String, Subscription> entry : consumers.entrySet()) {        String memberId = entry.getKey();        Optional<String> instanceId = entry.getValue().groupInstanceId();        instanceId.ifPresent(id -> assignmentByInstanceId.put(id, assignmentByMemberId.get(memberId)));    }    return assignmentByInstanceId;}
public void kafkatest_f6367_0()
{    String otherTopic = "other";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 3);    partitionsPerTopic.put(otherTopic, 3);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumerId, new Subscription(topics(topic))));    assertEquals(partitions(tp(topic, 0), tp(topic, 1), tp(topic, 2)), assignment.get(consumerId));}
public void kafkatest_f6368_0()
{    Map<String, Integer> partitionsPerTopic = setupPartitionsPerTopicWithTwoTopics(1, 2);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumerId, new Subscription(topics(topic1, topic2))));    assertEquals(partitions(tp(topic1, 0), tp(topic2, 0), tp(topic2, 1)), assignment.get(consumerId));}
private static List<String> kafkatest_f6377_0(String... topics)
{    return Arrays.asList(topics);}
private static List<TopicPartition> kafkatest_f6378_0(TopicPartition... partitions)
{    return Arrays.asList(partitions);}
private Subscription kafkatest_f6387_0(List<String> topics, List<TopicPartition> partitions, int generation)
{    return new Subscription(topics, serializeTopicPartitionAssignment(new MemberData(partitions, Optional.of(generation))));}
private static Subscription kafkatest_f6388_0(List<String> topics, List<TopicPartition> partitions)
{    Struct struct = new Struct(StickyAssignor.STICKY_ASSIGNOR_USER_DATA_V0);    List<Struct> topicAssignments = new ArrayList<>();    for (Map.Entry<String, List<Integer>> topicEntry : CollectionUtils.groupPartitionsByTopic(partitions).entrySet()) {        Struct topicAssignment = new Struct(StickyAssignor.TOPIC_ASSIGNMENT);        topicAssignment.set(StickyAssignor.TOPIC_KEY_NAME, topicEntry.getKey());        topicAssignment.set(StickyAssignor.PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());        topicAssignments.add(topicAssignment);    }    struct.set(StickyAssignor.TOPIC_PARTITIONS_KEY_NAME, topicAssignments.toArray());    ByteBuffer buffer = ByteBuffer.allocate(StickyAssignor.STICKY_ASSIGNOR_USER_DATA_V0.sizeOf(struct));    StickyAssignor.STICKY_ASSIGNOR_USER_DATA_V0.write(buffer, struct);    buffer.flip();    return new Subscription(topics, buffer);}
public void kafkatest_f6397_0()
{    FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);    FetchSessionHandler.Builder builder = handler.newBuilder();    builder.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));    FetchSessionHandler.FetchRequestData data = builder.build();    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 110, 210)), data.toSend(), data.sessionPartitions());    assertEquals(INVALID_SESSION_ID, data.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data.metadata().epoch());    FetchResponse<MemoryRecords> resp = new FetchResponse<>(Errors.NONE, respMap(new RespEntry("foo", 0, 0, 0), new RespEntry("foo", 1, 0, 0)), 0, INVALID_SESSION_ID);    handler.handleResponse(resp);    FetchSessionHandler.Builder builder2 = handler.newBuilder();    builder2.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    FetchSessionHandler.FetchRequestData data2 = builder2.build();    assertEquals(INVALID_SESSION_ID, data2.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data2.metadata().epoch());    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200)), data.toSend(), data.sessionPartitions());}
public void kafkatest_f6398_0()
{    FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);    FetchSessionHandler.Builder builder = handler.newBuilder();    builder.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));    FetchSessionHandler.FetchRequestData data = builder.build();    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 110, 210)), data.toSend(), data.sessionPartitions());    assertEquals(INVALID_SESSION_ID, data.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data.metadata().epoch());    FetchResponse<MemoryRecords> resp = new FetchResponse<>(Errors.NONE, respMap(new RespEntry("foo", 0, 10, 20), new RespEntry("foo", 1, 10, 20)), 0, 123);    handler.handleResponse(resp);    // Test an incremental fetch request which adds one partition and modifies another.    FetchSessionHandler.Builder builder2 = handler.newBuilder();    builder2.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder2.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 120, 210, Optional.empty()));    builder2.add(new TopicPartition("bar", 0), new FetchRequest.PartitionData(20, 200, 200, Optional.empty()));    FetchSessionHandler.FetchRequestData data2 = builder2.build();    assertFalse(data2.metadata().isFull());    assertMapEquals(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 120, 210), new ReqEntry("bar", 0, 20, 200, 200)), data2.sessionPartitions());    assertMapEquals(reqMap(new ReqEntry("bar", 0, 20, 200, 200), new ReqEntry("foo", 1, 10, 120, 210)), data2.toSend());    FetchResponse<MemoryRecords> resp2 = new FetchResponse<>(Errors.NONE, respMap(new RespEntry("foo", 1, 20, 20)), 0, 123);    handler.handleResponse(resp2);    // Skip building a new request.  Test that handling an invalid fetch session epoch response results    // in a request which closes the session.    FetchResponse<MemoryRecords> resp3 = new FetchResponse<>(Errors.INVALID_FETCH_SESSION_EPOCH, respMap(), 0, INVALID_SESSION_ID);    handler.handleResponse(resp3);    FetchSessionHandler.Builder builder4 = handler.newBuilder();    builder4.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder4.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 120, 210, Optional.empty()));    builder4.add(new TopicPartition("bar", 0), new FetchRequest.PartitionData(20, 200, 200, Optional.empty()));    FetchSessionHandler.FetchRequestData data4 = builder4.build();    assertTrue(data4.metadata().isFull());    assertEquals(data2.metadata().sessionId(), data4.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data4.metadata().epoch());    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 120, 210), new ReqEntry("bar", 0, 20, 200, 200)), data4.sessionPartitions(), data4.toSend());}
public void kafkatest_f6407_0()
{    inFlightRequests.completeLastSent(dest);}
private int kafkatest_f6408_0(String destination)
{    return addRequest(destination, 0, 10000);}
public void kafkatest_f6417_0()
{    assertFalse(metadata.updateRequested());    int[] epochs = { 42, 42, 41, 41, 42, 43, 43, 42, 41, 44 };    boolean[] updateResult = { true, false, false, false, false, true, false, false, false, true };    TopicPartition tp = new TopicPartition("topic", 0);    for (int i = 0; i < epochs.length; i++) {        metadata.updateLastSeenEpochIfNewer(tp, epochs[i]);        if (updateResult[i]) {            assertTrue("Expected metadata update to be requested [" + i + "]", metadata.updateRequested());        } else {            assertFalse("Did not expect metadata update to be requested [" + i + "]", metadata.updateRequested());        }        metadata.update(emptyMetadataResponse(), 0L);        assertFalse(metadata.updateRequested());    }}
public void kafkatest_f6418_0()
{    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put("topic-1", 1);    TopicPartition tp = new TopicPartition("topic-1", 0);    metadata.update(emptyMetadataResponse(), 0L);    // First epoch seen, accept it    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 100);        metadata.update(metadataResponse, 10L);        assertNotNull(metadata.fetch().partition(tp));        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }    // Fake an empty ISR, but with an older epoch, should reject it    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 99, (error, partition, leader, leaderEpoch, replicas, isr, offlineReplicas) -> new MetadataResponse.PartitionMetadata(error, partition, leader, leaderEpoch, replicas, Collections.emptyList(), offlineReplicas));        metadata.update(metadataResponse, 20L);        assertEquals(metadata.fetch().partition(tp).inSyncReplicas().length, 1);        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }    // Fake an empty ISR, with same epoch, accept it    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 100, (error, partition, leader, leaderEpoch, replicas, isr, offlineReplicas) -> new MetadataResponse.PartitionMetadata(error, partition, leader, leaderEpoch, replicas, Collections.emptyList(), offlineReplicas));        metadata.update(metadataResponse, 20L);        assertEquals(metadata.fetch().partition(tp).inSyncReplicas().length, 0);        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }    // Empty metadata response, should not keep old partition but should keep the last-seen epoch    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), Collections.emptyMap());        metadata.update(metadataResponse, 20L);        assertNull(metadata.fetch().partition(tp));        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }    // Back in the metadata, with old epoch, should not get added    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 99);        metadata.update(metadataResponse, 10L);        assertNull(metadata.fetch().partition(tp));        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }}
public void kafkatest_f6427_0()
{    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put("topic-1", 1);    Node node0 = new Node(0, "localhost", 9092);    Node node1 = new Node(1, "localhost", 9093);    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 2, Collections.emptyMap(), partitionCounts, _tp -> 99, (error, partition, leader, leaderEpoch, replicas, isr, offlineReplicas) -> new MetadataResponse.PartitionMetadata(error, partition, node0, leaderEpoch, Collections.singletonList(node0), Collections.emptyList(), Collections.singletonList(node1)));    metadata.update(emptyMetadataResponse(), 0L);    metadata.update(metadataResponse, 10L);    TopicPartition tp = new TopicPartition("topic-1", 0);    assertOptional(metadata.fetch().nodeIfOnline(tp, 0), node -> assertEquals(node.id(), 0));    assertFalse(metadata.fetch().nodeIfOnline(tp, 1).isPresent());    assertEquals(metadata.fetch().nodeById(0).id(), 0);    assertEquals(metadata.fetch().nodeById(1).id(), 1);}
public boolean kafkatest_f6428_0(AbstractRequest body)
{    return true;}
public void kafkatest_f6437_0(Node node, long durationMs)
{    connectionState(node.idString()).throttle(time.milliseconds() + durationMs);}
public void kafkatest_f6438_0(Node node, long durationMs)
{    connectionState(node.idString()).setReadyDelayed(time.milliseconds() + durationMs);}
private synchronized void kafkatest_f6447_0()
{    try {        int remainingBlockingWakeups = numBlockingWakeups;        if (remainingBlockingWakeups <= 0)            return;        while (numBlockingWakeups == remainingBlockingWakeups) wait();    } catch (InterruptedException e) {        throw new InterruptException(e);    }}
public List<ClientResponse> kafkatest_f6448_0(long timeoutMs, long now)
{    maybeAwaitWakeup();    checkTimeoutOfPendingRequests(now);    // We skip metadata updates if all nodes are currently blacked out    if (metadataUpdater.isUpdateNeeded() && leastLoadedNode(now) != null) {        MetadataUpdate metadataUpdate = metadataUpdates.poll();        if (metadataUpdate != null) {            metadataUpdater.update(time, metadataUpdate);        } else {            metadataUpdater.updateWithCurrentMetadata(time);        }    }    List<ClientResponse> copy = new ArrayList<>();    ClientResponse response;    while ((response = this.responses.poll()) != null) {        response.onComplete();        copy.add(response);    }    return copy;}
public void kafkatest_f6457_0(AbstractResponse response, Node node, boolean disconnected)
{    Iterator<ClientRequest> iterator = requests.iterator();    while (iterator.hasNext()) {        ClientRequest request = iterator.next();        if (request.destination().equals(node.idString())) {            iterator.remove();            short version = request.requestBuilder().latestAllowedVersion();            responses.add(new ClientResponse(request.makeHeader(version), request.callback(), request.destination(), request.createdTimeMs(), time.milliseconds(), disconnected, null, null, response));            return;        }    }    throw new IllegalArgumentException("No requests available to node " + node);}
public void kafkatest_f6458_0(AbstractResponse response)
{    prepareResponse(ALWAYS_TRUE, response, false);}
public void kafkatest_f6467_0(final int minRequests, long maxWaitMs) throws InterruptedException
{    TestUtils.waitForCondition(new TestCondition() {        @Override        public boolean conditionMet() {            return requests.size() >= minRequests;        }    }, maxWaitMs, "Expected requests have not been sent");}
public boolean kafkatest_f6468_0()
{    return requests.size() >= minRequests;}
public boolean kafkatest_f6477_0()
{    return !responses.isEmpty() || !futureResponses.isEmpty();}
public int kafkatest_f6478_0(String node)
{    int result = 0;    for (ClientRequest req : requests) {        if (req.destination().equals(node))            ++result;    }    return result;}
public Node kafkatest_f6487_0(long now)
{    // Consistent with NetworkClient, we do not return nodes awaiting reconnect backoff    for (Node node : metadataUpdater.fetchNodes()) {        if (!connectionState(node.idString()).isBackingOff(now))            return node;    }    return null;}
public void kafkatest_f6488_0(NodeApiVersions nodeApiVersions)
{    this.nodeApiVersions = nodeApiVersions;}
 void kafkatest_f6499_0(long untilMs)
{    throttledUntilMs = untilMs;}
 void kafkatest_f6500_0(long untilMs)
{    unreachableUntilMs = untilMs;}
 boolean kafkatest_f6509_0(long now)
{    switch(state) {        case CONNECTED:            return notThrottled(now);        case CONNECTING:            if (isReadyDelayed(now))                return false;            state = State.CONNECTED;            return ready(now);        case DISCONNECTED:            if (isBackingOff(now)) {                return false;            } else if (isUnreachable(now)) {                backingOffUntilMs = now + 100;                return false;            }            state = State.CONNECTING;            return ready(now);        default:            throw new IllegalArgumentException("Invalid state: " + state);    }}
private NetworkClient kafkatest_f6510_0(long reconnectBackoffMaxMs)
{    return new NetworkClient(selector, metadataUpdater, "mock", Integer.MAX_VALUE, reconnectBackoffMsTest, reconnectBackoffMaxMs, 64 * 1024, 64 * 1024, defaultRequestTimeoutMs, ClientDnsLookup.DEFAULT, time, true, new ApiVersions(), new LogContext());}
public void kafkatest_f6519_0()
{    client.ready(node, time.milliseconds());    awaitReady(client, node);    client.poll(1, time.milliseconds());    assertTrue("The client should be ready", client.isReady(node, time.milliseconds()));    ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000, Collections.<TopicPartition, MemoryRecords>emptyMap());    ClientRequest request = client.newClientRequest(node.idString(), builder, time.milliseconds(), true);    client.send(request, time.milliseconds());    assertEquals("There should be 1 in-flight request after send", 1, client.inFlightRequestCount(node.idString()));    assertTrue(client.hasInFlightRequests(node.idString()));    assertTrue(client.hasInFlightRequests());    client.close(node.idString());    assertEquals("There should be no in-flight request after close", 0, client.inFlightRequestCount(node.idString()));    assertFalse(client.hasInFlightRequests(node.idString()));    assertFalse(client.hasInFlightRequests());    assertFalse("Connection should not be ready after close", client.isReady(node, 0));}
public void kafkatest_f6520_0()
{    List<String> topics = Arrays.asList("topic_1");    // disabling auto topic creation for versions less than 4 is not supported    MetadataRequest.Builder builder = new MetadataRequest.Builder(topics, false, (short) 3);    client.sendInternalMetadataRequest(builder, node.idString(), time.milliseconds());    assertEquals(UnsupportedVersionException.class, metadataUpdater.getAndClearFailure().getClass());}
public void kafkatest_f6529_0()
{    // Instrument the test so that the max protocol version for PRODUCE returned from the node is 5 and thus    // client-side throttling is not enabled. Also, return a response with a 100ms throttle delay.    setExpectedApiVersionsResponse(createExpectedApiVersionsResponse(ApiKeys.PRODUCE, (short) 5));    while (!client.ready(node, time.milliseconds())) client.poll(1, time.milliseconds());    selector.clear();    int correlationId = sendEmptyProduceRequest();    client.poll(1, time.milliseconds());    sendThrottledProduceResponse(correlationId, 100);    client.poll(1, time.milliseconds());    // Since client-side throttling is disabled, the connection is ready even though the response indicated a    // throttle delay.    assertTrue(client.ready(node, time.milliseconds()));    assertEquals(0, client.throttleDelayMs(node, time.milliseconds()));}
private int kafkatest_f6530_0()
{    ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000, Collections.emptyMap());    TestCallbackHandler handler = new TestCallbackHandler();    ClientRequest request = client.newClientRequest(node.idString(), builder, time.milliseconds(), true, defaultRequestTimeoutMs, handler);    client.send(request, time.milliseconds());    return request.correlationId();}
public void kafkatest_f6539_0()
{    awaitReady(client, node);    long now = time.milliseconds();    long delay = client.connectionDelay(node, now);    assertEquals(Long.MAX_VALUE, delay);}
public void kafkatest_f6540_0()
{    awaitReady(client, node);    // First disconnection    selector.serverDisconnect(node.idString());    client.poll(defaultRequestTimeoutMs, time.milliseconds());    long delay = client.connectionDelay(node, time.milliseconds());    long expectedDelay = reconnectBackoffMsTest;    double jitter = 0.3;    assertEquals(expectedDelay, delay, expectedDelay * jitter);    // Sleep until there is no connection delay    time.sleep(delay);    assertEquals(0, client.connectionDelay(node, time.milliseconds()));    // Start connecting and disconnect before the connection is established    client.ready(node, time.milliseconds());    selector.serverDisconnect(node.idString());    client.poll(defaultRequestTimeoutMs, time.milliseconds());    // Second attempt should take twice as long with twice the jitter    expectedDelay = Math.round(delay * 2);    delay = client.connectionDelay(node, time.milliseconds());    jitter = 0.6;    assertEquals(expectedDelay, delay, expectedDelay * jitter);}
public void kafkatest_f6549_0(ClientResponse response)
{    this.executed = true;    this.response = response;}
public void kafkatest_f6550_0(KafkaException exception)
{    failure = exception;    super.handleFatalException(exception);}
public void kafkatest_f6559_0()
{    NodeApiVersions apiVersions = NodeApiVersions.create(Collections.singleton(new ApiVersion(ApiKeys.PRODUCE, (short) 300, (short) 300)));    apiVersions.latestUsableVersion(ApiKeys.PRODUCE);}
public void kafkatest_f6560_0()
{    List<ApiVersion> versionList = new LinkedList<>();    for (ApiVersion apiVersion : ApiVersionsResponse.defaultApiVersionsResponse().apiVersions()) {        versionList.add(apiVersion);    }    // Add an API key that we don't know about.    versionList.add(new ApiVersion((short) 100, (short) 0, (short) 1));    NodeApiVersions versions = new NodeApiVersions(versionList);    for (ApiKeys apiKey : ApiKeys.values()) {        assertEquals(apiKey.latestVersion(), versions.latestUsableVersion(apiKey));    }}
private CountDownLatch kafkatest_f6569_0(final BufferPool pool, final int size)
{    final CountDownLatch completed = new CountDownLatch(1);    Thread thread = new Thread() {        public void run() {            try {                pool.allocate(size, maxBlockTimeMs);            } catch (InterruptedException e) {                e.printStackTrace();            } finally {                completed.countDown();            }        }    };    thread.start();    return completed;}
public void kafkatest_f6570_0()
{    try {        pool.allocate(size, maxBlockTimeMs);    } catch (InterruptedException e) {        e.printStackTrace();    } finally {        completed.countDown();    }}
protected int kafkatest_f6579_0()
{    return freeSize.get();}
public void kafkatest_f6580_0()
{    BufferPool bufferPool = new BufferPool(1024, 1024, metrics, time, metricGroup) {        @Override        protected ByteBuffer allocateByteBuffer(int size) {            throw new OutOfMemoryError();        }    };    try {        bufferPool.allocateByteBuffer(1024);        // should not reach here        fail("Should have thrown OutOfMemoryError");    } catch (OutOfMemoryError ignored) {    }    assertEquals(bufferPool.availableMemory(), 1024);}
public void kafkatest_f6589_0() throws Exception
{    ProducerBatch batch = new ProducerBatch(new TopicPartition("topic", 1), memoryRecordsBuilder, now);    MockCallback callback = new MockCallback();    FutureRecordMetadata future = batch.tryAppend(now, null, new byte[10], Record.EMPTY_HEADERS, callback, now);    KafkaException exception = new KafkaException();    batch.abort(exception);    assertTrue(future.isDone());    assertEquals(1, callback.invocations);    assertEquals(exception, callback.exception);    assertNull(callback.metadata);    // subsequent completion should be ignored    assertFalse(batch.done(500L, 2342342341L, null));    assertFalse(batch.done(-1, -1, new KafkaException()));    assertEquals(1, callback.invocations);    assertTrue(future.isDone());    try {        future.get();        fail("Future should have thrown");    } catch (ExecutionException e) {        assertEquals(exception, e.getCause());    }}
public void kafkatest_f6590_0() throws Exception
{    ProducerBatch batch = new ProducerBatch(new TopicPartition("topic", 1), memoryRecordsBuilder, now);    MockCallback callback = new MockCallback();    FutureRecordMetadata future = batch.tryAppend(now, null, new byte[10], Record.EMPTY_HEADERS, callback, now);    KafkaException exception = new KafkaException();    batch.abort(exception);    assertEquals(1, callback.invocations);    assertEquals(exception, callback.exception);    assertNull(callback.metadata);    try {        batch.abort(new KafkaException());        fail("Expected exception from abort");    } catch (IllegalStateException e) {    // expected    }    assertEquals(1, callback.invocations);    assertTrue(future.isDone());    try {        future.get();        fail("Future should have thrown");    } catch (ExecutionException e) {        assertEquals(exception, e.getCause());    }}
public ProducerRecord<Integer, String> kafkatest_f6600_0(ProducerRecord<Integer, String> record)
{    onSendCount++;    if (throwExceptionOnSend)        throw new KafkaException("Injected exception in AppendProducerInterceptor.onSend");    return new ProducerRecord<>(record.topic(), record.partition(), record.key(), record.value().concat(appendStr));}
public void kafkatest_f6601_0(RecordMetadata metadata, Exception exception)
{    onAckCount++;    if (exception != null) {        onErrorAckCount++;        // if RecordMetadata.TopicPartition is null        if (metadata != null && metadata.topic().length() >= 0) {            onErrorAckWithTopicSetCount++;            if (metadata.partition() >= 0)                onErrorAckWithTopicPartitionSetCount++;        }    }    if (throwExceptionOnAck)        throw new KafkaException("Injected exception in AppendProducerInterceptor.onAcknowledgement");}
public void kafkatest_f6611_0() throws Exception
{    long time = 0;    metadata.update(responseWithCurrentTopics(), time);    assertTrue("No update needed.", metadata.timeToNextUpdate(time) > 0);    // first try with a max wait time of 0 and ensure that this returns back without waiting forever    try {        metadata.awaitUpdate(metadata.requestUpdate(), 0);        fail("Wait on metadata update was expected to timeout, but it didn't");    } catch (TimeoutException te) {    // expected    }    // now try with a higher timeout value once    final long twoSecondWait = 2000;    try {        metadata.awaitUpdate(metadata.requestUpdate(), twoSecondWait);        fail("Wait on metadata update was expected to timeout, but it didn't");    } catch (TimeoutException te) {    // expected    }}
public void kafkatest_f6612_0()
{    long now = 10000;    // New topic added to fetch set and update requested. It should allow immediate update.    metadata.update(responseWithCurrentTopics(), now);    metadata.add("new-topic");    assertEquals(0, metadata.timeToNextUpdate(now));    // Even though add is called, immediate update isn't necessary if the new topic set isn't    // containing a new topic,    metadata.update(responseWithCurrentTopics(), now);    metadata.add("new-topic");    assertEquals(metadataExpireMs, metadata.timeToNextUpdate(now));    // If the new set of topics containing a new topic then it should allow immediate update.    metadata.add("another-new-topic");    assertEquals(0, metadata.timeToNextUpdate(now));}
public void kafkatest_f6621_0() throws Exception
{    testAppendLarge(CompressionType.GZIP);}
public void kafkatest_f6622_0() throws Exception
{    testAppendLarge(CompressionType.NONE);}
public void kafkatest_f6631_0() throws Exception
{    // Next check time will use lingerMs since this test won't trigger any retries/backoff    int lingerMs = 10;    // test case assumes that the records do not fill the batch completely    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    // Just short of going over the limit so we trigger linger time    int appends = expectedNumAppends(batchSize);    // Partition on node1 only    for (int i = 0; i < appends; i++) accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    RecordAccumulator.ReadyCheckResult result = accum.ready(cluster, time.milliseconds());    assertEquals("No nodes should be ready.", 0, result.readyNodes.size());    assertEquals("Next check time should be the linger time", lingerMs, result.nextReadyCheckDelayMs);    time.sleep(lingerMs / 2);    // Add partition on node2 only    for (int i = 0; i < appends; i++) accum.append(tp3, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    result = accum.ready(cluster, time.milliseconds());    assertEquals("No nodes should be ready.", 0, result.readyNodes.size());    assertEquals("Next check time should be defined by node1, half remaining linger time", lingerMs / 2, result.nextReadyCheckDelayMs);    // Add data for another partition on node1, enough to make data sendable immediately    for (int i = 0; i < appends + 1; i++) accum.append(tp2, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    result = accum.ready(cluster, time.milliseconds());    assertEquals("Node1 should be ready", Collections.singleton(node1), result.readyNodes);    // Note this can actually be < linger time because it may use delays from partitions that aren't sendable    // but have leaders with other sendable data.    assertTrue("Next check time should be defined by node2, at most linger time", result.nextReadyCheckDelayMs <= lingerMs);}
public void kafkatest_f6632_0() throws Exception
{    int lingerMs = Integer.MAX_VALUE / 16;    long retryBackoffMs = Integer.MAX_VALUE / 8;    int deliveryTimeoutMs = Integer.MAX_VALUE;    long totalSize = 10 * 1024;    int batchSize = 1024 + DefaultRecordBatch.RECORD_BATCH_OVERHEAD;    String metricGrpName = "producer-metrics";    final RecordAccumulator accum = new RecordAccumulator(logContext, batchSize, CompressionType.NONE, lingerMs, retryBackoffMs, deliveryTimeoutMs, metrics, metricGrpName, time, new ApiVersions(), null, new BufferPool(totalSize, batchSize, metrics, time, metricGrpName));    long now = time.milliseconds();    accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    RecordAccumulator.ReadyCheckResult result = accum.ready(cluster, now + lingerMs + 1);    assertEquals("Node1 should be ready", Collections.singleton(node1), result.readyNodes);    Map<Integer, List<ProducerBatch>> batches = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, now + lingerMs + 1);    assertEquals("Node1 should be the only ready node.", 1, batches.size());    assertEquals("Partition 0 should only have one batch drained.", 1, batches.get(0).size());    // Reenqueue the batch    now = time.milliseconds();    accum.reenqueue(batches.get(0).get(0), now);    // Put message for partition 1 into accumulator    accum.append(tp2, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    result = accum.ready(cluster, now + lingerMs + 1);    assertEquals("Node1 should be ready", Collections.singleton(node1), result.readyNodes);    // tp1 should backoff while tp2 should not    batches = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, now + lingerMs + 1);    assertEquals("Node1 should be the only ready node.", 1, batches.size());    assertEquals("Node1 should only have one batch drained.", 1, batches.get(0).size());    assertEquals("Node1 should only have one batch for partition 1.", tp2, batches.get(0).get(0).topicPartition);    // Partition 0 can be drained after retry backoff    result = accum.ready(cluster, now + retryBackoffMs + 1);    assertEquals("Node1 should be ready", Collections.singleton(node1), result.readyNodes);    batches = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, now + retryBackoffMs + 1);    assertEquals("Node1 should be the only ready node.", 1, batches.size());    assertEquals("Node1 should only have one batch drained.", 1, batches.get(0).size());    assertEquals("Node1 should only have one batch for partition 0.", tp1, batches.get(0).get(0).topicPartition);}
private void kafkatest_f6641_0(int deliveryTimeoutMs) throws InterruptedException
{    int lingerMs = 300;    List<Boolean> muteStates = Arrays.asList(false, true);    Set<Node> readyNodes = null;    List<ProducerBatch> expiredBatches = new ArrayList<>();    // test case assumes that the records do not fill the batch completely    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(deliveryTimeoutMs, batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    // Make the batches ready due to linger. These batches are not in retry    for (Boolean mute : muteStates) {        if (time.milliseconds() < System.currentTimeMillis())            time.setCurrentTimeMs(System.currentTimeMillis());        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);        assertEquals("No partition should be ready.", 0, accum.ready(cluster, time.milliseconds()).readyNodes.size());        time.sleep(lingerMs);        readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;        assertEquals("Our partition's leader should be ready", Collections.singleton(node1), readyNodes);        expiredBatches = accum.expiredBatches(time.milliseconds());        assertEquals("The batch should not expire when just linger has passed", 0, expiredBatches.size());        if (mute)            accum.mutePartition(tp1);        else            accum.unmutePartition(tp1, 0L);        // Advance the clock to expire the batch.        time.sleep(deliveryTimeoutMs - lingerMs);        expiredBatches = accum.expiredBatches(time.milliseconds());        assertEquals("The batch may expire when the partition is muted", 1, expiredBatches.size());        assertEquals("No partitions should be ready.", 0, accum.ready(cluster, time.milliseconds()).readyNodes.size());    }}
public void kafkatest_f6642_0() throws InterruptedException
{    doExpireBatchSingle(3200);}
public void kafkatest_f6651_0() throws InterruptedException
{    int lingerMs = 500;    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    Set<Node> readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    Map<Integer, List<ProducerBatch>> drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertTrue(drained.isEmpty());    // assertTrue(accum.soonToExpireInFlightBatches().isEmpty());    // advanced clock and send one batch out but it should not be included in soon to expire inflight    // batches because batch's expiry is quite far.    time.sleep(lingerMs + 1);    readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertEquals("A batch did not drain after linger", 1, drained.size());    // assertTrue(accum.soonToExpireInFlightBatches().isEmpty());    // Queue another batch and advance clock such that batch expiry time is earlier than request timeout.    accum.append(tp2, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    time.sleep(lingerMs * 4);    // Now drain and check that accumulator picked up the drained batch because its expiry is soon.    readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertEquals("A batch did not drain after linger", 1, drained.size());}
public void kafkatest_f6652_0() throws InterruptedException
{    int lingerMs = 3000;    int rtt = 1000;    int deliveryTimeoutMs = 3200;    Set<Node> readyNodes;    List<ProducerBatch> expiredBatches;    List<Boolean> muteStates = Arrays.asList(false, true);    // test case assumes that the records do not fill the batch completely    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    // Test batches in retry.    for (Boolean mute : muteStates) {        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, 0, false);        time.sleep(lingerMs);        readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;        assertEquals("Our partition's leader should be ready", Collections.singleton(node1), readyNodes);        Map<Integer, List<ProducerBatch>> drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());        assertEquals("There should be only one batch.", 1, drained.get(node1.id()).size());        time.sleep(rtt);        accum.reenqueue(drained.get(node1.id()).get(0), time.milliseconds());        if (mute)            accum.mutePartition(tp1);        else            accum.unmutePartition(tp1, 0L);        // test expiration        time.sleep(deliveryTimeoutMs - rtt);        accum.drain(cluster, Collections.singleton(node1), Integer.MAX_VALUE, time.milliseconds());        expiredBatches = accum.expiredBatches(time.milliseconds());        assertEquals("RecordAccumulator has expired batches if the partition is not muted", mute ? 1 : 0, expiredBatches.size());    }}
private RecordAccumulator kafkatest_f6661_0(int deliveryTimeoutMs, int batchSize, long totalSize, CompressionType type, int lingerMs)
{    long retryBackoffMs = 100L;    String metricGrpName = "producer-metrics";    return new RecordAccumulator(logContext, batchSize, type, lingerMs, retryBackoffMs, deliveryTimeoutMs, metrics, metricGrpName, time, new ApiVersions(), null, new BufferPool(totalSize, batchSize, metrics, time, metricGrpName));}
public void kafkatest_f6662_0()
{    setupWithTransactionState(null);}
public void kafkatest_f6671_0() throws Exception
{    // create a sender with retries = 1    int maxRetries = 1;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    try {        Sender sender = new Sender(logContext, client, metadata, this.accumulator, false, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, null, apiVersions);        // do a successful retry        Future<RecordMetadata> future = accumulator.append(tp0, 0L, "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;        // connect        sender.runOnce();        // send produce request        sender.runOnce();        String id = client.requests().peek().destination();        Node node = new Node(Integer.parseInt(id), "localhost", 0);        assertEquals(1, client.inFlightRequestCount());        assertTrue(client.hasInFlightRequests());        assertEquals(1, sender.inFlightBatches(tp0).size());        assertTrue("Client ready status should be true", client.isReady(node, time.milliseconds()));        client.disconnect(id);        assertEquals(0, client.inFlightRequestCount());        assertFalse(client.hasInFlightRequests());        assertFalse("Client ready status should be false", client.isReady(node, time.milliseconds()));        // the batch is in accumulator.inFlightBatches until it expires        assertEquals(1, sender.inFlightBatches(tp0).size());        // receive error        sender.runOnce();        // reconnect        sender.runOnce();        // resend        sender.runOnce();        assertEquals(1, client.inFlightRequestCount());        assertTrue(client.hasInFlightRequests());        assertEquals(1, sender.inFlightBatches(tp0).size());        long offset = 0;        client.respond(produceResponse(tp0, offset, Errors.NONE, 0));        sender.runOnce();        assertTrue("Request should have retried and completed", future.isDone());        assertEquals(offset, future.get().offset());        assertEquals(0, sender.inFlightBatches(tp0).size());        // do an unsuccessful retry        future = accumulator.append(tp0, 0L, "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;        // send produce request        sender.runOnce();        assertEquals(1, sender.inFlightBatches(tp0).size());        for (int i = 0; i < maxRetries + 1; i++) {            client.disconnect(client.requests().peek().destination());            // receive error            sender.runOnce();            assertEquals(0, sender.inFlightBatches(tp0).size());            // reconnect            sender.runOnce();            // resend            sender.runOnce();            assertEquals(i > 0 ? 0 : 1, sender.inFlightBatches(tp0).size());        }        sender.runOnce();        assertFutureFailure(future, NetworkException.class);        assertEquals(0, sender.inFlightBatches(tp0).size());    } finally {        m.close();    }}
public void kafkatest_f6672_0() throws Exception
{    int maxRetries = 1;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    try {        Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, null, apiVersions);        // Create a two broker cluster, with partition 0 on broker 0 and partition 1 on broker 1        MetadataResponse metadataUpdate1 = TestUtils.metadataUpdateWith(2, Collections.singletonMap("test", 2));        client.prepareMetadataUpdate(metadataUpdate1);        // Send the first message.        TopicPartition tp2 = new TopicPartition("test", 1);        accumulator.append(tp2, 0L, "key1".getBytes(), "value1".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);        // connect        sender.runOnce();        // send produce request        sender.runOnce();        String id = client.requests().peek().destination();        assertEquals(ApiKeys.PRODUCE, client.requests().peek().requestBuilder().apiKey());        Node node = new Node(Integer.parseInt(id), "localhost", 0);        assertEquals(1, client.inFlightRequestCount());        assertTrue(client.hasInFlightRequests());        assertTrue("Client ready status should be true", client.isReady(node, time.milliseconds()));        assertEquals(1, sender.inFlightBatches(tp2).size());        time.sleep(900);        // Now send another message to tp2        accumulator.append(tp2, 0L, "key2".getBytes(), "value2".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);        // Update metadata before sender receives response from broker 0. Now partition 2 moves to broker 0        MetadataResponse metadataUpdate2 = TestUtils.metadataUpdateWith(1, Collections.singletonMap("test", 2));        client.prepareMetadataUpdate(metadataUpdate2);        // Sender should not send the second message to node 0.        assertEquals(1, sender.inFlightBatches(tp2).size());        // receive the response for the previous send, and send the new batch        sender.runOnce();        assertEquals(1, client.inFlightRequestCount());        assertTrue(client.hasInFlightRequests());        assertEquals(1, sender.inFlightBatches(tp2).size());    } finally {        m.close();    }}
public void kafkatest_f6681_0() throws Exception
{    // Send multiple in flight requests, retry them all one at a time, in the correct order.    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    assertEquals(0, transactionManager.sequenceNumber(tp0).longValue());    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    String nodeId = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(nodeId), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    // Send second ProduceRequest    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    // Send third ProduceRequest    Future<RecordMetadata> request3 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertEquals(3, client.inFlightRequestCount());    assertEquals(3, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertFalse(request1.isDone());    assertFalse(request2.isDone());    assertFalse(request3.isDone());    assertTrue(client.isReady(node, time.milliseconds()));    sendIdempotentProducerResponse(0, tp0, Errors.LEADER_NOT_AVAILABLE, -1L);    // receive response 0    sender.runOnce();    // Queue the fourth request, it shouldn't be sent until the first 3 complete.    Future<RecordMetadata> request4 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    assertEquals(2, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(1, tp0, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1L);    // re send request 1, receive response 2    sender.runOnce();    sendIdempotentProducerResponse(2, tp0, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1L);    // receive response 3    sender.runOnce();    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertEquals(1, client.inFlightRequestCount());    // Do nothing, we are reduced to one in flight request during retries.    sender.runOnce();    // the batch for request 4 shouldn't have been drained, and hence the sequence should not have been incremented.    assertEquals(3, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(0, tp0, Errors.NONE, 0L);    // receive response 1    sender.runOnce();    assertEquals(OptionalInt.of(0), transactionManager.lastAckedSequence(tp0));    assertTrue(request1.isDone());    assertEquals(0, request1.get().offset());    assertFalse(client.hasInFlightRequests());    assertEquals(0, sender.inFlightBatches(tp0).size());    // send request 2;    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    sendIdempotentProducerResponse(1, tp0, Errors.NONE, 1L);    // receive response 2    sender.runOnce();    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    assertTrue(request2.isDone());    assertEquals(1, request2.get().offset());    assertFalse(client.hasInFlightRequests());    assertEquals(0, sender.inFlightBatches(tp0).size());    // send request 3    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    sendIdempotentProducerResponse(2, tp0, Errors.NONE, 2L);    // receive response 3, send request 4 since we are out of 'retry' mode.    sender.runOnce();    assertEquals(OptionalInt.of(2), transactionManager.lastAckedSequence(tp0));    assertTrue(request3.isDone());    assertEquals(2, request3.get().offset());    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    sendIdempotentProducerResponse(3, tp0, Errors.NONE, 3L);    // receive response 4    sender.runOnce();    assertEquals(OptionalInt.of(3), transactionManager.lastAckedSequence(tp0));    assertTrue(request4.isDone());    assertEquals(3, request4.get().offset());}
public void kafkatest_f6682_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    assertEquals(0, transactionManager.sequenceNumber(tp0).longValue());    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    String nodeId = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(nodeId), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    // Send second ProduceRequest    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertEquals(2, client.inFlightRequestCount());    assertEquals(2, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertFalse(request1.isDone());    assertFalse(request2.isDone());    assertTrue(client.isReady(node, time.milliseconds()));    sendIdempotentProducerResponse(0, tp0, Errors.MESSAGE_TOO_LARGE, -1L);    // receive response 0, should adjust sequences of future batches.    sender.runOnce();    assertFutureFailure(request1, RecordTooLargeException.class);    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(1, tp0, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1L);    // receive response 1    sender.runOnce();    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertEquals(0, client.inFlightRequestCount());    // resend request 1    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(0, tp0, Errors.NONE, 0L);    // receive response 1    sender.runOnce();    assertEquals(OptionalInt.of(0), transactionManager.lastAckedSequence(tp0));    assertEquals(0, client.inFlightRequestCount());    assertTrue(request1.isDone());    assertEquals(0, request2.get().offset());}
public void kafkatest_f6691_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));    setupWithTransactionState(transactionManager);    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, 10, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // connect and send.    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    Map<TopicPartition, OffsetAndError> responses = new LinkedHashMap<>();    responses.put(tp1, new OffsetAndError(-1, Errors.NOT_LEADER_FOR_PARTITION));    responses.put(tp0, new OffsetAndError(-1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER));    client.respond(produceResponse(responses));    // initiate close    sender.initiateClose();    sender.runOnce();    assertTrue(failedResponse.isDone());    assertFalse("Expected transaction state to be reset upon receiving an OutOfOrderSequenceException", transactionManager.hasProducerId());    TestUtils.waitForCondition(new TestCondition() {        @Override        public boolean conditionMet() {            prepareInitProducerResponse(Errors.NONE, producerId + 1, (short) 1);            sender.runOnce();            return !accumulator.hasUndrained();        }    }, 5000, "Failed to drain batches");}
public boolean kafkatest_f6692_0()
{    prepareInitProducerResponse(Errors.NONE, producerId + 1, (short) 1);    sender.runOnce();    return !accumulator.hasUndrained();}
 void kafkatest_f6701_0(final int expectedSequence, TopicPartition tp, Errors responseError, long responseOffset, long logStartOffset)
{    client.respond(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            ProduceRequest produceRequest = (ProduceRequest) body;            assertTrue(produceRequest.hasIdempotentRecords());            MemoryRecords records = produceRequest.partitionRecordsOrFail().get(tp0);            Iterator<MutableRecordBatch> batchIterator = records.batches().iterator();            RecordBatch firstBatch = batchIterator.next();            assertFalse(batchIterator.hasNext());            assertEquals(expectedSequence, firstBatch.baseSequence());            return true;        }    }, produceResponse(tp, responseOffset, responseError, 0, logStartOffset));}
public boolean kafkatest_f6702_0(AbstractRequest body)
{    ProduceRequest produceRequest = (ProduceRequest) body;    assertTrue(produceRequest.hasIdempotentRecords());    MemoryRecords records = produceRequest.partitionRecordsOrFail().get(tp0);    Iterator<MutableRecordBatch> batchIterator = records.batches().iterator();    RecordBatch firstBatch = batchIterator.next();    assertFalse(batchIterator.hasNext());    assertEquals(expectedSequence, firstBatch.baseSequence());    return true;}
public boolean kafkatest_f6711_0(AbstractRequest body)
{    return body instanceof ProduceRequest && ((ProduceRequest) body).hasIdempotentRecords();}
public void kafkatest_f6712_0() throws InterruptedException
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));    setupWithTransactionState(transactionManager);    int maxRetries = 10;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            if (body instanceof ProduceRequest) {                ProduceRequest request = (ProduceRequest) body;                MemoryRecords records = request.partitionRecordsOrFail().get(tp0);                Iterator<MutableRecordBatch> batchIterator = records.batches().iterator();                assertTrue(batchIterator.hasNext());                RecordBatch batch = batchIterator.next();                assertFalse(batchIterator.hasNext());                assertEquals(0, batch.baseSequence());                assertEquals(producerId, batch.producerId());                assertEquals(0, batch.producerEpoch());                return true;            }            return false;        }    }, produceResponse(tp0, 0, Errors.NONE, 0));    // connect.    sender.runOnce();    // send.    sender.runOnce();    // receive response    sender.runOnce();    assertTrue(responseFuture.isDone());    assertEquals(OptionalInt.of(0), transactionManager.lastAckedSequence(tp0));    assertEquals(1L, (long) transactionManager.sequenceNumber(tp0));}
public void kafkatest_f6721_0() throws InterruptedException
{    long deliveryTimeoutMs = 1500L;    setupWithTransactionState(null, true, null);    // Send first ProduceRequest    accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);    // send request    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    time.sleep(deliveryTimeoutMs / 2);    // Send second ProduceRequest    accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);    // must not send request because the partition is muted    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    // expire the first batch only    time.sleep(deliveryTimeoutMs / 2);    client.respond(produceResponse(tp0, 0L, Errors.NONE, 0, 0L));    // receive response (offset=0)    sender.runOnce();    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());    // Drain the second request only this time    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());}
public void kafkatest_f6722_0() throws Exception
{    long deliverTimeoutMs = 1500L;    setupWithTransactionState(null, false, null);    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // send request    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    time.sleep(deliverTimeoutMs);    Map<TopicPartition, ProduceResponse.PartitionResponse> responseMap = new HashMap<>();    responseMap.put(tp0, new ProduceResponse.PartitionResponse(Errors.NONE, 0L, 0L, 0L));    // return a retriable error    client.respond(produceResponse(tp0, -1, Errors.NOT_LEADER_FOR_PARTITION, -1));    // expire the batch    sender.runOnce();    assertTrue(request1.isDone());    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());    // receive first response and do not reenqueue.    sender.runOnce();    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());    // run again and must not send anything.    sender.runOnce();    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());}
public boolean kafkatest_f6731_0(AbstractRequest body)
{    if (body instanceof EndTxnRequest) {        assertSame(requiredResult, ((EndTxnRequest) body).command());        matched = true;        return true;    } else {        return false;    }}
public ByteBuffer kafkatest_f6732_0(int size, long maxTimeToBlockMs) throws InterruptedException
{    ByteBuffer buffer = super.allocate(size, maxTimeToBlockMs);    allocatedBuffers.put(buffer, Boolean.TRUE);    return buffer;}
private void kafkatest_f6741_0(TransactionManager transactionManager, boolean guaranteeOrder, BufferPool customPool)
{    int deliveryTimeoutMs = 1500;    long totalSize = 1024 * 1024;    String metricGrpName = "producer-metrics";    MetricConfig metricConfig = new MetricConfig().tags(Collections.singletonMap("client-id", CLIENT_ID));    this.metrics = new Metrics(metricConfig, time);    BufferPool pool = (customPool == null) ? new BufferPool(totalSize, batchSize, metrics, time, metricGrpName) : customPool;    this.accumulator = new RecordAccumulator(logContext, batchSize, CompressionType.NONE, 0, 0L, deliveryTimeoutMs, metrics, metricGrpName, time, apiVersions, transactionManager, pool);    this.senderMetricsRegistry = new SenderMetricsRegistry(this.metrics);    this.sender = new Sender(logContext, this.client, this.metadata, this.accumulator, guaranteeOrder, MAX_REQUEST_SIZE, ACKS_ALL, Integer.MAX_VALUE, this.senderMetricsRegistry, this.time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    metadata.add("test");    this.client.updateMetadata(TestUtils.metadataUpdateWith(1, Collections.singletonMap("test", 2)));}
private void kafkatest_f6742_0(Class<? extends RuntimeException> expectedError) throws Exception
{    Future<RecordMetadata> future = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertTrue(future.isDone());    try {        future.get();        fail("Future should have raised " + expectedError.getSimpleName());    } catch (ExecutionException e) {        assertTrue(expectedError.isAssignableFrom(e.getCause().getClass()));    }}
public void kafkatest_f6751_0()
{    Map<String, String> metricTags = new LinkedHashMap<>();    metricTags.put("client-id", CLIENT_ID);    int batchSize = 16 * 1024;    int deliveryTimeoutMs = 3000;    long totalSize = 1024 * 1024;    String metricGrpName = "producer-metrics";    MetricConfig metricConfig = new MetricConfig().tags(metricTags);    this.brokerNode = new Node(0, "localhost", 2211);    this.transactionManager = new TransactionManager(logContext, transactionalId, transactionTimeoutMs, DEFAULT_RETRY_BACKOFF_MS);    Metrics metrics = new Metrics(metricConfig, time);    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(metrics);    this.accumulator = new RecordAccumulator(logContext, batchSize, CompressionType.NONE, 0, 0L, deliveryTimeoutMs, metrics, metricGrpName, time, apiVersions, transactionManager, new BufferPool(totalSize, batchSize, metrics, time, metricGrpName));    this.sender = new Sender(logContext, this.client, this.metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, MAX_RETRIES, senderMetrics, this.time, REQUEST_TIMEOUT, 50, transactionManager, apiVersions);    this.metadata.add("test");    this.client.updateMetadata(TestUtils.metadataUpdateWith(1, singletonMap("test", 2)));}
public void kafkatest_f6752_0() throws Exception
{    long pid = 13131L;    short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    FutureRecordMetadata sendFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(tp0, Errors.NONE);    prepareProduceResponse(Errors.NONE, pid, epoch);    sender.initiateClose();    sender.runOnce();    TransactionalRequestResult result = transactionManager.beginCommit();    sender.runOnce();    prepareEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertTrue(result.isCompleted());    sender.run();    assertTrue(sendFuture.isDone());}
public void kafkatest_f6761_0()
{    long pid = 13131L;    short epoch = 1;    TopicPartition partition = new TopicPartition("foo", 0);    assertFalse(transactionManager.hasOngoingTransaction());    doInitTransactions(pid, epoch);    assertFalse(transactionManager.hasOngoingTransaction());    transactionManager.beginTransaction();    assertTrue(transactionManager.hasOngoingTransaction());    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(partition);    assertTrue(transactionManager.hasOngoingTransaction());    prepareAddPartitionsToTxn(partition, Errors.NONE);    sender.runOnce();    transactionManager.beginCommit();    assertTrue(transactionManager.hasOngoingTransaction());    prepareEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertFalse(transactionManager.hasOngoingTransaction());}
public void kafkatest_f6762_0()
{    long pid = 13131L;    short epoch = 1;    TopicPartition partition = new TopicPartition("foo", 0);    assertFalse(transactionManager.hasOngoingTransaction());    doInitTransactions(pid, epoch);    assertFalse(transactionManager.hasOngoingTransaction());    transactionManager.beginTransaction();    assertTrue(transactionManager.hasOngoingTransaction());    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(partition);    assertTrue(transactionManager.hasOngoingTransaction());    prepareAddPartitionsToTxn(partition, Errors.NONE);    sender.runOnce();    transactionManager.transitionToAbortableError(new KafkaException());    assertTrue(transactionManager.hasOngoingTransaction());    transactionManager.beginAbort();    assertTrue(transactionManager.hasOngoingTransaction());    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    sender.runOnce();    assertFalse(transactionManager.hasOngoingTransaction());}
public void kafkatest_f6771_0()
{    long pid = 13131L;    short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.transitionToFatalError(new KafkaException());    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(new TopicPartition("foo", 0));}
public void kafkatest_f6772_0()
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    transactionManager.transitionToAbortableError(new KafkaException());    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    assertTrue(transactionManager.hasAbortableError());}
public void kafkatest_f6781_0()
{    final long producerId = 13131L;    final short epoch = 1;    ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(producerId, epoch);    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);    ProducerBatch b1 = writeIdempotentBatchWithValue(transactionManager, tp0, "1");    ProducerBatch b2 = writeIdempotentBatchWithValue(transactionManager, tp0, "2");    ProducerBatch b3 = writeIdempotentBatchWithValue(transactionManager, tp0, "3");    ProducerBatch b4 = writeIdempotentBatchWithValue(transactionManager, tp0, "4");    ProducerBatch b5 = writeIdempotentBatchWithValue(transactionManager, tp0, "5");    assertEquals(5, transactionManager.sequenceNumber(tp0).intValue());    // First batch succeeds    long b1AppendTime = time.milliseconds();    ProduceResponse.PartitionResponse b1Response = new ProduceResponse.PartitionResponse(Errors.NONE, 500L, b1AppendTime, 0L);    b1.done(500L, b1AppendTime, null);    transactionManager.handleCompletedBatch(b1, b1Response);    // Second batch fails with a fatal error. Sequence numbers are adjusted by one for remaining    // inflight batches.    ProduceResponse.PartitionResponse b2Response = new ProduceResponse.PartitionResponse(Errors.MESSAGE_TOO_LARGE, -1, -1, 0L);    assertFalse(transactionManager.canRetry(b2Response, b2));    b2.done(-1L, -1L, Errors.MESSAGE_TOO_LARGE.exception());    transactionManager.handleFailedBatch(b2, Errors.MESSAGE_TOO_LARGE.exception(), true);    assertEquals(4, transactionManager.sequenceNumber(tp0).intValue());    assertEquals(1, b3.baseSequence());    assertEquals(2, b4.baseSequence());    assertEquals(3, b5.baseSequence());    // The remaining batches are doomed to fail, but they can be retried. Expected    // sequence numbers should remain the same.    ProduceResponse.PartitionResponse b3Response = new ProduceResponse.PartitionResponse(Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1, -1, 0L);    assertTrue(transactionManager.canRetry(b3Response, b3));    assertEquals(4, transactionManager.sequenceNumber(tp0).intValue());    assertEquals(1, b3.baseSequence());    assertEquals(2, b4.baseSequence());    assertEquals(3, b5.baseSequence());}
public void kafkatest_f6782_0()
{    // This tests a scenario where the producerId is reset while pending requests are still inflight.    // The returned responses should not update internal state.    final long producerId = 13131L;    final short epoch = 1;    ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(producerId, epoch);    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);    ProducerBatch b1 = writeIdempotentBatchWithValue(transactionManager, tp0, "1");    ProducerIdAndEpoch updatedProducerIdAndEpoch = new ProducerIdAndEpoch(producerId + 1, epoch);    transactionManager.resetProducerId();    transactionManager.setProducerIdAndEpoch(updatedProducerIdAndEpoch);    ProducerBatch b2 = writeIdempotentBatchWithValue(transactionManager, tp0, "2");    assertEquals(1, transactionManager.sequenceNumber(tp0).intValue());    ProduceResponse.PartitionResponse b1Response = new ProduceResponse.PartitionResponse(Errors.UNKNOWN_PRODUCER_ID, -1, -1, 400L);    assertFalse(transactionManager.canRetry(b1Response, b1));    transactionManager.handleFailedBatch(b1, Errors.UNKNOWN_PRODUCER_ID.exception(), true);    assertEquals(1, transactionManager.sequenceNumber(tp0).intValue());    assertEquals(b2, transactionManager.nextBatchBySequence(tp0));}
public void kafkatest_f6791_0()
{    transactionManager.initializeTransactions();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    // InitProducerRequest is queued    sender.runOnce();    // FindCoordinator is queued after peeking InitProducerRequest    sender.runOnce();    assertFalse(transactionManager.hasError());    assertNotNull(transactionManager.coordinator(CoordinatorType.TRANSACTION));    client.prepareUnsupportedVersionResponse(body -> {        InitProducerIdRequest initProducerIdRequest = (InitProducerIdRequest) body;        assertEquals(initProducerIdRequest.data.transactionalId(), transactionalId);        assertEquals(initProducerIdRequest.data.transactionTimeoutMs(), transactionTimeoutMs);        return true;    });    // InitProducerRequest is dequeued    sender.runOnce();    assertTrue(transactionManager.hasFatalError());    assertTrue(transactionManager.lastError() instanceof UnsupportedVersionException);}
public void kafkatest_f6792_0()
{    final String consumerGroupId = "consumer";    final long pid = 13131L;    final short epoch = 1;    final TopicPartition tp = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    TransactionalRequestResult sendOffsetsResult = transactionManager.sendOffsetsToTransaction(singletonMap(tp, new OffsetAndMetadata(39L)), consumerGroupId);    prepareAddOffsetsToTxnResponse(Errors.NONE, consumerGroupId, pid, epoch);    // AddOffsetsToTxn Handled, TxnOffsetCommit Enqueued    sender.runOnce();    // FindCoordinator Enqueued    sender.runOnce();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.GROUP, consumerGroupId);    // FindCoordinator Returned    sender.runOnce();    prepareTxnOffsetCommitResponse(consumerGroupId, pid, epoch, singletonMap(tp, Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT));    // TxnOffsetCommit Handled    sender.runOnce();    assertTrue(transactionManager.hasError());    assertTrue(transactionManager.lastError() instanceof UnsupportedForMessageFormatException);    assertTrue(sendOffsetsResult.isCompleted());    assertFalse(sendOffsetsResult.isSuccessful());    assertTrue(sendOffsetsResult.error() instanceof UnsupportedForMessageFormatException);    assertFatalError(UnsupportedForMessageFormatException.class);}
public void kafkatest_f6801_0()
{    final String consumerGroupId = "consumer";    final long pid = 13131L;    final short epoch = 1;    final TopicPartition tp = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    TransactionalRequestResult sendOffsetsResult = transactionManager.sendOffsetsToTransaction(singletonMap(tp, new OffsetAndMetadata(39L)), consumerGroupId);    prepareAddOffsetsToTxnResponse(Errors.NONE, consumerGroupId, pid, epoch);    // AddOffsetsToTxn Handled, TxnOffsetCommit Enqueued    sender.runOnce();    // FindCoordinator Enqueued    sender.runOnce();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.GROUP, consumerGroupId);    // FindCoordinator Returned    sender.runOnce();    prepareTxnOffsetCommitResponse(consumerGroupId, pid, epoch, singletonMap(tp, Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED));    // TxnOffsetCommit Handled    sender.runOnce();    assertTrue(transactionManager.hasError());    assertTrue(transactionManager.lastError() instanceof TransactionalIdAuthorizationException);    assertTrue(sendOffsetsResult.isCompleted());    assertFalse(sendOffsetsResult.isSuccessful());    assertTrue(sendOffsetsResult.error() instanceof TransactionalIdAuthorizationException);    assertFatalError(TransactionalIdAuthorizationException.class);}
public void kafkatest_f6802_0()
{    final long pid = 13131L;    final short epoch = 1;    final TopicPartition tp0 = new TopicPartition("foo", 0);    final TopicPartition tp1 = new TopicPartition("bar", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp1);    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(tp0, Errors.TOPIC_AUTHORIZATION_FAILED);    errors.put(tp1, Errors.OPERATION_NOT_ATTEMPTED);    prepareAddPartitionsToTxn(errors);    sender.runOnce();    assertTrue(transactionManager.hasError());    assertTrue(transactionManager.lastError() instanceof TopicAuthorizationException);    assertFalse(transactionManager.isPartitionPendingAdd(tp0));    assertFalse(transactionManager.isPartitionPendingAdd(tp1));    assertFalse(transactionManager.isPartitionAdded(tp0));    assertFalse(transactionManager.isPartitionAdded(tp1));    assertFalse(transactionManager.hasPartitionsToAdd());    TopicAuthorizationException exception = (TopicAuthorizationException) transactionManager.lastError();    assertEquals(singleton(tp0.topic()), exception.unauthorizedTopics());    assertAbortableError(TopicAuthorizationException.class);}
public void kafkatest_f6811_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    prepareProduceResponse(Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, pid, epoch);    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    // Send AddPartitionsRequest    sender.runOnce();    // Send Produce Request, returns OutOfOrderSequenceException.    sender.runOnce();    TransactionalRequestResult abortResult = transactionManager.beginAbort();    // try to abort    sender.runOnce();    assertTrue(abortResult.isCompleted());    assertTrue(abortResult.isSuccessful());    // make sure we are ready for a transaction now.    assertTrue(transactionManager.isReady());}
public void kafkatest_f6812_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    // Send AddPartitionsRequest    sender.runOnce();    // Send Produce Request    sender.runOnce();    TransactionalRequestResult abortResult = transactionManager.beginAbort();    assertTrue(transactionManager.isAborting());    assertFalse(transactionManager.hasError());    sendProduceResponse(Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, pid, epoch);    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    // receive the produce response    sender.runOnce();    // we do not transition to ABORTABLE_ERROR since we were already aborting    assertTrue(transactionManager.isAborting());    assertFalse(transactionManager.hasError());    // handle the abort    sender.runOnce();    assertTrue(abortResult.isCompleted());    assertTrue(abortResult.isSuccessful());    // make sure we are ready for a transaction now.    assertTrue(transactionManager.isReady());}
public void kafkatest_f6821_0()
{    testRetriableErrorInTxnOffsetCommit(Errors.COORDINATOR_LOAD_IN_PROGRESS);}
private void kafkatest_f6822_0(Errors error)
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    offsets.put(tp0, new OffsetAndMetadata(1));    offsets.put(tp1, new OffsetAndMetadata(1));    final String consumerGroupId = "myconsumergroup";    TransactionalRequestResult addOffsetsResult = transactionManager.sendOffsetsToTransaction(offsets, consumerGroupId);    prepareAddOffsetsToTxnResponse(Errors.NONE, consumerGroupId, pid, epoch);    // send AddOffsetsToTxnResult    sender.runOnce();    // The request should complete only after the TxnOffsetCommit completes.    assertFalse(addOffsetsResult.isCompleted());    Map<TopicPartition, Errors> txnOffsetCommitResponse = new HashMap<>();    txnOffsetCommitResponse.put(tp0, Errors.NONE);    txnOffsetCommitResponse.put(tp1, error);    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.GROUP, consumerGroupId);    prepareTxnOffsetCommitResponse(consumerGroupId, pid, epoch, txnOffsetCommitResponse);    assertNull(transactionManager.coordinator(CoordinatorType.GROUP));    // try to send TxnOffsetCommitRequest, but find we don't have a group coordinator.    sender.runOnce();    // send find coordinator for group request    sender.runOnce();    assertNotNull(transactionManager.coordinator(CoordinatorType.GROUP));    assertTrue(transactionManager.hasPendingOffsetCommits());    // send TxnOffsetCommitRequest request.    sender.runOnce();    // The TxnOffsetCommit failed.    assertTrue(transactionManager.hasPendingOffsetCommits());    // We should only be done after both RPCs complete successfully.    assertFalse(addOffsetsResult.isCompleted());    txnOffsetCommitResponse.put(tp1, Errors.NONE);    prepareTxnOffsetCommitResponse(consumerGroupId, pid, epoch, txnOffsetCommitResponse);    // Send TxnOffsetCommitRequest again.    sender.runOnce();    assertTrue(addOffsetsResult.isCompleted());    assertTrue(addOffsetsResult.isSuccessful());}
public void kafkatest_f6831_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    assertFalse(transactionManager.transactionContainsPartition(tp0));    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    // send addPartitions.    sender.runOnce();    // Check that only addPartitions was sent.    assertTrue(transactionManager.transactionContainsPartition(tp0));    assertTrue(transactionManager.isSendToPartitionAllowed(tp0));    assertFalse(responseFuture.isDone());    // Sleep 10 seconds to make sure that the batches in the queue would be expired if they can't be drained.    time.sleep(10000);    // Disconnect the target node for the pending produce request. This will ensure that sender will try to    // expire the batch.    Node clusterNode = metadata.fetch().nodes().get(0);    client.disconnect(clusterNode.idString());    client.blackout(clusterNode, 100);    // We should try to flush the produce, but expire it instead without sending anything.    sender.runOnce();    assertTrue(responseFuture.isDone());    try {        // make sure the produce was expired.        responseFuture.get();        fail("Expected to get a TimeoutException since the queued ProducerBatch should have been expired");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    assertTrue(transactionManager.hasAbortableError());}
public void kafkatest_f6832_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp1);    Future<RecordMetadata> firstBatchResponse = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    Future<RecordMetadata> secondBatchResponse = accumulator.append(tp1, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(firstBatchResponse.isDone());    assertFalse(secondBatchResponse.isDone());    Map<TopicPartition, Errors> partitionErrors = new HashMap<>();    partitionErrors.put(tp0, Errors.NONE);    partitionErrors.put(tp1, Errors.NONE);    prepareAddPartitionsToTxn(partitionErrors);    assertFalse(transactionManager.transactionContainsPartition(tp0));    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    // send addPartitions.    sender.runOnce();    // Check that only addPartitions was sent.    assertTrue(transactionManager.transactionContainsPartition(tp0));    assertTrue(transactionManager.transactionContainsPartition(tp1));    assertTrue(transactionManager.isSendToPartitionAllowed(tp1));    assertTrue(transactionManager.isSendToPartitionAllowed(tp1));    assertFalse(firstBatchResponse.isDone());    assertFalse(secondBatchResponse.isDone());    // Sleep 10 seconds to make sure that the batches in the queue would be expired if they can't be drained.    time.sleep(10000);    // Disconnect the target node for the pending produce request. This will ensure that sender will try to    // expire the batch.    Node clusterNode = metadata.fetch().nodes().get(0);    client.disconnect(clusterNode.idString());    client.blackout(clusterNode, 100);    // We should try to flush the produce, but expire it instead without sending anything.    sender.runOnce();    assertTrue(firstBatchResponse.isDone());    assertTrue(secondBatchResponse.isDone());    try {        // make sure the produce was expired.        firstBatchResponse.get();        fail("Expected to get a TimeoutException since the queued ProducerBatch should have been expired");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    try {        // make sure the produce was expired.        secondBatchResponse.get();        fail("Expected to get a TimeoutException since the queued ProducerBatch should have been expired");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    assertTrue(transactionManager.hasAbortableError());}
public void kafkatest_f6841_0() throws InterruptedException
{    verifyCommitOrAbortTranscationRetriable(TransactionResult.ABORT, TransactionResult.COMMIT);}
private void kafkatest_f6842_0(TransactionResult firstTransactionResult, TransactionResult retryTransactionResult) throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false);    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    prepareProduceResponse(Errors.NONE, pid, epoch);    // send addPartitions.    sender.runOnce();    // send produce request.    sender.runOnce();    TransactionalRequestResult result = firstTransactionResult == TransactionResult.COMMIT ? transactionManager.beginCommit() : transactionManager.beginAbort();    prepareEndTxnResponse(Errors.NONE, firstTransactionResult, pid, epoch, true);    sender.runOnce();    assertFalse(result.isCompleted());    try {        result.await(MAX_BLOCK_TIMEOUT, TimeUnit.MILLISECONDS);        fail("Should have raised TimeoutException");    } catch (TimeoutException e) {    }    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    sender.runOnce();    TransactionalRequestResult retryResult = retryTransactionResult == TransactionResult.COMMIT ? transactionManager.beginCommit() : transactionManager.beginAbort();    // check if cached result is reused.    assertEquals(retryResult, result);    prepareEndTxnResponse(Errors.NONE, retryTransactionResult, pid, epoch, false);    sender.runOnce();    assertTrue(retryResult.isCompleted());    assertFalse(transactionManager.hasOngoingTransaction());}
private void kafkatest_f6851_0(Errors error, final TopicPartition topicPartition, final short epoch, final long pid)
{    client.prepareResponse(addPartitionsRequestMatcher(topicPartition, epoch, pid), new AddPartitionsToTxnResponse(0, singletonMap(topicPartition, error)));}
private void kafkatest_f6852_0(Errors error, final TopicPartition topicPartition, final short epoch, final long pid)
{    client.respond(addPartitionsRequestMatcher(topicPartition, epoch, pid), new AddPartitionsToTxnResponse(0, singletonMap(topicPartition, error)));}
private void kafkatest_f6861_0(long pid, short epoch)
{    transactionManager.initializeTransactions();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    // find coordinator    sender.runOnce();    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    prepareInitPidResponse(Errors.NONE, false, pid, epoch);    // get pid.    sender.runOnce();    assertTrue(transactionManager.hasProducerId());}
private void kafkatest_f6862_0(Class<? extends RuntimeException> cause)
{    try {        transactionManager.beginCommit();        fail("Should have raised " + cause.getSimpleName());    } catch (KafkaException e) {        assertTrue(cause.isAssignableFrom(e.getCause().getClass()));        assertTrue(transactionManager.hasError());    }    assertTrue(transactionManager.hasError());    transactionManager.beginAbort();    assertFalse(transactionManager.hasError());}
public void kafkatest_f6871_0()
{    try {        Properties props = new Properties();        // test with client ID assigned by KafkaProducer        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");        props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, MockProducerInterceptor.class.getName());        props.setProperty(MockProducerInterceptor.APPEND_STRING_PROP, "something");        KafkaProducer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());        assertEquals(1, MockProducerInterceptor.INIT_COUNT.get());        assertEquals(0, MockProducerInterceptor.CLOSE_COUNT.get());        // Cluster metadata will only be updated on calling onSend.        Assert.assertNull(MockProducerInterceptor.CLUSTER_META.get());        producer.close();        assertEquals(1, MockProducerInterceptor.INIT_COUNT.get());        assertEquals(1, MockProducerInterceptor.CLOSE_COUNT.get());    } finally {        // cleanup since we are using mutable static variables in MockProducerInterceptor        MockProducerInterceptor.resetCounters();    }}
public void kafkatest_f6872_0()
{    try {        Properties props = new Properties();        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");        MockPartitioner.resetCounters();        props.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG, MockPartitioner.class.getName());        KafkaProducer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());        assertEquals(1, MockPartitioner.INIT_COUNT.get());        assertEquals(0, MockPartitioner.CLOSE_COUNT.get());        producer.close();        assertEquals(1, MockPartitioner.INIT_COUNT.get());        assertEquals(1, MockPartitioner.CLOSE_COUNT.get());    } finally {        // cleanup since we are using mutable static variables in MockPartitioner        MockPartitioner.resetCounters();    }}
public void kafkatest_f6881_0() throws Exception
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    configs.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, 60000);    // Create a record with a partition higher than the initial (outdated) partition range    ProducerRecord<String, String> record = new ProducerRecord<>(topic, 2, null, "value");    ProducerMetadata metadata = mock(ProducerMetadata.class);    MockTime mockTime = new MockTime();    when(metadata.fetch()).thenReturn(onePartitionCluster, onePartitionCluster, threePartitionCluster);    KafkaProducer<String, String> producer = new KafkaProducer<String, String>(configs, new StringSerializer(), new StringSerializer(), metadata, new MockClient(Time.SYSTEM, metadata), null, mockTime) {        @Override        Sender newSender(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata) {            // give Sender its own Metadata instance so that we can isolate Metadata calls from KafkaProducer            return super.newSender(logContext, kafkaClient, newMetadata(0, 100_000));        }    };    // One request update if metadata is available but outdated for the given record    producer.send(record);    verify(metadata, times(2)).requestUpdate();    verify(metadata, times(2)).awaitUpdate(anyInt(), anyLong());    verify(metadata, times(3)).fetch();    producer.close(Duration.ofMillis(0));}
 Sender kafkatest_f6882_0(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata)
{    // give Sender its own Metadata instance so that we can isolate Metadata calls from KafkaProducer    return super.newSender(logContext, kafkaClient, newMetadata(0, 100_000));}
public void kafkatest_f6891_0()
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    configs.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, "1");    String topic = "topic";    ProducerRecord<String, String> record = new ProducerRecord<>(topic, "value");    ProducerMetadata metadata = newMetadata(0, 90000);    metadata.add(topic);    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap(topic, 1));    metadata.update(initialUpdateResponse, Time.SYSTEM.milliseconds());    // it is safe to suppress, since this is a mock class    @SuppressWarnings("unchecked")    ProducerInterceptors<String, String> interceptors = mock(ProducerInterceptors.class);    KafkaProducer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, null, interceptors, Time.SYSTEM);    when(interceptors.onSend(any())).then(invocation -> invocation.getArgument(0));    producer.send(record);    verify(interceptors).onSend(record);    verify(interceptors).onSendError(eq(record), notNull(), notNull());    producer.close(Duration.ofMillis(0));}
public void kafkatest_f6892_0()
{    Properties props = new Properties();    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");    try (KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(props, new ByteArraySerializer(), new ByteArraySerializer())) {        assertThrows(NullPointerException.class, () -> producer.partitionsFor(null));    }}
public void kafkatest_f6901_0() throws InterruptedException
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");    configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "this-is-a-transactional-id");    Time time = new MockTime();    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap("testTopic", 1));    ProducerMetadata metadata = newMetadata(0, Long.MAX_VALUE);    metadata.update(initialUpdateResponse, time.milliseconds());    MockClient client = new MockClient(time, metadata);    Producer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, client, null, time);    ExecutorService executorService = Executors.newSingleThreadExecutor();    CountDownLatch assertionDoneLatch = new CountDownLatch(1);    client.prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, host1));    executorService.submit(() -> {        assertThrows(KafkaException.class, producer::initTransactions);        assertionDoneLatch.countDown();    });    client.waitForRequests(1, 2000);    producer.close(Duration.ofMillis(1000));    assertionDoneLatch.await(5000, TimeUnit.MILLISECONDS);}
public void kafkatest_f6902_0() throws InterruptedException
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");    configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "this-is-a-transactional-id");    Time time = new MockTime();    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap("testTopic", 1));    ProducerMetadata metadata = newMetadata(0, Long.MAX_VALUE);    metadata.update(initialUpdateResponse, time.milliseconds());    MockClient client = new MockClient(time, metadata);    Producer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, client, null, time);    ExecutorService executorService = Executors.newSingleThreadExecutor();    CountDownLatch assertionDoneLatch = new CountDownLatch(1);    client.prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, host1));    executorService.submit(() -> {        assertThrows(KafkaException.class, producer::initTransactions);        assertionDoneLatch.countDown();    });    client.waitForRequests(1, 2000);    producer.close(Duration.ofMillis(1000));    assertionDoneLatch.await(5000, TimeUnit.MILLISECONDS);}
public void kafkatest_f6911_0()
{    buildMockProducer(true);    producer.beginTransaction();}
public void kafkatest_f6912_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    assertTrue(producer.transactionInFlight());}
public void kafkatest_f6921_0()
{    buildMockProducer(true);    producer.initTransactions();    try {        producer.abortTransaction();        fail("Should have thrown as producer has no open transaction");    } catch (IllegalStateException e) {    }}
public void kafkatest_f6922_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    producer.abortTransaction();    assertFalse(producer.transactionInFlight());    assertTrue(producer.transactionAborted());    assertFalse(producer.transactionCommitted());}
public void kafkatest_f6931_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    producer.send(record1);    producer.send(record2);    producer.abortTransaction();    assertTrue(producer.history().isEmpty());    producer.beginTransaction();    producer.commitTransaction();    assertTrue(producer.history().isEmpty());}
public void kafkatest_f6932_0() throws Exception
{    buildMockProducer(false);    producer.initTransactions();    producer.beginTransaction();    Future<RecordMetadata> md1 = producer.send(record1);    assertFalse(md1.isDone());    producer.abortTransaction();    assertTrue(md1.isDone());}
public void kafkatest_f6941_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    String group = "g";    Map<TopicPartition, OffsetAndMetadata> groupCommit = new HashMap<TopicPartition, OffsetAndMetadata>() {        {            put(new TopicPartition(topic, 0), new OffsetAndMetadata(42L, null));            put(new TopicPartition(topic, 1), new OffsetAndMetadata(73L, null));        }    };    producer.sendOffsetsToTransaction(groupCommit, group);    producer.commitTransaction();    producer.beginTransaction();    producer.abortTransaction();    Map<String, Map<TopicPartition, OffsetAndMetadata>> expectedResult = new HashMap<>();    expectedResult.put(group, groupCommit);    assertThat(producer.consumerGroupOffsetsHistory(), equalTo(Collections.singletonList(expectedResult)));}
public void kafkatest_f6942_0()
{    buildMockProducer(true);    producer.close();    try {        producer.initTransactions();        fail("Should have thrown as producer is already closed");    } catch (IllegalStateException e) {    }}
public void kafkatest_f6951_0()
{    buildMockProducer(true);    producer.send(record1);    assertTrue(producer.flushed());}
public void kafkatest_f6952_0()
{    buildMockProducer(false);    producer.send(record1);    assertFalse(producer.flushed());}
public void kafkatest_f6961_0() throws Exception
{    FutureRecordMetadata future = new FutureRecordMetadata(asyncRequest(baseOffset, new CorruptRecordException(), 50L), relOffset, RecordBatch.NO_TIMESTAMP, 0L, 0, 0, Time.SYSTEM);    future.get();}
public void kafkatest_f6962_0() throws Exception
{    FutureRecordMetadata future = new FutureRecordMetadata(asyncRequest(baseOffset, null, 50L), relOffset, RecordBatch.NO_TIMESTAMP, 0L, 0, 0, Time.SYSTEM);    assertEquals(baseOffset + relOffset, future.get().offset());}
public void kafkatest_f6971_0()
{    assertEquals(ACL1, ACL1);    final AclBinding acl1Copy = new AclBinding(new ResourcePattern(ResourceType.TOPIC, "mytopic", PatternType.LITERAL), new AccessControlEntry("User:ANONYMOUS", "", AclOperation.ALL, AclPermissionType.ALLOW));    assertEquals(ACL1, acl1Copy);    assertEquals(acl1Copy, ACL1);    assertEquals(ACL2, ACL2);    assertNotEquals(ACL1, ACL2);    assertNotEquals(ACL2, ACL1);    assertTrue(AclBindingFilter.ANY.matches(ACL1));    assertNotEquals(AclBindingFilter.ANY, ACL1);    assertTrue(AclBindingFilter.ANY.matches(ACL2));    assertNotEquals(AclBindingFilter.ANY, ACL2);    assertTrue(AclBindingFilter.ANY.matches(ACL3));    assertNotEquals(AclBindingFilter.ANY, ACL3);    assertEquals(AclBindingFilter.ANY, AclBindingFilter.ANY);    assertTrue(ANY_ANONYMOUS.matches(ACL1));    assertNotEquals(ANY_ANONYMOUS, ACL1);    assertFalse(ANY_ANONYMOUS.matches(ACL2));    assertNotEquals(ANY_ANONYMOUS, ACL2);    assertTrue(ANY_ANONYMOUS.matches(ACL3));    assertNotEquals(ANY_ANONYMOUS, ACL3);    assertFalse(ANY_DENY.matches(ACL1));    assertFalse(ANY_DENY.matches(ACL2));    assertTrue(ANY_DENY.matches(ACL3));    assertTrue(ANY_MYTOPIC.matches(ACL1));    assertTrue(ANY_MYTOPIC.matches(ACL2));    assertFalse(ANY_MYTOPIC.matches(ACL3));    assertTrue(ANY_ANONYMOUS.matches(UNKNOWN_ACL));    assertTrue(ANY_DENY.matches(UNKNOWN_ACL));    assertEquals(UNKNOWN_ACL, UNKNOWN_ACL);    assertFalse(ANY_MYTOPIC.matches(UNKNOWN_ACL));}
public void kafkatest_f6972_0()
{    assertFalse(ACL1.isUnknown());    assertFalse(ACL2.isUnknown());    assertFalse(ACL3.isUnknown());    assertFalse(ANY_ANONYMOUS.isUnknown());    assertFalse(ANY_DENY.isUnknown());    assertFalse(ANY_MYTOPIC.isUnknown());    assertTrue(UNKNOWN_ACL.isUnknown());}
public void kafkatest_f6981_0() throws Exception
{    for (AclOperationTestInfo info : INFOS) {        assertEquals("AclOperation.fromString(" + info.name + ") was supposed to be " + info.operation, info.operation, AclOperation.fromString(info.name));    }    assertEquals(AclOperation.UNKNOWN, AclOperation.fromString("something"));}
public void kafkatest_f6982_0() throws Exception
{    assertEquals(INFOS.length, AclOperation.values().length);    for (int i = 0; i < INFOS.length; i++) {        assertEquals(INFOS[i].operation, AclOperation.values()[i]);    }}
public void kafkatest_f6991_0()
{    assertFalse(new ResourcePatternFilter(TOPIC, "NAME", LITERAL).matches(new ResourcePattern(TOPIC, "Name", LITERAL)));}
public void kafkatest_f6992_0()
{    assertFalse(new ResourcePatternFilter(TOPIC, "Name", LITERAL).matches(new ResourcePattern(TOPIC, "Name", PREFIXED)));}
public void kafkatest_f7001_0()
{    assertTrue(new ResourcePatternFilter(TOPIC, "*", LITERAL).matches(new ResourcePattern(TOPIC, "*", LITERAL)));}
public void kafkatest_f7002_0()
{    assertFalse(new ResourcePatternFilter(TOPIC, "Name", LITERAL).matches(new ResourcePattern(TOPIC, "*", LITERAL)));}
public void kafkatest_f7011_0()
{    new ResourcePattern(ResourceType.ANY, "name", PatternType.LITERAL);}
public void kafkatest_f7012_0()
{    new ResourcePattern(ResourceType.TOPIC, "name", PatternType.MATCH);}
public void kafkatest_f7021_0()
{    AbstractConfig conf;    ConfigDef configDef = new ConfigDef().define("a", Type.LIST, "", new ConfigDef.NonNullValidator(), Importance.HIGH, "doc");    conf = new AbstractConfig(configDef, Collections.emptyMap());    assertEquals(Collections.emptyList(), conf.getList("a"));    conf = new AbstractConfig(configDef, Collections.singletonMap("a", ""));    assertEquals(Collections.emptyList(), conf.getList("a"));    conf = new AbstractConfig(configDef, Collections.singletonMap("a", "b,c,d"));    assertEquals(Arrays.asList("b", "c", "d"), conf.getList("a"));}
public void kafkatest_f7022_0()
{    Properties props = new Properties();    props.put("foo.bar", "abc");    props.put("setting", "def");    TestConfig config = new TestConfig(props);    Map<String, Object> originalsWithPrefix = config.originalsWithPrefix("foo.");    assertTrue(config.unused().contains("foo.bar"));    originalsWithPrefix.get("bar");    assertFalse(config.unused().contains("foo.bar"));    Map<String, Object> expected = new HashMap<>();    expected.put("bar", "abc");    assertEquals(expected, originalsWithPrefix);}
public Map<String, ?> kafkatest_f7031_0(Map<?, ?> props)
{    for (Map.Entry<?, ?> entry : props.entrySet()) {        if (!(entry.getKey() instanceof String))            throw new ConfigException(entry.getKey().toString(), entry.getValue(), "Key must be a string.");    }    return (Map<String, ?>) props;}
public void kafkatest_f7032_0()
{    Properties props = new Properties();    // Test Case: Valid Test Case for ConfigProviders as part of config.properties    props.put("config.providers", "file");    props.put("config.providers.file.class", MockFileConfigProvider.class.getName());    props.put("prefix.ssl.truststore.location.number", 5);    props.put("sasl.kerberos.service.name", "service name");    props.put("sasl.kerberos.key", "${file:/usr/kerberos:key}");    props.put("sasl.kerberos.password", "${file:/usr/kerberos:password}");    TestIndirectConfigResolution config = new TestIndirectConfigResolution(props);    assertEquals(config.originals().get("sasl.kerberos.key"), "testKey");    assertEquals(config.originals().get("sasl.kerberos.password"), "randomPassword");    assertEquals(config.originals().get("prefix.ssl.truststore.location.number"), 5);    assertEquals(config.originals().get("sasl.kerberos.service.name"), "service name");}
 void kafkatest_f7041_0(Class<?> expectedClassPropClass, Class<?>... expectedListPropClasses)
{    assertEquals(expectedClassPropClass, getConfiguredInstance("class.prop", MetricsReporter.class).getClass());    List<?> list = getConfiguredInstances("list.prop", MetricsReporter.class);    for (int i = 0; i < list.size(); i++) assertEquals(expectedListPropClasses[i], list.get(i).getClass());}
 static void kafkatest_f7042_0()
{    ClassTestConfig testConfig1 = new ClassTestConfig(RESTRICTED_CLASS, Arrays.asList(VISIBLE_CLASS, RESTRICTED_CLASS));    testConfig1.checkInstances(RESTRICTED_CLASS, VISIBLE_CLASS, RESTRICTED_CLASS);    ClassTestConfig testConfig2 = new ClassTestConfig(RESTRICTED_CLASS.getName(), Arrays.asList(VISIBLE_CLASS.getName(), RESTRICTED_CLASS.getName()));    testConfig2.checkInstances(RESTRICTED_CLASS, VISIBLE_CLASS, RESTRICTED_CLASS);    ClassTestConfig testConfig3 = new ClassTestConfig(RESTRICTED_CLASS.getName(), VISIBLE_CLASS.getName() + "," + RESTRICTED_CLASS.getName());    testConfig3.checkInstances(RESTRICTED_CLASS, VISIBLE_CLASS, RESTRICTED_CLASS);}
public void kafkatest_f7051_0()
{    testBadInputs(Type.INT, "hello", "42.5", 42.5, Long.MAX_VALUE, Long.toString(Long.MAX_VALUE), new Object());    testBadInputs(Type.LONG, "hello", "42.5", Long.toString(Long.MAX_VALUE) + "00", new Object());    testBadInputs(Type.DOUBLE, "hello", new Object());    testBadInputs(Type.STRING, new Object());    testBadInputs(Type.LIST, 53, new Object());    testBadInputs(Type.BOOLEAN, "hello", "truee", "fals");    testBadInputs(Type.CLASS, "ClassDoesNotExist");}
private void kafkatest_f7052_0(Type type, Object... values)
{    for (Object value : values) {        Map<String, Object> m = new HashMap<String, Object>();        m.put("name", value);        ConfigDef def = new ConfigDef().define("name", type, Importance.HIGH, "docs");        try {            def.parse(m);            fail("Expected a config exception on bad input for value " + value);        } catch (ConfigException e) {        // this is good        }    }}
public void kafkatest_f7061_0()
{    Map<String, ConfigValue> expected = new HashMap<>();    String errorMessageB = "Missing required configuration \"b\" which has no default value.";    String errorMessageC = "Missing required configuration \"c\" which has no default value.";    ConfigValue configA = new ConfigValue("a", 1, Arrays.<Object>asList(1, 2, 3), Collections.<String>emptyList());    ConfigValue configB = new ConfigValue("b", null, Arrays.<Object>asList(4, 5), Arrays.asList(errorMessageB, errorMessageB));    ConfigValue configC = new ConfigValue("c", null, Arrays.<Object>asList(4, 5), Arrays.asList(errorMessageC));    ConfigValue configD = new ConfigValue("d", 10, Arrays.<Object>asList(1, 2, 3), Collections.<String>emptyList());    expected.put("a", configA);    expected.put("b", configB);    expected.put("c", configC);    expected.put("d", configD);    ConfigDef def = new ConfigDef().define("a", Type.INT, Importance.HIGH, "docs", "group", 1, Width.SHORT, "a", Arrays.asList("b", "c"), new IntegerRecommender(false)).define("b", Type.INT, Importance.HIGH, "docs", "group", 2, Width.SHORT, "b", new IntegerRecommender(true)).define("c", Type.INT, Importance.HIGH, "docs", "group", 3, Width.SHORT, "c", new IntegerRecommender(true)).define("d", Type.INT, Importance.HIGH, "docs", "group", 4, Width.SHORT, "d", Arrays.asList("b"), new IntegerRecommender(false));    Map<String, String> props = new HashMap<>();    props.put("a", "1");    props.put("d", "10");    List<ConfigValue> configs = def.validate(props);    for (ConfigValue config : configs) {        String name = config.name();        ConfigValue expectedConfig = expected.get(name);        assertEquals(expectedConfig, config);    }}
public void kafkatest_f7062_0()
{    Map<String, ConfigValue> expected = new HashMap<>();    String errorMessageB = "Missing required configuration \"b\" which has no default value.";    String errorMessageC = "Missing required configuration \"c\" which has no default value.";    String errorMessageD = "d is referred in the dependents, but not defined.";    ConfigValue configA = new ConfigValue("a", 1, Arrays.<Object>asList(1, 2, 3), Collections.<String>emptyList());    ConfigValue configB = new ConfigValue("b", null, Arrays.<Object>asList(4, 5), Arrays.asList(errorMessageB));    ConfigValue configC = new ConfigValue("c", null, Arrays.<Object>asList(4, 5), Arrays.asList(errorMessageC));    ConfigValue configD = new ConfigValue("d", null, Collections.emptyList(), Arrays.asList(errorMessageD));    configD.visible(false);    expected.put("a", configA);    expected.put("b", configB);    expected.put("c", configC);    expected.put("d", configD);    ConfigDef def = new ConfigDef().define("a", Type.INT, Importance.HIGH, "docs", "group", 1, Width.SHORT, "a", Arrays.asList("b", "c", "d"), new IntegerRecommender(false)).define("b", Type.INT, Importance.HIGH, "docs", "group", 2, Width.SHORT, "b", new IntegerRecommender(true)).define("c", Type.INT, Importance.HIGH, "docs", "group", 3, Width.SHORT, "c", new IntegerRecommender(true));    Map<String, String> props = new HashMap<>();    props.put("a", "1");    List<ConfigValue> configs = def.validate(props);    for (ConfigValue config : configs) {        String name = config.name();        ConfigValue expectedConfig = expected.get(name);        assertEquals(expectedConfig, config);    }}
public boolean kafkatest_f7071_0(String name, Map<String, Object> parsedConfig)
{    return true;}
private void kafkatest_f7072_0(Type type, Validator validator, Object defaultVal, Object[] okValues, Object[] badValues)
{    ConfigDef def = new ConfigDef().define("name", type, defaultVal, validator, Importance.HIGH, "docs");    for (Object value : okValues) {        Map<String, Object> m = new HashMap<String, Object>();        m.put("name", value);        def.parse(m);    }    for (Object value : badValues) {        Map<String, Object> m = new HashMap<String, Object>();        m.put("name", value);        try {            def.parse(m);            fail("Expected a config exception due to invalid value " + value);        } catch (ConfigException e) {        // this is good        }    }}
public void kafkatest_f7081_0()
{    assertEquals(Password.HIDDEN, ConfigDef.convertToString(new Password("foobar"), Type.PASSWORD));    assertEquals("foobar", ConfigDef.convertToString("foobar", Type.PASSWORD));    assertNull(ConfigDef.convertToString(null, Type.PASSWORD));}
public void kafkatest_f7082_0()
{    assertEquals("a,bc,d", ConfigDef.convertToString(Arrays.asList("a", "bc", "d"), Type.LIST));    assertNull(ConfigDef.convertToString(null, Type.LIST));}
public void kafkatest_f7091_0() throws Exception
{    ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, "${test:testPath:testKey}"));    Map<String, String> data = result.data();    Map<String, Long> ttls = result.ttls();    assertEquals(TEST_RESULT, data.get(MY_KEY));    assertTrue(ttls.isEmpty());}
public void kafkatest_f7092_0() throws Exception
{    ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, "${test:testPath:testKeyWithTTL}"));    Map<String, String> data = result.data();    Map<String, Long> ttls = result.ttls();    assertEquals(TEST_RESULT_WITH_TTL, data.get(MY_KEY));    assertEquals(1L, ttls.get(TEST_PATH).longValue());}
public void kafkatest_f7103_0()
{    configProvider = new TestFileConfigProvider();}
public void kafkatest_f7104_0() throws Exception
{    ConfigData configData = configProvider.get("dummy");    Map<String, String> result = new HashMap<>();    result.put("testKey", "testResult");    result.put("testKey2", "testResult2");    assertEquals(result, configData.data());    assertEquals(null, configData.ttl());}
public void kafkatest_f7113_0(Map<String, ?> configs)
{    this.vaultConfigs = configs;    configured = true;}
public boolean kafkatest_f7114_0()
{    return configured;}
public void kafkatest_f7123_0()
{    Map<Object, Object> props = new HashMap<>();    props.put(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS, "901");    new ConfigDef().withClientSaslSupport().parse(props);}
public void kafkatest_f7124_0()
{    Map<Object, Object> props = new HashMap<>();    props.put(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS, "-1");    new ConfigDef().withClientSaslSupport().parse(props);}
public void kafkatest_f7133_0()
{    new RecordHeaders().add(null);}
private int kafkatest_f7134_0(Headers headers)
{    int count = 0;    Iterator<Header> headerIterator = headers.iterator();    while (headerIterator.hasNext()) {        headerIterator.next();        count++;    }    return count;}
public void kafkatest_f7143_0()
{    PartitionStates<String> states = new PartitionStates<>();    LinkedHashMap<TopicPartition, String> map = createMap();    states.set(map);    states.remove(new TopicPartition("foo", 2));    LinkedHashMap<TopicPartition, String> expected = new LinkedHashMap<>();    expected.put(new TopicPartition("foo", 0), "foo 0");    expected.put(new TopicPartition("blah", 2), "blah 2");    expected.put(new TopicPartition("blah", 1), "blah 1");    expected.put(new TopicPartition("baz", 2), "baz 2");    expected.put(new TopicPartition("baz", 3), "baz 3");    checkState(states, expected);    states.remove(new TopicPartition("blah", 1));    expected = new LinkedHashMap<>();    expected.put(new TopicPartition("foo", 0), "foo 0");    expected.put(new TopicPartition("blah", 2), "blah 2");    expected.put(new TopicPartition("baz", 2), "baz 2");    expected.put(new TopicPartition("baz", 3), "baz 3");    checkState(states, expected);    states.remove(new TopicPartition("baz", 3));    expected = new LinkedHashMap<>();    expected.put(new TopicPartition("foo", 0), "foo 0");    expected.put(new TopicPartition("blah", 2), "blah 2");    expected.put(new TopicPartition("baz", 2), "baz 2");    checkState(states, expected);}
public void kafkatest_f7144_0()
{    String maxLengthString = TestUtils.randomString(249);    String[] validTopicNames = { "valid", "TOPIC", "nAmEs", "ar6", "VaL1d", "_0-9_.", "...", maxLengthString };    for (String topicName : validTopicNames) {        Topic.validate(topicName);    }}
public void kafkatest_f7153_0()
{    try {        T value = future.get();        assertEquals(expected, value);    } catch (Throwable testException) {        this.testException = testException;    }}
public void kafkatest_f7154_0() throws Exception
{    final int numThreads = 5;    final List<KafkaFutureImpl<Integer>> futures = new ArrayList<>();    for (int i = 0; i < numThreads; i++) {        futures.add(new KafkaFutureImpl<>());    }    KafkaFuture<Void> allFuture = KafkaFuture.allOf(futures.toArray(new KafkaFuture[0]));    final List<CompleterThread> completerThreads = new ArrayList<>();    final List<WaiterThread> waiterThreads = new ArrayList<>();    for (int i = 0; i < numThreads; i++) {        completerThreads.add(new CompleterThread<>(futures.get(i), i));        waiterThreads.add(new WaiterThread<>(futures.get(i), i));    }    assertFalse(allFuture.isDone());    for (int i = 0; i < numThreads; i++) {        waiterThreads.get(i).start();    }    for (int i = 0; i < numThreads - 1; i++) {        completerThreads.get(i).start();    }    assertFalse(allFuture.isDone());    completerThreads.get(numThreads - 1).start();    allFuture.get();    assertTrue(allFuture.isDone());    for (int i = 0; i < numThreads; i++) {        assertEquals(Integer.valueOf(i), futures.get(i).get());    }    for (int i = 0; i < numThreads; i++) {        completerThreads.get(i).join();        waiterThreads.get(i).join();        assertEquals(null, completerThreads.get(i).testException);        assertEquals(null, waiterThreads.get(i).testException);    }}
public void kafkatest_f7163_0() throws Exception
{    GarbageCollectedMemoryPool pool = new GarbageCollectedMemoryPool(1000, 10, true, null);    pool.tryAllocate(0);}
public void kafkatest_f7164_0() throws Exception
{    GarbageCollectedMemoryPool pool = new GarbageCollectedMemoryPool(1000, 10, false, null);    pool.tryAllocate(-1);}
public void kafkatest_f7173_0() throws Exception
{    testAllMessageRoundTrips(new AddOffsetsToTxnRequestData().setTransactionalId("foobar").setProducerId(0xbadcafebadcafeL).setProducerEpoch((short) 123).setGroupId("baaz"));    testAllMessageRoundTrips(new AddOffsetsToTxnResponseData().setThrottleTimeMs(42).setErrorCode((short) 0));}
public void kafkatest_f7174_0() throws Exception
{    testAllMessageRoundTrips(new AddPartitionsToTxnRequestData().setTransactionalId("blah").setProducerId(0xbadcafebadcafeL).setProducerEpoch((short) 30000).setTopics(new AddPartitionsToTxnTopicCollection(singletonList(new AddPartitionsToTxnTopic().setName("Topic").setPartitions(singletonList(1))).iterator())));}
public void kafkatest_f7183_0() throws Exception
{    testAllMessageRoundTrips(new OffsetCommitRequestData().setTopics(new ArrayList<>()).setGroupId("groupId"));    Supplier<OffsetCommitRequestData> request = () -> new OffsetCommitRequestData().setGroupId("groupId").setMemberId(memberId).setTopics(new ArrayList<>()).setGenerationId(15);    testAllMessageRoundTripsFromVersion((short) 1, request.get());    testAllMessageRoundTripsFromVersion((short) 1, request.get().setGroupInstanceId(null));    testAllMessageRoundTripsFromVersion((short) 7, request.get().setGroupInstanceId(instanceId));}
public void kafkatest_f7184_0() throws Exception
{    // Version 2 adds optional current leader epoch    OffsetForLeaderEpochRequestData.OffsetForLeaderPartition partitionDataNoCurrentEpoch = new OffsetForLeaderEpochRequestData.OffsetForLeaderPartition().setPartitionIndex(0).setLeaderEpoch(3);    OffsetForLeaderEpochRequestData.OffsetForLeaderPartition partitionDataWithCurrentEpoch = new OffsetForLeaderEpochRequestData.OffsetForLeaderPartition().setPartitionIndex(0).setLeaderEpoch(3).setCurrentLeaderEpoch(5);    testAllMessageRoundTrips(new OffsetForLeaderEpochRequestData().setTopics(singletonList(new OffsetForLeaderEpochRequestData.OffsetForLeaderTopic().setName("foo").setPartitions(singletonList(partitionDataNoCurrentEpoch)))));    testAllMessageRoundTripsBeforeVersion((short) 2, partitionDataWithCurrentEpoch, partitionDataNoCurrentEpoch);    testAllMessageRoundTripsFromVersion((short) 2, partitionDataWithCurrentEpoch);    // Version 3 adds the optional replica Id field    testAllMessageRoundTripsFromVersion((short) 3, new OffsetForLeaderEpochRequestData().setReplicaId(5));    testAllMessageRoundTripsBeforeVersion((short) 3, new OffsetForLeaderEpochRequestData().setReplicaId(5), new OffsetForLeaderEpochRequestData());    testAllMessageRoundTripsBeforeVersion((short) 3, new OffsetForLeaderEpochRequestData().setReplicaId(5), new OffsetForLeaderEpochRequestData().setReplicaId(-2));}
private void kafkatest_f7193_0(short startVersion, short endVersion, Message message, Message expected) throws Exception
{    for (short version = startVersion; version < endVersion; version++) {        testMessageRoundTrip(version, message, expected);    }}
private void kafkatest_f7194_0(short fromVersion, Message message) throws Exception
{    for (short version = fromVersion; version < message.highestSupportedVersion(); version++) {        testEquivalentMessageRoundTrip(version, message);    }}
public String kafkatest_f7203_0()
{    return name + "[" + type + "]";}
private static void kafkatest_f7204_0(Schema schemaA, Schema schemaB)
{    compareTypes(new NamedType("schemaA", schemaA), new NamedType("schemaB", schemaB));}
public void kafkatest_f7213_0()
{    final UUID uuid = UUID.randomUUID();    final TestUUIDData out = new TestUUIDData();    out.setProcessId(uuid);    final Struct struct = out.toStruct((short) 1);    final TestUUIDData in = new TestUUIDData();    in.fromStruct(struct, (short) 1);    Assert.assertEquals(uuid, in.processId());}
public void kafkatest_f7214_0()
{    final UUID uuid = UUID.randomUUID();    final TestUUIDData out = new TestUUIDData();    out.setProcessId(uuid);    final ByteBuffer buffer = ByteBuffer.allocate(out.size((short) 1));    out.write(new ByteBufferAccessor(buffer), (short) 1);    buffer.rewind();    final TestUUIDData in = new TestUUIDData();    in.read(new ByteBufferAccessor(buffer), (short) 1);    Assert.assertEquals(uuid, in.processId());}
public void kafkatest_f7228_0() throws Exception
{    sensor.record(3.5);    sensor.record(4.0);    AttributeList attributeList = getAttributes(countMetricName, countMetricName.name(), sumMetricName.name(), "name");    List<Attribute> attributes = attributeList.asList();    assertEquals(2, attributes.size());    for (Attribute attribute : attributes) {        if (countMetricName.name().equals(attribute.getName()))            assertEquals(2.0, attribute.getValue());        else if (sumMetricName.name().equals(attribute.getName()))            assertEquals(7.5, attribute.getValue());        else            fail("Unexpected attribute returned: " + attribute.getName());    }}
public void kafkatest_f7229_0() throws Exception
{    try {        mBeanServer.invoke(objectName(countMetricName), "something", null, null);        fail("invoke should have failed");    } catch (RuntimeMBeanException e) {        assertThat(e.getCause(), instanceOf(UnsupportedOperationException.class));    }}
public void kafkatest_f7238_0()
{    MetricName n1 = metrics.metricName("name", "group", "description", "key1", "value1", "key2", "value2");    Map<String, String> tags = new HashMap<String, String>();    tags.put("key1", "value1");    tags.put("key2", "value2");    MetricName n2 = metrics.metricName("name", "group", "description", tags);    assertEquals("metric names created in two different ways should be equal", n1, n2);    try {        metrics.metricName("name", "group", "description", "key1");        fail("Creating MetricName with an odd number of keyValue should fail");    } catch (IllegalArgumentException e) {    // this is expected    }}
public void kafkatest_f7239_0() throws Exception
{    verifyStats(m -> (double) m.metricValue());}
public void kafkatest_f7248_0()
{    WindowedCount count = new WindowedCount();    MetricConfig config = new MetricConfig().timeWindow(1, TimeUnit.MILLISECONDS).samples(2);    count.record(config, 1.0, time.milliseconds());    time.sleep(1);    count.record(config, 1.0, time.milliseconds());    assertEquals(2.0, count.measure(config, time.milliseconds()), EPS);    time.sleep(1);    // oldest event times out    count.record(config, 1.0, time.milliseconds());    assertEquals(2.0, count.measure(config, time.milliseconds()), EPS);}
public void kafkatest_f7249_0()
{    Max max = new Max();    long windowMs = 100;    int samples = 2;    MetricConfig config = new MetricConfig().timeWindow(windowMs, TimeUnit.MILLISECONDS).samples(samples);    max.record(config, 50, time.milliseconds());    time.sleep(samples * windowMs);    assertEquals(Double.NaN, max.measure(config, time.milliseconds()), EPS);}
public void kafkatest_f7258_0()
{    SimpleRate rate = new SimpleRate();    // Given    MetricConfig config = new MetricConfig().timeWindow(1, TimeUnit.SECONDS).samples(10);    // In the first window the rate is a fraction of the whole (1s) window    // So when we record 1000 at t0, the rate should be 1000 until the window completes, or more data is recorded.    record(rate, config, 1000);    assertEquals(1000, measure(rate, config), 0);    time.sleep(100);    // 1000B / 0.1s    assertEquals(1000, measure(rate, config), 0);    time.sleep(100);    // 1000B / 0.2s    assertEquals(1000, measure(rate, config), 0);    time.sleep(200);    // 1000B / 0.4s    assertEquals(1000, measure(rate, config), 0);    // In the second (and subsequent) window(s), the rate will be in proportion to the elapsed time    // So the rate will degrade over time, as the time between measurement and the initial recording grows.    time.sleep(600);    // 1000B / 1.0s    assertEquals(1000, measure(rate, config), 0);    time.sleep(200);    // 1000B / 1.2s    assertEquals(1000 / 1.2, measure(rate, config), 0);    time.sleep(200);    // 1000B / 1.4s    assertEquals(1000 / 1.4, measure(rate, config), 0);    // Adding another value, inside the same window should double the rate    record(rate, config, 1000);    // 2000B / 1.4s    assertEquals(2000 / 1.4, measure(rate, config), 0);    // Going over the next window, should not change behaviour    time.sleep(1100);    // 2000B / 2.5s    assertEquals(2000 / 2.5, measure(rate, config), 0);    record(rate, config, 1000);    // 3000B / 2.5s    assertEquals(3000 / 2.5, measure(rate, config), 0);    // Sleeping for another 6.5 windows also should be the same    time.sleep(6500);    // 3000B / 9s    assertEquals(3000 / 9, measure(rate, config), 1);    record(rate, config, 1000);    // 4000B / 9s    assertEquals(4000 / 9, measure(rate, config), 1);    // Going over the 10 window boundary should cause the first window's values (1000) will be purged.    // So the rate is calculated based on the oldest reading, which is inside the second window, at 1.4s    time.sleep(1500);    assertEquals((4000 - 1000) / (10.5 - 1.4), measure(rate, config), 1);    record(rate, config, 1000);    assertEquals((5000 - 1000) / (10.5 - 1.4), measure(rate, config), 1);}
private void kafkatest_f7259_0(Rate rate, MetricConfig config, int value)
{    rate.record(config, value, time.milliseconds());}
 static StatType kafkatest_f7271_0(int id)
{    for (StatType statType : StatType.values()) {        if (statType.id == id)            return statType;    }    return null;}
private Sensor kafkatest_f7272_0(StatType statType, int index)
{    Sensor sensor = metrics.sensor("kafka.requests." + index);    Map<String, String> tags = Collections.singletonMap("tag", "tag" + index);    switch(statType) {        case AVG:            sensor.add(metrics.metricName("test.metric.avg", "avg", tags), new Avg());            break;        case TOTAL:            sensor.add(metrics.metricName("test.metric.total", "total", tags), new CumulativeSum());            break;        case COUNT:            sensor.add(metrics.metricName("test.metric.count", "count", tags), new WindowedCount());            break;        case MAX:            sensor.add(metrics.metricName("test.metric.max", "max", tags), new Max());            break;        case MIN:            sensor.add(metrics.metricName("test.metric.min", "min", tags), new Min());            break;        case RATE:            sensor.add(metrics.metricName("test.metric.rate", "rate", tags), new Rate());            break;        case SIMPLE_RATE:            sensor.add(metrics.metricName("test.metric.simpleRate", "simpleRate", tags), new SimpleRate());            break;        case SUM:            sensor.add(metrics.metricName("test.metric.sum", "sum", tags), new WindowedSum());            break;        case VALUE:            sensor.add(metrics.metricName("test.metric.value", "value", tags), new Value());            break;        case PERCENTILES:            sensor.add(metrics.metricName("test.metric.percentiles", "percentiles", tags), new Percentiles(100, -100, 100, Percentiles.BucketSizing.CONSTANT, new Percentile(metrics.metricName("test.median", "percentiles"), 50.0), new Percentile(metrics.metricName("test.perc99_9", "percentiles"), 99.9)));            break;        case METER:            sensor.add(new Meter(metrics.metricName("test.metric.meter.rate", "meter", tags), metrics.metricName("test.metric.meter.total", "meter", tags)));            break;        default:            throw new IllegalStateException("Invalid stat type " + statType);    }    return sensor;}
public void kafkatest_f7281_0()
{    metrics.close();}
public void kafkatest_f7282_0()
{    new Frequencies(4, 1.0, 4.0, freq("1", 1.0), freq("2", 20.0));}
public void kafkatest_f7291_0()
{    ConstantBinScheme scheme = new ConstantBinScheme(5, 0, 5);    assertEquals("A value below the lower bound should map to the first bin", 0, scheme.toBin(-1.0));    assertEquals("A value above the upper bound should map to the last bin", 4, scheme.toBin(5.01));    assertEquals("Check boundary of bucket 0", 0, scheme.toBin(-0.0001));    assertEquals("Check boundary of bucket 0", 0, scheme.toBin(0.0000));    assertEquals("Check boundary of bucket 0", 0, scheme.toBin(0.0001));    assertEquals("Check boundary of bucket 0", 0, scheme.toBin(0.9999));    assertEquals("Check boundary of bucket 1", 1, scheme.toBin(1.0000));    assertEquals("Check boundary of bucket 1", 1, scheme.toBin(1.0001));    assertEquals("Check boundary of bucket 1", 1, scheme.toBin(1.9999));    assertEquals("Check boundary of bucket 2", 2, scheme.toBin(2.0000));    assertEquals("Check boundary of bucket 2", 2, scheme.toBin(2.0001));    assertEquals("Check boundary of bucket 2", 2, scheme.toBin(2.9999));    assertEquals("Check boundary of bucket 3", 3, scheme.toBin(3.0000));    assertEquals("Check boundary of bucket 3", 3, scheme.toBin(3.0001));    assertEquals("Check boundary of bucket 3", 3, scheme.toBin(3.9999));    assertEquals("Check boundary of bucket 4", 4, scheme.toBin(4.0000));    assertEquals("Check boundary of bucket 4", 4, scheme.toBin(4.9999));    assertEquals("Check boundary of bucket 4", 4, scheme.toBin(5.0000));    assertEquals("Check boundary of bucket 4", 4, scheme.toBin(5.0001));    assertEquals(Float.NEGATIVE_INFINITY, scheme.fromBin(-1), 0.001d);    assertEquals(Float.POSITIVE_INFINITY, scheme.fromBin(5), 0.001d);    assertEquals(0.0, scheme.fromBin(0), 0.001d);    assertEquals(1.0, scheme.fromBin(1), 0.001d);    assertEquals(2.0, scheme.fromBin(2), 0.001d);    assertEquals(3.0, scheme.fromBin(3), 0.001d);    assertEquals(4.0, scheme.fromBin(4), 0.001d);    checkBinningConsistency(scheme);}
public void kafkatest_f7292_0()
{    LinearBinScheme scheme = new LinearBinScheme(10, 10);    assertEquals(Float.NEGATIVE_INFINITY, scheme.fromBin(-1), 0.001d);    assertEquals(Float.POSITIVE_INFINITY, scheme.fromBin(11), 0.001d);    assertEquals(0.0, scheme.fromBin(0), 0.001d);    assertEquals(0.2222, scheme.fromBin(1), 0.001d);    assertEquals(0.6666, scheme.fromBin(2), 0.001d);    assertEquals(1.3333, scheme.fromBin(3), 0.001d);    assertEquals(2.2222, scheme.fromBin(4), 0.001d);    assertEquals(3.3333, scheme.fromBin(5), 0.001d);    assertEquals(4.6667, scheme.fromBin(6), 0.001d);    assertEquals(6.2222, scheme.fromBin(7), 0.001d);    assertEquals(8.0000, scheme.fromBin(8), 0.001d);    assertEquals(10.000, scheme.fromBin(9), 0.001d);    assertEquals(0, scheme.toBin(0.0000));    assertEquals(0, scheme.toBin(0.2221));    assertEquals(1, scheme.toBin(0.2223));    assertEquals(2, scheme.toBin(0.6667));    assertEquals(3, scheme.toBin(1.3334));    assertEquals(4, scheme.toBin(2.2223));    assertEquals(5, scheme.toBin(3.3334));    assertEquals(6, scheme.toBin(4.6667));    assertEquals(7, scheme.toBin(6.2223));    assertEquals(8, scheme.toBin(8.0000));    assertEquals(9, scheme.toBin(10.000));    assertEquals(9, scheme.toBin(10.001));    assertEquals(Float.POSITIVE_INFINITY, scheme.fromBin(10), 0.001d);    checkBinningConsistency(scheme);}
public void kafkatest_f7301_0()
{    Map<String, Object> configs = new HashMap<>();    configs.put(BrokerSecurityConfigs.PRINCIPAL_BUILDER_CLASS_CONFIG, ConfigurableKafkaPrincipalBuilder.class);    KafkaPrincipalBuilder builder = ChannelBuilders.createPrincipalBuilder(configs, null, null, null, null);    assertTrue(builder instanceof ConfigurableKafkaPrincipalBuilder);    assertTrue(((ConfigurableKafkaPrincipalBuilder) builder).configured);}
public void kafkatest_f7302_0(Map<String, ?> configs)
{    configured = true;}
public void kafkatest_f7312_0() throws IOException, InterruptedException
{    closing = true;    this.serverSocket.close();    closeConnections();    for (Thread t : threads) t.join();    join();}
public static NioEchoServer kafkatest_f7313_0(ListenerName listenerName, SecurityProtocol securityProtocol, AbstractConfig serverConfig, CredentialCache credentialCache, Time time) throws Exception
{    return createEchoServer(listenerName, securityProtocol, serverConfig, credentialCache, 100, time);}
public String kafkatest_f7322_0()
{    return metricNameSuffix;}
public int kafkatest_f7323_0()
{    return port;}
public void kafkatest_f7332_0()
{    try {        acceptorThread.start();        while (serverSocketChannel.isOpen()) {            selector.poll(100);            synchronized (newChannels) {                for (SocketChannel socketChannel : newChannels) {                    String id = id(socketChannel);                    selector.register(id, socketChannel);                    socketChannels.add(socketChannel);                }                newChannels.clear();            }            if (closeKafkaChannels) {                for (KafkaChannel channel : selector.channels()) selector.close(channel.id());            }            List<NetworkReceive> completedReceives = selector.completedReceives();            for (NetworkReceive rcv : completedReceives) {                KafkaChannel channel = channel(rcv.source());                if (!maybeBeginServerReauthentication(channel, rcv, time)) {                    String channelId = channel.id();                    selector.mute(channelId);                    NetworkSend send = new NetworkSend(rcv.source(), rcv.payload());                    if (outputChannel == null)                        selector.send(send);                    else {                        for (ByteBuffer buffer : send.buffers) outputChannel.write(buffer);                        selector.unmute(channelId);                    }                }            }            for (Send send : selector.completedSends()) {                selector.unmute(send.destination());                numSent += 1;            }        }    } catch (IOException e) {    // ignore    }}
public int kafkatest_f7333_0()
{    return numSent;}
public void kafkatest_f7342_0()
{    try {        java.nio.channels.Selector acceptSelector = java.nio.channels.Selector.open();        serverSocketChannel.register(acceptSelector, SelectionKey.OP_ACCEPT);        while (serverSocketChannel.isOpen()) {            if (acceptSelector.select(1000) > 0) {                Iterator<SelectionKey> it = acceptSelector.selectedKeys().iterator();                while (it.hasNext()) {                    SelectionKey key = it.next();                    if (key.isAcceptable()) {                        SocketChannel socketChannel = ((ServerSocketChannel) key.channel()).accept();                        socketChannel.configureBlocking(false);                        newChannels.add(socketChannel);                        selector.wakeup();                    }                    it.remove();                }            }        }    } catch (IOException e) {    // ignore    }}
public void kafkatest_f7343_0()
{    try (Socket connection = new Socket(serverAddress.getAddress(), serverAddress.getPort());        OutputStream os = connection.getOutputStream()) {        os.write(payload);        os.flush();    } catch (Exception e) {        e.printStackTrace(System.err);    }}
public boolean kafkatest_f7352_0()
{    try {        selector.poll(1000L);        return selector.disconnected().containsKey(node);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void kafkatest_f7353_0() throws Exception
{    String node = "0";    blockingConnect(node);    selector.send(createSend(node, "test1"));    try {        selector.send(createSend(node, "test2"));        fail("IllegalStateException not thrown when sending a request with one in flight");    } catch (IllegalStateException e) {    // Expected exception    }    selector.poll(0);    assertTrue("Channel not closed", selector.disconnected().containsKey(node));    assertEquals(ChannelState.FAILED_SEND, selector.disconnected().get(node));}
public void kafkatest_f7362_0() throws Exception
{    blockingConnect("0");    blockingConnect("1");    selector.send(createSend("0", "hello"));    selector.send(createSend("1", "hi"));    selector.mute("1");    while (selector.completedReceives().isEmpty()) selector.poll(5);    assertEquals("We should have only one response", 1, selector.completedReceives().size());    assertEquals("The response should not be from the muted node", "0", selector.completedReceives().get(0).source());    selector.unmute("1");    do {        selector.poll(5);    } while (selector.completedReceives().isEmpty());    assertEquals("We should have only one response", 1, selector.completedReceives().size());    assertEquals("The response should be from the previously muted node", "1", selector.completedReceives().get(0).source());}
public void kafkatest_f7363_0() throws Exception
{    ChannelBuilder channelBuilder = new PlaintextChannelBuilder(null) {        @Override        public KafkaChannel buildChannel(String id, SelectionKey key, int maxReceiveSize, MemoryPool memoryPool) throws KafkaException {            throw new RuntimeException("Test exception");        }        @Override        public void close() {        }    };    Selector selector = new Selector(5000, new Metrics(), new MockTime(), "MetricGroup", channelBuilder, new LogContext());    SocketChannel socketChannel = SocketChannel.open();    socketChannel.configureBlocking(false);    try {        selector.register("1", socketChannel);        fail("Register did not fail");    } catch (IOException e) {        assertTrue("Unexpected exception: " + e, e.getCause().getMessage().contains("Test exception"));        assertFalse("Socket not closed", socketChannel.isOpen());    }    selector.close();}
protected SelectionKey kafkatest_f7373_0(String id, SocketChannel socketChannel, int interestedOps) throws IOException
{    SelectionKey key = super.registerChannel(id, socketChannel, interestedOps);    key.cancel();    if (throwIOException.get())        throw new IOException("Test exception");    return key;}
private void kafkatest_f7374_0(Selector selector, String id) throws Exception
{    try {        selector.connect(id, new InetSocketAddress("localhost", server.port), BUFFER_SIZE, BUFFER_SIZE);        fail("Expected exception not thrown");    } catch (Exception e) {        verifyEmptyImmediatelyConnectedKeys(selector);        assertNull("Channel not removed", selector.channel(id));        ensureEmptySelectorFields(selector);    }}
public void kafkatest_f7383_0() throws Exception
{    // create connections    int expectedConnections = 5;    InetSocketAddress addr = new InetSocketAddress("localhost", server.port);    for (int i = 0; i < expectedConnections; i++) connect(Integer.toString(i), addr);    // Poll continuously, as we cannot guarantee that the first call will see all connections    int seenConnections = 0;    for (int i = 0; i < 10; i++) {        selector.poll(100L);        seenConnections += selector.connected().size();        if (seenConnections == expectedConnections)            break;    }    assertEquals((double) expectedConnections, getMetric("connection-creation-total").metricValue());    assertEquals((double) expectedConnections, getMetric("connection-count").metricValue());}
public void kafkatest_f7384_0() throws Exception
{    int conns = 5;    try (ServerSocketChannel ss = ServerSocketChannel.open()) {        ss.bind(new InetSocketAddress(0));        InetSocketAddress serverAddress = (InetSocketAddress) ss.getLocalAddress();        for (int i = 0; i < conns; i++) {            Thread sender = createSender(serverAddress, randomPayload(1));            sender.start();            SocketChannel channel = ss.accept();            channel.configureBlocking(false);            selector.register(Integer.toString(i), channel);        }    }    assertEquals((double) conns, getMetric("connection-creation-total").metricValue());    assertEquals((double) conns, getMetric("connection-count").metricValue());}
protected String kafkatest_f7393_0(NetworkReceive receive)
{    return new String(Utils.toArray(receive.payload()));}
private void kafkatest_f7394_0(String node, String requestPrefix, int startIndex, int endIndex) throws Exception
{    int requests = startIndex;    int responses = startIndex;    selector.send(createSend(node, requestPrefix + "-" + startIndex));    requests++;    while (responses < endIndex) {        // do the i/o        selector.poll(0L);        assertEquals("No disconnects should have occurred.", 0, selector.disconnected().size());        // handle requests and responses of the fast node        for (NetworkReceive receive : selector.completedReceives()) {            assertEquals(requestPrefix + "-" + responses, asString(receive));            responses++;        }        for (int i = 0; i < selector.completedSends().size() && requests < endIndex; i++, requests++) {            selector.send(createSend(node, requestPrefix + "-" + requests));        }    }}
public void kafkatest_f7403_0() throws Exception
{    this.selector.close();    this.server.close();    this.metrics.close();}
public SecurityProtocol kafkatest_f7404_0()
{    return SecurityProtocol.PLAINTEXT;}
public void kafkatest_f7413_0() throws Exception
{    String node = "0";    // create connections    InetSocketAddress addr = new InetSocketAddress("localhost", server.port);    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    // send echo requests and receive responses    while (!selector.isChannelReady(node)) {        selector.poll(1000L);    }    selector.send(createSend(node, node + "-" + 0));    selector.poll(0L);    server.renegotiate();    selector.send(createSend(node, node + "-" + 1));    long expiryTime = System.currentTimeMillis() + 2000;    List<String> disconnected = new ArrayList<>();    while (!disconnected.contains(node) && System.currentTimeMillis() < expiryTime) {        selector.poll(10);        disconnected.addAll(selector.disconnected().keySet());    }    assertTrue("Renegotiation should cause disconnection", disconnected.contains(node));}
public void kafkatest_f7414_0() throws Exception
{    // clean up default selector, replace it with one that uses a finite mem pool    selector.close();    MemoryPool pool = new SimpleMemoryPool(900, 900, false, null);    // the initial channel builder is for clients, we need a server one    File trustStoreFile = File.createTempFile("truststore", ".jks");    Map<String, Object> sslServerConfigs = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile, "server");    channelBuilder = new SslChannelBuilder(Mode.SERVER, null, false);    channelBuilder.configure(sslServerConfigs);    selector = new Selector(NetworkReceive.UNLIMITED, 5000, metrics, time, "MetricGroup", new HashMap<String, String>(), true, false, channelBuilder, pool, new LogContext());    try (ServerSocketChannel ss = ServerSocketChannel.open()) {        ss.bind(new InetSocketAddress(0));        InetSocketAddress serverAddress = (InetSocketAddress) ss.getLocalAddress();        SslSender sender1 = createSender(serverAddress, randomPayload(900));        SslSender sender2 = createSender(serverAddress, randomPayload(900));        sender1.start();        sender2.start();        // not defined if its 1 or 2        SocketChannel channelX = ss.accept();        channelX.configureBlocking(false);        SocketChannel channelY = ss.accept();        channelY.configureBlocking(false);        selector.register("clientX", channelX);        selector.register("clientY", channelY);        boolean handshaked = false;        NetworkReceive firstReceive = null;        long deadline = System.currentTimeMillis() + 5000;        // 2. a single payload is actually read out completely (the other is too big to fit)        while (System.currentTimeMillis() < deadline) {            selector.poll(10);            List<NetworkReceive> completed = selector.completedReceives();            if (firstReceive == null) {                if (!completed.isEmpty()) {                    assertEquals("expecting a single request", 1, completed.size());                    firstReceive = completed.get(0);                    assertTrue(selector.isMadeReadProgressLastPoll());                    assertEquals(0, pool.availableMemory());                }            } else {                assertTrue("only expecting single request", completed.isEmpty());            }            handshaked = sender1.waitForHandshake(1) && sender2.waitForHandshake(1);            if (handshaked && firstReceive != null && selector.isOutOfMemory())                break;        }        assertTrue("could not initiate connections within timeout", handshaked);        selector.poll(10);        assertTrue(selector.completedReceives().isEmpty());        assertEquals(0, pool.availableMemory());        assertNotNull("First receive not complete", firstReceive);        assertTrue("Selector not out of memory", selector.isOutOfMemory());        firstReceive.close();        // memory has been released back to pool        assertEquals(900, pool.availableMemory());        List<NetworkReceive> completed = Collections.emptyList();        deadline = System.currentTimeMillis() + 5000;        while (System.currentTimeMillis() < deadline && completed.isEmpty()) {            selector.poll(1000);            completed = selector.completedReceives();        }        assertEquals("could not read remaining request within timeout", 1, completed.size());        assertEquals(0, pool.availableMemory());        assertFalse(selector.isOutOfMemory());    }}
public void kafkatest_f7423_0(X509Certificate[] x509Certificates, String s) throws CertificateException
{// nop}
public X509Certificate[] kafkatest_f7424_0()
{    return new X509Certificate[0];}
public void kafkatest_f7433_0() throws Exception
{    String node = "0";    serverCertStores = new CertStores(true, "server", "notahost");    clientCertStores = new CertStores(false, "client", "localhost");    sslServerConfigs = serverCertStores.getTrustingConfig(clientCertStores);    sslClientConfigs = clientCertStores.getTrustingConfig(serverCertStores);    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, "HTTPS");    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(selector, node, ChannelState.State.AUTHENTICATION_FAILED);    server.verifyAuthenticationMetrics(0, 1);}
public void kafkatest_f7434_0() throws Exception
{    serverCertStores = new CertStores(true, "server", "notahost");    clientCertStores = new CertStores(false, "client", "localhost");    sslServerConfigs = serverCertStores.getTrustingConfig(clientCertStores);    sslClientConfigs = clientCertStores.getTrustingConfig(serverCertStores);    SecurityProtocol securityProtocol = SecurityProtocol.SSL;    server = createEchoServer(SecurityProtocol.SSL);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    // Disable endpoint validation, connection should succeed    String node = "1";    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, "");    createSelector(sslClientConfigs);    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);    // Disable endpoint validation using null value, connection should succeed    String node2 = "2";    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, null);    createSelector(sslClientConfigs);    selector.connect(node2, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node2, 100, 10);    // Connection should fail with endpoint validation enabled    String node3 = "3";    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, "HTTPS");    createSelector(sslClientConfigs);    selector.connect(node3, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(selector, node3, ChannelState.State.AUTHENTICATION_FAILED);    selector.close();}
public void kafkatest_f7443_0() throws Exception
{    try (SslChannelBuilder channelBuilder = new SslChannelBuilder(Mode.CLIENT, null, false)) {        sslClientConfigs.put(SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG, "invalid");        channelBuilder.configure(sslClientConfigs);        fail("SSL channel configured with invalid SecureRandom implementation");    } catch (KafkaException e) {    // Expected exception    }}
public void kafkatest_f7444_0() throws Exception
{    try (SslChannelBuilder channelBuilder = new SslChannelBuilder(Mode.CLIENT, null, false)) {        sslClientConfigs.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "invalid");        channelBuilder.configure(sslClientConfigs);        fail("SSL channel configured with invalid truststore password");    } catch (KafkaException e) {    // Expected exception    }}
public boolean kafkatest_f7453_0()
{    return server.numSent() >= 2;}
public void kafkatest_f7454_0() throws Exception
{    String node = "0";    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs, 10, null, null);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 64000, 10);}
public void kafkatest_f7463_0() throws Exception
{    server = createEchoServer(SecurityProtocol.SSL);    testIOExceptionsDuringHandshake(FailureAction.NO_OP, server::closeKafkaChannels);}
public void kafkatest_f7464_0() throws Exception
{    server = createEchoServer(SecurityProtocol.SSL);    testIOExceptionsDuringHandshake(server::closeKafkaChannels, FailureAction.NO_OP);}
public void kafkatest_f7473_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SSL;    TestSecurityConfig config = new TestSecurityConfig(sslServerConfigs);    ListenerName listenerName = ListenerName.forSecurityProtocol(securityProtocol);    ChannelBuilder serverChannelBuilder = ChannelBuilders.serverChannelBuilder(listenerName, false, securityProtocol, config, null, null, time);    server = new NioEchoServer(listenerName, securityProtocol, config, "localhost", serverChannelBuilder, null, time);    server.start();    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    // Verify that client with matching truststore can authenticate, send and receive    String oldNode = "0";    Selector oldClientSelector = createSelector(sslClientConfigs);    oldClientSelector.connect(oldNode, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, oldNode, 100, 10);    CertStores newServerCertStores = new CertStores(true, "server", "localhost");    Map<String, Object> newKeystoreConfigs = newServerCertStores.keyStoreProps();    assertTrue("SslChannelBuilder not reconfigurable", serverChannelBuilder instanceof ListenerReconfigurable);    ListenerReconfigurable reconfigurableBuilder = (ListenerReconfigurable) serverChannelBuilder;    assertEquals(listenerName, reconfigurableBuilder.listenerName());    reconfigurableBuilder.validateReconfiguration(newKeystoreConfigs);    reconfigurableBuilder.reconfigure(newKeystoreConfigs);    // Verify that new client with old truststore fails    oldClientSelector.connect("1", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(oldClientSelector, "1", ChannelState.State.AUTHENTICATION_FAILED);    // Verify that new client with new truststore can authenticate, send and receive    sslClientConfigs = clientCertStores.getTrustingConfig(newServerCertStores);    Selector newClientSelector = createSelector(sslClientConfigs);    newClientSelector.connect("2", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(newClientSelector, "2", 100, 10);    // Verify that old client continues to work    NetworkTestUtils.checkClientConnection(oldClientSelector, oldNode, 100, 10);    CertStores invalidCertStores = new CertStores(true, "server", "127.0.0.1");    Map<String, Object> invalidConfigs = invalidCertStores.getTrustingConfig(clientCertStores);    verifyInvalidReconfigure(reconfigurableBuilder, invalidConfigs, "keystore with different SubjectAltName");    Map<String, Object> missingStoreConfigs = new HashMap<>();    missingStoreConfigs.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "PKCS12");    missingStoreConfigs.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, "some.keystore.path");    missingStoreConfigs.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, new Password("some.keystore.password"));    missingStoreConfigs.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, new Password("some.key.password"));    verifyInvalidReconfigure(reconfigurableBuilder, missingStoreConfigs, "keystore not found");    // Verify that new connections continue to work with the server with previously configured keystore after failed reconfiguration    newClientSelector.connect("3", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(newClientSelector, "3", 100, 10);}
public void kafkatest_f7474_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SSL;    sslServerConfigs.put(BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG, "required");    TestSecurityConfig config = new TestSecurityConfig(sslServerConfigs);    ListenerName listenerName = ListenerName.forSecurityProtocol(securityProtocol);    ChannelBuilder serverChannelBuilder = ChannelBuilders.serverChannelBuilder(listenerName, false, securityProtocol, config, null, null, time);    server = new NioEchoServer(listenerName, securityProtocol, config, "localhost", serverChannelBuilder, null, time);    server.start();    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    // Verify that client with matching keystore can authenticate, send and receive    String oldNode = "0";    Selector oldClientSelector = createSelector(sslClientConfigs);    oldClientSelector.connect(oldNode, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, oldNode, 100, 10);    CertStores newClientCertStores = new CertStores(true, "client", "localhost");    sslClientConfigs = newClientCertStores.getTrustingConfig(serverCertStores);    Map<String, Object> newTruststoreConfigs = newClientCertStores.trustStoreProps();    assertTrue("SslChannelBuilder not reconfigurable", serverChannelBuilder instanceof ListenerReconfigurable);    ListenerReconfigurable reconfigurableBuilder = (ListenerReconfigurable) serverChannelBuilder;    assertEquals(listenerName, reconfigurableBuilder.listenerName());    reconfigurableBuilder.validateReconfiguration(newTruststoreConfigs);    reconfigurableBuilder.reconfigure(newTruststoreConfigs);    // Verify that new client with old truststore fails    oldClientSelector.connect("1", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(oldClientSelector, "1", ChannelState.State.AUTHENTICATION_FAILED);    // Verify that new client with new truststore can authenticate, send and receive    Selector newClientSelector = createSelector(sslClientConfigs);    newClientSelector.connect("2", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(newClientSelector, "2", 100, 10);    // Verify that old client continues to work    NetworkTestUtils.checkClientConnection(oldClientSelector, oldNode, 100, 10);    Map<String, Object> invalidConfigs = new HashMap<>(newTruststoreConfigs);    invalidConfigs.put(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, "INVALID_TYPE");    verifyInvalidReconfigure(reconfigurableBuilder, invalidConfigs, "invalid truststore type");    Map<String, Object> missingStoreConfigs = new HashMap<>();    missingStoreConfigs.put(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, "PKCS12");    missingStoreConfigs.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, "some.truststore.path");    missingStoreConfigs.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, new Password("some.truststore.password"));    verifyInvalidReconfigure(reconfigurableBuilder, missingStoreConfigs, "truststore not found");    // Verify that new connections continue to work with the server with previously configured keystore after failed reconfiguration    newClientSelector.connect("3", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(newClientSelector, "3", 100, 10);}
protected int kafkatest_f7483_0()
{    ByteBuffer netReadBuffer = netReadBuffer();    // netReadBufferSize() is invoked in SSLTransportLayer.read() prior to the read    // operation. To avoid the read buffer being expanded too early, increase buffer size    // only when read buffer is full. This ensures that BUFFER_UNDERFLOW is always    // triggered in testNetReadBufferResize().    boolean updateBufSize = netReadBuffer != null && !netReadBuffer().hasRemaining();    return netReadBufSize.updateAndGet(super.netReadBufferSize(), updateBufSize);}
protected int kafkatest_f7484_0()
{    return netWriteBufSize.updateAndGet(super.netWriteBufferSize(), true);}
public void kafkatest_f7493_0()
{    ApiKeys.forId(10000);}
public void kafkatest_f7494_0()
{    ApiKeys.PRODUCE.requestSchema((short) ApiKeys.PRODUCE.requestSchemas.length);}
public void kafkatest_f7503_0()
{    validateUtf8Length("");    validateUtf8Length("abc");    validateUtf8Length("This is a test string.");}
public void kafkatest_f7504_0()
{    validateUtf8Length("A\u00ea\u00f1\u00fcC");    validateUtf8Length("\ud801\udc00");    validateUtf8Length("M\u00fcO");}
private void kafkatest_f7513_0(Type type, Object defaultValue)
{    // Should use default even if the field allows null values    Schema schema = new Schema(new Field("field", type, "doc", defaultValue));    Struct struct = new Struct(schema);    assertEquals("Should get the default value", defaultValue, struct.get("field"));    // should be valid even with missing value    struct.validate();}
public void kafkatest_f7514_0()
{    Type type = new ArrayOf(Type.INT8);    int size = 10;    ByteBuffer invalidBuffer = ByteBuffer.allocate(4 + size);    invalidBuffer.putInt(Integer.MAX_VALUE);    for (int i = 0; i < size; i++) invalidBuffer.put((byte) i);    invalidBuffer.rewind();    try {        type.read(invalidBuffer);        fail("Array size not validated");    } catch (SchemaException e) {    // Expected exception    }}
public void kafkatest_f7523_0()
{    Schema schema = new Schema(new Field("field1", Type.NULLABLE_STRING), new Field("field2", Type.NULLABLE_STRING));    Struct emptyStruct1 = new Struct(schema);    Struct emptyStruct2 = new Struct(schema);    assertEquals(emptyStruct1, emptyStruct2);    Struct mostlyEmptyStruct = new Struct(schema).set("field1", "foo");    assertNotEquals(emptyStruct1, mostlyEmptyStruct);    assertNotEquals(mostlyEmptyStruct, emptyStruct1);}
public void kafkatest_f7524_0()
{    Schema oldSchema = new Schema(new Field("field1", Type.NULLABLE_STRING), new Field("field2", Type.NULLABLE_STRING));    Schema newSchema = new Schema(new Field("field1", Type.NULLABLE_STRING));    String value = "foo bar baz";    Struct oldFormat = new Struct(oldSchema).set("field1", value).set("field2", "fine to ignore");    ByteBuffer buffer = ByteBuffer.allocate(oldSchema.sizeOf(oldFormat));    oldFormat.writeTo(buffer);    buffer.flip();    Struct newFormat = newSchema.read(buffer);    assertEquals(value, newFormat.get("field1"));}
public void kafkatest_f7533_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V0, 0L, CompressionType.GZIP, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    long logAppendTime = 15L;    ByteBufferLegacyRecordBatch batch = new ByteBufferLegacyRecordBatch(records.buffer());    batch.setMaxTimestamp(TimestampType.LOG_APPEND_TIME, logAppendTime);}
public void kafkatest_f7534_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V0, 0L, CompressionType.GZIP, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    long createTime = 15L;    ByteBufferLegacyRecordBatch batch = new ByteBufferLegacyRecordBatch(records.buffer());    batch.setMaxTimestamp(TimestampType.CREATE_TIME, createTime);}
public void kafkatest_f7543_0()
{    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    builder.append(15L, "a".getBytes(), "1".getBytes());    builder.append(20L, "b".getBytes(), "2".getBytes());    builder.close();    int position = buffer.position();    builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 2L);    builder.append(30L, "c".getBytes(), "3".getBytes());    builder.append(40L, "d".getBytes(), "4".getBytes());    builder.close();    buffer.flip();    buffer.put(position + DefaultRecordBatch.MAGIC_OFFSET, (byte) 37);    ByteBufferLogInputStream logInputStream = new ByteBufferLogInputStream(buffer, Integer.MAX_VALUE);    assertNotNull(logInputStream.nextBatch());    logInputStream.nextBatch();}
public void kafkatest_f7544_0()
{    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    builder.append(15L, "a".getBytes(), "1".getBytes());    builder.append(20L, "b".getBytes(), "2".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 2L);    builder.append(30L, "c".getBytes(), "3".getBytes());    builder.append(40L, "d".getBytes(), "4".getBytes());    builder.close();    buffer.flip();    ByteBufferLogInputStream logInputStream = new ByteBufferLogInputStream(buffer, 25);    assertNotNull(logInputStream.nextBatch());    logInputStream.nextBatch();}
public void kafkatest_f7553_0()
{    Header[] headers = new Header[] { new RecordHeader("foo", "value".getBytes()), new RecordHeader("bar", (byte[]) null) };    long timestamp = System.currentTimeMillis();    SimpleRecord[] records = new SimpleRecord[] { new SimpleRecord(timestamp, "key".getBytes(), "value".getBytes()), new SimpleRecord(timestamp + 30000, null, "value".getBytes()), new SimpleRecord(timestamp + 60000, "key".getBytes(), null), new SimpleRecord(timestamp + 60000, "key".getBytes(), "value".getBytes(), headers) };    int actualSize = MemoryRecords.withRecords(CompressionType.NONE, records).sizeInBytes();    assertEquals(actualSize, DefaultRecordBatch.sizeInBytes(Arrays.asList(records)));}
public void kafkatest_f7554_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V2, 0L, CompressionType.NONE, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    ByteBuffer buffer = records.buffer();    buffer.putInt(DefaultRecordBatch.LENGTH_OFFSET, 10);    DefaultRecordBatch batch = new DefaultRecordBatch(buffer);    assertFalse(batch.isValid());    batch.ensureValid();}
public void kafkatest_f7563_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V2, 0L, CompressionType.NONE, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    DefaultRecordBatch batch = new DefaultRecordBatch(records.buffer());    batch.setMaxTimestamp(TimestampType.NO_TIMESTAMP_TYPE, RecordBatch.NO_TIMESTAMP);}
public void kafkatest_f7564_0()
{    long producerId = 1L;    short producerEpoch = 0;    int coordinatorEpoch = 15;    ByteBuffer buffer = ByteBuffer.allocate(128);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.CREATE_TIME, 0L, RecordBatch.NO_TIMESTAMP, producerId, producerEpoch, RecordBatch.NO_SEQUENCE, true, true, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.remaining());    EndTransactionMarker marker = new EndTransactionMarker(ControlRecordType.COMMIT, coordinatorEpoch);    builder.appendEndTxnMarker(System.currentTimeMillis(), marker);    MemoryRecords records = builder.build();    List<MutableRecordBatch> batches = TestUtils.toList(records.batches());    assertEquals(1, batches.size());    MutableRecordBatch batch = batches.get(0);    assertTrue(batch.isControlBatch());    List<Record> logRecords = TestUtils.toList(records.records());    assertEquals(1, logRecords.size());    Record commitRecord = logRecords.get(0);    assertEquals(marker, EndTransactionMarker.deserialize(commitRecord));}
public void kafkatest_f7573_0()
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    // use a key size larger than the full message    int keySize = 105;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    ByteUtils.writeVarint(keySize, buf);    buf.position(buf.limit());    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
public void kafkatest_f7574_0() throws IOException
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    // use a key size larger than the full message    int keySize = 105;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    ByteUtils.writeVarint(keySize, buf);    buf.position(buf.limit());    buf.flip();    DataInputStream inputStream = new DataInputStream(new ByteBufferInputStream(buf));    DefaultRecord.readPartiallyFrom(inputStream, skipArray, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
public void kafkatest_f7583_0()
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    // null key    ByteUtils.writeVarint(-1, buf);    // null value    ByteUtils.writeVarint(-1, buf);    ByteUtils.writeVarint(1, buf);    ByteUtils.writeVarint(1, buf);    buf.put((byte) 1);    // header value too long    ByteUtils.writeVarint(105, buf);    buf.position(buf.limit());    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
public void kafkatest_f7584_0() throws IOException
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    // null key    ByteUtils.writeVarint(-1, buf);    // null value    ByteUtils.writeVarint(-1, buf);    ByteUtils.writeVarint(1, buf);    ByteUtils.writeVarint(1, buf);    buf.put((byte) 1);    // header value too long    ByteUtils.writeVarint(105, buf);    buf.position(buf.limit());    buf.flip();    DataInputStream inputStream = new DataInputStream(new ByteBufferInputStream(buf));    DefaultRecord.readPartiallyFrom(inputStream, skipArray, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
public void kafkatest_f7593_0()
{    int coordinatorEpoch = 79;    EndTransactionMarker marker = new EndTransactionMarker(ControlRecordType.COMMIT, coordinatorEpoch);    ByteBuffer buffer = marker.serializeValue();    EndTransactionMarker deserialized = EndTransactionMarker.deserializeValue(ControlRecordType.COMMIT, buffer);    assertEquals(coordinatorEpoch, deserialized.coordinatorEpoch());}
public void kafkatest_f7594_0()
{    int coordinatorEpoch = 79;    ByteBuffer buffer = ByteBuffer.allocate(8);    buffer.putShort((short) 5);    buffer.putInt(coordinatorEpoch);    // unexpected data    buffer.putShort((short) 0);    buffer.flip();    EndTransactionMarker deserialized = EndTransactionMarker.deserializeValue(ControlRecordType.COMMIT, buffer);    assertEquals(coordinatorEpoch, deserialized.coordinatorEpoch());}
private void kafkatest_f7603_0(RecordBatch batch)
{    assertEquals(RecordBatch.NO_PRODUCER_ID, batch.producerId());    assertEquals(RecordBatch.NO_PRODUCER_EPOCH, batch.producerEpoch());    assertEquals(RecordBatch.NO_SEQUENCE, batch.baseSequence());    assertEquals(RecordBatch.NO_SEQUENCE, batch.lastSequence());    assertFalse(batch.isTransactional());}
private void kafkatest_f7604_0(RecordBatch batch, long baseOffset, long maxTimestamp, SimpleRecord... records)
{    assertEquals(magic, batch.magic());    assertEquals(compression, batch.compressionType());    if (magic == MAGIC_VALUE_V0) {        assertEquals(NO_TIMESTAMP_TYPE, batch.timestampType());    } else {        assertEquals(CREATE_TIME, batch.timestampType());        assertEquals(maxTimestamp, batch.maxTimestamp());    }    assertEquals(baseOffset + records.length - 1, batch.lastOffset());    if (magic >= MAGIC_VALUE_V2)        assertEquals(Integer.valueOf(records.length), batch.countOrNull());    assertEquals(baseOffset, batch.baseOffset());    assertTrue(batch.isValid());    List<Record> batchRecords = TestUtils.toList(batch);    for (int i = 0; i < records.length; i++) {        assertEquals(baseOffset + i, batchRecords.get(i).offset());        assertEquals(records[i].key(), batchRecords.get(i).key());        assertEquals(records[i].value(), batchRecords.get(i).value());        if (magic == MAGIC_VALUE_V0)            assertEquals(NO_TIMESTAMP, batchRecords.get(i).timestamp());        else            assertEquals(records[i].timestamp(), batchRecords.get(i).timestamp());    }}
public void kafkatest_f7613_0() throws IOException
{    long position = fileRecords.channel().position();    Iterator<Record> records = fileRecords.records().iterator();    for (byte[] value : values) {        assertTrue(records.hasNext());        assertEquals(records.next().value(), ByteBuffer.wrap(value));    }    assertEquals(position, fileRecords.channel().position());}
public void kafkatest_f7614_0() throws IOException
{    FileRecords read = fileRecords.slice(0, fileRecords.sizeInBytes());    assertEquals(fileRecords.sizeInBytes(), read.sizeInBytes());    TestUtils.checkEquals(fileRecords.batches(), read.batches());    List<RecordBatch> items = batches(read);    RecordBatch first = items.get(0);    // read from second message until the end    read = fileRecords.slice(first.sizeInBytes(), fileRecords.sizeInBytes() - first.sizeInBytes());    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read from second message and size is past the end of the file    read = fileRecords.slice(first.sizeInBytes(), fileRecords.sizeInBytes());    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read from second message and position + size overflows    read = fileRecords.slice(first.sizeInBytes(), Integer.MAX_VALUE);    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read from second message and size is past the end of the file on a view/slice    read = fileRecords.slice(1, fileRecords.sizeInBytes() - 1).slice(first.sizeInBytes() - 1, fileRecords.sizeInBytes());    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read from second message and position + size overflows on a view/slice    read = fileRecords.slice(1, fileRecords.sizeInBytes() - 1).slice(first.sizeInBytes() - 1, Integer.MAX_VALUE);    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read a single message starting from second message    RecordBatch second = items.get(1);    read = fileRecords.slice(first.sizeInBytes(), second.sizeInBytes());    assertEquals(second.sizeInBytes(), read.sizeInBytes());    assertEquals("Read a single message starting from the second message", Collections.singletonList(second), batches(read));}
public void kafkatest_f7623_0() throws IOException
{    File temp = tempFile();    FileRecords fileRecords = FileRecords.open(temp, false, 1024 * 1024, true);    append(fileRecords, values);    int oldPosition = (int) fileRecords.channel().position();    int oldSize = fileRecords.sizeInBytes();    assertEquals(this.fileRecords.sizeInBytes(), oldPosition);    assertEquals(this.fileRecords.sizeInBytes(), oldSize);    fileRecords.close();    File tempReopen = new File(temp.getAbsolutePath());    FileRecords setReopen = FileRecords.open(tempReopen, true, 1024 * 1024, true);    int position = (int) setReopen.channel().position();    int size = setReopen.sizeInBytes();    assertEquals(oldPosition, position);    assertEquals(oldPosition, size);    assertEquals(oldPosition, tempReopen.length());}
public void kafkatest_f7624_0() throws IOException
{    RecordBatch batch = batches(fileRecords).get(1);    int start = fileRecords.searchForOffsetWithSize(1, 0).position;    int size = batch.sizeInBytes();    FileRecords slice = fileRecords.slice(start, size - 1);    Records messageV0 = slice.downConvert(RecordBatch.MAGIC_VALUE_V0, 0, time).records();    assertTrue("No message should be there", batches(messageV0).isEmpty());    assertEquals("There should be " + (size - 1) + " bytes", size - 1, messageV0.sizeInBytes());    // Lazy down-conversion will not return any messages for a partial input batch    TopicPartition tp = new TopicPartition("topic-1", 0);    LazyDownConversionRecords lazyRecords = new LazyDownConversionRecords(tp, slice, RecordBatch.MAGIC_VALUE_V0, 0, Time.SYSTEM);    Iterator<ConvertedRecords<?>> it = lazyRecords.iterator(16 * 1024L);    assertTrue("No messages should be returned", !it.hasNext());}
private void kafkatest_f7633_0(List<SimpleRecord> initialRecords, List<Long> initialOffsets, List<Records> convertedRecordsList, CompressionType compressionType, byte magicByte)
{    int i = 0;    for (Records convertedRecords : convertedRecordsList) {        for (RecordBatch batch : convertedRecords.batches()) {            assertTrue("Magic byte should be lower than or equal to " + magicByte, batch.magic() <= magicByte);            if (batch.magic() == RecordBatch.MAGIC_VALUE_V0)                assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());            else                assertEquals(TimestampType.CREATE_TIME, batch.timestampType());            assertEquals("Compression type should not be affected by conversion", compressionType, batch.compressionType());            for (Record record : batch) {                assertTrue("Inner record should have magic " + magicByte, record.hasMagic(batch.magic()));                assertEquals("Offset should not change", initialOffsets.get(i).longValue(), record.offset());                assertEquals("Key should not change", utf8(initialRecords.get(i).key()), utf8(record.key()));                assertEquals("Value should not change", utf8(initialRecords.get(i).value()), utf8(record.value()));                assertFalse(record.hasTimestampType(TimestampType.LOG_APPEND_TIME));                if (batch.magic() == RecordBatch.MAGIC_VALUE_V0) {                    assertEquals(RecordBatch.NO_TIMESTAMP, record.timestamp());                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                    assertTrue(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                } else if (batch.magic() == RecordBatch.MAGIC_VALUE_V1) {                    assertEquals("Timestamp should not change", initialRecords.get(i).timestamp(), record.timestamp());                    assertTrue(record.hasTimestampType(TimestampType.CREATE_TIME));                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                } else {                    assertEquals("Timestamp should not change", initialRecords.get(i).timestamp(), record.timestamp());                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                    assertArrayEquals("Headers should not change", initialRecords.get(i).headers(), record.headers());                }                i += 1;            }        }    }    assertEquals(initialOffsets.size(), i);}
private static List<RecordBatch> kafkatest_f7634_0(Records buffer)
{    return TestUtils.toList(buffer.batches());}
public void kafkatest_f7643_0() throws Exception
{    if (!close || (useBrokenFlagDescriptorChecksum && !ignoreFlagDescriptorChecksum))        return;    byte[] compressed = compressedBytes();    ByteBuffer buffer = ByteBuffer.wrap(compressed).order(ByteOrder.LITTLE_ENDIAN);    int blockSize = buffer.getInt(7);    blockSize = (blockSize & LZ4_FRAME_INCOMPRESSIBLE_MASK) | (1 << 24 & ~LZ4_FRAME_INCOMPRESSIBLE_MASK);    buffer.putInt(7, blockSize);    IOException e = assertThrows(IOException.class, () -> testDecompression(buffer));    assertThat(e.getMessage(), CoreMatchers.containsString("exceeded max"));}
public void kafkatest_f7644_0() throws Exception
{    byte[] compressed = compressedBytes();    // Check magic bytes stored as little-endian    int offset = 0;    assertEquals(0x04, compressed[offset++]);    assertEquals(0x22, compressed[offset++]);    assertEquals(0x4D, compressed[offset++]);    assertEquals(0x18, compressed[offset++]);    // Check flg descriptor    byte flg = compressed[offset++];    // 2-bit version must be 01    int version = (flg >>> 6) & 3;    assertEquals(1, version);    // Reserved bits should always be 0    int reserved = flg & 3;    assertEquals(0, reserved);    // Check block descriptor    byte bd = compressed[offset++];    // Block max-size    int blockMaxSize = (bd >>> 4) & 7;    // Only supported values are 4 (64KB), 5 (256KB), 6 (1MB), 7 (4MB)    assertTrue(blockMaxSize >= 4);    assertTrue(blockMaxSize <= 7);    // Multiple reserved bit ranges in block descriptor    reserved = bd & 15;    assertEquals(0, reserved);    reserved = (bd >>> 7) & 1;    assertEquals(0, reserved);    // If flg descriptor sets content size flag    // there are 8 additional bytes before checksum    boolean contentSize = ((flg >>> 3) & 1) != 0;    if (contentSize)        offset += 8;    // Checksum applies to frame descriptor: flg, bd, and optional contentsize    // so initial offset should be 4 (for magic bytes)    int off = 4;    int len = offset - 4;    // including magic bytes    if (this.useBrokenFlagDescriptorChecksum) {        off = 0;        len = offset;    }    int hash = XXHashFactory.fastestInstance().hash32().hash(compressed, off, len, 0);    byte hc = compressed[offset++];    assertEquals((byte) ((hash >> 8) & 0xFF), hc);    // Check EndMark, data block with size `0` expressed as a 32-bits value    if (this.close) {        offset = compressed.length - 4;        assertEquals(0, compressed[offset++]);        assertEquals(0, compressed[offset++]);        assertEquals(0, compressed[offset++]);        assertEquals(0, compressed[offset++]);    }}
public void kafkatest_f7653_0() throws IOException
{    doTestConversion(false);}
public void kafkatest_f7654_0() throws IOException
{    doTestConversion(true);}
public void kafkatest_f7663_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V0;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    Supplier<MemoryRecordsBuilder> builderSupplier = () -> new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    if (compressionType != CompressionType.ZSTD) {        MemoryRecords records = builderSupplier.get().build();        assertEquals(0, records.sizeInBytes());        assertEquals(bufferOffset, buffer.position());    } else {        Exception e = assertThrows(IllegalArgumentException.class, () -> builderSupplier.get().build());        assertEquals(e.getMessage(), "ZStandard compression is not supported for magic " + magic);    }}
public void kafkatest_f7664_0()
{    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    long pid = 9809;    short epoch = 15;    int sequence = 2342;    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, 0L, 0L, pid, epoch, sequence, true, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    builder.append(System.currentTimeMillis(), "foo".getBytes(), "bar".getBytes());    MemoryRecords records = builder.build();    List<MutableRecordBatch> batches = Utils.toList(records.batches().iterator());    assertEquals(1, batches.size());    assertTrue(batches.get(0).isTransactional());}
public void kafkatest_f7673_0()
{    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    long pid = 9809;    short epoch = 15;    int sequence = RecordBatch.NO_SEQUENCE;    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, 0L, 0L, pid, epoch, sequence, true, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    builder.appendEndTxnMarker(RecordBatch.NO_TIMESTAMP, new EndTransactionMarker(ControlRecordType.ABORT, 0));}
public void kafkatest_f7674_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V0;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(1024);    buffer.position(bufferOffset);    LegacyRecord[] records = new LegacyRecord[] { LegacyRecord.create(magic, 0L, "a".getBytes(), "1".getBytes()), LegacyRecord.create(magic, 1L, "b".getBytes(), "2".getBytes()), LegacyRecord.create(magic, 2L, "c".getBytes(), "3".getBytes()) };    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    int uncompressedSize = 0;    for (LegacyRecord record : records) {        uncompressedSize += record.sizeInBytes() + Records.LOG_OVERHEAD;        builder.append(record);    }    MemoryRecords built = builder.build();    if (compressionType == CompressionType.NONE) {        assertEquals(1.0, builder.compressionRatio(), 0.00001);    } else {        int compressedSize = built.sizeInBytes() - Records.LOG_OVERHEAD - LegacyRecord.RECORD_OVERHEAD_V0;        double computedCompressionRate = (double) compressedSize / uncompressedSize;        assertEquals(computedCompressionRate, builder.compressionRatio(), 0.00001);    }}
public void kafkatest_f7683_0()
{    ByteBuffer buffer = ByteBuffer.allocate(512);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, compressionType, TimestampType.LOG_APPEND_TIME, 0L);    builder.append(10L, "1".getBytes(), "a".getBytes());    builder.close();    int sizeExcludingTxnMarkers = buffer.position();    MemoryRecords.writeEndTransactionalMarker(buffer, 1L, System.currentTimeMillis(), 0, 15L, (short) 0, new EndTransactionMarker(ControlRecordType.ABORT, 0));    int position = buffer.position();    builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, compressionType, TimestampType.CREATE_TIME, 1L);    builder.append(12L, "2".getBytes(), "b".getBytes());    builder.append(13L, "3".getBytes(), "c".getBytes());    builder.close();    sizeExcludingTxnMarkers += buffer.position() - position;    MemoryRecords.writeEndTransactionalMarker(buffer, 14L, System.currentTimeMillis(), 0, 1L, (short) 0, new EndTransactionMarker(ControlRecordType.COMMIT, 0));    buffer.flip();    Supplier<ConvertedRecords<MemoryRecords>> convertedRecordsSupplier = () -> MemoryRecords.readableRecords(buffer).downConvert(RecordBatch.MAGIC_VALUE_V1, 0, time);    if (compressionType != CompressionType.ZSTD) {        ConvertedRecords<MemoryRecords> convertedRecords = convertedRecordsSupplier.get();        MemoryRecords records = convertedRecords.records();        // Transactional markers are skipped when down converting to V1, so exclude them from size        verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 3, records.sizeInBytes(), sizeExcludingTxnMarkers);        List<? extends RecordBatch> batches = Utils.toList(records.batches().iterator());        if (compressionType != CompressionType.NONE) {            assertEquals(2, batches.size());            assertEquals(TimestampType.LOG_APPEND_TIME, batches.get(0).timestampType());            assertEquals(TimestampType.CREATE_TIME, batches.get(1).timestampType());        } else {            assertEquals(3, batches.size());            assertEquals(TimestampType.LOG_APPEND_TIME, batches.get(0).timestampType());            assertEquals(TimestampType.CREATE_TIME, batches.get(1).timestampType());            assertEquals(TimestampType.CREATE_TIME, batches.get(2).timestampType());        }        List<Record> logRecords = Utils.toList(records.records().iterator());        assertEquals(3, logRecords.size());        assertEquals(ByteBuffer.wrap("1".getBytes()), logRecords.get(0).key());        assertEquals(ByteBuffer.wrap("2".getBytes()), logRecords.get(1).key());        assertEquals(ByteBuffer.wrap("3".getBytes()), logRecords.get(2).key());    } else {        Exception e = assertThrows(UnsupportedCompressionTypeException.class, convertedRecordsSupplier::get);        assertEquals("Down-conversion of zstandard-compressed batches is not supported", e.getMessage());    }}
public void kafkatest_f7684_0()
{    assumeAtLeastV2OrNotZstd(RecordBatch.MAGIC_VALUE_V0);    assumeAtLeastV2OrNotZstd(RecordBatch.MAGIC_VALUE_V1);    ByteBuffer buffer = ByteBuffer.allocate(512);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V0, compressionType, TimestampType.NO_TIMESTAMP_TYPE, 0L);    builder.append(RecordBatch.NO_TIMESTAMP, "1".getBytes(), "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, compressionType, TimestampType.CREATE_TIME, 1L);    builder.append(11L, "2".getBytes(), "b".getBytes());    builder.append(12L, "3".getBytes(), "c".getBytes());    builder.close();    buffer.flip();    ConvertedRecords<MemoryRecords> convertedRecords = MemoryRecords.readableRecords(buffer).downConvert(RecordBatch.MAGIC_VALUE_V1, 0, time);    MemoryRecords records = convertedRecords.records();    verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 2, records.sizeInBytes(), buffer.limit());    List<? extends RecordBatch> batches = Utils.toList(records.batches().iterator());    if (compressionType != CompressionType.NONE) {        assertEquals(2, batches.size());        assertEquals(RecordBatch.MAGIC_VALUE_V0, batches.get(0).magic());        assertEquals(0, batches.get(0).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(1).magic());        assertEquals(1, batches.get(1).baseOffset());    } else {        assertEquals(3, batches.size());        assertEquals(RecordBatch.MAGIC_VALUE_V0, batches.get(0).magic());        assertEquals(0, batches.get(0).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(1).magic());        assertEquals(1, batches.get(1).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(2).magic());        assertEquals(2, batches.get(2).baseOffset());    }    List<Record> logRecords = Utils.toList(records.records().iterator());    assertEquals("1", utf8(logRecords.get(0).key()));    assertEquals("2", utf8(logRecords.get(1).key()));    assertEquals("3", utf8(logRecords.get(2).key()));    convertedRecords = MemoryRecords.readableRecords(buffer).downConvert(RecordBatch.MAGIC_VALUE_V1, 2L, time);    records = convertedRecords.records();    batches = Utils.toList(records.batches().iterator());    logRecords = Utils.toList(records.records().iterator());    if (compressionType != CompressionType.NONE) {        assertEquals(2, batches.size());        assertEquals(RecordBatch.MAGIC_VALUE_V0, batches.get(0).magic());        assertEquals(0, batches.get(0).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(1).magic());        assertEquals(1, batches.get(1).baseOffset());        assertEquals("1", utf8(logRecords.get(0).key()));        assertEquals("2", utf8(logRecords.get(1).key()));        assertEquals("3", utf8(logRecords.get(2).key()));        verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 2, records.sizeInBytes(), buffer.limit());    } else {        assertEquals(2, batches.size());        assertEquals(RecordBatch.MAGIC_VALUE_V0, batches.get(0).magic());        assertEquals(0, batches.get(0).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(1).magic());        assertEquals(2, batches.get(1).baseOffset());        assertEquals("1", utf8(logRecords.get(0).key()));        assertEquals("3", utf8(logRecords.get(1).key()));        verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 1, records.sizeInBytes(), buffer.limit());    }}
public void kafkatest_f7693_0()
{    assumeAtLeastV2OrNotZstd();    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compression, TimestampType.CREATE_TIME, firstOffset, logAppendTime, pid, epoch, firstSequence, false, false, partitionLeaderEpoch, buffer.limit());    SimpleRecord[] records = new SimpleRecord[] { new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()), new SimpleRecord(4L, null, "4".getBytes()), new SimpleRecord(5L, "d".getBytes(), null), new SimpleRecord(6L, (byte[]) null, null) };    for (SimpleRecord record : records) builder.append(record);    MemoryRecords memoryRecords = builder.build();    for (int iteration = 0; iteration < 2; iteration++) {        int total = 0;        for (RecordBatch batch : memoryRecords.batches()) {            assertTrue(batch.isValid());            assertEquals(compression, batch.compressionType());            assertEquals(firstOffset + total, batch.baseOffset());            if (magic >= RecordBatch.MAGIC_VALUE_V2) {                assertEquals(pid, batch.producerId());                assertEquals(epoch, batch.producerEpoch());                assertEquals(firstSequence + total, batch.baseSequence());                assertEquals(partitionLeaderEpoch, batch.partitionLeaderEpoch());                assertEquals(records.length, batch.countOrNull().intValue());                assertEquals(TimestampType.CREATE_TIME, batch.timestampType());                assertEquals(records[records.length - 1].timestamp(), batch.maxTimestamp());            } else {                assertEquals(RecordBatch.NO_PRODUCER_ID, batch.producerId());                assertEquals(RecordBatch.NO_PRODUCER_EPOCH, batch.producerEpoch());                assertEquals(RecordBatch.NO_SEQUENCE, batch.baseSequence());                assertEquals(RecordBatch.NO_PARTITION_LEADER_EPOCH, batch.partitionLeaderEpoch());                assertNull(batch.countOrNull());                if (magic == RecordBatch.MAGIC_VALUE_V0)                    assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());                else                    assertEquals(TimestampType.CREATE_TIME, batch.timestampType());            }            int recordCount = 0;            for (Record record : batch) {                assertTrue(record.isValid());                assertTrue(record.hasMagic(batch.magic()));                assertFalse(record.isCompressed());                assertEquals(firstOffset + total, record.offset());                assertEquals(records[total].key(), record.key());                assertEquals(records[total].value(), record.value());                if (magic >= RecordBatch.MAGIC_VALUE_V2)                    assertEquals(firstSequence + total, record.sequence());                assertFalse(record.hasTimestampType(TimestampType.LOG_APPEND_TIME));                if (magic == RecordBatch.MAGIC_VALUE_V0) {                    assertEquals(RecordBatch.NO_TIMESTAMP, record.timestamp());                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                    assertTrue(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                } else {                    assertEquals(records[total].timestamp(), record.timestamp());                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                    if (magic < RecordBatch.MAGIC_VALUE_V2)                        assertTrue(record.hasTimestampType(TimestampType.CREATE_TIME));                    else                        assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                }                total++;                recordCount++;            }            assertEquals(batch.baseOffset() + recordCount - 1, batch.lastOffset());        }    }}
public void kafkatest_f7694_0()
{    assumeAtLeastV2OrNotZstd();    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), magic, compression, TimestampType.CREATE_TIME, 0L);    builder.append(0L, "a".getBytes(), "1".getBytes());    assertTrue(builder.hasRoomFor(1L, "b".getBytes(), "2".getBytes(), Record.EMPTY_HEADERS));    builder.close();    assertFalse(builder.hasRoomFor(1L, "b".getBytes(), "2".getBytes(), Record.EMPTY_HEADERS));}
protected boolean kafkatest_f7703_0(RecordBatch recordBatch, Record record)
{    return false;}
public void kafkatest_f7704_0()
{    if (magic >= RecordBatch.MAGIC_VALUE_V2) {        for (final BatchRetention deleteRetention : Arrays.asList(BatchRetention.DELETE, BatchRetention.DELETE_EMPTY)) {            ByteBuffer buffer = ByteBuffer.allocate(DefaultRecordBatch.RECORD_BATCH_OVERHEAD);            long producerId = 23L;            short producerEpoch = 5;            long baseOffset = 3L;            int baseSequence = 10;            int partitionLeaderEpoch = 293;            long timestamp = System.currentTimeMillis();            DefaultRecordBatch.writeEmptyHeader(buffer, RecordBatch.MAGIC_VALUE_V2, producerId, producerEpoch, baseSequence, baseOffset, baseOffset, partitionLeaderEpoch, TimestampType.CREATE_TIME, timestamp, false, false);            buffer.flip();            ByteBuffer filtered = ByteBuffer.allocate(2048);            MemoryRecords records = MemoryRecords.readableRecords(buffer);            MemoryRecords.FilterResult filterResult = records.filterTo(new TopicPartition("foo", 0), new MemoryRecords.RecordFilter() {                @Override                protected BatchRetention checkBatchRetention(RecordBatch batch) {                    return deleteRetention;                }                @Override                protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) {                    return false;                }            }, filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);            // Verify filter result            assertEquals(0, filterResult.outputBuffer().position());            // Verify filtered records            filtered.flip();            MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);            assertEquals(0, filteredRecords.sizeInBytes());        }    }}
public void kafkatest_f7713_0()
{    assumeAtLeastV2OrNotZstd();    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 0L);    builder.append(10L, null, "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 1L);    builder.append(11L, "1".getBytes(), new byte[128]);    builder.append(12L, "2".getBytes(), "c".getBytes());    builder.append(13L, null, "d".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 4L);    builder.append(14L, null, "e".getBytes());    builder.append(15L, "5".getBytes(), "f".getBytes());    builder.append(16L, "6".getBytes(), "g".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 7L);    builder.append(17L, "7".getBytes(), new byte[128]);    builder.close();    buffer.flip();    ByteBuffer output = ByteBuffer.allocate(64);    List<Record> records = new ArrayList<>();    while (buffer.hasRemaining()) {        output.rewind();        MemoryRecords.FilterResult result = MemoryRecords.readableRecords(buffer).filterTo(new TopicPartition("foo", 0), new RetainNonNullKeysFilter(), output, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);        buffer.position(buffer.position() + result.bytesRead());        result.outputBuffer().flip();        if (output != result.outputBuffer())            assertEquals(0, output.position());        MemoryRecords filtered = MemoryRecords.readableRecords(result.outputBuffer());        records.addAll(TestUtils.toList(filtered.records()));    }    assertEquals(5, records.size());    for (Record record : records) assertNotNull(record.key());}
public void kafkatest_f7714_0()
{    assumeAtLeastV2OrNotZstd();    ByteBuffer buffer = ByteBuffer.allocate(2048);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 0L);    builder.append(10L, null, "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 1L);    builder.append(11L, "1".getBytes(), "b".getBytes());    builder.append(12L, null, "c".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 3L);    builder.append(13L, null, "d".getBytes());    builder.append(20L, "4".getBytes(), "e".getBytes());    builder.append(15L, "5".getBytes(), "f".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 6L);    builder.append(16L, "6".getBytes(), "g".getBytes());    builder.close();    buffer.flip();    ByteBuffer filtered = ByteBuffer.allocate(2048);    MemoryRecords.FilterResult result = MemoryRecords.readableRecords(buffer).filterTo(new TopicPartition("foo", 0), new RetainNonNullKeysFilter(), filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);    filtered.flip();    assertEquals(7, result.messagesRead());    assertEquals(4, result.messagesRetained());    assertEquals(buffer.limit(), result.bytesRead());    assertEquals(filtered.limit(), result.bytesRetained());    if (magic > RecordBatch.MAGIC_VALUE_V0) {        assertEquals(20L, result.maxTimestamp());        if (compression == CompressionType.NONE && magic < RecordBatch.MAGIC_VALUE_V2)            assertEquals(4L, result.shallowOffsetOfMaxTimestamp());        else            assertEquals(5L, result.shallowOffsetOfMaxTimestamp());    }    MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);    List<MutableRecordBatch> batches = TestUtils.toList(filteredRecords.batches());    final List<Long> expectedEndOffsets;    final List<Long> expectedStartOffsets;    final List<Long> expectedMaxTimestamps;    if (magic < RecordBatch.MAGIC_VALUE_V2 && compression == CompressionType.NONE) {        expectedEndOffsets = asList(1L, 4L, 5L, 6L);        expectedStartOffsets = asList(1L, 4L, 5L, 6L);        expectedMaxTimestamps = asList(11L, 20L, 15L, 16L);    } else if (magic < RecordBatch.MAGIC_VALUE_V2) {        expectedEndOffsets = asList(1L, 5L, 6L);        expectedStartOffsets = asList(1L, 4L, 6L);        expectedMaxTimestamps = asList(11L, 20L, 16L);    } else {        expectedEndOffsets = asList(2L, 5L, 6L);        expectedStartOffsets = asList(1L, 3L, 6L);        expectedMaxTimestamps = asList(11L, 20L, 16L);    }    assertEquals(expectedEndOffsets.size(), batches.size());    for (int i = 0; i < expectedEndOffsets.size(); i++) {        RecordBatch batch = batches.get(i);        assertEquals(expectedStartOffsets.get(i).longValue(), batch.baseOffset());        assertEquals(expectedEndOffsets.get(i).longValue(), batch.lastOffset());        assertEquals(magic, batch.magic());        assertEquals(compression, batch.compressionType());        if (magic >= RecordBatch.MAGIC_VALUE_V1) {            assertEquals(expectedMaxTimestamps.get(i).longValue(), batch.maxTimestamp());            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());        } else {            assertEquals(RecordBatch.NO_TIMESTAMP, batch.maxTimestamp());            assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());        }    }    List<Record> records = TestUtils.toList(filteredRecords.records());    assertEquals(4, records.size());    Record first = records.get(0);    assertEquals(1L, first.offset());    if (magic > RecordBatch.MAGIC_VALUE_V0)        assertEquals(11L, first.timestamp());    assertEquals("1", Utils.utf8(first.key(), first.keySize()));    assertEquals("b", Utils.utf8(first.value(), first.valueSize()));    Record second = records.get(1);    assertEquals(4L, second.offset());    if (magic > RecordBatch.MAGIC_VALUE_V0)        assertEquals(20L, second.timestamp());    assertEquals("4", Utils.utf8(second.key(), second.keySize()));    assertEquals("e", Utils.utf8(second.value(), second.valueSize()));    Record third = records.get(2);    assertEquals(5L, third.offset());    if (magic > RecordBatch.MAGIC_VALUE_V0)        assertEquals(15L, third.timestamp());    assertEquals("5", Utils.utf8(third.key(), third.keySize()));    assertEquals("f", Utils.utf8(third.value(), third.valueSize()));    Record fourth = records.get(3);    assertEquals(6L, fourth.offset());    if (magic > RecordBatch.MAGIC_VALUE_V0)        assertEquals(16L, fourth.timestamp());    assertEquals("6", Utils.utf8(fourth.key(), fourth.keySize()));    assertEquals("g", Utils.utf8(fourth.value(), fourth.valueSize()));}
public long kafkatest_f7723_0(ByteBuffer[] srcs) throws IOException
{    // which allows us to test the MultiRecordsSend behavior on a per-send basis.    if (!buffer().hasRemaining())        return 0;    return super.write(srcs);}
public void kafkatest_f7724_0() throws Exception
{    ByteBuffer buffer = ByteBuffer.allocate(128);    DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));    AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);    LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null, CompressionType.GZIP, TimestampType.CREATE_TIME);    buffer.flip();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    if (records.records().iterator().hasNext())        fail("Iteration should have caused invalid record error");}
public void kafkatest_f7733_0()
{    final ApiVersionsResponse response = ApiVersionsResponse.apiVersionsResponse(10, RecordBatch.MAGIC_VALUE_V1);    verifyApiKeysForMagic(response, RecordBatch.MAGIC_VALUE_V1);    assertEquals(10, response.throttleTimeMs());}
public void kafkatest_f7734_0()
{    assertEquals(apiKeysInResponse(ApiVersionsResponse.defaultApiVersionsResponse()), Utils.mkSet(ApiKeys.values()));}
public void kafkatest_f7743_0() throws IOException
{    buf.flip();    closed = true;}
public ByteBuffer kafkatest_f7744_0()
{    return buf;}
private static void kafkatest_f7753_0(final CreateAclsRequest original, final CreateAclsRequest actual)
{    assertEquals("Number of Acls wrong", original.aclCreations().size(), actual.aclCreations().size());    for (int idx = 0; idx != original.aclCreations().size(); ++idx) {        final AclBinding originalBinding = original.aclCreations().get(idx).acl();        final AclBinding actualBinding = actual.aclCreations().get(idx).acl();        assertEquals(originalBinding, actualBinding);    }}
private static List<AclCreation> kafkatest_f7754_0(final AclBinding... acls)
{    return Arrays.stream(acls).map(AclCreation::new).collect(Collectors.toList());}
public void kafkatest_f7763_0()
{    new DeleteAclsResponse(10, aclResponses(UNKNOWN_RESPONSE)).toStruct(V1);}
public void kafkatest_f7764_0()
{    final DeleteAclsResponse original = new DeleteAclsResponse(10, aclResponses(LITERAL_RESPONSE));    final Struct struct = original.toStruct(V0);    final DeleteAclsResponse result = new DeleteAclsResponse(struct);    assertResponseEquals(original, result);}
public void kafkatest_f7773_0()
{    new DescribeAclsRequest(UNKNOWN_FILTER, V0);}
public void kafkatest_f7774_0()
{    final DescribeAclsRequest original = new DescribeAclsRequest(LITERAL_FILTER, V0);    final Struct struct = original.toStruct();    final DescribeAclsRequest result = new DescribeAclsRequest(struct, V0);    assertRequestEquals(original, result);}
public void kafkatest_f7783_0()
{    final DescribeAclsResponse original = new DescribeAclsResponse(100, ApiError.NONE, aclBindings(LITERAL_ACL1, PREFIXED_ACL1));    final Struct struct = original.toStruct(V1);    final DescribeAclsResponse result = new DescribeAclsResponse(struct);    assertResponseEquals(original, result);}
private static void kafkatest_f7784_0(final DescribeAclsResponse original, final DescribeAclsResponse actual)
{    final Set<AclBinding> originalBindings = new HashSet<>(original.acls());    final Set<AclBinding> actualBindings = new HashSet<>(actual.acls());    assertEquals(originalBindings, actualBindings);}
public void kafkatest_f7793_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.NOT_LEADER_FOR_PARTITION);    LeaderAndIsrResponse response = new LeaderAndIsrResponse(Errors.UNKNOWN_SERVER_ERROR, errors);    assertEquals(Collections.singletonMap(Errors.UNKNOWN_SERVER_ERROR, 2), response.errorCounts());}
public void kafkatest_f7794_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.CLUSTER_AUTHORIZATION_FAILED);    LeaderAndIsrResponse response = new LeaderAndIsrResponse(Errors.NONE, errors);    Map<Errors, Integer> errorCounts = response.errorCounts();    assertEquals(2, errorCounts.size());    assertEquals(1, errorCounts.get(Errors.NONE).intValue());    assertEquals(1, errorCounts.get(Errors.CLUSTER_AUTHORIZATION_FAILED).intValue());}
public void kafkatest_f7803_0()
{    LeaveGroupResponse response = new LeaveGroupResponse(new LeaveGroupResponseData());    for (short version = 0; version <= ApiKeys.LEAVE_GROUP.latestVersion(); version++) {        if (version >= 2) {            assertTrue(response.shouldClientThrottle(version));        } else {            assertFalse(response.shouldClientThrottle(version));        }    }}
public void kafkatest_f7804_0()
{    LeaveGroupResponseData responseData = new LeaveGroupResponseData().setErrorCode(Errors.NONE.code()).setThrottleTimeMs(throttleTimeMs);    for (short version = 0; version <= ApiKeys.LEAVE_GROUP.latestVersion(); version++) {        LeaveGroupResponse primaryResponse = new LeaveGroupResponse(responseData.toStruct(version), version);        LeaveGroupResponse secondaryResponse = new LeaveGroupResponse(responseData.toStruct(version), version);        assertEquals(primaryResponse, primaryResponse);        assertEquals(primaryResponse, secondaryResponse);        assertEquals(primaryResponse.hashCode(), secondaryResponse.hashCode());    }}
public void kafkatest_f7813_0()
{    expectedErrorCounts = new HashMap<>();    expectedErrorCounts.put(errorOne, 1);    expectedErrorCounts.put(errorTwo, 1);    errorsMap = new HashMap<>();    errorsMap.put(tp1, errorOne);    errorsMap.put(tp2, errorTwo);}
public void kafkatest_f7814_0()
{    OffsetCommitResponse response = new OffsetCommitResponse(throttleTimeMs, errorsMap);    assertEquals(expectedErrorCounts, response.errorCounts());    assertEquals(throttleTimeMs, response.throttleTimeMs());}
public void kafkatest_f7823_0()
{    partitionDataMap.clear();    partitionDataMap.put(new TopicPartition(topicOne, partitionOne), new PartitionData(offset, leaderEpochOne, null, Errors.UNKNOWN_TOPIC_OR_PARTITION));    OffsetFetchResponse response = new OffsetFetchResponse(throttleTimeMs, Errors.GROUP_AUTHORIZATION_FAILED, partitionDataMap);    OffsetFetchResponseData expectedData = new OffsetFetchResponseData().setErrorCode(Errors.GROUP_AUTHORIZATION_FAILED.code()).setThrottleTimeMs(throttleTimeMs).setTopics(Collections.singletonList(new OffsetFetchResponseTopic().setName(topicOne).setPartitions(Collections.singletonList(new OffsetFetchResponsePartition().setPartitionIndex(partitionOne).setCommittedOffset(offset).setCommittedLeaderEpoch(leaderEpochOne.orElse(-1)).setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code()).setMetadata(null)))));    assertEquals(expectedData, response.data);}
public void kafkatest_f7824_0()
{    final Optional<Integer> emptyLeaderEpoch = Optional.empty();    partitionDataMap.clear();    partitionDataMap.put(new TopicPartition(topicOne, partitionOne), new PartitionData(offset, emptyLeaderEpoch, metadata, Errors.UNKNOWN_TOPIC_OR_PARTITION));    OffsetFetchResponse response = new OffsetFetchResponse(throttleTimeMs, Errors.NOT_COORDINATOR, partitionDataMap);    OffsetFetchResponseData expectedData = new OffsetFetchResponseData().setErrorCode(Errors.NOT_COORDINATOR.code()).setThrottleTimeMs(throttleTimeMs).setTopics(Collections.singletonList(new OffsetFetchResponseTopic().setName(topicOne).setPartitions(Collections.singletonList(new OffsetFetchResponsePartition().setPartitionIndex(partitionOne).setCommittedOffset(offset).setCommittedLeaderEpoch(RecordBatch.NO_PARTITION_LEADER_EPOCH).setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code()).setMetadata(metadata)))));    assertEquals(expectedData, response.data);}
public void kafkatest_f7833_0()
{    ByteBuffer buffer = ByteBuffer.allocate(256);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    builder.append(10L, null, "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 1L);    builder.append(11L, "1".getBytes(), "b".getBytes());    builder.append(12L, null, "c".getBytes());    builder.close();    buffer.flip();    Map<TopicPartition, MemoryRecords> produceData = new HashMap<>();    produceData.put(new TopicPartition("test", 0), MemoryRecords.readableRecords(buffer));    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forCurrentMagic((short) 1, 5000, produceData);    assertThrowsInvalidRecordExceptionForAllVersions(requestBuilder);}
public void kafkatest_f7834_0()
{    Map<TopicPartition, MemoryRecords> produceData = new HashMap<>();    produceData.put(new TopicPartition("test", 0), MemoryRecords.EMPTY);    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forCurrentMagic((short) 1, 5000, produceData);    assertThrowsInvalidRecordExceptionForAllVersions(requestBuilder);}
public void kafkatest_f7843_0() throws Exception
{    int correlationId = 23423;    RequestHeader header = new RequestHeader(ApiKeys.API_VERSIONS, Short.MAX_VALUE, "", correlationId);    RequestContext context = new RequestContext(header, "0", InetAddress.getLocalHost(), KafkaPrincipal.ANONYMOUS, new ListenerName("ssl"), SecurityProtocol.SASL_SSL);    assertEquals(0, context.apiVersion());    // Write some garbage to the request buffer. This should be ignored since we will treat    // the unknown version type as v0 which has an empty request body.    ByteBuffer requestBuffer = ByteBuffer.allocate(8);    requestBuffer.putInt(3709234);    requestBuffer.putInt(29034);    requestBuffer.flip();    RequestAndSize requestAndSize = context.parseRequest(requestBuffer);    assertTrue(requestAndSize.request instanceof ApiVersionsRequest);    ApiVersionsRequest request = (ApiVersionsRequest) requestAndSize.request;    assertTrue(request.hasUnsupportedRequestVersion());    Send send = context.buildResponse(new ApiVersionsResponse(0, Errors.UNSUPPORTED_VERSION, Collections.<ApiVersionsResponse.ApiVersion>emptyList()));    ByteBufferChannel channel = new ByteBufferChannel(256);    send.writeTo(channel);    ByteBuffer responseBuffer = channel.buffer();    responseBuffer.flip();    // strip off the size    responseBuffer.getInt();    ResponseHeader responseHeader = ResponseHeader.parse(responseBuffer);    assertEquals(correlationId, responseHeader.correlationId());    Struct struct = ApiKeys.API_VERSIONS.parseResponse((short) 0, responseBuffer);    ApiVersionsResponse response = (ApiVersionsResponse) AbstractResponse.parseResponse(ApiKeys.API_VERSIONS, struct, (short) 0);    assertEquals(Errors.UNSUPPORTED_VERSION, response.error());    assertTrue(response.apiVersions().isEmpty());}
public void kafkatest_f7844_0()
{    // Verify that version 0 of controlled shutdown does not include the clientId field    int correlationId = 2342;    ByteBuffer rawBuffer = ByteBuffer.allocate(32);    rawBuffer.putShort(ApiKeys.CONTROLLED_SHUTDOWN.id);    rawBuffer.putShort((short) 0);    rawBuffer.putInt(correlationId);    rawBuffer.flip();    RequestHeader deserialized = RequestHeader.parse(rawBuffer);    assertEquals(ApiKeys.CONTROLLED_SHUTDOWN, deserialized.apiKey());    assertEquals(0, deserialized.apiVersion());    assertEquals(correlationId, deserialized.correlationId());    assertEquals("", deserialized.clientId());    Struct serialized = deserialized.toStruct();    ByteBuffer serializedBuffer = toBuffer(serialized);    assertEquals(ApiKeys.CONTROLLED_SHUTDOWN.id, serializedBuffer.getShort(0));    assertEquals(0, serializedBuffer.getShort(2));    assertEquals(correlationId, serializedBuffer.getInt(4));    assertEquals(8, serializedBuffer.limit());}
private void kafkatest_f7853_0(AbstractRequest req, boolean checkEqualityAndHashCode)
{    // in the request is a HashMap with multiple elements since ordering of the elements may vary)    try {        Struct struct = req.toStruct();        AbstractRequest deserialized = AbstractRequest.parseRequest(req.api, req.version(), struct);        Struct struct2 = deserialized.toStruct();        if (checkEqualityAndHashCode) {            assertEquals(struct, struct2);            assertEquals(struct.hashCode(), struct2.hashCode());        }    } catch (Exception e) {        throw new RuntimeException("Failed to deserialize request " + req + " with type " + req.getClass(), e);    }}
private void kafkatest_f7854_0(AbstractResponse response, int version, boolean checkEqualityAndHashCode)
{    // in the response is a HashMap with multiple elements since ordering of the elements may vary)    try {        Struct struct = response.toStruct((short) version);        AbstractResponse deserialized = (AbstractResponse) deserialize(response, struct, (short) version);        Struct struct2 = deserialized.toStruct((short) version);        if (checkEqualityAndHashCode) {            assertEquals(struct, struct2);            assertEquals(struct.hashCode(), struct2.hashCode());        }    } catch (Exception e) {        throw new RuntimeException("Failed to deserialize response " + response + " with type " + response.getClass(), e);    }}
public void kafkatest_f7863_0() throws Exception
{    verifyFetchResponseFullWrite(ApiKeys.FETCH.latestVersion(), createFetchResponse(123));    verifyFetchResponseFullWrite(ApiKeys.FETCH.latestVersion(), createFetchResponse(Errors.FETCH_SESSION_ID_NOT_FOUND, 123));    for (short version = 0; version <= ApiKeys.FETCH.latestVersion(); version++) {        verifyFetchResponseFullWrite(version, createFetchResponse());    }}
private void kafkatest_f7864_0(short apiVersion, FetchResponse fetchResponse) throws Exception
{    int correlationId = 15;    Send send = fetchResponse.toSend("1", new ResponseHeader(correlationId), apiVersion);    ByteBufferChannel channel = new ByteBufferChannel(send.size());    send.writeTo(channel);    channel.close();    ByteBuffer buf = channel.buffer();    // read the size    int size = buf.getInt();    assertTrue(size > 0);    // read the header    ResponseHeader responseHeader = ResponseHeader.parse(channel.buffer());    assertEquals(correlationId, responseHeader.correlationId());    // read the body    Struct responseBody = ApiKeys.FETCH.responseSchema(apiVersion).read(buf);    assertEquals(fetchResponse.toStruct(apiVersion), responseBody);    assertEquals(size, responseHeader.sizeOf() + responseBody.sizeOf());}
private ResponseHeader kafkatest_f7873_0()
{    return new ResponseHeader(10);}
private FindCoordinatorRequest kafkatest_f7874_0(int version)
{    return new FindCoordinatorRequest.Builder(new FindCoordinatorRequestData().setKeyType(CoordinatorType.GROUP.id()).setKey("test-group")).build((short) version);}
private HeartbeatResponse kafkatest_f7883_0()
{    return new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(Errors.NONE.code()));}
private JoinGroupRequest kafkatest_f7884_0(int version)
{    JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols = new JoinGroupRequestData.JoinGroupRequestProtocolCollection(Collections.singleton(new JoinGroupRequestData.JoinGroupRequestProtocol().setName("consumer-range").setMetadata(new byte[0])).iterator());    if (version <= 4) {        return new JoinGroupRequest.Builder(new JoinGroupRequestData().setGroupId("group1").setSessionTimeoutMs(30000).setMemberId("consumer1").setGroupInstanceId(null).setProtocolType("consumer").setProtocols(protocols).setRebalanceTimeoutMs(// v1 and above contains rebalance timeout        60000)).build((short) version);    } else {        return new JoinGroupRequest.Builder(new JoinGroupRequestData().setGroupId("group1").setSessionTimeoutMs(30000).setMemberId("consumer1").setGroupInstanceId(// v5 and above could set group instance id        "groupInstanceId").setProtocolType("consumer").setProtocols(protocols).setRebalanceTimeoutMs(// v1 and above contains rebalance timeout        60000)).build((short) version);    }}
private DeleteGroupsResponse kafkatest_f7893_0()
{    DeletableGroupResultCollection result = new DeletableGroupResultCollection();    result.add(new DeletableGroupResult().setGroupId("test-group").setErrorCode(Errors.NONE.code()));    return new DeleteGroupsResponse(new DeleteGroupsResponseData().setResults(result));}
private ListOffsetRequest kafkatest_f7894_0(int version)
{    if (version == 0) {        Map<TopicPartition, ListOffsetRequest.PartitionData> offsetData = Collections.singletonMap(new TopicPartition("test", 0), new ListOffsetRequest.PartitionData(1000000L, 10));        return ListOffsetRequest.Builder.forConsumer(false, IsolationLevel.READ_UNCOMMITTED).setTargetTimes(offsetData).build((short) version);    } else if (version == 1) {        Map<TopicPartition, ListOffsetRequest.PartitionData> offsetData = Collections.singletonMap(new TopicPartition("test", 0), new ListOffsetRequest.PartitionData(1000000L, Optional.empty()));        return ListOffsetRequest.Builder.forConsumer(true, IsolationLevel.READ_UNCOMMITTED).setTargetTimes(offsetData).build((short) version);    } else if (version == 2) {        Map<TopicPartition, ListOffsetRequest.PartitionData> offsetData = Collections.singletonMap(new TopicPartition("test", 0), new ListOffsetRequest.PartitionData(1000000L, Optional.of(5)));        return ListOffsetRequest.Builder.forConsumer(true, IsolationLevel.READ_COMMITTED).setTargetTimes(offsetData).build((short) version);    } else {        throw new IllegalArgumentException("Illegal ListOffsetRequest version " + version);    }}
private ProduceResponse kafkatest_f7903_0()
{    Map<TopicPartition, ProduceResponse.PartitionResponse> responseData = new HashMap<>();    responseData.put(new TopicPartition("test", 0), new ProduceResponse.PartitionResponse(Errors.NONE, 10000, RecordBatch.NO_TIMESTAMP, 100));    return new ProduceResponse(responseData, 0);}
private StopReplicaRequest kafkatest_f7904_0(int version, boolean deletePartitions)
{    Set<TopicPartition> partitions = Utils.mkSet(new TopicPartition("test", 0));    return new StopReplicaRequest.Builder((short) version, 0, 1, 0, deletePartitions, partitions).build();}
private SaslHandshakeRequest kafkatest_f7913_0()
{    return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism("PLAIN")).build();}
private SaslHandshakeResponse kafkatest_f7914_0()
{    return new SaslHandshakeResponse(new SaslHandshakeResponseData().setErrorCode(Errors.NONE.code()).setMechanisms(Collections.singletonList("GSSAPI")));}
private DeleteTopicsResponse kafkatest_f7923_0()
{    DeleteTopicsResponseData data = new DeleteTopicsResponseData();    data.responses().add(new DeletableTopicResult().setName("t1").setErrorCode(Errors.INVALID_TOPIC_EXCEPTION.code()));    data.responses().add(new DeletableTopicResult().setName("t2").setErrorCode(Errors.TOPIC_AUTHORIZATION_FAILED.code()));    return new DeleteTopicsResponse(data);}
private InitProducerIdRequest kafkatest_f7924_0()
{    InitProducerIdRequestData requestData = new InitProducerIdRequestData().setTransactionalId(null).setTransactionTimeoutMs(100);    return new InitProducerIdRequest.Builder(requestData).build();}
private AddOffsetsToTxnResponse kafkatest_f7933_0()
{    return new AddOffsetsToTxnResponse(0, Errors.NONE);}
private EndTxnRequest kafkatest_f7934_0()
{    return new EndTxnRequest.Builder("tid", 21L, (short) 42, TransactionResult.COMMIT).build();}
private CreateAclsResponse kafkatest_f7943_0()
{    return new CreateAclsResponse(0, Arrays.asList(new AclCreationResponse(ApiError.NONE), new AclCreationResponse(new ApiError(Errors.INVALID_REQUEST, "Foo bar"))));}
private DeleteAclsRequest kafkatest_f7944_0()
{    List<AclBindingFilter> filters = new ArrayList<>();    filters.add(new AclBindingFilter(new ResourcePatternFilter(ResourceType.ANY, null, PatternType.LITERAL), new AccessControlEntryFilter("User:ANONYMOUS", null, AclOperation.ANY, AclPermissionType.ANY)));    filters.add(new AclBindingFilter(new ResourcePatternFilter(ResourceType.ANY, null, PatternType.LITERAL), new AccessControlEntryFilter("User:bob", null, AclOperation.ANY, AclPermissionType.ANY)));    return new DeleteAclsRequest.Builder(filters).build();}
private CreatePartitionsResponse kafkatest_f7953_0()
{    Map<String, ApiError> results = new HashMap<>();    results.put("my_topic", ApiError.fromThrowable(new InvalidReplicaAssignmentException("The assigned brokers included an unknown broker")));    results.put("my_topic", ApiError.NONE);    return new CreatePartitionsResponse(42, results);}
private CreateDelegationTokenRequest kafkatest_f7954_0()
{    List<CreatableRenewers> renewers = new ArrayList<>();    renewers.add(new CreatableRenewers().setPrincipalType("User").setPrincipalName("user1"));    renewers.add(new CreatableRenewers().setPrincipalType("User").setPrincipalName("user2"));    return new CreateDelegationTokenRequest.Builder(new CreateDelegationTokenRequestData().setRenewers(renewers).setMaxLifetimeMs(System.currentTimeMillis())).build();}
private ElectLeadersRequest kafkatest_f7963_0()
{    List<TopicPartition> partitions = asList(new TopicPartition("data", 1), new TopicPartition("data", 2));    return new ElectLeadersRequest.Builder(ElectionType.PREFERRED, partitions, 100).build((short) 1);}
private ElectLeadersResponse kafkatest_f7964_0()
{    String topic = "myTopic";    List<ReplicaElectionResult> electionResults = new ArrayList<>();    ReplicaElectionResult electionResult = new ReplicaElectionResult();    electionResult.setTopic(topic);    // Add partition 1 result    PartitionResult partitionResult = new PartitionResult();    partitionResult.setPartitionId(0);    partitionResult.setErrorCode(ApiError.NONE.error().code());    partitionResult.setErrorMessage(ApiError.NONE.message());    electionResult.partitionResult().add(partitionResult);    // Add partition 2 result    partitionResult = new PartitionResult();    partitionResult.setPartitionId(1);    partitionResult.setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());    partitionResult.setErrorMessage(Errors.UNKNOWN_TOPIC_OR_PARTITION.message());    electionResult.partitionResult().add(partitionResult);    return new ElectLeadersResponse(200, Errors.NONE.code(), electionResults);}
public void kafkatest_f7973_0()
{    StopReplicaRequest request = new StopReplicaRequest.Builder(ApiKeys.STOP_REPLICA.latestVersion(), 15, 20, 0, false, Utils.mkSet(new TopicPartition("foo", 0), new TopicPartition("foo", 1))).build();    StopReplicaResponse response = request.getErrorResponse(0, Errors.CLUSTER_AUTHORIZATION_FAILED.exception());    assertEquals(Collections.singletonMap(Errors.CLUSTER_AUTHORIZATION_FAILED, 2), response.errorCounts());}
public void kafkatest_f7974_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.NOT_LEADER_FOR_PARTITION);    StopReplicaResponse response = new StopReplicaResponse(Errors.UNKNOWN_SERVER_ERROR, errors);    assertEquals(Collections.singletonMap(Errors.UNKNOWN_SERVER_ERROR, 2), response.errorCounts());}
public void kafkatest_f7983_0()
{    assertFalse(new ResourceFilter(TOPIC, "Different").matches(new Resource(TOPIC, "Name")));}
public void kafkatest_f7984_0()
{    assertFalse(new ResourceFilter(TOPIC, "NAME").matches(new Resource(TOPIC, "Name")));}
public void kafkatest_f7993_0()
{    for (AclResourceTypeTestInfo info : INFOS) {        assertEquals("ResourceType.fromString(" + info.name + ") was supposed to be " + info.resourceType, info.resourceType, ResourceType.fromString(info.name));    }    assertEquals(ResourceType.UNKNOWN, ResourceType.fromString("something"));}
public void kafkatest_f7994_0()
{    assertEquals(INFOS.length, ResourceType.values().length);    for (int i = 0; i < INFOS.length; i++) {        assertEquals(INFOS[i].resourceType, ResourceType.values()[i]);    }}
public String kafkatest_f8003_0()
{    return name;}
public void kafkatest_f8004_0()
{    String name = "KafkaUser";    KafkaPrincipal principal1 = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, name);    KafkaPrincipal principal2 = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, name);    Assert.assertEquals(principal1.hashCode(), principal2.hashCode());    Assert.assertEquals(principal1, principal2);}
public void kafkatest_f8013_0()
{    dynamicPlainContext = new Password(PlainLoginModule.class.getName() + " required user=\"plainuser\" password=\"plain-secret\";");    dynamicDigestContext = new Password(TestDigestLoginModule.class.getName() + " required user=\"digestuser\" password=\"digest-secret\";");    TestJaasConfig.createConfiguration("SCRAM-SHA-256", Collections.singletonList("SCRAM-SHA-256"));}
public void kafkatest_f8014_0()
{    LoginManager.closeAll();}
private void kafkatest_f8023_0(Selector selector)
{    try {        selector.poll(50);    } catch (IOException e) {        Assert.fail("Caught unexpected exception " + e);    }}
private TestJaasConfig kafkatest_f8024_0(String clientMechanism, List<String> serverMechanisms)
{    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, clientMechanism);    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, serverMechanisms);    if (serverMechanisms.contains("DIGEST-MD5")) {        saslServerConfigs.put("digest-md5." + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, TestDigestLoginModule.DigestServerCallbackHandler.class.getName());    }    return TestJaasConfig.createConfiguration(clientMechanism, serverMechanisms);}
public void kafkatest_f8033_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    server = createEchoServer(securityProtocol);    checkAuthenticationAndReauthentication(securityProtocol, node);}
public void kafkatest_f8034_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    server = createEchoServer(securityProtocol);    checkAuthenticationAndReauthentication(securityProtocol, node);}
public void kafkatest_f8044_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("DIGEST-MD5", Arrays.asList("DIGEST-MD5"));    configureDigestMd5ServerCallback(securityProtocol);    server = createEchoServer(securityProtocol);    createAndCheckClientConnection(securityProtocol, node);}
public void kafkatest_f8045_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("DIGEST-MD5", Arrays.asList("DIGEST-MD5", "PLAIN", "SCRAM-SHA-256"));    configureDigestMd5ServerCallback(securityProtocol);    server = createEchoServer(securityProtocol);    updateScramCredentialCache(TestJaasConfig.USERNAME, TestJaasConfig.PASSWORD);    String node1 = "1";    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "PLAIN");    createAndCheckClientConnection(securityProtocol, node1);    server.verifyAuthenticationMetrics(1, 0);    Selector selector2 = null;    Selector selector3 = null;    try {        String node2 = "2";        saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "DIGEST-MD5");        createSelector(securityProtocol, saslClientConfigs);        selector2 = selector;        InetSocketAddress addr = new InetSocketAddress("127.0.0.1", server.port());        selector.connect(node2, addr, BUFFER_SIZE, BUFFER_SIZE);        NetworkTestUtils.checkClientConnection(selector, node2, 100, 10);        // keeps it from being closed when next one is created        selector = null;        server.verifyAuthenticationMetrics(2, 0);        String node3 = "3";        saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "SCRAM-SHA-256");        createSelector(securityProtocol, saslClientConfigs);        selector3 = selector;        selector.connect(node3, new InetSocketAddress("127.0.0.1", server.port()), BUFFER_SIZE, BUFFER_SIZE);        NetworkTestUtils.checkClientConnection(selector, node3, 100, 10);        server.verifyAuthenticationMetrics(3, 0);        /*             * Now re-authenticate the connections. First we have to sleep long enough so             * that the next write will cause re-authentication, which we expect to succeed.             */        delay((long) (CONNECTIONS_MAX_REAUTH_MS_VALUE * 1.1));        server.verifyReauthenticationMetrics(0, 0);        NetworkTestUtils.checkClientConnection(selector2, node2, 100, 10);        server.verifyReauthenticationMetrics(1, 0);        NetworkTestUtils.checkClientConnection(selector3, node3, 100, 10);        server.verifyReauthenticationMetrics(2, 0);    } finally {        if (selector2 != null)            selector2.close();        if (selector3 != null)            selector3.close();    }}
public TokenInformation kafkatest_f8054_0(String tokenId)
{    TokenInformation baseTokenInfo = super.token(tokenId);    long thisLifetimeMs = System.currentTimeMillis() + tokenLifetime.apply(++callNum).longValue();    TokenInformation retvalTokenInfo = new TokenInformation(baseTokenInfo.tokenId(), baseTokenInfo.owner(), baseTokenInfo.renewers(), baseTokenInfo.issueTimestamp(), thisLifetimeMs, thisLifetimeMs);    return retvalTokenInfo;}
public void kafkatest_f8055_0() throws Exception
{    testUnauthenticatedApiVersionsRequest(SecurityProtocol.SASL_PLAINTEXT, (short) 0);}
public void kafkatest_f8064_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    server = createEchoServer(securityProtocol);    // Send metadata request before Kafka SASL handshake request    String node1 = "invalid1";    createClientConnection(SecurityProtocol.PLAINTEXT, node1);    MetadataRequest metadataRequest1 = new MetadataRequest.Builder(Collections.singletonList("sometopic"), true).build();    RequestHeader metadataRequestHeader1 = new RequestHeader(ApiKeys.METADATA, metadataRequest1.version(), "someclient", 1);    selector.send(metadataRequest1.toSend(node1, metadataRequestHeader1));    NetworkTestUtils.waitForChannelClose(selector, node1, ChannelState.READY.state());    selector.close();    // Test good connection still works    createAndCheckClientConnection(securityProtocol, "good1");    // Send metadata request after Kafka SASL handshake request    String node2 = "invalid2";    createClientConnection(SecurityProtocol.PLAINTEXT, node2);    sendHandshakeRequestReceiveResponse(node2, (short) 1);    MetadataRequest metadataRequest2 = new MetadataRequest.Builder(Collections.singletonList("sometopic"), true).build();    RequestHeader metadataRequestHeader2 = new RequestHeader(ApiKeys.METADATA, metadataRequest2.version(), "someclient", 2);    selector.send(metadataRequest2.toSend(node2, metadataRequestHeader2));    NetworkTestUtils.waitForChannelClose(selector, node2, ChannelState.READY.state());    selector.close();    // Test good connection still works    createAndCheckClientConnection(securityProtocol, "good2");}
public void kafkatest_f8065_0() throws Exception
{    TestJaasConfig jaasConfig = configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    jaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_CLIENT, "InvalidLoginModule", TestJaasConfig.defaultClientOptions());    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    server = createEchoServer(securityProtocol);    try {        createSelector(securityProtocol, saslClientConfigs);        fail("SASL/PLAIN channel created without valid login module");    } catch (KafkaException e) {    // Expected exception    }}
public void kafkatest_f8074_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "INVALID");    server = createEchoServer(securityProtocol);    createAndCheckClientConnectionFailure(securityProtocol, node);    server.verifyAuthenticationMetrics(0, 1);    server.verifyReauthenticationMetrics(0, 0);}
public void kafkatest_f8075_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "PLAIN");    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, Arrays.asList("PLAIN"));    Map<String, Object> serverOptions = new HashMap<>();    serverOptions.put("user_user1", "user1-secret");    serverOptions.put("user_user2", "user2-secret");    TestJaasConfig staticJaasConfig = new TestJaasConfig();    staticJaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_SERVER, PlainLoginModule.class.getName(), serverOptions);    staticJaasConfig.setClientOptions("PLAIN", "user1", "invalidpassword");    Configuration.setConfiguration(staticJaasConfig);    server = createEchoServer(securityProtocol);    // Check that client using static Jaas config does not connect since password is invalid    createAndCheckClientConnectionFailure(securityProtocol, "1");    // Check that 'user1' can connect with a Jaas config property override    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "user1", "user1-secret"));    createAndCheckClientConnection(securityProtocol, "2");    // Check that invalid password specified as Jaas config property results in connection failure    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "user1", "user2-secret"));    createAndCheckClientConnectionFailure(securityProtocol, "3");    // Check that another user 'user2' can also connect with a Jaas config override without any changes to static configuration    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "user2", "user2-secret"));    createAndCheckClientConnection(securityProtocol, "4");    // Check that clients specifying multiple login modules fail even if the credentials are valid    String module1 = TestJaasConfig.jaasConfigProperty("PLAIN", "user1", "user1-secret").value();    String module2 = TestJaasConfig.jaasConfigProperty("PLAIN", "user2", "user2-secret").value();    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, new Password(module1 + " " + module2));    try {        createClientConnection(securityProtocol, "1");        fail("Connection created with multiple login modules in sasl.jaas.config");    } catch (IllegalArgumentException e) {    // Expected    }}
public void kafkatest_f8084_0() throws Exception
{    verifySaslAuthenticateHeaderInterop(false, true, SecurityProtocol.SASL_SSL, "SCRAM-SHA-512");}
public void kafkatest_f8085_0() throws Exception
{    verifySaslAuthenticateHeaderInterop(true, false, SecurityProtocol.SASL_SSL, "SCRAM-SHA-512");}
public void kafkatest_f8094_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("OAUTHBEARER", Arrays.asList("OAUTHBEARER"));    server = createEchoServer(securityProtocol);    createAndCheckClientConnection(securityProtocol, node);}
public void kafkatest_f8095_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    saslClientConfigs.put(SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS, AlternateLoginCallbackHandler.class.getName());    configureMechanisms(OAuthBearerLoginModule.OAUTHBEARER_MECHANISM, Arrays.asList(OAuthBearerLoginModule.OAUTHBEARER_MECHANISM));    server = createEchoServer(securityProtocol);    // initial authentication must succeed    createClientConnection(securityProtocol, node);    checkClientConnection(node);    // ensure metrics are as expected before trying to re-authenticate    server.verifyAuthenticationMetrics(1, 0);    server.verifyReauthenticationMetrics(0, 0);    /*         * Now re-authenticate with a different principal and ensure it fails. We first         * have to sleep long enough for the background refresh thread to replace the         * original token with a new one.         */    delay(1000L);    try {        checkClientConnection(node);        fail("Re-authentication with a different principal should have failed but did not");    } catch (AssertionError e) {        // ignore, expected        server.verifyReauthenticationMetrics(0, 1);    }}
private void kafkatest_f8104_0(SecurityProtocol securityProtocol, String saslMechanism, String node, boolean enableSaslAuthenticateHeader) throws Exception
{    if (enableSaslAuthenticateHeader)        createClientConnection(securityProtocol, node);    else        createClientConnectionWithoutSaslAuthenticateHeader(securityProtocol, saslMechanism, node);}
private NioEchoServer kafkatest_f8105_0(final SecurityProtocol securityProtocol, String saslMechanism) throws Exception
{    final ListenerName listenerName = ListenerName.forSecurityProtocol(securityProtocol);    final Map<String, ?> configs = Collections.emptyMap();    final JaasContext jaasContext = JaasContext.loadServerContext(listenerName, saslMechanism, configs);    final Map<String, JaasContext> jaasContexts = Collections.singletonMap(saslMechanism, jaasContext);    boolean isScram = ScramMechanism.isScram(saslMechanism);    if (isScram)        ScramCredentialUtils.createCache(credentialCache, Arrays.asList(saslMechanism));    SaslChannelBuilder serverChannelBuilder = new SaslChannelBuilder(Mode.SERVER, jaasContexts, securityProtocol, listenerName, false, saslMechanism, true, credentialCache, null, time) {        @Override        protected SaslServerAuthenticator buildServerAuthenticator(Map<String, ?> configs, Map<String, AuthenticateCallbackHandler> callbackHandlers, String id, TransportLayer transportLayer, Map<String, Subject> subjects, Map<String, Long> connectionsMaxReauthMsByMechanism) {            return new SaslServerAuthenticator(configs, callbackHandlers, id, subjects, null, listenerName, securityProtocol, transportLayer, connectionsMaxReauthMsByMechanism, time) {                @Override                protected ApiVersionsResponse apiVersionsResponse() {                    List<ApiVersion> apiVersions = new ArrayList<>(ApiVersionsResponse.defaultApiVersionsResponse().apiVersions());                    for (Iterator<ApiVersion> it = apiVersions.iterator(); it.hasNext(); ) {                        ApiVersion apiVersion = it.next();                        if (apiVersion.apiKey == ApiKeys.SASL_AUTHENTICATE.id) {                            it.remove();                            break;                        }                    }                    return new ApiVersionsResponse(0, Errors.NONE, apiVersions);                }                @Override                protected void enableKafkaSaslAuthenticateHeaders(boolean flag) {                // Don't enable Kafka SASL_AUTHENTICATE headers                }            };        }    };    serverChannelBuilder.configure(saslServerConfigs);    server = new NioEchoServer(listenerName, securityProtocol, new TestSecurityConfig(saslServerConfigs), "localhost", serverChannelBuilder, credentialCache, time);    server.start();    return server;}
private void kafkatest_f8114_0(String node, boolean enableSaslAuthenticateHeader) throws Exception
{    // Authenticate using PLAIN username/password    String authString = "\u0000" + TestJaasConfig.USERNAME + "\u0000" + TestJaasConfig.PASSWORD;    ByteBuffer authBuf = ByteBuffer.wrap(authString.getBytes("UTF-8"));    if (enableSaslAuthenticateHeader) {        SaslAuthenticateRequestData data = new SaslAuthenticateRequestData().setAuthBytes(authBuf.array());        SaslAuthenticateRequest request = new SaslAuthenticateRequest.Builder(data).build();        sendKafkaRequestReceiveResponse(node, ApiKeys.SASL_AUTHENTICATE, request);    } else {        selector.send(new NetworkSend(node, authBuf));        waitForResponse();    }    // Check send/receive on the manually authenticated connection    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);}
private TestJaasConfig kafkatest_f8115_0(String clientMechanism, List<String> serverMechanisms)
{    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, clientMechanism);    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, serverMechanisms);    saslServerConfigs.put(BrokerSecurityConfigs.CONNECTIONS_MAX_REAUTH_MS, CONNECTIONS_MAX_REAUTH_MS_VALUE);    if (serverMechanisms.contains("DIGEST-MD5")) {        saslServerConfigs.put("digest-md5." + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, TestDigestLoginModule.DigestServerCallbackHandler.class.getName());    }    return TestJaasConfig.createConfiguration(clientMechanism, serverMechanisms);}
private void kafkatest_f8124_0(SecurityProtocol securityProtocol, String node) throws Exception
{    try {        createClientConnection(securityProtocol, node);        checkClientConnection(node);    } finally {        closeClientConnectionIfNecessary();    }}
private void kafkatest_f8125_0(SecurityProtocol securityProtocol, String node, String mechanism, String expectedErrorMessage) throws Exception
{    ChannelState finalState = createAndCheckClientConnectionFailure(securityProtocol, node);    Exception exception = finalState.exception();    assertTrue("Invalid exception class " + exception.getClass(), exception instanceof SaslAuthenticationException);    String expectedExceptionMessage = expectedErrorMessage != null ? expectedErrorMessage : "Authentication failed during authentication due to invalid credentials with SASL mechanism " + mechanism;    assertEquals(expectedExceptionMessage, exception.getMessage());}
private SaslHandshakeRequest kafkatest_f8134_0(String mechanism, short version)
{    return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism(mechanism)).build(version);}
private void kafkatest_f8135_0(String username, String password) throws NoSuchAlgorithmException
{    for (String mechanism : (List<String>) saslServerConfigs.get(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG)) {        ScramMechanism scramMechanism = ScramMechanism.forMechanismName(mechanism);        if (scramMechanism != null) {            ScramFormatter formatter = new ScramFormatter(scramMechanism);            ScramCredential credential = formatter.generateCredential(password, 4096);            credentialCache.cache(scramMechanism.mechanismName(), ScramCredential.class).put(username, credential);        }    }}
public String kafkatest_f8145_0()
{    return "kafka";}
public void kafkatest_f8147_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    if (configured)        throw new IllegalStateException("Login callback handler configured twice");    configured = true;}
protected SaslClientAuthenticator kafkatest_f8157_0(Map<String, ?> configs, AuthenticateCallbackHandler callbackHandler, String id, String serverHost, String servicePrincipal, TransportLayer transportLayer, Subject subject)
{    if (++numInvocations == 1)        return new SaslClientAuthenticator(configs, callbackHandler, id, subject, servicePrincipal, serverHost, "DIGEST-MD5", true, transportLayer, time);    else        return new SaslClientAuthenticator(configs, callbackHandler, id, subject, servicePrincipal, serverHost, "PLAIN", true, transportLayer, time) {            @Override            protected SaslHandshakeRequest createSaslHandshakeRequest(short version) {                return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism("PLAIN")).build(version);            }        };}
protected SaslHandshakeRequest kafkatest_f8158_0(short version)
{    return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism("PLAIN")).build(version);}
public void kafkatest_f8169_0(String name, String loginModule, Map<String, Object> options)
{    AppConfigurationEntry entry = new AppConfigurationEntry(loginModule, LoginModuleControlFlag.REQUIRED, options);    entryMap.put(name, new AppConfigurationEntry[] { entry });}
public void kafkatest_f8170_0(String name, String loginModule, Map<String, Object> options)
{    AppConfigurationEntry entry = new AppConfigurationEntry(loginModule, LoginModuleControlFlag.REQUIRED, options);    AppConfigurationEntry[] existing = entryMap.get(name);    AppConfigurationEntry[] newEntries = existing == null ? new AppConfigurationEntry[1] : Arrays.copyOf(existing, existing.length + 1);    newEntries[newEntries.length - 1] = entry;    entryMap.put(name, newEntries);}
public void kafkatest_f8179_0() throws Exception
{    LoginModuleControlFlag[] controlFlags = new LoginModuleControlFlag[] { LoginModuleControlFlag.REQUIRED, LoginModuleControlFlag.REQUISITE, LoginModuleControlFlag.SUFFICIENT, LoginModuleControlFlag.OPTIONAL };    Map<String, Object> options = new HashMap<>();    options.put("propName", "propValue");    for (LoginModuleControlFlag controlFlag : controlFlags) {        checkConfiguration("test.testControlFlag", controlFlag, options);    }}
public void kafkatest_f8180_0() throws Exception
{    Map<String, Object> options = new HashMap<>();    options.put("propName", "propValue");    checkConfiguration("test.testSingleOption", LoginModuleControlFlag.REQUISITE, options);}
public void kafkatest_f8189_0() throws Exception
{    checkInvalidConfiguration("test.testNumericOptionWithoutQuotes required option1=3;");}
public void kafkatest_f8190_0() throws Exception
{    checkInvalidConfiguration("test.testInvalidControlFlag { option1=3;");}
private void kafkatest_f8199_0(List<String> lines) throws IOException
{    Files.write(jaasConfigFile.toPath(), lines, StandardCharsets.UTF_8);    Configuration.setConfiguration(null);}
private void kafkatest_f8200_0(String loginModule, LoginModuleControlFlag controlFlag, Map<String, Object> options) throws Exception
{    String jaasConfigProp = jaasConfigProp(loginModule, controlFlag, options);    checkConfiguration(jaasConfigProp, loginModule, controlFlag, options);}
public void kafkatest_f8209_0()
{    ExpiringCredentialRefreshConfig expiringCredentialRefreshConfig = new ExpiringCredentialRefreshConfig(new ConfigDef().withClientSaslSupport().parse(Collections.emptyMap()), true);    assertEquals(Double.valueOf(SaslConfigs.DEFAULT_LOGIN_REFRESH_WINDOW_FACTOR), Double.valueOf(expiringCredentialRefreshConfig.loginRefreshWindowFactor()));    assertEquals(Double.valueOf(SaslConfigs.DEFAULT_LOGIN_REFRESH_WINDOW_JITTER), Double.valueOf(expiringCredentialRefreshConfig.loginRefreshWindowJitter()));    assertEquals(Short.valueOf(SaslConfigs.DEFAULT_LOGIN_REFRESH_MIN_PERIOD_SECONDS), Short.valueOf(expiringCredentialRefreshConfig.loginRefreshMinPeriodSeconds()));    assertEquals(Short.valueOf(SaslConfigs.DEFAULT_LOGIN_REFRESH_BUFFER_SECONDS), Short.valueOf(expiringCredentialRefreshConfig.loginRefreshBufferSeconds()));    assertTrue(expiringCredentialRefreshConfig.loginRefreshReloginAllowedBeforeLogout());}
public AppConfigurationEntry[] kafkatest_f8210_0(String name)
{    // match any name    return new AppConfigurationEntry[0];}
public long kafkatest_f8219_0()
{    return expireTimeMs;}
public Long kafkatest_f8220_0()
{    return absoluteLastRefreshTimeMs;}
public Subject kafkatest_f8229_0()
{    return testLoginContext.getSubject();}
public void kafkatest_f8230_0()
{    refresherThreadStartedFuture.complete(null);}
public void kafkatest_f8239_0() throws Exception
{    int numExpectedRefreshes = 1;    boolean clientReloginAllowedBeforeLogout = true;    Subject subject = new Subject();    final LoginContext mockLoginContext = mock(LoginContext.class);    when(mockLoginContext.getSubject()).thenReturn(subject);    MockTime mockTime = new MockTime();    long startMs = mockTime.milliseconds();    /*         * Identify the lifetime of each expiring credential         */    long lifetimeMinutes = 10L;    /*         * Identify the point at which refresh will occur in that lifetime         */    long refreshEveryMinutes = 8L;    /*         * Set an absolute last refresh time that will cause the login thread to exit         * after a certain number of re-logins (by adding an extra half of a refresh         * interval).         */    long absoluteLastRefreshMs = startMs + (1 + numExpectedRefreshes) * 1000 * 60 * refreshEveryMinutes - 1000 * 60 * refreshEveryMinutes / 2;    /*         * Identify a minimum period that will cause the refresh time to be delayed a         * bit.         */    int bufferIntrusionSeconds = 1;    short bufferSeconds = (short) ((lifetimeMinutes - refreshEveryMinutes) * 60 + bufferIntrusionSeconds);    short minPeriodSeconds = (short) 0;    /*         * Define some listeners so we can keep track of who gets done and when. All         * added listeners should end up done except the last, extra one, which should         * not.         */    MockScheduler mockScheduler = new MockScheduler(mockTime);    List<KafkaFutureImpl<Long>> waiters = addWaiters(mockScheduler, 1000 * (60 * refreshEveryMinutes - bufferIntrusionSeconds), numExpectedRefreshes + 1);    // Create the ExpiringCredentialRefreshingLogin instance under test    TestLoginContextFactory testLoginContextFactory = new TestLoginContextFactory();    TestExpiringCredentialRefreshingLogin testExpiringCredentialRefreshingLogin = new TestExpiringCredentialRefreshingLogin(refreshConfigThatPerformsReloginEveryGivenPercentageOfLifetime(1.0 * refreshEveryMinutes / lifetimeMinutes, minPeriodSeconds, bufferSeconds, clientReloginAllowedBeforeLogout), testLoginContextFactory, mockTime, 1000 * 60 * lifetimeMinutes, absoluteLastRefreshMs, clientReloginAllowedBeforeLogout);    testLoginContextFactory.configure(mockLoginContext, testExpiringCredentialRefreshingLogin);    /*         * Perform the login, wait up to a certain amount of time for the refresher         * thread to exit, and make sure the correct calls happened at the correct times         */    long expectedFinalMs = startMs + numExpectedRefreshes * 1000 * (60 * refreshEveryMinutes - bufferIntrusionSeconds);    assertFalse(testLoginContextFactory.refresherThreadStartedFuture().isDone());    assertFalse(testLoginContextFactory.refresherThreadDoneFuture().isDone());    testExpiringCredentialRefreshingLogin.login();    assertTrue(testLoginContextFactory.refresherThreadStartedFuture().isDone());    testLoginContextFactory.refresherThreadDoneFuture().get(1L, TimeUnit.SECONDS);    assertEquals(expectedFinalMs, mockTime.milliseconds());    for (int i = 0; i < numExpectedRefreshes; ++i) {        KafkaFutureImpl<Long> waiter = waiters.get(i);        assertTrue(waiter.isDone());        assertEquals((i + 1) * 1000 * (60 * refreshEveryMinutes - bufferIntrusionSeconds), waiter.get().longValue() - startMs);    }    assertFalse(waiters.get(numExpectedRefreshes).isDone());    InOrder inOrder = inOrder(mockLoginContext);    inOrder.verify(mockLoginContext).login();    for (int i = 0; i < numExpectedRefreshes; ++i) {        inOrder.verify(mockLoginContext).login();        inOrder.verify(mockLoginContext).logout();    }}
public void kafkatest_f8240_0() throws Exception
{    int numExpectedRefreshes = 3;    boolean clientReloginAllowedBeforeLogout = true;    Subject subject = new Subject();    final LoginContext mockLoginContext = mock(LoginContext.class);    when(mockLoginContext.getSubject()).thenReturn(subject);    Mockito.doNothing().doThrow(new LoginException()).doNothing().when(mockLoginContext).login();    MockTime mockTime = new MockTime();    long startMs = mockTime.milliseconds();    /*         * Identify the lifetime of each expiring credential         */    long lifetimeMinutes = 100L;    /*         * Identify the point at which refresh will occur in that lifetime         */    long refreshEveryMinutes = 80L;    /*         * Set an absolute last refresh time that will cause the login thread to exit         * after a certain number of re-logins (by adding an extra half of a refresh         * interval).         */    long absoluteLastRefreshMs = startMs + (1 + numExpectedRefreshes) * 1000 * 60 * refreshEveryMinutes - 1000 * 60 * refreshEveryMinutes / 2;    /*         * Identify buffer time on either side for the refresh algorithm         */    short minPeriodSeconds = (short) 0;    short bufferSeconds = minPeriodSeconds;    // Create the ExpiringCredentialRefreshingLogin instance under test    TestLoginContextFactory testLoginContextFactory = new TestLoginContextFactory();    TestExpiringCredentialRefreshingLogin testExpiringCredentialRefreshingLogin = new TestExpiringCredentialRefreshingLogin(refreshConfigThatPerformsReloginEveryGivenPercentageOfLifetime(1.0 * refreshEveryMinutes / lifetimeMinutes, minPeriodSeconds, bufferSeconds, clientReloginAllowedBeforeLogout), testLoginContextFactory, mockTime, 1000 * 60 * lifetimeMinutes, absoluteLastRefreshMs, clientReloginAllowedBeforeLogout);    testLoginContextFactory.configure(mockLoginContext, testExpiringCredentialRefreshingLogin);    /*         * Perform the login and wait up to a certain amount of time for the refresher         * thread to exit.  A timeout indicates the thread died due to logout()         * being invoked on an instance where the login() invocation had failed.         */    assertFalse(testLoginContextFactory.refresherThreadStartedFuture().isDone());    assertFalse(testLoginContextFactory.refresherThreadDoneFuture().isDone());    testExpiringCredentialRefreshingLogin.login();    assertTrue(testLoginContextFactory.refresherThreadStartedFuture().isDone());    testLoginContextFactory.refresherThreadDoneFuture().get(1L, TimeUnit.SECONDS);}
public void kafkatest_f8249_0() throws Exception
{    String message = "n,a=user@example.com,\u0001host=server.example.com\u0001port=143\u0001" + "auth=Bearer vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg\u0001\u0001";    OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse(message.getBytes(StandardCharsets.UTF_8));    assertEquals("vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg", response.tokenValue());    assertEquals("user@example.com", response.authorizationId());    assertEquals("server.example.com", response.extensions().map().get("host"));    assertEquals("143", response.extensions().map().get("port"));}
public void kafkatest_f8250_0() throws Exception
{    String message = "n,a=user@example.com,\u0001" + "auth=Bearer vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg\u0001\u0001";    OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse(message.getBytes(StandardCharsets.UTF_8));    assertEquals("vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg", response.tokenValue());    assertEquals("user@example.com", response.authorizationId());    assertTrue(response.extensions().map().isEmpty());}
public String kafkatest_f8259_0()
{    return "principalName";}
public Long kafkatest_f8260_0()
{    return null;}
public void kafkatest_f8270_0() throws Exception
{    saslServer = new OAuthBearerSaslServer(EXTENSIONS_VALIDATOR_CALLBACK_HANDLER);    Map<String, String> customExtensions = new HashMap<>();    customExtensions.put("firstKey", "value1");    customExtensions.put("secondKey", "value1");    customExtensions.put("thirdKey", "value1");    byte[] nextChallenge = saslServer.evaluateResponse(clientInitialResponse(null, false, customExtensions));    assertTrue("Next challenge is not empty", nextChallenge.length == 0);    assertNull("Extensions not recognized by the server must be ignored", saslServer.getNegotiatedProperty("thirdKey"));}
public void kafkatest_f8271_0() throws Exception
{    OAuthBearerUnsecuredValidatorCallbackHandler invalidHandler = new OAuthBearerUnsecuredValidatorCallbackHandler() {        @Override        public void handle(Callback[] callbacks) throws UnsupportedCallbackException {            for (Callback callback : callbacks) {                if (callback instanceof OAuthBearerValidatorCallback) {                    OAuthBearerValidatorCallback validationCallback = (OAuthBearerValidatorCallback) callback;                    validationCallback.token(new OAuthBearerTokenMock());                } else if (callback instanceof OAuthBearerExtensionsValidatorCallback) {                    OAuthBearerExtensionsValidatorCallback extensionsCallback = (OAuthBearerExtensionsValidatorCallback) callback;                    extensionsCallback.error("firstKey", "is not valid");                    extensionsCallback.error("secondKey", "is not valid either");                } else                    throw new UnsupportedCallbackException(callback);            }        }    };    saslServer = new OAuthBearerSaslServer(invalidHandler);    Map<String, String> customExtensions = new HashMap<>();    customExtensions.put("firstKey", "value");    customExtensions.put("secondKey", "value");    saslServer.evaluateResponse(clientInitialResponse(null, false, customExtensions));}
public void kafkatest_f8280_0()
{    for (String invalidScope : new String[] { "\"foo", "\\foo" }) {        try {            OAuthBearerScopeUtils.parseScope(invalidScope);            fail("did not detect invalid scope: " + invalidScope);        } catch (OAuthBearerConfigException expected) {        // empty        }    }}
public void kafkatest_f8281_0() throws OAuthBearerIllegalTokenException
{    double issuedAtSeconds = 100.1;    double expirationTimeSeconds = 300.3;    StringBuilder sb = new StringBuilder("{");    appendJsonText(sb, "sub", "SUBJECT");    appendCommaJsonText(sb, "iat", issuedAtSeconds);    appendCommaJsonText(sb, "exp", expirationTimeSeconds);    sb.append("}");    String compactSerialization = HEADER_COMPACT_SERIALIZATION + Base64.getUrlEncoder().withoutPadding().encodeToString(sb.toString().getBytes(StandardCharsets.UTF_8)) + ".";    OAuthBearerUnsecuredJws testJwt = new OAuthBearerUnsecuredJws(compactSerialization, "sub", "scope");    assertEquals(compactSerialization, testJwt.value());    assertEquals("sub", testJwt.principalClaimName());    assertEquals(1, testJwt.header().size());    assertEquals("none", testJwt.header().get("alg"));    assertEquals("scope", testJwt.scopeClaimName());    assertEquals(expirationTimeSeconds, testJwt.expirationTime());    assertTrue(testJwt.isClaimType("exp", Number.class));    assertEquals(issuedAtSeconds, testJwt.issuedAt());    assertEquals("SUBJECT", testJwt.subject());}
private static String kafkatest_f8290_0(String jsonStringValue)
{    return jsonStringValue.replace("\"", "\\\"").replace("\\", "\\\\");}
public void kafkatest_f8291_0() throws IOException, UnsupportedCallbackException
{    Map<String, String> options = new HashMap<>();    options.put("unsecuredLoginExtension_testId", "1");    OAuthBearerUnsecuredLoginCallbackHandler callbackHandler = createCallbackHandler(options, new MockTime());    SaslExtensionsCallback callback = new SaslExtensionsCallback();    callbackHandler.handle(new Callback[] { callback });    assertEquals("1", callback.extensions().map().get("testId"));}
public void kafkatest_f8300_0() throws IOException, UnsupportedCallbackException
{    String claimsJson = "{" + PRINCIPAL_CLAIM_TEXT + comma(ISSUED_AT_CLAIM_TEXT) + comma(TOO_EARLY_EXPIRATION_TIME_CLAIM_TEXT) + "}";    confirmFailsValidation(UNSECURED_JWT_HEADER_JSON, claimsJson, MODULE_OPTIONS_MAP_NO_SCOPE_REQUIRED);}
public void kafkatest_f8301_0()
{    String claimsJson = "{" + SUB_CLAIM_TEXT + comma(EXPIRATION_TIME_CLAIM_TEXT) + comma(SCOPE_CLAIM_TEXT) + "}";    Object validationResult = validationResult(UNSECURED_JWT_HEADER_JSON, claimsJson, MODULE_OPTIONS_MAP_REQUIRE_EXISTING_SCOPE);    assertTrue(validationResult instanceof OAuthBearerValidatorCallback);    assertTrue(((OAuthBearerValidatorCallback) validationResult).token() instanceof OAuthBearerUnsecuredJws);}
private static String kafkatest_f8310_0(long lifetimeSeconds)
{    return claimOrHeaderText("exp", MOCK_TIME.milliseconds() / 1000.0 + lifetimeSeconds);}
public void kafkatest_f8311_0() throws OAuthBearerIllegalTokenException
{    String claimName = "foo";    for (Boolean exists : new Boolean[] { null, Boolean.TRUE, Boolean.FALSE }) {        boolean useErrorValue = exists == null;        for (Boolean required : new boolean[] { true, false }) {            StringBuilder sb = new StringBuilder("{");            appendJsonText(sb, "exp", 100);            appendCommaJsonText(sb, "sub", "principalName");            if (useErrorValue)                appendCommaJsonText(sb, claimName, 1);            else if (exists != null && exists.booleanValue())                appendCommaJsonText(sb, claimName, claimName);            sb.append("}");            String compactSerialization = HEADER_COMPACT_SERIALIZATION + Base64.getUrlEncoder().withoutPadding().encodeToString(sb.toString().getBytes(StandardCharsets.UTF_8)) + ".";            OAuthBearerUnsecuredJws testJwt = new OAuthBearerUnsecuredJws(compactSerialization, "sub", "scope");            OAuthBearerValidationResult result = OAuthBearerValidationUtils.validateClaimForExistenceAndType(testJwt, required, claimName, String.class);            if (useErrorValue || required && !exists.booleanValue())                assertTrue(isFailureWithMessageAndNoFailureScope(result));            else                assertTrue(isSuccess(result));        }    }}
private static void kafkatest_f8320_0(StringBuilder sb, String claimName, Number claimValue)
{    sb.append(',').append(QUOTE).append(escape(claimName)).append(QUOTE).append(":").append(claimValue);}
private static void kafkatest_f8321_0(StringBuilder sb, String claimName, String claimValue)
{    sb.append(',').append(QUOTE).append(escape(claimName)).append(QUOTE).append(":").append(QUOTE).append(escape(claimValue)).append(QUOTE);}
public void kafkatest_f8330_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{// empty}
public void kafkatest_f8331_0()
{// empty}
public String kafkatest_f8340_0()
{    return null;}
public Long kafkatest_f8341_0()
{    return null;}
public Set<String> kafkatest_f8350_0()
{    return Collections.emptySet();}
public String kafkatest_f8351_0()
{    return "principalName";}
public String kafkatest_f8360_0()
{    return "value";}
public Long kafkatest_f8361_0()
{    return null;}
public void kafkatest_f8370_0() throws Exception
{    saslServer.evaluateResponse(saslMessage(USER_B, USER_A, PASSWORD_A));}
public void kafkatest_f8371_0()
{    Exception e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("", "", "")));    assertEquals("Authentication failed: username not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("", "", "p")));    assertEquals("Authentication failed: username not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("", "u", "")));    assertEquals("Authentication failed: password not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("a", "", "")));    assertEquals("Authentication failed: username not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("a", "", "p")));    assertEquals("Authentication failed: username not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("a", "u", "")));    assertEquals("Authentication failed: password not specified", e.getMessage());    String nul = "\u0000";    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(String.format("%s%s%s%s%s%s", "a", nul, "u", nul, "p", nul).getBytes(StandardCharsets.UTF_8)));    assertEquals("Invalid SASL/PLAIN response: expected 3 tokens, got 4", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(String.format("%s%s%s", "", nul, "u").getBytes(StandardCharsets.UTF_8)));    assertEquals("Invalid SASL/PLAIN response: expected 3 tokens, got 2", e.getMessage());}
public void kafkatest_f8380_0()
{    String cred = ScramCredentialUtils.credentialToString(formatter.generateCredential("password", 2048));    ScramCredentialUtils.credentialFromString(cred.substring(cred.indexOf(',')));}
public void kafkatest_f8381_0()
{    String cred = ScramCredentialUtils.credentialToString(formatter.generateCredential("password", 2048));    ScramCredentialUtils.credentialFromString(cred + ",a=test");}
public void kafkatest_f8390_0() throws SaslException
{    String nonce = formatter.secureRandomString();    String channelBinding = randomBytesAsString();    String proof = randomBytesAsString();    ClientFinalMessage m = new ClientFinalMessage(toBytes(channelBinding), nonce);    assertNull("Invalid proof", m.proof());    m.proof(toBytes(proof));    checkClientFinalMessage(m, channelBinding, nonce, proof);    // Default format used by Kafka client: channel-binding, nonce and proof are specified    String str = String.format("c=%s,r=%s,p=%s", channelBinding, nonce, proof);    m = createScramMessage(ClientFinalMessage.class, str);    checkClientFinalMessage(m, channelBinding, nonce, proof);    m = new ClientFinalMessage(m.toBytes());    checkClientFinalMessage(m, channelBinding, nonce, proof);    // Optional extension specified    for (String extension : VALID_EXTENSIONS) {        str = String.format("c=%s,r=%s,%s,p=%s", channelBinding, nonce, extension, proof);        checkClientFinalMessage(createScramMessage(ClientFinalMessage.class, str), channelBinding, nonce, proof);    }}
public void kafkatest_f8391_0()
{    String nonce = formatter.secureRandomString();    String channelBinding = randomBytesAsString();    String proof = randomBytesAsString();    // Invalid channel binding    String invalid = String.format("c=ab,r=%s,p=%s", nonce, proof);    checkInvalidScramMessage(ClientFirstMessage.class, invalid);    // Invalid proof    invalid = String.format("c=%s,r=%s,p=123", channelBinding, nonce);    checkInvalidScramMessage(ClientFirstMessage.class, invalid);    // Invalid extensions    for (String extension : INVALID_EXTENSIONS) {        invalid = String.format("c=%s,r=%s,%s,p=%s", channelBinding, nonce, extension, proof);        checkInvalidScramMessage(ClientFinalMessage.class, invalid);    }}
private T kafkatest_f8400_0(Class<T> clazz, String message) throws SaslException
{    byte[] bytes = message.getBytes(StandardCharsets.UTF_8);    if (clazz == ClientFirstMessage.class)        return (T) new ClientFirstMessage(bytes);    else if (clazz == ServerFirstMessage.class)        return (T) new ServerFirstMessage(bytes);    else if (clazz == ClientFinalMessage.class)        return (T) new ClientFinalMessage(bytes);    else if (clazz == ServerFinalMessage.class)        return (T) new ServerFinalMessage(bytes);    else        throw new IllegalArgumentException("Unknown message type: " + clazz);}
private void kafkatest_f8401_0(Class<T> clazz, String message)
{    try {        createScramMessage(clazz, message);        fail("Exception not throws for invalid message of type " + clazz + " : " + message);    } catch (SaslException e) {    // Expected exception    }}
public String[] kafkatest_f8412_0(String s, Principal[] principals)
{    return new String[] { ALIAS };}
public String kafkatest_f8413_0(String s, Principal[] principals, Socket socket)
{    return ALIAS;}
public void kafkatest_f8430_0()
{    TestProviderCreator testProviderCreator = new TestProviderCreator();    Map<String, Object> serverSslConfig = TestSslUtils.createSslConfig(TestKeyManagerFactory.ALGORITHM, TestTrustManagerFactory.ALGORITHM);    serverSslConfig.put(SecurityConfig.SECURITY_PROVIDERS_CONFIG, testProviderCreator.getClass().getName());    SslFactory sslFactory = new SslFactory(Mode.SERVER);    sslFactory.configure(serverSslConfig);    assertNotNull("SslEngineBuilder not created", sslFactory.sslEngineBuilder());    Security.removeProvider(testProviderCreator.getProvider().getName());}
public void kafkatest_f8431_0()
{    // An exception is thrown as the algorithm is not registered through a provider    Map<String, Object> serverSslConfig = TestSslUtils.createSslConfig(TestKeyManagerFactory.ALGORITHM, TestTrustManagerFactory.ALGORITHM);    SslFactory sslFactory = new SslFactory(Mode.SERVER);    sslFactory.configure(serverSslConfig);}
public void kafkatest_f8440_0() throws Exception
{    File trustStoreFile1 = File.createTempFile("truststore1", ".jks");    Map<String, Object> sslConfig1 = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile1, "server");    SslFactory sslFactory = new SslFactory(Mode.SERVER, null, true);    sslFactory.configure(sslConfig1);    File trustStoreFile2 = File.createTempFile("truststore2", ".jks");    Map<String, Object> sslConfig2 = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile2, "server");    // listener to stores that may not work with other brokers where the update hasn't yet been performed.    try {        sslFactory.validateReconfiguration(sslConfig2);        fail("ValidateReconfiguration did not fail as expected");    } catch (ConfigException e) {    // Expected exception    }}
public void kafkatest_f8441_0() throws Exception
{    File trustStoreFile = File.createTempFile("truststore", ".jks");    Map<String, Object> serverSslConfig = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile, "server");    Map<String, Object> newCnConfig = TestSslUtils.createSslConfig(false, true, Mode.SERVER, File.createTempFile("truststore", ".jks"), "server", "Another CN");    KeyStore ks1 = sslKeyStore(serverSslConfig).load();    KeyStore ks2 = sslKeyStore(serverSslConfig).load();    assertEquals(SslFactory.CertificateEntries.create(ks1), SslFactory.CertificateEntries.create(ks2));    // Use different alias name, validation should succeed    ks2.setCertificateEntry("another", ks1.getCertificate("localhost"));    assertEquals(SslFactory.CertificateEntries.create(ks1), SslFactory.CertificateEntries.create(ks2));    KeyStore ks3 = sslKeyStore(newCnConfig).load();    assertNotEquals(SslFactory.CertificateEntries.create(ks1), SslFactory.CertificateEntries.create(ks3));}
public void kafkatest_f8450_0()
{    // seeing is believing    testRulesSplitting("[]", "");    testRulesSplitting("[DEFAULT]", "DEFAULT");    testRulesSplitting("[RULE:/]", "RULE://");    testRulesSplitting("[RULE:/.*]", "RULE:/.*/");    testRulesSplitting("[RULE:/.*/L]", "RULE:/.*/L");    testRulesSplitting("[RULE:/, DEFAULT]", "RULE://,DEFAULT");    testRulesSplitting("[RULE:/, DEFAULT]", "  RULE:// ,  DEFAULT  ");    testRulesSplitting("[RULE:   /     , DEFAULT]", "  RULE:   /     / ,  DEFAULT  ");    testRulesSplitting("[RULE:  /     /U, DEFAULT]", "  RULE:  /     /U   ,DEFAULT  ");    testRulesSplitting("[RULE:([A-Z]*)/$1/U, RULE:([a-z]+)/$1, DEFAULT]", "  RULE:([A-Z]*)/$1/U   ,RULE:([a-z]+)/$1/,   DEFAULT  ");    // empty rules are ignored    testRulesSplitting("[]", ",   , , ,      , , ,   ");    testRulesSplitting("[RULE:/, DEFAULT]", ",,RULE://,,,DEFAULT,,");    testRulesSplitting("[RULE: /   , DEFAULT]", ",  , RULE: /   /    ,,,   DEFAULT, ,   ");    testRulesSplitting("[RULE:   /  /U, DEFAULT]", "     ,  , RULE:   /  /U    ,,  ,DEFAULT, ,");    // escape sequences    testRulesSplitting("[RULE:\\/\\\\\\(\\)\\n\\t/\\/\\/]", "RULE:\\/\\\\\\(\\)\\n\\t/\\/\\//");    testRulesSplitting("[RULE:\\**\\/+/*/L, RULE:\\/*\\**/**]", "RULE:\\**\\/+/*/L,RULE:\\/*\\**/**/");    // rules rule    testRulesSplitting("[RULE:,RULE:,/,RULE:,\\//U, RULE:,/RULE:,, RULE:,RULE:,/L,RULE:,/L, RULE:, DEFAULT, /DEFAULT, DEFAULT]", "RULE:,RULE:,/,RULE:,\\//U,RULE:,/RULE:,/,RULE:,RULE:,/L,RULE:,/L,RULE:, DEFAULT, /DEFAULT/,DEFAULT");}
public void kafkatest_f8451_0() throws Exception
{    String rules = "RULE:^CN=((\\\\, *|\\w)+)(,.*|$)/$1/,DEFAULT";    SslPrincipalMapper mapper = SslPrincipalMapper.fromRules(rules);    assertEquals("Tkac\\, Adam", mapper.getName("CN=Tkac\\, Adam,OU=ITZ,DC=geodis,DC=cz"));}
public void kafkatest_f8460_0()
{    int someNaNAsIntBits = 0x7f800001;    float someNaN = Float.intBitsToFloat(someNaNAsIntBits);    int anotherNaNAsIntBits = 0x7f800002;    float anotherNaN = Float.intBitsToFloat(anotherNaNAsIntBits);    try (Serde<Float> serde = Serdes.Float()) {        // Because of NaN semantics we must assert based on the raw int bits.        Float roundtrip = serde.deserializer().deserialize(topic, serde.serializer().serialize(topic, someNaN));        assertThat(Float.floatToRawIntBits(roundtrip), equalTo(someNaNAsIntBits));        Float otherRoundtrip = serde.deserializer().deserialize(topic, serde.serializer().serialize(topic, anotherNaN));        assertThat(Float.floatToRawIntBits(otherRoundtrip), equalTo(anotherNaNAsIntBits));    }}
private Serde<String> kafkatest_f8461_0(String encoder)
{    Map<String, Object> serializerConfigs = new HashMap<String, Object>();    serializerConfigs.put("key.serializer.encoding", encoder);    Serializer<String> serializer = Serdes.String().serializer();    serializer.configure(serializerConfigs, true);    Map<String, Object> deserializerConfigs = new HashMap<String, Object>();    deserializerConfigs.put("key.deserializer.encoding", encoder);    Deserializer<String> deserializer = Serdes.String().deserializer();    deserializer.configure(deserializerConfigs, true);    return Serdes.serdeFrom(serializer, deserializer);}
public void kafkatest_f8470_0() throws JMException
{    registerAppInfo();}
public void kafkatest_f8471_0() throws JMException
{    registerAppInfo();    AppInfoParser.unregisterAppInfo(METRICS_PREFIX, METRICS_ID, metrics);    assertFalse(mBeanServer.isRegistered(expectedAppObjectName()));    assertNull(metrics.metric(metrics.metricName("commit-id", "app-info")));    assertNull(metrics.metric(metrics.metricName("version", "app-info")));    assertNull(metrics.metric(metrics.metricName("start-time-ms", "app-info")));}
public void kafkatest_f8480_0() throws IOException
{    testWriteByteBuffer(ByteBuffer.allocate(16));}
public void kafkatest_f8481_0() throws IOException
{    testWriteByteBuffer(ByteBuffer.allocateDirect(16));}
public void kafkatest_f8490_0() throws Exception
{    assertVarlongSerde(0, new byte[] { x00 });    assertVarlongSerde(-1, new byte[] { x01 });    assertVarlongSerde(1, new byte[] { x02 });    assertVarlongSerde(63, new byte[] { x7E });    assertVarlongSerde(-64, new byte[] { x7F });    assertVarlongSerde(64, new byte[] { x80, x01 });    assertVarlongSerde(-65, new byte[] { x81, x01 });    assertVarlongSerde(8191, new byte[] { xFE, x7F });    assertVarlongSerde(-8192, new byte[] { xFF, x7F });    assertVarlongSerde(8192, new byte[] { x80, x80, x01 });    assertVarlongSerde(-8193, new byte[] { x81, x80, x01 });    assertVarlongSerde(1048575, new byte[] { xFE, xFF, x7F });    assertVarlongSerde(-1048576, new byte[] { xFF, xFF, x7F });    assertVarlongSerde(1048576, new byte[] { x80, x80, x80, x01 });    assertVarlongSerde(-1048577, new byte[] { x81, x80, x80, x01 });    assertVarlongSerde(134217727, new byte[] { xFE, xFF, xFF, x7F });    assertVarlongSerde(-134217728, new byte[] { xFF, xFF, xFF, x7F });    assertVarlongSerde(134217728, new byte[] { x80, x80, x80, x80, x01 });    assertVarlongSerde(-134217729, new byte[] { x81, x80, x80, x80, x01 });    assertVarlongSerde(Integer.MAX_VALUE, new byte[] { xFE, xFF, xFF, xFF, x0F });    assertVarlongSerde(Integer.MIN_VALUE, new byte[] { xFF, xFF, xFF, xFF, x0F });    assertVarlongSerde(17179869183L, new byte[] { xFE, xFF, xFF, xFF, x7F });    assertVarlongSerde(-17179869184L, new byte[] { xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(17179869184L, new byte[] { x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-17179869185L, new byte[] { x81, x80, x80, x80, x80, x01 });    assertVarlongSerde(2199023255551L, new byte[] { xFE, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(-2199023255552L, new byte[] { xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(2199023255552L, new byte[] { x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-2199023255553L, new byte[] { x81, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(281474976710655L, new byte[] { xFE, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(-281474976710656L, new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(281474976710656L, new byte[] { x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-281474976710657L, new byte[] { x81, x80, x80, x80, x80, x80, x80, 1 });    assertVarlongSerde(36028797018963967L, new byte[] { xFE, xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(-36028797018963968L, new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(36028797018963968L, new byte[] { x80, x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-36028797018963969L, new byte[] { x81, x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(4611686018427387903L, new byte[] { xFE, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(-4611686018427387904L, new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(4611686018427387904L, new byte[] { x80, x80, x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-4611686018427387905L, new byte[] { x81, x80, x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(Long.MAX_VALUE, new byte[] { xFE, xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x01 });    assertVarlongSerde(Long.MIN_VALUE, new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x01 });}
public void kafkatest_f8491_0()
{    // varint encoding has one overflow byte    ByteBuffer buf = ByteBuffer.wrap(new byte[] { xFF, xFF, xFF, xFF, xFF, x01 });    ByteUtils.readVarint(buf);}
public void kafkatest_f8500_0()
{    final long value = Integer.MAX_VALUE + 1;    final ByteBuffer buffer = ByteBuffer.allocate(8);    buffer.putLong(value);    Checksum crc1 = new Crc32();    Checksum crc2 = new Crc32();    Checksums.updateLong(crc1, value);    crc2.update(buffer.array(), buffer.arrayOffset(), 8);    assertEquals("Crc values should be the same", crc1.getValue(), crc2.getValue());}
private void kafkatest_f8501_0(byte[] bytes, ByteBuffer buffer, int offset)
{    buffer.put(bytes);    buffer.flip();    buffer.position(offset);    Checksum bufferCrc = Crc32C.create();    Checksums.update(bufferCrc, buffer, buffer.remaining());    assertEquals(Crc32C.compute(bytes, offset, buffer.remaining()), bufferCrc.getValue());    assertEquals(offset, buffer.position());}
public void kafkatest_f8510_0()
{    final FixedOrderMap<String, Integer> map = new FixedOrderMap<>();    map.put("a", 0);    try {        map.remove("a", 0);        fail("expected exception");    } catch (final RuntimeException e) {        assertThat(e, CoreMatchers.instanceOf(UnsupportedOperationException.class));    }    assertThat(map.get("a"), is(0));}
public void kafkatest_f8511_0()
{    final FixedOrderMap<String, Integer> map = new FixedOrderMap<>();    map.put("a", 0);    try {        map.clear();        fail("expected exception");    } catch (final RuntimeException e) {        assertThat(e, CoreMatchers.instanceOf(UnsupportedOperationException.class));    }    assertThat(map.get("a"), is(0));}
public void kafkatest_f8520_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>(100);    assertTrue(coll.add(new TestElement(1)));    TestElement second = new TestElement(2);    assertTrue(coll.add(second));    assertTrue(coll.add(new TestElement(3)));    assertFalse(coll.add(new TestElement(3)));    assertEquals(3, coll.size());    assertTrue(coll.contains(new TestElement(1)));    assertFalse(coll.contains(new TestElement(4)));    TestElement secondAgain = coll.find(new TestElement(2));    assertTrue(second == secondAgain);    assertTrue(coll.remove(new TestElement(1)));    assertFalse(coll.remove(new TestElement(1)));    assertEquals(2, coll.size());    coll.clear();    assertEquals(0, coll.size());}
 static void kafkatest_f8521_0(Iterator<TestElement> iterator, Integer... sequence)
{    int i = 0;    while (iterator.hasNext()) {        TestElement element = iterator.next();        Assert.assertTrue("Iterator yieled " + (i + 1) + " elements, but only " + sequence.length + " were expected.", i < sequence.length);        Assert.assertEquals("Iterator value number " + (i + 1) + " was incorrect.", sequence[i].intValue(), element.val);        i = i + 1;    }    Assert.assertTrue("Iterator yieled " + (i + 1) + " elements, but " + sequence.length + " were expected.", i == sequence.length);}
public void kafkatest_f8530_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>();    coll.add(new TestElement(1));    coll.add(new TestElement(2));    coll.add(new TestElement(3));    ListIterator<TestElement> iter = coll.valuesList().listIterator();    // Step the iterator forward to the end of the list    assertTrue(iter.hasNext());    assertFalse(iter.hasPrevious());    assertEquals(0, iter.nextIndex());    assertEquals(-1, iter.previousIndex());    assertEquals(1, iter.next().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    assertEquals(2, iter.next().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(2, iter.nextIndex());    assertEquals(1, iter.previousIndex());    assertEquals(3, iter.next().val);    assertFalse(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(3, iter.nextIndex());    assertEquals(2, iter.previousIndex());    // Step back to the middle of the list    assertEquals(3, iter.previous().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(2, iter.nextIndex());    assertEquals(1, iter.previousIndex());    assertEquals(2, iter.previous().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    // Step forward one and then back one, return value should remain the same    assertEquals(2, iter.next().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(2, iter.nextIndex());    assertEquals(1, iter.previousIndex());    assertEquals(2, iter.previous().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    // Step back to the front of the list    assertEquals(1, iter.previous().val);    assertTrue(iter.hasNext());    assertFalse(iter.hasPrevious());    assertEquals(0, iter.nextIndex());    assertEquals(-1, iter.previousIndex());}
public void kafkatest_f8531_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>();    coll.add(new TestElement(1));    coll.add(new TestElement(2));    coll.add(new TestElement(3));    coll.add(new TestElement(4));    coll.add(new TestElement(5));    ListIterator<TestElement> iter = coll.valuesList().listIterator();    try {        iter.remove();        fail("Calling remove() without calling next() or previous() should raise an exception");    } catch (IllegalStateException e) {    // expected    }    // Remove after next()    iter.next();    iter.next();    iter.next();    iter.remove();    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(2, iter.nextIndex());    assertEquals(1, iter.previousIndex());    try {        iter.remove();        fail("Calling remove() twice without calling next() or previous() in between should raise an exception");    } catch (IllegalStateException e) {    // expected    }    // Remove after previous()    assertEquals(2, iter.previous().val);    iter.remove();    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    // Remove the first element of the list    assertEquals(1, iter.previous().val);    iter.remove();    assertTrue(iter.hasNext());    assertFalse(iter.hasPrevious());    assertEquals(0, iter.nextIndex());    assertEquals(-1, iter.previousIndex());    // Remove the last element of the list    assertEquals(4, iter.next().val);    assertEquals(5, iter.next().val);    iter.remove();    assertFalse(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    // Remove the final remaining element of the list    assertEquals(4, iter.previous().val);    iter.remove();    assertFalse(iter.hasNext());    assertFalse(iter.hasPrevious());    assertEquals(0, iter.nextIndex());    assertEquals(-1, iter.previousIndex());}
public void kafkatest_f8540_0()
{    ImplicitLinkedHashMultiCollection<TestElement> coll = new ImplicitLinkedHashMultiCollection<>();    assertNull(coll.find(new TestElement(2)));    assertFalse(coll.contains(new TestElement(2)));    assertFalse(coll.remove(new TestElement(2)));    assertTrue(coll.findAll(new TestElement(2)).isEmpty());}
public void kafkatest_f8541_0()
{    ImplicitLinkedHashMultiCollection<TestElement> multiSet = new ImplicitLinkedHashMultiCollection<>(100);    TestElement e1 = new TestElement(1);    TestElement e2 = new TestElement(1);    TestElement e3 = new TestElement(2);    multiSet.mustAdd(e1);    multiSet.mustAdd(e2);    multiSet.mustAdd(e3);    assertFalse(multiSet.add(e3));    assertEquals(3, multiSet.size());    expectExactTraversal(multiSet.findAll(e1).iterator(), e1, e2);    expectExactTraversal(multiSet.findAll(e3).iterator(), e3);    multiSet.remove(e2);    expectExactTraversal(multiSet.findAll(e1).iterator(), e1);    assertTrue(multiSet.contains(e2));}
public void kafkatest_f8550_0() throws ClassNotFoundException
{    String clazz = Java.isIbmJdk() ? "com.ibm.security.auth.module.Krb5LoginModule" : "com.sun.security.auth.module.Krb5LoginModule";    Class.forName(clazz);}
public void kafkatest_f8551_0()
{    Java.Version v = Java.parseVersion("9");    assertEquals(9, v.majorVersion);    assertEquals(0, v.minorVersion);    assertTrue(v.isJava9Compatible());    v = Java.parseVersion("9.0.1");    assertEquals(9, v.majorVersion);    assertEquals(0, v.minorVersion);    assertTrue(v.isJava9Compatible());    // Azul Zulu    v = Java.parseVersion("9.0.0.15");    assertEquals(9, v.majorVersion);    assertEquals(0, v.minorVersion);    assertTrue(v.isJava9Compatible());    v = Java.parseVersion("9.1");    assertEquals(9, v.majorVersion);    assertEquals(1, v.minorVersion);    assertTrue(v.isJava9Compatible());    v = Java.parseVersion("1.8.0_152");    assertEquals(1, v.majorVersion);    assertEquals(8, v.minorVersion);    assertFalse(v.isJava9Compatible());    v = Java.parseVersion("1.7.0_80");    assertEquals(1, v.majorVersion);    assertEquals(7, v.minorVersion);    assertFalse(v.isJava9Compatible());}
public void kafkatest_f8560_0(MockTimeListener listener)
{    listeners.add(listener);}
public long kafkatest_f8561_0()
{    maybeSleep(autoTickMs);    return timeMs.get();}
protected Time kafkatest_f8570_0()
{    return new MockTime();}
public void kafkatest_f8571_0()
{    String principal = "CN=Some characters !@#$%&*()_-+=';:,/~";    String sanitizedPrincipal = Sanitizer.sanitize(principal);    assertTrue(sanitizedPrincipal.replace('%', '_').matches("[a-zA-Z0-9\\._\\-]+"));    assertEquals(principal, Sanitizer.desanitize(sanitizedPrincipal));}
private int kafkatest_f8580_0(String providerName, Provider[] providers)
{    for (int index = 0; index < providers.length; index++) {        if (providers[index].getName().equals(providerName)) {            return index;        }    }    return -1;}
public void kafkatest_f8581_0()
{    String customProviderClasses = testScramSaslServerProviderCreator.getClass().getName() + "," + testPlainSaslServerProviderCreator.getClass().getName();    Map<String, String> configs = new HashMap<>();    configs.put(SecurityConfig.SECURITY_PROVIDERS_CONFIG, customProviderClasses);    SecurityUtils.addConfiguredSecurityProviders(configs);    Provider[] providers = Security.getProviders();    int testScramSaslServerProviderIndex = getProviderIndexFromName(testScramSaslServerProvider.getName(), providers);    int testPlainSaslServerProviderIndex = getProviderIndexFromName(testPlainSaslServerProvider.getName(), providers);    // validations    MatcherAssert.assertThat(testScramSaslServerProvider.getName() + " testProvider not found at expected index", testScramSaslServerProviderIndex == 0);    MatcherAssert.assertThat(testPlainSaslServerProvider.getName() + " testProvider not found at expected index", testPlainSaslServerProviderIndex == 1);}
protected Time kafkatest_f8590_0()
{    return Time.SYSTEM;}
public void kafkatest_f8591_0()
{    Timer timer = time.timer(500);    assertEquals(500, timer.remainingMs());    assertEquals(0, timer.elapsedMs());    time.sleep(100);    timer.update();    assertEquals(400, timer.remainingMs());    assertEquals(100, timer.elapsedMs());    time.sleep(400);    timer.update(time.milliseconds());    assertEquals(0, timer.remainingMs());    assertEquals(500, timer.elapsedMs());    assertTrue(timer.isExpired());    // Going over the expiration is fine and the elapsed time can exceed    // the initial timeout. However, remaining time should be stuck at 0.    time.sleep(200);    timer.update(time.milliseconds());    assertTrue(timer.isExpired());    assertEquals(0, timer.remainingMs());    assertEquals(700, timer.elapsedMs());}
public void kafkatest_f8600_0()
{    assertEquals("127.0.0.1", getHost("127.0.0.1:8000"));    assertEquals("mydomain.com", getHost("PLAINTEXT://mydomain.com:8080"));    assertEquals("MyDomain.com", getHost("PLAINTEXT://MyDomain.com:8080"));    assertEquals("My_Domain.com", getHost("PLAINTEXT://My_Domain.com:8080"));    assertEquals("::1", getHost("[::1]:1234"));    assertEquals("2001:db8:85a3:8d3:1319:8a2e:370:7348", getHost("PLAINTEXT://[2001:db8:85a3:8d3:1319:8a2e:370:7348]:5678"));    assertEquals("2001:DB8:85A3:8D3:1319:8A2E:370:7348", getHost("PLAINTEXT://[2001:DB8:85A3:8D3:1319:8A2E:370:7348]:5678"));    assertEquals("fe80::b1da:69ca:57f7:63d8%3", getHost("PLAINTEXT://[fe80::b1da:69ca:57f7:63d8%3]:5678"));}
public void kafkatest_f8601_0()
{    assertTrue(validHostPattern("127.0.0.1"));    assertTrue(validHostPattern("mydomain.com"));    assertTrue(validHostPattern("MyDomain.com"));    assertTrue(validHostPattern("My_Domain.com"));    assertTrue(validHostPattern("::1"));    assertTrue(validHostPattern("2001:db8:85a3:8d3:1319:8a2e:370"));}
public void kafkatest_f8610_0()
{    byte[] input = { 0, 1, 2, 3, 4 };    ByteBuffer buffer = ByteBuffer.allocateDirect(5);    buffer.put(input);    buffer.rewind();    assertArrayEquals(input, Utils.toArray(buffer));    assertEquals(0, buffer.position());    assertArrayEquals(new byte[] { 1, 2 }, Utils.toArray(buffer, 1, 2));    assertEquals(0, buffer.position());    buffer.position(2);    assertArrayEquals(new byte[] { 2, 3, 4 }, Utils.toArray(buffer));    assertEquals(2, buffer.position());}
public void kafkatest_f8611_0()
{    String utf8String = "A\u00ea\u00f1\u00fcC";    byte[] utf8Bytes = utf8String.getBytes(StandardCharsets.UTF_8);    assertArrayEquals(utf8Bytes, Utils.utf8(utf8String));    assertEquals(utf8Bytes.length, Utils.utf8Length(utf8String));    assertEquals(utf8String, Utils.utf8(utf8Bytes));}
public void kafkatest_f8620_0() throws IOException
{    FileChannel channelMock = mock(FileChannel.class);    final int bufferSize = 100;    String expectedBufferContent = fileChannelMockExpectReadWithRandomBytes(channelMock, bufferSize);    ByteBuffer buffer = ByteBuffer.allocate(bufferSize);    Utils.readFully(channelMock, buffer, 0L);    assertEquals("The buffer should be populated correctly.", expectedBufferContent, new String(buffer.array()));    assertFalse("The buffer should be filled", buffer.hasRemaining());    verify(channelMock, atLeastOnce()).read(any(), anyLong());}
public void kafkatest_f8621_0() throws IOException
{    final FileChannel channelMock = mock(FileChannel.class);    final int bufferSize = 100;    final String fileChannelContent = "abcdefghkl";    ByteBuffer buffer = ByteBuffer.allocate(bufferSize);    when(channelMock.read(any(), anyLong())).then(invocation -> {        ByteBuffer bufferArg = invocation.getArgument(0);        bufferArg.put(fileChannelContent.getBytes());        return -1;    });    Utils.readFully(channelMock, buffer, 0L);    assertEquals("abcdefghkl", new String(buffer.array(), 0, buffer.position()));    assertEquals(fileChannelContent.length(), buffer.position());    assertTrue(buffer.hasRemaining());    verify(channelMock, atLeastOnce()).read(any(), anyLong());}
public NetworkReceive kafkatest_f8630_0()
{    return receive;}
public static void kafkatest_f8631_0(String[] args)
{    long iters = Long.parseLong(args[0]);    Metrics metrics = new Metrics();    try {        Sensor parent = metrics.sensor("parent");        Sensor child = metrics.sensor("child", parent);        for (Sensor sensor : Arrays.asList(parent, child)) {            sensor.add(metrics.metricName(sensor.name() + ".avg", "grp1"), new Avg());            sensor.add(metrics.metricName(sensor.name() + ".count", "grp1"), new WindowedCount());            sensor.add(metrics.metricName(sensor.name() + ".max", "grp1"), new Max());            sensor.add(new Percentiles(1024, 0.0, iters, BucketSizing.CONSTANT, new Percentile(metrics.metricName(sensor.name() + ".median", "grp1"), 50.0), new Percentile(metrics.metricName(sensor.name() + ".p_99", "grp1"), 99.0)));        }        long start = System.nanoTime();        for (int i = 0; i < iters; i++) parent.record(i);        double ellapsed = (System.nanoTime() - start) / (double) iters;        System.out.println(String.format("%.2f ns per metric recording.", ellapsed));    } finally {        metrics.close();    }}
private static long kafkatest_f8640_0(int iters)
{    long total = 0;    for (int i = 0; i < iters; i++) total += System.currentTimeMillis();    return total;}
public void kafkatest_f8641_0(ClusterResource clusterResource)
{    IS_ON_UPDATE_CALLED.set(true);    this.clusterResource = clusterResource;}
public void kafkatest_f8650_0(Map<String, ?> configs, boolean isKey)
{    this.configs = configs;    this.isKey = isKey;}
public byte[] kafkatest_f8651_0(String topic, byte[] data)
{    // This will ensure that we get the cluster metadata when deserialize is called for the first time    // as subsequent compareAndSet operations will fail.    clusterIdBeforeDeserialize.compareAndSet(noClusterId, clusterMeta.get());    return data;}
public void kafkatest_f8663_0(Map<String, ?> configs)
{    // ensure this method is called and expected configs are passed in    Object o = configs.get(APPEND_STRING_PROP);    if (o == null)        throw new ConfigException("Mock producer interceptor expects configuration " + APPEND_STRING_PROP);    if (o instanceof String)        appendStr = (String) o;    // clientId also must be in configs    Object clientIdValue = configs.get(ProducerConfig.CLIENT_ID_CONFIG);    if (clientIdValue == null)        throw new ConfigException("Mock producer interceptor expects configuration " + ProducerConfig.CLIENT_ID_CONFIG);}
public ProducerRecord<String, String> kafkatest_f8664_0(ProducerRecord<String, String> record)
{    ONSEND_COUNT.incrementAndGet();    return new ProducerRecord<>(record.topic(), record.partition(), record.key(), record.value().concat(appendStr));}
public void kafkatest_f8675_0()
{    this.completedSends.clear();    this.completedReceives.clear();    this.disconnected.clear();    this.connected.clear();}
public void kafkatest_f8676_0(Send send)
{    this.initiatedSends.add(send);}
public void kafkatest_f8685_0(DelayedReceive receive)
{    this.delayedReceives.add(receive);}
public Map<String, ChannelState> kafkatest_f8686_0()
{    return disconnected;}
private static KeyStore kafkatest_f8699_0() throws GeneralSecurityException, IOException
{    KeyStore ks = KeyStore.getInstance("JKS");    // initialize    ks.load(null, null);    return ks;}
private static void kafkatest_f8700_0(KeyStore ks, String filename, Password password) throws GeneralSecurityException, IOException
{    try (OutputStream out = Files.newOutputStream(Paths.get(filename))) {        ks.store(out, password.value().toCharArray());    }}
public CertificateBuilder kafkatest_f8709_0(String hostName) throws IOException
{    subjectAltName = new GeneralNames(new GeneralName(GeneralName.dNSName, hostName)).getEncoded();    return this;}
public CertificateBuilder kafkatest_f8710_0(InetAddress hostAddress) throws IOException
{    subjectAltName = new GeneralNames(new GeneralName(GeneralName.iPAddress, new DEROctetString(hostAddress.getAddress()))).getEncoded();    return this;}
public static MetadataResponse kafkatest_f8719_0(final String clusterId, final int numNodes, final Map<String, Errors> topicErrors, final Map<String, Integer> topicPartitionCounts, final Function<TopicPartition, Integer> epochSupplier)
{    return metadataUpdateWith(clusterId, numNodes, topicErrors, topicPartitionCounts, epochSupplier, MetadataResponse.PartitionMetadata::new);}
public static MetadataResponse kafkatest_f8720_0(final String clusterId, final int numNodes, final Map<String, Errors> topicErrors, final Map<String, Integer> topicPartitionCounts, final Function<TopicPartition, Integer> epochSupplier, final PartitionMetadataSupplier partitionSupplier)
{    final List<Node> nodes = new ArrayList<>(numNodes);    for (int i = 0; i < numNodes; i++) nodes.add(new Node(i, "localhost", 1969 + i));    List<MetadataResponse.TopicMetadata> topicMetadata = new ArrayList<>();    for (Map.Entry<String, Integer> topicPartitionCountEntry : topicPartitionCounts.entrySet()) {        String topic = topicPartitionCountEntry.getKey();        int numPartitions = topicPartitionCountEntry.getValue();        List<MetadataResponse.PartitionMetadata> partitionMetadata = new ArrayList<>(numPartitions);        for (int i = 0; i < numPartitions; i++) {            TopicPartition tp = new TopicPartition(topic, i);            Node leader = nodes.get(i % nodes.size());            List<Node> replicas = Collections.singletonList(leader);            partitionMetadata.add(partitionSupplier.supply(Errors.NONE, i, leader, Optional.ofNullable(epochSupplier.apply(tp)), replicas, replicas, replicas));        }        topicMetadata.add(new MetadataResponse.TopicMetadata(Errors.NONE, topic, Topic.isInternal(topic), partitionMetadata));    }    for (Map.Entry<String, Errors> topicErrorEntry : topicErrors.entrySet()) {        String topic = topicErrorEntry.getKey();        topicMetadata.add(new MetadataResponse.TopicMetadata(topicErrorEntry.getValue(), topic, Topic.isInternal(topic), Collections.emptyList()));    }    return MetadataResponse.prepareResponse(nodes, clusterId, 0, topicMetadata);}
public static Properties kafkatest_f8729_0(final String bootstrapServers, final Class keySerializer, final Class valueSerializer, final Properties additional)
{    final Properties properties = new Properties();    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);    properties.put(ProducerConfig.ACKS_CONFIG, "all");    properties.put(ProducerConfig.RETRIES_CONFIG, 0);    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);    properties.putAll(additional);    return properties;}
public static Properties kafkatest_f8730_0(final String bootstrapServers, final Class keySerializer, final Class valueSerializer)
{    return producerConfig(bootstrapServers, keySerializer, valueSerializer, new Properties());}
public static void kafkatest_f8739_0(Iterable<T> it1, Iterable<T> it2)
{    assertEquals(toList(it1), toList(it2));}
public static void kafkatest_f8740_0(Iterator<T> it1, Iterator<T> it2)
{    assertEquals(Utils.toList(it1), Utils.toList(it2));}
public void kafkatest_f8749_0(ConnectorContext ctx)
{    context = ctx;}
public void kafkatest_f8750_0(ConnectorContext ctx, List<Map<String, String>> taskConfigs)
{    context = ctx;// Ignore taskConfigs. May result in more churn of tasks during recovery if updated configs// are very different, but reduces the difficulty of implementing a Connector}
public Long kafkatest_f8759_0()
{    return timestamp;}
public Headers kafkatest_f8760_0()
{    return headers;}
public Type kafkatest_f8769_0()
{    return type;}
public boolean kafkatest_f8770_0()
{    return optional;}
public Schema kafkatest_f8779_0()
{    if (type != Type.MAP && type != Type.ARRAY)        throw new DataException("Cannot look up value schema on non-array and non-map type");    return valueSchema;}
public static void kafkatest_f8780_0(Schema schema, Object value)
{    validateValue(null, schema, value);}
public static SchemaBuilder kafkatest_f8789_0()
{    return SchemaBuilder.int32().name(LOGICAL_NAME).version(1);}
public static int kafkatest_f8790_0(Schema schema, java.util.Date value)
{    if (!(LOGICAL_NAME.equals(schema.name())))        throw new DataException("Requested conversion of Date object but the schema does not match.");    Calendar calendar = Calendar.getInstance(UTC);    calendar.setTime(value);    if (calendar.get(Calendar.HOUR_OF_DAY) != 0 || calendar.get(Calendar.MINUTE) != 0 || calendar.get(Calendar.SECOND) != 0 || calendar.get(Calendar.MILLISECOND) != 0) {        throw new DataException("Kafka Connect Date type should not have any time fields set to non-zero values.");    }    long unixMillis = calendar.getTimeInMillis();    return (int) (unixMillis / MILLIS_PER_DAY);}
public Schema kafkatest_f8799_0()
{    return schema;}
public boolean kafkatest_f8800_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Field field = (Field) o;    return Objects.equals(index, field.index) && Objects.equals(name, field.name) && Objects.equals(schema, field.schema);}
public String kafkatest_f8809_0()
{    return "SchemaAndValue{" + "schema=" + schema + ", value=" + value + '}';}
public boolean kafkatest_f8810_0()
{    return optional == null ? false : optional;}
public String kafkatest_f8819_0()
{    return doc;}
public SchemaBuilder kafkatest_f8820_0(String doc)
{    checkCanSet(DOC_FIELD, this.doc, doc);    this.doc = doc;    return this;}
public static SchemaBuilder kafkatest_f8829_0()
{    return new SchemaBuilder(Type.INT64);}
public static SchemaBuilder kafkatest_f8830_0()
{    return new SchemaBuilder(Type.FLOAT32);}
public static SchemaBuilder kafkatest_f8839_0(Schema valueSchema)
{    if (null == valueSchema)        throw new SchemaBuilderException("valueSchema cannot be null.");    SchemaBuilder builder = new SchemaBuilder(Type.ARRAY);    builder.valueSchema = valueSchema;    return builder;}
public static SchemaBuilder kafkatest_f8840_0(Schema keySchema, Schema valueSchema)
{    if (null == keySchema)        throw new SchemaBuilderException("keySchema cannot be null.");    if (null == valueSchema)        throw new SchemaBuilderException("valueSchema cannot be null.");    SchemaBuilder builder = new SchemaBuilder(Type.MAP);    builder.keySchema = keySchema;    builder.valueSchema = valueSchema;    return builder;}
private static Object kafkatest_f8849_0(Schema source, Struct sourceStruct, Schema target) throws SchemaProjectorException
{    Struct targetStruct = new Struct(target);    for (Field targetField : target.fields()) {        String fieldName = targetField.name();        Field sourceField = source.field(fieldName);        if (sourceField != null) {            Object sourceFieldValue = sourceStruct.get(fieldName);            try {                Object targetFieldValue = project(sourceField.schema(), sourceFieldValue, targetField.schema());                targetStruct.put(fieldName, targetFieldValue);            } catch (SchemaProjectorException e) {                throw new SchemaProjectorException("Error projecting " + sourceField.name(), e);            }        } else if (targetField.schema().isOptional()) {        // Ignore missing field        } else if (targetField.schema().defaultValue() != null) {            targetStruct.put(fieldName, targetField.schema().defaultValue());        } else {            throw new SchemaProjectorException("Required field `" + fieldName + "` is missing from source schema: " + source);        }    }    return targetStruct;}
private static void kafkatest_f8850_0(Schema source, Schema target)
{    if (source.type() != target.type() && !isPromotable(source.type(), target.type())) {        throw new SchemaProjectorException("Schema type mismatch. source type: " + source.type() + " and target type: " + target.type());    } else if (!Objects.equals(source.name(), target.name())) {        throw new SchemaProjectorException("Schema name mismatch. source name: " + source.name() + " and target name: " + target.name());    } else if (!Objects.equals(source.parameters(), target.parameters())) {        throw new SchemaProjectorException("Schema parameters not equal. source parameters: " + source.parameters() + " and target parameters: " + target.parameters());    }}
public Byte kafkatest_f8859_0(String fieldName)
{    return (Byte) getCheckType(fieldName, Schema.Type.INT8);}
public Short kafkatest_f8860_0(String fieldName)
{    return (Short) getCheckType(fieldName, Schema.Type.INT16);}
public Map<K, V> kafkatest_f8869_0(String fieldName)
{    return (Map<K, V>) getCheckType(fieldName, Schema.Type.MAP);}
public Struct kafkatest_f8870_0(String fieldName)
{    return (Struct) getCheckType(fieldName, Schema.Type.STRUCT);}
public static SchemaBuilder kafkatest_f8879_0()
{    return SchemaBuilder.int32().name(LOGICAL_NAME).version(1);}
public static int kafkatest_f8880_0(Schema schema, java.util.Date value)
{    if (!(LOGICAL_NAME.equals(schema.name())))        throw new DataException("Requested conversion of Time object but the schema does not match.");    Calendar calendar = Calendar.getInstance(UTC);    calendar.setTime(value);    long unixMillis = calendar.getTimeInMillis();    if (unixMillis < 0 || unixMillis > MILLIS_PER_DAY) {        throw new DataException("Kafka Connect Time type should not have any date fields set to non-zero values.");    }    return (int) unixMillis;}
public static Long kafkatest_f8889_0(Schema schema, Object value) throws DataException
{    return (Long) convertTo(Schema.OPTIONAL_INT64_SCHEMA, schema, value);}
public static Float kafkatest_f8890_0(Schema schema, Object value) throws DataException
{    return (Float) convertTo(Schema.OPTIONAL_FLOAT32_SCHEMA, schema, value);}
public static BigDecimal kafkatest_f8899_0(Schema schema, Object value, int scale)
{    return (BigDecimal) convertTo(Decimal.schema(scale), schema, value);}
public static Schema kafkatest_f8900_0(Object value)
{    if (value instanceof String) {        return Schema.STRING_SCHEMA;    }    if (value instanceof Boolean) {        return Schema.BOOLEAN_SCHEMA;    }    if (value instanceof Byte) {        return Schema.INT8_SCHEMA;    }    if (value instanceof Short) {        return Schema.INT16_SCHEMA;    }    if (value instanceof Integer) {        return Schema.INT32_SCHEMA;    }    if (value instanceof Long) {        return Schema.INT64_SCHEMA;    }    if (value instanceof Float) {        return Schema.FLOAT32_SCHEMA;    }    if (value instanceof Double) {        return Schema.FLOAT64_SCHEMA;    }    if (value instanceof byte[] || value instanceof ByteBuffer) {        return Schema.BYTES_SCHEMA;    }    if (value instanceof List) {        List<?> list = (List<?>) value;        if (list.isEmpty()) {            return null;        }        SchemaDetector detector = new SchemaDetector();        for (Object element : list) {            if (!detector.canDetect(element)) {                return null;            }        }        return SchemaBuilder.array(detector.schema()).build();    }    if (value instanceof Map) {        Map<?, ?> map = (Map<?, ?>) value;        if (map.isEmpty()) {            return null;        }        SchemaDetector keyDetector = new SchemaDetector();        SchemaDetector valueDetector = new SchemaDetector();        for (Map.Entry<?, ?> entry : map.entrySet()) {            if (!keyDetector.canDetect(entry.getKey()) || !valueDetector.canDetect(entry.getValue())) {                return null;            }        }        return SchemaBuilder.map(keyDetector.schema(), valueDetector.schema()).build();    }    if (value instanceof Struct) {        return ((Struct) value).schema();    }    return null;}
protected static SchemaAndValuef8909_1Parser parser, boolean embedded) throws NoSuchElementException
{    if (!parser.hasNext()) {        return null;    }    if (embedded) {        if (parser.canConsume(NULL_VALUE)) {            return null;        }        if (parser.canConsume(QUOTE_DELIMITER)) {            StringBuilder sb = new StringBuilder();            while (parser.hasNext()) {                if (parser.canConsume(QUOTE_DELIMITER)) {                    break;                }                sb.append(parser.next());            }            return new SchemaAndValue(Schema.STRING_SCHEMA, sb.toString());        }    }    if (parser.canConsume(TRUE_LITERAL)) {        return TRUE_SCHEMA_AND_VALUE;    }    if (parser.canConsume(FALSE_LITERAL)) {        return FALSE_SCHEMA_AND_VALUE;    }    int startPosition = parser.mark();    try {        if (parser.canConsume(ARRAY_BEGIN_DELIMITER)) {            List<Object> result = new ArrayList<>();            Schema elementSchema = null;            while (parser.hasNext()) {                if (parser.canConsume(ARRAY_END_DELIMITER)) {                    Schema listSchema = null;                    if (elementSchema != null) {                        listSchema = SchemaBuilder.array(elementSchema).schema();                    }                    result = alignListEntriesWithSchema(listSchema, result);                    return new SchemaAndValue(listSchema, result);                }                if (parser.canConsume(COMMA_DELIMITER)) {                    throw new DataException("Unable to parse an empty array element: " + parser.original());                }                SchemaAndValue element = parse(parser, true);                elementSchema = commonSchemaFor(elementSchema, element);                result.add(element.value());                parser.canConsume(COMMA_DELIMITER);            }            // Missing either a comma or an end delimiter            if (COMMA_DELIMITER.equals(parser.previous())) {                throw new DataException("Array is missing element after ',': " + parser.original());            }            throw new DataException("Array is missing terminating ']': " + parser.original());        }        if (parser.canConsume(MAP_BEGIN_DELIMITER)) {            Map<Object, Object> result = new LinkedHashMap<>();            Schema keySchema = null;            Schema valueSchema = null;            while (parser.hasNext()) {                if (parser.canConsume(MAP_END_DELIMITER)) {                    Schema mapSchema = null;                    if (keySchema != null && valueSchema != null) {                        mapSchema = SchemaBuilder.map(keySchema, valueSchema).schema();                    }                    result = alignMapKeysAndValuesWithSchema(mapSchema, result);                    return new SchemaAndValue(mapSchema, result);                }                if (parser.canConsume(COMMA_DELIMITER)) {                    throw new DataException("Unable to parse a map entry has no key or value: " + parser.original());                }                SchemaAndValue key = parse(parser, true);                if (key == null || key.value() == null) {                    throw new DataException("Map entry may not have a null key: " + parser.original());                }                if (!parser.canConsume(ENTRY_DELIMITER)) {                    throw new DataException("Map entry is missing '=': " + parser.original());                }                SchemaAndValue value = parse(parser, true);                Object entryValue = value != null ? value.value() : null;                result.put(key.value(), entryValue);                parser.canConsume(COMMA_DELIMITER);                keySchema = commonSchemaFor(keySchema, key);                valueSchema = commonSchemaFor(valueSchema, value);            }            // Missing either a comma or an end delimiter            if (COMMA_DELIMITER.equals(parser.previous())) {                throw new DataException("Map is missing element after ',': " + parser.original());            }            throw new DataException("Map is missing terminating ']': " + parser.original());        }    } catch (DataException e) {         reverting to string", e);        parser.rewindTo(startPosition);    }    String token = parser.next().trim();    // original can be empty string but is handled right away; no way for token to be empty here    assert !token.isEmpty();    char firstChar = token.charAt(0);    boolean firstCharIsDigit = Character.isDigit(firstChar);    if (firstCharIsDigit || firstChar == '+' || firstChar == '-') {        try {            // Try to parse as a number ...            BigDecimal decimal = new BigDecimal(token);            try {                return new SchemaAndValue(Schema.INT8_SCHEMA, decimal.byteValueExact());            } catch (ArithmeticException e) {            // continue            }            try {                return new SchemaAndValue(Schema.INT16_SCHEMA, decimal.shortValueExact());            } catch (ArithmeticException e) {            // continue            }            try {                return new SchemaAndValue(Schema.INT32_SCHEMA, decimal.intValueExact());            } catch (ArithmeticException e) {            // continue            }            try {                return new SchemaAndValue(Schema.INT64_SCHEMA, decimal.longValueExact());            } catch (ArithmeticException e) {            // continue            }            double dValue = decimal.doubleValue();            if (dValue != Double.NEGATIVE_INFINITY && dValue != Double.POSITIVE_INFINITY) {                return new SchemaAndValue(Schema.FLOAT64_SCHEMA, dValue);            }            Schema schema = Decimal.schema(decimal.scale());            return new SchemaAndValue(schema, decimal);        } catch (NumberFormatException e) {        // can't parse as a number        }    }    if (firstCharIsDigit) {        // Check for a date, time, or timestamp ...        int tokenLength = token.length();        if (tokenLength == ISO_8601_DATE_LENGTH) {            try {                return new SchemaAndValue(Date.SCHEMA, new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN).parse(token));            } catch (ParseException e) {            // not a valid date            }        } else if (tokenLength == ISO_8601_TIME_LENGTH) {            try {                return new SchemaAndValue(Time.SCHEMA, new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN).parse(token));            } catch (ParseException e) {            // not a valid date            }        } else if (tokenLength == ISO_8601_TIMESTAMP_LENGTH) {            try {                return new SchemaAndValue(Timestamp.SCHEMA, new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN).parse(token));            } catch (ParseException e) {            // not a valid date            }        }    }    // so this is not embedded and we can use the original string...    return new SchemaAndValue(Schema.STRING_SCHEMA, parser.original());}
protected static Schema kafkatest_f8910_0(Schema previous, SchemaAndValue latest)
{    if (latest == null) {        return previous;    }    if (previous == null) {        return latest.schema();    }    Schema newSchema = latest.schema();    Type previousType = previous.type();    Type newType = newSchema.type();    if (previousType != newType) {        switch(previous.type()) {            case INT8:                if (newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {                    return newSchema;                }                break;            case INT16:                if (newType == Type.INT8) {                    return previous;                }                if (newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {                    return newSchema;                }                break;            case INT32:                if (newType == Type.INT8 || newType == Type.INT16) {                    return previous;                }                if (newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {                    return newSchema;                }                break;            case INT64:                if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32) {                    return previous;                }                if (newType == Type.FLOAT32 || newType == Type.FLOAT64) {                    return newSchema;                }                break;            case FLOAT32:                if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64) {                    return previous;                }                if (newType == Type.FLOAT64) {                    return newSchema;                }                break;            case FLOAT64:                if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32) {                    return previous;                }                break;        }        return null;    }    if (previous.isOptional() == newSchema.isOptional()) {        // Use the optional one        return previous.isOptional() ? previous : newSchema;    }    if (!previous.equals(newSchema)) {        return null;    }    return previous;}
public boolean kafkatest_f8919_0()
{    return nextToken != null || canConsumeNextToken();}
protected boolean kafkatest_f8920_0()
{    return iter.getEndIndex() > iter.getIndex();}
public Schema kafkatest_f8929_0()
{    Schema schema = schemaAndValue.schema();    if (schema == null && value() instanceof Struct) {        schema = ((Struct) value()).schema();    }    return schema;}
public Header kafkatest_f8930_0(String key)
{    Objects.requireNonNull(key, "Null header keys are not permitted");    if (this.key.equals(key)) {        return this;    }    return new ConnectHeader(key, schemaAndValue);}
public boolean kafkatest_f8939_0()
{    return headers == null ? true : headers.isEmpty();}
public Headers kafkatest_f8940_0()
{    if (headers != null) {        headers.clear();    }    return this;}
public Headers kafkatest_f8949_0(String key, short value)
{    return addWithoutValidating(key, value, Schema.INT16_SCHEMA);}
public Headers kafkatest_f8950_0(String key, int value)
{    return addWithoutValidating(key, value, Schema.INT32_SCHEMA);}
public Headers kafkatest_f8959_0(String key, java.util.Date value)
{    if (value != null) {        // Check that this is a time ...        Time.fromLogical(Time.SCHEMA, value);    }    return addWithoutValidating(key, value, Time.SCHEMA);}
public Headers kafkatest_f8960_0(String key, java.util.Date value)
{    if (value != null) {        // Check that this is a timestamp ...        Timestamp.fromLogical(Timestamp.SCHEMA, value);    }    return addWithoutValidating(key, value, Timestamp.SCHEMA);}
public int kafkatest_f8969_0()
{    return isEmpty() ? EMPTY_HASH : Objects.hash(headers);}
public boolean kafkatest_f8970_0(Object obj)
{    if (obj == this) {        return true;    }    if (obj instanceof Headers) {        Headers that = (Headers) obj;        Iterator<Header> thisIter = this.iterator();        Iterator<Header> thatIter = that.iterator();        while (thisIter.hasNext() && thatIter.hasNext()) {            if (!Objects.equals(thisIter.next(), thatIter.next()))                return false;        }        return !thisIter.hasNext() && !thatIter.hasNext();    }    return false;}
public String kafkatest_f8979_0()
{    return traceMessage;}
 Map<String, String> kafkatest_f8980_0(String connName)
{    throw new UnsupportedOperationException();}
public int kafkatest_f8989_0()
{    return Objects.hash(taskId);}
public long kafkatest_f8990_0()
{    return kafkaOffset;}
public void kafkatest_f9000_0(Collection<TopicPartition> partitions)
{    this.onPartitionsAssigned(partitions);}
public void kafkatest_f9002_0(Collection<TopicPartition> partitions)
{    this.onPartitionsRevoked(partitions);}
public void kafkatest_f9012_0() throws InterruptedException
{// This space intentionally left blank.}
public void kafkatest_f9013_0(SourceRecord record) throws InterruptedException
{// This space intentionally left blank.}
public void kafkatest_f9022_0() throws IOException
{// do nothing}
public ConfigDef kafkatest_f9023_0()
{    return StringConverterConfig.configDef();}
public String kafkatest_f9032_0()
{    return getString(ENCODING_CONFIG);}
public static List<List<T>> kafkatest_f9033_0(List<T> elements, int numGroups)
{    if (numGroups <= 0)        throw new IllegalArgumentException("Number of groups must be positive.");    List<List<T>> result = new ArrayList<>(numGroups);    // Each group has either n+1 or n raw partitions    int perGroup = elements.size() / numGroups;    int leftover = elements.size() - (numGroups * perGroup);    int assigned = 0;    for (int group = 0; group < numGroups; group++) {        int numThisGroup = group < leftover ? perGroup + 1 : perGroup;        List<T> groupList = new ArrayList<>(numThisGroup);        for (int i = 0; i < numThisGroup; i++) {            groupList.add(elements.get(assigned));            assigned++;        }        result.add(groupList);    }    return result;}
public void kafkatest_f9042_0()
{    Schema schema = SchemaBuilder.struct().field("foo", Schema.BOOLEAN_SCHEMA).field("bar", Schema.INT32_SCHEMA).build();    assertEquals(2, schema.fields().size());    // Validate field lookup by name    Field foo = schema.field("foo");    assertEquals(0, foo.index());    Field bar = schema.field("bar");    assertEquals(1, bar.index());    // Any other field name should fail    assertNull(schema.field("other"));}
public void kafkatest_f9043_0()
{    Schema.INT8_SCHEMA.fields();}
public void kafkatest_f9052_0()
{    ConnectSchema.validateValue(Schema.BOOLEAN_SCHEMA, 1.f);}
public void kafkatest_f9053_0()
{    // CharSequence is a similar type (supertype of String), but we restrict to String.    CharBuffer cbuf = CharBuffer.wrap("abc");    ConnectSchema.validateValue(Schema.STRING_SCHEMA, cbuf);}
public void kafkatest_f9062_0()
{    // Top-level schema  matches, but nested does not.    ConnectSchema.validateValue(PARENT_STRUCT_SCHEMA, new Struct(PARENT_STRUCT_SCHEMA).put("nested", new Struct(SchemaBuilder.struct().field("x", Schema.INT32_SCHEMA).build()).put("x", 1)));}
public void kafkatest_f9063_0()
{    ConnectSchema.validateValue(Decimal.schema(2), new BigInteger("156"));}
public void kafkatest_f9072_0()
{    final ConnectSchema emptyStruct = new ConnectSchema(Schema.Type.STRUCT, false, null, null, null, null);    assertEquals(0, emptyStruct.fields().size());    new Struct(emptyStruct);}
public void kafkatest_f9073_0()
{    Schema plain = Date.SCHEMA;    assertEquals(Date.LOGICAL_NAME, plain.name());    assertEquals(1, (Object) plain.version());}
public Type kafkatest_f9082_0()
{    return null;}
public boolean kafkatest_f9083_0()
{    return false;}
public Field kafkatest_f9092_0(String fieldName)
{    return null;}
public Schema kafkatest_f9093_0()
{    return null;}
public void kafkatest_f9102_0()
{    SchemaBuilder.int64().defaultValue("invalid");}
public void kafkatest_f9103_0()
{    Schema schema = SchemaBuilder.float32().build();    assertTypeAndDefault(schema, Schema.Type.FLOAT32, false, null);    assertNoMetadata(schema);    schema = SchemaBuilder.float32().name(NAME).optional().defaultValue(12.f).version(VERSION).doc(DOC).build();    assertTypeAndDefault(schema, Schema.Type.FLOAT32, true, 12.f);    assertMetadata(schema, NAME, VERSION, DOC, NO_PARAMS);}
public void kafkatest_f9112_0()
{    SchemaBuilder.bytes().defaultValue("a string, not bytes");}
public void kafkatest_f9113_0()
{    Map<String, String> expectedParameters = new HashMap<>();    expectedParameters.put("foo", "val");    expectedParameters.put("bar", "baz");    Schema schema = SchemaBuilder.string().parameter("foo", "val").parameter("bar", "baz").build();    assertTypeAndDefault(schema, Schema.Type.STRING, false, null);    assertMetadata(schema, null, null, null, expectedParameters);    schema = SchemaBuilder.string().parameters(expectedParameters).build();    assertTypeAndDefault(schema, Schema.Type.STRING, false, null);    assertMetadata(schema, null, null, null, expectedParameters);}
public void kafkatest_f9122_0()
{    final SchemaBuilder schemaBuilder = SchemaBuilder.string().name("testing").version(123);    schemaBuilder.name("testing");    schemaBuilder.version(123);    assertEquals("testing", schemaBuilder.name());}
public void kafkatest_f9123_0()
{    final SchemaBuilder schemaBuilder = SchemaBuilder.string().name("testing").version(123);    schemaBuilder.name("testing");    schemaBuilder.version(456);}
private void kafkatest_f9132_0(Schema schema)
{    assertMetadata(schema, null, null, null, null);}
public void kafkatest_f9133_0()
{    Object projected;    projected = SchemaProjector.project(Schema.BOOLEAN_SCHEMA, false, Schema.BOOLEAN_SCHEMA);    assertEquals(false, projected);    byte[] bytes = { (byte) 1, (byte) 2 };    projected = SchemaProjector.project(Schema.BYTES_SCHEMA, bytes, Schema.BYTES_SCHEMA);    assertEquals(bytes, projected);    projected = SchemaProjector.project(Schema.STRING_SCHEMA, "abc", Schema.STRING_SCHEMA);    assertEquals("abc", projected);    projected = SchemaProjector.project(Schema.BOOLEAN_SCHEMA, false, Schema.OPTIONAL_BOOLEAN_SCHEMA);    assertEquals(false, projected);    projected = SchemaProjector.project(Schema.BYTES_SCHEMA, bytes, Schema.OPTIONAL_BYTES_SCHEMA);    assertEquals(bytes, projected);    projected = SchemaProjector.project(Schema.STRING_SCHEMA, "abc", Schema.OPTIONAL_STRING_SCHEMA);    assertEquals("abc", projected);    try {        SchemaProjector.project(Schema.OPTIONAL_BOOLEAN_SCHEMA, false, Schema.BOOLEAN_SCHEMA);        fail("Cannot project optional schema to schema with no default value.");    } catch (DataException e) {    // expected    }    try {        SchemaProjector.project(Schema.OPTIONAL_BYTES_SCHEMA, bytes, Schema.BYTES_SCHEMA);        fail("Cannot project optional schema to schema with no default value.");    } catch (DataException e) {    // expected    }    try {        SchemaProjector.project(Schema.OPTIONAL_STRING_SCHEMA, "abc", Schema.STRING_SCHEMA);        fail("Cannot project optional schema to schema with no default value.");    } catch (DataException e) {    // expected    }}
public void kafkatest_f9142_0()
{    Schema source = SchemaBuilder.map(Schema.INT32_SCHEMA, Schema.INT32_SCHEMA).optional().build();    Schema target = SchemaBuilder.map(Schema.INT32_SCHEMA, Schema.INT32_SCHEMA).defaultValue(Collections.singletonMap(1, 2)).build();    Object projected = SchemaProjector.project(source, Collections.singletonMap(3, 4), target);    assertEquals(Collections.singletonMap(3, 4), projected);    projected = SchemaProjector.project(source, null, target);    assertEquals(Collections.singletonMap(1, 2), projected);    Schema promotedTarget = SchemaBuilder.map(Schema.INT64_SCHEMA, Schema.FLOAT32_SCHEMA).defaultValue(Collections.singletonMap(3L, 4.5F)).build();    projected = SchemaProjector.project(source, Collections.singletonMap(3, 4), promotedTarget);    assertEquals(Collections.singletonMap(3L, 4.F), projected);    projected = SchemaProjector.project(source, null, promotedTarget);    assertEquals(Collections.singletonMap(3L, 4.5F), projected);    Schema noDefaultValueTarget = SchemaBuilder.map(Schema.INT32_SCHEMA, Schema.INT32_SCHEMA).build();    try {        SchemaProjector.project(source, null, noDefaultValueTarget);        fail("Reader does not provide a default value.");    } catch (SchemaProjectorException e) {    // expected    }    Schema nonPromotableTarget = SchemaBuilder.map(Schema.BOOLEAN_SCHEMA, Schema.STRING_SCHEMA).build();    try {        SchemaProjector.project(source, null, nonPromotableTarget);        fail("Neither source type matches target type nor source type can be promoted to target type");    } catch (SchemaProjectorException e) {    // expected    }}
public void kafkatest_f9143_0()
{    Schema source = SchemaBuilder.int32().name("source").build();    Schema target = SchemaBuilder.int32().name("target").build();    try {        SchemaProjector.project(source, 12, target);        fail("Source name and target name mismatch.");    } catch (SchemaProjectorException e) {    // expected    }    Schema targetWithParameters = SchemaBuilder.int32().parameters(Collections.singletonMap("key", "value"));    try {        SchemaProjector.project(source, 34, targetWithParameters);        fail("Source parameters and target parameters mismatch.");    } catch (SchemaProjectorException e) {    // expected    }}
public void kafkatest_f9152_0()
{    new Struct(NESTED_SCHEMA).put("map", Collections.singletonMap("should fail because keys should be int8s", (byte) 12));}
public void kafkatest_f9153_0()
{    new Struct(NESTED_SCHEMA).put("nested", new Struct(MAP_SCHEMA));}
public void kafkatest_f9162_0()
{    Schema schema = SchemaBuilder.struct().field("one", Schema.STRING_SCHEMA).field("two", Schema.STRING_SCHEMA).field("three", Schema.STRING_SCHEMA).build();    Struct struct = new Struct(schema);    Exception e = assertThrows(DataException.class, struct::validate);    assertEquals("Invalid value: null used for required field: \"one\", schema type: STRING", e.getMessage());}
public void kafkatest_f9163_0()
{    String fieldName = "field";    FakeSchema fakeSchema = new FakeSchema();    Exception e = assertThrows(DataException.class, () -> ConnectSchema.validateValue(fieldName, fakeSchema, new Object()));    assertEquals("Invalid Java object for schema type null: class java.lang.Object for field: \"field\"", e.getMessage());    e = assertThrows(DataException.class, () -> ConnectSchema.validateValue(fieldName, Schema.INT8_SCHEMA, new Object()));    assertEquals("Invalid Java object for schema type INT8: class java.lang.Object for field: \"field\"", e.getMessage());}
public void kafkatest_f9172_0()
{    assertEquals(0, Time.fromLogical(Time.SCHEMA, EPOCH.getTime()));    assertEquals(10000, Time.fromLogical(Time.SCHEMA, EPOCH_PLUS_TEN_THOUSAND_MILLIS.getTime()));}
public void kafkatest_f9173_0()
{    Time.fromLogical(Time.builder().name("invalid").build(), EPOCH.getTime());}
public void kafkatest_f9182_0()
{    assertRoundTrip(Schema.STRING_SCHEMA, "");}
public void kafkatest_f9183_0()
{    assertRoundTrip(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA, "three\"blind\\\"mice");    assertRoundTrip(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA, "string with delimiters: <>?,./\\=+-!@#$%^&*(){}[]|;':");}
public void kafkatest_f9192_0()
{    SchemaAndValue result = roundTrip(STRING_INT_MAP_SCHEMA, " { \"foo\" :  1234567890 , \"bar\" : 0,  \"baz\" : -987654321 }  ");    assertEquals(STRING_INT_MAP_SCHEMA, result.schema());    assertEquals(STRING_INT_MAP, result.value());}
public void kafkatest_f9193_0()
{    assertRoundTrip(STRING_LIST_SCHEMA, STRING_LIST_SCHEMA, STRING_LIST);}
public void kafkatest_f9202_0()
{    Values.convertToList(Schema.STRING_SCHEMA, " { \"foo\" :  1234567890 , \"a\", \"bar\" : 0,  \"baz\" : -987654321 }  ");}
public void kafkatest_f9203_0()
{    Values.convertToList(Schema.STRING_SCHEMA, " { ,,  , , }  ");}
public void kafkatest_f9212_0()
{    java.util.Date current = new java.util.Date();    long currentMillis = current.getTime() % MILLIS_PER_DAY;    // java.util.Date - just copy    java.util.Date ts1 = Values.convertToTimestamp(Timestamp.SCHEMA, current);    assertEquals(current, ts1);    // java.util.Date as a Timestamp - discard the day's milliseconds and keep the date    java.util.Date currentDate = new java.util.Date(current.getTime() - currentMillis);    ts1 = Values.convertToTimestamp(Date.SCHEMA, currentDate);    assertEquals(currentDate, ts1);    // java.util.Date as a Time - discard the date and keep the day's milliseconds    ts1 = Values.convertToTimestamp(Time.SCHEMA, currentMillis);    assertEquals(new java.util.Date(currentMillis), ts1);    // ISO8601 strings - currently broken because tokenization breaks at colon    // Millis as string    java.util.Date ts3 = Values.convertToTimestamp(Timestamp.SCHEMA, Long.toString(current.getTime()));    assertEquals(current, ts3);    // Millis as long    java.util.Date ts4 = Values.convertToTimestamp(Timestamp.SCHEMA, current.getTime());    assertEquals(current, ts4);}
protected void kafkatest_f9214_0(String input)
{    assertParsed(input, input);}
protected void kafkatest_f9223_0(Headers headers)
{    headers.addBoolean(key, true);    headers.addInt(key, 0);    headers.addString(other, "other value");    headers.addString(key, null);    headers.addString(key, "third");}
public void kafkatest_f9224_0()
{    Headers other = new ConnectHeaders();    assertEquals(headers, other);    assertEquals(headers.hashCode(), other.hashCode());    populate(headers);    assertNotEquals(headers, other);    assertNotEquals(headers.hashCode(), other.hashCode());    populate(other);    assertEquals(headers, other);    assertEquals(headers.hashCode(), other.hashCode());    headers.addString("wow", "some value");    assertNotEquals(headers, other);}
public void kafkatest_f9233_0()
{    populate(headers);    iter = headers.allWithName(key);    assertContainsHeader(key, Schema.BOOLEAN_SCHEMA, true);    assertContainsHeader(key, Schema.INT32_SCHEMA, 0);    assertContainsHeader(key, Schema.STRING_SCHEMA, "third");    assertOnlySingleHeader(other, Schema.STRING_SCHEMA, "other value");    headers.remove(key);    assertNoHeaderWithKey(key);    assertOnlySingleHeader(other, Schema.STRING_SCHEMA, "other value");}
public void kafkatest_f9234_0()
{    populate(headers);    iter = headers.allWithName(key);    assertContainsHeader(key, Schema.BOOLEAN_SCHEMA, true);    assertContainsHeader(key, Schema.INT32_SCHEMA, 0);    assertContainsHeader(key, Schema.STRING_SCHEMA, "third");    assertOnlySingleHeader(other, Schema.STRING_SCHEMA, "other value");    headers.clear();    assertNoHeaderWithKey(key);    assertNoHeaderWithKey(other);    assertEquals(0, headers.size());    assertTrue(headers.isEmpty());}
public void kafkatest_f9243_0()
{    assertSchemaMatches(Schema.OPTIONAL_BOOLEAN_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_BYTES_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_INT8_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_INT16_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_INT32_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_INT64_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_FLOAT32_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_FLOAT64_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_STRING_SCHEMA, null);    assertSchemaMatches(Schema.BOOLEAN_SCHEMA, true);    assertSchemaMatches(Schema.BYTES_SCHEMA, new byte[] {});    assertSchemaMatches(Schema.INT8_SCHEMA, (byte) 0);    assertSchemaMatches(Schema.INT16_SCHEMA, (short) 0);    assertSchemaMatches(Schema.INT32_SCHEMA, 0);    assertSchemaMatches(Schema.INT64_SCHEMA, 0L);    assertSchemaMatches(Schema.FLOAT32_SCHEMA, 1.0f);    assertSchemaMatches(Schema.FLOAT64_SCHEMA, 1.0d);    assertSchemaMatches(Schema.STRING_SCHEMA, "value");    assertSchemaMatches(SchemaBuilder.array(Schema.STRING_SCHEMA), new ArrayList<String>());    assertSchemaMatches(SchemaBuilder.array(Schema.STRING_SCHEMA), Collections.singletonList("value"));    assertSchemaMatches(SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.INT32_SCHEMA), new HashMap<String, Integer>());    assertSchemaMatches(SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.INT32_SCHEMA), Collections.singletonMap("a", 0));    Schema emptyStructSchema = SchemaBuilder.struct();    assertSchemaMatches(emptyStructSchema, new Struct(emptyStructSchema));    Schema structSchema = SchemaBuilder.struct().field("foo", Schema.OPTIONAL_BOOLEAN_SCHEMA).field("bar", Schema.STRING_SCHEMA).schema();    assertSchemaMatches(structSchema, new Struct(structSchema).put("foo", true).put("bar", "v"));}
public void kafkatest_f9244_0()
{    assertSchemaMatches(Decimal.schema(3), new BigDecimal(100.00));    assertSchemaMatches(Time.SCHEMA, new java.util.Date());    assertSchemaMatches(Date.SCHEMA, new java.util.Date());    assertSchemaMatches(Timestamp.SCHEMA, new java.util.Date());}
protected void kafkatest_f9253_0(Schema schema, Object value)
{    try {        assertSchemaMatches(schema, value);        fail("Should have failed to validate value '" + value + "' and schema: " + schema);    } catch (DataException e) {    // expected    }}
protected void kafkatest_f9254_0(String key, Schema schema, Object value)
{    try {        headers.add(key, value, schema);        fail("Should have failed to add header with key '" + key + "', value '" + value + "', and schema: " + schema);    } catch (DataException e) {    // expected    }}
public void kafkatest_f9263_0()
{    key = "key";    withString("value");}
protected Header kafkatest_f9264_0(Schema schema, Object value)
{    header = new ConnectHeader(key, new SchemaAndValue(schema, value));    return header;}
public void kafkatest_f9273_0()
{    assertEquals(TOPIC_NAME, record.topic());    assertEquals(PARTITION_NUMBER, record.kafkaPartition());    assertEquals(Schema.STRING_SCHEMA, record.keySchema());    assertEquals("key", record.key());    assertEquals(Schema.BOOLEAN_SCHEMA, record.valueSchema());    assertEquals(false, record.value());    assertEquals(KAFKA_OFFSET, record.kafkaOffset());    assertEquals(KAFKA_TIMESTAMP, record.timestamp());    assertEquals(TS_TYPE, record.timestampType());    assertNotNull(record.headers());    assertTrue(record.headers().isEmpty());}
public void kafkatest_f9274_0()
{    SinkRecord duplicate = record.newRecord(TOPIC_NAME, PARTITION_NUMBER, Schema.STRING_SCHEMA, "key", Schema.BOOLEAN_SCHEMA, false, KAFKA_TIMESTAMP);    assertEquals(TOPIC_NAME, duplicate.topic());    assertEquals(PARTITION_NUMBER, duplicate.kafkaPartition());    assertEquals(Schema.STRING_SCHEMA, duplicate.keySchema());    assertEquals("key", duplicate.key());    assertEquals(Schema.BOOLEAN_SCHEMA, duplicate.valueSchema());    assertEquals(false, duplicate.value());    assertEquals(KAFKA_OFFSET, duplicate.kafkaOffset());    assertEquals(KAFKA_TIMESTAMP, duplicate.timestamp());    assertEquals(TS_TYPE, duplicate.timestampType());    assertNotNull(duplicate.headers());    assertTrue(duplicate.headers().isEmpty());    assertNotSame(record.headers(), duplicate.headers());    assertEquals(record.headers(), duplicate.headers());}
public void kafkatest_f9283_0()
{    for (ConverterType type : ConverterType.values()) {        assertEquals(type, ConverterType.withName(type.getName()));    }}
public void kafkatest_f9284_0()
{    converter = new SimpleHeaderConverter();}
public void kafkatest_f9293_0()
{    SchemaAndValue result = roundTrip(Schema.STRING_SCHEMA, "{\"foo\":12345,\"bar\":0,\"baz\":-4321}");    assertEquals(STRING_SHORT_MAP_SCHEMA, result.schema());    assertEquals(STRING_SHORT_MAP, result.value());}
public void kafkatest_f9294_0()
{    SchemaAndValue result = roundTrip(Schema.STRING_SCHEMA, " { \"foo\" :  12345 , \"bar\" : 0,  \"baz\" : -4321 }  ");    assertEquals(STRING_SHORT_MAP_SCHEMA, result.schema());    assertEquals(STRING_SHORT_MAP, result.value());}
public void kafkatest_f9303_0()
{    assertRoundTrip(null, new ArrayList<>());}
protected SchemaAndValue kafkatest_f9304_0(Schema schema, Object input)
{    byte[] serialized = converter.fromConnectHeader(TOPIC, HEADER, schema, input);    return converter.toConnectHeader(TOPIC, HEADER, serialized);}
public void kafkatest_f9313_0() throws UnsupportedEncodingException
{    converter.configure(Collections.singletonMap("converter.encoding", "UTF-16"), true);    SchemaAndValue data = converter.toConnectData(TOPIC, SAMPLE_STRING.getBytes("UTF-16"));    assertEquals(Schema.OPTIONAL_STRING_SCHEMA, data.schema());    assertEquals(SAMPLE_STRING, data.value());}
public void kafkatest_f9314_0() throws UnsupportedEncodingException
{    assertArrayEquals(SAMPLE_STRING.getBytes("UTF8"), converter.fromConnectHeader(TOPIC, "hdr", Schema.STRING_SCHEMA, SAMPLE_STRING));}
public voidf9325_1Subject subject, CallbackHandler callbackHandler, Map<String, ?> sharedState, Map<String, ?> options)
{    this.callbackHandler = callbackHandler;    fileName = (String) options.get(FILE_OPTIONS);    if (fileName == null || fileName.trim().isEmpty()) {        throw new ConfigException("Property Credentials file must be specified");    }    if (!credentialPropertiesMap.containsKey(fileName)) {        Properties credentialProperties = new Properties();        try {            try (InputStream inputStream = Files.newInputStream(Paths.get(fileName))) {                credentialProperties.load(inputStream);            }            credentialPropertiesMap.putIfAbsent(fileName, credentialProperties);        } catch (IOException e) {                        throw new ConfigException("Error loading Property Credentials file");        }    }}
public boolean kafkatest_f9326_0() throws LoginException
{    Callback[] callbacks = configureCallbacks();    try {        callbackHandler.handle(callbacks);    } catch (Exception e) {        throw new LoginException(e.getMessage());    }    String username = ((NameCallback) callbacks[0]).getName();    char[] passwordChars = ((PasswordCallback) callbacks[1]).getPassword();    String password = passwordChars != null ? new String(passwordChars) : null;    Properties credentialProperties = credentialPropertiesMap.get(fileName);    authenticated = credentialProperties.isEmpty() || (password != null && password.equals(credentialProperties.get(username)));    return authenticated;}
public void kafkatest_f9335_0() throws IOException
{    setMock("Basic", "user", "password1", true);    jaasBasicAuthFilter.filter(requestContext);}
public void kafkatest_f9336_0() throws IOException
{    setMock("Unknown", "user", "password", true);    jaasBasicAuthFilter.filter(requestContext);}
public String kafkatest_f9345_0()
{    return AppInfoParser.getVersion();}
public void kafkatest_f9346_0(Map<String, String> props)
{    AbstractConfig parsedConfig = new AbstractConfig(CONFIG_DEF, props);    filename = parsedConfig.getString(FILE_CONFIG);}
public void kafkatest_f9355_0()
{    if (outputStream != null && outputStream != System.out)        outputStream.close();}
private String kafkatest_f9356_0()
{    return filename == null ? "stdout" : filename;}
public List<SourceRecord>f9365_1) throws InterruptedException
{    if (stream == null) {        try {            stream = Files.newInputStream(Paths.get(filename));            Map<String, Object> offset = context.offsetStorageReader().offset(Collections.singletonMap(FILENAME_FIELD, filename));            if (offset != null) {                Object lastRecordedOffset = offset.get(POSITION_FIELD);                if (lastRecordedOffset != null && !(lastRecordedOffset instanceof Long))                    throw new ConnectException("Offset position is the incorrect type");                if (lastRecordedOffset != null) {                                        long skipLeft = (Long) lastRecordedOffset;                    while (skipLeft > 0) {                        try {                            long skipped = stream.skip(skipLeft);                            skipLeft -= skipped;                        } catch (IOException e) {                                                        throw new ConnectException(e);                        }                    }                                    }                streamOffset = (lastRecordedOffset != null) ? (Long) lastRecordedOffset : 0L;            } else {                streamOffset = 0L;            }            reader = new BufferedReader(new InputStreamReader(stream, StandardCharsets.UTF_8));                    } catch (NoSuchFileException e) {                        synchronized (this) {                this.wait(1000);            }            return null;        } catch (IOException e) {                        throw new ConnectException(e);        }    }    // is available.    try {        final BufferedReader readerCopy;        synchronized (this) {            readerCopy = reader;        }        if (readerCopy == null)            return null;        ArrayList<SourceRecord> records = null;        int nread = 0;        while (readerCopy.ready()) {            nread = readerCopy.read(buffer, offset, buffer.length - offset);            log.trace("Read {} bytes from {}", nread, logFilename());            if (nread > 0) {                offset += nread;                if (offset == buffer.length) {                    char[] newbuf = new char[buffer.length * 2];                    System.arraycopy(buffer, 0, newbuf, 0, buffer.length);                    buffer = newbuf;                }                String line;                do {                    line = extractLine();                    if (line != null) {                        log.trace("Read a line from {}", logFilename());                        if (records == null)                            records = new ArrayList<>();                        records.add(new SourceRecord(offsetKey(filename), offsetValue(streamOffset), topic, null, null, null, VALUE_SCHEMA, line, System.currentTimeMillis()));                        if (records.size() >= batchSize) {                            return records;                        }                    }                } while (line != null);            }        }        if (nread <= 0)            synchronized (this) {                this.wait(1000);            }        return records;    } catch (IOException e) {    // Underlying stream was killed, probably as a result of calling stop. Allow to return    // null, and driving thread will handle any shutdown if necessary.    }    return null;}
private String kafkatest_f9366_0()
{    int until = -1, newStart = -1;    for (int i = 0; i < offset; i++) {        if (buffer[i] == '\n') {            until = i;            newStart = i + 1;            break;        } else if (buffer[i] == '\r') {            // We need to check for \r\n, so we must skip this if we can't check the next char            if (i + 1 >= offset)                return null;            until = i;            newStart = (buffer[i + 1] == '\n') ? i + 2 : i + 1;            break;        }    }    if (until != -1) {        String result = new String(buffer, 0, until);        System.arraycopy(buffer, newStart, buffer, 0, buffer.length - newStart);        offset = offset - newStart;        if (streamOffset != null)            streamOffset += newStart;        return result;    } else {        return null;    }}
public void kafkatest_f9375_0()
{    replayAll();    connector.start(sinkProperties);    assertEquals(FileStreamSinkTask.class, connector.taskClass());    verifyAll();}
public void kafkatest_f9376_0() throws Exception
{    os = new ByteArrayOutputStream();    printStream = new PrintStream(os);    task = new FileStreamSinkTask(printStream);    File outputDir = topDir.newFolder("file-stream-sink-" + UUID.randomUUID().toString());    outputFile = outputDir.getCanonicalPath() + "/connect.output";}
public void kafkatest_f9385_0()
{    sourceProperties.remove(FileStreamSourceConnector.TOPIC_CONFIG);    connector.start(sourceProperties);}
public void kafkatest_f9386_0()
{    // Because of trimming this tests is same as testing for empty string.    sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, "     ");    connector.start(sourceProperties);}
private void kafkatest_f9395_0()
{    EasyMock.expect(context.offsetStorageReader()).andReturn(offsetStorageReader);    EasyMock.expect(offsetStorageReader.offset(EasyMock.<Map<String, String>>anyObject())).andReturn(null);}
public Object kafkatest_f9396_0(Schema schema, JsonNode value)
{    return value.booleanValue();}
public Object kafkatest_f9405_0(Schema schema, JsonNode value)
{    Schema elemSchema = schema == null ? null : schema.valueSchema();    ArrayList<Object> result = new ArrayList<>();    for (JsonNode elem : value) {        result.add(convertToConnect(elemSchema, elem));    }    return result;}
public Object kafkatest_f9406_0(Schema schema, JsonNode value)
{    Schema keySchema = schema == null ? null : schema.keySchema();    Schema valueSchema = schema == null ? null : schema.valueSchema();    // If the map uses strings for keys, it should be encoded in the natural JSON format. If it uses other    // primitive types or a complex type as a key, it will be encoded as a list of pairs. If we don't have a    // schema, we default to encoding in a Map.    Map<Object, Object> result = new HashMap<>();    if (schema == null || keySchema.type() == Schema.Type.STRING) {        if (!value.isObject())            throw new DataException("Maps with string fields should be encoded as JSON objects, but found " + value.getNodeType());        Iterator<Map.Entry<String, JsonNode>> fieldIt = value.fields();        while (fieldIt.hasNext()) {            Map.Entry<String, JsonNode> entry = fieldIt.next();            result.put(entry.getKey(), convertToConnect(valueSchema, entry.getValue()));        }    } else {        if (!value.isArray())            throw new DataException("Maps with non-string fields should be encoded as JSON array of tuples, but found " + value.getNodeType());        for (JsonNode entry : value) {            if (!entry.isArray())                throw new DataException("Found invalid map entry instead of array tuple: " + entry.getNodeType());            if (entry.size() != 2)                throw new DataException("Found invalid map entry, expected length 2 but found :" + entry.size());            result.put(convertToConnect(keySchema, entry.get(0)), convertToConnect(valueSchema, entry.get(1)));        }    }    return result;}
public Object kafkatest_f9415_0(Schema schema, Object value)
{    if (!(value instanceof java.util.Date))        throw new DataException("Invalid type for Timestamp, expected Date but was " + value.getClass());    return Timestamp.fromLogical(schema, (java.util.Date) value);}
public ConfigDef kafkatest_f9416_0()
{    return JsonConverterConfig.configDef();}
public Schema kafkatest_f9425_0(JsonNode jsonSchema)
{    if (jsonSchema.isNull())        return null;    Schema cached = toConnectSchemaCache.get(jsonSchema);    if (cached != null)        return cached;    JsonNode schemaTypeNode = jsonSchema.get(JsonSchema.SCHEMA_TYPE_FIELD_NAME);    if (schemaTypeNode == null || !schemaTypeNode.isTextual())        throw new DataException("Schema must contain 'type' field");    final SchemaBuilder builder;    switch(schemaTypeNode.textValue()) {        case JsonSchema.BOOLEAN_TYPE_NAME:            builder = SchemaBuilder.bool();            break;        case JsonSchema.INT8_TYPE_NAME:            builder = SchemaBuilder.int8();            break;        case JsonSchema.INT16_TYPE_NAME:            builder = SchemaBuilder.int16();            break;        case JsonSchema.INT32_TYPE_NAME:            builder = SchemaBuilder.int32();            break;        case JsonSchema.INT64_TYPE_NAME:            builder = SchemaBuilder.int64();            break;        case JsonSchema.FLOAT_TYPE_NAME:            builder = SchemaBuilder.float32();            break;        case JsonSchema.DOUBLE_TYPE_NAME:            builder = SchemaBuilder.float64();            break;        case JsonSchema.BYTES_TYPE_NAME:            builder = SchemaBuilder.bytes();            break;        case JsonSchema.STRING_TYPE_NAME:            builder = SchemaBuilder.string();            break;        case JsonSchema.ARRAY_TYPE_NAME:            JsonNode elemSchema = jsonSchema.get(JsonSchema.ARRAY_ITEMS_FIELD_NAME);            if (elemSchema == null || elemSchema.isNull())                throw new DataException("Array schema did not specify the element type");            builder = SchemaBuilder.array(asConnectSchema(elemSchema));            break;        case JsonSchema.MAP_TYPE_NAME:            JsonNode keySchema = jsonSchema.get(JsonSchema.MAP_KEY_FIELD_NAME);            if (keySchema == null)                throw new DataException("Map schema did not specify the key type");            JsonNode valueSchema = jsonSchema.get(JsonSchema.MAP_VALUE_FIELD_NAME);            if (valueSchema == null)                throw new DataException("Map schema did not specify the value type");            builder = SchemaBuilder.map(asConnectSchema(keySchema), asConnectSchema(valueSchema));            break;        case JsonSchema.STRUCT_TYPE_NAME:            builder = SchemaBuilder.struct();            JsonNode fields = jsonSchema.get(JsonSchema.STRUCT_FIELDS_FIELD_NAME);            if (fields == null || !fields.isArray())                throw new DataException("Struct schema's \"fields\" argument is not an array.");            for (JsonNode field : fields) {                JsonNode jsonFieldName = field.get(JsonSchema.STRUCT_FIELD_NAME_FIELD_NAME);                if (jsonFieldName == null || !jsonFieldName.isTextual())                    throw new DataException("Struct schema's field name not specified properly");                builder.field(jsonFieldName.asText(), asConnectSchema(field));            }            break;        default:            throw new DataException("Unknown schema type: " + schemaTypeNode.textValue());    }    JsonNode schemaOptionalNode = jsonSchema.get(JsonSchema.SCHEMA_OPTIONAL_FIELD_NAME);    if (schemaOptionalNode != null && schemaOptionalNode.isBoolean() && schemaOptionalNode.booleanValue())        builder.optional();    else        builder.required();    JsonNode schemaNameNode = jsonSchema.get(JsonSchema.SCHEMA_NAME_FIELD_NAME);    if (schemaNameNode != null && schemaNameNode.isTextual())        builder.name(schemaNameNode.textValue());    JsonNode schemaVersionNode = jsonSchema.get(JsonSchema.SCHEMA_VERSION_FIELD_NAME);    if (schemaVersionNode != null && schemaVersionNode.isIntegralNumber()) {        builder.version(schemaVersionNode.intValue());    }    JsonNode schemaDocNode = jsonSchema.get(JsonSchema.SCHEMA_DOC_FIELD_NAME);    if (schemaDocNode != null && schemaDocNode.isTextual())        builder.doc(schemaDocNode.textValue());    JsonNode schemaParamsNode = jsonSchema.get(JsonSchema.SCHEMA_PARAMETERS_FIELD_NAME);    if (schemaParamsNode != null && schemaParamsNode.isObject()) {        Iterator<Map.Entry<String, JsonNode>> paramsIt = schemaParamsNode.fields();        while (paramsIt.hasNext()) {            Map.Entry<String, JsonNode> entry = paramsIt.next();            JsonNode paramValue = entry.getValue();            if (!paramValue.isTextual())                throw new DataException("Schema parameters must have string values.");            builder.parameter(entry.getKey(), paramValue.textValue());        }    }    JsonNode schemaDefaultNode = jsonSchema.get(JsonSchema.SCHEMA_DEFAULT_FIELD_NAME);    if (schemaDefaultNode != null)        builder.defaultValue(convertToConnect(builder, schemaDefaultNode));    Schema result = builder.build();    toConnectSchemaCache.put(jsonSchema, result);    return result;}
private JsonNode kafkatest_f9426_0(Schema schema, Object value)
{    return new JsonSchema.Envelope(asJsonSchema(schema), convertToJson(schema, value)).toJsonNode();}
public ObjectNode kafkatest_f9435_0()
{    return envelope(schema, payload);}
public byte[] kafkatest_f9436_0(String topic, JsonNode data)
{    if (data == null)        return null;    try {        return objectMapper.writeValueAsBytes(data);    } catch (Exception e) {        throw new SerializationException("Error serializing JSON message", e);    }}
public void kafkatest_f9445_0()
{    assertEquals(new SchemaAndValue(Schema.FLOAT64_SCHEMA, 12.34), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"double\" }, \"payload\": 12.34 }".getBytes()));}
public void kafkatest_f9446_0() throws UnsupportedEncodingException
{    ByteBuffer reference = ByteBuffer.wrap("test-string".getBytes("UTF-8"));    String msg = "{ \"schema\": { \"type\": \"bytes\" }, \"payload\": \"dGVzdC1zdHJpbmc=\" }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    ByteBuffer converted = ByteBuffer.wrap((byte[]) schemaAndValue.value());    assertEquals(reference, converted);}
public void kafkatest_f9455_0()
{    Schema schema = Decimal.builder(2).optional().schema();    String msg = "{ \"schema\": { \"type\": \"bytes\", \"name\": \"org.apache.kafka.connect.data.Decimal\", \"version\": 1, \"optional\": true, \"parameters\": { \"scale\": \"2\" } }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertNull(schemaAndValue.value());}
public void kafkatest_f9456_0()
{    BigDecimal reference = new BigDecimal(new BigInteger("156"), 2);    Schema schema = Decimal.builder(2).defaultValue(reference).build();    String msg = "{ \"schema\": { \"type\": \"bytes\", \"name\": \"org.apache.kafka.connect.data.Decimal\", \"version\": 1, \"default\": \"AJw=\", \"parameters\": { \"scale\": \"2\" } }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertEquals(reference, schemaAndValue.value());}
public void kafkatest_f9465_0()
{    java.util.Date reference = new java.util.Date(0);    Schema schema = Time.builder().optional().defaultValue(reference).schema();    String msg = "{ \"schema\": { \"type\": \"int32\", \"name\": \"org.apache.kafka.connect.data.Time\", \"version\": 1, \"optional\": true, \"default\": 0 }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertEquals(reference, schemaAndValue.value());}
public void kafkatest_f9466_0()
{    Schema schema = Timestamp.SCHEMA;    GregorianCalendar calendar = new GregorianCalendar(1970, Calendar.JANUARY, 1, 0, 0, 0);    calendar.setTimeZone(TimeZone.getTimeZone("UTC"));    calendar.add(Calendar.MILLISECOND, 2000000000);    calendar.add(Calendar.MILLISECOND, 2000000000);    java.util.Date reference = calendar.getTime();    String msg = "{ \"schema\": { \"type\": \"int64\", \"name\": \"org.apache.kafka.connect.data.Timestamp\", \"version\": 1 }, \"payload\": 4000000000 }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    java.util.Date converted = (java.util.Date) schemaAndValue.value();    assertEquals(schema, schemaAndValue.schema());    assertEquals(reference, converted);}
public void kafkatest_f9475_0()
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Schema.INT32_SCHEMA, 12));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"int32\", \"optional\": false }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertEquals(12, converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).intValue());}
public void kafkatest_f9476_0()
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Schema.INT64_SCHEMA, 4398046511104L));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"int64\", \"optional\": false }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertEquals(4398046511104L, converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).longValue());}
public void kafkatest_f9485_0()
{    Schema schema = SchemaBuilder.struct().field("field1", Schema.BOOLEAN_SCHEMA).field("field2", Schema.STRING_SCHEMA).field("field3", Schema.STRING_SCHEMA).field("field4", Schema.BOOLEAN_SCHEMA).build();    Schema inputSchema = SchemaBuilder.struct().field("field1", Schema.BOOLEAN_SCHEMA).field("field2", Schema.STRING_SCHEMA).field("field3", Schema.STRING_SCHEMA).field("field4", Schema.BOOLEAN_SCHEMA).build();    Struct input = new Struct(inputSchema).put("field1", true).put("field2", "string2").put("field3", "string3").put("field4", false);    assertStructSchemaEqual(schema, input);}
public void kafkatest_f9486_0() throws IOException
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Decimal.schema(2), new BigDecimal(new BigInteger("156"), 2)));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"bytes\", \"optional\": false, \"name\": \"org.apache.kafka.connect.data.Decimal\", \"version\": 1, \"parameters\": { \"scale\": \"2\" } }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertArrayEquals(new byte[] { 0, -100 }, converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).binaryValue());}
public void kafkatest_f9495_0()
{    // This characterizes the production of tombstone messages when Json schemas is not enabled    Map<String, Boolean> props = Collections.singletonMap("schemas.enable", false);    converter.configure(props, true);    byte[] converted = converter.fromConnectData(TOPIC, null, null);    assertNull(converted);}
public void kafkatest_f9496_0()
{    // If we have mismatching schema info, we should properly convert to a DataException    converter.fromConnectData(TOPIC, Schema.FLOAT64_SCHEMA, true);}
private void kafkatest_f9505_0(JsonNode env)
{    assertNotNull(env);    assertTrue(env.isObject());    assertEquals(2, env.size());    assertTrue(env.has(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertTrue(env.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME).isObject());    assertTrue(env.has(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME));}
private void kafkatest_f9506_0(JsonNode env)
{    assertNotNull(env);    assertTrue(env.isObject());    assertEquals(2, env.size());    assertTrue(env.has(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertTrue(env.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME).isNull());    assertTrue(env.has(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME));}
protected String kafkatest_f9516_0()
{    return "All";}
protected boolean kafkatest_f9517_0(ConfigValue configValue)
{    return true;}
public byte[] kafkatest_f9528_0(String topic, Schema schema, Object value)
{    if (schema != null && schema.type() != Schema.Type.BYTES)        throw new DataException("Invalid schema type for ByteArrayConverter: " + schema.type().toString());    if (value != null && !(value instanceof byte[]))        throw new DataException("ByteArrayConverter is not compatible with objects of type " + value.getClass());    return (byte[]) value;}
public SchemaAndValue kafkatest_f9529_0(String topic, byte[] value)
{    return new SchemaAndValue(Schema.OPTIONAL_BYTES_SCHEMA, value);}
public SchemaAndValue kafkatest_f9538_0(String topic, byte[] value)
{    try {        return new SchemaAndValue(schema, deserializer.deserialize(topic, value));    } catch (SerializationException e) {        throw new DataException("Failed to deserialize " + typeName + ": ", e);    }}
public byte[] kafkatest_f9539_0(String topic, String headerKey, Schema schema, Object value)
{    return fromConnectData(topic, schema, value);}
public void kafkatest_f9549_0(String connector)
{    statusBackingStore.putSafe(new ConnectorStatus(connector, ConnectorStatus.State.UNASSIGNED, workerId, generation()));}
public void kafkatest_f9550_0(String connector, Throwable cause)
{    statusBackingStore.putSafe(new ConnectorStatus(connector, ConnectorStatus.State.FAILED, trace(cause), workerId, generation()));}
public Plugins kafkatest_f9559_0()
{    return worker.getPlugins();}
public Collection<String> kafkatest_f9560_0()
{    return configBackingStore.snapshot().connectors();}
private static ConfigKeyInfo kafkatest_f9569_0(ConfigKey configKey)
{    return convertConfigKey(configKey, "");}
private static ConfigKeyInfo kafkatest_f9570_0(ConfigKey configKey, String prefix)
{    String name = prefix + configKey.name;    Type type = configKey.type;    String typeName = configKey.type.name();    boolean required = false;    String defaultValue;    if (ConfigDef.NO_DEFAULT_VALUE.equals(configKey.defaultValue)) {        defaultValue = null;        required = true;    } else {        defaultValue = ConfigDef.convertToString(configKey.defaultValue, type);    }    String importance = configKey.importance.name();    String documentation = configKey.documentation;    String group = configKey.group;    int orderInGroup = configKey.orderInGroup;    String width = configKey.width.name();    String displayName = configKey.displayName;    List<String> dependents = configKey.dependents;    return new ConfigKeyInfo(name, typeName, required, defaultValue, importance, documentation, group, orderInGroup, width, displayName, dependents);}
public State kafkatest_f9579_0()
{    return state;}
public String kafkatest_f9580_0()
{    return trace;}
public URI kafkatest_f9589_0()
{    return rest.serverUrl();}
public voidf9590_1)
{    try {        startLatch.await();        Connect.this.stop();    } catch (InterruptedException e) {            }}
public Map<String, String> kafkatest_f9599_0()
{    return tags;}
public boolean kafkatest_f9600_0(MetricName metricName)
{    return metricName != null && groupName.equals(metricName.group()) && tags.equals(metricName.tags());}
public void kafkatest_f9609_0(MetricNameTemplate nameTemplate, final LiteralSupplier<T> supplier)
{    MetricName metricName = metricName(nameTemplate);    if (metrics().metric(metricName) == null) {        metrics().addMetric(metricName, new Gauge<T>() {            @Override            public T value(MetricConfig config, long now) {                return supplier.metricValue(now);            }        });    }}
public T kafkatest_f9610_0(MetricConfig config, long now)
{    return supplier.metricValue(now);}
 static Map<String, String> kafkatest_f9619_0(String... keyValue)
{    if ((keyValue.length % 2) != 0)        throw new IllegalArgumentException("keyValue needs to be specified in pairs");    Map<String, String> tags = new LinkedHashMap<>();    for (int i = 0; i < keyValue.length; i += 2) {        tags.put(keyValue[i], keyValue[i + 1]);    }    return tags;}
public static void kafkatest_f9620_0(String[] args)
{    ConnectMetricsRegistry metrics = new ConnectMetricsRegistry();    System.out.println(Metrics.toHtmlTable(JMX_PREFIX, metrics.getAllTemplates()));}
public String kafkatest_f9629_0()
{    return WORKER_GROUP_NAME;}
public String kafkatest_f9630_0()
{    return WORKER_REBALANCE_GROUP_NAME;}
public ToleranceType kafkatest_f9639_0()
{    String tolerance = getString(ERRORS_TOLERANCE_CONFIG);    for (ToleranceType type : ToleranceType.values()) {        if (type.name().equalsIgnoreCase(tolerance)) {            return type;        }    }    return ERRORS_TOLERANCE_DEFAULT;}
public boolean kafkatest_f9640_0()
{    return getBoolean(ERRORS_LOG_ENABLE_CONFIG);}
public boolean kafkatest_f9649_0(String connector)
{    return connectorConfigs.containsKey(connector);}
public Set<String> kafkatest_f9650_0()
{    return connectorConfigs.keySet();}
public Set<String> kafkatest_f9659_0()
{    return inconsistentConnectors;}
public String kafkatest_f9660_0()
{    return "ClusterConfigState{" + "offset=" + offset + ", connectorTaskCounts=" + connectorTaskCounts + ", connectorConfigs=" + connectorConfigs + ", taskConfigs=" + taskConfigs + ", inconsistentConnectors=" + inconsistentConnectors + '}';}
public long kafkatest_f9669_0()
{    return offset;}
public String kafkatest_f9670_0()
{    return "WorkerState{" + "url='" + url + '\'' + ", offset=" + offset + '}';}
protected Map<String, Collection<Integer>> kafkatest_f9679_0()
{    // Using LinkedHashMap preserves the ordering, which is helpful for tests and debugging    Map<String, Collection<Integer>> taskMap = new LinkedHashMap<>();    for (String connectorId : new HashSet<>(connectorIds)) {        Collection<Integer> connectorTasks = taskMap.get(connectorId);        if (connectorTasks == null) {            connectorTasks = new ArrayList<>();            taskMap.put(connectorId, connectorTasks);        }        connectorTasks.add(CONNECTOR_TASK);    }    for (ConnectorTaskId taskId : taskIds) {        String connectorId = taskId.connector();        Collection<Integer> connectorTasks = taskMap.get(connectorId);        if (connectorTasks == null) {            connectorTasks = new ArrayList<>();            taskMap.put(connectorId, connectorTasks);        }        connectorTasks.add(taskId.task());    }    return taskMap;}
private static void kafkatest_f9680_0(short version)
{    // check for invalid versions    if (version < CONNECT_PROTOCOL_V0)        throw new SchemaException("Unsupported subscription version: " + version);// otherwise, assume versions can be parsed as V0}
public void kafkatest_f9689_0()
{    this.herderExecutor.submit(this);}
public voidf9690_1)
{    try {                startServices();                while (!stopping.get()) {            tick();        }        halt();                herderMetrics.close();    } catch (Throwable t) {                Exit.exit(1);    }}
public void kafkatest_f9699_0(final Callback<Collection<String>> callback)
{    log.trace("Submitting connector listing request");    addRequest(new Callable<Void>() {        @Override        public Void call() throws Exception {            if (checkRebalanceNeeded(callback))                return null;            callback.onCompletion(null, configState.connectors());            return null;        }    }, forwardErrorCallback(callback));}
public Void kafkatest_f9700_0() throws Exception
{    if (checkRebalanceNeeded(callback))        return null;    callback.onCompletion(null, configState.connectors());    return null;}
public void kafkatest_f9709_0(final String connName, final Map<String, String> config, final boolean allowReplace, final Callback<Created<ConnectorInfo>> callback)
{    log.trace("Submitting connector config write request {}", connName);    addRequest(new Callable<Void>() {        @Override        public Void call() throws Exception {            if (maybeAddConfigErrors(validateConnectorConfig(config), callback)) {                return null;            }            log.trace("Handling connector config request {}", connName);            if (!isLeader()) {                callback.onCompletion(new NotLeaderException("Only the leader can set connector configs.", leaderUrl()), null);                return null;            }            boolean exists = configState.contains(connName);            if (!allowReplace && exists) {                callback.onCompletion(new AlreadyExistsException("Connector " + connName + " already exists"), null);                return null;            }            log.trace("Submitting connector config {} {} {}", connName, allowReplace, configState.connectors());            configBackingStore.putConnectorConfig(connName, config);            // Note that we use the updated connector config despite the fact that we don't have an updated            // snapshot yet. The existing task info should still be accurate.            ConnectorInfo info = new ConnectorInfo(connName, config, configState.tasks(connName), // validateConnectorConfig have checked the existence of CONNECTOR_CLASS_CONFIG            connectorTypeForClass(config.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG)));            callback.onCompletion(null, new Created<>(!exists, info));            return null;        }    }, forwardErrorCallback(callback));}
public Void kafkatest_f9710_0() throws Exception
{    if (maybeAddConfigErrors(validateConnectorConfig(config), callback)) {        return null;    }    log.trace("Handling connector config request {}", connName);    if (!isLeader()) {        callback.onCompletion(new NotLeaderException("Only the leader can set connector configs.", leaderUrl()), null);        return null;    }    boolean exists = configState.contains(connName);    if (!allowReplace && exists) {        callback.onCompletion(new AlreadyExistsException("Connector " + connName + " already exists"), null);        return null;    }    log.trace("Submitting connector config {} {} {}", connName, allowReplace, configState.connectors());    configBackingStore.putConnectorConfig(connName, config);    // Note that we use the updated connector config despite the fact that we don't have an updated    // snapshot yet. The existing task info should still be accurate.    ConnectorInfo info = new ConnectorInfo(connName, config, configState.tasks(connName), // validateConnectorConfig have checked the existence of CONNECTOR_CLASS_CONFIG    connectorTypeForClass(config.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG)));    callback.onCompletion(null, new Created<>(!exists, info));    return null;}
public HerderRequest kafkatest_f9719_0(final long delayMs, final String connName, final Callback<Void> callback)
{    return addRequest(delayMs, new Callable<Void>() {        @Override        public Void call() throws Exception {            if (checkRebalanceNeeded(callback))                return null;            if (!configState.connectors().contains(connName)) {                callback.onCompletion(new NotFoundException("Unknown connector: " + connName), null);                return null;            }            if (assignment.connectors().contains(connName)) {                try {                    worker.stopConnector(connName);                    if (startConnector(connName))                        callback.onCompletion(null, null);                    else                        callback.onCompletion(new ConnectException("Failed to start connector: " + connName), null);                } catch (Throwable t) {                    callback.onCompletion(t, null);                }            } else if (isLeader()) {                callback.onCompletion(new NotAssignedException("Cannot restart connector since it is not assigned to this member", member.ownerUrl(connName)), null);            } else {                callback.onCompletion(new NotLeaderException("Cannot restart connector since it is not assigned to this member", leaderUrl()), null);            }            return null;        }    }, forwardErrorCallback(callback));}
public Void kafkatest_f9720_0() throws Exception
{    if (checkRebalanceNeeded(callback))        return null;    if (!configState.connectors().contains(connName)) {        callback.onCompletion(new NotFoundException("Unknown connector: " + connName), null);        return null;    }    if (assignment.connectors().contains(connName)) {        try {            worker.stopConnector(connName);            if (startConnector(connName))                callback.onCompletion(null, null);            else                callback.onCompletion(new ConnectException("Failed to start connector: " + connName), null);        } catch (Throwable t) {            callback.onCompletion(t, null);        }    } else if (isLeader()) {        callback.onCompletion(new NotAssignedException("Cannot restart connector since it is not assigned to this member", member.ownerUrl(connName)), null);    } else {        callback.onCompletion(new NotLeaderException("Cannot restart connector since it is not assigned to this member", leaderUrl()), null);    }    return null;}
private void kafkatest_f9729_0(Collection<Callable<Void>> callables)
{    try {        startAndStopExecutor.invokeAll(callables);    } catch (InterruptedException e) {    // ignore    }}
private voidf9730_1)
{    // Start assigned connectors and tasks        List<Callable<Void>> callables = new ArrayList<>();    for (String connectorName : assignmentDifference(assignment.connectors(), runningAssignment.connectors())) {        callables.add(getConnectorStartingCallable(connectorName));    }    // These tasks have been stopped by this worker due to task reconfiguration. In order to    // restart them, they are removed just before the overall task startup from the set of    // currently running tasks. Therefore, they'll be restarted only if they are included in    // the assignment that was just received after rebalancing.    runningAssignment.tasks().removeAll(tasksToRestart);    tasksToRestart.clear();    for (ConnectorTaskId taskId : assignmentDifference(assignment.tasks(), runningAssignment.tasks())) {        callables.add(getTaskStartingCallable(taskId));    }    startAndStop(callables);    runningAssignment = member.currentProtocolVersion() == CONNECT_PROTOCOL_V0 ? ExtendedAssignment.empty() : assignment;    }
public Voidf9739_1) throws Exception
{    try {        startConnector(connectorName);    } catch (Throwable t) {                onFailure(connectorName, t);    }    return null;}
private Callable<Void>f9740_1final String connectorName)
{    return new Callable<Void>() {        @Override        public Void call() throws Exception {            try {                worker.stopConnector(connectorName);            } catch (Throwable t) {                            }            return null;        }    };}
 DistributedHerderRequest kafkatest_f9749_0(Callable<Void> action, Callback<Void> callback)
{    return addRequest(0, action, callback);}
 DistributedHerderRequest kafkatest_f9750_0(long delayMs, Callable<Void> action, Callback<Void> callback)
{    DistributedHerderRequest req = new DistributedHerderRequest(time.milliseconds() + delayMs, requestSeqNum.incrementAndGet(), action, callback);    requests.add(req);    if (peekWithoutException() == req)        member.wakeup();    return req;}
public int kafkatest_f9759_0(DistributedHerderRequest o)
{    final int cmp = Long.compare(at, o.at);    return cmp == 0 ? Long.compare(seq, o.seq) : cmp;}
public boolean kafkatest_f9760_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof DistributedHerderRequest))        return false;    DistributedHerderRequest other = (DistributedHerderRequest) o;    return compareTo(other) == 0;}
public Double kafkatest_f9769_0(long now)
{    return (double) generation;}
public Double kafkatest_f9770_0(long now)
{    return rebalancing ? 1.0d : 0.0d;}
private Map<String, ByteBuffer>f9779_1Collection<String> members, short error, String leaderId, String leaderUrl, long maxOffset, Map<String, Collection<String>> connectorAssignments, Map<String, Collection<ConnectorTaskId>> taskAssignments)
{    Map<String, ByteBuffer> groupAssignment = new HashMap<>();    for (String member : members) {        Collection<String> connectors = connectorAssignments.get(member);        if (connectors == null) {            connectors = Collections.emptyList();        }        Collection<ConnectorTaskId> tasks = taskAssignments.get(member);        if (tasks == null) {            tasks = Collections.emptyList();        }        Assignment assignment = new Assignment(error, leaderId, leaderUrl, maxOffset, connectors, tasks);                groupAssignment.put(member, ConnectProtocol.serializeAssignment(assignment));    }        return groupAssignment;}
private longf9780_1Map<String, ExtendedWorkerState> memberConfigs, WorkerCoordinator coordinator)
{    // The new config offset is the maximum seen by any member. We always perform assignment using this offset,    // even if some members have fallen behind. The config offset used to generate the assignment is included in    // the response so members that have fallen behind will not use the assignment until they have caught up.    Long maxOffset = null;    for (Map.Entry<String, ExtendedWorkerState> stateEntry : memberConfigs.entrySet()) {        long memberRootOffset = stateEntry.getValue().offset();        if (maxOffset == null)            maxOffset = memberRootOffset;        else            maxOffset = Math.max(maxOffset, memberRootOffset);    }        return maxOffset;}
public Struct kafkatest_f9789_0()
{    Collection<Struct> assigned = taskAssignments(asMap());    Collection<Struct> revoked = taskAssignments(revokedAsMap());    return new Struct(ASSIGNMENT_V1).set(ERROR_KEY_NAME, error()).set(LEADER_KEY_NAME, leader()).set(LEADER_URL_KEY_NAME, leaderUrl()).set(CONFIG_OFFSET_KEY_NAME, offset()).set(ASSIGNMENT_KEY_NAME, assigned != null ? assigned.toArray() : null).set(REVOKED_KEY_NAME, revoked != null ? revoked.toArray() : null).set(SCHEDULED_DELAY_KEY_NAME, delay);}
public static ExtendedAssignment kafkatest_f9790_0(short version, Struct struct)
{    return struct == null ? null : new ExtendedAssignment(version, struct.getShort(ERROR_KEY_NAME), struct.getString(LEADER_KEY_NAME), struct.getString(LEADER_URL_KEY_NAME), struct.getLong(CONFIG_OFFSET_KEY_NAME), extractConnectors(struct, ASSIGNMENT_KEY_NAME), extractTasks(struct, ASSIGNMENT_KEY_NAME), extractConnectors(struct, REVOKED_KEY_NAME), extractTasks(struct, REVOKED_KEY_NAME), struct.getInt(SCHEDULED_DELAY_KEY_NAME));}
private Map<String, ConnectorsAndTasks>f9799_1ConnectorsAndTasks deleted, Map<String, Collection<String>> connectorAssignments, Map<String, Collection<ConnectorTaskId>> taskAssignments)
{    // Connector to worker reverse lookup map    Map<String, String> connectorOwners = WorkerCoordinator.invertAssignment(connectorAssignments);    // Task to worker reverse lookup map    Map<ConnectorTaskId, String> taskOwners = WorkerCoordinator.invertAssignment(taskAssignments);    Map<String, ConnectorsAndTasks> toRevoke = new HashMap<>();    // Add the connectors that have been deleted to the revoked set    deleted.connectors().forEach(c -> toRevoke.computeIfAbsent(connectorOwners.get(c), v -> new ConnectorsAndTasks.Builder().build()).connectors().add(c));    // Add the tasks that have been deleted to the revoked set    deleted.tasks().forEach(t -> toRevoke.computeIfAbsent(taskOwners.get(t), v -> new ConnectorsAndTasks.Builder().build()).tasks().add(t));        return toRevoke;}
private ConnectorsAndTasks kafkatest_f9800_0(Map<String, ConnectorsAndTasks> toRevoke, Map<String, Collection<String>> connectorAssignments, Map<String, Collection<ConnectorTaskId>> taskAssignments, ConnectorsAndTasks lostAssignments)
{    ConnectorsAndTasks previousAssignment = new ConnectorsAndTasks.Builder().with(connectorAssignments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet()), taskAssignments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet())).build();    for (ConnectorsAndTasks revoked : toRevoke.values()) {        previousAssignment.connectors().removeAll(revoked.connectors());        previousAssignment.tasks().removeAll(revoked.tasks());        previousRevocation.connectors().addAll(revoked.connectors());        previousRevocation.tasks().addAll(revoked.tasks());    }    // Depends on the previous assignment's collections being sets at the moment.    // TODO: make it independent    previousAssignment.connectors().addAll(lostAssignments.connectors());    previousAssignment.tasks().addAll(lostAssignments.tasks());    return previousAssignment;}
private ConnectorsAndTasksf9809_1Map<String, ExtendedWorkerState> memberConfigs)
{        Set<String> connectors = memberConfigs.values().stream().flatMap(state -> state.assignment().connectors().stream()).collect(Collectors.toSet());    Set<ConnectorTaskId> tasks = memberConfigs.values().stream().flatMap(state -> state.assignment().tasks().stream()).collect(Collectors.toSet());    return new ConnectorsAndTasks.Builder().with(connectors, tasks).build();}
private int kafkatest_f9810_0(long now)
{    long diff = scheduledRebalance - now;    return diff > 0 ? (int) Math.min(diff, maxDelay) : 0;}
private static void kafkatest_f9819_0(short version)
{    // check for invalid versions    if (version < CONNECT_PROTOCOL_V0)        throw new SchemaException("Unsupported subscription version: " + version);// otherwise, assume versions can be parsed}
public String kafkatest_f9820_0()
{    return forwardUrl;}
protected boolean kafkatest_f9829_0()
{    return super.rejoinNeededOrPending() || (assignmentSnapshot == null || assignmentSnapshot.failed()) || rejoinRequested;}
public String kafkatest_f9830_0()
{    Generation generation = generation();    if (generation != null)        return generation.memberId;    return JoinGroupRequest.UNKNOWN_MEMBER_ID;}
public short kafkatest_f9839_0()
{    return currentConnectProtocol == EAGER ? (short) 0 : (short) 1;}
public double kafkatest_f9840_0(MetricConfig config, long now)
{    return assignmentSnapshot.connectors().size();}
public Collection<ConnectorTaskId> kafkatest_f9849_0()
{    return tasks;}
public int kafkatest_f9850_0()
{    return connectors.size() + tasks.size();}
public int kafkatest_f9859_0()
{    return connectors.size();}
public int kafkatest_f9860_0()
{    return tasks.size();}
public int kafkatest_f9869_0()
{    return Objects.hash(worker, connectors, tasks);}
public void kafkatest_f9870_0()
{    if (stopped)        return;    stop(false);}
public short kafkatest_f9879_0()
{    return coordinator.currentProtocolVersion();}
private voidf9880_1boolean swallowException)
{    log.trace("Stopping the Connect group member.");    AtomicReference<Throwable> firstException = new AtomicReference<>();    this.stopped = true;    Utils.closeQuietly(coordinator, "coordinator", firstException);    Utils.closeQuietly(metrics, "consumer metrics", firstException);    Utils.closeQuietly(client, "consumer network client", firstException);    AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics);    if (firstException.get() != null && !swallowException)        throw new KafkaException("Failed to stop the Connect group member", firstException.get());    else        }
public void kafkatest_f9889_0()
{    recordProcessingErrors.record();}
public void kafkatest_f9890_0()
{    recordsSkipped.record();}
private void kafkatest_f9899_0()
{    attempt = 0;    position = null;    klass = null;    error = null;}
public void kafkatest_f9900_0(ConsumerRecord<byte[], byte[]> consumedMessage)
{    this.consumedMessage = consumedMessage;    reset();}
public void kafkatest_f9909_0()
{    for (ErrorReporter reporter : reporters) {        reporter.report(this);    }}
public String kafkatest_f9910_0()
{    return toString(false);}
protected V kafkatest_f9919_0(Operation<V> operation) throws Exception
{    int attempt = 0;    long startTime = time.milliseconds();    long deadline = startTime + errorRetryTimeout;    do {        try {            attempt++;            return operation.call();        } catch (RetriableException e) {            log.trace("Caught a retriable exception while executing {} operation with {}", context.stage(), context.executingClass());            errorHandlingMetrics.recordFailure();            if (checkRetry(startTime)) {                backoff(attempt, deadline);                if (Thread.currentThread().isInterrupted()) {                    log.trace("Thread was interrupted. Marking operation as failed.");                    context.error(e);                    return null;                }                errorHandlingMetrics.recordRetry();            } else {                log.trace("Can't retry. start={}, attempt={}, deadline={}", startTime, attempt, deadline);                context.error(e);                return null;            }        } finally {            context.attempt(attempt);        }    } while (true);}
protected V kafkatest_f9920_0(Operation<V> operation, Class<? extends Exception> tolerated)
{    try {        V result = execAndRetry(operation);        if (context.failed()) {            markAsFailed();            errorHandlingMetrics.recordSkipped();        }        return result;    } catch (Exception e) {        errorHandlingMetrics.recordFailure();        markAsFailed();        context.error(e);        if (!tolerated.isAssignableFrom(e.getClass())) {            throw new ConnectException("Unhandled exception in error handler", e);        }        if (!withinToleranceLimits()) {            throw new ConnectException("Tolerance exceeded in error handler", e);        }        errorHandlingMetrics.recordSkipped();        return null;    }}
public void kafkatest_f9929_0(ConsumerRecord<byte[], byte[]> consumedMessage)
{    this.context.consumerRecord(consumedMessage);}
public boolean kafkatest_f9930_0()
{    return this.context.failed();}
public T kafkatest_f9939_0()
{    return result;}
public boolean kafkatest_f9940_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Created<?> created1 = (Created<?>) o;    return Objects.equals(created, created1.created) && Objects.equals(result, created1.result);}
public Set<PluginDesc<ConnectRestExtension>> kafkatest_f9949_0()
{    return restExtensions;}
public Set<PluginDesc<ConnectorClientConfigOverridePolicy>> kafkatest_f9950_0()
{    return connectorClientConfigPolicies;}
private voidf9959_1final ClassLoader loader)
{    // Apply here what java.sql.DriverManager does to discover and register classes    // implementing the java.sql.Driver interface.    AccessController.doPrivileged(new PrivilegedAction<Void>() {        @Override        public Void run() {            ServiceLoader<Driver> loadedDrivers = ServiceLoader.load(Driver.class, loader);            Iterator<Driver> driversIterator = loadedDrivers.iterator();            try {                while (driversIterator.hasNext()) {                    Driver driver = driversIterator.next();                                    }            } catch (Throwable t) {                            }            return null;        }    });}
public Voidf9960_1)
{    ServiceLoader<Driver> loadedDrivers = ServiceLoader.load(Driver.class, loader);    Iterator<Driver> driversIterator = loadedDrivers.iterator();    try {        while (driversIterator.hasNext()) {            Driver driver = driversIterator.next();                    }    } catch (Throwable t) {            }    return null;}
protected voidf9969_1URL url)
{    try {        super.scan(url);    } catch (ReflectionsException e) {        Logger log = Reflections.log;        if (log != null && log.isWarnEnabled()) {                    }    }}
public URL kafkatest_f9970_0(String name)
{    if (serviceLoaderManifestForPlugin(name)) {        // This will enable thePluginClassLoader to limit its resource search only to its own URL paths.        return null;    } else {        return super.getResource(name);    }}
public String kafkatest_f9979_0()
{    return version;}
public PluginType kafkatest_f9980_0()
{    return type;}
protected static Class<? extends U> kafkatest_f9989_0(DelegatingClassLoader loader, String classOrAlias, Class<U> pluginClass) throws ClassNotFoundException
{    Class<?> klass = loader.loadClass(classOrAlias, false);    if (pluginClass.isAssignableFrom(klass)) {        return (Class<? extends U>) klass;    }    throw new ClassNotFoundException("Requested class: " + classOrAlias + " does not extend " + pluginClass.getSimpleName());}
protected static boolean kafkatest_f9990_0(String classPropertyName)
{    return classPropertyName.equals(WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG) || classPropertyName.equals(WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG);}
public Set<PluginDesc<ConfigProvider>> kafkatest_f9999_0()
{    return delegatingLoader.configProviders();}
public Connector kafkatest_f10000_0(String connectorClassOrAlias)
{    Class<? extends Connector> klass = connectorClass(connectorClassOrAlias);    return newPlugin(klass);}
public Transformation<R> kafkatest_f10009_0(String transformationClassOrAlias)
{    return null;}
public Collection<PluginDesc<Connector>> kafkatest_f10010_0()
{    return connectors;}
public String kafkatest_f10019_0()
{    return klass.getSimpleName();}
public String kafkatest_f10020_0()
{    return super.toString().toLowerCase(Locale.ROOT);}
public static String kafkatest_f10029_0(PluginDesc<?> plugin)
{    // It's currently simpler to switch on type than do pattern matching.    switch(plugin.type()) {        case SOURCE:        case SINK:        case CONNECTOR:            return prunePluginName(plugin, "Connector");        default:            return prunePluginName(plugin, plugin.type().simpleName());    }}
public static boolean kafkatest_f10030_0(PluginDesc<U> alias, Collection<PluginDesc<U>> plugins)
{    boolean matched = false;    for (PluginDesc<U> plugin : plugins) {        if (simpleName(alias).equals(simpleName(plugin)) || prunedName(alias).equals(prunedName(plugin))) {            if (matched) {                return false;            }            matched = true;        }    }    return true;}
public ResourceConfig kafkatest_f10039_0(Class<?> componentClass, Class<?>... contracts)
{    if (allowedToRegister(componentClass)) {        resourceConfig.register(componentClass, contracts);    }    return resourceConfig;}
public ResourceConfig kafkatest_f10040_0(Class<?> componentClass, int priority)
{    if (allowedToRegister(componentClass)) {        resourceConfig.register(componentClass, priority);    }    return resourceConfig;}
public int kafkatest_f10049_0()
{    return Objects.hash(configKey, configValue);}
public String kafkatest_f10050_0()
{    return "[" + configKey.toString() + "," + configValue.toString() + "]";}
public String kafkatest_f10059_0()
{    return type;}
public boolean kafkatest_f10060_0()
{    return required;}
public boolean kafkatest_f10069_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ConfigKeyInfo that = (ConfigKeyInfo) o;    return Objects.equals(name, that.name) && Objects.equals(type, that.type) && Objects.equals(required, that.required) && Objects.equals(defaultValue, that.defaultValue) && Objects.equals(importance, that.importance) && Objects.equals(documentation, that.documentation) && Objects.equals(group, that.group) && Objects.equals(orderInGroup, that.orderInGroup) && Objects.equals(width, that.width) && Objects.equals(displayName, that.displayName) && Objects.equals(dependents, that.dependents);}
public int kafkatest_f10070_0()
{    return Objects.hash(name, type, required, defaultValue, importance, documentation, group, orderInGroup, width, displayName, dependents);}
public String kafkatest_f10079_0()
{    StringBuffer sb = new StringBuffer();    sb.append("[").append(name).append(",").append(value).append(",").append(recommendedValues).append(",").append(errors).append(",").append(visible).append("]");    return sb.toString();}
public String kafkatest_f10080_0()
{    return name;}
public boolean kafkatest_f10089_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    ConnectorPluginInfo that = (ConnectorPluginInfo) o;    return Objects.equals(className, that.className) && type == that.type && Objects.equals(version, that.version);}
public int kafkatest_f10090_0()
{    return Objects.hash(className, type, version);}
public int kafkatest_f10099_0()
{    return id;}
public int kafkatest_f10100_0(TaskState that)
{    return Integer.compare(this.id, that.id);}
public int kafkatest_f10109_0()
{    return Objects.hash(name, config);}
public int kafkatest_f10110_0()
{    return errorCode;}
public boolean kafkatest_f10119_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    TaskInfo taskInfo = (TaskInfo) o;    return Objects.equals(id, taskInfo.id) && Objects.equals(config, taskInfo.config);}
public int kafkatest_f10120_0()
{    return Objects.hash(id, config);}
public Response kafkatest_f10129_0(@QueryParam("forward") final Boolean forward, @Context final HttpHeaders headers, final CreateConnectorRequest createRequest) throws Throwable
{    // Trim leading and trailing whitespaces from the connector name, replace null with empty string    // if no name element present to keep validation within validator (NonEmptyStringWithoutControlChars    // allows null values)    String name = createRequest.name() == null ? "" : createRequest.name().trim();    Map<String, String> configs = createRequest.config();    checkAndPutConnectorConfigName(name, configs);    FutureCallback<Herder.Created<ConnectorInfo>> cb = new FutureCallback<>();    herder.putConnectorConfig(name, configs, false, cb);    Herder.Created<ConnectorInfo> info = completeOrForwardRequest(cb, "/connectors", "POST", headers, createRequest, new TypeReference<ConnectorInfo>() {    }, new CreatedConnectorInfoTranslator(), forward);    URI location = UriBuilder.fromUri("/connectors").path(name).build();    return Response.created(location).entity(info.result()).build();}
public ConnectorInfo kafkatest_f10130_0(@PathParam("connector") final String connector, @Context final HttpHeaders headers, kafkatest_f10130_0("forward") final Boolean forward) throws Throwable
{    FutureCallback<ConnectorInfo> cb = new FutureCallback<>();    herder.connectorInfo(connector, cb);    return completeOrForwardRequest(cb, "/connectors/" + connector, "GET", headers, null, forward);}
public ConnectorStateInfo.TaskState kafkatest_f10139_0(@PathParam("connector") final String connector, @Context final HttpHeaders headers, kafkatest_f10139_0("task") final Integer task) throws Throwable
{    return herder.taskStatus(new ConnectorTaskId(connector, task));}
public void kafkatest_f10140_0(@PathParam("connector") final String connector, kafkatest_f10140_0("task") final Integer task, @Context final HttpHeaders headers, kafkatest_f10140_0("forward") final Boolean forward) throws Throwable
{    FutureCallback<Void> cb = new FutureCallback<>();    ConnectorTaskId taskId = new ConnectorTaskId(connector, task);    herder.restartTask(taskId, cb);    completeOrForwardRequest(cb, "/connectors/" + connector + "/tasks/" + task + "/restart", "POST", headers, null, forward);}
public static HttpResponse<T>f10149_1String url, String method, HttpHeaders headers, Object requestBodyData, TypeReference<T> responseFormat, WorkerConfig config)
{    HttpClient client;    if (url.startsWith("https://")) {        client = new HttpClient(SSLUtils.createClientSideSslContextFactory(config));    } else {        client = new HttpClient();    }    client.setFollowRedirects(false);    try {        client.start();    } catch (Exception e) {                throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR, "Failed to start RestClient: " + e.getMessage(), e);    }    try {        String serializedBody = requestBodyData == null ? null : JSON_SERDE.writeValueAsString(requestBodyData);        log.trace("Sending {} with input {} to {}", method, serializedBody, url);        Request req = client.newRequest(url);        req.method(method);        req.accept("application/json");        req.agent("kafka-connect");        addHeadersToRequest(headers, req);        if (serializedBody != null) {            req.content(new StringContentProvider(serializedBody, StandardCharsets.UTF_8), "application/json");        }        ContentResponse res = req.send();        int responseCode = res.getStatus();                if (responseCode == HttpStatus.NO_CONTENT_204) {            return new HttpResponse<>(responseCode, convertHttpFieldsToMap(res.getHeaders()), null);        } else if (responseCode >= 400) {            ErrorMessage errorMessage = JSON_SERDE.readValue(res.getContentAsString(), ErrorMessage.class);            throw new ConnectRestException(responseCode, errorMessage.errorCode(), errorMessage.message());        } else if (responseCode >= 200 && responseCode < 300) {            T result = JSON_SERDE.readValue(res.getContentAsString(), responseFormat);            return new HttpResponse<>(responseCode, convertHttpFieldsToMap(res.getHeaders()), result);        } else {            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR, Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), "Unexpected status code when handling forwarded request: " + responseCode);        }    } catch (IOException | InterruptedException | TimeoutException | ExecutionException e) {                throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR, "IO Error trying to forward REST request: " + e.getMessage(), e);    } finally {        if (client != null)            try {                client.stop();            } catch (Exception e) {                            }    }}
private static void kafkatest_f10150_0(HttpHeaders headers, Request req)
{    if (headers != null) {        String credentialAuthorization = headers.getHeaderString(HttpHeaders.AUTHORIZATION);        if (credentialAuthorization != null) {            req.header(HttpHeaders.AUTHORIZATION, credentialAuthorization);        }    }}
public voidf10159_1Herder herder)
{        ResourceConfig resourceConfig = new ResourceConfig();    resourceConfig.register(new JacksonJsonProvider());    resourceConfig.register(new RootResource(herder));    resourceConfig.register(new ConnectorsResource(herder, config));    resourceConfig.register(new ConnectorPluginsResource(herder));    resourceConfig.register(ConnectExceptionMapper.class);    resourceConfig.property(ServerProperties.WADL_FEATURE_DISABLE, true);    registerRestExtensions(herder, resourceConfig);    ServletContainer servletContainer = new ServletContainer(resourceConfig);    ServletHolder servletHolder = new ServletHolder(servletContainer);    ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);    context.setContextPath("/");    context.addServlet(servletHolder, "/*");    String allowedOrigins = config.getString(WorkerConfig.ACCESS_CONTROL_ALLOW_ORIGIN_CONFIG);    if (allowedOrigins != null && !allowedOrigins.trim().isEmpty()) {        FilterHolder filterHolder = new FilterHolder(new CrossOriginFilter());        filterHolder.setName("cross-origin");        filterHolder.setInitParameter(CrossOriginFilter.ALLOWED_ORIGINS_PARAM, allowedOrigins);        String allowedMethods = config.getString(WorkerConfig.ACCESS_CONTROL_ALLOW_METHODS_CONFIG);        if (allowedMethods != null && !allowedOrigins.trim().isEmpty()) {            filterHolder.setInitParameter(CrossOriginFilter.ALLOWED_METHODS_PARAM, allowedMethods);        }        context.addFilter(filterHolder, "/*", EnumSet.of(DispatcherType.REQUEST));    }    RequestLogHandler requestLogHandler = new RequestLogHandler();    Slf4jRequestLogWriter slf4jRequestLogWriter = new Slf4jRequestLogWriter();    slf4jRequestLogWriter.setLoggerName(RestServer.class.getCanonicalName());    CustomRequestLog requestLog = new CustomRequestLog(slf4jRequestLogWriter, CustomRequestLog.EXTENDED_NCSA_FORMAT + " %msT");    requestLogHandler.setRequestLog(requestLog);    handlers.setHandlers(new Handler[] { context, new DefaultHandler(), requestLogHandler });    try {        context.start();    } catch (Exception e) {        throw new ConnectException("Unable to initialize REST resources", e);    }     server is started and ready to handle requests");}
public URI kafkatest_f10160_0()
{    return jettyServer.getURI();}
protected static void kafkatest_f10169_0(SslContextFactory ssl, Map<String, Object> sslConfigValues)
{    ssl.setKeyStoreType((String) getOrDefault(sslConfigValues, SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, SslConfigs.DEFAULT_SSL_KEYSTORE_TYPE));    String sslKeystoreLocation = (String) sslConfigValues.get(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG);    if (sslKeystoreLocation != null)        ssl.setKeyStorePath(sslKeystoreLocation);    Password sslKeystorePassword = (Password) sslConfigValues.get(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG);    if (sslKeystorePassword != null)        ssl.setKeyStorePassword(sslKeystorePassword.value());    Password sslKeyPassword = (Password) sslConfigValues.get(SslConfigs.SSL_KEY_PASSWORD_CONFIG);    if (sslKeyPassword != null)        ssl.setKeyManagerPassword(sslKeyPassword.value());}
protected static Object kafkatest_f10170_0(Map<String, Object> configMap, String key, Object defaultValue)
{    if (configMap.containsKey(key))        return configMap.get(key);    return defaultValue;}
public String kafkatest_f10179_0()
{    return getString(DLQ_TOPIC_NAME_CONFIG);}
public short kafkatest_f10180_0()
{    return getShort(DLQ_TOPIC_REPLICATION_FACTOR_CONFIG);}
private voidf10189_1WorkerSourceTask workerTask)
{        try {        if (workerTask.commitOffsets()) {            return;        }            } catch (Throwable t) {        // We're very careful about exceptions here since any uncaught exceptions in the commit        // thread would cause the fixed interval schedule on the ExecutorService to stop running        // for that task            }}
public synchronized voidf10190_1)
{        startServices();    }
public synchronized void kafkatest_f10199_0(String connName, Callback<Created<ConnectorInfo>> callback)
{    try {        if (!configState.contains(connName)) {            // Deletion, must already exist            callback.onCompletion(new NotFoundException("Connector " + connName + " not found", null), null);            return;        }        removeConnectorTasks(connName);        worker.stopConnector(connName);        configBackingStore.removeConnectorConfig(connName);        onDeletion(connName);        callback.onCompletion(null, new Created<ConnectorInfo>(false, null));    } catch (ConnectException e) {        callback.onCompletion(e, null);    }}
public synchronized void kafkatest_f10200_0(String connName, final Map<String, String> config, boolean allowReplace, final Callback<Created<ConnectorInfo>> callback)
{    try {        if (maybeAddConfigErrors(validateConnectorConfig(config), callback)) {            return;        }        boolean created = false;        if (configState.contains(connName)) {            if (!allowReplace) {                callback.onCompletion(new AlreadyExistsException("Connector " + connName + " already exists"), null);                return;            }            worker.stopConnector(connName);        } else {            created = true;        }        configBackingStore.putConnectorConfig(connName, config);        if (!startConnector(connName)) {            callback.onCompletion(new ConnectException("Failed to start connector: " + connName), null);            return;        }        updateConnectorTasks(connName);        callback.onCompletion(null, new Created<>(created, createConnectorInfo(connName)));    } catch (ConnectException e) {        callback.onCompletion(e, null);    }}
private List<Map<String, String>> kafkatest_f10209_0(String connName)
{    Map<String, String> config = configState.connectorConfig(connName);    ConnectorConfig connConfig = worker.isSinkConnector(connName) ? new SinkConnectorConfig(plugins(), config) : new SourceConnectorConfig(plugins(), config);    return worker.connectorTaskConfigs(connName, connConfig);}
private void kafkatest_f10210_0(String connName, TargetState initialState)
{    Map<String, String> connConfigs = configState.connectorConfig(connName);    for (ConnectorTaskId taskId : configState.tasks(connName)) {        Map<String, String> taskConfigMap = configState.taskConfig(taskId);        worker.startTask(taskId, configState, connConfigs, taskConfigMap, this, initialState);    }}
public int kafkatest_f10219_0()
{    return Objects.hash(seq);}
public synchronized void kafkatest_f10220_0(State newState, long now)
{    // JDK8: remove synchronization by using lastState.getAndUpdate(oldState->oldState.newState(newState, now));    lastState.set(lastState.get().newState(newState, now));}
public String kafkatest_f10229_0()
{    StringJoiner chain = new StringJoiner(", ", getClass().getName() + "{", "}");    for (Transformation<R> transformation : transformations) {        chain.add(transformation.getClass().getName());    }    return chain.toString();}
private WorkerConfigTransformer kafkatest_f10230_0()
{    final List<String> providerNames = config.getList(WorkerConfig.CONFIG_PROVIDERS_CONFIG);    Map<String, ConfigProvider> providerMap = new HashMap<>();    for (String providerName : providerNames) {        ConfigProvider configProvider = plugins.newConfigProvider(config, WorkerConfig.CONFIG_PROVIDERS_CONFIG + "." + providerName, ClassLoaderUsage.PLUGINS);        providerMap.put(providerName, configProvider);    }    return new WorkerConfigTransformer(this, providerMap);}
public booleanf10239_1String connName)
{    try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {                WorkerConnector workerConnector = connectors.remove(connName);        if (workerConnector == null) {                        return false;        }        ClassLoader savedLoader = plugins.currentThreadLoader();        try {            savedLoader = plugins.compareAndSwapLoaders(workerConnector.connector());            workerConnector.shutdown();        } finally {            Plugins.compareAndSwapLoaders(savedLoader);        }            }    return true;}
public Set<String> kafkatest_f10240_0()
{    return connectors.keySet();}
private List<ErrorReporter> kafkatest_f10249_0(ConnectorTaskId id, SinkConnectorConfig connConfig, ErrorHandlingMetrics errorHandlingMetrics, Class<? extends Connector> connectorClass)
{    ArrayList<ErrorReporter> reporters = new ArrayList<>();    LogReporter logReporter = new LogReporter(id, connConfig, errorHandlingMetrics);    reporters.add(logReporter);    // check if topic for dead letter queue exists    String topic = connConfig.dlqTopicName();    if (topic != null && !topic.isEmpty()) {        Map<String, Object> producerProps = producerConfigs(id, "connector-dlq-producer-" + id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);        Map<String, Object> adminProps = adminConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);        DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(adminProps, id, connConfig, producerProps, errorHandlingMetrics);        reporters.add(reporter);    }    return reporters;}
private List<ErrorReporter> kafkatest_f10250_0(ConnectorTaskId id, ConnectorConfig connConfig, ErrorHandlingMetrics errorHandlingMetrics)
{    List<ErrorReporter> reporters = new ArrayList<>();    LogReporter logReporter = new LogReporter(id, connConfig, errorHandlingMetrics);    reporters.add(logReporter);    return reporters;}
public Converter kafkatest_f10259_0()
{    return internalKeyConverter;}
public Converter kafkatest_f10260_0()
{    return internalValueConverter;}
 void kafkatest_f10269_0()
{    metricGroup.close();}
 void kafkatest_f10270_0()
{    connectorStartupAttempts.record(1.0);    connectorStartupFailures.record(1.0);    connectorStartupResults.record(0.0);}
protected Map<String, Object> kafkatest_f10279_0(final Map<String, Object> parsedValues)
{    return CommonClientConfigs.postProcessReconnectBackoffConfigs(this, parsedValues);}
public static List<String> kafkatest_f10280_0(Map<String, String> props)
{    String locationList = props.get(WorkerConfig.PLUGIN_PATH_CONFIG);    return locationList == null ? new ArrayList<String>() : Arrays.asList(COMMA_WITH_WHITESPACE.split(locationList.trim(), -1));}
private booleanf10289_1)
{    try {        switch(state) {            case STARTED:                return false;            case INIT:            case STOPPED:                connector.start(config);                this.state = State.STARTED;                return true;            default:                throw new IllegalArgumentException("Cannot start connector in state " + state);        }    } catch (Throwable t) {                onFailure(t);        return false;    }}
private void kafkatest_f10290_0(Throwable t)
{    statusListener.onFailure(connName, t);    this.state = State.FAILED;}
protected String kafkatest_f10299_0()
{    if (isSinkConnector())        return "sink";    if (isSourceConnector())        return "source";    return "unknown";}
public Connector kafkatest_f10300_0()
{    return connector;}
public void kafkatest_f10309_0(String connector, Throwable cause)
{    state = AbstractStatus.State.FAILED;    delegate.onFailure(connector, cause);}
public void kafkatest_f10310_0(String connector)
{    state = AbstractStatus.State.DESTROYED;    delegate.onDeletion(connector);}
protected void kafkatest_f10319_0()
{    String[] osInfo = { OS.getName(), OS.getArch(), OS.getVersion() };    values.put("os.spec", Utils.join(osInfo, ", "));    values.put("os.vcpus", String.valueOf(OS.getAvailableProcessors()));}
public voidf10320_1TaskConfig taskConfig)
{    try {        this.taskConfig = taskConfig.originalsStrings();        this.context = new WorkerSinkTaskContext(consumer, this, configState);    } catch (Throwable t) {                onFailure(t);    }}
protected voidf10329_1)
{    SinkConnectorConfig.validate(taskConfig);    if (SinkConnectorConfig.hasTopicsConfig(taskConfig)) {        String[] topics = taskConfig.get(SinkTask.TOPICS_CONFIG).split(",");        consumer.subscribe(Arrays.asList(topics), new HandleRebalance());            } else {        String topicsRegexStr = taskConfig.get(SinkTask.TOPICS_REGEX_CONFIG);        Pattern pattern = Pattern.compile(topicsRegexStr);        consumer.subscribe(pattern, new HandleRebalance());            }    task.initialize(context);    task.start(taskConfig);    }
protected void kafkatest_f10330_0(long timeoutMs)
{    rewind();    long retryTimeout = context.timeout();    if (retryTimeout > 0) {        timeoutMs = Math.min(timeoutMs, retryTimeout);        context.timeout(-1L);    }    log.trace("{} Polling consumer with timeout {} ms", this, timeoutMs);    ConsumerRecords<byte[], byte[]> msgs = pollConsumer(timeoutMs);    assert messageBatch.isEmpty() || msgs.isEmpty();    log.trace("{} Polling returned {} messages", this, msgs.count());    convertMessages(msgs);    deliverMessages();}
private void kafkatest_f10339_0(ConsumerRecords<byte[], byte[]> msgs)
{    origOffsets.clear();    for (ConsumerRecord<byte[], byte[]> msg : msgs) {        log.trace("{} Consuming and converting message in topic '{}' partition {} at offset {} and timestamp {}", this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp());        retryWithToleranceOperator.consumerRecord(msg);        SinkRecord transRecord = convertAndTransformRecord(msg);        origOffsets.put(new TopicPartition(msg.topic(), msg.partition()), new OffsetAndMetadata(msg.offset() + 1));        if (transRecord != null) {            messageBatch.add(transRecord);        } else {            log.trace("{} Converters and transformations returned null, possibly because of too many retries, so " + "dropping record in topic '{}' partition {} at offset {}", this, msg.topic(), msg.partition(), msg.offset());        }    }    sinkTaskMetricsGroup.recordConsumedOffsets(origOffsets);}
private SinkRecord kafkatest_f10340_0(final ConsumerRecord<byte[], byte[]> msg)
{    SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(() -> keyConverter.toConnectData(msg.topic(), msg.key()), Stage.KEY_CONVERTER, keyConverter.getClass());    SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(() -> valueConverter.toConnectData(msg.topic(), msg.value()), Stage.VALUE_CONVERTER, valueConverter.getClass());    Headers headers = retryWithToleranceOperator.execute(() -> convertHeadersFor(msg), Stage.HEADER_CONVERTER, headerConverter.getClass());    if (retryWithToleranceOperator.failed()) {        return null;    }    Long timestamp = ConnectUtils.checkAndConvertTimestamp(msg.timestamp());    SinkRecord origRecord = new SinkRecord(msg.topic(), msg.partition(), keyAndSchema.schema(), keyAndSchema.value(), valueAndSchema.schema(), valueAndSchema.value(), msg.offset(), timestamp, msg.timestampType(), headers);    log.trace("{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}", this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());    return transformationChain.apply(origRecord);}
protected void kafkatest_f10349_0(long duration, Throwable error)
{    super.recordCommitFailure(duration, error);}
protected void kafkatest_f10350_0(long duration)
{    super.recordCommitSuccess(duration);    sinkTaskMetricsGroup.recordOffsetCommitSuccess();}
 void kafkatest_f10359_0(long duration)
{    putBatchTime.record(duration);}
 void kafkatest_f10360_0(int assignedPartitionCount)
{    partitionCount.record(assignedPartitionCount);}
public Map<String, String> kafkatest_f10369_0()
{    return configState.taskConfig(sinkTask.id());}
public voidf10370_1Map<TopicPartition, Long> offsets)
{        this.offsets.putAll(offsets);}
public Set<TopicPartition> kafkatest_f10379_0()
{    return pausedPartitions;}
public voidf10380_1)
{        commitRequested = true;}
public voidf10389_1)
{    try {        task.initialize(new WorkerSourceTaskContext(offsetReader, this, configState));        task.start(taskConfig);                synchronized (this) {            if (startedShutdownBeforeStartCompleted) {                tryStop();                return;            }            finishedStart = true;        }        while (!isStopping()) {            if (shouldPause()) {                onPause();                if (awaitUnpause()) {                    onResume();                }                continue;            }            maybeThrowProducerSendException();            if (toSend == null) {                log.trace("{} Nothing to send to Kafka. Polling source for additional records", this);                long start = time.milliseconds();                toSend = poll();                if (toSend != null) {                    recordPollReturned(toSend.size(), time.milliseconds() - start);                }            }            if (toSend == null)                continue;            log.trace("{} About to send {} records to Kafka", this, toSend.size());            if (!sendRecords())                stopRequestedLatch.await(SEND_FAILED_BACKOFF_MS, TimeUnit.MILLISECONDS);        }    } catch (InterruptedException e) {    // Ignore and allow to exit.    } finally {        // It should still be safe to commit offsets since any exception would have        // simply resulted in not getting more records but all the existing records should be ok to flush        // and commit offsets. Worst case, task.flush() will also throw an exception causing the offset commit        // to fail.        commitOffsets();    }}
private void kafkatest_f10390_0()
{    if (producerSendException.get() != null) {        throw new ConnectException("Unrecoverable exception from producer send callback", producerSendException.get());    }}
public voidf10399_1Throwable error, Void result)
{    if (error != null) {            } else {        log.trace("{} Finished flushing offsets to storage", WorkerSourceTask.this);    }}
private voidf10400_1)
{    try {        this.task.commit();    } catch (Throwable t) {            }}
private void kafkatest_f10409_0()
{    if (!completed) {        metricsGroup.recordWrite(batchSize - counter);        completed = true;    }}
 void kafkatest_f10410_0()
{    metricGroup.close();}
public void kafkatest_f10419_0()
{    triggerStop();}
public void kafkatest_f10420_0()
{    cancelled = true;}
public void kafkatest_f10429_0()
{    // Clear all MDC parameters, in case this thread is being reused    LoggingContext.clear();    try (LoggingContext loggingContext = LoggingContext.forTask(id())) {        ClassLoader savedLoader = Plugins.compareAndSwapLoaders(loader);        String savedName = Thread.currentThread().getName();        try {            Thread.currentThread().setName(THREAD_NAME_PREFIX + id);            doRun();            onShutdown();        } catch (Throwable t) {            onFailure(t);            if (t instanceof Error)                throw (Error) t;        } finally {            try {                Thread.currentThread().setName(savedName);                Plugins.compareAndSwapLoaders(savedLoader);                shutdownLatch.countDown();            } finally {                try {                    releaseResources();                } finally {                    taskMetricsGroup.close();                }            }        }    }}
public boolean kafkatest_f10430_0()
{    return this.targetState == TargetState.PAUSED;}
public double kafkatest_f10439_0(MetricConfig config, long now)
{    return taskStateTimer.durationRatio(matchingState, now);}
 void kafkatest_f10440_0()
{    metricGroup.close();}
public State kafkatest_f10449_0()
{    return taskStateTimer.currentState();}
protected MetricGroup kafkatest_f10450_0()
{    return metricGroup;}
public static String kafkatest_f10459_0(String connectorName)
{    return COMMIT_TASKS_PREFIX + connectorName;}
public void kafkatest_f10460_0(UpdateListener listener)
{    this.updateListener = listener;}
public voidf10469_1String connector, List<Map<String, String>> configs)
{    // any outstanding lagging data to consume.    try {        configLog.readToEnd().get(READ_TO_END_TIMEOUT_MS, TimeUnit.MILLISECONDS);    } catch (InterruptedException | ExecutionException | TimeoutException e) {                throw new ConnectException("Error writing root configuration to Kafka", e);    }    int taskCount = configs.size();    // Start sending all the individual updates    int index = 0;    for (Map<String, String> taskConfig : configs) {        Struct connectConfig = new Struct(TASK_CONFIGURATION_V0);        connectConfig.put("properties", taskConfig);        byte[] serializedConfig = converter.fromConnectData(topic, TASK_CONFIGURATION_V0, connectConfig);                ConnectorTaskId connectorTaskId = new ConnectorTaskId(connector, index);        configLog.send(TASK_KEY(connectorTaskId), serializedConfig);        index++;    }    // the end of the log    try {        // Read to end to ensure all the task configs have been written        if (taskCount > 0) {            configLog.readToEnd().get(READ_TO_END_TIMEOUT_MS, TimeUnit.MILLISECONDS);        }        // Write the commit message        Struct connectConfig = new Struct(CONNECTOR_TASKS_COMMIT_V0);        connectConfig.put("tasks", taskCount);        byte[] serializedConfig = converter.fromConnectData(topic, CONNECTOR_TASKS_COMMIT_V0, connectConfig);                configLog.send(COMMIT_TASKS_KEY(connector), serializedConfig);        // Read to end to ensure all the commit messages have been written        configLog.readToEnd().get(READ_TO_END_TIMEOUT_MS, TimeUnit.MILLISECONDS);    } catch (InterruptedException | ExecutionException | TimeoutException e) {                throw new ConnectException("Error writing root configuration to Kafka", e);    }}
public void kafkatest_f10470_0(long timeout, TimeUnit unit) throws TimeoutException
{    try {        configLog.readToEnd().get(timeout, unit);    } catch (InterruptedException | ExecutionException e) {        throw new ConnectException("Error trying to read to end of config log", e);    }}
private static int kafkatest_f10479_0(Object value)
{    if (value instanceof Integer)        return (int) value;    else if (value instanceof Long)        return (int) (long) value;    else        throw new ConnectException("Expected integer value to be either Integer or Long");}
public void kafkatest_f10480_0(final WorkerConfig config)
{    String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);    if (topic == null || topic.trim().length() == 0)        throw new ConfigException("Offset storage topic must be specified");    data = new HashMap<>();    Map<String, Object> originals = config.originals();    Map<String, Object> producerProps = new HashMap<>(originals);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);    Map<String, Object> consumerProps = new HashMap<>(originals);    consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());    consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());    Map<String, Object> adminProps = new HashMap<>(originals);    NewTopic topicDescription = TopicAdmin.defineTopic(topic).compacted().partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).build();    offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);}
public synchronized void kafkatest_f10489_0(RecordMetadata metadata, Exception exception)
{    if (exception != null) {        if (!completed) {            this.exception = exception;            callback.onCompletion(exception, null);            completed = true;            this.notify();        }        return;    }    numLeft -= 1;    if (numLeft == 0) {        callback.onCompletion(null, null);        completed = true;        this.notify();    }}
public synchronized boolean kafkatest_f10490_0(boolean mayInterruptIfRunning)
{    return false;}
public void kafkatest_f10499_0()
{    kafkaLog.start();    // read to the end on startup to ensure that api requests see the most recent states    kafkaLog.readToEnd();}
public void kafkatest_f10500_0()
{    kafkaLog.stop();}
public voidf10509_1RecordMetadata metadata, Exception exception)
{    if (exception == null)        return;    if (exception instanceof RetriableException) {        synchronized (KafkaStatusBackingStore.this) {            if (entry.isDeleted() || status.generation() != generation || (safeWrite && !entry.canWriteSafely(status, sequence)))                return;        }        kafkaLog.send(key, value, this);    } else {            }}
private synchronized CacheEntry<ConnectorStatus> kafkatest_f10510_0(String connector)
{    CacheEntry<ConnectorStatus> entry = connectors.get(connector);    if (entry == null) {        entry = new CacheEntry<>();        connectors.put(connector, entry);    }    return entry;}
private TaskStatusf10519_1ConnectorTaskId taskId, byte[] data)
{    try {        SchemaAndValue schemaAndValue = converter.toConnectData(topic, data);        if (!(schemaAndValue.value() instanceof Map)) {                        return null;        }        @SuppressWarnings("unchecked")        Map<String, Object> statusMap = (Map<String, Object>) schemaAndValue.value();        TaskStatus.State state = TaskStatus.State.valueOf((String) statusMap.get(STATE_KEY_NAME));        String trace = (String) statusMap.get(TRACE_KEY_NAME);        String workerUrl = (String) statusMap.get(WORKER_ID_KEY_NAME);        int generation = ((Long) statusMap.get(GENERATION_KEY_NAME)).intValue();        return new TaskStatus(taskId, state, workerUrl, generation, trace);    } catch (Exception e) {                return null;    }}
private byte[] kafkatest_f10520_0(AbstractStatus status)
{    Struct struct = new Struct(STATUS_SCHEMA_V0);    struct.put(STATE_KEY_NAME, status.state().name());    if (status.trace() != null)        struct.put(TRACE_KEY_NAME, status.trace());    struct.put(WORKER_ID_KEY_NAME, status.workerId());    struct.put(GENERATION_KEY_NAME, status.generation());    return converter.fromConnectData(topic, STATUS_SCHEMA_V0, struct);}
public void kafkatest_f10529_0()
{    this.deleted = true;}
public boolean kafkatest_f10530_0()
{    return deleted;}
public synchronized void kafkatest_f10542_0(String connector, TargetState state)
{    ConnectorState connectorState = connectors.get(connector);    if (connectorState == null)        throw new IllegalArgumentException("No connector `" + connector + "` configured");    connectorState.targetState = state;    if (updateListener != null)        updateListener.onConnectorTargetStateChange(connector);}
public synchronized void kafkatest_f10543_0(UpdateListener listener)
{    this.updateListener = listener;}
public synchronized void kafkatest_f10557_0(ConnectorStatus status)
{    put(status);}
public synchronized void kafkatest_f10558_0(TaskStatus status)
{    if (status.state() == TaskStatus.State.DESTROYED)        tasks.remove(status.id().connector(), status.id().task());    else        tasks.put(status.id().connector(), status.id().task(), status);}
private boolean kafkatest_f10568_0()
{    return toFlush != null;}
public synchronized booleanf10569_1)
{    if (flushing()) {                throw new ConnectException("OffsetStorageWriter is already flushing");    }    if (data.isEmpty())        return false;    assert !flushing();    toFlush = data;    data = new HashMap<>();    return true;}
public voidf10578_1)
{        context.raiseError(new RuntimeException());}
public Class<? extends Task> kafkatest_f10579_0()
{    throw new UnsupportedOperationException();}
public void kafkatest_f10588_0(Map<String, String> props)
{    delegate.start(props);}
public Class<? extends Task> kafkatest_f10589_0()
{    return MockSinkTask.class;}
public void kafkatest_f10600_0(ConnectorContext ctx, List<Map<String, String>> taskConfigs)
{    delegate.initialize(ctx, taskConfigs);}
public void kafkatest_f10601_0(Map<String, String> props)
{    delegate.reconfigure(props);}
public voidf10610_1Map<String, String> config)
{    this.mockMode = config.get(MockConnector.MOCK_MODE_KEY);    if (MockConnector.TASK_FAILURE.equals(mockMode)) {        this.startTimeMs = System.currentTimeMillis();        String delayMsString = config.get(MockConnector.DELAY_MS_KEY);        this.failureDelayMs = MockConnector.DEFAULT_FAILURE_DELAY_MS;        if (delayMsString != null)            failureDelayMs = Long.parseLong(delayMsString);            }}
public List<SourceRecord>f10611_1) throws InterruptedException
{    if (MockConnector.TASK_FAILURE.equals(mockMode)) {        long now = System.currentTimeMillis();        if (now > startTimeMs + failureDelayMs) {                        throw new RuntimeException();        }    }    return Collections.emptyList();}
public void kafkatest_f10622_0()
{    throttler.wakeup();}
private static void kafkatest_f10623_0(PrintStream out, DocInfo docInfo)
{    out.println("<div id=\"" + docInfo.transformationName + "\">");    out.print("<h5>");    out.print(docInfo.transformationName);    out.println("</h5>");    out.println(docInfo.overview);    out.println("<p/>");    out.println(docInfo.configDef.toHtml());    out.println("</div>");}
public void kafkatest_f10633_0(Map<String, String> props)
{    try {        name = props.get(NAME_CONFIG);        id = Integer.parseInt(props.get(ID_CONFIG));    } catch (NumberFormatException e) {        throw new ConnectException("Invalid VerifiableSourceTask configuration", e);    }}
public void kafkatest_f10634_0(Collection<SinkRecord> records)
{    long nowMs = System.currentTimeMillis();    for (SinkRecord record : records) {        Map<String, Object> data = new HashMap<>();        data.put("name", name);        // VerifiableSourceTask's input task (source partition)        data.put("task", record.key());        data.put("sinkTask", id);        data.put("topic", record.topic());        data.put("time_ms", nowMs);        data.put("seqno", record.value());        data.put("offset", record.kafkaOffset());        String dataJson;        try {            dataJson = JSON_SERDE.writeValueAsString(data);        } catch (JsonProcessingException e) {            dataJson = "Bad data can't be written as json: " + e.getMessage();        }        System.out.println(dataJson);        unflushed.add(data);    }}
public List<SourceRecord> kafkatest_f10645_0() throws InterruptedException
{    long sendStartMs = System.currentTimeMillis();    if (throttler.shouldThrottle(seqno - startingSeqno, sendStartMs))        throttler.throttle();    long nowMs = System.currentTimeMillis();    Map<String, Object> data = new HashMap<>();    data.put("name", name);    data.put("task", id);    data.put("topic", this.topic);    data.put("time_ms", nowMs);    data.put("seqno", seqno);    String dataJson;    try {        dataJson = JSON_SERDE.writeValueAsString(data);    } catch (JsonProcessingException e) {        dataJson = "Bad data can't be written as json: " + e.getMessage();    }    System.out.println(dataJson);    Map<String, Long> ccOffset = Collections.singletonMap(SEQNO_FIELD, seqno);    SourceRecord srcRecord = new SourceRecord(partition, ccOffset, topic, Schema.INT32_SCHEMA, id, Schema.INT64_SCHEMA, seqno);    List<SourceRecord> result = Collections.singletonList(srcRecord);    seqno++;    return result;}
public void kafkatest_f10646_0(SourceRecord record) throws InterruptedException
{    Map<String, Object> data = new HashMap<>();    data.put("name", name);    data.put("task", id);    data.put("topic", this.topic);    data.put("time_ms", System.currentTimeMillis());    data.put("seqno", record.value());    data.put("committed", true);    String dataJson;    try {        dataJson = JSON_SERDE.writeValueAsString(data);    } catch (JsonProcessingException e) {        dataJson = "Bad data can't be written as json: " + e.getMessage();    }    System.out.println(dataJson);}
public static Stringf10655_1WorkerConfig config)
{        try (Admin adminClient = Admin.create(config.originals())) {        return lookupKafkaClusterId(adminClient);    }}
 static Stringf10656_1Admin adminClient)
{        try {        KafkaFuture<String> clusterIdFuture = adminClient.describeCluster().clusterId();        if (clusterIdFuture == null) {                        return null;        }                String kafkaClusterId = clusterIdFuture.get();                return kafkaClusterId;    } catch (InterruptedException e) {        throw new ConnectException("Unexpectedly interrupted when looking up Kafka cluster info", e);    } catch (ExecutionException e) {        throw new ConnectException("Failed to connect to and describe Kafka cluster. " + "Check worker's broker connection and security properties.", e);    }}
public voidf10666_1)
{        initializer.run();    producer = createProducer();    consumer = createConsumer();    List<TopicPartition> partitions = new ArrayList<>();    // We expect that the topics will have been created either manually by the user or automatically by the herder    List<PartitionInfo> partitionInfos = null;    long started = time.milliseconds();    while (partitionInfos == null && time.milliseconds() - started < CREATE_TOPIC_TIMEOUT_MS) {        partitionInfos = consumer.partitionsFor(topic);        Utils.sleep(Math.min(time.milliseconds() - started, 1000));    }    if (partitionInfos == null)        throw new ConnectException("Could not look up partition metadata for offset backing store topic in" + " allotted period. This could indicate a connectivity issue, unavailable topic partitions, or if" + " this is your first use of the topic it may have taken too long to create.");    for (PartitionInfo partition : partitionInfos) partitions.add(new TopicPartition(partition.topic(), partition.partition()));    consumer.assign(partitions);    // Always consume from the beginning of all partitions. Necessary to ensure that we don't use committed offsets    // when a 'group.id' is specified (if offsets happen to have been committed unexpectedly).    consumer.seekToBeginning(partitions);    readToLogEnd();    thread = new WorkThread();    thread.start();        }
public voidf10667_1)
{        synchronized (this) {        stopRequested = true;    }    consumer.wakeup();    try {        thread.join();    } catch (InterruptedException e) {        throw new ConnectException("Failed to stop KafkaBasedLog. Exiting without cleanly shutting " + "down it's producer and consumer.", e);    }    try {        producer.close();    } catch (KafkaException e) {            }    try {        consumer.close();    } catch (KafkaException e) {            }    }
private void kafkatest_f10676_0()
{    log.trace("Reading to end of offset log");    Set<TopicPartition> assignment = consumer.assignment();    Map<TopicPartition, Long> endOffsets = consumer.endOffsets(assignment);    log.trace("Reading to end of log offsets {}", endOffsets);    while (!endOffsets.isEmpty()) {        Iterator<Map.Entry<TopicPartition, Long>> it = endOffsets.entrySet().iterator();        while (it.hasNext()) {            Map.Entry<TopicPartition, Long> entry = it.next();            if (consumer.position(entry.getKey()) >= entry.getValue())                it.remove();            else {                poll(Integer.MAX_VALUE);                break;            }        }    }}
public voidf10677_1)
{    try {        log.trace("{} started execution", this);        while (true) {            int numCallbacks;            synchronized (KafkaBasedLog.this) {                if (stopRequested)                    break;                numCallbacks = readLogEndOffsetCallbacks.size();            }            if (numCallbacks > 0) {                try {                    readToLogEnd();                    log.trace("Finished read to end log for topic {}", topic);                } catch (TimeoutException e) {                                        continue;                } catch (WakeupException e) {                    // called. Both are handled by restarting this loop.                    continue;                }            }            synchronized (KafkaBasedLog.this) {                // since it is possible for another write + readToEnd to sneak in the meantime                for (int i = 0; i < numCallbacks; i++) {                    Callback<Void> cb = readLogEndOffsetCallbacks.poll();                    cb.onCompletion(null, null);                }            }            try {                poll(Integer.MAX_VALUE);            } catch (WakeupException e) {                // See previous comment, both possible causes of this wakeup are handled by starting this loop again                continue;            }        }    } catch (Throwable t) {            }}
public static void kafkatest_f10686_0()
{    final List<UrlType> urlTypes = new LinkedList<>();    urlTypes.add(new EmptyUrlType(ENDINGS));    urlTypes.addAll(Arrays.asList(Vfs.DefaultUrlTypes.values()));    Vfs.setDefaultURLTypes(urlTypes);}
public boolean kafkatest_f10687_0(URL url)
{    final String protocol = url.getProtocol();    final String externalForm = url.toExternalForm();    if (!protocol.equals(FILE_PROTOCOL)) {        return false;    }    for (String ending : endings) {        if (externalForm.endsWith(ending)) {            return true;        }    }    return false;}
public void kafkatest_f10697_0(long gracefulTimeout, TimeUnit unit) throws InterruptedException
{    boolean success = gracefulShutdown(gracefulTimeout, unit);    if (!success)        forceShutdown();}
public boolean kafkatest_f10698_0(long timeout, TimeUnit unit) throws InterruptedException
{    startGracefulShutdown();    return awaitShutdown(timeout, unit);}
public Map<C, V> kafkatest_f10707_0(R row)
{    Map<C, V> columns = table.get(row);    if (columns == null)        return Collections.emptyMap();    return Collections.unmodifiableMap(columns);}
public NewTopicBuilder kafkatest_f10708_0(int numPartitions)
{    this.numPartitions = numPartitions;    return this;}
public Set<String>f10717_1NewTopic... topics)
{    Map<String, NewTopic> topicsByName = new HashMap<>();    if (topics != null) {        for (NewTopic topic : topics) {            if (topic != null)                topicsByName.put(topic.name(), topic);        }    }    if (topicsByName.isEmpty())        return Collections.emptySet();    String bootstrapServers = bootstrapServers();    String topicNameList = Utils.join(topicsByName.keySet(), "', '");    // Attempt to create any missing topics    CreateTopicsOptions args = new CreateTopicsOptions().validateOnly(false);    Map<String, KafkaFuture<Void>> newResults = admin.createTopics(topicsByName.values(), args).values();    // Iterate over each future so that we can handle individual failures like when some topics already exist    Set<String> newlyCreatedTopicNames = new HashSet<>();    for (Map.Entry<String, KafkaFuture<Void>> entry : newResults.entrySet()) {        String topic = entry.getKey();        try {            entry.getValue().get();                        newlyCreatedTopicNames.add(topic);        } catch (ExecutionException e) {            Throwable cause = e.getCause();            if (cause instanceof TopicExistsException) {                                continue;            }            if (cause instanceof UnsupportedVersionException) {                                return Collections.emptySet();            }            if (cause instanceof ClusterAuthorizationException) {                                return Collections.emptySet();            }            if (cause instanceof TopicAuthorizationException) {                                return Collections.emptySet();            }            if (cause instanceof TimeoutException) {                // Timed out waiting for the operation to complete                throw new ConnectException("Timed out while checking for or creating topic(s) '" + topicNameList + "'." + " This could indicate a connectivity issue, unavailable topic partitions, or if" + " this is your first use of the topic it may have taken too long to create.", cause);            }            throw new ConnectException("Error while attempting to create/find topic(s) '" + topicNameList + "'", e);        } catch (InterruptedException e) {            Thread.interrupted();            throw new ConnectException("Interrupted while attempting to create/find topic(s) '" + topicNameList + "'", e);        }    }    return newlyCreatedTopicNames;}
public void kafkatest_f10718_0()
{    admin.close();}
protected ConnectorClientConfigOverridePolicy kafkatest_f10727_0()
{    return noneConnectorClientConfigOverridePolicy;}
public void kafkatest_f10728_0()
{    Map<String, Object> clientConfig = Collections.singletonMap(SaslConfigs.SASL_JAAS_CONFIG, "test");    testValidOverride(clientConfig);}
public void kafkatest_f10737_0()
{    SchemaAndValue data = converter.toConnectData(TOPIC, SAMPLE_BYTES);    assertEquals(Schema.OPTIONAL_BYTES_SCHEMA, data.schema());    assertTrue(Arrays.equals(SAMPLE_BYTES, (byte[]) data.value()));}
public void kafkatest_f10738_0()
{    SchemaAndValue data = converter.toConnectData(TOPIC, null);    assertEquals(Schema.OPTIONAL_BYTES_SCHEMA, data.schema());    assertNull(data.value());}
public Integer[] kafkatest_f10747_0()
{    return new Integer[] { Integer.MIN_VALUE, 1234, Integer.MAX_VALUE };}
protected Schema kafkatest_f10748_0()
{    return Schema.OPTIONAL_INT32_SCHEMA;}
public void kafkatest_f10757_0()
{    converter.toConnectData(TOPIC, new byte[10]);}
public void kafkatest_f10758_0()
{    converter.toConnectHeader(TOPIC, HEADER_NAME, new byte[10]);}
public void kafkatest_f10768_0() throws Exception
{    Map<String, String> props = basicConnectorConfig();    props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + SaslConfigs.SASL_JAAS_CONFIG, "sasl");    assertFailCreateConnector("None", props);}
public void kafkatest_f10769_0() throws Exception
{    Map<String, String> props = basicConnectorConfig();    props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + SaslConfigs.SASL_JAAS_CONFIG, "sasl");    props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");    assertFailCreateConnector("Principal", props);}
public Collection<TaskHandle> kafkatest_f10778_0()
{    return taskHandles.values();}
public voidf10779_1String taskId)
{        taskHandles.remove(taskId);}
public void kafkatest_f10788_0()
{    startAndStopCounter.recordStart();}
public void kafkatest_f10789_0()
{    startAndStopCounter.recordStop();}
private Optional<Boolean>f10798_1int numWorkers)
{    try {        int numUp = connect.activeWorkers().size();        return Optional.of(numUp >= numWorkers);    } catch (Exception e) {                return Optional.empty();    }}
private Optional<Boolean>f10799_1String connectorName, int numTasks)
{    try {        ConnectorStateInfo info = connect.connectorStatus(connectorName);        boolean result = info != null && info.tasks().size() == numTasks && info.connector().state().equals(AbstractStatus.State.RUNNING.toString()) && info.tasks().stream().allMatch(s -> s.state().equals(AbstractStatus.State.RUNNING.toString()));        return Optional.of(result);    } catch (Exception e) {                return Optional.empty();    }}
public void kafkatest_f10810_0()
{    // delete connector handle    RuntimeHandles.get().deleteConnector(CONNECTOR_NAME);    // stop all Connect, Kafka and Zk threads.    connect.stop();}
public void kafkatest_f10811_0() throws Exception
{    // create test topic    connect.kafka().createTopic("test-topic", NUM_TOPIC_PARTITIONS);    // setup up props for the sink connector    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put(TOPICS_CONFIG, "test-topic");    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    // expect all records to be consumed by the connector    connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);    // expect all records to be consumed by the connector    connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);    // start a sink connector    connect.configureConnector(CONNECTOR_NAME, props);    waitForCondition(this::checkForPartitionAssignment, CONNECTOR_SETUP_DURATION_MS, "Connector tasks were not assigned a partition each.");    // produce some messages into source topic partitions    for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {        connect.kafka().produce("test-topic", i % NUM_TOPIC_PARTITIONS, "key", "simple-message-value-" + i);    }    // consume all records from the source topic or fail, to ensure that they were correctly produced.    assertEquals("Unexpected number of records consumed", NUM_RECORDS_PRODUCED, connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, "test-topic").count());    // wait for the connector tasks to consume all records.    connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);    // wait for the connector tasks to commit all records.    connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);    // delete connector    connect.deleteConnector(CONNECTOR_NAME);}
public voidf10820_1Map<String, String> props)
{    taskId = props.get("task.id");    connectorName = props.get("connector.name");    taskHandle = RuntimeHandles.get().connectorHandle(connectorName).taskHandle(taskId);        taskHandle.recordTaskStart();}
public voidf10821_1Collection<TopicPartition> partitions)
{        assignments.addAll(partitions);    taskHandle.partitionsAssigned(partitions.size());}
public String kafkatest_f10830_0()
{    return "unknown";}
public voidf10831_1Map<String, String> props)
{    taskId = props.get("task.id");    connectorName = props.get("connector.name");    topicName = props.getOrDefault(TOPIC_CONFIG, "sequential-topic");    throughput = Long.valueOf(props.getOrDefault("throughput", "-1"));    batchSize = Integer.valueOf(props.getOrDefault("messages.per.poll", "1"));    taskHandle = RuntimeHandles.get().connectorHandle(connectorName).taskHandle(taskId);    Map<String, Object> offset = Optional.ofNullable(context.offsetStorageReader().offset(Collections.singletonMap("task.id", taskId))).orElse(Collections.emptyMap());    startingSeqno = Optional.ofNullable((Long) offset.get("saved")).orElse(0L);        throttler = new ThroughputThrottler(throughput, System.currentTimeMillis());    taskHandle.recordTaskStart();}
public void kafkatest_f10840_0() throws Exception
{    // create test topic    connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);    // setup up props for the source connector    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put("throughput", String.valueOf(1));    props.put("messages.per.poll", String.valueOf(10));    props.put(TOPIC_CONFIG, TOPIC_NAME);    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    waitForCondition(() -> this.assertWorkersUp(3), WORKER_SETUP_DURATION_MS, "Connect workers did not start in time.");    // start a source connector    IntStream.range(0, 4).forEachOrdered(i -> {        try {            connect.configureConnector(CONNECTOR_NAME + i, props);        } catch (IOException e) {            throw new ConnectException(e);        }    });    waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(true), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not start in time.");    // delete connector    connect.deleteConnector(CONNECTOR_NAME + 3);    waitForCondition(() -> !this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(true), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not stop in time.");    waitForCondition(this::assertConnectorAndTasksAreUnique, WORKER_SETUP_DURATION_MS, "Connect and tasks are imbalanced between the workers.");}
public void kafkatest_f10841_0() throws Exception
{    // create test topic    connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);    // setup up props for the source connector    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put("throughput", String.valueOf(1));    props.put("messages.per.poll", String.valueOf(10));    props.put(TOPIC_CONFIG, TOPIC_NAME);    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    waitForCondition(() -> this.assertWorkersUp(3), WORKER_SETUP_DURATION_MS, "Connect workers did not start in time.");    // start a source connector    IntStream.range(0, 4).forEachOrdered(i -> {        try {            connect.configureConnector(CONNECTOR_NAME + i, props);        } catch (IOException e) {            throw new ConnectException(e);        }    });    waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not start in time.");    connect.addWorker();    waitForCondition(() -> this.assertWorkersUp(4), WORKER_SETUP_DURATION_MS, "Connect workers did not start in time.");    waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not start in time.");    waitForCondition(this::assertConnectorAndTasksAreUnique, WORKER_SETUP_DURATION_MS, "Connect and tasks are imbalanced between the workers.");}
public String kafkatest_f10852_0()
{    return "test";}
public boolean kafkatest_f10853_0()
{    return true;}
public StartAndStopLatch kafkatest_f10862_0(int expectedStarts, int expectedStops, List<StartAndStopLatch> dependents)
{    StartAndStopLatch latch = new StartAndStopLatch(expectedStarts, expectedStops, this::remove, dependents, clock);    restartLatches.add(latch);    return latch;}
public StartAndStopLatch kafkatest_f10863_0(int expectedRestarts)
{    return expectedRestarts(expectedRestarts, expectedRestarts);}
public void kafkatest_f10872_0()
{    assertEquals(0, counter.starts());    counter.recordStart();    assertEquals(1, counter.starts());    counter.recordStart();    assertEquals(2, counter.starts());    assertEquals(2, counter.starts());}
public void kafkatest_f10873_0()
{    assertEquals(0, counter.stops());    counter.recordStop();    assertEquals(1, counter.stops());    counter.recordStop();    assertEquals(2, counter.stops());    assertEquals(2, counter.stops());}
public void kafkatest_f10882_0() throws Throwable
{    latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);    future = asyncAwait(100);    clock.sleep(10);    assertFalse(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
public void kafkatest_f10883_0() throws Throwable
{    latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);    future = asyncAwait(100);    latch.recordStart();    clock.sleep(10);    assertFalse(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
public void kafkatest_f10892_0()
{    if (recordsToCommitLatch != null) {        recordsToCommitLatch.countDown();    }    connectorHandle.commit();}
public void kafkatest_f10893_0(int batchSize)
{    if (recordsToCommitLatch != null) {        IntStream.range(0, batchSize).forEach(i -> recordsToCommitLatch.countDown());    }    connectorHandle.commit(batchSize);}
public void kafkatest_f10902_0()
{    startAndStopCounter.recordStart();}
public void kafkatest_f10903_0()
{    startAndStopCounter.recordStop();}
public void kafkatest_f10912_0()
{    AbstractHerder herder = createConfigValidationHerder(TestSourceConnector.class, noneConnectorClientConfigOverridePolicy);    replayAll();    herder.validateConnectorConfig(new HashMap<String, String>());    verifyAll();}
public void kafkatest_f10913_0()
{    AbstractHerder herder = createConfigValidationHerder(TestSourceConnector.class, noneConnectorClientConfigOverridePolicy);    replayAll();    Map<String, String> config = Collections.singletonMap(ConnectorConfig.CONNECTOR_CLASS_CONFIG, TestSourceConnector.class.getName());    ConfigInfos result = herder.validateConnectorConfig(config);    // We expect there to be errors due to the missing name and .... Note that these assertions depend heavily on    // the config fields for SourceConnectorConfig, but we expect these to change rarely.    assertEquals(TestSourceConnector.class.getName(), result.name());    assertEquals(Arrays.asList(ConnectorConfig.COMMON_GROUP, ConnectorConfig.TRANSFORMS_GROUP, ConnectorConfig.ERROR_GROUP), result.groups());    assertEquals(2, result.errorCount());    // Base connector config has 13 fields, connector's configs add 2    assertEquals(15, result.values().size());    // Missing name should generate an error    assertEquals(ConnectorConfig.NAME_CONFIG, result.values().get(0).configValue().name());    assertEquals(1, result.values().get(0).configValue().errors().size());    // "required" config from connector should generate an error    assertEquals("required", result.values().get(13).configValue().name());    assertEquals(1, result.values().get(13).configValue().errors().size());    verifyAll();}
private AbstractHerder kafkatest_f10922_0(Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy)
{    ConfigBackingStore configStore = strictMock(ConfigBackingStore.class);    StatusBackingStore statusStore = strictMock(StatusBackingStore.class);    AbstractHerder herder = partialMockBuilder(AbstractHerder.class).withConstructor(Worker.class, String.class, String.class, StatusBackingStore.class, ConfigBackingStore.class, ConnectorClientConfigOverridePolicy.class).withArgs(worker, workerId, kafkaClusterId, statusStore, configStore, connectorClientConfigOverridePolicy).addMockedMethod("generation").createMock();    EasyMock.expect(herder.generation()).andStubReturn(generation);    // Call to validateConnectorConfig    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andStubReturn(plugins);    final Connector connector;    try {        connector = connectorClass.newInstance();    } catch (InstantiationException | IllegalAccessException e) {        throw new RuntimeException("Couldn't create connector", e);    }    EasyMock.expect(plugins.newConnector(connectorClass.getName())).andReturn(connector);    EasyMock.expect(plugins.compareAndSwapLoaders(connector)).andReturn(classLoader);    return herder;}
public R kafkatest_f10924_0(R record)
{    return record;}
public void kafkatest_f10934_0()
{    MetricGroup group1 = metrics.group("name", "k1", "v1", "k2", "v2");    assertEquals("v1", group1.tags().get("k1"));    assertEquals("v2", group1.tags().get("k2"));    assertEquals(2, group1.tags().size());}
public void kafkatest_f10935_0()
{    MetricGroup group1 = metrics.group("name");    MetricGroup group2 = metrics.group("name");    assertNotNull(group1);    assertSame(group1, group2);    MetricGroup group3 = metrics.group("other");    assertNotNull(group3);    assertNotSame(group1, group3);    // Now with tags    MetricGroup group4 = metrics.group("name", "k1", "v1");    assertNotNull(group4);    assertNotSame(group1, group4);    assertNotSame(group2, group4);    assertNotSame(group3, group4);    MetricGroup group5 = metrics.group("name", "k1", "v1");    assertSame(group4, group5);}
public R kafkatest_f10944_0(R record)
{    return null;}
public void kafkatest_f10945_0()
{    magicNumber = 0;}
public void kafkatest_f10954_0()
{    Map<String, String> props = new HashMap<>();    props.put("name", "test");    props.put("connector.class", TestConnector.class.getName());    props.put("transforms", "a, b");    props.put("transforms.a.type", SimpleTransformation.class.getName());    props.put("transforms.a.magic.number", "42");    new ConnectorConfig(MOCK_PLUGINS, props);}
public void kafkatest_f10955_0()
{    Map<String, String> props = new HashMap<>();    props.put("name", "test");    props.put("connector.class", TestConnector.class.getName());    props.put("transforms", "a, b");    props.put("transforms.a.type", SimpleTransformation.class.getName());    props.put("transforms.a.magic.number", "42");    props.put("transforms.b.type", SimpleTransformation.class.getName());    props.put("transforms.b.magic.number", "84");    final ConnectorConfig config = new ConnectorConfig(MOCK_PLUGINS, props);    final List<Transformation<R>> transformations = config.transformations();    assertEquals(2, transformations.size());    assertEquals(42, ((SimpleTransformation) transformations.get(0)).magicNumber);    assertEquals(84, ((SimpleTransformation) transformations.get(1)).magicNumber);}
public void kafkatest_f10964_0()
{    ConnectProtocol.Assignment assignment = new ConnectProtocol.Assignment(ConnectProtocol.Assignment.NO_ERROR, "leader", LEADER_URL, 1L, Arrays.asList(connectorId1, connectorId3), Arrays.asList(taskId2x0));    ByteBuffer leaderBuf = ConnectProtocol.serializeAssignment(assignment);    ConnectProtocol.Assignment leaderAssignment = IncrementalCooperativeConnectProtocol.deserializeAssignment(leaderBuf);    assertEquals(false, leaderAssignment.failed());    assertEquals("leader", leaderAssignment.leader());    assertEquals(1, leaderAssignment.offset());    assertEquals(Arrays.asList(connectorId1, connectorId3), leaderAssignment.connectors());    assertEquals(Collections.singletonList(taskId2x0), leaderAssignment.tasks());    ConnectProtocol.Assignment assignment2 = new ConnectProtocol.Assignment(ConnectProtocol.Assignment.NO_ERROR, "member", LEADER_URL, 1L, Arrays.asList(connectorId2), Arrays.asList(taskId1x0, taskId3x0));    ByteBuffer memberBuf = ConnectProtocol.serializeAssignment(assignment2);    ConnectProtocol.Assignment memberAssignment = IncrementalCooperativeConnectProtocol.deserializeAssignment(memberBuf);    assertEquals(false, memberAssignment.failed());    assertEquals("member", memberAssignment.leader());    assertEquals(1, memberAssignment.offset());    assertEquals(Collections.singletonList(connectorId2), memberAssignment.connectors());    assertEquals(Arrays.asList(taskId1x0, taskId3x0), memberAssignment.tasks());}
public void kafkatest_f10965_0()
{    ExtendedAssignment assignment = new ExtendedAssignment(CONNECT_PROTOCOL_V1, ConnectProtocol.Assignment.NO_ERROR, "leader", LEADER_URL, 1L, Arrays.asList(connectorId1, connectorId3), Arrays.asList(taskId2x0), Collections.emptyList(), Collections.emptyList(), 0);    ByteBuffer leaderBuf = IncrementalCooperativeConnectProtocol.serializeAssignment(assignment);    ConnectProtocol.Assignment leaderAssignment = ConnectProtocol.deserializeAssignment(leaderBuf);    assertEquals(false, leaderAssignment.failed());    assertEquals("leader", leaderAssignment.leader());    assertEquals(1, leaderAssignment.offset());    assertEquals(Arrays.asList(connectorId1, connectorId3), leaderAssignment.connectors());    assertEquals(Collections.singletonList(taskId2x0), leaderAssignment.tasks());    ExtendedAssignment assignment2 = new ExtendedAssignment(CONNECT_PROTOCOL_V1, ConnectProtocol.Assignment.NO_ERROR, "member", LEADER_URL, 1L, Arrays.asList(connectorId2), Arrays.asList(taskId1x0, taskId3x0), Collections.emptyList(), Collections.emptyList(), 0);    ByteBuffer memberBuf = IncrementalCooperativeConnectProtocol.serializeAssignment(assignment2);    ConnectProtocol.Assignment memberAssignment = ConnectProtocol.deserializeAssignment(memberBuf);    assertEquals(false, memberAssignment.failed());    assertEquals("member", memberAssignment.leader());    assertEquals(1, memberAssignment.offset());    assertEquals(Collections.singletonList(connectorId2), memberAssignment.connectors());    assertEquals(Arrays.asList(taskId1x0, taskId3x0), memberAssignment.tasks());}
public void kafkatest_f10974_0()
{    EasyMock.expect(worker.connectorNames()).andReturn(Collections.singleton(CONN1));    worker.stopConnector(CONN1);    PowerMock.expectLastCall().andReturn(true);    EasyMock.expect(worker.taskIds()).andReturn(Collections.singleton(TASK1));    worker.stopAndAwaitTask(TASK1);    PowerMock.expectLastCall();    member.stop();    PowerMock.expectLastCall();    configBackingStore.stop();    PowerMock.expectLastCall();    statusBackingStore.stop();    PowerMock.expectLastCall();    worker.stop();    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.halt();    PowerMock.verifyAll();}
public void kafkatest_f10975_0() throws Exception
{    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    member.wakeup();    PowerMock.expectLastCall();    // config validation    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    EasyMock.expect(connectorMock.config()).andReturn(new ConfigDef());    EasyMock.expect(connectorMock.validate(CONN2_CONFIG)).andReturn(new Config(Collections.<ConfigValue>emptyList()));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    // CONN2 is new, should succeed    configBackingStore.putConnectorConfig(CONN2, CONN2_CONFIG);    PowerMock.expectLastCall();    ConnectorInfo info = new ConnectorInfo(CONN2, CONN2_CONFIG, Collections.<ConnectorTaskId>emptyList(), ConnectorType.SOURCE);    putConnectorCallback.onCompletion(null, new Herder.Created<>(true, info));    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // No immediate action besides this -- change will be picked up via the config log    PowerMock.replayAll();    herder.putConnectorConfig(CONN2, CONN2_CONFIG, false, putConnectorCallback);    herder.tick();    time.sleep(1000L);    assertStatistics(3, 1, 100, 1000L);    PowerMock.verifyAll();}
public void kafkatest_f10984_0() throws Exception
{    // get the initial assignment    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // now handle the connector restart    member.wakeup();    PowerMock.expectLastCall();    member.ensureActive();    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    String ownerUrl = "ownerUrl";    EasyMock.expect(member.ownerUrl(CONN1)).andReturn(ownerUrl);    PowerMock.replayAll();    herder.tick();    time.sleep(1000L);    assertStatistics(3, 1, 100, 1000L);    FutureCallback<Void> callback = new FutureCallback<>();    herder.restartConnector(CONN1, callback);    herder.tick();    time.sleep(2000L);    assertStatistics(3, 1, 100, 3000L);    try {        callback.get(1000L, TimeUnit.MILLISECONDS);        fail("Expected NotLeaderException to be raised");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof NotAssignedException);        NotAssignedException notAssignedException = (NotAssignedException) e.getCause();        assertEquals(ownerUrl, notAssignedException.forwardUrl());    }    PowerMock.verifyAll();}
public void kafkatest_f10985_0() throws Exception
{    EasyMock.expect(worker.connectorTaskConfigs(CONN1, conn1SinkConfig)).andStubReturn(TASK_CONFIGS);    // get the initial assignment    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));    expectPostRebalanceCatchup(SNAPSHOT);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    // now handle the task restart    member.wakeup();    PowerMock.expectLastCall();    member.ensureActive();    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    worker.stopAndAwaitTask(TASK0);    PowerMock.expectLastCall();    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    PowerMock.replayAll();    herder.tick();    FutureCallback<Void> callback = new FutureCallback<>();    herder.restartTask(TASK0, callback);    herder.tick();    callback.get(1000L, TimeUnit.MILLISECONDS);    PowerMock.verifyAll();}
public void kafkatest_f10994_0() throws Exception
{    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    EasyMock.expect(worker.connectorNames()).andStubReturn(Collections.singleton(CONN1));    // join    expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));    expectPostRebalanceCatchup(SNAPSHOT);    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // state change is ignored since we have no target state    member.wakeup();    member.ensureActive();    PowerMock.expectLastCall();    EasyMock.expect(configBackingStore.snapshot()).andReturn(SNAPSHOT);    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    // join    herder.tick();    configUpdateListener.onConnectorTargetStateChange("unknown-connector");    // continue    herder.tick();    PowerMock.verifyAll();}
public void kafkatest_f10995_0() throws Exception
{    // even if we don't own the connector, we should still propagate target state    // changes to the worker so that tasks will transition correctly    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    EasyMock.expect(worker.connectorNames()).andStubReturn(Collections.<String>emptySet());    // join    expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));    expectPostRebalanceCatchup(SNAPSHOT);    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // handle the state change    member.wakeup();    member.ensureActive();    PowerMock.expectLastCall();    EasyMock.expect(configBackingStore.snapshot()).andReturn(SNAPSHOT_PAUSED_CONN1);    PowerMock.expectLastCall();    worker.setTargetState(CONN1, TargetState.PAUSED);    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    // join    herder.tick();    // state changes to paused    configUpdateListener.onConnectorTargetStateChange(CONN1);    // apply state change    herder.tick();    PowerMock.verifyAll();}
private void kafkatest_f11004_0(final Collection<String> revokedConnectors, final List<ConnectorTaskId> revokedTasks, final short error, final long offset, final List<String> assignedConnectors, final List<ConnectorTaskId> assignedTasks)
{    expectRebalance(revokedConnectors, revokedTasks, error, offset, assignedConnectors, assignedTasks, 0);}
private void kafkatest_f11005_0(final Collection<String> revokedConnectors, final List<ConnectorTaskId> revokedTasks, final short error, final long offset, final List<String> assignedConnectors, final List<ConnectorTaskId> assignedTasks, int delay)
{    member.ensureActive();    PowerMock.expectLastCall().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            ExtendedAssignment assignment;            if (!revokedConnectors.isEmpty() || !revokedTasks.isEmpty()) {                rebalanceListener.onRevoked("leader", revokedConnectors, revokedTasks);            }            if (connectProtocolVersion == CONNECT_PROTOCOL_V0) {                assignment = new ExtendedAssignment(connectProtocolVersion, error, "leader", "leaderUrl", offset, assignedConnectors, assignedTasks, Collections.emptyList(), Collections.emptyList(), 0);            } else {                assignment = new ExtendedAssignment(connectProtocolVersion, error, "leader", "leaderUrl", offset, assignedConnectors, assignedTasks, new ArrayList<>(revokedConnectors), new ArrayList<>(revokedTasks), delay);            }            rebalanceListener.onAssigned(assignment, 3);            time.sleep(100L);            return null;        }    });    if (!revokedConnectors.isEmpty()) {        for (String connector : revokedConnectors) {            worker.stopConnector(connector);            PowerMock.expectLastCall().andReturn(true);        }    }    if (!revokedTasks.isEmpty()) {        worker.stopAndAwaitTask(EasyMock.anyObject(ConnectorTaskId.class));        PowerMock.expectLastCall();    }    if (!revokedConnectors.isEmpty()) {        statusBackingStore.flush();        PowerMock.expectLastCall();    }    member.wakeup();    PowerMock.expectLastCall();}
public void kafkatest_f11014_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    when(coordinator.configSnapshot()).thenReturn(configState);    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    // First assignment with 2 workers and 2 connectors configured but not yet assigned    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(2, 8, 0, 0, "worker1", "worker2");    // Second assignment with only one worker remaining in the group. The worker that left the    // group was a follower. No re-assignments take place immediately and the count    // down for the rebalance delay starts    applyAssignments(returnedAssignments);    assignments.remove("worker2");    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1");    time.sleep(rebalanceDelay / 2);    // Third (incidental) assignment with still only one worker in the group. Max delay has not    // been reached yet    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay / 2, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1");    time.sleep(rebalanceDelay / 2 + 1);    // Fourth assignment after delay expired    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(1, 4, 0, 0, "worker1");    verify(coordinator, times(rebalanceNum)).configSnapshot();    verify(coordinator, times(rebalanceNum)).leaderState(any());}
public void kafkatest_f11015_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    when(coordinator.configSnapshot()).thenReturn(configState);    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    // First assignment with 2 workers and 2 connectors configured but not yet assigned    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(2, 8, 0, 0, "worker1", "worker2");    // Second assignment with only one worker remaining in the group. The worker that left the    // group was a follower. No re-assignments take place immediately and the count    // down for the rebalance delay starts    applyAssignments(returnedAssignments);    assignments.remove("worker2");    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1");    time.sleep(rebalanceDelay / 2);    // Third (incidental) assignment with still only one worker in the group. Max delay has not    // been reached yet    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay / 2, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1");    time.sleep(rebalanceDelay / 4);    // Fourth assignment with the second worker returning before the delay expires    // Since the delay is still active, lost assignments are not reassigned yet    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay / 4, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1", "worker2");    time.sleep(rebalanceDelay / 4);    // Fifth assignment with the same two workers. The delay has expired, so the lost    // assignments ought to be assigned to the worker that has appeared as returned.    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(1, 4, 0, 0, "worker1", "worker2");    verify(coordinator, times(rebalanceNum)).configSnapshot();    verify(coordinator, times(rebalanceNum)).leaderState(any());}
public void kafkatest_f11024_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    assertTrue(assignor.candidateWorkersForReassignment.isEmpty());    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    Map<String, WorkerLoad> configuredAssignment = new HashMap<>();    configuredAssignment.put("worker0", workerLoad("worker0", 0, 2, 0, 4));    configuredAssignment.put("worker2", workerLoad("worker2", 4, 2, 8, 4));    ConnectorsAndTasks newSubmissions = new ConnectorsAndTasks.Builder().build();    // No lost assignments    assignor.handleLostAssignments(new ConnectorsAndTasks.Builder().build(), newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    String flakyWorker = "worker1";    WorkerLoad lostLoad = workerLoad(flakyWorker, 2, 2, 4, 4);    ConnectorsAndTasks lostAssignments = new ConnectorsAndTasks.Builder().withCopies(lostLoad.connectors(), lostLoad.tasks()).build();    // Lost assignments detected - No candidate worker has appeared yet (worker with no assignments)    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay / 2);    rebalanceDelay /= 2;    // A new worker (probably returning worker) has joined    configuredAssignment.put(flakyWorker, new WorkerLoad.Builder(flakyWorker).build());    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.singleton(flakyWorker), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay);    // The new worker has still no assignments    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertTrue("Wrong assignment of lost connectors", configuredAssignment.getOrDefault(flakyWorker, new WorkerLoad.Builder(flakyWorker).build()).connectors().containsAll(lostAssignments.connectors()));    assertTrue("Wrong assignment of lost tasks", configuredAssignment.getOrDefault(flakyWorker, new WorkerLoad.Builder(flakyWorker).build()).tasks().containsAll(lostAssignments.tasks()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);}
public void kafkatest_f11025_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    assertTrue(assignor.candidateWorkersForReassignment.isEmpty());    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    Map<String, WorkerLoad> configuredAssignment = new HashMap<>();    configuredAssignment.put("worker0", workerLoad("worker0", 0, 2, 0, 4));    configuredAssignment.put("worker2", workerLoad("worker2", 4, 2, 8, 4));    ConnectorsAndTasks newSubmissions = new ConnectorsAndTasks.Builder().build();    // No lost assignments    assignor.handleLostAssignments(new ConnectorsAndTasks.Builder().build(), newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    String removedWorker = "worker1";    WorkerLoad lostLoad = workerLoad(removedWorker, 2, 2, 4, 4);    ConnectorsAndTasks lostAssignments = new ConnectorsAndTasks.Builder().withCopies(lostLoad.connectors(), lostLoad.tasks()).build();    // Lost assignments detected - No candidate worker has appeared yet (worker with no assignments)    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay / 2);    rebalanceDelay /= 2;    // No new worker has joined    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay);    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertTrue("Wrong assignment of lost connectors", newSubmissions.connectors().containsAll(lostAssignments.connectors()));    assertTrue("Wrong assignment of lost tasks", newSubmissions.tasks().containsAll(lostAssignments.tasks()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);}
private static Map<String, ExtendedWorkerState> kafkatest_f11034_0(String givenLeader, long givenOffset, int start, int connectorNum)
{    return IntStream.range(start, connectorNum + 1).mapToObj(i -> new SimpleEntry<>("worker" + i, new ExtendedWorkerState(expectedLeaderUrl(givenLeader), givenOffset, null))).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
private static Map<String, Integer> kafkatest_f11035_0(int start, int connectorNum, int taskCounts)
{    return IntStream.range(start, connectorNum + 1).mapToObj(i -> new SimpleEntry<>("connector" + i, taskCounts)).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
private void kafkatest_f11044_0(int expectedDelay, Map<String, ExtendedAssignment> newAssignments)
{    newAssignments.values().stream().forEach(a -> assertEquals("Wrong rebalance delay in " + a, expectedDelay, a.delay()));}
private void kafkatest_f11045_0(Map<String, ExtendedWorkerState> existingAssignments, Map<String, ExtendedWorkerState> newAssignments)
{    assertNoDuplicateInAssignment(existingAssignments);    assertNoDuplicateInAssignment(newAssignments);    List<String> existingConnectors = existingAssignments.values().stream().flatMap(a -> a.assignment().connectors().stream()).collect(Collectors.toList());    List<String> newConnectors = newAssignments.values().stream().flatMap(a -> a.assignment().connectors().stream()).collect(Collectors.toList());    List<ConnectorTaskId> existingTasks = existingAssignments.values().stream().flatMap(a -> a.assignment().tasks().stream()).collect(Collectors.toList());    List<ConnectorTaskId> newTasks = newAssignments.values().stream().flatMap(a -> a.assignment().tasks().stream()).collect(Collectors.toList());    existingConnectors.retainAll(newConnectors);    assertThat("Found connectors in new assignment that already exist in current assignment", Collections.emptyList(), is(existingConnectors));    existingTasks.retainAll(newTasks);    assertThat("Found tasks in new assignment that already exist in current assignment", Collections.emptyList(), is(existingConnectors));}
public void kafkatest_f11054_0()
{    when(configStorage.snapshot()).thenReturn(configState1);    coordinator.metadata();    ++configStorageCalls;    List<JoinGroupResponseMember> responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, null);    addJoinGroupResponseMember(responseMembers, memberId, offset, null);    Map<String, ByteBuffer> result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    ExtendedAssignment leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId1), 4, Collections.emptyList(), 0, leaderAssignment);    ExtendedAssignment memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId2), 4, Collections.emptyList(), 0, memberAssignment);    coordinator.metadata();    ++configStorageCalls;    responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, leaderAssignment);    addJoinGroupResponseMember(responseMembers, memberId, offset, memberAssignment);    addJoinGroupResponseMember(responseMembers, anotherMemberId, offset, null);    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 2, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, memberAssignment);    ExtendedAssignment anotherMemberAssignment = deserializeAssignment(result, anotherMemberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, anotherMemberAssignment);    verify(configStorage, times(configStorageCalls)).snapshot();}
public void kafkatest_f11055_0()
{    when(configStorage.snapshot()).thenReturn(configState1);    // First assignment distributes configured connectors and tasks    coordinator.metadata();    ++configStorageCalls;    List<JoinGroupResponseMember> responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, null);    addJoinGroupResponseMember(responseMembers, memberId, offset, null);    addJoinGroupResponseMember(responseMembers, anotherMemberId, offset, null);    Map<String, ByteBuffer> result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    ExtendedAssignment leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId1), 3, Collections.emptyList(), 0, leaderAssignment);    ExtendedAssignment memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId2), 3, Collections.emptyList(), 0, memberAssignment);    ExtendedAssignment anotherMemberAssignment = deserializeAssignment(result, anotherMemberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 2, Collections.emptyList(), 0, anotherMemberAssignment);    // Second rebalance detects a worker is missing    coordinator.metadata();    ++configStorageCalls;    // Mark everyone as in sync with configState1    responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, leaderAssignment);    addJoinGroupResponseMember(responseMembers, memberId, offset, memberAssignment);    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, memberAssignment);    rebalanceDelay /= 2;    time.sleep(rebalanceDelay);    // A third rebalance before the delay expires won't change the assignments    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, memberAssignment);    time.sleep(rebalanceDelay + 1);    // A rebalance after the delay expires re-assigns the lost tasks    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 1, Collections.emptyList(), 0, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 1, Collections.emptyList(), 0, memberAssignment);    verify(configStorage, times(configStorageCalls)).snapshot();}
public void kafkatest_f11064_0()
{    EasyMock.expect(configStorage.snapshot()).andReturn(configState1);    PowerMock.replayAll();    JoinGroupRequestData.JoinGroupRequestProtocolCollection serialized = coordinator.metadata();    assertEquals(expectedMetadataSize, serialized.size());    Iterator<JoinGroupRequestData.JoinGroupRequestProtocol> protocolIterator = serialized.iterator();    assertTrue(protocolIterator.hasNext());    JoinGroupRequestData.JoinGroupRequestProtocol defaultMetadata = protocolIterator.next();    assertEquals(compatibility.protocol(), defaultMetadata.name());    ConnectProtocol.WorkerState state = ConnectProtocol.deserializeMetadata(ByteBuffer.wrap(defaultMetadata.metadata()));    assertEquals(1, state.offset());    PowerMock.verifyAll();}
public void kafkatest_f11065_0()
{    EasyMock.expect(configStorage.snapshot()).andReturn(configState1);    PowerMock.replayAll();    final String consumerId = "leader";    client.prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, node));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // normal join group    Map<String, Long> memberConfigOffsets = new HashMap<>();    memberConfigOffsets.put("leader", 1L);    memberConfigOffsets.put("member", 1L);    client.prepareResponse(joinGroupLeaderResponse(1, consumerId, memberConfigOffsets, Errors.NONE));    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            SyncGroupRequest sync = (SyncGroupRequest) body;            return sync.data.memberId().equals(consumerId) && sync.data.generationId() == 1 && sync.groupAssignments().containsKey(consumerId);        }    }, syncGroupResponse(ConnectProtocol.Assignment.NO_ERROR, "leader", 1L, Collections.singletonList(connectorId1), Collections.<ConnectorTaskId>emptyList(), Errors.NONE));    coordinator.ensureActiveGroup();    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(0, rebalanceListener.revokedCount);    assertEquals(1, rebalanceListener.assignedCount);    assertFalse(rebalanceListener.assignment.failed());    assertEquals(1L, rebalanceListener.assignment.offset());    assertEquals("leader", rebalanceListener.assignment.leader());    assertEquals(Collections.singletonList(connectorId1), rebalanceListener.assignment.connectors());    assertEquals(Collections.emptyList(), rebalanceListener.assignment.tasks());    PowerMock.verifyAll();}
public void kafkatest_f11074_0() throws Exception
{    // Since all the protocol responses are mocked, the other tests validate doSync runs, but don't validate its    // output. So we test it directly here.    EasyMock.expect(configStorage.snapshot()).andReturn(configStateSingleTaskConnectors);    PowerMock.replayAll();    // Prime the current configuration state    coordinator.metadata();    // Mark everyone as in sync with configState1    List<JoinGroupResponseData.JoinGroupResponseMember> responseMembers = new ArrayList<>();    responseMembers.add(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId("leader").setMetadata(ConnectProtocol.serializeMetadata(new ConnectProtocol.WorkerState(LEADER_URL, 1L)).array()));    responseMembers.add(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId("member").setMetadata(ConnectProtocol.serializeMetadata(new ConnectProtocol.WorkerState(MEMBER_URL, 1L)).array()));    Map<String, ByteBuffer> result = coordinator.performAssignment("leader", WorkerCoordinator.DEFAULT_SUBPROTOCOL, responseMembers);    // Round robin assignment when there are the same number of connectors and tasks should result in each being    // evenly distributed across the workers, i.e. round robin assignment of connectors first, then followed by tasks    ConnectProtocol.Assignment leaderAssignment = ConnectProtocol.deserializeAssignment(result.get("leader"));    assertEquals(false, leaderAssignment.failed());    assertEquals("leader", leaderAssignment.leader());    assertEquals(1, leaderAssignment.offset());    assertEquals(Arrays.asList(connectorId1, connectorId3), leaderAssignment.connectors());    assertEquals(Arrays.asList(taskId2x0), leaderAssignment.tasks());    ConnectProtocol.Assignment memberAssignment = ConnectProtocol.deserializeAssignment(result.get("member"));    assertEquals(false, memberAssignment.failed());    assertEquals("leader", memberAssignment.leader());    assertEquals(1, memberAssignment.offset());    assertEquals(Collections.singletonList(connectorId2), memberAssignment.connectors());    assertEquals(Arrays.asList(taskId1x0, taskId3x0), memberAssignment.tasks());    PowerMock.verifyAll();}
private JoinGroupResponse kafkatest_f11075_0(int generationId, String memberId, Map<String, Long> configOffsets, Errors error)
{    List<JoinGroupResponseData.JoinGroupResponseMember> metadata = new ArrayList<>();    for (Map.Entry<String, Long> configStateEntry : configOffsets.entrySet()) {        // We need a member URL, but it doesn't matter for the purposes of this test. Just set it to the member ID        String memberUrl = configStateEntry.getKey();        long configOffset = configStateEntry.getValue();        ByteBuffer buf = ConnectProtocol.serializeMetadata(new ConnectProtocol.WorkerState(memberUrl, configOffset));        metadata.add(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId(configStateEntry.getKey()).setMetadata(buf.array()));    }    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(error.code()).setGenerationId(generationId).setProtocolName(WorkerCoordinator.DEFAULT_SUBPROTOCOL).setLeader(memberId).setMemberId(memberId).setMembers(metadata));}
public void kafkatest_f11084_0() throws Exception
{    Map<String, String> reportProps = new HashMap<>();    reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, "true");    reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, "true");    LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);    RetryWithToleranceOperator retryWithToleranceOperator = operator();    retryWithToleranceOperator.metrics(errorHandlingMetrics);    retryWithToleranceOperator.reporters(singletonList(reporter));    createSourceTask(initialState, retryWithToleranceOperator);    // valid json    Schema valSchema = SchemaBuilder.struct().field("val", Schema.INT32_SCHEMA).build();    Struct struct1 = new Struct(valSchema).put("val", 1234);    SourceRecord record1 = new SourceRecord(emptyMap(), emptyMap(), TOPIC, PARTITION1, valSchema, struct1);    Struct struct2 = new Struct(valSchema).put("val", 6789);    SourceRecord record2 = new SourceRecord(emptyMap(), emptyMap(), TOPIC, PARTITION1, valSchema, struct2);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(false);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(false);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(true);    EasyMock.expect(workerSourceTask.commitOffsets()).andReturn(true);    offsetWriter.offset(EasyMock.anyObject(), EasyMock.anyObject());    EasyMock.expectLastCall().times(2);    sourceTask.initialize(EasyMock.anyObject());    EasyMock.expectLastCall();    sourceTask.start(EasyMock.anyObject());    EasyMock.expectLastCall();    EasyMock.expect(sourceTask.poll()).andReturn(singletonList(record1));    EasyMock.expect(sourceTask.poll()).andReturn(singletonList(record2));    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andReturn(null).times(2);    PowerMock.replayAll();    workerSourceTask.initialize(TASK_CONFIG);    workerSourceTask.execute();    // two records were consumed from Kafka    assertSourceMetricValue("source-record-poll-total", 2.0);    // only one was written to the task    assertSourceMetricValue("source-record-write-total", 0.0);    // one record completely failed (converter issues)    assertErrorHandlingMetricValue("total-record-errors", 0.0);    // 2 failures in the transformation, and 1 in the converter    assertErrorHandlingMetricValue("total-record-failures", 4.0);    // one record completely failed (converter issues), and thus was skipped    assertErrorHandlingMetricValue("total-records-skipped", 0.0);    PowerMock.verifyAll();}
private ConnectorConfig kafkatest_f11085_0(Map<String, String> connProps)
{    Map<String, String> props = new HashMap<>();    props.put(ConnectorConfig.NAME_CONFIG, "test");    props.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, SinkTask.class.getName());    props.putAll(connProps);    return new ConnectorConfig(plugins, props);}
private void kafkatest_f11094_0(TargetState initialState, RetryWithToleranceOperator retryWithToleranceOperator, Converter converter)
{    TransformationChain<SourceRecord> sourceTransforms = new TransformationChain<>(singletonList(new FaultyPassthrough<SourceRecord>()), retryWithToleranceOperator);    workerSourceTask = PowerMock.createPartialMock(WorkerSourceTask.class, new String[] { "commitOffsets", "isStopping" }, taskId, sourceTask, statusListener, initialState, converter, converter, headerConverter, sourceTransforms, producer, offsetReader, offsetWriter, workerConfig, ClusterConfigState.EMPTY, metrics, pluginLoader, time, retryWithToleranceOperator);}
private ConsumerRecords<byte[], byte[]> kafkatest_f11095_0(ConsumerRecord<byte[], byte[]> record)
{    return new ConsumerRecords<>(Collections.singletonMap(new TopicPartition(record.topic(), record.partition()), singletonList(record)));}
public void kafkatest_f11104_0()
{    DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(emptyMap()), TASK_ID, errorHandlingMetrics);    ProcessingContext context = processingContext();    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andThrow(new RuntimeException());    replay(producer);    // since topic name is empty, this method should be a NOOP.    // if it attempts to log to the DLQ via the producer, the send mock will throw a RuntimeException.    deadLetterQueueReporter.report(context);}
public void kafkatest_f11105_0()
{    DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);    ProcessingContext context = processingContext();    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andReturn(metadata);    replay(producer);    deadLetterQueueReporter.report(context);    PowerMock.verifyAll();}
public void kafkatest_f11114_0()
{    Map<String, String> props = new HashMap<>();    props.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);    props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, "true");    DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);    ProcessingContext context = new ProcessingContext();    context.consumerRecord(new ConsumerRecord<>("source-topic", 7, 10, "source-key".getBytes(), "source-value".getBytes()));    context.currentContext(Stage.TRANSFORMATION, Transformation.class);    context.error(new ConnectException("Test Exception"));    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(DLQ_TOPIC, "source-key".getBytes(), "source-value".getBytes());    producerRecord.headers().add(ERROR_HEADER_ORIG_TOPIC, "dummy".getBytes());    deadLetterQueueReporter.populateContextHeaders(producerRecord, context);    int appearances = 0;    for (Header header : producerRecord.headers()) {        if (ERROR_HEADER_ORIG_TOPIC.equalsIgnoreCase(header.key())) {            appearances++;        }    }    assertEquals("source-topic", headerValue(producerRecord, ERROR_HEADER_ORIG_TOPIC));    assertEquals(2, appearances);}
private String kafkatest_f11115_0(ProducerRecord<byte[], byte[]> producerRecord, String headerSuffix)
{    return new String(producerRecord.headers().lastHeader(headerSuffix).value());}
public void kafkatest_f11124_0()
{    testHandleExceptionInStage(Stage.TASK_POLL, new org.apache.kafka.connect.errors.RetriableException("Test"));}
public void kafkatest_f11125_0()
{    testHandleExceptionInStage(Stage.TASK_PUT, new Exception());}
public void kafkatest_f11134_0() throws Exception
{    execAndHandleNonRetriableError(3, 0, new Exception("Non Retriable Test"));}
public void kafkatest_f11135_0(int numRetriableExceptionsThrown, long expectedWait, Exception e) throws Exception
{    MockTime time = new MockTime(0, 0, 0);    RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(6000, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time);    retryWithToleranceOperator.metrics(errorHandlingMetrics);    EasyMock.expect(mockOperation.call()).andThrow(e).times(numRetriableExceptionsThrown);    EasyMock.expect(mockOperation.call()).andReturn("Success");    replay(mockOperation);    String result = retryWithToleranceOperator.execAndHandleError(mockOperation, Exception.class);    assertFalse(retryWithToleranceOperator.failed());    assertEquals("Success", result);    assertEquals(expectedWait, time.hiResClockMs());    PowerMock.verifyAll();}
public void kafkatest_f11144_0()
{    expectedConnectors = Arrays.asList("sink1", "source1", "source2");    connectClusterState = new ConnectClusterStateImpl(herderRequestTimeoutMs, new ConnectClusterDetailsImpl(KAFKA_CLUSTER_ID), herder);}
public void kafkatest_f11145_0()
{    Capture<Callback<Collection<String>>> callback = EasyMock.newCapture();    herder.connectors(EasyMock.capture(callback));    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() {            callback.getValue().onCompletion(null, expectedConnectors);            return null;        }    });    EasyMock.replay(herder);    assertEquals(expectedConnectors, connectClusterState.connectors());}
public void kafkatest_f11154_0() throws Exception
{    // Fairly simple use case, thus no need to create a random directory here yet.    URL location = Paths.get("/tmp").toUri().toURL();    // Normally parent will be a DelegatingClassLoader.    pluginLoader = new PluginClassLoader(location, new URL[0], systemLoader);}
public void kafkatest_f11155_0()
{    PluginDesc<Connector> connectorDesc = new PluginDesc<>(Connector.class, regularVersion, pluginLoader);    assertPluginDesc(connectorDesc, Connector.class, regularVersion, pluginLoader.location());    PluginDesc<Converter> converterDesc = new PluginDesc<>(Converter.class, snaphotVersion, pluginLoader);    assertPluginDesc(converterDesc, Converter.class, snaphotVersion, pluginLoader.location());    PluginDesc<Transformation> transformDesc = new PluginDesc<>(Transformation.class, noVersion, pluginLoader);    assertPluginDesc(transformDesc, Transformation.class, noVersion, pluginLoader.location());}
protected void kafkatest_f11164_0()
{    this.config = new TestableWorkerConfig(props);}
public void kafkatest_f11165_0()
{    instantiateAndConfigureConverter(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);    // Validate extra configs got passed through to overridden converters    assertEquals("true", converter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));    assertEquals("foo1", converter.configs.get("extra.config"));    instantiateAndConfigureConverter(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);    // Validate extra configs got passed through to overridden converters    assertEquals("true", converter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));    assertEquals("foo2", converter.configs.get("extra.config"));}
public ConfigDef kafkatest_f11174_0()
{    return JsonConverterConfig.configDef();}
public void kafkatest_f11175_0(Map<String, ?> configs)
{    this.configs = configs;    // requires the `converter.type` config be set    new JsonConverterConfig(configs);}
public String kafkatest_f11187_0()
{    return "test";}
public void kafkatest_f11188_0(Map<String, ?> configs)
{    this.configs = configs;    super.configure(configs);}
public void kafkatest_f11197_0() throws Exception
{    createBasicDirectoryLayout();    assertEquals(Collections.<Path>emptyList(), PluginUtils.pluginUrls(pluginPath));}
public void kafkatest_f11198_0() throws Exception
{    createBasicDirectoryLayout();    List<Path> expectedUrls = createBasicExpectedUrls();    assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));}
private void kafkatest_f11207_0(List<Path> expected, List<Path> actual)
{    Collections.sort(expected);    // not sorting 'actual' because it should be returned sorted from withing the PluginUtils.    assertEquals(expected, actual);}
public MockTime kafkatest_f11208_0()
{    return (MockTime) super.time();}
public void kafkatest_f11217_0(KafkaMetric metric)
{    metricsByName.put(metric.metricName(), metric);}
public void kafkatest_f11218_0(KafkaMetric metric)
{// don't remove metrics, or else we won't be able to access them after the metric metricGroup is closed}
public void kafkatest_f11227_0() throws Throwable
{    herder.validateConnectorConfig(EasyMock.eq(props));    PowerMock.expectLastCall().andAnswer((IAnswer<ConfigInfos>) () -> {        ConfigDef connectorConfigDef = ConnectorConfig.configDef();        List<ConfigValue> connectorConfigValues = connectorConfigDef.validate(props);        Connector connector = new ConnectorPluginsResourceTestConnector();        Config config = connector.validate(props);        ConfigDef configDef = connector.config();        Map<String, ConfigDef.ConfigKey> configKeys = configDef.configKeys();        List<ConfigValue> configValues = config.configValues();        Map<String, ConfigDef.ConfigKey> resultConfigKeys = new HashMap<>(configKeys);        resultConfigKeys.putAll(connectorConfigDef.configKeys());        configValues.addAll(connectorConfigValues);        return AbstractHerder.generateResult(ConnectorPluginsResourceTestConnector.class.getName(), resultConfigKeys, configValues, Collections.singletonList("Test"));    });    PowerMock.replayAll();    // make a request to connector-plugins resource using a valid alias.    ConfigInfos configInfos = connectorPluginsResource.validateConfigs("ConnectorPluginsResourceTest", props);    assertEquals(CONFIG_INFOS.name(), configInfos.name());    assertEquals(0, configInfos.errorCount());    assertEquals(CONFIG_INFOS.groups(), configInfos.groups());    assertEquals(new HashSet<>(CONFIG_INFOS.values()), new HashSet<>(configInfos.values()));    PowerMock.verifyAll();}
public void kafkatest_f11228_0() throws Throwable
{    herder.validateConnectorConfig(EasyMock.eq(props));    PowerMock.expectLastCall().andAnswer((IAnswer<ConfigInfos>) () -> {        ConfigDef connectorConfigDef = ConnectorConfig.configDef();        List<ConfigValue> connectorConfigValues = connectorConfigDef.validate(props);        Connector connector = new ConnectorPluginsResourceTestConnector();        Config config = connector.validate(props);        ConfigDef configDef = connector.config();        Map<String, ConfigDef.ConfigKey> configKeys = configDef.configKeys();        List<ConfigValue> configValues = config.configValues();        Map<String, ConfigDef.ConfigKey> resultConfigKeys = new HashMap<>(configKeys);        resultConfigKeys.putAll(connectorConfigDef.configKeys());        configValues.addAll(connectorConfigValues);        return AbstractHerder.generateResult(ConnectorPluginsResourceTestConnector.class.getName(), resultConfigKeys, configValues, Collections.singletonList("Test"));    });    PowerMock.replayAll();    // make a request to connector-plugins resource using a non-loaded connector with the same    // simple name but different package.    String customClassname = "com.custom.package." + ConnectorPluginsResourceTestConnector.class.getSimpleName();    connectorPluginsResource.validateConfigs(customClassname, props);    PowerMock.verifyAll();}
public List<Map<String, String>> kafkatest_f11238_0(int maxTasks)
{    return null;}
public ConfigDef kafkatest_f11240_0()
{    return CONFIG_DEF;}
public void kafkatest_f11249_0() throws Throwable
{    EasyMock.expect(herder.connectors()).andReturn(Arrays.asList(CONNECTOR2_NAME, CONNECTOR_NAME));    ConnectorInfo connector = EasyMock.mock(ConnectorInfo.class);    ConnectorInfo connector2 = EasyMock.mock(ConnectorInfo.class);    EasyMock.expect(herder.connectorInfo(CONNECTOR2_NAME)).andReturn(connector2);    EasyMock.expect(herder.connectorInfo(CONNECTOR_NAME)).andReturn(connector);    forward = EasyMock.mock(UriInfo.class);    MultivaluedMap<String, String> queryParams = new MultivaluedHashMap<>();    queryParams.putSingle("expand", "info");    EasyMock.expect(forward.getQueryParameters()).andReturn(queryParams).anyTimes();    EasyMock.replay(forward);    PowerMock.replayAll();    Map<String, Map<String, Object>> expanded = (Map<String, Map<String, Object>>) connectorsResource.listConnectors(forward, NULL_HEADERS).getEntity();    // Ordering isn't guaranteed, compare sets    assertEquals(new HashSet<>(Arrays.asList(CONNECTOR_NAME, CONNECTOR2_NAME)), expanded.keySet());    assertEquals(connector2, expanded.get(CONNECTOR2_NAME).get("info"));    assertEquals(connector, expanded.get(CONNECTOR_NAME).get("info"));    PowerMock.verifyAll();}
public void kafkatest_f11250_0() throws Throwable
{    EasyMock.expect(herder.connectors()).andReturn(Arrays.asList(CONNECTOR2_NAME, CONNECTOR_NAME));    ConnectorInfo connectorInfo = EasyMock.mock(ConnectorInfo.class);    ConnectorInfo connectorInfo2 = EasyMock.mock(ConnectorInfo.class);    EasyMock.expect(herder.connectorInfo(CONNECTOR2_NAME)).andReturn(connectorInfo2);    EasyMock.expect(herder.connectorInfo(CONNECTOR_NAME)).andReturn(connectorInfo);    ConnectorStateInfo connector = EasyMock.mock(ConnectorStateInfo.class);    ConnectorStateInfo connector2 = EasyMock.mock(ConnectorStateInfo.class);    EasyMock.expect(herder.connectorStatus(CONNECTOR2_NAME)).andReturn(connector2);    EasyMock.expect(herder.connectorStatus(CONNECTOR_NAME)).andReturn(connector);    forward = EasyMock.mock(UriInfo.class);    MultivaluedMap<String, String> queryParams = new MultivaluedHashMap<>();    queryParams.put("expand", Arrays.asList("info", "status"));    EasyMock.expect(forward.getQueryParameters()).andReturn(queryParams).anyTimes();    EasyMock.replay(forward);    PowerMock.replayAll();    Map<String, Map<String, Object>> expanded = (Map<String, Map<String, Object>>) connectorsResource.listConnectors(forward, NULL_HEADERS).getEntity();    // Ordering isn't guaranteed, compare sets    assertEquals(new HashSet<>(Arrays.asList(CONNECTOR_NAME, CONNECTOR2_NAME)), expanded.keySet());    assertEquals(connectorInfo2, expanded.get(CONNECTOR2_NAME).get("info"));    assertEquals(connectorInfo, expanded.get(CONNECTOR_NAME).get("info"));    assertEquals(connector2, expanded.get(CONNECTOR2_NAME).get("status"));    assertEquals(connector, expanded.get(CONNECTOR_NAME).get("status"));    PowerMock.verifyAll();}
public void kafkatest_f11259_0() throws Throwable
{    // Clone CONNECTOR_CONFIG_WITHOUT_NAME Map, as createConnector changes it (puts the name in it) and this    // will affect later tests    Map<String, String> inputConfig = getConnectorConfig(CONNECTOR_CONFIG_WITHOUT_NAME);    final CreateConnectorRequest bodyIn = new CreateConnectorRequest(null, inputConfig);    final CreateConnectorRequest bodyOut = new CreateConnectorRequest("", CONNECTOR_CONFIG_WITH_EMPTY_NAME);    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.putConnectorConfig(EasyMock.eq(bodyOut.name()), EasyMock.eq(bodyOut.config()), EasyMock.eq(false), EasyMock.capture(cb));    expectAndCallbackResult(cb, new Herder.Created<>(true, new ConnectorInfo(bodyOut.name(), bodyOut.config(), CONNECTOR_TASK_NAMES, ConnectorType.SOURCE)));    PowerMock.replayAll();    connectorsResource.createConnector(FORWARD, NULL_HEADERS, bodyIn);    PowerMock.verifyAll();}
public void kafkatest_f11260_0() throws Throwable
{    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.deleteConnectorConfig(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(cb));    expectAndCallbackResult(cb, null);    PowerMock.replayAll();    connectorsResource.destroyConnector(CONNECTOR_NAME, NULL_HEADERS, FORWARD);    PowerMock.verifyAll();}
public void kafkatest_f11269_0() throws Throwable
{    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.putConnectorConfig(EasyMock.eq(CONNECTOR_NAME_SPECIAL_CHARS), EasyMock.eq(CONNECTOR_CONFIG_SPECIAL_CHARS), EasyMock.eq(true), EasyMock.capture(cb));    expectAndCallbackResult(cb, new Herder.Created<>(true, new ConnectorInfo(CONNECTOR_NAME_SPECIAL_CHARS, CONNECTOR_CONFIG_SPECIAL_CHARS, CONNECTOR_TASK_NAMES, ConnectorType.SINK)));    PowerMock.replayAll();    String rspLocation = connectorsResource.putConnectorConfig(CONNECTOR_NAME_SPECIAL_CHARS, NULL_HEADERS, FORWARD, CONNECTOR_CONFIG_SPECIAL_CHARS).getLocation().toString();    String decoded = new URI(rspLocation).getPath();    Assert.assertEquals("/connectors/" + CONNECTOR_NAME_SPECIAL_CHARS, decoded);    PowerMock.verifyAll();}
public void kafkatest_f11270_0() throws Throwable
{    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.putConnectorConfig(EasyMock.eq(CONNECTOR_NAME_CONTROL_SEQUENCES1), EasyMock.eq(CONNECTOR_CONFIG_CONTROL_SEQUENCES), EasyMock.eq(true), EasyMock.capture(cb));    expectAndCallbackResult(cb, new Herder.Created<>(true, new ConnectorInfo(CONNECTOR_NAME_CONTROL_SEQUENCES1, CONNECTOR_CONFIG_CONTROL_SEQUENCES, CONNECTOR_TASK_NAMES, ConnectorType.SINK)));    PowerMock.replayAll();    String rspLocation = connectorsResource.putConnectorConfig(CONNECTOR_NAME_CONTROL_SEQUENCES1, NULL_HEADERS, FORWARD, CONNECTOR_CONFIG_CONTROL_SEQUENCES).getLocation().toString();    String decoded = new URI(rspLocation).getPath();    Assert.assertEquals("/connectors/" + CONNECTOR_NAME_CONTROL_SEQUENCES1, decoded);    PowerMock.verifyAll();}
public void kafkatest_f11279_0() throws Throwable
{    final Capture<Callback<Void>> cb = Capture.newInstance();    herder.restartConnector(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(cb));    String ownerUrl = "http://owner:8083";    expectAndCallbackException(cb, new NotAssignedException("not owner test", ownerUrl));    EasyMock.expect(RestClient.httpRequest(EasyMock.eq("http://owner:8083/connectors/" + CONNECTOR_NAME + "/restart?forward=false"), EasyMock.eq("POST"), EasyMock.isNull(), EasyMock.isNull(), EasyMock.<TypeReference>anyObject(), EasyMock.anyObject(WorkerConfig.class))).andReturn(new RestClient.HttpResponse<>(202, new HashMap<String, String>(), null));    PowerMock.replayAll();    connectorsResource.restartConnector(CONNECTOR_NAME, NULL_HEADERS, true);    PowerMock.verifyAll();}
public void kafkatest_f11280_0() throws Throwable
{    ConnectorTaskId taskId = new ConnectorTaskId(CONNECTOR_NAME, 0);    final Capture<Callback<Void>> cb = Capture.newInstance();    herder.restartTask(EasyMock.eq(taskId), EasyMock.capture(cb));    expectAndCallbackException(cb, new NotFoundException("not found"));    PowerMock.replayAll();    connectorsResource.restartTask(CONNECTOR_NAME, 0, NULL_HEADERS, FORWARD);    PowerMock.verifyAll();}
public void kafkatest_f11289_0()
{    EasyMock.expect(herder.kafkaClusterId()).andReturn(MockAdminClient.DEFAULT_CLUSTER_ID);    replayAll();    ServerInfo info = rootResource.serverInfo();    assertEquals(AppInfoParser.getVersion(), info.version());    assertEquals(AppInfoParser.getCommitId(), info.commit());    assertEquals(MockAdminClient.DEFAULT_CLUSTER_ID, info.clusterId());    verifyAll();}
public void kafkatest_f11290_0()
{    server.stop();}
public void kafkatest_f11299_0()
{    String existingKey = "exists";    String missingKey = "missing";    String value = "value";    String defaultValue = "default";    Map<String, Object> map = new HashMap<>();    map.put("exists", "value");    Assert.assertEquals(SSLUtils.getOrDefault(map, existingKey, defaultValue), value);    Assert.assertEquals(SSLUtils.getOrDefault(map, missingKey, defaultValue), defaultValue);}
public void kafkatest_f11300_0()
{    Map<String, String> configMap = new HashMap<>(DEFAULT_CONFIG);    configMap.put("ssl.keystore.location", "/path/to/keystore");    configMap.put("ssl.keystore.password", "123456");    configMap.put("ssl.key.password", "123456");    configMap.put("ssl.truststore.location", "/path/to/truststore");    configMap.put("ssl.truststore.password", "123456");    configMap.put("ssl.provider", "SunJSSE");    configMap.put("ssl.cipher.suites", "SSL_RSA_WITH_RC4_128_SHA,SSL_RSA_WITH_RC4_128_MD5");    configMap.put("ssl.secure.random.implementation", "SHA1PRNG");    configMap.put("ssl.client.auth", "required");    configMap.put("ssl.endpoint.identification.algorithm", "HTTPS");    configMap.put("ssl.keystore.type", "JKS");    configMap.put("ssl.protocol", "TLS");    configMap.put("ssl.truststore.type", "JKS");    configMap.put("ssl.enabled.protocols", "TLSv1.2,TLSv1.1,TLSv1");    configMap.put("ssl.keymanager.algorithm", "SunX509");    configMap.put("ssl.trustmanager.algorithm", "PKIX");    DistributedConfig config = new DistributedConfig(configMap);    SslContextFactory ssl = SSLUtils.createServerSideSslContextFactory(config);    Assert.assertEquals("file:///path/to/keystore", ssl.getKeyStorePath());    Assert.assertEquals("file:///path/to/truststore", ssl.getTrustStorePath());    Assert.assertEquals("SunJSSE", ssl.getProvider());    Assert.assertArrayEquals(new String[] { "SSL_RSA_WITH_RC4_128_SHA", "SSL_RSA_WITH_RC4_128_MD5" }, ssl.getIncludeCipherSuites());    Assert.assertEquals("SHA1PRNG", ssl.getSecureRandomAlgorithm());    Assert.assertTrue(ssl.getNeedClientAuth());    Assert.assertFalse(ssl.getWantClientAuth());    Assert.assertEquals("JKS", ssl.getKeyStoreType());    Assert.assertEquals("JKS", ssl.getTrustStoreType());    Assert.assertEquals("TLS", ssl.getProtocol());    Assert.assertArrayEquals(new String[] { "TLSv1.2", "TLSv1.1", "TLSv1" }, ssl.getIncludeProtocols());    Assert.assertEquals("SunX509", ssl.getKeyManagerFactoryAlgorithm());    Assert.assertEquals("PKIX", ssl.getTrustManagerFactoryAlgorithm());}
public void kafkatest_f11309_0() throws Exception
{    connector = PowerMock.createMock(BogusSourceConnector.class);    expectAdd(SourceSink.SOURCE);    Map<String, String> config = connectorConfig(SourceSink.SOURCE);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    expectConfigValidation(connectorMock, true, config);    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, config, false, createCallback);    PowerMock.verifyAll();}
public void kafkatest_f11310_0()
{    // Basic validation should be performed and return an error, but should still evaluate the connector's config    connector = PowerMock.createMock(BogusSourceConnector.class);    Map<String, String> config = connectorConfig(SourceSink.SOURCE);    config.remove(ConnectorConfig.NAME_CONFIG);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    EasyMock.expect(connectorMock.config()).andStubReturn(new ConfigDef());    ConfigValue validatedValue = new ConfigValue("foo.bar");    EasyMock.expect(connectorMock.validate(config)).andReturn(new Config(singletonList(validatedValue)));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    createCallback.onCompletion(EasyMock.<BadRequestException>anyObject(), EasyMock.<Herder.Created<ConnectorInfo>>isNull());    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, config, false, createCallback);    PowerMock.verifyAll();}
public void kafkatest_f11319_0() throws Exception
{    connector = PowerMock.createMock(BogusSourceConnector.class);    expectAdd(SourceSink.SOURCE);    Map<String, String> connectorConfig = connectorConfig(SourceSink.SOURCE);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    expectConfigValidation(connectorMock, true, connectorConfig);    // herder.stop() should stop any running connectors and tasks even if destroyConnector was not invoked    expectStop();    statusBackingStore.stop();    EasyMock.expectLastCall();    worker.stop();    EasyMock.expectLastCall();    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, connectorConfig, false, createCallback);    herder.stop();    PowerMock.verifyAll();}
public void kafkatest_f11320_0() throws Exception
{    Map<String, String> connConfig = connectorConfig(SourceSink.SOURCE);    System.out.println(connConfig);    Callback<Collection<String>> listConnectorsCb = PowerMock.createMock(Callback.class);    Callback<ConnectorInfo> connectorInfoCb = PowerMock.createMock(Callback.class);    Callback<Map<String, String>> connectorConfigCb = PowerMock.createMock(Callback.class);    Callback<List<TaskInfo>> taskConfigsCb = PowerMock.createMock(Callback.class);    // Check accessors with empty worker    listConnectorsCb.onCompletion(null, Collections.EMPTY_SET);    EasyMock.expectLastCall();    connectorInfoCb.onCompletion(EasyMock.<NotFoundException>anyObject(), EasyMock.<ConnectorInfo>isNull());    EasyMock.expectLastCall();    connectorConfigCb.onCompletion(EasyMock.<NotFoundException>anyObject(), EasyMock.<Map<String, String>>isNull());    EasyMock.expectLastCall();    taskConfigsCb.onCompletion(EasyMock.<NotFoundException>anyObject(), EasyMock.<List<TaskInfo>>isNull());    EasyMock.expectLastCall();    // Create connector    connector = PowerMock.createMock(BogusSourceConnector.class);    expectAdd(SourceSink.SOURCE);    expectConfigValidation(connector, true, connConfig);    // Validate accessors with 1 connector    listConnectorsCb.onCompletion(null, singleton(CONNECTOR_NAME));    EasyMock.expectLastCall();    ConnectorInfo connInfo = new ConnectorInfo(CONNECTOR_NAME, connConfig, Arrays.asList(new ConnectorTaskId(CONNECTOR_NAME, 0)), ConnectorType.SOURCE);    connectorInfoCb.onCompletion(null, connInfo);    EasyMock.expectLastCall();    connectorConfigCb.onCompletion(null, connConfig);    EasyMock.expectLastCall();    TaskInfo taskInfo = new TaskInfo(new ConnectorTaskId(CONNECTOR_NAME, 0), taskConfig(SourceSink.SOURCE));    taskConfigsCb.onCompletion(null, Arrays.asList(taskInfo));    EasyMock.expectLastCall();    PowerMock.replayAll();    // All operations are synchronous for StandaloneHerder, so we don't need to actually wait after making each call    herder.connectors(listConnectorsCb);    herder.connectorInfo(CONNECTOR_NAME, connectorInfoCb);    herder.connectorConfig(CONNECTOR_NAME, connectorConfigCb);    herder.taskConfigs(CONNECTOR_NAME, taskConfigsCb);    herder.putConnectorConfig(CONNECTOR_NAME, connConfig, false, createCallback);    EasyMock.reset(transformer);    EasyMock.expect(transformer.transform(EasyMock.eq(CONNECTOR_NAME), EasyMock.anyObject())).andThrow(new AssertionError("Config transformation should not occur when requesting connector or task info")).anyTimes();    EasyMock.replay(transformer);    herder.connectors(listConnectorsCb);    herder.connectorInfo(CONNECTOR_NAME, connectorInfoCb);    herder.connectorConfig(CONNECTOR_NAME, connectorConfigCb);    herder.taskConfigs(CONNECTOR_NAME, taskConfigsCb);    PowerMock.verifyAll();}
private void kafkatest_f11329_0(Connector connectorMock, boolean shouldCreateConnector, Map<String, String>... configs)
{    // config validation    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    if (shouldCreateConnector) {        EasyMock.expect(worker.getPlugins()).andReturn(plugins);        EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    }    EasyMock.expect(connectorMock.config()).andStubReturn(new ConfigDef());    for (Map<String, String> config : configs) EasyMock.expect(connectorMock.validate(config)).andReturn(new Config(Collections.<ConfigValue>emptyList()));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);}
public void kafkatest_f11330_0()
{    time = new MockTime();    time.sleep(1000L);    tracker = new StateTracker();    state = State.UNASSIGNED;}
public Class<? extends Task> kafkatest_f11342_0()
{    return null;}
public List<Map<String, String>> kafkatest_f11343_0(int maxTasks)
{    return null;}
public void kafkatest_f11353_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", ReplaceField.Value.class.getName());    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
public void kafkatest_f11354_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", SetSchemaMetadata.Value.class.getName());    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
public void kafkatest_f11363_0()
{    assertNull(configTransformer.transform(MY_CONNECTOR, null));}
public ConfigData kafkatest_f11365_0(String path)
{    return null;}
public void kafkatest_f11375_0()
{    connector.version();    expectLastCall().andReturn(VERSION);    connector.initialize(EasyMock.notNull(ConnectorContext.class));    expectLastCall();    connector.start(CONFIG);    expectLastCall();    listener.onStartup(CONNECTOR);    expectLastCall();    connector.stop();    expectLastCall();    listener.onPause(CONNECTOR);    expectLastCall();    listener.onShutdown(CONNECTOR);    expectLastCall();    replayAll();    WorkerConnector workerConnector = new WorkerConnector(CONNECTOR, connector, ctx, metrics, listener);    workerConnector.initialize(connectorConfig);    assertInitializedMetric(workerConnector);    workerConnector.transitionTo(TargetState.STARTED);    assertRunningMetric(workerConnector);    workerConnector.transitionTo(TargetState.PAUSED);    assertPausedMetric(workerConnector);    workerConnector.shutdown();    assertStoppedMetric(workerConnector);    verifyAll();}
public void kafkatest_f11376_0()
{    connector.version();    expectLastCall().andReturn(VERSION);    connector.initialize(EasyMock.notNull(ConnectorContext.class));    expectLastCall();    listener.onPause(CONNECTOR);    expectLastCall();    connector.start(CONFIG);    expectLastCall();    listener.onResume(CONNECTOR);    expectLastCall();    connector.stop();    expectLastCall();    listener.onShutdown(CONNECTOR);    expectLastCall();    replayAll();    WorkerConnector workerConnector = new WorkerConnector(CONNECTOR, connector, ctx, metrics, listener);    workerConnector.initialize(connectorConfig);    assertInitializedMetric(workerConnector);    workerConnector.transitionTo(TargetState.PAUSED);    assertPausedMetric(workerConnector);    workerConnector.transitionTo(TargetState.STARTED);    assertRunningMetric(workerConnector);    workerConnector.shutdown();    assertStoppedMetric(workerConnector);    verifyAll();}
protected void kafkatest_f11385_0(WorkerConnector workerConnector)
{    assertTrue(workerConnector.metrics().isUnassigned());    assertFalse(workerConnector.metrics().isFailed());    assertFalse(workerConnector.metrics().isPaused());    assertFalse(workerConnector.metrics().isRunning());}
protected void kafkatest_f11386_0(WorkerConnector workerConnector)
{    assertTrue(workerConnector.metrics().isUnassigned());    assertFalse(workerConnector.metrics().isFailed());    assertFalse(workerConnector.metrics().isPaused());    assertFalse(workerConnector.metrics().isRunning());    MetricGroup metricGroup = workerConnector.metrics().metricGroup();    String status = metrics.currentMetricValueAsString(metricGroup, "status");    String type = metrics.currentMetricValueAsString(metricGroup, "connector-type");    String clazz = metrics.currentMetricValueAsString(metricGroup, "connector-class");    String version = metrics.currentMetricValueAsString(metricGroup, "connector-version");    assertEquals(type, "unknown");    assertNotNull(clazz);    assertEquals(VERSION, version);}
public void kafkatest_f11395_0() throws Exception
{    createTask(initialState);    expectInitializeTask();    expectPollInitialAssignment();    expectConsumerPoll(1);    expectConversionAndTransformation(1);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    final List<TopicPartition> partitions = asList(TOPIC_PARTITION, TOPIC_PARTITION2);    final Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    offsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    offsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    sinkTask.preCommit(offsets);    EasyMock.expectLastCall().andReturn(offsets);    // first one raises wakeup    consumer.commitSync(EasyMock.<Map<TopicPartition, OffsetAndMetadata>>anyObject());    EasyMock.expectLastCall().andThrow(new WakeupException());    // we should retry and complete the commit    consumer.commitSync(EasyMock.<Map<TopicPartition, OffsetAndMetadata>>anyObject());    EasyMock.expectLastCall();    sinkTask.close(new HashSet<>(partitions));    EasyMock.expectLastCall();    EasyMock.expect(consumer.position(TOPIC_PARTITION)).andReturn(FIRST_OFFSET);    EasyMock.expect(consumer.position(TOPIC_PARTITION2)).andReturn(FIRST_OFFSET);    sinkTask.open(partitions);    EasyMock.expectLastCall();    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            rebalanceListener.getValue().onPartitionsRevoked(partitions);            rebalanceListener.getValue().onPartitionsAssigned(partitions);            return ConsumerRecords.empty();        }    });    EasyMock.expect(consumer.assignment()).andReturn(new HashSet<>(partitions));    consumer.resume(Collections.singleton(TOPIC_PARTITION));    EasyMock.expectLastCall();    consumer.resume(Collections.singleton(TOPIC_PARTITION2));    EasyMock.expectLastCall();    statusListener.onResume(taskId);    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    time.sleep(30000L);    workerTask.initializeAndStart();    time.sleep(30000L);    // poll for initial assignment    workerTask.iteration();    time.sleep(30000L);    // first record delivered    workerTask.iteration();    // now rebalance with the wakeup triggered    workerTask.iteration();    time.sleep(30000L);    assertSinkMetricValue("partition-count", 2);    assertSinkMetricValue("sink-record-read-total", 1.0);    assertSinkMetricValue("sink-record-send-total", 1.0);    assertSinkMetricValue("sink-record-active-count", 0.0);    assertSinkMetricValue("sink-record-active-count-max", 1.0);    assertSinkMetricValue("sink-record-active-count-avg", 0.33333);    assertSinkMetricValue("offset-commit-seq-no", 1.0);    assertSinkMetricValue("offset-commit-completion-total", 1.0);    assertSinkMetricValue("offset-commit-skip-total", 0.0);    assertTaskMetricValue("status", "running");    assertTaskMetricValue("running-ratio", 1.0);    assertTaskMetricValue("pause-ratio", 0.0);    assertTaskMetricValue("batch-size-max", 1.0);    assertTaskMetricValue("batch-size-avg", 1.0);    assertTaskMetricValue("offset-commit-max-time-ms", 0.0);    assertTaskMetricValue("offset-commit-avg-time-ms", 0.0);    assertTaskMetricValue("offset-commit-failure-percentage", 0.0);    assertTaskMetricValue("offset-commit-success-percentage", 1.0);    PowerMock.verifyAll();}
public ConsumerRecords<byte[], byte[]> kafkatest_f11396_0() throws Throwable
{    rebalanceListener.getValue().onPartitionsRevoked(partitions);    rebalanceListener.getValue().onPartitionsAssigned(partitions);    return ConsumerRecords.empty();}
public void kafkatest_f11405_0() throws Exception
{    createTask(initialState);    expectInitializeTask();    // iter 1    expectPollInitialAssignment();    // iter 2    expectConsumerPoll(1);    expectConversionAndTransformation(4);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    final Map<TopicPartition, OffsetAndMetadata> workerStartingOffsets = new HashMap<>();    workerStartingOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET));    workerStartingOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    final Map<TopicPartition, OffsetAndMetadata> workerCurrentOffsets = new HashMap<>();    workerCurrentOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    workerCurrentOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    final List<TopicPartition> originalPartitions = asList(TOPIC_PARTITION, TOPIC_PARTITION2);    final List<TopicPartition> rebalancedPartitions = asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3);    final Map<TopicPartition, OffsetAndMetadata> rebalanceOffsets = new HashMap<>();    rebalanceOffsets.put(TOPIC_PARTITION, workerCurrentOffsets.get(TOPIC_PARTITION));    rebalanceOffsets.put(TOPIC_PARTITION2, workerCurrentOffsets.get(TOPIC_PARTITION2));    rebalanceOffsets.put(TOPIC_PARTITION3, new OffsetAndMetadata(FIRST_OFFSET));    final Map<TopicPartition, OffsetAndMetadata> postRebalanceCurrentOffsets = new HashMap<>();    postRebalanceCurrentOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 3));    postRebalanceCurrentOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    postRebalanceCurrentOffsets.put(TOPIC_PARTITION3, new OffsetAndMetadata(FIRST_OFFSET + 2));    // iter 3 - note that we return the current offset to indicate they should be committed    sinkTask.preCommit(workerCurrentOffsets);    EasyMock.expectLastCall().andReturn(workerCurrentOffsets);    // We need to delay the result of trying to commit offsets to Kafka via the consumer.commitAsync    // method. We do this so that we can test that the callback is not called until after the rebalance    // changes the lastCommittedOffsets. To fake this for tests we have the commitAsync build a function    // that will call the callback with the appropriate parameters, and we'll run that function later.    final AtomicReference<Runnable> asyncCallbackRunner = new AtomicReference<>();    final AtomicBoolean asyncCallbackRan = new AtomicBoolean();    consumer.commitAsync(EasyMock.eq(workerCurrentOffsets), EasyMock.<OffsetCommitCallback>anyObject());    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @SuppressWarnings("unchecked")        @Override        public Void answer() throws Throwable {            // Grab the arguments passed to the consumer.commitAsync method            final Object[] args = EasyMock.getCurrentArguments();            final Map<TopicPartition, OffsetAndMetadata> offsets = (Map<TopicPartition, OffsetAndMetadata>) args[0];            final OffsetCommitCallback callback = (OffsetCommitCallback) args[1];            asyncCallbackRunner.set(new Runnable() {                @Override                public void run() {                    callback.onComplete(offsets, null);                    asyncCallbackRan.set(true);                }            });            return null;        }    });    // Expect the next poll to discover and perform the rebalance, THEN complete the previous callback handler,    // and then return one record for TP1 and one for TP3.    final AtomicBoolean rebalanced = new AtomicBoolean();    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            // Rebalance always begins with revoking current partitions ...            rebalanceListener.getValue().onPartitionsRevoked(originalPartitions);            // Respond to the rebalance            Map<TopicPartition, Long> offsets = new HashMap<>();            offsets.put(TOPIC_PARTITION, rebalanceOffsets.get(TOPIC_PARTITION).offset());            offsets.put(TOPIC_PARTITION2, rebalanceOffsets.get(TOPIC_PARTITION2).offset());            offsets.put(TOPIC_PARTITION3, rebalanceOffsets.get(TOPIC_PARTITION3).offset());            sinkTaskContext.getValue().offset(offsets);            rebalanceListener.getValue().onPartitionsAssigned(rebalancedPartitions);            rebalanced.set(true);            // Run the previous async commit handler            asyncCallbackRunner.get().run();            // And prep the two records to return            long timestamp = RecordBatch.NO_TIMESTAMP;            TimestampType timestampType = TimestampType.NO_TIMESTAMP_TYPE;            List<ConsumerRecord<byte[], byte[]>> records = new ArrayList<>();            records.add(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturnedTp1 + 1, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));            records.add(new ConsumerRecord<>(TOPIC, PARTITION3, FIRST_OFFSET + recordsReturnedTp3 + 1, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));            recordsReturnedTp1 += 1;            recordsReturnedTp3 += 1;            return new ConsumerRecords<>(Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), records));        }    });    // onPartitionsRevoked    sinkTask.preCommit(workerCurrentOffsets);    EasyMock.expectLastCall().andReturn(workerCurrentOffsets);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    sinkTask.close(workerCurrentOffsets.keySet());    EasyMock.expectLastCall();    consumer.commitSync(workerCurrentOffsets);    EasyMock.expectLastCall();    // onPartitionsAssigned - step 1    final long offsetTp1 = rebalanceOffsets.get(TOPIC_PARTITION).offset();    final long offsetTp2 = rebalanceOffsets.get(TOPIC_PARTITION2).offset();    final long offsetTp3 = rebalanceOffsets.get(TOPIC_PARTITION3).offset();    EasyMock.expect(consumer.position(TOPIC_PARTITION)).andReturn(offsetTp1);    EasyMock.expect(consumer.position(TOPIC_PARTITION2)).andReturn(offsetTp2);    EasyMock.expect(consumer.position(TOPIC_PARTITION3)).andReturn(offsetTp3);    // onPartitionsAssigned - step 2    sinkTask.open(rebalancedPartitions);    EasyMock.expectLastCall();    // onPartitionsAssigned - step 3 rewind    consumer.seek(TOPIC_PARTITION, offsetTp1);    EasyMock.expectLastCall();    consumer.seek(TOPIC_PARTITION2, offsetTp2);    EasyMock.expectLastCall();    consumer.seek(TOPIC_PARTITION3, offsetTp3);    EasyMock.expectLastCall();    // iter 4 - note that we return the current offset to indicate they should be committed    sinkTask.preCommit(postRebalanceCurrentOffsets);    EasyMock.expectLastCall().andReturn(postRebalanceCurrentOffsets);    final Capture<OffsetCommitCallback> callback = EasyMock.newCapture();    consumer.commitAsync(EasyMock.eq(postRebalanceCurrentOffsets), EasyMock.capture(callback));    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callback.getValue().onComplete(postRebalanceCurrentOffsets, null);            return null;        }    });    // no actual consumer.commit() triggered    expectConsumerPoll(1);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    workerTask.initializeAndStart();    // iter 1 -- initial assignment    workerTask.iteration();    assertEquals(workerStartingOffsets, Whitebox.getInternalState(workerTask, "currentOffsets"));    assertEquals(workerStartingOffsets, Whitebox.getInternalState(workerTask, "lastCommittedOffsets"));    time.sleep(WorkerConfig.OFFSET_COMMIT_TIMEOUT_MS_DEFAULT);    // iter 2 -- deliver 2 records    workerTask.iteration();    sinkTaskContext.getValue().requestCommit();    // iter 3 -- commit in progress    workerTask.iteration();    assertSinkMetricValue("partition-count", 3);    assertSinkMetricValue("sink-record-read-total", 3.0);    assertSinkMetricValue("sink-record-send-total", 3.0);    assertSinkMetricValue("sink-record-active-count", 4.0);    assertSinkMetricValue("sink-record-active-count-max", 4.0);    assertSinkMetricValue("sink-record-active-count-avg", 0.71429);    assertSinkMetricValue("offset-commit-seq-no", 2.0);    assertSinkMetricValue("offset-commit-completion-total", 1.0);    assertSinkMetricValue("offset-commit-skip-total", 1.0);    assertTaskMetricValue("status", "running");    assertTaskMetricValue("running-ratio", 1.0);    assertTaskMetricValue("pause-ratio", 0.0);    assertTaskMetricValue("batch-size-max", 2.0);    assertTaskMetricValue("batch-size-avg", 1.0);    assertTaskMetricValue("offset-commit-max-time-ms", 0.0);    assertTaskMetricValue("offset-commit-avg-time-ms", 0.0);    assertTaskMetricValue("offset-commit-failure-percentage", 0.0);    assertTaskMetricValue("offset-commit-success-percentage", 1.0);    assertTrue(asyncCallbackRan.get());    assertTrue(rebalanced.get());    // Check that the offsets were not reset by the out-of-order async commit callback    assertEquals(postRebalanceCurrentOffsets, Whitebox.getInternalState(workerTask, "currentOffsets"));    assertEquals(rebalanceOffsets, Whitebox.getInternalState(workerTask, "lastCommittedOffsets"));    time.sleep(WorkerConfig.OFFSET_COMMIT_TIMEOUT_MS_DEFAULT);    sinkTaskContext.getValue().requestCommit();    // iter 4 -- commit in progress    workerTask.iteration();    // Check that the offsets were not reset by the out-of-order async commit callback    assertEquals(postRebalanceCurrentOffsets, Whitebox.getInternalState(workerTask, "currentOffsets"));    assertEquals(postRebalanceCurrentOffsets, Whitebox.getInternalState(workerTask, "lastCommittedOffsets"));    assertSinkMetricValue("partition-count", 3);    assertSinkMetricValue("sink-record-read-total", 4.0);    assertSinkMetricValue("sink-record-send-total", 4.0);    assertSinkMetricValue("sink-record-active-count", 0.0);    assertSinkMetricValue("sink-record-active-count-max", 4.0);    assertSinkMetricValue("sink-record-active-count-avg", 0.5555555);    assertSinkMetricValue("offset-commit-seq-no", 3.0);    assertSinkMetricValue("offset-commit-completion-total", 2.0);    assertSinkMetricValue("offset-commit-skip-total", 1.0);    assertTaskMetricValue("status", "running");    assertTaskMetricValue("running-ratio", 1.0);    assertTaskMetricValue("pause-ratio", 0.0);    assertTaskMetricValue("batch-size-max", 2.0);    assertTaskMetricValue("batch-size-avg", 1.0);    assertTaskMetricValue("offset-commit-max-time-ms", 0.0);    assertTaskMetricValue("offset-commit-avg-time-ms", 0.0);    assertTaskMetricValue("offset-commit-failure-percentage", 0.0);    assertTaskMetricValue("offset-commit-success-percentage", 1.0);    PowerMock.verifyAll();}
public Void kafkatest_f11406_0() throws Throwable
{    // Grab the arguments passed to the consumer.commitAsync method    final Object[] args = EasyMock.getCurrentArguments();    final Map<TopicPartition, OffsetAndMetadata> offsets = (Map<TopicPartition, OffsetAndMetadata>) args[0];    final OffsetCommitCallback callback = (OffsetCommitCallback) args[1];    asyncCallbackRunner.set(new Runnable() {        @Override        public void run() {            callback.onComplete(offsets, null);            asyncCallbackRan.set(true);        }    });    return null;}
public void kafkatest_f11415_0()
{    SinkTaskMetricsGroup group = new SinkTaskMetricsGroup(taskId, metrics);    SinkTaskMetricsGroup group1 = new SinkTaskMetricsGroup(taskId1, metrics);    for (int i = 0; i != 10; ++i) {        group.recordRead(1);        group.recordSend(2);        group.recordPut(3);        group.recordPartitionCount(4);        group.recordOffsetSequenceNumber(5);    }    Map<TopicPartition, OffsetAndMetadata> committedOffsets = new HashMap<>();    committedOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    group.recordCommittedOffsets(committedOffsets);    Map<TopicPartition, OffsetAndMetadata> consumedOffsets = new HashMap<>();    consumedOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 10));    group.recordConsumedOffsets(consumedOffsets);    for (int i = 0; i != 20; ++i) {        group1.recordRead(1);        group1.recordSend(2);        group1.recordPut(30);        group1.recordPartitionCount(40);        group1.recordOffsetSequenceNumber(50);    }    committedOffsets = new HashMap<>();    committedOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET + 2));    committedOffsets.put(TOPIC_PARTITION3, new OffsetAndMetadata(FIRST_OFFSET + 3));    group1.recordCommittedOffsets(committedOffsets);    consumedOffsets = new HashMap<>();    consumedOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET + 20));    consumedOffsets.put(TOPIC_PARTITION3, new OffsetAndMetadata(FIRST_OFFSET + 30));    group1.recordConsumedOffsets(consumedOffsets);    assertEquals(0.333, metrics.currentMetricValueAsDouble(group.metricGroup(), "sink-record-read-rate"), 0.001d);    assertEquals(0.667, metrics.currentMetricValueAsDouble(group.metricGroup(), "sink-record-send-rate"), 0.001d);    assertEquals(9, metrics.currentMetricValueAsDouble(group.metricGroup(), "sink-record-active-count"), 0.001d);    assertEquals(4, metrics.currentMetricValueAsDouble(group.metricGroup(), "partition-count"), 0.001d);    assertEquals(5, metrics.currentMetricValueAsDouble(group.metricGroup(), "offset-commit-seq-no"), 0.001d);    assertEquals(3, metrics.currentMetricValueAsDouble(group.metricGroup(), "put-batch-max-time-ms"), 0.001d);    // Close the group    group.close();    for (MetricName metricName : group.metricGroup().metrics().metrics().keySet()) {        // Metrics for this group should no longer exist        assertFalse(group.metricGroup().groupId().includes(metricName));    }    // Sensors for this group should no longer exist    assertNull(group.metricGroup().metrics().getSensor("source-record-poll"));    assertNull(group.metricGroup().metrics().getSensor("source-record-write"));    assertNull(group.metricGroup().metrics().getSensor("poll-batch-time"));    assertEquals(0.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), "sink-record-read-rate"), 0.001d);    assertEquals(1.333, metrics.currentMetricValueAsDouble(group1.metricGroup(), "sink-record-send-rate"), 0.001d);    assertEquals(45, metrics.currentMetricValueAsDouble(group1.metricGroup(), "sink-record-active-count"), 0.001d);    assertEquals(40, metrics.currentMetricValueAsDouble(group1.metricGroup(), "partition-count"), 0.001d);    assertEquals(50, metrics.currentMetricValueAsDouble(group1.metricGroup(), "offset-commit-seq-no"), 0.001d);    assertEquals(30, metrics.currentMetricValueAsDouble(group1.metricGroup(), "put-batch-max-time-ms"), 0.001d);}
private void kafkatest_f11416_0() throws Exception
{    consumer.subscribe(EasyMock.eq(asList(TOPIC)), EasyMock.capture(rebalanceListener));    PowerMock.expectLastCall();    sinkTask.initialize(EasyMock.capture(sinkTaskContext));    PowerMock.expectLastCall();    sinkTask.start(TASK_PROPS);    PowerMock.expectLastCall();}
private void kafkatest_f11425_0(final int numMessages, final long timestamp, final TimestampType timestampType)
{    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            List<ConsumerRecord<byte[], byte[]>> records = new ArrayList<>();            for (int i = 0; i < numMessages; i++) records.add(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturnedTp1 + i, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));            recordsReturnedTp1 += numMessages;            return new ConsumerRecords<>(numMessages > 0 ? Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), records) : Collections.<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>emptyMap());        }    });}
public ConsumerRecords<byte[], byte[]> kafkatest_f11426_0() throws Throwable
{    List<ConsumerRecord<byte[], byte[]>> records = new ArrayList<>();    for (int i = 0; i < numMessages; i++) records.add(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturnedTp1 + i, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));    recordsReturnedTp1 += numMessages;    return new ConsumerRecords<>(numMessages > 0 ? Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), records) : Collections.<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>emptyMap());}
private double kafkatest_f11435_0(String metricName)
{    MetricGroup taskGroup = workerTask.taskMetricsGroup().metricGroup();    double value = metrics.currentMetricValueAsDouble(taskGroup, metricName);    System.out.println("** " + metricName + "=" + value);    return value;}
private void kafkatest_f11436_0(int minimumPollCountExpected)
{    MetricGroup sinkTaskGroup = workerTask.sinkTaskMetricsGroup().metricGroup();    MetricGroup taskGroup = workerTask.taskMetricsGroup().metricGroup();    double readRate = metrics.currentMetricValueAsDouble(sinkTaskGroup, "sink-record-read-rate");    double readTotal = metrics.currentMetricValueAsDouble(sinkTaskGroup, "sink-record-read-total");    double sendRate = metrics.currentMetricValueAsDouble(sinkTaskGroup, "sink-record-send-rate");    double sendTotal = metrics.currentMetricValueAsDouble(sinkTaskGroup, "sink-record-send-total");}
public void kafkatest_f11445_0() throws Exception
{    // Just validate that the calls are passed through to the consumer, and that where appropriate errors are    // converted    expectInitializeTask();    expectPollInitialAssignment();    expectOnePoll().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            assertEquals(new HashSet<>(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3)), sinkTaskContext.getValue().assignment());            return null;        }    });    EasyMock.expect(consumer.assignment()).andReturn(new HashSet<>(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3)));    expectOnePoll().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            try {                sinkTaskContext.getValue().pause(UNASSIGNED_TOPIC_PARTITION);                fail("Trying to pause unassigned partition should have thrown an Connect exception");            } catch (ConnectException e) {            // expected            }            sinkTaskContext.getValue().pause(TOPIC_PARTITION, TOPIC_PARTITION2);            return null;        }    });    consumer.pause(Arrays.asList(UNASSIGNED_TOPIC_PARTITION));    PowerMock.expectLastCall().andThrow(new IllegalStateException("unassigned topic partition"));    consumer.pause(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2));    PowerMock.expectLastCall();    expectOnePoll().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            try {                sinkTaskContext.getValue().resume(UNASSIGNED_TOPIC_PARTITION);                fail("Trying to resume unassigned partition should have thrown an Connect exception");            } catch (ConnectException e) {            // expected            }            sinkTaskContext.getValue().resume(TOPIC_PARTITION, TOPIC_PARTITION2);            return null;        }    });    consumer.resume(Arrays.asList(UNASSIGNED_TOPIC_PARTITION));    PowerMock.expectLastCall().andThrow(new IllegalStateException("unassigned topic partition"));    consumer.resume(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2));    PowerMock.expectLastCall();    expectStopTask();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    workerTask.initializeAndStart();    workerTask.iteration();    workerTask.iteration();    workerTask.iteration();    workerTask.iteration();    workerTask.stop();    workerTask.close();    PowerMock.verifyAll();}
public Object kafkatest_f11446_0() throws Throwable
{    assertEquals(new HashSet<>(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3)), sinkTaskContext.getValue().assignment());    return null;}
private void kafkatest_f11455_0() throws Exception
{    final List<TopicPartition> partitions = Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3);    sinkTask.open(partitions);    EasyMock.expectLastCall();    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            rebalanceListener.getValue().onPartitionsAssigned(partitions);            return ConsumerRecords.empty();        }    });    EasyMock.expect(consumer.position(TOPIC_PARTITION)).andReturn(FIRST_OFFSET);    EasyMock.expect(consumer.position(TOPIC_PARTITION2)).andReturn(FIRST_OFFSET);    EasyMock.expect(consumer.position(TOPIC_PARTITION3)).andReturn(FIRST_OFFSET);    sinkTask.put(Collections.<SinkRecord>emptyList());    EasyMock.expectLastCall();}
public ConsumerRecords<byte[], byte[]> kafkatest_f11456_0() throws Throwable
{    rebalanceListener.getValue().onPartitionsAssigned(partitions);    return ConsumerRecords.empty();}
public Object kafkatest_f11465_0() throws Throwable
{    time.sleep(consumerCommitDelayMs);    if (invokeCallback)        capturedCallback.getValue().onComplete(offsetsToCommit, consumerCommitError);    return null;}
public void kafkatest_f11466_0()
{    super.setup();    Map<String, String> workerProps = new HashMap<>();    workerProps.put("key.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("value.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.key.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.value.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.key.converter.schemas.enable", "false");    workerProps.put("internal.value.converter.schemas.enable", "false");    workerProps.put("offset.storage.file.filename", "/tmp/connect.offsets");    plugins = new Plugins(workerProps);    config = new StandaloneConfig(workerProps);    producerCallbacks = EasyMock.newCapture();    metrics = new MockConnectMetrics();}
public List<SourceRecord> kafkatest_f11475_0() throws Throwable
{    pollLatch.countDown();    throw exception;}
public void kafkatest_f11476_0() throws Exception
{    // Test that the task commits properly when prompted    createWorkerTask();    sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));    EasyMock.expectLastCall();    sourceTask.start(TASK_PROPS);    EasyMock.expectLastCall();    statusListener.onStartup(taskId);    EasyMock.expectLastCall();    // We'll wait for some data, then trigger a flush    final CountDownLatch pollLatch = expectPolls(1);    expectOffsetFlush(true);    sourceTask.stop();    EasyMock.expectLastCall();    expectOffsetFlush(true);    statusListener.onShutdown(taskId);    EasyMock.expectLastCall();    producer.close(EasyMock.anyObject(Duration.class));    EasyMock.expectLastCall();    transformationChain.close();    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    Future<?> taskFuture = executor.submit(workerTask);    assertTrue(awaitLatch(pollLatch));    assertTrue(workerTask.commitOffsets());    workerTask.stop();    assertTrue(workerTask.awaitStop(1000));    taskFuture.get();    assertPollMetrics(1);    PowerMock.verifyAll();}
public void kafkatest_f11485_0() throws Exception
{    final CountDownLatch startupLatch = new CountDownLatch(1);    final CountDownLatch finishStartupLatch = new CountDownLatch(1);    createWorkerTask();    sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));    EasyMock.expectLastCall();    sourceTask.start(TASK_PROPS);    EasyMock.expectLastCall().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            startupLatch.countDown();            assertTrue(awaitLatch(finishStartupLatch));            return null;        }    });    statusListener.onStartup(taskId);    EasyMock.expectLastCall();    sourceTask.stop();    EasyMock.expectLastCall();    expectOffsetFlush(true);    statusListener.onShutdown(taskId);    EasyMock.expectLastCall();    producer.close(EasyMock.anyObject(Duration.class));    EasyMock.expectLastCall();    transformationChain.close();    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    Future<?> workerTaskFuture = executor.submit(workerTask);    // Stopping immediately while the other thread has work to do should result in no polling, no offset commits,    // exiting the work thread immediately, and the stop() method will be invoked in the background thread since it    // cannot be invoked immediately in the thread trying to stop the task.    assertTrue(awaitLatch(startupLatch));    workerTask.stop();    finishStartupLatch.countDown();    assertTrue(workerTask.awaitStop(1000));    workerTaskFuture.get();    PowerMock.verifyAll();}
public Object kafkatest_f11486_0() throws Throwable
{    startupLatch.countDown();    assertTrue(awaitLatch(finishStartupLatch));    return null;}
private Capture<ProducerRecord<byte[], byte[]>> kafkatest_f11495_0(boolean anyTimes, boolean isRetry) throws InterruptedException
{    return expectSendRecord(anyTimes, isRetry, true, true);}
private Capture<ProducerRecord<byte[], byte[]>> kafkatest_f11496_0(boolean anyTimes, boolean isRetry) throws InterruptedException
{    return expectSendRecord(anyTimes, isRetry, true, false);}
private void kafkatest_f11505_0(boolean succeed) throws Exception
{    EasyMock.expect(offsetWriter.beginFlush()).andReturn(true);    Future<Void> flushFuture = PowerMock.createMock(Future.class);    EasyMock.expect(offsetWriter.doFlush(EasyMock.anyObject(Callback.class))).andReturn(flushFuture);    // Should throw for failure    IExpectationSetters<Void> futureGetExpect = EasyMock.expect(flushFuture.get(EasyMock.anyLong(), EasyMock.anyObject(TimeUnit.class)));    if (succeed) {        sourceTask.commit();        EasyMock.expectLastCall();        futureGetExpect.andReturn(null);    } else {        futureGetExpect.andThrow(new TimeoutException());        offsetWriter.cancelFlush();        PowerMock.expectLastCall();    }}
private void kafkatest_f11506_0(int minimumPollCountExpected)
{    MetricGroup sourceTaskGroup = workerTask.sourceTaskMetricsGroup().metricGroup();    MetricGroup taskGroup = workerTask.taskMetricsGroup().metricGroup();    double pollRate = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-poll-rate");    double pollTotal = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-poll-total");    if (minimumPollCountExpected > 0) {        assertEquals(RECORDS.size(), metrics.currentMetricValueAsDouble(taskGroup, "batch-size-max"), 0.000001d);        assertEquals(RECORDS.size(), metrics.currentMetricValueAsDouble(taskGroup, "batch-size-avg"), 0.000001d);        assertTrue(pollRate > 0.0d);    } else {        assertTrue(pollRate == 0.0d);    }    assertTrue(pollTotal >= minimumPollCountExpected);    double writeRate = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-write-rate");    double writeTotal = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-write-total");    if (minimumPollCountExpected > 0) {        assertTrue(writeRate > 0.0d);    } else {        assertTrue(writeRate == 0.0d);    }    assertTrue(writeTotal >= minimumPollCountExpected);    double pollBatchTimeMax = metrics.currentMetricValueAsDouble(sourceTaskGroup, "poll-batch-max-time-ms");    double pollBatchTimeAvg = metrics.currentMetricValueAsDouble(sourceTaskGroup, "poll-batch-avg-time-ms");    if (minimumPollCountExpected > 0) {        assertTrue(pollBatchTimeMax >= 0.0d);    }    assertTrue(Double.isNaN(pollBatchTimeAvg) || pollBatchTimeAvg > 0.0d);    double activeCount = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-active-count");    double activeCountMax = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-active-count-max");    assertEquals(0, activeCount, 0.000001d);    if (minimumPollCountExpected > 0) {        assertEquals(RECORDS.size(), activeCountMax, 0.000001d);    }}
public void kafkatest_f11515_0()
{    ConnectorTaskId taskId = new ConnectorTaskId("foo", 0);    MockConnectMetrics metrics = new MockConnectMetrics();    MockTime time = metrics.time();    ConnectException error = new ConnectException("error");    TaskMetricsGroup group = new TaskMetricsGroup(taskId, metrics, statusListener);    statusListener.onStartup(taskId);    expectLastCall();    statusListener.onPause(taskId);    expectLastCall();    statusListener.onResume(taskId);    expectLastCall();    statusListener.onPause(taskId);    expectLastCall();    statusListener.onResume(taskId);    expectLastCall();    statusListener.onFailure(taskId, error);    expectLastCall();    statusListener.onShutdown(taskId);    expectLastCall();    replay(statusListener);    time.sleep(1000L);    group.onStartup(taskId);    assertRunningMetric(group);    time.sleep(2000L);    group.onPause(taskId);    assertPausedMetric(group);    time.sleep(3000L);    group.onResume(taskId);    assertRunningMetric(group);    time.sleep(4000L);    group.onPause(taskId);    assertPausedMetric(group);    time.sleep(5000L);    group.onResume(taskId);    assertRunningMetric(group);    time.sleep(6000L);    group.onFailure(taskId, error);    assertFailedMetric(group);    time.sleep(7000L);    group.onShutdown(taskId);    assertStoppedMetric(group);    verify(statusListener);    long totalTime = 27000L;    double pauseTimeRatio = (double) (3000L + 5000L) / totalTime;    double runningTimeRatio = (double) (2000L + 4000L + 6000L) / totalTime;    assertEquals(pauseTimeRatio, metrics.currentMetricValueAsDouble(group.metricGroup(), "pause-ratio"), 0.000001d);    assertEquals(runningTimeRatio, metrics.currentMetricValueAsDouble(group.metricGroup(), "running-ratio"), 0.000001d);}
protected void kafkatest_f11516_0(TaskMetricsGroup metricsGroup)
{    assertEquals(AbstractStatus.State.FAILED, metricsGroup.state());}
public void kafkatest_f11525_0()
{    expectConverters();    expectStartStorage();    PowerMock.replayAll();    worker = new Worker(WORKER_ID, new MockTime(), plugins, config, offsetBackingStore, noneConnectorClientConfigOverridePolicy);    worker.start();    worker.stopConnector(CONNECTOR_ID);    PowerMock.verifyAll();}
public void kafkatest_f11526_0()
{    expectConverters();    expectStartStorage();    EasyMock.expect(plugins.currentThreadLoader()).andReturn(delegatingLoader).times(3);    EasyMock.expect(plugins.newConnector(WorkerTestConnector.class.getName())).andReturn(connector);    EasyMock.expect(connector.version()).andReturn("1.0");    Map<String, String> props = new HashMap<>();    props.put(SinkConnectorConfig.TOPICS_CONFIG, "foo,bar");    props.put(ConnectorConfig.TASKS_MAX_CONFIG, "1");    props.put(ConnectorConfig.NAME_CONFIG, CONNECTOR_ID);    props.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, WorkerTestConnector.class.getName());    EasyMock.expect(connector.version()).andReturn("1.0");    EasyMock.expect(plugins.compareAndSwapLoaders(connector)).andReturn(delegatingLoader).times(3);    connector.initialize(anyObject(ConnectorContext.class));    EasyMock.expectLastCall();    connector.start(props);    EasyMock.expectLastCall();    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader).times(3);    connectorStatusListener.onStartup(CONNECTOR_ID);    EasyMock.expectLastCall();    // Reconfigure    EasyMock.<Class<? extends Task>>expect(connector.taskClass()).andReturn(TestSourceTask.class);    Map<String, String> taskProps = new HashMap<>();    taskProps.put("foo", "bar");    EasyMock.expect(connector.taskConfigs(2)).andReturn(Arrays.asList(taskProps, taskProps));    // Remove    connector.stop();    EasyMock.expectLastCall();    connectorStatusListener.onShutdown(CONNECTOR_ID);    EasyMock.expectLastCall();    expectStopStorage();    PowerMock.replayAll();    worker = new Worker(WORKER_ID, new MockTime(), plugins, config, offsetBackingStore, noneConnectorClientConfigOverridePolicy);    worker.start();    assertStatistics(worker, 0, 0);    assertEquals(Collections.emptySet(), worker.connectorNames());    worker.startConnector(CONNECTOR_ID, props, ctx, connectorStatusListener, TargetState.STARTED);    assertStatistics(worker, 1, 0);    assertEquals(new HashSet<>(Arrays.asList(CONNECTOR_ID)), worker.connectorNames());    try {        worker.startConnector(CONNECTOR_ID, props, ctx, connectorStatusListener, TargetState.STARTED);        fail("Should have thrown exception when trying to add connector with same name.");    } catch (ConnectException e) {    // expected    }    Map<String, String> connProps = new HashMap<>(props);    connProps.put(ConnectorConfig.TASKS_MAX_CONFIG, "2");    ConnectorConfig connConfig = new SinkConnectorConfig(plugins, connProps);    List<Map<String, String>> taskConfigs = worker.connectorTaskConfigs(CONNECTOR_ID, connConfig);    Map<String, String> expectedTaskProps = new HashMap<>();    expectedTaskProps.put("foo", "bar");    expectedTaskProps.put(TaskConfig.TASK_CLASS_CONFIG, TestSourceTask.class.getName());    expectedTaskProps.put(SinkTask.TOPICS_CONFIG, "foo,bar");    assertEquals(2, taskConfigs.size());    assertEquals(expectedTaskProps, taskConfigs.get(0));    assertEquals(expectedTaskProps, taskConfigs.get(1));    assertStatistics(worker, 1, 0);    assertStartupStatistics(worker, 1, 0, 0, 0);    worker.stopConnector(CONNECTOR_ID);    assertStatistics(worker, 0, 0);    assertStartupStatistics(worker, 1, 0, 0, 0);    assertEquals(Collections.emptySet(), worker.connectorNames());    // Nothing should be left, so this should effectively be a nop    worker.stop();    assertStatistics(worker, 0, 0);    PowerMock.verifyAll();}
public void kafkatest_f11535_0()
{    Map<String, String> props = new HashMap<>(workerProps);    props.put("consumer.auto.offset.reset", "latest");    props.put("consumer.max.poll.records", "1000");    props.put("consumer.client.id", "consumer-test-id");    WorkerConfig configWithOverrides = new StandaloneConfig(props);    Map<String, String> expectedConfigs = new HashMap<>(defaultConsumerConfigs);    expectedConfigs.put("group.id", "connect-test");    expectedConfigs.put("auto.offset.reset", "latest");    expectedConfigs.put("max.poll.records", "1000");    expectedConfigs.put("client.id", "consumer-test-id");    EasyMock.expect(connectorConfig.originalsWithPrefix(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX)).andReturn(new HashMap<>());    PowerMock.replayAll();    assertEquals(expectedConfigs, Worker.consumerConfigs(new ConnectorTaskId("test", 1), configWithOverrides, connectorConfig, null, noneConnectorClientConfigOverridePolicy));}
public void kafkatest_f11536_0()
{    Map<String, String> props = new HashMap<>(workerProps);    props.put("consumer.auto.offset.reset", "latest");    props.put("consumer.max.poll.records", "5000");    WorkerConfig configWithOverrides = new StandaloneConfig(props);    Map<String, String> expectedConfigs = new HashMap<>(defaultConsumerConfigs);    expectedConfigs.put("group.id", "connect-test");    expectedConfigs.put("auto.offset.reset", "latest");    expectedConfigs.put("max.poll.records", "5000");    expectedConfigs.put("max.poll.interval.ms", "1000");    expectedConfigs.put("client.id", "connector-consumer-test-1");    Map<String, Object> connConfig = new HashMap<String, Object>();    connConfig.put("max.poll.records", "5000");    connConfig.put("max.poll.interval.ms", "1000");    EasyMock.expect(connectorConfig.originalsWithPrefix(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX)).andReturn(connConfig);    PowerMock.replayAll();    assertEquals(expectedConfigs, Worker.consumerConfigs(new ConnectorTaskId("test", 1), configWithOverrides, connectorConfig, null, allConnectorClientConfigOverridePolicy));}
private void kafkatest_f11545_0(Boolean expectDefaultConverters)
{    expectConverters(JsonConverter.class, expectDefaultConverters);}
private void kafkatest_f11546_0(Class<? extends Converter> converterClass, Boolean expectDefaultConverters)
{    // As default converters are instantiated when a task starts, they are expected only if the `startTask` method is called    if (expectDefaultConverters) {        // Instantiate and configure default        EasyMock.expect(plugins.newConverter(config, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS)).andReturn(keyConverter);        EasyMock.expect(plugins.newConverter(config, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS)).andReturn(valueConverter);        EasyMock.expectLastCall();    }    // internal    Converter internalKeyConverter = PowerMock.createMock(converterClass);    Converter internalValueConverter = PowerMock.createMock(converterClass);    // Instantiate and configure internal    EasyMock.expect(plugins.newConverter(config, WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS)).andReturn(internalKeyConverter);    EasyMock.expect(plugins.newConverter(config, WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS)).andReturn(internalValueConverter);    EasyMock.expectLastCall();}
public String kafkatest_f11557_0()
{    return "1.0";}
public List<SourceRecord> kafkatest_f11559_0() throws InterruptedException
{    return null;}
public static WorkerLoad kafkatest_f11569_0(String worker)
{    return new WorkerLoad.Builder(worker).build();}
public WorkerLoad kafkatest_f11570_0(String worker, int connectorStart, int connectorNum, int taskStart, int taskNum)
{    return new WorkerLoad.Builder(worker).with(newConnectors(connectorStart, connectorStart + connectorNum), newTasks(taskStart, taskStart + taskNum)).build();}
public static Map<ConnectorTaskId, Map<String, String>> kafkatest_f11579_0(int start, int connectorNum, int taskNum)
{    return IntStream.range(start, taskNum + 1).mapToObj(i -> new SimpleEntry<>(new ConnectorTaskId("connector" + i / connectorNum + 1, i), new HashMap<String, String>())).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
public static String kafkatest_f11580_0(String givenLeader)
{    return "http://" + givenLeader + ":8083";}
private Callback<Void> kafkatest_f11589_0()
{    @SuppressWarnings("unchecked")    Callback<Void> setCallback = PowerMock.createMock(Callback.class);    setCallback.onCompletion(EasyMock.isNull(Throwable.class), EasyMock.isNull(Void.class));    PowerMock.expectLastCall();    return setCallback;}
private Callback<Map<ByteBuffer, ByteBuffer>> kafkatest_f11590_0()
{    Callback<Map<ByteBuffer, ByteBuffer>> getCallback = PowerMock.createMock(Callback.class);    getCallback.onCompletion(EasyMock.isNull(Throwable.class), EasyMock.anyObject(Map.class));    PowerMock.expectLastCall();    return getCallback;}
public void kafkatest_f11599_0() throws Exception
{    // verify that we handle connector deletions correctly when they come up through the log    expectConfigure();    List<ConsumerRecord<String, byte[]>> existingRecords = Arrays.asList(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0)), new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(1)), new ConsumerRecord<>(TOPIC, 0, 2, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(1), CONFIGS_SERIALIZED.get(2)), new ConsumerRecord<>(TOPIC, 0, 3, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, COMMIT_TASKS_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(3)));    LinkedHashMap<byte[], Struct> deserialized = new LinkedHashMap<>();    deserialized.put(CONFIGS_SERIALIZED.get(0), CONNECTOR_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(1), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(2), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(3), TASKS_COMMIT_STRUCT_TWO_TASK_CONNECTOR);    logOffset = 5;    expectStart(existingRecords, deserialized);    LinkedHashMap<String, byte[]> serializedData = new LinkedHashMap<>();    serializedData.put(CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0));    serializedData.put(TARGET_STATE_KEYS.get(0), CONFIGS_SERIALIZED.get(1));    Map<String, Struct> deserializedData = new HashMap<>();    deserializedData.put(CONNECTOR_CONFIG_KEYS.get(0), null);    deserializedData.put(TARGET_STATE_KEYS.get(0), null);    expectRead(serializedData, deserializedData);    configUpdateListener.onConnectorConfigRemove(CONNECTOR_IDS.get(0));    EasyMock.expectLastCall();    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    configStorage.start();    // Should see a single connector with initial state paused    ClusterConfigState configState = configStorage.snapshot();    assertEquals(TargetState.STARTED, configState.targetState(CONNECTOR_IDS.get(0)));    configStorage.refresh(0, TimeUnit.SECONDS);    configStorage.stop();    PowerMock.verifyAll();}
public void kafkatest_f11600_0() throws Exception
{    expectConfigure();    List<ConsumerRecord<String, byte[]>> existingRecords = Arrays.asList(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0)), new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(1)), new ConsumerRecord<>(TOPIC, 0, 2, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(1), CONFIGS_SERIALIZED.get(2)), new ConsumerRecord<>(TOPIC, 0, 3, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TARGET_STATE_KEYS.get(0), CONFIGS_SERIALIZED.get(3)), new ConsumerRecord<>(TOPIC, 0, 4, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, COMMIT_TASKS_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(4)));    LinkedHashMap<byte[], Struct> deserialized = new LinkedHashMap<>();    deserialized.put(CONFIGS_SERIALIZED.get(0), CONNECTOR_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(1), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(2), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(3), null);    deserialized.put(CONFIGS_SERIALIZED.get(4), TASKS_COMMIT_STRUCT_TWO_TASK_CONNECTOR);    logOffset = 5;    expectStart(existingRecords, deserialized);    // Shouldn't see any callbacks since this is during startup    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    configStorage.start();    // The target state deletion should reset the state to STARTED    ClusterConfigState configState = configStorage.snapshot();    // Should always be next to be read, even if uncommitted    assertEquals(5, configState.offset());    assertEquals(Arrays.asList(CONNECTOR_IDS.get(0)), new ArrayList<>(configState.connectors()));    assertEquals(TargetState.STARTED, configState.targetState(CONNECTOR_IDS.get(0)));    configStorage.stop();    PowerMock.verifyAll();}
private void kafkatest_f11609_0(LinkedHashMap<String, byte[]> serializedValues, Map<String, Struct> deserializedValues)
{    expectReadToEnd(serializedValues);    for (Map.Entry<String, Struct> deserializedValueEntry : deserializedValues.entrySet()) {        byte[] serializedValue = serializedValues.get(deserializedValueEntry.getKey());        EasyMock.expect(converter.toConnectData(EasyMock.eq(TOPIC), EasyMock.aryEq(serializedValue))).andReturn(new SchemaAndValue(null, structToMap(deserializedValueEntry.getValue())));    }}
private void kafkatest_f11610_0(final String key, final byte[] serializedValue, Struct deserializedValue)
{    LinkedHashMap<String, byte[]> serializedData = new LinkedHashMap<>();    serializedData.put(key, serializedValue);    expectRead(serializedData, Collections.singletonMap(key, deserializedValue));}
public void kafkatest_f11619_0() throws Exception
{    store = PowerMock.createPartialMockAndInvokeDefaultConstructor(KafkaOffsetBackingStore.class, "createKafkaBasedLog");}
public void kafkatest_f11620_0() throws Exception
{    expectConfigure();    expectStart(Collections.emptyList());    expectStop();    PowerMock.replayAll();    store.configure(DEFAULT_DISTRIBUTED_CONFIG);    assertEquals(TOPIC, capturedTopic.getValue());    assertEquals("org.apache.kafka.common.serialization.ByteArraySerializer", capturedProducerProps.getValue().get(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.ByteArraySerializer", capturedProducerProps.getValue().get(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.ByteArrayDeserializer", capturedConsumerProps.getValue().get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.ByteArrayDeserializer", capturedConsumerProps.getValue().get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG));    assertEquals(TOPIC, capturedNewTopic.getValue().name());    assertEquals(TOPIC_PARTITIONS, capturedNewTopic.getValue().numPartitions());    assertEquals(TOPIC_REPLICATION_FACTOR, capturedNewTopic.getValue().replicationFactor());    store.start();    store.stop();    PowerMock.verifyAll();}
public void kafkatest_f11629_0() throws Exception
{    expectConfigure();    expectStart(Collections.emptyList());    // Set offsets    Capture<org.apache.kafka.clients.producer.Callback> callback0 = EasyMock.newCapture();    storeLog.send(EasyMock.isNull(byte[].class), EasyMock.aryEq(TP0_VALUE.array()), EasyMock.capture(callback0));    PowerMock.expectLastCall();    Capture<org.apache.kafka.clients.producer.Callback> callback1 = EasyMock.newCapture();    storeLog.send(EasyMock.aryEq(TP1_KEY.array()), EasyMock.isNull(byte[].class), EasyMock.capture(callback1));    PowerMock.expectLastCall();    // Second get() should get the produced data and return the new values    final Capture<Callback<Void>> secondGetReadToEndCallback = EasyMock.newCapture();    storeLog.readToEnd(EasyMock.capture(secondGetReadToEndCallback));    PowerMock.expectLastCall().andAnswer(() -> {        capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, (byte[]) null, TP0_VALUE.array()));        capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), (byte[]) null));        secondGetReadToEndCallback.getValue().onCompletion(null, null);        return null;    });    expectStop();    PowerMock.replayAll();    store.configure(DEFAULT_DISTRIBUTED_CONFIG);    store.start();    // Set offsets using null keys and values    Map<ByteBuffer, ByteBuffer> toSet = new HashMap<>();    toSet.put(null, TP0_VALUE);    toSet.put(TP1_KEY, null);    final AtomicBoolean invoked = new AtomicBoolean(false);    Future<Void> setFuture = store.set(toSet, new Callback<Void>() {        @Override        public void onCompletion(Throwable error, Void result) {            invoked.set(true);        }    });    assertFalse(setFuture.isDone());    // Out of order callbacks shouldn't matter, should still require all to be invoked before invoking the callback    // for the store's set callback    callback1.getValue().onCompletion(null, null);    assertFalse(invoked.get());    callback0.getValue().onCompletion(null, null);    setFuture.get(10000, TimeUnit.MILLISECONDS);    assertTrue(invoked.get());    // Getting data should read to end of our published data and return it    final AtomicBoolean secondGetInvokedAndPassed = new AtomicBoolean(false);    store.get(Arrays.asList(null, TP1_KEY), new Callback<Map<ByteBuffer, ByteBuffer>>() {        @Override        public void onCompletion(Throwable error, Map<ByteBuffer, ByteBuffer> result) {            assertEquals(TP0_VALUE, result.get(null));            assertNull(result.get(TP1_KEY));            secondGetInvokedAndPassed.set(true);        }    }).get(10000, TimeUnit.MILLISECONDS);    assertTrue(secondGetInvokedAndPassed.get());    store.stop();    PowerMock.verifyAll();}
public void kafkatest_f11630_0(Throwable error, Void result)
{    invoked.set(true);}
public void kafkatest_f11639_0()
{    KafkaBasedLog<String, byte[]> kafkaBasedLog = mock(KafkaBasedLog.class);    Converter converter = mock(Converter.class);    KafkaStatusBackingStore store = new KafkaStatusBackingStore(new MockTime(), converter, STATUS_TOPIC, kafkaBasedLog);    byte[] value = new byte[0];    expect(converter.fromConnectData(eq(STATUS_TOPIC), anyObject(Schema.class), anyObject(Struct.class))).andStubReturn(value);    final Capture<Callback> callbackCapture = newCapture();    kafkaBasedLog.send(eq("status-connector-conn"), eq(value), capture(callbackCapture));    expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callbackCapture.getValue().onCompletion(null, null);            return null;        }    });    replayAll();    ConnectorStatus status = new ConnectorStatus(CONNECTOR, ConnectorStatus.State.RUNNING, WORKER_ID, 0);    store.put(status);    // state is not visible until read back from the log    assertEquals(null, store.get(CONNECTOR));    verifyAll();}
public Void kafkatest_f11640_0() throws Throwable
{    callbackCapture.getValue().onCompletion(null, null);    return null;}
public Void kafkatest_f11649_0() throws Throwable
{    callbackCapture.getValue().onCompletion(null, null);    store.read(consumerRecord(1, "status-connector-conn", value));    return null;}
public void kafkatest_f11650_0()
{    final byte[] value = new byte[0];    String otherWorkerId = "anotherhost:8083";    KafkaBasedLog<String, byte[]> kafkaBasedLog = mock(KafkaBasedLog.class);    Converter converter = mock(Converter.class);    final KafkaStatusBackingStore store = new KafkaStatusBackingStore(new MockTime(), converter, STATUS_TOPIC, kafkaBasedLog);    // the persisted came from a different host and has a newer generation    Map<String, Object> firstStatusRead = new HashMap<>();    firstStatusRead.put("worker_id", otherWorkerId);    firstStatusRead.put("state", "RUNNING");    firstStatusRead.put("generation", 1L);    Map<String, Object> secondStatusRead = new HashMap<>();    secondStatusRead.put("worker_id", WORKER_ID);    secondStatusRead.put("state", "UNASSIGNED");    secondStatusRead.put("generation", 0L);    expect(converter.toConnectData(STATUS_TOPIC, value)).andReturn(new SchemaAndValue(null, firstStatusRead)).andReturn(new SchemaAndValue(null, secondStatusRead));    expect(converter.fromConnectData(eq(STATUS_TOPIC), anyObject(Schema.class), anyObject(Struct.class))).andStubReturn(value);    final Capture<Callback> callbackCapture = newCapture();    kafkaBasedLog.send(eq("status-connector-conn"), eq(value), capture(callbackCapture));    expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callbackCapture.getValue().onCompletion(null, null);            store.read(consumerRecord(1, "status-connector-conn", value));            return null;        }    });    replayAll();    store.read(consumerRecord(0, "status-connector-conn", value));    ConnectorStatus status = new ConnectorStatus(CONNECTOR, ConnectorStatus.State.UNASSIGNED, WORKER_ID, 0);    store.put(status);    assertEquals(status, store.get(CONNECTOR));    verifyAll();}
public void kafkatest_f11659_0()
{    MemoryStatusBackingStore store = new MemoryStatusBackingStore();    store.put(new ConnectorStatus("connector", ConnectorStatus.State.RUNNING, "localhost:8083", 0));    store.put(new ConnectorStatus("connector", ConnectorStatus.State.DESTROYED, "localhost:8083", 0));    assertNull(store.get("connector"));}
public void kafkatest_f11660_0()
{    MemoryStatusBackingStore store = new MemoryStatusBackingStore();    ConnectorTaskId taskId = new ConnectorTaskId("connector", 0);    store.put(new TaskStatus(taskId, ConnectorStatus.State.RUNNING, "localhost:8083", 0));    store.put(new TaskStatus(taskId, ConnectorStatus.State.DESTROYED, "localhost:8083", 0));    assertNull(store.get(taskId));}
public void kafkatest_f11669_0()
{    PowerMock.replayAll();    writer.offset(OFFSET_KEY, OFFSET_VALUE);    assertTrue(writer.beginFlush());    writer.cancelFlush();    PowerMock.verifyAll();}
public void kafkatest_f11670_0() throws Exception
{    @SuppressWarnings("unchecked")    Callback<Void> callback = PowerMock.createMock(Callback.class);    CountDownLatch allowStoreCompleteCountdown = new CountDownLatch(1);    // In this test, the write should be cancelled so the callback will not be invoked and is not    // passed to the expectStore call    expectStore(OFFSET_KEY, OFFSET_KEY_SERIALIZED, OFFSET_VALUE, OFFSET_VALUE_SERIALIZED, null, false, allowStoreCompleteCountdown);    PowerMock.replayAll();    writer.offset(OFFSET_KEY, OFFSET_VALUE);    assertTrue(writer.beginFlush());    // Start the flush, then immediately cancel before allowing the mocked store request to finish    Future<Void> flushFuture = writer.doFlush(callback);    writer.cancelFlush();    allowStoreCompleteCountdown.countDown();    flushFuture.get(1000, TimeUnit.MILLISECONDS);    PowerMock.verifyAll();}
public WorkerHandlef11679_1)
{    WorkerHandle worker = WorkerHandle.start(workerNamePrefix + nextWorkerId.getAndIncrement(), workerProps);    connectCluster.add(worker);        return worker;}
public void kafkatest_f11680_0()
{    WorkerHandle toRemove = null;    for (Iterator<WorkerHandle> it = connectCluster.iterator(); it.hasNext(); toRemove = it.next()) {    }    removeWorker(toRemove);}
public ConnectorStateInfof11689_1String connectorName)
{    ObjectMapper mapper = new ObjectMapper();    try {        String url = endpointForResource(String.format("connectors/%s/status", connectorName));        return mapper.readerFor(ConnectorStateInfo.class).readValue(executeGet(url));    } catch (IOException e) {                throw new ConnectException("Could not read connector state", e);    }}
public String kafkatest_f11690_0(String resource) throws IOException
{    String url = connectCluster.stream().map(WorkerHandle::url).filter(Objects::nonNull).findFirst().orElseThrow(() -> new IOException("Connect workers have not been provisioned")).toString();    return url + resource;}
public Builder kafkatest_f11699_0(int numWorkers)
{    this.numWorkers = numWorkers;    return this;}
public Builder kafkatest_f11700_0(int numBrokers)
{    this.numBrokers = numBrokers;    return this;}
private String kafkatest_f11709_0() throws IOException
{    TemporaryFolder tmpFolder = new TemporaryFolder();    tmpFolder.create();    return tmpFolder.newFolder().getAbsolutePath();}
public String kafkatest_f11710_0()
{    return Arrays.stream(brokers).map(this::address).collect(Collectors.joining(","));}
public Admin kafkatest_f11719_0()
{    final Properties adminClientConfig = new Properties();    adminClientConfig.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers());    final Object listeners = brokerConfig.get(KafkaConfig$.MODULE$.ListenersProp());    if (listeners != null && listeners.toString().contains("SSL")) {        adminClientConfig.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));        adminClientConfig.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());        adminClientConfig.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");    }    return Admin.create(adminClientConfig);}
public ConsumerRecords<byte[], byte[]>f11720_1int n, long maxDuration, String... topics)
{    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> records = new HashMap<>();    int consumedRecords = 0;    try (KafkaConsumer<byte[], byte[]> consumer = createConsumerAndSubscribeTo(Collections.emptyMap(), topics)) {        final long startMillis = System.currentTimeMillis();        long allowedDuration = maxDuration;        while (allowedDuration > 0) {                        ConsumerRecords<byte[], byte[]> rec = consumer.poll(Duration.ofMillis(allowedDuration));            if (rec.isEmpty()) {                allowedDuration = maxDuration - (System.currentTimeMillis() - startMillis);                continue;            }            for (TopicPartition partition : rec.partitions()) {                final List<ConsumerRecord<byte[], byte[]>> r = rec.records(partition);                records.computeIfAbsent(partition, t -> new ArrayList<>()).addAll(r);                consumedRecords += r.size();            }            if (consumedRecords >= n) {                return new ConsumerRecords<>(records);            }            allowedDuration = maxDuration - (System.currentTimeMillis() - startMillis);        }    }    throw new RuntimeException("Could not find enough records. found " + consumedRecords + ", expected " + n);}
public boolean kafkatest_f11729_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof WorkerHandle)) {        return false;    }    WorkerHandle that = (WorkerHandle) o;    return Objects.equals(workerName, that.workerName) && Objects.equals(worker, that.worker);}
public int kafkatest_f11730_0()
{    return Objects.hash(workerName, worker);}
public void kafkatest_f11739_0()
{    consumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE));}
public void kafkatest_f11740_0()
{    consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY, TP1_VALUE));}
public void kafkatest_f11749_0()
{    consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY, TP1_VALUE_NEW));}
public void kafkatest_f11750_0() throws Exception
{    expectStart();    expectStop();    PowerMock.replayAll();    final CountDownLatch finishedLatch = new CountDownLatch(1);    Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(TP0, 1L);    endOffsets.put(TP1, 1L);    consumer.updateEndOffsets(endOffsets);    consumer.schedulePollTask(new Runnable() {        @Override        public void run() {            // Trigger exception            consumer.schedulePollTask(new Runnable() {                @Override                public void run() {                    consumer.setPollException(Errors.COORDINATOR_NOT_AVAILABLE.exception());                }            });            // Should keep polling until it reaches current log end offset for all partitions            consumer.scheduleNopPollTask();            consumer.scheduleNopPollTask();            consumer.schedulePollTask(new Runnable() {                @Override                public void run() {                    consumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE_NEW));                    consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE_NEW));                }            });            consumer.schedulePollTask(new Runnable() {                @Override                public void run() {                    finishedLatch.countDown();                }            });        }    });    store.start();    assertTrue(finishedLatch.await(10000, TimeUnit.MILLISECONDS));    assertEquals(CONSUMER_ASSIGNMENT, consumer.assignment());    assertEquals(1L, consumer.position(TP0));    store.stop();    assertFalse(Whitebox.<Thread>getInternalState(store, "thread").isAlive());    assertTrue(consumer.closed());    PowerMock.verifyAll();}
public void kafkatest_f11759_0()
{    finishedLatch.countDown();}
public void kafkatest_f11760_0() throws Exception
{    expectStart();    TestFuture<RecordMetadata> tp0Future = new TestFuture<>();    ProducerRecord<String, String> tp0Record = new ProducerRecord<>(TOPIC, TP0_KEY, TP0_VALUE);    Capture<org.apache.kafka.clients.producer.Callback> callback0 = EasyMock.newCapture();    EasyMock.expect(producer.send(EasyMock.eq(tp0Record), EasyMock.capture(callback0))).andReturn(tp0Future);    expectStop();    PowerMock.replayAll();    Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(TP0, 0L);    endOffsets.put(TP1, 0L);    consumer.updateEndOffsets(endOffsets);    store.start();    assertEquals(CONSUMER_ASSIGNMENT, consumer.assignment());    assertEquals(0L, consumer.position(TP0));    assertEquals(0L, consumer.position(TP1));    final AtomicReference<Throwable> setException = new AtomicReference<>();    store.send(TP0_KEY, TP0_VALUE, new org.apache.kafka.clients.producer.Callback() {        @Override        public void onCompletion(RecordMetadata metadata, Exception exception) {            // Should only be invoked once            assertNull(setException.get());            setException.set(exception);        }    });    KafkaException exc = new LeaderNotAvailableException("Error");    tp0Future.resolve(exc);    callback0.getValue().onCompletion(null, exc);    assertNotNull(setException.get());    store.stop();    assertFalse(Whitebox.<Thread>getInternalState(store, "thread").isAlive());    assertTrue(consumer.closed());    PowerMock.verifyAll();}
public void kafkatest_f11769_0()
{    LoggingContext.forOffsets(null);}
public voidf11770_1)
{    MDC.clear();    assertMdc(null, null, null);    try (LoggingContext loggingContext = LoggingContext.forConnector(CONNECTOR_NAME)) {        assertMdc(CONNECTOR_NAME, null, Scope.WORKER);            }    assertMdc(null, null, null);}
public void kafkatest_f11779_0() throws InterruptedException
{    ShutdownableThread thread = new ShutdownableThread("graceful") {        @Override        public void execute() {            while (getRunning()) {                try {                    Thread.sleep(1);                } catch (InterruptedException e) {                // Ignore                }            }        }    };    thread.start();    Thread.sleep(10);    assertTrue(thread.gracefulShutdown(1000, TimeUnit.MILLISECONDS));}
public void kafkatest_f11780_0()
{    while (getRunning()) {        try {            Thread.sleep(1);        } catch (InterruptedException e) {        // Ignore        }    }}
public boolean kafkatest_f11789_0()
{    return false;}
public boolean kafkatest_f11790_0()
{    return resolved;}
public void kafkatest_f11799_0()
{    backgroundThreadExceptionHandler.verifyNoExceptions();    ShutdownableThread.funcaughtExceptionHandler = null;}
public void kafkatest_f11800_0()
{    final NewTopic newTopic = TopicAdmin.defineTopic("myTopic").partitions(1).compacted().build();    Cluster cluster = createCluster(1);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(new MockTime(), cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(createTopicResponseWithUnsupportedVersion(newTopic));        TopicAdmin admin = new TopicAdmin(null, env.adminClient());        boolean created = admin.createTopic(newTopic);        assertFalse(created);    }}
private CreateTopicsResponse kafkatest_f11809_0(NewTopic... topics)
{    return createTopicResponse(new ApiError(Errors.CLUSTER_AUTHORIZATION_FAILED, "Not authorized to create topic(s)"), topics);}
private CreateTopicsResponse kafkatest_f11810_0(NewTopic... topics)
{    return createTopicResponse(new ApiError(Errors.TOPIC_AUTHORIZATION_FAILED, "Not authorized to create topic(s)"), topics);}
private Schema kafkatest_f11820_0(Schema valueSchema)
{    Schema updatedSchema = schemaUpdateCache.get(valueSchema);    if (updatedSchema != null)        return updatedSchema;    final SchemaBuilder builder;    if (wholeValueCastType != null) {        builder = SchemaUtil.copySchemaBasics(valueSchema, convertFieldType(wholeValueCastType));    } else {        builder = SchemaUtil.copySchemaBasics(valueSchema, SchemaBuilder.struct());        for (Field field : valueSchema.fields()) {            if (casts.containsKey(field.name())) {                SchemaBuilder fieldBuilder = convertFieldType(casts.get(field.name()));                if (field.schema().isOptional())                    fieldBuilder.optional();                if (field.schema().defaultValue() != null) {                    Schema fieldSchema = field.schema();                    fieldBuilder.defaultValue(castValueToType(fieldSchema, fieldSchema.defaultValue(), fieldBuilder.type()));                }                builder.field(field.name(), fieldBuilder.build());            } else {                builder.field(field.name(), field.schema());            }        }    }    if (valueSchema.isOptional())        builder.optional();    if (valueSchema.defaultValue() != null)        builder.defaultValue(castValueToType(valueSchema, valueSchema.defaultValue(), builder.type()));    updatedSchema = builder.build();    schemaUpdateCache.put(valueSchema, updatedSchema);    return updatedSchema;}
private SchemaBuilder kafkatest_f11821_0(Schema.Type type)
{    switch(type) {        case INT8:            return SchemaBuilder.int8();        case INT16:            return SchemaBuilder.int16();        case INT32:            return SchemaBuilder.int32();        case INT64:            return SchemaBuilder.int64();        case FLOAT32:            return SchemaBuilder.float32();        case FLOAT64:            return SchemaBuilder.float64();        case BOOLEAN:            return SchemaBuilder.bool();        case STRING:            return SchemaBuilder.string();        default:            throw new DataException("Unexpected type in Cast transformation: " + type);    }}
private static String kafkatest_f11830_0(Object value)
{    if (value instanceof java.util.Date) {        java.util.Date dateValue = (java.util.Date) value;        return Values.dateFormatFor(dateValue).format(dateValue);    } else {        return value.toString();    }}
private static Map<String, Schema.Type> kafkatest_f11831_0(List<String> mappings)
{    final Map<String, Schema.Type> m = new HashMap<>();    boolean isWholeValueCast = false;    for (String mapping : mappings) {        final String[] parts = mapping.split(":");        if (parts.length > 2) {            throw new ConfigException(ReplaceField.ConfigName.RENAME, mappings, "Invalid rename mapping: " + mapping);        }        if (parts.length == 1) {            Schema.Type targetType = Schema.Type.valueOf(parts[0].trim().toUpperCase(Locale.ROOT));            m.put(WHOLE_VALUE_CAST, validCastType(targetType, FieldType.OUTPUT));            isWholeValueCast = true;        } else {            Schema.Type type;            try {                type = Schema.Type.valueOf(parts[1].trim().toUpperCase(Locale.ROOT));            } catch (IllegalArgumentException e) {                throw new ConfigException("Invalid type found in casting spec: " + parts[1].trim(), e);            }            m.put(parts[0].trim(), validCastType(type, FieldType.OUTPUT));        }    }    if (isWholeValueCast && mappings.size() > 1) {        throw new ConfigException("Cast transformations that specify a type to cast the entire value to " + "may ony specify a single cast in their spec");    }    return m;}
public R kafkatest_f11840_0(R record)
{    final Schema schema = operatingSchema(record);    if (schema == null) {        final Map<String, Object> value = requireMapOrNull(operatingValue(record), PURPOSE);        return newRecord(record, null, value == null ? null : value.get(fieldName));    } else {        final Struct value = requireStructOrNull(operatingValue(record), PURPOSE);        return newRecord(record, schema.field(fieldName).schema(), value == null ? null : value.get(fieldName));    }}
public ConfigDef kafkatest_f11842_0()
{    return CONFIG_DEF;}
public ConfigDef kafkatest_f11852_0()
{    return CONFIG_DEF;}
private R kafkatest_f11853_0(R record)
{    final Map<String, Object> value = requireMap(operatingValue(record), PURPOSE);    final Map<String, Object> newValue = new LinkedHashMap<>();    applySchemaless(value, "", newValue);    return newRecord(record, null, newValue);}
protected R kafkatest_f11862_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());}
protected Schema kafkatest_f11863_0(R record)
{    return record.valueSchema();}
protected R kafkatest_f11872_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());}
protected Schema kafkatest_f11873_0(R record)
{    return record.valueSchema();}
public void kafkatest_f11882_0()
{    schemaUpdateCache = null;}
public ConfigDef kafkatest_f11883_0()
{    return CONFIG_DEF;}
private R kafkatest_f11892_0(R record)
{    final Map<String, Object> value = requireMap(operatingValue(record), PURPOSE);    final HashMap<String, Object> updatedValue = new HashMap<>(value);    for (String field : maskedFields) {        updatedValue.put(field, masked(value.get(field)));    }    return newRecord(record, updatedValue);}
private R kafkatest_f11893_0(R record)
{    final Struct value = requireStruct(operatingValue(record), PURPOSE);    final Struct updatedValue = new Struct(value.schema());    for (Field field : value.schema().fields()) {        final Object origFieldValue = value.get(field);        updatedValue.put(field, maskedFields.contains(field.name()) ? masked(origFieldValue) : origFieldValue);    }    return newRecord(record, updatedValue);}
public void kafkatest_f11903_0(Map<String, ?> props)
{    final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);    regex = Pattern.compile(config.getString(ConfigName.REGEX));    replacement = config.getString(ConfigName.REPLACEMENT);}
public R kafkatest_f11904_0(R record)
{    final Matcher matcher = regex.matcher(record.topic());    if (matcher.matches()) {        final String topic = matcher.replaceFirst(replacement);        return record.newRecord(topic, record.kafkaPartition(), record.keySchema(), record.key(), record.valueSchema(), record.value(), record.timestamp());    }    return record;}
 String kafkatest_f11914_0(String fieldName)
{    final String mapping = reverseRenames.get(fieldName);    return mapping == null ? fieldName : mapping;}
public R kafkatest_f11915_0(R record)
{    if (operatingSchema(record) == null) {        return applySchemaless(record);    } else {        return applyWithSchema(record);    }}
protected Schema kafkatest_f11924_0(R record)
{    return record.valueSchema();}
protected Object kafkatest_f11925_0(R record)
{    return record.value();}
protected static Object kafkatest_f11935_0(Object keyOrValue, Schema updatedSchema)
{    if (keyOrValue instanceof Struct) {        Struct origStruct = (Struct) keyOrValue;        Struct newStruct = new Struct(updatedSchema);        for (Field field : updatedSchema.fields()) {            // assume both schemas have exact same fields with same names and schemas ...            newStruct.put(field, origStruct.get(field));        }        return newStruct;    }    return keyOrValue;}
public Date kafkatest_f11936_0(Config config, Object orig)
{    if (!(orig instanceof String))        throw new DataException("Expected string timestamp to be a String, but found " + orig.getClass());    try {        return config.format.parse((String) orig);    } catch (ParseException e) {        throw new DataException("Could not parse timestamp: value (" + orig + ") does not match pattern (" + config.format.toPattern() + ")", e);    }}
public Date kafkatest_f11945_0(Config config, Object orig)
{    if (!(orig instanceof Date))        throw new DataException("Expected Time to be a java.util.Date, but found " + orig.getClass());    // Already represented as a java.util.Date and Connect Times are a subset of valid java.util.Date values    return (Date) orig;}
public Schema kafkatest_f11946_0(boolean isOptional)
{    return isOptional ? OPTIONAL_TIME_SCHEMA : Time.SCHEMA;}
protected Object kafkatest_f11956_0(R record)
{    return record.key();}
protected R kafkatest_f11957_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());}
private Object kafkatest_f11966_0(Object timestamp, String timestampFormat)
{    if (timestamp == null) {        return null;    }    if (timestampFormat == null) {        timestampFormat = inferTimestampType(timestamp);    }    TimestampTranslator sourceTranslator = TRANSLATORS.get(timestampFormat);    if (sourceTranslator == null) {        throw new ConnectException("Unsupported timestamp type: " + timestampFormat);    }    Date rawTimestamp = sourceTranslator.toRaw(config, timestamp);    TimestampTranslator targetTranslator = TRANSLATORS.get(config.type);    if (targetTranslator == null) {        throw new ConnectException("Unsupported timestamp type: " + config.type);    }    return targetTranslator.toType(config, rawTimestamp);}
private Object kafkatest_f11967_0(Object timestamp)
{    return convertTimestamp(timestamp, null);}
public String kafkatest_f11976_0()
{    return "valid regex";}
public static void kafkatest_f11977_0(Schema schema, String purpose)
{    if (schema == null) {        throw new DataException("Schema required for [" + purpose + "]");    }}
public void kafkatest_f11986_0(Map<String, ?> configs)
{    final SimpleConfig config = new SimpleConfig(CONFIG_DEF, configs);    fields = config.getList(FIELDS_CONFIG);    valueToKeySchemaCache = new SynchronizedCache<>(new LRUCache<Schema, Schema>(16));}
public R kafkatest_f11987_0(R record)
{    if (record.valueSchema() == null) {        return applySchemaless(record);    } else {        return applyWithSchema(record);    }}
public void kafkatest_f11996_0()
{    xformKey.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "foo:bytes"));}
public void kafkatest_f11997_0()
{    xformKey.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "foo:int8:extra"));}
public void kafkatest_f12006_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "boolean"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, Schema.INT32_SCHEMA, 42));    assertEquals(Schema.Type.BOOLEAN, transformed.valueSchema().type());    assertEquals(true, transformed.value());}
public void kafkatest_f12007_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "boolean"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, Schema.INT32_SCHEMA, 0));    assertEquals(Schema.Type.BOOLEAN, transformed.valueSchema().type());    assertEquals(false, transformed.value());}
public void kafkatest_f12016_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "int64"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, null, 42));    assertNull(transformed.valueSchema());    assertEquals((long) 42, transformed.value());}
public void kafkatest_f12017_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "float32"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, null, 42));    assertNull(transformed.valueSchema());    assertEquals(42.f, transformed.value());}
public void kafkatest_f12026_0()
{    xform.configure(Collections.singletonMap("field", "magic"));    final SinkRecord record = new SinkRecord("test", 0, null, Collections.singletonMap("magic", 42), null, null, 0);    final SinkRecord transformedRecord = xform.apply(record);    assertNull(transformedRecord.keySchema());    assertEquals(42, transformedRecord.key());}
public void kafkatest_f12027_0()
{    xform.configure(Collections.singletonMap("field", "magic"));    final Map<String, Object> key = null;    final SinkRecord record = new SinkRecord("test", 0, null, key, null, null, 0);    final SinkRecord transformedRecord = xform.apply(record);    assertNull(transformedRecord.keySchema());    assertNull(transformedRecord.key());}
public void kafkatest_f12036_0()
{    xformValue.configure(Collections.<String, String>emptyMap());    SchemaBuilder builder = SchemaBuilder.struct().optional();    builder.field("opt_int32", Schema.OPTIONAL_INT32_SCHEMA);    Schema schema = builder.build();    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, schema, null));    assertEquals(Schema.Type.STRUCT, transformed.valueSchema().type());    assertNull(transformed.value());}
public void kafkatest_f12037_0()
{    xformValue.configure(Collections.<String, String>emptyMap());    SchemaBuilder builder = SchemaBuilder.struct().optional();    builder.field("opt_int32", Schema.OPTIONAL_INT32_SCHEMA);    Schema supportedTypesSchema = builder.build();    builder = SchemaBuilder.struct();    builder.field("B", supportedTypesSchema);    Schema oneLevelNestedSchema = builder.build();    Struct oneLevelNestedStruct = new Struct(oneLevelNestedSchema);    oneLevelNestedStruct.put("B", null);    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, oneLevelNestedSchema, oneLevelNestedStruct));    assertEquals(Schema.Type.STRUCT, transformed.valueSchema().type());    Struct transformedStruct = (Struct) transformed.value();    assertNull(transformedStruct.get("B.opt_int32"));}
public void kafkatest_f12046_0()
{    xform.configure(Collections.singletonMap("topic.field", "topic_field"));    xform.apply(new SourceRecord(null, null, "", 0, Schema.INT32_SCHEMA, 42));}
public void kafkatest_f12047_0()
{    final Map<String, Object> props = new HashMap<>();    props.put("topic.field", "topic_field!");    props.put("partition.field", "partition_field");    props.put("timestamp.field", "timestamp_field?");    props.put("static.field", "instance_id");    props.put("static.value", "my-instance-id");    xform.configure(props);    final Schema simpleStructSchema = SchemaBuilder.struct().name("name").version(1).doc("doc").field("magic", Schema.OPTIONAL_INT64_SCHEMA).build();    final Struct simpleStruct = new Struct(simpleStructSchema).put("magic", 42L);    final SourceRecord record = new SourceRecord(null, null, "test", 0, simpleStructSchema, simpleStruct);    final SourceRecord transformedRecord = xform.apply(record);    assertEquals(simpleStructSchema.name(), transformedRecord.valueSchema().name());    assertEquals(simpleStructSchema.version(), transformedRecord.valueSchema().version());    assertEquals(simpleStructSchema.doc(), transformedRecord.valueSchema().doc());    assertEquals(Schema.OPTIONAL_INT64_SCHEMA, transformedRecord.valueSchema().field("magic").schema());    assertEquals(42L, ((Struct) transformedRecord.value()).getInt64("magic").longValue());    assertEquals(Schema.STRING_SCHEMA, transformedRecord.valueSchema().field("topic_field").schema());    assertEquals("test", ((Struct) transformedRecord.value()).getString("topic_field"));    assertEquals(Schema.OPTIONAL_INT32_SCHEMA, transformedRecord.valueSchema().field("partition_field").schema());    assertEquals(0, ((Struct) transformedRecord.value()).getInt32("partition_field").intValue());    assertEquals(Timestamp.builder().optional().build(), transformedRecord.valueSchema().field("timestamp_field").schema());    assertEquals(null, ((Struct) transformedRecord.value()).getInt64("timestamp_field"));    assertEquals(Schema.OPTIONAL_STRING_SCHEMA, transformedRecord.valueSchema().field("instance_id").schema());    assertEquals("my-instance-id", ((Struct) transformedRecord.value()).getString("instance_id"));    // Exercise caching    final SourceRecord transformedRecord2 = xform.apply(new SourceRecord(null, null, "test", 1, simpleStructSchema, new Struct(simpleStructSchema)));    assertSame(transformedRecord.valueSchema(), transformedRecord2.valueSchema());}
public void kafkatest_f12056_0()
{    assertEquals("orig", apply("(.*)", "$1", "orig"));}
public void kafkatest_f12057_0()
{    assertEquals("prefix-orig", apply("(.*)", "prefix-$1", "orig"));}
public void kafkatest_f12066_0()
{    final Map<String, String> props = new HashMap<>();    props.put("schema.name", "foo");    props.put("schema.version", "42");    xform.configure(props);    final SinkRecord record = new SinkRecord("", 0, null, null, SchemaBuilder.struct().build(), null, 0);    final SinkRecord updatedRecord = xform.apply(record);    assertEquals("foo", updatedRecord.valueSchema().name());    assertEquals(new Integer(42), updatedRecord.valueSchema().version());}
public void kafkatest_f12067_0()
{    final String fieldName1 = "f1";    final String fieldName2 = "f2";    final String fieldValue1 = "value1";    final int fieldValue2 = 1;    final Schema schema = SchemaBuilder.struct().name("my.orig.SchemaDefn").field(fieldName1, Schema.STRING_SCHEMA).field(fieldName2, Schema.INT32_SCHEMA).build();    final Struct value = new Struct(schema).put(fieldName1, fieldValue1).put(fieldName2, fieldValue2);    final Map<String, String> props = new HashMap<>();    props.put("schema.name", "foo");    props.put("schema.version", "42");    xform.configure(props);    final SinkRecord record = new SinkRecord("", 0, null, null, schema, value, 0);    final SinkRecord updatedRecord = xform.apply(record);    assertEquals("foo", updatedRecord.valueSchema().name());    assertEquals(new Integer(42), updatedRecord.valueSchema().version());    // Make sure the struct's schema and fields all point to the new schema    assertMatchingSchema((Struct) updatedRecord.value(), updatedRecord.valueSchema());}
public void kafkatest_f12076_0()
{    Map<String, String> config = new HashMap<>();    config.put(TimestampConverter.TARGET_TYPE_CONFIG, "string");    config.put(TimestampConverter.FORMAT_CONFIG, "bad-format");    xformValue.configure(config);}
public void kafkatest_f12077_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformValue.apply(createRecordSchemaless(DATE_PLUS_TIME.getTime()));    assertNull(transformed.valueSchema());    assertEquals(DATE_PLUS_TIME.getTime(), transformed.value());}
public void kafkatest_f12086_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Timestamp.SCHEMA, DATE_PLUS_TIME.getTime()));    assertEquals(Timestamp.SCHEMA, transformed.valueSchema());    assertEquals(DATE_PLUS_TIME.getTime(), transformed.value());}
public void kafkatest_f12087_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Date"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Timestamp.SCHEMA, DATE_PLUS_TIME.getTime()));    assertEquals(Date.SCHEMA, transformed.valueSchema());    assertEquals(DATE.getTime(), transformed.value());}
private void kafkatest_f12096_0(String targetType)
{    Map<String, String> config = new HashMap<>();    config.put(TimestampConverter.TARGET_TYPE_CONFIG, targetType);    config.put(TimestampConverter.FORMAT_CONFIG, STRING_DATE_FMT);    xformValue.configure(config);    SourceRecord transformed = xformValue.apply(createRecordSchemaless(null));    assertNull(transformed.valueSchema());    assertNull(transformed.value());}
private void kafkatest_f12097_0(String targetType)
{    Map<String, String> config = new HashMap<>();    config.put(TimestampConverter.TARGET_TYPE_CONFIG, targetType);    config.put(TimestampConverter.FORMAT_CONFIG, STRING_DATE_FMT);    config.put(TimestampConverter.FIELD_CONFIG, "ts");    xformValue.configure(config);    SourceRecord transformed = xformValue.apply(createRecordSchemaless(null));    assertNull(transformed.valueSchema());    assertNull(transformed.value());}
public void kafkatest_f12106_0()
{    testWithSchemaNullValueConversion("Time", Schema.OPTIONAL_INT64_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullValueConversion("Time", TimestampConverter.OPTIONAL_TIME_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullValueConversion("Time", TimestampConverter.OPTIONAL_DATE_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullValueConversion("Time", Schema.OPTIONAL_STRING_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullValueConversion("Time", TimestampConverter.OPTIONAL_TIMESTAMP_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);}
public void kafkatest_f12107_0()
{    testWithSchemaNullFieldConversion("Time", Schema.OPTIONAL_INT64_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullFieldConversion("Time", TimestampConverter.OPTIONAL_TIME_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullFieldConversion("Time", TimestampConverter.OPTIONAL_DATE_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullFieldConversion("Time", Schema.OPTIONAL_STRING_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullFieldConversion("Time", TimestampConverter.OPTIONAL_TIMESTAMP_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);}
public void kafkatest_f12116_0()
{    xformKey.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformKey.apply(new SourceRecord(null, null, "topic", 0, null, DATE_PLUS_TIME.getTime(), null, null));    assertNull(transformed.keySchema());    assertEquals(DATE_PLUS_TIME.getTime(), transformed.key());}
private SourceRecord kafkatest_f12117_0(Schema schema, Object value)
{    return new SourceRecord(null, null, "topic", 0, schema, value);}
public void kafkatest_f12126_0()
{    xform.configure(Collections.singletonMap("fields", "a,b"));    final Schema valueSchema = SchemaBuilder.struct().field("a", Schema.INT32_SCHEMA).field("b", Schema.INT32_SCHEMA).field("c", Schema.INT32_SCHEMA).build();    final Struct value = new Struct(valueSchema);    value.put("a", 1);    value.put("b", 2);    value.put("c", 3);    final SinkRecord record = new SinkRecord("", 0, null, null, valueSchema, value, 0);    final SinkRecord transformedRecord = xform.apply(record);    final Schema expectedKeySchema = SchemaBuilder.struct().field("a", Schema.INT32_SCHEMA).field("b", Schema.INT32_SCHEMA).build();    final Struct expectedKey = new Struct(expectedKeySchema).put("a", 1).put("b", 2);    assertEquals(expectedKeySchema, transformedRecord.keySchema());    assertEquals(expectedKey, transformedRecord.key());}
public int kafkatest_f12127_0(final String[] args)
{    return run(args, new Properties());}
private Map<TopicPartition, Long> kafkatest_f12136_0(final String resetPlanPath) throws IOException, ParseException
{    final String resetPlanCsv = Utils.readFileAsString(resetPlanPath);    return parseResetPlan(resetPlanCsv);}
private void kafkatest_f12137_0(final Consumer<byte[], byte[]> client, final Set<TopicPartition> inputTopicPartitions, final Duration duration)
{    final Instant now = Instant.now();    final long timestamp = now.minus(duration).toEpochMilli();    final Map<TopicPartition, Long> topicPartitionsAndTimes = new HashMap<>(inputTopicPartitions.size());    for (final TopicPartition topicPartition : inputTopicPartitions) {        topicPartitionsAndTimes.put(topicPartition, timestamp);    }    final Map<TopicPartition, OffsetAndTimestamp> topicPartitionsAndOffset = client.offsetsForTimes(topicPartitionsAndTimes);    for (final TopicPartition topicPartition : inputTopicPartitions) {        client.seek(topicPartition, topicPartitionsAndOffset.get(topicPartition).offset());    }}
private void kafkatest_f12146_0(final Admin adminClient, final boolean dryRun)
{    System.out.println("Deleting all internal/auto-created topics for application " + options.valueOf(applicationIdOption));    final List<String> topicsToDelete = new ArrayList<>();    for (final String listing : allTopics) {        if (isInternalTopic(listing)) {            if (!dryRun) {                topicsToDelete.add(listing);            } else {                System.out.println("Topic: " + listing);            }        }    }    if (!dryRun) {        doDelete(topicsToDelete, adminClient);    }    System.out.println("Done.");}
public void kafkatest_f12147_0(final List<String> topicsToDelete, final Admin adminClient)
{    boolean hasDeleteErrors = false;    final DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(topicsToDelete);    final Map<String, KafkaFuture<Void>> results = deleteTopicsResult.values();    for (final Map.Entry<String, KafkaFuture<Void>> entry : results.entrySet()) {        try {            entry.getValue().get(30, TimeUnit.SECONDS);        } catch (final Exception e) {            System.err.println("ERROR: deleting topic " + entry.getKey());            e.printStackTrace(System.err);            hasDeleteErrors = true;        }    }    if (hasDeleteErrors) {        throw new RuntimeException("Encountered an error deleting one or more topics");    }}
 String kafkatest_f12156_0()
{    if (requestSpec != null) {        return MessageGenerator.stripSuffix(requestSpec.name(), MessageGenerator.REQUEST_SUFFIX);    } else if (responseSpec != null) {        return MessageGenerator.stripSuffix(responseSpec.name(), MessageGenerator.RESPONSE_SUFFIX);    } else {        throw new RuntimeException("Neither requestSpec nor responseSpec is defined " + "for API key " + apiKey);    }}
 String kafkatest_f12157_0()
{    if (requestSpec == null) {        return "null";    } else {        return String.format("%sData.SCHEMAS", requestSpec.name());    }}
private void kafkatest_f12166_0(String type)
{    headerGenerator.addImport(MessageGenerator.API_MESSAGE_CLASS);    buffer.printf("public ApiMessage new%s() {%n", MessageGenerator.capitalizeFirst(type));    buffer.incrementIndent();    buffer.printf("switch (apiKey) {%n");    buffer.incrementIndent();    for (Map.Entry<Short, ApiData> entry : apis.entrySet()) {        buffer.printf("case %d:%n", entry.getKey());        buffer.incrementIndent();        buffer.printf("return new %s%sData();%n", entry.getValue().name(), MessageGenerator.capitalizeFirst(type));        buffer.decrementIndent();    }    buffer.printf("default:%n");    buffer.incrementIndent();    headerGenerator.addImport(MessageGenerator.UNSUPPORTED_VERSION_EXCEPTION_CLASS);    buffer.printf("throw new UnsupportedVersionException(\"Unsupported %s API key \"" + " + apiKey);%n", type);    buffer.decrementIndent();    buffer.decrementIndent();    buffer.printf("}%n");    buffer.decrementIndent();    buffer.printf("}%n");}
private void kafkatest_f12167_0(String name, String type)
{    buffer.printf("public %s %s() {%n", type, name);    buffer.incrementIndent();    buffer.printf("return this.%s;%n", name);    buffer.decrementIndent();    buffer.printf("}%n");}
public boolean kafkatest_f12176_0(Object other)
{    if (!(other instanceof CodeBuffer)) {        return false;    }    CodeBuffer o = (CodeBuffer) other;    return lines.equals(o.lines);}
public int kafkatest_f12177_0()
{    return lines.hashCode();}
public String kafkatest_f12186_0()
{    return type.toString();}
public FieldType kafkatest_f12187_0()
{    return type;}
public boolean kafkatest_f12196_0()
{    return true;}
public Optional<Integer> kafkatest_f12197_0()
{    return Optional.of(1);}
public Optional<Integer> kafkatest_f12206_0()
{    return Optional.of(8);}
public String kafkatest_f12207_0()
{    return NAME;}
public boolean kafkatest_f12216_0()
{    return true;}
public String kafkatest_f12217_0()
{    return type;}
 boolean kafkatest_f12226_0()
{    return false;}
 boolean kafkatest_f12227_0()
{    return false;}
 void kafkatest_f12236_0(MessageSpec message) throws Exception
{    if (message.struct().versions().contains(Short.MAX_VALUE)) {        throw new RuntimeException("Message " + message.name() + " does " + "not specify a maximum version.");    }    structRegistry.register(message);    schemaGenerator.generateSchemas(message);    generateClass(Optional.of(message), message.name() + "Data", message.struct(), message.struct().versions());    headerGenerator.generate();}
 void kafkatest_f12237_0(Writer writer) throws Exception
{    headerGenerator.buffer().write(writer);    buffer.write(writer);}
private void kafkatest_f12246_0(String className, StructSpec struct)
{    headerGenerator.addImport(MessageGenerator.LIST_CLASS);    buffer.printf("public List<%s> findAll(%s) {%n", className, commaSeparatedHashSetFieldAndTypes(struct));    buffer.incrementIndent();    generateKeyElement(className, struct);    headerGenerator.addImport(MessageGenerator.IMPLICIT_LINKED_HASH_MULTI_COLLECTION_CLASS);    buffer.printf("return findAll(key);%n");    buffer.decrementIndent();    buffer.printf("}%n");    buffer.printf("%n");}
private void kafkatest_f12247_0(String className, StructSpec struct)
{    buffer.printf("%s key = new %s();%n", className, className);    for (FieldSpec field : struct.fields()) {        if (field.mapKey()) {            buffer.printf("key.set%s(%s);%n", field.capitalizedCamelCaseName(), field.camelCaseName());        }    }}
private void kafkatest_f12256_0(String className, StructSpec struct)
{    headerGenerator.addImport(MessageGenerator.READABLE_CLASS);    buffer.printf("public %s(Readable readable, short version) {%n", className);    buffer.incrementIndent();    initializeArrayDefaults(struct);    buffer.printf("read(readable, version);%n");    buffer.decrementIndent();    buffer.printf("}%n");    buffer.printf("%n");    headerGenerator.addImport(MessageGenerator.STRUCT_CLASS);    buffer.printf("public %s(Struct struct, short version) {%n", className);    buffer.incrementIndent();    initializeArrayDefaults(struct);    buffer.printf("fromStruct(struct, version);%n");    buffer.decrementIndent();    buffer.printf("}%n");    buffer.printf("%n");    buffer.printf("public %s() {%n", className);    buffer.incrementIndent();    for (FieldSpec field : struct.fields()) {        buffer.printf("this.%s = %s;%n", field.camelCaseName(), fieldDefault(field));    }    buffer.decrementIndent();    buffer.printf("}%n");}
private void kafkatest_f12257_0(StructSpec struct)
{    for (FieldSpec field : struct.fields()) {        if (field.type().isArray()) {            buffer.printf("this.%s = %s;%n", field.camelCaseName(), fieldDefault(field));        }    }}
private void kafkatest_f12266_0(String className, StructSpec struct, Versions parentVersions)
{    headerGenerator.addImport(MessageGenerator.WRITABLE_CLASS);    buffer.printf("@Override%n");    buffer.printf("public void write(Writable writable, short version) {%n");    buffer.incrementIndent();    if (generateInverseVersionCheck(parentVersions, struct.versions())) {        buffer.incrementIndent();        headerGenerator.addImport(MessageGenerator.UNSUPPORTED_VERSION_EXCEPTION_CLASS);        buffer.printf("throw new UnsupportedVersionException(\"Can't write " + "version \" + version + \" of %s\");%n", className);        buffer.decrementIndent();        buffer.printf("}%n");    }    Versions curVersions = parentVersions.intersect(struct.versions());    if (curVersions.empty()) {        throw new RuntimeException("Version ranges " + parentVersions + " and " + struct.versions() + " have no versions in common.");    }    for (FieldSpec field : struct.fields()) {        generateFieldWriter(field, curVersions);    }    buffer.decrementIndent();    buffer.printf("}%n");}
private String kafkatest_f12267_0(FieldType type, boolean nullable, String name)
{    if (type instanceof FieldType.BoolFieldType) {        return String.format("writable.writeByte(%s ? (byte) 1 : (byte) 0)", name);    } else if (type instanceof FieldType.Int8FieldType) {        return String.format("writable.writeByte(%s)", name);    } else if (type instanceof FieldType.Int16FieldType) {        return String.format("writable.writeShort(%s)", name);    } else if (type instanceof FieldType.Int32FieldType) {        return String.format("writable.writeInt(%s)", name);    } else if (type instanceof FieldType.Int64FieldType) {        return String.format("writable.writeLong(%s)", name);    } else if (type instanceof FieldType.UUIDFieldType) {        return String.format("writable.writeUUID(%s)", name);    } else if (type instanceof FieldType.StringFieldType) {        if (nullable) {            return String.format("writable.writeNullableString(%s)", name);        } else {            return String.format("writable.writeString(%s)", name);        }    } else if (type instanceof FieldType.BytesFieldType) {        if (nullable) {            return String.format("writable.writeNullableBytes(%s)", name);        } else {            return String.format("writable.writeBytes(%s)", name);        }    } else if (type instanceof FieldType.StructType) {        return String.format("%s.write(writable, version)", name);    } else {        throw new RuntimeException("Unsupported field type " + type);    }}
private void kafkatest_f12276_0(FieldSpec field)
{    if (field.type().isString() || field.type().isArray() || field.type().isStruct()) {        buffer.printf("if (this.%s == null) {%n", field.camelCaseName());        buffer.incrementIndent();        buffer.printf("if (other.%s != null) return false;%n", field.camelCaseName());        buffer.decrementIndent();        buffer.printf("} else {%n");        buffer.incrementIndent();        buffer.printf("if (!this.%s.equals(other.%s)) return false;%n", field.camelCaseName(), field.camelCaseName());        buffer.decrementIndent();        buffer.printf("}%n");    } else if (field.type().isBytes()) {        // Arrays#equals handles nulls.        headerGenerator.addImport(MessageGenerator.ARRAYS_CLASS);        buffer.printf("if (!Arrays.equals(this.%s, other.%s)) return false;%n", field.camelCaseName(), field.camelCaseName());    } else {        buffer.printf("if (%s != other.%s) return false;%n", field.camelCaseName(), field.camelCaseName());    }}
private void kafkatest_f12277_0(StructSpec struct, boolean onlyMapKeys)
{    buffer.printf("@Override%n");    buffer.printf("public int hashCode() {%n");    buffer.incrementIndent();    buffer.printf("int hashCode = 0;%n");    for (FieldSpec field : struct.fields()) {        if ((!onlyMapKeys) || field.mapKey()) {            generateFieldHashCode(field);        }    }    buffer.printf("return hashCode;%n");    buffer.decrementIndent();    buffer.printf("}%n");}
private void kafkatest_f12286_0(FieldSpec field)
{    if (!(field.nullableVersions().contains(field.versions()))) {        throw new RuntimeException("null cannot be the default for field " + field.name() + ", because not all versions of this field are " + "nullable.");    }}
private void kafkatest_f12287_0(FieldSpec field)
{    buffer.printf("%n");    generateAccessor(fieldAbstractJavaType(field), field.camelCaseName(), field.camelCaseName());}
 static String kafkatest_f12296_0(String str, String suffix)
{    if (str.endsWith(suffix)) {        return str.substring(0, str.length() - suffix.length());    } else {        throw new RuntimeException("String " + str + " does not end with the " + "expected suffix " + suffix);    }}
public static void kafkatest_f12297_0(String[] args) throws Exception
{    if (args.length == 0) {        System.out.println(USAGE);        System.exit(0);    } else if (args.length != 2) {        System.out.println(USAGE);        System.exit(1);    }    processDirectories(args[0], args[1]);}
 void kafkatest_f12306_0(MessageSpec message) throws Exception
{    // Generate schemas for inline structures    generateSchemas(message.generatedClassName(), message.struct(), message.struct().versions());    // Generate schemas for common structures    for (Iterator<StructSpec> iter = structRegistry.commonStructs(); iter.hasNext(); ) {        StructSpec struct = iter.next();        generateSchemas(struct.name(), struct, struct.versions());    }}
 void kafkatest_f12307_0(String className, StructSpec struct, Versions parentVersions) throws Exception
{    Versions versions = parentVersions.intersect(struct.versions());    MessageInfo messageInfo = messages.get(className);    if (messageInfo != null) {        return;    }    messageInfo = new MessageInfo(versions);    messages.put(className, messageInfo);    // Process the leaf classes first.    for (FieldSpec field : struct.fields()) {        if (field.type().isStructArray()) {            FieldType.ArrayType arrayType = (FieldType.ArrayType) field.type();            generateSchemas(arrayType.elementType().toString(), structRegistry.findStruct(field), versions);        } else if (field.type().isStruct()) {            generateSchemas(field.type().toString(), structRegistry.findStruct(field), versions);        }    }    CodeBuffer prev = null;    for (short v = versions.lowest(); v <= versions.highest(); v++) {        CodeBuffer cur = new CodeBuffer();        generateSchemaForVersion(struct, v, cur);        // create a new map entry.        if (!cur.equals(prev)) {            messageInfo.schemaForVersion.put(v, cur);        }        prev = cur;    }}
 boolean kafkatest_f12316_0(FieldSpec field)
{    if (!field.type().isArray()) {        return false;    }    FieldType.ArrayType arrayType = (FieldType.ArrayType) field.type();    if (!arrayType.isStructArray()) {        return false;    }    StructSpec struct = structSpecs.get(arrayType.elementName());    if (struct == null) {        throw new RuntimeException("Unable to locate a specification for the structure " + arrayType.elementName());    }    return struct.hasKeys();}
 Set<String> kafkatest_f12317_0()
{    return commonStructNames;}
public static Versions kafkatest_f12326_0(String input, Versions defaultVersions)
{    if (input == null) {        return defaultVersions;    }    String trimmedInput = input.trim();    if (trimmedInput.length() == 0) {        return defaultVersions;    }    if (trimmedInput.equals(NONE_STRING)) {        return NONE;    }    if (trimmedInput.endsWith("+")) {        return new Versions(Short.parseShort(trimmedInput.substring(0, trimmedInput.length() - 1)), Short.MAX_VALUE);    } else {        int dashIndex = trimmedInput.indexOf("-");        if (dashIndex < 0) {            short version = Short.parseShort(trimmedInput);            return new Versions(version, version);        }        return new Versions(Short.parseShort(trimmedInput.substring(0, dashIndex)), Short.parseShort(trimmedInput.substring(dashIndex + 1)));    }}
public short kafkatest_f12327_0()
{    return lowest;}
public void kafkatest_f12336_0()
{    for (FieldType type : new FieldType[] { FieldType.StringFieldType.INSTANCE, FieldType.Int8FieldType.INSTANCE, FieldType.Int16FieldType.INSTANCE, FieldType.Int32FieldType.INSTANCE, FieldType.Int64FieldType.INSTANCE, new FieldType.ArrayType(FieldType.StringFieldType.INSTANCE) }) {        EntityType.UNKNOWN.verifyTypeMatches("unknown", type);    }}
public void kafkatest_f12337_0()
{    EntityType.TRANSACTIONAL_ID.verifyTypeMatches("transactionalIdField", FieldType.StringFieldType.INSTANCE);    EntityType.TRANSACTIONAL_ID.verifyTypeMatches("transactionalIdField", new FieldType.ArrayType(FieldType.StringFieldType.INSTANCE));    EntityType.PRODUCER_ID.verifyTypeMatches("producerIdField", FieldType.Int64FieldType.INSTANCE);    EntityType.PRODUCER_ID.verifyTypeMatches("producerIdField", new FieldType.ArrayType(FieldType.Int64FieldType.INSTANCE));    EntityType.GROUP_ID.verifyTypeMatches("groupIdField", FieldType.StringFieldType.INSTANCE);    EntityType.GROUP_ID.verifyTypeMatches("groupIdField", new FieldType.ArrayType(FieldType.StringFieldType.INSTANCE));    EntityType.TOPIC_NAME.verifyTypeMatches("topicNameField", FieldType.StringFieldType.INSTANCE);    EntityType.TOPIC_NAME.verifyTypeMatches("topicNameField", new FieldType.ArrayType(FieldType.StringFieldType.INSTANCE));    EntityType.BROKER_ID.verifyTypeMatches("brokerIdField", FieldType.Int32FieldType.INSTANCE);    EntityType.BROKER_ID.verifyTypeMatches("brokerIdField", new FieldType.ArrayType(FieldType.Int32FieldType.INSTANCE));}
public void kafkatest_f12346_0() throws Exception
{    assertEquals("", MessageGenerator.toSnakeCase(""));    assertEquals("foo_bar_baz", MessageGenerator.toSnakeCase("FooBarBaz"));    assertEquals("foo_bar_baz", MessageGenerator.toSnakeCase("fooBarBaz"));    assertEquals("fortran", MessageGenerator.toSnakeCase("FORTRAN"));}
public void kafkatest_f12347_0() throws Exception
{    assertEquals("FooBa", MessageGenerator.stripSuffix("FooBar", "r"));    assertEquals("", MessageGenerator.stripSuffix("FooBar", "FooBar"));    assertEquals("Foo", MessageGenerator.stripSuffix("FooBar", "Bar"));    try {        MessageGenerator.stripSuffix("FooBar", "Baz");        fail("expected exception");    } catch (RuntimeException e) {    }}
public void kafkatest_f12356_0()
{    assertTrue(newVersions(2, 3).contains((short) 3));    assertTrue(newVersions(2, 3).contains((short) 2));    assertFalse(newVersions(0, 1).contains((short) 2));    assertTrue(newVersions(0, Short.MAX_VALUE).contains((short) 100));    assertFalse(newVersions(2, Short.MAX_VALUE).contains((short) 0));    assertTrue(newVersions(2, 3).contains(newVersions(2, 3)));    assertTrue(newVersions(2, 3).contains(newVersions(2, 2)));    assertFalse(newVersions(2, 3).contains(newVersions(2, 4)));    assertTrue(newVersions(2, 3).contains(Versions.NONE));    assertTrue(Versions.ALL.contains(newVersions(1, 2)));}
public void kafkatest_f12357_0()
{    for (int i = 0; i < DISTINCT_KEYS; ++i) {        keys[i] = KEY + i;        values[i] = VALUE + i;    }    lruCache = new LRUCache<>(100);}
public void kafkatest_f12366_0(Blackhole bh) throws IOException
{    for (int i = 0; i < batchCount; ++i) {        for (MutableRecordBatch batch : MemoryRecords.readableRecords(batchBuffers[i].duplicate()).batches()) {            try (CloseableIterator<Record> iterator = batch.skipKeyValueIterator(bufferSupplier)) {                while (iterator.hasNext()) bh.consume(iterator.next());            }        }    }}
public Producer<byte[], byte[]> kafkatest_f12367_0()
{    return producer;}
public String kafkatest_f12376_0()
{    return compressionType;}
public void kafkatest_f12377_0(String compressionType)
{    this.compressionType = compressionType;}
public String kafkatest_f12386_0()
{    return securityProtocol;}
public void kafkatest_f12387_0(String securityProtocol)
{    this.securityProtocol = securityProtocol;}
public String kafkatest_f12396_0()
{    return sslKeystoreLocation;}
public String kafkatest_f12397_0()
{    return sslKeystoreType;}
public int kafkatest_f12406_0()
{    return maxBlockMs;}
public void kafkatest_f12407_0(int maxBlockMs)
{    this.maxBlockMs = maxBlockMs;}
public void kafkatest_f12416_0()
{    Properties props = getLog4jConfig(false);    props.put("log4j.appender.KAFKA.SaslMechanism", "PLAIN");    PropertyConfigurator.configure(props);    MockKafkaLog4jAppender mockKafkaLog4jAppender = getMockKafkaLog4jAppender();    assertThat(mockKafkaLog4jAppender.getProducerProperties().getProperty(SaslConfigs.SASL_MECHANISM), equalTo("PLAIN"));}
public void kafkatest_f12417_0()
{    testProducerPropertyNotSet(SaslConfigs.SASL_MECHANISM);}
private void kafkatest_f12426_0(MockKafkaLog4jAppender mockKafkaLog4jAppender, boolean success)
{    @SuppressWarnings("unchecked")    MockProducer<byte[], byte[]> producer = EasyMock.niceMock(MockProducer.class);    @SuppressWarnings("unchecked")    Future<RecordMetadata> futureMock = EasyMock.niceMock(Future.class);    try {        if (!success)            EasyMock.expect(futureMock.get()).andThrow(new ExecutionException("simulated timeout", new TimeoutException()));    } catch (InterruptedException | ExecutionException e) {    // just mocking    }    EasyMock.expect(producer.send(EasyMock.anyObject())).andReturn(futureMock);    EasyMock.replay(producer, futureMock);    // reconfiguring mock appender    mockKafkaLog4jAppender.setKafkaProducer(producer);    mockKafkaLog4jAppender.activateOptions();}
private MockKafkaLog4jAppender kafkatest_f12427_0()
{    return (MockKafkaLog4jAppender) Logger.getRootLogger().getAppender("KAFKA");}
public long kafkatest_f12436_0(final ConsumerRecord<Object, Object> record, final long partitionTime)
{    if (record.value() instanceof PageViewTypedDemo.PageView) {        return ((PageViewTypedDemo.PageView) record.value()).timestamp;    }    if (record.value() instanceof PageViewTypedDemo.UserProfile) {        return ((PageViewTypedDemo.UserProfile) record.value()).timestamp;    }    if (record.value() instanceof JsonNode) {        return ((JsonNode) record.value()).get("timestamp").longValue();    }    throw new IllegalArgumentException("JsonTimestampExtractor cannot recognize the record value " + record.value());}
public T kafkatest_f12438_0(final String topic, final byte[] data)
{    if (data == null) {        return null;    }    try {        return (T) OBJECT_MAPPER.readValue(data, JSONSerdeCompatible.class);    } catch (final IOException e) {        throw new SerializationException(e);    }}
public static void kafkatest_f12448_0(final String[] args)
{    final Properties props = new Properties();    props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-temperature");    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> source = builder.stream("iot-temperature");    final KStream<Windowed<String>, String> max = source.selectKey((key, value) -> "temp").groupByKey().windowedBy(TimeWindows.of(Duration.ofSeconds(TEMPERATURE_WINDOW_SIZE))).reduce((value1, value2) -> {        if (Integer.parseInt(value1) > Integer.parseInt(value2)) {            return value1;        } else {            return value2;        }    }).toStream().filter((key, value) -> Integer.parseInt(value) > TEMPERATURE_THRESHOLD);    final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);    // need to override key serde to Windowed<String> type    max.to("iot-temperature-max", Produced.with(windowedSerde, Serdes.String()));    final KafkaStreams streams = new KafkaStreams(builder.build(), props);    final CountDownLatch latch = new CountDownLatch(1);    // attach shutdown handler to catch control-c    Runtime.getRuntime().addShutdownHook(new Thread("streams-temperature-shutdown-hook") {        @Override        public void run() {            streams.close();            latch.countDown();        }    });    try {        streams.start();        latch.await();    } catch (final Throwable e) {        System.exit(1);    }    System.exit(0);}
public void kafkatest_f12449_0()
{    streams.close();    latch.countDown();}
public ProductionExceptionHandlerResponse kafkatest_f12459_0(final ProducerRecord<byte[], byte[]> record, final Exception exception)
{    return ProductionExceptionHandlerResponse.FAIL;}
public void kafkatest_f12460_0(final Map<String, ?> configs)
{// ignore}
 AdminClient kafkatest_f12469_0(final Map<String, Object> config)
{    throw new UnsupportedOperationException("Direct use of this method is deprecated. " + "Implementations of KafkaClientSupplier should implement the getAdmin() method instead. " + "The method will be removed in a future release.");}
 Admin kafkatest_f12470_0(final Map<String, Object> config)
{    return getAdminClient(config);}
public void kafkatest_f12479_0(final Thread.UncaughtExceptionHandler eh)
{    synchronized (stateLock) {        if (state == State.CREATED) {            for (final StreamThread thread : threads) {                thread.setUncaughtExceptionHandler(eh);            }            if (globalStreamThread != null) {                globalStreamThread.setUncaughtExceptionHandler(eh);            }        } else {            throw new IllegalStateException("Can only set UncaughtExceptionHandler in CREATED state. " + "Current state is: " + state);        }    }}
public void kafkatest_f12480_0(final StateRestoreListener globalStateRestoreListener)
{    synchronized (stateLock) {        if (state == State.CREATED) {            this.globalStateRestoreListener = globalStateRestoreListener;        } else {            throw new IllegalStateException("Can only set GlobalStateRestoreListener in CREATED state. " + "Current state is: " + state);        }    }}
private static HostInfo kafkatest_f12489_0(final String endPoint)
{    if (endPoint == null || endPoint.trim().isEmpty()) {        return StreamsMetadataState.UNKNOWN_HOST;    }    final String host = getHost(endPoint);    final Integer port = getPort(endPoint);    if (host == null || port == null) {        throw new ConfigException(String.format("Error parsing host address %s. Expected format host:port.", endPoint));    }    return new HostInfo(host, port);}
public synchronized voidf12490_1) throws IllegalStateException, StreamsException
{    if (setState(State.REBALANCING)) {                if (globalStreamThread != null) {            globalStreamThread.start();        }        for (final StreamThread thread : threads) {            thread.start();        }        final Long cleanupDelay = config.getLong(StreamsConfig.STATE_CLEANUP_DELAY_MS_CONFIG);        stateDirCleaner.scheduleAtFixedRate(() -> {            // we do not use lock here since we only read on the value and act on it            if (state == State.RUNNING) {                stateDirectory.cleanRemovedTasks(cleanupDelay);            }        }, cleanupDelay, cleanupDelay, TimeUnit.MILLISECONDS);    } else {        throw new IllegalStateException("The client is either already started or already stopped, cannot re-start");    }}
public StreamsMetadata kafkatest_f12499_0(final String storeName, final K key, final StreamPartitioner<? super K, ?> partitioner)
{    validateIsRunning();    return streamsMetadataState.getMetadataWithKey(storeName, key, partitioner);}
public T kafkatest_f12500_0(final String storeName, final QueryableStoreType<T> queryableStoreType)
{    validateIsRunning();    return queryableStoreProvider.getStore(storeName, queryableStoreType);}
public static Consumed<K, V> kafkatest_f12509_0(final Topology.AutoOffsetReset resetPolicy)
{    return new Consumed<>(null, null, null, resetPolicy, null);}
public static Consumed<K, V> kafkatest_f12510_0(final String processorName)
{    return new Consumed<>(null, null, null, null, processorName);}
public static Grouped kafkatest_f12519_0(final Serde<K> keySerde)
{    return new Grouped<>(null, keySerde, null);}
public static Grouped kafkatest_f12520_0(final Serde<V> valueSerde)
{    return new Grouped<>(null, null, valueSerde);}
 static ValueMapperWithKey<K, V, VR> kafkatest_f12529_0(final ValueMapper<V, VR> valueMapper)
{    Objects.requireNonNull(valueMapper, "valueMapper can't be null");    return (readOnlyKey, value) -> valueMapper.apply(value);}
 static ValueTransformerWithKeySupplier<K, V, VR> kafkatest_f12530_0(final ValueTransformerSupplier<V, VR> valueTransformerSupplier)
{    Objects.requireNonNull(valueTransformerSupplier, "valueTransformerSupplier can't be null");    return () -> {        final ValueTransformer<V, VR> valueTransformer = valueTransformerSupplier.get();        return new ValueTransformerWithKey<K, V, VR>() {            @Override            public void init(final ProcessorContext context) {                valueTransformer.init(context);            }            @Override            public VR transform(final K readOnlyKey, final V value) {                return valueTransformer.transform(value);            }            @Override            public void close() {                valueTransformer.close();            }        };    };}
public Deserializer<T> kafkatest_f12539_0()
{    return inner;}
public void kafkatest_f12540_0(final Deserializer<T> inner)
{    this.inner = inner;}
public Serde<K> kafkatest_f12549_0()
{    return keySerde;}
public Deserializer<K> kafkatest_f12550_0()
{    return keySerde == null ? null : keySerde.deserializer();}
public Change<T> kafkatest_f12559_0(final String topic, final Change<byte[]> serialChange)
{    if (serialChange == null) {        return null;    }    final Deserializer<T> innerDeserializer = innerSerde().deserializer();    final T oldValue = serialChange.oldValue == null ? null : innerDeserializer.deserialize(topic, serialChange.oldValue);    final T newValue = serialChange.newValue == null ? null : innerDeserializer.deserialize(topic, serialChange.newValue);    return new Change<>(newValue, oldValue);}
public static byte[] kafkatest_f12560_0(final Change<byte[]> serialChange)
{    if (serialChange == null) {        return null;    }    final int oldSize = serialChange.oldValue == null ? -1 : serialChange.oldValue.length;    final int newSize = serialChange.newValue == null ? -1 : serialChange.newValue.length;    final ByteBuffer buffer = ByteBuffer.allocate(Integer.BYTES * 2 + Math.max(0, oldSize) + Math.max(0, newSize));    buffer.putInt(oldSize);    if (serialChange.oldValue != null) {        buffer.put(serialChange.oldValue);    }    buffer.putInt(newSize);    if (serialChange.newValue != null) {        buffer.put(serialChange.newValue);    }    return buffer.array();}
 String kafkatest_f12569_0()
{    return otherJoinSideNodeName;}
public String kafkatest_f12570_0()
{    return "BaseJoinProcessorNode{" + "joinThisProcessorParameters=" + joinThisProcessorParameters + ", joinOtherProcessorParameters=" + joinOtherProcessorParameters + ", joinMergeProcessorParameters=" + joinMergeProcessorParameters + ", valueJoiner=" + valueJoiner + ", thisJoinSideNodeName='" + thisJoinSideNodeName + '\'' + ", otherJoinSideNodeName='" + otherJoinSideNodeName + '\'' + "} " + super.toString();}
 Deserializer<V> kafkatest_f12579_0()
{    final Deserializer<? extends V> valueDeserializer = valueSerde == null ? null : valueSerde.deserializer();    return unsafeCastChangedToValueDeserializer(valueDeserializer);}
private Deserializer<V> kafkatest_f12580_0(final Deserializer<? extends V> valueDeserializer)
{    return (Deserializer<V>) new ChangedDeserializer<>(valueDeserializer);}
public GroupedTableOperationRepartitionNodeBuilder<K, V> kafkatest_f12589_0(final String repartitionTopic)
{    this.repartitionTopic = repartitionTopic;    return this;}
public GroupedTableOperationRepartitionNodeBuilder<K, V> kafkatest_f12590_0(final ProcessorParameters processorParameters)
{    this.processorParameters = processorParameters;    return this;}
public String kafkatest_f12599_0()
{    return "KTableKTableJoinNode{" + "joinThisStoreNames=" + Arrays.toString(joinThisStoreNames()) + ", joinOtherStoreNames=" + Arrays.toString(joinOtherStoreNames()) + "} " + super.toString();}
public static KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12600_0()
{    return new KTableKTableJoinNodeBuilder<>();}
public KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12609_0(final String[] joinOtherStoreNames)
{    this.joinOtherStoreNames = joinOtherStoreNames;    return this;}
public KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12610_0(final String queryableStoreName)
{    this.queryableStoreName = queryableStoreName;    return this;}
public void kafkatest_f12619_0(final InternalTopologyBuilder topologyBuilder)
{    final Serializer<K> keySerializer = keySerde != null ? keySerde.serializer() : null;    final Deserializer<K> keyDeserializer = keySerde != null ? keySerde.deserializer() : null;    topologyBuilder.addInternalTopic(repartitionTopic);    topologyBuilder.addProcessor(processorParameters.processorName(), processorParameters.processorSupplier(), parentNodeNames());    topologyBuilder.addSink(sinkName, repartitionTopic, keySerializer, getValueSerializer(), null, processorParameters.processorName());    topologyBuilder.addSource(null, sourceName, new FailOnInvalidTimestamp(), keyDeserializer, getValueDeserializer(), repartitionTopic);}
public static OptimizableRepartitionNodeBuilder<K, V> kafkatest_f12620_0()
{    return new OptimizableRepartitionNodeBuilder<>();}
public ProcessorParameters kafkatest_f12629_0()
{    return processorParameters;}
public String kafkatest_f12630_0()
{    return "ProcessorNode{" + "processorParameters=" + processorParameters + "} " + super.toString();}
public Collection<StreamsGraphNode> kafkatest_f12639_0()
{    return parentNodes;}
 String[] kafkatest_f12640_0()
{    final String[] parentNames = new String[parentNodes.size()];    int index = 0;    for (final StreamsGraphNode parentNode : parentNodes) {        parentNames[index++] = parentNode.nodeName();    }    return parentNames;}
public boolean kafkatest_f12649_0()
{    return mergeNode;}
public void kafkatest_f12650_0(final boolean mergeNode)
{    this.mergeNode = mergeNode;}
public void kafkatest_f12659_0(final InternalTopologyBuilder topologyBuilder)
{    final Serializer<K> keySerializer = producedInternal.keySerde() == null ? null : producedInternal.keySerde().serializer();    final Serializer<V> valSerializer = producedInternal.valueSerde() == null ? null : producedInternal.valueSerde().serializer();    final StreamPartitioner<? super K, ? super V> partitioner = producedInternal.streamPartitioner();    final String[] parentNames = parentNodeNames();    if (partitioner == null && keySerializer instanceof WindowedSerializer) {        @SuppressWarnings("unchecked")        final StreamPartitioner<K, V> windowedPartitioner = (StreamPartitioner<K, V>) new WindowedStreamPartitioner<Object, V>((WindowedSerializer) keySerializer);        topologyBuilder.addSink(nodeName(), topicNameExtractor, keySerializer, valSerializer, windowedPartitioner, parentNames);    } else {        topologyBuilder.addSink(nodeName(), topicNameExtractor, keySerializer, valSerializer, partitioner, parentNames);    }}
public Collection<String> kafkatest_f12660_0()
{    return new ArrayList<>(topicNames);}
public static StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12669_0()
{    return new StreamStreamJoinNodeBuilder<>();}
public StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12670_0(final ValueJoiner<? super V1, ? super V2, ? extends VR> valueJoiner)
{    this.valueJoiner = valueJoiner;    return this;}
public StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12679_0(final Joined<K, V1, V2> joined)
{    this.joined = joined;    return this;}
public StreamStreamJoinNode<K, V1, V2, VR> kafkatest_f12680_0()
{    return new StreamStreamJoinNode<>(nodeName, valueJoiner, joinThisProcessorParameters, joinOtherProcessorParameters, joinMergeProcessorParameters, thisWindowedStreamProcessorParameters, otherWindowedStreamProcessorParameters, thisWindowStoreBuilder, otherWindowStoreBuilder, joined);}
public TableSourceNodeBuilder<K, V> kafkatest_f12689_0(final String sourceName)
{    this.sourceName = sourceName;    return this;}
public TableSourceNodeBuilder<K, V> kafkatest_f12690_0(final String topic)
{    this.topic = topic;    return this;}
public String kafkatest_f12699_0()
{    return name;}
 KTable<KR, VR> kafkatest_f12700_0(final String functionName, final StoreBuilder<? extends StateStore> storeBuilder, final KStreamAggProcessorSupplier<K, KR, V, VR> aggregateSupplier, final String queryableStoreName, final Serde<KR> keySerde, final Serde<VR> valSerde)
{    assert queryableStoreName == null || queryableStoreName.equals(storeBuilder.name());    final String aggFunctionName = builder.newProcessorName(functionName);    String sourceName = this.name;    StreamsGraphNode parentNode = streamsGraphNode;    if (repartitionRequired) {        final OptimizableRepartitionNodeBuilder<K, V> repartitionNodeBuilder = optimizableRepartitionNodeBuilder();        final String repartitionTopicPrefix = userProvidedRepartitionTopicName != null ? userProvidedRepartitionTopicName : storeBuilder.name();        sourceName = createRepartitionSource(repartitionTopicPrefix, repartitionNodeBuilder);        // the existing repartition node, otherwise we create a new one.        if (repartitionNode == null || userProvidedRepartitionTopicName == null) {            repartitionNode = repartitionNodeBuilder.build();        }        builder.addGraphNode(parentNode, repartitionNode);        parentNode = repartitionNode;    }    final StatefulProcessorNode<K, V> statefulProcessorNode = new StatefulProcessorNode<>(aggFunctionName, new ProcessorParameters<>(aggregateSupplier, aggFunctionName), storeBuilder);    builder.addGraphNode(parentNode, statefulProcessorNode);    return new KTableImpl<>(aggFunctionName, keySerde, valSerde, sourceName.equals(this.name) ? sourceNodes : Collections.singleton(sourceName), queryableStoreName, aggregateSupplier, statefulProcessorNode, builder);}
public synchronized void kafkatest_f12709_0(final StoreBuilder builder)
{    addGraphNode(root, new StateStoreNode(builder));}
public synchronized void kafkatest_f12710_0(final StoreBuilder<KeyValueStore> storeBuilder, final String sourceName, final String topic, final ConsumedInternal consumed, final String processorName, final ProcessorSupplier stateUpdateSupplier)
{    final StreamsGraphNode globalStoreNode = new GlobalStoreNode(storeBuilder, sourceName, topic, consumed, processorName, stateUpdateSupplier);    addGraphNode(root, globalStoreNode);}
private voidf12719_1)
{    maybeUpdateKeyChangingRepartitionNodeMap();    final Iterator<Entry<StreamsGraphNode, LinkedHashSet<OptimizableRepartitionNode>>> entryIterator = keyChangingOperationsToOptimizableRepartitionNodes.entrySet().iterator();    while (entryIterator.hasNext()) {        final Map.Entry<StreamsGraphNode, LinkedHashSet<OptimizableRepartitionNode>> entry = entryIterator.next();        final StreamsGraphNode keyChangingNode = entry.getKey();        if (entry.getValue().isEmpty()) {            continue;        }        final GroupedInternal groupedInternal = new GroupedInternal(getRepartitionSerdes(entry.getValue()));        final String repartitionTopicName = getFirstRepartitionTopicName(entry.getValue());        // passing in the name of the first repartition topic, re-used to create the optimized repartition topic        final StreamsGraphNode optimizedSingleRepartition = createRepartitionNode(repartitionTopicName, groupedInternal.keySerde(), groupedInternal.valueSerde());        // re-use parent buildPriority to make sure the single repartition graph node is evaluated before downstream nodes        optimizedSingleRepartition.setBuildPriority(keyChangingNode.buildPriority());        for (final OptimizableRepartitionNode repartitionNodeToBeReplaced : entry.getValue()) {            final StreamsGraphNode keyChangingNodeChild = findParentNodeMatching(repartitionNodeToBeReplaced, gn -> gn.parentNodes().contains(keyChangingNode));            if (keyChangingNodeChild == null) {                throw new StreamsException(String.format("Found a null keyChangingChild node for %s", repartitionNodeToBeReplaced));            }                        // need to add children of key-changing node as children of optimized repartition            // in order to process records from re-partitioning            optimizedSingleRepartition.addChild(keyChangingNodeChild);                        // now remove children from key-changing node            keyChangingNode.removeChild(keyChangingNodeChild);            // now need to get children of repartition node so we can remove repartition node            final Collection<StreamsGraphNode> repartitionNodeToBeReplacedChildren = repartitionNodeToBeReplaced.children();            final Collection<StreamsGraphNode> parentsOfRepartitionNodeToBeReplaced = repartitionNodeToBeReplaced.parentNodes();            for (final StreamsGraphNode repartitionNodeToBeReplacedChild : repartitionNodeToBeReplacedChildren) {                for (final StreamsGraphNode parentNode : parentsOfRepartitionNodeToBeReplaced) {                    parentNode.addChild(repartitionNodeToBeReplacedChild);                }            }            for (final StreamsGraphNode parentNode : parentsOfRepartitionNodeToBeReplaced) {                parentNode.removeChild(repartitionNodeToBeReplaced);            }            repartitionNodeToBeReplaced.clearChildren();                    }        keyChangingNode.addChild(optimizedSingleRepartition);        entryIterator.remove();    }}
private void kafkatest_f12720_0()
{    final Map<StreamsGraphNode, Set<StreamsGraphNode>> mergeNodesToKeyChangers = new HashMap<>();    for (final StreamsGraphNode mergeNode : mergeNodes) {        mergeNodesToKeyChangers.put(mergeNode, new LinkedHashSet<>());        final Collection<StreamsGraphNode> keys = keyChangingOperationsToOptimizableRepartitionNodes.keySet();        for (final StreamsGraphNode key : keys) {            final StreamsGraphNode maybeParentKey = findParentNodeMatching(mergeNode, node -> node.parentNodes().contains(key));            if (maybeParentKey != null) {                mergeNodesToKeyChangers.get(mergeNode).add(key);            }        }    }    for (final Map.Entry<StreamsGraphNode, Set<StreamsGraphNode>> entry : mergeNodesToKeyChangers.entrySet()) {        final StreamsGraphNode mergeKey = entry.getKey();        final Collection<StreamsGraphNode> keyChangingParents = entry.getValue();        final LinkedHashSet<OptimizableRepartitionNode> repartitionNodes = new LinkedHashSet<>();        for (final StreamsGraphNode keyChangingParent : keyChangingParents) {            repartitionNodes.addAll(keyChangingOperationsToOptimizableRepartitionNodes.get(keyChangingParent));            keyChangingOperationsToOptimizableRepartitionNodes.remove(keyChangingParent);        }        keyChangingOperationsToOptimizableRepartitionNodes.put(mergeKey, repartitionNodes);    }}
public Serde<VO> kafkatest_f12729_0()
{    return otherValueSerde;}
public String kafkatest_f12730_0()
{    return name;}
public SessionWindowedKStream<K, V> kafkatest_f12739_0(final SessionWindows windows)
{    return new SessionWindowedKStreamImpl<>(windows, builder, sourceNodes, name, keySerde, valSerde, aggregateBuilder, streamsGraphNode);}
private KTable<K, T> kafkatest_f12740_0(final KStreamAggProcessorSupplier<K, K, V, T> aggregateSupplier, final String functionName, final MaterializedInternal<K, T, KeyValueStore<Bytes, byte[]>> materializedInternal)
{    return aggregateBuilder.build(functionName, new TimestampedKeyValueStoreMaterializer<>(materializedInternal).materialize(), aggregateSupplier, materializedInternal.queryableStoreName(), materializedInternal.keySerde(), materializedInternal.valueSerde());}
public Processor<K, V> kafkatest_f12749_0()
{    return new KStreamAggregateProcessor();}
public void kafkatest_f12750_0()
{    sendOldValues = true;}
public void kafkatest_f12760_0(final K key, final V value)
{    for (int i = 0; i < predicates.length; i++) {        if (predicates[i].test(key, value)) {            // use forward with child here and then break the loop            // so that no record is going to be piped to multiple streams            context().forward(key, value, To.child(childNodes[i]));            break;        }    }}
public Processor<K, V> kafkatest_f12761_0()
{    return new KStreamFilterProcessor();}
public void kafkatest_f12770_0()
{    transformer.close();}
public Processor<KIn, VIn> kafkatest_f12771_0()
{    return new KStreamFlatTransformValuesProcessor<>(valueTransformerSupplier.get());}
public KStream<KR, V> kafkatest_f12780_0(final KeyValueMapper<? super K, ? super V, ? extends KR> mapper)
{    return selectKey(mapper, NamedInternal.empty());}
public KStream<KR, V> kafkatest_f12781_0(final KeyValueMapper<? super K, ? super V, ? extends KR> mapper, final Named named)
{    Objects.requireNonNull(mapper, "mapper can't be null");    Objects.requireNonNull(named, "named can't be null");    final ProcessorGraphNode<K, V> selectKeyProcessorNode = internalSelectKey(mapper, new NamedInternal(named));    selectKeyProcessorNode.keyChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, selectKeyProcessorNode);    // key serde cannot be preserved    return new KStreamImpl<>(selectKeyProcessorNode.nodeName(), null, valSerde, sourceNodes, true, selectKeyProcessorNode, builder);}
public KStream<KR, VR> kafkatest_f12790_0(final KeyValueMapper<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends KR, ? extends VR>>> mapper)
{    return flatMap(mapper, NamedInternal.empty());}
public KStream<KR, VR> kafkatest_f12791_0(final KeyValueMapper<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends KR, ? extends VR>>> mapper, final Named named)
{    Objects.requireNonNull(mapper, "mapper can't be null");    Objects.requireNonNull(named, "named can't be null");    final String name = new NamedInternal(named).orElseGenerateWithPrefix(builder, FLATMAP_NAME);    final ProcessorParameters<? super K, ? super V> processorParameters = new ProcessorParameters<>(new KStreamFlatMap<>(mapper), name);    final ProcessorGraphNode<? super K, ? super V> flatMapNode = new ProcessorGraphNode<>(name, processorParameters);    flatMapNode.keyChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, flatMapNode);    // key and value serde cannot be preserved    return new KStreamImpl<>(name, null, null, sourceNodes, true, flatMapNode, builder);}
public KStream<K, V> kafkatest_f12800_0(final KStream<K, V> stream, final Named processorName)
{    Objects.requireNonNull(stream);    return merge(builder, stream, new NamedInternal(processorName));}
private KStream<K, V> kafkatest_f12801_0(final InternalStreamsBuilder builder, final KStream<K, V> stream, final NamedInternal processorName)
{    final KStreamImpl<K, V> streamImpl = (KStreamImpl<K, V>) stream;    final String name = processorName.orElseGenerateWithPrefix(builder, MERGE_NAME);    final Set<String> allSourceNodes = new HashSet<>();    final boolean requireRepartitioning = streamImpl.repartitionRequired || repartitionRequired;    allSourceNodes.addAll(sourceNodes);    allSourceNodes.addAll(streamImpl.sourceNodes);    final ProcessorParameters<? super K, ? super V> processorParameters = new ProcessorParameters<>(new KStreamPassThrough<>(), name);    final ProcessorGraphNode<? super K, ? super V> mergeNode = new ProcessorGraphNode<>(name, processorParameters);    mergeNode.setMergeNode(true);    builder.addGraphNode(Arrays.asList(this.streamsGraphNode, streamImpl.streamsGraphNode), mergeNode);    // drop the serde as we cannot safely use either one to represent both streams    return new KStreamImpl<>(name, null, null, allSourceNodes, requireRepartitioning, mergeNode, builder);}
public void kafkatest_f12810_0(final TopicNameExtractor<K, V> topicExtractor)
{    to(topicExtractor, Produced.with(keySerde, valSerde, null));}
public void kafkatest_f12811_0(final TopicNameExtractor<K, V> topicExtractor, final Produced<K, V> produced)
{    Objects.requireNonNull(topicExtractor, "topic extractor can't be null");    Objects.requireNonNull(produced, "Produced can't be null");    final ProducedInternal<K, V> producedInternal = new ProducedInternal<>(produced);    if (producedInternal.keySerde() == null) {        producedInternal.withKeySerde(keySerde);    }    if (producedInternal.valueSerde() == null) {        producedInternal.withValueSerde(valSerde);    }    to(topicExtractor, producedInternal);}
public KStream<K, VR> kafkatest_f12820_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> valueTransformerSupplier, final Named named, final String... stateStoreNames)
{    Objects.requireNonNull(valueTransformerSupplier, "valueTransformSupplier can't be null");    Objects.requireNonNull(named, "named can't be null");    return doTransformValues(valueTransformerSupplier, new NamedInternal(named), stateStoreNames);}
private KStream<K, VR> kafkatest_f12821_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> valueTransformerWithKeySupplier, final NamedInternal named, final String... stateStoreNames)
{    final String name = named.orElseGenerateWithPrefix(builder, TRANSFORMVALUES_NAME);    final StatefulProcessorNode<? super K, ? super V> transformNode = new StatefulProcessorNode<>(name, new ProcessorParameters<>(new KStreamTransformValues<>(valueTransformerWithKeySupplier), name), stateStoreNames);    transformNode.setValueChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, transformNode);    // cannot inherit value serde    return new KStreamImpl<>(name, keySerde, null, sourceNodes, repartitionRequired, transformNode, builder);}
public KStream<K, VR> kafkatest_f12830_0(final KStream<K, VO> otherStream, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final JoinWindows windows, final Joined<K, V, VO> joined)
{    return doJoin(otherStream, joiner, windows, joined, new KStreamImplJoin(false, false));}
public KStream<K, VR> kafkatest_f12831_0(final KStream<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final JoinWindows windows)
{    return outerJoin(other, joiner, windows, Joined.with(null, null, null));}
public KStream<K, VR> kafkatest_f12840_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner)
{    return leftJoin(other, joiner, Joined.with(null, null, null));}
public KStream<K, VR> kafkatest_f12841_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Joined<K, V, VO> joined)
{    Objects.requireNonNull(other, "other can't be null");    Objects.requireNonNull(joiner, "joiner can't be null");    Objects.requireNonNull(joined, "joined can't be null");    final JoinedInternal<K, V, VO> joinedInternal = new JoinedInternal<>(joined);    final String internalName = joinedInternal.name();    if (repartitionRequired) {        final KStreamImpl<K, V> thisStreamRepartitioned = repartitionForJoin(internalName != null ? internalName : name, joined.keySerde(), joined.valueSerde());        return thisStreamRepartitioned.doStreamTableJoin(other, joiner, joined, true);    } else {        return doStreamTableJoin(other, joiner, joined, true);    }}
public KGroupedStream<KR, V> kafkatest_f12850_0(final KeyValueMapper<? super K, ? super V, KR> selector, final Grouped<KR, V> grouped)
{    Objects.requireNonNull(selector, "selector can't be null");    Objects.requireNonNull(grouped, "grouped can't be null");    final GroupedInternal<KR, V> groupedInternal = new GroupedInternal<>(grouped);    final ProcessorGraphNode<K, V> selectKeyMapNode = internalSelectKey(selector, new NamedInternal(groupedInternal.name()));    selectKeyMapNode.keyChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, selectKeyMapNode);    return new KGroupedStreamImpl<>(selectKeyMapNode.nodeName(), sourceNodes, groupedInternal, true, selectKeyMapNode, builder);}
public KGroupedStream<K, V> kafkatest_f12851_0()
{    return groupByKey(Grouped.with(keySerde, valSerde));}
public void kafkatest_f12860_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    otherWindow = (WindowStore<K, V2>) context.getStateStore(otherWindowName);}
public voidf12861_1final K key, final V1 value)
{    // thus, to be consistent and to avoid ambiguous null semantics, null values are ignored    if (key == null || value == null) {                skippedRecordsSensor.record();        return;    }    boolean needOuterJoin = outer;    final long inputRecordTimestamp = context().timestamp();    final long timeFrom = Math.max(0L, inputRecordTimestamp - joinBeforeMs);    final long timeTo = Math.max(0L, inputRecordTimestamp + joinAfterMs);    try (final WindowStoreIterator<V2> iter = otherWindow.fetch(key, timeFrom, timeTo)) {        while (iter.hasNext()) {            needOuterJoin = false;            final KeyValue<Long, V2> otherRecord = iter.next();            context().forward(key, joiner.apply(value, otherRecord.value), To.all().withTimestamp(Math.max(inputRecordTimestamp, otherRecord.key)));        }        if (needOuterJoin) {            context().forward(key, joiner.apply(value, null));        }    }}
public void kafkatest_f12870_0(final K readOnlyKey, final V value)
{    final V1 newValue = mapper.apply(readOnlyKey, value);    context().forward(readOnlyKey, newValue);}
public Processor<K, V> kafkatest_f12871_0()
{    return new KStreamPassThroughProcessor<>();}
public void kafkatest_f12880_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    store = (TimestampedKeyValueStore<K, V>) context.getStateStore(storeName);    tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);}
public voidf12881_1final K key, final V value)
{    // If the key or value is null we don't need to proceed    if (key == null || value == null) {                skippedRecordsSensor.record();        return;    }    final ValueAndTimestamp<V> oldAggAndTimestamp = store.get(key);    final V oldAgg = getValueOrNull(oldAggAndTimestamp);    final V newAgg;    final long newTimestamp;    if (oldAgg == null) {        newAgg = value;        newTimestamp = context().timestamp();    } else {        newAgg = reducer.apply(oldAgg, value);        newTimestamp = Math.max(context().timestamp(), oldAggAndTimestamp.timestamp());    }    store.put(key, ValueAndTimestamp.make(newAgg, newTimestamp));    tupleForwarder.maybeForward(key, newAgg, sendOldValues ? oldAgg : null, newTimestamp);}
public void kafkatest_f12891_0(final ProcessorContext context)
{    super.init(context);    internalProcessorContext = (InternalProcessorContext) context;    metrics = (StreamsMetricsImpl) context.metrics();    lateRecordDropSensor = Sensors.lateRecordDropSensor(internalProcessorContext);    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    store = (SessionStore<K, Agg>) context.getStateStore(storeName);    tupleForwarder = new SessionTupleForwarder<>(store, context, new SessionCacheFlushListener<>(context), sendOldValues);}
public voidf12892_1final K key, final V value)
{    // the record with the table    if (key == null) {                skippedRecordsSensor.record();        return;    }    final long timestamp = context().timestamp();    observedStreamTime = Math.max(observedStreamTime, timestamp);    final long closeTime = observedStreamTime - windows.gracePeriodMs();    final List<KeyValue<Windowed<K>, Agg>> merged = new ArrayList<>();    final SessionWindow newSessionWindow = new SessionWindow(timestamp, timestamp);    SessionWindow mergedWindow = newSessionWindow;    Agg agg = initializer.apply();    try (final KeyValueIterator<Windowed<K>, Agg> iterator = store.findSessions(key, timestamp - windows.inactivityGap(), timestamp + windows.inactivityGap())) {        while (iterator.hasNext()) {            final KeyValue<Windowed<K>, Agg> next = iterator.next();            merged.add(next);            agg = sessionMerger.apply(key, agg, next.value);            mergedWindow = mergeSessionWindow(mergedWindow, (SessionWindow) next.key.window());        }    }    if (mergedWindow.end() < closeTime) {                lateRecordDropSensor.record();    } else {        if (!mergedWindow.equals(newSessionWindow)) {            for (final KeyValue<Windowed<K>, Agg> session : merged) {                store.remove(session.key);                tupleForwarder.maybeForward(session.key, null, sendOldValues ? session.value : null);            }        }        agg = aggregator.apply(key, value, agg);        final Windowed<K> sessionKey = new Windowed<>(key, mergedWindow);        store.put(sessionKey, agg);        tupleForwarder.maybeForward(sessionKey, agg, null);    }}
public void kafkatest_f12902_0(final K key, final V value)
{    context.forward(key, valueTransformer.transform(key, value));}
public void kafkatest_f12903_0()
{    valueTransformer.close();}
public void kafkatest_f12912_0(final ProcessorContext context)
{    windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);}
public ValueAndTimestamp<Agg> kafkatest_f12913_0(final Windowed<K> windowedKey)
{    final K key = windowedKey.key();    final W window = (W) windowedKey.window();    return windowStore.fetch(key, window.start());}
private ValueAndTimestamp<V> kafkatest_f12923_0(final K key, final ValueAndTimestamp<V> valueAndTimestamp)
{    ValueAndTimestamp<V> newValueAndTimestamp = null;    if (valueAndTimestamp != null) {        final V value = valueAndTimestamp.value();        if (filterNot ^ predicate.test(key, value)) {            newValueAndTimestamp = valueAndTimestamp;        }    }    return newValueAndTimestamp;}
public void kafkatest_f12924_0(final ProcessorContext context)
{    super.init(context);    if (queryableName != null) {        store = (TimestampedKeyValueStore<K, V>) context.getStateStore(queryableName);        tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);    }}
private KTable<K, V> kafkatest_f12933_0(final Predicate<? super K, ? super V> predicate, final Named named, final MaterializedInternal<K, V, KeyValueStore<Bytes, byte[]>> materializedInternal, final boolean filterNot)
{    final Serde<K> keySerde;    final Serde<V> valueSerde;    final String queryableStoreName;    final StoreBuilder<TimestampedKeyValueStore<K, V>> storeBuilder;    if (materializedInternal != null) {        // materialize the store; but we still need to burn one index BEFORE generating the processor to keep compatibility.        if (materializedInternal.storeName() == null) {            builder.newStoreName(FILTER_NAME);        }        // we can inherit parent key and value serde if user do not provide specific overrides, more specifically:        // we preserve the key following the order of 1) materialized, 2) parent        keySerde = materializedInternal.keySerde() != null ? materializedInternal.keySerde() : this.keySerde;        // we preserve the value following the order of 1) materialized, 2) parent        valueSerde = materializedInternal.valueSerde() != null ? materializedInternal.valueSerde() : this.valSerde;        queryableStoreName = materializedInternal.queryableStoreName();        // only materialize if materialized is specified and it has queryable name        storeBuilder = queryableStoreName != null ? (new TimestampedKeyValueStoreMaterializer<>(materializedInternal)).materialize() : null;    } else {        keySerde = this.keySerde;        valueSerde = this.valSerde;        queryableStoreName = null;        storeBuilder = null;    }    final String name = new NamedInternal(named).orElseGenerateWithPrefix(builder, FILTER_NAME);    final KTableProcessorSupplier<K, V, V> processorSupplier = new KTableFilter<>(this, predicate, filterNot, queryableStoreName);    final ProcessorParameters<K, V> processorParameters = unsafeCastProcessorParametersToCompletelyDifferentType(new ProcessorParameters<>(processorSupplier, name));    final StreamsGraphNode tableNode = new TableProcessorNode<>(name, processorParameters, storeBuilder);    builder.addGraphNode(this.streamsGraphNode, tableNode);    return new KTableImpl<>(name, keySerde, valueSerde, sourceNodes, queryableStoreName, processorSupplier, tableNode, builder);}
public KTable<K, V> kafkatest_f12934_0(final Predicate<? super K, ? super V> predicate)
{    Objects.requireNonNull(predicate, "predicate can't be null");    return doFilter(predicate, NamedInternal.empty(), null, false);}
public KTable<K, VR> kafkatest_f12943_0(final ValueMapper<? super V, ? extends VR> mapper)
{    Objects.requireNonNull(mapper, "mapper can't be null");    return doMapValues(withKey(mapper), NamedInternal.empty(), null);}
public KTable<K, VR> kafkatest_f12944_0(final ValueMapper<? super V, ? extends VR> mapper, final Named named)
{    Objects.requireNonNull(mapper, "mapper can't be null");    return doMapValues(withKey(mapper), named, null);}
public KTable<K, VR> kafkatest_f12953_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> transformerSupplier, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized, final String... stateStoreNames)
{    return transformValues(transformerSupplier, materialized, NamedInternal.empty(), stateStoreNames);}
public KTable<K, VR> kafkatest_f12954_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> transformerSupplier, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized, final Named named, final String... stateStoreNames)
{    Objects.requireNonNull(materialized, "materialized can't be null");    Objects.requireNonNull(named, "named can't be null");    final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized);    return doTransformValues(transformerSupplier, materializedInternal, new NamedInternal(named), stateStoreNames);}
public KTable<K, R> kafkatest_f12963_0(final KTable<K, V1> other, final ValueJoiner<? super V, ? super V1, ? extends R> joiner, final Named named)
{    return doJoin(other, joiner, named, null, false, false);}
public KTable<K, VR> kafkatest_f12964_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized)
{    return join(other, joiner, NamedInternal.empty(), materialized);}
public KTable<K, VR> kafkatest_f12973_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Named named, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(materialized, "materialized can't be null");    final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, MERGE_NAME);    return doJoin(other, joiner, named, materializedInternal, true, false);}
private KTable<K, VR> kafkatest_f12974_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Named joinName, final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal, final boolean leftOuter, final boolean rightOuter)
{    Objects.requireNonNull(other, "other can't be null");    Objects.requireNonNull(joiner, "joiner can't be null");    Objects.requireNonNull(joinName, "joinName can't be null");    final NamedInternal renamed = new NamedInternal(joinName);    final String joinMergeName = renamed.orElseGenerateWithPrefix(builder, MERGE_NAME);    final Set<String> allSourceNodes = ensureJoinableWith((AbstractStream<K, VO>) other);    if (leftOuter) {        enableSendingOldValues();    }    if (rightOuter) {        ((KTableImpl) other).enableSendingOldValues();    }    final KTableKTableAbstractJoin<K, VR, V, VO> joinThis;    final KTableKTableAbstractJoin<K, VR, VO, V> joinOther;    if (!leftOuter) {        // inner        joinThis = new KTableKTableInnerJoin<>(this, (KTableImpl<K, ?, VO>) other, joiner);        joinOther = new KTableKTableInnerJoin<>((KTableImpl<K, ?, VO>) other, this, reverseJoiner(joiner));    } else if (!rightOuter) {        // left        joinThis = new KTableKTableLeftJoin<>(this, (KTableImpl<K, ?, VO>) other, joiner);        joinOther = new KTableKTableRightJoin<>((KTableImpl<K, ?, VO>) other, this, reverseJoiner(joiner));    } else {        // outer        joinThis = new KTableKTableOuterJoin<>(this, (KTableImpl<K, ?, VO>) other, joiner);        joinOther = new KTableKTableOuterJoin<>((KTableImpl<K, ?, VO>) other, this, reverseJoiner(joiner));    }    final String joinThisName = renamed.suffixWithOrElseGet("-join-this", builder, JOINTHIS_NAME);    final String joinOtherName = renamed.suffixWithOrElseGet("-join-other", builder, JOINOTHER_NAME);    final ProcessorParameters<K, Change<V>> joinThisProcessorParameters = new ProcessorParameters<>(joinThis, joinThisName);    final ProcessorParameters<K, Change<VO>> joinOtherProcessorParameters = new ProcessorParameters<>(joinOther, joinOtherName);    final Serde<K> keySerde;    final Serde<VR> valueSerde;    final String queryableStoreName;    final StoreBuilder<TimestampedKeyValueStore<K, VR>> storeBuilder;    if (materializedInternal != null) {        keySerde = materializedInternal.keySerde() != null ? materializedInternal.keySerde() : this.keySerde;        valueSerde = materializedInternal.valueSerde();        queryableStoreName = materializedInternal.storeName();        storeBuilder = new TimestampedKeyValueStoreMaterializer<>(materializedInternal).materialize();    } else {        keySerde = this.keySerde;        valueSerde = null;        queryableStoreName = null;        storeBuilder = null;    }    final KTableKTableJoinNode<K, V, VO, VR> kTableKTableJoinNode = KTableKTableJoinNode.<K, V, VO, VR>kTableKTableJoinNodeBuilder().withNodeName(joinMergeName).withJoinThisProcessorParameters(joinThisProcessorParameters).withJoinOtherProcessorParameters(joinOtherProcessorParameters).withThisJoinSideNodeName(name).withOtherJoinSideNodeName(((KTableImpl) other).name).withJoinThisStoreNames(valueGetterSupplier().storeNames()).withJoinOtherStoreNames(((KTableImpl) other).valueGetterSupplier().storeNames()).withKeySerde(keySerde).withValueSerde(valueSerde).withQueryableStoreName(queryableStoreName).withStoreBuilder(storeBuilder).build();    builder.addGraphNode(this.streamsGraphNode, kTableKTableJoinNode);    // we can inherit parent key serde if user do not provide specific overrides    return new KTableImpl<K, Change<VR>, VR>(kTableKTableJoinNode.nodeName(), kTableKTableJoinNode.keySerde(), kTableKTableJoinNode.valueSerde(), allSourceNodes, kTableKTableJoinNode.queryableStoreName(), kTableKTableJoinNode.joinMerger(), kTableKTableJoinNode, builder);}
public String[] kafkatest_f12983_0()
{    final String[] storeNames1 = valueGetterSupplier1.storeNames();    final String[] storeNames2 = valueGetterSupplier2.storeNames();    final Set<String> stores = new HashSet<>(storeNames1.length + storeNames2.length);    Collections.addAll(stores, storeNames1);    Collections.addAll(stores, storeNames2);    return stores.toArray(new String[stores.size()]);}
public Processor<K, Change<V1>> kafkatest_f12984_0()
{    return new KTableKTableJoinProcessor(valueGetterSupplier2.get());}
public String kafkatest_f12993_0()
{    return queryableName;}
public Processor<K, Change<V>> kafkatest_f12994_0()
{    return new KTableKTableJoinMergeProcessor();}
public Processor<K, Change<V1>> kafkatest_f13003_0()
{    return new KTableKTableLeftJoinProcessor(valueGetterSupplier2.get());}
public KTableValueGetterSupplier<K, R> kafkatest_f13004_0()
{    return new KTableKTableLeftJoinValueGetterSupplier(valueGetterSupplier1, valueGetterSupplier2);}
public KTableValueGetterSupplier<K, R> kafkatest_f13013_0()
{    return new KTableKTableOuterJoinValueGetterSupplier(valueGetterSupplier1, valueGetterSupplier2);}
public KTableValueGetter<K, R> kafkatest_f13014_0()
{    return new KTableKTableOuterJoinValueGetter(valueGetterSupplier1.get(), valueGetterSupplier2.get());}
public KTableValueGetter<K, R> kafkatest_f13023_0()
{    return new KTableKTableRightJoinValueGetter(valueGetterSupplier1.get(), valueGetterSupplier2.get());}
public void kafkatest_f13024_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    valueGetter.init(context);}
public String[] kafkatest_f13033_0()
{    return parentValueGetterSupplier.storeNames();}
public void kafkatest_f13034_0()
{    parent.enableSendingOldValues();    sendOldValues = true;}
public String[] kafkatest_f13043_0()
{    return new String[] { storeName };}
public void kafkatest_f13044_0(final ProcessorContext context)
{    store = (TimestampedKeyValueStore<K, V>) context.getStateStore(storeName);}
public KTableValueGetter<K, KeyValue<K1, V1>> kafkatest_f13054_0()
{    return new KTableMapValueGetter(parentValueGetterSupplier.get());}
public String[] kafkatest_f13055_0()
{    throw new StreamsException("Underlying state store not accessible due to repartitioning.");}
public void kafkatest_f13064_0()
{    this.queryableName = storeName;}
public void kafkatest_f13065_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    if (queryableName != null) {        store = (TimestampedKeyValueStore<K, V>) context.getStateStore(queryableName);        tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);    }}
public String[] kafkatest_f13075_0()
{    return parentValueGetterSupplier.storeNames();}
public void kafkatest_f13076_0()
{    parent.enableSendingOldValues();    sendOldValues = true;}
public StoreSupplier<S> kafkatest_f13085_0()
{    return storeSupplier;}
public Serde<K> kafkatest_f13086_0()
{    return keySerde;}
public static NamedInternal kafkatest_f13095_0()
{    return new NamedInternal((String) null);}
public static NamedInternal kafkatest_f13096_0(final String name)
{    return new NamedInternal(name);}
public void kafkatest_f13105_0()
{    if (closable) {        printWriter.close();    } else {        printWriter.flush();    }}
public Serde<K> kafkatest_f13106_0()
{    return keySerde;}
public KTable<Windowed<K>, Long> kafkatest_f13115_0()
{    return doCount(Materialized.with(keySerde, Serdes.Long()));}
public KTable<Windowed<K>, Long> kafkatest_f13116_0(final Materialized<K, Long, SessionStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(materialized, "materialized can't be null");    // we used to burn a topology name here, so we have to keep doing it for compatibility    if (new MaterializedInternal<>(materialized).storeName() == null) {        builder.newStoreName(AGGREGATE_NAME);    }    return doCount(materialized);}
public Suppressed.StrictBufferConfig kafkatest_f13125_0()
{    return new StrictBufferConfigImpl(Long.MAX_VALUE, Long.MAX_VALUE, // doesn't matter, given the bounds    SHUT_DOWN);}
public Suppressed.StrictBufferConfig kafkatest_f13126_0()
{    return new StrictBufferConfigImpl(maxRecords(), maxBytes(), SHUT_DOWN);}
public String kafkatest_f13135_0()
{    return "EagerBufferConfigImpl{maxRecords=" + maxRecords + ", maxBytes=" + maxBytes + '}';}
public SuppressedInternal<K> kafkatest_f13136_0(final Duration gracePeriod)
{    return new SuppressedInternal<>(name, gracePeriod, bufferConfig, TimeDefinitions.WindowEndTimeDefinition.instance(), true);}
public void kafkatest_f13145_0(final ProcessorContext context)
{    parentGetter.init(context);    // the main processor is responsible for the buffer's lifecycle    buffer = requireNonNull((TimeOrderedKeyValueBuffer<K, V>) context.getStateStore(storeName));}
public ValueAndTimestamp<V> kafkatest_f13146_0(final K key)
{    final Maybe<ValueAndTimestamp<V>> maybeValue = buffer.priorValueForBuffered(key);    if (maybeValue.isDefined()) {        return maybeValue.getNullableValue();    } else {        // not buffered, so the suppressed view is equal to the parent view        return parentGetter.get(key);    }}
private void kafkatest_f13155_0(final TimeOrderedKeyValueBuffer.Eviction<K, V> toEmit)
{    if (shouldForward(toEmit.value())) {        final ProcessorRecordContext prevRecordContext = internalProcessorContext.recordContext();        internalProcessorContext.setRecordContext(toEmit.recordContext());        try {            internalProcessorContext.forward(toEmit.key(), toEmit.value());            suppressionEmitSensor.record();        } finally {            internalProcessorContext.setRecordContext(prevRecordContext);        }    }}
private boolean kafkatest_f13156_0(final Change<V> value)
{    return value.newValue != null || !safeToDropTombstones;}
public Suppressed<K> kafkatest_f13166_0(final String name)
{    return new SuppressedInternal<>(name, timeToWaitForMoreEvents, bufferConfig, timeDefinition, safeToDropTombstones);}
public String kafkatest_f13167_0()
{    return name;}
public long kafkatest_f13176_0(final ProcessorContext context, final K key)
{    return context.timestamp();}
public TimeDefinitionType kafkatest_f13177_0()
{    return TimeDefinitionType.RECORD_TIME;}
public KTable<Windowed<K>, Long> kafkatest_f13186_0()
{    return doCount(Materialized.with(keySerde, Serdes.Long()));}
public KTable<Windowed<K>, Long> kafkatest_f13187_0(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(materialized, "materialized can't be null");    // we used to burn a topology name here, so we have to keep doing it for compatibility    if (new MaterializedInternal<>(materialized).storeName() == null) {        builder.newStoreName(AGGREGATE_NAME);    }    return doCount(materialized);}
public void kafkatest_f13196_0(final ProcessorContext context)
{    transformer.init(context);}
public Iterable<KeyValue<KOut, VOut>> kafkatest_f13197_0(final KIn key, final VIn value)
{    final KeyValue<KOut, VOut> pair = transformer.transform(key, value);    if (pair != null) {        return Collections.singletonList(pair);    }    return Collections.emptyList();}
public static Joined<K, V, VO> kafkatest_f13206_0(final String name)
{    return new Joined<>(null, null, null, name);}
public static Joined<K, V, VO> kafkatest_f13207_0(final String name)
{    return new Joined<>(null, null, null, name);}
public static JoinWindows kafkatest_f13216_0(final long timeDifferenceMs) throws IllegalArgumentException
{    // This is a static factory method, so we initialize grace and retention to the defaults.    return new JoinWindows(timeDifferenceMs, timeDifferenceMs, -1L, DEFAULT_RETENTION_MS);}
public static JoinWindows kafkatest_f13217_0(final Duration timeDifference) throws IllegalArgumentException
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeDifference, "timeDifference");    return of(ApiUtils.validateMillisecondDuration(timeDifference, msgPrefix));}
public JoinWindows kafkatest_f13226_0(final long durationMs) throws IllegalArgumentException
{    if (durationMs < size()) {        throw new IllegalArgumentException("Window retention time (durationMs) cannot be smaller than the window size.");    }    return new JoinWindows(beforeMs, afterMs, graceMs, durationMs, segments);}
public long kafkatest_f13227_0()
{    return Math.max(maintainDurationMs, size());}
public Materialized<K, V, S> kafkatest_f13236_0(final Serde<V> valueSerde)
{    this.valueSerde = valueSerde;    return this;}
public Materialized<K, V, S> kafkatest_f13237_0(final Serde<K> keySerde)
{    this.keySerde = keySerde;    return this;}
private static boolean kafkatest_f13246_0(final String topic)
{    for (int i = 0; i < topic.length(); ++i) {        final char c = topic.charAt(i);        // We don't use Character.isLetterOrDigit(c) because it's slower        final boolean validLetterOrDigit = (c >= 'a' && c <= 'z') || (c >= '0' && c <= '9') || (c >= 'A' && c <= 'Z');        final boolean validChar = validLetterOrDigit || c == '.' || c == '_' || c == '-';        if (!validChar) {            return false;        }    }    return true;}
public String kafkatest_f13247_0(final K key, final V value)
{    return String.format("%s, %s", key, value);}
public static Produced<K, V> kafkatest_f13256_0(final Serde<K> keySerde)
{    return new Produced<>(keySerde, null, null, null);}
public static Produced<K, V> kafkatest_f13257_0(final Serde<V> valueSerde)
{    return new Produced<>(null, valueSerde, null, null);}
public Serialized<K, V> kafkatest_f13266_0(final Serde<K> keySerde)
{    return new Serialized<>(keySerde, null);}
public Serialized<K, V> kafkatest_f13267_0(final Serde<V> valueSerde)
{    return new Serialized<>(null, valueSerde);}
 Serializer<T> kafkatest_f13276_0()
{    return inner;}
public static SessionWindows kafkatest_f13277_0(final long inactivityGapMs)
{    if (inactivityGapMs <= 0) {        throw new IllegalArgumentException("Gap time (inactivityGapMs) cannot be zero or negative.");    }    return new SessionWindows(inactivityGapMs, DEFAULT_RETENTION_MS, -1);}
public String kafkatest_f13286_0()
{    return "SessionWindows{" + "gapMs=" + gapMs + ", maintainDurationMs=" + maintainDurationMs + ", graceMs=" + graceMs + '}';}
 static EagerBufferConfig kafkatest_f13287_0(final long recordLimit)
{    return new EagerBufferConfigImpl(recordLimit, Long.MAX_VALUE);}
public void kafkatest_f13296_0(final boolean isChangelogTopic)
{    this.isChangelogTopic = isChangelogTopic;}
 Deserializer<T> kafkatest_f13297_0()
{    return inner;}
public TimeWindows kafkatest_f13306_0(final Duration advance)
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(advance, "advance");    return advanceBy(ApiUtils.validateMillisecondDuration(advance, msgPrefix));}
public Map<Long, TimeWindow> kafkatest_f13307_0(final long timestamp)
{    long windowStart = (Math.max(0, timestamp - sizeMs + advanceMs) / advanceMs) * advanceMs;    final Map<Long, TimeWindow> windows = new LinkedHashMap<>();    while (windowStart <= timestamp) {        final TimeWindow window = new TimeWindow(windowStart, windowStart + sizeMs);        windows.put(windowStart, window);        windowStart += advanceMs;    }    return windows;}
public static UnlimitedWindows kafkatest_f13316_0()
{    return new UnlimitedWindows(DEFAULT_START_TIMESTAMP_MS);}
public UnlimitedWindows kafkatest_f13317_0(final long startMs) throws IllegalArgumentException
{    if (startMs < 0) {        throw new IllegalArgumentException("Window start time (startMs) cannot be negative.");    }    return new UnlimitedWindows(startMs);}
public String kafkatest_f13326_0()
{    return "UnlimitedWindows{" + "startMs=" + startMs + ", segments=" + segments + '}';}
public long kafkatest_f13327_0()
{    return startMs;}
public String kafkatest_f13336_0()
{    return "[" + key + "@" + window.start() + "/" + window.end() + "]";}
public boolean kafkatest_f13337_0(final Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof Windowed)) {        return false;    }    final Windowed<?> that = (Windowed) obj;    return window.equals(that.window) && key.equals(that.key);}
public long kafkatest_f13346_0()
{    return maintainDurationMs;}
protected Windows<W> kafkatest_f13347_0(final int segments) throws IllegalArgumentException
{    if (segments < 2) {        throw new IllegalArgumentException("Number of segments must be at least 2.");    }    this.segments = segments;    return this;}
public String kafkatest_f13362_0()
{    return applicationId;}
public TaskId kafkatest_f13363_0()
{    return taskId;}
public Headers kafkatest_f13372_0()
{    if (recordContext == null) {        throw new IllegalStateException("This should not happen as headers() should only be called while a record is processed");    }    return recordContext.headers();}
public long kafkatest_f13373_0()
{    if (recordContext == null) {        throw new IllegalStateException("This should not happen as timestamp() should only be called while a record is processed");    }    return recordContext.timestamp();}
public void kafkatest_f13382_0()
{    initialized = false;}
public TaskId kafkatest_f13383_0()
{    return id;}
 void kafkatest_f13392_0()
{    stateMgr.flush();}
 void kafkatest_f13393_0()
{    if (topology.stateStores().isEmpty()) {        return;    }    try {        if (!stateDirectory.lock(id)) {            throw new LockException(String.format("%sFailed to lock the state directory for task %s", logPrefix, id));        }    } catch (final IOException e) {        throw new StreamsException(String.format("%sFatal error while trying to lock the state directory for task %s", logPrefix, id));    }    log.trace("Initializing state stores");    for (final StateStore store : topology.stateStores()) {        log.trace("Initializing store {}", store.name());        processorContext.uninitialize();        store.init(processorContext, store);    }}
public StreamTask kafkatest_f13402_0(final TopicPartition partition)
{    return restoringByPartition.get(partition);}
 List<StreamTask> kafkatest_f13403_0()
{    final List<StreamTask> tasks = super.allTasks();    tasks.addAll(restoring.values());    return tasks;}
 intf13412_1)
{    int punctuated = 0;    final Iterator<Map.Entry<TaskId, StreamTask>> it = running.entrySet().iterator();    while (it.hasNext()) {        final StreamTask task = it.next().getValue();        try {            if (task.maybePunctuateStreamTime()) {                punctuated++;            }            if (task.maybePunctuateSystemTime()) {                punctuated++;            }        } catch (final TaskMigratedException e) {                        final RuntimeException fatalException = closeZombieTask(task);            if (fatalException != null) {                throw fatalException;            }            it.remove();            throw e;        } catch (final KafkaException e) {                        throw e;        }    }    return punctuated;}
 void kafkatest_f13413_0()
{    super.clear();    restoring.clear();    restoringByPartition.clear();    restoredPartitions.clear();}
private RuntimeExceptionf13422_1final Collection<T> tasks)
{    final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);    for (final Iterator<T> it = tasks.iterator(); it.hasNext(); ) {        final T task = it.next();        try {            task.suspend();            suspended.put(task.id(), task);        } catch (final TaskMigratedException closeAsZombieAndSwallow) {            // as we suspend a task, we are either shutting down or rebalancing, thus, we swallow and move on                        firstException.compareAndSet(null, closeZombieTask(task));            it.remove();        } catch (final RuntimeException e) {                        firstException.compareAndSet(null, e);            try {                task.close(false, false);            } catch (final RuntimeException f) {                            }        }    }    return firstException.get();}
 RuntimeExceptionf13423_1final T task)
{    try {        task.close(false, true);    } catch (final RuntimeException e) {         ignore and proceed.", taskTypeName, task.id(), e.toString());        return e;    }    return null;}
 void kafkatest_f13432_0(final StringBuilder builder, final Collection<T> tasks, final String indent, final String name)
{    builder.append(indent).append(name);    for (final T t : tasks) {        builder.append(indent).append(t.toString(indent + "\t\t"));    }    builder.append("\n");}
 List<T> kafkatest_f13433_0()
{    final List<T> tasks = new ArrayList<>();    tasks.addAll(running.values());    tasks.addAll(suspended.values());    tasks.addAll(created.values());    return tasks;}
public int kafkatest_f13442_0()
{    return errCode;}
public int kafkatest_f13443_0()
{    return latestSupportedVersion;}
private void kafkatest_f13452_0(final DataOutputStream out) throws IOException
{    // Build a dictionary to encode topicNames    int topicIndex = 0;    final Map<String, Integer> topicNameDict = new HashMap<>();    for (final Map.Entry<HostInfo, Set<TopicPartition>> entry : partitionsByHost.entrySet()) {        for (final TopicPartition topicPartition : entry.getValue()) {            if (!topicNameDict.containsKey(topicPartition.topic())) {                topicNameDict.put(topicPartition.topic(), topicIndex++);            }        }    }    // write the topic name dictionary out    out.writeInt(topicNameDict.size());    for (final Map.Entry<String, Integer> entry : topicNameDict.entrySet()) {        out.writeInt(entry.getValue());        out.writeUTF(entry.getKey());    }    // encode partitions by host    out.writeInt(partitionsByHost.size());    // Write the topic index, partition    for (final Map.Entry<HostInfo, Set<TopicPartition>> entry : partitionsByHost.entrySet()) {        writeHostInfo(out, entry.getKey());        out.writeInt(entry.getValue().size());        for (final TopicPartition partition : entry.getValue()) {            out.writeInt(topicNameDict.get(partition.topic()));            out.writeInt(partition.partition());        }    }}
private void kafkatest_f13453_0(final DataOutputStream out, final HostInfo hostInfo) throws IOException
{    out.writeUTF(hostInfo.host());    out.writeInt(hostInfo.port());}
private static void kafkatest_f13462_0(final AssignmentInfo assignmentInfo, final DataInputStream in) throws IOException
{    decodeActiveTasks(assignmentInfo, in);    decodeStandbyTasks(assignmentInfo, in);    decodePartitionsByHost(assignmentInfo, in);}
private static void kafkatest_f13463_0(final AssignmentInfo assignmentInfo, final DataInputStream in) throws IOException
{    assignmentInfo.partitionsByHost = new HashMap<>();    final int numEntries = in.readInt();    for (int i = 0; i < numEntries; i++) {        final HostInfo hostInfo = new HostInfo(in.readUTF(), in.readInt());        assignmentInfo.partitionsByHost.put(hostInfo, readTopicPartitions(in));    }}
public String kafkatest_f13472_0()
{    return "[version=" + usedVersion + ", supported version=" + latestSupportedVersion + ", active tasks=" + activeTasks + ", standby tasks=" + standbyTasks + ", partitions by host=" + partitionsByHost + "]";}
public AtomicIntegerf13473_1final Map<String, ?> configs)
{    final Object ai = configs.get(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE);    if (ai == null) {        final KafkaException fatalException = new KafkaException("assignmentErrorCode is not specified");                throw fatalException;    }    if (!(ai instanceof AtomicInteger)) {        final KafkaException fatalException = new KafkaException(String.format("%s is not an instance of %s", ai.getClass().getName(), AtomicInteger.class.getName()));                throw fatalException;    }    return (AtomicInteger) ai;}
public int kafkatest_f13482_0()
{    return code;}
public ClientState kafkatest_f13483_0()
{    return new ClientState(new HashSet<>(activeTasks), new HashSet<>(standbyTasks), new HashSet<>(assignedTasks), new HashSet<>(prevActiveTasks), new HashSet<>(prevStandbyTasks), new HashSet<>(prevAssignedTasks), capacity);}
public void kafkatest_f13492_0(final Set<TaskId> prevTasks)
{    prevActiveTasks.addAll(prevTasks);    prevAssignedTasks.addAll(prevTasks);}
public void kafkatest_f13493_0(final Set<TaskId> standbyTasks)
{    prevStandbyTasks.addAll(standbyTasks);    prevAssignedTasks.addAll(standbyTasks);}
 int kafkatest_f13502_0()
{    return capacity;}
 boolean kafkatest_f13503_0(final int tasksPerThread)
{    return activeTasks.size() < capacity * tasksPerThread;}
private Set<ID> kafkatest_f13512_0(final TaskId taskId)
{    final Set<ID> clientIds = new HashSet<>();    for (final Map.Entry<ID, ClientState> client : clients.entrySet()) {        if (!client.getValue().hasAssignedTask(taskId)) {            clientIds.add(client.getKey());        }    }    return clientIds;}
private ClientState kafkatest_f13513_0(final TaskId taskId, final Set<ID> clientsWithin)
{    // optimize the case where there is only 1 id to search within.    if (clientsWithin.size() == 1) {        return clients.get(clientsWithin.iterator().next());    }    final ClientState previous = findClientsWithPreviousAssignedTask(taskId, clientsWithin);    if (previous == null) {        return leastLoaded(taskId, clientsWithin);    }    if (shouldBalanceLoad(previous)) {        final ClientState standby = findLeastLoadedClientWithPreviousStandByTask(taskId, clientsWithin);        if (standby == null || shouldBalanceLoad(standby)) {            return leastLoaded(taskId, clientsWithin);        }        return standby;    }    return previous;}
 boolean kafkatest_f13522_0(final TaskId task1, final Set<TaskId> taskIds)
{    if (pairs.size() == maxPairs) {        return false;    }    for (final TaskId taskId : taskIds) {        if (!pairs.contains(pair(task1, taskId))) {            return true;        }    }    return false;}
 void kafkatest_f13523_0(final TaskId taskId, final Set<TaskId> assigned)
{    for (final TaskId id : assigned) {        pairs.add(pair(id, taskId));    }}
public String kafkatest_f13532_0()
{    return userEndPoint;}
public ByteBuffer kafkatest_f13533_0()
{    final ByteBuffer buf;    switch(usedVersion) {        case 1:            buf = encodeVersionOne();            break;        case 2:            buf = encodeVersionTwo();            break;        case 3:            buf = encodeVersionThree();            break;        case 4:            buf = encodeVersionFour();            break;        case 5:            buf = encodeVersionFive();            break;        default:            throw new IllegalStateException("Unknown metadata version: " + usedVersion + "; latest supported version: " + LATEST_SUPPORTED_VERSION);    }    buf.rewind();    return buf;}
private ByteBuffer kafkatest_f13542_0(final int usedVersion)
{    final byte[] endPointBytes = prepareUserEndPoint();    final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeFourAndFiveByteLength(endPointBytes));    // used version    buf.putInt(usedVersion);    // supported version    buf.putInt(LATEST_SUPPORTED_VERSION);    encodeClientUUID(buf);    encodeTasks(buf, prevTasks);    encodeTasks(buf, standbyTasks);    encodeUserEndPoint(buf, endPointBytes);    return buf;}
private ByteBuffer kafkatest_f13543_0()
{    return encodeVersionThreeFourAndFive(3);}
private static void kafkatest_f13552_0(final SubscriptionInfo subscriptionInfo, final ByteBuffer data)
{    final int bytesLength = data.getInt();    if (bytesLength != 0) {        final byte[] bytes = new byte[bytesLength];        data.get(bytes);        subscriptionInfo.userEndPoint = new String(bytes, StandardCharsets.UTF_8);    }}
private static void kafkatest_f13553_0(final SubscriptionInfo subscriptionInfo, final ByteBuffer data)
{    decodeClientUUID(subscriptionInfo, data);    decodeTasks(subscriptionInfo, data);    decodeUserEndPoint(subscriptionInfo, data);}
public void kafkatest_f13562_0(final Collection<KeyValue<byte[], byte[]>> records)
{    throw new UnsupportedOperationException();}
public void kafkatest_f13563_0(final byte[] key, final byte[] value)
{    throw new UnsupportedOperationException("Single restore functionality shouldn't be called directly but " + "through the delegated StateRestoreCallback instance");}
public TaskId kafkatest_f13573_0()
{    return delegate.taskId();}
public Serde<?> kafkatest_f13574_0()
{    return delegate.keySerde();}
public void kafkatest_f13583_0(final K key, final V value, final To to)
{    throw new StreamsException("ProcessorContext#forward() not supported.");}
public void kafkatest_f13584_0(final K key, final V value, final int childIndex)
{    throw new StreamsException("ProcessorContext#forward() not supported.");}
public Map<String, Object> kafkatest_f13593_0(final String prefix)
{    return delegate.appConfigsWithPrefix(prefix);}
public StateStore kafkatest_f13594_0(final String name)
{    final StateStore store = stateManager.getGlobalStore(name);    if (store instanceof TimestampedKeyValueStore) {        return new TimestampedKeyValueStoreReadWriteDecorator((TimestampedKeyValueStore) store);    } else if (store instanceof KeyValueStore) {        return new KeyValueStoreReadWriteDecorator((KeyValueStore) store);    } else if (store instanceof TimestampedWindowStore) {        return new TimestampedWindowStoreReadWriteDecorator((TimestampedWindowStore) store);    } else if (store instanceof WindowStore) {        return new WindowStoreReadWriteDecorator((WindowStore) store);    } else if (store instanceof SessionStore) {        return new SessionStoreReadWriteDecorator((SessionStore) store);    }    return store;}
public Set<String>f13603_1)
{    try {        if (!stateDirectory.lockGlobalState()) {            throw new LockException(String.format("Failed to lock the global state directory: %s", baseDir));        }    } catch (final IOException e) {        throw new LockException(String.format("Failed to lock the global state directory: %s", baseDir), e);    }    try {        checkpointFileCache.putAll(checkpointFile.read());    } catch (final IOException e) {        try {            stateDirectory.unlockGlobalState();        } catch (final IOException e1) {                    }        throw new StreamsException("Failed to read checkpoints for global state globalStores", e);    }    final List<StateStore> stateStores = topology.globalStateStores();    for (final StateStore stateStore : stateStores) {        globalStoreNames.add(stateStore.name());        stateStore.init(globalProcessorContext, stateStore);    }    return Collections.unmodifiableSet(globalStoreNames);}
public void kafkatest_f13604_0(final Collection<TopicPartition> partitions, final InternalProcessorContext processorContext)
{    StateManagerUtil.reinitializeStateStoresForPartitions(log, eosEnabled, baseDir, globalStores, topology.storeToChangelogTopic(), partitions, processorContext, checkpointFile, checkpointFileCache);    globalConsumer.assign(partitions);    globalConsumer.seekToBeginning(partitions);}
public voidf13613_1final Map<TopicPartition, Long> offsets)
{    checkpointFileCache.putAll(offsets);    final Map<TopicPartition, Long> filteredOffsets = new HashMap<>();    // Skip non persistent store    for (final Map.Entry<TopicPartition, Long> topicPartitionOffset : checkpointFileCache.entrySet()) {        final String topic = topicPartitionOffset.getKey().topic();        if (!globalNonPersistentStoresTopics.contains(topic)) {            filteredOffsets.put(topicPartitionOffset.getKey(), topicPartitionOffset.getValue());        }    }    try {        checkpointFile.write(filteredOffsets);    } catch (final IOException e) {            }}
public Map<TopicPartition, Long> kafkatest_f13614_0()
{    return Collections.unmodifiableMap(checkpointFileCache);}
public State kafkatest_f13623_0()
{    // we do not need to use the stat lock since the variable is volatile    return state;}
private voidf13624_1final State newState)
{    final State oldState = state;    synchronized (stateLock) {        if (state == State.PENDING_SHUTDOWN && newState == State.PENDING_SHUTDOWN) {            // will be refused but we do not throw exception here            return;        } else if (state == State.DEAD) {            // will be refused but we do not throw exception here            return;        } else if (!state.isValidTransition(newState)) {                        throw new StreamsException(logPrefix + "Unexpected state transition from " + oldState + " to " + newState);        } else {                    }        state = newState;    }    if (stateListener != null) {        stateListener.onChange(this, state, oldState);    }}
public Map<MetricName, Metric> kafkatest_f13633_0()
{    return Collections.unmodifiableMap(globalConsumer.metrics());}
public String kafkatest_f13634_0()
{    return name;}
private Set<String> kafkatest_f13643_0()
{    return users;}
public boolean kafkatest_f13644_0()
{    return builder.loggingEnabled();}
private boolean kafkatest_f13653_0(final String topic)
{    return pattern.matcher(topic).matches();}
 Source kafkatest_f13654_0()
{    return new Source(name, topics.size() == 0 ? null : new HashSet<>(topics), pattern);}
public final void kafkatest_f13663_0(final String name, final ProcessorSupplier supplier, final String... predecessorNames)
{    Objects.requireNonNull(name, "name must not be null");    Objects.requireNonNull(supplier, "supplier must not be null");    Objects.requireNonNull(predecessorNames, "predecessor names must not be null");    if (nodeFactories.containsKey(name)) {        throw new TopologyException("Processor " + name + " is already added.");    }    if (predecessorNames.length == 0) {        throw new TopologyException("Processor " + name + " must have at least one parent");    }    for (final String predecessor : predecessorNames) {        Objects.requireNonNull(predecessor, "predecessor name must not be null");        if (predecessor.equals(name)) {            throw new TopologyException("Processor " + name + " cannot be a predecessor of itself.");        }        if (!nodeFactories.containsKey(predecessor)) {            throw new TopologyException("Predecessor processor " + predecessor + " is not added yet for " + name);        }    }    nodeFactories.put(name, new ProcessorNodeFactory(name, predecessorNames, supplier));    nodeGrouper.add(name);    nodeGrouper.unite(name, predecessorNames);    nodeGroups = null;}
public final void kafkatest_f13664_0(final StoreBuilder<?> storeBuilder, final String... processorNames)
{    addStateStore(storeBuilder, false, processorNames);}
private void kafkatest_f13673_0(final String processorName, final String stateStoreName)
{    if (globalStateBuilders.containsKey(stateStoreName)) {        throw new TopologyException("Global StateStore " + stateStoreName + " can be used by a Processor without being specified; it should not be explicitly passed.");    }    if (!stateFactories.containsKey(stateStoreName)) {        throw new TopologyException("StateStore " + stateStoreName + " is not added yet.");    }    if (!nodeFactories.containsKey(processorName)) {        throw new TopologyException("Processor " + processorName + " is not added yet.");    }    final StateStoreFactory stateStoreFactory = stateFactories.get(stateStoreName);    final Iterator<String> iter = stateStoreFactory.users().iterator();    if (iter.hasNext()) {        final String user = iter.next();        nodeGrouper.unite(user, processorName);    }    stateStoreFactory.users().add(processorName);    final NodeFactory nodeFactory = nodeFactories.get(processorName);    if (nodeFactory instanceof ProcessorNodeFactory) {        final ProcessorNodeFactory processorNodeFactory = (ProcessorNodeFactory) nodeFactory;        processorNodeFactory.addStateStore(stateStoreName);        connectStateStoreNameToSourceTopicsOrPattern(stateStoreName, processorNodeFactory);    } else {        throw new TopologyException("cannot connect a state store " + stateStoreName + " to a source node or a sink node.");    }}
private Set<SourceNodeFactory> kafkatest_f13674_0(final String[] predecessors)
{    final Set<SourceNodeFactory> sourceNodes = new HashSet<>();    for (final String predecessor : predecessors) {        final NodeFactory nodeFactory = nodeFactories.get(predecessor);        if (nodeFactory instanceof SourceNodeFactory) {            sourceNodes.add((SourceNodeFactory) nodeFactory);        } else if (nodeFactory instanceof ProcessorNodeFactory) {            sourceNodes.addAll(findSourcesForProcessorPredecessors(((ProcessorNodeFactory) nodeFactory).predecessors));        }    }    return sourceNodes;}
private Set<String> kafkatest_f13683_0()
{    final Set<String> globalGroups = new HashSet<>();    for (final Map.Entry<Integer, Set<String>> nodeGroup : nodeGroups().entrySet()) {        final Set<String> nodes = nodeGroup.getValue();        for (final String node : nodes) {            if (isGlobalSource(node)) {                globalGroups.addAll(nodes);            }        }    }    return globalGroups;}
private ProcessorTopology kafkatest_f13684_0(final Set<String> nodeGroup)
{    Objects.requireNonNull(applicationId, "topology has not completed optimization");    final Map<String, ProcessorNode> processorMap = new LinkedHashMap<>();    final Map<String, SourceNode> topicSourceMap = new HashMap<>();    final Map<String, SinkNode> topicSinkMap = new HashMap<>();    final Map<String, StateStore> stateStoreMap = new LinkedHashMap<>();    final Set<String> repartitionTopics = new HashSet<>();    // also make sure the state store map values following the insertion ordering    for (final NodeFactory factory : nodeFactories.values()) {        if (nodeGroup == null || nodeGroup.contains(factory.name)) {            final ProcessorNode node = factory.build();            processorMap.put(node.name(), node);            if (factory instanceof ProcessorNodeFactory) {                buildProcessorNode(processorMap, stateStoreMap, (ProcessorNodeFactory) factory, node);            } else if (factory instanceof SourceNodeFactory) {                buildSourceNode(topicSourceMap, repartitionTopics, (SourceNodeFactory) factory, (SourceNode) node);            } else if (factory instanceof SinkNodeFactory) {                buildSinkNode(processorMap, topicSinkMap, repartitionTopics, (SinkNodeFactory) factory, (SinkNode) node);            } else {                throw new TopologyException("Unknown definition class: " + factory.getClass().getName());            }        }    }    return new ProcessorTopology(new ArrayList<>(processorMap.values()), topicSourceMap, topicSinkMap, new ArrayList<>(stateStoreMap.values()), new ArrayList<>(globalStateStores.values()), storeToChangelogTopic, repartitionTopics);}
private InternalTopicConfig kafkatest_f13693_0(final StateStoreFactory factory, final String name)
{    if (factory.isWindowStore()) {        final WindowedChangelogTopicConfig config = new WindowedChangelogTopicConfig(name, factory.logConfig());        config.setRetentionMs(factory.retentionPeriod());        return config;    } else {        return new UnwindowedChangelogTopicConfig(name, factory.logConfig());    }}
public synchronized Pattern kafkatest_f13694_0()
{    return resetTopicsPattern(earliestResetTopics, earliestResetPatterns);}
 synchronized Pattern kafkatest_f13703_0()
{    if (topicPattern == null) {        final List<String> allSourceTopics = new ArrayList<>();        if (!nodeToSourceTopics.isEmpty()) {            for (final List<String> topics : nodeToSourceTopics.values()) {                allSourceTopics.addAll(maybeDecorateInternalSourceTopics(topics));            }            allSourceTopics.removeAll(globalTopics);        }        Collections.sort(allSourceTopics);        topicPattern = buildPatternForOffsetResetTopics(allSourceTopics, nodeToSourcePatterns.values());    }    return topicPattern;}
 synchronized voidf13704_1final SubscriptionUpdates subscriptionUpdates, final String logPrefix)
{        this.subscriptionUpdates = subscriptionUpdates;    setRegexMatchedTopicsToSourceNodes();    setRegexMatchedTopicToStateStore();}
public TopologyDescription.Source kafkatest_f13713_0()
{    return source;}
public TopologyDescription.Processor kafkatest_f13714_0()
{    return processor;}
public String kafkatest_f13723_0()
{    return topics.toString();}
public Set<String> kafkatest_f13724_0()
{    return topics;}
public int kafkatest_f13733_0()
{    // omit successor as it might change and alter the hash code    return Objects.hash(name, stores);}
public String kafkatest_f13734_0()
{    if (topicNameExtractor instanceof StaticTopicNameExtractor) {        return ((StaticTopicNameExtractor) topicNameExtractor).topicName;    } else {        return null;    }}
public String kafkatest_f13743_0()
{    return "Sub-topology: " + id + "\n" + nodesAsString() + "\n";}
private String kafkatest_f13744_0()
{    final StringBuilder sb = new StringBuilder();    for (final TopologyDescription.Node node : nodes) {        sb.append("    ");        sb.append(node);        sb.append('\n');    }    return sb.toString();}
public void kafkatest_f13753_0(final TopologyDescription.GlobalStore globalStore)
{    globalStores.add(globalStore);}
public Set<TopologyDescription.Subtopology> kafkatest_f13754_0()
{    return Collections.unmodifiableSet(subtopologies);}
public String kafkatest_f13763_0()
{    return String.format("SubscriptionUpdates{updatedTopicSubscriptions=%s}", updatedTopicSubscriptions);}
 voidf13764_1final Set<String> topics, final String logPrefix)
{    final SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();        // update the topic groups with the returned subscription set for regex pattern subscriptions    subscriptionUpdates.updateTopics(topics);    updateSubscriptions(subscriptionUpdates, logPrefix);}
public final void kafkatest_f13773_0()
{    synchronized (threadLevelSensors) {        while (!threadLevelSensors.isEmpty()) {            metrics.removeSensor(threadLevelSensors.pop());        }    }}
public Map<String, String> kafkatest_f13774_0(final String taskName)
{    final Map<String, String> tagMap = threadLevelTagMap();    tagMap.put(TASK_ID_TAG, taskName);    return tagMap;}
public final void kafkatest_f13783_0(final String taskName, final String cacheName)
{    final String key = cacheSensorPrefix(taskName, cacheName);    synchronized (cacheLevelSensors) {        final Deque<String> strings = cacheLevelSensors.remove(key);        while (strings != null && !strings.isEmpty()) {            metrics.removeSensor(strings.pop());        }    }}
private String kafkatest_f13784_0(final String taskName, final String cacheName)
{    return taskSensorPrefix(taskName) + SENSOR_PREFIX_DELIMITER + "cache" + SENSOR_PREFIX_DELIMITER + cacheName;}
public final Map<String, String> kafkatest_f13793_0(final String... tags)
{    final Map<String, String> tagMap = new LinkedHashMap<>();    tagMap.put("client-id", threadName);    if (tags != null) {        if ((tags.length % 2) != 0) {            throw new IllegalArgumentException("Tags needs to be specified in key-value pairs");        }        for (int i = 0; i < tags.length; i += 2) {            tagMap.put(tags[i], tags[i + 1]);        }    }    return tagMap;}
private Map<String, String> kafkatest_f13794_0(final String scopeName, final String entityName, final String... tags)
{    final String[] updatedTags = Arrays.copyOf(tags, tags.length + 2);    updatedTags[tags.length] = scopeName + "-id";    updatedTags[tags.length + 1] = entityName;    return tagMap(updatedTags);}
public static void kafkatest_f13803_0(final Sensor sensor, final String group, final Map<String, String> tags, final String operation, final String descriptionOfRate, final String descriptionOfTotal)
{    addRateOfSumMetricToSensor(sensor, group, tags, operation, descriptionOfRate);    addSumMetricToSensor(sensor, group, tags, operation, descriptionOfTotal);}
public static void kafkatest_f13804_0(final Sensor sensor, final String group, final Map<String, String> tags, final String operation, final String description)
{    sensor.add(new MetricName(operation + RATE_SUFFIX, group, description, tags), new Rate(TimeUnit.SECONDS, new WindowedSum()));}
public static Sensor kafkatest_f13813_0(final StreamsMetricsImpl streamsMetrics)
{    final Sensor commitSensor = streamsMetrics.threadLevelSensor(COMMIT, Sensor.RecordingLevel.INFO);    final Map<String, String> tagMap = streamsMetrics.threadLevelTagMap();    addAvgAndMaxToSensor(commitSensor, THREAD_LEVEL_GROUP, tagMap, COMMIT_LATENCY);    addInvocationRateAndCountToSensor(commitSensor, THREAD_LEVEL_GROUP, tagMap, COMMIT, COMMIT_TOTAL_DESCRIPTION, COMMIT_RATE_DESCRIPTION);    return commitSensor;}
public static Sensor kafkatest_f13814_0(final StreamsMetricsImpl streamsMetrics)
{    final Sensor pollSensor = streamsMetrics.threadLevelSensor(POLL, Sensor.RecordingLevel.INFO);    final Map<String, String> tagMap = streamsMetrics.threadLevelTagMap();    addAvgAndMaxToSensor(pollSensor, THREAD_LEVEL_GROUP, tagMap, POLL_LATENCY);    addInvocationRateAndCountToSensor(pollSensor, THREAD_LEVEL_GROUP, tagMap, POLL, POLL_TOTAL_DESCRIPTION, POLL_RATE_DESCRIPTION);    return pollSensor;}
 int kafkatest_f13823_0(final TopicPartition partition, final Iterable<ConsumerRecord<byte[], byte[]>> rawRecords)
{    final RecordQueue recordQueue = partitionQueues.get(partition);    final int oldSize = recordQueue.size();    final int newSize = recordQueue.addRawRecords(rawRecords);    // add this record queue to be considered for processing in the future if it was empty before    if (oldSize == 0 && newSize > 0) {        nonEmptyQueuesByTime.offer(recordQueue);        // processed next, and hence the stream-time will be updated when we retrieved records by then        if (nonEmptyQueuesByTime.size() == this.partitionQueues.size()) {            allBuffered = true;        }    }    totalBuffered += newSize - oldSize;    return newSize;}
public Set<TopicPartition> kafkatest_f13824_0()
{    return Collections.unmodifiableSet(partitionQueues.keySet());}
public StateStore kafkatest_f13833_0(final String name)
{    if (currentNode() == null) {        throw new StreamsException("Accessing from an unknown node");    }    final StateStore global = stateManager.getGlobalStore(name);    if (global != null) {        if (global instanceof TimestampedKeyValueStore) {            return new TimestampedKeyValueStoreReadOnlyDecorator((TimestampedKeyValueStore) global);        } else if (global instanceof KeyValueStore) {            return new KeyValueStoreReadOnlyDecorator((KeyValueStore) global);        } else if (global instanceof TimestampedWindowStore) {            return new TimestampedWindowStoreReadOnlyDecorator((TimestampedWindowStore) global);        } else if (global instanceof WindowStore) {            return new WindowStoreReadOnlyDecorator((WindowStore) global);        } else if (global instanceof SessionStore) {            return new SessionStoreReadOnlyDecorator((SessionStore) global);        }        return global;    }    if (!currentNode().stateStores.contains(name)) {        throw new StreamsException("Processor " + currentNode().name() + " has no access to StateStore " + name + " as the store is not connected to the processor. If you add stores manually via '.addStateStore()' " + "make sure to connect the added store to the processor by providing the processor name to " + "'.addStateStore()' or connect them via '.connectProcessorAndStateStores()'. " + "DSL users need to provide the store name to '.process()', '.transform()', or '.transformValues()' " + "to connect the store to the corresponding operator. If you do not add stores manually, " + "please file a bug report at https://issues.apache.org/jira/projects/KAFKA.");    }    final StateStore store = stateManager.getStore(name);    if (store instanceof TimestampedKeyValueStore) {        return new TimestampedKeyValueStoreReadWriteDecorator((TimestampedKeyValueStore) store);    } else if (store instanceof KeyValueStore) {        return new KeyValueStoreReadWriteDecorator((KeyValueStore) store);    } else if (store instanceof TimestampedWindowStore) {        return new TimestampedWindowStoreReadWriteDecorator((TimestampedWindowStore) store);    } else if (store instanceof WindowStore) {        return new WindowStoreReadWriteDecorator((WindowStore) store);    } else if (store instanceof SessionStore) {        return new SessionStoreReadWriteDecorator((SessionStore) store);    }    return store;}
public void kafkatest_f13834_0(final K key, final V value)
{    forward(key, value, SEND_TO_ALL);}
public void kafkatest_f13843_0(final ProcessorContext context, final StateStore root)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
public void kafkatest_f13844_0()
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
public void kafkatest_f13853_0(final K key, final V value)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
public void kafkatest_f13854_0(final K key, final V value, final long windowStartTimestamp)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
public void kafkatest_f13863_0(final Windowed<K> sessionKey, final AGG aggregate)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
public AGG kafkatest_f13864_0(final K key, final long startTime, final long endTime)
{    return wrapped().fetchSession(key, startTime, endTime);}
public void kafkatest_f13873_0(final K key, final V value)
{    wrapped().put(key, value);}
public V kafkatest_f13874_0(final K key, final V value)
{    return wrapped().putIfAbsent(key, value);}
public KeyValueIterator<Windowed<K>, V> kafkatest_f13883_0()
{    return wrapped().all();}
public KeyValueIterator<Windowed<K>, AGG> kafkatest_f13884_0(final K key, final long earliestSessionEndTime, final long latestSessionStartTime)
{    return wrapped().findSessions(key, earliestSessionEndTime, latestSessionStartTime);}
public List<ProcessorNode<?, ?>> kafkatest_f13893_0()
{    return children;}
 ProcessorNode kafkatest_f13894_0(final String childName)
{    return childByName.get(childName);}
private void kafkatest_f13903_0()
{    metrics.removeAllNodeLevelSensors(taskName, processorNodeName);}
private static Sensor kafkatest_f13904_0(final String operation, final StreamsMetricsImpl metrics, final String taskName, final String processorNodeName, final Map<String, String> taskTags, final Map<String, String> nodeTags)
{    final Sensor parent = metrics.taskLevelSensor(taskName, operation, Sensor.RecordingLevel.DEBUG);    addAvgAndMaxLatencyToSensor(parent, PROCESSOR_NODE_METRICS_GROUP, taskTags, operation);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(parent, PROCESSOR_NODE_METRICS_GROUP, taskTags, operation);    final Sensor sensor = metrics.nodeLevelSensor(taskName, processorNodeName, operation, Sensor.RecordingLevel.DEBUG, parent);    addAvgAndMaxLatencyToSensor(sensor, PROCESSOR_NODE_METRICS_GROUP, nodeTags, operation);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(sensor, PROCESSOR_NODE_METRICS_GROUP, nodeTags, operation);    return sensor;}
public boolean kafkatest_f13913_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final ProcessorRecordContext that = (ProcessorRecordContext) o;    return timestamp == that.timestamp && offset == that.offset && partition == that.partition && Objects.equals(topic, that.topic) && Objects.equals(headers, that.headers);}
public int kafkatest_f13914_0()
{    throw new UnsupportedOperationException("ProcessorRecordContext is unsafe for use in Hash collections");}
 void kafkatest_f13923_0(final TopicPartition partition, final long limit)
{    log.trace("Updating store offset limit for partition {} to {}", partition, limit);    offsetLimits.put(partition, limit);}
 long kafkatest_f13924_0(final TopicPartition partition)
{    final Long limit = offsetLimits.get(partition);    return limit != null ? limit : Long.MAX_VALUE;}
 Collection<TopicPartition> kafkatest_f13933_0()
{    return unmodifiableList(changelogPartitions);}
 void kafkatest_f13934_0()
{    for (final Map.Entry<String, Optional<StateStore>> entry : registeredStores.entrySet()) {        if (!entry.getValue().isPresent()) {            throw new IllegalStateException("store [" + entry.getKey() + "] has not been correctly registered. This is a bug in Kafka Streams.");        }    }}
public List<ProcessorNode> kafkatest_f13943_0()
{    return processorNodes;}
public List<StateStore> kafkatest_f13944_0()
{    return stateStores;}
public Set<String> kafkatest_f13953_0(final String processorName)
{    for (final ProcessorNode<?, ?> node : processorNodes) {        if (node.name().equals(processorName)) {            return node.stateStores;        }    }    return Collections.emptySet();}
public Cancellable kafkatest_f13954_0(final PunctuationSchedule sched)
{    synchronized (pq) {        pq.add(sched);    }    return sched.cancellable();}
public boolean kafkatest_f13963_0(final Object other)
{    return super.equals(other);}
public int kafkatest_f13964_0()
{    return super.hashCode();}
 void kafkatest_f13973_0(final byte[] key, final byte[] value)
{    throw new UnsupportedOperationException();}
public void kafkatest_f13974_0(final Producer<byte[], byte[]> producer)
{    this.producer = producer;}
public Map<TopicPartition, Long> kafkatest_f13983_0()
{    return Collections.unmodifiableMap(offsets);}
 Producer<byte[], byte[]> kafkatest_f13984_0()
{    return producer;}
public long kafkatest_f13993_0()
{    return headRecord == null ? UNKNOWN : headRecord.timestamp;}
 long kafkatest_f13994_0()
{    return partitionTime;}
public void kafkatest_f14003_0(final K key, final V value)
{    final RecordCollector collector = ((RecordCollector.Supplier) context).recordCollector();    final long timestamp = context.timestamp();    if (timestamp < 0) {        throw new StreamsException("Invalid (negative) timestamp of " + timestamp + " for output record <" + key + ":" + value + ">.");    }    final String topic = topicExtractor.extract(key, value, this.context.recordContext());    try {        collector.send(topic, key, value, context.headers(), timestamp, keySerializer, valSerializer, partitioner);    } catch (final ClassCastException e) {        final String keyClass = key == null ? "unknown because key is null" : key.getClass().getName();        final String valueClass = value == null ? "unknown because value is null" : value.getClass().getName();        throw new StreamsException(String.format("A serializer (key: %s / value: %s) is not compatible to the actual key or value type " + "(key type: %s / value type: %s). Change the default Serdes in StreamConfig or " + "provide correct Serdes via method parameters.", keySerializer.getClass().getName(), valSerializer.getClass().getName(), keyClass, valueClass), e);    }}
public String kafkatest_f14004_0()
{    return toString("");}
public int kafkatest_f14013_0(final Object other)
{    final long otherTimestamp = ((Stamped<?>) other).timestamp;    if (timestamp < otherTimestamp) {        return -1;    } else if (timestamp > otherTimestamp) {        return 1;    }    return 0;}
public boolean kafkatest_f14014_0(final Object other)
{    if (other == null || getClass() != other.getClass()) {        return false;    }    final long otherTimestamp = ((Stamped<?>) other).timestamp;    return timestamp == otherTimestamp;}
public Map<TopicPartition, Long> kafkatest_f14028_0()
{    return Collections.emptyMap();}
 StateManager kafkatest_f14029_0()
{    return stateManager;}
public void kafkatest_f14038_0(final K key, final V value, final int childIndex)
{    throw new UnsupportedOperationException("this should not happen: forward() not supported in standby tasks.");}
public void kafkatest_f14039_0(final K key, final V value, final String childName)
{    throw new UnsupportedOperationException("this should not happen: forward() not supported in standby tasks.");}
public void kafkatest_f14048_0()
{// no-op}
public voidf14049_1)
{        allowUpdateOfOffsetLimit();}
 void kafkatest_f14058_0()
{    updateableOffsetLimits.addAll(offsetLimits.keySet());}
public File kafkatest_f14059_0(final TaskId taskId)
{    final File taskDir = new File(stateDir, taskId.toString());    if (createStateDirectory && !taskDir.exists() && !taskDir.mkdir()) {        throw new ProcessorStateException(String.format("task directory [%s] doesn't exist and couldn't be created", taskDir.getPath()));    }    return taskDir;}
private synchronized voidf14068_1final long cleanupDelayMs, final boolean manualUserCall) throws Exception
{    final File[] taskDirs = listTaskDirectories();    if (taskDirs == null || taskDirs.length == 0) {        // nothing to do        return;    }    for (final File taskDir : taskDirs) {        final String dirName = taskDir.getName();        final TaskId id = TaskId.parse(dirName);        if (!locks.containsKey(id)) {            try {                if (lock(id)) {                    final long now = time.milliseconds();                    final long lastModifiedMs = taskDir.lastModified();                    if (now > lastModifiedMs + cleanupDelayMs || manualUserCall) {                        if (!manualUserCall) {                                                    } else {                                                    }                        Utils.delete(taskDir);                    }                }            } catch (final OverlappingFileLockException e) {                // locked by another thread                if (manualUserCall) {                                        throw e;                }            } catch (final IOException e) {                                if (manualUserCall) {                    throw e;                }            } finally {                try {                    unlock(id);                } catch (final IOException e) {                                        if (manualUserCall) {                        throw e;                    }                }            }        }    }}
 File[] kafkatest_f14069_0()
{    return !stateDir.exists() ? new File[0] : stateDir.listFiles(pathname -> pathname.isDirectory() && PATH_NAME.matcher(pathname.getName()).matches());}
 long kafkatest_f14078_0()
{    return checkpointOffset;}
 void kafkatest_f14079_0(final long checkpointOffset)
{    this.checkpointOffset = checkpointOffset;}
 void kafkatest_f14088_0(final long endingOffset)
{    this.endingOffset = Math.min(offsetLimit, endingOffset);}
 long kafkatest_f14089_0()
{    return startingOffset;}
public Collection<TopicPartition>f14098_1final RestoringTasks active)
{    if (!needsInitializing.isEmpty()) {        initialize(active);    }    if (needsRestoring.isEmpty()) {        restoreConsumer.unsubscribe();        return completed();    }    try {        final ConsumerRecords<byte[], byte[]> records = restoreConsumer.poll(pollTime);        for (final TopicPartition partition : needsRestoring) {            final StateRestorer restorer = stateRestorers.get(partition);            final long pos = processNext(records.records(partition), restorer, restoreToOffsets.get(partition));            restorer.setRestoredOffset(pos);            if (restorer.hasCompleted(pos, restoreToOffsets.get(partition))) {                restorer.restoreDone();                restoreToOffsets.remove(partition);                completedRestorers.add(partition);            }        }    } catch (final InvalidOffsetException recoverableException) {                final Set<TopicPartition> partitions = recoverableException.partitions();        for (final TopicPartition partition : partitions) {            final StreamTask task = active.restoringTaskFor(partition);                        needsInitializing.remove(partition);            needsRestoring.remove(partition);            final StateRestorer restorer = stateRestorers.get(partition);            restorer.setCheckpointOffset(StateRestorer.NO_CHECKPOINT);            task.reinitializeStateStoresForPartitions(recoverableException.partitions());        }        restoreConsumer.seekToBeginning(partitions);    }    needsRestoring.removeAll(completedRestorers);    if (needsRestoring.isEmpty()) {        restoreConsumer.unsubscribe();    }    return completed();}
private voidf14099_1final RestoringTasks active)
{    if (!restoreConsumer.subscription().isEmpty()) {        throw new StreamsException("Restore consumer should not be subscribed to any topics (" + restoreConsumer.subscription() + ")");    }    // first refresh the changelog partition information from brokers, since initialize is only called when    // the needsInitializing map is not empty, meaning we do not know the metadata for some of them yet    refreshChangelogInfo();    final Set<TopicPartition> initializable = new HashSet<>();    for (final TopicPartition topicPartition : needsInitializing) {        if (hasPartition(topicPartition)) {            initializable.add(topicPartition);        }    }    // try to fetch end offsets for the initializable restorers and remove any partitions    // where we already have all of the data    final Map<TopicPartition, Long> endOffsets;    try {        endOffsets = restoreConsumer.endOffsets(initializable);    } catch (final TimeoutException e) {        // if timeout exception gets thrown we just give up this time and retry in the next run loop         will fall back to partition by partition fetching", initializable);        return;    }    endOffsets.forEach((partition, endOffset) -> {        if (endOffset != null) {            final StateRestorer restorer = stateRestorers.get(partition);            final long offsetLimit = restorer.offsetLimit();            restoreToOffsets.put(partition, Math.min(endOffset, offsetLimit));        } else {             removing this partition from the current run loop");            initializable.remove(partition);        }    });    final Iterator<TopicPartition> iter = initializable.iterator();    while (iter.hasNext()) {        final TopicPartition topicPartition = iter.next();        final Long restoreOffset = restoreToOffsets.get(topicPartition);        final StateRestorer restorer = stateRestorers.get(topicPartition);        if (restorer.checkpoint() >= restoreOffset) {            restorer.setRestoredOffset(restorer.checkpoint());            iter.remove();            completedRestorers.add(topicPartition);        } else if (restoreOffset == 0) {            restorer.setRestoredOffset(0);            iter.remove();            completedRestorers.add(topicPartition);        } else {            restorer.setEndingOffset(restoreOffset);        }        needsInitializing.remove(topicPartition);    }    // set up restorer for those initializable    if (!initializable.isEmpty()) {        startRestoration(initializable, active);    }}
public String kafkatest_f14108_0()
{    return toString("");}
public String kafkatest_f14109_0(final String indent)
{    final StringBuilder builder = new StringBuilder();    builder.append(indent).append("GlobalMetadata: ").append(allMetadata).append("\n");    builder.append(indent).append("GlobalStores: ").append(globalStores).append("\n");    builder.append(indent).append("My HostInfo: ").append(thisHost).append("\n");    builder.append(indent).append(clusterMetadata).append("\n");    return builder.toString();}
private SourceTopicsInfo kafkatest_f14118_0(final String storeName)
{    final List<String> sourceTopics = builder.stateStoreNameToSourceTopics().get(storeName);    if (sourceTopics == null || sourceTopics.isEmpty()) {        return null;    }    return new SourceTopicsInfo(sourceTopics);}
private boolean kafkatest_f14119_0()
{    return clusterMetadata != null && !clusterMetadata.topics().isEmpty();}
public String kafkatest_f14128_0()
{    return "stream";}
public ByteBuffer kafkatest_f14129_0(final Set<String> topics)
{    // Adds the following information to subscription    // 1. Client UUID (a unique id assigned to an instance of KafkaStreams)    // 2. Task ids of previously running tasks    // 3. Task ids of valid local states on the client's state directory.    final Set<TaskId> previousActiveTasks = taskManager.prevActiveTaskIds();    final Set<TaskId> standbyTasks = taskManager.cachedTasksIds();    standbyTasks.removeAll(previousActiveTasks);    final SubscriptionInfo data = new SubscriptionInfo(usedSubscriptionMetadataVersion, taskManager.processId(), previousActiveTasks, standbyTasks, userEndPoint);    taskManager.updateSubscriptionsFromMetadata(topics);    return data.encode();}
public static void kafkatest_f14138_0(final String logPrefix, final AssignmentInfo info, final List<TopicPartition> partitions, final Map<TaskId, Set<TopicPartition>> activeTasks, final Map<TopicPartition, PartitionInfo> topicToPartitionInfo)
{    processVersionOneAssignment(logPrefix, info, partitions, activeTasks);    // process partitions by host    final Map<HostInfo, Set<TopicPartition>> partitionsByHost = info.partitionsByHost();    for (final Set<TopicPartition> value : partitionsByHost.values()) {        for (final TopicPartition topicPartition : value) {            topicToPartitionInfo.put(topicPartition, new PartitionInfo(topicPartition.topic(), topicPartition.partition(), null, new Node[0], new Node[0]));        }    }}
private voidf14139_1final Map<String, InternalTopicConfig> topicPartitions)
{        // first construct the topics to make ready    final Map<String, InternalTopicConfig> topicsToMakeReady = new HashMap<>();    for (final InternalTopicConfig topic : topicPartitions.values()) {        final Optional<Integer> numPartitions = topic.numberOfPartitions();        if (!numPartitions.isPresent()) {            throw new StreamsException(String.format("%sTopic [%s] number of partitions not defined", logPrefix, topic.name()));        }        topic.setNumberOfPartitions(numPartitions.get());        topicsToMakeReady.put(topic.name(), topic);    }    if (!topicsToMakeReady.isEmpty()) {        internalTopicManager.makeReady(topicsToMakeReady);    }    }
private Stringf14148_1final KafkaException e)
{    String stacktrace = null;    try (final StringWriter stringWriter = new StringWriter();        final PrintWriter printWriter = new PrintWriter(stringWriter)) {        e.printStackTrace(printWriter);        stacktrace = stringWriter.toString();    } catch (final IOException ioe) {            }    return stacktrace;}
public void kafkatest_f14149_0(final ProcessorNode node, final long timestamp, final PunctuationType type, final Punctuator punctuator)
{    if (processorContext.currentNode() != null) {        throw new IllegalStateException(String.format("%sCurrent node is not null", logPrefix));    }    updateProcessorContext(new StampedRecord(DUMMY_RECORD, timestamp), node);    if (log.isTraceEnabled()) {        log.trace("Punctuating processor {} with timestamp {} and punctuation type {}", node.name(), timestamp, type);    }    try {        node.punctuate(timestamp, punctuator);    } catch (final ProducerFencedException fatal) {        throw new TaskMigratedException(this, fatal);    } catch (final KafkaException e) {        throw new StreamsException(String.format("%sException caught while punctuating processor '%s'", logPrefix, node.name()), e);    } finally {        processorContext.setCurrentNode(null);    }}
 void kafkatest_f14158_0(final boolean clean, final boolean isZombie)
{    try {        // should we call this only on clean suspend?        closeTopology();    } catch (final RuntimeException fatal) {        if (clean) {            throw fatal;        }    }    if (clean) {        TaskMigratedException taskMigratedException = null;        try {            commit(false);        } finally {            if (eosEnabled) {                stateMgr.checkpoint(activeTaskCheckpointableOffsets());                try {                    recordCollector.close();                } catch (final ProducerFencedException e) {                    taskMigratedException = new TaskMigratedException(this, e);                } finally {                    producer = null;                }            }        }        if (taskMigratedException != null) {            throw taskMigratedException;        }    } else {        maybeAbortTransactionAndCloseRecordCollector(isZombie);    }}
private voidf14159_1final boolean isZombie)
{    if (eosEnabled && !isZombie) {        try {            if (transactionInFlight) {                producer.abortTransaction();            }            transactionInFlight = false;        } catch (final ProducerFencedException ignore) {        /* TODO                 * this should actually never happen atm as we guard the call to #abortTransaction                 * -> the reason for the guard is a "bug" in the Producer -- it throws IllegalStateException                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got                 * fixed and fall-back to this catch-and-swallow code                 */        // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens        }    }    if (eosEnabled) {        try {            recordCollector.close();        } catch (final Throwable e) {                    } finally {            producer = null;        }    }}
public boolean kafkatest_f14168_0()
{    final long systemTime = time.milliseconds();    final boolean punctuated = systemTimePunctuationQueue.mayPunctuate(systemTime, PunctuationType.WALL_CLOCK_TIME, this);    if (punctuated) {        commitNeeded = true;    }    return punctuated;}
 void kafkatest_f14169_0()
{    commitRequested = true;}
 Statef14178_1final State newState)
{    final State oldState;    synchronized (stateLock) {        oldState = state;        if (state == State.PENDING_SHUTDOWN && newState != State.DEAD) {                        // refused but we do not throw exception here            return null;        } else if (state == State.DEAD) {                        // will be refused but we do not throw exception here            return null;        } else if (state == State.PARTITIONS_REVOKED && newState == State.PARTITIONS_REVOKED) {                        // refused but we do not throw exception here            return null;        } else if (!state.isValidTransition(newState)) {                        throw new StreamsException(logPrefix + "Unexpected state transition from " + oldState + " to " + newState);        } else {                    }        state = newState;        if (newState == State.RUNNING) {            updateThreadMetadata(taskManager.activeTasks(), taskManager.standbyTasks());        } else {            updateThreadMetadata(Collections.emptyMap(), Collections.emptyMap());        }    }    if (stateListener != null) {        stateListener.onChange(this, state, oldState);    }    return oldState;}
public boolean kafkatest_f14179_0()
{    // we do not need to grab stateLock since it is a single read    return state == State.RUNNING;}
public voidf14189_1)
{    if (threadProducer != null) {        try {            threadProducer.close();        } catch (final Throwable e) {                    }    }}
 StandbyTask kafkatest_f14190_0(final Consumer<byte[], byte[]> consumer, final TaskId taskId, final Set<TopicPartition> partitions)
{    createTaskSensor.record();    final ProcessorTopology topology = builder.build(taskId.topicGroupId);    if (!topology.stateStores().isEmpty() && !topology.storeToChangelogTopic().isEmpty()) {        return new StandbyTask(taskId, partitions, topology, consumer, storeChangelogReader, config, streamsMetrics, stateDirectory);    } else {        log.trace("Skipped standby task {} with assigned partitions {} " + "since it does not have any state stores to materialize", taskId, partitions);        return null;    }}
private voidf14199_1)
{    consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);    while (isRunning()) {        try {            runOnce();            if (assignmentErrorCode.get() == AssignorError.VERSION_PROBING.code()) {                                assignmentErrorCode.set(AssignorError.NONE.code());                enforceRebalance();            }        } catch (final TaskMigratedException ignoreAndRejoinGroup) {                        enforceRebalance();        }    }}
private void kafkatest_f14200_0()
{    consumer.unsubscribe();    consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);}
private long kafkatest_f14209_0()
{    final long previous = now;    now = time.milliseconds();    return Math.max(now - previous, 0);}
public voidf14210_1)
{        final State oldState = setState(State.PENDING_SHUTDOWN);    if (oldState == State.CREATED) {        // The thread may not have been started. Take responsibility for shutting down        completeShutdown(true);    }}
public Map<MetricName, Metric> kafkatest_f14219_0()
{    final LinkedHashMap<MetricName, Metric> result = new LinkedHashMap<>();    if (producer != null) {        final Map<MetricName, ? extends Metric> producerMetrics = producer.metrics();        if (producerMetrics != null) {            result.putAll(producerMetrics);        }    } else {        // all the active tasks and add their metrics to the output metrics map.        for (final StreamTask task : taskManager.activeTasks().values()) {            final Map<MetricName, ? extends Metric> taskProducerMetrics = task.getProducer().metrics();            result.putAll(taskProducerMetrics);        }    }    return result;}
public Map<MetricName, Metric> kafkatest_f14220_0()
{    final Map<MetricName, ? extends Metric> consumerMetrics = consumer.metrics();    final Map<MetricName, ? extends Metric> restoreConsumerMetrics = restoreConsumer.metrics();    final LinkedHashMap<MetricName, Metric> result = new LinkedHashMap<>();    result.putAll(consumerMetrics);    result.putAll(restoreConsumerMetrics);    return result;}
private voidf14229_1final Collection<TopicPartition> assignment)
{    if (assignedActiveTasks == null || assignedActiveTasks.isEmpty()) {        return;    }    final Map<TaskId, Set<TopicPartition>> newTasks = new HashMap<>();    // collect newly assigned tasks and reopen re-assigned tasks        for (final Map.Entry<TaskId, Set<TopicPartition>> entry : assignedActiveTasks.entrySet()) {        final TaskId taskId = entry.getKey();        final Set<TopicPartition> partitions = entry.getValue();        if (assignment.containsAll(partitions)) {            try {                if (!active.maybeResumeSuspendedTask(taskId, partitions)) {                    newTasks.put(taskId, partitions);                }            } catch (final StreamsException e) {                                throw e;            }        } else {                    }    }    if (newTasks.isEmpty()) {        return;    }    // CANNOT FIND RETRY AND BACKOFF LOGIC    // create all newly assigned tasks (guard against race condition with other thread via backoff and retry)    // -> other thread will call removeSuspendedTasks(); eventually        for (final StreamTask task : taskCreator.createTasks(consumer, newTasks)) {        active.addNewTask(task);    }}
private voidf14230_1)
{    if (assignedStandbyTasks == null || assignedStandbyTasks.isEmpty()) {        return;    }        final Map<TaskId, Set<TopicPartition>> newStandbyTasks = new HashMap<>();    // collect newly assigned standby tasks and reopen re-assigned standby tasks    for (final Map.Entry<TaskId, Set<TopicPartition>> entry : assignedStandbyTasks.entrySet()) {        final TaskId taskId = entry.getKey();        final Set<TopicPartition> partitions = entry.getValue();        if (!standby.maybeResumeSuspendedTask(taskId, partitions)) {            newStandbyTasks.put(taskId, partitions);        }    }    if (newStandbyTasks.isEmpty()) {        return;    }    // create all newly assigned standby tasks (guard against race condition with other thread via backoff and retry)    // -> other thread will call removeSuspendedStandbyTasks(); eventually    log.trace("New standby tasks to be created: {}", newStandbyTasks);    for (final StandbyTask task : standbyTaskCreator.createTasks(consumer, newStandbyTasks)) {        standby.addNewTask(task);    }}
 Admin kafkatest_f14239_0()
{    return adminClient;}
 Set<TaskId> kafkatest_f14240_0()
{    return active.previousTaskIds();}
 boolean kafkatest_f14249_0()
{    return standby.hasRunningTasks();}
private void kafkatest_f14250_0()
{    final Collection<StandbyTask> running = standby.running();    final Map<TopicPartition, Long> checkpointedOffsets = new HashMap<>();    for (final StandbyTask standbyTask : running) {        checkpointedOffsets.putAll(standbyTask.checkpointedOffsets());    }    restoreConsumer.assign(checkpointedOffsets.keySet());    for (final Map.Entry<TopicPartition, Long> entry : checkpointedOffsets.entrySet()) {        final TopicPartition partition = entry.getKey();        final long offset = entry.getValue();        if (offset >= 0) {            restoreConsumer.seek(partition, offset);        } else {            restoreConsumer.seekToBeginning(singleton(partition));        }    }}
 int kafkatest_f14259_0()
{    return active.maybeCommitPerUserRequested();}
 voidf14260_1)
{    // newer offsets anyways.    if (deleteRecordsResult == null || deleteRecordsResult.all().isDone()) {        if (deleteRecordsResult != null && deleteRecordsResult.all().isCompletedExceptionally()) {                    }        final Map<TopicPartition, RecordsToDelete> recordsToDelete = new HashMap<>();        for (final Map.Entry<TopicPartition, Long> entry : active.recordsToDelete().entrySet()) {            recordsToDelete.put(entry.getKey(), RecordsToDelete.beforeOffset(entry.getValue()));        }        if (!recordsToDelete.isEmpty()) {            deleteRecordsResult = adminClient.deleteRecords(recordsToDelete);            log.trace("Sent delete-records request: {}", recordsToDelete);        }    }}
public Map<String, String> kafkatest_f14269_0(final Map<String, String> defaultProperties, final long additionalRetentionMs)
{    // internal topic config overridden rule: library overrides < global config overrides < per-topic config overrides    final Map<String, String> topicConfig = new HashMap<>(UNWINDOWED_STORE_CHANGELOG_TOPIC_DEFAULT_OVERRIDES);    topicConfig.putAll(defaultProperties);    topicConfig.putAll(topicConfigs);    return topicConfig;}
public boolean kafkatest_f14270_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final UnwindowedChangelogTopicConfig that = (UnwindowedChangelogTopicConfig) o;    return Objects.equals(name, that.name) && Objects.equals(topicConfigs, that.topicConfigs);}
public String kafkatest_f14279_0()
{    return topicGroupId + "_" + partition;}
public static TaskId kafkatest_f14280_0(final String taskIdStr)
{    final int index = taskIdStr.indexOf('_');    if (index <= 0 || index + 1 >= taskIdStr.length()) {        throw new TaskIdFormatException(taskIdStr);    }    try {        final int topicGroupId = Integer.parseInt(taskIdStr.substring(0, index));        final int partition = Integer.parseInt(taskIdStr.substring(index + 1));        return new TaskId(topicGroupId, partition);    } catch (final Exception e) {        throw new TaskIdFormatException(taskIdStr);    }}
public Set<TopicPartition> kafkatest_f14289_0()
{    return topicPartitions;}
public boolean kafkatest_f14290_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final TaskMetadata that = (TaskMetadata) o;    return Objects.equals(taskId, that.taskId) && Objects.equals(topicPartitions, that.topicPartitions);}
public Set<String> kafkatest_f14299_0()
{    return producerClientIds;}
public String kafkatest_f14300_0()
{    return adminClientId;}
public int kafkatest_f14309_0()
{    throw new UnsupportedOperationException("To is unsafe for use in Hash collections");}
public long kafkatest_f14310_0(final ConsumerRecord<Object, Object> record, final long recordTimestamp, final long partitionTime) throws StreamsException
{    if (partitionTime < 0) {        throw new StreamsException("Could not infer new timestamp for input record " + record + " because partition time is unknown.");    }    return partitionTime;}
public KeyValue<K, V> kafkatest_f14319_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    final Bytes nextCacheKey = cacheIterator.hasNext() ? cacheIterator.peekNextKey() : null;    final KS nextStoreKey = storeIterator.hasNext() ? storeIterator.peekNextKey() : null;    if (nextCacheKey == null) {        return nextStoreValue(nextStoreKey);    }    if (nextStoreKey == null) {        return nextCacheValue(nextCacheKey);    }    final int comparison = compare(nextCacheKey, nextStoreKey);    if (comparison > 0) {        return nextStoreValue(nextStoreKey);    } else if (comparison < 0) {        return nextCacheValue(nextCacheKey);    } else {        // skip the same keyed element        storeIterator.next();        return nextCacheValue(nextCacheKey);    }}
private KeyValue<K, V> kafkatest_f14320_0(final KS nextStoreKey)
{    final KeyValue<KS, VS> next = storeIterator.next();    if (!next.key.equals(nextStoreKey)) {        throw new IllegalStateException("Next record key is not the peeked key value; this should not happen");    }    return deserializeStorePair(next);}
public voidf14329_1final Bytes key, final byte[] value)
{    final long timestamp = keySchema.segmentTimestamp(key);    observedStreamTime = Math.max(observedStreamTime, timestamp);    final long segmentId = segments.segmentId(timestamp);    final S segment = segments.getOrCreateSegmentIfLive(segmentId, context, observedStreamTime);    if (segment == null) {        expiredRecordSensor.record();            } else {        segment.put(key, value);    }}
public byte[] kafkatest_f14330_0(final Bytes key)
{    final S segment = segments.getSegmentForTimestamp(keySchema.segmentTimestamp(key));    if (segment == null) {        return null;    }    return segment.get(key);}
 Map<S, WriteBatch> kafkatest_f14339_0(final Collection<KeyValue<byte[], byte[]>> records)
{    // advance stream time to the max timestamp in the batch    for (final KeyValue<byte[], byte[]> record : records) {        final long timestamp = keySchema.segmentTimestamp(Bytes.wrap(record.key));        observedStreamTime = Math.max(observedStreamTime, timestamp);    }    final Map<S, WriteBatch> writeBatchMap = new HashMap<>();    for (final KeyValue<byte[], byte[]> record : records) {        final long timestamp = keySchema.segmentTimestamp(Bytes.wrap(record.key));        final long segmentId = segments.segmentId(timestamp);        final S segment = segments.getOrCreateSegmentIfLive(segmentId, context, observedStreamTime);        if (segment != null) {            // will only close the database and open it again with bulk loading enabled.            if (!bulkLoadSegments.contains(segment)) {                segment.toggleDbForBulkLoading(true);                // If the store does not exist yet, the getOrCreateSegmentIfLive will call openDB that                // makes the open flag for the newly created store.                // if the store does exist already, then toggleDbForBulkLoading will make sure that                // the store is already open here.                bulkLoadSegments = new HashSet<>(segments.allSegments());            }            try {                final WriteBatch batch = writeBatchMap.computeIfAbsent(segment, s -> new WriteBatch());                segment.addToBatch(record, batch);            } catch (final RocksDBException e) {                throw new ProcessorStateException("Error restoring batch to store " + this.name, e);            }        }    }    return writeBatchMap;}
private void kafkatest_f14340_0(final boolean prepareForBulkload)
{    for (final S segment : segments.allSegments()) {        segment.toggleDbForBulkLoading(prepareForBulkload);    }}
public List<S> kafkatest_f14349_0(final long timeFrom, final long timeTo)
{    final List<S> result = new ArrayList<>();    final NavigableMap<Long, S> segmentsInRange = segments.subMap(segmentId(timeFrom), true, segmentId(timeTo), true);    for (final S segment : segmentsInRange.values()) {        if (segment.isOpen()) {            result.add(segment);        }    }    return result;}
public List<S> kafkatest_f14350_0()
{    final List<S> result = new ArrayList<>();    for (final S segment : segments.values()) {        if (segment.isOpen()) {            result.add(segment);        }    }    return result;}
public StoreBuilder<T> kafkatest_f14359_0()
{    enableLogging = false;    logConfig.clear();    return this;}
public Map<String, String> kafkatest_f14360_0()
{    return logConfig;}
 byte[] kafkatest_f14369_0()
{    return priorValue;}
 byte[] kafkatest_f14370_0()
{    return oldValue;}
public int kafkatest_f14379_0()
{    int result = Objects.hash(recordContext);    result = 31 * result + Arrays.hashCode(priorValue);    result = 31 * result + Arrays.hashCode(oldValue);    result = 31 * result + Arrays.hashCode(newValue);    return result;}
public String kafkatest_f14380_0()
{    return "BufferValue{" + "priorValue=" + Arrays.toString(priorValue) + ", oldValue=" + Arrays.toString(oldValue) + ", newValue=" + Arrays.toString(newValue) + ", recordContext=" + recordContext + '}';}
public byte[] kafkatest_f14389_0(final Bytes key)
{    Objects.requireNonNull(key, "key cannot be null");    validateStoreOpen();    lock.writeLock().lock();    try {        return deleteInternal(key);    } finally {        lock.writeLock().unlock();    }}
private byte[] kafkatest_f14390_0(final Bytes key)
{    final byte[] v = getInternal(key);    putInternal(key, null);    return v;}
private void kafkatest_f14399_0(final InternalProcessorContext context)
{    this.context = context;    cacheName = context.taskId() + "-" + name();    cache = context.getCache();    cache.addDirtyEntryFlushListener(cacheName, entries -> {        for (final ThreadCache.DirtyEntry entry : entries) {            putAndMaybeForward(entry, context);        }    });}
private void kafkatest_f14400_0(final ThreadCache.DirtyEntry entry, final InternalProcessorContext context)
{    final Bytes binaryKey = cacheFunction.key(entry.key());    final Windowed<Bytes> bytesKey = SessionKeySchema.from(binaryKey);    if (flushListener != null) {        final byte[] newValueBytes = entry.newValue();        final byte[] oldValueBytes = newValueBytes == null || sendOldValues ? wrapped().fetchSession(bytesKey.key(), bytesKey.window().start(), bytesKey.window().end()) : null;        // we can skip flushing to downstream as well as writing to underlying store        if (newValueBytes != null || oldValueBytes != null) {            // we need to get the old values if needed, and then put to store, and then flush            wrapped().put(bytesKey, entry.newValue());            final ProcessorRecordContext current = context.recordContext();            context.setRecordContext(entry.entry().context());            try {                flushListener.apply(binaryKey.get(), newValueBytes, sendOldValues ? oldValueBytes : null, entry.entry().context().timestamp());            } finally {                context.setRecordContext(current);            }        }    } else {        wrapped().put(bytesKey, entry.newValue());    }}
public void kafkatest_f14409_0()
{    cache.flush(cacheName);    super.flush();}
public void kafkatest_f14410_0()
{    flush();    cache.close(cacheName);    super.close();}
private void kafkatest_f14419_0(final long lowerRangeEndTime, final long upperRangeEndTime)
{    if (cacheFunction.segmentId(lowerRangeEndTime) != cacheFunction.segmentId(upperRangeEndTime)) {        throw new IllegalStateException("Error iterating over segments: segment interval has changed");    }    if (keyFrom == keyTo) {        cacheKeyFrom = cacheFunction.cacheKey(segmentLowerRangeFixedSize(keyFrom, lowerRangeEndTime));        cacheKeyTo = cacheFunction.cacheKey(segmentUpperRangeFixedSize(keyTo, upperRangeEndTime));    } else {        cacheKeyFrom = cacheFunction.cacheKey(keySchema.lowerRange(keyFrom, lowerRangeEndTime), currentSegmentId);        cacheKeyTo = cacheFunction.cacheKey(keySchema.upperRange(keyTo, latestSessionStartTime), currentSegmentId);    }}
private Bytes kafkatest_f14420_0(final Bytes key, final long segmentBeginTime)
{    final Windowed<Bytes> sessionKey = new Windowed<>(key, new SessionWindow(0, Math.max(0, segmentBeginTime)));    return SessionKeySchema.toBinary(sessionKey);}
public synchronized WindowStoreIterator<byte[]> kafkatest_f14429_0(final Bytes key, final long timeFrom, final long timeTo)
{    // since this function may not access the underlying inner store, we need to validate    // if store is open outside as well.    validateStoreOpen();    final WindowStoreIterator<byte[]> underlyingIterator = wrapped().fetch(key, timeFrom, timeTo);    if (cache == null) {        return underlyingIterator;    }    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> cacheIterator = wrapped().persistent() ? new CacheIteratorWrapper(key, timeFrom, timeTo) : cache.range(name, cacheFunction.cacheKey(keySchema.lowerRangeFixedSize(key, timeFrom)), cacheFunction.cacheKey(keySchema.upperRangeFixedSize(key, timeTo)));    final HasNextCondition hasNextCondition = keySchema.hasNextCondition(key, key, timeFrom, timeTo);    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> filteredCacheIterator = new FilteredCacheIterator(cacheIterator, hasNextCondition, cacheFunction);    return new MergedSortedCacheWindowStoreIterator(filteredCacheIterator, underlyingIterator);}
public KeyValueIterator<Windowed<Bytes>, byte[]>f14430_1final Bytes from, final Bytes to, final long timeFrom, final long timeTo)
{    if (from.compareTo(to) > 0) {                return KeyValueIterators.emptyIterator();    }    // since this function may not access the underlying inner store, we need to validate    // if store is open outside as well.    validateStoreOpen();    final KeyValueIterator<Windowed<Bytes>, byte[]> underlyingIterator = wrapped().fetch(from, to, timeFrom, timeTo);    if (cache == null) {        return underlyingIterator;    }    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> cacheIterator = wrapped().persistent() ? new CacheIteratorWrapper(from, to, timeFrom, timeTo) : cache.range(name, cacheFunction.cacheKey(keySchema.lowerRange(from, timeFrom)), cacheFunction.cacheKey(keySchema.upperRange(to, timeTo)));    final HasNextCondition hasNextCondition = keySchema.hasNextCondition(from, to, timeFrom, timeTo);    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> filteredCacheIterator = new FilteredCacheIterator(cacheIterator, hasNextCondition, cacheFunction);    return new MergedSortedCacheWindowStoreKeyValueIterator(filteredCacheIterator, underlyingIterator, bytesSerdes, windowSize, cacheFunction);}
public void kafkatest_f14439_0()
{    current.close();}
private long kafkatest_f14440_0()
{    return currentSegmentId * segmentInterval;}
public byte[] kafkatest_f14449_0(final Bytes key, final byte[] value)
{    final byte[] previous = wrapped().putIfAbsent(key, value);    if (previous == null) {        // then it was absent        log(key, value);    }    return previous;}
public void kafkatest_f14450_0(final List<KeyValue<Bytes, byte[]>> entries)
{    wrapped().putAll(entries);    for (final KeyValue<Bytes, byte[]> entry : entries) {        log(entry.key, entry.value);    }}
public void kafkatest_f14459_0(final Windowed<Bytes> sessionKey)
{    wrapped().remove(sessionKey);    changeLogger.logChange(SessionKeySchema.toBinary(sessionKey), null);}
public void kafkatest_f14460_0(final Windowed<Bytes> sessionKey, final byte[] aggregate)
{    wrapped().put(sessionKey, aggregate);    changeLogger.logChange(SessionKeySchema.toBinary(sessionKey), aggregate);}
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14469_0(final Bytes keyFrom, final Bytes keyTo, final long from, final long to)
{    return wrapped().fetch(keyFrom, keyTo, from, to);}
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14470_0()
{    return wrapped().all();}
public KeyValue<K, V> kafkatest_f14479_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return current.next();}
public V kafkatest_f14480_0(final K key)
{    Objects.requireNonNull(key);    final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);    for (final ReadOnlyKeyValueStore<K, V> store : stores) {        try {            final V result = store.get(key);            if (result != null) {                return result;            }        } catch (final InvalidStateStoreException e) {            throw new InvalidStateStoreException("State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.");        }    }    return null;}
public WindowStoreIterator<V> kafkatest_f14489_0(final K key, final long timeFrom, final long timeTo)
{    Objects.requireNonNull(key, "key can't be null");    final List<ReadOnlyWindowStore<K, V>> stores = provider.stores(storeName, windowStoreType);    for (final ReadOnlyWindowStore<K, V> windowStore : stores) {        try {            final WindowStoreIterator<V> result = windowStore.fetch(key, timeFrom, timeTo);            if (!result.hasNext()) {                result.close();            } else {                return result;            }        } catch (final InvalidStateStoreException e) {            throw new InvalidStateStoreException("State store is not available anymore and may have been migrated to another instance; " + "please re-discover its location from the state metadata.");        }    }    return KeyValueIterators.emptyWindowStoreIterator();}
public WindowStoreIterator<V> kafkatest_f14490_0(final K key, final Instant from, final Instant to) throws IllegalArgumentException
{    return fetch(key, ApiUtils.validateMillisecondInstant(from, prepareMillisCheckFailMsgPrefix(from, "from")), ApiUtils.validateMillisecondInstant(to, prepareMillisCheckFailMsgPrefix(to, "to")));}
 ByteBuffer kafkatest_f14499_0(final int endPadding)
{    final byte[] serializedContext = recordContext.serialize();    final int sizeOfContext = serializedContext.length;    final int sizeOfValueLength = Integer.BYTES;    final int sizeOfValue = value == null ? 0 : value.length;    final ByteBuffer buffer = ByteBuffer.allocate(sizeOfContext + sizeOfValueLength + sizeOfValue + endPadding);    buffer.put(serializedContext);    if (value == null) {        buffer.putInt(-1);    } else {        buffer.putInt(value.length);        buffer.put(value);    }    return buffer;}
 static ContextualRecord kafkatest_f14500_0(final ByteBuffer buffer)
{    final ProcessorRecordContext context = ProcessorRecordContext.deserialize(buffer);    final int valueLength = buffer.getInt();    if (valueLength == -1) {        return new ContextualRecord(null, context);    } else {        final byte[] value = new byte[valueLength];        buffer.get(value);        return new ContextualRecord(value, context);    }}
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f14509_0()
{    return cachedPair(cacheIterator.peekNext());}
public void kafkatest_f14510_0()
{    cacheIterator.close();}
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f14519_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return cacheIterator.peekNext();}
public List<T> kafkatest_f14520_0(final String storeName, final QueryableStoreType<T> queryableStoreType)
{    final StateStore store = globalStateStores.get(storeName);    if (store == null || !queryableStoreType.accepts(store)) {        return Collections.emptyList();    }    if (!store.isOpen()) {        throw new InvalidStateStoreException("the state store, " + storeName + ", is not open.");    }    if (store instanceof TimestampedKeyValueStore && queryableStoreType instanceof QueryableStoreTypes.KeyValueStoreType) {        return (List<T>) Collections.singletonList(new ReadOnlyKeyValueStoreFacade((TimestampedKeyValueStore<Object, Object>) store));    } else if (store instanceof TimestampedWindowStore && queryableStoreType instanceof QueryableStoreTypes.WindowStoreType) {        return (List<T>) Collections.singletonList(new ReadOnlyWindowStoreFacade((TimestampedWindowStore<Object, Object>) store));    }    return (List<T>) Collections.singletonList(store);}
public synchronized byte[] kafkatest_f14529_0(final Bytes key)
{    final byte[] oldValue = map.remove(key);    size -= oldValue == null ? 0 : 1;    return oldValue;}
public synchronized KeyValueIterator<Bytes, byte[]>f14530_1final Bytes from, final Bytes to)
{    if (from.compareTo(to) > 0) {                return KeyValueIterators.emptyIterator();    }    return new DelegatingPeekingKeyValueIterator<>(name, new InMemoryKeyValueIterator(map.subMap(from, true, to, true).keySet()));}
public String kafkatest_f14539_0()
{    return name;}
public SessionStore<Bytes, byte[]> kafkatest_f14540_0()
{    return new InMemorySessionStore(name, retentionPeriod, metricsScope());}
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14549_0(final Bytes key, final long earliestSessionEndTime, final long latestSessionStartTime)
{    Objects.requireNonNull(key, "key cannot be null");    removeExpiredSegments();    return registerNewIterator(key, key, latestSessionStartTime, endTimeMap.tailMap(earliestSessionEndTime, true).entrySet().iterator());}
public KeyValueIterator<Windowed<Bytes>, byte[]>f14550_1final Bytes keyFrom, final Bytes keyTo, final long earliestSessionEndTime, final long latestSessionStartTime)
{    Objects.requireNonNull(keyFrom, "from key cannot be null");    Objects.requireNonNull(keyTo, "to key cannot be null");    removeExpiredSegments();    if (keyFrom.compareTo(keyTo) > 0) {                return KeyValueIterators.emptyIterator();    }    return registerNewIterator(keyFrom, keyTo, latestSessionStartTime, endTimeMap.tailMap(earliestSessionEndTime, true).entrySet().iterator());}
public boolean kafkatest_f14559_0()
{    if (next != null) {        return true;    } else if (recordIterator == null) {        return false;    } else {        next = getNext();        return next != null;    }}
public Windowed<Bytes> kafkatest_f14560_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return next.key;}
public StoreBuilder<InMemoryTimeOrderedKeyValueBuffer<K, V>> kafkatest_f14569_0()
{    return this;}
public StoreBuilder<InMemoryTimeOrderedKeyValueBuffer<K, V>> kafkatest_f14570_0(final Map<String, String> config)
{    throw new UnsupportedOperationException();}
public void kafkatest_f14579_0(final ProcessorContext context, final StateStore root)
{    final InternalProcessorContext internalProcessorContext = (InternalProcessorContext) context;    bufferSizeSensor = Sensors.createBufferSizeSensor(this, internalProcessorContext);    bufferCountSensor = Sensors.createBufferCountSensor(this, internalProcessorContext);    context.register(root, (RecordBatchingStateRestoreCallback) this::restoreBatch);    if (loggingEnabled) {        collector = ((RecordCollector.Supplier) context).recordCollector();        changelogTopic = ProcessorStateManager.storeChangelogTopic(context.applicationId(), storeName);    }    updateBufferMetrics();    open = true;    partition = context.taskId().partition;}
public boolean kafkatest_f14580_0()
{    return open;}
public void kafkatest_f14589_0(final long time, final K key, final Change<V> value, final ProcessorRecordContext recordContext)
{    requireNonNull(value, "value cannot be null");    requireNonNull(recordContext, "recordContext cannot be null");    final Bytes serializedKey = Bytes.wrap(keySerde.serializer().serialize(changelogTopic, key));    final Change<byte[]> serialChange = valueSerde.serializeParts(changelogTopic, value);    final BufferValue buffered = getBuffered(serializedKey);    final byte[] serializedPriorValue;    if (buffered == null) {        final V priorValue = value.oldValue;        serializedPriorValue = (priorValue == null) ? null : valueSerde.innerSerde().serializer().serialize(changelogTopic, priorValue);    } else {        serializedPriorValue = buffered.priorValue();    }    cleanPut(time, serializedKey, new BufferValue(serializedPriorValue, serialChange.oldValue, serialChange.newValue, recordContext));    dirtyKeys.add(serializedKey);    updateBufferMetrics();}
private BufferValue kafkatest_f14590_0(final Bytes key)
{    final BufferKey bufferKey = index.get(key);    return bufferKey == null ? null : sortedMap.get(bufferKey);}
public WindowStore<Bytes, byte[]> kafkatest_f14599_0()
{    return new InMemoryWindowStore(name, retentionPeriod, windowSize, retainDuplicates, metricsScope());}
public String kafkatest_f14600_0()
{    return "in-memory-window";}
public voidf14609_1final Bytes key, final byte[] value, final long windowStartTimestamp)
{    removeExpiredSegments();    maybeUpdateSeqnumForDups();    observedStreamTime = Math.max(observedStreamTime, windowStartTimestamp);    final Bytes keyBytes = retainDuplicates ? wrapForDups(key, seqnum) : key;    if (windowStartTimestamp <= observedStreamTime - retentionPeriod) {        expiredRecordSensor.record();            } else {        if (value != null) {            segmentMap.computeIfAbsent(windowStartTimestamp, t -> new ConcurrentSkipListMap<>());            segmentMap.get(windowStartTimestamp).put(keyBytes, value);        } else {            segmentMap.computeIfPresent(windowStartTimestamp, (t, kvMap) -> {                kvMap.remove(keyBytes);                if (kvMap.isEmpty()) {                    segmentMap.remove(windowStartTimestamp);                }                return kvMap;            });        }    }}
public byte[] kafkatest_f14610_0(final Bytes key, final long windowStartTimestamp)
{    Objects.requireNonNull(key, "key cannot be null");    removeExpiredSegments();    if (windowStartTimestamp <= observedStreamTime - retentionPeriod) {        return null;    }    final ConcurrentNavigableMap<Bytes, byte[]> kvMap = segmentMap.get(windowStartTimestamp);    if (kvMap == null) {        return null;    } else {        return kvMap.get(key);    }}
private void kafkatest_f14619_0()
{    long minLiveTime = Math.max(0L, observedStreamTime - retentionPeriod + 1);    for (final InMemoryWindowStoreIteratorWrapper it : openIterators) {        minLiveTime = Math.min(minLiveTime, it.minTime());    }    segmentMap.headMap(minLiveTime, false).clear();}
private void kafkatest_f14620_0()
{    if (retainDuplicates) {        seqnum = (seqnum + 1) & 0x7FFFFFFF;    }}
 Long kafkatest_f14629_0()
{    return currentTime;}
public Long kafkatest_f14630_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return super.currentTime;}
public void kafkatest_f14639_0()
{    innerIterator.close();}
public K kafkatest_f14641_0()
{    throw new NoSuchElementException();}
public boolean kafkatest_f14650_0(final Object obj)
{    if (obj == null || getClass() != obj.getClass()) {        return false;    }    final KeyValueSegment segment = (KeyValueSegment) obj;    return id == segment.id;}
public int kafkatest_f14651_0()
{    return Objects.hash(id);}
public byte[] kafkatest_f14660_0(final Bytes key)
{    return convertToTimestampedFormat(store.delete(key));}
public String kafkatest_f14661_0()
{    return store.name();}
public long kafkatest_f14670_0()
{    return store.approximateNumEntries();}
public void kafkatest_f14671_0()
{    innerIterator.close();}
public boolean kafkatest_f14680_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final LRUCacheEntry that = (LRUCacheEntry) o;    return sizeBytes == that.sizeBytes && isDirty() == that.isDirty() && Objects.equals(record, that.record);}
public int kafkatest_f14681_0()
{    return Objects.hash(record, sizeBytes, isDirty());}
 void kafkatest_f14690_0(final EldestEntryRemovalListener listener)
{    this.listener = listener;}
public String kafkatest_f14691_0()
{    return this.name;}
public KeyValueIterator<Bytes, byte[]> kafkatest_f14700_0(final Bytes from, final Bytes to)
{    throw new UnsupportedOperationException("MemoryLRUCache does not support range() function.");}
public KeyValueIterator<Bytes, byte[]> kafkatest_f14701_0()
{    throw new UnsupportedOperationException("MemoryLRUCache does not support all() function.");}
public KeyValue<Bytes, byte[]> kafkatest_f14710_0()
{    lastKey = keys.next();    return new KeyValue<>(lastKey, entries.get(lastKey));}
public void kafkatest_f14711_0()
{// do nothing}
 byte[] kafkatest_f14720_0(final LRUCacheEntry cacheEntry)
{    return cacheEntry.value();}
public Windowed<Bytes> kafkatest_f14721_0(final Windowed<Bytes> key)
{    return key;}
 Windowed<Bytes> kafkatest_f14730_0(final Bytes cacheKey)
{    final byte[] binaryKey = cacheFunction.key(cacheKey).get();    return WindowKeySchema.fromStoreKey(binaryKey, windowSize, serdes.keyDeserializer(), serdes.topic());}
 byte[] kafkatest_f14731_0(final LRUCacheEntry cacheEntry)
{    return cacheEntry.value();}
public V kafkatest_f14740_0(final K key)
{    try {        if (deleteTime.shouldRecord()) {            return measureLatency(() -> outerValue(wrapped().delete(keyBytes(key))), deleteTime);        } else {            return outerValue(wrapped().delete(keyBytes(key)));        }    } catch (final ProcessorStateException e) {        final String message = String.format(e.getMessage(), key);        throw new ProcessorStateException(message, e);    }}
public KeyValueIterator<K, V> kafkatest_f14741_0(final K from, final K to)
{    return new MeteredKeyValueIterator(wrapped().range(Bytes.wrap(serdes.rawKey(from)), Bytes.wrap(serdes.rawKey(to))), rangeTime);}
public boolean kafkatest_f14750_0()
{    return iter.hasNext();}
public KeyValue<K, V> kafkatest_f14751_0()
{    final KeyValue<Bytes, byte[]> keyValue = iter.next();    return KeyValue.pair(serdes.keyFrom(keyValue.key.get()), outerValue(keyValue.value));}
public KeyValueIterator<Windowed<K>, V> kafkatest_f14760_0(final K from, final K to)
{    Objects.requireNonNull(from, "from cannot be null");    Objects.requireNonNull(to, "to cannot be null");    return new MeteredWindowedKeyValueIterator<>(wrapped().fetch(keyBytes(from), keyBytes(to)), fetchTime, metrics, serdes, time);}
public KeyValueIterator<Windowed<K>, V> kafkatest_f14761_0(final K key, final long earliestSessionEndTime, final long latestSessionStartTime)
{    Objects.requireNonNull(key, "key cannot be null");    final Bytes bytesKey = keyBytes(key);    return new MeteredWindowedKeyValueIterator<>(wrapped().findSessions(bytesKey, earliestSessionEndTime, latestSessionStartTime), fetchTime, metrics, serdes, time);}
private Windowed<K> kafkatest_f14770_0(final Windowed<Bytes> bytesKey)
{    final K key = serdes.keyFrom(bytesKey.key().get());    return new Windowed<>(key, bytesKey.window());}
public void kafkatest_f14771_0()
{    try {        iter.close();    } finally {        metrics.recordLatency(sensor, startNs, time.nanoseconds());    }}
public KeyValueIterator<Windowed<K>, V> kafkatest_f14780_0(final K from, final K to, final long timeFrom, final long timeTo)
{    return new MeteredWindowedKeyValueIterator<>(wrapped().fetch(keyBytes(from), keyBytes(to), timeFrom, timeTo), fetchTime, metrics, serdes, time);}
public KeyValueIterator<Windowed<K>, V> kafkatest_f14781_0(final long timeFrom, final long timeTo)
{    return new MeteredWindowedKeyValueIterator<>(wrapped().fetchAll(timeFrom, timeTo), fetchTime, metrics, serdes, time);}
public String kafkatest_f14790_0()
{    return taskName;}
public String kafkatest_f14791_0()
{    return metricsScope;}
public static Sensor kafkatest_f14800_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, MEMTABLE_FLUSH_TIME_MIN);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), MEMTABLE_FLUSH_TIME_MIN, MEMTABLE_FLUSH_TIME_MIN_DESCRIPTION);    return sensor;}
public static Sensor kafkatest_f14801_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, MEMTABLE_FLUSH_TIME_MAX);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), MEMTABLE_FLUSH_TIME_MAX, MEMTABLE_FLUSH_TIME_MAX_DESCRIPTION);    return sensor;}
public static Sensor kafkatest_f14810_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, COMPACTION_TIME_MAX);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), COMPACTION_TIME_MAX, COMPACTION_TIME_MAX_DESCRIPTION);    return sensor;}
public static Sensor kafkatest_f14811_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, NUMBER_OF_OPEN_FILES);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), NUMBER_OF_OPEN_FILES, NUMBER_OF_OPEN_FILES_DESCRIPTION);    return sensor;}
public static Sensor kafkatest_f14820_0(final StateStore store, final InternalProcessorContext context)
{    return getBufferSizeOrCountSensor(store, context, "size");}
public static Sensor kafkatest_f14821_0(final StateStore store, final InternalProcessorContext context)
{    return getBufferSizeOrCountSensor(store, context, "count");}
 synchronized void kafkatest_f14830_0()
{    flush(null);}
private void kafkatest_f14831_0(final LRUNode evicted)
{    numFlushes++;    if (log.isTraceEnabled()) {        log.trace("Named cache {} stats on flush: #hits={}, #misses={}, #overwrites={}, #flushes={}", name, hits(), misses(), overwrites(), flushes());    }    if (listener == null) {        throw new IllegalArgumentException("No listener for namespace " + name + " registered with cache");    }    if (dirtyKeys.isEmpty()) {        return;    }    final List<ThreadCache.DirtyEntry> entries = new ArrayList<>();    final List<Bytes> deleted = new ArrayList<>();    // flushed entries and remove from dirtyKeys.    if (evicted != null) {        entries.add(new ThreadCache.DirtyEntry(evicted.key, evicted.entry.value(), evicted.entry));        dirtyKeys.remove(evicted.key);    }    for (final Bytes key : dirtyKeys) {        final LRUNode node = getInternal(key);        if (node == null) {            throw new IllegalStateException("Key = " + key + " found in dirty key set, but entry is null");        }        entries.add(new ThreadCache.DirtyEntry(key, node.entry.value(), node.entry));        node.entry.markClean();        if (node.entry.value() == null) {            deleted.add(node.key);        }    }    // clear dirtyKeys before the listener is applied as it may be re-entrant.    dirtyKeys.clear();    listener.apply(entries);    for (final Bytes key : deleted) {        delete(key);    }}
 synchronized void kafkatest_f14840_0(final List<KeyValue<byte[], LRUCacheEntry>> entries)
{    for (final KeyValue<byte[], LRUCacheEntry> entry : entries) {        put(Bytes.wrap(entry.key), entry.value);    }}
 synchronized LRUCacheEntry kafkatest_f14841_0(final Bytes key)
{    final LRUNode node = cache.remove(key);    if (node == null) {        return null;    }    remove(node);    dirtyKeys.remove(key);    currentSizeBytes -= node.size();    return node.entry();}
 synchronized LRUNode kafkatest_f14850_0()
{    return tail;}
 synchronized void kafkatest_f14851_0()
{    head = tail = null;    listener = null;    currentSizeBytes = 0;    dirtyKeys.clear();    cache.clear();    namedCacheMetrics.removeAllSensors();}
private void kafkatest_f14860_0(final BufferedWriter writer, final int number) throws IOException
{    writer.write(Integer.toString(number));    writer.newLine();}
private void kafkatest_f14861_0(final BufferedWriter writer, final TopicPartition part, final long offset) throws IOException
{    writer.write(part.topic());    writer.write(' ');    writer.write(Integer.toString(part.partition()));    writer.write(' ');    writer.write(Long.toString(offset));    writer.newLine();}
public KeyValueIterator<K, V> kafkatest_f14870_0(final K from, final K to)
{    return new KeyValueIteratorFacade<>(inner.range(from, to));}
public KeyValueIterator<K, V> kafkatest_f14871_0()
{    return new KeyValueIteratorFacade<>(inner.all());}
public KeyValueIterator<Windowed<K>, V> kafkatest_f14880_0()
{    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<V>> innerIterator = inner.all();    return new KeyValueIteratorFacade<>(innerIterator);}
public void kafkatest_f14881_0()
{    innerIterator.close();}
public Options kafkatest_f14890_0(final Env env)
{    dbOptions.setEnv(env);    return this;}
public Env kafkatest_f14891_0()
{    return dbOptions.getEnv();}
public Options kafkatest_f14900_0(final long memtableMemoryBudget)
{    columnFamilyOptions.optimizeUniversalStyleCompaction(memtableMemoryBudget);    return this;}
public Options kafkatest_f14901_0(final BuiltinComparator builtinComparator)
{    columnFamilyOptions.setComparator(builtinComparator);    return this;}
public Options kafkatest_f14910_0(final boolean errorIfExists)
{    dbOptions.setErrorIfExists(errorIfExists);    return this;}
public boolean kafkatest_f14911_0()
{    final boolean columnFamilyParanoidFileChecks = columnFamilyOptions.paranoidFileChecks();    final boolean dbOptionsParanoidChecks = dbOptions.paranoidChecks();    if (columnFamilyParanoidFileChecks != dbOptionsParanoidChecks) {        throw new IllegalStateException("Config for paranoid checks for RockDB and ColumnFamilies should be the same.");    }    return dbOptionsParanoidChecks;}
public Options kafkatest_f14920_0(final boolean useFsync)
{    dbOptions.setUseFsync(useFsync);    return this;}
public Options kafkatest_f14921_0(final Collection<DbPath> dbPaths)
{    dbOptions.setDbPaths(dbPaths);    return this;}
public Options kafkatest_f14930_0(final Statistics statistics)
{    dbOptions.setStatistics(statistics);    return this;}
public Statistics kafkatest_f14931_0()
{    return dbOptions.statistics();}
public Options kafkatest_f14940_0(final int maxBackgroundJobs)
{    dbOptions.setMaxBackgroundJobs(maxBackgroundJobs);    return this;}
public long kafkatest_f14941_0()
{    return dbOptions.maxLogFileSize();}
public Options kafkatest_f14950_0(final long maxManifestFileSize)
{    dbOptions.setMaxManifestFileSize(maxManifestFileSize);    return this;}
public Options kafkatest_f14951_0(final long maxTableFilesSize)
{    columnFamilyOptions.setMaxTableFilesSizeFIFO(maxTableFilesSize);    return this;}
public Options kafkatest_f14960_0(final long size)
{    dbOptions.setManifestPreallocationSize(size);    return this;}
public Options kafkatest_f14961_0(final boolean useDirectReads)
{    dbOptions.setUseDirectReads(useDirectReads);    return this;}
public Options kafkatest_f14970_0(final boolean allowMmapWrites)
{    dbOptions.setAllowMmapWrites(allowMmapWrites);    return this;}
public boolean kafkatest_f14971_0()
{    return dbOptions.isFdCloseOnExec();}
public AccessHint kafkatest_f14980_0()
{    return dbOptions.accessHintOnCompactionStart();}
public Options kafkatest_f14981_0(final boolean newTableReaderForCompactionInputs)
{    dbOptions.setNewTableReaderForCompactionInputs(newTableReaderForCompactionInputs);    return this;}
public Options kafkatest_f14990_0(final boolean useAdaptiveMutex)
{    dbOptions.setUseAdaptiveMutex(useAdaptiveMutex);    return this;}
public long kafkatest_f14991_0()
{    return dbOptions.bytesPerSync();}
public boolean kafkatest_f15000_0()
{    return dbOptions.allowConcurrentMemtableWrite();}
public Options kafkatest_f15001_0(final boolean enableWriteThreadAdaptiveYield)
{    dbOptions.setEnableWriteThreadAdaptiveYield(enableWriteThreadAdaptiveYield);    return this;}
public WALRecoveryMode kafkatest_f15010_0()
{    return dbOptions.walRecoveryMode();}
public Options kafkatest_f15011_0(final boolean allow2pc)
{    dbOptions.setAllow2pc(allow2pc);    return this;}
public boolean kafkatest_f15020_0()
{    return dbOptions.avoidFlushDuringRecovery();}
public Options kafkatest_f15021_0(final boolean avoidFlushDuringShutdown)
{    dbOptions.setAvoidFlushDuringShutdown(avoidFlushDuringShutdown);    return this;}
public String kafkatest_f15030_0()
{    return columnFamilyOptions.memTableFactoryName();}
public TableFormatConfig kafkatest_f15031_0()
{    return columnFamilyOptions.tableFormatConfig();}
public Options kafkatest_f15040_0(final CompressionType bottommostCompressionType)
{    columnFamilyOptions.setBottommostCompressionType(bottommostCompressionType);    return this;}
public CompressionType kafkatest_f15041_0()
{    return columnFamilyOptions.bottommostCompressionType();}
public int kafkatest_f15050_0()
{    return columnFamilyOptions.levelZeroSlowdownWritesTrigger();}
public Options kafkatest_f15051_0(final int numFiles)
{    columnFamilyOptions.setLevelZeroSlowdownWritesTrigger(numFiles);    return this;}
public Options kafkatest_f15060_0(final boolean enableLevelCompactionDynamicLevelBytes)
{    columnFamilyOptions.setLevelCompactionDynamicLevelBytes(enableLevelCompactionDynamicLevelBytes);    return this;}
public boolean kafkatest_f15061_0()
{    return columnFamilyOptions.levelCompactionDynamicLevelBytes();}
public long kafkatest_f15070_0()
{    return columnFamilyOptions.maxSequentialSkipInIterations();}
public Options kafkatest_f15071_0(final long maxSequentialSkipInIterations)
{    columnFamilyOptions.setMaxSequentialSkipInIterations(maxSequentialSkipInIterations);    return this;}
public long kafkatest_f15080_0()
{    return columnFamilyOptions.maxSuccessiveMerges();}
public Options kafkatest_f15081_0(final long maxSuccessiveMerges)
{    columnFamilyOptions.setMaxSuccessiveMerges(maxSuccessiveMerges);    return this;}
public Options kafkatest_f15090_0(final long hardPendingCompactionBytesLimit)
{    columnFamilyOptions.setHardPendingCompactionBytesLimit(hardPendingCompactionBytesLimit);    return this;}
public long kafkatest_f15091_0()
{    return columnFamilyOptions.hardPendingCompactionBytesLimit();}
public Options kafkatest_f15100_0(final boolean paranoidFileChecks)
{    columnFamilyOptions.setParanoidFileChecks(paranoidFileChecks);    return this;}
public boolean kafkatest_f15101_0()
{    return columnFamilyOptions.paranoidFileChecks();}
public Options kafkatest_f15110_0(final CompactionOptionsFIFO compactionOptionsFIFO)
{    columnFamilyOptions.setCompactionOptionsFIFO(compactionOptionsFIFO);    return this;}
public CompactionOptionsFIFO kafkatest_f15111_0()
{    return columnFamilyOptions.compactionOptionsFIFO();}
public KeyValue<Bytes, byte[]> kafkatest_f15120_0()
{    if (!iter.isValid()) {        return allDone();    } else {        next = getKeyValue();        iter.next();        return next;    }}
private KeyValue<Bytes, byte[]> kafkatest_f15121_0()
{    return new KeyValue<>(new Bytes(iter.key()), iter.value());}
public String kafkatest_f15130_0()
{    return "rocksdb-session";}
public long kafkatest_f15131_0()
{    // Selected somewhat arbitrarily. Profiling may reveal a different value is preferable.    return Math.max(retentionPeriod / 2, 60_000L);}
 void kafkatest_f15140_0(final ProcessorContext context)
{    // initialize the default rocksdb options    final DBOptions dbOptions = new DBOptions();    final ColumnFamilyOptions columnFamilyOptions = new ColumnFamilyOptions();    userSpecifiedOptions = new RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter(dbOptions, columnFamilyOptions);    final BlockBasedTableConfig tableConfig = new BlockBasedTableConfig();    cache = new LRUCache(BLOCK_CACHE_SIZE);    tableConfig.setBlockCache(cache);    tableConfig.setBlockSize(BLOCK_SIZE);    filter = new BloomFilter();    tableConfig.setFilter(filter);    userSpecifiedOptions.optimizeFiltersForHits();    userSpecifiedOptions.setTableFormatConfig(tableConfig);    userSpecifiedOptions.setWriteBufferSize(WRITE_BUFFER_SIZE);    userSpecifiedOptions.setCompressionType(COMPRESSION_TYPE);    userSpecifiedOptions.setCompactionStyle(COMPACTION_STYLE);    userSpecifiedOptions.setMaxWriteBufferNumber(MAX_WRITE_BUFFERS);    userSpecifiedOptions.setCreateIfMissing(true);    userSpecifiedOptions.setErrorIfExists(false);    userSpecifiedOptions.setInfoLogLevel(InfoLogLevel.ERROR_LEVEL);    // this is the recommended way to increase parallelism in RocksDb    // note that the current implementation of setIncreaseParallelism affects the number    // of compaction threads but not flush threads (the latter remains one). Also    // the parallelism value needs to be at least two because of the code in    // https://github.com/facebook/rocksdb/blob/62ad0a9b19f0be4cefa70b6b32876e764b7f3c11/util/options.cc#L580    // subtracts one from the value passed to determine the number of compaction threads    // (this could be a bug in the RocksDB code and their devs have been contacted).    userSpecifiedOptions.setIncreaseParallelism(Math.max(Runtime.getRuntime().availableProcessors(), 2));    wOptions = new WriteOptions();    wOptions.setDisableWAL(true);    fOptions = new FlushOptions();    fOptions.setWaitForFlush(true);    final Map<String, Object> configs = context.appConfigs();    final Class<RocksDBConfigSetter> configSetterClass = (Class<RocksDBConfigSetter>) configs.get(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG);    if (configSetterClass != null) {        configSetter = Utils.newInstance(configSetterClass);        configSetter.setConfig(name, userSpecifiedOptions, configs);    }    if (prepareForBulkload) {        userSpecifiedOptions.prepareForBulkLoad();    }    dbDir = new File(new File(context.stateDir(), parentDir), name);    try {        Files.createDirectories(dbDir.getParentFile().toPath());        Files.createDirectories(dbDir.getAbsoluteFile().toPath());    } catch (final IOException fatal) {        throw new ProcessorStateException(fatal);    }    setUpMetrics(context, configs);    openRocksDB(dbOptions, columnFamilyOptions);    open = true;}
private void kafkatest_f15141_0(final ProcessorContext context, final Map<String, Object> configs)
{    if (userSpecifiedOptions.statistics() == null && RecordingLevel.forName((String) configs.get(METRICS_RECORDING_LEVEL_CONFIG)) == RecordingLevel.DEBUG) {        // metrics recorder will clean up statistics object        final Statistics statistics = new Statistics();        userSpecifiedOptions.setStatistics(statistics);        metricsRecorder.addStatistics(name, statistics, (StreamsMetricsImpl) context.metrics(), context.taskId());        removeStatisticsFromMetricsRecorder = true;    }}
public synchronized byte[] kafkatest_f15150_0(final Bytes key, final byte[] value)
{    Objects.requireNonNull(key, "key cannot be null");    final byte[] originalValue = get(key);    if (originalValue == null) {        put(key, value);    }    return originalValue;}
public void kafkatest_f15151_0(final List<KeyValue<Bytes, byte[]>> entries)
{    try (final WriteBatch batch = new WriteBatch()) {        dbAccessor.prepareBatch(entries, batch);        write(batch);    } catch (final RocksDBException e) {        throw new ProcessorStateException("Error while batch writing to store " + name, e);    }}
public void kafkatest_f15160_0(final KeyValue<byte[], byte[]> record, final WriteBatch batch) throws RocksDBException
{    dbAccessor.addToBatch(record.key, record.value, batch);}
public void kafkatest_f15161_0(final WriteBatch batch) throws RocksDBException
{    db.write(wOptions, batch);}
public KeyValueIterator<Bytes, byte[]> kafkatest_f15170_0()
{    final RocksIterator innerIterWithTimestamp = db.newIterator(columnFamily);    innerIterWithTimestamp.seekToFirst();    return new RocksDbIterator(name, innerIterWithTimestamp, openIterators);}
public long kafkatest_f15171_0() throws RocksDBException
{    return db.getLongProperty(columnFamily, "rocksdb.estimate-num-keys");}
public Options kafkatest_f15180_0()
{    return userSpecifiedOptions;}
 void kafkatest_f15181_0(final DBOptions dbOptions, final ColumnFamilyOptions columnFamilyOptions)
{    final List<ColumnFamilyDescriptor> columnFamilyDescriptors = asList(new ColumnFamilyDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY, columnFamilyOptions), new ColumnFamilyDescriptor("keyValueWithTimestamp".getBytes(StandardCharsets.UTF_8), columnFamilyOptions));    final List<ColumnFamilyHandle> columnFamilies = new ArrayList<>(columnFamilyDescriptors.size());    try {        db = RocksDB.open(dbOptions, dbDir.getAbsolutePath(), columnFamilyDescriptors, columnFamilies);        setDbAccessor(columnFamilies.get(0), columnFamilies.get(1));    } catch (final RocksDBException e) {        if ("Column family not found: : keyValueWithTimestamp".equals(e.getMessage())) {            try {                db = RocksDB.open(dbOptions, dbDir.getAbsolutePath(), columnFamilyDescriptors.subList(0, 1), columnFamilies);                columnFamilies.add(db.createColumnFamily(columnFamilyDescriptors.get(1)));            } catch (final RocksDBException fatal) {                throw new ProcessorStateException("Error opening store " + name + " at location " + dbDir.toString(), fatal);            }            setDbAccessor(columnFamilies.get(0), columnFamilies.get(1));        } else {            throw new ProcessorStateException("Error opening store " + name + " at location " + dbDir.toString(), e);        }    }}
public void kafkatest_f15190_0() throws RocksDBException
{    db.flush(fOptions, oldColumnFamily);    db.flush(fOptions, newColumnFamily);}
public void kafkatest_f15191_0(final Collection<KeyValue<byte[], byte[]>> records, final WriteBatch batch) throws RocksDBException
{    for (final KeyValue<byte[], byte[]> record : records) {        addToBatch(record.key, record.value, batch);    }}
public KeyValue<Bytes, byte[]> kafkatest_f15200_0()
{    final KeyValue<Bytes, byte[]> next = super.makeNext();    if (next == null) {        return allDone();    } else {        if (comparator.compare(next.key.get(), upperBoundKey) <= 0) {            return next;        } else {            return allDone();        }    }}
public String kafkatest_f15201_0()
{    return name;}
public void kafkatest_f15210_0(final Bytes key, final byte[] value)
{    put(key, value, context.timestamp());}
public void kafkatest_f15211_0(final Bytes key, final byte[] value, final long windowStartTimestamp)
{    maybeUpdateSeqnumForDups();    wrapped().put(WindowKeySchema.toStoreKeyBinary(key, windowStartTimestamp, seqnum), value);}
 Bytes kafkatest_f15220_0(final Bytes key, final long segmentId)
{    final byte[] keyBytes = key.get();    final ByteBuffer buf = ByteBuffer.allocate(SEGMENT_ID_BYTES + keyBytes.length);    buf.putLong(segmentId).put(keyBytes);    return Bytes.wrap(buf.array());}
 static byte[] kafkatest_f15221_0(final Bytes cacheKey)
{    final byte[] binaryKey = new byte[cacheKey.get().length - SEGMENT_ID_BYTES];    System.arraycopy(cacheKey.get(), SEGMENT_ID_BYTES, binaryKey, 0, binaryKey.length);    return binaryKey;}
public KeyValue<Bytes, byte[]> kafkatest_f15230_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return currentIterator.next();}
public Bytes kafkatest_f15231_0(final Bytes key, final long to)
{    final Windowed<Bytes> sessionKey = new Windowed<>(key, new SessionWindow(to, Long.MAX_VALUE));    return SessionKeySchema.toBinary(sessionKey);}
 static long kafkatest_f15240_0(final byte[] binaryKey)
{    return ByteBuffer.wrap(binaryKey).getLong(binaryKey.length - 2 * TIMESTAMP_SIZE);}
 static long kafkatest_f15241_0(final byte[] binaryKey)
{    return ByteBuffer.wrap(binaryKey).getLong(binaryKey.length - TIMESTAMP_SIZE);}
private SessionStore<Bytes, byte[]> kafkatest_f15250_0(final SessionStore<Bytes, byte[]> inner)
{    if (!enableCaching) {        return inner;    }    return new CachingSessionStore(inner, storeSupplier.segmentIntervalMs());}
private SessionStore<Bytes, byte[]> kafkatest_f15251_0(final SessionStore<Bytes, byte[]> inner)
{    if (!enableLogging) {        return inner;    }    return new ChangeLoggingSessionBytesStore(inner);}
public static String kafkatest_f15260_0(final String taskIDString, final String underlyingStoreName)
{    return taskIDString + "-" + underlyingStoreName;}
public static String kafkatest_f15261_0(final String cacheName)
{    final String[] tokens = cacheName.split("-", 2);    return tokens[0];}
public MemoryLRUCacheBytesIterator kafkatest_f15270_0(final String namespace, final Bytes from, final Bytes to)
{    final NamedCache cache = getCache(namespace);    if (cache == null) {        return new MemoryLRUCacheBytesIterator(Collections.<Bytes>emptyIterator(), new NamedCache(namespace, this.metrics));    }    return new MemoryLRUCacheBytesIterator(cache.keyRange(from, to), cache);}
public MemoryLRUCacheBytesIterator kafkatest_f15271_0(final String namespace)
{    final NamedCache cache = getCache(namespace);    if (cache == null) {        return new MemoryLRUCacheBytesIterator(Collections.<Bytes>emptyIterator(), new NamedCache(namespace, this.metrics));    }    return new MemoryLRUCacheBytesIterator(cache.allKeys(), cache);}
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f15280_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return nextEntry;}
public boolean kafkatest_f15281_0()
{    if (nextEntry != null) {        return true;    }    while (keys.hasNext() && nextEntry == null) {        internalNext();    }    return nextEntry != null;}
public ProcessorRecordContext kafkatest_f15290_0()
{    return recordContext;}
public String kafkatest_f15291_0()
{    return "Eviction{key=" + key + ", value=" + value + ", recordContext=" + recordContext + '}';}
public void kafkatest_f15300_0(final List<KeyValue<Bytes, byte[]>> entries)
{    wrapped.putAll(entries);}
public byte[] kafkatest_f15301_0(final Bytes key)
{    return wrapped.delete(key);}
public boolean kafkatest_f15310_0()
{    return false;}
public void kafkatest_f15311_0() throws IOException
{    Utils.delete(dbDir);}
private WindowStore<Bytes, byte[]> kafkatest_f15320_0(final WindowStore<Bytes, byte[]> inner)
{    if (!enableCaching) {        return inner;    }    return new CachingWindowStore(inner, storeSupplier.windowSize(), storeSupplier.segmentIntervalMs());}
private WindowStore<Bytes, byte[]> kafkatest_f15321_0(final WindowStore<Bytes, byte[]> inner)
{    if (!enableLogging) {        return inner;    }    return new ChangeLoggingTimestampedWindowBytesStore(inner, storeSupplier.retainDuplicates());}
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f15330_0()
{    return wrapped.all();}
public void kafkatest_f15331_0()
{    wrapped.flush();}
private static byte[] kafkatest_f15340_0(final byte[] rawValueAndTimestamp)
{    return ByteBuffer.allocate(8).put(rawValueAndTimestamp, 0, 8).array();}
 static long kafkatest_f15341_0(final byte[] rawValueAndTimestamp)
{    return LONG_DESERIALIZER.deserialize(null, rawTimestamp(rawValueAndTimestamp));}
public Bytes kafkatest_f15350_0(final Bytes key, final long to)
{    final byte[] maxSuffix = ByteBuffer.allocate(SUFFIX_SIZE).putLong(to).putInt(Integer.MAX_VALUE).array();    return OrderedBytes.upperRange(key, maxSuffix);}
public Bytes kafkatest_f15351_0(final Bytes key, final long from)
{    return OrderedBytes.lowerRange(key, MIN_SUFFIX);}
private static Window kafkatest_f15360_0(final byte[] binaryKey, final long windowSize)
{    final ByteBuffer buffer = ByteBuffer.wrap(binaryKey);    final long start = buffer.getLong(binaryKey.length - TIMESTAMP_SIZE);    return timeWindowForSize(start, windowSize);}
public static Bytes kafkatest_f15361_0(final Bytes key, final long timestamp, final int seqnum)
{    final byte[] serializedKey = key.get();    return toStoreKeyBinary(serializedKey, timestamp, seqnum);}
public static Windowed<K> kafkatest_f15370_0(final byte[] binaryKey, final long windowSize, final Deserializer<K> deserializer, final String topic)
{    final K key = deserializer.deserialize(topic, extractStoreKeyBytes(binaryKey));    final Window window = extractStoreWindow(binaryKey, windowSize);    return new Windowed<>(key, window);}
public static Windowed<K> kafkatest_f15371_0(final Windowed<Bytes> windowedKey, final Deserializer<K> deserializer, final String topic)
{    final K key = deserializer.deserialize(topic, windowedKey.key().get());    return new Windowed<>(key, windowedKey.window());}
public Long kafkatest_f15380_0()
{    return WindowKeySchema.extractStoreTimestamp(bytesIterator.peekNextKey().get());}
public boolean kafkatest_f15381_0()
{    return bytesIterator.hasNext();}
public byte[] kafkatest_f15390_0(final Bytes key, final long time)
{    return convertToTimestampedFormat(store.fetch(key, time));}
public WindowStoreIterator<byte[]> kafkatest_f15391_0(final Bytes key, final long timeFrom, final long timeTo)
{    return new WindowToTimestampedWindowIteratorAdapter(store.fetch(key, timeFrom, timeTo));}
public void kafkatest_f15400_0()
{    store.flush();}
public void kafkatest_f15401_0()
{    store.close();}
public boolean kafkatest_f15410_0(final CacheFlushListener<K, V> listener, final boolean sendOldValues)
{    if (wrapped instanceof CachedStateStore) {        return ((CachedStateStore<K, V>) wrapped).setFlushListener(listener, sendOldValues);    }    return false;}
public String kafkatest_f15411_0()
{    return wrapped.name();}
public static QueryableStoreType<ReadOnlyKeyValueStore<K, ValueAndTimestamp<V>>> kafkatest_f15420_0()
{    return new TimestampedKeyValueStoreType<>();}
public static QueryableStoreType<ReadOnlyWindowStore<K, V>> kafkatest_f15421_0()
{    return new WindowStoreType<>();}
 voidf15430_1final String storeName, final Options options)
{    }
public static StateSerdes<K, V> kafkatest_f15431_0(final String topic, final Class<K> keyClass, final Class<V> valueClass)
{    return new StateSerdes<>(topic, Serdes.serdeFrom(keyClass), Serdes.serdeFrom(valueClass));}
public V kafkatest_f15440_0(final byte[] rawValue)
{    return valueSerde.deserializer().deserialize(topic, rawValue);}
public byte[] kafkatest_f15441_0(final K key)
{    try {        return keySerde.serializer().serialize(topic, key);    } catch (final ClassCastException e) {        final String keyClass = key == null ? "unknown because key is null" : key.getClass().getName();        throw new StreamsException(String.format("A serializer (%s) is not compatible to the actual key type " + "(key type: %s). Change the default Serdes in StreamConfig or " + "provide correct Serdes via method parameters.", keySerializer().getClass().getName(), keyClass), e);    }}
public String kafkatest_f15450_0()
{    return name;}
public KeyValueStore<Bytes, byte[]> kafkatest_f15451_0()
{    return new MemoryNavigableLRUCache(name, maxCacheSize);}
public static SessionBytesStoreSupplier kafkatest_f15460_0(final String name, final Duration retentionPeriod)
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(retentionPeriod, "retentionPeriod");    return persistentSessionStore(name, ApiUtils.validateMillisecondDuration(retentionPeriod, msgPrefix));}
public static SessionBytesStoreSupplier kafkatest_f15461_0(final String name, final Duration retentionPeriod)
{    Objects.requireNonNull(name, "name cannot be null");    final String msgPrefix = prepareMillisCheckFailMsgPrefix(retentionPeriod, "retentionPeriod");    final long retentionPeriodMs = ApiUtils.validateMillisecondDuration(retentionPeriod, msgPrefix);    if (retentionPeriodMs < 0) {        throw new IllegalArgumentException("retentionPeriod cannot be negative");    }    return new InMemorySessionBytesStoreSupplier(name, retentionPeriodMs);}
public String kafkatest_f15470_0()
{    return hostInfo.host();}
public int kafkatest_f15471_0()
{    return hostInfo.port();}
public String kafkatest_f15480_0()
{    return "<" + value + "," + timestamp + ">";}
public boolean kafkatest_f15481_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final ValueAndTimestamp<?> that = (ValueAndTimestamp<?>) o;    return timestamp == that.timestamp && Objects.equals(value, that.value);}
public synchronized KStream<K, V> kafkatest_f15490_0(final Pattern topicPattern)
{    return stream(topicPattern, Consumed.with(null, null));}
public synchronized KStream<K, V> kafkatest_f15491_0(final Pattern topicPattern, final Consumed<K, V> consumed)
{    Objects.requireNonNull(topicPattern, "topicPattern can't be null");    Objects.requireNonNull(consumed, "consumed can't be null");    return internalStreamsBuilder.stream(topicPattern, new ConsumedInternal<>(consumed));}
public synchronized StreamsBuilder kafkatest_f15500_0(final StoreBuilder builder)
{    Objects.requireNonNull(builder, "builder can't be null");    internalStreamsBuilder.addStateStore(builder);    return this;}
public synchronized StreamsBuilder kafkatest_f15501_0(final StoreBuilder storeBuilder, final String topic, final String sourceName, final Consumed consumed, final String processorName, final ProcessorSupplier stateUpdateSupplier)
{    Objects.requireNonNull(storeBuilder, "storeBuilder can't be null");    Objects.requireNonNull(consumed, "consumed can't be null");    internalStreamsBuilder.addGlobalStore(storeBuilder, sourceName, topic, new ConsumedInternal<>(consumed), processorName, stateUpdateSupplier);    return this;}
public static String kafkatest_f15510_0(final String adminClientProp)
{    return ADMIN_CLIENT_PREFIX + adminClientProp;}
public static String kafkatest_f15511_0(final String topicProp)
{    return TOPIC_PREFIX + topicProp;}
public Map<String, Object> kafkatest_f15520_0(final String clientId)
{    final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(PRODUCER_PREFIX, ProducerConfig.configNames());    checkIfUnexpectedUserSpecifiedConsumerConfig(clientProvidedProps, NON_CONFIGURABLE_PRODUCER_EOS_CONFIGS);    // generate producer configs from original properties and overridden maps    final Map<String, Object> props = new HashMap<>(eosEnabled ? PRODUCER_EOS_OVERRIDES : PRODUCER_DEFAULT_OVERRIDES);    props.putAll(getClientCustomProps());    props.putAll(clientProvidedProps);    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, originals().get(BOOTSTRAP_SERVERS_CONFIG));    // add client id with stream client id prefix    props.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);    return props;}
public Map<String, Object> kafkatest_f15521_0(final String clientId)
{    final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(ADMIN_CLIENT_PREFIX, AdminClientConfig.configNames());    final Map<String, Object> props = new HashMap<>();    props.putAll(getClientCustomProps());    props.putAll(clientProvidedProps);    // add client id with stream client id prefix    props.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);    return props;}
public static void kafkatest_f15530_0(final String[] args)
{    System.out.println(CONFIG.toHtml());}
public synchronized Topology kafkatest_f15531_0(final String name, final String... topics)
{    internalTopologyBuilder.addSource(null, name, null, null, null, topics);    return this;}
public synchronized Topology kafkatest_f15540_0(final String name, final Deserializer keyDeserializer, final Deserializer valueDeserializer, final Pattern topicPattern)
{    internalTopologyBuilder.addSource(null, name, null, keyDeserializer, valueDeserializer, topicPattern);    return this;}
public synchronized Topology kafkatest_f15541_0(final AutoOffsetReset offsetReset, final String name, final Deserializer keyDeserializer, final Deserializer valueDeserializer, final String... topics)
{    internalTopologyBuilder.addSource(offsetReset, name, null, keyDeserializer, valueDeserializer, topics);    return this;}
public synchronized Topology kafkatest_f15550_0(final String name, final TopicNameExtractor<K, V> topicExtractor, final StreamPartitioner<? super K, ? super V> partitioner, final String... parentNames)
{    internalTopologyBuilder.addSink(name, topicExtractor, null, null, partitioner, parentNames);    return this;}
public synchronized Topology kafkatest_f15551_0(final String name, final TopicNameExtractor<K, V> topicExtractor, final Serializer<K> keySerializer, final Serializer<V> valueSerializer, final String... parentNames)
{    internalTopologyBuilder.addSink(name, topicExtractor, keySerializer, valueSerializer, null, parentNames);    return this;}
public static void kafkatest_f15560_0(final T o1, final T o2)
{    // making sure we don't get an NPE in the test    if (o1 == null && o2 == null) {        throw new AssertionError("Both o1 and o2 were null.");    } else if (o1 == null) {        return;    } else if (o2 == null) {        return;    }    verifyGeneralEqualityProperties(o1, o2);    // these two objects should NOT equal each other    if (o1.equals(o2)) {        throw new AssertionError(String.format("o1[%s] was equal to o2[%s].", o1, o2));    }    if (o2.equals(o1)) {        throw new AssertionError(String.format("o2[%s] was equal to o1[%s].", o2, o1));    }    verifyHashCodeConsistency(o1, o2);    // since these objects are NOT equal, their hashcode SHOULD PROBABLY not be the same    if (o1.hashCode() == o2.hashCode()) {        throw new AssertionError(String.format("o1[%s].hash[%d] was equal to o2[%s].hash[%d], even though !o1.equals(o2). " + "This is NOT A BUG, but it is undesirable for hash collection performance.", o1, o1.hashCode(), o2, o2.hashCode()));    }}
private static void kafkatest_f15561_0(final T o1, final T o2)
{    // objects should equal themselves    if (!o1.equals(o1)) {        throw new AssertionError(String.format("o1[%s] was not equal to itself.", o1));    }    if (!o2.equals(o2)) {        throw new AssertionError(String.format("o2[%s] was not equal to itself.", o2));    }    // non-null objects should not equal null    if (o1.equals(null)) {        throw new AssertionError(String.format("o1[%s] was equal to null.", o1));    }    if (o2.equals(null)) {        throw new AssertionError(String.format("o2[%s] was equal to null.", o2));    }    // objects should not equal some random object    if (o1.equals(new Object())) {        throw new AssertionError(String.format("o1[%s] was equal to an anonymous Object.", o1));    }    if (o2.equals(new Object())) {        throw new AssertionError(String.format("o2[%s] was equal to an anonymous Object.", o2));    }}
private void kafkatest_f15570_0(final String outputTopic, final KeyValueTimestamp<Long, String> expectedFinalResult, final int expectedTotalNumRecords) throws InterruptedException
{    final List<KeyValueTimestamp<Long, String>> result = IntegrationTestUtils.waitUntilMinKeyValueWithTimestampRecordsReceived(RESULT_CONSUMER_CONFIG, outputTopic, expectedTotalNumRecords, 30 * 1000L);    assertThat(result.get(result.size() - 1), is(expectedFinalResult));}
 void kafkatest_f15571_0(final List<List<KeyValueTimestamp<Long, String>>> expectedResult) throws Exception
{    runTest(expectedResult, null);}
public boolean kafkatest_f15580_0()
{    try {        final ConsumerGroupDescription groupDescription = adminClient.describeConsumerGroups(Collections.singletonList(appID)).describedGroups().get(appID).get();        return groupDescription.members().isEmpty();    } catch (final ExecutionException | InterruptedException e) {        return false;    }}
 void kafkatest_f15581_0() throws Exception
{    prepareConfigs();    prepareEnvironment();    // busy wait until cluster (ie, ConsumerGroupCoordinator) is available    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Test consumer group " + appID + " still active even after waiting " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    cluster.deleteAndRecreateTopics(INPUT_TOPIC, OUTPUT_TOPIC, OUTPUT_TOPIC_2, OUTPUT_TOPIC_2_RERUN);    add10InputElements();}
 void kafkatest_f15590_0() throws Exception
{    appID = testId + "-from-datetime";    streamsConfig.put(StreamsConfig.APPLICATION_ID_CONFIG, appID);    // RUN    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.start();    final List<KeyValue<Long, Long>> result = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultConsumerConfig, OUTPUT_TOPIC, 10);    streams.close();    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * STREAMS_CONSUMER_TIMEOUT, "Streams Application consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * STREAMS_CONSUMER_TIMEOUT) + " ms.");    // RESET    final File resetFile = File.createTempFile("reset", ".csv");    try (final BufferedWriter writer = new BufferedWriter(new FileWriter(resetFile))) {        writer.write(INPUT_TOPIC + ",0,1");    }    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.cleanUp();    final SimpleDateFormat format = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSS");    final Calendar calendar = Calendar.getInstance();    calendar.add(Calendar.DATE, -1);    cleanGlobal(false, "--to-datetime", format.format(calendar.getTime()));    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Reset Tool consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    assertInternalTopicsGotDeleted(null);    resetFile.deleteOnExit();    // RE-RUN    streams.start();    final List<KeyValue<Long, Long>> resultRerun = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultConsumerConfig, OUTPUT_TOPIC, 10);    streams.close();    assertThat(resultRerun, equalTo(result));    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Reset Tool consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    cleanGlobal(false, null, null);}
 void kafkatest_f15591_0() throws Exception
{    appID = testId + "-from-duration";    streamsConfig.put(StreamsConfig.APPLICATION_ID_CONFIG, appID);    // RUN    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.start();    final List<KeyValue<Long, Long>> result = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultConsumerConfig, OUTPUT_TOPIC, 10);    streams.close();    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * STREAMS_CONSUMER_TIMEOUT, "Streams Application consumer group " + appID + "  did not time out after " + (TIMEOUT_MULTIPLIER * STREAMS_CONSUMER_TIMEOUT) + " ms.");    // RESET    final File resetFile = File.createTempFile("reset", ".csv");    try (final BufferedWriter writer = new BufferedWriter(new FileWriter(resetFile))) {        writer.write(INPUT_TOPIC + ",0,1");    }    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.cleanUp();    cleanGlobal(false, "--by-duration", "PT1M");    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Reset Tool consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    assertInternalTopicsGotDeleted(null);    resetFile.deleteOnExit();    // RE-RUN    streams.start();    final List<KeyValue<Long, Long>> resultRerun = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultConsumerConfig, OUTPUT_TOPIC, 10);    streams.close();    assertThat(resultRerun, equalTo(result));    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Reset Tool consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    cleanGlobal(false, null, null);}
public void kafkatest_f15600_0() throws Exception
{    runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, null, SINGLE_PARTITION_OUTPUT_TOPIC);}
public void kafkatest_f15601_0() throws Exception
{    runSimpleCopyTest(1, SINGLE_PARTITION_INPUT_TOPIC, SINGLE_PARTITION_THROUGH_TOPIC, SINGLE_PARTITION_OUTPUT_TOPIC);}
public void kafkatest_f15610_0() throws Exception
{    try (final KafkaStreams streams1 = getKafkaStreams(false, "appDir1", 1);        final KafkaStreams streams2 = getKafkaStreams(false, "appDir2", 1)) {        streams1.start();        streams2.start();        final List<KeyValue<Long, Long>> committedDataBeforeGC = prepareData(0L, 10L, 0L, 1L);        final List<KeyValue<Long, Long>> uncommittedDataBeforeGC = prepareData(10L, 15L, 0L, 1L);        final List<KeyValue<Long, Long>> dataBeforeGC = new ArrayList<>();        dataBeforeGC.addAll(committedDataBeforeGC);        dataBeforeGC.addAll(uncommittedDataBeforeGC);        final List<KeyValue<Long, Long>> dataToTriggerFirstRebalance = prepareData(15L, 20L, 0L, 1L);        final List<KeyValue<Long, Long>> dataAfterSecondRebalance = prepareData(20L, 30L, 0L, 1L);        writeInputData(committedDataBeforeGC);        TestUtils.waitForCondition(() -> commitRequested.get() == 2, MAX_WAIT_TIME_MS, "SteamsTasks did not request commit.");        writeInputData(uncommittedDataBeforeGC);        final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeGC.size(), null);        final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeGC.size(), CONSUMER_GROUP_ID);        checkResultPerKey(committedRecords, committedDataBeforeGC);        checkResultPerKey(uncommittedRecords, dataBeforeGC);        gcInjected.set(true);        writeInputData(dataToTriggerFirstRebalance);        TestUtils.waitForCondition(() -> streams1.allMetadata().size() == 1 && streams2.allMetadata().size() == 1 && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2 || streams2.allMetadata().iterator().next().topicPartitions().size() == 2), MAX_WAIT_TIME_MS, "Should have rebalanced.");        final List<KeyValue<Long, Long>> committedRecordsAfterRebalance = readResult(uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size(), CONSUMER_GROUP_ID);        final List<KeyValue<Long, Long>> expectedCommittedRecordsAfterRebalance = new ArrayList<>();        expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeGC);        expectedCommittedRecordsAfterRebalance.addAll(dataToTriggerFirstRebalance);        checkResultPerKey(committedRecordsAfterRebalance, expectedCommittedRecordsAfterRebalance);        doGC = false;        TestUtils.waitForCondition(() -> streams1.allMetadata().size() == 1 && streams2.allMetadata().size() == 1 && streams1.allMetadata().iterator().next().topicPartitions().size() == 1 && streams2.allMetadata().iterator().next().topicPartitions().size() == 1, MAX_WAIT_TIME_MS, "Should have rebalanced.");        writeInputData(dataAfterSecondRebalance);        final List<KeyValue<Long, Long>> allCommittedRecords = readResult(committedDataBeforeGC.size() + uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size() + dataAfterSecondRebalance.size(), CONSUMER_GROUP_ID + "_ALL");        final List<KeyValue<Long, Long>> allExpectedCommittedRecordsAfterRecovery = new ArrayList<>();        allExpectedCommittedRecordsAfterRecovery.addAll(committedDataBeforeGC);        allExpectedCommittedRecordsAfterRecovery.addAll(uncommittedDataBeforeGC);        allExpectedCommittedRecordsAfterRecovery.addAll(dataToTriggerFirstRebalance);        allExpectedCommittedRecordsAfterRecovery.addAll(dataAfterSecondRebalance);        checkResultPerKey(allCommittedRecords, allExpectedCommittedRecordsAfterRecovery);    }}
private List<KeyValue<Long, Long>> kafkatest_f15611_0(final long fromInclusive, final long toExclusive, final Long... keys)
{    final List<KeyValue<Long, Long>> data = new ArrayList<>();    for (final Long k : keys) {        for (long v = fromInclusive; v < toExclusive; ++v) {            data.add(new KeyValue<>(k, v));        }    }    return data;}
private void kafkatest_f15621_0(final KafkaStreams streams, final Set<KeyValue<Long, Long>> expectedStoreContent)
{    ReadOnlyKeyValueStore<Long, Long> store = null;    final long maxWaitingTime = System.currentTimeMillis() + 300000L;    while (System.currentTimeMillis() < maxWaitingTime) {        try {            store = streams.store(storeName, QueryableStoreTypes.keyValueStore());            break;        } catch (final InvalidStateStoreException okJustRetry) {            try {                Thread.sleep(5000L);            } catch (final Exception ignore) {            }        }    }    assertNotNull(store);    final KeyValueIterator<Long, Long> it = store.all();    while (it.hasNext()) {        assertTrue(expectedStoreContent.remove(it.next()));    }    assertTrue(expectedStoreContent.isEmpty());}
public static void kafkatest_f15622_0() throws InterruptedException
{    CLUSTER.createTopics(TOPIC_1_0, TOPIC_2_0, TOPIC_A_0, TOPIC_C_0, TOPIC_Y_0, TOPIC_Z_0, TOPIC_1_1, TOPIC_2_1, TOPIC_A_1, TOPIC_C_1, TOPIC_Y_1, TOPIC_Z_1, TOPIC_1_2, TOPIC_2_2, TOPIC_A_2, TOPIC_C_2, TOPIC_Y_2, TOPIC_Z_2, NOOP, DEFAULT_OUTPUT_TOPIC, OUTPUT_TOPIC_0, OUTPUT_TOPIC_1, OUTPUT_TOPIC_2);}
public void kafkatest_f15631_0() throws InterruptedException
{    final Properties props = new Properties();    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    props.put(ConsumerConfig.METADATA_MAX_AGE_CONFIG, "1000");    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "none");    final Properties localConfig = StreamsTestUtils.getStreamsConfig("testAutoOffsetWithNone", CLUSTER.bootstrapServers(), STRING_SERDE_CLASSNAME, STRING_SERDE_CLASSNAME, props);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> exceptionStream = builder.stream(NOOP);    exceptionStream.to(DEFAULT_OUTPUT_TOPIC, Produced.with(stringSerde, stringSerde));    final KafkaStreams streams = new KafkaStreams(builder.build(), localConfig);    final TestingUncaughtExceptionHandler uncaughtExceptionHandler = new TestingUncaughtExceptionHandler();    streams.setUncaughtExceptionHandler(uncaughtExceptionHandler);    streams.start();    TestUtils.waitForCondition(() -> uncaughtExceptionHandler.correctExceptionThrown, "The expected NoOffsetForPartitionException was never thrown");    streams.close();}
public void kafkatest_f15632_0(final Thread t, final Throwable e)
{    assertThat(e.getClass().getSimpleName(), is("StreamsException"));    assertThat(e.getCause().getClass().getSimpleName(), is("NoOffsetForPartitionException"));    correctExceptionThrown = true;}
private void kafkatest_f15641_0(final String topic) throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronously(topic, Arrays.asList(new KeyValue<>("a", 1L), new KeyValue<>("b", 2L), new KeyValue<>("c", 3L), new KeyValue<>("d", 4L), new KeyValue<>("e", 5L)), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, LongSerializer.class, new Properties()), mockTime);}
private void kafkatest_f15642_0() throws Exception
{    final Properties properties = new Properties();    properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "someid");    properties.put(ProducerConfig.RETRIES_CONFIG, 1);    IntegrationTestUtils.produceAbortedKeyValuesSynchronouslyWithTimestamp(globalTableTopic, Arrays.asList(new KeyValue<>(1L, "A"), new KeyValue<>(2L, "B"), new KeyValue<>(3L, "C"), new KeyValue<>(4L, "D")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), LongSerializer.class, StringSerializer.class, properties), mockTime.milliseconds());}
private void kafkatest_f15651_0() throws Exception
{    streamTopic = "stream-" + testNo;    globalTableTopic = "globalTable-" + testNo;    CLUSTER.createTopics(streamTopic);    CLUSTER.createTopic(globalTableTopic, 2, 1);}
private void kafkatest_f15652_0()
{    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();}
public void kafkatest_f15661_0(final ProcessorContext context)
{    super.init(context);    store = (KeyValueStore<String, Long>) context.getStateStore(storeName);}
public void kafkatest_f15662_0(final String key, final Long value)
{    firstRecordProcessed = true;}
public void kafkatest_f15671_0() throws Exception
{    final String appID = APP_ID + "-compact-delete";    streamsProp.put(StreamsConfig.APPLICATION_ID_CONFIG, appID);    //     // Step 1: Configure and start a simple word count topology    //     final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> textLines = builder.stream(DEFAULT_INPUT_TOPIC);    final int durationMs = 2000;    textLines.flatMapValues(value -> Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+"))).groupBy(MockMapper.selectValueMapper()).windowedBy(TimeWindows.of(ofSeconds(1L)).grace(ofMillis(0L))).count(Materialized.<String, Long, WindowStore<Bytes, byte[]>>as("CountWindows").withRetention(ofSeconds(2L)));    final KafkaStreams streams = new KafkaStreams(builder.build(), streamsProp);    streams.start();    //     // Step 2: Produce some input data to the input topic.    //     produceData(Arrays.asList("hello", "world", "world", "hello world"));    //     // Step 3: Verify the state changelog topics are compact    //     waitForCompletion(streams, 2, 30000);    streams.close();    final Properties properties = getTopicProperties(ProcessorStateManager.storeChangelogTopic(appID, "CountWindows"));    final List<String> policies = Arrays.asList(properties.getProperty(LogConfig.CleanupPolicyProp()).split(","));    assertEquals(2, policies.size());    assertTrue(policies.contains(LogConfig.Compact()));    assertTrue(policies.contains(LogConfig.Delete()));    // retention should be 1 day + the window duration    final long retention = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS) + durationMs;    assertEquals(retention, Long.parseLong(properties.getProperty(LogConfig.RetentionMsProp())));    final Properties repartitionProps = getTopicProperties(appID + "-CountWindows-repartition");    assertEquals(LogConfig.Delete(), repartitionProps.getProperty(LogConfig.CleanupPolicyProp()));    assertEquals(3, repartitionProps.size());}
public void kafkatest_f15672_0() throws InterruptedException
{    builder = new StreamsBuilder();    createTopics();    streamsConfiguration = new Properties();    final String applicationId = "kgrouped-stream-test-" + testNo.incrementAndGet();    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL_MS);    streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024L);    final KeyValueMapper<Integer, String, String> mapper = MockMapper.selectValueMapper();    stream = builder.stream(streamOneInput, Consumed.with(Serdes.Integer(), Serdes.String()));    groupedStream = stream.groupBy(mapper, Grouped.with(Serdes.String(), Serdes.String()));    reducer = (value1, value2) -> value1 + ":" + value2;}
public void kafkatest_f15681_0() throws InterruptedException
{    builder = new StreamsBuilder();    createTopics();    streamsConfiguration = new Properties();    final String applicationId = "kgrouped-stream-test-" + testNo.incrementAndGet();    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());    streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    final KeyValueMapper<Integer, String, String> mapper = MockMapper.selectValueMapper();    stream = builder.stream(streamOneInput, Consumed.with(Serdes.Integer(), Serdes.String()));    groupedStream = stream.groupBy(mapper, Grouped.with(Serdes.String(), Serdes.String()));    reducer = (value1, value2) -> value1 + ":" + value2;    initializer = () -> 0;    aggregator = (aggKey, value, aggregate) -> aggregate + value.length();}
public void kafkatest_f15682_0() throws IOException
{    if (kafkaStreams != null) {        kafkaStreams.close();    }    IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);}
public void kafkatest_f15691_0() throws Exception
{    final long timestamp = mockTime.milliseconds();    produceMessages(timestamp);    produceMessages(timestamp);    stream.groupByKey(Grouped.with(Serdes.Integer(), Serdes.String())).windowedBy(TimeWindows.of(ofMillis(500L))).count().toStream((windowedKey, value) -> windowedKey.key() + "@" + windowedKey.window().start()).to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));    startStreams();    final List<KeyValueTimestamp<String, Long>> results = receiveMessages(new StringDeserializer(), new LongDeserializer(), 10);    results.sort(KStreamAggregationIntegrationTest::compare);    final long window = timestamp / 500 * 500;    assertThat(results, is(Arrays.asList(new KeyValueTimestamp("1@" + window, 1L, timestamp), new KeyValueTimestamp("1@" + window, 2L, timestamp), new KeyValueTimestamp("2@" + window, 1L, timestamp), new KeyValueTimestamp("2@" + window, 2L, timestamp), new KeyValueTimestamp("3@" + window, 1L, timestamp), new KeyValueTimestamp("3@" + window, 2L, timestamp), new KeyValueTimestamp("4@" + window, 1L, timestamp), new KeyValueTimestamp("4@" + window, 2L, timestamp), new KeyValueTimestamp("5@" + window, 1L, timestamp), new KeyValueTimestamp("5@" + window, 2L, timestamp))));}
public void kafkatest_f15692_0() throws Exception
{    final long sessionGap = 5 * 60 * 1000L;    final List<KeyValue<String, String>> t1Messages = Arrays.asList(new KeyValue<>("bob", "start"), new KeyValue<>("penny", "start"), new KeyValue<>("jo", "pause"), new KeyValue<>("emily", "pause"));    final long t1 = mockTime.milliseconds() - TimeUnit.MILLISECONDS.convert(1, TimeUnit.HOURS);    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, t1Messages, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t1);    final long t2 = t1 + (sessionGap / 2);    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Collections.singletonList(new KeyValue<>("emily", "resume")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t2);    final long t3 = t1 + sessionGap + 1;    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Arrays.asList(new KeyValue<>("bob", "pause"), new KeyValue<>("penny", "stop")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t3);    final long t4 = t3 + (sessionGap / 2);    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Arrays.asList(// bobs session continues    new KeyValue<>("bob", "resume"), // jo's starts new session    new KeyValue<>("jo", "resume")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t4);    final long t5 = t4 - 1;    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Collections.singletonList(// jo has late arrival    new KeyValue<>("jo", "late")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t5);    final Map<Windowed<String>, KeyValue<Long, Long>> results = new HashMap<>();    final CountDownLatch latch = new CountDownLatch(13);    builder.stream(userSessionsStream, Consumed.with(Serdes.String(), Serdes.String())).groupByKey(Grouped.with(Serdes.String(), Serdes.String())).windowedBy(SessionWindows.with(ofMillis(sessionGap))).count().toStream().transform(() -> new Transformer<Windowed<String>, Long, KeyValue<Object, Object>>() {        private ProcessorContext context;        @Override        public void init(final ProcessorContext context) {            this.context = context;        }        @Override        public KeyValue<Object, Object> transform(final Windowed<String> key, final Long value) {            results.put(key, KeyValue.pair(value, context.timestamp()));            latch.countDown();            return null;        }        @Override        public void close() {        }    });    startStreams();    latch.await(30, TimeUnit.SECONDS);    assertThat(results.get(new Windowed<>("bob", new SessionWindow(t1, t1))), equalTo(KeyValue.pair(1L, t1)));    assertThat(results.get(new Windowed<>("penny", new SessionWindow(t1, t1))), equalTo(KeyValue.pair(1L, t1)));    assertThat(results.get(new Windowed<>("jo", new SessionWindow(t1, t1))), equalTo(KeyValue.pair(1L, t1)));    assertThat(results.get(new Windowed<>("jo", new SessionWindow(t5, t4))), equalTo(KeyValue.pair(2L, t4)));    assertThat(results.get(new Windowed<>("emily", new SessionWindow(t1, t2))), equalTo(KeyValue.pair(2L, t2)));    assertThat(results.get(new Windowed<>("bob", new SessionWindow(t3, t4))), equalTo(KeyValue.pair(2L, t4)));    assertThat(results.get(new Windowed<>("penny", new SessionWindow(t3, t3))), equalTo(KeyValue.pair(1L, t3)));}
private void kafkatest_f15704_0(final long timestamp) throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(streamOneInput, Arrays.asList(new KeyValue<>(1, "A"), new KeyValue<>(2, "B"), new KeyValue<>(3, "C"), new KeyValue<>(4, "D"), new KeyValue<>(5, "E")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, StringSerializer.class, new Properties()), timestamp);}
private void kafkatest_f15705_0() throws InterruptedException
{    streamOneInput = "stream-one-" + testNo;    outputTopic = "output-" + testNo;    userSessionsStream = userSessionsStream + "-" + testNo;    CLUSTER.createTopic(streamOneInput, 3, 1);    CLUSTER.createTopics(userSessionsStream, outputTopic);}
public void kafkatest_f15714_0(final ProcessorContext context)
{    state = (KeyValueStore<Integer, Integer>) context.getStateStore(stateStoreName);}
public KeyValue<Integer, Integer> kafkatest_f15715_0(final Integer key, final Integer value)
{    state.putIfAbsent(key, 0);    Integer storedValue = state.get(key);    final KeyValue<Integer, Integer> result = new KeyValue<>(key + 1, value + storedValue++);    state.put(key, storedValue);    return result;}
public Integer kafkatest_f15727_0(final Integer value)
{    state.putIfAbsent(value, 0);    Integer counter = state.get(value);    state.put(value, ++counter);    return counter;}
public void kafkatest_f15729_0()
{    stream.flatTransformValues(() -> new ValueTransformerWithKey<Integer, Integer, Iterable<Integer>>() {        private KeyValueStore<Integer, Integer> state;        @Override        public void init(final ProcessorContext context) {            state = (KeyValueStore<Integer, Integer>) context.getStateStore("myTransformState");        }        @Override        public Iterable<Integer> transform(final Integer key, final Integer value) {            final List<Integer> result = new ArrayList<>();            state.putIfAbsent(key, 0);            Integer storedValue = state.get(key);            for (int i = 0; i < 3; i++) {                result.add(value + storedValue++);            }            state.put(key, storedValue);            return result;        }        @Override        public void close() {        }    }, "myTransformState").foreach(action);    final List<KeyValue<Integer, Integer>> expected = Arrays.asList(KeyValue.pair(1, 1), KeyValue.pair(1, 2), KeyValue.pair(1, 3), KeyValue.pair(2, 2), KeyValue.pair(2, 3), KeyValue.pair(2, 4), KeyValue.pair(3, 3), KeyValue.pair(3, 4), KeyValue.pair(3, 5), KeyValue.pair(2, 4), KeyValue.pair(2, 5), KeyValue.pair(2, 6), KeyValue.pair(2, 9), KeyValue.pair(2, 10), KeyValue.pair(2, 11), KeyValue.pair(1, 6), KeyValue.pair(1, 7), KeyValue.pair(1, 8));    verifyResult(expected);}
public void kafkatest_f15740_0() throws Exception
{    try {        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        streamsOne.start();        produceKeyValues("a", "b", "c");        assertNumberValuesRead(readKeyValues, expectedInitialResultsMap, "Table did not read all values");        streamsOne.close();        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        // the state restore listener will append one record to the log        streamsOne.setGlobalStateRestoreListener(new UpdatingSourceTopicOnRestoreStartStateRestoreListener());        streamsOne.start();        produceKeyValues("f", "g", "h");        assertNumberValuesRead(readKeyValues, expectedResultsWithDataWrittenDuringRestoreMap, "Table did not get all values after restart");    } finally {        streamsOne.close(Duration.ofSeconds(5));    }}
public void kafkatest_f15741_0() throws Exception
{    try {        STREAMS_CONFIG.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        streamsOne.start();        produceKeyValues("a", "b", "c");        assertNumberValuesRead(readKeyValues, expectedInitialResultsMap, "Table did not read all values");        streamsOne.close();        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        // the state restore listener will append one record to the log        streamsOne.setGlobalStateRestoreListener(new UpdatingSourceTopicOnRestoreStartStateRestoreListener());        streamsOne.start();        produceKeyValues("f", "g", "h");        assertNumberValuesRead(readKeyValues, expectedResultsWithDataWrittenDuringRestoreMap, "Table did not get all values after restart");    } finally {        streamsOne.close(Duration.ofSeconds(5));    }}
private void kafkatest_f15752_0(final Duration segmentInterval) throws Exception
{    final MockTime mockTime = new MockTime(Math.max(segmentInterval.toMillis(), 60_000L));    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(STREAM_INPUT, Collections.singletonList(new KeyValue<>(1, "A")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, StringSerializer.class, new Properties()), mockTime.milliseconds());    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(STREAM_INPUT, Collections.singletonList(new KeyValue<>(1, "B")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, StringSerializer.class, new Properties()), mockTime.milliseconds());}
private void kafkatest_f15753_0() throws Exception
{    IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(TestUtils.consumerConfig(CLUSTER.bootstrapServers(), "consumerApp", LongDeserializer.class, LongDeserializer.class, new Properties()), STREAM_OUTPUT_1, 2);}
private void kafkatest_f15762_0(final String tag)
{    final List<Metric> listMetricStore = new ArrayList<Metric>(kafkaStreams.metrics().values()).stream().filter(m -> m.metricName().group().equals("stream-state-metrics") && m.metricName().tags().containsKey(tag)).collect(Collectors.toList());    checkMetricByName(listMetricStore, BYTES_WRITTEN_RATE, 1);    checkMetricByName(listMetricStore, BYTES_WRITTEN_TOTAL, 1);    checkMetricByName(listMetricStore, BYTES_READ_RATE, 1);    checkMetricByName(listMetricStore, BYTES_READ_TOTAL, 1);    checkMetricByName(listMetricStore, MEMTABLE_BYTES_FLUSHED_RATE, 1);    checkMetricByName(listMetricStore, MEMTABLE_BYTES_FLUSHED_TOTAL, 1);    checkMetricByName(listMetricStore, MEMTABLE_HIT_RATIO, 1);    checkMetricByName(listMetricStore, MEMTABLE_FLUSH_TIME_AVG, 1);    checkMetricByName(listMetricStore, MEMTABLE_FLUSH_TIME_MIN, 1);    checkMetricByName(listMetricStore, MEMTABLE_FLUSH_TIME_MAX, 1);    checkMetricByName(listMetricStore, WRITE_STALL_DURATION_AVG, 1);    checkMetricByName(listMetricStore, WRITE_STALL_DURATION_TOTAL, 1);    checkMetricByName(listMetricStore, BLOCK_CACHE_DATA_HIT_RATIO, 1);    checkMetricByName(listMetricStore, BLOCK_CACHE_INDEX_HIT_RATIO, 1);    checkMetricByName(listMetricStore, BLOCK_CACHE_FILTER_HIT_RATIO, 1);    checkMetricByName(listMetricStore, BYTES_READ_DURING_COMPACTION_RATE, 1);    checkMetricByName(listMetricStore, BYTES_WRITTEN_DURING_COMPACTION_RATE, 1);    checkMetricByName(listMetricStore, COMPACTION_TIME_AVG, 1);    checkMetricByName(listMetricStore, COMPACTION_TIME_MIN, 1);    checkMetricByName(listMetricStore, COMPACTION_TIME_MAX, 1);    checkMetricByName(listMetricStore, NUMBER_OF_OPEN_FILES, 1);    checkMetricByName(listMetricStore, NUMBER_OF_FILE_ERRORS, 1);}
private void kafkatest_f15763_0(final String group)
{    final List<Metric> listMetricStore = new ArrayList<Metric>(kafkaStreams.metrics().values()).stream().filter(m -> m.metricName().group().equals(group)).collect(Collectors.toList());    checkMetricByName(listMetricStore, PUT_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, PUT_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, GET_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, GET_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, DELETE_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, DELETE_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, PUT_ALL_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, PUT_ALL_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, ALL_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, ALL_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, RANGE_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, RANGE_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, FLUSH_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, FLUSH_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, RESTORE_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, RESTORE_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, PUT_RATE, 2);    checkMetricByName(listMetricStore, PUT_TOTAL, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_RATE, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_TOTAL, 2);    checkMetricByName(listMetricStore, GET_RATE, 2);    checkMetricByName(listMetricStore, DELETE_RATE, 2);    checkMetricByName(listMetricStore, DELETE_TOTAL, 2);    checkMetricByName(listMetricStore, PUT_ALL_RATE, 2);    checkMetricByName(listMetricStore, PUT_ALL_TOTAL, 2);    checkMetricByName(listMetricStore, ALL_RATE, 2);    checkMetricByName(listMetricStore, ALL_TOTAL, 2);    checkMetricByName(listMetricStore, RANGE_RATE, 2);    checkMetricByName(listMetricStore, RANGE_TOTAL, 2);    checkMetricByName(listMetricStore, FLUSH_RATE, 2);    checkMetricByName(listMetricStore, FLUSH_TOTAL, 2);    checkMetricByName(listMetricStore, RESTORE_RATE, 2);    checkMetricByName(listMetricStore, RESTORE_TOTAL, 2);}
public void kafkatest_f15772_0() throws Exception
{    final int batch1NumMessages = 100;    final int batch2NumMessages = 100;    final int key = 1;    final Semaphore semaphore = new Semaphore(0);    final StreamsBuilder builder = new StreamsBuilder();    builder.table(INPUT_TOPIC_NAME, Consumed.with(Serdes.Integer(), Serdes.Integer()), Materialized.<Integer, Integer, KeyValueStore<Bytes, byte[]>>as(TABLE_NAME).withCachingDisabled()).toStream().peek((k, v) -> semaphore.release());    final KafkaStreams kafkaStreams1 = createKafkaStreams(builder, streamsConfiguration());    final KafkaStreams kafkaStreams2 = createKafkaStreams(builder, streamsConfiguration());    final List<KafkaStreams> kafkaStreamsList = Arrays.asList(kafkaStreams1, kafkaStreams2);    final AtomicLong restoreStartOffset = new AtomicLong(-1L);    final AtomicLong restoreEndOffset = new AtomicLong(-1L);    kafkaStreamsList.forEach(kafkaStreams -> {        kafkaStreams.setGlobalStateRestoreListener(createTrackingRestoreListener(restoreStartOffset, restoreEndOffset));        kafkaStreams.start();    });    waitForKafkaStreamssToEnterRunningState(kafkaStreamsList, 60, TimeUnit.SECONDS);    produceValueRange(key, 0, batch1NumMessages);    // Assert that all messages in the first batch were processed in a timely manner    assertThat(semaphore.tryAcquire(batch1NumMessages, 60, TimeUnit.SECONDS), is(equalTo(true)));    final ReadOnlyKeyValueStore<Integer, Integer> store1 = kafkaStreams1.store(TABLE_NAME, QueryableStoreTypes.keyValueStore());    final ReadOnlyKeyValueStore<Integer, Integer> store2 = kafkaStreams2.store(TABLE_NAME, QueryableStoreTypes.keyValueStore());    final boolean kafkaStreams1WasFirstActive;    if (store1.get(key) != null) {        kafkaStreams1WasFirstActive = true;    } else {        // Assert that data from the job was sent to the store        assertThat(store2.get(key), is(notNullValue()));        kafkaStreams1WasFirstActive = false;    }    // Assert that no restore has occurred, ensures that when we check later that the restore    // notification actually came from after the rebalance.    assertThat(restoreStartOffset.get(), is(equalTo(-1L)));    // Assert that the current value in store reflects all messages being processed    assertThat(kafkaStreams1WasFirstActive ? store1.get(key) : store2.get(key), is(equalTo(batch1NumMessages - 1)));    if (kafkaStreams1WasFirstActive) {        kafkaStreams1.close();    } else {        kafkaStreams2.close();    }    final ReadOnlyKeyValueStore<Integer, Integer> newActiveStore = kafkaStreams1WasFirstActive ? store2 : store1;    retryOnExceptionWithTimeout(100, 60 * 1000, TimeUnit.MILLISECONDS, () -> {        // Assert that after failover we have recovered to the last store write        assertThat(newActiveStore.get(key), is(equalTo(batch1NumMessages - 1)));    });    final int totalNumMessages = batch1NumMessages + batch2NumMessages;    produceValueRange(key, batch1NumMessages, totalNumMessages);    // Assert that all messages in the second batch were processed in a timely manner    assertThat(semaphore.tryAcquire(batch2NumMessages, 60, TimeUnit.SECONDS), is(equalTo(true)));    // Assert that either restore was unnecessary or we restored from an offset later than 0    assertThat(restoreStartOffset.get(), is(anyOf(greaterThan(0L), equalTo(-1L))));    // Assert that either restore was unnecessary or we restored to the last offset before we closed the kafkaStreams    assertThat(restoreEndOffset.get(), is(anyOf(equalTo(batch1NumMessages - 1), equalTo(-1L))));    // Assert that the current value in store reflects all messages being processed    assertThat(newActiveStore.get(key), is(equalTo(totalNumMessages - 1)));}
private void kafkatest_f15773_0(final int key, final int start, final int endExclusive) throws Exception
{    final Properties producerProps = new Properties();    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers());    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);    IntegrationTestUtils.produceKeyValuesSynchronously(INPUT_TOPIC_NAME, IntStream.range(start, endExclusive).mapToObj(i -> KeyValue.pair(key, i)).collect(Collectors.toList()), producerProps, mockTime);}
public static void kafkatest_f15784_0() throws Exception
{    CLUSTER.createTopic(INPUT_TOPIC, 1, 1);}
public void kafkatest_f15785_0()
{    // create admin client for verification    final Properties adminConfig = new Properties();    adminConfig.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    adminClient = Admin.create(adminConfig);    final Properties streamsConfiguration = new Properties();    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, APPLICATION_ID);    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, PURGE_INTERVAL_MS);    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(APPLICATION_ID).getPath());    streamsConfiguration.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_MS_CONFIG), PURGE_INTERVAL_MS);    streamsConfiguration.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG), PURGE_SEGMENT_BYTES);    // we cannot allow batch size larger than segment size    streamsConfiguration.put(StreamsConfig.producerPrefix(ProducerConfig.BATCH_SIZE_CONFIG), PURGE_SEGMENT_BYTES / 2);    final StreamsBuilder builder = new StreamsBuilder();    builder.stream(INPUT_TOPIC).groupBy(MockMapper.selectKeyKeyValueMapper()).count();    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration, time);}
public void kafkatest_f15794_0()
{    if (!closed) {        myStream.close();        closed = true;    }}
public boolean kafkatest_f15795_0()
{    return closed;}
public void kafkatest_f15804_0() throws Exception
{    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Long().getClass());    final StreamsBuilder builder = new StreamsBuilder();    final String[] keys = { "hello", "goodbye", "welcome", "go", "kafka" };    final Set<KeyValue<String, Long>> batch1 = new HashSet<>(Arrays.asList(new KeyValue<>(keys[0], 1L), new KeyValue<>(keys[1], 1L), new KeyValue<>(keys[2], 3L), new KeyValue<>(keys[3], 5L), new KeyValue<>(keys[4], 2L)));    final Set<KeyValue<String, Long>> expectedBatch1 = new HashSet<>(Collections.singleton(new KeyValue<>(keys[4], 2L)));    IntegrationTestUtils.produceKeyValuesSynchronously(streamOne, batch1, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, LongSerializer.class, new Properties()), mockTime);    final Predicate<String, Long> filterPredicate = (key, value) -> key.contains("kafka");    final KTable<String, Long> t1 = builder.table(streamOne);    final KTable<String, Long> t2 = t1.filter(filterPredicate, Materialized.as("queryFilter"));    t1.filterNot(filterPredicate, Materialized.as("queryFilterNot"));    t2.toStream().to(outputTopic);    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();    waitUntilAtLeastNumRecordProcessed(outputTopic, 1);    final ReadOnlyKeyValueStore<String, Long> myFilterStore = kafkaStreams.store("queryFilter", QueryableStoreTypes.keyValueStore());    final ReadOnlyKeyValueStore<String, Long> myFilterNotStore = kafkaStreams.store("queryFilterNot", QueryableStoreTypes.keyValueStore());    for (final KeyValue<String, Long> expectedEntry : expectedBatch1) {        TestUtils.waitForCondition(() -> expectedEntry.value.equals(myFilterStore.get(expectedEntry.key)), "Cannot get expected result");    }    for (final KeyValue<String, Long> batchEntry : batch1) {        if (!expectedBatch1.contains(batchEntry)) {            TestUtils.waitForCondition(() -> myFilterStore.get(batchEntry.key) == null, "Cannot get null result");        }    }    for (final KeyValue<String, Long> expectedEntry : expectedBatch1) {        TestUtils.waitForCondition(() -> myFilterNotStore.get(expectedEntry.key) == null, "Cannot get null result");    }    for (final KeyValue<String, Long> batchEntry : batch1) {        if (!expectedBatch1.contains(batchEntry)) {            TestUtils.waitForCondition(() -> batchEntry.value.equals(myFilterNotStore.get(batchEntry.key)), "Cannot get expected result");        }    }}
public void kafkatest_f15805_0() throws Exception
{    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());    final StreamsBuilder builder = new StreamsBuilder();    final String[] keys = { "hello", "goodbye", "welcome", "go", "kafka" };    final Set<KeyValue<String, String>> batch1 = new HashSet<>(Arrays.asList(new KeyValue<>(keys[0], "1"), new KeyValue<>(keys[1], "1"), new KeyValue<>(keys[2], "3"), new KeyValue<>(keys[3], "5"), new KeyValue<>(keys[4], "2")));    IntegrationTestUtils.produceKeyValuesSynchronously(streamOne, batch1, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), mockTime);    final KTable<String, String> t1 = builder.table(streamOne);    t1.mapValues((ValueMapper<String, Long>) Long::valueOf, Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as("queryMapValues").withValueSerde(Serdes.Long())).toStream().to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();    waitUntilAtLeastNumRecordProcessed(outputTopic, 5);    final ReadOnlyKeyValueStore<String, Long> myMapStore = kafkaStreams.store("queryMapValues", QueryableStoreTypes.keyValueStore());    for (final KeyValue<String, String> batchEntry : batch1) {        assertEquals(Long.valueOf(batchEntry.value), myMapStore.get(batchEntry.key));    }}
private void kafkatest_f15814_0(final String topic, final int numRecs) throws Exception
{    final Properties config = new Properties();    config.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    config.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "queryable-state-consumer");    config.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    config.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    config.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, LongDeserializer.class.getName());    IntegrationTestUtils.waitUntilMinValuesRecordsReceived(config, topic, numRecs, 120 * 1000);}
private Set<KeyValue<String, Long>> kafkatest_f15815_0(final ReadOnlyWindowStore<String, Long> store, final String key)
{    final WindowStoreIterator<Long> fetch = store.fetch(key, ofEpochMilli(0), ofEpochMilli(System.currentTimeMillis()));    if (fetch.hasNext()) {        final KeyValue<Long, Long> next = fetch.next();        return Collections.singleton(KeyValue.pair(key, next.value));    }    return Collections.emptySet();}
public void kafkatest_f15824_0() throws Exception
{    final Serde<String> stringSerde = Serdes.String();    final List<String> expectedFirstAssignment = Collections.singletonList("TEST-TOPIC-1");    final List<String> expectedSecondAssignment = Arrays.asList("TEST-TOPIC-1", "TEST-TOPIC-2");    CLUSTER.createTopic("TEST-TOPIC-1");    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> pattern1Stream = builder.stream(Pattern.compile("TEST-TOPIC-\\d"));    pattern1Stream.to(outputTopic, Produced.with(stringSerde, stringSerde));    final List<String> assignedTopics = new CopyOnWriteArrayList<>();    streams = new KafkaStreams(builder.build(), streamsConfiguration, new DefaultKafkaClientSupplier() {        @Override        public Consumer<byte[], byte[]> getConsumer(final Map<String, Object> config) {            return new KafkaConsumer<byte[], byte[]>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer()) {                @Override                public void subscribe(final Pattern topics, final ConsumerRebalanceListener listener) {                    super.subscribe(topics, new TheConsumerRebalanceListener(assignedTopics, listener));                }            };        }    });    streams.start();    TestUtils.waitForCondition(() -> assignedTopics.equals(expectedFirstAssignment), STREAM_TASKS_NOT_UPDATED);    CLUSTER.createTopic("TEST-TOPIC-2");    TestUtils.waitForCondition(() -> assignedTopics.equals(expectedSecondAssignment), STREAM_TASKS_NOT_UPDATED);}
public Consumer<byte[], byte[]> kafkatest_f15825_0(final Map<String, Object> config)
{    return new KafkaConsumer<byte[], byte[]>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer()) {        @Override        public void subscribe(final Pattern topics, final ConsumerRebalanceListener listener) {            super.subscribe(topics, new TheConsumerRebalanceListener(assignedTopics, listener));        }    };}
public Consumer<byte[], byte[]> kafkatest_f15834_0(final Map<String, Object> config)
{    return new KafkaConsumer<byte[], byte[]>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer()) {        @Override        public void subscribe(final Pattern topics, final ConsumerRebalanceListener listener) {            super.subscribe(topics, new TheConsumerRebalanceListener(leaderAssignment, listener));        }    };}
public void kafkatest_f15835_0(final Pattern topics, final ConsumerRebalanceListener listener)
{    super.subscribe(topics, new TheConsumerRebalanceListener(leaderAssignment, listener));}
public void kafkatest_f15844_0() throws Exception
{    runIntegrationTest(StreamsConfig.NO_OPTIMIZATION, FOUR_REPARTITION_TOPICS);}
private void kafkatest_f15845_0(final String optimizationConfig, final int expectedNumberRepartitionTopics) throws Exception
{    final Initializer<Integer> initializer = () -> 0;    final Aggregator<String, String, Integer> aggregator = (k, v, agg) -> agg + v.length();    final Reducer<String> reducer = (v1, v2) -> v1 + ":" + v2;    final List<String> processorValueCollector = new ArrayList<>();    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> sourceStream = builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()));    final KStream<String, String> mappedStream = sourceStream.map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v));    mappedStream.filter((k, v) -> k.equals("B")).mapValues(v -> v.toUpperCase(Locale.getDefault())).process(() -> new SimpleProcessor(processorValueCollector));    final KStream<String, Long> countStream = mappedStream.groupByKey().count(Materialized.with(Serdes.String(), Serdes.Long())).toStream();    countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()));    mappedStream.groupByKey().aggregate(initializer, aggregator, Materialized.with(Serdes.String(), Serdes.Integer())).toStream().to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()));    // adding operators for case where the repartition node is further downstream    mappedStream.filter((k, v) -> true).peek((k, v) -> System.out.println(k + ":" + v)).groupByKey().reduce(reducer, Materialized.with(Serdes.String(), Serdes.String())).toStream().to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));    mappedStream.filter((k, v) -> k.equals("A")).join(countStream, (v1, v2) -> v1 + ":" + v2.toString(), JoinWindows.of(ofMillis(5000)), Joined.with(Serdes.String(), Serdes.String(), Serdes.Long())).to(JOINED_TOPIC);    streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);    final Properties producerConfig = TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class);    IntegrationTestUtils.produceKeyValuesSynchronously(INPUT_TOPIC, getKeyValues(), producerConfig, mockTime);    final Properties consumerConfig1 = TestUtils.consumerConfig(CLUSTER.bootstrapServers(), StringDeserializer.class, LongDeserializer.class);    final Properties consumerConfig2 = TestUtils.consumerConfig(CLUSTER.bootstrapServers(), StringDeserializer.class, IntegerDeserializer.class);    final Properties consumerConfig3 = TestUtils.consumerConfig(CLUSTER.bootstrapServers(), StringDeserializer.class, StringDeserializer.class);    final Topology topology = builder.build(streamsConfiguration);    final String topologyString = topology.describe().toString();    if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {        assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);    } else {        assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);    }    /*           confirming number of expected repartition topics here         */    assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));    final KafkaStreams streams = new KafkaStreams(topology, streamsConfiguration);    streams.start();    final List<KeyValue<String, Long>> expectedCountKeyValues = Arrays.asList(KeyValue.pair("A", 3L), KeyValue.pair("B", 3L), KeyValue.pair("C", 3L));    IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig1, COUNT_TOPIC, expectedCountKeyValues);    final List<KeyValue<String, Integer>> expectedAggKeyValues = Arrays.asList(KeyValue.pair("A", 9), KeyValue.pair("B", 9), KeyValue.pair("C", 9));    IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig2, AGGREGATION_TOPIC, expectedAggKeyValues);    final List<KeyValue<String, String>> expectedReduceKeyValues = Arrays.asList(KeyValue.pair("A", "foo:bar:baz"), KeyValue.pair("B", "foo:bar:baz"), KeyValue.pair("C", "foo:bar:baz"));    IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig3, REDUCE_TOPIC, expectedReduceKeyValues);    final List<KeyValue<String, String>> expectedJoinKeyValues = Arrays.asList(KeyValue.pair("A", "foo:3"), KeyValue.pair("A", "bar:3"), KeyValue.pair("A", "baz:3"));    IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig3, JOINED_TOPIC, expectedJoinKeyValues);    final List<String> expectedCollectedProcessorValues = Arrays.asList("FOO", "BAR", "BAZ");    assertThat(3, equalTo(processorValueCollector.size()));    assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));    streams.close(ofSeconds(5));}
private int kafkatest_f15854_0(final String topologyString)
{    final Matcher matcher = repartitionTopicPattern.matcher(topologyString);    final List<String> repartitionTopicsFound = new ArrayList<>();    while (matcher.find()) {        repartitionTopicsFound.add(matcher.group());    }    return repartitionTopicsFound.size();}
private List<KeyValue<String, String>> kafkatest_f15855_0()
{    final List<KeyValue<String, String>> keyValueList = new ArrayList<>();    final String[] keys = new String[] { "X", "Y", "Z" };    final String[] values = new String[] { "A:foo", "B:foo", "C:foo" };    for (final String key : keys) {        for (final String value : values) {            keyValueList.add(KeyValue.pair(key, value));        }    }    return keyValueList;}
public void kafkatest_f15864_0() throws Exception
{    super.shouldNotAllowToResetWhileStreamsIsRunning();}
public void kafkatest_f15865_0() throws Exception
{    super.shouldNotAllowToResetWhenInputTopicAbsent();}
public void kafkatest_f15874_0()
{    if (kafkaStreams != null) {        kafkaStreams.close(Duration.ofSeconds(30));    }}
public void kafkatest_f15875_0() throws Exception
{    final AtomicInteger numReceived = new AtomicInteger(0);    final StreamsBuilder builder = new StreamsBuilder();    final Properties props = props(APPID);    props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    // restoring from 1000 to 4000 (committed), and then process from 4000 to 5000 on each of the two partitions    final int offsetLimitDelta = 1000;    final int offsetCheckpointed = 1000;    createStateForRestoration(INPUT_STREAM);    setCommittedOffset(INPUT_STREAM, offsetLimitDelta);    final StateDirectory stateDirectory = new StateDirectory(new StreamsConfig(props), new MockTime(), true);    new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 0)), ".checkpoint")).write(Collections.singletonMap(new TopicPartition(INPUT_STREAM, 0), (long) offsetCheckpointed));    new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 1)), ".checkpoint")).write(Collections.singletonMap(new TopicPartition(INPUT_STREAM, 1), (long) offsetCheckpointed));    final CountDownLatch startupLatch = new CountDownLatch(1);    final CountDownLatch shutdownLatch = new CountDownLatch(1);    builder.table(INPUT_STREAM, Materialized.<Integer, Integer, KeyValueStore<Bytes, byte[]>>as("store").withKeySerde(Serdes.Integer()).withValueSerde(Serdes.Integer())).toStream().foreach((key, value) -> {        if (numReceived.incrementAndGet() == 2 * offsetLimitDelta) {            shutdownLatch.countDown();        }    });    kafkaStreams = new KafkaStreams(builder.build(props), props);    kafkaStreams.setStateListener((newState, oldState) -> {        if (newState == KafkaStreams.State.RUNNING && oldState == KafkaStreams.State.REBALANCING) {            startupLatch.countDown();        }    });    final AtomicLong restored = new AtomicLong(0);    kafkaStreams.setGlobalStateRestoreListener(new StateRestoreListener() {        @Override        public void onRestoreStart(final TopicPartition topicPartition, final String storeName, final long startingOffset, final long endingOffset) {        }        @Override        public void onBatchRestored(final TopicPartition topicPartition, final String storeName, final long batchEndOffset, final long numRestored) {        }        @Override        public void onRestoreEnd(final TopicPartition topicPartition, final String storeName, final long totalRestored) {            restored.addAndGet(totalRestored);        }    });    kafkaStreams.start();    assertTrue(startupLatch.await(30, TimeUnit.SECONDS));    assertThat(restored.get(), equalTo((long) numberOfKeys - offsetLimitDelta * 2 - offsetCheckpointed * 2));    assertTrue(shutdownLatch.await(30, TimeUnit.SECONDS));    assertThat(numReceived.get(), equalTo(offsetLimitDelta * 2));}
private void kafkatest_f15889_0(final String topic, final int limitDelta)
{    final Properties consumerConfig = new Properties();    consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, APPID);    consumerConfig.put(ConsumerConfig.CLIENT_ID_CONFIG, "commit-consumer");    consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);    consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);    final Consumer<Integer, Integer> consumer = new KafkaConsumer<>(consumerConfig);    final List<TopicPartition> partitions = Arrays.asList(new TopicPartition(topic, 0), new TopicPartition(topic, 1));    consumer.assign(partitions);    consumer.seekToEnd(partitions);    for (final TopicPartition partition : partitions) {        final long position = consumer.position(partition);        consumer.seek(partition, position - limitDelta);    }    consumer.commitSync();    consumer.close();}
public void kafkatest_f15890_0()
{    try {        final Map<String, Set<Integer>> allData = generate(bootstrapServers, numKeys, maxRecordsPerKey, Duration.ofSeconds(20));        result = verify(bootstrapServers, allData, maxRecordsPerKey);    } catch (final Exception ex) {        this.exception = ex;    }}
public void kafkatest_f15901_0() throws Exception
{    final Properties streamsConfiguration1 = streamsConfiguration();    streamsConfiguration1.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    final Properties streamsConfiguration2 = streamsConfiguration();    streamsConfiguration2.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    final StreamsBuilder builder = new StreamsBuilder();    builder.table(INPUT_TOPIC, Consumed.with(Serdes.Integer(), Serdes.Integer()), Materialized.as("source-table"));    createClients(builder.build(streamsConfiguration1), streamsConfiguration1, builder.build(streamsConfiguration2), streamsConfiguration2);    setStateListenersForVerification(thread -> !thread.standbyTasks().isEmpty() && !thread.activeTasks().isEmpty());    startClients();    waitUntilBothClientAreOK("At least one client did not reach state RUNNING with active tasks and stand-by tasks");}
private void kafkatest_f15902_0(final Topology topology1, final Properties streamsConfiguration1, final Topology topology2, final Properties streamsConfiguration2)
{    client1 = new KafkaStreams(topology1, streamsConfiguration1);    client2 = new KafkaStreams(topology2, streamsConfiguration2);}
public void kafkatest_f15911_0() throws Exception
{    shouldMigrateKeyValueStoreToTimestampedKeyValueStoreUsingPapi(false);}
public void kafkatest_f15912_0() throws Exception
{    shouldMigrateKeyValueStoreToTimestampedKeyValueStoreUsingPapi(true);}
public void kafkatest_f15921_0() throws Exception
{    final StreamsBuilder streamsBuilderForOldStore = new StreamsBuilder();    streamsBuilderForOldStore.addStateStore(Stores.windowStoreBuilder(Stores.persistentWindowStore(STORE_NAME, Duration.ofMillis(1000L), Duration.ofMillis(1000L), false), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(WindowedProcessor::new, STORE_NAME);    final StreamsBuilder streamsBuilderForNewStore = new StreamsBuilder();    streamsBuilderForNewStore.addStateStore(Stores.timestampedWindowStoreBuilder(Stores.persistentTimestampedWindowStore(STORE_NAME, Duration.ofMillis(1000L), Duration.ofMillis(1000L), false), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(TimestampedWindowedProcessor::new, STORE_NAME);    final Properties props = props();    shouldMigrateWindowStoreToTimestampedWindowStoreUsingPapi(new KafkaStreams(streamsBuilderForOldStore.build(), props), new KafkaStreams(streamsBuilderForNewStore.build(), props), true);}
private void kafkatest_f15922_0(final KafkaStreams kafkaStreamsOld, final KafkaStreams kafkaStreamsNew, final boolean persistentStore) throws Exception
{    kafkaStreams = kafkaStreamsOld;    kafkaStreams.start();    processWindowedKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 1L)));    processWindowedKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L)));    final long lastUpdateKeyOne = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processWindowedKeyValueAndVerifyPlainCount(2, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L)));    final long lastUpdateKeyTwo = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processWindowedKeyValueAndVerifyPlainCount(3, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L)));    final long lastUpdateKeyThree = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 1L)));    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 2L)));    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 3L)));    final long lastUpdateKeyFour = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    kafkaStreams.close();    kafkaStreams = null;    kafkaStreams = kafkaStreamsNew;    kafkaStreams.start();    verifyWindowedCountWithTimestamp(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L, lastUpdateKeyOne);    verifyWindowedCountWithTimestamp(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L, lastUpdateKeyTwo);    verifyWindowedCountWithTimestamp(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L, lastUpdateKeyThree);    verifyWindowedCountWithTimestamp(new Windowed<>(4, new TimeWindow(0L, 1000L)), 3L, lastUpdateKeyFour);    final long currentTime = CLUSTER.time.milliseconds();    processKeyValueAndVerifyWindowedCountWithTimestamp(1, currentTime + 42L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyTwo)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, lastUpdateKeyFour))));    processKeyValueAndVerifyWindowedCountWithTimestamp(2, currentTime + 45L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, lastUpdateKeyFour))));    // can process "out of order" record for different key    processKeyValueAndVerifyWindowedCountWithTimestamp(4, currentTime + 21L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(4L, currentTime + 21L))));    processKeyValueAndVerifyWindowedCountWithTimestamp(4, currentTime + 42L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(5L, currentTime + 42L))));    // out of order (same key) record should not reduce result timestamp    processKeyValueAndVerifyWindowedCountWithTimestamp(4, currentTime + 10L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(6L, currentTime + 42L))));    // test new segment    processKeyValueAndVerifyWindowedCountWithTimestamp(10, currentTime + 100001L, singletonList(KeyValue.pair(new Windowed<>(10, new TimeWindow(100000L, 101000L)), ValueAndTimestamp.make(1L, currentTime + 100001L))));    kafkaStreams.close();}
public void kafkatest_f15932_0(final Integer key, final Integer value)
{    final long newCount;    final ValueAndTimestamp<Long> oldCountWithTimestamp = store.get(key);    final long newTimestamp;    if (oldCountWithTimestamp == null) {        newCount = 1L;        newTimestamp = context.timestamp();    } else {        newCount = oldCountWithTimestamp.value() + 1L;        newTimestamp = Math.max(oldCountWithTimestamp.timestamp(), context.timestamp());    }    store.put(key, ValueAndTimestamp.make(newCount, newTimestamp));}
public void kafkatest_f15934_0(final ProcessorContext context)
{    store = (WindowStore<Integer, Long>) context.getStateStore(STORE_NAME);}
public void kafkatest_f15945_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer");    final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null", 3L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b", 6L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-a", 9L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-b", 9L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c", 10L)), null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-d", 14L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-a", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-b", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-c", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d", 15L)));    leftStream.outerJoin(rightStream, valueJoiner, JoinWindows.of(ofSeconds(10))).to(OUTPUT_TOPIC);    runTest(expectedResult);}
public void kafkatest_f15946_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer");    final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null", 3L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b", 6L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-a", 9L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-b", 9L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c", 10L)), null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-d", 14L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-a", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-b", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-c", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d", 15L)));    leftStream.map(MockMapper.noOpKeyValueMapper()).outerJoin(rightStream.flatMap(MockMapper.noOpFlatKeyValueMapper()).selectKey(MockMapper.selectKeyKeyValueMapper()), valueJoiner, JoinWindows.of(ofSeconds(10))).to(OUTPUT_TOPIC);    runTest(expectedResult);}
public void kafkatest_f15955_0(final ProcessorContext context)
{    this.context = context;}
public KeyValue<String, Long>f15956_1final String key, final Long value)
{    try {        assertThat(context.topic(), equalTo(topic));    } catch (final Throwable e) {        firstException.compareAndSet(null, e);            }    return new KeyValue<>(key, value);}
private static boolean kafkatest_f15966_0(final String topic)
{    final Properties properties = new Properties();    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);    properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);    try (final Consumer<Object, Object> consumer = new KafkaConsumer<>(properties)) {        final List<TopicPartition> partitions = consumer.partitionsFor(topic).stream().map(pi -> new TopicPartition(pi.topic(), pi.partition())).collect(Collectors.toList());        consumer.assign(partitions);        consumer.seekToBeginning(partitions);        final long start = System.currentTimeMillis();        while ((System.currentTimeMillis() - start) < DEFAULT_TIMEOUT) {            final ConsumerRecords<Object, Object> records = consumer.poll(ofMillis(500));            if (!records.isEmpty()) {                return true;            }        }        return false;    }}
public void kafkatest_f15967_0() throws InterruptedException
{    final String testId = "-shouldShutdownWhenRecordConstraintIsViolated";    final String appId = getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;    final String input = "input" + testId;    final String outputSuppressed = "output-suppressed" + testId;    final String outputRaw = "output-raw" + testId;    cleanStateBeforeTest(CLUSTER, input, outputRaw, outputSuppressed);    final StreamsBuilder builder = new StreamsBuilder();    final KTable<String, Long> valueCounts = buildCountsTable(input, builder);    valueCounts.suppress(untilTimeLimit(ofMillis(MAX_VALUE), maxRecords(1L).shutDownWhenFull())).toStream().to(outputSuppressed, Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().to(outputRaw, Produced.with(STRING_SERDE, Serdes.Long()));    final Properties streamsConfig = getStreamsConfig(appId);    final KafkaStreams driver = IntegrationTestUtils.getStartedStreams(streamsConfig, builder, true);    try {        produceSynchronously(input, asList(new KeyValueTimestamp<>("k1", "v1", scaledTime(0L)), new KeyValueTimestamp<>("k1", "v2", scaledTime(1L)), new KeyValueTimestamp<>("k2", "v1", scaledTime(2L)), new KeyValueTimestamp<>("x", "x", scaledTime(3L))));        verifyErrorShutdown(driver);    } finally {        driver.close();        cleanStateAfterTest(CLUSTER, driver);    }}
public void kafkatest_f15976_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-left");    if (cacheEnabled) {        leftTable.leftJoin(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(false)).to(OUTPUT_TOPIC);        runTest(expectedFinalJoinResult, storeName);    } else {        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null", 3L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a", 5L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 7L)), null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null", 9L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c", 10L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null", 11L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 12L)), null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d", 15L)));        leftTable.leftJoin(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
public void kafkatest_f15977_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer");    if (cacheEnabled) {        leftTable.outerJoin(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(false)).to(OUTPUT_TOPIC);        runTest(expectedFinalJoinResult, storeName);    } else {        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null", 3L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a", 5L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-b", 7L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 8L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null", 9L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c", 10L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null", 11L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 12L)), null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-d", 14L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d", 15L)));        leftTable.outerJoin(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
public void kafkatest_f15986_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-inner-outer");    if (cacheEnabled) {        leftTable.outerJoin(rightTable, valueJoiner).outerJoin(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(true)).to(OUTPUT_TOPIC);        runTest(expectedFinalMultiJoinResult, storeName);    } else {        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null-null", 3L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-b-b", 7L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 8L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 8L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null-null", 9L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null-null", 11L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null-null", 11L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 12L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-d-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-d-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-d", 15L)));        leftTable.outerJoin(rightTable, valueJoiner).outerJoin(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
public voidf15987_1) throws IOException, InterruptedException
{            zookeeper = new EmbeddedZookeeper();        brokerConfig.put(KafkaConfig$.MODULE$.ZkConnectProp(), zKConnectString());    brokerConfig.put(KafkaConfig$.MODULE$.PortProp(), DEFAULT_BROKER_PORT);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.DeleteTopicEnableProp(), true);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.LogCleanerDedupeBufferSizeProp(), 2 * 1024 * 1024L);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.GroupMinSessionTimeoutMsProp(), 0);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.GroupInitialRebalanceDelayMsProp(), 0);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.OffsetsTopicReplicationFactorProp(), (short) 1);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.OffsetsTopicPartitionsProp(), 5);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.AutoCreateTopicsEnableProp(), true);    for (int i = 0; i < brokers.length; i++) {        brokerConfig.put(KafkaConfig$.MODULE$.BrokerIdProp(), i);                brokers[i] = new KafkaEmbedded(brokerConfig, time);            }}
public void kafkatest_f15996_0(final String topic, final int partitions, final int replication) throws InterruptedException
{    createTopic(topic, partitions, replication, Collections.emptyMap());}
public void kafkatest_f15997_0(final String topic, final int partitions, final int replication, final Map<String, String> topicConfig) throws InterruptedException
{    brokers[0].createTopic(topic, partitions, replication, topicConfig);    final List<TopicPartition> topicPartitions = new ArrayList<>();    for (int partition = 0; partition < partitions; partition++) {        topicPartitions.add(new TopicPartition(topic, partition));    }    IntegrationTestUtils.waitForTopicPartitions(brokers(), topicPartitions, TOPIC_CREATION_TIMEOUT);}
public void kafkatest_f16006_0(final long timeoutMs, final String... topics) throws InterruptedException
{    deleteTopicsAndWait(timeoutMs, topics);    createTopics(topics);}
public void kafkatest_f16007_0(final long timeoutMs, final String... topics) throws InterruptedException
{    TestUtils.waitForCondition(new TopicsRemainingCondition(topics), timeoutMs, "Topics are not expected after " + timeoutMs + " milli seconds.");}
public static void kafkatest_f16016_0(final EmbeddedKafkaCluster cluster, final KafkaStreams driver)
{    driver.cleanUp();    try {        cluster.deleteAllTopicsAndWait(DEFAULT_TIMEOUT);    } catch (final InterruptedException e) {        throw new RuntimeException(e);    }}
public static void kafkatest_f16017_0(final String topic, final Collection<KeyValue<K, V>> records, final Properties producerConfig, final Time time) throws ExecutionException, InterruptedException
{    produceKeyValuesSynchronously(topic, records, producerConfig, time, false);}
public static void kafkatest_f16026_0(final String topic, final Collection<V> records, final Properties producerConfig, final Time time) throws ExecutionException, InterruptedException
{    produceValuesSynchronously(topic, records, producerConfig, time, false);}
public static void kafkatest_f16027_0(final String topic, final Collection<V> records, final Properties producerConfig, final Time time, final boolean enableTransactions) throws ExecutionException, InterruptedException
{    final Collection<KeyValue<Object, V>> keyedRecords = new ArrayList<>();    for (final V value : records) {        final KeyValue<Object, V> kv = new KeyValue<>(null, value);        keyedRecords.add(kv);    }    produceKeyValuesSynchronously(topic, keyedRecords, producerConfig, time, enableTransactions);}
public static List<KeyValue<K, V>> kafkatest_f16036_0(final Properties consumerConfig, final String topic, final List<KeyValue<K, V>> expectedRecords, final long waitTime) throws InterruptedException
{    return waitUntilFinalKeyValueRecordsReceived(consumerConfig, topic, expectedRecords, waitTime, false);}
public static List<KeyValueTimestamp<K, V>> kafkatest_f16037_0(final Properties consumerConfig, final String topic, final List<KeyValueTimestamp<K, V>> expectedRecords, final long waitTime) throws InterruptedException
{    return waitUntilFinalKeyValueRecordsReceived(consumerConfig, topic, expectedRecords, waitTime, true);}
private static String kafkatest_f16046_0(final List<ConsumerRecord<K, V>> result)
{    final StringBuilder resultStr = new StringBuilder();    resultStr.append("[\n");    for (final ConsumerRecord<?, ?> record : result) {        resultStr.append("  ").append(record.toString()).append("\n");    }    resultStr.append("]");    return resultStr.toString();}
public static List<V> kafkatest_f16047_0(final String topic, final Properties consumerConfig, final long waitTime, final int maxMessages)
{    final List<V> returnList;    try (final Consumer<Object, V> consumer = createConsumer(consumerConfig)) {        returnList = readValues(topic, consumer, waitTime, maxMessages);    }    return returnList;}
private Properties kafkatest_f16056_0(final Properties initialConfig)
{    final Properties effectiveConfig = new Properties();    effectiveConfig.put(KafkaConfig$.MODULE$.BrokerIdProp(), 0);    effectiveConfig.put(KafkaConfig$.MODULE$.HostNameProp(), "localhost");    effectiveConfig.put(KafkaConfig$.MODULE$.PortProp(), "9092");    effectiveConfig.put(KafkaConfig$.MODULE$.NumPartitionsProp(), 1);    effectiveConfig.put(KafkaConfig$.MODULE$.AutoCreateTopicsEnableProp(), true);    effectiveConfig.put(KafkaConfig$.MODULE$.MessageMaxBytesProp(), 1000000);    effectiveConfig.put(KafkaConfig$.MODULE$.ControlledShutdownEnableProp(), true);    effectiveConfig.put(KafkaConfig$.MODULE$.ZkSessionTimeoutMsProp(), 10000);    effectiveConfig.putAll(initialConfig);    effectiveConfig.setProperty(KafkaConfig$.MODULE$.LogDirProp(), logDir.getAbsolutePath());    return effectiveConfig;}
public String kafkatest_f16057_0()
{    final Object listenerConfig = effectiveConfig.get(KafkaConfig$.MODULE$.InterBrokerListenerNameProp());    return kafka.config().hostName() + ":" + kafka.boundPort(new ListenerName(listenerConfig != null ? listenerConfig.toString() : "PLAINTEXT"));}
public void kafkatest_f16066_0()
{    final String nullDurationPrefix = prepareMillisCheckFailMsgPrefix(null, "nullDuration");    try {        validateMillisecondDuration(null, nullDurationPrefix);        fail("Expected exception when null passed to duration.");    } catch (final IllegalArgumentException e) {        assertThat(e.getMessage(), containsString(nullDurationPrefix));    }}
public void kafkatest_f16067_0()
{    final Duration maxDurationInDays = Duration.ofDays(MAX_ACCEPTABLE_DAYS_FOR_DURATION);    final String maxDurationPrefix = prepareMillisCheckFailMsgPrefix(maxDurationInDays, "maxDuration");    try {        validateMillisecondDuration(maxDurationInDays, maxDurationPrefix);        fail("Expected exception when maximum days passed for duration, because of long overflow");    } catch (final IllegalArgumentException e) {        assertThat(e.getMessage(), containsString(maxDurationPrefix));    }}
public void kafkatest_f16076_0()
{    props.put(CommonClientConfigs.SEND_BUFFER_CONFIG, -2);    final KafkaStreams streams = new KafkaStreams(builder.build(), props);    streams.close();}
public void kafkatest_f16077_0()
{    props.put(CommonClientConfigs.RECEIVE_BUFFER_CONFIG, -2);    final KafkaStreams streams = new KafkaStreams(builder.build(), props);    streams.close();}
public void kafkatest_f16086_0()
{    final int oldInitCount = MockMetricsReporter.INIT_COUNT.get();    try (final KafkaStreams streams = new KafkaStreams(builder.build(), props)) {        final int newInitCount = MockMetricsReporter.INIT_COUNT.get();        final int initDiff = newInitCount - oldInitCount;        assertTrue("some reporters should be initialized by calling on construction", initDiff > 0);        streams.start();        final int oldCloseCount = MockMetricsReporter.CLOSE_COUNT.get();        streams.close();        assertEquals(oldCloseCount + initDiff, MockMetricsReporter.CLOSE_COUNT.get());    }}
public void kafkatest_f16087_0()
{    globalStreams.close();    final int closeCount = MockMetricsReporter.CLOSE_COUNT.get();    globalStreams.close();    Assert.assertEquals("subsequent close() calls should do nothing", closeCount, MockMetricsReporter.CLOSE_COUNT.get());}
public void kafkatest_f16096_0()
{    globalStreams.allMetadataForStore("store");}
public void kafkatest_f16097_0()
{    globalStreams.metadataForKey("store", "key", Serdes.String().serializer());}
public void kafkatest_f16106_0() throws Exception
{    final String inputTopic = testName.getMethodName() + "-input";    final String outputTopic = testName.getMethodName() + "-output";    CLUSTER.createTopics(inputTopic, outputTopic);    final Topology topology = new Topology();    topology.addSource("source", Serdes.String().deserializer(), Serdes.String().deserializer(), inputTopic).addProcessor("process", () -> new AbstractProcessor<String, String>() {        @Override        public void process(final String key, final String value) {            if (value.length() % 2 == 0) {                context().forward(key, key + value);            }        }    }, "source").addSink("sink", outputTopic, new StringSerializer(), new StringSerializer(), "process");    startStreamsAndCheckDirExists(topology, Collections.singleton(inputTopic), outputTopic, false);}
public void kafkatest_f16107_0(final String key, final String value)
{    if (value.length() % 2 == 0) {        context().forward(key, key + value);    }}
public void kafkatest_f16116_0()
{    final KeyValue<String, Long> kv = KeyValue.pair("key1", 1L);    final KeyValue<String, Long> copyOfKV = KeyValue.pair(kv.key, kv.value);    // Reflexive    assertTrue(kv.equals(kv));    assertTrue(kv.hashCode() == kv.hashCode());    // Symmetric    assertTrue(kv.equals(copyOfKV));    assertTrue(kv.hashCode() == copyOfKV.hashCode());    assertTrue(copyOfKV.hashCode() == kv.hashCode());    // Transitive    final KeyValue<String, Long> copyOfCopyOfKV = KeyValue.pair(copyOfKV.key, copyOfKV.value);    assertTrue(copyOfKV.equals(copyOfCopyOfKV));    assertTrue(copyOfKV.hashCode() == copyOfCopyOfKV.hashCode());    assertTrue(kv.equals(copyOfCopyOfKV));    assertTrue(kv.hashCode() == copyOfCopyOfKV.hashCode());    // Inequality scenarios    assertFalse("must be false for null", kv.equals(null));    assertFalse("must be false if key is non-null and other key is null", kv.equals(KeyValue.pair(null, kv.value)));    assertFalse("must be false if value is non-null and other value is null", kv.equals(KeyValue.pair(kv.key, null)));    final KeyValue<Long, Long> differentKeyType = KeyValue.pair(1L, kv.value);    assertFalse("must be false for different key types", kv.equals(differentKeyType));    final KeyValue<String, String> differentValueType = KeyValue.pair(kv.key, "anyString");    assertFalse("must be false for different value types", kv.equals(differentValueType));    final KeyValue<Long, String> differentKeyValueTypes = KeyValue.pair(1L, "anyString");    assertFalse("must be false for different key and value types", kv.equals(differentKeyValueTypes));    assertFalse("must be false for different types of objects", kv.equals(new Object()));    final KeyValue<String, Long> differentKey = KeyValue.pair(kv.key + "suffix", kv.value);    assertFalse("must be false if key is different", kv.equals(differentKey));    assertFalse("must be false if key is different", differentKey.equals(kv));    final KeyValue<String, Long> differentValue = KeyValue.pair(kv.key, kv.value + 1L);    assertFalse("must be false if value is different", kv.equals(differentValue));    assertFalse("must be false if value is different", differentValue.equals(kv));    final KeyValue<String, Long> differentKeyAndValue = KeyValue.pair(kv.key + "suffix", kv.value + 1L);    assertFalse("must be false if key and value are different", kv.equals(differentKeyAndValue));    assertFalse("must be false if key and value are different", differentKeyAndValue.equals(kv));}
public K kafkatest_f16117_0()
{    return key;}
 KStream<K, V> kafkatest_f16126_0()
{    final String name = builder.newProcessorName("RANDOM-FILTER-");    final ProcessorGraphNode<K, V> processorNode = new ProcessorGraphNode<>(name, new ProcessorParameters<>(new ExtendedKStreamDummy<>(), name));    builder.addGraphNode(this.streamsGraphNode, processorNode);    return new KStreamImpl<>(name, null, null, sourceNodes, false, processorNode, builder);}
public Processor<K, V> kafkatest_f16127_0()
{    return new ExtendedKStreamDummyProcessor();}
public void kafkatest_f16136_0()
{    final MockProcessorSupplier<String, String> supplier = new MockProcessorSupplier<>();    stream.join(global, keyValueMapper, MockValueJoiner.TOSTRING_JOINER).process(supplier);    final Map<String, ValueAndTimestamp<String>> expected = new HashMap<>();    expected.put("1", ValueAndTimestamp.make("a+A", 2L));    expected.put("2", ValueAndTimestamp.make("b+B", 10L));    verifyJoin(expected, supplier);}
private void kafkatest_f16137_0(final Map<String, ValueAndTimestamp<String>> expected, final MockProcessorSupplier<String, String> supplier)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());    final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        // write some data to the global table        driver.pipeInput(recordFactory.create(globalTopic, "a", "A", 1L));        driver.pipeInput(recordFactory.create(globalTopic, "b", "B", 5L));        // write some data to the stream        driver.pipeInput(recordFactory.create(streamTopic, "1", "a", 2L));        driver.pipeInput(recordFactory.create(streamTopic, "2", "b", 10L));        driver.pipeInput(recordFactory.create(streamTopic, "3", "c", 3L));    }    assertEquals(expected, supplier.theCapturedProcessor().lastValueAndTimestampPerKey);}
public void kafkatest_f16152_0()
{    final Properties properties = new Properties();    properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "test-application");    properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");    properties.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> inputStream = builder.stream("inputTopic");    final KStream<String, String> changedKeyStream = inputStream.selectKey((k, v) -> v.substring(0, 5));    // first repartition    changedKeyStream.groupByKey(Grouped.as("count-repartition")).count(Materialized.as("count-store")).toStream().to("count-topic", Produced.with(Serdes.String(), Serdes.Long()));    // second repartition    changedKeyStream.groupByKey(Grouped.as("windowed-repartition")).windowedBy(TimeWindows.of(Duration.ofSeconds(5))).count(Materialized.as("windowed-count-store")).toStream().map((k, v) -> KeyValue.pair(k.key(), v)).to("windowed-count", Produced.with(Serdes.String(), Serdes.Long()));    builder.build(properties);}
public void kafkatest_f16153_0()
{    final Topology attemptedOptimize = getTopologyWithChangingValuesAfterChangingKey(StreamsConfig.OPTIMIZE);    final Topology noOptimization = getTopologyWithChangingValuesAfterChangingKey(StreamsConfig.NO_OPTIMIZATION);    assertEquals(attemptedOptimize.describe().toString(), noOptimization.describe().toString());    assertEquals(2, getCountOfRepartitionTopicsFound(attemptedOptimize.describe().toString()));    assertEquals(2, getCountOfRepartitionTopicsFound(noOptimization.describe().toString()));}
public String kafkatest_f16165_0(final String value)
{    return value;}
public boolean kafkatest_f16166_0(final String key, final String value)
{    return true;}
public void kafkatest_f16175_0()
{    final KStream<String, String> playEvents = builder.stream(Collections.singleton("events"), consumed);    final MaterializedInternal<String, String, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(Materialized.as("table-store"), builder, storePrefix);    final KTable<String, String> table = builder.table("table-topic", consumed, materializedInternal);    final KStream<String, String> mapped = playEvents.map(MockMapper.<String, String>selectValueKeyValueMapper());    mapped.leftJoin(table, MockValueJoiner.TOSTRING_JOINER).groupByKey().count(Materialized.as("count"));    builder.buildAndOptimizeTopology();    builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(APP_ID)));    assertEquals(Collections.singletonList("table-topic"), builder.internalTopologyBuilder.stateStoreNameToSourceTopics().get("table-store"));    assertEquals(Collections.singletonList(APP_ID + "-KSTREAM-MAP-0000000003-repartition"), builder.internalTopologyBuilder.stateStoreNameToSourceTopics().get("count"));}
public void kafkatest_f16176_0()
{    final String topicName = "topic-1";    final ConsumedInternal consumed = new ConsumedInternal<>(Consumed.with(AutoOffsetReset.EARLIEST));    builder.stream(Collections.singleton(topicName), consumed);    builder.buildAndOptimizeTopology();    assertTrue(builder.internalTopologyBuilder.earliestResetTopicsPattern().matcher(topicName).matches());    assertFalse(builder.internalTopologyBuilder.latestResetTopicsPattern().matcher(topicName).matches());}
public void kafkatest_f16185_0()
{    final ConsumedInternal consumed = new ConsumedInternal<>(Consumed.with(new MockTimestampExtractor()));    builder.stream(Collections.singleton("topic"), consumed);    builder.buildAndOptimizeTopology();    final ProcessorTopology processorTopology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(APP_ID))).build(null);    assertThat(processorTopology.source("topic").getTimestampExtractor(), instanceOf(MockTimestampExtractor.class));}
public void kafkatest_f16186_0()
{    builder.table("topic", consumed, materialized);    builder.buildAndOptimizeTopology();    final ProcessorTopology processorTopology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(APP_ID))).build(null);    assertNull(processorTopology.source("topic").getTimestampExtractor());}
public void kafkatest_f16195_0()
{    groupedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as("store"));}
public void kafkatest_f16196_0()
{    groupedStream.aggregate(MockInitializer.STRING_INIT, null, Materialized.as("store"));}
private void kafkatest_f16205_0(final MockProcessorSupplier<Windowed<String>, Long> supplier)
{    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 10));        driver.pipeInput(recordFactory.create(TOPIC, "2", "2", 15));        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 30));        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 70));        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 100));        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 90));    }    final Map<Windowed<String>, ValueAndTimestamp<Long>> result = supplier.theCapturedProcessor().lastValueAndTimestampPerKey;    assertEquals(ValueAndTimestamp.make(2L, 30L), result.get(new Windowed<>("1", new SessionWindow(10L, 30L))));    assertEquals(ValueAndTimestamp.make(1L, 15L), result.get(new Windowed<>("2", new SessionWindow(15L, 15L))));    assertEquals(ValueAndTimestamp.make(3L, 100L), result.get(new Windowed<>("1", new SessionWindow(70L, 100L))));}
public void kafkatest_f16206_0()
{    final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();    final KTable<Windowed<String>, Long> table = groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).count(Materialized.as("session-store"));    table.toStream().process(supplier);    doCountSessionWindows(supplier);    assertEquals(table.queryableStoreName(), "session-store");}
public void kafkatest_f16215_0()
{    groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).aggregate(null, MockAggregator.TOSTRING_ADDER, (aggKey, aggOne, aggTwo) -> null, Materialized.as("storeName"));}
public void kafkatest_f16216_0()
{    groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).aggregate(MockInitializer.STRING_INIT, null, (aggKey, aggOne, aggTwo) -> null, Materialized.as("storeName"));}
public void kafkatest_f16225_0()
{    groupedStream.count(Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as("count").withKeySerde(Serdes.String()));    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);        LogCaptureAppender.unregister(appender);        final Map<MetricName, ? extends Metric> metrics = driver.metrics();        assertEquals(1.0, getMetricByName(metrics, "skipped-records-total", "stream-metrics").metricValue());        assertNotEquals(0.0, getMetricByName(metrics, "skipped-records-rate", "stream-metrics").metricValue());        assertThat(appender.getMessages(), hasItem("Skipping record due to null key or value. key=[3] value=[null] topic=[topic] partition=[0] offset=[6]"));    }}
public void kafkatest_f16226_0()
{    groupedStream.reduce(MockReducer.STRING_ADDER, Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as("reduce").withKeySerde(Serdes.String()).withValueSerde(Serdes.String()));    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);        {            final KeyValueStore<String, String> reduced = driver.getKeyValueStore("reduce");            assertThat(reduced.get("1"), equalTo("A+C+D"));            assertThat(reduced.get("2"), equalTo("B"));            assertThat(reduced.get("3"), equalTo("E+F"));        }        {            final KeyValueStore<String, ValueAndTimestamp<String>> reduced = driver.getTimestampedKeyValueStore("reduce");            assertThat(reduced.get("1"), equalTo(ValueAndTimestamp.make("A+C+D", 10L)));            assertThat(reduced.get("2"), equalTo(ValueAndTimestamp.make("B", 1L)));            assertThat(reduced.get("3"), equalTo(ValueAndTimestamp.make("E+F", 9L)));        }    }}
public void kafkatest_f16235_0()
{    groupedTable.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, Materialized.as(INVALID_STORE_NAME));}
public void kafkatest_f16236_0()
{    groupedTable.aggregate(null, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, Materialized.as("store"));}
public void kafkatest_f16245_0()
{    final KeyValueMapper<String, Number, KeyValue<String, Integer>> intProjection = (key, value) -> KeyValue.pair(key, value.intValue());    final KTable<String, Integer> reduced = builder.table(topic, Consumed.with(Serdes.String(), Serdes.Double()), Materialized.<String, Double, KeyValueStore<Bytes, byte[]>>as("store").withKeySerde(Serdes.String()).withValueSerde(Serdes.Double())).groupBy(intProjection).reduce(MockReducer.INTEGER_ADDER, MockReducer.INTEGER_SUBTRACTOR);    final MockProcessorSupplier<String, Integer> supplier = getReducedResults(reduced);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        assertReduced(supplier.theCapturedProcessor().lastValueAndTimestampPerKey, topic, driver);        assertNull(reduced.queryableStoreName());    }}
public void kafkatest_f16246_0()
{    final KeyValueMapper<String, Number, KeyValue<String, Integer>> intProjection = (key, value) -> KeyValue.pair(key, value.intValue());    final KTable<String, Integer> reduced = builder.table(topic, Consumed.with(Serdes.String(), Serdes.Double())).groupBy(intProjection).reduce(MockReducer.INTEGER_ADDER, MockReducer.INTEGER_SUBTRACTOR, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>as("reduce").withKeySerde(Serdes.String()).withValueSerde(Serdes.Integer()));    final MockProcessorSupplier<String, Integer> supplier = getReducedResults(reduced);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        assertReduced(supplier.theCapturedProcessor().lastValueAndTimestampPerKey, topic, driver);        {            final KeyValueStore<String, Integer> reduce = driver.getKeyValueStore("reduce");            assertThat(reduce.get("A"), equalTo(5));            assertThat(reduce.get("B"), equalTo(6));        }        {            final KeyValueStore<String, ValueAndTimestamp<Integer>> reduce = driver.getTimestampedKeyValueStore("reduce");            assertThat(reduce.get("A"), equalTo(ValueAndTimestamp.make(5, 50L)));            assertThat(reduce.get("B"), equalTo(ValueAndTimestamp.make(6, 30L)));        }    }}
public void kafkatest_f16255_0()
{    groupedTable.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, null, Materialized.as("store"));}
public void kafkatest_f16256_0()
{    groupedTable.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, (Materialized) null);}
public void kafkatest_f16265_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final ValueMapperWithKey<Integer, Number, Iterable<String>> mapper = (readOnlyKey, value) -> {        final ArrayList<String> result = new ArrayList<>();        result.add("v" + value);        result.add("k" + readOnlyKey);        return result;    };    final int[] expectedKeys = { 0, 1, 2, 3 };    final KStream<Integer, Integer> stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.Integer()));    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream.flatMapValues(mapper).process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        for (final int expectedKey : expectedKeys) {            // passing the timestamp to recordFactory.create to disambiguate the call            driver.pipeInput(recordFactory.create(topicName, expectedKey, expectedKey, 0L));        }    }    final KeyValueTimestamp[] expected = { new KeyValueTimestamp<>(0, "v0", 0), new KeyValueTimestamp<>(0, "k0", 0), new KeyValueTimestamp<>(1, "v1", 0), new KeyValueTimestamp<>(1, "k1", 0), new KeyValueTimestamp<>(2, "v2", 0), new KeyValueTimestamp<>(2, "k2", 0), new KeyValueTimestamp<>(3, "v3", 0), new KeyValueTimestamp<>(3, "k3", 0) };    assertArrayEquals(expected, supplier.theCapturedProcessor().processed.toArray());}
public void kafkatest_f16266_0()
{    inputKey = 1;    inputValue = 10;    transformer = mock(Transformer.class);    context = strictMock(ProcessorContext.class);    processor = new KStreamFlatTransformProcessor<>(transformer);}
public void kafkatest_f16275_0()
{    final Iterable<String> outputValues = Arrays.asList("Hello", "Blue", "Planet");    processor.init(context);    EasyMock.reset(valueTransformer);    EasyMock.expect(valueTransformer.transform(inputKey, inputValue)).andReturn(outputValues);    for (final String outputValue : outputValues) {        context.forward(inputKey, outputValue);    }    replayAll();    processor.process(inputKey, inputValue);    verifyAll();}
public void kafkatest_f16276_0()
{    processor.init(context);    EasyMock.reset(valueTransformer);    EasyMock.expect(valueTransformer.transform(inputKey, inputValue)).andReturn(Collections.<String>emptyList());    replayAll();    processor.process(inputKey, inputValue);    verifyAll();}
private void kafkatest_f16285_0(final int messageCount, final String valuePrefix)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(globalTableTopic, "FKey" + expectedKeys[i], valuePrefix + expectedKeys[i]));    }}
private void kafkatest_f16286_0(final int messageCount)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(globalTableTopic, "FKey" + expectedKeys[i], (String) null));    }}
private void kafkatest_f16295_0(final int messageCount, final String valuePrefix, final boolean includeForeignKey)
{    final ConsumerRecordFactory<Integer, String> recordFactory = new ConsumerRecordFactory<>(new IntegerSerializer(), new StringSerializer(), 0L, 1L);    for (int i = 0; i < messageCount; i++) {        String value = valuePrefix + expectedKeys[i];        if (includeForeignKey) {            value = value + ",FKey" + expectedKeys[i];        }        driver.pipeInput(recordFactory.create(streamTopic, expectedKeys[i], value));    }}
private void kafkatest_f16296_0(final int messageCount, final String valuePrefix)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer(), 0L, 1L);    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(globalTableTopic, "FKey" + expectedKeys[i], valuePrefix + expectedKeys[i]));    }}
public void kafkatest_f16305_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> source1 = builder.stream(Arrays.asList("topic-1", "topic-2"), stringConsumed);    final KStream<String, String> source2 = builder.stream(Arrays.asList("topic-3", "topic-4"), stringConsumed);    final KStream<String, String> stream1 = source1.filter((key, value) -> true).filterNot((key, value) -> false);    final KStream<String, Integer> stream2 = stream1.mapValues(Integer::new);    final KStream<String, Integer> stream3 = source2.flatMapValues((ValueMapper<String, Iterable<Integer>>) value -> Collections.singletonList(new Integer(value)));    final KStream<String, Integer>[] streams2 = stream2.branch((key, value) -> (value % 2) == 0, (key, value) -> true);    final KStream<String, Integer>[] streams3 = stream3.branch((key, value) -> (value % 2) == 0, (key, value) -> true);    final int anyWindowSize = 1;    final Joined<String, Integer, Integer> joined = Joined.with(Serdes.String(), Serdes.Integer(), Serdes.Integer());    final KStream<String, Integer> stream4 = streams2[0].join(streams3[0], (value1, value2) -> value1 + value2, JoinWindows.of(ofMillis(anyWindowSize)), joined);    streams2[1].join(streams3[1], (value1, value2) -> value1 + value2, JoinWindows.of(ofMillis(anyWindowSize)), joined);    stream4.to("topic-5");    streams2[1].through("topic-6").process(new MockProcessorSupplier<>());    assertEquals(// sources    2 + // stream1    2 + // stream2    1 + // stream3    1 + 1 + // streams2    2 + 1 + // streams3    2 + // stream2-stream3 joins    5 * 2 + // to    1 + // through    2 + // process    1, TopologyWrapper.getInternalTopologyBuilder(builder.build()).setApplicationId("X").build(null).processors().size());}
public void kafkatest_f16306_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> stream1 = builder.stream(Collections.singleton("topic-1"), stringConsumed);    final KTable<String, String> table1 = builder.table("topic-2", stringConsumed);    final GlobalKTable<String, String> table2 = builder.globalTable("topic-2", stringConsumed);    final ConsumedInternal<String, String> consumedInternal = new ConsumedInternal<>(stringConsumed);    final KeyValueMapper<String, String, String> selector = (key, value) -> key;    final KeyValueMapper<String, String, Iterable<KeyValue<String, String>>> flatSelector = (key, value) -> Collections.singleton(new KeyValue<>(key, value));    final ValueMapper<String, String> mapper = value -> value;    final ValueMapper<String, Iterable<String>> flatMapper = Collections::singleton;    final ValueJoiner<String, String, String> joiner = (value1, value2) -> value1;    final TransformerSupplier<String, String, KeyValue<String, String>> transformerSupplier = () -> new Transformer<String, String, KeyValue<String, String>>() {        @Override        public void init(final ProcessorContext context) {        }        @Override        public KeyValue<String, String> transform(final String key, final String value) {            return new KeyValue<>(key, value);        }        @Override        public void close() {        }    };    final ValueTransformerSupplier<String, String> valueTransformerSupplier = () -> new ValueTransformer<String, String>() {        @Override        public void init(final ProcessorContext context) {        }        @Override        public String transform(final String value) {            return value;        }        @Override        public void close() {        }    };    assertEquals(((AbstractStream) stream1.filter((key, value) -> false)).keySerde(), consumedInternal.keySerde());    assertEquals(((AbstractStream) stream1.filter((key, value) -> false)).valueSerde(), consumedInternal.valueSerde());    assertEquals(((AbstractStream) stream1.filterNot((key, value) -> false)).keySerde(), consumedInternal.keySerde());    assertEquals(((AbstractStream) stream1.filterNot((key, value) -> false)).valueSerde(), consumedInternal.valueSerde());    assertNull(((AbstractStream) stream1.selectKey(selector)).keySerde());    assertEquals(((AbstractStream) stream1.selectKey(selector)).valueSerde(), consumedInternal.valueSerde());    assertNull(((AbstractStream) stream1.map(KeyValue::new)).keySerde());    assertNull(((AbstractStream) stream1.map(KeyValue::new)).valueSerde());    assertEquals(((AbstractStream) stream1.mapValues(mapper)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.mapValues(mapper)).valueSerde());    assertNull(((AbstractStream) stream1.flatMap(flatSelector)).keySerde());    assertNull(((AbstractStream) stream1.flatMap(flatSelector)).valueSerde());    assertEquals(((AbstractStream) stream1.flatMapValues(flatMapper)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.flatMapValues(flatMapper)).valueSerde());    assertNull(((AbstractStream) stream1.transform(transformerSupplier)).keySerde());    assertNull(((AbstractStream) stream1.transform(transformerSupplier)).valueSerde());    assertEquals(((AbstractStream) stream1.transformValues(valueTransformerSupplier)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.transformValues(valueTransformerSupplier)).valueSerde());    assertNull(((AbstractStream) stream1.merge(stream1)).keySerde());    assertNull(((AbstractStream) stream1.merge(stream1)).valueSerde());    assertEquals(((AbstractStream) stream1.through("topic-3")).keySerde(), consumedInternal.keySerde());    assertEquals(((AbstractStream) stream1.through("topic-3")).valueSerde(), consumedInternal.valueSerde());    assertEquals(((AbstractStream) stream1.through("topic-3", Produced.with(mySerde, mySerde))).keySerde(), mySerde);    assertEquals(((AbstractStream) stream1.through("topic-3", Produced.with(mySerde, mySerde))).valueSerde(), mySerde);    assertEquals(((AbstractStream) stream1.groupByKey()).keySerde(), consumedInternal.keySerde());    assertEquals(((AbstractStream) stream1.groupByKey()).valueSerde(), consumedInternal.valueSerde());    assertEquals(((AbstractStream) stream1.groupByKey(Grouped.with(mySerde, mySerde))).keySerde(), mySerde);    assertEquals(((AbstractStream) stream1.groupByKey(Grouped.with(mySerde, mySerde))).valueSerde(), mySerde);    assertNull(((AbstractStream) stream1.groupBy(selector)).keySerde());    assertEquals(((AbstractStream) stream1.groupBy(selector)).valueSerde(), consumedInternal.valueSerde());    assertEquals(((AbstractStream) stream1.groupBy(selector, Grouped.with(mySerde, mySerde))).keySerde(), mySerde);    assertEquals(((AbstractStream) stream1.groupBy(selector, Grouped.with(mySerde, mySerde))).valueSerde(), mySerde);    assertNull(((AbstractStream) stream1.join(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).keySerde());    assertNull(((AbstractStream) stream1.join(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).valueSerde());    assertEquals(((AbstractStream) stream1.join(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.join(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertNull(((AbstractStream) stream1.leftJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).keySerde());    assertNull(((AbstractStream) stream1.leftJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).valueSerde());    assertEquals(((AbstractStream) stream1.leftJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.leftJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertNull(((AbstractStream) stream1.outerJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).keySerde());    assertNull(((AbstractStream) stream1.outerJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).valueSerde());    assertEquals(((AbstractStream) stream1.outerJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.outerJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertEquals(((AbstractStream) stream1.join(table1, joiner)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.join(table1, joiner)).valueSerde());    assertEquals(((AbstractStream) stream1.join(table1, joiner, Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.join(table1, joiner, Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertEquals(((AbstractStream) stream1.leftJoin(table1, joiner)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.leftJoin(table1, joiner)).valueSerde());    assertEquals(((AbstractStream) stream1.leftJoin(table1, joiner, Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.leftJoin(table1, joiner, Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertEquals(((AbstractStream) stream1.join(table2, selector, joiner)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.join(table2, selector, joiner)).valueSerde());    assertEquals(((AbstractStream) stream1.leftJoin(table2, selector, joiner)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.leftJoin(table2, selector, joiner)).valueSerde());}
public void kafkatest_f16319_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final GlobalKTable<String, String> globalKTable = builder.globalTable("globalTopic");    final KeyValueMapper<String, String, String> kvMappper = (k, v) -> k + v;    final ValueJoiner<String, String, String> valueJoiner = (v1, v2) -> v1 + v2;    builder.<String, String>stream("topic").selectKey((k, v) -> v).join(globalKTable, kvMappper, valueJoiner).groupByKey().count();    final Pattern repartitionTopicPattern = Pattern.compile("Sink: .*-repartition");    final String topology = builder.build().describe().toString();    final Matcher matcher = repartitionTopicPattern.matcher(topology);    assertTrue(matcher.find());    final String match = matcher.group();    assertThat(match, notNullValue());    assertTrue(match.endsWith("repartition"));}
public void kafkatest_f16320_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final Consumed<String, String> consumed = Consumed.with(Serdes.String(), Serdes.String());    final KStream<String, String> inputStream = builder.stream(Collections.singleton("input"), consumed);    inputStream.to("output", Produced.with(Serdes.String(), Serdes.String()));}
public void kafkatest_f16329_0()
{    testStream.flatMapValues((ValueMapperWithKey<? super String, ? super String, ? extends Iterable<? extends String>>) null);}
public void kafkatest_f16330_0()
{    testStream.branch();}
public void kafkatest_f16339_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> testStream.flatTransformValues((ValueTransformerWithKeySupplier) null));    assertEquals("valueTransformerSupplier can't be null", e.getMessage());}
public void kafkatest_f16340_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> testStream.flatTransformValues((ValueTransformerSupplier) null));    assertEquals("valueTransformerSupplier can't be null", e.getMessage());}
public void kafkatest_f16349_0()
{    testStream.join((GlobalKTable) null, MockMapper.selectValueMapper(), MockValueJoiner.TOSTRING_JOINER);}
public void kafkatest_f16350_0()
{    testStream.join(builder.globalTable("global", stringConsumed), null, MockValueJoiner.TOSTRING_JOINER);}
public void kafkatest_f16359_0()
{    final KTable<String, String> table = builder.table("blah", stringConsumed);    try {        testStream.join(table, MockValueJoiner.TOSTRING_JOINER, null);        fail("Should have thrown NullPointerException");    } catch (final NullPointerException e) {    // ok    }}
public void kafkatest_f16360_0()
{    testStream.join(testStream, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(10)), null);}
public void kafkatest_f16369_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KStream<Integer, String> stream1;    final KStream<Integer, String> stream2;    final KStream<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream1 = builder.stream(topic1, consumed);    stream2 = builder.stream(topic2, consumed);    joined = stream1.join(stream2, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(100)), Joined.with(Serdes.Integer(), Serdes.String(), Serdes.String()));    joined.process(supplier);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> processor = supplier.theCapturedProcessor();        long time = 0L;        // w2 = {}        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "A" + expectedKeys[i], time));        }        processor.checkAndClearProcessResult(EMPTY);        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0) }        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "a" + expectedKeys[i], time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+a0", 0), new KeyValueTimestamp<>(1, "A1+a1", 0));        // push four items to the primary stream with larger and increasing timestamp; this should produce no items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0) }        time = 1000L;        for (int i = 0; i < expectedKeys.length; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "B" + expectedKeys[i], time + i));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items to the other stream with fixed larger timestamp; this should produce four items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100) }        time += 100L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "b" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+b0", 1100), new KeyValueTimestamp<>(1, "B1+b1", 1100), new KeyValueTimestamp<>(2, "B2+b2", 1100), new KeyValueTimestamp<>(3, "B3+b3", 1100));        // push four items to the other stream with incremented timestamp; this should produce three items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "c" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(1, "B1+c1", 1101), new KeyValueTimestamp<>(2, "B2+c2", 1101), new KeyValueTimestamp<>(3, "B3+c3", 1101));        // push four items to the other stream with incremented timestamp; this should produce two items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "d" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(2, "B2+d2", 1102), new KeyValueTimestamp<>(3, "B3+d3", 1102));        // push four items to the other stream with incremented timestamp; this should produce one item        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "e" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(3, "B3+e3", 1103));        // push four items to the other stream with incremented timestamp; this should produce no items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "f" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items to the other stream with timestamp before the window bound; this should produce no items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899) }        time = 1000L - 100L - 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "g" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items to the other stream with with incremented timestamp; this should produce one item        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "h" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+h0", 1000));        // push four items to the other stream with with incremented timestamp; this should produce two items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "i" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+i0", 1000), new KeyValueTimestamp<>(1, "B1+i1", 1001));        // push four items to the other stream with with incremented timestamp; this should produce three items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901),        // 0:j0 (ts: 902), 1:j1 (ts: 902), 2:j2 (ts: 902), 3:j3 (ts: 902) }        time += 1;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "j" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+j0", 1000), new KeyValueTimestamp<>(1, "B1+j1", 1001), new KeyValueTimestamp<>(2, "B2+j2", 1002));        // push four items to the other stream with with incremented timestamp; this should produce four items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901),        // 0:j0 (ts: 902), 1:j1 (ts: 902), 2:j2 (ts: 902), 3:j3 (ts: 902) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901),        // 0:j0 (ts: 902), 1:j1 (ts: 902), 2:j2 (ts: 902), 3:j3 (ts: 902) }        // 0:k0 (ts: 903), 1:k1 (ts: 903), 2:k2 (ts: 903), 3:k3 (ts: 903) }        time += 1;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "k" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+k0", 1000), new KeyValueTimestamp<>(1, "B1+k1", 1001), new KeyValueTimestamp<>(2, "B2+k2", 1002), new KeyValueTimestamp<>(3, "B3+k3", 1003));        // advance time to not join with existing data        // we omit above exiting data, even if it's still in the window        //         // push four items with increasing timestamps to the other stream. the primary window is empty; this should produce no items        // w1 = {}        // w2 = {}        // --> w1 = {}        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time = 2000L;        for (int i = 0; i < expectedKeys.length; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "l" + expectedKeys[i], time + i));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with larger timestamps to the primary stream; this should produce four items        // w1 = {}        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time = 2000L + 100L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "C" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "C0+l0", 2100), new KeyValueTimestamp<>(1, "C1+l1", 2100), new KeyValueTimestamp<>(2, "C2+l2", 2100), new KeyValueTimestamp<>(3, "C3+l3", 2100));        // push four items with increase timestamps to the primary stream; this should produce three items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "D" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(1, "D1+l1", 2101), new KeyValueTimestamp<>(2, "D2+l2", 2101), new KeyValueTimestamp<>(3, "D3+l3", 2101));        // push four items with increase timestamps to the primary stream; this should produce two items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "E" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(2, "E2+l2", 2102), new KeyValueTimestamp<>(3, "E3+l3", 2102));        // push four items with increase timestamps to the primary stream; this should produce one item        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "F" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(3, "F3+l3", 2103));        // push four items with increase timestamps (now out of window) to the primary stream; this should produce no items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "G" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with smaller timestamps (before window) to the primary stream; this should produce no items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time = 2000L - 100L - 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "H" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with increased timestamps to the primary stream; this should produce one item        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "I" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "I0+l0", 2000));        // push four items with increased timestamps to the primary stream; this should produce two items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "J" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "J0+l0", 2000), new KeyValueTimestamp<>(1, "J1+l1", 2001));        // push four items with increased timestamps to the primary stream; this should produce three items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901),        // 0:K0 (ts: 1902), 1:K1 (ts: 1902), 2:K2 (ts: 1902), 3:K3 (ts: 1902) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "K" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "K0+l0", 2000), new KeyValueTimestamp<>(1, "K1+l1", 2001), new KeyValueTimestamp<>(2, "K2+l2", 2002));        // push four items with increased timestamps to the primary stream; this should produce four items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901) }        // 0:K0 (ts: 1902), 1:K1 (ts: 1902), 2:K2 (ts: 1902), 3:K3 (ts: 1902) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901),        // 0:K0 (ts: 1902), 1:K1 (ts: 1902), 2:K2 (ts: 1902), 3:K3 (ts: 1902),        // 0:L0 (ts: 1903), 1:L1 (ts: 1903), 2:L2 (ts: 1903), 3:L3 (ts: 1903) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "L" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "L0+l0", 2000), new KeyValueTimestamp<>(1, "L1+l1", 2001), new KeyValueTimestamp<>(2, "L2+l2", 2002), new KeyValueTimestamp<>(3, "L3+l3", 2003));    }}
public void kafkatest_f16370_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KStream<Integer, String> stream1;    final KStream<Integer, String> stream2;    final KStream<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream1 = builder.stream(topic1, consumed);    stream2 = builder.stream(topic2, consumed);    joined = stream1.join(stream2, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(0)).after(ofMillis(100)), Joined.with(Serdes.Integer(), Serdes.String(), Serdes.String()));    joined.process(supplier);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> processor = supplier.theCapturedProcessor();        long time = 1000L;        // w2 = {}        for (int i = 0; i < expectedKeys.length; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "A" + expectedKeys[i], time + i));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items smaller timestamps (out of window) to the secondary stream; this should produce no items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = {}        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999) }        time = 1000L - 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "a" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with increased timestamps to the secondary stream; this should produce one item        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "b" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+b0", 1000));        // push four items with increased timestamps to the secondary stream; this should produce two items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "c" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+c0", 1001), new KeyValueTimestamp<>(1, "A1+c1", 1001));        // push four items with increased timestamps to the secondary stream; this should produce three items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "d" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+d0", 1002), new KeyValueTimestamp<>(1, "A1+d1", 1002), new KeyValueTimestamp<>(2, "A2+d2", 1002));        // push four items with increased timestamps to the secondary stream; this should produce four items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "e" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+e0", 1003), new KeyValueTimestamp<>(1, "A1+e1", 1003), new KeyValueTimestamp<>(2, "A2+e2", 1003), new KeyValueTimestamp<>(3, "A3+e3", 1003));        // push four items with larger timestamps to the secondary stream; this should produce four items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100) }        time = 1000 + 100L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "f" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+f0", 1100), new KeyValueTimestamp<>(1, "A1+f1", 1100), new KeyValueTimestamp<>(2, "A2+f2", 1100), new KeyValueTimestamp<>(3, "A3+f3", 1100));        // push four items with increased timestamps to the secondary stream; this should produce three items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "g" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(1, "A1+g1", 1101), new KeyValueTimestamp<>(2, "A2+g2", 1101), new KeyValueTimestamp<>(3, "A3+g3", 1101));        // push four items with increased timestamps to the secondary stream; this should produce two items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "h" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(2, "A2+h2", 1102), new KeyValueTimestamp<>(3, "A3+h3", 1102));        // push four items with increased timestamps to the secondary stream; this should produce one item        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102),        // 0:i0 (ts: 1103), 1:i1 (ts: 1103), 2:i2 (ts: 1103), 3:i3 (ts: 1103) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "i" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(3, "A3+i3", 1103));        // push four items with increased timestamps (no out of window) to the secondary stream; this should produce no items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102),        // 0:i0 (ts: 1103), 1:i1 (ts: 1103), 2:i2 (ts: 1103), 3:i3 (ts: 1103) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102),        // 0:i0 (ts: 1103), 1:i1 (ts: 1103), 2:i2 (ts: 1103), 3:i3 (ts: 1103),        // 0:j0 (ts: 1104), 1:j1 (ts: 1104), 2:j2 (ts: 1104), 3:j3 (ts: 1104) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "j" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);    }}
private void kafkatest_f16379_0(final int messageCount, final String valuePrefix)
{    final Random r = new Random(System.currentTimeMillis());    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(tableTopic, expectedKeys[i], valuePrefix + expectedKeys[i], r.nextInt(Integer.MAX_VALUE)));    }}
private void kafkatest_f16380_0()
{    for (int i = 0; i < 2; i++) {        driver.pipeInput(recordFactory.create(tableTopic, expectedKeys[i], null));    }}
public void kafkatest_f16389_0()
{    driver.close();}
private void kafkatest_f16390_0(final int messageCount, final String valuePrefix)
{    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(streamTopic, expectedKeys[i], valuePrefix + expectedKeys[i], i));    }}
public void kafkatest_f16399_0()
{    new StreamsBuilder().<Integer, String>stream("numbers").map((key, value) -> KeyValue.pair(key, key + ":" + value)).to("strings");}
public void kafkatest_f16400_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = { 1, 10, 100, 1000 };    final KStream<Integer, String> stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.String()));    stream.mapValues(CharSequence::length).process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topicName, expectedKey, Integer.toString(expectedKey), expectedKey / 2L));        }    }    final KeyValueTimestamp[] expected = { new KeyValueTimestamp<>(1, 1, 0), new KeyValueTimestamp<>(10, 2, 5), new KeyValueTimestamp<>(100, 3, 50), new KeyValueTimestamp<>(1000, 4, 500) };    assertArrayEquals(expected, supplier.theCapturedProcessor().processed.toArray());}
public void kafkatest_f16409_0()
{    final File stateDir = TestUtils.tempDirectory();    metrics = new Metrics();    final MockStreamsMetrics metrics = new MockStreamsMetrics(KStreamSessionWindowAggregateProcessorTest.this.metrics);    ThreadMetrics.skipRecordSensor(metrics);    context = new InternalMockProcessorContext(stateDir, Serdes.String(), Serdes.String(), metrics, new StreamsConfig(StreamsTestUtils.getStreamsConfig()), NoOpRecordCollector::new, new ThreadCache(new LogContext("testCache "), 100000, metrics)) {        @Override        public <K, V> void forward(final K key, final V value, final To to) {            toInternal.update(to);            results.add(new KeyValueTimestamp<>(key, value, toInternal.timestamp()));        }    };    initStore(true);    processor.init(context);}
public void kafkatest_f16410_0(final K key, final V value, final To to)
{    toInternal.update(to);    results.add(new KeyValueTimestamp<>(key, value, toInternal.timestamp()));}
public void kafkatest_f16419_0()
{    final KTableValueGetter<Windowed<String>, Long> getter = sessionAggregator.view().get();    getter.init(context);    context.setTime(0);    processor.process("a", "1");    context.setTime(GAP_MS + 1);    processor.process("a", "1");    processor.process("a", "2");    final long t0 = getter.get(new Windowed<>("a", new SessionWindow(0, 0))).value();    final long t1 = getter.get(new Windowed<>("a", new SessionWindow(GAP_MS + 1, GAP_MS + 1))).value();    assertEquals(1L, t0);    assertEquals(2L, t1);}
public void kafkatest_f16420_0()
{    initStore(false);    processor.init(context);    context.setTime(0);    processor.process("a", "1");    processor.process("b", "1");    processor.process("c", "1");    assertEquals(Arrays.asList(new KeyValueTimestamp<>(new Windowed<>("a", new SessionWindow(0, 0)), new Change<>(1L, null), 0L), new KeyValueTimestamp<>(new Windowed<>("b", new SessionWindow(0, 0)), new Change<>(1L, null), 0L), new KeyValueTimestamp<>(new Windowed<>("c", new SessionWindow(0, 0)), new Change<>(1L, null), 0L)), results);}
public void kafkatest_f16430_0(final ProcessorContext context)
{    context.schedule(Duration.ofMillis(1), PunctuationType.WALL_CLOCK_TIME, timestamp -> context.forward(-1, (int) timestamp));}
public KeyValue<Integer, Integer> kafkatest_f16431_0(final Number key, final Number value)
{    total += value.intValue();    return KeyValue.pair(key.intValue() * 2, total);}
public void kafkatest_f16445_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic = "topic";    final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));    stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String())).windowedBy(TimeWindows.of(ofMillis(10)).advanceBy(ofMillis(5)).until(100)).aggregate(() -> "", MockAggregator.toStringInstance("+"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as("topic1-Canonicalized").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()).toStream().map((key, value) -> new KeyValue<>(key.toString(), value)).to("output");    LogCaptureAppender.setClassLoggerToDebug(KStreamWindowAggregate.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, "k", "100", 100L));        driver.pipeInput(recordFactory.create(topic, "k", "0", 0L));        driver.pipeInput(recordFactory.create(topic, "k", "1", 1L));        driver.pipeInput(recordFactory.create(topic, "k", "2", 2L));        driver.pipeInput(recordFactory.create(topic, "k", "3", 3L));        driver.pipeInput(recordFactory.create(topic, "k", "4", 4L));        driver.pipeInput(recordFactory.create(topic, "k", "5", 5L));        driver.pipeInput(recordFactory.create(topic, "k", "6", 6L));        LogCaptureAppender.unregister(appender);        assertLatenessMetrics(driver, // how many events get dropped        is(7.0), // k:0 is 100ms late, since its time is 0, but it arrives at stream time 100.        is(100.0), // (0 + 100 + 99 + 98 + 97 + 96 + 95 + 94) / 8        is(84.875));        assertThat(appender.getMessages(), hasItems("Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[0] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[1] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[2] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[3] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[4] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[5] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[6] window=[0,10) expiration=[10] streamTime=[100]"));        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@95/105]", "+100", 100);        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@100/110]", "+100", 100);        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@5/15]", "+5", 5);        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@5/15]", "+5+6", 6);        assertThat(driver.readOutput("output"), nullValue());    }}
public void kafkatest_f16446_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic = "topic";    final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));    stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String())).windowedBy(TimeWindows.of(ofMillis(10)).advanceBy(ofMillis(10)).grace(ofMillis(90L))).aggregate(() -> "", MockAggregator.toStringInstance("+"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as("topic1-Canonicalized").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()).toStream().map((key, value) -> new KeyValue<>(key.toString(), value)).to("output");    LogCaptureAppender.setClassLoggerToDebug(KStreamWindowAggregate.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, "k", "100", 200L));        driver.pipeInput(recordFactory.create(topic, "k", "0", 100L));        driver.pipeInput(recordFactory.create(topic, "k", "1", 101L));        driver.pipeInput(recordFactory.create(topic, "k", "2", 102L));        driver.pipeInput(recordFactory.create(topic, "k", "3", 103L));        driver.pipeInput(recordFactory.create(topic, "k", "4", 104L));        driver.pipeInput(recordFactory.create(topic, "k", "5", 105L));        driver.pipeInput(recordFactory.create(topic, "k", "6", 6L));        LogCaptureAppender.unregister(appender);        assertLatenessMetrics(driver, is(7.0), is(194.0), is(97.375));        assertThat(appender.getMessages(), hasItems("Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[6] window=[0,10) expiration=[110] streamTime=[200]"));        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@200/210]", "+100", 200);        assertThat(driver.readOutput("output"), nullValue());    }}
public void kafkatest_f16455_0()
{    // disable caching at the config level    props.setProperty(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, "0");}
private void kafkatest_f16456_0(final StreamsBuilder builder, final KTable<String, Integer> table2, final KTable<String, Integer> table3, final String topic)
{    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();    table2.toStream().process(supplier);    table3.toStream().process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, "A", 1, 10L));        driver.pipeInput(recordFactory.create(topic, "B", 2, 5L));        driver.pipeInput(recordFactory.create(topic, "C", 3, 8L));        driver.pipeInput(recordFactory.create(topic, "D", 4, 14L));        driver.pipeInput(recordFactory.create(topic, "A", null, 18L));        driver.pipeInput(recordFactory.create(topic, "B", null, 15L));    }    final List<MockProcessor<String, Integer>> processors = supplier.capturedProcessors(2);    processors.get(0).checkAndClearProcessResult(new KeyValueTimestamp<>("A", null, 10), new KeyValueTimestamp<>("B", 2, 5), new KeyValueTimestamp<>("C", null, 8), new KeyValueTimestamp<>("D", 4, 14), new KeyValueTimestamp<>("A", null, 18), new KeyValueTimestamp<>("B", null, 15));    processors.get(1).checkAndClearProcessResult(new KeyValueTimestamp<>("A", 1, 10), new KeyValueTimestamp<>("B", null, 5), new KeyValueTimestamp<>("C", 3, 8), new KeyValueTimestamp<>("D", null, 14), new KeyValueTimestamp<>("A", null, 18), new KeyValueTimestamp<>("B", null, 15));}
public void kafkatest_f16465_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTableImpl<String, Integer, Integer> table1 = (KTableImpl<String, Integer, Integer>) builder.table(topic1, consumed);    final KTableImpl<String, Integer, Integer> table2 = (KTableImpl<String, Integer, Integer>) table1.filter(predicate);    doTestSendingOldValue(builder, table1, table2, topic1);}
public void kafkatest_f16466_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTableImpl<String, Integer, Integer> table1 = (KTableImpl<String, Integer, Integer>) builder.table(topic1, consumed);    final KTableImpl<String, Integer, Integer> table2 = (KTableImpl<String, Integer, Integer>) table1.filter(predicate, Materialized.as("store2"));    doTestSendingOldValue(builder, table1, table2, topic1);}
public void kafkatest_f16477_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final String topic2 = "topic2";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    builder.table(topic2, consumed);    final KTableImpl<String, String, Integer> table1Mapped = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new);    table1Mapped.filter((key, value) -> (value % 2) == 0);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        assertEquals(0, driver.getAllStateStores().size());    }}
public void kafkatest_f16478_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final String topic2 = "topic2";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    final KTableImpl<String, String, String> table2 = (KTableImpl<String, String, String>) builder.table(topic2, consumed);    final KTableImpl<String, String, Integer> table1Mapped = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new);    final KTableImpl<String, Integer, Integer> table1MappedFiltered = (KTableImpl<String, Integer, Integer>) table1Mapped.filter((key, value) -> (value % 2) == 0);    table2.join(table1MappedFiltered, (v1, v2) -> v1 + v2);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        assertEquals(2, driver.getAllStateStores().size());    }}
public void kafkatest_f16487_0()
{    table.join(null, MockValueJoiner.TOSTRING_JOINER);}
public void kafkatest_f16488_0()
{    table.join(table, MockValueJoiner.TOSTRING_JOINER);}
public void kafkatest_f16497_0()
{    table.leftJoin(table, MockValueJoiner.TOSTRING_JOINER, (Materialized) null);}
public void kafkatest_f16498_0()
{    table.outerJoin(table, MockValueJoiner.TOSTRING_JOINER, (Materialized) null);}
public void kafkatest_f16507_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final Processor<String, Change<String>> join = new KTableKTableInnerJoin<>((KTableImpl<String, String, String>) builder.table("left", Consumed.with(Serdes.String(), Serdes.String())), (KTableImpl<String, String, String>) builder.table("right", Consumed.with(Serdes.String(), Serdes.String())), null).get();    final MockProcessorContext context = new MockProcessorContext();    context.setRecordMetadata("left", -1, -2, null, -3);    join.init(context);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    join.process(null, new Change<>("new", "old"));    LogCaptureAppender.unregister(appender);    assertEquals(1.0, getMetricByName(context.metrics().metrics(), "skipped-records-total", "stream-metrics").metricValue());    assertThat(appender.getMessages(), hasItem("Skipping record due to null key. change=[(new<-old)] topic=[left] partition=[-1] offset=[-2]"));}
private void kafkatest_f16508_0(final StreamsBuilder builder, final int[] expectedKeys, final KTable<Integer, String> table1, final KTable<Integer, String> table2, final MockProcessorSupplier<Integer, String> supplier, final KTable<Integer, String> joined)
{    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> proc = supplier.theCapturedProcessor();        assertFalse(((KTableImpl<?, ?, ?>) table1).sendingOldValueEnabled());        assertFalse(((KTableImpl<?, ?, ?>) table2).sendingOldValueEnabled());        assertFalse(((KTableImpl<?, ?, ?>) joined).sendingOldValueEnabled());        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        proc.checkAndClearProcessResult(EMPTY);        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("X0+Y0", null), 5), new KeyValueTimestamp<>(1, new Change<>("X1+Y1", null), 10));        // push all four items to the primary stream. this should produce two items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+Y0", null), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+Y1", null), 10));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+YY0", null), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+YY1", null), 7), new KeyValueTimestamp<>(2, new Change<>("XX2+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("XX3+YY3", null), 15));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXX0+YY0", null), 6), new KeyValueTimestamp<>(1, new Change<>("XXX1+YY1", null), 6), new KeyValueTimestamp<>(2, new Change<>("XXX2+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("XXX3+YY3", null), 15));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>(null, null), 6), new KeyValueTimestamp<>(1, new Change<>(null, null), 7));        // push all four items to the primary stream. this should produce two items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(2, new Change<>("XXXX2+YY2", null), 13), new KeyValueTimestamp<>(3, new Change<>("XXXX3+YY3", null), 15));        // push four items to the primary stream with null. this should produce two items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(2, new Change<>(null, null), 10), new KeyValueTimestamp<>(3, new Change<>(null, null), 20));    }}
public void kafkatest_f16517_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KTable<Integer, String> table1;    final KTable<Integer, String> table2;    final KTable<Integer, String> joined;    table1 = builder.table(topic1, consumed);    table2 = builder.table(topic2, consumed);    joined = table1.outerJoin(table2, MockValueJoiner.TOSTRING_JOINER);    joined.toStream().to(output);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        assertOutputKeyValueTimestamp(driver, 0, "X0+null", 5L);        assertOutputKeyValueTimestamp(driver, 1, "X1+null", 6L);        assertNull(driver.readOutput(output));        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        assertOutputKeyValueTimestamp(driver, 0, "X0+Y0", 5L);        assertOutputKeyValueTimestamp(driver, 1, "X1+Y1", 10L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        assertOutputKeyValueTimestamp(driver, 0, "XX0+Y0", 7L);        assertOutputKeyValueTimestamp(driver, 1, "XX1+Y1", 10L);        assertOutputKeyValueTimestamp(driver, 2, "XX2+null", 7L);        assertOutputKeyValueTimestamp(driver, 3, "XX3+null", 7L);        assertNull(driver.readOutput(output));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XX0+YY0", 7L);        assertOutputKeyValueTimestamp(driver, 1, "XX1+YY1", 7L);        assertOutputKeyValueTimestamp(driver, 2, "XX2+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "XX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXX0+YY0", 6L);        assertOutputKeyValueTimestamp(driver, 1, "XXX1+YY1", 6L);        assertOutputKeyValueTimestamp(driver, 2, "XXX2+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "XXX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXX0+null", 6L);        assertOutputKeyValueTimestamp(driver, 1, "XXX1+null", 7L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXXX0+null", 13L);        assertOutputKeyValueTimestamp(driver, 1, "XXXX1+null", 13L);        assertOutputKeyValueTimestamp(driver, 2, "XXXX2+YY2", 13L);        assertOutputKeyValueTimestamp(driver, 3, "XXXX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push four items to the primary stream with null. this should produce four items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, null, 0L);        assertOutputKeyValueTimestamp(driver, 1, null, 42L);        assertOutputKeyValueTimestamp(driver, 2, "null+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "null+YY3", 20L);        assertNull(driver.readOutput(output));    }}
public void kafkatest_f16518_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KTable<Integer, String> table1;    final KTable<Integer, String> table2;    final KTable<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier;    table1 = builder.table(topic1, consumed);    table2 = builder.table(topic2, consumed);    joined = table1.outerJoin(table2, MockValueJoiner.TOSTRING_JOINER);    supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build().addProcessor("proc", supplier, ((KTableImpl<?, ?, ?>) joined).name);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        final MockProcessor<Integer, String> proc = supplier.theCapturedProcessor();        assertTrue(((KTableImpl<?, ?, ?>) table1).sendingOldValueEnabled());        assertTrue(((KTableImpl<?, ?, ?>) table2).sendingOldValueEnabled());        assertFalse(((KTableImpl<?, ?, ?>) joined).sendingOldValueEnabled());        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("X0+null", null), 5), new KeyValueTimestamp<>(1, new Change<>("X1+null", null), 6));        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("X0+Y0", null), 5), new KeyValueTimestamp<>(1, new Change<>("X1+Y1", null), 10));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+Y0", null), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+Y1", null), 10), new KeyValueTimestamp<>(2, new Change<>("XX2+null", null), 7), new KeyValueTimestamp<>(3, new Change<>("XX3+null", null), 7));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+YY0", null), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+YY1", null), 7), new KeyValueTimestamp<>(2, new Change<>("XX2+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("XX3+YY3", null), 15));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXX0+YY0", null), 6), new KeyValueTimestamp<>(1, new Change<>("XXX1+YY1", null), 6), new KeyValueTimestamp<>(2, new Change<>("XXX2+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("XXX3+YY3", null), 15));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXX0+null", null), 6), new KeyValueTimestamp<>(1, new Change<>("XXX1+null", null), 7));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXXX0+null", null), 13), new KeyValueTimestamp<>(1, new Change<>("XXXX1+null", null), 13), new KeyValueTimestamp<>(2, new Change<>("XXXX2+YY2", null), 13), new KeyValueTimestamp<>(3, new Change<>("XXXX3+YY3", null), 15));        // push four items to the primary stream with null. this should produce four items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>(null, null), 0), new KeyValueTimestamp<>(1, new Change<>(null, null), 42), new KeyValueTimestamp<>(2, new Change<>("null+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("null+YY3", null), 20));    }}
private void kafkatest_f16527_0(final StreamsBuilder builder, final String topic1, final KTableImpl<String, String, Integer> table2, final KTableImpl<String, String, Integer> table3)
{    final Topology topology = builder.build();    final KTableValueGetterSupplier<String, Integer> getterSupplier2 = table2.valueGetterSupplier();    final KTableValueGetterSupplier<String, Integer> getterSupplier3 = table3.valueGetterSupplier();    final InternalTopologyBuilder topologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology);    topologyBuilder.connectProcessorAndStateStores(table2.name, getterSupplier2.storeNames());    topologyBuilder.connectProcessorAndStateStores(table3.name, getterSupplier3.storeNames());    try (final TopologyTestDriverWrapper driver = new TopologyTestDriverWrapper(builder.build(), props)) {        final KTableValueGetter<String, Integer> getter2 = getterSupplier2.get();        final KTableValueGetter<String, Integer> getter3 = getterSupplier3.get();        getter2.init(driver.setCurrentNodeForProcessorContext(table2.name));        getter3.init(driver.setCurrentNodeForProcessorContext(table3.name));        driver.pipeInput(recordFactory.create(topic1, "A", "01", 50L));        driver.pipeInput(recordFactory.create(topic1, "B", "01", 10L));        driver.pipeInput(recordFactory.create(topic1, "C", "01", 30L));        assertEquals(ValueAndTimestamp.make(1, 50L), getter2.get("A"));        assertEquals(ValueAndTimestamp.make(1, 10L), getter2.get("B"));        assertEquals(ValueAndTimestamp.make(1, 30L), getter2.get("C"));        assertEquals(ValueAndTimestamp.make(-1, 50L), getter3.get("A"));        assertEquals(ValueAndTimestamp.make(-1, 10L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(-1, 30L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", "02", 25L));        driver.pipeInput(recordFactory.create(topic1, "B", "02", 20L));        assertEquals(ValueAndTimestamp.make(2, 25L), getter2.get("A"));        assertEquals(ValueAndTimestamp.make(2, 20L), getter2.get("B"));        assertEquals(ValueAndTimestamp.make(1, 30L), getter2.get("C"));        assertEquals(ValueAndTimestamp.make(-2, 25L), getter3.get("A"));        assertEquals(ValueAndTimestamp.make(-2, 20L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(-1, 30L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", "03", 35L));        assertEquals(ValueAndTimestamp.make(3, 35L), getter2.get("A"));        assertEquals(ValueAndTimestamp.make(2, 20L), getter2.get("B"));        assertEquals(ValueAndTimestamp.make(1, 30L), getter2.get("C"));        assertEquals(ValueAndTimestamp.make(-3, 35L), getter3.get("A"));        assertEquals(ValueAndTimestamp.make(-2, 20L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(-1, 30L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", (String) null, 1L));        assertNull(getter2.get("A"));        assertEquals(ValueAndTimestamp.make(2, 20L), getter2.get("B"));        assertEquals(ValueAndTimestamp.make(1, 30L), getter2.get("C"));        assertNull(getter3.get("A"));        assertEquals(ValueAndTimestamp.make(-2, 20L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(-1, 30L), getter3.get("C"));    }}
public void kafkatest_f16528_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final String storeName2 = "store2";    final String storeName3 = "store3";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    final KTableImpl<String, String, Integer> table2 = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>as(storeName2).withValueSerde(Serdes.Integer()));    final KTableImpl<String, String, Integer> table3 = (KTableImpl<String, String, Integer>) table1.mapValues(value -> new Integer(value) * (-1), Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>as(storeName3).withValueSerde(Serdes.Integer()));    final KTableImpl<String, String, Integer> table4 = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new);    assertEquals(storeName2, table2.queryableStoreName());    assertEquals(storeName3, table3.queryableStoreName());    assertNull(table4.queryableStoreName());    doTestValueGetter(builder, topic1, table2, table3);}
public void kafkatest_f16537_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    @SuppressWarnings("unchecked")    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, stringConsumed);    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build().addProcessor("proc1", supplier, table1.name);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        final MockProcessor<String, Integer> proc1 = supplier.theCapturedProcessor();        driver.pipeInput(recordFactory.create(topic1, "A", "01", 10L));        driver.pipeInput(recordFactory.create(topic1, "B", "01", 20L));        driver.pipeInput(recordFactory.create(topic1, "C", "01", 15L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("01", null), 10), new KeyValueTimestamp<>("B", new Change<>("01", null), 20), new KeyValueTimestamp<>("C", new Change<>("01", null), 15));        driver.pipeInput(recordFactory.create(topic1, "A", "02", 8L));        driver.pipeInput(recordFactory.create(topic1, "B", "02", 22L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("02", null), 8), new KeyValueTimestamp<>("B", new Change<>("02", null), 22));        driver.pipeInput(recordFactory.create(topic1, "A", "03", 12L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("03", null), 12));        driver.pipeInput(recordFactory.create(topic1, "A", (String) null, 15L));        driver.pipeInput(recordFactory.create(topic1, "B", (String) null, 20L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(null, null), 15), new KeyValueTimestamp<>("B", new Change<>(null, null), 20));    }}
public void kafkatest_f16538_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    @SuppressWarnings("unchecked")    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, stringConsumed);    table1.enableSendingOldValues();    assertTrue(table1.sendingOldValueEnabled());    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build().addProcessor("proc1", supplier, table1.name);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        final MockProcessor<String, Integer> proc1 = supplier.theCapturedProcessor();        driver.pipeInput(recordFactory.create(topic1, "A", "01", 10L));        driver.pipeInput(recordFactory.create(topic1, "B", "01", 20L));        driver.pipeInput(recordFactory.create(topic1, "C", "01", 15L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("01", null), 10), new KeyValueTimestamp<>("B", new Change<>("01", null), 20), new KeyValueTimestamp<>("C", new Change<>("01", null), 15));        driver.pipeInput(recordFactory.create(topic1, "A", "02", 8L));        driver.pipeInput(recordFactory.create(topic1, "B", "02", 22L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("02", "01"), 8), new KeyValueTimestamp<>("B", new Change<>("02", "01"), 22));        driver.pipeInput(recordFactory.create(topic1, "A", "03", 12L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("03", "02"), 12));        driver.pipeInput(recordFactory.create(topic1, "A", (String) null, 15L));        driver.pipeInput(recordFactory.create(topic1, "B", (String) null, 20L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(null, "03"), 15), new KeyValueTimestamp<>("B", new Change<>(null, "02"), 20));    }}
public void kafkatest_f16547_0()
{    final KTableTransformValues<String, String, String> transformValues = new KTableTransformValues<>(parent, new ExclamationValueTransformerSupplier(), null);    expect(parent.valueGetterSupplier()).andReturn(parentGetterSupplier);    expect(parentGetterSupplier.get()).andReturn(parentGetter);    expect(parentGetter.get("Key")).andReturn(ValueAndTimestamp.make("Value", -1L));    replay(parent, parentGetterSupplier, parentGetter);    final KTableValueGetter<String, String> getter = transformValues.view().get();    getter.init(context);    final String result = getter.get("Key").value();    assertThat(result, is("Key->Value!"));}
public void kafkatest_f16548_0()
{    final KTableTransformValues<String, String, String> transformValues = new KTableTransformValues<>(parent, new ExclamationValueTransformerSupplier(), QUERYABLE_NAME);    expect(context.getStateStore(QUERYABLE_NAME)).andReturn(stateStore);    expect(stateStore.get("Key")).andReturn(ValueAndTimestamp.make("something", 0L));    replay(context, stateStore);    final KTableValueGetter<String, String> getter = transformValues.view().get();    getter.init(context);    final String result = getter.get("Key").value();    assertThat(result, is("something"));}
public void kafkatest_f16557_0()
{    builder.table(INPUT_TOPIC, CONSUMED).transformValues(new StatelessTransformerSupplier()).groupBy(toForceSendingOfOldValues(), Grouped.with(Serdes.String(), Serdes.Integer())).reduce(MockReducer.INTEGER_ADDER, MockReducer.INTEGER_SUBTRACTOR).mapValues(mapBackToStrings()).toStream().process(capture);    driver = new TopologyTestDriver(builder.build(), props());    driver.pipeInput(recordFactory.create(INPUT_TOPIC, "A", "a", 5L));    driver.pipeInput(recordFactory.create(INPUT_TOPIC, "A", "aa", 15L));    driver.pipeInput(recordFactory.create(INPUT_TOPIC, "A", "aaa", 10));    assertThat(output(), hasItems(new KeyValueTimestamp<>("A", "1", 5), new KeyValueTimestamp<>("A", "0", 15), new KeyValueTimestamp<>("A", "2", 15), new KeyValueTimestamp<>("A", "0", 15), new KeyValueTimestamp<>("A", "3", 15)));}
private ArrayList<KeyValueTimestamp<Object, Object>> kafkatest_f16558_0()
{    return capture.capturedProcessors(1).get(0).processed;}
public ValueTransformerWithKey<String, String, String> kafkatest_f16568_0()
{    return null;}
public ValueTransformerWithKey<String, String, Integer> kafkatest_f16569_0()
{    return new StatefulTransformer();}
public void kafkatest_f16582_0()
{    final String name = "foo";    final TestNameProvider provider = new TestNameProvider();    assertEquals(name + TEST_SUFFIX, NamedInternal.with(name).suffixWithOrElseGet(TEST_SUFFIX, provider, TEST_PREFIX));    // 1, not 0, indicates that the named call still burned an index number.    assertEquals("prefix-PROCESSOR-1", NamedInternal.with(null).suffixWithOrElseGet(TEST_SUFFIX, provider, TEST_PREFIX));}
public void kafkatest_f16583_0()
{    final String prefix = "KSTREAM-MAP-";    assertEquals(prefix + "PROCESSOR-0", NamedInternal.with(null).orElseGenerateWithPrefix(new TestNameProvider(), prefix));}
public void kafkatest_f16592_0()
{    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    shouldCountSessionWindowed();}
public void kafkatest_f16593_0()
{    shouldCountSessionWindowed();}
public void kafkatest_f16602_0()
{    stream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, null);}
public void kafkatest_f16603_0()
{    stream.reduce(null);}
public void kafkatest_f16612_0()
{    /*         * This:        [-------]         * Other: [---]         */    assertFalse(window.overlap(new SessionWindow(0, 25)));    assertFalse(window.overlap(new SessionWindow(0, start - 1)));    assertFalse(window.overlap(new SessionWindow(start - 1, start - 1)));}
public void kafkatest_f16613_0()
{    /*         * This:        [-------]         * Other: [---------]         */    assertTrue(window.overlap(new SessionWindow(0, start)));    assertTrue(window.overlap(new SessionWindow(0, start + 1)));    assertTrue(window.overlap(new SessionWindow(0, 75)));    assertTrue(window.overlap(new SessionWindow(0, end - 1)));    assertTrue(window.overlap(new SessionWindow(0, end)));    assertTrue(window.overlap(new SessionWindow(start - 1, start)));    assertTrue(window.overlap(new SessionWindow(start - 1, start + 1)));    assertTrue(window.overlap(new SessionWindow(start - 1, 75)));    assertTrue(window.overlap(new SessionWindow(start - 1, end - 1)));    assertTrue(window.overlap(new SessionWindow(start - 1, end)));}
public void kafkatest_f16622_0()
{    final Harness<Windowed<String>, Long> harness = new Harness<>(untilTimeLimit(ZERO, unbounded()), timeWindowedSerdeFrom(String.class, 100L), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = ARBITRARY_LONG;    context.setRecordMetadata("", 0, 0L, null, timestamp);    final Windowed<String> key = new Windowed<>("hey", new TimeWindow(0L, 100L));    final Change<Long> value = ARBITRARY_CHANGE;    harness.processor.process(key, value);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
public void kafkatest_f16623_0()
{    final Harness<String, Long> harness = new Harness<>(untilTimeLimit(ofMillis(1), unbounded()), String(), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 0L;    context.setRecordMetadata("topic", 0, 0, null, timestamp);    final String key = "hey";    final Change<Long> value = new Change<>(null, 1L);    harness.processor.process(key, value);    assertThat(context.forwarded(), hasSize(0));    context.setRecordMetadata("topic", 0, 1, null, 1L);    harness.processor.process("tick", new Change<>(null, null));    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
public void kafkatest_f16632_0()
{    final Harness<String, Long> harness = new Harness<>(untilTimeLimit(Duration.ofDays(100), maxRecords(1)), String(), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 100L;    context.setRecordMetadata("", 0, 0L, null, timestamp);    final String key = "hey";    final Change<Long> value = new Change<>(null, ARBITRARY_LONG);    harness.processor.process(key, value);    context.setRecordMetadata("", 0, 1L, null, timestamp + 1);    harness.processor.process("dummyKey", value);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
public void kafkatest_f16633_0()
{    final Harness<String, Long> harness = new Harness<>(untilTimeLimit(Duration.ofDays(100), maxBytes(60L)), String(), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 100L;    context.setRecordMetadata("", 0, 0L, null, timestamp);    final String key = "hey";    final Change<Long> value = new Change<>(null, ARBITRARY_LONG);    harness.processor.process(key, value);    context.setRecordMetadata("", 0, 1L, null, timestamp + 1);    harness.processor.process("dummyKey", value);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
public void kafkatest_f16642_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<String, Long> valueCounts = builder.table("input", Consumed.with(STRING_SERDE, STRING_SERDE), Materialized.<String, String, KeyValueStore<Bytes, byte[]>>with(STRING_SERDE, STRING_SERDE).withCachingDisabled().withLoggingDisabled()).groupBy((k, v) -> new KeyValue<>(v, k), Grouped.with(STRING_SERDE, STRING_SERDE)).count();    valueCounts.suppress(untilTimeLimit(ofMillis(2L), unbounded())).toStream().to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().to("output-raw", Produced.with(STRING_SERDE, Serdes.Long()));    final Topology topology = builder.build();    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(STRING_SERIALIZER, STRING_SERIALIZER);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, config)) {        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v2", 1L));        driver.pipeInput(recordFactory.create("input", "k2", "v1", 2L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("v1", 1L, 0L), new KeyValueTimestamp<>("v1", 0L, 1L), new KeyValueTimestamp<>("v2", 1L, 1L), new KeyValueTimestamp<>("v1", 1L, 2L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("v1", 1L, 2L)));        // inserting a dummy "tick" record just to advance stream time        driver.pipeInput(recordFactory.create("input", "tick", "tick", 3L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("tick", 1L, 3L)));        // the stream time is now 3, so it's time to emit this record        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("v2", 1L, 1L)));        driver.pipeInput(recordFactory.create("input", "tick", "tick", 4L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("tick", 0L, 4L), new KeyValueTimestamp<>("tick", 1L, 4L)));        // tick is still buffered, since it was first inserted at time 3, and it is only time 4 right now.        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), emptyList());    }}
public void kafkatest_f16643_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<String, Long> valueCounts = builder.table("input", Consumed.with(STRING_SERDE, STRING_SERDE), Materialized.<String, String, KeyValueStore<Bytes, byte[]>>with(STRING_SERDE, STRING_SERDE).withCachingDisabled().withLoggingDisabled()).groupBy((k, v) -> new KeyValue<>(v, k), Grouped.with(STRING_SERDE, STRING_SERDE)).count(Materialized.with(STRING_SERDE, Serdes.Long()));    valueCounts.suppress(untilTimeLimit(ofMillis(Long.MAX_VALUE), maxRecords(1L).emitEarlyWhenFull())).toStream().to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().to("output-raw", Produced.with(STRING_SERDE, Serdes.Long()));    final Topology topology = builder.build();    System.out.println(topology.describe());    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(STRING_SERIALIZER, STRING_SERIALIZER);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, config)) {        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v2", 1L));        driver.pipeInput(recordFactory.create("input", "k2", "v1", 2L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("v1", 1L, 0L), new KeyValueTimestamp<>("v1", 0L, 1L), new KeyValueTimestamp<>("v2", 1L, 1L), new KeyValueTimestamp<>("v1", 1L, 2L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(// consecutive updates to v1 get suppressed into only the latter.        new KeyValueTimestamp<>("v1", 0L, 1L), new KeyValueTimestamp<>("v2", 1L, 1L)));        driver.pipeInput(recordFactory.create("input", "x", "x", 3L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("x", 1L, 3L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(// now we see that last update to v1, but we won't see the update to x until it gets evicted        new KeyValueTimestamp<>("v1", 1L, 2L)));    }}
private static List<ProducerRecord<K, V>> kafkatest_f16652_0(final TopologyTestDriver driver, final String topic, final Deserializer<K> keyDeserializer, final Deserializer<V> valueDeserializer)
{    final List<ProducerRecord<K, V>> result = new LinkedList<>();    for (ProducerRecord<K, V> next = driver.readOutput(topic, keyDeserializer, valueDeserializer); next != null; next = driver.readOutput(topic, keyDeserializer, valueDeserializer)) {        result.add(next);    }    return new ArrayList<>(result);}
private static String kafkatest_f16653_0(final List<ProducerRecord<K, V>> result)
{    final StringBuilder resultStr = new StringBuilder();    resultStr.append("[\n");    for (final ProducerRecord<?, ?> record : result) {        resultStr.append("  ").append(record).append("\n");    }    resultStr.append("]");    return resultStr.toString();}
public void kafkatest_f16662_0()
{    shouldForwardRecordsIfWrappedStateStoreDoesNotCache(false);    shouldForwardRecordsIfWrappedStateStoreDoesNotCache(true);}
private void kafkatest_f16663_0(final boolean sendOldValues)
{    final WrappedStateStore<StateStore, String, String> store = mock(WrappedStateStore.class);    final ProcessorContext context = mock(ProcessorContext.class);    expect(store.setFlushListener(null, sendOldValues)).andReturn(false);    if (sendOldValues) {        context.forward("key1", new Change<>("newValue1", "oldValue1"));        context.forward("key2", new Change<>("newValue2", "oldValue2"), To.all().withTimestamp(42L));    } else {        context.forward("key1", new Change<>("newValue1", null));        context.forward("key2", new Change<>("newValue2", null), To.all().withTimestamp(42L));    }    expectLastCall();    replay(store, context);    final TimestampedTupleForwarder<String, String> forwarder = new TimestampedTupleForwarder<>(store, context, null, sendOldValues);    forwarder.maybeForward("key1", "newValue1", "oldValue1");    forwarder.maybeForward("key2", "newValue2", "oldValue2", 42L);    verify(store, context);}
public void kafkatest_f16672_0()
{    windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER);}
public void kafkatest_f16673_0()
{    windowedStream.aggregate(MockInitializer.STRING_INIT, null);}
public void kafkatest_f16682_0()
{    new TimeWindow(start, start);}
public void kafkatest_f16683_0()
{    /*         * This:        [-------)         * Other: [-----)         */    assertFalse(window.overlap(new TimeWindow(0, 25)));    assertFalse(window.overlap(new TimeWindow(0, start - 1)));    assertFalse(window.overlap(new TimeWindow(0, start)));}
public void kafkatest_f16692_0()
{    EasyMock.expect(transformerSupplier.get()).andReturn(transformer);    transformer.init(context);    replayAll();    final TransformerSupplierAdapter<String, String, Integer, Integer> adapter = new TransformerSupplierAdapter<>(transformerSupplier);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adaptedTransformer = adapter.get();    adaptedTransformer.init(context);    verifyAll();}
public void kafkatest_f16693_0()
{    EasyMock.expect(transformerSupplier.get()).andReturn(transformer);    transformer.close();    replayAll();    final TransformerSupplierAdapter<String, String, Integer, Integer> adapter = new TransformerSupplierAdapter<>(transformerSupplier);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adaptedTransformer = adapter.get();    adaptedTransformer.close();    verifyAll();}
public void kafkatest_f16702_0()
{    final JoinWindows windowSpec = JoinWindows.of(ofMillis(ANY_SIZE));    try {        windowSpec.after(ofMillis(-ANY_SIZE - 1));        fail("window end time should not be before window start time");    } catch (final IllegalArgumentException e) {    // expected    }}
public void kafkatest_f16703_0()
{    final JoinWindows windowSpec = JoinWindows.of(ofMillis(ANY_SIZE));    try {        windowSpec.before(ofMillis(-ANY_SIZE - 1));        fail("window start time should not be after window end time");    } catch (final IllegalArgumentException e) {    // expected    }}
public void kafkatest_f16712_0()
{    Materialized.as((KeyValueBytesStoreSupplier) null);}
public void kafkatest_f16713_0()
{    Materialized.as((SessionBytesStoreSupplier) null);}
public String kafkatest_f16722_0(final String key, final Integer value)
{    return String.format("%s -> %d", key, value);}
public void kafkatest_f16723_0()
{    Printed.toFile(null);}
public void kafkatest_f16732_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KGroupedStream<String, String> kGroupedStream = builder.<String, String>stream("topic").selectKey((k, v) -> k).groupByKey(Grouped.as("grouping"));    final SessionWindowedKStream<String, String> sessionWindowedKStream = kGroupedStream.windowedBy(SessionWindows.with(Duration.ofMillis(10L)));    sessionWindowedKStream.count().toStream().to("output-one");    sessionWindowedKStream.reduce((v, v2) -> v + v2).toStream().to("output-two");    kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(30L))).count().toStream().to("output-two");    final String topologyString = builder.build().describe().toString();    assertThat(1, is(getCountOfRepartitionTopicsFound(topologyString, repartitionTopicPattern)));    assertTrue(topologyString.contains("grouping-repartition"));}
public void kafkatest_f16733_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KGroupedTable<String, String> kGroupedTable = builder.<String, String>table("topic").groupBy(KeyValue::pair, Grouped.as("grouping"));    kGroupedTable.count().toStream().to("output-count");    kGroupedTable.reduce((v, v2) -> v2, (v, v2) -> v2).toStream().to("output-reduce");    final String topologyString = builder.build().describe().toString();    assertThat(1, is(getCountOfRepartitionTopicsFound(topologyString, repartitionTopicPattern)));    assertTrue(topologyString.contains("grouping-repartition"));}
public void kafkatest_f16742_0()
{    final String expectedNoWindowRepartitionTopic = "(topic: kstream-grouping-repartition)";    final String noWindowGroupingRepartitionTopology = buildStreamGroupByKeyNoWindows(false, true);    assertTrue(noWindowGroupingRepartitionTopology.contains(expectedNoWindowRepartitionTopic));    final String noWindowGroupingUpdatedTopology = buildStreamGroupByKeyNoWindows(true, true);    assertTrue(noWindowGroupingUpdatedTopology.contains(expectedNoWindowRepartitionTopic));}
public void kafkatest_f16743_0()
{    final String expectedNoWindowRepartitionTopic = "(topic: kstream-grouping-repartition)";    final String noWindowGroupingRepartitionTopology = buildStreamGroupByKeyNoWindows(false, false);    assertTrue(noWindowGroupingRepartitionTopology.contains(expectedNoWindowRepartitionTopic));    final String noWindowGroupingUpdatedTopology = buildStreamGroupByKeyNoWindows(true, false);    assertTrue(noWindowGroupingUpdatedTopology.contains(expectedNoWindowRepartitionTopic));}
private int kafkatest_f16752_0(final String topologyString, final Pattern repartitionTopicPattern)
{    final Matcher matcher = repartitionTopicPattern.matcher(topologyString);    final List<String> repartitionTopicsFound = new ArrayList<>();    while (matcher.find()) {        repartitionTopicsFound.add(matcher.group());    }    return repartitionTopicsFound.size();}
private Topology kafkatest_f16753_0(final String optimizationConfig)
{    final Initializer<Integer> initializer = () -> 0;    final Aggregator<String, String, Integer> aggregator = (k, v, agg) -> agg + v.length();    final Reducer<String> reducer = (v1, v2) -> v1 + ":" + v2;    final List<String> processorValueCollector = new ArrayList<>();    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> sourceStream = builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()));    final KStream<String, String> mappedStream = sourceStream.map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v));    mappedStream.filter((k, v) -> k.equals("B")).mapValues(v -> v.toUpperCase(Locale.getDefault())).process(() -> new SimpleProcessor(processorValueCollector));    final KStream<String, Long> countStream = mappedStream.groupByKey(Grouped.as(firstRepartitionTopicName)).count(Materialized.with(Serdes.String(), Serdes.Long())).toStream();    countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()));    mappedStream.groupByKey(Grouped.as(secondRepartitionTopicName)).aggregate(initializer, aggregator, Materialized.with(Serdes.String(), Serdes.Integer())).toStream().to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()));    // adding operators for case where the repartition node is further downstream    mappedStream.filter((k, v) -> true).peek((k, v) -> System.out.println(k + ":" + v)).groupByKey(Grouped.as(thirdRepartitionTopicName)).reduce(reducer, Materialized.with(Serdes.String(), Serdes.String())).toStream().to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));    mappedStream.filter((k, v) -> k.equals("A")).join(countStream, (v1, v2) -> v1 + ":" + v2.toString(), JoinWindows.of(Duration.ofMillis(5000L)), Joined.with(Serdes.String(), Serdes.String(), Serdes.Long(), fourthRepartitionTopicName)).to(JOINED_TOPIC);    final Properties properties = new Properties();    properties.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);    return builder.build(properties);}
public void kafkatest_f16762_0()
{    final long anyRetentionTime = 42L;    assertEquals(anyRetentionTime, SessionWindows.with(ofMillis(1)).grace(ofMillis(anyRetentionTime)).gracePeriodMs());}
public void kafkatest_f16763_0()
{    SessionWindows.with(ofMillis(3L)).grace(ofMillis(0));    try {        SessionWindows.with(ofMillis(3L)).grace(ofMillis(-1L));        fail("should not accept negatives");    } catch (final IllegalArgumentException e) {    // expected    }}
public void kafkatest_f16772_0()
{    assertThat(untilWindowCloses(unbounded()), is(new FinalResultsSuppressionBuilder<>(null, unbounded())));    assertThat(untilWindowCloses(maxRecords(2L).shutDownWhenFull()), is(new FinalResultsSuppressionBuilder<>(null, new StrictBufferConfigImpl(2L, MAX_VALUE, SHUT_DOWN))));    assertThat(untilWindowCloses(maxBytes(2L).shutDownWhenFull()), is(new FinalResultsSuppressionBuilder<>(null, new StrictBufferConfigImpl(MAX_VALUE, 2L, SHUT_DOWN))));    assertThat(untilWindowCloses(unbounded()).withName("name"), is(new FinalResultsSuppressionBuilder<>("name", unbounded())));    assertThat(untilWindowCloses(maxRecords(2L).shutDownWhenFull()).withName("name"), is(new FinalResultsSuppressionBuilder<>("name", new StrictBufferConfigImpl(2L, MAX_VALUE, SHUT_DOWN))));    assertThat(untilWindowCloses(maxBytes(2L).shutDownWhenFull()).withName("name"), is(new FinalResultsSuppressionBuilder<>("name", new StrictBufferConfigImpl(MAX_VALUE, 2L, SHUT_DOWN))));}
public void kafkatest_f16773_0()
{    props.put(StreamsConfig.DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS, Serdes.StringSerde.class.getName());    props.put(StreamsConfig.DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS, Serdes.ByteArraySerde.class.getName());}
public void kafkatest_f16782_0()
{    final long windowSize = 2 * TimeWindows.of(ofMillis(1)).maintainMs();    assertEquals(windowSize, TimeWindows.of(ofMillis(windowSize)).maintainMs());}
public void kafkatest_f16783_0()
{    TimeWindows.of(ofMillis(0));}
public void kafkatest_f16792_0()
{    final TimeWindows windows = TimeWindows.of(ofMillis(12L));    final Map<Long, TimeWindow> matched = windows.windowsFor(21L);    assertEquals(1, matched.size());    assertEquals(new TimeWindow(12L, 24L), matched.get(12L));}
public void kafkatest_f16793_0()
{    verifyEquality(TimeWindows.of(ofMillis(3)), TimeWindows.of(ofMillis(3)));    verifyEquality(TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)), TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)));    verifyEquality(TimeWindows.of(ofMillis(3)).grace(ofMillis(1)), TimeWindows.of(ofMillis(3)).grace(ofMillis(1)));    verifyEquality(TimeWindows.of(ofMillis(3)).grace(ofMillis(4)), TimeWindows.of(ofMillis(3)).grace(ofMillis(4)));    verifyEquality(TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)).grace(ofMillis(1)).grace(ofMillis(4)), TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)).grace(ofMillis(1)).grace(ofMillis(4)));}
public void kafkatest_f16802_0()
{    verifyInEquality(UnlimitedWindows.of().startOn(ofEpochMilli(9)), UnlimitedWindows.of().startOn(ofEpochMilli(1)));}
public void kafkatest_f16803_0()
{    final Serde<Windowed<String>> serde = WindowedSerdes.timeWindowedSerdeFrom(String.class);    assertTrue(serde.serializer() instanceof TimeWindowedSerializer);    assertTrue(serde.deserializer() instanceof TimeWindowedDeserializer);    assertTrue(((TimeWindowedSerializer) serde.serializer()).innerSerializer() instanceof StringSerializer);    assertTrue(((TimeWindowedDeserializer) serde.deserializer()).innerDeserializer() instanceof StringDeserializer);}
public void kafkatest_f16812_0()
{    final SessionWindowedDeserializer<byte[]> deserializer = new SessionWindowedDeserializer<>();    final NullPointerException exception = assertThrows(NullPointerException.class, () -> deserializer.deserialize("topic", new byte[0]));    assertThat(exception.getMessage(), equalTo("Inner deserializer is `null`. User code must use constructor " + "`SessionWindowedDeserializer(final Deserializer<T> inner)` instead of the no-arg constructor."));}
public void kafkatest_f16813_0()
{    new TimeWindowedSerializer<>().close();}
public void kafkatest_f16822_0()
{    new TestWindows().until(-1);}
public boolean kafkatest_f16823_0(final Window other)
{    return false;}
private void kafkatest_f16832_0()
{    switch(testName) {        // loading phases        case "load-one":            produce(LOADING_PRODUCER_CLIENT_ID, SOURCE_TOPIC_ONE, numRecords, keySkew, valueSize);            break;        case "load-two":            produce(LOADING_PRODUCER_CLIENT_ID, SOURCE_TOPIC_ONE, numRecords, keySkew, valueSize);            produce(LOADING_PRODUCER_CLIENT_ID, SOURCE_TOPIC_TWO, numRecords, keySkew, valueSize);            break;        // testing phases        case "consume":            consume(SOURCE_TOPIC_ONE);            break;        case "consumeproduce":            consumeAndProduce(SOURCE_TOPIC_ONE);            break;        case "streamcount":            countStreamsNonWindowed(SOURCE_TOPIC_ONE);            break;        case "streamcountwindowed":            countStreamsWindowed(SOURCE_TOPIC_ONE);            break;        case "streamprocess":            processStream(SOURCE_TOPIC_ONE);            break;        case "streamprocesswithsink":            processStreamWithSink(SOURCE_TOPIC_ONE);            break;        case "streamprocesswithstatestore":            processStreamWithStateStore(SOURCE_TOPIC_ONE);            break;        case "streamprocesswithwindowstore":            processStreamWithWindowStore(SOURCE_TOPIC_ONE);            break;        case "streamtablejoin":            streamTableJoin(SOURCE_TOPIC_ONE, SOURCE_TOPIC_TWO);            break;        case "streamstreamjoin":            streamStreamJoin(SOURCE_TOPIC_ONE, SOURCE_TOPIC_TWO);            break;        case "tabletablejoin":            tableTableJoin(SOURCE_TOPIC_ONE, SOURCE_TOPIC_TWO);            break;        case "yahoo":            yahooBenchmark(YAHOO_CAMPAIGNS_TOPIC, YAHOO_EVENTS_TOPIC);            break;        default:            throw new RuntimeException("Unknown test name " + testName);    }}
public static void kafkatest_f16833_0(final String[] args) throws IOException
{    if (args.length < 5) {        System.err.println("Not enough parameters are provided; expecting propFileName, testName, numRecords, keySkew, valueSize");        System.exit(1);    }    final String propFileName = args[0];    final String testName = args[1].toLowerCase(Locale.ROOT);    final int numRecords = Integer.parseInt(args[2]);    // 0d means even distribution    final double keySkew = Double.parseDouble(args[3]);    final int valueSize = Integer.parseInt(args[4]);    final Properties props = Utils.loadProps(propFileName);    final String kafka = props.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);    if (kafka == null) {        System.err.println("No bootstrap kafka servers specified in " + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);        System.exit(1);    }    // Note: this output is needed for automated tests and must not be removed    System.out.println("StreamsTest instance started");    System.out.println("testName=" + testName);    System.out.println("streamsProperties=" + props);    System.out.println("numRecords=" + numRecords);    System.out.println("keySkew=" + keySkew);    System.out.println("valueSize=" + valueSize);    final SimpleBenchmark benchmark = new SimpleBenchmark(props, testName, numRecords, keySkew, valueSize);    benchmark.run();}
private void kafkatest_f16842_0(final String topic)
{    final CountDownLatch latch = new CountDownLatch(1);    setStreamProperties("simple-benchmark-streams-with-store");    final StreamsBuilder builder = new StreamsBuilder();    final StoreBuilder<KeyValueStore<Integer, byte[]>> storeBuilder = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore("store"), INTEGER_SERDE, BYTE_SERDE);    builder.addStateStore(storeBuilder.withCachingEnabled());    final KStream<Integer, byte[]> source = builder.stream(topic);    source.peek(new CountDownAction(latch)).process(new ProcessorSupplier<Integer, byte[]>() {        @Override        public Processor<Integer, byte[]> get() {            return new AbstractProcessor<Integer, byte[]>() {                KeyValueStore<Integer, byte[]> store;                @SuppressWarnings("unchecked")                @Override                public void init(final ProcessorContext context) {                    super.init(context);                    store = (KeyValueStore<Integer, byte[]>) context.getStateStore("store");                }                @Override                public void process(final Integer key, final byte[] value) {                    store.get(key);                    store.put(key, value);                }            };        }    }, "store");    final KafkaStreams streams = createKafkaStreamsWithExceptionHandler(builder, props);    runGenericBenchmark(streams, "Streams Stateful Performance [records/latency/rec-sec/MB-sec joined]: ", latch);}
public Processor<Integer, byte[]> kafkatest_f16843_0()
{    return new AbstractProcessor<Integer, byte[]>() {        KeyValueStore<Integer, byte[]> store;        @SuppressWarnings("unchecked")        @Override        public void init(final ProcessorContext context) {            super.init(context);            store = (KeyValueStore<Integer, byte[]>) context.getStateStore("store");        }        @Override        public void process(final Integer key, final byte[] value) {            store.get(key);            store.put(key, value);        }    };}
private void kafkatest_f16852_0(final String kStreamTopic, final String kTableTopic)
{    final CountDownLatch latch = new CountDownLatch(1);    setStreamProperties("simple-benchmark-stream-table-join");    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Integer, byte[]> input1 = builder.stream(kStreamTopic);    final KTable<Integer, byte[]> input2 = builder.table(kTableTopic);    input1.leftJoin(input2, VALUE_JOINER).foreach(new CountDownAction(latch));    final KafkaStreams streams = createKafkaStreamsWithExceptionHandler(builder, props);    // run benchmark    runGenericBenchmark(streams, "Streams KStreamKTable LeftJoin Performance [records/latency/rec-sec/MB-sec joined]: ", latch);}
private void kafkatest_f16853_0(final String kStreamTopic1, final String kStreamTopic2)
{    final CountDownLatch latch = new CountDownLatch(1);    setStreamProperties("simple-benchmark-stream-stream-join");    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Integer, byte[]> input1 = builder.stream(kStreamTopic1);    final KStream<Integer, byte[]> input2 = builder.stream(kStreamTopic2);    input1.leftJoin(input2, VALUE_JOINER, JoinWindows.of(ofMillis(STREAM_STREAM_JOIN_WINDOW))).foreach(new CountDownAction(latch));    final KafkaStreams streams = createKafkaStreamsWithExceptionHandler(builder, props);    // run benchmark    runGenericBenchmark(streams, "Streams KStreamKStream LeftJoin Performance [records/latency/rec-sec/MB-sec  joined]: ", latch);}
private List<TopicPartition> kafkatest_f16862_0(final KafkaConsumer<?, ?> consumer, final String... topics)
{    final ArrayList<TopicPartition> partitions = new ArrayList<>();    for (final String topic : topics) {        for (final PartitionInfo info : consumer.partitionsFor(topic)) {            partitions.add(new TopicPartition(info.topic(), info.partition()));        }    }    return partitions;}
private void kafkatest_f16863_0(final String campaignsTopic, final String eventsTopic)
{    final YahooBenchmark benchmark = new YahooBenchmark(this, campaignsTopic, eventsTopic);    benchmark.run();}
public void kafkatest_f16872_0()
{    final PartitionGrouper grouper = new DefaultPartitionGrouper();    final Map<TaskId, Set<TopicPartition>> expectedPartitionsForTask = new HashMap<>();    final Map<Integer, Set<String>> topicGroups = new HashMap<>();    int topicGroupId = 0;    topicGroups.put(topicGroupId, mkSet("topic1"));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 0), mkSet(new TopicPartition("topic1", 0)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 1), mkSet(new TopicPartition("topic1", 1)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 2), mkSet(new TopicPartition("topic1", 2)));    topicGroups.put(++topicGroupId, mkSet("topic2"));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 0), mkSet(new TopicPartition("topic2", 0)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 1), mkSet(new TopicPartition("topic2", 1)));    assertEquals(expectedPartitionsForTask, grouper.partitionGroups(topicGroups, metadata));}
public void kafkatest_f16873_0()
{    final PartitionGrouper grouper = new DefaultPartitionGrouper();    final Map<TaskId, Set<TopicPartition>> expectedPartitionsForTask = new HashMap<>();    final Map<Integer, Set<String>> topicGroups = new HashMap<>();    final int topicGroupId = 0;    topicGroups.put(topicGroupId, mkSet("topic1", "topic2"));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 0), mkSet(new TopicPartition("topic1", 0), new TopicPartition("topic2", 0)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 1), mkSet(new TopicPartition("topic1", 1), new TopicPartition("topic2", 1)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 2), mkSet(new TopicPartition("topic1", 2)));    assertEquals(expectedPartitionsForTask, grouper.partitionGroups(topicGroups, metadata));}
public void kafkatest_f16882_0()
{    assertThat(context.topic(), equalTo(recordContext.topic()));}
public void kafkatest_f16883_0()
{    context.setRecordContext(new ProcessorRecordContext(0, 0, 0, AbstractProcessorContext.NONEXIST_TOPIC, null));    assertThat(context.topic(), nullValue());}
public void kafkatest_f16892_0()
{    context.setRecordContext(null);    try {        context.headers();    } catch (final IllegalStateException e) {    // pass    }}
public void kafkatest_f16893_0()
{    assertThat(context.appConfigs().get(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG), equalTo(RocksDBConfigSetter.class));}
private AbstractTask kafkatest_f16907_0(final Consumer consumer, final Map<StateStore, String> stateStoresToChangelogTopics)
{    return createTask(consumer, stateStoresToChangelogTopics, stateDirectory);}
private AbstractTask kafkatest_f16908_0(final Consumer consumer, final Map<StateStore, String> stateStoresToChangelogTopics, final StateDirectory stateDirectory)
{    final Properties properties = new Properties();    properties.put(StreamsConfig.APPLICATION_ID_CONFIG, "app");    properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummyhost:9092");    final StreamsConfig config = new StreamsConfig(properties);    final Map<String, String> storeNamesToChangelogTopics = new HashMap<>(stateStoresToChangelogTopics.size());    for (final Map.Entry<StateStore, String> e : stateStoresToChangelogTopics.entrySet()) {        storeNamesToChangelogTopics.put(e.getKey().name(), e.getValue());    }    return new AbstractTask(id, storeTopicPartitions, withLocalStores(new ArrayList<>(stateStoresToChangelogTopics.keySet()), storeNamesToChangelogTopics), consumer, new StoreChangelogReader(consumer, Duration.ZERO, new MockStateRestoreListener(), new LogContext("stream-task-test ")), false, stateDirectory, config) {        @Override        public void resume() {        }        @Override        public void commit() {        }        @Override        public void suspend() {        }        @Override        public void close(final boolean clean, final boolean isZombie) {        }        @Override        public void closeSuspended(final boolean clean, final boolean isZombie, final RuntimeException e) {        }        @Override        public boolean initializeStateStores() {            return false;        }        @Override        public void initializeTopology() {        }    };}
public void kafkatest_f16923_0()
{    t1.close(false, false);    EasyMock.expectLastCall();    EasyMock.replay(t1);    assignedTasks.addNewTask(t1);    assertThat(assignedTasks.suspend(), nullValue());    EasyMock.verify(t1);}
public void kafkatest_f16924_0()
{    mockRunningTaskSuspension();    EasyMock.replay(t1);    assertThat(suspendTask(), nullValue());    assertThat(assignedTasks.suspend(), nullValue());    EasyMock.verify(t1);}
public void kafkatest_f16933_0()
{    mockTaskInitialization();    EasyMock.expect(t1.commitRequested()).andReturn(true);    EasyMock.expect(t1.commitNeeded()).andReturn(true);    t1.commit();    EasyMock.expectLastCall();    EasyMock.replay(t1);    addAndInitTask();    assertThat(assignedTasks.maybeCommitPerUserRequested(), equalTo(1));    EasyMock.verify(t1);}
public void kafkatest_f16934_0()
{    mockTaskInitialization();    EasyMock.expect(t1.commitRequested()).andReturn(true);    EasyMock.expect(t1.commitNeeded()).andReturn(true);    t1.commit();    EasyMock.expectLastCall().andThrow(new TaskMigratedException());    t1.close(false, true);    EasyMock.expectLastCall();    EasyMock.replay(t1);    addAndInitTask();    try {        assignedTasks.maybeCommitPerUserRequested();        fail("Should have thrown TaskMigratedException.");    } catch (final TaskMigratedException expected) {    /* ignore */    }    assertThat(assignedTasks.runningTaskIds(), equalTo(Collections.EMPTY_SET));    EasyMock.verify(t1);}
private void kafkatest_f16943_0()
{    assignedTasks.addNewTask(t1);    assignedTasks.initializeNewTasks();}
private RuntimeException kafkatest_f16944_0()
{    addAndInitTask();    return assignedTasks.suspend();}
public void kafkatest_f16953_0()
{    final AssignmentInfo info = new AssignmentInfo(5, activeTasks, standbyTasks, globalAssignment, 2);    final AssignmentInfo expectedInfo = new AssignmentInfo(5, LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, globalAssignment, 2);    assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));}
public void kafkatest_f16954_0()
{    assertFalse(client.reachedCapacity());}
public void kafkatest_f16963_0()
{    final ClientState c2 = new ClientState(1);    client.assign(new TaskId(0, 1), true);    assertTrue(c2.hasMoreAvailableCapacityThan(client));    assertFalse(client.hasMoreAvailableCapacityThan(c2));}
public void kafkatest_f16964_0()
{    final ClientState c2 = new ClientState(2);    assertTrue(c2.hasMoreAvailableCapacityThan(client));    assertFalse(client.hasMoreAvailableCapacityThan(c2));}
public void kafkatest_f16973_0()
{    createClient(p1, 2);    createClient(p2, 2);    createClient(p3, 2);    final StickyTaskAssignor taskAssignor = createTaskAssignor(task20, task11, task12, task10, task21, task22);    taskAssignor.assign(1);    assertActiveTaskTopicGroupIdsEvenlyDistributed();}
public void kafkatest_f16974_0()
{    createClientWithPreviousActiveTasks(p1, 1, task00);    createClientWithPreviousActiveTasks(p2, 1, task01);    final StickyTaskAssignor firstAssignor = createTaskAssignor(task00, task01, task02);    firstAssignor.assign(0);    assertThat(clients.get(p1).activeTasks(), hasItems(task00));    assertThat(clients.get(p2).activeTasks(), hasItems(task01));    assertThat(allActiveTasks(), equalTo(Arrays.asList(task00, task01, task02)));    clients.clear();    // flip the previous active tasks assignment around.    createClientWithPreviousActiveTasks(p1, 1, task01);    createClientWithPreviousActiveTasks(p2, 1, task02);    final StickyTaskAssignor secondAssignor = createTaskAssignor(task00, task01, task02);    secondAssignor.assign(0);    assertThat(clients.get(p1).activeTasks(), hasItems(task01));    assertThat(clients.get(p2).activeTasks(), hasItems(task02));    assertThat(allActiveTasks(), equalTo(Arrays.asList(task00, task01, task02)));}
public void kafkatest_f16983_0()
{    createClient(p1, 1);    final StickyTaskAssignor taskAssignor = createTaskAssignor(task00);    taskAssignor.assign(1);    assertThat(clients.get(p1).standbyTasks().size(), equalTo(0));}
public void kafkatest_f16984_0()
{    createClient(p1, 1);    createClient(p2, 1);    createClient(p3, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task01, task02);    taskAssignor.assign(1);    assertThat(allActiveTasks(), equalTo(Arrays.asList(task00, task01, task02)));    assertThat(allStandbyTasks(), equalTo(Arrays.asList(task00, task01, task02)));}
public void kafkatest_f16993_0()
{    createClientWithPreviousActiveTasks(p3, 1, task00, task01, task02, task03);    createClient(p1, 1);    createClient(p2, 1);    createClient(p4, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task02, task01, task03);    taskAssignor.assign(0);    assertThat(clients.get(p1).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p2).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p3).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p4).assignedTaskCount(), equalTo(1));}
public void kafkatest_f16994_0()
{    createClientWithPreviousActiveTasks(p3, 1, task00, task01, task02, task03);    createClient(p1, 1);    createClient(p2, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task02, task01, task03);    taskAssignor.assign(0);    assertThat(clients.get(p3).assignedTaskCount(), equalTo(2));    assertThat(clients.get(p1).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p2).assignedTaskCount(), equalTo(1));}
public void kafkatest_f17003_0()
{    final TaskId task06 = new TaskId(0, 6);    final ClientState c1 = createClientWithPreviousActiveTasks(p1, 1, task00, task01, task02, task06);    final ClientState c2 = createClient(p2, 1);    c2.addPreviousStandbyTasks(Utils.mkSet(task03, task04, task05));    final ClientState newClient = createClient(p3, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task01, task02, task03, task04, task05, task06);    taskAssignor.assign(0);    assertThat(c1.activeTasks(), not(hasItem(task03)));    assertThat(c1.activeTasks(), not(hasItem(task04)));    assertThat(c1.activeTasks(), not(hasItem(task05)));    assertThat(c1.activeTaskCount(), equalTo(3));    assertThat(c2.activeTasks(), not(hasItems(task00)));    assertThat(c2.activeTasks(), not(hasItems(task01)));    assertThat(c2.activeTasks(), not(hasItems(task02)));    assertThat(c2.activeTaskCount(), equalTo(2));    assertThat(newClient.activeTaskCount(), equalTo(2));}
private StickyTaskAssignor<Integer> kafkatest_f17004_0(final TaskId... tasks)
{    final List<TaskId> taskIds = Arrays.asList(tasks);    Collections.shuffle(taskIds);    return new StickyTaskAssignor<>(clients, new HashSet<>(taskIds));}
public void kafkatest_f17013_0()
{    new SubscriptionInfo(0, processId, activeTasks, standbyTasks, "localhost:80");}
public void kafkatest_f17014_0()
{    new SubscriptionInfo(LATEST_SUPPORTED_VERSION + 1, processId, activeTasks, standbyTasks, "localhost:80");}
public void kafkatest_f17023_0()
{    setUpCompositeRestoreListener(stateRestoreCallback);    compositeRestoreListener.onRestoreStart(topicPartition, storeName, startOffset, endOffset);    assertStateRestoreListenerOnStartNotification(stateRestoreCallback);    assertStateRestoreListenerOnStartNotification(reportingStoreListener);}
public void kafkatest_f17024_0()
{    setUpCompositeRestoreListener(batchingStateRestoreCallback);    compositeRestoreListener.onRestoreStart(topicPartition, storeName, startOffset, endOffset);    assertStateRestoreListenerOnStartNotification(batchingStateRestoreCallback);    assertStateRestoreListenerOnStartNotification(reportingStoreListener);}
private void kafkatest_f17033_0(final MockStateRestoreListener restoreListener)
{    assertTrue(restoreListener.storeNameCalledStates.containsKey(RESTORE_START));    assertThat(restoreListener.restoreTopicPartition, is(topicPartition));    assertThat(restoreListener.restoreStartOffset, is(startOffset));    assertThat(restoreListener.restoreEndOffset, is(endOffset));}
private void kafkatest_f17034_0(final MockStateRestoreListener restoreListener)
{    assertTrue(restoreListener.storeNameCalledStates.containsKey(RESTORE_BATCH));    assertThat(restoreListener.restoreTopicPartition, is(topicPartition));    assertThat(restoreListener.restoredBatchOffset, is(batchOffset));    assertThat(restoreListener.numBatchRestored, is(numberRestored));}
public void kafkatest_f17043_0()
{    final InternalTopicConfig config = createTopicConfig("repartitioned", 10);    validator.enforce(Utils.mkSet("first", "second", config.name()), Collections.singletonMap(config.name(), config), cluster.withPartitions(partitions));    assertThat(config.numberOfPartitions(), equalTo(Optional.of(2)));}
public void kafkatest_f17044_0()
{    final InternalTopicConfig one = createTopicConfig("one", 1);    final InternalTopicConfig two = createTopicConfig("two", 15);    final InternalTopicConfig three = createTopicConfig("three", 5);    final Map<String, InternalTopicConfig> repartitionTopicConfig = new HashMap<>();    repartitionTopicConfig.put(one.name(), one);    repartitionTopicConfig.put(two.name(), two);    repartitionTopicConfig.put(three.name(), three);    validator.enforce(Utils.mkSet(one.name(), two.name(), three.name()), repartitionTopicConfig, cluster);    assertThat(one.numberOfPartitions(), equalTo(Optional.of(15)));    assertThat(two.numberOfPartitions(), equalTo(Optional.of(15)));    assertThat(three.numberOfPartitions(), equalTo(Optional.of(15)));}
public void kafkatest_f17053_0()
{    child.process(null, null);    expectLastCall();    replay(child, recordContext);    globalContext.forward(null, null);    verify(child, recordContext);}
public void kafkatest_f17054_0()
{    globalContext.forward(null, null, To.all());}
public void kafkatest_f17063_0()
{    final StateStore store = globalContext.getStateStore(GLOBAL_TIMESTAMPED_WINDOW_STORE_NAME);    try {        store.init(null, null);        fail("Should have thrown UnsupportedOperationException.");    } catch (final UnsupportedOperationException expected) {    }}
public void kafkatest_f17064_0()
{    final StateStore store = globalContext.getStateStore(GLOBAL_SESSION_STORE_NAME);    try {        store.init(null, null);        fail("Should have thrown UnsupportedOperationException.");    } catch (final UnsupportedOperationException expected) {    }}
public void kafkatest_f17073_0()
{    stateManager.initialize();    assertTrue(new File(stateDirectory.globalStateDir(), ".lock").exists());}
public void kafkatest_f17074_0() throws IOException
{    final StateDirectory stateDir = new StateDirectory(streamsConfig, time, true);    try {        stateDir.lockGlobalState();        stateManager.initialize();    } finally {        stateDir.unlockGlobalState();    }}
public void kafkatest_f17083_0()
{    initializeConsumer(1, 0, t1);    stateManager.initialize();    stateManager.register(store1, stateRestoreCallback);    final KeyValue<byte[], byte[]> restoredRecord = stateRestoreCallback.restored.get(0);    assertEquals(3, restoredRecord.key.length);    assertEquals(5, restoredRecord.value.length);}
public void kafkatest_f17084_0()
{    initializeConsumer(1, 0, t1);    stateManager.initialize();    stateManager.register(new WrappedStateStore<NoOpReadOnlyStore<Object, Object>, Object, Object>(store1) {    }, stateRestoreCallback);    final KeyValue<byte[], byte[]> restoredRecord = stateRestoreCallback.restored.get(0);    assertEquals(3, restoredRecord.key.length);    assertEquals(5, restoredRecord.value.length);}
public void kafkatest_f17093_0()
{    stateManager.initialize();    // register the stores    initializeConsumer(1, 0, t1);    stateManager.register(new NoOpReadOnlyStore(store1.name()) {        @Override        public void flush() {            throw new RuntimeException("KABOOM!");        }    }, stateRestoreCallback);    stateManager.flush();}
public void kafkatest_f17094_0()
{    throw new RuntimeException("KABOOM!");}
public void kafkatest_f17103_0()
{    super.close();    throw new RuntimeException("KABOOM!");}
public void kafkatest_f17104_0() throws IOException
{    writeCorruptCheckpoint();    try {        stateManager.initialize();    } catch (final StreamsException e) {    // expected    }    final StateDirectory stateDir = new StateDirectory(streamsConfig, new MockTime(), true);    try {        // should be able to get the lock now as it should've been released        assertTrue(stateDir.lockGlobalState());    } finally {        stateDir.unlockGlobalState();    }}
public void kafkatest_f17113_0()
{    final int retries = 2;    final AtomicInteger numberOfCalls = new AtomicInteger(0);    consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {        @Override        public synchronized Map<TopicPartition, Long> endOffsets(final Collection<org.apache.kafka.common.TopicPartition> partitions) {            numberOfCalls.incrementAndGet();            throw new TimeoutException();        }    };    streamsConfig = new StreamsConfig(new Properties() {        {            put(StreamsConfig.APPLICATION_ID_CONFIG, "appId");            put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummy:1234");            put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());            put(StreamsConfig.RETRIES_CONFIG, retries);        }    });    try {        new GlobalStateManagerImpl(new LogContext("mock"), topology, consumer, stateDirectory, stateRestoreListener, streamsConfig);    } catch (final StreamsException expected) {        assertEquals(numberOfCalls.get(), retries);    }}
public synchronized Map<TopicPartition, Long> kafkatest_f17114_0(final Collection<org.apache.kafka.common.TopicPartition> partitions)
{    numberOfCalls.incrementAndGet();    throw new TimeoutException();}
public void kafkatest_f17123_0()
{    final Map<TopicPartition, Long> startingOffsets = globalStateTask.initialize();    assertTrue(stateMgr.initialized);    assertEquals(offsets, startingOffsets);}
public void kafkatest_f17124_0()
{    globalStateTask.initialize();    assertTrue(context.initialized);}
public void kafkatest_f17133_0() throws IOException
{    final Map<TopicPartition, Long> expectedOffsets = new HashMap<>();    expectedOffsets.put(t1, 52L);    expectedOffsets.put(t2, 100L);    globalStateTask.initialize();    globalStateTask.update(new ConsumerRecord<>(topic1, 1, 51, "foo".getBytes(), "foo".getBytes()));    globalStateTask.flushState();    assertEquals(expectedOffsets, stateMgr.checkpointed());}
public void kafkatest_f17134_0()
{    final Map<TopicPartition, Long> expectedOffsets = new HashMap<>();    expectedOffsets.put(t1, 102L);    expectedOffsets.put(t2, 100L);    globalStateTask.initialize();    globalStateTask.update(new ConsumerRecord<>(topic1, 1, 101, "foo".getBytes(), "foo".getBytes()));    globalStateTask.flushState();    assertThat(stateMgr.checkpointed(), equalTo(expectedOffsets));}
public void kafkatest_f17143_0() throws Exception
{    initializeConsumer();    globalStreamThread.start();    final StateStore globalStore = builder.globalStateStores().get(GLOBAL_STORE_NAME);    assertTrue(globalStore.isOpen());    globalStreamThread.shutdown();    globalStreamThread.join();    assertFalse(globalStore.isOpen());}
public void kafkatest_f17144_0() throws Exception
{    initializeConsumer();    globalStreamThread.start();    globalStreamThread.shutdown();    globalStreamThread.join();    assertEquals(GlobalStreamThread.State.DEAD, globalStreamThread.state());}
public void kafkatest_f17153_0()
{    final Map<String, String> configs = new HashMap<>();    configs.put("retention.ms", "1000");    configs.put("retention.bytes", "10000");    final UnwindowedChangelogTopicConfig topicConfig = new UnwindowedChangelogTopicConfig("name", configs);    final Map<String, String> properties = topicConfig.getProperties(Collections.<String, String>emptyMap(), 0);    assertEquals("1000", properties.get("retention.ms"));    assertEquals("10000", properties.get("retention.bytes"));}
public void kafkatest_f17154_0()
{    final Map<String, String> configs = new HashMap<>();    configs.put("retention.ms", "1000");    final RepartitionTopicConfig topicConfig = new RepartitionTopicConfig("name", configs);    assertEquals("1000", topicConfig.getProperties(Collections.<String, String>emptyMap(), 0).get(TopicConfig.RETENTION_MS_CONFIG));}
public void kafkatest_f17163_0()
{    LogCaptureAppender.setClassLoggerToDebug(InternalTopicManager.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    mockAdminClient.addTopic(false, topic, Collections.singletonList(new TopicPartitionInfo(0, broker1, cluster, Collections.emptyList())), null);    final InternalTopicConfig internalTopicConfig = new RepartitionTopicConfig(topic, Collections.emptyMap());    internalTopicConfig.setNumberOfPartitions(1);    final InternalTopicConfig internalTopicConfigII = new RepartitionTopicConfig("internal-topic", Collections.emptyMap());    internalTopicConfigII.setNumberOfPartitions(1);    final Map<String, InternalTopicConfig> topicConfigMap = new HashMap<>();    topicConfigMap.put(topic, internalTopicConfig);    topicConfigMap.put("internal-topic", internalTopicConfigII);    internalTopicManager.makeReady(topicConfigMap);    boolean foundExpectedMessage = false;    for (final String message : appender.getMessages()) {        foundExpectedMessage |= message.contains("Topic internal-topic is unknown or not found, hence not existed yet.");    }    assertTrue(foundExpectedMessage);}
public void kafkatest_f17164_0()
{    mockAdminClient.addTopic(false, topic, Collections.singletonList(new TopicPartitionInfo(0, broker1, cluster, Collections.emptyList())), null);    mockAdminClient.markTopicForDeletion(topic);    final InternalTopicConfig internalTopicConfig = new RepartitionTopicConfig(topic, Collections.emptyMap());    internalTopicConfig.setNumberOfPartitions(1);    try {        internalTopicManager.makeReady(Collections.singletonMap(topic, internalTopicConfig));        fail("Should have thrown StreamsException.");    } catch (final StreamsException expected) {        assertNull(expected.getCause());        assertTrue(expected.getMessage().startsWith("Could not create topics after 1 retries"));    }}
public void kafkatest_f17173_0()
{    builder.addSource(null, "source", null, null, null, "topic-1");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    try {        builder.addProcessor("processor", new MockProcessorSupplier(), "source");        fail("Should throw TopologyException with processor name conflict");    } catch (final TopologyException expected) {    /* ok */    }}
public void kafkatest_f17174_0()
{    builder.addProcessor("processor", new MockProcessorSupplier(), "source");}
public void kafkatest_f17183_0()
{    builder.addSource(null, "source", null, null, null, "source-topic");    builder.addSink("sink", "dest-topic", null, null, null, "source");    final Map<Integer, Set<String>> nodeGroups = builder.nodeGroups();    final Set<String> nodeGroup = nodeGroups.get(0);    assertTrue(nodeGroup.contains("sink"));    assertTrue(nodeGroup.contains("source"));}
public void kafkatest_f17184_0()
{    builder.addSource(null, "source", null, null, null, "source-topic");    builder.addSource(null, "sourceII", null, null, null, "source-topicII");    builder.addSink("sink", "dest-topic", null, null, null, "source", "sourceII");    final Map<Integer, Set<String>> nodeGroups = builder.nodeGroups();    final Set<String> nodeGroup = nodeGroups.get(0);    assertTrue(nodeGroup.contains("sink"));    assertTrue(nodeGroup.contains("source"));    assertTrue(nodeGroup.contains("sourceII"));}
public void kafkatest_f17193_0()
{    builder.addSource(null, "source-1", null, null, null, "topic-1");    try {        builder.addStateStore(storeBuilder, "source-1");        fail("Should throw TopologyException with store cannot be added to source");    } catch (final TopologyException expected) {    /* ok */    }}
public void kafkatest_f17194_0()
{    builder.addSource(null, "source-1", null, null, null, "topic-1");    builder.addSink("sink-1", "topic-1", null, null, null, "source-1");    try {        builder.addStateStore(storeBuilder, "sink-1");        fail("Should throw TopologyException with store cannot be added to sink");    } catch (final TopologyException expected) {    /* ok */    }}
public void kafkatest_f17203_0()
{    builder.addSink("name", (TopicNameExtractor<Object, Object>) null, null, null, null);}
public void kafkatest_f17204_0()
{    builder.addProcessor(null, () -> null);}
public void kafkatest_f17213_0()
{    builder.addSource(null, "source", null, null, null, "topic");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    builder.addStateStore(storeBuilder, "processor");    final Map<String, List<String>> stateStoreNameToSourceTopic = builder.stateStoreNameToSourceTopics();    assertEquals(1, stateStoreNameToSourceTopic.size());    assertEquals(Collections.singletonList("topic"), stateStoreNameToSourceTopic.get("store"));}
public void kafkatest_f17214_0()
{    builder.addSource(null, "source", null, null, null, "topic");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    builder.addStateStore(storeBuilder, "processor");    final Map<String, List<String>> stateStoreNameToSourceTopic = builder.stateStoreNameToSourceTopics();    assertEquals(1, stateStoreNameToSourceTopic.size());    assertEquals(Collections.singletonList("topic"), stateStoreNameToSourceTopic.get("store"));}
public void kafkatest_f17223_0() throws Exception
{    builder.addSource(null, "ingest", null, null, null, Pattern.compile("topic-\\d+"));    builder.addProcessor("my-processor", new MockProcessorSupplier(), "ingest");    builder.addStateStore(storeBuilder, "my-processor");    final InternalTopologyBuilder.SubscriptionUpdates subscriptionUpdates = new InternalTopologyBuilder.SubscriptionUpdates();    final Field updatedTopicsField = subscriptionUpdates.getClass().getDeclaredField("updatedTopicSubscriptions");    updatedTopicsField.setAccessible(true);    final Set<String> updatedTopics = (Set<String>) updatedTopicsField.get(subscriptionUpdates);    updatedTopics.add("topic-2");    updatedTopics.add("topic-3");    updatedTopics.add("topic-A");    builder.updateSubscriptions(subscriptionUpdates, "test-thread");    builder.setApplicationId("test-app");    final Map<String, List<String>> stateStoreAndTopics = builder.stateStoreNameToSourceTopics();    final List<String> topics = stateStoreAndTopics.get(storeBuilder.name());    assertEquals("Expected to contain two topics", 2, topics.size());    assertTrue(topics.contains("topic-2"));    assertTrue(topics.contains("topic-3"));    assertFalse(topics.contains("topic-A"));}
public void kafkatest_f17224_0()
{    final String sameNameForSourceAndProcessor = "sameName";    builder.addGlobalStore((StoreBuilder<KeyValueStore>) storeBuilder, sameNameForSourceAndProcessor, null, null, null, "anyTopicName", sameNameForSourceAndProcessor, new MockProcessorSupplier());}
public void kafkatest_f17233_0()
{    final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source("name", null, Pattern.compile("topic"));    final InternalTopologyBuilder.Source differentPattern = new InternalTopologyBuilder.Source("name", null, Pattern.compile("topic2"));    final InternalTopologyBuilder.Source overlappingPattern = new InternalTopologyBuilder.Source("name", null, Pattern.compile("top*"));    assertThat(base, not(equalTo(differentPattern)));    assertThat(base, not(equalTo(overlappingPattern)));}
public void kafkatest_f17234_0()
{    final Metrics metrics = mock(Metrics.class);    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, THREAD_NAME, VERSION);    final String sensorName = "sensor1";    final String expectedFullSensorName = INTERNAL_PREFIX + SENSOR_PREFIX_DELIMITER + THREAD_NAME + SENSOR_NAME_DELIMITER + sensorName;    final RecordingLevel recordingLevel = RecordingLevel.DEBUG;    final Sensor[] parents = {};    EasyMock.expect(metrics.sensor(expectedFullSensorName, recordingLevel, parents)).andReturn(null);    replayAll();    final Sensor sensor = streamsMetrics.threadLevelSensor(sensorName, recordingLevel);    verifyAll();    assertNull(sensor);}
public void kafkatest_f17243_0()
{    StreamsMetricsImpl.addRateOfSumAndSumMetricsToSensor(sensor, group, tags, metricNamePrefix, description1, description2);    final double valueToRecord1 = 18.0;    final double valueToRecord2 = 72.0;    final long defaultWindowSizeInSeconds = Duration.ofMillis(new MetricConfig().timeWindowMs()).getSeconds();    final double expectedRateMetricValue = (valueToRecord1 + valueToRecord2) / defaultWindowSizeInSeconds;    verifyMetric(metricNamePrefix + "-rate", description1, valueToRecord1, valueToRecord2, expectedRateMetricValue);    // values are recorded once for each metric verification    final double expectedSumMetricValue = 2 * valueToRecord1 + 2 * valueToRecord2;    verifyMetric(metricNamePrefix + "-total", description2, valueToRecord1, valueToRecord2, expectedSumMetricValue);    // one metric is added automatically in the constructor of Metrics    assertThat(metrics.metrics().size(), equalTo(2 + 1));}
public void kafkatest_f17244_0()
{    StreamsMetricsImpl.addSumMetricToSensor(sensor, group, tags, metricNamePrefix, description1);    final double valueToRecord1 = 18.0;    final double valueToRecord2 = 42.0;    final double expectedSumMetricValue = valueToRecord1 + valueToRecord2;    verifyMetric(metricNamePrefix + "-total", description1, valueToRecord1, valueToRecord2, expectedSumMetricValue);    // one metric is added automatically in the constructor of Metrics    assertThat(metrics.metrics().size(), equalTo(1 + 1));}
public void kafkatest_f17253_0()
{    final String operation = "commit";    final String operationLatency = operation + StreamsMetricsImpl.LATENCY_SUFFIX;    final String totalDescription = "The total number of commit calls";    final String rateDescription = "The average per-second number of commit calls";    mockStatic(StreamsMetricsImpl.class);    expect(streamsMetrics.threadLevelSensor(operation, RecordingLevel.INFO)).andReturn(dummySensor);    expect(streamsMetrics.threadLevelTagMap()).andReturn(dummyTagMap);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operation, totalDescription, rateDescription);    StreamsMetricsImpl.addAvgAndMaxToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operationLatency);    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = ThreadMetrics.commitSensor(streamsMetrics);    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(dummySensor));}
public void kafkatest_f17254_0()
{    final String operation = "poll";    final String operationLatency = operation + StreamsMetricsImpl.LATENCY_SUFFIX;    final String totalDescription = "The total number of poll calls";    final String rateDescription = "The average per-second number of poll calls";    mockStatic(StreamsMetricsImpl.class);    expect(streamsMetrics.threadLevelSensor(operation, RecordingLevel.INFO)).andReturn(dummySensor);    expect(streamsMetrics.threadLevelTagMap()).andReturn(dummyTagMap);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operation, totalDescription, rateDescription);    StreamsMetricsImpl.addAvgAndMaxToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operationLatency);    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = ThreadMetrics.pollSensor(streamsMetrics);    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(dummySensor));}
public void kafkatest_f17263_0()
{    registered.clear();}
public boolean kafkatest_f17264_0(final TopicPartition partition)
{    return registered.contains(partition);}
public void kafkatest_f17273_0()
{    doTest("GlobalWindowStore", (Consumer<WindowStore<String, Long>>) store -> {        verifyStoreCannotBeInitializedOrClosed(store);        checkThrowsUnsupportedOperation(store::flush, "flush()");        checkThrowsUnsupportedOperation(() -> store.put("1", 1L, 1L), "put()");        checkThrowsUnsupportedOperation(() -> store.put("1", 1L), "put()");        assertEquals(iters.get(0), store.fetchAll(0L, 0L));        assertEquals(windowStoreIter, store.fetch(KEY, 0L, 1L));        assertEquals(iters.get(1), store.fetch(KEY, KEY, 0L, 1L));        assertEquals((Long) VALUE, store.fetch(KEY, 1L));        assertEquals(iters.get(2), store.all());    });}
public void kafkatest_f17274_0()
{    doTest("GlobalTimestampedWindowStore", (Consumer<TimestampedWindowStore<String, Long>>) store -> {        verifyStoreCannotBeInitializedOrClosed(store);        checkThrowsUnsupportedOperation(store::flush, "flush()");        checkThrowsUnsupportedOperation(() -> store.put("1", ValueAndTimestamp.make(1L, 1L), 1L), "put() [with timestamp]");        checkThrowsUnsupportedOperation(() -> store.put("1", ValueAndTimestamp.make(1L, 1L)), "put() [no timestamp]");        assertEquals(timestampedIters.get(0), store.fetchAll(0L, 0L));        assertEquals(windowStoreIter, store.fetch(KEY, 0L, 1L));        assertEquals(timestampedIters.get(1), store.fetch(KEY, KEY, 0L, 1L));        assertEquals(VALUE_AND_TIMESTAMP, store.fetch(KEY, 1L));        assertEquals(timestampedIters.get(2), store.all());    });}
private WindowStore<String, Long> kafkatest_f17283_0()
{    final WindowStore<String, Long> windowStore = mock(WindowStore.class);    initStateStoreMock(windowStore);    expect(windowStore.fetchAll(anyLong(), anyLong())).andReturn(iters.get(0));    expect(windowStore.fetch(anyString(), anyString(), anyLong(), anyLong())).andReturn(iters.get(1));    expect(windowStore.fetch(anyString(), anyLong(), anyLong())).andReturn(windowStoreIter);    expect(windowStore.fetch(anyString(), anyLong())).andReturn(VALUE);    expect(windowStore.all()).andReturn(iters.get(2));    windowStore.put(anyString(), anyLong());    expectLastCall().andAnswer(() -> {        putExecuted = true;        return null;    });    replay(windowStore);    return windowStore;}
private TimestampedWindowStore<String, Long> kafkatest_f17284_0()
{    final TimestampedWindowStore<String, Long> windowStore = mock(TimestampedWindowStore.class);    initStateStoreMock(windowStore);    expect(windowStore.fetchAll(anyLong(), anyLong())).andReturn(timestampedIters.get(0));    expect(windowStore.fetch(anyString(), anyString(), anyLong(), anyLong())).andReturn(timestampedIters.get(1));    expect(windowStore.fetch(anyString(), anyLong(), anyLong())).andReturn(windowStoreIter);    expect(windowStore.fetch(anyString(), anyLong())).andReturn(VALUE_AND_TIMESTAMP);    expect(windowStore.all()).andReturn(timestampedIters.get(2));    windowStore.put(anyString(), anyObject(ValueAndTimestamp.class));    expectLastCall().andAnswer(() -> {        putExecuted = true;        return null;    });    windowStore.put(anyString(), anyObject(ValueAndTimestamp.class), anyLong());    expectLastCall().andAnswer(() -> {        putWithTimestampExecuted = true;        return null;    });    replay(windowStore);    return windowStore;}
public void kafkatest_f17293_0()
{    final StreamsConfig streamsConfig = mock(StreamsConfig.class);    expect(streamsConfig.getString(StreamsConfig.APPLICATION_ID_CONFIG)).andReturn("add-id");    expect(streamsConfig.defaultValueSerde()).andReturn(Serdes.ByteArray());    expect(streamsConfig.defaultKeySerde()).andReturn(Serdes.ByteArray());    replay(streamsConfig);    context = new ProcessorContextImpl(mock(TaskId.class), mock(StreamTask.class), streamsConfig, mock(RecordCollector.class), mock(ProcessorStateManager.class), mock(StreamsMetricsImpl.class), mock(ThreadCache.class));}
public void kafkatest_f17294_0()
{    try {        context.schedule(Duration.ofMillis(0L), null, null);        fail("Should have thrown IllegalArgumentException");    } catch (final IllegalArgumentException expected) {        assertThat(expected.getMessage(), equalTo("The minimum supported scheduling interval is 1 millisecond."));    }}
public void kafkatest_f17306_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(42L, 73L, 0, null, new RecordHeaders());    assertEquals(MIN_SIZE, context.residentMemorySizeEstimate());}
public void kafkatest_f17307_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(42L, 73L, 0, "topic", null);    assertEquals(MIN_SIZE + 5L, context.residentMemorySizeEstimate());}
public void kafkatest_f17316_0() throws IOException
{    final MockKeyValueStore nonPersistentStore = // non persistent store    new MockKeyValueStore(nonPersistentStoreName, false);    final ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 2), noPartitions, false, stateDirectory, mkMap(mkEntry(persistentStoreName, persistentStoreTopicName), mkEntry(nonPersistentStoreName, nonPersistentStoreTopicName)), changelogReader, false, logContext);    try {        stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);        assertTrue(changelogReader.wasRegistered(new TopicPartition(nonPersistentStoreTopicName, 2)));    } finally {        stateMgr.close(true);    }}
public void kafkatest_f17317_0() throws IOException
{    final TaskId taskId = new TaskId(0, 0);    final long storeTopic1LoadedCheckpoint = 10L;    final String storeName1 = "store1";    final String storeName2 = "store2";    final String storeName3 = "store3";    final String storeTopicName1 = ProcessorStateManager.storeChangelogTopic(applicationId, storeName1);    final String storeTopicName2 = ProcessorStateManager.storeChangelogTopic(applicationId, storeName2);    final String storeTopicName3 = ProcessorStateManager.storeChangelogTopic(applicationId, storeName3);    final Map<String, String> storeToChangelogTopic = new HashMap<>();    storeToChangelogTopic.put(storeName1, storeTopicName1);    storeToChangelogTopic.put(storeName2, storeTopicName2);    storeToChangelogTopic.put(storeName3, storeTopicName3);    final OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(stateDirectory.directoryForTask(taskId), StateManagerUtil.CHECKPOINT_FILE_NAME));    checkpoint.write(singletonMap(new TopicPartition(storeTopicName1, 0), storeTopic1LoadedCheckpoint));    final TopicPartition partition1 = new TopicPartition(storeTopicName1, 0);    final TopicPartition partition2 = new TopicPartition(storeTopicName2, 0);    final TopicPartition partition3 = new TopicPartition(storeTopicName3, 1);    final MockKeyValueStore store1 = new MockKeyValueStore(storeName1, true);    final MockKeyValueStore store2 = new MockKeyValueStore(storeName2, true);    final MockKeyValueStore store3 = new MockKeyValueStore(storeName3, true);    // if there is a source partition, inherit the partition id    final Set<TopicPartition> sourcePartitions = Utils.mkSet(new TopicPartition(storeTopicName3, 1));    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, sourcePartitions, // standby    true, stateDirectory, storeToChangelogTopic, changelogReader, false, logContext);    try {        stateMgr.register(store1, store1.stateRestoreCallback);        stateMgr.register(store2, store2.stateRestoreCallback);        stateMgr.register(store3, store3.stateRestoreCallback);        final Map<TopicPartition, Long> changeLogOffsets = stateMgr.checkpointed();        assertEquals(3, changeLogOffsets.size());        assertTrue(changeLogOffsets.containsKey(partition1));        assertTrue(changeLogOffsets.containsKey(partition2));        assertTrue(changeLogOffsets.containsKey(partition3));        assertEquals(storeTopic1LoadedCheckpoint, (long) changeLogOffsets.get(partition1));        assertEquals(-1L, (long) changeLogOffsets.get(partition2));        assertEquals(-1L, (long) changeLogOffsets.get(partition3));    } finally {        stateMgr.close(true);    }}
public void kafkatest_f17326_0() throws IOException
{    final Map<TopicPartition, Long> offsets = singletonMap(persistentStorePartition, 99L);    checkpoint.write(offsets);    final MockKeyValueStore persistentStore = new MockKeyValueStore(persistentStoreName, true);    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, singletonMap(persistentStoreName, persistentStorePartition.topic()), changelogReader, false, logContext);    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    // should ignore irrelevant topic partitions    changelogReader.setRestoredOffsets(mkMap(mkEntry(persistentStorePartition, 110L), mkEntry(new TopicPartition("sillytopic", 5000), 1234L)));    // should ignore irrelevant topic partitions    stateMgr.checkpoint(mkMap(mkEntry(persistentStorePartition, 220L), mkEntry(new TopicPartition("ignoreme", 42), 9000L)));    stateMgr.close(true);    final Map<TopicPartition, Long> read = checkpoint.read();    // the checkpoint gets incremented to be the log position _after_ the committed offset    assertThat(read, equalTo(singletonMap(persistentStorePartition, 221L)));}
public void kafkatest_f17327_0() throws IOException
{    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, singletonMap(persistentStore.name(), persistentStoreTopicName), changelogReader, false, logContext);    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    stateMgr.checkpoint(singletonMap(persistentStorePartition, 10L));    final Map<TopicPartition, Long> read = checkpoint.read();    assertThat(read, equalTo(singletonMap(persistentStorePartition, 11L)));}
public void kafkatest_f17336_0()
{    throw new RuntimeException("KABOOM!");}
public void kafkatest_f17337_0()
{    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    final ProcessorStateManager stateMgr;    try {        stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, singletonMap(persistentStore.name(), persistentStoreTopicName), changelogReader, false, logContext);    } catch (final IOException e) {        e.printStackTrace();        throw new AssertionError(e);    }    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    stateDirectory.clean();    stateMgr.checkpoint(singletonMap(persistentStorePartition, 10L));    LogCaptureAppender.unregister(appender);    boolean foundExpectedLogMessage = false;    for (final LogCaptureAppender.Event event : appender.getEvents()) {        if ("WARN".equals(event.getLevel()) && event.getMessage().startsWith("process-state-manager-test Failed to write offset checkpoint file to [") && event.getMessage().endsWith(".checkpoint]") && event.getThrowableInfo().get().startsWith("java.io.FileNotFoundException: ")) {            foundExpectedLogMessage = true;            break;        }    }    assertTrue(foundExpectedLogMessage);}
public void kafkatest_f17346_0() throws Exception
{    shouldSuccessfullyReInitializeStateStores(true);}
private void kafkatest_f17347_0(final boolean eosEnabled) throws Exception
{    final String store2Name = "store2";    final String store2Changelog = "store2-changelog";    final TopicPartition store2Partition = new TopicPartition(store2Changelog, 0);    final List<TopicPartition> changelogPartitions = asList(changelogTopicPartition, store2Partition);    final Map<String, String> storeToChangelog = mkMap(mkEntry(storeName, changelogTopic), mkEntry(store2Name, store2Changelog));    final MockKeyValueStore stateStore = new MockKeyValueStore(storeName, true);    final MockKeyValueStore stateStore2 = new MockKeyValueStore(store2Name, true);    final ProcessorStateManager stateManager = new ProcessorStateManager(taskId, changelogPartitions, false, stateDirectory, storeToChangelog, changelogReader, eosEnabled, logContext);    stateManager.register(stateStore, stateStore.stateRestoreCallback);    stateManager.register(stateStore2, stateStore2.stateRestoreCallback);    stateStore.initialized = false;    stateStore2.initialized = false;    stateManager.reinitializeStateStoresForPartitions(changelogPartitions, new NoOpProcessorContext() {        @Override        public void register(final StateStore store, final StateRestoreCallback stateRestoreCallback) {            stateManager.register(store, stateRestoreCallback);        }    });    assertTrue(stateStore.initialized);    assertTrue(stateStore2.initialized);}
public void kafkatest_f17356_0()
{    topology.addSource("source-1", "topic-1");    topology.addSource("source-2", "topic-2", "topic-3");    topology.addProcessor("processor-1", new MockProcessorSupplier<>(), "source-1");    topology.addProcessor("processor-2", new MockProcessorSupplier<>(), "source-1", "source-2");    topology.addSink("sink-1", "topic-3", "processor-1");    topology.addSink("sink-2", "topic-4", "processor-1", "processor-2");    final ProcessorTopology processorTopology = topology.getInternalBuilder("X").build();    assertEquals(6, processorTopology.processors().size());    assertEquals(2, processorTopology.sources().size());    assertEquals(3, processorTopology.sourceTopics().size());    assertNotNull(processorTopology.source("topic-1"));    assertNotNull(processorTopology.source("topic-2"));    assertNotNull(processorTopology.source("topic-3"));    assertEquals(processorTopology.source("topic-2"), processorTopology.source("topic-3"));}
public void kafkatest_f17357_0()
{    final int partition = 10;    driver = new TopologyTestDriver(createSimpleTopology(partition), props);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key1", "value1"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key1", "value1", partition);    assertNoOutputRecord(OUTPUT_TOPIC_2);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key2", "value2"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key2", "value2", partition);    assertNoOutputRecord(OUTPUT_TOPIC_2);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key3", "value3"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key4", "value4"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key5", "value5"));    assertNoOutputRecord(OUTPUT_TOPIC_2);    assertNextOutputRecord(OUTPUT_TOPIC_1, "key3", "value3", partition);    assertNextOutputRecord(OUTPUT_TOPIC_1, "key4", "value4", partition);    assertNextOutputRecord(OUTPUT_TOPIC_1, "key5", "value5", partition);}
public void kafkatest_f17366_0()
{    topology.addSource("source", "topic1", "topic2");    final ProcessorTopology processorTopology = topology.getInternalBuilder().build();    final String result = processorTopology.toString();    assertThat(result, containsString("source:\n\t\ttopics:\t\t[topic1, topic2]\n"));}
public void kafkatest_f17367_0()
{    topology.addSource("source", "topic1", "topic2");    topology.addSource("source2", "t", "t1", "t2");    final ProcessorTopology processorTopology = topology.getInternalBuilder().build();    final String result = processorTopology.toString();    assertThat(result, containsString("source:\n\t\ttopics:\t\t[topic1, topic2]\n"));    assertThat(result, containsString("source2:\n\t\ttopics:\t\t[t, t1, t2]\n"));}
public void kafkatest_f17376_0()
{    final ProcessorTopology processorTopology = createLocalStoreTopology(Stores.inMemoryKeyValueStore("my-store"));    assertFalse(processorTopology.hasPersistentLocalStore());}
public void kafkatest_f17377_0()
{    final ProcessorTopology processorTopology = createLocalStoreTopology(Stores.persistentKeyValueStore("my-store"));    assertTrue(processorTopology.hasPersistentLocalStore());}
private void kafkatest_f17386_0(final String topic, final String key, final String value, final Headers headers, final Integer partition, final Long timestamp)
{    final ProducerRecord<String, String> record = driver.readOutput(topic, STRING_DESERIALIZER, STRING_DESERIALIZER);    assertEquals(topic, record.topic());    assertEquals(key, record.key());    assertEquals(value, record.value());    assertEquals(partition, record.partition());    assertEquals(timestamp, record.timestamp());    assertEquals(headers, record.headers());}
private void kafkatest_f17387_0(final String topic)
{    assertNull(driver.readOutput(topic));}
private Topology kafkatest_f17396_0()
{    topology.addSource("source", INPUT_TOPIC_1).addProcessor("processor", define(new ValueTimestampProcessor()), "source").addSink("sink0", THROUGH_TOPIC_1, "processor").addSource("source1", THROUGH_TOPIC_1).addSink("sink1", OUTPUT_TOPIC_1, "source1");    // use wrapper to get the internal topology builder to add internal topic    final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology);    internalTopologyBuilder.addInternalTopic(THROUGH_TOPIC_1);    return topology;}
private Topology kafkatest_f17397_0()
{    return topology.addSource("source-1", INPUT_TOPIC_1).addSink("sink-1", OUTPUT_TOPIC_1, "source-1").addSource("source-2", OUTPUT_TOPIC_1).addSink("sink-2", OUTPUT_TOPIC_2, "source-2");}
public void kafkatest_f17406_0(final String key, final String value)
{    for (int i = 0; i != numChildren; ++i) {        context().forward(key, value + "(" + (i + 1) + ")", "sink" + i);    }}
public void kafkatest_f17407_0(final ProcessorContext context)
{    super.init(context);    store = (KeyValueStore<String, String>) context.getStateStore(storeName);}
public void kafkatest_f17416_0()
{    final PunctuationSchedule sched = new PunctuationSchedule(node, 0L, 100L, punctuator);    final long now = sched.timestamp - 100L;    final Cancellable cancellable = queue.schedule(sched);    final ProcessorNodePunctuator processorNodePunctuator = new ProcessorNodePunctuator() {        @Override        public void punctuate(final ProcessorNode node, final long time, final PunctuationType type, final Punctuator punctuator) {            punctuator.punctuate(time);            // simulate scheduler cancelled from within punctuator            cancellable.cancel();        }    };    queue.mayPunctuate(now, PunctuationType.STREAM_TIME, processorNodePunctuator);    assertEquals(0, node.mockProcessor.punctuatedStreamTime.size());    queue.mayPunctuate(now + 100L, PunctuationType.STREAM_TIME, processorNodePunctuator);    assertEquals(1, node.mockProcessor.punctuatedStreamTime.size());    queue.mayPunctuate(now + 200L, PunctuationType.STREAM_TIME, processorNodePunctuator);    assertEquals(1, node.mockProcessor.punctuatedStreamTime.size());}
public void kafkatest_f17417_0(final ProcessorNode node, final long time, final PunctuationType type, final Punctuator punctuator)
{    punctuator.punctuate(time);    // simulate scheduler cancelled from within punctuator    cancellable.cancel();}
public void kafkatest_f17429_0()
{    final RecordCollector collector = new RecordCollectorImpl("test", logContext, new DefaultProductionExceptionHandler(), new Metrics().sensor("skipped-records"));    collector.init(new MockProducer(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {        @Override        public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {            callback.onCompletion(null, new Exception());            return null;        }    });    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);    try {        collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);        fail("Should have thrown StreamsException");    } catch (final StreamsException expected) {    /* ok */    }}
public synchronized Future<RecordMetadata> kafkatest_f17430_0(final ProducerRecord record, final Callback callback)
{    callback.onCompletion(null, new Exception());    return null;}
public void kafkatest_f17439_0()
{    final RecordCollector collector = new RecordCollectorImpl("test", logContext, new DefaultProductionExceptionHandler(), new Metrics().sensor("skipped-records"));    collector.init(new MockProducer(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {        @Override        public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {            callback.onCompletion(null, new Exception());            return null;        }    });    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);    try {        collector.close();        fail("Should have thrown StreamsException");    } catch (final StreamsException expected) {    /* ok */    }}
public synchronized Future<RecordMetadata> kafkatest_f17440_0(final ProducerRecord record, final Callback callback)
{    callback.onCompletion(null, new Exception());    return null;}
public void kafkatest_f17449_0(final Map<String, ?> configs, final boolean isKey)
{    this.isKey = isKey;    super.configure(configs, isKey);}
public byte[] kafkatest_f17450_0(final String topic, final Headers headers, final String data)
{    if (isKey) {        headers.add(new RecordHeader("key", "key".getBytes()));    } else {        headers.add(new RecordHeader("value", "value".getBytes()));    }    return serialize(topic, data);}
public void kafkatest_f17459_0()
{    final byte[] value = Serdes.Long().serializer().serialize("foo", 1L);    final List<ConsumerRecord<byte[], byte[]>> records = Collections.singletonList(new ConsumerRecord<>("topic", 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, value));    queue.addRawRecords(records);}
public void kafkatest_f17460_0()
{    final byte[] key = Serdes.Long().serializer().serialize("foo", 1L);    final List<ConsumerRecord<byte[], byte[]>> records = Collections.singletonList(new ConsumerRecord<>("topic", 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, key, recordValue));    queueThatSkipsDeserializeErrors.addRawRecords(records);    assertEquals(0, queueThatSkipsDeserializeErrors.size());}
public void kafkatest_f17469_0()
{    final String keyOfDifferentTypeThanSerializer = "key with different type";    final String valueOfDifferentTypeThanSerializer = "value with different type";    // When/Then    context.setTime(0);    try {        illTypedSink.process(keyOfDifferentTypeThanSerializer, valueOfDifferentTypeThanSerializer);        fail("Should have thrown StreamsException");    } catch (final StreamsException e) {        assertThat(e.getCause(), instanceOf(ClassCastException.class));    }}
public void kafkatest_f17470_0()
{    final String invalidValueToTriggerSerializerMismatch = "";    // When/Then    context.setTime(1);    try {        illTypedSink.process(null, invalidValueToTriggerSerializerMismatch);        fail("Should have thrown StreamsException");    } catch (final StreamsException e) {        assertThat(e.getCause(), instanceOf(ClassCastException.class));        assertThat(e.getMessage(), containsString("unknown because key is null"));    }}
public void kafkatest_f17479_0() throws IOException
{    final StreamsConfig config = createConfig(baseDir);    task = new StandbyTask(taskId, topicPartitions, topology, consumer, changelogReader, config, streamsMetrics, stateDirectory);    task.initializeStateStores();    assertEquals(Utils.mkSet(partition2, partition1), new HashSet<>(task.checkpointedOffsets().keySet()));}
public void kafkatest_f17480_0() throws IOException
{    final StreamsConfig config = createConfig(baseDir);    task = new StandbyTask(taskId, topicPartitions, topology, consumer, changelogReader, config, streamsMetrics, stateDirectory);    restoreStateConsumer.assign(new ArrayList<>(task.checkpointedOffsets().keySet()));    try {        task.update(partition1, singletonList(new ConsumerRecord<>(partition1.topic(), partition1.partition(), 10, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, recordValue)));        fail("expected an exception");    } catch (final NullPointerException npe) {        assertThat(npe.getMessage(), containsString("stateRestoreCallback must not be null"));    }}
public synchronized OffsetAndMetadata kafkatest_f17489_0(final TopicPartition partition)
{    committedCallCount.getAndIncrement();    return super.committed(partition);}
public void kafkatest_f17490_0() throws IOException
{    final AtomicInteger committedCallCount = new AtomicInteger();    final Consumer<byte[], byte[]> consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {        @Override        public synchronized OffsetAndMetadata committed(final TopicPartition partition) {            committedCallCount.getAndIncrement();            return super.committed(partition);        }    };    consumer.assign(Collections.singletonList(globalTopicPartition));    consumer.commitSync(mkMap(mkEntry(globalTopicPartition, new OffsetAndMetadata(0L))));    task = new StandbyTask(taskId, ktablePartitions, ktableTopology, consumer, changelogReader, createConfig(baseDir), streamsMetrics, stateDirectory);    task.initializeStateStores();    task.update(globalTopicPartition, Collections.singletonList(makeConsumerRecord(globalTopicPartition, 1, 1)));    assertThat(committedCallCount.get(), equalTo(1));    task.update(globalTopicPartition, Collections.singletonList(makeConsumerRecord(globalTopicPartition, 1, 1)));    // We should not make another consumer.committed() call until we commit    assertThat(committedCallCount.get(), equalTo(1));    task.commit();    task.update(globalTopicPartition, Collections.singletonList(makeConsumerRecord(globalTopicPartition, 1, 1)));    // We committed so we're allowed to make another consumer.committed() call    assertThat(committedCallCount.get(), equalTo(2));}
private MetricName kafkatest_f17499_0()
{    final MetricName metricName = new MetricName("name", "group", "description", Collections.emptyMap());    final Sensor sensor = streamsMetrics.threadLevelSensor("task-closed", Sensor.RecordingLevel.INFO);    sensor.add(metricName, new CumulativeSum());    return metricName;}
private void kafkatest_f17500_0(final double expected, final StreamsMetricsImpl streamsMetrics, final MetricName metricName)
{    final KafkaMetric metric = (KafkaMetric) streamsMetrics.metrics().get(metricName);    final double totalCloses = metric.measurable().measure(metric.config(), System.currentTimeMillis());    assertThat(totalCloses, equalTo(expected));}
public void kafkatest_f17509_0()
{    stateConsumer.initialize();    consumer.addRecord(new ConsumerRecord<>("topic-one", 1, 20L, new byte[0], new byte[0]));    time.sleep(FLUSH_INTERVAL / 2);    stateConsumer.pollAndUpdate();    assertFalse(stateMaintainer.flushed);}
public void kafkatest_f17510_0() throws IOException
{    stateConsumer.close();    assertTrue(consumer.closed());}
public void kafkatest_f17519_0()
{    assertTrue(stateDir.exists());    assertTrue(stateDir.isDirectory());    assertTrue(appDir.exists());    assertTrue(appDir.isDirectory());}
public void kafkatest_f17520_0()
{    final TaskId taskId = new TaskId(0, 0);    final File taskDirectory = directory.directoryForTask(taskId);    assertTrue(taskDirectory.exists());    assertTrue(taskDirectory.isDirectory());}
public void kafkatest_f17529_0()
{    final File otherDir = TestUtils.tempDirectory(stateDir.toPath(), "foo");    directory.cleanRemovedTasks(0);    assertTrue(otherDir.exists());}
public void kafkatest_f17530_0()
{    TestUtils.tempDirectory(stateDir.toPath(), "foo");    final File taskDir1 = directory.directoryForTask(new TaskId(0, 0));    final File taskDir2 = directory.directoryForTask(new TaskId(0, 1));    final List<File> dirs = Arrays.asList(directory.listTaskDirectories());    assertEquals(2, dirs.size());    assertTrue(dirs.contains(taskDir1));    assertTrue(dirs.contains(taskDir2));}
public void kafkatest_f17539_0() throws Exception
{    initializeStateDirectory(false);    final File globalStateDir = directory.globalStateDir();    assertFalse(globalStateDir.exists());}
public void kafkatest_f17540_0() throws Exception
{    initializeStateDirectory(false);    final TaskId taskId = new TaskId(0, 0);    assertTrue(directory.lock(taskId));}
public void kafkatest_f17554_0()
{    final ArrayList<KeyValue<byte[], byte[]>> actual = new ArrayList<>();    final BatchingStateRestoreCallback callback = new BatchingStateRestoreCallback() {        @Override        public void restoreAll(final Collection<KeyValue<byte[], byte[]>> records) {            actual.addAll(records);        }        @Override        public void restore(final byte[] key, final byte[] value) {        // unreachable        }    };    final RecordBatchingStateRestoreCallback adapted = adapt(callback);    final byte[] key1 = { 1 };    final byte[] value1 = { 2 };    final byte[] key2 = { 3 };    final byte[] value2 = { 4 };    adapted.restoreBatch(asList(new ConsumerRecord<>("topic1", 0, 0L, key1, value1), new ConsumerRecord<>("topic2", 1, 1L, key2, value2)));    assertThat(actual, is(asList(new KeyValue<>(key1, value1), new KeyValue<>(key2, value2))));}
public void kafkatest_f17555_0(final Collection<KeyValue<byte[], byte[]>> records)
{    actual.addAll(records);}
public void kafkatest_f17564_0()
{    final StateRestorer restorer = new StateRestorer(new TopicPartition("topic", 1), compositeRestoreListener, null, 0, true, "storeName", identity());    assertTrue(restorer.hasCompleted(0, 10));}
public void kafkatest_f17565_0()
{    restorer.setRestoredOffset(20);    assertThat(restorer.restoredOffset(), equalTo(20L));    restorer.setRestoredOffset(100);    assertThat(restorer.restoredOffset(), equalTo(OFFSET_LIMIT));}
public Set<TopicPartition> kafkatest_f17574_0()
{    return Collections.singleton(topicPartition);}
public void kafkatest_f17575_0()
{    final int messages = 10;    final int startOffset = 5;    final long expiredCheckpoint = 1L;    assignPartition(messages, topicPartition);    consumer.updateBeginningOffsets(Collections.singletonMap(topicPartition, (long) startOffset));    consumer.updateEndOffsets(Collections.singletonMap(topicPartition, (long) (messages + startOffset)));    addRecords(messages, topicPartition, startOffset);    consumer.assign(Collections.emptyList());    final StateRestorer stateRestorer = new StateRestorer(topicPartition, restoreListener, expiredCheckpoint, Long.MAX_VALUE, true, "storeName", identity());    changelogReader.register(stateRestorer);    EasyMock.expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);    EasyMock.replay(active, task);    // first restore call "fails" since OffsetOutOfRangeException but we should not die with an exception    assertEquals(0, changelogReader.restore(active).size());    // the starting offset for stateRestorer is set to NO_CHECKPOINT    assertThat(stateRestorer.checkpoint(), equalTo(-1L));    // restore the active task again    changelogReader.register(stateRestorer);    // the restored task should return completed partition without Exception.    assertEquals(1, changelogReader.restore(active).size());    // the restored size should be equal to message length.    assertThat(callback.restored.size(), equalTo(messages));}
public void kafkatest_f17584_0()
{    final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, "storeName", identity());    setupConsumer(0, topicPartition);    changelogReader.register(restorer);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(0));    assertThat(restorer.restoredOffset(), equalTo(0L));}
public void kafkatest_f17585_0()
{    final long endOffset = 10L;    setupConsumer(endOffset, topicPartition);    final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, endOffset, Long.MAX_VALUE, true, "storeName", identity());    changelogReader.register(restorer);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(0));    assertThat(restorer.restoredOffset(), equalTo(endOffset));}
public void kafkatest_f17594_0()
{    final int messages = 10;    setupConsumer(messages, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andReturn(task);    replay(active);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(5));}
public void kafkatest_f17595_0()
{    final int messages = 10;    setupConsumer(messages, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 10, true, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andReturn(task);    replay(active);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(10));}
public void kafkatest_f17604_0()
{    builder = new StreamsBuilder();    final KStream<Object, Object> one = builder.stream("topic-one");    one.groupByKey().count(Materialized.<Object, Long, KeyValueStore<Bytes, byte[]>>as("table-one"));    final KStream<Object, Object> two = builder.stream("topic-two");    two.groupByKey().count(Materialized.<Object, Long, KeyValueStore<Bytes, byte[]>>as("table-two"));    builder.stream("topic-three").groupByKey().count(Materialized.<Object, Long, KeyValueStore<Bytes, byte[]>>as("table-three"));    one.merge(two).groupByKey().count(Materialized.<Object, Long, KeyValueStore<Bytes, byte[]>>as("merged-table"));    builder.stream("topic-four").mapValues(new ValueMapper<Object, Object>() {        @Override        public Object apply(final Object value) {            return value;        }    });    builder.globalTable("global-topic", Consumed.with(null, null), Materialized.<Object, Object, KeyValueStore<Bytes, byte[]>>as(globalTable));    TopologyWrapper.getInternalTopologyBuilder(builder.build()).setApplicationId("appId");    topic1P0 = new TopicPartition("topic-one", 0);    topic1P1 = new TopicPartition("topic-one", 1);    topic2P0 = new TopicPartition("topic-two", 0);    topic2P1 = new TopicPartition("topic-two", 1);    topic3P0 = new TopicPartition("topic-three", 0);    topic4P0 = new TopicPartition("topic-four", 0);    hostOne = new HostInfo("host-one", 8080);    hostTwo = new HostInfo("host-two", 9090);    hostThree = new HostInfo("host-three", 7070);    hostToPartitions = new HashMap<>();    hostToPartitions.put(hostOne, Utils.mkSet(topic1P0, topic2P1, topic4P0));    hostToPartitions.put(hostTwo, Utils.mkSet(topic2P0, topic1P1));    hostToPartitions.put(hostThree, Collections.singleton(topic3P0));    final List<PartitionInfo> partitionInfos = Arrays.asList(new PartitionInfo("topic-one", 0, null, null, null), new PartitionInfo("topic-one", 1, null, null, null), new PartitionInfo("topic-two", 0, null, null, null), new PartitionInfo("topic-two", 1, null, null, null), new PartitionInfo("topic-three", 0, null, null, null), new PartitionInfo("topic-four", 0, null, null, null));    cluster = new Cluster(null, Collections.<Node>emptyList(), partitionInfos, Collections.<String>emptySet(), Collections.<String>emptySet());    metadataState = new StreamsMetadataState(TopologyWrapper.getInternalTopologyBuilder(builder.build()), hostOne);    metadataState.onChange(hostToPartitions, cluster);    partitioner = new StreamPartitioner<String, Object>() {        @Override        public Integer partition(final String topic, final String key, final Object value, final int numPartitions) {            return 1;        }    };}
public Object kafkatest_f17605_0(final Object value)
{    return value;}
public void kafkatest_f17614_0()
{    final TopicPartition tp4 = new TopicPartition("topic-three", 1);    hostToPartitions.put(hostTwo, Utils.mkSet(topic2P0, tp4));    metadataState.onChange(hostToPartitions, cluster.withPartitions(Collections.singletonMap(tp4, new PartitionInfo("topic-three", 1, null, null, null))));    final StreamsMetadata expected = new StreamsMetadata(hostThree, Utils.mkSet(globalTable, "table-three"), Collections.singleton(topic3P0));    final StreamsMetadata actual = metadataState.getMetadataWithKey("table-three", "the-key", Serdes.String().serializer());    assertEquals(expected, actual);}
public void kafkatest_f17615_0()
{    final TopicPartition tp4 = new TopicPartition("topic-three", 1);    hostToPartitions.put(hostTwo, Utils.mkSet(topic2P0, tp4));    metadataState.onChange(hostToPartitions, cluster.withPartitions(Collections.singletonMap(tp4, new PartitionInfo("topic-three", 1, null, null, null))));    final StreamsMetadata expected = new StreamsMetadata(hostTwo, Utils.mkSet(globalTable, "table-two", "table-three", "merged-table"), Utils.mkSet(topic2P0, tp4));    final StreamsMetadata actual = metadataState.getMetadataWithKey("table-three", "the-key", partitioner);    assertEquals(expected, actual);}
public void kafkatest_f17624_0()
{    final Collection<StreamsMetadata> metadata = metadataState.getAllMetadataForStore(globalTable);    assertEquals(3, metadata.size());    for (final StreamsMetadata streamsMetadata : metadata) {        assertTrue(streamsMetadata.stateStoreNames().contains(globalTable));    }}
public void kafkatest_f17625_0()
{    final StreamsMetadata metadata = metadataState.getMetadataWithKey(globalTable, "key", Serdes.String().serializer());    assertEquals(hostOne, metadata.hostInfo());}
public void kafkatest_f17634_0()
{    final TaskId taskIdA0 = new TaskId(0, 0);    final TaskId taskIdA1 = new TaskId(0, 1);    final TaskId taskIdA2 = new TaskId(0, 2);    final TaskId taskIdA3 = new TaskId(0, 3);    final TaskId taskIdB0 = new TaskId(1, 0);    final TaskId taskIdB1 = new TaskId(1, 1);    final TaskId taskIdB2 = new TaskId(1, 2);    final TaskId taskIdC0 = new TaskId(2, 0);    final TaskId taskIdC1 = new TaskId(2, 1);    final List<TaskId> expectedSubList1 = asList(taskIdA0, taskIdA3, taskIdB2);    final List<TaskId> expectedSubList2 = asList(taskIdA1, taskIdB0, taskIdC0);    final List<TaskId> expectedSubList3 = asList(taskIdA2, taskIdB1, taskIdC1);    final List<List<TaskId>> embeddedList = asList(expectedSubList1, expectedSubList2, expectedSubList3);    final List<TaskId> tasks = asList(taskIdC0, taskIdC1, taskIdB0, taskIdB1, taskIdB2, taskIdA0, taskIdA1, taskIdA2, taskIdA3);    Collections.shuffle(tasks);    final List<List<TaskId>> interleavedTaskIds = StreamsPartitionAssignor.interleaveTasksByGroupId(tasks, 3);    assertThat(interleavedTaskIds, equalTo(embeddedList));}
public void kafkatest_f17635_0()
{    builder.addSource(null, "source1", null, null, null, "topic1");    builder.addSource(null, "source2", null, null, null, "topic2");    builder.addProcessor("processor", new MockProcessorSupplier(), "source1", "source2");    final Set<TaskId> prevTasks = Utils.mkSet(new TaskId(0, 1), new TaskId(1, 1), new TaskId(2, 1));    final Set<TaskId> cachedTasks = Utils.mkSet(new TaskId(0, 1), new TaskId(1, 1), new TaskId(2, 1), new TaskId(0, 2), new TaskId(1, 2), new TaskId(2, 2));    final UUID processId = UUID.randomUUID();    createMockTaskManager(prevTasks, cachedTasks, processId, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    final Set<String> topics = Utils.mkSet("topic1", "topic2");    final ConsumerPartitionAssignor.Subscription subscription = new ConsumerPartitionAssignor.Subscription(new ArrayList<>(topics), partitionAssignor.subscriptionUserData(topics));    Collections.sort(subscription.topics());    assertEquals(asList("topic1", "topic2"), subscription.topics());    final Set<TaskId> standbyTasks = new HashSet<>(cachedTasks);    standbyTasks.removeAll(prevTasks);    final SubscriptionInfo info = new SubscriptionInfo(processId, prevTasks, standbyTasks, null);    assertEquals(info.encode(), subscription.userData());}
public void kafkatest_f17644_0()
{    createMockTaskManager();    final Map<HostInfo, Set<TopicPartition>> hostState = Collections.singletonMap(new HostInfo("localhost", 9090), Utils.mkSet(t3p0, t3p3));    taskManager.setPartitionsByHostState(hostState);    EasyMock.expectLastCall();    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    activeTasks.put(task0, Utils.mkSet(t3p0));    activeTasks.put(task3, Utils.mkSet(t3p3));    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    standbyTasks.put(task1, Utils.mkSet(t3p1));    standbyTasks.put(task2, Utils.mkSet(t3p2));    taskManager.setAssignmentMetadata(activeTasks, standbyTasks);    EasyMock.expectLastCall();    final Capture<Cluster> capturedCluster = EasyMock.newCapture();    taskManager.setClusterMetadata(EasyMock.capture(capturedCluster));    EasyMock.expectLastCall();    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    final List<TaskId> activeTaskList = asList(task0, task3);    final AssignmentInfo info = new AssignmentInfo(activeTaskList, standbyTasks, hostState);    final ConsumerPartitionAssignor.Assignment assignment = new ConsumerPartitionAssignor.Assignment(asList(t3p0, t3p3), info.encode());    partitionAssignor.onAssignment(assignment, null);    EasyMock.verify(taskManager);    assertEquals(Collections.singleton(t3p0.topic()), capturedCluster.getValue().topics());    assertEquals(2, capturedCluster.getValue().partitionsForTopic(t3p0.topic()).size());}
public void kafkatest_f17645_0()
{    builder.setApplicationId(applicationId);    builder.addInternalTopic("topicX");    builder.addSource(null, "source1", null, null, null, "topic1");    builder.addProcessor("processor1", new MockProcessorSupplier(), "source1");    builder.addSink("sink1", "topicX", null, null, null, "processor1");    builder.addSource(null, "source2", null, null, null, "topicX");    builder.addProcessor("processor2", new MockProcessorSupplier(), "source2");    final List<String> topics = asList("topic1", applicationId + "-topicX");    final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);    final UUID uuid1 = UUID.randomUUID();    createMockTaskManager(emptyTasks, emptyTasks, uuid1, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    final MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);    partitionAssignor.setInternalTopicManager(internalTopicManager);    subscriptions.put("consumer10", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));    partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();    // check prepared internal topics    assertEquals(1, internalTopicManager.readyTopics.size());    assertEquals(allTasks.size(), (long) internalTopicManager.readyTopics.get(applicationId + "-topicX"));}
public void kafkatest_f17654_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("topic1").groupByKey().count();    final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(builder.build());    internalTopologyBuilder.setApplicationId(applicationId);    final UUID uuid = UUID.randomUUID();    createMockTaskManager(emptyTasks, emptyTasks, uuid, internalTopologyBuilder);    EasyMock.replay(taskManager);    final Map<String, Object> props = new HashMap<>();    props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);    props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint);    configurePartitionAssignor(props);    partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));    subscriptions.put("consumer1", new ConsumerPartitionAssignor.Subscription(Collections.singletonList("topic1"), new SubscriptionInfo(uuid, emptyTasks, emptyTasks, userEndPoint).encode()));    subscriptions.put("consumer2", new ConsumerPartitionAssignor.Subscription(Collections.singletonList("topic1"), new SubscriptionInfo(UUID.randomUUID(), emptyTasks, emptyTasks, "other:9090").encode()));    final Set<TopicPartition> allPartitions = Utils.mkSet(t1p0, t1p1, t1p2);    final Map<String, ConsumerPartitionAssignor.Assignment> assign = partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();    final ConsumerPartitionAssignor.Assignment consumer1Assignment = assign.get("consumer1");    final AssignmentInfo assignmentInfo = AssignmentInfo.decode(consumer1Assignment.userData());    final Set<TopicPartition> consumer1partitions = assignmentInfo.partitionsByHost().get(new HostInfo("localhost", 8080));    final Set<TopicPartition> consumer2Partitions = assignmentInfo.partitionsByHost().get(new HostInfo("other", 9090));    final HashSet<TopicPartition> allAssignedPartitions = new HashSet<>(consumer1partitions);    allAssignedPartitions.addAll(consumer2Partitions);    assertThat(consumer1partitions, not(allPartitions));    assertThat(consumer2Partitions, not(allPartitions));    assertThat(allAssignedPartitions, equalTo(allPartitions));}
public void kafkatest_f17655_0()
{    final Map<String, Object> config = configProps();    config.remove(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);    try {        partitionAssignor.configure(config);        fail("Should have thrown KafkaException");    } catch (final KafkaException expected) {        assertThat(expected.getMessage(), equalTo("TaskManager is not specified"));    }}
public void kafkatest_f17664_0()
{    shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0101);}
public void kafkatest_f17665_0()
{    shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0102);}
private void kafkatest_f17674_0(final int oldVersion)
{    subscriptions.put("consumer1", new ConsumerPartitionAssignor.Subscription(Collections.singletonList("topic1"), new SubscriptionInfo(oldVersion, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()));    subscriptions.put("future-consumer", new ConsumerPartitionAssignor.Subscription(Collections.singletonList("topic1"), encodeFutureSubscription()));    createMockTaskManager(emptyTasks, emptyTasks, UUID.randomUUID(), builder);    EasyMock.replay(taskManager);    partitionAssignor.configure(configProps());    try {        partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();        fail("Should have thrown IllegalStateException");    } catch (final IllegalStateException expected) {    // pass    }}
private ConsumerPartitionAssignor.Assignment kafkatest_f17675_0(final Map<HostInfo, Set<TopicPartition>> firstHostState)
{    final AssignmentInfo info = new AssignmentInfo(Collections.emptyList(), Collections.emptyMap(), firstHostState);    return new ConsumerPartitionAssignor.Assignment(Collections.emptyList(), info.encode());}
public void kafkatest_f17684_0()
{    consumer.assign(asList(partition1, partition2));    stateDirectory = new StateDirectory(createConfig(false), new MockTime(), true);}
public void kafkatest_f17685_0() throws IOException
{    try {        if (task != null) {            try {                task.close(true, false);            } catch (final Exception e) {            // swallow            }        }    } finally {        Utils.delete(BASE_DIR);    }}
public void kafkatest_f17694_0()
{    task = createStatelessTask(createConfig(false));    task.addRecords(partition1, asList(getConsumerRecord(partition1, 10), getConsumerRecord(partition1, 20)));    task.addRecords(partition2, asList(getConsumerRecord(partition2, 35), getConsumerRecord(partition2, 45), getConsumerRecord(partition2, 55), getConsumerRecord(partition2, 65)));    assertTrue(task.process());    assertEquals(1, source1.numReceived);    assertEquals(0, source2.numReceived);    assertEquals(1, consumer.paused().size());    assertTrue(consumer.paused().contains(partition2));    task.addRecords(partition1, asList(getConsumerRecord(partition1, 30), getConsumerRecord(partition1, 40), getConsumerRecord(partition1, 50)));    assertEquals(2, consumer.paused().size());    assertTrue(consumer.paused().contains(partition1));    assertTrue(consumer.paused().contains(partition2));    assertTrue(task.process());    assertEquals(2, source1.numReceived);    assertEquals(0, source2.numReceived);    assertEquals(1, consumer.paused().size());    assertTrue(consumer.paused().contains(partition2));    assertTrue(task.process());    assertEquals(3, source1.numReceived);    assertEquals(0, source2.numReceived);    assertEquals(1, consumer.paused().size());    assertTrue(consumer.paused().contains(partition2));    assertTrue(task.process());    assertEquals(3, source1.numReceived);    assertEquals(1, source2.numReceived);    assertEquals(0, consumer.paused().size());}
public void kafkatest_f17695_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    task.addRecords(partition1, asList(getConsumerRecord(partition1, 20), getConsumerRecord(partition1, 142), getConsumerRecord(partition1, 155), getConsumerRecord(partition1, 160)));    task.addRecords(partition2, asList(getConsumerRecord(partition2, 25), getConsumerRecord(partition2, 145), getConsumerRecord(partition2, 159), getConsumerRecord(partition2, 161)));    // st: -1    // punctuate at 20    assertFalse(task.maybePunctuateStreamTime());    // st: 20    assertTrue(task.process());    assertEquals(7, task.numBuffered());    assertEquals(1, source1.numReceived);    assertEquals(0, source2.numReceived);    assertTrue(task.maybePunctuateStreamTime());    // st: 25    assertTrue(task.process());    assertEquals(6, task.numBuffered());    assertEquals(1, source1.numReceived);    assertEquals(1, source2.numReceived);    assertFalse(task.maybePunctuateStreamTime());    // st: 142    // punctuate at 142    assertTrue(task.process());    assertEquals(5, task.numBuffered());    assertEquals(2, source1.numReceived);    assertEquals(1, source2.numReceived);    assertTrue(task.maybePunctuateStreamTime());    // st: 145    // only one punctuation after 100ms gap    assertTrue(task.process());    assertEquals(4, task.numBuffered());    assertEquals(2, source1.numReceived);    assertEquals(2, source2.numReceived);    assertFalse(task.maybePunctuateStreamTime());    // st: 155    // punctuate at 155    assertTrue(task.process());    assertEquals(3, task.numBuffered());    assertEquals(3, source1.numReceived);    assertEquals(2, source2.numReceived);    assertTrue(task.maybePunctuateStreamTime());    // st: 159    assertTrue(task.process());    assertEquals(2, task.numBuffered());    assertEquals(3, source1.numReceived);    assertEquals(3, source2.numReceived);    assertFalse(task.maybePunctuateStreamTime());    // st: 160, aligned at 0    assertTrue(task.process());    assertEquals(1, task.numBuffered());    assertEquals(4, source1.numReceived);    assertEquals(3, source2.numReceived);    assertTrue(task.maybePunctuateStreamTime());    // st: 161    assertTrue(task.process());    assertEquals(0, task.numBuffered());    assertEquals(4, source1.numReceived);    assertEquals(4, source2.numReceived);    assertFalse(task.maybePunctuateStreamTime());    processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 20L, 142L, 155L, 160L);}
public void kafkatest_f17704_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    final long now = time.milliseconds();    time.sleep(100);    assertTrue(task.maybePunctuateSystemTime());    assertFalse(task.maybePunctuateSystemTime());    time.sleep(10);    assertTrue(task.maybePunctuateSystemTime());    time.sleep(12);    assertTrue(task.maybePunctuateSystemTime());    time.sleep(7);    assertFalse(task.maybePunctuateSystemTime());    // punctuate at now + 130    time.sleep(1);    assertTrue(task.maybePunctuateSystemTime());    // punctuate at now + 235    time.sleep(105);    assertTrue(task.maybePunctuateSystemTime());    assertFalse(task.maybePunctuateSystemTime());    // punctuate at now + 240, still aligned on the initial punctuation    time.sleep(5);    assertTrue(task.maybePunctuateSystemTime());    assertFalse(task.maybePunctuateSystemTime());    processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 100, now + 110, now + 122, now + 130, now + 235, now + 240);}
public void kafkatest_f17705_0()
{    task = createTaskThatThrowsException(false);    task.initializeStateStores();    task.initializeTopology();    task.addRecords(partition2, singletonList(getConsumerRecord(partition2, 0)));    try {        task.process();        fail("Should've thrown StreamsException");    } catch (final Exception e) {        assertThat(task.processorContext.currentNode(), nullValue());    }}
public void kafkatest_f17714_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    task.processorContext.setCurrentNode(processorStreamTime);    try {        task.punctuate(processorStreamTime, 10, PunctuationType.STREAM_TIME, punctuator);        fail("Should throw illegal state exception as current node is not null");    } catch (final IllegalStateException e) {    // pass    }}
public void kafkatest_f17715_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    task.punctuate(processorStreamTime, 5, PunctuationType.STREAM_TIME, punctuator);    assertThat(punctuatedAt, equalTo(5L));    task.punctuate(processorStreamTime, 10, PunctuationType.STREAM_TIME, punctuator);    assertThat(punctuatedAt, equalTo(10L));}
public void kafkatest_f17724_0()
{    task = createTaskThatThrowsException(false);    task.close(false, false);    task = null;    assertFalse(producer.closed());}
public void kafkatest_f17725_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    task.close(true, false);    task = null;    assertTrue(producer.transactionCommitted());    assertFalse(producer.transactionInFlight());    assertTrue(producer.closed());}
public void kafkatest_f17734_0()
{    task = createTaskThatThrowsException(false);    task.initializeStateStores();    task.initializeTopology();    try {        task.close(true, false);        fail("should have thrown runtime exception");    } catch (final RuntimeException expected) {        task = null;    }    assertTrue(processorSystemTime.closed);    assertTrue(processorStreamTime.closed);    assertTrue(source1.closed);}
public void kafkatest_f17735_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    assertTrue(producer.transactionInitialized());    assertTrue(producer.transactionInFlight());}
public void kafkatest_f17744_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0)));    task.process();    task.suspend();    task.resume();    task.initializeTopology();    assertTrue(producer.transactionInFlight());}
public void kafkatest_f17745_0()
{    task = createStatelessTask(createConfig(false));    task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0)));    task.process();    task.suspend();    task.resume();    assertFalse(producer.transactionInFlight());}
public void kafkatest_f17754_0()
{    final StreamTask task = createTaskThatThrowsException(false);    task.initializeStateStores();    task.initializeTopology();    try {        task.suspend();        fail("should have thrown an exception");    } catch (final Exception e) {    // all good    }}
public void kafkatest_f17755_0()
{    task = createStatefulTaskThatThrowsExceptionOnClose();    task.initializeStateStores();    task.initializeTopology();    try {        task.close(true, false);        fail("should have thrown an exception");    } catch (final Exception e) {    // all good    }    task = null;    assertFalse(stateStore.isOpen());}
public void kafkatest_f17764_0()
{    final Consumer<byte[], byte[]> consumer = mockConsumerWithCommittedException(new KafkaException("message"));    final AbstractTask task = createOptimizedStatefulTask(createConfig(false), consumer);    task.initializeStateStores();}
public void kafkatest_f17765_0()
{    final Consumer<byte[], byte[]> consumer = mockConsumerWithCommittedException(new WakeupException());    final AbstractTask task = createOptimizedStatefulTask(createConfig(false), consumer);    task.initializeStateStores();}
private ConsumerRecord<byte[], byte[]> kafkatest_f17774_0(final TopicPartition topicPartition, final long offset)
{    return new ConsumerRecord<>(topicPartition.topic(), topicPartition.partition(), offset, // use the offset as the timestamp    offset, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, recordValue);}
public void kafkatest_f17775_0()
{    processId = UUID.randomUUID();    internalTopologyBuilder = InternalStreamsBuilderTest.internalTopologyBuilder(internalStreamsBuilder);    internalTopologyBuilder.setApplicationId(applicationId);    streamsMetadataState = new StreamsMetadataState(internalTopologyBuilder, StreamsMetadataState.UNKNOWN_HOST);}
public void kafkatest_f17784_0()
{    final long commitInterval = 1000L;    final Properties props = configProps(false);    props.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);    props.setProperty(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Long.toString(commitInterval));    final StreamsConfig config = new StreamsConfig(props);    final Consumer<byte[], byte[]> consumer = EasyMock.createNiceMock(Consumer.class);    final TaskManager taskManager = mockTaskManagerCommit(consumer, 1, 0);    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId, StreamsConfig.METRICS_LATEST);    final StreamThread thread = new StreamThread(mockTime, config, null, consumer, consumer, null, taskManager, streamsMetrics, internalTopologyBuilder, clientId, new LogContext(""), new AtomicInteger());    thread.setNow(mockTime.milliseconds());    thread.maybeCommit();    mockTime.sleep(commitInterval - 10L);    thread.setNow(mockTime.milliseconds());    thread.maybeCommit();    EasyMock.verify(taskManager);}
public void kafkatest_f17785_0()
{    final long commitInterval = 1000L;    final Properties props = configProps(false);    props.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);    props.setProperty(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Long.toString(commitInterval));    final StreamsConfig config = new StreamsConfig(props);    final Consumer<byte[], byte[]> consumer = EasyMock.createNiceMock(Consumer.class);    final TaskManager taskManager = mockTaskManagerCommit(consumer, 2, 1);    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId, StreamsConfig.METRICS_LATEST);    final StreamThread thread = new StreamThread(mockTime, config, null, consumer, consumer, null, taskManager, streamsMetrics, internalTopologyBuilder, clientId, new LogContext(""), new AtomicInteger());    thread.setNow(mockTime.milliseconds());    thread.maybeCommit();    mockTime.sleep(commitInterval + 1);    thread.setNow(mockTime.milliseconds());    thread.maybeCommit();    EasyMock.verify(taskManager);}
private void kafkatest_f17794_0(final boolean shutdownOnPoll)
{    final Collection<TopicPartition> assignedPartitions = Collections.singletonList(t1p1);    class MockStreamThreadConsumer<K, V> extends MockConsumer<K, V> {        private StreamThread streamThread;        private MockStreamThreadConsumer(final OffsetResetStrategy offsetResetStrategy) {            super(offsetResetStrategy);        }        @Override        public synchronized ConsumerRecords<K, V> poll(final Duration timeout) {            assertNotNull(streamThread);            if (shutdownOnPoll) {                streamThread.shutdown();            }            streamThread.rebalanceListener.onPartitionsAssigned(assignedPartitions);            return super.poll(timeout);        }        private void setStreamThread(final StreamThread streamThread) {            this.streamThread = streamThread;        }    }    final MockStreamThreadConsumer<byte[], byte[]> mockStreamThreadConsumer = new MockStreamThreadConsumer<>(OffsetResetStrategy.EARLIEST);    final TaskManager taskManager = new TaskManager(new MockChangelogReader(), processId, "log-prefix", mockStreamThreadConsumer, streamsMetadataState, null, null, null, new AssignedStreamsTasks(new LogContext()), new AssignedStandbyTasks(new LogContext()));    taskManager.setConsumer(mockStreamThreadConsumer);    taskManager.setAssignmentMetadata(Collections.emptyMap(), Collections.emptyMap());    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId, StreamsConfig.METRICS_LATEST);    final StreamThread thread = new StreamThread(mockTime, config, null, mockStreamThreadConsumer, mockStreamThreadConsumer, null, taskManager, streamsMetrics, internalTopologyBuilder, clientId, new LogContext(""), new AtomicInteger()).updateThreadMetadata(getSharedAdminClientId(clientId));    mockStreamThreadConsumer.setStreamThread(thread);    mockStreamThreadConsumer.assign(assignedPartitions);    mockStreamThreadConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));    addRecord(mockStreamThreadConsumer, 1L, 0L);    thread.setState(StreamThread.State.STARTING);    thread.setState(StreamThread.State.PARTITIONS_REVOKED);    thread.runOnce();}
public synchronized ConsumerRecords<K, V> kafkatest_f17795_0(final Duration timeout)
{    assertNotNull(streamThread);    if (shutdownOnPoll) {        streamThread.shutdown();    }    streamThread.rebalanceListener.onPartitionsAssigned(assignedPartitions);    return super.poll(timeout);}
public void kafkatest_f17804_0()
{    internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).groupByKey().count(Materialized.as("count-one"));    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread(clientId, config, false);    final MockConsumer<byte[], byte[]> restoreConsumer = clientSupplier.restoreConsumer;    restoreConsumer.updatePartitions("stream-thread-test-count-one-changelog", singletonList(new PartitionInfo("stream-thread-test-count-one-changelog", 0, null, new Node[0], new Node[0])));    final HashMap<TopicPartition, Long> offsets = new HashMap<>();    offsets.put(new TopicPartition("stream-thread-test-count-one-changelog", 1), 0L);    restoreConsumer.updateEndOffsets(offsets);    restoreConsumer.updateBeginningOffsets(offsets);    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(null);    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    // assign single partition    standbyTasks.put(task1, Collections.singleton(t1p1));    thread.taskManager().setAssignmentMetadata(Collections.emptyMap(), standbyTasks);    thread.rebalanceListener.onPartitionsAssigned(Collections.emptyList());    thread.runOnce();    final ThreadMetadata threadMetadata = thread.threadMetadata();    assertEquals(StreamThread.State.RUNNING.name(), threadMetadata.threadState());    assertTrue(threadMetadata.standbyTasks().contains(new TaskMetadata(task1.toString(), Utils.mkSet(t1p1))));    assertTrue(threadMetadata.activeTasks().isEmpty());}
public void kafkatest_f17805_0() throws Exception
{    final String storeName1 = "count-one";    final String storeName2 = "table-two";    final String changelogName1 = applicationId + "-" + storeName1 + "-changelog";    final String changelogName2 = applicationId + "-" + storeName2 + "-changelog";    final TopicPartition partition1 = new TopicPartition(changelogName1, 1);    final TopicPartition partition2 = new TopicPartition(changelogName2, 1);    internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).groupByKey().count(Materialized.as(storeName1));    final MaterializedInternal<Object, Object, KeyValueStore<Bytes, byte[]>> materialized = new MaterializedInternal<>(Materialized.as(storeName2), internalStreamsBuilder, "");    internalStreamsBuilder.table(topic2, new ConsumedInternal<>(), materialized);    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread(clientId, config, false);    final MockConsumer<byte[], byte[]> restoreConsumer = clientSupplier.restoreConsumer;    restoreConsumer.updatePartitions(changelogName1, singletonList(new PartitionInfo(changelogName1, 1, null, new Node[0], new Node[0])));    restoreConsumer.assign(Utils.mkSet(partition1, partition2));    restoreConsumer.updateEndOffsets(Collections.singletonMap(partition1, 10L));    restoreConsumer.updateBeginningOffsets(Collections.singletonMap(partition1, 0L));    restoreConsumer.updateEndOffsets(Collections.singletonMap(partition2, 10L));    restoreConsumer.updateBeginningOffsets(Collections.singletonMap(partition2, 0L));    // let the store1 be restored from 0 to 10; store2 be restored from 5 (checkpointed) to 10    final OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(stateDirectory.directoryForTask(task3), CHECKPOINT_FILE_NAME));    checkpoint.write(Collections.singletonMap(partition2, 5L));    for (long i = 0L; i < 10L; i++) {        restoreConsumer.addRecord(new ConsumerRecord<>(changelogName1, 1, i, ("K" + i).getBytes(), ("V" + i).getBytes()));        restoreConsumer.addRecord(new ConsumerRecord<>(changelogName2, 1, i, ("K" + i).getBytes(), ("V" + i).getBytes()));    }    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(null);    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    // assign single partition    standbyTasks.put(task1, Collections.singleton(t1p1));    standbyTasks.put(task3, Collections.singleton(t2p1));    thread.taskManager().setAssignmentMetadata(Collections.emptyMap(), standbyTasks);    thread.rebalanceListener.onPartitionsAssigned(Collections.emptyList());    thread.runOnce();    final StandbyTask standbyTask1 = thread.taskManager().standbyTask(partition1);    final StandbyTask standbyTask2 = thread.taskManager().standbyTask(partition2);    final KeyValueStore<Object, Long> store1 = (KeyValueStore<Object, Long>) standbyTask1.getStore(storeName1);    final KeyValueStore<Object, Long> store2 = (KeyValueStore<Object, Long>) standbyTask2.getStore(storeName2);    assertEquals(10L, store1.approximateNumEntries());    assertEquals(5L, store2.approximateNumEntries());    assertEquals(0, thread.standbyRecords().size());}
public void kafkatest_f17816_0()
{    internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).groupByKey().count(Materialized.as("count-one"));    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread(clientId, config, false);    final MockConsumer<byte[], byte[]> restoreConsumer = clientSupplier.restoreConsumer;    restoreConsumer.updatePartitions("stream-thread-test-count-one-changelog", asList(new PartitionInfo("stream-thread-test-count-one-changelog", 0, null, new Node[0], new Node[0]), new PartitionInfo("stream-thread-test-count-one-changelog", 1, null, new Node[0], new Node[0])));    final HashMap<TopicPartition, Long> offsets = new HashMap<>();    offsets.put(new TopicPartition("stream-thread-test-count-one-changelog", 0), 0L);    offsets.put(new TopicPartition("stream-thread-test-count-one-changelog", 1), 0L);    restoreConsumer.updateEndOffsets(offsets);    restoreConsumer.updateBeginningOffsets(offsets);    clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));    final List<TopicPartition> assignedPartitions = new ArrayList<>();    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(assignedPartitions);    assertThreadMetadataHasEmptyTasksWithState(thread.threadMetadata(), StreamThread.State.PARTITIONS_REVOKED);    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    // assign single partition    assignedPartitions.add(t1p1);    activeTasks.put(task1, Collections.singleton(t1p1));    standbyTasks.put(task2, Collections.singleton(t1p2));    thread.taskManager().setAssignmentMetadata(activeTasks, standbyTasks);    thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);    assertThreadMetadataHasEmptyTasksWithState(thread.threadMetadata(), StreamThread.State.PARTITIONS_ASSIGNED);}
public void kafkatest_f17817_0() throws Exception
{    internalStreamsBuilder.stream(Collections.singleton("topic"), consumed).groupByKey().count(Materialized.as("count"));    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread("clientId", config, false);    final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;    final MockConsumer<byte[], byte[]> mockRestoreConsumer = (MockConsumer<byte[], byte[]>) thread.restoreConsumer;    final TopicPartition topicPartition = new TopicPartition("topic", 0);    final Set<TopicPartition> topicPartitionSet = Collections.singleton(topicPartition);    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    activeTasks.put(new TaskId(0, 0), topicPartitionSet);    thread.taskManager().setAssignmentMetadata(activeTasks, Collections.emptyMap());    mockConsumer.updatePartitions("topic", singletonList(new PartitionInfo("topic", 0, null, new Node[0], new Node[0])));    mockConsumer.updateBeginningOffsets(Collections.singletonMap(topicPartition, 0L));    mockRestoreConsumer.updatePartitions("stream-thread-test-count-changelog", singletonList(new PartitionInfo("stream-thread-test-count-changelog", 0, null, new Node[0], new Node[0])));    final TopicPartition changelogPartition = new TopicPartition("stream-thread-test-count-changelog", 0);    final Set<TopicPartition> changelogPartitionSet = Collections.singleton(changelogPartition);    mockRestoreConsumer.updateBeginningOffsets(Collections.singletonMap(changelogPartition, 0L));    mockRestoreConsumer.updateEndOffsets(Collections.singletonMap(changelogPartition, 2L));    mockConsumer.schedulePollTask(() -> {        thread.setState(StreamThread.State.PARTITIONS_REVOKED);        thread.rebalanceListener.onPartitionsAssigned(topicPartitionSet);    });    try {        thread.start();        TestUtils.waitForCondition(() -> mockRestoreConsumer.assignment().size() == 1, "Never restore first record");        mockRestoreConsumer.addRecord(new ConsumerRecord<>("stream-thread-test-count-changelog", 0, 0L, "K1".getBytes(), "V1".getBytes()));        TestUtils.waitForCondition(() -> mockRestoreConsumer.position(changelogPartition) == 1L, "Never restore first record");        mockRestoreConsumer.setPollException(new InvalidOffsetException("Try Again!") {            @Override            public Set<TopicPartition> partitions() {                return changelogPartitionSet;            }        });        mockRestoreConsumer.addRecord(new ConsumerRecord<>("stream-thread-test-count-changelog", 0, 0L, "K1".getBytes(), "V1".getBytes()));        mockRestoreConsumer.addRecord(new ConsumerRecord<>("stream-thread-test-count-changelog", 0, 1L, "K2".getBytes(), "V2".getBytes()));        TestUtils.waitForCondition(() -> {            mockRestoreConsumer.assign(changelogPartitionSet);            return mockRestoreConsumer.position(changelogPartition) == 2L;        }, "Never finished restore");    } finally {        thread.shutdown();        thread.join(10000);    }}
public void kafkatest_f17826_0()
{    taskManager = new TaskManager(changeLogReader, UUID.randomUUID(), "", restoreConsumer, streamsMetadataState, activeTaskCreator, standbyTaskCreator, adminClient, active, standby);    taskManager.setConsumer(consumer);}
private void kafkatest_f17827_0()
{    EasyMock.replay(changeLogReader, restoreConsumer, consumer, activeTaskCreator, standbyTaskCreator, active, standby, adminClient);}
public void kafkatest_f17836_0()
{    checkOrder(active, true);    expect(active.maybeResumeSuspendedTask(taskId0, taskId0Partitions)).andReturn(true);    replay();    taskManager.setAssignmentMetadata(taskId0Assignment, Collections.<TaskId, Set<TopicPartition>>emptyMap());    taskManager.createTasks(taskId0Partitions);    // should be no calls to activeTaskCreator and no calls to active.addNewTasks(..)    verify(active, activeTaskCreator);}
public void kafkatest_f17837_0()
{    mockStandbyTaskExpectations();    expect(standby.maybeResumeSuspendedTask(taskId0, taskId0Partitions)).andReturn(false);    standby.addNewTask(EasyMock.same(standbyTask));    replay();    taskManager.setAssignmentMetadata(Collections.<TaskId, Set<TopicPartition>>emptyMap(), taskId0Assignment);    taskManager.createTasks(taskId0Partitions);    verify(standbyTaskCreator, active);}
public void kafkatest_f17846_0()
{    restoreConsumer.unsubscribe();    expectLastCall();    replay();    taskManager.shutdown(true);    verify(restoreConsumer);}
public void kafkatest_f17847_0()
{    active.updateRestored(EasyMock.<Collection<TopicPartition>>anyObject());    expectLastCall();    replay();    taskManager.updateNewAndRestoringTasks();    verify(active);}
public void kafkatest_f17856_0()
{    mockAssignStandbyPartitions(-1L);    restoreConsumer.seekToBeginning(taskId0Partitions);    expectLastCall();    replay();    taskManager.updateNewAndRestoringTasks();    verify(restoreConsumer);}
public void kafkatest_f17857_0()
{    expect(active.commit()).andReturn(1);    expect(standby.commit()).andReturn(2);    replay();    assertThat(taskManager.commitAll(), equalTo(3));    verify(active, standby);}
public void kafkatest_f17866_0()
{    expect(active.allTasksRunning()).andReturn(false);    final Consumer<byte[], byte[]> consumer = EasyMock.createStrictMock(Consumer.class);    taskManager.setConsumer(consumer);    EasyMock.replay(active, consumer);    // shouldn't invoke `resume` method in consumer    taskManager.updateNewAndRestoringTasks();    EasyMock.verify(consumer);}
public void kafkatest_f17867_0()
{    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    taskManager.setAssignmentMetadata(activeTasks, standbyTasks);    assertTrue(taskManager.assignedActiveTasks().isEmpty());    // assign two active tasks with two partitions each    activeTasks.put(task01, new HashSet<>(asList(t1p1, t2p1)));    activeTasks.put(task02, new HashSet<>(asList(t1p2, t2p2)));    // assign one standby task with two partitions    standbyTasks.put(task03, new HashSet<>(asList(t1p3, t2p3)));    taskManager.setAssignmentMetadata(activeTasks, standbyTasks);    assertThat(taskManager.assignedActiveTasks(), equalTo(activeTasks));    assertThat(taskManager.assignedStandbyTasks(), equalTo(standbyTasks));}
public static void kafkatest_f17876_0(final Class<?> clazz)
{    Logger.getLogger(clazz).setLevel(Level.DEBUG);}
public static void kafkatest_f17877_0(final LogCaptureAppender logCaptureAppender)
{    Logger.getRootLogger().removeAppender(logCaptureAppender);}
public void kafkatest_f17887_0()
{    final KeyValueBytesStoreSupplier supplier = EasyMock.createNiceMock(KeyValueBytesStoreSupplier.class);    final InMemoryKeyValueStore store = new InMemoryKeyValueStore("name");    EasyMock.expect(supplier.name()).andReturn("name").anyTimes();    EasyMock.expect(supplier.get()).andReturn(store);    EasyMock.replay(supplier);    final MaterializedInternal<String, Integer, KeyValueStore<Bytes, byte[]>> materialized = new MaterializedInternal<>(Materialized.as(supplier), nameProvider, storePrefix);    final TimestampedKeyValueStoreMaterializer<String, Integer> materializer = new TimestampedKeyValueStoreMaterializer<>(materialized);    final StoreBuilder<TimestampedKeyValueStore<String, Integer>> builder = materializer.materialize();    final TimestampedKeyValueStore<String, Integer> built = builder.build();    assertThat(store.name(), CoreMatchers.equalTo(built.name()));}
public void kafkatest_f17888_0()
{    testExtractMetadataTimestamp(new LogAndSkipOnInvalidTimestamp());}
public void kafkatest_f17899_0()
{    store.close();    driver.clear();}
private static Map<Integer, String> kafkatest_f17900_0(final KeyValueIterator<Integer, String> iter)
{    final HashMap<Integer, String> result = new HashMap<>();    while (iter.hasNext()) {        final KeyValue<Integer, String> entry = iter.next();        result.put(entry.key, entry.value);    }    return result;}
public void kafkatest_f17909_0()
{    // Verify that the store reads and writes correctly ...    assertNull(store.putIfAbsent(0, "zero"));    assertNull(store.putIfAbsent(1, "one"));    assertNull(store.putIfAbsent(2, "two"));    assertNull(store.putIfAbsent(4, "four"));    assertEquals("four", store.putIfAbsent(4, "unexpected value"));    assertEquals(4, driver.sizeOf(store));    assertEquals("zero", store.get(0));    assertEquals("one", store.get(1));    assertEquals("two", store.get(2));    assertNull(store.get(3));    assertEquals("four", store.get(4));    // Flush the store and verify all current entries were properly flushed ...    store.flush();    assertEquals("zero", driver.flushedEntryStored(0));    assertEquals("one", driver.flushedEntryStored(1));    assertEquals("two", driver.flushedEntryStored(2));    assertEquals("four", driver.flushedEntryStored(4));    assertFalse(driver.flushedEntryRemoved(0));    assertFalse(driver.flushedEntryRemoved(1));    assertFalse(driver.flushedEntryRemoved(2));    assertFalse(driver.flushedEntryRemoved(4));}
public void kafkatest_f17910_0()
{    store.put(null, "anyValue");}
public void kafkatest_f17919_0()
{    store.range(2, null);}
public void kafkatest_f17920_0()
{    assertEquals("A newly created store should have no entries", 0, store.approximateNumEntries());    store.put(0, "zero");    store.put(1, "one");    store.put(2, "two");    store.put(4, "four");    store.put(5, "five");    store.flush();    assertEquals(5, store.approximateNumEntries());}
public void kafkatest_f17929_0()
{    final String key = "a";    bytesStore.put(serializeKey(new Windowed<>(key, windows[0])), serializeValue(10));    bytesStore.put(serializeKey(new Windowed<>(key, windows[1])), serializeValue(50));    bytesStore.put(serializeKey(new Windowed<>(key, windows[2])), serializeValue(100));    final KeyValueIterator<Bytes, byte[]> values = bytesStore.fetch(Bytes.wrap(key.getBytes()), 0, 500);    final List<KeyValue<Windowed<String>, Long>> expected = Arrays.asList(KeyValue.pair(new Windowed<>(key, windows[0]), 10L), KeyValue.pair(new Windowed<>(key, windows[1]), 50L));    assertEquals(expected, toList(values));}
public void kafkatest_f17930_0()
{    final String key = "a";    bytesStore.put(serializeKey(new Windowed<>(key, windows[0])), serializeValue(10));    bytesStore.put(serializeKey(new Windowed<>(key, windows[1])), serializeValue(50));    bytesStore.put(serializeKey(new Windowed<>(key, windows[2])), serializeValue(100));    final KeyValueIterator<Bytes, byte[]> results = bytesStore.fetch(Bytes.wrap(key.getBytes()), 1, 999);    final List<KeyValue<Windowed<String>, Long>> expected = Arrays.asList(KeyValue.pair(new Windowed<>(key, windows[0]), 10L), KeyValue.pair(new Windowed<>(key, windows[1]), 50L));    assertEquals(expected, toList(results));}
public void kafkatest_f17939_0()
{    // 0 segments initially.    assertEquals(0, bytesStore.getSegments().size());    final String key = "a";    final Collection<KeyValue<byte[], byte[]>> records = new ArrayList<>();    records.add(new KeyValue<>(serializeKey(new Windowed<>(key, windows[0])).get(), serializeValue(50L)));    records.add(new KeyValue<>(serializeKey(new Windowed<>(key, windows[3])).get(), serializeValue(100L)));    bytesStore.restoreAllInternal(records);    // 2 segments are created during restoration.    assertEquals(2, bytesStore.getSegments().size());    // Bulk loading is enabled during recovery.    for (final S segment : bytesStore.getSegments()) {        assertThat(getOptions(segment).level0FileNumCompactionTrigger(), equalTo(1 << 30));    }    final List<KeyValue<Windowed<String>, Long>> expected = new ArrayList<>();    expected.add(new KeyValue<>(new Windowed<>(key, windows[0]), 50L));    expected.add(new KeyValue<>(new Windowed<>(key, windows[3]), 100L));    final List<KeyValue<Windowed<String>, Long>> results = toList(bytesStore.all());    assertEquals(expected, results);}
public void kafkatest_f17940_0()
{    bytesStore.init(context, bytesStore);    final String key = "a";    bytesStore.put(serializeKey(new Windowed<>(key, windows[0])), serializeValue(50L));    bytesStore.put(serializeKey(new Windowed<>(key, windows[3])), serializeValue(100L));    assertEquals(2, bytesStore.getSegments().size());    final StateRestoreListener restoreListener = context.getRestoreListener(bytesStore.name());    restoreListener.onRestoreStart(null, bytesStore.name(), 0L, 0L);    for (final S segment : bytesStore.getSegments()) {        assertThat(getOptions(segment).level0FileNumCompactionTrigger(), equalTo(1 << 30));    }    restoreListener.onRestoreEnd(null, bytesStore.name(), 0L);    for (final S segment : bytesStore.getSegments()) {        assertThat(getOptions(segment).level0FileNumCompactionTrigger(), equalTo(4));    }}
public void kafkatest_f17949_0()
{    final byte[] priorValue = { (byte) 0 };    final byte[] oldValue = { (byte) 1 };    final BufferValue bufferValue = new BufferValue(priorValue, oldValue, null, null);    assertSame(priorValue, bufferValue.priorValue());    assertSame(oldValue, bufferValue.oldValue());    assertNotEquals(bufferValue.priorValue(), bufferValue.oldValue());}
public void kafkatest_f17950_0()
{    final byte[] priorValue = null;    final byte[] oldValue = { (byte) 1 };    final BufferValue bufferValue = new BufferValue(priorValue, oldValue, null, null);    assertNull(bufferValue.priorValue());    assertSame(oldValue, bufferValue.oldValue());    assertNotEquals(bufferValue.priorValue(), bufferValue.oldValue());}
public void kafkatest_f17959_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(0L, 0L, 0, "topic", null);    final byte[] serializedContext = context.serialize();    final byte[] oldValue = { (byte) 5 };    final ByteBuffer serialValue = ByteBuffer.allocate(serializedContext.length + Integer.BYTES * 3 + oldValue.length).put(serializedContext).putInt(-1).putInt(1).put(oldValue).putInt(-1);    serialValue.position(0);    assertThat(BufferValue.deserialize(serialValue), is(new BufferValue(null, oldValue, null, context)));}
public void kafkatest_f17960_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(0L, 0L, 0, "topic", null);    final byte[] serializedContext = context.serialize();    final byte[] newValue = { (byte) 5 };    final ByteBuffer serialValue = ByteBuffer.allocate(serializedContext.length + Integer.BYTES * 3 + newValue.length).put(serializedContext).putInt(-1).putInt(-1).putInt(1).put(newValue);    serialValue.position(0);    assertThat(BufferValue.deserialize(serialValue), is(new BufferValue(null, null, newValue, context)));}
private byte[] kafkatest_f17969_0(final String value)
{    return value.getBytes();}
private Bytes kafkatest_f17970_0(final String key)
{    return Bytes.wrap(key.getBytes());}
public void kafkatest_f17979_0()
{    store.put(bytesKey("a"), bytesValue("a"));    store.flush();    store.delete(bytesKey("a"));    assertNull(store.get(bytesKey("a")));    assertFalse(store.range(bytesKey("a"), bytesKey("b")).hasNext());    assertFalse(store.all().hasNext());}
public void kafkatest_f17980_0()
{    store.put(bytesKey("a"), bytesValue("a"));    assertEquals(1, cache.size());    store.close();    assertEquals(0, cache.size());}
public void kafkatest_f17989_0()
{    store.putIfAbsent(null, bytesValue("c"));}
public void kafkatest_f17990_0()
{    final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();    entries.add(new KeyValue<>(null, bytesValue("a")));    try {        store.putAll(entries);        fail("Should have thrown NullPointerException while putAll null key");    } catch (final NullPointerException expected) {    }}
public void kafkatest_f17999_0()
{    cachingStore.put(new Windowed<>(keyA, new SessionWindow(0, 0)), "1".getBytes());    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(0, 0)), "1".getBytes());    cachingStore.put(new Windowed<>(keyB, new SessionWindow(0, 0)), "1".getBytes());    assertEquals(3, cache.size());    final KeyValueIterator<Windowed<Bytes>, byte[]> all = cachingStore.findSessions(keyA, keyB, 0, 0);    verifyWindowedKeyValue(all.next(), new Windowed<>(keyA, new SessionWindow(0, 0)), "1");    verifyWindowedKeyValue(all.next(), new Windowed<>(keyAA, new SessionWindow(0, 0)), "1");    verifyWindowedKeyValue(all.next(), new Windowed<>(keyB, new SessionWindow(0, 0)), "1");    assertFalse(all.hasNext());}
public void kafkatest_f18000_0()
{    cachingStore.put(new Windowed<>(keyA, new SessionWindow(0, 0)), "1".getBytes());    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(0, 0)), "1".getBytes());    cachingStore.put(new Windowed<>(keyB, new SessionWindow(0, 0)), "1".getBytes());    assertEquals(3, cache.size());    final KeyValueIterator<Windowed<Bytes>, byte[]> some = cachingStore.findSessions(keyAA, keyB, 0, 0);    verifyWindowedKeyValue(some.next(), new Windowed<>(keyAA, new SessionWindow(0, 0)), "1");    verifyWindowedKeyValue(some.next(), new Windowed<>(keyB, new SessionWindow(0, 0)), "1");    assertFalse(some.hasNext());}
public void kafkatest_f18009_0()
{    final Windowed<Bytes> a = new Windowed<>(keyA, new SessionWindow(0, 0));    final Windowed<String> aDeserialized = new Windowed<>("a", new SessionWindow(0, 0));    final CacheFlushListenerStub<Windowed<String>, String> flushListener = new CacheFlushListenerStub<>(new SessionWindowedDeserializer<>(new StringDeserializer()), new StringDeserializer());    cachingStore.setFlushListener(flushListener, false);    cachingStore.put(a, "1".getBytes());    cachingStore.flush();    cachingStore.put(a, "2".getBytes());    cachingStore.flush();    cachingStore.remove(a);    cachingStore.flush();    assertEquals(asList(new KeyValueTimestamp<>(aDeserialized, new Change<>("1", null), DEFAULT_TIMESTAMP), new KeyValueTimestamp<>(aDeserialized, new Change<>("2", null), DEFAULT_TIMESTAMP), new KeyValueTimestamp<>(aDeserialized, new Change<>(null, null), DEFAULT_TIMESTAMP)), flushListener.forwarded);    flushListener.forwarded.clear();    cachingStore.put(a, "1".getBytes());    cachingStore.put(a, "2".getBytes());    cachingStore.remove(a);    cachingStore.flush();    assertEquals(Collections.emptyList(), flushListener.forwarded);    flushListener.forwarded.clear();}
public void kafkatest_f18010_0()
{    cachingStore.put(new Windowed<>(keyA, new SessionWindow(0, 1)), "1".getBytes());    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(2, 3)), "2".getBytes());    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(4, 5)), "3".getBytes());    cachingStore.put(new Windowed<>(keyB, new SessionWindow(6, 7)), "4".getBytes());    final KeyValueIterator<Windowed<Bytes>, byte[]> singleKeyIterator = cachingStore.findSessions(keyAA, 0L, 10L);    final KeyValueIterator<Windowed<Bytes>, byte[]> keyRangeIterator = cachingStore.findSessions(keyAA, keyAA, 0L, 10L);    assertEquals(singleKeyIterator.next(), keyRangeIterator.next());    assertEquals(singleKeyIterator.next(), keyRangeIterator.next());    assertFalse(singleKeyIterator.hasNext());    assertFalse(keyRangeIterator.hasNext());}
public void kafkatest_f18019_0()
{    cachingStore.fetch(null, keyA);}
public void kafkatest_f18020_0()
{    cachingStore.fetch(keyA, null);}
public void kafkatest_f18029_0()
{    cachingStore.close();}
public void kafkatest_f18030_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final StoreBuilder<WindowStore<String, String>> storeBuilder = Stores.windowStoreBuilder(Stores.persistentWindowStore("store-name", ofHours(1L), ofMinutes(1L), false), Serdes.String(), Serdes.String()).withCachingEnabled();    builder.addStateStore(storeBuilder);    builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String())).transform(() -> new Transformer<String, String, KeyValue<String, String>>() {        private WindowStore<String, String> store;        private int numRecordsProcessed;        @SuppressWarnings("unchecked")        @Override        public void init(final ProcessorContext processorContext) {            this.store = (WindowStore<String, String>) processorContext.getStateStore("store-name");            int count = 0;            final KeyValueIterator<Windowed<String>, String> all = store.all();            while (all.hasNext()) {                count++;                all.next();            }            assertThat(count, equalTo(0));        }        @Override        public KeyValue<String, String> transform(final String key, final String value) {            int count = 0;            final KeyValueIterator<Windowed<String>, String> all = store.all();            while (all.hasNext()) {                count++;                all.next();            }            assertThat(count, equalTo(numRecordsProcessed));            store.put(value, value);            numRecordsProcessed++;            return new KeyValue<>(key, value);        }        @Override        public void close() {        }    }, "store-name");    final String bootstrapServers = "localhost:9092";    final Properties streamsConfiguration = new Properties();    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, "test-app");    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 10 * 1000);    final long initialWallClockTime = 0L;    final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), streamsConfiguration, initialWallClockTime);    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(Serdes.String().serializer(), Serdes.String().serializer(), initialWallClockTime);    for (int i = 0; i < 5; i++) {        driver.pipeInput(recordFactory.create(topic, UUID.randomUUID().toString(), UUID.randomUUID().toString()));    }    driver.advanceWallClockTime(10 * 1000L);    recordFactory.advanceTimeMs(10 * 1000L);    for (int i = 0; i < 5; i++) {        driver.pipeInput(recordFactory.create(topic, UUID.randomUUID().toString(), UUID.randomUUID().toString()));    }    driver.advanceWallClockTime(10 * 1000L);    recordFactory.advanceTimeMs(10 * 1000L);    for (int i = 0; i < 5; i++) {        driver.pipeInput(recordFactory.create(topic, UUID.randomUUID().toString(), UUID.randomUUID().toString()));    }    driver.advanceWallClockTime(10 * 1000L);    recordFactory.advanceTimeMs(10 * 1000L);    for (int i = 0; i < 5; i++) {        driver.pipeInput(recordFactory.create(topic, UUID.randomUUID().toString(), UUID.randomUUID().toString()));    }}
public void kafkatest_f18040_0()
{    cachingStore.put(bytesKey("a"), bytesValue("a"));    cachingStore.put(bytesKey("b"), bytesValue("b"));    cachingStore.put(bytesKey("c"), bytesValue("c"));    cachingStore.put(bytesKey("d"), bytesValue("d"));    cachingStore.put(bytesKey("e"), bytesValue("e"));    cachingStore.put(bytesKey("f"), bytesValue("f"));    cachingStore.put(bytesKey("g"), bytesValue("g"));    cachingStore.put(bytesKey("h"), bytesValue("h"));    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.all();    final String[] array = { "a", "b", "c", "d", "e", "f", "g", "h" };    for (final String s : array) {        verifyWindowedKeyValue(iterator.next(), new Windowed<>(bytesKey(s), new TimeWindow(DEFAULT_TIMESTAMP, DEFAULT_TIMESTAMP + WINDOW_SIZE)), s);    }    assertFalse(iterator.hasNext());}
public void kafkatest_f18041_0()
{    final String[] array = { "a", "b", "c", "d", "e", "f", "g", "h" };    for (int i = 0; i < array.length; i++) {        context.setTime(i);        cachingStore.put(bytesKey(array[i]), bytesValue(array[i]));    }    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.fetchAll(ofEpochMilli(0), ofEpochMilli(7));    for (int i = 0; i < array.length; i++) {        final String str = array[i];        verifyWindowedKeyValue(iterator.next(), new Windowed<>(bytesKey(str), new TimeWindow(i, i + WINDOW_SIZE)), str);    }    assertFalse(iterator.hasNext());    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator1 = cachingStore.fetchAll(ofEpochMilli(2), ofEpochMilli(4));    for (int i = 2; i <= 4; i++) {        final String str = array[i];        verifyWindowedKeyValue(iterator1.next(), new Windowed<>(bytesKey(str), new TimeWindow(i, i + WINDOW_SIZE)), str);    }    assertFalse(iterator1.hasNext());    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator2 = cachingStore.fetchAll(ofEpochMilli(5), ofEpochMilli(7));    for (int i = 5; i <= 7; i++) {        final String str = array[i];        verifyWindowedKeyValue(iterator2.next(), new Windowed<>(bytesKey(str), new TimeWindow(i, i + WINDOW_SIZE)), str);    }    assertFalse(iterator2.hasNext());}
public void kafkatest_f18050_0()
{    final Bytes key = Bytes.wrap("1".getBytes());    underlying.put(WindowKeySchema.toStoreKeyBinary(key, DEFAULT_TIMESTAMP, 0), "a".getBytes());    cachingStore.put(key, bytesValue("b"), DEFAULT_TIMESTAMP + WINDOW_SIZE);    final WindowStoreIterator<byte[]> fetch = cachingStore.fetch(bytesKey("1"), ofEpochMilli(DEFAULT_TIMESTAMP), ofEpochMilli(DEFAULT_TIMESTAMP + WINDOW_SIZE));    verifyKeyValue(fetch.next(), DEFAULT_TIMESTAMP, "a");    verifyKeyValue(fetch.next(), DEFAULT_TIMESTAMP + WINDOW_SIZE, "b");    assertFalse(fetch.hasNext());}
public void kafkatest_f18051_0()
{    final Bytes key = Bytes.wrap("1".getBytes());    underlying.put(WindowKeySchema.toStoreKeyBinary(key, DEFAULT_TIMESTAMP, 0), "a".getBytes());    cachingStore.put(key, bytesValue("b"), DEFAULT_TIMESTAMP + WINDOW_SIZE);    final KeyValueIterator<Windowed<Bytes>, byte[]> fetchRange = cachingStore.fetch(key, bytesKey("2"), ofEpochMilli(DEFAULT_TIMESTAMP), ofEpochMilli(DEFAULT_TIMESTAMP + WINDOW_SIZE));    verifyWindowedKeyValue(fetchRange.next(), new Windowed<>(key, new TimeWindow(DEFAULT_TIMESTAMP, DEFAULT_TIMESTAMP + WINDOW_SIZE)), "a");    verifyWindowedKeyValue(fetchRange.next(), new Windowed<>(key, new TimeWindow(DEFAULT_TIMESTAMP + WINDOW_SIZE, DEFAULT_TIMESTAMP + WINDOW_SIZE + WINDOW_SIZE)), "b");    assertFalse(fetchRange.hasNext());}
public void kafkatest_f18060_0()
{    cachingStore.put(bytesKey("a"), null);}
public void kafkatest_f18061_0()
{    cachingStore.fetch(null, ofEpochMilli(1L), ofEpochMilli(2L));}
public void kafkatest_f18070_0()
{    store.put(hi, there);    assertThat(inner.get(hi), equalTo(there));}
public void kafkatest_f18071_0()
{    store.put(hi, there);    assertThat(sent.get(hi), equalTo(there));}
public void kafkatest_f18080_0()
{    store.put(hi, there);    store.putIfAbsent(hi, world);    assertThat(sent.get(hi), equalTo(there));}
public void kafkatest_f18081_0()
{    store.put(hi, there);    assertThat(store.putIfAbsent(hi, world), equalTo(there));}
public void kafkatest_f18090_0()
{    EasyMock.expect(inner.fetch(bytesKey)).andReturn(KeyValueIterators.<Windowed<Bytes>, byte[]>emptyIterator());    init();    store.fetch(bytesKey);    EasyMock.verify(inner);}
public void kafkatest_f18091_0()
{    EasyMock.expect(inner.fetch(bytesKey, bytesKey)).andReturn(KeyValueIterators.<Windowed<Bytes>, byte[]>emptyIterator());    init();    store.fetch(bytesKey, bytesKey);    EasyMock.verify(inner);}
public void kafkatest_f18100_0()
{    store.put(hi, rawThere);    final ValueAndTimestamp<byte[]> logged = sent.get(hi);    assertThat(logged.value(), equalTo(there.value()));    assertThat(logged.timestamp(), equalTo(there.timestamp()));}
public void kafkatest_f18101_0()
{    store.putAll(Arrays.asList(KeyValue.pair(hi, rawThere), KeyValue.pair(hello, rawWorld)));    assertThat(root.get(hi), equalTo(rawThere));    assertThat(root.get(hello), equalTo(rawWorld));}
public void kafkatest_f18110_0()
{    store.put(hi, rawThere);    assertThat(store.putIfAbsent(hi, rawWorld), equalTo(rawThere));}
public void kafkatest_f18111_0()
{    assertThat(store.putIfAbsent(hi, rawThere), is(nullValue()));}
public void kafkatest_f18120_0()
{    store = new ChangeLoggingTimestampedWindowBytesStore(inner, true);    inner.put(bytesKey, valueAndTimestamp, 0);    EasyMock.expectLastCall().times(2);    init();    store.put(bytesKey, valueAndTimestamp);    store.put(bytesKey, valueAndTimestamp);    assertArrayEquals(value, (byte[]) sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 1)).value());    assertEquals(42L, sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 1)).timestamp());    assertArrayEquals(value, (byte[]) sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 2)).value());    assertEquals(42L, sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 2)).timestamp());    EasyMock.verify(inner);}
public void kafkatest_f18121_0(final String topic, final K key, final V value, final Headers headers, final Integer partition, final Long timestamp, final Serializer<K> keySerializer, final Serializer<V> valueSerializer)
{    sent.put(key, value);}
public void kafkatest_f18130_0()
{    assertNull(theStore.get("whatever"));}
public void kafkatest_f18131_0()
{    theStore.get(null);}
public void kafkatest_f18140_0()
{    final KeyValueStore<String, String> cache = newStoreInstance();    stubProviderTwo.addStore(storeName, cache);    cache.put("key-two", "key-two-value");    stubOneUnderlying.put("key-one", "key-one-value");    assertEquals("key-two-value", theStore.get("key-two"));    assertEquals("key-one-value", theStore.get("key-one"));}
public void kafkatest_f18141_0()
{    stubOneUnderlying.put("a", "a");    stubOneUnderlying.put("b", "b");    stubOneUnderlying.put("c", "c");    final List<KeyValue<String, String>> results = toList(theStore.range("a", "b"));    assertTrue(results.contains(new KeyValue<>("a", "a")));    assertTrue(results.contains(new KeyValue<>("b", "b")));    assertEquals(2, results.size());}
public long kafkatest_f18150_0()
{    return Long.MAX_VALUE;}
public void kafkatest_f18151_0()
{    stubProviderTwo.addStore(storeName, new NoOpReadOnlyStore<Object, Object>() {        @Override        public long approximateNumEntries() {            return Long.MAX_VALUE;        }    });    stubProviderTwo.addStore(storeNameA, new NoOpReadOnlyStore<Object, Object>() {        @Override        public long approximateNumEntries() {            return Long.MAX_VALUE;        }    });    assertEquals(Long.MAX_VALUE, theStore.approximateNumEntries());}
public void kafkatest_f18160_0()
{    final CompositeReadOnlySessionStore<String, String> store = new CompositeReadOnlySessionStore<>(new StateStoreProviderStub(true), QueryableStoreTypes.sessionStore(), "whateva");    store.fetch("a");}
public void kafkatest_f18161_0()
{    underlyingSessionStore.setOpen(false);    try {        sessionStore.fetch("key");        fail("Should have thrown InvalidStateStoreException with session store");    } catch (final InvalidStateStoreException e) {    }}
public void kafkatest_f18170_0()
{    final ReadOnlyWindowStoreStub<String, String> secondUnderlying = new ReadOnlyWindowStoreStub<>(WINDOW_SIZE);    stubProviderTwo.addStore(storeName, secondUnderlying);    underlyingWindowStore.put("key-one", "value-one", 0L);    secondUnderlying.put("key-two", "value-two", 10L);    final List<KeyValue<Long, String>> keyOneResults = StreamsTestUtils.toList(windowStore.fetch("key-one", ofEpochMilli(0L), ofEpochMilli(1L)));    final List<KeyValue<Long, String>> keyTwoResults = StreamsTestUtils.toList(windowStore.fetch("key-two", ofEpochMilli(10L), ofEpochMilli(11L)));    assertEquals(Collections.singletonList(KeyValue.pair(0L, "value-one")), keyOneResults);    assertEquals(Collections.singletonList(KeyValue.pair(10L, "value-two")), keyTwoResults);}
public void kafkatest_f18171_0()
{    otherUnderlyingStore.put("some-key", "some-value", 0L);    underlyingWindowStore.put("some-key", "my-value", 1L);    final List<KeyValue<Long, String>> results = StreamsTestUtils.toList(windowStore.fetch("some-key", ofEpochMilli(0L), ofEpochMilli(2L)));    assertEquals(Collections.singletonList(new KeyValue<>(1L, "my-value")), results);}
public void kafkatest_f18180_0()
{    final ReadOnlyWindowStoreStub<String, String> secondUnderlying = new ReadOnlyWindowStoreStub<>(WINDOW_SIZE);    stubProviderTwo.addStore(storeName, secondUnderlying);    underlyingWindowStore.put("a", "a", 0L);    secondUnderlying.put("b", "b", 10L);    final List<KeyValue<Windowed<String>, String>> results = StreamsTestUtils.toList(windowStore.fetchAll(ofEpochMilli(0), ofEpochMilli(10)));    assertThat(results, equalTo(Arrays.asList(KeyValue.pair(new Windowed<>("a", new TimeWindow(0, WINDOW_SIZE)), "a"), KeyValue.pair(new Windowed<>("b", new TimeWindow(10, 10 + WINDOW_SIZE)), "b"))));}
public void kafkatest_f18181_0()
{    windowStore.fetch(null, ofEpochMilli(0), ofEpochMilli(0));}
public Bytes kafkatest_f18190_0(final Bytes cacheKey)
{    return cacheKey;}
public Bytes kafkatest_f18191_0(final Bytes key)
{    return key;}
public void kafkatest_f18200_0()
{    allIterator.remove();}
public void kafkatest_f18201_0()
{    stores.put("kv-store", Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore("kv-store"), Serdes.String(), Serdes.String()).build());    stores.put("ts-kv-store", Stores.timestampedKeyValueStoreBuilder(Stores.inMemoryKeyValueStore("ts-kv-store"), Serdes.String(), Serdes.String()).build());    stores.put("w-store", Stores.windowStoreBuilder(Stores.inMemoryWindowStore("w-store", Duration.ofMillis(10L), Duration.ofMillis(2L), false), Serdes.String(), Serdes.String()).build());    stores.put("ts-w-store", Stores.timestampedWindowStoreBuilder(Stores.inMemoryWindowStore("ts-w-store", Duration.ofMillis(10L), Duration.ofMillis(2L), false), Serdes.String(), Serdes.String()).build());    final ProcessorContextImpl mockContext = mock(ProcessorContextImpl.class);    expect(mockContext.applicationId()).andReturn("appId").anyTimes();    expect(mockContext.metrics()).andReturn(new StreamsMetricsImpl(new Metrics(), "threadName", StreamsConfig.METRICS_LATEST)).anyTimes();    expect(mockContext.taskId()).andReturn(new TaskId(0, 0)).anyTimes();    expect(mockContext.recordCollector()).andReturn(null).anyTimes();    replay(mockContext);    for (final StateStore store : stores.values()) {        store.init(mockContext, null);    }}
protected KeyValueStore<K, V> kafkatest_f18210_0(final ProcessorContext context)
{    final StoreBuilder storeBuilder = Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore("my-store"), (Serde<K>) context.keySerde(), (Serde<V>) context.valueSerde()).withLoggingEnabled(Collections.singletonMap("retention.ms", "1000"));    final StateStore store = storeBuilder.build();    store.init(context, store);    return (KeyValueStore<K, V>) store;}
protected KeyValueStore<K, V> kafkatest_f18211_0(final ProcessorContext context)
{    final StoreBuilder storeBuilder = Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore("my-store"), (Serde<K>) context.keySerde(), (Serde<V>) context.valueSerde());    final StateStore store = storeBuilder.build();    store.init(context, store);    return (KeyValueStore<K, V>) store;}
 void kafkatest_f18220_0()
{    LogCaptureAppender.setClassLoggerToDebug(InMemorySessionStore.class);}
public void kafkatest_f18221_0()
{    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 0)), 1L);    sessionStore.put(new Windowed<>("aa", new SessionWindow(0, 10)), 2L);    sessionStore.put(new Windowed<>("a", new SessionWindow(10, 20)), 3L);    // Advance stream time to expire the first record    sessionStore.put(new Windowed<>("aa", new SessionWindow(10, RETENTION_PERIOD)), 4L);    try (final KeyValueIterator<Windowed<String>, Long> iterator = sessionStore.findSessions("a", "b", 0L, Long.MAX_VALUE)) {        assertEquals(valuesToSet(iterator), new HashSet<>(Arrays.asList(2L, 3L, 4L)));    }}
public void kafkatest_f18230_0()
{    long currentTime = 0;    setCurrentTime(currentTime);    windowStore.put(1, "one");    currentTime += RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "two");    currentTime += RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "three");    currentTime += RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "four");    // increase current time to the full RETENTION_PERIOD to expire first record    currentTime = currentTime + RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "five");    KeyValueIterator<Windowed<Integer>, String> iterator = windowStore.fetchAll(0L, currentTime);    // effect of this put (expires next oldest record, adds new one) should not be reflected in the already fetched results    currentTime = currentTime + RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "six");    // should only have middle 4 values, as (only) the first record was expired at the time of the fetch    // and the last was inserted after the fetch    assertEquals(windowedPair(1, "two", RETENTION_PERIOD / 4), iterator.next());    assertEquals(windowedPair(1, "three", RETENTION_PERIOD / 2), iterator.next());    assertEquals(windowedPair(1, "four", 3 * (RETENTION_PERIOD / 4)), iterator.next());    assertEquals(windowedPair(1, "five", RETENTION_PERIOD), iterator.next());    assertFalse(iterator.hasNext());    iterator = windowStore.fetchAll(0L, currentTime);    // If we fetch again after the last put, the second oldest record should have expired and newest should appear in results    assertEquals(windowedPair(1, "three", RETENTION_PERIOD / 2), iterator.next());    assertEquals(windowedPair(1, "four", 3 * (RETENTION_PERIOD / 4)), iterator.next());    assertEquals(windowedPair(1, "five", RETENTION_PERIOD), iterator.next());    assertEquals(windowedPair(1, "six", 5 * (RETENTION_PERIOD / 4)), iterator.next());    assertFalse(iterator.hasNext());}
public void kafkatest_f18231_0()
{    keyValueIteratorFacade = new KeyValueIteratorFacade<>(mockedKeyValueIterator);}
public void kafkatest_f18240_0()
{    assertEquals("test.0", segments.segmentName(0));    assertEquals("test." + SEGMENT_INTERVAL, segments.segmentName(1));    assertEquals("test." + 2 * SEGMENT_INTERVAL, segments.segmentName(2));}
public void kafkatest_f18241_0()
{    final KeyValueSegment segment1 = segments.getOrCreateSegmentIfLive(0, context, -1L);    final KeyValueSegment segment2 = segments.getOrCreateSegmentIfLive(1, context, -1L);    final KeyValueSegment segment3 = segments.getOrCreateSegmentIfLive(2, context, -1L);    assertTrue(new File(context.stateDir(), "test/test.0").isDirectory());    assertTrue(new File(context.stateDir(), "test/test." + SEGMENT_INTERVAL).isDirectory());    assertTrue(new File(context.stateDir(), "test/test." + 2 * SEGMENT_INTERVAL).isDirectory());    assertTrue(segment1.isOpen());    assertTrue(segment2.isOpen());    assertTrue(segment3.isOpen());}
public void kafkatest_f18250_0()
{    updateStreamTimeAndCreateSegment(0);    verifyCorrectSegments(0, 1);    updateStreamTimeAndCreateSegment(1);    verifyCorrectSegments(0, 2);    updateStreamTimeAndCreateSegment(2);    verifyCorrectSegments(0, 3);    updateStreamTimeAndCreateSegment(3);    verifyCorrectSegments(0, 4);    updateStreamTimeAndCreateSegment(4);    verifyCorrectSegments(0, 5);    updateStreamTimeAndCreateSegment(5);    verifyCorrectSegments(1, 5);    updateStreamTimeAndCreateSegment(6);    verifyCorrectSegments(2, 5);}
public void kafkatest_f18251_0()
{    updateStreamTimeAndCreateSegment(0);    verifyCorrectSegments(0, 1);    updateStreamTimeAndCreateSegment(1);    verifyCorrectSegments(0, 2);    updateStreamTimeAndCreateSegment(2);    verifyCorrectSegments(0, 3);    updateStreamTimeAndCreateSegment(3);    verifyCorrectSegments(0, 4);    final long streamTime = updateStreamTimeAndCreateSegment(4);    verifyCorrectSegments(0, 5);    segments.getOrCreateSegmentIfLive(5, context, streamTime);    verifyCorrectSegments(0, 6);    segments.getOrCreateSegmentIfLive(6, context, streamTime);    verifyCorrectSegments(0, 7);}
public void kafkatest_f18260_0()
{    final KeyValueSegment segment1 = new KeyValueSegment("a", "C", 50L, metricsRecorder);    final KeyValueSegment segment2 = new KeyValueSegment("b", "B", 100L, metricsRecorder);    final KeyValueSegment segment3 = new KeyValueSegment("c", "A", 0L, metricsRecorder);    assertThat(segment1.compareTo(segment1), equalTo(0));    assertThat(segment1.compareTo(segment2), equalTo(-1));    assertThat(segment2.compareTo(segment1), equalTo(1));    assertThat(segment1.compareTo(segment3), equalTo(1));    assertThat(segment3.compareTo(segment1), equalTo(-1));    assertThat(segment2.compareTo(segment3), equalTo(1));    assertThat(segment3.compareTo(segment2), equalTo(-1));}
public void kafkatest_f18261_0()
{    EasyMock.expect(supplier.get()).andReturn(inner);    EasyMock.expect(supplier.name()).andReturn("name");    EasyMock.replay(supplier);    builder = new KeyValueStoreBuilder<>(supplier, Serdes.String(), Serdes.String(), new MockTime());}
public void kafkatest_f18270_0()
{    new KeyValueStoreBuilder<>(supplier, Serdes.String(), null, new MockTime());}
public void kafkatest_f18271_0()
{    new KeyValueStoreBuilder<>(supplier, Serdes.String(), Serdes.String(), null);}
public void kafkatest_f18280_0() throws Exception
{    final byte[][] bytes = { { 0 }, { 1 } };    store.put(Bytes.wrap(bytes[0]), bytes[0]);    cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));    final MergedSortedCacheKeyValueBytesStoreIterator iterator = createIterator();    assertArrayEquals(bytes[0], iterator.next().key.get());    assertFalse(iterator.hasNext());}
public void kafkatest_f18281_0() throws Exception
{    final byte[][] bytes = { { 0 }, { 1 } };    cache.put(namespace, Bytes.wrap(bytes[0]), new LRUCacheEntry(null));    store.put(Bytes.wrap(bytes[1]), bytes[1]);    final MergedSortedCacheKeyValueBytesStoreIterator iterator = createIterator();    assertArrayEquals(bytes[1], iterator.next().key.get());    assertFalse(iterator.hasNext());}
public void kafkatest_f18290_0()
{    final MergedSortedCacheSessionStoreIterator mergeIterator = createIterator(storeKvs, Collections.emptyIterator());    assertThat(mergeIterator.next(), equalTo(KeyValue.pair(new Windowed<>(storeKey, storeWindow), storeKey.get())));}
public void kafkatest_f18291_0()
{    final MergedSortedCacheSessionStoreIterator mergeIterator = createIterator(storeKvs, Collections.emptyIterator());    assertThat(mergeIterator.peekNextKey(), equalTo(new Windowed<>(storeKey, storeWindow)));}
public void kafkatest_f18300_0()
{    windowStoreKvPairs.add(KeyValue.pair(0L, "a".getBytes()));    cache.put(namespace, SINGLE_SEGMENT_CACHE_FUNCTION.cacheKey(WindowKeySchema.toStoreKeyBinary("a", 10L, 0, stateSerdes)), new LRUCacheEntry("b".getBytes()));    final Bytes fromBytes = WindowKeySchema.toStoreKeyBinary("a", 0, 0, stateSerdes);    final Bytes toBytes = WindowKeySchema.toStoreKeyBinary("a", 100, 0, stateSerdes);    final KeyValueIterator<Long, byte[]> storeIterator = new DelegatingPeekingKeyValueIterator<>("store", new KeyValueIteratorStub<>(windowStoreKvPairs.iterator()));    final ThreadCache.MemoryLRUCacheBytesIterator cacheIterator = cache.range(namespace, SINGLE_SEGMENT_CACHE_FUNCTION.cacheKey(fromBytes), SINGLE_SEGMENT_CACHE_FUNCTION.cacheKey(toBytes));    final MergedSortedCacheWindowStoreIterator iterator = new MergedSortedCacheWindowStoreIterator(cacheIterator, storeIterator);    assertThat(iterator.peekNextKey(), equalTo(0L));    iterator.next();    assertThat(iterator.peekNextKey(), equalTo(10L));    iterator.close();}
public long kafkatest_f18301_0(final Bytes key)
{    return 0;}
private Windowed<String> kafkatest_f18310_0(final Windowed<Bytes> bytesWindowed)
{    final String key = deserializer.deserialize("", bytesWindowed.key().get());    return new Windowed<>(key, bytesWindowed.window());}
private MergedSortedCacheWindowStoreKeyValueIterator kafkatest_f18311_0(final Iterator<KeyValue<Windowed<Bytes>, byte[]>> storeKvs, final Iterator<KeyValue<Bytes, LRUCacheEntry>> cacheKvs)
{    final DelegatingPeekingKeyValueIterator<Windowed<Bytes>, byte[]> storeIterator = new DelegatingPeekingKeyValueIterator<>("store", new KeyValueIteratorStub<>(storeKvs));    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> cacheIterator = new DelegatingPeekingKeyValueIterator<>("cache", new KeyValueIteratorStub<>(cacheKvs));    return new MergedSortedCacheWindowStoreKeyValueIterator(cacheIterator, storeIterator, new StateSerdes<>("name", Serdes.Bytes(), Serdes.ByteArray()), WINDOW_SIZE, SINGLE_SEGMENT_CACHE_FUNCTION);}
public void kafkatest_f18320_0()
{    expect(inner.delete(keyBytes)).andReturn(valueBytes);    init();    metered.delete(key);    final KafkaMetric metric = metric("delete-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
public void kafkatest_f18321_0()
{    expect(inner.range(keyBytes, keyBytes)).andReturn(new KeyValueIteratorStub<>(Collections.singletonList(byteKeyValuePair).iterator()));    init();    final KeyValueIterator<String, String> iterator = metered.range(key, key);    assertThat(iterator.next().value, equalTo(value));    assertFalse(iterator.hasNext());    iterator.close();    final KafkaMetric metric = metric("range-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
public void kafkatest_f18330_0()
{    init();    final JmxReporter reporter = new JmxReporter("kafka.streams");    metrics.addReporter(reporter);    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", taskId.toString(), "scope", "metered")));    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", taskId.toString(), "scope", "all")));}
public void kafkatest_f18331_0()
{    inner.put(eq(windowedKeyBytes), aryEq(keyBytes));    expectLastCall();    init();    metered.put(new Windowed<>(key, new SessionWindow(0, 0)), key);    final KafkaMetric metric = metric("put-rate");    assertTrue(((Double) metric.metricValue()) > 0);    verify(inner);}
public void kafkatest_f18340_0()
{    metered.remove(null);}
public void kafkatest_f18341_0()
{    metered.fetch(null);}
public void kafkatest_f18350_0()
{    metered = new MeteredTimestampedKeyValueStore<>(inner, "scope", new MockTime(), Serdes.String(), new ValueAndTimestampSerde<>(Serdes.String()));    metrics.config().recordLevel(Sensor.RecordingLevel.DEBUG);    expect(context.metrics()).andReturn(new MockStreamsMetrics(metrics));    expect(context.taskId()).andReturn(taskId);    expect(inner.name()).andReturn("metered").anyTimes();}
private void kafkatest_f18351_0()
{    replay(inner, context);    metered.init(context, metered);}
public void kafkatest_f18360_0()
{    expect(inner.all()).andReturn(new KeyValueIteratorStub<>(Collections.singletonList(byteKeyValueTimestampPair).iterator()));    init();    final KeyValueIterator<String, ValueAndTimestamp<String>> iterator = metered.all();    assertThat(iterator.next().value, equalTo(valueAndTimestamp));    assertFalse(iterator.hasNext());    iterator.close();    final KafkaMetric metric = metric(new MetricName("all-rate", "stream-scope-state-metrics", "", tags));    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
public void kafkatest_f18361_0()
{    inner.flush();    expectLastCall().once();    init();    metered.flush();    final KafkaMetric metric = metric("flush-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
public void kafkatest_f18370_0()
{    EasyMock.expect(innerStoreMock.name()).andStubReturn("mocked-store");    EasyMock.replay(innerStoreMock);    final MeteredTimestampedWindowStore<String, Long> store = new MeteredTimestampedWindowStore<>(innerStoreMock, // any size    10L, "scope", new MockTime(), null, null);    store.init(context, innerStoreMock);    try {        store.put("key", ValueAndTimestamp.make(42L, 60000));    } catch (final StreamsException exception) {        if (exception.getCause() instanceof ClassCastException) {            fail("Serdes are not correctly set from processor context.");        }        throw exception;    }}
public void kafkatest_f18371_0()
{    EasyMock.expect(innerStoreMock.name()).andStubReturn("mocked-store");    EasyMock.replay(innerStoreMock);    final MeteredTimestampedWindowStore<String, Long> store = new MeteredTimestampedWindowStore<>(innerStoreMock, // any size    10L, "scope", new MockTime(), Serdes.String(), new ValueAndTimestampSerde<>(Serdes.Long()));    store.init(context, innerStoreMock);    try {        store.put("key", ValueAndTimestamp.make(42L, 60000));    } catch (final StreamsException exception) {        if (exception.getCause() instanceof ClassCastException) {            fail("Serdes are not correctly set from constructor parameters.");        }        throw exception;    }}
public void kafkatest_f18380_0()
{    expect(innerStoreMock.fetch(Bytes.wrap("a".getBytes()), 0)).andReturn(null);    replay(innerStoreMock);    store.init(context, store);    assertNull(store.fetch("a", 0));}
public void kafkatest_f18381_0()
{    final CachedWindowStore cachedWindowStore = mock(CachedWindowStore.class);    expect(cachedWindowStore.setFlushListener(anyObject(CacheFlushListener.class), eq(false))).andReturn(true);    replay(cachedWindowStore);    final MeteredWindowStore<String, String> metered = new MeteredWindowStore<>(cachedWindowStore, // any size    10L, "scope", new MockTime(), Serdes.String(), new SerdeThatDoesntHandleNull());    assertTrue(metered.setFlushListener(null, false));    verify(cachedWindowStore);}
public void kafkatest_f18390_0()
{    final String metricNamePrefix = "memtable-hit-ratio";    final String description = "Ratio of memtable hits relative to all lookups to the memtable";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::memtableHitRatioSensor);}
public void kafkatest_f18391_0()
{    final String metricNamePrefix = "memtable-bytes-flushed";    final String descriptionOfTotal = "Total number of bytes flushed from the memtable to disk";    final String descriptionOfRate = "Average number of bytes flushed per second from the memtable to disk";    verifyRateAndTotalSensor(metricNamePrefix, descriptionOfTotal, descriptionOfRate, RocksDBMetrics::memtableBytesFlushedSensor);}
public void kafkatest_f18400_0()
{    final String metricNamePrefix = "bytes-written-compaction";    final String description = "Average number of bytes written per second during compaction";    verifyRateSensor(metricNamePrefix, description, RocksDBMetrics::bytesWrittenDuringCompactionSensor);}
public void kafkatest_f18401_0()
{    final String metricNamePrefix = "compaction-time-avg";    final String description = "Average time spent on compaction in ms";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::compactionTimeAvgSensor);}
private void kafkatest_f18410_0(final SensorCreator sensorCreator)
{    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = sensorCreator.sensor(streamsMetrics, new RocksDBMetricContext(taskName, storeType, storeName));    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(this.sensor));}
public void kafkatest_f18411_0()
{    innerMetrics = new Metrics();    metrics = new MockStreamsMetrics(innerMetrics);    cache = new NamedCache(taskIDString + "-" + underlyingStoreName, metrics);}
public void kafkatest_f18420_0()
{    cache.put(Bytes.wrap(new byte[] { 0 }), new LRUCacheEntry(new byte[] { 10 }));    cache.put(Bytes.wrap(new byte[] { 1 }), new LRUCacheEntry(new byte[] { 20 }));    cache.put(Bytes.wrap(new byte[] { 2 }), new LRUCacheEntry(new byte[] { 30 }));    cache.evict();    assertNull(cache.get(Bytes.wrap(new byte[] { 0 })));    assertEquals(2, cache.size());}
public void kafkatest_f18421_0()
{    final List<ThreadCache.DirtyEntry> flushed = new ArrayList<>();    cache.put(Bytes.wrap(new byte[] { 0 }), new LRUCacheEntry(new byte[] { 10 }, headers, true, 0, 0, 0, ""));    cache.put(Bytes.wrap(new byte[] { 1 }), new LRUCacheEntry(new byte[] { 20 }));    cache.put(Bytes.wrap(new byte[] { 2 }), new LRUCacheEntry(new byte[] { 30 }, headers, true, 0, 0, 0, ""));    cache.setListener(new ThreadCache.DirtyEntryFlushListener() {        @Override        public void apply(final List<ThreadCache.DirtyEntry> dirty) {            flushed.addAll(dirty);        }    });    cache.evict();    assertEquals(2, flushed.size());    assertEquals(Bytes.wrap(new byte[] { 0 }), flushed.get(0).key());    assertEquals(headers, flushed.get(0).entry().context().headers());    assertArrayEquals(new byte[] { 10 }, flushed.get(0).newValue());    assertEquals(Bytes.wrap(new byte[] { 2 }), flushed.get(1).key());    assertArrayEquals(new byte[] { 30 }, flushed.get(1).newValue());    assertEquals(cache.flushes(), 1);}
public void kafkatest_f18430_0(final List<ThreadCache.DirtyEntry> dirty)
{    cache.put(key, clean);}
public void kafkatest_f18431_0()
{    assertNull(cache.get(null));}
public void kafkatest_f18440_0()
{    storeProvider.getStore(keyValueStore, QueryableStoreTypes.windowStore());}
public void kafkatest_f18441_0()
{    globalStateStores.put("global", new NoOpReadOnlyStore<>());    assertNotNull(storeProvider.getStore("global", QueryableStoreTypes.keyValueStore()));}
public void kafkatest_f18450_0()
{    expect(mockedWindowTimestampIterator.next()).andReturn(KeyValue.pair(21L, ValueAndTimestamp.make("value1", 22L))).andReturn(KeyValue.pair(42L, ValueAndTimestamp.make("value2", 23L)));    expect(mockedWindowTimestampStore.fetch("key1", Instant.ofEpochMilli(21L), Instant.ofEpochMilli(42L))).andReturn(mockedWindowTimestampIterator);    replay(mockedWindowTimestampIterator, mockedWindowTimestampStore);    final WindowStoreIterator<String> iterator = readOnlyWindowStoreFacade.fetch("key1", Instant.ofEpochMilli(21L), Instant.ofEpochMilli(42L));    assertThat(iterator.next(), is(KeyValue.pair(21L, "value1")));    assertThat(iterator.next(), is(KeyValue.pair(42L, "value2")));    verify(mockedWindowTimestampIterator, mockedWindowTimestampStore);}
public void kafkatest_f18451_0()
{    expect(mockedKeyValueWindowTimestampIterator.next()).andReturn(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), ValueAndTimestamp.make("value1", 22L))).andReturn(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), ValueAndTimestamp.make("value2", 100L)));    expect(mockedWindowTimestampStore.fetch("key1", "key2", 21L, 42L)).andReturn(mockedKeyValueWindowTimestampIterator);    replay(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);    final KeyValueIterator<Windowed<String>, String> iterator = readOnlyWindowStoreFacade.fetch("key1", "key2", 21L, 42L);    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), "value1")));    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), "value2")));    verify(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);}
public Windowed<K> kafkatest_f18461_0()
{    throw new UnsupportedOperationException("peekNextKey() not supported in " + getClass().getName());}
public boolean kafkatest_f18462_0()
{    return iterator.hasNext();}
public boolean kafkatest_f18473_0()
{    return iterator.hasNext();}
public KeyValue<Windowed<K>, V> kafkatest_f18474_0()
{    return iterator.next();}
public KeyValue<Long, E> kafkatest_f18487_0()
{    return underlying.next();}
public void kafkatest_f18488_0()
{    final ConsumerRecord<byte[], byte[]> nullValueRecord = new ConsumerRecord<>("", 0, 0L, new byte[0], null);    assertNull(timestampedValueConverter.convert(nullValueRecord).value());}
public AbstractCompactionFilter<?> kafkatest_f18498_0(final Context context)
{    return null;}
public String kafkatest_f18499_0()
{    return "AbstractCompactionFilterFactory";}
 Options kafkatest_f18508_0(final KeyValueSegment segment)
{    return segment.getOptions();}
 SessionStore<K, V> kafkatest_f18509_0(final long retentionPeriod, final Serde<K> keySerde, final Serde<V> valueSerde)
{    return Stores.sessionStoreBuilder(Stores.persistentSessionStore(STORE_NAME, ofMillis(retentionPeriod)), keySerde, valueSerde).build();}
public void kafkatest_f18518_0(final String storeName, final Options options, final Map<String, Object> configs)
{    options.setStatistics(new Statistics());}
public void kafkatest_f18519_0(final String storeName, final Options options)
{    options.statistics().close();}
public void kafkatest_f18528_0()
{    final List<KeyValue<byte[], byte[]>> entries = getKeyValueEntries();    rocksDBStore.init(context, rocksDBStore);    context.restore(rocksDBStore.name(), entries);    assertEquals("a", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "1")))));    assertEquals("b", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "2")))));    assertEquals("c", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "3")))));}
public void kafkatest_f18529_0()
{    rocksDBStore.init(context, rocksDBStore);    final Bytes keyBytes = new Bytes(stringSerializer.serialize(null, "one"));    final byte[] valueBytes = stringSerializer.serialize(null, "A");    final byte[] valueBytesUpdate = stringSerializer.serialize(null, "B");    rocksDBStore.putIfAbsent(keyBytes, valueBytes);    rocksDBStore.putIfAbsent(keyBytes, valueBytesUpdate);    final String retrievedValue = stringDeserializer.deserialize(null, rocksDBStore.get(keyBytes));    assertEquals("A", retrievedValue);}
public void kafkatest_f18538_0() throws IOException
{    rocksDBStore.init(context, rocksDBStore);    Utils.delete(dir);    rocksDBStore.put(new Bytes(stringSerializer.serialize(null, "anyKey")), stringSerializer.serialize(null, "anyValue"));    rocksDBStore.flush();}
public void kafkatest_f18539_0()
{    final Properties props = StreamsTestUtils.getStreamsConfig();    props.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, TestingBloomFilterRocksDBConfigSetter.class);    rocksDBStore = getRocksDBStore();    dir = TestUtils.tempDirectory();    context = new InternalMockProcessorContext(dir, Serdes.String(), Serdes.String(), new StreamsConfig(props));    enableBloomFilters = false;    rocksDBStore.init(context, rocksDBStore);    final List<String> expectedValues = new ArrayList<>();    expectedValues.add("a");    expectedValues.add("b");    expectedValues.add("c");    final List<KeyValue<byte[], byte[]>> keyValues = getKeyValueEntries();    for (final KeyValue<byte[], byte[]> keyValue : keyValues) {        rocksDBStore.put(new Bytes(keyValue.key), keyValue.value);    }    int expectedIndex = 0;    for (final KeyValue<byte[], byte[]> keyValue : keyValues) {        final byte[] valBytes = rocksDBStore.get(new Bytes(keyValue.key));        assertThat(new String(valBytes, UTF_8), is(expectedValues.get(expectedIndex++)));    }    assertFalse(TestingBloomFilterRocksDBConfigSetter.bloomFiltersSet);    rocksDBStore.close();    expectedIndex = 0;    // reopen with Bloom Filters enabled    // should open fine without errors    enableBloomFilters = true;    rocksDBStore.init(context, rocksDBStore);    for (final KeyValue<byte[], byte[]> keyValue : keyValues) {        final byte[] valBytes = rocksDBStore.get(new Bytes(keyValue.key));        assertThat(new String(valBytes, UTF_8), is(expectedValues.get(expectedIndex++)));    }    assertTrue(TestingBloomFilterRocksDBConfigSetter.bloomFiltersSet);}
public void kafkatest_f18548_0()
{    LogCaptureAppender.setClassLoggerToDebug(RocksDBTimestampedStore.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    rocksDBStore.init(context, rocksDBStore);    assertThat(appender.getMessages(), hasItem("Opening store " + DB_NAME + " in regular mode"));    LogCaptureAppender.unregister(appender);    try (final KeyValueIterator<Bytes, byte[]> iterator = rocksDBStore.all()) {        assertThat(iterator.hasNext(), is(false));    }}
public void kafkatest_f18549_0() throws Exception
{    LogCaptureAppender.setClassLoggerToDebug(RocksDBTimestampedStore.class);    // prepare store    rocksDBStore.init(context, rocksDBStore);    rocksDBStore.put(new Bytes("key".getBytes()), "timestamped".getBytes());    rocksDBStore.close();    // re-open store    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    rocksDBStore = getRocksDBStore();    rocksDBStore.init(context, rocksDBStore);    assertThat(appender.getMessages(), hasItem("Opening store " + DB_NAME + " in regular mode"));    LogCaptureAppender.unregister(appender);    rocksDBStore.close();    // verify store    final DBOptions dbOptions = new DBOptions();    final ColumnFamilyOptions columnFamilyOptions = new ColumnFamilyOptions();    final List<ColumnFamilyDescriptor> columnFamilyDescriptors = asList(new ColumnFamilyDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY, columnFamilyOptions), new ColumnFamilyDescriptor("keyValueWithTimestamp".getBytes(StandardCharsets.UTF_8), columnFamilyOptions));    final List<ColumnFamilyHandle> columnFamilies = new ArrayList<>(columnFamilyDescriptors.size());    RocksDB db = null;    ColumnFamilyHandle noTimestampColumnFamily = null, withTimestampColumnFamily = null;    try {        db = RocksDB.open(dbOptions, new File(new File(context.stateDir(), "rocksdb"), DB_NAME).getAbsolutePath(), columnFamilyDescriptors, columnFamilies);        noTimestampColumnFamily = columnFamilies.get(0);        withTimestampColumnFamily = columnFamilies.get(1);        assertThat(db.get(noTimestampColumnFamily, "key".getBytes()), new IsNull<>());        assertThat(db.getLongProperty(noTimestampColumnFamily, "rocksdb.estimate-num-keys"), is(0L));        assertThat(db.get(withTimestampColumnFamily, "key".getBytes()).length, is(11));        assertThat(db.getLongProperty(withTimestampColumnFamily, "rocksdb.estimate-num-keys"), is(1L));    } finally {        // Order of closing must follow: ColumnFamilyHandle > RocksDB > DBOptions > ColumnFamilyOptions        if (noTimestampColumnFamily != null) {            noTimestampColumnFamily.close();        }        if (withTimestampColumnFamily != null) {            withTimestampColumnFamily.close();        }        if (db != null) {            db.close();        }        dbOptions.close();        columnFamilyOptions.close();    }}
public void kafkatest_f18558_0()
{    // to validate segments    final long startTime = SEGMENT_INTERVAL * 2;    final long increment = SEGMENT_INTERVAL / 2;    setCurrentTime(startTime);    windowStore.put(0, "zero");    assertEquals(Utils.mkSet(segments.segmentName(2)), segmentDirs(baseDir));    setCurrentTime(startTime + increment);    windowStore.put(1, "one");    assertEquals(Utils.mkSet(segments.segmentName(2)), segmentDirs(baseDir));    setCurrentTime(startTime + increment * 2);    windowStore.put(2, "two");    assertEquals(Utils.mkSet(segments.segmentName(2), segments.segmentName(3)), segmentDirs(baseDir));    setCurrentTime(startTime + increment * 4);    windowStore.put(4, "four");    assertEquals(Utils.mkSet(segments.segmentName(2), segments.segmentName(3), segments.segmentName(4)), segmentDirs(baseDir));    setCurrentTime(startTime + increment * 5);    windowStore.put(5, "five");    assertEquals(Utils.mkSet(segments.segmentName(2), segments.segmentName(3), segments.segmentName(4)), segmentDirs(baseDir));    assertEquals(new HashSet<>(Collections.singletonList("zero")), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("one")), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("two")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    setCurrentTime(startTime + increment * 6);    windowStore.put(6, "six");    assertEquals(Utils.mkSet(segments.segmentName(3), segments.segmentName(4), segments.segmentName(5)), segmentDirs(baseDir));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("two")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("six")), toSet(windowStore.fetch(6, ofEpochMilli(startTime + increment * 6 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 6 + WINDOW_SIZE))));    setCurrentTime(startTime + increment * 7);    windowStore.put(7, "seven");    assertEquals(Utils.mkSet(segments.segmentName(3), segments.segmentName(4), segments.segmentName(5)), segmentDirs(baseDir));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("two")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("six")), toSet(windowStore.fetch(6, ofEpochMilli(startTime + increment * 6 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 6 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("seven")), toSet(windowStore.fetch(7, ofEpochMilli(startTime + increment * 7 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 7 + WINDOW_SIZE))));    setCurrentTime(startTime + increment * 8);    windowStore.put(8, "eight");    assertEquals(Utils.mkSet(segments.segmentName(4), segments.segmentName(5), segments.segmentName(6)), segmentDirs(baseDir));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("six")), toSet(windowStore.fetch(6, ofEpochMilli(startTime + increment * 6 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 6 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("seven")), toSet(windowStore.fetch(7, ofEpochMilli(startTime + increment * 7 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 7 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("eight")), toSet(windowStore.fetch(8, ofEpochMilli(startTime + increment * 8 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 8 + WINDOW_SIZE))));    // check segment directories    windowStore.flush();    assertEquals(Utils.mkSet(segments.segmentName(4), segments.segmentName(5), segments.segmentName(6)), segmentDirs(baseDir));}
public void kafkatest_f18559_0()
{    windowStore = buildWindowStore(RETENTION_PERIOD, WINDOW_SIZE, true, Serdes.Integer(), Serdes.String());    windowStore.init(context, windowStore);    context.setTime(0L);    setCurrentTime(0);    windowStore.put(0, "v");    assertEquals(Utils.mkSet(segments.segmentName(0L)), segmentDirs(baseDir));    setCurrentTime(SEGMENT_INTERVAL - 1);    windowStore.put(0, "v");    windowStore.put(0, "v");    assertEquals(Utils.mkSet(segments.segmentName(0L)), segmentDirs(baseDir));    setCurrentTime(SEGMENT_INTERVAL);    windowStore.put(0, "v");    assertEquals(Utils.mkSet(segments.segmentName(0L), segments.segmentName(1L)), segmentDirs(baseDir));    WindowStoreIterator iter;    int fetchedCount;    iter = windowStore.fetch(0, ofEpochMilli(0L), ofEpochMilli(SEGMENT_INTERVAL * 4));    fetchedCount = 0;    while (iter.hasNext()) {        iter.next();        fetchedCount++;    }    assertEquals(4, fetchedCount);    assertEquals(Utils.mkSet(segments.segmentName(0L), segments.segmentName(1L)), segmentDirs(baseDir));    setCurrentTime(SEGMENT_INTERVAL * 3);    windowStore.put(0, "v");    iter = windowStore.fetch(0, ofEpochMilli(0L), ofEpochMilli(SEGMENT_INTERVAL * 4));    fetchedCount = 0;    while (iter.hasNext()) {        iter.next();        fetchedCount++;    }    assertEquals(2, fetchedCount);    assertEquals(Utils.mkSet(segments.segmentName(1L), segments.segmentName(3L)), segmentDirs(baseDir));    setCurrentTime(SEGMENT_INTERVAL * 5);    windowStore.put(0, "v");    iter = windowStore.fetch(0, ofEpochMilli(SEGMENT_INTERVAL * 4), ofEpochMilli(SEGMENT_INTERVAL * 10));    fetchedCount = 0;    while (iter.hasNext()) {        iter.next();        fetchedCount++;    }    assertEquals(1, fetchedCount);    assertEquals(Utils.mkSet(segments.segmentName(3L), segments.segmentName(5L)), segmentDirs(baseDir));}
public void kafkatest_f18568_0()
{    if (iterator != null) {        iterator.close();        iterator = null;    }    segmentOne.close();    segmentTwo.close();}
public void kafkatest_f18569_0()
{    iterator = new SegmentIterator<>(Arrays.asList(segmentOne, segmentTwo).iterator(), hasNextCondition, Bytes.wrap("a".getBytes()), Bytes.wrap("z".getBytes()));    assertTrue(iterator.hasNext());    assertEquals("a", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("a", "1"), toStringKeyValue(iterator.next()));    assertTrue(iterator.hasNext());    assertEquals("b", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("b", "2"), toStringKeyValue(iterator.next()));    assertTrue(iterator.hasNext());    assertEquals("c", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("c", "3"), toStringKeyValue(iterator.next()));    assertTrue(iterator.hasNext());    assertEquals("d", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("d", "4"), toStringKeyValue(iterator.next()));    assertFalse(iterator.hasNext());}
private RecordCollectorImpl kafkatest_f18578_0(final String name)
{    return new RecordCollectorImpl(name, new LogContext(name), new DefaultProductionExceptionHandler(), new Metrics().sensor("skipped-records")) {        @Override        public <K1, V1> void send(final String topic, final K1 key, final V1 value, final Headers headers, final Integer partition, final Long timestamp, final Serializer<K1> keySerializer, final Serializer<V1> valueSerializer) {            changeLog.add(new KeyValue<>(keySerializer.serialize(topic, headers, key), valueSerializer.serialize(topic, headers, value)));        }    };}
public void kafkatest_f18579_0(final String topic, final K1 key, final V1 value, final Headers headers, final Integer partition, final Long timestamp, final Serializer<K1> keySerializer, final Serializer<V1> valueSerializer)
{    changeLog.add(new KeyValue<>(keySerializer.serialize(topic, headers, key), valueSerializer.serialize(topic, headers, value)));}
public void kafkatest_f18588_0()
{    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 1000)), 1L);    sessionStore.put(new Windowed<>("a", new SessionWindow(1500, 2500)), 2L);    sessionStore.remove(new Windowed<>("a", new SessionWindow(0, 1000)));    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 0L, 1000L)) {        assertFalse(results.hasNext());    }    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 1500L, 2500L)) {        assertTrue(results.hasNext());    }}
public void kafkatest_f18589_0()
{    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 1000)), 1L);    sessionStore.put(new Windowed<>("a", new SessionWindow(1500, 2500)), 2L);    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 1000)), null);    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 0L, 1000L)) {        assertFalse(results.hasNext());    }    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 1500L, 2500L)) {        assertTrue(results.hasNext());    }}
public void kafkatest_f18598_0()
{    sessionStore.remove(new Windowed<>("a", new SessionWindow(0, 1)));}
public void kafkatest_f18599_0()
{    sessionStore.findSessions(null, 1L, 2L);}
protected static Set<V> kafkatest_f18608_0(final Iterator<KeyValue<K, V>> iterator)
{    final Set<V> results = new HashSet<>();    while (iterator.hasNext()) {        results.add(iterator.next().value);    }    return results;}
protected static Set<KeyValue<K, V>> kafkatest_f18609_0(final Iterator<KeyValue<K, V>> iterator)
{    final Set<KeyValue<K, V>> results = new HashSet<>();    while (iterator.hasNext()) {        results.add(iterator.next());    }    return results;}
public void kafkatest_f18618_0()
{    final Bytes lower = sessionKeySchema.lowerRange(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), Long.MAX_VALUE);    assertThat("appending zeros to key should still be in range", lower.compareTo(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }), new SessionWindow(Long.MAX_VALUE, Long.MAX_VALUE)))) < 0);    assertThat(lower, equalTo(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), new SessionWindow(0, 0)))));}
public void kafkatest_f18619_0()
{    final byte[] bytes = keySerde.serializer().serialize(topic, windowedKey);    final Windowed<String> result = keySerde.deserializer().deserialize(topic, bytes);    assertEquals(windowedKey, result);}
public void kafkatest_f18628_0()
{    final byte[] serialized = SessionKeySchema.toBinary(windowedKey, serde.serializer(), "dummy");    assertEquals(windowedKey, SessionKeySchema.from(serialized, serde.deserializer(), "dummy"));}
public void kafkatest_f18629_0()
{    final Bytes bytesKey = Bytes.wrap(key.getBytes());    final Windowed<Bytes> windowedBytesKey = new Windowed<>(bytesKey, window);    final Bytes serialized = SessionKeySchema.toBinary(windowedBytesKey);    assertEquals(windowedBytesKey, SessionKeySchema.from(serialized));}
public void kafkatest_f18638_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> new SessionStoreBuilder<>(null, Serdes.String(), Serdes.String(), new MockTime()));    assertThat(e.getMessage(), equalTo("supplier cannot be null"));}
public void kafkatest_f18639_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> new SessionStoreBuilder<>(supplier, null, Serdes.String(), new MockTime()));    assertThat(e.getMessage(), equalTo("name cannot be null"));}
public void kafkatest_f18648_0() throws IOException
{    Utils.delete(stateDir);}
public void kafkatest_f18649_0()
{    mockThread(true);    final List<ReadOnlyKeyValueStore<String, String>> kvStores = provider.stores("kv-store", QueryableStoreTypes.keyValueStore());    assertEquals(2, kvStores.size());    for (final ReadOnlyKeyValueStore<String, String> store : kvStores) {        assertThat(store, instanceOf(ReadOnlyKeyValueStore.class));        assertThat(store, not(instanceOf(TimestampedKeyValueStore.class)));    }}
public void kafkatest_f18658_0()
{    mockThread(true);    taskOne.getStore("timestamped-kv-store").close();    provider.stores("timestamped-kv-store", QueryableStoreTypes.timestampedKeyValueStore());}
public void kafkatest_f18659_0()
{    mockThread(true);    taskOne.getStore("window-store").close();    provider.stores("window-store", QueryableStoreTypes.windowStore());}
private void kafkatest_f18668_0(final double entryFactor, final double systemFactor, final long desiredCacheSize, final int keySizeBytes, final int valueSizeBytes)
{    final Runtime runtime = Runtime.getRuntime();    final long numElements = desiredCacheSize / memoryCacheEntrySize(new byte[keySizeBytes], new byte[valueSizeBytes], "");    System.gc();    final long prevRuntimeMemory = runtime.totalMemory() - runtime.freeMemory();    final ThreadCache cache = new ThreadCache(logContext, desiredCacheSize, new MockStreamsMetrics(new Metrics()));    final long size = cache.sizeBytes();    assertEquals(size, 0);    for (int i = 0; i < numElements; i++) {        final String keyStr = "K" + i;        final Bytes key = Bytes.wrap(keyStr.getBytes());        final byte[] value = new byte[valueSizeBytes];        cache.put(namespace, key, new LRUCacheEntry(value, null, true, 1L, 1L, 1, ""));    }    System.gc();    final double ceiling = desiredCacheSize + desiredCacheSize * entryFactor;    final long usedRuntimeMemory = runtime.totalMemory() - runtime.freeMemory() - prevRuntimeMemory;    assertTrue((double) cache.sizeBytes() <= ceiling);    assertTrue("Used memory size " + usedRuntimeMemory + " greater than expected " + cache.sizeBytes() * systemFactor, cache.sizeBytes() * systemFactor >= usedRuntimeMemory);}
public void kafkatest_f18669_0()
{    final Runtime runtime = Runtime.getRuntime();    final double factor = 0.05;    // if I ask for a cache size of 10 MB, accept an overhead of 3x, i.e., 30 MBs might be allocated    final double systemFactor = 3;    final long desiredCacheSize = Math.min(100 * 1024 * 1024L, runtime.maxMemory());    final int keySizeBytes = 8;    final int valueSizeBytes = 100;    checkOverheads(factor, systemFactor, desiredCacheSize, keySizeBytes, valueSizeBytes);}
public void kafkatest_f18678_0()
{    final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics()));    final Bytes theByte = Bytes.wrap(new byte[] { 0 });    cache.put(namespace, theByte, dirtyEntry(theByte.get()));    final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.range(namespace, theByte, Bytes.wrap(new byte[] { 1 }));    assertEquals(theByte, iterator.peekNextKey());    assertEquals(theByte, iterator.peekNextKey());}
public void kafkatest_f18679_0()
{    final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics()));    final Bytes theByte = Bytes.wrap(new byte[] { 0 });    cache.put(namespace, theByte, dirtyEntry(theByte.get()));    final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.range(namespace, theByte, Bytes.wrap(new byte[] { 1 }));    assertEquals(iterator.peekNextKey(), iterator.next().key);}
public void kafkatest_f18688_0()
{    final ThreadCache cache = new ThreadCache(logContext, 0, new MockStreamsMetrics(new Metrics()));    shouldEvictImmediatelyIfCacheSizeIsZeroOrVerySmall(cache);}
public void kafkatest_f18689_0()
{    final List<ThreadCache.DirtyEntry> received = new ArrayList<>();    final ThreadCache cache = new ThreadCache(logContext, 1, new MockStreamsMetrics(new Metrics()));    cache.addDirtyEntryFlushListener(namespace, received::addAll);    cache.putAll(namespace, Arrays.asList(KeyValue.pair(Bytes.wrap(new byte[] { 0 }), dirtyEntry(new byte[] { 5 })), KeyValue.pair(Bytes.wrap(new byte[] { 1 }), dirtyEntry(new byte[] { 6 }))));    assertEquals(cache.evicts(), 2);    assertEquals(received.size(), 2);}
private LRUCacheEntry kafkatest_f18698_0(final byte[] key)
{    return new LRUCacheEntry(key, null, true, -1, -1, -1, "");}
private LRUCacheEntry kafkatest_f18699_0(final byte[] key)
{    return new LRUCacheEntry(key);}
public void kafkatest_f18708_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    putRecord(buffer, context, 0L, 0L, "asdf", "eyt");    putRecord(buffer, context, 1L, 0L, "zxcv", "rtg");    assertThat(buffer.numRecords(), is(2));    final List<Eviction<String, String>> evicted = new LinkedList<>();    buffer.evictWhile(() -> buffer.numRecords() > 1, evicted::add);    assertThat(buffer.numRecords(), is(1));    assertThat(evicted, is(singletonList(new Eviction<>("asdf", new Change<>("eyt", null), getContext(0L)))));    cleanup(context, buffer);}
public void kafkatest_f18709_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    putRecord(buffer, context, 0L, 0L, "asdf", "oin");    assertThat(buffer.numRecords(), is(1));    putRecord(buffer, context, 1L, 0L, "asdf", "wekjn");    assertThat(buffer.numRecords(), is(1));    putRecord(buffer, context, 0L, 0L, "zxcv", "24inf");    assertThat(buffer.numRecords(), is(2));    cleanup(context, buffer);}
public void kafkatest_f18718_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    final RecordBatchingStateRestoreCallback stateRestoreCallback = (RecordBatchingStateRestoreCallback) context.stateRestoreCallback(testName);    context.setRecordContext(new ProcessorRecordContext(0, 0, 0, "", null));    final RecordHeaders v2FlagHeaders = new RecordHeaders(new Header[] { new RecordHeader("v", new byte[] { (byte) 2 }) });    final byte[] todeleteValue = getBufferValue("doomed", 0).serialize(0).array();    final byte[] asdfValue = getBufferValue("qwer", 1).serialize(0).array();    final FullChangeSerde<String> fullChangeSerde = FullChangeSerde.wrap(Serdes.String());    final byte[] zxcvValue1 = new BufferValue(Serdes.String().serializer().serialize(null, "previous"), Serdes.String().serializer().serialize(null, "IGNORED"), Serdes.String().serializer().serialize(null, "3o4im"), getContext(2L)).serialize(0).array();    final FullChangeSerde<String> fullChangeSerde1 = FullChangeSerde.wrap(Serdes.String());    final byte[] zxcvValue2 = new BufferValue(Serdes.String().serializer().serialize(null, "previous"), Serdes.String().serializer().serialize(null, "3o4im"), Serdes.String().serializer().serialize(null, "next"), getContext(3L)).serialize(0).array();    stateRestoreCallback.restoreBatch(asList(new ConsumerRecord<>("changelog-topic", 0, 0, 999, TimestampType.CREATE_TIME, -1L, -1, -1, "todelete".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + todeleteValue.length).put(todeleteValue).putLong(0L).array(), v2FlagHeaders), new ConsumerRecord<>("changelog-topic", 0, 1, 9999, TimestampType.CREATE_TIME, -1L, -1, -1, "asdf".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + asdfValue.length).put(asdfValue).putLong(2L).array(), v2FlagHeaders), new ConsumerRecord<>("changelog-topic", 0, 2, 99, TimestampType.CREATE_TIME, -1L, -1, -1, "zxcv".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + zxcvValue1.length).put(zxcvValue1).putLong(1L).array(), v2FlagHeaders), new ConsumerRecord<>("changelog-topic", 0, 2, 100, TimestampType.CREATE_TIME, -1L, -1, -1, "zxcv".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + zxcvValue2.length).put(zxcvValue2).putLong(1L).array(), v2FlagHeaders)));    assertThat(buffer.numRecords(), is(3));    assertThat(buffer.minTimestamp(), is(0L));    assertThat(buffer.bufferSize(), is(142L));    stateRestoreCallback.restoreBatch(singletonList(new ConsumerRecord<>("changelog-topic", 0, 3, 3, TimestampType.CREATE_TIME, -1L, -1, -1, "todelete".getBytes(UTF_8), null)));    assertThat(buffer.numRecords(), is(2));    assertThat(buffer.minTimestamp(), is(1L));    assertThat(buffer.bufferSize(), is(95L));    assertThat(buffer.priorValueForBuffered("todelete"), is(Maybe.undefined()));    assertThat(buffer.priorValueForBuffered("asdf"), is(Maybe.defined(null)));    assertThat(buffer.priorValueForBuffered("zxcv"), is(Maybe.defined(ValueAndTimestamp.make("previous", -1))));    // flush the buffer into a list in buffer order so we can make assertions about the contents.    final List<Eviction<String, String>> evicted = new LinkedList<>();    buffer.evictWhile(() -> true, evicted::add);    // Several things to note:    // * The buffered records are ordered according to their buffer time (serialized in the value of the changelog)    // * The record timestamps are properly restored, and not conflated with the record's buffer time.    // * The keys and values are properly restored    // * The record topic is set to the original input topic, *not* the changelog topic    // * The record offset preserves the original input record's offset, *not* the offset of the changelog record    assertThat(evicted, is(asList(new Eviction<>("zxcv", new Change<>("next", "3o4im"), getContext(3L)), new Eviction<>("asdf", new Change<>("qwer", null), getContext(1L)))));    cleanup(context, buffer);}
public void kafkatest_f18719_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    final RecordBatchingStateRestoreCallback stateRestoreCallback = (RecordBatchingStateRestoreCallback) context.stateRestoreCallback(testName);    context.setRecordContext(new ProcessorRecordContext(0, 0, 0, "", null));    final RecordHeaders unknownFlagHeaders = new RecordHeaders(new Header[] { new RecordHeader("v", new byte[] { (byte) -1 }) });    final byte[] todeleteValue = getBufferValue("doomed", 0).serialize(0).array();    try {        stateRestoreCallback.restoreBatch(singletonList(new ConsumerRecord<>("changelog-topic", 0, 0, 999, TimestampType.CREATE_TIME, -1L, -1, -1, "todelete".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + todeleteValue.length).putLong(0L).put(todeleteValue).array(), unknownFlagHeaders)));        fail("expected an exception");    } catch (final IllegalArgumentException expected) {    // nothing to do.    } finally {        cleanup(context, buffer);    }}
public void kafkatest_f18728_0()
{    final TimestampedKeyValueStore<String, String> store = builder.withCachingEnabled().build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredTimestampedKeyValueStore.class));    assertThat(wrapped, instanceOf(CachingKeyValueStore.class));}
public void kafkatest_f18729_0()
{    final TimestampedKeyValueStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredTimestampedKeyValueStore.class));    assertThat(wrapped, instanceOf(ChangeLoggingTimestampedKeyValueBytesStore.class));    assertThat(((WrappedStateStore) wrapped).wrapped(), CoreMatchers.equalTo(inner));}
public void kafkatest_f18738_0()
{    stateDirectory = TestUtils.tempDirectory();    context = new InternalMockProcessorContext(stateDirectory, Serdes.String(), Serdes.Long(), new NoOpRecordCollector(), new ThreadCache(new LogContext("testCache "), 0, new MockStreamsMetrics(new Metrics())));    segments = new TimestampedSegments(storeName, METRICS_SCOPE, RETENTION_PERIOD, SEGMENT_INTERVAL);}
public void kafkatest_f18739_0()
{    segments.close();}
public void kafkatest_f18748_0()
{    final TimestampedSegment first = segments.getOrCreateSegmentIfLive(0, context, -1L);    final TimestampedSegment second = segments.getOrCreateSegmentIfLive(1, context, -1L);    final TimestampedSegment third = segments.getOrCreateSegmentIfLive(2, context, -1L);    segments.close();    assertFalse(first.isOpen());    assertFalse(second.isOpen());    assertFalse(third.isOpen());}
public void kafkatest_f18749_0()
{    segments = new TimestampedSegments("test", METRICS_SCOPE, 4, 1);    segments.getOrCreateSegmentIfLive(0, context, -1L);    segments.getOrCreateSegmentIfLive(1, context, -1L);    segments.getOrCreateSegmentIfLive(2, context, -1L);    segments.getOrCreateSegmentIfLive(3, context, -1L);    segments.getOrCreateSegmentIfLive(4, context, -1L);    // close existing.    segments.close();    segments = new TimestampedSegments("test", METRICS_SCOPE, 4, 1);    segments.openExisting(context, -1L);    assertTrue(segments.getSegmentForTimestamp(0).isOpen());    assertTrue(segments.getSegmentForTimestamp(1).isOpen());    assertTrue(segments.getSegmentForTimestamp(2).isOpen());    assertTrue(segments.getSegmentForTimestamp(3).isOpen());    assertTrue(segments.getSegmentForTimestamp(4).isOpen());}
private void kafkatest_f18758_0(final long first, final int numSegments)
{    final List<TimestampedSegment> result = this.segments.segments(0, Long.MAX_VALUE);    assertEquals(numSegments, result.size());    for (int i = 0; i < numSegments; i++) {        assertEquals(i + first, result.get(i).id);    }}
public void kafkatest_f18759_0() throws Exception
{    final TimestampedSegment segment = new TimestampedSegment("segment", "window", 0L, metricsRecorder);    final String directoryPath = TestUtils.tempDirectory().getAbsolutePath();    final File directory = new File(directoryPath);    final ProcessorContext mockContext = mock(ProcessorContext.class);    expect(mockContext.appConfigs()).andReturn(mkMap(mkEntry(METRICS_RECORDING_LEVEL_CONFIG, "INFO")));    expect(mockContext.stateDir()).andReturn(directory);    replay(mockContext);    segment.openDB(mockContext);    assertTrue(new File(directoryPath, "window").exists());    assertTrue(new File(directoryPath + File.separator + "window", "segment").exists());    assertTrue(new File(directoryPath + File.separator + "window", "segment").list().length > 0);    segment.destroy();    assertFalse(new File(directoryPath + File.separator + "window", "segment").exists());    assertTrue(new File(directoryPath, "window").exists());}
public void kafkatest_f18768_0()
{    final TimestampedWindowStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredTimestampedWindowStore.class));    assertThat(wrapped, instanceOf(ChangeLoggingTimestampedWindowBytesStore.class));    assertThat(((WrappedStateStore) wrapped).wrapped(), CoreMatchers.equalTo(inner));}
public void kafkatest_f18769_0()
{    final TimestampedWindowStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).withCachingEnabled().build();    final WrappedStateStore caching = (WrappedStateStore) ((WrappedStateStore) store).wrapped();    final WrappedStateStore changeLogging = (WrappedStateStore) caching.wrapped();    assertThat(store, instanceOf(MeteredTimestampedWindowStore.class));    assertThat(caching, instanceOf(CachingWindowStore.class));    assertThat(changeLogging, instanceOf(ChangeLoggingTimestampedWindowBytesStore.class));    assertThat(changeLogging.wrapped(), CoreMatchers.equalTo(inner));}
public void kafkatest_f18778_0()
{    windowStore = buildWindowStore(RETENTION_PERIOD, WINDOW_SIZE, false, Serdes.Integer(), Serdes.String());    final RecordCollector recordCollector = createRecordCollector(windowStore.name());    recordCollector.init(producer);    context = new InternalMockProcessorContext(baseDir, Serdes.String(), Serdes.Integer(), recordCollector, new ThreadCache(new LogContext("testCache"), 0, new MockStreamsMetrics(new Metrics())));    windowStore.init(context, windowStore);}
public void kafkatest_f18779_0()
{    windowStore.close();}
public void kafkatest_f18788_0()
{    final long windowSize = 0x7a00000000000000L;    final long retentionPeriod = 0x7a00000000000000L;    final WindowStore<String, String> windowStore = buildWindowStore(retentionPeriod, windowSize, false, Serdes.String(), Serdes.String());    windowStore.init(context, windowStore);    windowStore.put("a", "0001", 0);    windowStore.put("aa", "0002", 0);    windowStore.put("a", "0003", 1);    windowStore.put("aa", "0004", 1);    windowStore.put("a", "0005", 0x7a00000000000000L - 1);    final Set expected = new HashSet<>(asList("0001", "0003", "0005"));    assertThat(toSet(windowStore.fetch("a", ofEpochMilli(0), ofEpochMilli(Long.MAX_VALUE))), equalTo(expected));    Set<KeyValue<Windowed<String>, String>> set = toSet(windowStore.fetch("a", "a", ofEpochMilli(0), ofEpochMilli(Long.MAX_VALUE)));    assertThat(set, equalTo(new HashSet<>(asList(windowedPair("a", "0001", 0, windowSize), windowedPair("a", "0003", 1, windowSize), windowedPair("a", "0005", 0x7a00000000000000L - 1, windowSize)))));    set = toSet(windowStore.fetch("aa", "aa", ofEpochMilli(0), ofEpochMilli(Long.MAX_VALUE)));    assertThat(set, equalTo(new HashSet<>(asList(windowedPair("aa", "0002", 0, windowSize), windowedPair("aa", "0004", 1, windowSize)))));}
public void kafkatest_f18789_0()
{    final long currentTime = 0;    setCurrentTime(currentTime);    windowStore.put(1, "one");    windowStore.put(1, "one v2");    WindowStoreIterator<String> iterator = windowStore.fetch(1, 0, currentTime);    assertEquals(new KeyValue<>(currentTime, "one v2"), iterator.next());    windowStore.put(1, null);    iterator = windowStore.fetch(1, 0, currentTime);    assertFalse(iterator.hasNext());}
public void kafkatest_f18798_0()
{    setClassLoggerToDebug();    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    // Advance stream time by inserting record with large enough timestamp that records with timestamp 0 are expired    windowStore.put(1, "initial record", 2 * RETENTION_PERIOD);    // Try inserting a record with timestamp 0 -- should be dropped    windowStore.put(1, "late record", 0L);    windowStore.put(1, "another on-time record", RETENTION_PERIOD + 1);    LogCaptureAppender.unregister(appender);    final Map<MetricName, ? extends Metric> metrics = context.metrics().metrics();    final String metricScope = getMetricsScope();    final Metric dropTotal = metrics.get(new MetricName("expired-window-record-drop-total", "stream-" + metricScope + "-metrics", "The total number of occurrence of expired-window-record-drop operations.", mkMap(mkEntry("client-id", "mock"), mkEntry("task-id", "0_0"), mkEntry(metricScope + "-id", windowStore.name()))));    final Metric dropRate = metrics.get(new MetricName("expired-window-record-drop-rate", "stream-" + metricScope + "-metrics", "The average number of occurrence of expired-window-record-drop operation per second.", mkMap(mkEntry("client-id", "mock"), mkEntry("task-id", "0_0"), mkEntry(metricScope + "-id", windowStore.name()))));    assertEquals(1.0, dropTotal.metricValue());    assertNotEquals(0.0, dropRate.metricValue());    final List<String> messages = appender.getMessages();    assertThat(messages, hasItem("Skipping record for expired segment."));}
public void kafkatest_f18799_0()
{    windowStore.put(1, "one", 0L);    windowStore.put(1, "two", 4 * RETENTION_PERIOD);    final WindowStoreIterator<String> iterator = windowStore.fetch(1, 0L, 10L);    assertFalse(iterator.hasNext());}
private Map<Integer, Set<String>> kafkatest_f18808_0(final List<KeyValue<byte[], byte[]>> changeLog, kafkatest_f18808_0("SameParameterValue") final long startTime)
{    final HashMap<Integer, Set<String>> entriesByKey = new HashMap<>();    for (final KeyValue<byte[], byte[]> entry : changeLog) {        final long timestamp = WindowKeySchema.extractStoreTimestamp(entry.key);        final Integer key = WindowKeySchema.extractStoreKey(entry.key, serdes);        final String value = entry.value == null ? null : serdes.valueFrom(entry.value);        final Set<String> entries = entriesByKey.computeIfAbsent(key, k -> new HashSet<>());        entries.add(value + "@" + (timestamp - startTime));    }    return entriesByKey;}
protected static KeyValue<Windowed<K>, V> kafkatest_f18809_0(final K key, final V value, final long timestamp)
{    return windowedPair(key, value, timestamp, WINDOW_SIZE);}
public void kafkatest_f18818_0()
{    final Bytes lower = windowKeySchema.lowerRange(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), 0);    assertThat(lower, equalTo(WindowKeySchema.toStoreKeyBinary(new byte[] { 0xA, 0xB, 0xC }, 0, 0)));}
public void kafkatest_f18819_0()
{    final Bytes lower = windowKeySchema.lowerRange(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), 42);    assertThat(lower, equalTo(WindowKeySchema.toStoreKeyBinary(new byte[] { 0xA, 0xB, 0xC }, 0, 0)));}
public void kafkatest_f18828_0()
{    final Bytes serialized = WindowKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);    final Windowed<String> result = WindowKeySchema.fromStoreKey(serialized.get(), endTime - startTime, stateSerdes.keyDeserializer(), stateSerdes.topic());    assertEquals(windowedKey, result);}
public void kafkatest_f18829_0()
{    final Bytes serialized = WindowKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);    assertEquals(0, WindowKeySchema.extractStoreSequence(serialized.get()));}
public void kafkatest_f18838_0()
{    final WindowStore<String, String> store = builder.withLoggingDisabled().build();    final StateStore next = ((WrappedStateStore) store).wrapped();    assertThat(next, CoreMatchers.equalTo(inner));}
public void kafkatest_f18839_0()
{    final WindowStore<String, String> store = builder.withCachingEnabled().build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredWindowStore.class));    assertThat(wrapped, instanceOf(CachingWindowStore.class));}
public void kafkatest_f18848_0()
{    final List<ReadOnlyWindowStore<Object, Object>> windowStores = wrappingStoreProvider.stores("window", windowStore());    assertEquals(2, windowStores.size());}
public void kafkatest_f18849_0()
{    wrappingStoreProvider.stores("doesn't exist", QueryableStoreTypes.keyValueStore());}
public Iterable<KeyValue<byte[], byte[]>> kafkatest_f18858_0()
{    return restorableEntries;}
public void kafkatest_f18859_0(final K key, final V value)
{    restorableEntries.add(new KeyValue<>(stateSerdes.rawKey(key), stateSerdes.rawValue(value)));}
public Long kafkatest_f18869_0()
{    throw new NoSuchElementException();}
public boolean kafkatest_f18870_0()
{    return false;}
public KeyValueIterator kafkatest_f18882_0(final Object from, final Object to, final Instant fromTime, final Instant toTime) throws IllegalArgumentException
{    return EMPTY_WINDOW_STORE_ITERATOR;}
public WindowStoreIterator<KeyValue> kafkatest_f18883_0()
{    return EMPTY_WINDOW_STORE_ITERATOR;}
public void kafkatest_f18892_0()
{    new StateSerdes<>(null, Serdes.ByteArray(), Serdes.ByteArray());}
public void kafkatest_f18893_0()
{    new StateSerdes<>("anyName", null, Serdes.ByteArray());}
public void kafkatest_f18902_0()
{    final Exception e = assertThrows(IllegalArgumentException.class, () -> Stores.lruMap("anyName", -1));    assertEquals("maxCacheSize cannot be negative", e.getMessage());}
public void kafkatest_f18903_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> Stores.persistentWindowStore(null, ZERO, ZERO, false));    assertEquals("name cannot be null", e.getMessage());}
public void kafkatest_f18912_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> Stores.windowStoreBuilder(null, Serdes.ByteArray(), Serdes.ByteArray()));    assertEquals("supplier cannot be null", e.getMessage());}
public void kafkatest_f18913_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> Stores.keyValueStoreBuilder(null, Serdes.ByteArray(), Serdes.ByteArray()));    assertEquals("supplier cannot be null", e.getMessage());}
public void kafkatest_f18922_0()
{    final KeyValueStore<String, String> store = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore("name"), Serdes.String(), Serdes.String()).build();    assertThat(store, not(nullValue()));}
public void kafkatest_f18923_0()
{    final TimestampedKeyValueStore<String, String> store = Stores.timestampedKeyValueStoreBuilder(Stores.persistentTimestampedKeyValueStore("name"), Serdes.String(), Serdes.String()).build();    assertThat(store, not(nullValue()));}
public void kafkatest_f18932_0()
{    final KTable<Bytes, String> filteredKTable = builder.<Bytes, String>table(TABLE_TOPIC).filter(MockPredicate.allGoodPredicate());    builder.<Bytes, String>stream(STREAM_TOPIC).join(filteredKTable, MockValueJoiner.TOSTRING_JOINER);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertThat(topology.stateStores().size(), equalTo(1));    assertThat(topology.processorConnectedStateStores("KSTREAM-JOIN-0000000005"), equalTo(Collections.singleton(topology.stateStores().get(0).name())));    assertTrue(topology.processorConnectedStateStores("KTABLE-FILTER-0000000003").isEmpty());}
public void kafkatest_f18933_0()
{    final KTable<Bytes, String> filteredKTable = builder.<Bytes, String>table(TABLE_TOPIC).filter(MockPredicate.allGoodPredicate(), Materialized.as("store"));    builder.<Bytes, String>stream(STREAM_TOPIC).join(filteredKTable, MockValueJoiner.TOSTRING_JOINER);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertThat(topology.stateStores().size(), equalTo(1));    assertThat(topology.processorConnectedStateStores("KSTREAM-JOIN-0000000005"), equalTo(Collections.singleton("store")));    assertThat(topology.processorConnectedStateStores("KTABLE-FILTER-0000000003"), equalTo(Collections.singleton("store")));}
public void kafkatest_f18942_0()
{    final Map<Long, String> results = new HashMap<>();    final String topic = "topic";    final ForeachAction<Long, String> action = results::put;    builder.table(topic, Materialized.<Long, String, KeyValueStore<Bytes, byte[]>>as("store").withKeySerde(Serdes.Long()).withValueSerde(Serdes.String())).toStream().foreach(action);    final ConsumerRecordFactory<Long, String> recordFactory = new ConsumerRecordFactory<>(new LongSerializer(), new StringSerializer());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, 1L, "value1"));        driver.pipeInput(recordFactory.create(topic, 2L, "value2"));        final KeyValueStore<Long, String> store = driver.getKeyValueStore("store");        assertThat(store.get(1L), equalTo("value1"));        assertThat(store.get(2L), equalTo("value2"));        assertThat(results.get(1L), equalTo("value1"));        assertThat(results.get(2L), equalTo("value2"));    }}
public void kafkatest_f18943_0()
{    final String topic = "topic";    builder.globalTable(topic, Materialized.<Long, String, KeyValueStore<Bytes, byte[]>>as("store").withKeySerde(Serdes.Long()).withValueSerde(Serdes.String()));    final ConsumerRecordFactory<Long, String> recordFactory = new ConsumerRecordFactory<>(new LongSerializer(), new StringSerializer());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, 1L, "value1"));        driver.pipeInput(recordFactory.create(topic, 2L, "value2"));        final KeyValueStore<Long, String> store = driver.getKeyValueStore("store");        assertThat(store.get(1L), equalTo("value1"));        assertThat(store.get(2L), equalTo("value2"));    }}
public void kafkatest_f18952_0()
{    final String expected = "sink-processor";    final KStream<Object, Object> stream = builder.stream(STREAM_TOPIC);    stream.to(STREAM_TOPIC_TWO, Produced.as(expected));    stream.to(STREAM_TOPIC_TWO);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", expected, "KSTREAM-SINK-0000000002");}
public void kafkatest_f18953_0()
{    builder.stream(STREAM_TOPIC).map(KeyValue::pair, Named.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", STREAM_OPERATION_NAME);}
public void kafkatest_f18962_0()
{    final KStream<String, String> streamOne = builder.stream(STREAM_TOPIC);    final KTable<String, String> streamTwo = builder.table("table-topic");    streamOne.join(streamTwo, (value1, value2) -> value1, Joined.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", "KSTREAM-SOURCE-0000000002", "KTABLE-SOURCE-0000000003", STREAM_OPERATION_NAME);}
public void kafkatest_f18963_0()
{    final KStream<String, String> streamOne = builder.stream(STREAM_TOPIC);    final KTable<String, String> streamTwo = builder.table(STREAM_TOPIC_TWO);    streamOne.leftJoin(streamTwo, (value1, value2) -> value1, Joined.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", "KSTREAM-SOURCE-0000000002", "KTABLE-SOURCE-0000000003", STREAM_OPERATION_NAME);}
public void kafkatest_f18972_0()
{    builder.table(STREAM_TOPIC).toStream(Named.as("to-stream"));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000001", "KTABLE-SOURCE-0000000002", "to-stream");}
public void kafkatest_f18973_0()
{    builder.table(STREAM_TOPIC).toStream(KeyValue::pair, Named.as("to-stream"));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000001", "KTABLE-SOURCE-0000000002", "to-stream", "KSTREAM-KEY-SELECT-0000000004");}
public void kafkatest_f18982_0()
{    props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 42);    props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);    props.put(StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG, 7L);    props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, "dummy:host");    props.put(StreamsConfig.RETRIES_CONFIG, 10);    props.put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), 5);    props.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG), 100);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> returnedProps = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertEquals(42, returnedProps.get(StreamsConfig.REPLICATION_FACTOR_CONFIG));    assertEquals(1, returnedProps.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG));    assertEquals(StreamsPartitionAssignor.class.getName(), returnedProps.get(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG));    assertEquals(7L, returnedProps.get(StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG));    assertEquals("dummy:host", returnedProps.get(StreamsConfig.APPLICATION_SERVER_CONFIG));    assertNull(returnedProps.get(StreamsConfig.RETRIES_CONFIG));    assertEquals(5, returnedProps.get(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG)));    assertEquals(100, returnedProps.get(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG)));}
public void kafkatest_f18983_0()
{    props.put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), 20);    props.put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRY_BACKOFF_MS_CONFIG), 200L);    props.put(StreamsConfig.RETRIES_CONFIG, 10);    props.put(StreamsConfig.RETRY_BACKOFF_MS_CONFIG, 100L);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> returnedProps = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertEquals(20, returnedProps.get(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG)));    assertEquals(200L, returnedProps.get(StreamsConfig.adminClientPrefix(StreamsConfig.RETRY_BACKOFF_MS_CONFIG)));}
public void kafkatest_f18992_0()
{    props.put(producerPrefix("interceptor.statsd.host"), "host");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(clientId);    assertEquals("host", producerConfigs.get("interceptor.statsd.host"));}
public void kafkatest_f18993_0()
{    props.put(producerPrefix(ProducerConfig.BUFFER_MEMORY_CONFIG), 10);    props.put(producerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> configs = streamsConfig.getProducerConfigs(clientId);    assertEquals(10, configs.get(ProducerConfig.BUFFER_MEMORY_CONFIG));    assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));}
public void kafkatest_f19002_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), "latest");    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "10");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertEquals("latest", consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));    assertEquals("10", consumerConfigs.get(ConsumerConfig.MAX_POLL_RECORDS_CONFIG));}
public void kafkatest_f19003_0()
{    props.put(StreamsConfig.producerPrefix(ProducerConfig.LINGER_MS_CONFIG), "10000");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(clientId);    assertEquals("10000", producerConfigs.get(ProducerConfig.LINGER_MS_CONFIG));}
public void kafkatest_f19012_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), "true");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getGlobalConsumerConfigs(clientId);    assertEquals("false", consumerConfigs.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));}
public void kafkatest_f19013_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "5");    props.put(StreamsConfig.globalConsumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "50");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> returnedProps = streamsConfig.getGlobalConsumerConfigs(clientId);    assertEquals("50", returnedProps.get(ConsumerConfig.MAX_POLL_RECORDS_CONFIG));}
public void kafkatest_f19022_0()
{    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "anyValue");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertThat(consumerConfigs.get(ConsumerConfig.ISOLATION_LEVEL_CONFIG), equalTo(READ_COMMITTED.name().toLowerCase(Locale.ROOT)));}
public void kafkatest_f19023_0()
{    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, READ_UNCOMMITTED.name().toLowerCase(Locale.ROOT));    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertThat(consumerConfigs.get(ConsumerConfig.ISOLATION_LEVEL_CONFIG), equalTo(READ_UNCOMMITTED.name().toLowerCase(Locale.ROOT)));}
public void kafkatest_f19032_0()
{    final Properties props = getStreamsConfig();    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);    final StreamsConfig config = new StreamsConfig(props);    try {        config.defaultKeySerde();        fail("Test should throw a StreamsException");    } catch (final StreamsException e) {        assertEquals("Failed to configure key serde class org.apache.kafka.streams.StreamsConfigTest$MisconfiguredSerde", e.getMessage());    }}
public void kafkatest_f19033_0()
{    final Properties props = getStreamsConfig();    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);    final StreamsConfig config = new StreamsConfig(props);    try {        config.defaultValueSerde();        fail("Test should throw a StreamsException");    } catch (final StreamsException e) {        assertEquals("Failed to configure value serde class org.apache.kafka.streams.StreamsConfigTest$MisconfiguredSerde", e.getMessage());    }}
public Deserializer kafkatest_f19042_0()
{    return null;}
public long kafkatest_f19043_0(final ConsumerRecord<Object, Object> record, final long partitionTime)
{    return 0;}
private KafkaStreams kafkatest_f19052_0(final Properties props)
{    props.put(StreamsConfig.APPLICATION_ID_CONFIG, APP_ID);    props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);    props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);    props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, Integer> data = builder.stream("data");    data.to("echo");    data.process(SmokeTestUtil.printProcessorSupplier("data"));    final KGroupedStream<String, Integer> groupedData = data.groupByKey();    // min    groupedData.aggregate(new Initializer<Integer>() {        @Override        public Integer apply() {            return Integer.MAX_VALUE;        }    }, new Aggregator<String, Integer, Integer>() {        @Override        public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {            return (value < aggregate) ? value : aggregate;        }    }, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>with(null, intSerde)).toStream().to("min", Produced.with(stringSerde, intSerde));    // sum    groupedData.aggregate(new Initializer<Long>() {        @Override        public Long apply() {            return 0L;        }    }, new Aggregator<String, Integer, Long>() {        @Override        public Long apply(final String aggKey, final Integer value, final Long aggregate) {            return (long) value + aggregate;        }    }, Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>with(null, longSerde)).toStream().to("sum", Produced.with(stringSerde, longSerde));    if (withRepartitioning) {        final KStream<String, Integer> repartitionedData = data.through("repartition");        repartitionedData.process(SmokeTestUtil.printProcessorSupplier("repartition"));        final KGroupedStream<String, Integer> groupedDataAfterRepartitioning = repartitionedData.groupByKey();        // max        groupedDataAfterRepartitioning.aggregate(new Initializer<Integer>() {            @Override            public Integer apply() {                return Integer.MIN_VALUE;            }        }, new Aggregator<String, Integer, Integer>() {            @Override            public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {                return (value > aggregate) ? value : aggregate;            }        }, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>with(null, intSerde)).toStream().to("max", Produced.with(stringSerde, intSerde));        // count        groupedDataAfterRepartitioning.count().toStream().to("cnt", Produced.with(stringSerde, longSerde));    }    return new KafkaStreams(builder.build(), props);}
public Integer kafkatest_f19053_0()
{    return Integer.MAX_VALUE;}
public static void kafkatest_f19062_0(final String kafka, final boolean withRepartitioning)
{    final Properties props = new Properties();    props.put(ConsumerConfig.CLIENT_ID_CONFIG, "verifier");    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));    final Map<TopicPartition, Long> committedOffsets;    try (final Admin adminClient = Admin.create(props)) {        ensureStreamsApplicationDown(adminClient);        committedOffsets = getCommittedOffsets(adminClient, withRepartitioning);    }    final String[] allInputTopics;    final String[] allOutputTopics;    if (withRepartitioning) {        allInputTopics = new String[] { "data", "repartition" };        allOutputTopics = new String[] { "echo", "min", "sum", "repartition", "max", "cnt" };    } else {        allInputTopics = new String[] { "data" };        allOutputTopics = new String[] { "echo", "min", "sum" };    }    final Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> inputRecordsPerTopicPerPartition;    try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {        final List<TopicPartition> partitions = getAllPartitions(consumer, allInputTopics);        consumer.assign(partitions);        consumer.seekToBeginning(partitions);        inputRecordsPerTopicPerPartition = getRecords(consumer, committedOffsets, withRepartitioning, true);    } catch (final Exception e) {        e.printStackTrace(System.err);        System.out.println("FAILED");        return;    }    final Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> outputRecordsPerTopicPerPartition;    try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {        final List<TopicPartition> partitions = getAllPartitions(consumer, allOutputTopics);        consumer.assign(partitions);        consumer.seekToBeginning(partitions);        outputRecordsPerTopicPerPartition = getRecords(consumer, consumer.endOffsets(partitions), withRepartitioning, false);    } catch (final Exception e) {        e.printStackTrace(System.err);        System.out.println("FAILED");        return;    }    verifyReceivedAllRecords(inputRecordsPerTopicPerPartition.get("data"), outputRecordsPerTopicPerPartition.get("echo"));    if (withRepartitioning) {        verifyReceivedAllRecords(inputRecordsPerTopicPerPartition.get("data"), outputRecordsPerTopicPerPartition.get("repartition"));    }    verifyMin(inputRecordsPerTopicPerPartition.get("data"), outputRecordsPerTopicPerPartition.get("min"));    verifySum(inputRecordsPerTopicPerPartition.get("data"), outputRecordsPerTopicPerPartition.get("sum"));    if (withRepartitioning) {        verifyMax(inputRecordsPerTopicPerPartition.get("repartition"), outputRecordsPerTopicPerPartition.get("max"));        verifyCnt(inputRecordsPerTopicPerPartition.get("repartition"), outputRecordsPerTopicPerPartition.get("cnt"));    }    try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {        final List<TopicPartition> partitions = getAllPartitions(consumer, allOutputTopics);        consumer.assign(partitions);        consumer.seekToBeginning(partitions);        verifyAllTransactionFinished(consumer, kafka, withRepartitioning);    } catch (final Exception e) {        e.printStackTrace(System.err);        System.out.println("FAILED");        return;    }    // do not modify: required test output    System.out.println("ALL-RECORDS-DELIVERED");    System.out.flush();}
private static void kafkatest_f19063_0(final Admin adminClient)
{    final long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;    ConsumerGroupDescription description;    do {        description = getConsumerGroupDescription(adminClient);        if (System.currentTimeMillis() > maxWaitTime && !description.members().isEmpty()) {            throw new RuntimeException("Streams application not down after " + (MAX_IDLE_TIME_MS / 1000) + " seconds. " + "Group: " + description);        }        sleep(1000);    } while (!description.members().isEmpty());}
private static void kafkatest_f19072_0(final Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> inputPerTopicPerPartition, final Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> cntPerTopicPerPartition)
{    final StringDeserializer stringDeserializer = new StringDeserializer();    final LongDeserializer longDeserializer = new LongDeserializer();    final HashMap<String, Long> currentSumPerKey = new HashMap<>();    for (final Map.Entry<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> partitionRecords : cntPerTopicPerPartition.entrySet()) {        final TopicPartition inputTopicPartition = new TopicPartition("repartition", partitionRecords.getKey().partition());        final List<ConsumerRecord<byte[], byte[]>> partitionInput = inputPerTopicPerPartition.get(inputTopicPartition);        final List<ConsumerRecord<byte[], byte[]>> partitionCnt = partitionRecords.getValue();        if (partitionInput.size() != partitionCnt.size()) {            throw new RuntimeException("Result verification failed: expected " + partitionInput.size() + " records for " + partitionRecords.getKey() + " but received " + partitionCnt.size());        }        final Iterator<ConsumerRecord<byte[], byte[]>> inputRecords = partitionInput.iterator();        for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionCnt) {            final ConsumerRecord<byte[], byte[]> input = inputRecords.next();            final String receivedKey = stringDeserializer.deserialize(receivedRecord.topic(), receivedRecord.key());            final long receivedValue = longDeserializer.deserialize(receivedRecord.topic(), receivedRecord.value());            final String key = stringDeserializer.deserialize(input.topic(), input.key());            Long cnt = currentSumPerKey.get(key);            if (cnt == null) {                cnt = 0L;            }            currentSumPerKey.put(key, ++cnt);            if (!receivedKey.equals(key) || receivedValue != cnt) {                throw new RuntimeException("Result verification failed for " + receivedRecord + " expected <" + key + "," + cnt + "> but was <" + receivedKey + "," + receivedValue + ">");            }        }    }}
private static void kafkatest_f19073_0(final KafkaConsumer<byte[], byte[]> consumer, final String kafka, final boolean withRepartitioning)
{    final String[] topics;    if (withRepartitioning) {        topics = new String[] { "echo", "min", "sum", "repartition", "max", "cnt" };    } else {        topics = new String[] { "echo", "min", "sum" };    }    final List<TopicPartition> partitions = getAllPartitions(consumer, topics);    consumer.assign(partitions);    consumer.seekToEnd(partitions);    for (final TopicPartition tp : partitions) {        System.out.println(tp + " at position " + consumer.position(tp));    }    final Properties producerProps = new Properties();    producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, "VerifyProducer");    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);    try (final KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps)) {        for (final TopicPartition tp : partitions) {            final ProducerRecord<String, String> record = new ProducerRecord<>(tp.topic(), tp.partition(), "key", "value");            producer.send(record, (metadata, exception) -> {                if (exception != null) {                    exception.printStackTrace(System.err);                    System.err.flush();                    Exit.exit(1);                }            });        }    }    final StringDeserializer stringDeserializer = new StringDeserializer();    long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;    while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {        final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));        if (records.isEmpty()) {            System.out.println("No data received.");            for (final TopicPartition tp : partitions) {                System.out.println(tp + " at position " + consumer.position(tp));            }        }        for (final ConsumerRecord<byte[], byte[]> record : records) {            maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;            final String topic = record.topic();            final TopicPartition tp = new TopicPartition(topic, record.partition());            try {                final String key = stringDeserializer.deserialize(topic, record.key());                final String value = stringDeserializer.deserialize(topic, record.value());                if (!("key".equals(key) && "value".equals(value) && partitions.remove(tp))) {                    throw new RuntimeException("Post transactions verification failed. Received unexpected verification record: " + "Expected record <'key','value'> from one of " + partitions + " but got" + " <" + key + "," + value + "> [" + record.topic() + ", " + record.partition() + "]");                } else {                    System.out.println("Verifying " + tp + " successful.");                }            } catch (final SerializationException e) {                throw new RuntimeException("Post transactions verification failed. Received unexpected verification record: " + "Expected record <'key','value'> from one of " + partitions + " but got " + record, e);            }        }    }    if (!partitions.isEmpty()) {        throw new RuntimeException("Could not read all verification records. Did not receive any new record within the last " + (MAX_IDLE_TIME_MS / 1000) + " sec.");    }}
public void kafkatest_f19082_0()
{    streams.close(Duration.ZERO);}
public void kafkatest_f19083_0()
{    streams.close(Duration.ofSeconds(5));    // do not remove these printouts since they are needed for health scripts    if (!uncaughtException) {        System.out.println(name + ": SMOKE-TEST-CLIENT-CLOSED");    }    try {        thread.join();    } catch (final Exception ex) {        // do not remove these printouts since they are needed for health scripts        System.out.println(name + ": SMOKE-TEST-CLIENT-EXCEPTION");    // ignore    }}
private static Properties kafkatest_f19092_0(final String kafka)
{    final Properties producerProps = new Properties();    producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, "SmokeTest");    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);    producerProps.put(ProducerConfig.ACKS_CONFIG, "all");    return producerProps;}
public void kafkatest_f19093_0(final RecordMetadata metadata, final Exception exception)
{    if (exception != null) {        if (exception instanceof TimeoutException) {            needRetry.add(originalRecord);        } else {            exception.printStackTrace();            Exit.exit(1);        }    }}
private static String kafkatest_f19102_0(@SuppressWarnings("SameParameterValue") final String prefix, final Iterable<ConsumerRecord<String, Number>> list)
{    final StringBuilder stringBuilder = new StringBuilder();    for (final ConsumerRecord<String, Number> record : list) {        stringBuilder.append(prefix).append(record).append('\n');    }    return stringBuilder.toString();}
private static Long kafkatest_f19103_0(final String key)
{    final int min = getMin(key).intValue();    final int max = getMax(key).intValue();    return ((long) min + max) * (max - min + 1L) / 2L;}
public void kafkatest_f19112_0(final ProcessorContext context)
{    super.init(context);    System.out.println("[DEV] initializing processor: topic=" + topic + " taskId=" + context.taskId());    numRecordsProcessed = 0;}
public void kafkatest_f19113_0(final Object key, final Object value)
{    numRecordsProcessed++;    if (numRecordsProcessed % 100 == 0) {        System.out.printf("%s: %s%n", name, Instant.now());        System.out.println("processed " + numRecordsProcessed + " records from topic=" + topic);    }}
public Long kafkatest_f19122_0(final String aggKey, final Long value, final Long aggregate)
{    return aggregate - value;}
 static File kafkatest_f19123_0(final File parent, final String child)
{    final File dir = new File(parent, child);    dir.mkdir();    return dir;}
private static Map<String, String> kafkatest_f19132_0(final String formattedConfigs)
{    final String[] parts = formattedConfigs.split(",");    final Map<String, String> updatedConfigs = new HashMap<>();    for (final String part : parts) {        final String[] keyValue = part.split("=");        updatedConfigs.put(keyValue[KEY], keyValue[VALUE]);    }    return updatedConfigs;}
public static void kafkatest_f19133_0(final String[] args) throws IOException
{    if (args.length < 2) {        System.err.println("StreamsEosTest are expecting two parameters: propFile, command; but only see " + args.length + " parameter");        System.exit(1);    }    final String propFileName = args[0];    final String command = args[1];    final Properties streamsProperties = Utils.loadProps(propFileName);    final String kafka = streamsProperties.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);    if (kafka == null) {        System.err.println("No bootstrap kafka servers specified in " + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);        System.exit(1);    }    System.out.println("StreamsTest instance started");    System.out.println("kafka=" + kafka);    System.out.println("props=" + streamsProperties);    System.out.println("command=" + command);    System.out.flush();    if (command == null || propFileName == null) {        System.exit(-1);    }    switch(command) {        case "run":            EosTestDriver.generate(kafka);            break;        case "process":            new EosTestClient(streamsProperties, false).start();            break;        case "process-complex":            new EosTestClient(streamsProperties, true).start();            break;        case "verify":            EosTestDriver.verify(kafka, false);            break;        case "verify-complex":            EosTestDriver.verify(kafka, true);            break;        default:            System.out.println("unknown command: " + command);            System.out.flush();            System.exit(-1);    }}
public static void kafkatest_f19142_0(final String[] args) throws Exception
{    if (args.length < 1) {        System.err.println("StreamsUpgradeTest requires one argument (properties-file) but no provided: ");    }    final String propFileName = args.length > 0 ? args[0] : null;    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started (StreamsUpgradeTest trunk)");    System.out.println("props=" + streamsProperties);    final StreamsBuilder builder = new StreamsBuilder();    final KStream dataStream = builder.stream("data");    dataStream.process(SmokeTestUtil.printProcessorSupplier("data"));    dataStream.to("echo");    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsUpgradeTest");    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    final KafkaClientSupplier kafkaClientSupplier;    if (streamsProperties.containsKey("test.future.metadata")) {        streamsProperties.remove("test.future.metadata");        kafkaClientSupplier = new FutureKafkaClientSupplier();    } else {        kafkaClientSupplier = new DefaultKafkaClientSupplier();    }    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder.build(), config, kafkaClientSupplier);    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread(() -> {        System.out.println("closing Kafka Streams instance");        System.out.flush();        streams.close();        System.out.println("UPGRADE-TEST-CLIENT-CLOSED");        System.out.flush();    }));}
public Consumer<byte[], byte[]> kafkatest_f19143_0(final Map<String, Object> config)
{    config.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, FutureStreamsPartitionAssignor.class.getName());    return new KafkaConsumer<>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());}
public void kafkatest_f19152_0()
{    final String formattedConfigs = "foo=foo1,bar=bar1,baz=baz1";    final Map<String, String> parsedMap = SystemTestUtil.parseConfigs(formattedConfigs);    final TreeMap<String, String> sortedParsedMap = new TreeMap<>(parsedMap);    assertEquals(sortedParsedMap, expectedParsedMap);}
public void kafkatest_f19153_0()
{    SystemTestUtil.parseConfigs(null);}
public void kafkatest_f19162_0()
{    final Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(topicPartition, 4L);    consumer.updateEndOffsets(endOffsets);    final Map<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(topicPartition, 0L);    consumer.updateBeginningOffsets(beginningOffsets);    streamsResetter.shiftOffsetsBy(consumer, inputTopicPartitions, -3L);    final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(500));    assertEquals(5, records.count());}
public void kafkatest_f19163_0()
{    final Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(topicPartition, 3L);    consumer.updateEndOffsets(endOffsets);    final Map<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(topicPartition, 0L);    consumer.updateBeginningOffsets(beginningOffsets);    streamsResetter.shiftOffsetsBy(consumer, inputTopicPartitions, 5L);    final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(500));    assertEquals(2, records.count());}
private void kafkatest_f19172_0(final SimpleDateFormat format) throws ParseException
{    final Date checkpoint = new Date();    final StreamsResetter streamsResetter = new StreamsResetter();    final String formattedCheckpoint = format.format(checkpoint);    streamsResetter.getDateTime(formattedCheckpoint);}
public void kafkatest_f19173_0()
{    topology.addSource((String) null, "topic");}
public void kafkatest_f19182_0()
{    topology.addSink("name", (TopicNameExtractor<Object, Object>) null);}
public void kafkatest_f19183_0()
{    topology.connectProcessorAndStateStores(null, "store");}
public void kafkatest_f19192_0()
{    topology.addSource("source", "topic-1");    try {        topology.addProcessor("processor", new MockProcessorSupplier());        fail("Should throw TopologyException for processor without at least one parent node");    } catch (final TopologyException expected) {    }}
public void kafkatest_f19193_0()
{    topology.addSource("source", "topic-1");    try {        topology.addProcessor("processor", new MockProcessorSupplier(), (String) null);        fail("Should throw NullPointerException for processor when null parent names are provided");    } catch (final NullPointerException expected) {    }}
public void kafkatest_f19202_0()
{    mockStoreBuilder();    EasyMock.replay(storeBuilder);    topology.addStateStore(storeBuilder, "no-such-processor");}
public void kafkatest_f19203_0()
{    mockStoreBuilder();    EasyMock.replay(storeBuilder);    topology.addSource("source-1", "topic-1");    try {        topology.addStateStore(storeBuilder, "source-1");        fail("Should have thrown TopologyException for adding store to source node");    } catch (final TopologyException expected) {    }}
public void kafkatest_f19214_0()
{    final TopologyDescription.Sink expectedSinkNode = new InternalTopologyBuilder.Sink("sink", (key, value, record) -> record.topic() + "-" + key);    assertThat(expectedSinkNode.topic(), equalTo(null));}
public void kafkatest_f19215_0()
{    final TopicNameExtractor topicNameExtractor = (key, value, record) -> record.topic() + "-" + key;    final TopologyDescription.Sink expectedSinkNode = new InternalTopologyBuilder.Sink("sink", topicNameExtractor);    assertThat(expectedSinkNode.topicNameExtractor(), equalTo(topicNameExtractor));}
public void kafkatest_f19224_0()
{    final TopologyDescription.Source expectedSourceNode1 = addSource("source1", "topic0");    final TopologyDescription.Source expectedSourceNode2 = addSource("source2", Pattern.compile("topic[1-9]"));    final TopologyDescription.Processor expectedProcessorNode = addProcessor("processor", expectedSourceNode1, expectedSourceNode2);    final Set<TopologyDescription.Node> allNodes = new HashSet<>();    allNodes.add(expectedSourceNode1);    allNodes.add(expectedSourceNode2);    allNodes.add(expectedProcessorNode);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, allNodes));    assertThat(topology.describe(), equalTo(expectedDescription));}
public void kafkatest_f19225_0()
{    final TopologyDescription.Source expectedSourceNode1 = addSource("source1", "topic1");    final TopologyDescription.Processor expectedProcessorNode1 = addProcessor("processor1", expectedSourceNode1);    final TopologyDescription.Source expectedSourceNode2 = addSource("source2", "topic2");    final TopologyDescription.Processor expectedProcessorNode2 = addProcessor("processor2", expectedSourceNode2);    final TopologyDescription.Source expectedSourceNode3 = addSource("source3", "topic3");    final TopologyDescription.Processor expectedProcessorNode3 = addProcessor("processor3", expectedSourceNode3);    final Set<TopologyDescription.Node> allNodes1 = new HashSet<>();    allNodes1.add(expectedSourceNode1);    allNodes1.add(expectedProcessorNode1);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, allNodes1));    final Set<TopologyDescription.Node> allNodes2 = new HashSet<>();    allNodes2.add(expectedSourceNode2);    allNodes2.add(expectedProcessorNode2);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(1, allNodes2));    final Set<TopologyDescription.Node> allNodes3 = new HashSet<>();    allNodes3.add(expectedSourceNode3);    allNodes3.add(expectedProcessorNode3);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(2, allNodes3));    assertThat(topology.describe(), equalTo(expectedDescription));}
public void kafkatest_f19234_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("input-topic").groupByKey().count();    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])\n" + "      --> KSTREAM-AGGREGATE-0000000002\n" + "    Processor: KSTREAM-AGGREGATE-0000000002 (stores: [KSTREAM-AGGREGATE-STATE-STORE-0000000001])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000000\n\n", describe.toString());}
public void kafkatest_f19235_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("input-topic").groupByKey().count(Materialized.as("count-store"));    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])\n" + "      --> KSTREAM-AGGREGATE-0000000001\n" + "    Processor: KSTREAM-AGGREGATE-0000000001 (stores: [count-store])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000000\n\n", describe.toString());}
public void kafkatest_f19244_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.table("input-topic").groupBy((key, value) -> null).count(Materialized.as("count-store"));    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000001 (topics: [input-topic])\n" + "      --> KTABLE-SOURCE-0000000002\n" + "    Processor: KTABLE-SOURCE-0000000002 (stores: [input-topic-STATE-STORE-0000000000])\n" + "      --> KTABLE-SELECT-0000000003\n" + "      <-- KSTREAM-SOURCE-0000000001\n" + "    Processor: KTABLE-SELECT-0000000003 (stores: [])\n" + "      --> KSTREAM-SINK-0000000004\n" + "      <-- KTABLE-SOURCE-0000000002\n" + "    Sink: KSTREAM-SINK-0000000004 (topic: count-store-repartition)\n" + "      <-- KTABLE-SELECT-0000000003\n" + "\n" + "  Sub-topology: 1\n" + "    Source: KSTREAM-SOURCE-0000000005 (topics: [count-store-repartition])\n" + "      --> KTABLE-AGGREGATE-0000000006\n" + "    Processor: KTABLE-AGGREGATE-0000000006 (stores: [count-store])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000005\n" + "\n", describe.toString());}
public void kafkatest_f19245_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.table("input-topic").groupBy((key, value) -> null).count(Materialized.with(null, Serdes.Long()));    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000001 (topics: [input-topic])\n" + "      --> KTABLE-SOURCE-0000000002\n" + "    Processor: KTABLE-SOURCE-0000000002 (stores: [input-topic-STATE-STORE-0000000000])\n" + "      --> KTABLE-SELECT-0000000003\n" + "      <-- KSTREAM-SOURCE-0000000001\n" + "    Processor: KTABLE-SELECT-0000000003 (stores: [])\n" + "      --> KSTREAM-SINK-0000000005\n" + "      <-- KTABLE-SOURCE-0000000002\n" + "    Sink: KSTREAM-SINK-0000000005 (topic: KTABLE-AGGREGATE-STATE-STORE-0000000004-repartition)\n" + "      <-- KTABLE-SELECT-0000000003\n" + "\n" + "  Sub-topology: 1\n" + "    Source: KSTREAM-SOURCE-0000000006 (topics: [KTABLE-AGGREGATE-STATE-STORE-0000000004-repartition])\n" + "      --> KTABLE-AGGREGATE-0000000007\n" + "    Processor: KTABLE-AGGREGATE-0000000007 (stores: [KTABLE-AGGREGATE-STATE-STORE-0000000004])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000006\n" + "\n", describe.toString());}
private TopologyDescription.Processor kafkatest_f19254_0(final String processorName, final TopologyDescription.Node... parents)
{    return addProcessorWithNewStore(processorName, new String[0], parents);}
private TopologyDescription.Processor kafkatest_f19255_0(final String processorName, final String[] storeNames, final TopologyDescription.Node... parents)
{    return addProcessorWithStore(processorName, storeNames, true, parents);}
public InternalTopologyBuilder kafkatest_f19264_0(final String applicationId)
{    return internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(applicationId)));}
public String kafkatest_f19265_0()
{    return this.name;}
public synchronized V kafkatest_f19274_0(final K key)
{    return this.map.remove(key);}
public synchronized KeyValueIterator<K, V> kafkatest_f19275_0(final K from, final K to)
{    return new DelegatingPeekingKeyValueIterator<>(name, new GenericInMemoryKeyValueIterator<>(this.map.subMap(from, true, to, true).entrySet().iterator()));}
public String kafkatest_f19284_0()
{    return this.name;}
public void kafkatest_f19285_0(final ProcessorContext context, final StateStore root)
{    if (root != null) {        context.register(root, null);    }    this.open = true;}
public synchronized KeyValueIterator<K, ValueAndTimestamp<V>> kafkatest_f19294_0(final K from, final K to)
{    return new DelegatingPeekingKeyValueIterator<>(name, new GenericInMemoryKeyValueIterator<>(this.map.subMap(from, true, to, true).entrySet().iterator()));}
public synchronized KeyValueIterator<K, ValueAndTimestamp<V>> kafkatest_f19295_0()
{    final TreeMap<K, ValueAndTimestamp<V>> copy = new TreeMap<>(this.map);    return new DelegatingPeekingKeyValueIterator<>(name, new GenericInMemoryKeyValueIterator<>(copy.entrySet().iterator()));}
public File kafkatest_f19306_0()
{    return null;}
public void kafkatest_f19309_0(final boolean clean) throws IOException
{    this.offsets.putAll(offsets);    closed = true;}
public Serde<?> kafkatest_f19318_0()
{    return valSerde;}
public File kafkatest_f19320_0()
{    if (stateDir == null) {        throw new UnsupportedOperationException("State directory not specified");    }    return stateDir;}
public void kafkatest_f19330_0(final long timestamp)
{    if (recordContext != null) {        recordContext = new ProcessorRecordContext(timestamp, recordContext.offset(), recordContext.partition(), recordContext.topic(), recordContext.headers());    }    this.timestamp = timestamp;}
public long kafkatest_f19331_0()
{    if (recordContext == null) {        return timestamp;    }    return recordContext.timestamp();}
public void kafkatest_f19340_0()
{// no-op}
public K kafkatest_f19341_0()
{    return null;}
public void kafkatest_f19350_0(final Cluster cluster)
{    this.cluster = cluster;}
public Admin kafkatest_f19351_0(final Map<String, Object> config)
{    return new MockAdminClient(cluster.nodes(), cluster.nodeById(0));}
public List<ProducerRecord<byte[], byte[]>> kafkatest_f19363_0()
{    return unmodifiableList(collected);}
public StreamsMetricsImpl kafkatest_f19364_0()
{    return (StreamsMetricsImpl) super.metrics();}
public StateRestoreCallback kafkatest_f19375_0(final String storeName)
{    return restoreCallbacks.get(storeName);}
public void kafkatest_f19376_0(final Map<String, InternalTopicConfig> topics)
{    for (final InternalTopicConfig topic : topics.values()) {        final String topicName = topic.name();        final int numberOfPartitions = topic.numberOfPartitions().get();        readyTopics.put(topicName, numberOfPartitions);        final List<PartitionInfo> partitions = new ArrayList<>();        for (int i = 0; i < numberOfPartitions; i++) {            partitions.add(new PartitionInfo(topicName, i, null, null, null));        }        restoreConsumer.updatePartitions(topicName, partitions);    }}
public void kafkatest_f19385_0(final byte[] key, final byte[] value)
{    keys.add(deserializer.deserialize("", key));    values.add(value);}
public Object kafkatest_f19387_0(final Object key, final Object value)
{    return null;}
public KeyValue<V, V> kafkatest_f19397_0(final K key, final V value)
{    return KeyValue.pair(value, value);}
public V kafkatest_f19398_0(final K key, final V value)
{    return value;}
public boolean kafkatest_f19407_0(final K key, final V value)
{    return true;}
public static Predicate<K, V> kafkatest_f19408_0()
{    return new AllGoodPredicate<>();}
public void kafkatest_f19417_0()
{    super.close();    this.closed = true;}
public Processor<K, V> kafkatest_f19418_0()
{    final MockProcessor<K, V> processor = new MockProcessor<>(punctuationType, scheduleInterval);    processors.add(processor);    return processor;}
public void kafkatest_f19427_0()
{    assignedPartition = null;    seekOffset = -1L;    endOffset = 0L;    recordBuffer.clear();}
public void kafkatest_f19428_0(final ConsumerRecord<K, V> record)
{    recordBuffer.add(new ConsumerRecord<>(record.topic(), record.partition(), record.offset(), record.timestamp(), record.timestampType(), 0L, 0, 0, keySerializer.serialize(record.topic(), record.headers(), record.key()), valueSerializer.serialize(record.topic(), record.headers(), record.value()), record.headers()));    endOffset = record.offset();    super.updateEndOffsets(Collections.singletonMap(assignedPartition, endOffset));}
public void kafkatest_f19437_0()
{    super.close();    this.closed = true;}
public void kafkatest_f19438_0(final byte[] key, final byte[] value)
{    restored.add(KeyValue.pair(key, value));}
public StateStore kafkatest_f19447_0(final String name)
{    return null;}
public Cancellable kafkatest_f19448_0(final long interval, final PunctuationType type, final Punctuator callback)
{    return null;}
public KeyValueIterator<K, V> kafkatest_f19459_0()
{    return null;}
public long kafkatest_f19460_0()
{    return 0L;}
public KeyValueIterator<Windowed<K>, V> kafkatest_f19474_0(final K key)
{    if (!open) {        throw new InvalidStateStoreException("not open");    }    if (!sessions.containsKey(key)) {        return new KeyValueIteratorStub<>(Collections.<KeyValue<Windowed<K>, V>>emptyIterator());    }    return new KeyValueIteratorStub<>(sessions.get(key).iterator());}
public KeyValueIterator<Windowed<K>, V> kafkatest_f19475_0(final K from, final K to)
{    if (!open) {        throw new InvalidStateStoreException("not open");    }    if (sessions.subMap(from, true, to, true).isEmpty()) {        return new KeyValueIteratorStub<>(Collections.<KeyValue<Windowed<K>, V>>emptyIterator());    }    final Iterator<List<KeyValue<Windowed<K>, V>>> keysIterator = sessions.subMap(from, true, to, true).values().iterator();    return new KeyValueIteratorStub<>(new Iterator<KeyValue<Windowed<K>, V>>() {        Iterator<KeyValue<Windowed<K>, V>> it;        @Override        public boolean hasNext() {            while (it == null || !it.hasNext()) {                if (!keysIterator.hasNext()) {                    return false;                }                it = keysIterator.next().iterator();            }            return true;        }        @Override        public KeyValue<Windowed<K>, V> next() {            return it.next();        }    });}
public KeyValueIterator<Bytes, byte[]> kafkatest_f19487_0(final Bytes key, final long from, final long to)
{    return fetch(key, key, from, to);}
public KeyValueIterator<Bytes, byte[]> kafkatest_f19488_0(final Bytes keyFrom, final Bytes keyTo, final long from, final long to)
{    fetchCalled = true;    return new KeyValueIteratorStub<>(Collections.<KeyValue<Bytes, byte[]>>emptyIterator());}
public boolean kafkatest_f19497_0()
{    return false;}
public void kafkatest_f19498_0(final ProcessorContext context)
{    SingletonNoOpValueTransformer.this.context = context;}
public static List<KeyValue<K, V>> kafkatest_f19508_0(final Iterator<KeyValue<K, V>> iterator)
{    final List<KeyValue<K, V>> results = new ArrayList<>();    while (iterator.hasNext()) {        results.add(iterator.next());    }    return results;}
public static List<V> kafkatest_f19509_0(final Iterator<KeyValue<K, V>> iterator)
{    final List<V> results = new ArrayList<>();    while (iterator.hasNext()) {        results.add(iterator.next().value);    }    return results;}
public V kafkatest_f19518_0(final K key)
{    return getValueOrNull(inner.delete(key));}
public void kafkatest_f19519_0()
{    inner.flush();}
public void kafkatest_f19528_0()
{    inner.close();}
public String kafkatest_f19529_0()
{    return inner.name();}
public long kafkatest_f19538_0()
{    return timestamp;}
public KeyValue kafkatest_f19539_0()
{    return keyValue;}
public void kafkatest_f19548_0(final String topic, final int partition, final long offset, final Headers headers, final long timestamp)
{    this.topic = topic;    this.partition = partition;    this.offset = offset;    this.headers = headers;    this.timestamp = timestamp;}
public void kafkatest_f19549_0(final String topic)
{    this.topic = topic;}
public long kafkatest_f19558_0()
{    if (timestamp == null) {        throw new IllegalStateException("Timestamp must be set before use via setRecordMetadata() or setTimestamp().");    }    return timestamp;}
public void kafkatest_f19559_0(final StateStore store, final StateRestoreCallback stateRestoreCallbackIsIgnoredInMock)
{    stateStores.put(store.name(), store);}
public List<CapturedForward> kafkatest_f19568_0()
{    return new LinkedList<>(capturedForwards);}
public List<CapturedForward> kafkatest_f19569_0(final String childName)
{    final LinkedList<CapturedForward> result = new LinkedList<>();    for (final CapturedForward capture : capturedForwards) {        if (capture.childName() == null || capture.childName().equals(childName)) {            result.add(capture);        }    }    return result;}
public ConsumerRecord<byte[], byte[]> kafkatest_f19578_0(final K key, final V value, final long timestampMs)
{    return create(key, value, new RecordHeaders(), timestampMs);}
public ConsumerRecord<byte[], byte[]> kafkatest_f19579_0(final K key, final V value, final Headers headers, final long timestampMs)
{    if (topicName == null) {        throw new IllegalStateException("ConsumerRecordFactory was created without defaultTopicName. " + "Use #create(String topicName, K key, V value, long timestampMs) instead.");    }    return create(topicName, key, value, headers, timestampMs);}
public ConsumerRecord<byte[], byte[]> kafkatest_f19588_0(final String topicName, final V value, final Headers headers)
{    return create(topicName, null, value, headers);}
public ConsumerRecord<byte[], byte[]> kafkatest_f19589_0(final String topicName, final V value)
{    return create(topicName, null, value, new RecordHeaders());}
public static void kafkatest_f19598_0(final ProducerRecord<K, V> record, final V expectedValue) throws AssertionError
{    Objects.requireNonNull(record);    final V recordValue = record.value();    final AssertionError error = new AssertionError("Expected value=" + expectedValue + " but was value=" + recordValue);    if (recordValue != null) {        if (!recordValue.equals(expectedValue)) {            throw error;        }    } else if (expectedValue != null) {        throw error;    }}
public static void kafkatest_f19599_0(final ProducerRecord<K, V> record, final ProducerRecord<K, V> expectedRecord) throws AssertionError
{    Objects.requireNonNull(expectedRecord);    compareValue(record, expectedRecord.value());}
public static void kafkatest_f19608_0(final ProducerRecord<K, V> record, final K expectedKey, final V expectedValue, final Headers expectedHeaders) throws AssertionError
{    Objects.requireNonNull(record);    final K recordKey = record.key();    final V recordValue = record.value();    final Headers recordHeaders = record.headers();    final AssertionError error = new AssertionError("Expected <" + expectedKey + ", " + expectedValue + "> with headers=" + expectedHeaders + " but was <" + recordKey + ", " + recordValue + "> with headers=" + recordHeaders);    if (recordKey != null) {        if (!recordKey.equals(expectedKey)) {            throw error;        }    } else if (expectedKey != null) {        throw error;    }    if (recordValue != null) {        if (!recordValue.equals(expectedValue)) {            throw error;        }    } else if (expectedValue != null) {        throw error;    }    if (recordHeaders != null) {        if (!recordHeaders.equals(expectedHeaders)) {            throw error;        }    } else if (expectedHeaders != null) {        throw error;    }}
public static void kafkatest_f19609_0(final ProducerRecord<K, V> record, final ProducerRecord<K, V> expectedRecord) throws AssertionError
{    Objects.requireNonNull(expectedRecord);    compareKeyValueHeaders(record, expectedRecord.key(), expectedRecord.value(), expectedRecord.headers());}
public void kafkatest_f19621_0(final List<ConsumerRecord<byte[], byte[]>> records)
{    for (final ConsumerRecord<byte[], byte[]> record : records) {        pipeInput(record);    }}
public void kafkatest_f19622_0(final long advanceMs)
{    mockWallClockTime.sleep(advanceMs);    if (task != null) {        task.maybePunctuateSystemTime();        task.commit();    }    captureOutputRecords();}
public WindowStore<K, V>f19631_1final String name)
{    final StateStore store = getStateStore(name, false);    if (store instanceof TimestampedWindowStore) {                return new WindowStoreFacade<>((TimestampedWindowStore<K, V>) store);    }    return store instanceof WindowStore ? (WindowStore<K, V>) store : null;}
public WindowStore<K, ValueAndTimestamp<V>> kafkatest_f19632_0(final String name)
{    final StateStore store = getStateStore(name, false);    return store instanceof TimestampedWindowStore ? (TimestampedWindowStore<K, V>) store : null;}
public synchronized long kafkatest_f19643_0(final TopicPartition partition)
{    return 0L;}
public void kafkatest_f19644_0()
{    keyValueStoreFacade = new KeyValueStoreFacade<>(mockedKeyValueTimestampStore);}
public void kafkatest_f19653_0()
{    expect(mockedKeyValueTimestampStore.persistent()).andReturn(true).andReturn(false);    replay(mockedKeyValueTimestampStore);    assertThat(keyValueStoreFacade.persistent(), is(true));    assertThat(keyValueStoreFacade.persistent(), is(false));    verify(mockedKeyValueTimestampStore);}
public void kafkatest_f19654_0()
{    expect(mockedKeyValueTimestampStore.isOpen()).andReturn(true).andReturn(false);    replay(mockedKeyValueTimestampStore);    assertThat(keyValueStoreFacade.isOpen(), is(true));    assertThat(keyValueStoreFacade.isOpen(), is(false));    verify(mockedKeyValueTimestampStore);}
public void kafkatest_f19663_0()
{    expect(mockedWindowTimestampStore.isOpen()).andReturn(true).andReturn(false);    replay(mockedWindowTimestampStore);    assertThat(windowStoreFacade.isOpen(), is(true));    assertThat(windowStoreFacade.isOpen(), is(false));    verify(mockedWindowTimestampStore);}
public void kafkatest_f19664_0()
{    final AbstractProcessor<String, Long> processor = new AbstractProcessor<String, Long>() {        @Override        public void process(final String key, final Long value) {            context().forward(key + value, key.length() + value);        }    };    final MockProcessorContext context = new MockProcessorContext();    processor.init(context);    processor.process("foo", 5L);    processor.process("barbaz", 50L);    final Iterator<CapturedForward> forwarded = context.forwarded().iterator();    assertEquals(new KeyValue<>("foo5", 8L), forwarded.next().keyValue());    assertEquals(new KeyValue<>("barbaz50", 56L), forwarded.next().keyValue());    assertFalse(forwarded.hasNext());    context.resetForwards();    assertEquals(0, context.forwarded().size());}
public void kafkatest_f19673_0(final String key, final Long value)
{    context().forward(key, value, "child1");}
public void kafkatest_f19674_0()
{    final AbstractProcessor<String, Long> processor = new AbstractProcessor<String, Long>() {        private int count = 0;        @Override        public void process(final String key, final Long value) {            if (++count > 2) {                context().commit();            }        }    };    final MockProcessorContext context = new MockProcessorContext();    processor.init(context);    processor.process("foo", 5L);    processor.process("barbaz", 50L);    assertFalse(context.committed());    processor.process("foobar", 500L);    assertTrue(context.committed());    context.resetCommit();    assertFalse(context.committed());}
public void kafkatest_f19685_0()
{    final TopologyTestDriver.MockTime time = new TopologyTestDriver.MockTime(42L);    assertEquals(42L, time.milliseconds());    assertEquals(42L * 1000L * 1000L, time.nanoseconds());}
public void kafkatest_f19686_0()
{    final TopologyTestDriver.MockTime time = new TopologyTestDriver.MockTime(42L);    assertEquals(42L, time.hiResClockMs());}
public void kafkatest_f19695_0()
{    factory.create(null, Collections.singletonList(KeyValue.pair(rawKey, value)));}
public void kafkatest_f19696_0()
{    factory.create(null, Collections.singletonList(KeyValue.pair(rawKey, value)), timestamp, 2L);}
public void kafkatest_f19705_0()
{    verifyRecord(otherTopicName, rawKey, rawValue, 0L, factory.create(otherTopicName, rawKey, value));    factory.advanceTimeMs(3L);    verifyRecord(otherTopicName, rawKey, rawValue, 3L, factory.create(otherTopicName, rawKey, value));}
public void kafkatest_f19706_0()
{    verifyRecord(topicName, rawKey, rawValue, 0L, factory.create(rawKey, value));    factory.advanceTimeMs(3L);    verifyRecord(topicName, rawKey, rawValue, 3L, factory.create(rawKey, value));}
public void kafkatest_f19715_0()
{    OutputVerifier.compareValue((ProducerRecord<byte[], byte[]>) null, producerRecord);}
public void kafkatest_f19716_0()
{    OutputVerifier.compareValue(producerRecord, (ProducerRecord<byte[], byte[]>) null);}
public void kafkatest_f19725_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, null);}
public void kafkatest_f19726_0()
{    OutputVerifier.compareValue(producerRecord, value);}
public void kafkatest_f19735_0()
{    OutputVerifier.compareValue(nullKeyValueRecord, new ProducerRecord<>("sameTopic", Integer.MAX_VALUE, Long.MAX_VALUE, value, value));}
public void kafkatest_f19736_0()
{    OutputVerifier.compareKeyValue(producerRecord, key, value);}
public void kafkatest_f19745_0()
{    OutputVerifier.compareKeyValue(nullKeyValueRecord, new ProducerRecord<byte[], byte[]>("otherTopic", 0, 0L, null, null));}
public void kafkatest_f19746_0()
{    OutputVerifier.compareKeyValue(producerRecord, new ProducerRecord<>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, value, value));}
public void kafkatest_f19755_0()
{    OutputVerifier.compareValueTimestamp(producerRecord, null, Long.MAX_VALUE);}
public void kafkatest_f19756_0()
{    OutputVerifier.compareValueTimestamp(nullKeyValueRecord, value, Long.MAX_VALUE);}
public void kafkatest_f19765_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, key, value, Long.MAX_VALUE);}
public void kafkatest_f19766_0()
{    OutputVerifier.compareKeyValueTimestamp(nullKeyValueRecord, null, null, Long.MAX_VALUE);}
public void kafkatest_f19775_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, new ProducerRecord<>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, value, value));}
public void kafkatest_f19776_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, new ProducerRecord<byte[], byte[]>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, null, value));}
public void kafkatest_f19785_0(final long timestamp)
{    punctuatedAt.add(timestamp);}
public void kafkatest_f19786_0(final ProcessorContext context)
{    initialized = true;    this.context = context;    for (final Punctuation punctuation : punctuations) {        this.context.schedule(Duration.ofMillis(punctuation.intervalMs), punctuation.punctuationType, punctuation.callback);    }}
private Topology kafkatest_f19795_0(final String... sourceTopicNames)
{    final Topology topology = new Topology();    final String[] processorNames = new String[sourceTopicNames.length];    int i = 0;    for (final String sourceTopicName : sourceTopicNames) {        final String sourceName = sourceTopicName + "-source";        final String processorName = sourceTopicName + "-processor";        topology.addSource(sourceName, sourceTopicName);        processorNames[i++] = processorName;        topology.addProcessor(processorName, new MockProcessorSupplier(), sourceName);    }    topology.addSink("sink-topic", SINK_TOPIC_1, processorNames);    return topology;}
private Topology kafkatest_f19796_0(final String... sourceTopicNames)
{    if (sourceTopicNames.length == 0) {        throw new IllegalArgumentException("sourceTopicNames cannot be empty");    }    final Topology topology = new Topology();    for (final String sourceTopicName : sourceTopicNames) {        topology.addGlobalStore(Stores.<Bytes, byte[]>keyValueStoreBuilder(Stores.inMemoryKeyValueStore(sourceTopicName + "-globalStore"), null, null).withLoggingDisabled(), sourceTopicName, null, null, sourceTopicName, sourceTopicName + "-processor", new MockProcessorSupplier());    }    return topology;}
public void kafkatest_f19805_0()
{    testDriver = new TopologyTestDriver(setupMultipleSourceTopology(SOURCE_TOPIC_1, SOURCE_TOPIC_2), config);    final List<Record> processedRecords1 = mockProcessors.get(0).processedRecords;    final List<Record> processedRecords2 = mockProcessors.get(1).processedRecords;    final List<ConsumerRecord<byte[], byte[]>> testRecords = new ArrayList<>(2);    testRecords.add(consumerRecord1);    testRecords.add(consumerRecord2);    testDriver.pipeInput(testRecords);    assertEquals(1, processedRecords1.size());    assertEquals(1, processedRecords2.size());    Record record = processedRecords1.get(0);    Record expectedResult = new Record(consumerRecord1, 0L);    assertThat(record, equalTo(expectedResult));    record = processedRecords2.get(0);    expectedResult = new Record(consumerRecord2, 0L);    assertThat(record, equalTo(expectedResult));}
public void kafkatest_f19806_0()
{    testDriver = new TopologyTestDriver(setupTopologyWithTwoSubtopologies(), config);    testDriver.pipeInput(consumerRecord1);    ProducerRecord outputRecord = testDriver.readOutput(SINK_TOPIC_1);    assertEquals(key1, outputRecord.key());    assertEquals(value1, outputRecord.value());    assertEquals(SINK_TOPIC_1, outputRecord.topic());    outputRecord = testDriver.readOutput(SINK_TOPIC_2);    assertEquals(key1, outputRecord.key());    assertEquals(value1, outputRecord.value());    assertEquals(SINK_TOPIC_2, outputRecord.topic());}
public void kafkatest_f19815_0()
{    shouldThrowIfBuiltInStoreIsAccessedWithUntypedMethod(true);}
private void kafkatest_f19816_0(final boolean persistent)
{    final String keyValueStoreName = "keyValueStore";    final String timestampedKeyValueStoreName = "keyValueTimestampStore";    final String windowStoreName = "windowStore";    final String timestampedWindowStoreName = "windowTimestampStore";    final String sessionStoreName = "sessionStore";    final String globalKeyValueStoreName = "globalKeyValueStore";    final String globalTimestampedKeyValueStoreName = "globalKeyValueTimestampStore";    final Topology topology = setupSingleProcessorTopology();    addStoresToTopology(topology, persistent, keyValueStoreName, timestampedKeyValueStoreName, windowStoreName, timestampedWindowStoreName, sessionStoreName, globalKeyValueStoreName, globalTimestampedKeyValueStoreName);    testDriver = new TopologyTestDriver(topology, config);    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(keyValueStoreName));        assertThat(e.getMessage(), equalTo("Store " + keyValueStoreName + " is a key-value store and should be accessed via `getKeyValueStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(timestampedKeyValueStoreName));        assertThat(e.getMessage(), equalTo("Store " + timestampedKeyValueStoreName + " is a timestamped key-value store and should be accessed via `getTimestampedKeyValueStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(windowStoreName));        assertThat(e.getMessage(), equalTo("Store " + windowStoreName + " is a window store and should be accessed via `getWindowStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(timestampedWindowStoreName));        assertThat(e.getMessage(), equalTo("Store " + timestampedWindowStoreName + " is a timestamped window store and should be accessed via `getTimestampedWindowStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(sessionStoreName));        assertThat(e.getMessage(), equalTo("Store " + sessionStoreName + " is a session store and should be accessed via `getSessionStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(globalKeyValueStoreName));        assertThat(e.getMessage(), equalTo("Store " + globalKeyValueStoreName + " is a key-value store and should be accessed via `getKeyValueStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(globalTimestampedKeyValueStoreName));        assertThat(e.getMessage(), equalTo("Store " + globalTimestampedKeyValueStoreName + " is a timestamped key-value store and should be accessed via `getTimestampedKeyValueStore()`"));    }}
public void kafkatest_f19825_0()
{    setup();    testDriver.pipeInput(recordFactory.create("input-topic", "a", 1L, 9999L));    OutputVerifier.compareKeyValue(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer), "a", 21L);    testDriver.pipeInput(recordFactory.create("input-topic", "a", 1L, 9999L));    assertNull(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer));    testDriver.pipeInput(recordFactory.create("input-topic", "a", 1L, 10000L));    OutputVerifier.compareKeyValue(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer), "a", 21L);    assertNull(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer));}
public void kafkatest_f19826_0()
{    setup();    testDriver.advanceWallClockTime(60000);    OutputVerifier.compareKeyValue(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer), "a", 21L);    assertNull(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer));}
public void kafkatest_f19836_0(final String key, final Long value)
{    store.put(key, value);}
public void kafkatest_f19838_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.globalTable("topic", Consumed.with(Serdes.String(), Serdes.String()), Materialized.as("globalStore"));    try (final TopologyTestDriver testDriver = new TopologyTestDriver(builder.build(), config)) {        final KeyValueStore<String, String> globalStore = testDriver.getKeyValueStore("globalStore");        Assert.assertNotNull(globalStore);        Assert.assertNotNull(testDriver.getAllStateStores().get("globalStore"));        final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());        testDriver.pipeInput(recordFactory.create("topic", "k1", "value1"));        // we expect to have both in the global store, the one from pipeInput and the one from the producer        Assert.assertEquals("value1", globalStore.get("k1"));    }}
private static ProcessorSupplier<K, V> kafkatest_f19847_0()
{    return new ProcessorSupplier<K, V>() {        public Processor<K, V> get() {            return new AbstractProcessor<K, V>() {                private int numRecordsProcessed = 0;                @Override                public void init(final ProcessorContext context) {                    System.out.println("[0.10.0] initializing processor: topic=data taskId=" + context.taskId());                    numRecordsProcessed = 0;                }                @Override                public void process(final K key, final V value) {                    numRecordsProcessed++;                    if (numRecordsProcessed % 100 == 0) {                        System.out.println("processed " + numRecordsProcessed + " records from topic=data");                    }                }                @Override                public void punctuate(final long timestamp) {                }                @Override                public void close() {                }            };        }    };}
public Processor<K, V> kafkatest_f19848_0()
{    return new AbstractProcessor<K, V>() {        private int numRecordsProcessed = 0;        @Override        public void init(final ProcessorContext context) {            System.out.println("[0.10.0] initializing processor: topic=data taskId=" + context.taskId());            numRecordsProcessed = 0;        }        @Override        public void process(final K key, final V value) {            numRecordsProcessed++;            if (numRecordsProcessed % 100 == 0) {                System.out.println("processed " + numRecordsProcessed + " records from topic=data");            }        }        @Override        public void punctuate(final long timestamp) {        }        @Override        public void close() {        }    };}
public static void kafkatest_f19861_0(final String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only " + args.length + " provided: " + (args.length > 0 ? args[0] : ""));    }    final String kafka = args[0];    final String propFileName = args.length > 1 ? args[1] : null;    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started (StreamsUpgradeTest v0.10.2)");    System.out.println("kafka=" + kafka);    System.out.println("props=" + streamsProperties);    final KStreamBuilder builder = new KStreamBuilder();    final KStream dataStream = builder.stream("data");    dataStream.process(printProcessorSupplier());    dataStream.to("echo");    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsUpgradeTest");    config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder, config);    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread() {        @Override        public void run() {            streams.close();            System.out.println("UPGRADE-TEST-CLIENT-CLOSED");            System.out.flush();        }    });}
public void kafkatest_f19862_0()
{    streams.close();    System.out.println("UPGRADE-TEST-CLIENT-CLOSED");    System.out.flush();}
public void kafkatest_f19873_0(final ProcessorContext context)
{    System.out.println("[0.11.0] initializing processor: topic=data taskId=" + context.taskId());    numRecordsProcessed = 0;}
public void kafkatest_f19874_0(final K key, final V value)
{    numRecordsProcessed++;    if (numRecordsProcessed % 100 == 0) {        System.out.println("processed " + numRecordsProcessed + " records from topic=data");    }}
private static ProcessorSupplier<K, V> kafkatest_f19887_0()
{    return new ProcessorSupplier<K, V>() {        public Processor<K, V> get() {            return new AbstractProcessor<K, V>() {                private int numRecordsProcessed = 0;                @Override                public void init(final ProcessorContext context) {                    System.out.println("[1.1] initializing processor: topic=data taskId=" + context.taskId());                    numRecordsProcessed = 0;                }                @Override                public void process(final K key, final V value) {                    numRecordsProcessed++;                    if (numRecordsProcessed % 100 == 0) {                        System.out.println("processed " + numRecordsProcessed + " records from topic=data");                    }                }                @Override                public void punctuate(final long timestamp) {                }                @Override                public void close() {                }            };        }    };}
public Processor<K, V> kafkatest_f19888_0()
{    return new AbstractProcessor<K, V>() {        private int numRecordsProcessed = 0;        @Override        public void init(final ProcessorContext context) {            System.out.println("[1.1] initializing processor: topic=data taskId=" + context.taskId());            numRecordsProcessed = 0;        }        @Override        public void process(final K key, final V value) {            numRecordsProcessed++;            if (numRecordsProcessed % 100 == 0) {                System.out.println("processed " + numRecordsProcessed + " records from topic=data");            }        }        @Override        public void punctuate(final long timestamp) {        }        @Override        public void close() {        }    };}
public void kafkatest_f19900_0(final ProcessorContext context)
{    System.out.println("[2.1] initializing processor: topic=data taskId=" + context.taskId());    numRecordsProcessed = 0;}
public void kafkatest_f19901_0(final K key, final V value)
{    numRecordsProcessed++;    if (numRecordsProcessed % 100 == 0) {        System.out.println("processed " + numRecordsProcessed + " records from topic=data");    }}
public static void kafkatest_f19913_0(String[] args) throws Exception
{    ArgumentParser parser = ArgumentParsers.newArgumentParser("client-compatibility-test").defaultHelp(true).description("This tool is used to verify client compatibility guarantees.");    parser.addArgument("--topic").action(store()).required(true).type(String.class).dest("topic").metavar("TOPIC").help("the compatibility test will produce messages to this topic");    parser.addArgument("--bootstrap-server").action(store()).required(true).type(String.class).dest("bootstrapServer").metavar("BOOTSTRAP_SERVER").help("The server(s) to use for bootstrapping");    parser.addArgument("--offsets-for-times-supported").action(store()).required(true).type(Boolean.class).dest("offsetsForTimesSupported").metavar("OFFSETS_FOR_TIMES_SUPPORTED").help("True if KafkaConsumer#offsetsForTimes is supported by the current broker version");    parser.addArgument("--cluster-id-supported").action(store()).required(true).type(Boolean.class).dest("clusterIdSupported").metavar("CLUSTER_ID_SUPPORTED").help("True if cluster IDs are supported.  False if cluster ID always appears as null.");    parser.addArgument("--expect-record-too-large-exception").action(store()).required(true).type(Boolean.class).dest("expectRecordTooLargeException").metavar("EXPECT_RECORD_TOO_LARGE_EXCEPTION").help("True if we should expect a RecordTooLargeException when trying to read from a topic " + "that contains a message that is bigger than " + ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG + ".  This is pre-KIP-74 behavior.");    parser.addArgument("--num-cluster-nodes").action(store()).required(true).type(Integer.class).dest("numClusterNodes").metavar("NUM_CLUSTER_NODES").help("The number of cluster nodes we should expect to see from the AdminClient.");    parser.addArgument("--create-topics-supported").action(store()).required(true).type(Boolean.class).dest("createTopicsSupported").metavar("CREATE_TOPICS_SUPPORTED").help("Whether we should be able to create topics via the AdminClient.");    parser.addArgument("--describe-acls-supported").action(store()).required(true).type(Boolean.class).dest("describeAclsSupported").metavar("DESCRIBE_ACLS_SUPPORTED").help("Whether describeAcls is supported in the AdminClient.");    Namespace res = null;    try {        res = parser.parseArgs(args);    } catch (ArgumentParserException e) {        if (args.length == 0) {            parser.printHelp();            Exit.exit(0);        } else {            parser.handleError(e);            Exit.exit(1);        }    }    TestConfig testConfig = new TestConfig(res);    ClientCompatibilityTest test = new ClientCompatibilityTest(testConfig);    try {        test.run();    } catch (Throwable t) {        System.out.printf("FAILED: Caught exception %s%n%n", t.getMessage());        t.printStackTrace();        Exit.exit(1);    }    System.out.println("SUCCESS.");    Exit.exit(0);}
private static String kafkatest_f19914_0(byte[] buf)
{    StringBuilder bld = new StringBuilder();    for (byte b : buf) {        bld.append(String.format("%02x", b));    }    return bld.toString();}
public void kafkatest_f19923_0(ClusterResource clusterResource)
{    if (expectClusterId) {        if (clusterResource.clusterId() == null) {            throw new RuntimeException("Expected cluster id to be supported, but it was null.");        }    } else {        if (clusterResource.clusterId() != null) {            throw new RuntimeException("Expected cluster id to be null, but it was supported.");        }    }}
public voidf19924_1final long prodTimeMs) throws Throwable
{    Properties consumerProps = new Properties();    consumerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, testConfig.bootstrapServer);    consumerProps.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 512);    ClientCompatibilityTestDeserializer deserializer = new ClientCompatibilityTestDeserializer(testConfig.expectClusterId);    try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps, deserializer, deserializer)) {        final List<PartitionInfo> partitionInfos = consumer.partitionsFor(testConfig.topic);        if (partitionInfos.size() < 1)            throw new RuntimeException("Expected at least one partition for topic " + testConfig.topic);        final Map<TopicPartition, Long> timestampsToSearch = new HashMap<>();        final LinkedList<TopicPartition> topicPartitions = new LinkedList<>();        for (PartitionInfo partitionInfo : partitionInfos) {            TopicPartition topicPartition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());            timestampsToSearch.put(topicPartition, prodTimeMs);            topicPartitions.add(topicPartition);        }        final OffsetsForTime offsetsForTime = new OffsetsForTime();        tryFeature("offsetsForTimes", testConfig.offsetsForTimesSupported, () -> offsetsForTime.result = consumer.offsetsForTimes(timestampsToSearch), () ->         // Whether or not offsetsForTimes works, beginningOffsets and endOffsets        // should work.        consumer.beginningOffsets(timestampsToSearch.keySet());        consumer.endOffsets(timestampsToSearch.keySet());        consumer.assign(topicPartitions);        consumer.seekToBeginning(topicPartitions);        final Iterator<byte[]> iter = new Iterator<byte[]>() {            private static final int TIMEOUT_MS = 10000;            private Iterator<ConsumerRecord<byte[], byte[]>> recordIter = null;            private byte[] next = null;            private byte[] fetchNext() {                while (true) {                    long curTime = Time.SYSTEM.milliseconds();                    if (curTime - prodTimeMs > TIMEOUT_MS)                        throw new RuntimeException("Timed out after " + TIMEOUT_MS + " ms.");                    if (recordIter == null) {                        ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));                        recordIter = records.iterator();                    }                    if (recordIter.hasNext())                        return recordIter.next().value();                    recordIter = null;                }            }            @Override            public boolean hasNext() {                if (next != null)                    return true;                next = fetchNext();                return next != null;            }            @Override            public byte[] next() {                if (!hasNext())                    throw new NoSuchElementException();                byte[] cur = next;                next = null;                return cur;            }            @Override            public void remove() {                throw new UnsupportedOperationException();            }        };        byte[] next = iter.next();        try {            compareArrays(message1, next);                    } catch (RuntimeException e) {            throw new RuntimeException("The first message in this topic was not ours. Please use a new topic when " + "running this program.");        }        try {            next = iter.next();            if (testConfig.expectRecordTooLargeException) {                throw new RuntimeException("Expected to get a RecordTooLargeException when reading a record " + "bigger than " + ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG);            }            try {                compareArrays(message2, next);            } catch (RuntimeException e) {                System.out.println("The second message in this topic was not ours. Please use a new " + "topic when running this program.");                Exit.exit(1);            }        } catch (RecordTooLargeException e) {                        if (!testConfig.expectRecordTooLargeException)                throw new RuntimeException("Got an unexpected RecordTooLargeException when reading a record " + "bigger than " + ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG);        }            }    }
public void kafkatest_f19933_0(int iter, int latency, int bytes, long time)
{    this.count++;    this.bytes += bytes;    this.totalLatency += latency;    this.maxLatency = Math.max(this.maxLatency, latency);    this.windowCount++;    this.windowBytes += bytes;    this.windowTotalLatency += latency;    this.windowMaxLatency = Math.max(windowMaxLatency, latency);    if (iter % this.sampling == 0) {        this.latencies[index] = latency;        this.index++;    }    /* maybe report the recent perf */    if (time - windowStart >= reportingInterval) {        printWindow();        newWindow();    }}
public Callback kafkatest_f19934_0(long start, int bytes, Stats stats)
{    Callback cb = new PerfCallback(this.iteration, start, bytes, stats);    this.iteration++;    return cb;}
public voidf19943_1KafkaMetric metric)
{    synchronized (lock) {                metrics.remove(metric.metricName());    }}
public void kafkatest_f19944_0()
{    executor.shutdown();    try {        executor.awaitTermination(30, TimeUnit.SECONDS);    } catch (InterruptedException e) {        throw new KafkaException("Interrupted when shutting down PushHttpMetricsReporter", e);    }}
public String kafkatest_f19953_0()
{    return name;}
public String kafkatest_f19954_0()
{    return group;}
private static KafkaProducer<String, String> kafkatest_f19963_0(Namespace parsedArgs)
{    String transactionalId = parsedArgs.getString("transactionalId");    String brokerList = parsedArgs.getString("brokerList");    Properties props = new Properties();    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);    props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");    // We set a small batch size to ensure that we have multiple inflight requests per transaction.    // If it is left at the default, each transaction will have only one batch per partition, hence not testing    // the case with multiple inflights.    props.put(ProducerConfig.BATCH_SIZE_CONFIG, "512");    props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "5");    return new KafkaProducer<>(props);}
private static KafkaConsumer<String, String> kafkatest_f19964_0(Namespace parsedArgs)
{    String consumerGroup = parsedArgs.getString("consumerGroup");    String brokerList = parsedArgs.getString("brokerList");    Integer numMessagesPerTransaction = parsedArgs.getInt("messagesPerTransaction");    Properties props = new Properties();    props.put(ConsumerConfig.GROUP_ID_CONFIG, consumerGroup);    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");    props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, numMessagesPerTransaction.toString());    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");    props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "10000");    props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, "3000");    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");    return new KafkaConsumer<>(props);}
private void kafkatest_f19973_0()
{    SimpleModule kafka = new SimpleModule();    kafka.addSerializer(TopicPartition.class, new JsonSerializer<TopicPartition>() {        @Override        public void serialize(TopicPartition tp, JsonGenerator gen, SerializerProvider serializers) throws IOException {            gen.writeStartObject();            gen.writeObjectField("topic", tp.topic());            gen.writeObjectField("partition", tp.partition());            gen.writeEndObject();        }    });    mapper.registerModule(kafka);}
public void kafkatest_f19974_0(TopicPartition tp, JsonGenerator gen, SerializerProvider serializers) throws IOException
{    gen.writeStartObject();    gen.writeObjectField("topic", tp.topic());    gen.writeObjectField("partition", tp.partition());    gen.writeEndObject();}
public voidf19983_1)
{    try {        printJson(new StartupComplete());        consumer.subscribe(Collections.singletonList(topic), this);        while (!isFinished()) {            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(Long.MAX_VALUE));            Map<TopicPartition, OffsetAndMetadata> offsets = onRecordsReceived(records);            if (!useAutoCommit) {                if (useAsyncCommit)                    consumer.commitAsync(offsets, this);                else                    commitSync(offsets);            }        }    } catch (WakeupException e) {        // ignore, we are closing        log.trace("Caught WakeupException because consumer is shutdown, ignore and terminate.", e);    } catch (Throwable t) {        // Log the error so it goes to the service log and not stdout            } finally {        consumer.close();        printJson(new ShutdownComplete());        shutdownLatch.countDown();    }}
public void kafkatest_f19984_0()
{    boolean interrupted = false;    try {        consumer.wakeup();        while (true) {            try {                shutdownLatch.await();                return;            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted)            Thread.currentThread().interrupt();    }}
public long kafkatest_f19993_0()
{    return count;}
public List<RecordSetSummary> kafkatest_f19994_0()
{    return partitionSummaries;}
public String kafkatest_f20003_0()
{    return "offsets_committed";}
public List<CommitData> kafkatest_f20004_0()
{    return offsets;}
public static void kafkatest_f20013_0(String[] args)
{    ArgumentParser parser = argParser();    if (args.length == 0) {        parser.printHelp();        Exit.exit(0);    }    try {        final VerifiableConsumer consumer = createFromArgs(parser, args);        Runtime.getRuntime().addShutdownHook(new Thread(() -> consumer.close()));        consumer.run();    } catch (ArgumentParserException e) {        parser.handleError(e);        Exit.exit(1);    }}
private static ArgumentParser kafkatest_f20014_0()
{    ArgumentParser parser = ArgumentParsers.newArgumentParser("verifiable-log4j-appender").defaultHelp(true).description("This tool produces increasing integers to the specified topic using KafkaLog4jAppender.");    parser.addArgument("--topic").action(store()).required(true).type(String.class).metavar("TOPIC").help("Produce messages to this topic.");    parser.addArgument("--broker-list").action(store()).required(true).type(String.class).metavar("HOST1:PORT1[,HOST2:PORT2[...]]").dest("brokerList").help("Comma-separated list of Kafka brokers in the form HOST1:PORT1,HOST2:PORT2,...");    parser.addArgument("--max-messages").action(store()).required(false).setDefault(-1).type(Integer.class).metavar("MAX-MESSAGES").dest("maxMessages").help("Produce this many messages. If -1, produce messages until the process is killed externally.");    parser.addArgument("--acks").action(store()).required(false).setDefault("-1").type(String.class).choices("0", "1", "-1").metavar("ACKS").help("Acks required on each produced message. See Kafka docs on request.required.acks for details.");    parser.addArgument("--security-protocol").action(store()).required(false).setDefault("PLAINTEXT").type(String.class).choices("PLAINTEXT", "SSL", "SASL_PLAINTEXT", "SASL_SSL").metavar("SECURITY-PROTOCOL").dest("securityProtocol").help("Security protocol to be used while communicating with Kafka brokers.");    parser.addArgument("--ssl-truststore-location").action(store()).required(false).type(String.class).metavar("SSL-TRUSTSTORE-LOCATION").dest("sslTruststoreLocation").help("Location of SSL truststore to use.");    parser.addArgument("--ssl-truststore-password").action(store()).required(false).type(String.class).metavar("SSL-TRUSTSTORE-PASSWORD").dest("sslTruststorePassword").help("Password for SSL truststore to use.");    parser.addArgument("--appender.config").action(store()).required(false).type(String.class).metavar("CONFIG_FILE").help("Log4jAppender config properties file.");    parser.addArgument("--sasl-kerberos-service-name").action(store()).required(false).type(String.class).metavar("SASL-KERBEROS-SERVICE-NAME").dest("saslKerberosServiceName").help("Name of sasl kerberos service.");    parser.addArgument("--client-jaas-conf-path").action(store()).required(false).type(String.class).metavar("CLIENT-JAAS-CONF-PATH").dest("clientJaasConfPath").help("Path of JAAS config file of Kafka client.");    parser.addArgument("--kerb5-conf-path").action(store()).required(false).type(String.class).metavar("KERB5-CONF-PATH").dest("kerb5ConfPath").help("Path of Kerb5 config file.");    return parser;}
public String kafkatest_f20023_0(long val)
{    if (this.valuePrefix != null) {        return String.format("%d.%d", this.valuePrefix, val);    }    return String.format("%d", val);}
public String kafkatest_f20024_0()
{    String key = null;    if (repeatingKeys != null) {        key = Integer.toString(keyCounter++);        if (keyCounter == repeatingKeys) {            keyCounter = 0;        }    }    return key;}
public int kafkatest_f20033_0()
{    return recordMetadata.partition();}
public long kafkatest_f20034_0()
{    return recordMetadata.offset();}
public long kafkatest_f20043_0()
{    return this.acked;}
public long kafkatest_f20044_0()
{    return this.targetThroughput;}
public AgentStatusResponse kafkatest_f20053_0() throws Exception
{    return new AgentStatusResponse(serverStartMs, workerManager.workerStates());}
public UptimeResponse kafkatest_f20054_0()
{    return new UptimeResponse(serverStartMs, time.milliseconds());}
public Builder kafkatest_f20063_0(String target)
{    this.target = target;    return this;}
public Builder kafkatest_f20064_0(String host, int port)
{    this.target = String.format("%s:%d", host, port);    return this;}
public void kafkatest_f20073_0(DestroyWorkerRequest request) throws Exception
{    UriBuilder uriBuilder = UriBuilder.fromPath(url("/agent/worker"));    uriBuilder.queryParam("workerId", request.workerId());    HttpResponse<Empty> resp = JsonRestServer.<Empty>httpRequest(uriBuilder.build().toString(), "DELETE", null, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
public void kafkatest_f20074_0() throws Exception
{    HttpResponse<Empty> resp = JsonRestServer.<Empty>httpRequest(url("/agent/shutdown"), "PUT", null, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
public Empty kafkatest_f20083_0(StopWorkerRequest req) throws Throwable
{    agent().stopWorker(req);    return Empty.INSTANCE;}
public Empty kafkatest_f20084_0(@DefaultValue("0") kafkatest_f20084_0("workerId") long workerId) throws Throwable
{    agent().destroyWorker(new DestroyWorkerRequest(workerId));    return Empty.INSTANCE;}
 TaskSpec kafkatest_f20093_0()
{    return spec;}
 WorkerState kafkatest_f20094_0()
{    switch(state) {        case STARTING:            return new WorkerStarting(taskId, spec);        case RUNNING:            return new WorkerRunning(taskId, spec, startedMs, status.get());        case CANCELLING:        case STOPPING:            return new WorkerStopping(taskId, spec, startedMs, status.get());        case DONE:            return new WorkerDone(taskId, spec, startedMs, doneMs, status.get(), error);    }    throw new RuntimeException("unreachable");}
public Voidf20103_1) throws Exception
{    if (worker.error.isEmpty() && !failure.isEmpty()) {        worker.error = failure;    }    worker.transitionToDone();    if (worker.mustDestroy) {                workers.remove(worker.workerId);    } else {            }    return null;}
public void kafkatest_f20104_0(long workerId, boolean mustDestroy) throws Throwable
{    try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {        stateChangeExecutor.submit(new StopWorker(workerId, mustDestroy)).get();    } catch (ExecutionException e) {        throw e.getCause();    }}
public String kafkatest_f20113_0()
{    return name;}
public String kafkatest_f20114_0()
{    return hostname;}
public Scheduler kafkatest_f20123_0()
{    return scheduler;}
public String kafkatest_f20124_0(String[] command) throws IOException
{    return commandRunner.run(curNode, command);}
public static int kafkatest_f20133_0(Node node)
{    return getIntConfig(node, Platform.Config.TROGDOR_COORDINATOR_PORT, Coordinator.DEFAULT_PORT);}
public static Platform kafkatest_f20134_0(String curNodeName, String path) throws Exception
{    JsonNode root = JsonUtil.JSON_SERDE.readTree(new File(path));    JsonNode platformNode = root.get("platform");    if (platformNode == null) {        throw new RuntimeException("Expected to find a 'platform' field " + "in the root JSON configuration object");    }    String platformName = platformNode.textValue();    return Utils.newParameterizedInstance(platformName, String.class, curNodeName, Scheduler.class, Scheduler.SYSTEM, JsonNode.class, root);}
public static int kafkatest_f20143_0(float perSec, long periodMs)
{    float period = ((float) periodMs) / 1000.0f;    float perPeriod = perSec * period;    perPeriod = Math.max(1.0f, perPeriod);    return (int) perPeriod;}
public static void kafkatest_f20144_0(Properties props, Map<String, String> commonConf, Map<String, String> clientConf)
{    for (Map.Entry<String, String> commonEntry : commonConf.entrySet()) {        props.setProperty(commonEntry.getKey(), commonEntry.getValue());    }    for (Map.Entry<String, String> entry : clientConf.entrySet()) {        props.setProperty(entry.getKey(), entry.getValue());    }}
public CoordinatorStatusResponse kafkatest_f20153_0() throws Exception
{    return new CoordinatorStatusResponse(startTimeMs);}
public UptimeResponse kafkatest_f20154_0()
{    return new UptimeResponse(startTimeMs, time.milliseconds());}
public Builder kafkatest_f20163_0(Logger log)
{    this.log = log;    return this;}
public Builder kafkatest_f20164_0(int maxTries)
{    this.maxTries = maxTries;    return this;}
public void kafkatest_f20173_0(StopTaskRequest request) throws Exception
{    HttpResponse<Empty> resp = JsonRestServer.httpRequest(log, url("/coordinator/task/stop"), "PUT", request, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
public void kafkatest_f20174_0(DestroyTaskRequest request) throws Exception
{    UriBuilder uriBuilder = UriBuilder.fromPath(url("/coordinator/tasks"));    uriBuilder.queryParam("taskId", request.id());    HttpResponse<Empty> resp = JsonRestServer.httpRequest(log, uriBuilder.build().toString(), "DELETE", null, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
public void kafkatest_f20183_0(Coordinator myCoordinator)
{    coordinator.set(myCoordinator);}
public CoordinatorStatusResponse kafkatest_f20184_0() throws Throwable
{    return coordinator().status();}
 voidf20193_1)
{    try {        client.createWorker(new CreateWorkerRequest(workerId, taskId, spec));    } catch (Throwable e) {            }}
 voidf20194_1)
{    try {        client.stopWorker(new StopWorkerRequest(workerId));    } catch (Throwable e) {            }}
public Voidf20203_1) throws Exception
{    ManagedWorker worker = workers.get(workerId);    if (worker == null) {                return null;    }    if (!worker.shouldRun) {                return null;    }        worker.shouldRun = false;    rescheduleNextHeartbeat(0);    return null;}
public void kafkatest_f20204_0(long workerId)
{    executor.submit(new DestroyWorker(workerId));}
private JsonNode kafkatest_f20213_0()
{    if (workerIds.size() == 1) {        return workerStates.get(workerIds.values().iterator().next()).status();    } else {        ObjectNode objectNode = new ObjectNode(JsonNodeFactory.instance);        for (Map.Entry<String, Long> entry : workerIds.entrySet()) {            String nodeName = entry.getKey();            Long workerId = entry.getValue();            WorkerState state = workerStates.get(workerId);            JsonNode node = state.status();            if (node != null) {                objectNode.set(nodeName, node);            }        }        return objectNode;    }}
 TreeMap<String, Long> kafkatest_f20214_0()
{    TreeMap<String, Long> activeWorkerIds = new TreeMap<>();    for (Map.Entry<String, Long> entry : workerIds.entrySet()) {        WorkerState workerState = workerStates.get(entry.getValue());        if (!workerState.done()) {            activeWorkerIds.put(entry.getKey(), entry.getValue());        }    }    return activeWorkerIds;}
public Voidf20223_1) throws Exception
{    try {        WorkerState prevState = workerStates.get(workerId);        if (prevState == null) {            throw new RuntimeException("Unable to find workerId " + workerId);        }        ManagedTask task = tasks.get(prevState.taskId());        if (task == null) {            throw new RuntimeException("Unable to find taskId " + prevState.taskId());        }                workerStates.put(workerId, nextState);        if (nextState.done() && (!prevState.done())) {            handleWorkerCompletion(task, nodeName, (WorkerDone) nextState);        }    } catch (Exception e) {                nodeManagers.get(nodeName).stopWorker(workerId);    }    return null;}
private voidf20224_1ManagedTask task, String nodeName, WorkerDone state)
{    if (state.error().isEmpty()) {            } else {                task.maybeSetError(state.error());    }    TreeMap<String, Long> activeWorkerIds = task.activeWorkerIds();    if (activeWorkerIds.isEmpty()) {        task.doneMs = time.milliseconds();        task.state = TaskStateType.DONE;            } else if ((task.state == TaskStateType.RUNNING) && (!task.error.isEmpty())) {                task.state = TaskStateType.STOPPING;        for (Map.Entry<String, Long> entry : activeWorkerIds.entrySet()) {            nodeManagers.get(entry.getKey()).stopWorker(entry.getValue());        }    }}
public int kafkatest_f20233_0()
{    return latencyMs;}
public TaskController kafkatest_f20234_0(String id)
{    return topology -> nodeSpecs.keySet();}
public String kafkatest_f20243_0()
{    return mountPath;}
public String kafkatest_f20244_0()
{    return prefix;}
 synchronized void kafkatest_f20253_0(KiboshFaultSpec toAdd) throws IOException
{    KiboshControlFile file = KiboshControlFile.read(controlPath);    List<KiboshFaultSpec> faults = new ArrayList<>(file.faults());    faults.add(toAdd);    new KiboshControlFile(faults).write(controlPath);}
 synchronized void kafkatest_f20254_0(KiboshFaultSpec toRemove) throws IOException
{    KiboshControlFile file = KiboshControlFile.read(controlPath);    List<KiboshFaultSpec> faults = new ArrayList<>();    boolean foundToRemove = false;    for (KiboshFaultSpec fault : file.faults()) {        if (fault.equals(toRemove)) {            foundToRemove = true;        } else {            faults.add(fault);        }    }    if (!foundToRemove) {        throw new RuntimeException("Failed to find fault " + toRemove + ". ");    }    new KiboshControlFile(faults).write(controlPath);}
 void kafkatest_f20263_0(String mountPath, KiboshFaultSpec spec) throws IOException
{    KiboshProcess process = findProcessObject(mountPath);    process.removeFault(spec);}
public Set<String> kafkatest_f20264_0(Topology topology)
{    return nodeNames;}
public voidf20273_1Platform platform) throws Exception
{        this.status.update(new TextNode("removing network partition " + id));    runIptablesCommands(platform, "-D");    this.status.update(new TextNode("removed network partition " + id));}
private void kafkatest_f20274_0(Platform platform, String iptablesAction) throws Exception
{    Node curNode = platform.curNode();    Topology topology = platform.topology();    TreeSet<String> toBlock = new TreeSet<>();    for (Set<String> partitionSet : partitionSets) {        if (!partitionSet.contains(curNode.name())) {            for (String nodeName : partitionSet) {                toBlock.add(nodeName);            }        }    }    for (String nodeName : toBlock) {        Node node = topology.node(nodeName);        InetAddress addr = InetAddress.getByName(node.hostname());        platform.runCommand(new String[] { "sudo", "iptables", iptablesAction, "INPUT", "-p", "tcp", "-s", addr.getHostAddress(), "-j", "DROP", "-m", "comment", "--comment", nodeName });    }}
public long kafkatest_f20283_0()
{    return serverStartMs;}
public TreeMap<Long, WorkerState> kafkatest_f20284_0()
{    return workers;}
public long kafkatest_f20293_0()
{    return workerId;}
public boolean kafkatest_f20294_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    return true;}
public int kafkatest_f20303_0()
{    return connector.getLocalPort();}
public voidf20304_1)
{    if (!shutdownExecutor.isShutdown()) {        shutdownExecutor.submit((Callable<Void>) () -> {            try {                                jettyServer.stop();                jettyServer.join();                            } catch (Exception e) {                            } finally {                jettyServer.destroy();            }            shutdownExecutor.shutdown();            return null;        });    }}
public final String kafkatest_f20313_0()
{    return JsonUtil.toJsonString(this);}
public Responsef20314_1Throwable e)
{    if (log.isDebugEnabled()) {            } else if (log.isInfoEnabled()) {            }    if (e instanceof NotFoundException) {        return buildResponse(Response.Status.NOT_FOUND, e);    } else if (e instanceof InvalidRequestException) {        return buildResponse(Response.Status.BAD_REQUEST, e);    } else if (e instanceof InvalidTypeIdException) {        return buildResponse(Response.Status.NOT_IMPLEMENTED, e);    } else if (e instanceof JsonMappingException) {        return buildResponse(Response.Status.BAD_REQUEST, e);    } else if (e instanceof ClassNotFoundException) {        return buildResponse(Response.Status.NOT_IMPLEMENTED, e);    } else if (e instanceof SerializationException) {        return buildResponse(Response.Status.BAD_REQUEST, e);    } else if (e instanceof RequestConflictException) {        return buildResponse(Response.Status.CONFLICT, e);    } else {        return buildResponse(Response.Status.INTERNAL_SERVER_ERROR, e);    }}
public TaskStateType kafkatest_f20323_0()
{    return TaskStateType.DONE;}
public TaskStateType kafkatest_f20324_0()
{    return TaskStateType.PENDING;}
public Optional<TaskStateType> kafkatest_f20333_0()
{    return state;}
public boolean kafkatest_f20334_0(String taskId, long startMs, long endMs, TaskStateType state)
{    if ((!taskIds.isEmpty()) && (!taskIds.contains(taskId))) {        return false;    }    if ((firstStartMs > 0) && (startMs < firstStartMs)) {        return false;    }    if ((lastStartMs > 0) && ((startMs < 0) || (startMs > lastStartMs))) {        return false;    }    if ((firstEndMs > 0) && (endMs < firstEndMs)) {        return false;    }    if ((lastEndMs > 0) && ((endMs < 0) || (endMs > lastEndMs))) {        return false;    }    if (this.state.isPresent() && !this.state.get().equals(state)) {        return false;    }    return true;}
public long kafkatest_f20343_0()
{    return doneMs;}
public JsonNode kafkatest_f20344_0()
{    return status;}
public TaskSpec kafkatest_f20353_0()
{    return spec;}
public boolean kafkatest_f20354_0()
{    return false;}
public synchronized JsonNode kafkatest_f20363_0()
{    return status;}
public Set<String> kafkatest_f20364_0(Topology topology)
{    return Topology.Util.agentNodeNames(topology);}
public final int kafkatest_f20373_0()
{    return Objects.hashCode(toString());}
public String kafkatest_f20374_0()
{    return JsonUtil.toJsonString(this);}
public TaskWorker kafkatest_f20383_0(String id)
{    return new ConnectionStressWorker(id, this);}
public voidf20384_1Platform platform, WorkerStatusTracker status, KafkaFutureImpl<String> doneFuture) throws Exception
{    if (!running.compareAndSet(false, true)) {        throw new IllegalStateException("ConnectionStressWorker is already running.");    }        this.doneFuture = doneFuture;    this.status = status;    synchronized (ConnectionStressWorker.this) {        this.totalConnections = 0;        this.totalFailedConnections = 0;        this.startTimeMs = TIME.milliseconds();    }    this.statusUpdaterExecutor = Executors.newScheduledThreadPool(1, ThreadUtils.createThreadFactory("StatusUpdaterWorkerThread%d", false));    this.statusUpdaterFuture = this.statusUpdaterExecutor.scheduleAtFixedRate(new StatusUpdater(), 0, REPORT_INTERVAL_MS, TimeUnit.MILLISECONDS);    this.workerExecutor = Executors.newFixedThreadPool(spec.numThreads(), ThreadUtils.createThreadFactory("ConnectionStressWorkerThread%d", false));    for (int i = 0; i < spec.numThreads(); i++) {        this.workerExecutor.submit(new ConnectLoop());    }}
public double kafkatest_f20394_0()
{    return connectsPerSec;}
public voidf20395_1Platform platform) throws Exception
{    if (!running.compareAndSet(true, false)) {        throw new IllegalStateException("ConnectionStressWorker is not running.");    }        // Shut down the periodic status updater and perform a final update on the    // statistics.  We want to do this first, before deactivating any threads.    // Otherwise, if some threads take a while to terminate, this could lead    // to a misleading rate getting reported.    this.statusUpdaterFuture.cancel(false);    this.statusUpdaterExecutor.shutdown();    this.statusUpdaterExecutor.awaitTermination(1, TimeUnit.DAYS);    this.statusUpdaterExecutor = null;    new StatusUpdater().run();    doneFuture.complete("");    workerExecutor.shutdownNow();    workerExecutor.awaitTermination(1, TimeUnit.DAYS);    this.workerExecutor = null;    this.status = null;}
public int kafkatest_f20404_0()
{    return threadsPerWorker;}
public Map<String, String> kafkatest_f20405_0()
{    return consumerConf;}
public void kafkatest_f20414_0()
{    try {        List<Future<Void>> consumeTasks = new ArrayList<>();        for (ConsumeMessages task : consumeTasks()) {            consumeTasks.add(executor.submit(task));        }        executor.submit(new CloseStatusUpdater(consumeTasks));    } catch (Throwable e) {        WorkerUtils.abort(log, "Prepare", e, doneFuture);    }}
private List<ConsumeMessages> kafkatest_f20415_0()
{    List<ConsumeMessages> tasks = new ArrayList<>();    String consumerGroup = consumerGroup();    int consumerCount = spec.threadsPerWorker();    Map<String, List<TopicPartition>> partitionsByTopic = spec.materializeTopics();    boolean toUseGroupPartitionAssignment = partitionsByTopic.values().stream().allMatch(List::isEmpty);    if (!toUseGroupPartitionAssignment && !toUseRandomConsumeGroup() && consumerCount > 1)        throw new ConfigException("You may not specify an explicit partition assignment when using multiple consumers in the same group." + "Please leave the consumer group unset, specify topics instead of partitions or use a single consumer.");    consumer = consumer(consumerGroup, clientId(0));    if (toUseGroupPartitionAssignment) {        Set<String> topics = partitionsByTopic.keySet();        tasks.add(new ConsumeMessages(consumer, topics));        for (int i = 0; i < consumerCount - 1; i++) {            tasks.add(new ConsumeMessages(consumer(consumerGroup(), clientId(i + 1)), topics));        }    } else {        List<TopicPartition> partitions = populatePartitionsByTopic(consumer.consumer(), partitionsByTopic).values().stream().flatMap(List::stream).collect(Collectors.toList());        tasks.add(new ConsumeMessages(consumer, partitions));        for (int i = 0; i < consumerCount - 1; i++) {            tasks.add(new ConsumeMessages(consumer(consumerGroup(), clientId(i + 1)), partitions));        }    }    return tasks;}
 synchronized void kafkatest_f20424_0()
{    workerStatus.update(JsonUtil.JSON_SERDE.valueToTree(statuses));}
 synchronized void kafkatest_f20425_0(String clientId, StatusData status)
{    statuses.put(clientId, JsonUtil.JSON_SERDE.valueToTree(status));}
public int kafkatest_f20434_0()
{    return p95LatencyMs;}
public int kafkatest_f20435_0()
{    return p99LatencyMs;}
public String kafkatest_f20444_0()
{    return commandNode;}
public List<String> kafkatest_f20445_0()
{    return command;}
public voidf20454_1)
{    log.trace("{}: starting stderr monitor.", id);    try (BufferedReader br = new BufferedReader(new InputStreamReader(process.getErrorStream(), StandardCharsets.UTF_8))) {        String line;        while (true) {            try {                line = br.readLine();                if (line == null) {                    throw new IOException("EOF");                }            } catch (IOException e) {                                return;            }                    }    } catch (Throwable e) {            }}
public voidf20455_1)
{    OutputStreamWriter stdinWriter = new OutputStreamWriter(process.getOutputStream(), StandardCharsets.UTF_8);    try {        while (true) {                        Optional<JsonNode> node = stdinQueue.take();            if (!node.isPresent()) {                log.trace("{}: StdinWriter terminating.", id);                return;            }            String inputString = JsonUtil.toJsonString(node.get());                        stdinWriter.write(inputString + "\n");            stdinWriter.flush();        }    } catch (IOException e) {            } catch (Throwable e) {            } finally {        try {            stdinWriter.close();        } catch (IOException e) {                    }    }}
public float kafkatest_f20464_0()
{    return fraction;}
public int kafkatest_f20465_0()
{    return value;}
public Map<String, String> kafkatest_f20474_0()
{    return configs;}
public NewTopic kafkatest_f20475_0(String topicName)
{    NewTopic newTopic;    if (partitionAssignments.isEmpty()) {        int effectiveNumPartitions = numPartitions <= 0 ? DEFAULT_NUM_PARTITIONS : numPartitions;        short effectiveReplicationFactor = replicationFactor <= 0 ? DEFAULT_REPLICATION_FACTOR : replicationFactor;        newTopic = new NewTopic(topicName, effectiveNumPartitions, effectiveReplicationFactor);    } else {        newTopic = new NewTopic(topicName, partitionAssignments);    }    if (!configs.isEmpty()) {        newTopic.configs(configs);    }    return newTopic;}
public int kafkatest_f20484_0()
{    return targetMessagesPerSec;}
public long kafkatest_f20485_0()
{    return maxMessages;}
public boolean kafkatest_f20494_0()
{    return useConfiguredPartitioner;}
public boolean kafkatest_f20495_0()
{    return skipFlush;}
private void kafkatest_f20504_0() throws InterruptedException
{    if (!partitionsIterator.hasNext())        partitionsIterator = activePartitions.iterator();    TopicPartition partition = partitionsIterator.next();    ProducerRecord<byte[], byte[]> record;    if (spec.useConfiguredPartitioner()) {        record = new ProducerRecord<>(partition.topic(), keys.next(), values.next());    } else {        record = new ProducerRecord<>(partition.topic(), partition.partition(), keys.next(), values.next());    }    sendFuture = producer.send(record, new SendRecordsCallback(this, Time.SYSTEM.milliseconds()));    throttle.increment();}
 void kafkatest_f20505_0(long durationMs)
{    histogram.add(durationMs);}
public voidf20514_1Platform platform) throws Exception
{    if (!running.compareAndSet(true, false)) {        throw new IllegalStateException("ProduceBenchWorker is not running.");    }        doneFuture.complete("");    executor.shutdownNow();    executor.awaitTermination(1, TimeUnit.DAYS);    this.executor = null;    this.status = null;    this.doneFuture = null;}
public int kafkatest_f20515_0()
{    return percent;}
 synchronized ToSendTrackerResult kafkatest_f20524_0()
{    if (failed.isEmpty()) {        if (frontier >= maxMessages) {            return null;        } else {            return new ToSendTrackerResult(frontier++, true);        }    } else {        return new ToSendTrackerResult(failed.remove(0), false);    }}
public voidf20525_1)
{    long messagesSent = 0;    long uniqueMessagesSent = 0;        try {        Iterator<TopicPartition> iter = partitions.iterator();        while (true) {            final ToSendTrackerResult result = toSendTracker.next();            if (result == null) {                break;            }            throttle.increment();            final long messageIndex = result.index;            if (result.firstSend) {                toReceiveTracker.addPending(messageIndex);                uniqueMessagesSent++;            }            messagesSent++;            if (!iter.hasNext()) {                iter = partitions.iterator();            }            TopicPartition partition = iter.next();            // we explicitly specify generator position based on message index            ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(partition.topic(), partition.partition(), KEY_GENERATOR.generate(messageIndex), spec.valueGenerator().generate(messageIndex));            producer.send(record, (metadata, exception) -> {                if (exception == null) {                    try {                        lock.lock();                        unackedSends -= 1;                        if (unackedSends <= 0)                            unackedSendsAreZero.signalAll();                    } finally {                        lock.unlock();                    }                } else {                                        toSendTracker.addFailed(messageIndex);                }            });        }    } catch (Throwable e) {        WorkerUtils.abort(log, "ProducerRunnable", e, doneFuture);    } finally {        try {            lock.lock();             uniqueMessagesSent={}; " + "ackedSends={}/{}.", id, messagesSent, uniqueMessagesSent, spec.maxMessages() - unackedSends, spec.maxMessages());        } finally {            lock.unlock();        }    }}
public long kafkatest_f20534_0()
{    return totalReceived;}
public voidf20535_1Platform platform) throws Exception
{    if (!running.compareAndSet(true, false)) {        throw new IllegalStateException("RoundTripWorker is not running.");    }        doneFuture.complete("");    executor.shutdownNow();    executor.awaitTermination(1, TimeUnit.DAYS);    Utils.closeQuietly(consumer, "consumer");    Utils.closeQuietly(producer, "producer");    this.consumer = null;    this.producer = null;    this.unackedSends = null;    this.executor = null;    this.doneFuture = null;    }
public Map<String, String> kafkatest_f20544_0()
{    return producerConf;}
public Map<String, String> kafkatest_f20545_0()
{    return consumerConf;}
public Map<String, String> kafkatest_f20554_0()
{    return consumerConf;}
public Map<String, String> kafkatest_f20555_0()
{    return adminClientConf;}
public int kafkatest_f20564_0()
{    return refreshRateMs;}
public TaskController kafkatest_f20565_0(String id)
{    return topology -> Collections.singleton(clientNode);}
public voidf20574_1)
{    try {        if (this.producer == null) {            // Housekeeping to track the number of opened connections.            SustainedConnectionWorker.this.totalProducerConnections.incrementAndGet();            // Create the producer, fetch the specified topic's partitions and randomize them.            this.producer = new KafkaProducer<>(this.props, new ByteArraySerializer(), new ByteArraySerializer());            this.partitions = this.producer.partitionsFor(this.topicName).stream().map(partitionInfo -> new TopicPartition(partitionInfo.topic(), partitionInfo.partition())).collect(Collectors.toList());            Collections.shuffle(this.partitions);        }        // Create a new iterator over the partitions if the current one doesn't exist or is exhausted.        if (this.partitionsIterator == null || !this.partitionsIterator.hasNext()) {            this.partitionsIterator = this.partitions.iterator();        }        // Produce a single record and send it synchronously.        TopicPartition partition = this.partitionsIterator.next();        ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(partition.topic(), partition.partition(), keys.next(), values.next());        producer.send(record).get();    } catch (Throwable e) {        // Set the producer to be recreated on the next cycle.        this.closeQuietly();        // Housekeeping to track the number of opened connections and failed connection attempts.        SustainedConnectionWorker.this.totalProducerConnections.decrementAndGet();        SustainedConnectionWorker.this.totalProducerFailedConnections.incrementAndGet();        SustainedConnectionWorker.    }    // Schedule this again and set to not in use.    this.completeRefresh();}
protected void kafkatest_f20575_0()
{    Utils.closeQuietly(this.producer, "KafkaProducer");    this.producer = null;    this.partitions = null;    this.partitionsIterator = null;}
public long kafkatest_f20584_0()
{    return totalConsumerFailedConnections;}
public long kafkatest_f20585_0()
{    return totalMetadataConnections;}
public int kafkatest_f20594_0()
{    return intervalMs;}
public synchronized TransactionAction kafkatest_f20595_0()
{    if (lastTransactionStartMs == NULL_START_MS) {        lastTransactionStartMs = time.milliseconds();        return TransactionAction.BEGIN_TRANSACTION;    }    if (time.milliseconds() - lastTransactionStartMs >= intervalMs) {        lastTransactionStartMs = NULL_START_MS;        return TransactionAction.COMMIT_TRANSACTION;    }    return TransactionAction.NO_OP;}
public int kafkatest_f20604_0()
{    return messagesPerTransaction;}
public synchronized TransactionAction kafkatest_f20605_0()
{    if (messagesInTransaction == -1) {        messagesInTransaction = 0;        return TransactionAction.BEGIN_TRANSACTION;    }    if (messagesInTransaction == messagesPerTransaction) {        messagesInTransaction = -1;        return TransactionAction.COMMIT_TRANSACTION;    }    messagesInTransaction += 1;    return TransactionAction.NO_OP;}
private void kafkatest_f20614_0()
{    EasyMock.expect(executor.scheduleAtFixedRate(EasyMock.capture(reportRunnable), EasyMock.eq(5L), EasyMock.eq(5L), EasyMock.eq(TimeUnit.SECONDS))).andReturn(// return value not expected to be used    null);}
private void kafkatest_f20615_0()
{    Map<String, String> config = new HashMap<>();    config.put(PushHttpMetricsReporter.METRICS_URL_CONFIG, URL.toString());    config.put(PushHttpMetricsReporter.METRICS_PERIOD_CONFIG, "5");    reporter.configure(config);}
public void kafkatest_f20624_0() throws Exception
{    Agent agent = createAgent(Scheduler.SYSTEM);    agent.beginShutdown();    agent.waitForShutdown();}
public void kafkatest_f20625_0() throws Exception
{    Agent agent = createAgent(Scheduler.SYSTEM);    AgentClient client = new AgentClient.Builder().maxTries(10).target("localhost", agent.port()).build();    client.invokeShutdown();    agent.waitForShutdown();}
public void kafkatest_f20634_0() throws Exception
{    Utils.delete(tempDir);}
public void kafkatest_f20635_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    MockScheduler scheduler = new MockScheduler(time);    Agent agent = createAgent(scheduler);    AgentClient client = new AgentClient.Builder().maxTries(10).target("localhost", agent.port()).build();    new ExpectedTasks().waitFor(client);    try (MockKibosh mockKibosh = new MockKibosh()) {        Assert.assertEquals(KiboshControlFile.EMPTY, mockKibosh.read());        FilesUnreadableFaultSpec fooSpec = new FilesUnreadableFaultSpec(0, 900000, Collections.singleton("myAgent"), mockKibosh.tempDir.getPath(), "/foo", 123);        client.createWorker(new CreateWorkerRequest(0, "foo", fooSpec));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerRunning("foo", fooSpec, 0, new TextNode("Added fault foo"))).build()).waitFor(client);        Assert.assertEquals(new KiboshControlFile(Collections.<Kibosh.KiboshFaultSpec>singletonList(new KiboshFilesUnreadableFaultSpec("/foo", 123))), mockKibosh.read());        FilesUnreadableFaultSpec barSpec = new FilesUnreadableFaultSpec(0, 900000, Collections.singleton("myAgent"), mockKibosh.tempDir.getPath(), "/bar", 456);        client.createWorker(new CreateWorkerRequest(1, "bar", barSpec));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerRunning("foo", fooSpec, 0, new TextNode("Added fault foo"))).build()).addTask(new ExpectedTaskBuilder("bar").workerState(new WorkerRunning("bar", barSpec, 0, new TextNode("Added fault bar"))).build()).waitFor(client);        Assert.assertEquals(new KiboshControlFile(new ArrayList<Kibosh.KiboshFaultSpec>() {            {                add(new KiboshFilesUnreadableFaultSpec("/foo", 123));                add(new KiboshFilesUnreadableFaultSpec("/bar", 456));            }        }), mockKibosh.read());        time.sleep(1);        client.stopWorker(new StopWorkerRequest(0));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerDone("foo", fooSpec, 0, 1, new TextNode("Removed fault foo"), "")).build()).addTask(new ExpectedTaskBuilder("bar").workerState(new WorkerRunning("bar", barSpec, 0, new TextNode("Added fault bar"))).build()).waitFor(client);        Assert.assertEquals(new KiboshControlFile(Collections.<Kibosh.KiboshFaultSpec>singletonList(new KiboshFilesUnreadableFaultSpec("/bar", 456))), mockKibosh.read());    }}
public ExpectedTaskBuilder kafkatest_f20644_0(TaskSpec taskSpec)
{    this.taskSpec = taskSpec;    return this;}
public ExpectedTaskBuilder kafkatest_f20645_0(TaskState taskState)
{    this.taskState = taskState;    return this;}
public ExpectedTasks kafkatest_f20654_0(ExpectedTask task)
{    expected.put(task.id, task);    return this;}
public ExpectedTasksf20655_1final CoordinatorClient client) throws InterruptedException
{    TestUtils.waitForCondition(() -> {        TasksResponse tasks = null;        try {            tasks = client.tasks(new TasksRequest(null, 0, 0, 0, 0, Optional.empty()));        } catch (Exception e) {                        throw new RuntimeException(e);        }        StringBuilder errors = new StringBuilder();        for (Map.Entry<String, ExpectedTask> entry : expected.entrySet()) {            String id = entry.getKey();            ExpectedTask task = entry.getValue();            String differences = task.compare(tasks.tasks().get(id));            if (differences != null) {                errors.append(differences);            }        }        String errorString = errors.toString();        if (!errorString.isEmpty()) {                                                return false;        }        return true;    }, "Timed out waiting for expected tasks " + JsonUtil.toJsonString(expected));    return this;}
public Builder kafkatest_f20664_0(String nodeName)
{    if (agentNames.contains(nodeName)) {        throw new RuntimeException("There is already an agent on node " + nodeName);    }    agentNames.add(nodeName);    return this;}
private NodeData kafkatest_f20665_0(String nodeName, TreeMap<String, NodeData> nodes)
{    NodeData data = nodes.get(nodeName);    if (data != null)        return data;    data = new NodeData();    data.hostname = "127.0.0.1";    nodes.put(nodeName, data);    return data;}
public void kafkatest_f20674_0() throws Exception
{    HashSet<String> expected1 = new HashSet<>(Arrays.asList("foo1", "foo2", "foo3"));    assertEquals(expected1, StringExpander.expand("foo[1-3]"));    HashSet<String> expected2 = new HashSet<>(Arrays.asList("foo bar baz 0"));    assertEquals(expected2, StringExpander.expand("foo bar baz [0-0]"));    HashSet<String> expected3 = new HashSet<>(Arrays.asList("[[ wow50 ]]", "[[ wow51 ]]", "[[ wow52 ]]"));    assertEquals(expected3, StringExpander.expand("[[ wow[50-52] ]]"));    HashSet<String> expected4 = new HashSet<>(Arrays.asList("foo1bar", "foo2bar", "foo3bar"));    assertEquals(expected4, StringExpander.expand("foo[1-3]bar"));    // should expand latest range first    HashSet<String> expected5 = new HashSet<>(Arrays.asList("start[1-3]middle1epilogue", "start[1-3]middle2epilogue", "start[1-3]middle3epilogue"));    assertEquals(expected5, StringExpander.expand("start[1-3]middle[1-3]epilogue"));}
public void kafkatest_f20675_0()
{    assertEquals("2019-01-08T20:59:29.85Z", dateString(1546981169850L, ZoneOffset.UTC));}
public void kafkatest_f20684_0() throws Throwable
{    List<TopicPartitionInfo> tpInfo = new ArrayList<>();    tpInfo.add(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList()));    tpInfo.add(new TopicPartitionInfo(1, broker2, singleReplica, Collections.<Node>emptyList()));    adminClient.addTopic(false, TEST_TOPIC, tpInfo, null);    WorkerUtils.createTopics(log, adminClient, Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC), false);}
public void kafkatest_f20685_0() throws Throwable
{    final String existingTopic = "existing-topic";    List<TopicPartitionInfo> tpInfo = new ArrayList<>();    tpInfo.add(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList()));    tpInfo.add(new TopicPartitionInfo(1, broker2, singleReplica, Collections.<Node>emptyList()));    tpInfo.add(new TopicPartitionInfo(2, broker3, singleReplica, Collections.<Node>emptyList()));    adminClient.addTopic(false, existingTopic, tpInfo, null);    WorkerUtils.createTopics(log, adminClient, Collections.singletonMap(existingTopic, new NewTopic(existingTopic, tpInfo.size(), TEST_REPLICATION_FACTOR)), false);    assertEquals(Collections.singleton(existingTopic), adminClient.listTopics().names().get());}
private void kafkatest_f20694_0(String topicName, int numPartitions)
{    List<TopicPartitionInfo> tpInfo = new ArrayList<>();    int brokerIndex = 0;    for (int i = 0; i < numPartitions; ++i) {        Node broker = cluster.get(brokerIndex);        tpInfo.add(new TopicPartitionInfo(i, broker, singleReplica, Collections.<Node>emptyList()));        brokerIndex = (brokerIndex + 1) % cluster.size();    }    adminClient.addTopic(false, topicName, tpInfo, null);}
public void kafkatest_f20695_0() throws Throwable
{    Map<String, NewTopic> newTopics = Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC);    WorkerUtils.createTopics(log, adminClient, newTopics, true);    adminClient.setFetchesRemainingUntilVisible(TEST_TOPIC, 2);    WorkerUtils.verifyTopics(log, adminClient, Collections.singleton(TEST_TOPIC), Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC), 3, 1);    adminClient.setFetchesRemainingUntilVisible(TEST_TOPIC, 100);    try {        WorkerUtils.verifyTopics(log, adminClient, Collections.singleton(TEST_TOPIC), Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC), 2, 1);        Assert.fail("expected to get UnknownTopicOrPartitionException");    } catch (UnknownTopicOrPartitionException e) {    // expected    }}
public ExpectedLines kafkatest_f20704_0(final String nodeName, final CapturingCommandRunner runner) throws InterruptedException
{    TestUtils.waitForCondition(() -> linesMatch(nodeName, runner.lines(nodeName)), "failed to find the expected lines " + this.toString());    return this;}
private booleanf20705_1final String nodeName, List<String> actualLines)
{    int matchIdx = 0, i = 0;    while (true) {        if (matchIdx == expectedLines.size()) {                        return true;        }        if (i == actualLines.size()) {                        return false;        }        String actualLine = actualLines.get(i++);        String expectedLine = expectedLines.get(matchIdx);        if (expectedLine.equals(actualLine)) {            matchIdx++;        } else {            log.trace("Expected:\n'{}', Got:\n'{}'", expectedLine, actualLine);            matchIdx = 0;        }    }}
public void kafkatest_f20714_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    Scheduler scheduler = new MockScheduler(time);    try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().addCoordinator("node01").addAgent("node02").scheduler(scheduler).build()) {        NoOpTaskSpec fooSpec = new NoOpTaskSpec(1000, 500);        time.sleep(999);        CoordinatorClient coordinatorClient = cluster.coordinatorClient();        coordinatorClient.createTask(new CreateTaskRequest("fooSpec", fooSpec));        TaskState expectedState = new ExpectedTaskBuilder("fooSpec").taskState(new TaskPending(fooSpec)).build().taskState();        TaskState resp = coordinatorClient.task(new TaskRequest("fooSpec"));        assertEquals(expectedState, resp);    }}
public void kafkatest_f20715_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    Scheduler scheduler = new MockScheduler(time);    try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().addCoordinator("node01").addAgent("node02").scheduler(scheduler).build()) {        CoordinatorClient coordinatorClient = cluster.coordinatorClient();        NoOpTaskSpec fooSpec = new NoOpTaskSpec(1, 10);        coordinatorClient.createTask(new CreateTaskRequest("foo", fooSpec));        TaskState expectedState = new ExpectedTaskBuilder("foo").taskState(new TaskPending(fooSpec)).build().taskState();        TaskState resp = coordinatorClient.task(new TaskRequest("foo"));        assertEquals(expectedState, resp);        time.sleep(2);        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").taskState(new TaskRunning(fooSpec, 2, new TextNode("active"))).workerState(new WorkerRunning("foo", fooSpec, 2, new TextNode("active"))).build()).waitFor(coordinatorClient).waitFor(cluster.agentClient("node02"));        try {            coordinatorClient.task(new TaskRequest("non-existent-foo"));            fail("Non existent task request should have raised a NotFoundException");        } catch (NotFoundException ignored) {        }    }}
public void kafkatest_f20724_0() throws Exception
{    RestExceptionMapper.toException(Response.Status.NOT_FOUND.getStatusCode(), "Not Found");}
public void kafkatest_f20725_0() throws Exception
{    RestExceptionMapper.toException(Response.Status.NOT_IMPLEMENTED.getStatusCode(), "Not Implemented");}
public void kafkatest_f20734_0(Platform platform) throws Exception
{    this.future.cancel(false);    this.executor.shutdown();    this.executor.awaitTermination(1, TimeUnit.DAYS);    this.status.update(new TextNode("halted"));}
public void kafkatest_f20735_0() throws Exception
{    try {        JsonUtil.JSON_SERDE.readValue("{\"startMs\":123,\"durationMs\":456,\"exitMs\":1000,\"error\":\"foo\"}", SampleTaskSpec.class);        fail("Expected InvalidTypeIdException because type id is missing.");    } catch (InvalidTypeIdException e) {    }    String inputJson = "{\"class\":\"org.apache.kafka.trogdor.task.SampleTaskSpec\"," + "\"startMs\":123,\"durationMs\":456,\"nodeToExitMs\":{\"node01\":1000},\"error\":\"foo\"}";    SampleTaskSpec spec = JsonUtil.JSON_SERDE.readValue(inputJson, SampleTaskSpec.class);    assertEquals(123, spec.startMs());    assertEquals(456, spec.durationMs());    assertEquals(Long.valueOf(1000), spec.nodeToExitMs().get("node01"));    assertEquals("foo", spec.error());    String outputJson = JsonUtil.toJsonString(spec);    assertEquals(inputJson, outputJson);}
public void kafkatest_f20744_0() throws Exception
{    if (OperatingSystem.IS_WINDOWS)        return;    ExternalCommandWorker worker = new ExternalCommandWorkerBuilder("falseTask").command("false").build();    KafkaFutureImpl<String> doneFuture = new KafkaFutureImpl<>();    worker.start(null, new AgentWorkerStatusTracker(), doneFuture);    assertEquals("exited with return code 1", doneFuture.get());    worker.stop(null);}
public void kafkatest_f20745_0() throws Exception
{    ExternalCommandWorker worker = new ExternalCommandWorkerBuilder("notFoundTask").command("/dev/null/non/existent/script/path").build();    KafkaFutureImpl<String> doneFuture = new KafkaFutureImpl<>();    worker.start(null, new AgentWorkerStatusTracker(), doneFuture);    String errorString = doneFuture.get();    assertTrue(errorString.startsWith("Unable to start process"));    worker.stop(null);}
public void kafkatest_f20754_0()
{    SequentialPayloadGenerator g4 = new SequentialPayloadGenerator(4, 1);    assertLittleEndianArrayEquals(1, g4.generate(0));    assertLittleEndianArrayEquals(2, g4.generate(1));    SequentialPayloadGenerator g8 = new SequentialPayloadGenerator(8, 0);    assertLittleEndianArrayEquals(0, g8.generate(0));    assertLittleEndianArrayEquals(1, g8.generate(1));    assertLittleEndianArrayEquals(123123123123L, g8.generate(123123123123L));    SequentialPayloadGenerator g2 = new SequentialPayloadGenerator(2, 0);    assertLittleEndianArrayEquals(0, g2.generate(0));    assertLittleEndianArrayEquals(1, g2.generate(1));    assertLittleEndianArrayEquals(1, g2.generate(1));    assertLittleEndianArrayEquals(1, g2.generate(131073));}
private static void kafkatest_f20755_0(long expected, byte[] actual)
{    byte[] longActual = new byte[8];    System.arraycopy(actual, 0, longActual, 0, Math.min(actual.length, longActual.length));    ByteBuffer buf = ByteBuffer.wrap(longActual).order(ByteOrder.LITTLE_ENDIAN);    assertEquals(expected, buf.getLong());}
protected synchronized void kafkatest_f20764_0(long amount) throws InterruptedException
{    time.sleep(amount);}
public void kafkatest_f20765_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    ThrottleMock throttle = new ThrottleMock(time, 3);    Assert.assertFalse(throttle.increment());    Assert.assertEquals(0, time.milliseconds());    Assert.assertFalse(throttle.increment());    Assert.assertEquals(0, time.milliseconds());    Assert.assertFalse(throttle.increment());    Assert.assertEquals(0, time.milliseconds());    Assert.assertTrue(throttle.increment());    Assert.assertEquals(100, time.milliseconds());    time.sleep(50);    Assert.assertFalse(throttle.increment());    Assert.assertEquals(150, time.milliseconds());    Assert.assertFalse(throttle.increment());    Assert.assertEquals(150, time.milliseconds());    Assert.assertTrue(throttle.increment());    Assert.assertEquals(200, time.milliseconds());}
