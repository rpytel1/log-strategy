public void beam_f5_0() throws IOException
{    ExampleBigQueryTableOptions bigQueryTableOptions = options.as(ExampleBigQueryTableOptions.class);    if (bigQueryTableOptions.getBigQueryDataset() != null && bigQueryTableOptions.getBigQueryTable() != null && bigQueryTableOptions.getBigQuerySchema() != null) {        pendingMessages.add("******************Set Up Big Query Table*******************");        setupBigQueryTable(bigQueryTableOptions.getProject(), bigQueryTableOptions.getBigQueryDataset(), bigQueryTableOptions.getBigQueryTable(), bigQueryTableOptions.getBigQuerySchema());        pendingMessages.add("The BigQuery table has been set up for this example: " + bigQueryTableOptions.getProject() + ":" + bigQueryTableOptions.getBigQueryDataset() + "." + bigQueryTableOptions.getBigQueryTable());    }}
private void beam_f6_0()
{    pendingMessages.add("*************************Tear Down*************************");    ExamplePubsubTopicAndSubscriptionOptions pubsubOptions = options.as(ExamplePubsubTopicAndSubscriptionOptions.class);    if (!pubsubOptions.getPubsubTopic().isEmpty()) {        try {            deletePubsubTopic(pubsubOptions.getPubsubTopic());            pendingMessages.add("The Pub/Sub topic has been deleted: " + pubsubOptions.getPubsubTopic());        } catch (IOException e) {            pendingMessages.add("Failed to delete the Pub/Sub topic : " + pubsubOptions.getPubsubTopic());        }        if (!pubsubOptions.getPubsubSubscription().isEmpty()) {            try {                deletePubsubSubscription(pubsubOptions.getPubsubSubscription());                pendingMessages.add("The Pub/Sub subscription has been deleted: " + pubsubOptions.getPubsubSubscription());            } catch (IOException e) {                pendingMessages.add("Failed to delete the Pub/Sub subscription : " + pubsubOptions.getPubsubSubscription());            }        }    }    ExampleBigQueryTableOptions bigQueryTableOptions = options.as(ExampleBigQueryTableOptions.class);    if (bigQueryTableOptions.getBigQueryDataset() != null && bigQueryTableOptions.getBigQueryTable() != null && bigQueryTableOptions.getBigQuerySchema() != null) {        pendingMessages.add("The BigQuery table might contain the example's output, " + "and it is not deleted automatically: " + bigQueryTableOptions.getProject() + ":" + bigQueryTableOptions.getBigQueryDataset() + "." + bigQueryTableOptions.getBigQueryTable());        pendingMessages.add("Please go to the Developers Console to delete it manually." + " Otherwise, you may be charged for its usage.");    }}
public void beam_f15_0(PipelineResult result)
{    pipelinesToCancel.add(result);    if (!options.as(ExampleOptions.class).getKeepJobsRunning()) {        addShutdownHook(pipelinesToCancel);    }    try {        result.waitUntilFinish();    } catch (UnsupportedOperationException e) {                        tearDown();        printPendingMessages();    } catch (Exception e) {        throw new RuntimeException("Failed to wait the pipeline until finish: " + result);    }}
private void beam_f16_0(final Collection<PipelineResult> pipelineResults)
{    Runtime.getRuntime().addShutdownHook(new Thread(() -> {        tearDown();        printPendingMessages();        for (PipelineResult pipelineResult : pipelineResults) {            try {                pipelineResult.cancel();            } catch (IOException e) {                System.out.println("Failed to cancel the job.");                System.out.println(e.getMessage());            }        }        for (PipelineResult pipelineResult : pipelineResults) {            boolean cancellationVerified = false;            for (int retryAttempts = 6; retryAttempts > 0; retryAttempts--) {                if (pipelineResult.getState().isTerminal()) {                    cancellationVerified = true;                    break;                } else {                    System.out.println("The example pipeline is still running. Verifying the cancellation.");                }                Uninterruptibles.sleepUninterruptibly(10, TimeUnit.SECONDS);            }            if (!cancellationVerified) {                System.out.println("Failed to verify the cancellation for job: " + pipelineResult);            }        }    }));}
public void beam_f25_0(ProcessContext c)
{    c.output(new CompletionCandidate(c.element().getKey(), c.element().getValue()));}
public PCollection<KV<String, List<CompletionCandidate>>> beam_f26_0(PCollection<CompletionCandidate> input)
{    return input.apply(ParDo.of(new AllPrefixes(minPrefix))).apply(Top.<String, CompletionCandidate>largestPerKey(candidatesPerPrefix).withHotKeyFanout(new HotKeyFanout()));}
public boolean beam_f35_0(Object other)
{    if (other instanceof CompletionCandidate) {        CompletionCandidate that = (CompletionCandidate) other;        return this.count == that.count && this.value.equals(that.value);    } else {        return false;    }}
public int beam_f36_0()
{    return Long.valueOf(count).hashCode() ^ value.hashCode();}
public PCollection<KV<String, Integer>> beam_f45_1(PCollection<KV<String, Integer>> userScores)
{        PCollection<KV<String, Integer>> sumScores = userScores.apply("UserSum", Sum.integersPerKey());        final PCollectionView<Double> globalMeanScore = sumScores.apply(Values.create()).apply(Mean.<Integer>globally().asSingletonView());        PCollection<KV<String, Integer>> filtered = sumScores.apply("ProcessAndFilter", ParDo.of(new DoFn<KV<String, Integer>, KV<String, Integer>>() {        private final Counter numSpammerUsers = Metrics.counter("main", "SpammerUsers");        @ProcessElement        public void processElement(ProcessContext c) {            Integer score = c.element().getValue();            Double gmc = c.sideInput(globalMeanScore);            if (score > (gmc * SCORE_WEIGHT)) {                                numSpammerUsers.inc();                c.output(c.element());            }        }    }).withSideInputs(globalMeanScore));    return filtered;}
public void beam_f46_1(ProcessContext c)
{    Integer score = c.element().getValue();    Double gmc = c.sideInput(globalMeanScore);    if (score > (gmc * SCORE_WEIGHT)) {                numSpammerUsers.inc();        c.output(c.element());    }}
 String beam_f55_0()
{    return robot;}
 long beam_f56_0()
{    return startTimeInMillis;}
private static String beam_f65_0(Long currTime, int delayInMillis)
{    TeamInfo team = randomTeam(liveTeams);    String teamName = team.getTeamName();    String user;    final int parseErrorRate = 900000;    String robot = team.getRobot();        if (robot != null) {                if (random.nextInt(team.numMembers() / 2) == 0) {            user = robot;        } else {            user = team.getRandomUser();        }    } else {                user = team.getRandomUser();    }    String event = user + "," + teamName + "," + random.nextInt(MAX_SCORE);        if (random.nextInt(parseErrorRate) == 0) {        System.out.println("Introducing a parse error.");        event = "THIS LINE REPRESENTS CORRUPT DATA AND WILL CAUSE A PARSE ERROR";    }    return addTimeInfoToEvent(event, currTime, delayInMillis);}
private static String beam_f66_0(String message, Long currTime, int delayInMillis)
{    String eventTimeString = Long.toString((currTime - delayInMillis) / 1000 * 1000);        String dateString = GameConstants.DATE_TIME_FORMATTER.print(currTime);    message = message + "," + eventTimeString + "," + dateString;    return message;}
protected static Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> beam_f75_0()
{    Map<String, WriteWindowedToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = new HashMap<>();    tableConfigure.put("team", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> c.element().getKey()));    tableConfigure.put("total_score", new WriteWindowedToBigQuery.FieldInfo<>("INTEGER", (c, w) -> c.element().getValue()));    tableConfigure.put("window_start", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> {        IntervalWindow window = (IntervalWindow) w;        return GameConstants.DATE_TIME_FORMATTER.print(window.start());    }));    tableConfigure.put("processing_time", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> GameConstants.DATE_TIME_FORMATTER.print(Instant.now())));    tableConfigure.put("timing", new WriteWindowedToBigQuery.FieldInfo<>("STRING", (c, w) -> c.pane().getTiming().toString()));    return tableConfigure;}
protected static Map<String, WriteToBigQuery.FieldInfo<KV<String, Integer>>> beam_f76_0()
{    Map<String, WriteToBigQuery.FieldInfo<KV<String, Integer>>> tableConfigure = new HashMap<>();    tableConfigure.put("user", new WriteToBigQuery.FieldInfo<>("STRING", (c, w) -> c.element().getKey()));    tableConfigure.put("total_score", new WriteToBigQuery.FieldInfo<>("INTEGER", (c, w) -> c.element().getValue()));    return tableConfigure;}
public String beam_f85_0()
{    return this.team;}
public Integer beam_f86_0()
{    return this.score;}
 String beam_f95_0()
{    return this.fieldType;}
 FieldFn<InputT> beam_f96_0()
{    return this.fieldFn;}
public ResourceId beam_f105_0(int shardNumber, int numShards, OutputFileHints outputFileHints)
{    throw new UnsupportedOperationException("Unsupported.");}
public PDone beam_f106_0(PCollection<InputT> teamAndScore)
{    if (windowed) {        teamAndScore.apply("ConvertToRow", ParDo.of(new BuildRowFn())).apply(new WriteToText.WriteOneFilePerWindow(filenamePrefix));    } else {        teamAndScore.apply("ConvertToRow", ParDo.of(new BuildRowFn())).apply(TextIO.write().to(filenamePrefix));    }    return PDone.in(teamAndScore.getPipeline());}
public PCollection<KV<URI, String>> beam_f115_0(PBegin input)
{    Pipeline pipeline = input.getPipeline();            PCollectionList<KV<URI, String>> urisToLines = PCollectionList.empty(pipeline);        for (final URI uri : uris) {        String uriString;        if ("file".equals(uri.getScheme())) {            uriString = new File(uri).getPath();        } else {            uriString = uri.toString();        }        PCollection<KV<URI, String>> oneUriToLines = pipeline.apply("TextIO.Read(" + uriString + ")", TextIO.read().from(uriString)).apply("WithKeys(" + uriString + ")", WithKeys.of(uri)).setCoder(KvCoder.of(StringDelegateCoder.of(URI.class), StringUtf8Coder.of()));        urisToLines = urisToLines.and(oneUriToLines);    }    return urisToLines.apply(Flatten.pCollections());}
public PCollection<KV<String, KV<URI, Double>>> beam_f116_1(PCollection<KV<URI, String>> uriToContent)
{                final PCollectionView<Long> totalDocuments = uriToContent.apply("GetURIs", Keys.create()).apply("DistinctDocs", Distinct.create()).apply(Count.globally()).apply(View.asSingleton());            PCollection<KV<URI, String>> uriToWords = uriToContent.apply("SplitWords", ParDo.of(new DoFn<KV<URI, String>, KV<URI, String>>() {        @ProcessElement        public void processElement(ProcessContext c) {            URI uri = c.element().getKey();            String line = c.element().getValue();            for (String word : line.split("\\W+", -1)) {                                if ("love".equalsIgnoreCase(word)) {                                    }                if (!word.isEmpty()) {                    c.output(KV.of(uri, word.toLowerCase()));                }            }        }    }));            PCollection<KV<String, Long>> wordToDocCount = uriToWords.apply("DistinctWords", Distinct.create()).apply(Values.create()).apply("CountDocs", Count.perElement());            PCollection<KV<URI, Long>> uriToWordTotal = uriToWords.apply("GetURIs2", Keys.create()).apply("CountWords", Count.perElement());                PCollection<KV<KV<URI, String>, Long>> uriAndWordToCount = uriToWords.apply("CountWordDocPairs", Count.perElement());                    PCollection<KV<URI, KV<String, Long>>> uriToWordAndCount = uriAndWordToCount.apply("ShiftKeys", ParDo.of(new DoFn<KV<KV<URI, String>, Long>, KV<URI, KV<String, Long>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            URI uri = c.element().getKey().getKey();            String word = c.element().getKey().getValue();            Long occurrences = c.element().getValue();            c.output(KV.of(uri, KV.of(word, occurrences)));        }    }));                            final TupleTag<Long> wordTotalsTag = new TupleTag<>();    final TupleTag<KV<String, Long>> wordCountsTag = new TupleTag<>();    KeyedPCollectionTuple<URI> coGbkInput = KeyedPCollectionTuple.of(wordTotalsTag, uriToWordTotal).and(wordCountsTag, uriToWordAndCount);                                    PCollection<KV<URI, CoGbkResult>> uriToWordAndCountAndTotal = coGbkInput.apply("CoGroupByUri", CoGroupByKey.create());                    PCollection<KV<String, KV<URI, Double>>> wordToUriAndTf = uriToWordAndCountAndTotal.apply("ComputeTermFrequencies", ParDo.of(new DoFn<KV<URI, CoGbkResult>, KV<String, KV<URI, Double>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            URI uri = c.element().getKey();            Long wordTotal = c.element().getValue().getOnly(wordTotalsTag);            for (KV<String, Long> wordAndCount : c.element().getValue().getAll(wordCountsTag)) {                String word = wordAndCount.getKey();                Long wordCount = wordAndCount.getValue();                Double termFrequency = wordCount.doubleValue() / wordTotal.doubleValue();                c.output(KV.of(word, KV.of(uri, termFrequency)));            }        }    }));                            PCollection<KV<String, Double>> wordToDf = wordToDocCount.apply("ComputeDocFrequencies", ParDo.of(new DoFn<KV<String, Long>, KV<String, Double>>() {        @ProcessElement        public void processElement(ProcessContext c) {            String word = c.element().getKey();            Long documentCount = c.element().getValue();            Long documentTotal = c.sideInput(totalDocuments);            Double documentFrequency = documentCount.doubleValue() / documentTotal.doubleValue();            c.output(KV.of(word, documentFrequency));        }    }).withSideInputs(totalDocuments));            final TupleTag<KV<URI, Double>> tfTag = new TupleTag<>();    final TupleTag<Double> dfTag = new TupleTag<>();    PCollection<KV<String, CoGbkResult>> wordToUriAndTfAndDf = KeyedPCollectionTuple.of(tfTag, wordToUriAndTf).and(dfTag, wordToDf).apply(CoGroupByKey.create());                        PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf = wordToUriAndTfAndDf.apply("ComputeTfIdf", ParDo.of(new DoFn<KV<String, CoGbkResult>, KV<String, KV<URI, Double>>>() {        @ProcessElement        public void processElement(ProcessContext c) {            String word = c.element().getKey();            Double df = c.element().getValue().getOnly(dfTag);            for (KV<URI, Double> uriAndTf : c.element().getValue().getAll(tfTag)) {                URI uri = uriAndTf.getKey();                Double tf = uriAndTf.getValue();                Double tfIdf = tf * Math.log(1 / df);                c.output(KV.of(word, KV.of(uri, tfIdf)));            }        }    }));    return wordToUriAndTfIdf;}
public static void beam_f125_0(String[] args) throws Exception
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    runTfIdf(options);}
public void beam_f126_0(ProcessContext c)
{    TableRow row = c.element();    int timestamp;        try {        timestamp = ((BigDecimal) row.get("timestamp")).intValue();    } catch (ClassCastException e) {        timestamp = ((Integer) row.get("timestamp")).intValue();    }    String userName = (String) row.get("contributor_username");    if (userName != null) {                c.outputWithTimestamp(userName, new Instant(timestamp * 1000L));    }}
public static void beam_f135_0(String[] args)
{    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);    run(options);}
public String beam_f136_0()
{    return this.stationId;}
public void beam_f145_0(DoFn<String, String>.ProcessContext c) throws Exception
{    String[] items = c.element().split(",", -1);    if (items.length > 0) {        try {            String timestamp = items[0];            c.outputWithTimestamp(c.element(), new Instant(dateTimeFormat.parseMillis(timestamp)));        } catch (IllegalArgumentException e) {                }    }}
public void beam_f146_0(ProcessContext c)
{    String[] items = c.element().split(",", -1);    if (items.length < 48) {                return;    }        String timestamp = items[0];    String stationId = items[1];    String freeway = items[2];    String direction = items[3];    Integer totalFlow = tryIntParse(items[7]);    for (int i = 1; i <= 8; ++i) {        Integer laneFlow = tryIntParse(items[6 + 5 * i]);        Double laneAvgOccupancy = tryDoubleParse(items[7 + 5 * i]);        Double laneAvgSpeed = tryDoubleParse(items[8 + 5 * i]);        if (laneFlow == null || laneAvgOccupancy == null || laneAvgSpeed == null) {            return;        }        LaneInfo laneInfo = new LaneInfo(stationId, "lane" + i, direction, freeway, timestamp, laneFlow, laneAvgOccupancy, laneAvgSpeed, totalFlow);        c.output(KV.of(stationId, laneInfo));    }}
private static Double beam_f155_0(String number)
{    try {        return Double.parseDouble(number);    } catch (NumberFormatException e) {        return null;    }}
public String beam_f156_0()
{    return this.stationId;}
public void beam_f165_0(ProcessContext c)
{    String[] items = c.element().split(",");    String stationType = tryParseStationType(items);        if ("ML".equals(stationType)) {        Double avgSpeed = tryParseAvgSpeed(items);        String stationId = tryParseStationId(items);                if (avgSpeed != null && stationId != null && sdStations.containsKey(stationId)) {            StationSpeed stationSpeed = new StationSpeed(stationId, avgSpeed, c.timestamp().getMillis());                        KV<String, StationSpeed> outputValue = KV.of(sdStations.get(stationId), stationSpeed);            c.output(outputValue);        }    }}
public void beam_f166_0(ProcessContext c) throws IOException
{    String route = c.element().getKey();    double speedSum = 0.0;    int speedCount = 0;    int speedups = 0;    int slowdowns = 0;    List<StationSpeed> infoList = Lists.newArrayList(c.element().getValue());        Collections.sort(infoList);    Map<String, Double> prevSpeeds = new HashMap<>();        for (StationSpeed item : infoList) {        Double speed = item.getAvgSpeed();        if (speed != null) {            speedSum += speed;            speedCount++;            Double lastSpeed = prevSpeeds.get(item.getStationId());            if (lastSpeed != null) {                if (lastSpeed < speed) {                    speedups += 1;                } else {                    slowdowns += 1;                }            }            prevSpeeds.put(item.getStationId(), speed);        }    }    if (speedCount == 0) {                return;    }    double speedAvg = speedSum / speedCount;    boolean slowdownEvent = slowdowns >= 2 * speedups;    RouteInfo routeInfo = new RouteInfo(route, speedAvg, slowdownEvent);    c.output(KV.of(route, routeInfo));}
private static String beam_f175_0(String[] inputItems)
{    return tryParseString(inputItems, 1);}
private static String beam_f176_0(String[] inputItems)
{    return tryParseString(inputItems, 0);}
public void beam_f185_0(ProcessContext c)
{    TableRow row = new TableRow().set("word", c.element().getKey()).set("all_plays", c.element().getValue());    c.output(row);}
public PCollection<TableRow> beam_f186_0(PCollection<TableRow> rows)
{        PCollection<KV<String, String>> words = rows.apply(ParDo.of(new ExtractLargeWordsFn()));        PCollection<KV<String, String>> wordAllPlays = words.apply(Combine.perKey(new ConcatWords()));        PCollection<TableRow> results = wordAllPlays.apply(ParDo.of(new FormatShakespeareOutputFn()));    return results;}
public void beam_f195_0(ProcessContext c)
{    Double meanTemp = Double.parseDouble(c.element().get("mean_temp").toString());    Double gTemp = c.sideInput(globalMeanTemp);    if (meanTemp < gTemp) {        c.output(c.element());    }}
private static TableSchema beam_f196_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("year").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("month").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("day").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("mean_temp").setType("FLOAT"));    return new TableSchema().setFields(fields);}
public void beam_f205_0(ProcessContext c)
{    TableRow row = new TableRow().set("month", c.element().getKey()).set("max_mean_temp", c.element().getValue());    c.output(row);}
public PCollection<TableRow> beam_f206_0(PCollection<TableRow> rows)
{        PCollection<KV<Integer, Double>> temps = rows.apply(ParDo.of(new ExtractTempFn()));        PCollection<KV<Integer, Double>> tempMaxes = temps.apply(Max.doublesPerKey());        PCollection<TableRow> results = tempMaxes.apply(ParDo.of(new FormatMaxesFn()));    return results;}
private static TableReference beam_f215_0(String project, String dataset, String table)
{    TableReference tableRef = new TableReference();    tableRef.setProjectId(project);    tableRef.setDatasetId(dataset);    tableRef.setTableId(table);    return tableRef;}
private static TableSchema beam_f216_0()
{    List<TableFieldSchema> fields = new ArrayList<>();    fields.add(new TableFieldSchema().setName("trigger_type").setType("STRING"));    fields.add(new TableFieldSchema().setName("freeway").setType("STRING"));    fields.add(new TableFieldSchema().setName("total_flow").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("number_of_records").setType("INTEGER"));    fields.add(new TableFieldSchema().setName("window").setType("STRING"));    fields.add(new TableFieldSchema().setName("isFirst").setType("BOOLEAN"));    fields.add(new TableFieldSchema().setName("isLast").setType("BOOLEAN"));    fields.add(new TableFieldSchema().setName("timing").setType("STRING"));    fields.add(new TableFieldSchema().setName("event_time").setType("TIMESTAMP"));    fields.add(new TableFieldSchema().setName("processing_time").setType("TIMESTAMP"));    return new TableSchema().setFields(fields);}
public TableDestination beam_f225_0(Long destination)
{    return new TableDestination(new TableReference().setProjectId(writeProject).setDatasetId(writeDataset).setTableId(writeTable + "_" + destination), "Table for year " + destination);}
public TableSchema beam_f226_0(Long destination)
{    return new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("year").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("month").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("day").setType("INTEGER").setMode("REQUIRED"), new TableFieldSchema().setName("maxTemp").setType("FLOAT").setMode("NULLABLE")));}
public static Map<String, String> beam_f235_0()
{    Map<String, String> map = new HashMap<>();    Instant now = Instant.now();    DateTimeFormatter dtf = DateTimeFormat.forPattern("HH:MM:SS");    map.put("Key_A", now.minus(Duration.standardSeconds(30)).toString(dtf));    map.put("Key_B", now.minus(Duration.standardSeconds(30)).toString());    return map;}
public static void beam_f236_1(String[] args)
{    MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class);        Pipeline p = Pipeline.create(options);        p.apply(Create.of(1)).apply(ParDo.of(new DoFn<Integer, Integer>() {                @ProcessElement        public void process(ProcessContext c) {            MyOptions ops = c.getPipelineOptions().as(MyOptions.class);                                            }    }));        p.apply(Create.of(1, 2, 3, 4)).apply(Sum.integersGlobally());    p.run();}
public Boolean beam_f246_0()
{    return onlyUpLoadLogsOnError;}
public void beam_f247_0(Boolean onlyUpLoadLogsOnError)
{    this.onlyUpLoadLogsOnError = onlyUpLoadLogsOnError;}
public static void beam_f256_0(String[] args) throws Exception
{        SubProcessPipelineOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(SubProcessPipelineOptions.class);    Pipeline p = Pipeline.create(options);        SubProcessConfiguration configuration = options.getSubProcessConfiguration();        List<KV<String, String>> sampleData = new ArrayList<>();    for (int i = 0; i < 10000; i++) {        String str = String.valueOf(i);        sampleData.add(KV.of(str, str));    }        p.apply(Create.of(sampleData)).apply("Echo inputs round 1", ParDo.of(new EchoInputDoFn(configuration, "Echo"))).apply("Echo inputs round 2", ParDo.of(new EchoInputDoFn(configuration, "EchoAgain")));    p.run();}
public void beam_f257_0() throws Exception
{    CallingSubProcessUtils.setUp(configuration, binaryName);}
public String beam_f266_0()
{    return value;}
public void beam_f267_0(String value)
{    this.value = value;}
public byte[] beam_f276_1(SubProcessCommandLineArgs commands) throws Exception
{    try (CallingSubProcessUtils.Permit permit = new CallingSubProcessUtils.Permit(processBuilder.command().get(0))) {        try (SubProcessIOFiles outputFiles = new SubProcessIOFiles(configuration.getWorkerPath())) {            try {                Process process = execBinary(processBuilder, commands, outputFiles);                return collectProcessResultsBytes(process, processBuilder, outputFiles);            } catch (Exception ex) {                                throw ex;            }        } catch (IOException ex) {                    }        return new byte[0];    }}
private ProcessBuilder beam_f277_0(ProcessBuilder builder, SubProcessCommandLineArgs commands, SubProcessIOFiles outPutFiles) throws IllegalStateException
{        if (getTotalCommandBytes(commands) > MAX_SIZE_COMMAND_LINE_ARGS) {        throw new IllegalStateException("Command is over 2MB in size");    }    appendExecutablePath(builder);        builder.command().add(1, outPutFiles.resultFile.toString());        for (SubProcessCommandLineArgs.Command s : commands.getParameters()) {        builder.command().add(s.ordinalPosition + 2, s.value);    }    builder.redirectError(Redirect.appendTo(outPutFiles.errFile.toFile()));    builder.redirectOutput(Redirect.appendTo(outPutFiles.outFile.toFile()));    return builder;}
public static void beam_f286_1(SubProcessConfiguration configuration, String binaryName) throws Exception
{    if (!semaphores.containsKey(binaryName)) {        initSemaphore(configuration.getConcurrency(), binaryName);    }    synchronized (downloadedFiles) {        if (!downloadedFiles.contains(binaryName)) {                        FileUtils.createDirectoriesOnWorker(configuration);                        ExecutableFile executableFile = new ExecutableFile(configuration, binaryName);            FileUtils.copyFileFromGCSToWorker(executableFile);            downloadedFiles.add(binaryName);        }    }}
public static synchronized void beam_f287_1(Integer permits, String binaryName)
{    if (!semaphores.containsKey(binaryName)) {                semaphores.put(binaryName, new Semaphore(permits));    }}
private void beam_f296_0(SubProcessConfiguration configuration)
{    this.destinationLocation = FileUtils.getFileResourceId(configuration.getWorkerPath(), fileName).toString();}
public static ResourceId beam_f297_0(String directory, String fileName)
{    ResourceId resourceID = FileSystems.matchNewResource(directory, true);    return resourceID.getCurrentDirectory().resolve(fileName, StandardResolveOptions.RESOLVE_FILE);}
public Long beam_f306_0(PipelineOptions options)
{    return options.as(Options.class).getMinTimestampMillis() + Duration.standardHours(1).getMillis();}
 static void beam_f307_0(Options options) throws IOException
{    final String output = options.getOutput();    final Instant minTimestamp = new Instant(options.getMinTimestampMillis());    final Instant maxTimestamp = new Instant(options.getMaxTimestampMillis());    Pipeline pipeline = Pipeline.create(options);    /*     * Concept #1: the Beam SDK lets us run the same pipeline with either a bounded or     * unbounded input source.     */    PCollection<String> input = pipeline.apply(TextIO.read().from(options.getInputFile())).apply(ParDo.of(new AddTimestampFn(minTimestamp, maxTimestamp)));    /*     * Concept #3: Window into fixed windows. The fixed window size for this example defaults to 1     * minute (you can change this with a command-line option). See the documentation for more     * information on how fixed windows work, and for information on the other types of windowing     * available (e.g., sliding windows).     */    PCollection<String> windowedWords = input.apply(Window.into(FixedWindows.of(Duration.standardMinutes(options.getWindowSize()))));    /*     * Concept #4: Re-use our existing CountWords transform that does not have knowledge of     * windows over a PCollection containing windowed values.     */    PCollection<KV<String, Long>> wordCounts = windowedWords.apply(new WordCount.CountWords());    /*     * Concept #5: Format the results and write to a sharded file partitioned by window, using a     * simple ParDo operation. Because there may be failures followed by retries, the     * writes must be idempotent, but the details of writing to files is elided here.     */    wordCounts.apply(MapElements.via(new WordCount.FormatAsTextFn())).apply(new WriteOneFilePerWindow(output, options.getNumShards()));    PipelineResult result = pipeline.run();    try {        result.waitUntilFinish();    } catch (Exception exc) {        result.cancel();    }}
public static Collection<Object[]> beam_f316_0()
{    return Arrays.asList(new Object[][] { { true }, { false } });}
public void beam_f317_0()
{    List<String> words = Arrays.asList("apple", "apple", "apricot", "banana", "blackberry", "blackberry", "blackberry", "blueberry", "blueberry", "cherry");    PCollection<String> input = p.apply(Create.of(words));    PCollection<KV<String, List<CompletionCandidate>>> output = input.apply(new ComputeTopCompletions(2, recursive)).apply(Filter.by(element -> element.getKey().length() <= 2));    PAssert.that(output).containsInAnyOrder(KV.of("a", parseList("apple:2", "apricot:1")), KV.of("ap", parseList("apple:2", "apricot:1")), KV.of("b", parseList("blackberry:3", "blueberry:2")), KV.of("ba", parseList("banana:1")), KV.of("bl", parseList("blackberry:3", "blueberry:2")), KV.of("c", parseList("cherry:1")), KV.of("ch", parseList("cherry:1")));    p.run().waitUntilFinish();}
public String beam_f326_0()
{    return teamName;}
public void beam_f327_0()
{    TestStream<GameActionInfo> createEvents = TestStream.create(AvroCoder.of(GameActionInfo.class)).advanceWatermarkTo(baseTime).addElements(event(TestUser.BLUE_ONE, 3, Duration.standardSeconds(3)), event(TestUser.BLUE_ONE, 2, Duration.standardMinutes(1)), event(TestUser.RED_TWO, 3, Duration.standardSeconds(22)), event(TestUser.BLUE_TWO, 5, Duration.standardMinutes(3))).advanceWatermarkTo(baseTime.plus(Duration.standardMinutes(3))).addElements(event(TestUser.RED_ONE, 1, Duration.standardMinutes(4)), event(TestUser.BLUE_ONE, 2, Duration.standardSeconds(270))).advanceWatermarkToInfinity();    PCollection<KV<String, Integer>> teamScores = p.apply(createEvents).apply(new CalculateTeamScores(TEAM_WINDOW_DURATION, ALLOWED_LATENESS));    String blueTeam = TestUser.BLUE_ONE.getTeam();    String redTeam = TestUser.RED_ONE.getTeam();    PAssert.that(teamScores).inOnTimePane(new IntervalWindow(baseTime, TEAM_WINDOW_DURATION)).containsInAnyOrder(KV.of(blueTeam, 12), KV.of(redTeam, 4));    p.run().waitUntilFinish();}
public String beam_f336_0()
{    return teamName;}
public void beam_f337_0()
{    TestStream<KV<String, GameActionInfo>> createEvents = TestStream.create(KvCoder.of(StringUtf8Coder.of(), AvroCoder.of(GameActionInfo.class))).advanceWatermarkTo(baseTime).addElements(event(TestUser.RED_TWO, 99, Duration.standardSeconds(10)), event(TestUser.RED_ONE, 1, Duration.standardSeconds(20)), event(TestUser.RED_ONE, 0, Duration.standardSeconds(30)), event(TestUser.RED_TWO, 100, Duration.standardSeconds(40)), event(TestUser.RED_TWO, 201, Duration.standardSeconds(50))).advanceWatermarkToInfinity();    PCollection<KV<String, Integer>> teamScores = p.apply(createEvents).apply(ParDo.of(new UpdateTeamScoreFn(100)));    String redTeam = TestUser.RED_ONE.getTeam();    PAssert.that(teamScores).inWindow(GlobalWindow.INSTANCE).containsInAnyOrder(KV.of(redTeam, 100), KV.of(redTeam, 200), KV.of(redTeam, 401));    p.run().waitUntilFinish();}
public void beam_f346_0() throws Exception
{    TfIdfITOptions options = TestPipeline.testingPipelineOptions().as(TfIdfITOptions.class);    options.setInput(DEFAULT_INPUT);    options.setOutput(FileSystems.matchNewResource(options.getTempRoot(), true).resolve(String.format("TfIdfIT-%tF-%<tH-%<tM-%<tS-%<tL", new Date()), StandardResolveOptions.RESOLVE_DIRECTORY).resolve("output", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("results", StandardResolveOptions.RESOLVE_FILE).toString());    options.setOnSuccessMatcher(new FileChecksumMatcher(EXPECTED_OUTPUT_CHECKSUM, options.getOutput() + "*-of-*.csv", DEFAULT_SHARD_TEMPLATE));    TfIdf.runTfIdf(options);}
public void beam_f347_0() throws Exception
{    pipeline.getCoderRegistry().registerCoderForClass(URI.class, StringDelegateCoder.of(URI.class));    PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf = pipeline.apply(Create.of(KV.of(new URI("x"), "a b c d"), KV.of(new URI("y"), "a b c"), KV.of(new URI("z"), "a m n"))).apply(new TfIdf.ComputeTfIdf());    PCollection<String> words = wordToUriAndTfIdf.apply(Keys.create()).apply(Distinct.create());    PAssert.that(words).containsInAnyOrder(Arrays.asList("a", "m", "n", "b", "c", "d"));    pipeline.run().waitUntilFinish();}
public void beam_f356_0() throws Exception
{    this.options.setBigQuerySchema(FormatStatsFn.getSchema());    this.options.setProject(this.projectId);    this.options.setBigQueryDataset(this.outputDatasetId);    this.options.setBigQueryTable(this.outputTable);    TrafficRoutes.runTrafficRoutes(options);    FluentBackoff backoffFactory = FluentBackoff.DEFAULT.withMaxRetries(4).withInitialBackoff(Duration.standardSeconds(1L));    Sleeper sleeper = Sleeper.DEFAULT;    BackOff backoff = BackOffAdapter.toGcpBackOff(backoffFactory.backoff());    String res = "empty_result";    do {        QueryResponse response = this.bqClient.queryWithRetries(String.format("SELECT count(*) as total FROM [%s:%s.%s]", this.projectId, this.outputDatasetId, this.outputTable), this.projectId);                try {            res = response.getRows().get(0).getF().get(0).getV().toString();            break;        } catch (NullPointerException e) {                }    } while (BackOffUtils.next(sleeper, backoff));    assertEquals("27", res);}
public static void beam_f357_0()
{    PipelineOptionsFactory.register(BigQueryTornadoesITOptions.class);}
public void beam_f366_0() throws Exception
{    DoFnTester<KV<String, String>, TableRow> formatShakespeareOutputFn = DoFnTester.of(new FormatShakespeareOutputFn());    List<TableRow> results = formatShakespeareOutputFn.processBundle(COMBINED_TUPLES_ARRAY);    Assert.assertThat(results, CoreMatchers.hasItem(resultRow1));    Assert.assertThat(results, CoreMatchers.hasItem(resultRow2));}
public void beam_f367_0()
{    List<String> strings = Arrays.asList("k1", "k5", "k5", "k2", "k1", "k2", "k3");    PCollection<String> input = p.apply(Create.of(strings).withCoder(StringUtf8Coder.of()));    PCollection<String> output = input.apply(Distinct.create());    PAssert.that(output).containsInAnyOrder("k1", "k5", "k2", "k3");    p.run().waitUntilFinish();}
public void beam_f376_0()
{    PCollection<KV<String, Integer>> output = pipeline.apply(Create.of(Arrays.asList(INPUT)).withCoder(StringUtf8Coder.of())).apply(ParDo.of(new ExtractFlowInfo()));    KV<String, Integer> expectedOutput = KV.of("94", 29);    PAssert.that(output).containsInAnyOrder(Arrays.asList(expectedOutput));    pipeline.run().waitUntilFinish();}
public void beam_f377_0()
{    PCollection<KV<String, Integer>> flow = pipeline.apply(Create.timestamped(TIME_STAMPED_INPUT)).apply(ParDo.of(new ExtractFlowInfo()));    PCollection<TableRow> totalFlow = flow.apply(Window.into(FixedWindows.of(Duration.standardMinutes(1)))).apply(new TotalFlow("default"));    PCollection<String> results = totalFlow.apply(ParDo.of(new FormatResults()));    PAssert.that(results).containsInAnyOrder(canonicalFormat(OUT_ROW_1), canonicalFormat(OUT_ROW_2));    pipeline.run().waitUntilFinish();}
public void beam_f386_0() throws Exception
{        Path fileA = Files.createTempFile("test-Echo", ".sh");    Path fileB = Files.createTempFile("test-EchoAgain", ".sh");    Path workerTempFiles = Files.createTempDirectory("test-Echoo");    try (SeekableByteChannel channel = FileChannel.open(fileA, StandardOpenOption.CREATE, StandardOpenOption.WRITE)) {        channel.write(ByteBuffer.wrap(getTestShellEcho().getBytes(UTF_8)));    }    try (SeekableByteChannel channel = FileChannel.open(fileB, StandardOpenOption.CREATE, StandardOpenOption.WRITE)) {        channel.write(ByteBuffer.wrap(getTestShellEchoAgain().getBytes(UTF_8)));    }        SubProcessPipelineOptions options = PipelineOptionsFactory.as(SubProcessPipelineOptions.class);    options.setConcurrency(2);    options.setSourcePath(fileA.getParent().toString());    options.setWorkerPath(workerTempFiles.toAbsolutePath().toString());    p.getOptions().as(GcsOptions.class).setGcsUtil(buildMockGcsUtil());        SubProcessConfiguration configuration = options.getSubProcessConfiguration();        List<KV<String, String>> sampleData = new ArrayList<>();    for (int i = 0; i < 100; i++) {        String str = String.valueOf(i);        sampleData.add(KV.of(str, str));    }                PCollection<KV<String, String>> output = p.apply(Create.of(sampleData)).apply("Echo inputs round 1", ParDo.of(new EchoInputDoFn(configuration, fileA.getFileName().toString()))).apply("Echo inputs round 2", ParDo.of(new EchoInputDoFn(configuration, fileB.getFileName().toString())));    PAssert.that(output).containsInAnyOrder(sampleData);    p.run();}
public void beam_f387_0() throws Exception
{    CallingSubProcessUtils.setUp(configuration, binaryName);}
public void beam_f396_0() throws Exception
{    WindowedWordCountITOptions options = batchOptions();    options.setNumShards(3);    testWindowedWordCountPipeline(options);}
public void beam_f397_0() throws Exception
{    WindowedWordCountITOptions options = streamingOptions();    options.setNumShards(3);    testWindowedWordCountPipeline(options);}
public void beam_f406_0() throws Exception
{    WordCountITOptions options = TestPipeline.testingPipelineOptions().as(WordCountITOptions.class);    options.setInputFile(DEFAULT_INPUT);    options.setOutput(FileSystems.matchNewResource(options.getTempRoot(), true).resolve(String.format("WordCountIT-%tF-%<tH-%<tM-%<tS-%<tL", new Date()), StandardResolveOptions.RESOLVE_DIRECTORY).resolve("output", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("results", StandardResolveOptions.RESOLVE_FILE).toString());    options.setOnSuccessMatcher(new FileChecksumMatcher(DEFAULT_OUTPUT_CHECKSUM, options.getOutput() + "*-of-*"));    WordCount.runWordCount(options);}
public void beam_f407_0() throws Exception
{    List<String> words = Arrays.asList(" some  input  words ", " ", " cool ", " foo", " bar");    PCollection<String> output = p.apply(Create.of(words).withCoder(StringUtf8Coder.of())).apply(ParDo.of(new ExtractWordsFn()));    PAssert.that(output).containsInAnyOrder("some", "input", "words", "cool", "foo", "bar");    p.run().waitUntilFinish();}
 static PCollection<Long> beam_f416_0(PCollection<Integer> input)
{    return input.apply(Count.globally());}
public void beam_f417_0()
{    Create.Values<Integer> values = Create.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);    PCollection<Integer> numbers = testPipeline.apply(values);    PCollection<Long> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(10L);    testPipeline.run().waitUntilFinish();}
public void beam_f426_0()
{    Create.Values<Integer> values = Create.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);    PCollection<Integer> numbers = testPipeline.apply(values);    PCollection<Integer> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(1);    testPipeline.run().waitUntilFinish();}
public static void beam_f427_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));    PCollection<Integer> output = applyTransform(numbers);    output.apply(Log.ofElements());    pipeline.run();}
public void beam_f436_0()
{    Create.Values<Integer> values = Create.of(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);    PCollection<Integer> numbers = testPipeline.apply(values);    PCollection<Integer> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder(1, 3, 5, 7, 9);    testPipeline.run().waitUntilFinish();}
public static void beam_f437_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> words = pipeline.apply(Create.of("apple", "banana", "cherry", "durian", "guava", "melon"));    PCollection<KV<String, String>> output = applyTransform(words);    output.apply(Log.ofElements());    pipeline.run();}
public void beam_f446_0(@Element KV<String, CoGbkResult> element, OutputReceiver<String> out)
{    String alphabet = element.getKey();    CoGbkResult coGbkResult = element.getValue();    String fruit = coGbkResult.getOnly(fruitsTag);    String country = coGbkResult.getOnly(countriesTag);    out.output(new WordsAlphabet(alphabet, fruit, country).toString());}
public String beam_f447_0()
{    return "WordsAlphabet{" + "alphabet='" + alphabet + '\'' + ", fruit='" + fruit + '\'' + ", country='" + country + '\'' + '}';}
public static void beam_f456_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<KV<String, Integer>> scores = pipeline.apply(Create.of(KV.of(PLAYER_1, 15), KV.of(PLAYER_2, 10), KV.of(PLAYER_1, 100), KV.of(PLAYER_3, 25), KV.of(PLAYER_2, 75)));    PCollection<KV<String, Integer>> output = applyTransform(scores);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<KV<String, Integer>> beam_f457_0(PCollection<KV<String, Integer>> input)
{    return input.apply(Combine.perKey(new SumIntBinaryCombineFn()));}
public Accum beam_f466_0(Iterable<Accum> accumulators)
{    Accum merged = createAccumulator();    for (Accum accumulator : accumulators) {        merged.sum += accumulator.sum;        merged.count += accumulator.count;    }    return merged;}
public Double beam_f467_0(Accum accumulator)
{    return ((double) accumulator.sum) / accumulator.count;}
public void beam_f476_0()
{    Create.Values<String> values = Create.of("1,2,3,4,5", "6,7,8,9,10");    PCollection<Integer> results = testPipeline.apply(values).apply(new ExtractAndMultiplyNumbers());    PAssert.that(results).containsInAnyOrder(10, 20, 30, 40, 50, 60, 70, 80, 90, 100);    testPipeline.run().waitUntilFinish();}
public static void beam_f477_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    pipeline.run();}
public void beam_f486_0()
{    Create.Values<String> values = Create.of("Apache Beam", "Unified Batch and Streaming");    PCollection<String> numbers = testPipeline.apply(values);    PCollection<String> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder("Apache", "Beam", "Unified", "Batch", "and", "Streaming");    testPipeline.run().waitUntilFinish();}
public static void beam_f487_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<Integer> numbers = pipeline.apply(Create.of(10, 20, 30, 40, 50));    PCollection<Integer> output = applyTransform(numbers);    output.apply(Log.ofElements());    pipeline.run();}
public void beam_f496_0(@Element String sentence, OutputReceiver<String> out)
{    String[] words = sentence.split(" ");    for (String word : words) {        out.output(word);    }}
public void beam_f497_0()
{    Create.Values<String> values = Create.of("Hello Beam", "It is awesome");    PCollection<String> numbers = testPipeline.apply(values);    PCollection<String> results = Task.applyTransform(numbers);    PAssert.that(results).containsInAnyOrder("Hello", "Beam", "It", "is", "awesome");    testPipeline.run().waitUntilFinish();}
public String beam_f506_0()
{    return "Person{" + "name='" + name + '\'' + ", city='" + city + '\'' + ", country='" + country + '\'' + '}';}
public static void beam_f507_0(String[] args)
{    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<KV<String, String>> citiesToCountries = pipeline.apply("Cities and Countries", Create.of(KV.of("Beijing", "China"), KV.of("London", "United Kingdom"), KV.of("San Francisco", "United States"), KV.of("Singapore", "Singapore"), KV.of("Sydney", "Australia")));    PCollectionView<Map<String, String>> citiesToCountriesView = createView(citiesToCountries);    PCollection<Person> persons = pipeline.apply("Persons", Create.of(new Person("Henry", "Singapore"), new Person("Jane", "San Francisco"), new Person("Lee", "Beijing"), new Person("John", "Sydney"), new Person("Alfred", "London")));    PCollection<Person> output = applyTransform(persons, citiesToCountriesView);    output.apply(Log.ofElements());    pipeline.run();}
public static void beam_f516_0(String[] args)
{    String[] lines = { "apple orange grape banana apple banana", "banana orange banana papaya" };    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> wordCounts = pipeline.apply(Create.of(Arrays.asList(lines)));    PCollection<String> output = applyTransform(wordCounts);    output.apply(Log.ofElements());    pipeline.run();}
 static PCollection<String> beam_f517_0(PCollection<String> input)
{    return input.apply(FlatMapElements.into(TypeDescriptors.strings()).via(line -> Arrays.asList(line.split(" ")))).apply(Count.perElement()).apply(ParDo.of(new DoFn<KV<String, Long>, String>() {        @ProcessElement        public void processElement(@Element KV<String, Long> element, OutputReceiver<String> out) {            out.output(element.getKey() + ":" + element.getValue());        }    }));}
public void beam_f526_0()
{    PCollection<String> countries = testPipeline.apply(TextIO.read().from("countries.txt"));    PCollection<String> results = Task.applyTransform(countries);    PAssert.that(results).containsInAnyOrder("AUSTRALIA", "CHINA", "ENGLAND", "FRANCE", "GERMANY", "INDONESIA", "JAPAN", "MEXICO", "SINGAPORE", "UNITED STATES");    testPipeline.run().waitUntilFinish();}
 static GenerateEvent beam_f527_0()
{    return new GenerateEvent();}
public void beam_f536_0()
{    TestStream<String> testStream = TestStream.create(SerializableCoder.of(String.class)).addElements(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:00+00:00"))).addElements(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:01+00:00"))).addElements(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:02+00:00"))).addElements(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:03+00:00"))).addElements(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:04+00:00"))).advanceWatermarkTo(Instant.parse("2019-06-01T00:00:05+00:00")).addElements(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:05+00:00"))).addElements(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:06+00:00"))).addElements(TimestampedValue.of("event", Instant.parse("2019-06-01T00:00:07+00:00"))).advanceWatermarkTo(Instant.parse("2019-06-01T00:00:10+00:00")).advanceWatermarkToInfinity();    PCollection<String> eventsPColl = testPipeline.apply(testStream);    PCollection<Long> results = Task.applyTransform(eventsPColl);    PAssert.that(results).inWindow(createIntervalWindow("2019-06-01T00:00:00+00:00", "2019-06-01T00:00:05+00:00")).containsInAnyOrder(5L).inWindow(createIntervalWindow("2019-06-01T00:00:05+00:00", "2019-06-01T00:00:10+00:00")).containsInAnyOrder(3L);    testPipeline.run().waitUntilFinish();}
private IntervalWindow beam_f537_0(String startStr, String endStr)
{    return new IntervalWindow(Instant.parse(startStr), Instant.parse(endStr));}
public void beam_f546_1(@Element T element, OutputReceiver<T> out, BoundedWindow window)
{    String message = prefix + element.toString();    if (!(window instanceof GlobalWindow)) {        message = message + "  Window:" + window.toString();    }        out.output(element);}
public static SerializableFunction<Iterable<KV<String, Iterable<String>>>, Void> beam_f547_0(KV<String, Iterable<String>>... kvs)
{    return new ContainsKvs(ImmutableList.copyOf(kvs));}
public DateTime beam_f556_0()
{    return date;}
public void beam_f557_0(DateTime date)
{    this.date = date;}
public String beam_f566_0()
{    return id;}
public void beam_f567_0(String id)
{    this.id = id;}
 static PCollection<Event> beam_f576_0(PCollection<Event> events)
{    return events.apply(WithTimestamps.of(event -> event.getDate().toInstant()));}
public void beam_f577_0()
{    List<Event> events = Arrays.asList(new Event("1", "book-order", DateTime.parse("2019-06-01T00:00:00+00:00")), new Event("2", "pencil-order", DateTime.parse("2019-06-02T00:00:00+00:00")), new Event("3", "paper-order", DateTime.parse("2019-06-03T00:00:00+00:00")), new Event("4", "pencil-order", DateTime.parse("2019-06-04T00:00:00+00:00")), new Event("5", "book-order", DateTime.parse("2019-06-05T00:00:00+00:00")));    PCollection<Event> eventsPColl = testPipeline.apply(Create.of(events));    PCollection<Event> results = Task.applyTransform(eventsPColl);    PCollection<KV<Event, Instant>> timestampedResults = results.apply("KV<Event, Instant>", ParDo.of(new DoFn<Event, KV<Event, Instant>>() {        @ProcessElement        public void processElement(@Element Event event, ProcessContext context, OutputReceiver<KV<Event, Instant>> out) {            out.output(KV.of(event, context.timestamp()));        }    }));    PAssert.that(results).containsInAnyOrder(events);    PAssert.that(timestampedResults).containsInAnyOrder(KV.of(events.get(0), events.get(0).getDate().toInstant()), KV.of(events.get(1), events.get(1).getDate().toInstant()), KV.of(events.get(2), events.get(2).getDate().toInstant()), KV.of(events.get(3), events.get(3).getDate().toInstant()), KV.of(events.get(4), events.get(4).getDate().toInstant()));    testPipeline.run().waitUntilFinish();}
public static ApexRunner beam_f586_0(PipelineOptions options)
{    ApexPipelineOptions apexPipelineOptions = PipelineOptionsValidator.validate(ApexPipelineOptions.class, options);    return new ApexRunner(apexPipelineOptions);}
protected List<PTransformOverride> beam_f587_0()
{    return ImmutableList.<PTransformOverride>builder().add(PTransformOverride.of(PTransformMatchers.classEqualTo(Create.Values.class), new PrimitiveCreate.Factory())).add(PTransformOverride.of(PTransformMatchers.createViewWithViewFn(PCollectionViews.IterableViewFn.class), new StreamingViewAsIterable.Factory())).add(PTransformOverride.of(PTransformMatchers.createViewWithViewFn(PCollectionViews.ListViewFn.class), new StreamingViewAsIterable.Factory())).add(PTransformOverride.of(PTransformMatchers.createViewWithViewFn(PCollectionViews.MapViewFn.class), new StreamingViewAsIterable.Factory())).add(PTransformOverride.of(PTransformMatchers.createViewWithViewFn(PCollectionViews.MultimapViewFn.class), new StreamingViewAsIterable.Factory())).add(PTransformOverride.of(PTransformMatchers.createViewWithViewFn(PCollectionViews.SingletonViewFn.class), new StreamingWrapSingletonInList.Factory())).add(PTransformOverride.of(PTransformMatchers.splittableParDoMulti(), new SplittableParDo.OverrideFactory())).add(PTransformOverride.of(PTransformMatchers.splittableProcessKeyedBounded(), new SplittableParDoNaiveBounded.OverrideFactory<>())).add(PTransformOverride.of(PTransformMatchers.splittableProcessKeyedUnbounded(), new SplittableParDoViaKeyedWorkItems.OverrideFactory<>())).add(PTransformOverride.of(PTransformMatchers.requiresStableInputParDoMulti(), UnsupportedOverrideFactory.withMessage("Apex runner currently doesn't support @RequiresStableInput annotation."))).build();}
public PTransformReplacement<PCollection<T>, PCollection<T>> beam_f596_0(AppliedPTransform<PCollection<T>, PCollection<T>, CreatePCollectionView<T, T>> transform)
{    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new StreamingWrapSingletonInList<>(transform.getTransform()));}
public PCollection<T> beam_f597_0(PCollection<T> input)
{    return ((PCollection<T>) input.apply(Combine.globally(new Concatenate<T>()).withoutDefaults())).apply(CreateApexPCollectionView.of(view));}
public Iterable<Class<? extends PipelineRunner<?>>> beam_f606_0()
{    return ImmutableList.of(ApexRunner.class, TestApexRunner.class);}
public Iterable<Class<? extends PipelineOptions>> beam_f607_0()
{    return ImmutableList.of(ApexPipelineOptions.class);}
public boolean beam_f617_1()
{            return true;}
public void beam_f618_0(ShutdownMode arg0) throws LauncherException
{        throw new UnsupportedOperationException();}
protected String beam_f627_0()
{    return cmd;}
protected Map<String, String> beam_f628_0()
{    return env;}
public void beam_f637_1(TransformHierarchy.Node node)
{    }
public void beam_f638_1(TransformHierarchy.Node node)
{        PTransform transform = node.getTransform();    TransformTranslator translator = getTransformTranslator(transform.getClass());    if (null == translator) {        throw new UnsupportedOperationException("no translator registered for " + transform);    }    translationContext.setCurrentTransform(node.toAppliedPTransform(getPipeline()));    translator.translate(transform, translationContext);}
public void beam_f647_0(Flatten.PCollections<T> transform, TranslationContext context)
{    List<PCollection<T>> inputCollections = extractPCollections(context.getInputs());    if (inputCollections.isEmpty()) {                @SuppressWarnings("unchecked")        UnboundedSource<T, ?> unboundedSource = new ValuesSource<>(Collections.EMPTY_LIST, VoidCoder.of());        ApexReadUnboundedInputOperator<T, ?> operator = new ApexReadUnboundedInputOperator<>(unboundedSource, context.getPipelineOptions());        context.addOperator(operator, operator.output);    } else if (inputCollections.size() == 1) {        context.addAlias(context.getOutput(), inputCollections.get(0));    } else {        @SuppressWarnings("unchecked")        PCollection<T> output = (PCollection<T>) context.getOutput();        Map<PCollection<?>, Integer> unionTags = Collections.emptyMap();        flattenCollections(inputCollections, unionTags, output, context);    }}
private List<PCollection<T>> beam_f648_0(Map<TupleTag<?>, PValue> inputs)
{    List<PCollection<T>> collections = Lists.newArrayList();    for (PValue pv : inputs.values()) {        checkArgument(pv instanceof PCollection, "Non-PCollection provided as input to flatten: %s of type %s", pv, pv.getClass().getSimpleName());        collections.add((PCollection<T>) pv);    }    return collections;}
public void beam_f659_1(KV<K, Iterable<V>> output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    if (traceTuples) {            }    ApexGroupByKeyOperator.this.output.emit(ApexStreamTuple.DataTuple.of(WindowedValue.of(output, timestamp, windows, pane)));}
public void beam_f660_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    throw new UnsupportedOperationException("GroupAlsoByWindow should not use side outputs");}
private void beam_f669_1(ApexStreamTuple.WatermarkTuple<?> mark)
{    this.currentInputWatermark = mark.getTimestamp();    long minEventTimeTimer = currentKeyTimerInternals.fireReadyTimers(this.currentInputWatermark, this, TimeDomain.EVENT_TIME);    checkState(minEventTimeTimer >= currentInputWatermark, "Event time timer processing generates new timer(s) behind watermark.");                    long minProcessingTimeTimer = Long.MIN_VALUE;    while (minProcessingTimeTimer < currentInputWatermark) {        minProcessingTimeTimer = currentKeyTimerInternals.fireReadyTimers(this.currentInputWatermark, this, TimeDomain.PROCESSING_TIME);        if (minProcessingTimeTimer < currentInputWatermark) {                    }    }    if (Iterables.isEmpty(sideInputs)) {        outputWatermark(mark);        return;    }    long potentialOutputWatermark = Math.min(pushedBackWatermark.get(), currentInputWatermark);    if (potentialOutputWatermark > currentOutputWatermark) {        currentOutputWatermark = potentialOutputWatermark;        outputWatermark(ApexStreamTuple.WatermarkTuple.of(currentOutputWatermark));    }}
private void beam_f670_1(ApexStreamTuple.WatermarkTuple<?> mark)
{    if (traceTuples) {            }    output.emit(mark);    if (!additionalOutputPortMapping.isEmpty()) {        for (DefaultOutputPort<ApexStreamTuple<?>> additionalOutput : additionalOutputPortMapping.values()) {            additionalOutput.emit(mark);        }    }}
public long beam_f680_0()
{    return state;}
public void beam_f681_0()
{    state = Long.MAX_VALUE;}
public void beam_f690_0(ApexStreamTuple<WindowedValue<InputT>> tuple)
{    try {        fn.process(tuple, outputEmitter);    } catch (Exception e) {        Throwables.throwIfUnchecked(e);        throw new RuntimeException(e);    }}
public void beam_f691_0(long windowId)
{    if (!available && (isBoundedSource || source instanceof ValuesSource)) {                emitWatermarkIfNecessary(BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis());    } else {        emitWatermarkIfNecessary(reader.getWatermark().getMillis());    }}
public void beam_f700_0(TimerData timerData)
{    getTimerSet(timerData.getDomain()).addTimer(getKeyBytes(this.currentKey), timerData);}
public void beam_f701_0(StateNamespace namespace, String timerId, TimeDomain timeDomain)
{    getTimerSet(timeDomain).deleteTimer(getKeyBytes(this.currentKey), namespace, timerId);}
public void beam_f710_0(Slice keyBytes, TimerData timer)
{    Set<Slice> timersForKey = activeTimers.get(keyBytes);    if (timersForKey == null) {        timersForKey = new HashSet<>();    }    try {        Slice timerBytes = new Slice(CoderUtils.encodeToByteArray(timerDataCoder, timer));        timersForKey.add(timerBytes);    } catch (CoderException e) {        throw new RuntimeException(e);    }    activeTimers.put(keyBytes, timersForKey);    this.minTimestamp = Math.min(minTimestamp, timer.getTimestamp().getMillis());}
public void beam_f711_0(Slice keyBytes, StateNamespace namespace, String timerId)
{    Set<Slice> timersForKey = activeTimers.get(keyBytes);    if (timersForKey == null) {        return;    }    Iterator<Slice> timerIt = timersForKey.iterator();    while (timerIt.hasNext()) {        try {            TimerData timerData = CoderUtils.decodeFromByteArray(timerDataCoder, timerIt.next().buffer);            ComparisonChain chain = ComparisonChain.start().compare(timerData.getTimerId(), timerId);            if (chain.result() == 0 && !timerData.getNamespace().equals(namespace)) {                                chain = chain.compare(timerData.getNamespace().stringKey(), namespace.stringKey());            }            if (chain.result() == 0) {                timerIt.remove();            }        } catch (CoderException e) {            throw new RuntimeException(e);        }    }    if (timersForKey.isEmpty()) {        activeTimers.remove(keyBytes);    }}
public InputT beam_f720_0(PCollectionView<?> view)
{    PInput input = this.viewInputs.get(view);    checkArgument(input != null, "unknown view " + view.getName());    return (InputT) input;}
public void beam_f721_0(AppliedPTransform<?, ?, ?> transform)
{    this.currentTransform = transform;}
public void beam_f730_0(Operator operator, Map<PCollection<?>, OutputPort<?>> ports)
{    boolean first = true;    for (Map.Entry<PCollection<?>, OutputPort<?>> portEntry : ports.entrySet()) {        if (first) {            addOperator(operator, portEntry.getValue(), portEntry.getKey());            first = false;        } else {            this.streams.put(portEntry.getKey(), (Pair) new ImmutablePair<>(new OutputPortInfo(portEntry.getValue(), getCurrentTransform()), new ArrayList<>()));        }    }}
public void beam_f731_0(Operator operator, OutputPort port, PCollection output)
{            String name = getCurrentTransform().getFullName();    for (int i = 1; this.operators.containsKey(name); i++) {        name = getCurrentTransform().getFullName() + i;    }    this.operators.put(name, operator);    this.streams.put(output, (Pair) new ImmutablePair<>(new OutputPortInfo(port, getCurrentTransform()), new ArrayList<>()));}
public BagState<T> beam_f740_0(final StateTag<BagState<T>> address, Coder<T> elemCoder)
{    return new ApexBagState<>(namespace, address, elemCoder);}
public SetState<T> beam_f741_0(StateTag<SetState<T>> address, Coder<T> elemCoder)
{    throw new UnsupportedOperationException(String.format("%s is not supported", SetState.class.getSimpleName()));}
public int beam_f750_0()
{    int result = namespace.hashCode();    result = 31 * result + address.hashCode();    return result;}
public ApexValueState<T> beam_f751_0()
{    return this;}
public TimestampCombiner beam_f760_0()
{    return timestampCombiner;}
public ApexCombiningState<InputT, AccumT, OutputT> beam_f761_0()
{    return this;}
public ApexBagState<T> beam_f770_0()
{    return this;}
public List<T> beam_f771_0()
{    List<T> value = super.readValue();    if (value == null) {        value = new ArrayList<>();    }    return value;}
public T beam_f780_0()
{    return value;}
public void beam_f781_0(T value)
{    this.value = value;}
public String beam_f790_0()
{    return "[Watermark " + getTimestamp() + "]";}
public static ApexStreamTupleCoder<T> beam_f791_0(Coder<T> valueCoder)
{    return new ApexStreamTupleCoder<>(valueCoder);}
public Coder<?> beam_f800_0()
{    return this.coder;}
public Object beam_f801_0(Slice fragment)
{    ByteArrayInputStream bis = new ByteArrayInputStream(fragment.buffer, fragment.offset, fragment.length);    try {        return coder.decode(bis, Context.OUTER);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public T beam_f810_0(StateNamespace namespace, StateTag<T> address)
{    return factory.stateInternalsForKey(currentKey).state(namespace, address);}
public T beam_f811_0(StateNamespace namespace, StateTag<T> address, StateContext<?> c)
{    return factory.stateInternalsForKey(currentKey).state(namespace, address, c);}
public boolean beam_f820_0() throws IOException
{    if (iterator.hasNext()) {        current = iterator.next();        return true;    } else {        return false;    }}
public T beam_f821_0() throws NoSuchElementException
{    return current;}
public void beam_f831_0() throws Exception
{    Pipeline p = Pipeline.create();    long numElements = 1000;    PCollection<Long> input = p.apply(GenerateSequence.from(0).to(numElements));    PAssert.thatSingleton(input.apply("Count", Count.globally())).isEqualTo(numElements);    ApexPipelineOptions options = PipelineOptionsFactory.as(ApexPipelineOptions.class);    DAG dag = TestApexRunner.translate(p, options);    String[] expectedThreadLocal = { "/GroupGlobally/RewindowActuals/Window.Assign" };    Set<String> actualThreadLocal = Sets.newHashSet();    for (DAG.StreamMeta sm : dag.getAllStreamsMeta()) {        DAG.OutputPortMeta opm = sm.getSource();        if (sm.getLocality() == Locality.THREAD_LOCAL) {            String name = opm.getOperatorMeta().getName();            String prefix = "PAssert$";            if (name.startsWith(prefix)) {                                name = name.substring(prefix.length() + 1);            }            actualThreadLocal.add(name);        }    }    Assert.assertThat(actualThreadLocal, Matchers.hasItems(expectedThreadLocal));}
public void beam_f832_0() throws Exception
{    List<File> deps = ApexYarnLauncher.getYarnDeployDependencies();    String depsToString = deps.toString();                assertThat(depsToString, containsString("apex-common-"));    assertThat(depsToString, not(containsString("hadoop-")));    assertThat(depsToString, not(containsString("zookeeper-")));}
public Coder<String> beam_f842_0()
{    return StringUtf8Coder.of();}
public boolean beam_f843_0() throws IOException
{    currentRecord = texts[0];    currentTimestamp = new Instant(0);    return true;}
public void beam_f853_0(ProcessContext c)
{    if (c.element().trim().isEmpty()) {        emptyLines.inc(1);    }        String[] words = c.element().split("[^a-zA-Z']+");        for (String word : words) {        if (!word.isEmpty()) {            c.output(word);        }    }}
 static void beam_f854_0(WordCountOptions options)
{    Pipeline p = Pipeline.create(options);    p.apply("ReadLines", TextIO.read().from(options.getInputFile())).apply(ParDo.of(new ExtractWordsFn())).apply(Count.perElement()).apply(ParDo.of(new FormatAsStringFn())).apply("WriteCounts", TextIO.write().to(options.getOutput()));    p.run().waitUntilFinish();}
public void beam_f863_0(ProcessContext c) throws Exception
{    RESULTS.add(c.element());}
public void beam_f864_0()
{    ApexPipelineOptions options = PipelineOptionsFactory.as(ApexPipelineOptions.class);    Pipeline p = Pipeline.create();    PCollection<String> single = p.apply(Create.of(Collections.singletonList("1")));    PCollectionList.of(single).apply(Flatten.pCollections()).apply(ParDo.of(new EmbeddedCollector()));    DAG dag = TestApexRunner.translate(p, options);    Assert.assertNotNull(dag.getOperatorMeta("ParDo(EmbeddedCollector)/ParMultiDo(EmbeddedCollector)"));}
public boolean beam_f873_0() throws IOException
{    if (iterator.hasNext()) {        KV<String, Instant> kv = iterator.next();        collected = false;        currentRecord = kv.getKey();        currentTimestamp = kv.getValue();        return true;    } else {        return false;    }}
public byte[] beam_f874_0() throws NoSuchElementException
{    return new byte[0];}
public void beam_f884_1() throws Exception
{    ApexPipelineOptions options = PipelineOptionsFactory.create().as(ApexPipelineOptions.class);    options.setApplicationName("ParDoBound");    options.setRunner(ApexRunner.class);    Pipeline p = Pipeline.create(options);    List<Integer> collection = Lists.newArrayList(1, 2, 3, 4, 5);    List<Integer> expected = Lists.newArrayList(6, 7, 8, 9, 10);    p.apply(Create.of(collection).withCoder(SerializableCoder.of(Integer.class))).apply(ParDo.of(new Add(5))).apply(ParDo.of(new EmbeddedCollector()));    ApexRunnerResult result = (ApexRunnerResult) p.run();    DAG dag = result.getApexDAG();    DAG.OperatorMeta om = dag.getOperatorMeta("Create.Values");    Assert.assertNotNull(om);    Assert.assertEquals(om.getOperator().getClass(), ApexReadUnboundedInputOperator.class);    om = dag.getOperatorMeta("ParDo(Add)/ParMultiDo(Add)");    Assert.assertNotNull(om);    Assert.assertEquals(om.getOperator().getClass(), ApexParDoOperator.class);    long timeout = System.currentTimeMillis() + TIMEOUT_MILLIS;    while (System.currentTimeMillis() < timeout) {        if (EmbeddedCollector.RESULTS.containsAll(expected)) {            break;        }                Thread.sleep(SLEEP_MILLIS);    }    Assert.assertEquals(Sets.newHashSet(expected), EmbeddedCollector.RESULTS);}
public void beam_f885_0(ProcessContext c) throws Exception
{    if (sideInputView != null) {        number = c.sideInput(sideInputView);    }    c.output(c.element() + number);}
public void beam_f894_0(ProcessContext c) throws Exception
{    outputToAllWithSideInputs(c, "processing: " + c.element());}
private void beam_f895_0(ProcessContext c, String value)
{    if (!sideInputViews.isEmpty()) {        List<Integer> sideInputValues = new ArrayList<>();        for (PCollectionView<Integer> sideInputView : sideInputViews) {            sideInputValues.add(c.sideInput(sideInputView));        }        value += ": " + sideInputValues;    }    c.output(value);    for (TupleTag<String> additionalOutputTupleTag : additionalOutputTupleTags) {        c.output(additionalOutputTupleTag, additionalOutputTupleTag.getId() + ": " + value);    }}
private Coder<?> beam_f904_0(List<T> items, PTransform<PCollection<T>, ? extends PCollectionView<?>> viewTransform) throws Exception
{    Pipeline p = Pipeline.create();    PCollectionView<?> view = p.apply(Create.of(items)).apply(viewTransform);    p.apply(Create.of(1)).apply("ParDo", ParDo.of(new DoFn<Integer, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {        }    }).withSideInputs(view));    DAG dag = TestApexRunner.translate(p, PipelineOptionsFactory.create().as(ApexPipelineOptions.class));    OperatorMeta om = dag.getOperatorMeta("ParDo/ParMultiDo(Anonymous)");    assertNotNull(om);    assertEquals(2, om.getInputStreams().size());    DAG.InputPortMeta sideInput = null;    for (DAG.InputPortMeta input : om.getInputStreams().keySet()) {        if ("sideInput1".equals(((LogicalPlan.InputPortMeta) input).getPortName())) {            sideInput = input;        }    }    assertNotNull("could not find stream for: sideInput1", sideInput);    CoderAdapterStreamCodec sc = (CoderAdapterStreamCodec) sideInput.getAttributes().get(PortContext.STREAM_CODEC);    @SuppressWarnings("rawtypes")    ApexStreamTupleCoder<?> coder = (ApexStreamTupleCoder<?>) sc.getCoder();    @SuppressWarnings("rawtypes")    FullWindowedValueCoder<?> fwvc = (FullWindowedValueCoder) coder.getValueCoder();    return fwvc.getValueCoder();}
private static StateInternals beam_f906_0()
{    return new ApexStateInternals.ApexStateBackend().newStateInternalsFactory(StringUtf8Coder.of()).stateInternalsForKey("dummyKey");}
public Instant beam_f922_0()
{    return Instant.now();}
public UnboundedSource.CheckpointMark beam_f923_0()
{    return null;}
public void beam_f934_0(Throwable t)
{    err.set(t);    completed.countDown();    throw new RuntimeException(t);}
public void beam_f935_0()
{    completed.countDown();}
public Map<Class<? extends Coder>, CoderTranslator<? extends Coder>> beam_f944_0()
{    return ImmutableMap.of(AvroCoder.class, new AvroCoderTranslator());}
public List<? extends Coder<?>> beam_f945_0(AvroCoder from)
{    return Collections.emptyList();}
private static List<String> beam_f954_0(T coder, CoderTranslator<T> translator, SdkComponents components) throws IOException
{    List<String> componentIds = new ArrayList<>();    for (Coder<?> component : translator.getComponents(coder)) {        componentIds.add(components.registerCoder(component));    }    return componentIds;}
private static RunnerApi.Coder beam_f955_0(Coder<?> coder) throws IOException
{    RunnerApi.Coder.Builder coderBuilder = RunnerApi.Coder.newBuilder();    return coderBuilder.setSpec(FunctionSpec.newBuilder().setUrn(JAVA_SERIALIZED_CODER_URN).setPayload(ByteString.copyFrom(SerializableUtils.serializeToByteArray(coder))).build()).build();}
public List<? extends Coder<?>> beam_f964_0(KvCoder<?, ?> from)
{    return ImmutableList.of(from.getKeyCoder(), from.getValueCoder());}
public KvCoder<?, ?> beam_f965_0(List<Coder<?>> components)
{    return KvCoder.of(components.get(0), components.get(1));}
public LengthPrefixCoder<?> beam_f974_0(List<Coder<?>> components)
{    return LengthPrefixCoder.of(components.get(0));}
 static CoderTranslator<FullWindowedValueCoder<?>> beam_f975_0()
{    return new SimpleStructuredCoderTranslator<FullWindowedValueCoder<?>>() {        @Override        public List<? extends Coder<?>> getComponents(FullWindowedValueCoder<?> from) {            return ImmutableList.of(from.getValueCoder(), from.getWindowCoder());        }        @Override        public FullWindowedValueCoder<?> fromComponents(List<Coder<?>> components) {            return WindowedValue.getFullCoder(components.get(0), (Coder<BoundedWindow>) components.get(1));        }    };}
private static Coder<AccumT> beam_f984_0(GlobalCombineFn<InputT, AccumT, ?> combineFn, AppliedPTransform<PCollection<InputT>, ?, Combine.Globally<InputT, ?>> transform) throws IOException
{    try {        @SuppressWarnings("unchecked")        PCollection<InputT> mainInput = (PCollection<InputT>) Iterables.getOnlyElement(TransformInputs.nonAdditionalInputs(transform));        return combineFn.getAccumulatorCoder(transform.getPipeline().getCoderRegistry(), mainInput.getCoder());    } catch (CannotProvideCoderException e) {        throw new IOException("Could not obtain a Coder for the accumulator", e);    }}
 static CombinePayload beam_f985_0(final AppliedPTransform<PCollection<InputT>, PCollection<OutputT>, Combine.Globally<InputT, OutputT>> transform, final SdkComponents components) throws IOException
{    GlobalCombineFn<?, ?, ?> combineFn = transform.getTransform().getFn();    Coder<?> accumulatorCoder = extractAccumulatorCoder(combineFn, (AppliedPTransform) transform);    return combinePayload(combineFn, accumulatorCoder, components);}
public FunctionSpec beam_f994_0(AppliedPTransform<?, ?, View.CreatePCollectionView<?, ?>> transform, SdkComponents components)
{    return FunctionSpec.newBuilder().setUrn(getUrn(transform.getTransform())).setPayload(ByteString.copyFrom(SerializableUtils.serializeToByteArray(transform.getTransform().getView()))).build();}
public Map<? extends Class<? extends PTransform>, ? extends TransformPayloadTranslator> beam_f995_0()
{    return Collections.singletonMap(View.CreatePCollectionView.class, new CreatePCollectionViewTranslator());}
public ExpansionApi.ExpansionResponse beam_f1004_0(ExpansionApi.ExpansionRequest request)
{    return service.expand(request);}
public void beam_f1005_0() throws Exception
{    channel.shutdown();}
private static Environment beam_f1014_0(String config)
{    try {        ProcessPayloadReferenceJSON payloadReferenceJSON = MAPPER.readValue(config, ProcessPayloadReferenceJSON.class);        return createProcessEnvironment(payloadReferenceJSON.getOs(), payloadReferenceJSON.getArch(), payloadReferenceJSON.getCommand(), payloadReferenceJSON.getEnv());    } catch (IOException e) {        throw new RuntimeException(String.format("Unable to parse process environment config: %s", config), e);    }}
private static Environment beam_f1015_0(String config)
{    return Environment.newBuilder().setUrn(ENVIRONMENT_EMBEDDED).setPayload(ByteString.copyFromUtf8(MoreObjects.firstNonNull(config, ""))).build();}
public String beam_f1024_0()
{    return arch;}
public String beam_f1025_0()
{    return command;}
public Map<String, ExpansionService.TransformProvider> beam_f1034_0()
{    ImmutableMap.Builder<String, ExpansionService.TransformProvider> builder = ImmutableMap.builder();    for (ExternalTransformRegistrar registrar : ServiceLoader.load(ExternalTransformRegistrar.class)) {        for (Map.Entry<String, Class<? extends ExternalTransformBuilder>> entry : registrar.knownBuilders().entrySet()) {            String urn = entry.getKey();            Class<? extends ExternalTransformBuilder> builderClass = entry.getValue();            builder.put(urn, spec -> {                try {                    ExternalTransforms.ExternalConfigurationPayload payload = ExternalTransforms.ExternalConfigurationPayload.parseFrom(spec.getPayload());                    return translate(payload, builderClass);                } catch (Exception e) {                    throw new RuntimeException(String.format("Failed to build transform %s from spec %s", urn, spec), e);                }            });        }    }    return builder.build();}
private static PTransform beam_f1035_0(ExternalTransforms.ExternalConfigurationPayload payload, Class<? extends ExternalTransformBuilder> builderClass) throws Exception
{    Preconditions.checkState(ExternalTransformBuilder.class.isAssignableFrom(builderClass), "Provided identifier %s is not an ExternalTransformBuilder.", builderClass.getName());    Object configObject = initConfiguration(builderClass);    populateConfiguration(configObject, payload);    return buildTransform(builderClass, configObject);}
private Map<String, TransformProvider> beam_f1044_0()
{    ImmutableMap.Builder<String, TransformProvider> registeredTransforms = ImmutableMap.builder();    for (ExpansionServiceRegistrar registrar : ServiceLoader.load(ExpansionServiceRegistrar.class)) {        registeredTransforms.putAll(registrar.knownTransforms());    }    return registeredTransforms.build();}
 ExpansionApi.ExpansionResponse beam_f1045_1(ExpansionApi.ExpansionRequest request)
{            Set<String> existingTransformIds = request.getComponents().getTransformsMap().keySet();    Pipeline pipeline = Pipeline.create();    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(request.getComponents()).withPipeline(pipeline);    Map<String, PCollection<?>> inputs = request.getTransform().getInputsMap().entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, input -> {        try {            return rehydratedComponents.getPCollection(input.getValue());        } catch (IOException exn) {            throw new RuntimeException(exn);        }    }));    if (!registeredTransforms.containsKey(request.getTransform().getSpec().getUrn())) {        throw new UnsupportedOperationException("Unknown urn: " + request.getTransform().getSpec().getUrn());    }    registeredTransforms.get(request.getTransform().getSpec().getUrn()).apply(pipeline, request.getTransform().getUniqueName(), request.getTransform().getSpec(), inputs);        SdkComponents sdkComponents = rehydratedComponents.getSdkComponents().withNewIdPrefix(request.getNamespace());    sdkComponents.registerEnvironment(Environments.JAVA_SDK_HARNESS_ENVIRONMENT);    pipeline.replaceAll(ImmutableList.of(JavaReadViaImpulse.boundedOverride()));    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(pipeline, sdkComponents);    String expandedTransformId = Iterables.getOnlyElement(pipelineProto.getRootTransformIdsList().stream().filter(id -> !existingTransformIds.contains(id)).collect(Collectors.toList()));    RunnerApi.Components components = pipelineProto.getComponents();    RunnerApi.PTransform expandedTransform = components.getTransformsOrThrow(expandedTransformId).toBuilder().setUniqueName(expandedTransformId).build();        return ExpansionApi.ExpansionResponse.newBuilder().setComponents(components.toBuilder().removeTransforms(expandedTransformId)).setTransform(expandedTransform).build();}
 String beam_f1055_0()
{    return String.format("External_%s", namespaceIndex);}
 String beam_f1056_0()
{    return IMPULSE_PREFIX;}
public String beam_f1065_0(External.ExpandableTransform transform)
{    return EXTERNAL_TRANSFORM_URN;}
public boolean beam_f1066_0(PTransform<?, ?> pTransform)
{    return pTransform instanceof External.ExpandableTransform;}
public String beam_f1075_0()
{    return delegate().getName();}
public void beam_f1076_0(DisplayData.Builder builder)
{    builder.delegate(delegate());}
private static boolean beam_f1085_0(PTransformNode parDo, PTransformNode other, QueryablePipeline pipeline)
{        return parDo.equals(other) ||     (pipeline.getSideInputs(parDo).isEmpty() &&     pipeline.getUserStates(parDo).isEmpty() &&     pipeline.getTimers(parDo).isEmpty() && compatibleEnvironments(parDo, other, pipeline));}
private static boolean beam_f1086_0(PTransformNode operation, Environment environment, @SuppressWarnings("unused") PCollectionNode candidate, @SuppressWarnings("unused") Collection<PCollectionNode> stagePCollections, QueryablePipeline pipeline)
{        Optional<Environment> operationEnvironment = pipeline.getEnvironment(operation);    return environment.equals(operationEnvironment.orElse(null));}
private DescendantConsumers beam_f1095_0(PTransformNode rootNode)
{    checkArgument(rootNode.getTransform().getInputsCount() == 0, "Transform %s is not at the root of the graph (consumes %s)", rootNode.getId(), rootNode.getTransform().getInputsMap());    checkArgument(!pipeline.getEnvironment(rootNode).isPresent(), "%s requires all root nodes to be runner-implemented %s or %s primitives, " + "but transform %s executes in environment %s", GreedyPipelineFuser.class.getSimpleName(), PTransformTranslation.IMPULSE_TRANSFORM_URN, PTransformTranslation.READ_TRANSFORM_URN, rootNode.getId(), pipeline.getEnvironment(rootNode));    Set<PTransformNode> unfused = new HashSet<>();    unfused.add(rootNode);    NavigableSet<CollectionConsumer> environmentNodes = new TreeSet<>();        for (PCollectionNode output : pipeline.getOutputPCollections(rootNode)) {        DescendantConsumers descendants = getDescendantConsumers(output);        unfused.addAll(descendants.getUnfusedNodes());        environmentNodes.addAll(descendants.getFusibleConsumers());    }    return DescendantConsumers.of(unfused, environmentNodes);}
private DescendantConsumers beam_f1096_1(PCollectionNode inputPCollection)
{    Set<PTransformNode> unfused = new HashSet<>();    NavigableSet<CollectionConsumer> downstreamConsumers = new TreeSet<>();    for (PTransformNode consumer : pipeline.getPerElementConsumers(inputPCollection)) {        if (pipeline.getEnvironment(consumer).isPresent()) {                        downstreamConsumers.add(CollectionConsumer.of(inputPCollection, consumer));        } else {                        unfused.add(consumer);            for (PCollectionNode output : pipeline.getOutputPCollections(consumer)) {                                DescendantConsumers descendants = getDescendantConsumers(output);                unfused.addAll(descendants.getUnfusedNodes());                downstreamConsumers.addAll(descendants.getFusibleConsumers());            }        }    }    return DescendantConsumers.of(unfused, downstreamConsumers);}
private static PCollectionFusibility beam_f1105_0(QueryablePipeline pipeline, PCollectionNode candidate, Environment environment, Set<PCollectionNode> fusedPCollections)
{    for (PTransformNode consumer : pipeline.getPerElementConsumers(candidate)) {        if (anyInputsSideInputs(consumer, pipeline) || !(GreedyPCollectionFusers.canFuse(consumer, environment, candidate, fusedPCollections, pipeline))) {                        return PCollectionFusibility.MATERIALIZE;        }    }        if (!pipeline.getSingletonConsumers(candidate).isEmpty()) {        return PCollectionFusibility.MATERIALIZE;    }    return PCollectionFusibility.FUSE;}
private static boolean beam_f1106_0(PTransformNode consumer, QueryablePipeline pipeline)
{    for (String inputPCollectionId : consumer.getTransform().getInputsMap().values()) {        RunnerApi.PCollection pCollection = pipeline.getComponents().getPcollectionsMap().get(inputPCollectionId);        PCollectionNode pCollectionNode = PipelineNode.pCollection(inputPCollectionId, pCollection);        if (!pipeline.getSingletonConsumers(pCollectionNode).isEmpty()) {            return true;        }    }    return false;}
public int beam_f1115_0(@Nonnull NodeT t0, @Nonnull NodeT t1)
{    return (network.outDegree(t0) - network.inDegree(t0)) - (network.outDegree(t1) - network.inDegree(t1));}
public static String beam_f1116_0(Network<NodeT, EdgeT> network)
{    StringBuilder builder = new StringBuilder();    builder.append(String.format("digraph network {%n"));    Map<NodeT, String> nodeName = Maps.newIdentityHashMap();    network.nodes().forEach(node -> nodeName.put(node, "n" + nodeName.size()));    for (Entry<NodeT, String> nodeEntry : nodeName.entrySet()) {        builder.append(String.format("  %s [fontname=\"Courier New\" label=\"%s\"];%n", nodeEntry.getValue(), escapeDot(nodeEntry.getKey().toString())));    }    for (EdgeT edge : network.edges()) {        EndpointPair<NodeT> endpoints = network.incidentNodes(edge);        builder.append(String.format("  %s -> %s [fontname=\"Courier New\" label=\"%s\"];%n", nodeName.get(endpoints.source()), nodeName.get(endpoints.target()), escapeDot(edge.toString())));    }    builder.append("}");    return builder.toString();}
private static StageDeduplication beam_f1125_0(ExecutableStage stage, Collection<PCollectionNode> duplicates, Predicate<String> existingPCollectionIds)
{    Map<String, PCollectionNode> unzippedOutputs = createPartialPCollections(duplicates, existingPCollectionIds);    ExecutableStage updatedStage = deduplicateStageOutput(stage, unzippedOutputs);    return StageDeduplication.of(updatedStage, unzippedOutputs);}
public static StageDeduplication beam_f1126_0(ExecutableStage updatedStage, Map<String, PCollectionNode> originalToPartial)
{    return new AutoValue_OutputDeduplicator_StageDeduplication(updatedStage, originalToPartial);}
private static RunnerApi.Pipeline beam_f1135_1(RunnerApi.Pipeline pipeline, Set<String> knownUrns)
{    RunnerApi.Pipeline.Builder trimmedPipeline = pipeline.toBuilder();    for (String ptransformId : pipeline.getComponents().getTransformsMap().keySet()) {        if (knownUrns.contains(pipeline.getComponents().getTransformsOrThrow(ptransformId).getSpec().getUrn())) {                        removeDescendants(trimmedPipeline, ptransformId);        }    }    return trimmedPipeline.build();}
private static void beam_f1136_0(RunnerApi.Pipeline.Builder pipeline, String parentId)
{    RunnerApi.PTransform parentProto = pipeline.getComponents().getTransformsOrDefault(parentId, null);    if (parentProto != null) {        for (String childId : parentProto.getSubtransformsList()) {            removeDescendants(pipeline, childId);            pipeline.getComponentsBuilder().removeTransforms(childId);        }        pipeline.getComponentsBuilder().putTransforms(parentId, parentProto.toBuilder().clearSubtransforms().build());    }}
public static Pipeline beam_f1145_0(String urn, Pipeline originalPipeline, TransformReplacement compositeBuilder)
{    Components.Builder resultComponents = originalPipeline.getComponents().toBuilder();    for (Map.Entry<String, PTransform> pt : originalPipeline.getComponents().getTransformsMap().entrySet()) {        if (pt.getValue().getSpec() != null && urn.equals(pt.getValue().getSpec().getUrn())) {            MessageWithComponents updated = compositeBuilder.getReplacement(pt.getKey(), originalPipeline.getComponents());            checkArgument(updated.getPtransform().getOutputsMap().equals(pt.getValue().getOutputsMap()), "A %s must produce all of the outputs of the original %s", TransformReplacement.class.getSimpleName(), PTransform.class.getSimpleName());            removeSubtransforms(pt.getValue(), resultComponents);            resultComponents.mergeFrom(updated.getComponents()).putTransforms(pt.getKey(), updated.getPtransform());        }    }    return originalPipeline.toBuilder().setComponents(resultComponents).build();}
private static void beam_f1146_0(PTransform pt, Components.Builder target)
{    for (String subtransformId : pt.getSubtransformsList()) {        PTransform subtransform = target.getTransformsOrThrow(subtransformId);        removeSubtransforms(subtransform, target);        target.removeTransforms(subtransformId);        }}
public Set<PTransformNode> beam_f1155_0()
{    return pipelineNetwork.nodes().stream().filter(pipelineNode -> pipelineNetwork.inEdges(pipelineNode).isEmpty()).map(pipelineNode -> (PTransformNode) pipelineNode).collect(Collectors.toSet());}
public PTransformNode beam_f1156_0(PCollectionNode pcollection)
{    return (PTransformNode) Iterables.getOnlyElement(pipelineNetwork.predecessors(pcollection));}
private Set<String> beam_f1165_0(PTransform transform)
{    if (PAR_DO_TRANSFORM_URN.equals(transform.getSpec().getUrn())) {        try {            return ParDoPayload.parseFrom(transform.getSpec().getPayload()).getSideInputsMap().keySet();        } catch (InvalidProtocolBufferException e) {            throw new RuntimeException(e);        }    } else {        return Collections.emptySet();    }}
private Set<String> beam_f1166_0(PTransform transform)
{    if (PAR_DO_TRANSFORM_URN.equals(transform.getSpec().getUrn())) {        try {            return ParDoPayload.parseFrom(transform.getSpec().getPayload()).getStateSpecsMap().keySet();        } catch (InvalidProtocolBufferException e) {            throw new RuntimeException(e);        }    } else {        return Collections.emptySet();    }}
public static TimerReference beam_f1175_0(RunnerApi.ExecutableStagePayload.TimerId timerId, RunnerApi.Components components)
{    String transformId = timerId.getTransformId();    String localName = timerId.getLocalName();    RunnerApi.PTransform transform = components.getTransformsOrThrow(transformId);    return of(PipelineNode.pTransform(transformId, transform), localName);}
public static UserStateReference beam_f1176_0(PTransformNode transform, String localName, PCollectionNode collection)
{    return new AutoValue_UserStateReference(transform, localName, collection);}
public static PTransformOverride beam_f1185_0()
{    return PTransformOverride.of(boundedMatcher(), new BoundedOverrideFactory<>());}
private static PTransformMatcher beam_f1186_0()
{    return PTransformMatchers.urnEqualTo(PTransformTranslation.READ_TRANSFORM_URN).and(transform -> ReadTranslation.sourceIsBounded(transform) == PCollection.IsBounded.BOUNDED);}
public Map<Class<? extends Coder>, String> beam_f1195_0()
{    return BEAM_MODEL_CODER_URNS;}
public Map<Class<? extends Coder>, CoderTranslator<? extends Coder>> beam_f1196_0()
{    return BEAM_MODEL_CODERS;}
public boolean beam_f1205_0(PTransform<?, ?> pTransform)
{    return pTransform instanceof ParDo.MultiOutput;}
public RunnerApi.PTransform beam_f1206_0(AppliedPTransform<?, ?, ?> appliedPTransform, List<AppliedPTransform<?, ?, ?>> subtransforms, SdkComponents components) throws IOException
{    RunnerApi.PTransform.Builder builder = PTransformTranslation.translateAppliedPTransform(appliedPTransform, subtransforms, components);    AppliedPTransform<?, ?, ParDo.MultiOutput<?, ?>> appliedParDo = (AppliedPTransform<?, ?, ParDo.MultiOutput<?, ?>>) appliedPTransform;    ParDoPayload payload = translateParDo(appliedParDo, components);    builder.setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn(PAR_DO_TRANSFORM_URN).setPayload(payload.toByteString()).build());    String mainInputName = getMainInputName(builder, payload);    PCollection<KV<?, ?>> mainInput = (PCollection) appliedPTransform.getInputs().get(new TupleTag(mainInputName));        for (String localTimerName : payload.getTimerSpecsMap().keySet()) {        PCollection<?> timerPCollection = PCollection.createPrimitiveOutputInternal(        Pipeline.create(), mainInput.getWindowingStrategy(), mainInput.isBounded(), KvCoder.of(((KvCoder) mainInput.getCoder()).getKeyCoder(),         Timer.Coder.of(VoidCoder.of())));        timerPCollection.setName(String.format("%s.%s", appliedPTransform.getFullName(), localTimerName));        String timerPCollectionId = components.registerPCollection(timerPCollection);        builder.putInputs(localTimerName, timerPCollectionId);        builder.putOutputs(localTimerName, timerPCollectionId);    }    return builder.build();}
public String beam_f1215_0(SdkComponents newComponents)
{    return restrictionCoderId;}
public static List<RunnerApi.Parameter> beam_f1216_0(List<Parameter> params)
{    List<RunnerApi.Parameter> parameters = new ArrayList<>();    for (Parameter parameter : params) {        RunnerApi.Parameter protoParameter = translateParameter(parameter);        if (protoParameter != null) {            parameters.add(protoParameter);        }    }    return parameters;}
public static Map<String, PCollectionView<?>> beam_f1225_0(ParDoPayload payload)
{    return doFnWithExecutionInformationFromProto(payload.getDoFn()).getSideInputMapping();}
public static TupleTag<?> beam_f1226_0(AppliedPTransform<?, ?, ?> application) throws IOException
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof ParDo.MultiOutput) {        return ((ParDo.MultiOutput<?, ?>) transform).getMainOutputTag();    }    return getMainOutputTag(getParDoPayload(application));}
public RunnerApi.StateSpec beam_f1235_0(Coder<?> elementCoder)
{    return builder.setBagSpec(RunnerApi.BagStateSpec.newBuilder().setElementCoderId(registerCoderOrThrow(components, elementCoder))).build();}
public RunnerApi.StateSpec beam_f1236_0(Combine.CombineFn<?, ?, ?> combineFn, Coder<?> accumCoder)
{    return builder.setCombiningSpec(RunnerApi.CombiningStateSpec.newBuilder().setAccumulatorCoderId(registerCoderOrThrow(components, accumCoder)).setCombineFn(CombineTranslation.toProto(combineFn, components))).build();}
public static RunnerApi.Parameter beam_f1245_0(Parameter parameter)
{    return parameter.match(new Cases.WithDefault</* @Nullable in Java 8 */    RunnerApi.Parameter>() {        @Override        public RunnerApi.Parameter dispatch(WindowParameter p) {            return RunnerApi.Parameter.newBuilder().setType(Type.Enum.WINDOW).build();        }        @Override        public RunnerApi.Parameter dispatch(RestrictionTrackerParameter p) {            return RunnerApi.Parameter.newBuilder().setType(Type.Enum.RESTRICTION_TRACKER).build();        }        @Override                @Nullable        protected RunnerApi.Parameter dispatchDefault(Parameter p) {            return null;        }    });}
public RunnerApi.Parameter beam_f1246_0(WindowParameter p)
{    return RunnerApi.Parameter.newBuilder().setType(Type.Enum.WINDOW).build();}
public static boolean beam_f1255_0(AppliedPTransform<?, ?, ?> transform) throws IOException
{    ParDoPayload payload = getParDoPayload(transform);    return payload.getSplittable();}
public static SdkFunctionSpec beam_f1256_0(WindowMappingFn<?> windowMappingFn, SdkComponents components)
{    return SdkFunctionSpec.newBuilder().setEnvironmentId(components.getOnlyEnvironmentId()).setSpec(FunctionSpec.newBuilder().setUrn(CUSTOM_JAVA_WINDOW_MAPPING_FN_URN).setPayload(ByteString.copyFrom(SerializableUtils.serializeToByteArray(windowMappingFn))).build()).build();}
public static WindowMappingFn<?> beam_f1265_0(RunnerApi.SdkFunctionSpec windowMappingFn) throws InvalidProtocolBufferException
{    RunnerApi.FunctionSpec spec = windowMappingFn.getSpec();    checkArgument(spec.getUrn().equals(ParDoTranslation.CUSTOM_JAVA_WINDOW_MAPPING_FN_URN), "Can't deserialize unknown %s type %s", WindowMappingFn.class.getSimpleName(), spec.getUrn());    return (WindowMappingFn<?>) SerializableUtils.deserializeFromByteArray(spec.getPayload().toByteArray(), "Custom WinodwMappingFn");}
public static Struct beam_f1266_0(PipelineOptions options)
{    Struct.Builder builder = Struct.newBuilder();    try {                TreeNode treeNode = MAPPER.valueToTree(options);        TreeNode rootOptions = treeNode.get("options");        Iterator<String> optionsKeys = rootOptions.fieldNames();        Map<String, TreeNode> optionsUsingUrns = new HashMap<>();        while (optionsKeys.hasNext()) {            String optionKey = optionsKeys.next();            TreeNode optionValue = rootOptions.get(optionKey);            optionsUsingUrns.put("beam:option:" + CaseFormat.LOWER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, optionKey) + ":v1", optionValue);        }                        JsonFormat.parser().merge(MAPPER.writeValueAsString(optionsUsingUrns), builder);        return builder.build();    } catch (IOException e) {        throw new RuntimeException("Failed to convert PipelineOptions to Protocol", e);    }}
private static void beam_f1275_0(File directoryToStage, String uniqueDirectoryPath)
{    try {        ZipFiles.zipDirectory(directoryToStage, new FileOutputStream(uniqueDirectoryPath));    } catch (IOException e) {        throw new RuntimeException(e);    }}
public static RunnerApi.Pipeline beam_f1276_0(Pipeline pipeline)
{    return toProto(pipeline, SdkComponents.create(pipeline.getOptions()));}
public PTransformReplacement<PBegin, PCollection<T>> beam_f1285_0(AppliedPTransform<PBegin, PCollection<T>, Values<T>> transform)
{    return PTransformReplacement.of(transform.getPipeline().begin(), new PrimitiveCreate<>(transform.getTransform(), ((PCollection<T>) Iterables.getOnlyElement(transform.getOutputs().values())).getCoder()));}
public Map<PValue, ReplacementOutput> beam_f1286_0(Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput)
{    return ReplacementOutputs.singleton(outputs, newOutput);}
public static PTransformMatcher beam_f1295_0()
{    return new PTransformMatcher() {        @Override        public boolean matches(AppliedPTransform<?, ?, ?> application) {            PTransform<?, ?> transform = application.getTransform();            if (transform instanceof ParDo.SingleOutput) {                DoFn<?, ?> fn = ((ParDo.SingleOutput<?, ?>) transform).getFn();                DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);                return signature.processElement().requiresStableInput();            }            return false;        }        @Override        public boolean matchesDuringValidation(AppliedPTransform<?, ?, ?> application) {            return false;        }        @Override        public String toString() {            return MoreObjects.toStringHelper("RequiresStableInputParDoSingleMatcher").toString();        }    };}
public boolean beam_f1296_0(AppliedPTransform<?, ?, ?> application)
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof ParDo.SingleOutput) {        DoFn<?, ?> fn = ((ParDo.SingleOutput<?, ?>) transform).getFn();        DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);        return signature.processElement().requiresStableInput();    }    return false;}
public String beam_f1305_0()
{    return MoreObjects.toStringHelper("SplittableParDoSingleMatcher").toString();}
public static PTransformMatcher beam_f1306_0()
{    return new PTransformMatcher() {        @Override        public boolean matches(AppliedPTransform<?, ?, ?> application) {            PTransform<?, ?> transform = application.getTransform();            if (transform instanceof ParDo.SingleOutput) {                DoFn<?, ?> fn = ((ParDo.SingleOutput<?, ?>) transform).getFn();                DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);                return signature.usesState() || signature.usesTimers();            }            return false;        }        @Override        public String toString() {            return MoreObjects.toStringHelper("StateOrTimerParDoSingleMatcher").toString();        }    };}
public static PTransformMatcher beam_f1315_0()
{    return new PTransformMatcher() {        @Override        public boolean matches(AppliedPTransform<?, ?, ?> application) {            PTransform<?, ?> transform = application.getTransform();            if (transform instanceof SplittableParDo.ProcessKeyedElements) {                DoFn<?, ?> fn = ((SplittableParDo.ProcessKeyedElements) transform).getFn();                DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);                return signature.processElement().isSplittable() && signature.isBoundedPerElement() == IsBounded.BOUNDED;            }            return false;        }        @Override        public String toString() {            return MoreObjects.toStringHelper("SplittableProcessKeyedBoundedMatcher").toString();        }    };}
public boolean beam_f1316_0(AppliedPTransform<?, ?, ?> application)
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof SplittableParDo.ProcessKeyedElements) {        DoFn<?, ?> fn = ((SplittableParDo.ProcessKeyedElements) transform).getFn();        DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);        return signature.processElement().isSplittable() && signature.isBoundedPerElement() == IsBounded.BOUNDED;    }    return false;}
public boolean beam_f1325_0(AppliedPTransform<?, ?, ?> application)
{    PTransform<?, ?> transform = application.getTransform();    if (transform instanceof ParDo.MultiOutput) {        DoFn<?, ?> fn = ((ParDo.MultiOutput<?, ?>) transform).getFn();        DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);        return signature.usesState() || signature.usesTimers();    }    return false;}
public String beam_f1326_0()
{    return MoreObjects.toStringHelper("StateOrTimerParDoMultiMatcher").toString();}
public boolean beam_f1335_0(AppliedPTransform<?, ?, ?> application)
{    if (application.getTransform() instanceof Flatten.PCollections) {        Set<PValue> observed = new HashSet<>();        for (PValue pvalue : application.getInputs().values()) {            boolean firstInstance = observed.add(pvalue);            if (!firstInstance) {                return true;            }        }    }    return false;}
public String beam_f1336_0()
{    return MoreObjects.toStringHelper("FlattenWithDuplicateInputsMatcher").toString();}
public static String beam_f1345_0(PTransform<?, ?> transform)
{    TransformTranslator<?> transformTranslator = Iterables.find(KNOWN_TRANSLATORS, translator -> translator.canTranslate(transform), DefaultUnknownTransformTranslator.INSTANCE);    return ((TransformTranslator) transformTranslator).getUrn(transform);}
public static String beam_f1346_0(PTransform<?, ?> transform)
{    String urn = urnForTransformOrNull(transform);    if (urn == null) {        throw new IllegalStateException(String.format("No translator known for %s", transform.getClass().getName()));    }    return urn;}
public boolean beam_f1355_0(PTransform pTransform)
{    return KNOWN_PAYLOAD_TRANSLATORS.containsKey(pTransform.getClass());}
public String beam_f1356_0(PTransform transform)
{    return KNOWN_PAYLOAD_TRANSLATORS.get(transform.getClass()).getUrn(transform);}
public static ReadPayload beam_f1365_0(Read.Bounded<?> read, SdkComponents components)
{    return ReadPayload.newBuilder().setIsBounded(IsBounded.Enum.BOUNDED).setSource(toProto(read.getSource(), components)).build();}
public static ReadPayload beam_f1366_0(Unbounded<?> read, SdkComponents components)
{    return ReadPayload.newBuilder().setIsBounded(IsBounded.Enum.UNBOUNDED).setSource(toProto(read.getSource(), components)).build();}
public static PCollection.IsBounded beam_f1375_0(AppliedPTransform<?, ?, ?> transform)
{    try {        SdkComponents components = SdkComponents.create(transform.getPipeline().getOptions());        return PCollectionTranslation.fromProto(ReadPayload.parseFrom(PTransformTranslation.toProto(transform, Collections.emptyList(), components).getSpec().getPayload()).getIsBounded());    } catch (IOException e) {        throw new RuntimeException("Internal error determining boundedness of Read", e);    }}
public static TransformPayloadTranslator beam_f1376_0()
{    return new UnboundedReadPayloadTranslator();}
public PCollection<?> beam_f1385_0(String id) throws Exception
{    checkState(pipeline != null, "%s Cannot rehydrate %s without a %s:" + " provide one via .withPipeline(...)", RehydratedComponents.class.getSimpleName(), PCollection.class.getSimpleName(), Pipeline.class.getSimpleName());    return PCollectionTranslation.fromProto(components.getPcollectionsOrThrow(id), pipeline, RehydratedComponents.this).setName(id);}
public static RehydratedComponents beam_f1386_0(RunnerApi.Components components)
{    return new RehydratedComponents(components, null);}
public static String beam_f1395_0(RunnerApi.Pipeline pipeline)
{    return PortablePipelineDotRenderer.toDotString(pipeline);}
public CompositeBehavior beam_f1398_0(TransformHierarchy.Node node)
{    writeLine("subgraph cluster_%d {", nextNodeId++);    enterBlock();    writeLine("label = \"%s\"", escapeString(node.getFullName()));    return CompositeBehavior.ENTER_TRANSFORM;}
private static String beam_f1408_0(String tag)
{    return tag.replaceFirst(".*:([a-zA-Z#0-9]+).*", "$1");}
 static String beam_f1409_0(RunnerApi.Pipeline pipeline)
{    return new PortablePipelineDotRenderer().toDot(pipeline);}
public static Map<PValue, ReplacementOutput> beam_f1418_0(Map<TupleTag<?>, PValue> original, PValue replacement)
{    Entry<TupleTag<?>, PValue> originalElement = Iterables.getOnlyElement(original.entrySet());    TupleTag<?> replacementTag = Iterables.getOnlyElement(replacement.expand().entrySet()).getKey();    return Collections.singletonMap(replacement, ReplacementOutput.of(TaggedPValue.of(originalElement.getKey(), originalElement.getValue()), TaggedPValue.of(replacementTag, replacement)));}
public static Map<PValue, ReplacementOutput> beam_f1419_0(Map<TupleTag<?>, PValue> original, POutput replacement)
{    Map<TupleTag<?>, TaggedPValue> originalTags = new HashMap<>();    for (Map.Entry<TupleTag<?>, PValue> originalValue : original.entrySet()) {        originalTags.put(originalValue.getKey(), TaggedPValue.of(originalValue.getKey(), originalValue.getValue()));    }    ImmutableMap.Builder<PValue, ReplacementOutput> resultBuilder = ImmutableMap.builder();    Set<TupleTag<?>> missingTags = new HashSet<>(originalTags.keySet());    for (Map.Entry<TupleTag<?>, PValue> replacementValue : replacement.expand().entrySet()) {        TaggedPValue mapped = originalTags.get(replacementValue.getKey());        checkArgument(mapped != null, "Missing original output for Tag %s and Value %s Between original %s and replacement %s", replacementValue.getKey(), replacementValue.getValue(), original, replacement.expand());        resultBuilder.put(replacementValue.getValue(), ReplacementOutput.of(mapped, TaggedPValue.of(replacementValue.getKey(), replacementValue.getValue())));        missingTags.remove(replacementValue.getKey());    }    ImmutableMap<PValue, ReplacementOutput> result = resultBuilder.build();    checkArgument(missingTags.isEmpty(), "Missing replacement for tags %s. Encountered tags: %s", missingTags, result.keySet());    return result;}
public Coder<?> beam_f1428_0()
{    return coder;}
public Map<TupleTag<?>, PValue> beam_f1429_0()
{    throw new UnsupportedOperationException(String.format("A %s cannot be expanded", RunnerPCollectionView.class.getSimpleName()));}
private static FieldType beam_f1438_0(SchemaApi.FieldType protoFieldType)
{    switch(protoFieldType.getTypeInfoCase()) {        case ATOMIC_TYPE:            switch(protoFieldType.getAtomicType()) {                case BYTE:                    return FieldType.of(TypeName.BYTE);                case INT16:                    return FieldType.of(TypeName.INT16);                case INT32:                    return FieldType.of(TypeName.INT32);                case INT64:                    return FieldType.of(TypeName.INT64);                case FLOAT:                    return FieldType.of(TypeName.FLOAT);                case DOUBLE:                    return FieldType.of(TypeName.DOUBLE);                case STRING:                    return FieldType.of(TypeName.STRING);                case BOOLEAN:                    return FieldType.of(TypeName.BOOLEAN);                case BYTES:                    return FieldType.of(TypeName.BYTES);                case UNSPECIFIED:                    throw new IllegalArgumentException("Encountered UNSPECIFIED AtomicType");                default:                    throw new IllegalArgumentException("Encountered unknown AtomicType: " + protoFieldType.getAtomicType());            }        case ROW_TYPE:            return FieldType.row(fromProto(protoFieldType.getRowType().getSchema()));        case ARRAY_TYPE:            return FieldType.array(fieldTypeFromProto(protoFieldType.getArrayType().getElementType()));        case MAP_TYPE:            return FieldType.map(fieldTypeFromProto(protoFieldType.getMapType().getKeyType()), fieldTypeFromProto(protoFieldType.getMapType().getValueType()));        case LOGICAL_TYPE:                                    String urn = protoFieldType.getLogicalType().getUrn();            if (urn.equals(URN_BEAM_LOGICAL_DATETIME)) {                return FieldType.DATETIME;            } else if (urn.equals(URN_BEAM_LOGICAL_DECIMAL)) {                return FieldType.DECIMAL;            } else if (urn.equals(URN_BEAM_LOGICAL_JAVASDK)) {                return FieldType.logicalType((LogicalType) SerializableUtils.deserializeFromByteArray(protoFieldType.getLogicalType().getPayload().toByteArray(), "logicalType"));            } else {                throw new IllegalArgumentException("Encountered unsupported logical type URN: " + urn);            }        default:            throw new IllegalArgumentException("Unexpected type_info: " + protoFieldType.getTypeInfoCase());    }}
public static SdkComponents beam_f1439_0()
{    return new SdkComponents(RunnerApi.Components.getDefaultInstance(), "");}
public String beam_f1448_0(AppliedPTransform<?, ?, ?> appliedPTransform)
{    String existing = transformIds.get(appliedPTransform);    checkArgument(existing != null, "PTransform id not found for: %s", appliedPTransform);    return existing;}
public String beam_f1449_0(PCollection<?> pCollection) throws IOException
{    String existing = pCollectionIds.get(pCollection);    if (existing != null) {        return existing;    }    String uniqueName = uniqify(pCollection.getName(), pCollectionIds.values());    pCollectionIds.put(pCollection, uniqueName);    componentsBuilder.putPcollections(uniqueName, PCollectionTranslation.toProto(pCollection, this));    return uniqueName;}
private static String beam_f1458_0(PipelineOptions options)
{    try {        return MAPPER.writeValueAsString(options);    } catch (JsonProcessingException e) {        throw new IllegalArgumentException("Failed to serialize PipelineOptions", e);    }}
private static PipelineOptions beam_f1459_0(String options)
{    try {        return MAPPER.readValue(options, PipelineOptions.class);    } catch (IOException e) {        throw new IllegalArgumentException("Failed to deserialize PipelineOptions", e);    }}
public Map<TupleTag<?>, PValue> beam_f1468_0()
{    return PCollectionViews.toAdditionalInputs(sideInputs);}
public void beam_f1469_0(ProcessContext c, BoundedWindow window)
{    c.output(c.element());}
public PCollectionTuple beam_f1478_0(PCollection<KV<byte[], KV<InputT, RestrictionT>>> input)
{    return createPrimitiveOutputFor(input, fn, mainOutputTag, additionalOutputTags, outputTagsToCoders, windowingStrategy);}
public static PCollectionTuple beam_f1479_0(PCollection<?> input, DoFn<?, OutputT> fn, TupleTag<OutputT> mainOutputTag, TupleTagList additionalOutputTags, Map<TupleTag<?>, Coder<?>> outputTagsToCoders, WindowingStrategy<?, ?> windowingStrategy)
{    DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());    PCollectionTuple outputs = PCollectionTuple.ofPrimitiveOutputsInternal(input.getPipeline(), TupleTagList.of(mainOutputTag).and(additionalOutputTags.getAll()), outputTagsToCoders, windowingStrategy, input.isBounded().and(signature.isBoundedPerElement()));        outputs.get(mainOutputTag).setTypeDescriptor(fn.getOutputTypeDescriptor());    return outputs;}
public Map<String, StateSpec> beam_f1488_0(SdkComponents components)
{        return ImmutableMap.of();}
public Map<String, TimerSpec> beam_f1489_0(SdkComponents components)
{        return ImmutableMap.of();}
public void beam_f1498_0(RestrictionT part)
{    c.output(KV.of(element, part));}
public void beam_f1499_0(RestrictionT part, Instant timestamp)
{    throw new UnsupportedOperationException();}
public void beam_f1508_0(FinishBundleContext c)
{    invoker.invokeFinishBundle(new DoFn<InputT, OutputT>.FinishBundleContext() {        @Override        public PipelineOptions getPipelineOptions() {            return c.getPipelineOptions();        }        @Override        public void output(@Nullable OutputT output, Instant timestamp, BoundedWindow window) {            throw new UnsupportedOperationException("Output from FinishBundle for SDF is not supported");        }        @Override        public <T> void output(TupleTag<T> tag, T output, Instant timestamp, BoundedWindow window) {            throw new UnsupportedOperationException("Output from FinishBundle for SDF is not supported");        }    });}
public PipelineOptions beam_f1509_0()
{    return c.getPipelineOptions();}
public InputT beam_f1518_0(DoFn<InputT, OutputT> doFn)
{    return element;}
public Object beam_f1519_0(String tagId)
{    throw new UnsupportedOperationException();}
public void beam_f1528_0(T output, Instant timestamp)
{    outerContext.outputWithTimestamp(tag, output, timestamp);}
public OutputReceiver<Row> beam_f1529_0(TupleTag<T> tag)
{    throw new UnsupportedOperationException();}
public Instant beam_f1538_0()
{    return outerContext.timestamp();}
public PaneInfo beam_f1539_0()
{    return outerContext.pane();}
public static TestStream<T> beam_f1549_0(AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> application) throws IOException
{                    SdkComponents sdkComponents = SdkComponents.create(application.getPipeline().getOptions());    RunnerApi.PTransform transformProto = PTransformTranslation.toProto(application, sdkComponents);    checkArgument(TEST_STREAM_TRANSFORM_URN.equals(transformProto.getSpec().getUrn()), "Attempt to get %s from a transform with wrong URN %s", TestStream.class.getSimpleName(), transformProto.getSpec().getUrn());    RunnerApi.TestStreamPayload testStreamPayload = RunnerApi.TestStreamPayload.parseFrom(transformProto.getSpec().getPayload());    return (TestStream<T>) testStreamFromProtoPayload(testStreamPayload, RehydratedComponents.forComponents(sdkComponents.toComponents()));}
 static RunnerApi.TestStreamPayload.Event beam_f1550_0(TestStream.Event<T> event, Coder<T> coder) throws IOException
{    switch(event.getType()) {        case WATERMARK:            return RunnerApi.TestStreamPayload.Event.newBuilder().setWatermarkEvent(RunnerApi.TestStreamPayload.Event.AdvanceWatermark.newBuilder().setNewWatermark(((TestStream.WatermarkEvent<T>) event).getWatermark().getMillis())).build();        case PROCESSING_TIME:            return RunnerApi.TestStreamPayload.Event.newBuilder().setProcessingTimeEvent(RunnerApi.TestStreamPayload.Event.AdvanceProcessingTime.newBuilder().setAdvanceDuration(((TestStream.ProcessingTimeEvent<T>) event).getProcessingTimeAdvance().getMillis())).build();        case ELEMENT:            RunnerApi.TestStreamPayload.Event.AddElements.Builder builder = RunnerApi.TestStreamPayload.Event.AddElements.newBuilder();            for (TimestampedValue<T> element : ((TestStream.ElementEvent<T>) event).getElements()) {                builder.addElements(RunnerApi.TestStreamPayload.TimestampedElement.newBuilder().setTimestamp(element.getTimestamp().getMillis()).setEncodedElement(ByteString.copyFrom(CoderUtils.encodeToByteArray(coder, element.getValue()))));            }            return RunnerApi.TestStreamPayload.Event.newBuilder().setElementEvent(builder).build();        default:            throw new IllegalArgumentException(String.format("Unsupported type of %s: %s", TestStream.Event.class.getCanonicalName(), event.getType()));    }}
public static Coder beam_f1559_0(org.apache.beam.sdk.coders.Coder<T> payloadCoder)
{    return new Coder(payloadCoder);}
public void beam_f1560_0(Timer<T> timer, OutputStream outStream) throws CoderException, IOException
{    InstantCoder.of().encode(timer.getTimestamp(), outStream);    payloadCoder.encode(timer.getPayload(), outStream);}
public static RunnerApi.Trigger beam_f1569_0(Trigger trigger)
{    return CONVERTER.convertTrigger(trigger);}
public RunnerApi.Trigger beam_f1570_0(Trigger trigger)
{    Method evaluationMethod = getEvaluationMethod(trigger.getClass());    return tryConvert(evaluationMethod, trigger);}
private RunnerApi.Trigger beam_f1579_0(AfterFirst v)
{    RunnerApi.Trigger.AfterAny.Builder builder = RunnerApi.Trigger.AfterAny.newBuilder();    for (Trigger subtrigger : v.subTriggers()) {        builder.addSubtriggers(toProto(subtrigger));    }    return RunnerApi.Trigger.newBuilder().setAfterAny(builder).build();}
private RunnerApi.Trigger beam_f1580_0(AfterAll v)
{    RunnerApi.Trigger.AfterAll.Builder builder = RunnerApi.Trigger.AfterAll.newBuilder();    for (Trigger subtrigger : v.subTriggers()) {        builder.addSubtriggers(toProto(subtrigger));    }    return RunnerApi.Trigger.newBuilder().setAfterAll(builder).build();}
private static List<Trigger> beam_f1589_0(List<RunnerApi.Trigger> triggers)
{    List<Trigger> result = Lists.newArrayList();    for (RunnerApi.Trigger trigger : triggers) {        result.add(fromProto(trigger));    }    return result;}
public PCollection<T> beam_f1590_0(PBegin input)
{    return input.getPipeline().apply(Read.from(new BoundedToUnboundedSourceAdapter<>(source)));}
 List<TimestampedValue<T>> beam_f1600_0()
{    return residualElements;}
 BoundedSource<T> beam_f1601_0()
{    return residualSource;}
public T beam_f1610_0() throws NoSuchElementException
{    if (residualElements.hasCurrent()) {        return residualElements.getCurrent();    } else if (residualSource != null) {        return residualSource.getCurrent();    } else {        throw new NoSuchElementException();    }}
public Instant beam_f1611_0() throws NoSuchElementException
{    if (residualElements.hasCurrent()) {        return residualElements.getCurrentTimestamp();    } else if (residualSource != null) {        return residualSource.getCurrentTimestamp();    } else {        throw new NoSuchElementException();    }}
 Instant beam_f1620_0()
{    return getCurrentTimestampedValue().getTimestamp();}
 List<TimestampedValue<T>> beam_f1621_0()
{    if (elementsIterator == null) {        return elementsList;    } else {        List<TimestampedValue<T>> newResidualElements = Lists.newArrayList();        while (elementsIterator.hasNext()) {            newResidualElements.add(elementsIterator.next());        }        return newResidualElements;    }}
public void beam_f1630_0(PValue value, Node producer)
{    String urn = PTransformTranslation.urnForTransformOrNull(producer.getTransform());    if (PTransformTranslation.READ_TRANSFORM_URN.equals(urn)) {        unconsumed.add((PCollection<?>) value);    }}
private static void beam_f1631_0(PCollection<T> unconsumedPCollection, int uniq)
{        String uniqueName = "DropInputs" + (uniq == 0 ? "" : uniq);    unconsumedPCollection.apply(uniqueName, ParDo.of(new NoOpDoFn<>()));}
public static OnTimeBehavior beam_f1641_0(RunnerApi.OnTimeBehavior.Enum proto)
{    switch(proto) {        case FIRE_ALWAYS:            return OnTimeBehavior.FIRE_ALWAYS;        case FIRE_IF_NONEMPTY:            return OnTimeBehavior.FIRE_IF_NON_EMPTY;        case UNRECOGNIZED:        default:                        throw new IllegalArgumentException(String.format("Cannot convert unknown %s to %s: %s", RunnerApi.OnTimeBehavior.class.getCanonicalName(), OnTimeBehavior.class.getCanonicalName(), proto));    }}
public static RunnerApi.OutputTime.Enum beam_f1642_0(TimestampCombiner timestampCombiner)
{    switch(timestampCombiner) {        case EARLIEST:            return OutputTime.Enum.EARLIEST_IN_PANE;        case END_OF_WINDOW:            return OutputTime.Enum.END_OF_WINDOW;        case LATEST:            return OutputTime.Enum.LATEST_IN_PANE;        default:            throw new IllegalArgumentException(String.format("Unknown %s: %s", TimestampCombiner.class.getSimpleName(), timestampCombiner));    }}
public FunctionSpec beam_f1651_0(AppliedPTransform<?, ?, Window.Assign<?>> transform, SdkComponents components)
{    return FunctionSpec.newBuilder().setUrn("beam:transform:window:v1").setPayload(WindowIntoTranslation.toProto(transform.getTransform(), components).toByteString()).build();}
public static WindowIntoPayload beam_f1652_0(Window.Assign<?> transform, SdkComponents components)
{    return WindowIntoPayload.newBuilder().setWindowFn(WindowingStrategyTranslation.toProto(transform.getWindowFn(), components)).build();}
public Map<String, SideInput> beam_f1661_0(SdkComponents components)
{    Map<String, SideInput> sideInputs = new HashMap<>();    for (PCollectionView<?> view : transform.getSink().getDynamicDestinations().getSideInputs()) {        sideInputs.put(view.getTagInternal().getId(), ParDoTranslation.translateView(view, components));    }    return sideInputs;}
public boolean beam_f1662_0()
{    return transform.getWindowedWrites();}
private static WriteFilesPayload beam_f1671_0(AppliedPTransform<PCollection<T>, WriteFilesResult<DestinationT>, ? extends PTransform<PCollection<T>, WriteFilesResult<DestinationT>>> transform) throws IOException
{    SdkComponents components = SdkComponents.create(transform.getPipeline().getOptions());    return WriteFilesPayload.parseFrom(PTransformTranslation.toProto(transform, Collections.emptyList(), components).getSpec().getPayload());}
public FunctionSpec beam_f1672_0()
{    return spec;}
public Map<Class<? extends PTransform>, TransformPayloadTranslator> beam_f1681_0()
{    return Collections.singletonMap(WriteFiles.CONCRETE_CLASS, new WriteFilesTranslator());}
public static WriteFilesPayload beam_f1682_0(WriteFilesLike writeFiles, SdkComponents components) throws IOException
{    return WriteFilesPayload.newBuilder().setSink(writeFiles.translateSink(components)).putAllSideInputs(writeFiles.translateSideInputs(components)).setWindowedWrites(writeFiles.isWindowedWrites()).setRunnerDeterminedSharding(writeFiles.isRunnerDeterminedSharding()).build();}
public Record beam_f1692_0(InputStream inStream) throws CoderException, IOException
{    return new Record();}
public static Iterable<Combine.CombineFn<Integer, ?, ?>> beam_f1693_0()
{    BinaryCombineIntegerFn sum = Sum.ofIntegers();    CombineFn<Integer, ?, Long> count = Count.combineFn();    TestCombineFn test = new TestCombineFn();    return ImmutableList.<CombineFn<Integer, ?, ?>>builder().add(sum).add(count).add(test).build();}
public Void beam_f1702_0()
{    return null;}
public Coder<Void> beam_f1703_0(CoderRegistry registry, Coder<Integer> inputCoder)
{    return (Coder) VoidCoder.of();}
public Integer beam_f1712_0(int[] accumulator, Context c)
{    return accumulator[0];}
public boolean beam_f1713_0(Object other)
{    return other instanceof TestCombineFnWithContext;}
private static Object beam_f1722_0(Object value, CommonCoder coderSpec, Coder coder)
{    String s = coderSpec.getUrn();    if (s.equals(getUrn(StandardCoders.Enum.BYTES))) {        return ((String) value).getBytes(StandardCharsets.ISO_8859_1);    } else if (s.equals(getUrn(StandardCoders.Enum.STRING_UTF8))) {        return value;    } else if (s.equals(getUrn(StandardCoders.Enum.KV))) {        Coder keyCoder = ((KvCoder) coder).getKeyCoder();        Coder valueCoder = ((KvCoder) coder).getValueCoder();        Map<String, Object> kvMap = (Map<String, Object>) value;        Object k = convertValue(kvMap.get("key"), coderSpec.getComponents().get(0), keyCoder);        Object v = convertValue(kvMap.get("value"), coderSpec.getComponents().get(1), valueCoder);        return KV.of(k, v);    } else if (s.equals(getUrn(StandardCoders.Enum.VARINT))) {        return ((Number) value).longValue();    } else if (s.equals(getUrn(StandardCoders.Enum.TIMER))) {        Map<String, Object> kvMap = (Map<String, Object>) value;        Coder<?> payloadCoder = (Coder) coder.getCoderArguments().get(0);        return Timer.of(new Instant(((Number) kvMap.get("timestamp")).longValue()), convertValue(kvMap.get("payload"), coderSpec.getComponents().get(0), payloadCoder));    } else if (s.equals(getUrn(StandardCoders.Enum.INTERVAL_WINDOW))) {        Map<String, Object> kvMap = (Map<String, Object>) value;        Instant end = new Instant(((Number) kvMap.get("end")).longValue());        Duration span = Duration.millis(((Number) kvMap.get("span")).longValue());        return new IntervalWindow(end.minus(span), span);    } else if (s.equals(getUrn(StandardCoders.Enum.ITERABLE))) {        Coder elementCoder = ((IterableCoder) coder).getElemCoder();        List<Object> elements = (List<Object>) value;        List<Object> convertedElements = new ArrayList<>();        for (Object element : elements) {            convertedElements.add(convertValue(element, coderSpec.getComponents().get(0), elementCoder));        }        return convertedElements;    } else if (s.equals(getUrn(StandardCoders.Enum.GLOBAL_WINDOW))) {        return GlobalWindow.INSTANCE;    } else if (s.equals(getUrn(StandardCoders.Enum.WINDOWED_VALUE))) {        Map<String, Object> kvMap = (Map<String, Object>) value;        Coder valueCoder = ((WindowedValue.FullWindowedValueCoder) coder).getValueCoder();        Coder windowCoder = ((WindowedValue.FullWindowedValueCoder) coder).getWindowCoder();        Object windowValue = convertValue(kvMap.get("value"), coderSpec.getComponents().get(0), valueCoder);        Instant timestamp = new Instant(((Number) kvMap.get("timestamp")).longValue());        List<BoundedWindow> windows = new ArrayList<>();        for (Object window : (List<Object>) kvMap.get("windows")) {            windows.add((BoundedWindow) convertValue(window, coderSpec.getComponents().get(1), windowCoder));        }        Map<String, Object> paneInfoMap = (Map<String, Object>) kvMap.get("pane");        PaneInfo paneInfo = PaneInfo.createPane((boolean) paneInfoMap.get("is_first"), (boolean) paneInfoMap.get("is_last"), PaneInfo.Timing.valueOf((String) paneInfoMap.get("timing")), (int) paneInfoMap.get("index"), (int) paneInfoMap.get("on_time_index"));        return WindowedValue.of(windowValue, timestamp, windows, paneInfo);    } else if (s.equals(getUrn(StandardCoders.Enum.DOUBLE))) {        return Double.parseDouble((String) value);    } else {        throw new IllegalStateException("Unknown coder URN: " + coderSpec.getUrn());    }}
private static Coder<?> beam_f1723_0(CommonCoder coder)
{    List<Coder<?>> components = new ArrayList<>();    for (CommonCoder innerCoder : coder.getComponents()) {        components.add(instantiateCoder(innerCoder));    }    String s = coder.getUrn();    if (s.equals(getUrn(StandardCoders.Enum.BYTES))) {        return ByteArrayCoder.of();    } else if (s.equals(getUrn(StandardCoders.Enum.STRING_UTF8))) {        return StringUtf8Coder.of();    } else if (s.equals(getUrn(StandardCoders.Enum.KV))) {        return KvCoder.of(components.get(0), components.get(1));    } else if (s.equals(getUrn(StandardCoders.Enum.VARINT))) {        return VarLongCoder.of();    } else if (s.equals(getUrn(StandardCoders.Enum.INTERVAL_WINDOW))) {        return IntervalWindowCoder.of();    } else if (s.equals(getUrn(StandardCoders.Enum.ITERABLE))) {        return IterableCoder.of(components.get(0));    } else if (s.equals(getUrn(StandardCoders.Enum.TIMER))) {        return Timer.Coder.of(components.get(0));    } else if (s.equals(getUrn(StandardCoders.Enum.GLOBAL_WINDOW))) {        return GlobalWindow.Coder.INSTANCE;    } else if (s.equals(getUrn(StandardCoders.Enum.WINDOWED_VALUE))) {        return WindowedValue.FullWindowedValueCoder.of(components.get(0), (Coder<BoundedWindow>) components.get(1));    } else if (s.equals(getUrn(StandardCoders.Enum.DOUBLE))) {        return DoubleCoder.of();    } else {        throw new IllegalStateException("Unknown coder URN: " + coder.getUrn());    }}
public void beam_f1732_0()
{    final PCollectionList<String> inputList = PCollectionList.of(first).and(second).and(first).and(first);    PTransform<PCollectionList<String>, PCollection<String>> replacement = new FlattenWithoutDuplicateInputs<>();    PCollection<String> flattened = inputList.apply(replacement);    PAssert.that(flattened).containsInAnyOrder("one", "two", "one", "one");    pipeline.run();}
public void beam_f1733_0()
{    final PCollectionList<String> inputList = PCollectionList.of(first).and(second).and(first).and(first);    PCollection<String> original = inputList.apply(Flatten.pCollections());    PCollection<String> replacement = inputList.apply(new FlattenWithoutDuplicateInputs<>());    assertThat(factory.mapOutputs(original.expand(), replacement), Matchers.hasEntry(replacement, ReplacementOutput.of(TaggedPValue.ofExpandedValue(original), TaggedPValue.ofExpandedValue(replacement))));}
public void beam_f1743_0() throws IOException
{    SdkComponents components = SdkComponents.create();    components.registerEnvironment(Environments.createDockerEnvironment("java"));    WindowIntoPayload payload = WindowIntoPayload.newBuilder().setWindowFn(WindowingStrategyTranslation.toProto(new PartitioningWindowFn<Object, BoundedWindow>() {        @Override        public BoundedWindow assignWindow(Instant timestamp) {            return null;        }        @Override        public boolean isCompatible(WindowFn<?, ?> other) {            return false;        }        @Override        public Coder<BoundedWindow> windowCoder() {            return null;        }    }, components)).build();    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(components.toComponents());    PTransform builder = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(payload.toByteString()).build()).build();    Environment env = Environments.getEnvironment(builder, rehydratedComponents).get();    assertThat(env, equalTo(components.toComponents().getEnvironmentsOrThrow(payload.getWindowFn().getEnvironmentId())));}
public BoundedWindow beam_f1744_0(Instant timestamp)
{    return null;}
public void beam_f1756_0() throws Exception
{    ExpansionServer expansionServer;    try (ExpansionServer expServer = ExpansionServer.create(new ExpansionService(), "localhost", 0)) {        expansionServer = expServer;    }    assertThat(expansionServer.getHost(), is("localhost"));    assertThat(expansionServer.getPort(), greaterThan(0));}
public Map<String, ExpansionService.TransformProvider> beam_f1757_0()
{    return ImmutableMap.of(TEST_URN, spec -> Count.perElement());}
public static void beam_f1766_0() throws IOException
{    expansionPort = Integer.valueOf(System.getProperty("expansionPort"));    int localExpansionPort = expansionPort + 100;    localExpansionAddr = String.format("localhost:%s", localExpansionPort);    localExpansionServer = ServerBuilder.forPort(localExpansionPort).addService(new ExpansionService()).build();    localExpansionServer.start();}
public static void beam_f1767_0()
{    localExpansionServer.shutdownNow();}
public void beam_f1776_0()
{    @SuppressWarnings("unchecked")    PCollection<Integer> collection = mock(PCollection.class);    @SuppressWarnings("unchecked")    PCollection<String> output = mock(PCollection.class);    when(delegate.expand(collection)).thenReturn(output);    PCollection<String> result = forwarding.expand(collection);    assertThat(result, equalTo(output));}
public void beam_f1777_0()
{    String name = "My_forwardingptransform-name;for!thisTest";    when(delegate.getName()).thenReturn(name);    assertThat(forwarding.getName(), equalTo(name));}
public ExecutableStageMatcher beam_f1786_0(Matcher<Iterable<? extends String>> pCollections)
{    return new ExecutableStageMatcher(inputPCollectionId, sideInputIds, pCollections, fusedTransforms);}
public ExecutableStageMatcher beam_f1787_0(String... pCollections)
{    return new ExecutableStageMatcher(inputPCollectionId, sideInputIds, Matchers.containsInAnyOrder(pCollections), fusedTransforms);}
private static PCollection beam_f1796_0(String name)
{    return PCollection.newBuilder().setUniqueName(name).setCoderId("coder").setWindowingStrategyId("ws").build();}
public void beam_f1797_0()
{    String name = "read.out";    Components components = partialComponents.toBuilder().putTransforms("read", PTransform.newBuilder().setUniqueName("Read").putInputs("input", "impulse.out").putOutputs("output", "read.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("read.out", pc(name)).putTransforms("parDo", PTransform.newBuilder().setUniqueName("ParDo").putInputs("input", "read.out").putOutputs("output", "parDo.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("parDo.out", pc("parDo.out")).putTransforms("window", PTransform.newBuilder().setUniqueName("Window").putInputs("input", "parDo.out").putOutputs("output", "window.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("py")).build().toByteString())).build()).putPcollections("window.out", pc("window.out")).build();    FusedPipeline fused = GreedyPipelineFuser.fuse(Pipeline.newBuilder().setComponents(components).build());    assertThat(fused.getRunnerExecutedTransforms(), contains(PipelineNode.pTransform("impulse", components.getTransformsOrThrow("impulse"))));    assertThat(fused.getFusedStages(), contains(ExecutableStageMatcher.withInput("impulse.out").withNoOutputs().withTransforms("read", "parDo", "window")));}
public void beam_f1806_0()
{                PTransform parDoTransform = PTransform.newBuilder().setUniqueName("ParDo").putInputs("input", "impulse.out").putOutputs("output", "parDo.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    PTransform timerTransform = PTransform.newBuilder().setUniqueName("TimerParDo").putInputs("input", "parDo.out").putInputs("timer", "timer.out").putOutputs("timer", "timer.out").putOutputs("output", "output.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).putTimerSpecs("timer", TimerSpec.getDefaultInstance()).build().toByteString())).build();    Components components = partialComponents.toBuilder().putTransforms("parDo", parDoTransform).putPcollections("parDo.out", pc("parDo.out")).putTransforms("timer", timerTransform).putPcollections("timer.out", pc("timer.out")).putPcollections("output.out", pc("output.out")).putEnvironments("common", Environments.createDockerEnvironment("common")).build();    FusedPipeline fused = GreedyPipelineFuser.fuse(Pipeline.newBuilder().setComponents(components).build());    assertThat(fused.getRunnerExecutedTransforms(), containsInAnyOrder(PipelineNode.pTransform("impulse", components.getTransformsOrThrow("impulse"))));    assertThat(fused.getFusedStages(), containsInAnyOrder(ExecutableStageMatcher.withInput("impulse.out").withOutputs("parDo.out").withTransforms("parDo"), ExecutableStageMatcher.withInput("parDo.out").withNoOutputs().withTransforms("timer")));}
public void beam_f1807_0()
{    PTransform timerTransform = PTransform.newBuilder().setUniqueName("TimerParDo").putInputs("input", "impulse.out").putInputs("timer", "timer.out").putOutputs("timer", "timer.out").putOutputs("output", "output.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).putStateSpecs("state", StateSpec.getDefaultInstance()).putTimerSpecs("timer", TimerSpec.getDefaultInstance()).build().toByteString())).build();    Components components = partialComponents.toBuilder().putTransforms("timer", timerTransform).putPcollections("timer.out", pc("timer.out")).putPcollections("output.out", pc("output.out")).putEnvironments("common", Environments.createDockerEnvironment("common")).build();    FusedPipeline fused = GreedyPipelineFuser.fuse(Pipeline.newBuilder().setComponents(components).build());    assertThat(fused.getRunnerExecutedTransforms(), containsInAnyOrder(PipelineNode.pTransform("impulse", components.getTransformsOrThrow("impulse"))));    assertThat(fused.getFusedStages(), contains(ExecutableStageMatcher.withInput("impulse.out").withNoOutputs().withTransforms("timer")));}
public void beam_f1816_0()
{                PTransform parDoTransform = PTransform.newBuilder().putInputs("input", "impulse.out").putOutputs("output", "parDo.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    PTransform timerTransform = PTransform.newBuilder().putInputs("input", "parDo.out").putOutputs("output", "timer.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).putTimerSpecs("timer", TimerSpec.getDefaultInstance()).build().toByteString())).build();    QueryablePipeline p = QueryablePipeline.forPrimitivesIn(partialComponents.toBuilder().putTransforms("parDo", parDoTransform).putPcollections("parDo.out", PCollection.newBuilder().setUniqueName("parDo.out").build()).putTransforms("timer", timerTransform).putPcollections("timer.out", PCollection.newBuilder().setUniqueName("timer.out").build()).putEnvironments("common", Environments.createDockerEnvironment("common")).build());    ExecutableStage subgraph = GreedyStageFuser.forGrpcPortRead(p, impulseOutputNode, ImmutableSet.of(PipelineNode.pTransform("parDo", parDoTransform)));    assertThat(subgraph.getOutputPCollections(), contains(PipelineNode.pCollection("parDo.out", PCollection.newBuilder().setUniqueName("parDo.out").build())));    assertThat(subgraph, hasSubtransforms("parDo"));}
public void beam_f1817_0()
{                        PTransform readTransform = PTransform.newBuilder().putInputs("input", "impulse.out").putOutputs("output", "read.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    PTransform parDoTransform = PTransform.newBuilder().putInputs("input", "impulse.out").putOutputs("output", "parDo.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    PTransform flattenTransform = PTransform.newBuilder().putInputs("readInput", "read.out").putInputs("parDoInput", "parDo.out").putOutputs("output", "flatten.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN)).build();    PTransform windowTransform = PTransform.newBuilder().putInputs("input", "flatten.out").putOutputs("output", "window.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    QueryablePipeline p = QueryablePipeline.forPrimitivesIn(partialComponents.toBuilder().putTransforms("read", readTransform).putPcollections("read.out", PCollection.newBuilder().setUniqueName("read.out").build()).putTransforms("parDo", parDoTransform).putPcollections("parDo.out", PCollection.newBuilder().setUniqueName("parDo.out").build()).putTransforms("flatten", flattenTransform).putPcollections("flatten.out", PCollection.newBuilder().setUniqueName("flatten.out").build()).putTransforms("window", windowTransform).putPcollections("window.out", PCollection.newBuilder().setUniqueName("window.out").build()).putEnvironments("common", Environments.createDockerEnvironment("common")).build());    ExecutableStage subgraph = GreedyStageFuser.forGrpcPortRead(p, impulseOutputNode, p.getPerElementConsumers(impulseOutputNode));    assertThat(subgraph.getOutputPCollections(), emptyIterable());    assertThat(subgraph, hasSubtransforms("read", "parDo", "flatten", "window"));}
public void beam_f1826_0()
{                    Environment env = Environments.createDockerEnvironment("common");    PTransform readTransform = PTransform.newBuilder().putInputs("input", "impulse.out").putOutputs("output", "read.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(ParDoPayload.newBuilder().setDoFn(SdkFunctionSpec.newBuilder().setEnvironmentId("common")).build().toByteString())).build();    QueryablePipeline p = QueryablePipeline.forPrimitivesIn(partialComponents.toBuilder().putTransforms("read", readTransform).putPcollections("read.out", PCollection.newBuilder().setUniqueName("read.out").build()).putTransforms("gbk", PTransform.newBuilder().putInputs("input", "read.out").putOutputs("output", "gbk.out").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN)).build()).putPcollections("gbk.out", PCollection.newBuilder().setUniqueName("parDo.out").build()).putEnvironments("common", env).build());    PTransformNode readNode = PipelineNode.pTransform("read", readTransform);    PCollectionNode readOutput = getOnlyElement(p.getOutputPCollections(readNode));    ExecutableStage subgraph = GreedyStageFuser.forGrpcPortRead(p, impulseOutputNode, ImmutableSet.of(readNode));    assertThat(subgraph.getOutputPCollections(), contains(readOutput));    assertThat(subgraph, hasSubtransforms(readNode.getId()));}
private static TypeSafeMatcher<ExecutableStage> beam_f1827_0(String id, String... ids)
{    Set<String> expectedTransforms = ImmutableSet.<String>builder().add(id).add(ids).build();    return new TypeSafeMatcher<ExecutableStage>() {        @Override        protected boolean matchesSafely(ExecutableStage executableStage) {                        Set<String> stageTransforms = executableStage.getTransforms().stream().map(PTransformNode::getId).collect(Collectors.toSet());            return stageTransforms.containsAll(expectedTransforms) && expectedTransforms.containsAll(stageTransforms);        }        @Override        public void describeTo(Description description) {            description.appendText("ExecutableStage with subtransform ids: " + expectedTransforms);        }    };}
public void beam_f1836_0()
{    assertEquals(createNetwork().nodes(), Networks.reachableNodes(createNetwork(), ImmutableSet.of("A", "D", "I", "M", "O"), Collections.emptySet()));}
public void beam_f1837_0()
{    assertEquals(ImmutableSet.of("A", "D", "I", "M", "O"), Networks.reachableNodes(createNetwork(), ImmutableSet.of("A", "D", "I", "M", "O"), ImmutableSet.of("A", "D", "I", "M", "O")));}
public void beam_f1846_0()
{    /* When both a stage and a runner-executed transform produce a PCollection, all should be     * replaced with synthetic flattens.     * original graph:     *             --> one -> .out \     * red -> .out |                -> shared -> .out     *             --------------> /     *     * fused graph:     *             --> [one -> .out -> shared ->] .out     * red -> .out |     *             ------------------> shared --> .out     *     * deduplicated graph:     *             --> [one -> .out -> shared ->] .out:0 \     * red -> .out |                                      -> shared -> .out     *             -----------------> shared:0 -> .out:1 /     */    PCollection redOut = PCollection.newBuilder().setUniqueName("red.out").build();    PTransform red = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putOutputs("out", redOut.getUniqueName()).build();    PCollection oneOut = PCollection.newBuilder().setUniqueName("one.out").build();    PTransform one = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("in", redOut.getUniqueName()).putOutputs("out", oneOut.getUniqueName()).build();    PCollection sharedOut = PCollection.newBuilder().setUniqueName("shared.out").build();    PTransform shared = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("one", oneOut.getUniqueName()).putInputs("red", redOut.getUniqueName()).putOutputs("shared", sharedOut.getUniqueName()).build();    PCollection blueOut = PCollection.newBuilder().setUniqueName("blue.out").build();    PTransform blue = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("in", sharedOut.getUniqueName()).putOutputs("out", blueOut.getUniqueName()).build();    RunnerApi.Components components = Components.newBuilder().putTransforms("one", one).putPcollections(oneOut.getUniqueName(), oneOut).putTransforms("red", red).putPcollections(redOut.getUniqueName(), redOut).putTransforms("shared", shared).putPcollections(sharedOut.getUniqueName(), sharedOut).putTransforms("blue", blue).putPcollections(blueOut.getUniqueName(), blueOut).build();    PTransformNode sharedTransform = PipelineNode.pTransform("shared", shared);    ExecutableStage oneStage = ImmutableExecutableStage.of(components, Environment.getDefaultInstance(), PipelineNode.pCollection(redOut.getUniqueName(), redOut), ImmutableList.of(), ImmutableList.of(), ImmutableList.of(), ImmutableList.of(PipelineNode.pTransform("one", one), sharedTransform), ImmutableList.of(PipelineNode.pCollection(sharedOut.getUniqueName(), sharedOut)));    PTransformNode redTransform = PipelineNode.pTransform("red", red);    PTransformNode blueTransform = PipelineNode.pTransform("blue", blue);    QueryablePipeline pipeline = QueryablePipeline.forPrimitivesIn(components);    DeduplicationResult result = OutputDeduplicator.ensureSingleProducer(pipeline, ImmutableList.of(oneStage), ImmutableList.of(redTransform, blueTransform, sharedTransform));    assertThat(result.getIntroducedTransforms(), hasSize(1));    PTransformNode introduced = getOnlyElement(result.getIntroducedTransforms());    assertThat(introduced.getTransform().getOutputsMap().size(), equalTo(1));    assertThat(getOnlyElement(introduced.getTransform().getOutputsMap().values()), equalTo(sharedOut.getUniqueName()));    assertThat(result.getDeduplicatedComponents().getPcollectionsMap().keySet(), hasItems(introduced.getTransform().getInputsMap().values().toArray(new String[0])));    assertThat(result.getDeduplicatedStages().keySet(), hasSize(1));    assertThat(result.getDeduplicatedTransforms().keySet(), containsInAnyOrder("shared"));    List<String> introducedOutputs = new ArrayList<>();    introducedOutputs.addAll(result.getDeduplicatedTransforms().get("shared").getTransform().getOutputsMap().values());    introducedOutputs.addAll(result.getDeduplicatedStages().get(oneStage).getOutputPCollections().stream().map(PCollectionNode::getId).collect(Collectors.toList()));    assertThat(introduced.getTransform().getInputsMap().values(), containsInAnyOrder(introducedOutputs.toArray(new String[0])));    assertThat(result.getDeduplicatedComponents().getPcollectionsMap().keySet(), hasItems(introducedOutputs.toArray(new String[0])));    assertThat(result.getDeduplicatedComponents().getTransformsMap(), hasEntry(introduced.getId(), introduced.getTransform()));}
public void beam_f1847_0()
{    /* A stage that produces multiple duplicates should have them all synthesized.     *     * Original Pipeline:     * red -> .out ---> one -> .out -----\     *             \                      -> shared.out     *              \--> two -> .out ----|     *               \                    -> otherShared -> .out     *                \-> three --> .out /     *     * Fused Pipeline:     *      -> .out [-> one -> .out -> shared -> .out] \     *     /                                            -> blue -> .out     *     |                        -> shared -> .out] /     * red -> .out [-> two -> .out |     *     |                        -> otherShared -> .out]     *     \     *      -> .out [-> three -> .out -> otherShared -> .out]     *     * Deduplicated Pipeline:     *           [-> one -> .out -> shared -> .out:0] --\     *           |                                       -> shared -> .out -> blue -> .out     *           |                 -> shared -> .out:1] /     * red -> .out [-> two -> .out |     *           |                  -> otherShared -> .out:0] --\     *           |                                               -> otherShared -> .out     *           [-> three -> .out -> otherShared -> .out:1] ---/     */    PCollection redOut = PCollection.newBuilder().setUniqueName("red.out").build();    PTransform red = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putOutputs("out", redOut.getUniqueName()).build();    PCollection threeOut = PCollection.newBuilder().setUniqueName("three.out").build();    PTransform three = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("in", redOut.getUniqueName()).putOutputs("out", threeOut.getUniqueName()).build();    PCollection oneOut = PCollection.newBuilder().setUniqueName("one.out").build();    PTransform one = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("in", redOut.getUniqueName()).putOutputs("out", oneOut.getUniqueName()).build();    PCollection twoOut = PCollection.newBuilder().setUniqueName("two.out").build();    PTransform two = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("in", redOut.getUniqueName()).putOutputs("out", twoOut.getUniqueName()).build();    PCollection sharedOut = PCollection.newBuilder().setUniqueName("shared.out").build();    PTransform shared = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("one", oneOut.getUniqueName()).putInputs("two", twoOut.getUniqueName()).putOutputs("shared", sharedOut.getUniqueName()).build();    PCollection otherSharedOut = PCollection.newBuilder().setUniqueName("shared.out2").build();    PTransform otherShared = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("multi", threeOut.getUniqueName()).putInputs("two", twoOut.getUniqueName()).putOutputs("out", otherSharedOut.getUniqueName()).build();    PCollection blueOut = PCollection.newBuilder().setUniqueName("blue.out").build();    PTransform blue = PTransform.newBuilder().setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).build()).putInputs("in", sharedOut.getUniqueName()).putOutputs("out", blueOut.getUniqueName()).build();    RunnerApi.Components components = Components.newBuilder().putTransforms("one", one).putPcollections(oneOut.getUniqueName(), oneOut).putTransforms("two", two).putPcollections(twoOut.getUniqueName(), twoOut).putTransforms("multi", three).putPcollections(threeOut.getUniqueName(), threeOut).putTransforms("shared", shared).putPcollections(sharedOut.getUniqueName(), sharedOut).putTransforms("otherShared", otherShared).putPcollections(otherSharedOut.getUniqueName(), otherSharedOut).putTransforms("red", red).putPcollections(redOut.getUniqueName(), redOut).putTransforms("blue", blue).putPcollections(blueOut.getUniqueName(), blueOut).build();    ExecutableStage multiStage = ImmutableExecutableStage.of(components, Environment.getDefaultInstance(), PipelineNode.pCollection(redOut.getUniqueName(), redOut), ImmutableList.of(), ImmutableList.of(), ImmutableList.of(), ImmutableList.of(PipelineNode.pTransform("multi", three), PipelineNode.pTransform("shared", shared), PipelineNode.pTransform("otherShared", otherShared)), ImmutableList.of(PipelineNode.pCollection(sharedOut.getUniqueName(), sharedOut), PipelineNode.pCollection(otherSharedOut.getUniqueName(), otherSharedOut)));    ExecutableStage oneStage = ImmutableExecutableStage.of(components, Environment.getDefaultInstance(), PipelineNode.pCollection(redOut.getUniqueName(), redOut), ImmutableList.of(), ImmutableList.of(), ImmutableList.of(), ImmutableList.of(PipelineNode.pTransform("one", one), PipelineNode.pTransform("shared", shared)), ImmutableList.of(PipelineNode.pCollection(sharedOut.getUniqueName(), sharedOut)));    ExecutableStage twoStage = ImmutableExecutableStage.of(components, Environment.getDefaultInstance(), PipelineNode.pCollection(redOut.getUniqueName(), redOut), ImmutableList.of(), ImmutableList.of(), ImmutableList.of(), ImmutableList.of(PipelineNode.pTransform("two", two), PipelineNode.pTransform("otherShared", otherShared)), ImmutableList.of(PipelineNode.pCollection(otherSharedOut.getUniqueName(), otherSharedOut)));    PTransformNode redTransform = PipelineNode.pTransform("red", red);    PTransformNode blueTransform = PipelineNode.pTransform("blue", blue);    QueryablePipeline pipeline = QueryablePipeline.forPrimitivesIn(components);    DeduplicationResult result = OutputDeduplicator.ensureSingleProducer(pipeline, ImmutableList.of(oneStage, twoStage, multiStage), ImmutableList.of(redTransform, blueTransform));    assertThat(result.getIntroducedTransforms(), hasSize(2));    assertThat(result.getDeduplicatedStages().keySet(), containsInAnyOrder(multiStage, oneStage, twoStage));    assertThat(result.getDeduplicatedTransforms().keySet(), empty());    Collection<String> introducedIds = result.getIntroducedTransforms().stream().flatMap(pt -> pt.getTransform().getInputsMap().values().stream()).collect(Collectors.toList());    String[] stageOutputs = result.getDeduplicatedStages().values().stream().flatMap(s -> s.getOutputPCollections().stream().map(PCollectionNode::getId)).toArray(String[]::new);    assertThat(introducedIds, containsInAnyOrder(stageOutputs));    assertThat(result.getDeduplicatedComponents().getPcollectionsMap().keySet(), hasItems(introducedIds.toArray(new String[0])));    assertThat(result.getDeduplicatedComponents().getTransformsMap().entrySet(), hasItems(result.getIntroducedTransforms().stream().collect(Collectors.toMap(PTransformNode::getId, PTransformNode::getTransform)).entrySet().toArray(new Map.Entry[0])));}
public void beam_f1856_0()
{    Pipeline p = Pipeline.create();    p.apply("UnboundedRead", Read.from(CountingSource.unbounded())).apply(Window.into(FixedWindows.of(Duration.millis(5L)))).apply(Count.perElement());    p.apply("BoundedRead", Read.from(CountingSource.upTo(100L)));    Components components = PipelineTranslation.toProto(p).getComponents();    QueryablePipeline qp = QueryablePipeline.forPrimitivesIn(components);    assertThat(qp.getRootTransforms(), hasSize(2));    for (PTransformNode rootTransform : qp.getRootTransforms()) {        assertThat("Root transforms should have no inputs", rootTransform.getTransform().getInputsCount(), equalTo(0));        assertThat("Only added source reads to the pipeline", rootTransform.getTransform().getSpec().getUrn(), equalTo(PTransformTranslation.READ_TRANSFORM_URN));    }}
public void beam_f1857_0()
{    Pipeline p = Pipeline.create();    PCollection<Long> longs = p.apply("BoundedRead", Read.from(CountingSource.upTo(100L)));    PCollectionView<String> view = p.apply("Create", Create.of("foo")).apply("View", View.asSingleton());    longs.apply("par_do", ParDo.of(new TestFn()).withSideInputs(view).withOutputTags(new TupleTag<>(), TupleTagList.empty()));    Components components = PipelineTranslation.toProto(p).getComponents();    QueryablePipeline qp = QueryablePipeline.forPrimitivesIn(components);    String mainInputName = getOnlyElement(PipelineNode.pTransform("BoundedRead", components.getTransformsOrThrow("BoundedRead")).getTransform().getOutputsMap().values());    PCollectionNode mainInput = PipelineNode.pCollection(mainInputName, components.getPcollectionsOrThrow(mainInputName));    PTransform parDoTransform = components.getTransformsOrThrow("par_do");    String sideInputLocalName = getOnlyElement(parDoTransform.getInputsMap().entrySet().stream().filter(entry -> !entry.getValue().equals(mainInputName)).map(Map.Entry::getKey).collect(Collectors.toSet()));    String sideInputCollectionId = parDoTransform.getInputsOrThrow(sideInputLocalName);    PCollectionNode sideInput = PipelineNode.pCollection(sideInputCollectionId, components.getPcollectionsOrThrow(sideInputCollectionId));    PTransformNode parDoNode = PipelineNode.pTransform("par_do", components.getTransformsOrThrow("par_do"));    SideInputReference sideInputRef = SideInputReference.of(parDoNode, sideInputLocalName, sideInput);    assertThat(qp.getSideInputs(parDoNode), contains(sideInputRef));    assertThat(qp.getPerElementConsumers(mainInput), contains(parDoNode));    assertThat(qp.getPerElementConsumers(sideInput), not(contains(parDoNode)));}
public PCollection<Long> beam_f1867_0(PBegin input)
{    return input.apply(GenerateSequence.from(2L)).apply(Window.into(FixedWindows.of(Duration.standardMinutes(5L)))).apply(MapElements.into(TypeDescriptors.longs()).via(l -> l + 1));}
public void beam_f1868_0() throws Exception
{    assertThat(PTransformTranslation.urnForTransform(GroupByKey.create()), equalTo(GROUP_BY_KEY_TRANSFORM_URN));}
public void beam_f1877_0(Throwable t)
{    onCompleted();}
public void beam_f1879_0()
{    PCollection<Long> read = p.apply(JavaReadViaImpulse.bounded(CountingSource.upTo(10L)));    PAssert.that(read).containsInAnyOrder(0L, 9L, 8L, 1L, 2L, 7L, 6L, 3L, 4L, 5L);    p.run();}
public long beam_f1892_0(PipelineOptions options) throws Exception
{    return 0;}
public BoundedReader<Integer> beam_f1893_0(PipelineOptions options) throws IOException
{    throw new AssertionError("Not the point");}
public void beam_f1902_0()
{    Assert.assertTrue(NativeTransforms.isNative(RunnerApi.PTransform.newBuilder().setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn("test").build()).build()));}
public void beam_f1903_0()
{    Assert.assertFalse(NativeTransforms.isNative(RunnerApi.PTransform.getDefaultInstance()));}
public void beam_f1912_0(ProcessContext context, RestrictionTracker<Integer, ?> restriction)
{    context.output(null);}
public Integer beam_f1913_0(KV<Long, String> elem)
{    return 42;}
public static Iterable<PCollection<?>> beam_f1924_0()
{    Pipeline pipeline = TestPipeline.create();    PCollection<Integer> ints = pipeline.apply("ints", Create.of(1, 2, 3));    PCollection<Long> longs = pipeline.apply("unbounded longs", GenerateSequence.from(0));    PCollection<Long> windowedLongs = longs.apply("into fixed windows", Window.into(FixedWindows.of(Duration.standardMinutes(10L))));    PCollection<KV<String, Iterable<String>>> groupedStrings = pipeline.apply("kvs", Create.of(KV.of("foo", "spam"), KV.of("bar", "ham"), KV.of("baz", "eggs"))).apply("group", GroupByKey.create());    PCollection<Long> coderLongs = pipeline.apply("counts with alternative coder", GenerateSequence.from(0).to(10)).setCoder(BigEndianLongCoder.of());    pipeline.apply("intsWithCustomCoder", Create.of(1, 2).withCoder(new AutoValue_PCollectionTranslationTest_CustomIntCoder())).apply("into custom windows", Window.into(new CustomWindows()).triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterFirst.of(AfterPane.elementCountAtLeast(5), AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(227L))))).accumulatingFiredPanes().withAllowedLateness(Duration.standardMinutes(12L)));    return ImmutableList.of(ints, longs, windowedLongs, coderLongs, groupedStrings);}
public void beam_f1925_0() throws Exception
{        SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environments.createDockerEnvironment("java"));    RunnerApi.PCollection protoCollection = PCollectionTranslation.toProto(testCollection, sdkComponents);    RehydratedComponents protoComponents = RehydratedComponents.forComponents(sdkComponents.toComponents());        Pipeline pipeline = Pipeline.create();    PCollection<?> decodedCollection = PCollectionTranslation.fromProto(protoCollection, pipeline, protoComponents);        assertThat(decodedCollection.getCoder(), equalTo(testCollection.getCoder()));    assertThat(decodedCollection.getWindowingStrategy(), equalTo(testCollection.getWindowingStrategy().fixDefaults()));    assertThat(decodedCollection.isBounded(), equalTo(testCollection.isBounded()));}
public void beam_f1935_0(BoundedWindow value, OutputStream outStream) throws IOException
{    VarInt.encode(value.maxTimestamp().getMillis(), outStream);}
public BoundedWindow beam_f1936_0(InputStream inStream) throws IOException
{    final Instant ts = new Instant(VarInt.decodeLong(inStream));    return new BoundedWindow() {        @Override        public Instant maxTimestamp() {            return ts;        }    };}
public int beam_f1945_0()
{    return 0;}
public static Iterable<? extends PipelineOptions> beam_f1946_0()
{    PipelineOptionsFactory.register(TestUnserializableOptions.class);    PipelineOptionsFactory.register(TestDefaultOptions.class);    PipelineOptionsFactory.register(TestOptions.class);    PipelineOptions emptyOptions = PipelineOptionsFactory.create();    TestUnserializableOptions withNonSerializable = PipelineOptionsFactory.as(TestUnserializableOptions.class);    withNonSerializable.setUnserializable(new Object());    TestOptions withCustomField = PipelineOptionsFactory.as(TestOptions.class);    withCustomField.setExample(99);    PipelineOptions withSettings = PipelineOptionsFactory.create();    withSettings.as(ApplicationNameOptions.class).setAppName("my_app");    withSettings.setJobName("my_job");    PipelineOptions withParsedSettings = PipelineOptionsFactory.fromArgs("--jobName=my_job --appName=my_app").create();    return ImmutableList.of(emptyOptions, withNonSerializable, withCustomField, withSettings, withParsedSettings);}
public void beam_f1955_0()
{    ClassLoader mockClassLoader = Mockito.mock(ClassLoader.class);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Unable to use ClassLoader to detect classpath elements.");    PipelineResources.detectClassPathResourcesToStage(mockClassLoader);}
public void beam_f1956_0() throws Exception
{    String url = "http://www.google.com/all-the-secrets.jar";    URLClassLoader classLoader = new URLClassLoader(new URL[] { new URL(url) });    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Unable to convert url (" + url + ") to file.");    PipelineResources.detectClassPathResourcesToStage(classLoader);}
public void beam_f1965_0(Node node)
{    transforms.add(node);    if (!useDeprecatedViewTransforms && PTransformTranslation.CREATE_VIEW_TRANSFORM_URN.equals(PTransformTranslation.urnForTransformOrNull(node.getTransform()))) {        missingViewTransforms += 1;    }}
public void beam_f1966_0(PValue value, Node producer)
{    if (value instanceof PCollection) {        PCollection pc = (PCollection) value;        pcollections.add(pc);        addCoders(pc.getCoder());        windowingStrategies.add(pc.getWindowingStrategy());        addCoders(pc.getWindowingStrategy().getWindowFn().windowCoder());    }}
public void beam_f1976_0(ProcessContext ctxt)
{    ctxt.output(ctxt.element().getValue() + 1);}
public Void beam_f1978_0(KV<String, Integer> element)
{    return null;}
public void beam_f1987_0()
{    AppliedPTransform<?, ?, ?> parDoApplication = getAppliedTransform(ParDo.of(doFn).withOutputTags(new TupleTag<>(), TupleTagList.empty()));    assertThat(PTransformMatchers.splittableParDoMulti().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.stateOrTimerParDoMulti().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.splittableParDoSingle().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.stateOrTimerParDoSingle().matches(parDoApplication), is(false));}
public void beam_f1988_0()
{    AppliedPTransform<?, ?, ?> parDoApplication = getAppliedTransform(ParDo.of(splittableDoFn).withOutputTags(new TupleTag<>(), TupleTagList.empty()));    assertThat(PTransformMatchers.splittableParDoMulti().matches(parDoApplication), is(true));    assertThat(PTransformMatchers.stateOrTimerParDoMulti().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.splittableParDoSingle().matches(parDoApplication), is(false));    assertThat(PTransformMatchers.stateOrTimerParDoSingle().matches(parDoApplication), is(false));}
public void beam_f2000_0()
{    PCollection<Integer> input = p.apply(Create.of(1));    PCollectionView<Iterable<Integer>> view = input.apply(View.asIterable());    ViewFn<?, ?> viewFn = view.getViewFn();    CreatePCollectionView<?, ?> createView = CreatePCollectionView.of(view);    PTransformMatcher matcher = PTransformMatchers.createViewWithViewFn(viewFn.getClass());    assertThat(matcher.matches(getAppliedTransform(createView)), is(true));}
public void beam_f2001_0()
{    PCollection<Integer> input = p.apply(Create.of(1));    PCollectionView<Iterable<Integer>> view = input.apply(View.asIterable());        IterableViewFn<Integer> viewFn = new PCollectionViews.IterableViewFn<Integer>(() -> TypeDescriptors.integers()) {    };    CreatePCollectionView<?, ?> createView = CreatePCollectionView.of(view);    PTransformMatcher matcher = PTransformMatchers.createViewWithViewFn(viewFn.getClass());    assertThat(matcher.matches(getAppliedTransform(createView)), is(false));}
public WriteOperation<Void, Integer> beam_f2010_0()
{    return null;}
private AppliedPTransform<?, ?, ?> beam_f2011_0(WriteFiles<Integer, Void, Integer> write)
{    return AppliedPTransform.of("WriteFiles", Collections.emptyMap(), Collections.emptyMap(), write, p);}
public static ToAndFromProtoSpec beam_f2021_0(AppliedPTransform<?, ?, ?> topLevel, ToAndFromProtoSpec spec, ToAndFromProtoSpec... specs)
{    List<ToAndFromProtoSpec> childSpecs = new ArrayList<>();    childSpecs.add(spec);    childSpecs.addAll(Arrays.asList(specs));    return new AutoValue_PTransformTranslationTest_ToAndFromProtoSpec(topLevel, childSpecs);}
public void beam_f2022_0() throws IOException
{    SdkComponents components = SdkComponents.create(spec.getTransform().getPipeline().getOptions());    RunnerApi.PTransform converted = convert(spec, components);    Components protoComponents = components.toComponents();        assertThat(converted.getInputsCount(), equalTo(spec.getTransform().getInputs().size()));    assertThat(converted.getOutputsCount(), equalTo(spec.getTransform().getOutputs().size()));    assertThat(converted.getSubtransformsCount(), equalTo(spec.getChildren().size()));    assertThat(converted.getUniqueName(), equalTo(spec.getTransform().getFullName()));    for (PValue inputValue : spec.getTransform().getInputs().values()) {        PCollection<?> inputPc = (PCollection<?>) inputValue;        protoComponents.getPcollectionsOrThrow(components.registerPCollection(inputPc));    }    for (PValue outputValue : spec.getTransform().getOutputs().values()) {        PCollection<?> outputPc = (PCollection<?>) outputValue;        protoComponents.getPcollectionsOrThrow(components.registerPCollection(outputPc));    }}
public void beam_f2032_0() throws Exception
{        assumeThat(source, instanceOf(BoundedSource.class));    BoundedSource<?> boundedSource = (BoundedSource<?>) this.source;    Read.Bounded<?> boundedRead = Read.from(boundedSource);    SdkComponents components = SdkComponents.create();    components.registerEnvironment(Environments.createDockerEnvironment("java"));    ReadPayload payload = ReadTranslation.toProto(boundedRead, components);    assertThat(payload.getIsBounded(), equalTo(RunnerApi.IsBounded.Enum.BOUNDED));    BoundedSource<?> deserializedSource = ReadTranslation.boundedSourceFromProto(payload);    assertThat(deserializedSource, equalTo(source));}
public void beam_f2033_0() throws Exception
{    assumeThat(source, instanceOf(UnboundedSource.class));    UnboundedSource<?, ?> unboundedSource = (UnboundedSource<?, ?>) this.source;    Read.Unbounded<?> unboundedRead = Read.from(unboundedSource);    SdkComponents components = SdkComponents.create();        ReadPayload payload = ReadTranslation.toProto(unboundedRead, components);    assertThat(payload.getIsBounded(), equalTo(RunnerApi.IsBounded.Enum.UNBOUNDED));    UnboundedSource<?, ?> deserializedSource = ReadTranslation.unboundedSourceFromProto(payload);    assertThat(deserializedSource, equalTo(source));}
public UnboundedReader<byte[]> beam_f2042_0(PipelineOptions options, @Nullable CheckpointMark checkpointMark) throws IOException
{    throw new UnsupportedOperationException();}
public Coder<CheckpointMark> beam_f2043_0()
{    return new TestCheckpointMarkCoder();}
public void beam_f2052_0()
{    assertEquals("digraph {" + "    rankdir=LR" + "    subgraph cluster_0 {" + "        label = \"\"" + "    }" + "}", PipelineDotRenderer.toDotString(p).replaceAll(System.lineSeparator(), ""));}
public void beam_f2053_0()
{    p.apply(Create.timestamped(TimestampedValue.of(KV.of(1, 1), new Instant(1)))).apply(Window.into(FixedWindows.of(Duration.millis(10)))).apply(Sum.integersPerKey());    assertEquals("digraph {" + "    rankdir=LR" + "    subgraph cluster_0 {" + "        label = \"\"" + "        subgraph cluster_1 {" + "            label = \"Create.TimestampedValues\"" + "            subgraph cluster_2 {" + "                label = \"Create.TimestampedValues/Create.Values\"" + "                3 [label=\"Read(CreateSource)\"]" + "            }" + "            subgraph cluster_4 {" + "                label = \"Create.TimestampedValues/ParDo(ConvertTimestamps)\"" + "                5 [label=\"ParMultiDo(ConvertTimestamps)\"]" + "                3 -> 5 [style=solid label=\"\"]" + "            }" + "        }" + "        subgraph cluster_6 {" + "            label = \"Window.Into()\"" + "            7 [label=\"Window.Assign\"]" + "            5 -> 7 [style=solid label=\"\"]" + "        }" + "        subgraph cluster_8 {" + "            label = \"Combine.perKey(SumInteger)\"" + "            9 [label=\"GroupByKey\"]" + "            7 -> 9 [style=solid label=\"\"]" + "            subgraph cluster_10 {" + "                label = \"Combine.perKey(SumInteger)/Combine.GroupedValues\"" + "                subgraph cluster_11 {" + "                    label = \"Combine.perKey(SumInteger)/Combine.GroupedValues/ParDo(Anonymous)\"" + "                    12 [label=\"ParMultiDo(Anonymous)\"]" + "                    9 -> 12 [style=solid label=\"\"]" + "                }" + "            }" + "        }" + "    }" + "}", PipelineDotRenderer.toDotString(p).replaceAll(System.lineSeparator(), ""));}
public static Iterable<Schema> beam_f2062_0()
{    return ImmutableList.<Schema>builder().add(Schema.of(Field.of("string", FieldType.STRING))).add(Schema.of(Field.of("boolean", FieldType.BOOLEAN), Field.of("byte", FieldType.BYTE), Field.of("int16", FieldType.INT16), Field.of("int32", FieldType.INT32), Field.of("int64", FieldType.INT64))).add(Schema.of(Field.of("row", FieldType.row(Schema.of(Field.of("foo", FieldType.STRING), Field.of("bar", FieldType.DOUBLE), Field.of("baz", FieldType.BOOLEAN)))))).add(Schema.of(Field.of("array(array(int64)))", FieldType.array(FieldType.array(FieldType.INT64.withNullable(true)))))).add(Schema.of(Field.of("nullable", FieldType.STRING.withNullable(true)), Field.of("non_nullable", FieldType.STRING.withNullable(false)))).add(Schema.of(Field.of("decimal", FieldType.DECIMAL), Field.of("datetime", FieldType.DATETIME))).add(Schema.of(Field.of("logical", FieldType.logicalType(LogicalTypes.FixedBytes.of(24))))).build();}
public void beam_f2063_0() throws Exception
{    SchemaApi.Schema schemaProto = SchemaTranslation.schemaToProto(schema);    Schema decodedSchema = SchemaTranslation.fromProto(schemaProto);    assertThat(decodedSchema, equalTo(schema));}
public void beam_f2072_0() throws IOException
{    PCollection<Long> pCollection = pipeline.apply("FirstCount", GenerateSequence.from(0)).setName("foo");    String firstId = components.registerPCollection(pCollection);    PCollection<Long> duplicate = pipeline.apply("SecondCount", GenerateSequence.from(0)).setName("foo");    String secondId = components.registerPCollection(duplicate);    assertThat(firstId, equalTo("foo"));    assertThat(secondId, containsString("foo"));    assertThat(secondId, not(equalTo("foo")));    components.toComponents().getPcollectionsOrThrow(firstId);    components.toComponents().getPcollectionsOrThrow(secondId);}
public void beam_f2073_0() throws IOException
{    WindowingStrategy<?, ?> strategy = WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);    String name = components.registerWindowingStrategy(strategy);    assertThat(name, not(isEmptyOrNullString()));    components.toComponents().getWindowingStrategiesOrThrow(name);}
public SomeRestrictionTracker beam_f2082_0()
{    return new SomeRestrictionTracker(this);}
public boolean beam_f2083_0(Void position)
{    return false;}
public void beam_f2094_0()
{    pipeline.enableAbandonedNodeEnforcement(false);    DoFn<Integer, String> boundedFn = new BoundedFakeFn();    assertEquals("Applying a bounded SDF to a bounded collection produces a bounded collection", PCollection.IsBounded.BOUNDED, applySplittableParDo("bounded to bounded", makeBoundedCollection(pipeline), boundedFn).isBounded());    assertEquals("Applying a bounded SDF to an unbounded collection produces an unbounded collection", PCollection.IsBounded.UNBOUNDED, applySplittableParDo("bounded to unbounded", makeUnboundedCollection(pipeline), boundedFn).isBounded());}
public void beam_f2095_0()
{    pipeline.enableAbandonedNodeEnforcement(false);    DoFn<Integer, String> unboundedFn = new UnboundedFakeFn();    assertEquals("Applying an unbounded SDF to a bounded collection produces a bounded collection", PCollection.IsBounded.UNBOUNDED, applySplittableParDo("unbounded to bounded", makeBoundedCollection(pipeline), unboundedFn).isBounded());    assertEquals("Applying an unbounded SDF to an unbounded collection produces an unbounded collection", PCollection.IsBounded.UNBOUNDED, applySplittableParDo("unbounded to unbounded", makeUnboundedCollection(pipeline), unboundedFn).isBounded());}
public void beam_f2104_0()
{    AppliedPTransform<PInput, POutput, TestTransform> transform = AppliedPTransform.of("input-free", Collections.emptyMap(), Collections.emptyMap(), new TestTransform(), pipeline);    assertThat(TransformInputs.nonAdditionalInputs(transform), Matchers.empty());}
public void beam_f2105_0()
{    PCollection<Long> input = pipeline.apply(GenerateSequence.from(1L));    AppliedPTransform<PInput, POutput, TestTransform> transform = AppliedPTransform.of("input-single", Collections.singletonMap(new TupleTag<Long>() {    }, input), Collections.emptyMap(), new TestTransform(), pipeline);    assertThat(TransformInputs.nonAdditionalInputs(transform), Matchers.containsInAnyOrder(input));}
public void beam_f2114_0() throws Exception
{    CheckpointCoder<String> coder = new CheckpointCoder<>(StringUtf8Coder.of());    Checkpoint<String> emptyCheckpoint = new Checkpoint<>(null, null);    Checkpoint<String> decodedEmptyCheckpoint = CoderUtils.decodeFromByteArray(coder, CoderUtils.encodeToByteArray(coder, emptyCheckpoint));    assertNull(decodedEmptyCheckpoint.getResidualElements());    assertNull(decodedEmptyCheckpoint.getResidualSource());}
public void beam_f2115_0() throws Exception
{    CoderProperties.coderSerializable(new CheckpointCoder<>(GlobalWindow.Coder.INSTANCE));}
public void beam_f2124_0() throws Exception
{    UnboundedSource<Long, ?> unboundedCountingSource = new BoundedToUnboundedSourceAdapter<Long>(CountingSource.upTo(1));    PipelineOptions options = PipelineOptionsFactory.create();    List<?> splits = unboundedCountingSource.split(100, options);    assertEquals(1, splits.size());    assertNotEquals(splits.get(0), unboundedCountingSource);}
public void beam_f2125_0() throws Exception
{    UnboundedSource<Long, ?> unboundedCountingSource = new BoundedToUnboundedSourceAdapter<Long>(CountingSource.upTo(0));    PipelineOptions options = PipelineOptionsFactory.create();    List<?> splits = unboundedCountingSource.split(100, options);    assertEquals(1, splits.size());    assertNotEquals(splits.get(0), unboundedCountingSource);}
protected boolean beam_f2134_0()
{    return true;}
protected void beam_f2135_0(ReadableByteChannel channel) throws IOException
{    this.channel = channel;}
public void beam_f2144_0(PValue value, Node producer)
{    if (producer.getTransform() instanceof Read.Bounded || producer.getTransform() instanceof Read.Unbounded) {        allReadOutputs.add(value);    }}
public void beam_f2145_0()
{    thrown.expect(UnsupportedOperationException.class);    thrown.expectMessage(message);    factory.getReplacementTransform(null);}
public BoundedWindow beam_f2154_0(Instant timestamp)
{    return GlobalWindow.INSTANCE;}
public boolean beam_f2155_0(WindowFn<?, ?> other)
{    return getClass().equals(other.getClass());}
public int beam_f2164_0()
{    return Objects.hash(DummySink.class, getTempDirectoryProvider().isAccessible() ? getTempDirectoryProvider().get() : null);}
public FileBasedSink.Writer<Void, Object> beam_f2165_0() throws Exception
{    throw new UnsupportedOperationException("Should never be called.");}
public static GlobalCombineFnRunner<InputT, AccumT, OutputT> beam_f2174_0(GlobalCombineFn<InputT, AccumT, OutputT> globalCombineFn)
{    if (globalCombineFn instanceof CombineFnWithContext) {        return new CombineFnWithContextRunner<>((CombineFnWithContext<InputT, AccumT, OutputT>) globalCombineFn);    } else if (globalCombineFn instanceof CombineFn) {        return new CombineFnRunner<>((CombineFn<InputT, AccumT, OutputT>) globalCombineFn);    } else {        throw new IllegalStateException(String.format("Unknown type of CombineFn: %s", globalCombineFn.getClass()));    }}
private static CombineWithContext.Context beam_f2175_0(final PipelineOptions options, final SideInputReader sideInputReader, final BoundedWindow mainInputWindow)
{    return new CombineWithContext.Context() {        @Override        public PipelineOptions getPipelineOptions() {            return options;        }        @Override        public <T> T sideInput(PCollectionView<T> view) {            if (!sideInputReader.contains(view)) {                throw new IllegalArgumentException("calling sideInput() with unknown view");            }            BoundedWindow sideInputWindow = view.getWindowMappingFn().getSideInputWindow(mainInputWindow);            return sideInputReader.get(view, sideInputWindow);        }    };}
public String beam_f2184_0()
{    return combineFnWithContext.toString();}
public AccumT beam_f2185_0(PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    return combineFnWithContext.createAccumulator(createFromComponents(options, sideInputReader, Iterables.getOnlyElement(windows)));}
public void beam_f2194_0(ProcessContext c) throws Exception
{    KeyedWorkItem<K, InputT> keyedWorkItem = c.element();    K key = keyedWorkItem.key();    StateInternals stateInternals = stateInternalsFactory.stateInternalsForKey(key);    TimerInternals timerInternals = timerInternalsFactory.timerInternalsForKey(key);    ReduceFnRunner<K, InputT, OutputT, W> reduceFnRunner = new ReduceFnRunner<>(key, windowingStrategy, ExecutableTriggerStateMachine.create(TriggerStateMachines.stateMachineForTrigger(TriggerTranslation.toProto(windowingStrategy.getTrigger()))), stateInternals, timerInternals, outputWindowedValue(), sideInputReader, reduceFn, c.getPipelineOptions());    reduceFnRunner.processElements(keyedWorkItem.elementsIterable());    reduceFnRunner.onTimers(keyedWorkItem.timersIterable());    reduceFnRunner.persist();}
public PCollection<KV<K, Iterable<V>>> beam_f2195_0(PCollection<KV<K, V>> input)
{    WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();    return input.apply(new GroupByKeyOnly<>()).apply(new SortValuesByTimestamp<>()).apply(new GroupAlsoByWindow<>(windowingStrategy)).setWindowingStrategyInternal(gbkTransform.updateWindowingStrategy(windowingStrategy));}
public static MultimapView<K, V> beam_f2204_0(Coder<K> keyCoder, Iterable<KV<K, V>> values)
{                    Multimap<Object, Object> multimap = ArrayListMultimap.create();    for (KV<K, V> value : values) {        multimap.put(keyCoder.structuralValue(value.getKey()), value.getValue());    }    return new InMemoryMultimapSideInputView(keyCoder, Multimaps.unmodifiableMultimap(multimap));}
public Iterable<V> beam_f2205_0(K k)
{    return structuralKeyToValuesMap.get(keyCoder.structuralValue(k));}
public SetState<T> beam_f2214_0(StateTag<SetState<T>> spec, Coder<T> elemCoder)
{    return new InMemorySet<>(elemCoder);}
public MapState<KeyT, ValueT> beam_f2215_0(StateTag<MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    return new InMemoryMap<>(mapKeyCoder, mapValueCoder);}
public boolean beam_f2224_0()
{    return isCleared;}
public InMemoryWatermarkHold beam_f2225_0()
{    return this;}
public String beam_f2234_0()
{    return Objects.toString(combinedHold);}
public InMemoryWatermarkHold<W> beam_f2235_0()
{    InMemoryWatermarkHold<W> that = new InMemoryWatermarkHold<>(timestampCombiner);    that.combinedHold = this.combinedHold;    return that;}
public void beam_f2244_0(AccumT accum)
{    isCleared = false;    this.accum = combineFn.mergeAccumulators(Arrays.asList(this.accum, accum));}
public AccumT beam_f2245_0(Iterable<AccumT> accumulators)
{    return combineFn.mergeAccumulators(accumulators);}
public ReadableState<Boolean> beam_f2254_0()
{    return this;}
public Boolean beam_f2255_0()
{    return contents.isEmpty();}
public boolean beam_f2264_0()
{    return contents.isEmpty();}
public ReadableState<Boolean> beam_f2265_0()
{    return new ReadableState<Boolean>() {        @Override        public ReadableState<Boolean> readLater() {            return this;        }        @Override        public Boolean read() {            return contents.isEmpty();        }    };}
public static CollectionViewState<T> beam_f2274_0(Collection<T> collection)
{    return new CollectionViewState<>(collection);}
public Iterable<T> beam_f2275_0()
{    return ImmutableList.copyOf(collection);}
public Instant beam_f2284_0(TimeDomain domain)
{    try {        switch(domain) {            case EVENT_TIME:                return watermarkTimers.first().getTimestamp();            case PROCESSING_TIME:                return processingTimers.first().getTimestamp();            case SYNCHRONIZED_PROCESSING_TIME:                return synchronizedProcessingTimers.first().getTimestamp();            default:                throw new IllegalArgumentException("Unexpected time domain: " + domain);        }    } catch (NoSuchElementException exc) {        return null;    }}
private NavigableSet<TimerData> beam_f2285_0(TimeDomain domain)
{    switch(domain) {        case EVENT_TIME:            return watermarkTimers;        case PROCESSING_TIME:            return processingTimers;        case SYNCHRONIZED_PROCESSING_TIME:            return synchronizedProcessingTimers;        default:            throw new IllegalArgumentException("Unexpected time domain: " + domain);    }}
public String beam_f2294_0()
{    return MoreObjects.toStringHelper(getClass()).add("watermarkTimers", watermarkTimers).add("processingTimers", processingTimers).add("synchronizedProcessingTimers", synchronizedProcessingTimers).add("inputWatermarkTime", inputWatermarkTime).add("outputWatermarkTime", outputWatermarkTime).add("processingTime", processingTime).toString();}
public void beam_f2295_0(Instant newInputWatermark) throws Exception
{    checkNotNull(newInputWatermark);    checkState(!newInputWatermark.isBefore(inputWatermarkTime), "Cannot move input watermark time backwards from %s to %s", inputWatermarkTime, newInputWatermark);    WindowTracing.trace("{}.advanceInputWatermark: from {} to {}", getClass().getSimpleName(), inputWatermarkTime, newInputWatermark);    inputWatermarkTime = newInputWatermark;}
public Coder<K> beam_f2304_0()
{    return keyCoder;}
public Coder<ElemT> beam_f2305_0()
{    return elemCoder;}
public K beam_f2314_0()
{    return key;}
public Iterable<TimerData> beam_f2315_0()
{    return timers;}
public void beam_f2324_0()
{    doFnRunner.finishBundle();}
public Iterable<WindowedValue<InputT>> beam_f2325_0(final K key, Iterable<WindowedValue<InputT>> elements)
{    Iterable<Iterable<WindowedValue<InputT>>> windowsExpandedElements = StreamSupport.stream(elements.spliterator(), false).map(input -> input.getWindows().stream().map(window -> WindowedValue.of(input.getValue(), input.getTimestamp(), window, input.getPane())).collect(Collectors.toList())).collect(Collectors.toList());    Iterable<WindowedValue<InputT>> concatElements = Iterables.concat(windowsExpandedElements);        for (WindowedValue<InputT> input : concatElements) {        BoundedWindow window = Iterables.getOnlyElement(input.getWindows());        if (canDropDueToExpiredWindow(window)) {                        droppedDueToLateness.inc();            WindowTracing.debug("{}: Dropping element at {} for key:{}; window:{} " + "since too far behind inputWatermark:{}; outputWatermark:{}", LateDataFilter.class.getSimpleName(), input.getTimestamp(), key, window, timerInternals.currentInputWatermarkTime(), timerInternals.currentOutputWatermarkTime());        }    }        return StreamSupport.stream(concatElements.spliterator(), false).filter(input -> {        BoundedWindow window = Iterables.getOnlyElement(input.getWindows());        return !canDropDueToExpiredWindow(window);    }).collect(Collectors.toList());}
public boolean beam_f2334_0(W window)
{    return activeWindowToStateAddressWindows.containsKey(window);}
public void beam_f2335_0(W window)
{    if (!activeWindowToStateAddressWindows.containsKey(window)) {                activeWindowToStateAddressWindows.put(window, new LinkedHashSet<>());    }}
private void beam_f2344_0(Collection<W> toBeMerged, W mergeResult) throws Exception
{                        Set<W> newStateAddressWindows = new LinkedHashSet<>();    Set<W> existingStateAddressWindows = activeWindowToStateAddressWindows.get(mergeResult);    if (existingStateAddressWindows != null) {                newStateAddressWindows.addAll(existingStateAddressWindows);    }    for (W other : toBeMerged) {        Set<W> otherStateAddressWindows = activeWindowToStateAddressWindows.get(other);        checkState(otherStateAddressWindows != null, "Window %s is not ACTIVE or NEW", other);        for (W otherStateAddressWindow : otherStateAddressWindows) {                                    newStateAddressWindows.add(otherStateAddressWindow);        }        activeWindowToStateAddressWindows.remove(other);        }    if (newStateAddressWindows.isEmpty()) {                        newStateAddressWindows.add(mergeResult);    }    activeWindowToStateAddressWindows.put(mergeResult, newStateAddressWindows);    merged(mergeResult);}
public void beam_f2345_0(W window)
{        Set<W> stateAddressWindows = activeWindowToStateAddressWindows.get(window);    checkState(stateAddressWindows != null, "Window %s is not ACTIVE", window);    W first = Iterables.getFirst(stateAddressWindows, null);    stateAddressWindows.clear();    stateAddressWindows.add(first);}
private static Map<W, Set<W>> beam_f2354_0(Map<W, Set<W>> multimap)
{    Map<W, Set<W>> newMultimap = new HashMap<>();    for (Map.Entry<W, Set<W>> entry : multimap.entrySet()) {        newMultimap.put(entry.getKey(), new LinkedHashSet<>(entry.getValue()));    }    return newMultimap;}
public void beam_f2355_0(long n)
{    value.addAndGet(n);    dirty.afterModification();}
public MetricQueryResults beam_f2364_0(@Nullable MetricsFilter filter)
{    return MetricQueryResults.create(Iterables.filter(counters, counter -> MetricFiltering.matches(filter, counter.getKey())), Iterables.filter(distributions, distribution -> MetricFiltering.matches(filter, distribution.getKey())), Iterables.filter(gauges, gauge -> MetricFiltering.matches(filter, gauge.getKey())));}
public void beam_f2365_0()
{    dirty.set(State.DIRTY);}
public DistributionData beam_f2374_0()
{    return value.get();}
public MetricName beam_f2375_0()
{    return name;}
public static ExecutionStateSampler beam_f2384_0(MillisProvider clock)
{    return new ExecutionStateSampler(checkNotNull(clock));}
public void beam_f2385_0()
{    start(Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true).setNameFormat("state-sampler-%d").build()));}
public static ExecutionStateTracker beam_f2395_0()
{    return new ExecutionStateTracker(ExecutionStateSampler.newForTest());}
public int beam_f2396_0()
{    return System.identityHashCode(this);}
private void beam_f2405_0()
{    numTransitions++;}
public long beam_f2406_0()
{    return numTransitions;}
public boolean beam_f2415_0(Object object)
{    if (object instanceof GaugeCell) {        GaugeCell gaugeCell = (GaugeCell) object;        return Objects.equals(dirty, gaugeCell.dirty) && Objects.equals(gaugeValue.get(), gaugeCell.gaugeValue.get()) && Objects.equals(name, gaugeCell.name);    }    return false;}
public int beam_f2416_0()
{    return Objects.hash(dirty, gaugeValue.get(), name);}
public CounterCell beam_f2425_0(MetricName metricName)
{    return counters.get(metricName);}
public CounterCell beam_f2426_0(MetricName metricName)
{    return counters.tryGet(metricName);}
public Iterable<MonitoringInfo> beam_f2435_0()
{        ArrayList<MonitoringInfo> monitoringInfos = new ArrayList<MonitoringInfo>();    MetricUpdates metricUpdates = this.getUpdates();    for (MetricUpdate<Long> metricUpdate : metricUpdates.counterUpdates()) {        MonitoringInfo mi = counterUpdateToMonitoringInfo(metricUpdate);        if (mi != null) {            monitoringInfos.add(mi);        }    }    for (MetricUpdate<org.apache.beam.runners.core.metrics.DistributionData> metricUpdate : metricUpdates.distributionUpdates()) {        MonitoringInfo mi = distributionUpdateToMonitoringInfo(metricUpdate);        if (mi != null) {            monitoringInfos.add(mi);        }    }    return monitoringInfos;}
private void beam_f2436_0(MetricsMap<MetricName, ? extends MetricCell<?>> cells)
{    for (MetricCell<?> cell : cells.values()) {        cell.getDirty().afterCommit();    }}
public boolean beam_f2445_0(Object object)
{    if (object instanceof MetricsContainerImpl) {        MetricsContainerImpl metricsContainerImpl = (MetricsContainerImpl) object;        return Objects.equals(stepName, metricsContainerImpl.stepName) && Objects.equals(counters, metricsContainerImpl.counters) && Objects.equals(distributions, metricsContainerImpl.distributions) && Objects.equals(gauges, metricsContainerImpl.gauges);    }    return false;}
public int beam_f2446_0()
{    return Objects.hash(stepName, counters, distributions, gauges);}
public Iterable<MonitoringInfo> beam_f2455_0()
{        ArrayList<MonitoringInfo> monitoringInfos = new ArrayList<>();    for (MetricsContainerImpl container : getMetricsContainers()) {        for (MonitoringInfo mi : container.getMonitoringInfos()) {            monitoringInfos.add(mi);        }    }    return monitoringInfos;}
public String beam_f2456_0()
{    return asAttemptedOnlyMetricResults(this).toString();}
public int beam_f2465_0()
{    return metrics.hashCode();}
public void beam_f2466_0()
{    if (!(metricsSink instanceof NoOpMetricsSink)) {        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryBuilder().setDaemon(true).setNameFormat("MetricsPusher-thread").build());        scheduledFuture = scheduler.scheduleAtFixedRate(this::run, 0, period, TimeUnit.SECONDS);    }}
public static MetricUpdate<T> beam_f2475_0(MetricKey key, T update)
{    return new AutoValue_MetricUpdates_MetricUpdate(key, update);}
public boolean beam_f2476_0()
{    return Iterables.isEmpty(counterUpdates()) && Iterables.isEmpty(distributionUpdates());}
public static MonitoringInfoMetricName beam_f2485_0(MetricsApi.MonitoringInfo mi)
{    return new MonitoringInfoMetricName(mi.getUrn(), mi.getLabelsMap());}
public static MonitoringInfoMetricName beam_f2486_0(String urn, HashMap<String, String> labels)
{    return new MonitoringInfoMetricName(urn, labels);}
public void beam_f2496_1(Thread trackedThread, long millis)
{    }
 static String beam_f2497_0(Duration duration)
{    return DURATION_FORMATTER.print(duration.toPeriod());}
public void beam_f2506_0(MonitoringInfo monitoringInfo)
{    this.builder.mergeFrom(monitoringInfo);}
public static MonitoringInfo beam_f2507_0(MonitoringInfo input)
{    MonitoringInfo.Builder builder = MonitoringInfo.newBuilder();    builder.mergeFrom(input);    builder.clearTimestamp();    return builder.build();}
public ReadableState<Boolean> beam_f2520_0(StateAccessor<K> state)
{    return state.access(PANE_ADDITIONS_TAG).isEmpty();}
public void beam_f2521_0(MergingStateAccessor<K, W> state)
{    StateMerging.prefetchCombiningValues(state, PANE_ADDITIONS_TAG);}
public static NullSideInputReader beam_f2538_0(Iterable<? extends PCollectionView<?>> views)
{    return new NullSideInputReader(views);}
public T beam_f2539_0(PCollectionView<T> view, BoundedWindow window)
{    throw new IllegalArgumentException("cannot call NullSideInputReader.get()");}
public TimeDomain beam_f2548_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Access to time domain not supported in ProcessElement");}
public OutputReceiver<OutputT> beam_f2549_0(DoFn<InputT, OutputT> doFn)
{    return DoFnOutputReceivers.windowedReceiver(processContext, null);}
public DoFn<InputT, OutputT>.OnTimerContext beam_f2558_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Access to timers not supported in Splittable DoFn");}
public State beam_f2559_0(String stateId)
{    throw new UnsupportedOperationException("Access to state not supported in Splittable DoFn");}
public Instant beam_f2568_0()
{    return element.getTimestamp();}
public PaneInfo beam_f2569_0()
{    return element.getPane();}
public void beam_f2578_0(StateAccessor<?> state)
{    state.access(PANE_INFO_TAG).clear();}
public void beam_f2579_0(ReduceFn<?, ?, ?, ?>.Context context)
{    context.state().access(PaneInfoTracker.PANE_INFO_TAG).readLater();}
public PeekingReiterator<T> beam_f2588_0()
{    return new PeekingReiterator<>(this);}
public T beam_f2589_0()
{    computeNext();    if (!nextElementComputed) {        throw new NoSuchElementException();    }    return nextElement;}
private boolean beam_f2598_0(BoundedWindow mainInputWindow)
{    for (PCollectionView<?> view : views) {        BoundedWindow sideInputWindow = view.getWindowMappingFn().getSideInputWindow(mainInputWindow);        if (!sideInputReader.isReady(view, sideInputWindow)) {            return false;        }    }    return true;}
private StateAccessorImpl<K, W> beam_f2601_0(W window, StateStyle style)
{    return new StateAccessorImpl<>(activeWindows, windowingStrategy.getWindowFn().windowCoder(), stateInternals, stateContextFromComponents(options, sideInputReader, window), style);}
public Instant beam_f2610_0()
{    return timerInternals.currentSynchronizedProcessingTime();}
public Instant beam_f2611_0()
{    return timerInternals.currentInputWatermarkTime();}
public Map<W, StateT> beam_f2620_0(StateTag<StateT> address)
{    ImmutableMap.Builder<W, StateT> builder = ImmutableMap.builder();    for (W stateAddressWindow : activeWindows.readStateAddresses(context.window())) {        StateT stateForWindow = stateInternals.state(namespaceFor(stateAddressWindow), address, context);        builder.put(stateAddressWindow, stateForWindow);    }    return builder.build();}
public K beam_f2621_0()
{    return key;}
public InputT beam_f2630_0()
{    return value;}
public Instant beam_f2631_0()
{    return timestamp;}
public K beam_f2640_0()
{    return key;}
public WindowingStrategy<?, W> beam_f2641_0()
{    return windowingStrategy;}
private static StateContext<W> beam_f2650_0(@Nullable final PipelineOptions options, @Nullable final SideInputReader sideInputReader, final W mainInputWindow)
{    if (options == null || sideInputReader == null) {        return StateContexts.windowOnlyContext(mainInputWindow);    } else {        return new StateContext<W>() {            @Override            public PipelineOptions getPipelineOptions() {                return options;            }            @Override            public <T> T sideInput(PCollectionView<T> view) {                return sideInputReader.get(view, view.getWindowMappingFn().getSideInputWindow(mainInputWindow));            }            @Override            public W window() {                return mainInputWindow;            }        };    }}
public PipelineOptions beam_f2651_0()
{    return options;}
 boolean beam_f2660_0()
{    return activeWindows.getActiveAndNewWindows().isEmpty();}
private Set<W> beam_f2661_0(Collection<W> windows)
{    Set<W> result = new HashSet<>();    for (W window : windows) {        ReduceFn<K, InputT, OutputT, W>.Context directContext = contextFactory.base(window, StateStyle.DIRECT);        if (!triggerRunner.isClosed(directContext.state())) {            result.add(window);        }    }    return result;}
private ImmutableSet<W> beam_f2670_0(final Map<W, W> windowToMergeResult, final Collection<? extends BoundedWindow> windows)
{    return ImmutableSet.copyOf(FluentIterable.from(windows).transform(untypedWindow -> {        @SuppressWarnings("unchecked")        W window = (W) untypedWindow;        W mergedWindow = windowToMergeResult.get(window);                return (mergedWindow == null) ? window : mergedWindow;    }));}
private void beam_f2671_0(Collection<W> windows)
{        for (W window : windows) {        ReduceFn<K, InputT, OutputT, W>.Context directContext = contextFactory.base(window, StateStyle.DIRECT);        triggerRunner.prefetchForValue(window, directContext.state());    }}
private void beam_f2680_0(final ReduceFn<K, InputT, OutputT, W>.Context directContext, ReduceFn<K, InputT, OutputT, W>.Context renamedContext)
{    paneInfoTracker.prefetchPaneInfo(directContext);    watermarkHold.prefetchExtract(renamedContext);    nonEmptyPanes.isEmpty(renamedContext.state()).readLater();    reduceFn.prefetchOnTrigger(directContext.state());}
private Instant beam_f2681_0(final ReduceFn<K, InputT, OutputT, W>.Context directContext, ReduceFn<K, InputT, OutputT, W>.Context renamedContext, final boolean isFinished, boolean isEndOfWindow) throws Exception
{        final WatermarkHold.OldAndNewHolds pair = watermarkHold.extractAndRelease(renamedContext, isFinished).read();        final Instant outputTimestamp = pair.oldHold;    @Nullable    Instant newHold = pair.newHold;    final boolean isEmpty = nonEmptyPanes.isEmpty(renamedContext.state()).read();    if (isEmpty && windowingStrategy.getClosingBehavior() == ClosingBehavior.FIRE_IF_NON_EMPTY && windowingStrategy.getOnTimeBehavior() == Window.OnTimeBehavior.FIRE_IF_NON_EMPTY) {        return newHold;    }    Instant inputWM = timerInternals.currentInputWatermarkTime();    if (newHold != null) {                checkState(!isFinished, "new hold at %s but finished %s", newHold, directContext.window());                checkState(!newHold.isBefore(inputWM), "new hold %s is before input watermark %s", newHold, inputWM);        if (newHold.isAfter(directContext.window().maxTimestamp())) {                        checkState(newHold.isEqual(LateDataUtils.garbageCollectionTime(directContext.window(), windowingStrategy)), "new hold %s should be at garbage collection for window %s plus %s", newHold, directContext.window(), windowingStrategy.getAllowedLateness());        } else {                        checkState(newHold.isEqual(directContext.window().maxTimestamp()), "new hold %s should be at end of window %s", newHold, directContext.window());            checkState(!isEndOfWindow, "new hold at %s for %s but this is the watermark trigger", newHold, directContext.window());        }    }        final PaneInfo pane = paneInfoTracker.getNextPaneInfo(directContext, isFinished).read();        if (needToEmit(isEmpty, isFinished, pane.getTiming())) {                final List<W> windows = Collections.singletonList(directContext.window());        ReduceFn<K, InputT, OutputT, W>.OnTriggerContext renamedTriggerContext = contextFactory.forTrigger(directContext.window(), pane, StateStyle.RENAMED, toOutput -> {                        if (!isFinished) {                paneInfoTracker.storeCurrentPaneInfo(directContext, pane);            }                        outputter.outputWindowedValue(KV.of(key, toOutput), outputTimestamp, windows, pane);        });        reduceFn.onTrigger(renamedTriggerContext);    }    return newHold;}
public T beam_f2690_0(PCollectionView<T> view, BoundedWindow window)
{    Iterable<?> elements = getIterable(view, window);                ViewFn<MultimapView, T> viewFn = (ViewFn<MultimapView, T>) view.getViewFn();    Coder<?> keyCoder = ((KvCoder<?, ?>) view.getCoderInternal()).getKeyCoder();    return (T) viewFn.apply(InMemoryMultimapSideInputView.fromIterable(keyCoder, (Iterable) elements));}
public Iterable<?> beam_f2691_0(PCollectionView<T> view, BoundedWindow window)
{    @SuppressWarnings("unchecked")    Coder<BoundedWindow> windowCoder = (Coder<BoundedWindow>) view.getWindowingStrategyInternal().getWindowFn().windowCoder();    StateTag<ValueState<Iterable<?>>> stateTag = sideInputContentsTags.get(view);    ValueState<Iterable<?>> state = stateInternals.state(StateNamespaces.window(windowCoder, window), stateTag);    Iterable<?> elements = state.read();        return (elements != null) ? elements : Collections.emptyList();}
public void beam_f2700_0()
{        try {        invoker.invokeStartBundle(new DoFnStartBundleContext());    } catch (Throwable t) {                throw wrapUserCodeException(t);    }}
public void beam_f2701_0(WindowedValue<InputT> compressedElem)
{    if (observesWindow) {        for (WindowedValue<InputT> elem : compressedElem.explodeWindows()) {            invokeProcessElement(elem);        }    } else {        invokeProcessElement(compressedElem);    }}
public BoundedWindow beam_f2710_0()
{    throw new UnsupportedOperationException("Cannot access window outside of @ProcessElement and @OnTimer methods.");}
public PaneInfo beam_f2711_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access paneInfo outside of @ProcessElement methods.");}
public TimeDomain beam_f2720_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access time domain outside of @ProcessTimer method.");}
public OutputReceiver<OutputT> beam_f2721_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access output receiver outside of @ProcessElement method.");}
public PaneInfo beam_f2730_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access paneInfo outside of @ProcessElement methods.");}
public PipelineOptions beam_f2731_0()
{    return getPipelineOptions();}
public OutputReceiver<OutputT> beam_f2740_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access outputReceiver in @FinishBundle method.");}
public OutputReceiver<Row> beam_f2741_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException("Cannot access outputReceiver in @FinishBundle method.");}
public PipelineOptions beam_f2750_0()
{    return options;}
public InputT beam_f2751_0()
{    return elem.getValue();}
public Collection<? extends BoundedWindow> beam_f2760_0()
{    return elem.getWindows();}
private void beam_f2761_0(Instant timestamp)
{        if (fn.getAllowedTimestampSkew().getMillis() != Long.MAX_VALUE && timestamp.isBefore(elem.getTimestamp().minus(fn.getAllowedTimestampSkew()))) {        throw new IllegalArgumentException(String.format("Cannot output with timestamp %s. Output timestamps must be no earlier than the " + "timestamp of the current input (%s) minus the allowed skew (%s). See the " + "DoFn#getAllowedTimestampSkew() Javadoc for details on changing the allowed " + "skew.", timestamp, elem.getTimestamp(), PeriodFormat.getDefault().print(fn.getAllowedTimestampSkew().toPeriod())));    }}
public Object beam_f2770_0(int index)
{    SerializableFunction converter = doFnSchemaInformation.getElementConverters().get(index);    return converter.apply(element());}
public Instant beam_f2771_0(DoFn<InputT, OutputT> doFn)
{    return timestamp();}
private StateNamespace beam_f2780_0()
{    if (namespace == null) {        namespace = StateNamespaces.window(windowCoder, window);    }    return namespace;}
public Instant beam_f2781_0()
{    return timestamp;}
public Object beam_f2790_0(String tagId)
{    throw new UnsupportedOperationException("SideInput parameters are not supported.");}
public Object beam_f2791_0(int index)
{    throw new UnsupportedOperationException("Element parameters are not supported.");}
public Timer beam_f2800_0(String timerId)
{    try {        TimerSpec spec = (TimerSpec) signature.timerDeclarations().get(timerId).field().get(fn);        return new TimerInternalsTimer(window, getNamespace(), timerId, spec, stepContext.timerInternals());    } catch (IllegalAccessException e) {        throw new RuntimeException(e);    }}
public PipelineOptions beam_f2801_0()
{    return options;}
private Instant beam_f2810_0(Instant target)
{    if (TimeDomain.EVENT_TIME.equals(spec.getTimeDomain())) {        Instant windowExpiry = LateDataUtils.garbageCollectionTime(window, allowedLateness);        if (target.isAfter(windowExpiry)) {            return windowExpiry;        }    }    return target;}
private void beam_f2811_0(Instant target)
{    timerInternals.setTimer(namespace, timerId, target, spec.getTimeDomain());}
public PCollection<KeyedWorkItem<KeyT, InputT>> beam_f2820_0(PCollection<KV<KeyT, InputT>> input)
{    KvCoder<KeyT, InputT> kvCoder = (KvCoder<KeyT, InputT>) input.getCoder();    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), input.isBounded(), KeyedWorkItemCoder.of(kvCoder.getKeyCoder(), kvCoder.getValueCoder(), input.getWindowingStrategy().getWindowFn().windowCoder()));}
public String beam_f2821_0()
{    return SplittableParDo.SPLITTABLE_GBKIKWI_URN;}
public TupleTagList beam_f2830_0()
{    return original.getAdditionalOutputTags();}
public PCollectionTuple beam_f2831_0(PCollection<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>> input)
{    return ProcessKeyedElements.createPrimitiveOutputFor(input, original.getFn(), original.getMainOutputTag(), original.getAdditionalOutputTags(), original.getOutputTagsToCoders(), original.getInputWindowingStrategy());}
public void beam_f2840_0() throws Exception
{    invoker.invokeTeardown();}
public void beam_f2841_0(StartBundleContext c) throws Exception
{    invoker.invokeStartBundle(wrapContextAsStartBundle(c));}
private void beam_f2850_0()
{    throw new UnsupportedOperationException(String.format("Splittable DoFn can only output from @%s", ProcessElement.class.getSimpleName()));}
public RestrictionT beam_f2851_0()
{    return residualRestriction;}
public void beam_f2860_0()
{    doFnRunner.finishBundle();}
public Instant beam_f2861_0()
{    return timerInternals.currentInputWatermarkTime();}
public static void beam_f2870_0(MergingStateAccessor<K, W> context, StateTag<SetState<T>> address)
{    mergeSets(context.accessInEachMergingWindow(address).values(), context.access(address));}
public static void beam_f2871_0(Collection<SetState<T>> sources, SetState<T> result)
{    if (sources.isEmpty()) {                return;    }        List<ReadableState<Iterable<T>>> futures = new ArrayList<>(sources.size());    for (SetState<T> source : sources) {        if (!source.equals(result)) {            prefetchRead(source);            futures.add(source);        }    }    if (futures.isEmpty()) {                return;    }        for (ReadableState<Iterable<T>> future : futures) {        for (T element : future.read()) {            result.add(element);        }    }        for (SetState<T> source : sources) {        if (!source.equals(result)) {            source.clear();        }    }}
public void beam_f2880_0(Appendable sb) throws IOException
{    sb.append(key);}
public static StateNamespace beam_f2881_0()
{    return new GlobalNamespace();}
public W beam_f2890_0()
{    return window;}
public String beam_f2891_0()
{    try {                return "/" + CoderUtils.encodeToBase64(windowCoder, window) + "/";    } catch (CoderException e) {        throw new RuntimeException("Unable to generate string key from window " + window, e);    }}
public String beam_f2900_0()
{    try {                return "/" + CoderUtils.encodeToBase64(windowCoder, window) +         "/" + Integer.toString(triggerIndex, TRIGGER_RADIX).toUpperCase() + "/";    } catch (CoderException e) {        throw new RuntimeException("Unable to generate string key from window " + window, e);    }}
public void beam_f2901_0(Appendable sb) throws IOException
{    sb.append('/').append(CoderUtils.encodeToBase64(windowCoder, window));    sb.append('/').append(Integer.toString(triggerIndex, TRIGGER_RADIX).toUpperCase());    sb.append('/');}
public void beam_f2910_0(StateNamespace namespace)
{    stateTable.rowKeySet().remove(namespace);}
public void beam_f2911_0()
{    stateTable.clear();}
public BagState<T> beam_f2920_0(String id, StateSpec<BagState<T>> spec, Coder<T> elemCoder)
{    return binder.bindBag(tagForSpec(id, spec), elemCoder);}
public SetState<T> beam_f2921_0(String id, StateSpec<SetState<T>> spec, Coder<T> elemCoder)
{    return binder.bindSet(tagForSpec(id, spec), elemCoder);}
public static StateTag<CombiningState<InputT, AccumT, OutputT>> beam_f2930_0(String id, Coder<InputT> inputCoder, CombineFn<InputT, AccumT, OutputT> combineFn)
{    return new SimpleStateTag<>(new StructuredId(id), StateSpecs.combiningFromInputInternal(inputCoder, combineFn));}
public static StateTag<BagState<T>> beam_f2931_0(String id, Coder<T> elemCoder)
{    return new SimpleStateTag<>(new StructuredId(id), StateSpecs.bag(elemCoder));}
public String beam_f2940_0()
{    return MoreObjects.toStringHelper(getClass()).add("id", rawId).add("kind", kind).toString();}
public boolean beam_f2941_0(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof StructuredId)) {        return false;    }    StructuredId that = (StructuredId) obj;    return Objects.equals(this.kind, that.kind) && Objects.equals(this.rawId, that.rawId);}
public int beam_f2950_0()
{    return Objects.hash(getClass(), this.getId());}
public static SystemReduceFn<K, T, Iterable<T>, Iterable<T>, W> beam_f2951_0(final Coder<T> inputCoder)
{    final StateTag<BagState<T>> bufferTag = StateTags.makeSystemTagInternal(StateTags.bag(BUFFER_NAME, inputCoder));    return new SystemReduceFn<K, T, Iterable<T>, Iterable<T>, W>(bufferTag) {        @Override        public void prefetchOnMerge(MergingStateAccessor<K, W> state) throws Exception {            StateMerging.prefetchBags(state, bufferTag);        }        @Override        public void onMerge(OnMergeContext c) throws Exception {            StateMerging.mergeBags(c.state(), bufferTag);        }    };}
public void beam_f2960_0(OnTriggerContext c) throws Exception
{    c.output(c.state().access(bufferTag).read());}
public void beam_f2961_0(Context c) throws Exception
{    c.state().access(bufferTag).clear();}
public void beam_f2970_0(TimerData timer, OutputStream outStream) throws CoderException, IOException
{    STRING_CODER.encode(timer.getTimerId(), outStream);    STRING_CODER.encode(timer.getNamespace().stringKey(), outStream);    INSTANT_CODER.encode(timer.getTimestamp(), outStream);    STRING_CODER.encode(timer.getDomain().name(), outStream);}
public TimerData beam_f2971_0(InputStream inStream) throws CoderException, IOException
{    String timerId = STRING_CODER.decode(inStream);    StateNamespace namespace = StateNamespaces.fromString(STRING_CODER.decode(inStream), windowCoder);    Instant timestamp = INSTANT_CODER.decode(inStream);    TimeDomain domain = TimeDomain.valueOf(STRING_CODER.decode(inStream));    return TimerData.of(timerId, namespace, timestamp, domain);}
public String beam_f2980_0()
{    StringBuilder builder = new StringBuilder("AfterAll.of(");    Joiner.on(", ").appendTo(builder, subTriggers);    builder.append(")");    return builder.toString();}
private Instant beam_f2981_0(OnElementContext c)
{    return computeTargetTimestamp(c.currentProcessingTime());}
public void beam_f2990_0(OnElementContext c) throws Exception
{    GroupingState<Instant, Instant> delayUntilState = c.state().access(DELAYED_UNTIL_TAG);    Instant oldDelayUntil = delayUntilState.read();        if (oldDelayUntil != null) {        return;    }    Instant targetTimestamp = getTargetTimestamp(c);    delayUntilState.add(targetTimestamp);    c.setTimer(targetTimestamp, timeDomain);}
public void beam_f2991_0(MergingStateAccessor<?, ?> state)
{    super.prefetchOnMerge(state);    StateMerging.prefetchCombiningValues(state, DELAYED_UNTIL_TAG);}
public int beam_f3000_0()
{    return Objects.hash(delay);}
public String beam_f3001_0()
{    return PERIOD_FORMATTER.print(delay.toPeriod());}
public void beam_f3010_0(TriggerStateMachine.TriggerContext context) throws Exception
{    context.trigger().firstUnfinishedSubTrigger().invokeOnFire(context);        if (context.trigger().isMerging()) {        for (ExecutableTriggerStateMachine subTrigger : context.trigger().subTriggers()) {            subTrigger.invokeClear(context);        }    }    updateFinishedState(context);}
public String beam_f3011_0()
{    StringBuilder builder = new StringBuilder("AfterEach.inOrder(");    Joiner.on(", ").appendTo(builder, subTriggers);    builder.append(")");    return builder.toString();}
private void beam_f3020_0(TriggerContext c)
{    boolean anyFinished = false;    for (ExecutableTriggerStateMachine subTrigger : c.trigger().subTriggers()) {        anyFinished |= c.forTrigger(subTrigger).trigger().isFinished();    }    c.trigger().setFinished(anyFinished);}
public int beam_f3021_0()
{    return countElems;}
public String beam_f3030_0()
{    return "AfterPane.elementCountAtLeast(" + countElems + ")";}
public boolean beam_f3031_0(Object obj)
{    if (this == obj) {        return true;    }    if (!(obj instanceof AfterPaneStateMachine)) {        return false;    }    AfterPaneStateMachine that = (AfterPaneStateMachine) obj;    return this.countElems == that.countElems;}
public static AfterSynchronizedProcessingTimeStateMachine beam_f3040_0()
{    return new AfterSynchronizedProcessingTimeStateMachine();}
public Instant beam_f3041_0(TriggerStateMachine.TriggerContext context)
{    return context.currentSynchronizedProcessingTime();}
public void beam_f3050_0(OnMergeContext c) throws Exception
{            ExecutableTriggerStateMachine earlySubtrigger = c.trigger().subTrigger(EARLY_INDEX);            OnMergeContext earlyContext = c.forTrigger(earlySubtrigger);        if (!earlyContext.trigger().finishedInAllMergingWindows() || !endOfWindowReached(c)) {        earlySubtrigger.invokeOnMerge(earlyContext);        earlyContext.trigger().setFinished(false);        if (lateTrigger != null) {            ExecutableTriggerStateMachine lateSubtrigger = c.trigger().subTrigger(LATE_INDEX);            OnMergeContext lateContext = c.forTrigger(lateSubtrigger);            lateContext.trigger().setFinished(false);            lateSubtrigger.invokeClear(lateContext);        }    } else {                earlyContext.trigger().setFinished(true);        if (lateTrigger != null) {            c.trigger().subTrigger(LATE_INDEX).invokeOnMerge(c);        }    }}
private boolean beam_f3051_0(TriggerStateMachine.TriggerContext context)
{    return context.currentEventTime() != null && context.currentEventTime().isAfter(context.window().maxTimestamp());}
public void beam_f3060_0(OnMergeContext c) throws Exception
{    if (!c.trigger().finishedInAllMergingWindows()) {                                c.trigger().setFinished(false);    } else if (!endOfWindowReached(c)) {                c.trigger().setFinished(false);    } else {                c.trigger().setFinished(true);    }}
public String beam_f3061_0()
{    return TO_STRING;}
public boolean beam_f3071_0(TriggerStateMachine other)
{        return other instanceof DefaultTriggerStateMachine;}
public boolean beam_f3072_0(TriggerStateMachine.TriggerContext context) throws Exception
{    return endOfWindowReached(context);}
public final int beam_f3082_0()
{    return firstIndexAfterSubtree;}
public boolean beam_f3083_0(ExecutableTriggerStateMachine other)
{    return trigger.isCompatible(other.trigger);}
public BitSet beam_f3092_0()
{    return bitSet;}
public boolean beam_f3093_0(ExecutableTriggerStateMachine trigger)
{    return bitSet.get(trigger.getTriggerIndex());}
public FinishedTriggersSet beam_f3102_0()
{    return fromSet(Sets.newHashSet(finishedTriggers));}
public static NeverStateMachine beam_f3103_0()
{        return new NeverStateMachine();}
public static RepeatedlyStateMachine beam_f3114_0(TriggerStateMachine repeated)
{    return new RepeatedlyStateMachine(repeated);}
public void beam_f3115_0(OnElementContext c) throws Exception
{    getRepeated(c).invokeOnElement(c);}
public void beam_f3127_0(StateAccessor<?> state)
{    if (subTriggers != null) {        for (TriggerStateMachine trigger : subTriggers) {            trigger.prefetchOnElement(state);        }    }}
public void beam_f3128_0(MergingStateAccessor<?, ?> state)
{    if (subTriggers != null) {        for (TriggerStateMachine trigger : subTriggers) {            trigger.prefetchOnMerge(state);        }    }}
public TriggerStateMachine beam_f3137_0(TriggerStateMachine until)
{    return new OrFinallyStateMachine(this, until);}
public TriggerStateMachine.TriggerContext beam_f3138_0(W window, Timers timers, ExecutableTriggerStateMachine rootTrigger, FinishedTriggers finishedSet)
{    return new TriggerContextImpl(window, timers, rootTrigger, finishedSet);}
public boolean beam_f3147_0(int subtriggerIndex)
{    return finishedSet.isFinished(subTrigger(subtriggerIndex));}
public boolean beam_f3148_0()
{    return Iterables.isEmpty(unfinishedSubTriggers());}
public Instant beam_f3157_0()
{    return timers.currentSynchronizedProcessingTime();}
public Instant beam_f3158_0()
{    return timers.currentEventTime();}
public StateAccessor<?> beam_f3167_0()
{    return state;}
public W beam_f3168_0()
{    return window;}
public W beam_f3177_0()
{    return window;}
public void beam_f3178_0(Instant timestamp, TimeDomain domain)
{    timers.setTimer(timestamp, domain);}
public void beam_f3187_0(Instant timestamp, TimeDomain domain)
{    timers.setTimer(timestamp, domain);}
public void beam_f3188_0(Instant timestamp, TimeDomain domain)
{    timers.setTimer(timestamp, domain);}
public void beam_f3197_0(W window, StateAccessor<?> state)
{    prefetchIsClosed(state);    rootTrigger.getSpec().prefetchOnFire(contextFactory.createStateAccessor(window, rootTrigger));}
public void beam_f3198_0(W window, StateAccessor<?> state)
{    prefetchIsClosed(state);    rootTrigger.getSpec().prefetchShouldFire(contextFactory.createStateAccessor(window, rootTrigger));}
private boolean beam_f3207_0()
{        return !(rootTrigger.getSpec() instanceof DefaultTriggerStateMachine);}
public static TriggerStateMachine beam_f3208_0(RunnerApi.Trigger trigger)
{    switch(trigger.getTriggerCase()) {        case AFTER_ALL:            return AfterAllStateMachine.of(stateMachinesForTriggers(trigger.getAfterAll().getSubtriggersList()));        case AFTER_ANY:            return AfterFirstStateMachine.of(stateMachinesForTriggers(trigger.getAfterAny().getSubtriggersList()));        case AFTER_END_OF_WINDOW:            return stateMachineForAfterEndOfWindow(trigger.getAfterEndOfWindow());        case ELEMENT_COUNT:            return AfterPaneStateMachine.elementCountAtLeast(trigger.getElementCount().getElementCount());        case AFTER_SYNCHRONIZED_PROCESSING_TIME:            return AfterSynchronizedProcessingTimeStateMachine.ofFirstElement();        case DEFAULT:            return DefaultTriggerStateMachine.of();        case NEVER:            return NeverStateMachine.ever();        case ALWAYS:            return ReshuffleTriggerStateMachine.create();        case OR_FINALLY:            return stateMachineForTrigger(trigger.getOrFinally().getMain()).orFinally(stateMachineForTrigger(trigger.getOrFinally().getFinally()));        case REPEAT:            return RepeatedlyStateMachine.forever(stateMachineForTrigger(trigger.getRepeat().getSubtrigger()));        case AFTER_EACH:            return AfterEachStateMachine.inOrder(stateMachinesForTriggers(trigger.getAfterEach().getSubtriggersList()));        case AFTER_PROCESSING_TIME:            return stateMachineForAfterProcessingTime(trigger.getAfterProcessingTime());        case TRIGGER_NOT_SET:            throw new IllegalArgumentException(String.format("Required field 'trigger' not set on %s", trigger));        default:            throw new IllegalArgumentException(String.format("Unknown trigger type %s", trigger));    }}
private Instant beam_f3217_0(Instant timestamp, W window)
{    Instant shifted = windowingStrategy.getTimestampCombiner().assign(window, windowingStrategy.getWindowFn().getOutputTime(timestamp, window));        if (shifted.isBefore(timestamp)) {        throw new IllegalStateException(String.format("TimestampCombiner moved element from %s to earlier time %s for window %s", BoundedWindow.formatTimestamp(timestamp), BoundedWindow.formatTimestamp(shifted), window));    }    checkState(timestamp.isAfter(window.maxTimestamp()) || !shifted.isAfter(window.maxTimestamp()), "TimestampCombiner moved element from %s to %s which is beyond end of " + "window %s", timestamp, shifted, window);    return shifted;}
private Instant beam_f3218_0(Instant timestamp, ReduceFn<?, ?, ?, W>.Context context)
{                Instant elementHold = shift(timestamp, context.window());    Instant outputWM = timerInternals.currentOutputWatermarkTime();    Instant inputWM = timerInternals.currentInputWatermarkTime();    String which;    boolean tooLate;        if (outputWM != null && elementHold.isBefore(outputWM)) {        which = "too late to effect output watermark";        tooLate = true;    } else if (context.window().maxTimestamp().isBefore(inputWM)) {        which = "too late for end-of-window timer";        tooLate = true;    } else {        which = "on time";        tooLate = false;        checkState(!elementHold.isAfter(BoundedWindow.TIMESTAMP_MAX_VALUE), "Element hold %s is beyond end-of-time", elementHold);        context.state().access(elementHoldTag).add(elementHold);    }    WindowTracing.trace("WatermarkHold.addHolds: element hold at {} is {} for " + "key:{}; window:{}; inputWatermark:{}; outputWatermark:{}", elementHold, which, context.key(), context.window(), inputWM, outputWM);    return tooLate ? null : elementHold;}
public Instant beam_f3227_0(ReduceFn<?, ?, ?, W>.Context context)
{    return context.state().access(elementHoldTag).read();}
public void beam_f3228_0()
{    MultimapView<byte[], Integer> view = InMemoryMultimapSideInputView.fromIterable(ByteArrayCoder.of(), ImmutableList.of(KV.of(new byte[] { 0x00 }, 0), KV.of(new byte[] { 0x01 }, 1)));    assertEquals(view.get(new byte[] { 0x00 }), ImmutableList.of(0));    assertEquals(view.get(new byte[] { 0x01 }), ImmutableList.of(1));    assertEquals(view.get(new byte[] { 0x02 }), ImmutableList.of());}
public void beam_f3237_0() throws Exception
{    InMemoryTimerInternals underTest = new InMemoryTimerInternals();    TimerData processingTime1 = TimerData.of(NS1, new Instant(19), TimeDomain.PROCESSING_TIME);    TimerData processingTime2 = TimerData.of(NS1, new Instant(29), TimeDomain.PROCESSING_TIME);    underTest.setTimer(processingTime1);    underTest.setTimer(processingTime2);    underTest.advanceProcessingTime(new Instant(20));    assertThat(underTest.removeNextProcessingTimer(), equalTo(processingTime1));    assertThat(underTest.removeNextProcessingTimer(), nullValue());        underTest.advanceProcessingTime(new Instant(21));    assertThat(underTest.removeNextProcessingTimer(), nullValue());        underTest.setTimer(processingTime1);    underTest.advanceProcessingTime(new Instant(21));    assertThat(underTest.removeNextProcessingTimer(), equalTo(processingTime1));    assertThat(underTest.removeNextProcessingTimer(), nullValue());        underTest.advanceProcessingTime(new Instant(30));    assertThat(underTest.removeNextProcessingTimer(), equalTo(processingTime2));    assertThat(underTest.removeNextProcessingTimer(), nullValue());}
public void beam_f3238_0() throws Exception
{    InMemoryTimerInternals underTest = new InMemoryTimerInternals();    TimerData eventTime1 = TimerData.of(NS1, new Instant(19), TimeDomain.EVENT_TIME);    TimerData processingTime1 = TimerData.of(NS1, new Instant(19), TimeDomain.PROCESSING_TIME);    TimerData synchronizedProcessingTime1 = TimerData.of(NS1, new Instant(19), TimeDomain.SYNCHRONIZED_PROCESSING_TIME);    TimerData eventTime2 = TimerData.of(NS1, new Instant(29), TimeDomain.EVENT_TIME);    TimerData processingTime2 = TimerData.of(NS1, new Instant(29), TimeDomain.PROCESSING_TIME);    TimerData synchronizedProcessingTime2 = TimerData.of(NS1, new Instant(29), TimeDomain.SYNCHRONIZED_PROCESSING_TIME);    underTest.setTimer(processingTime1);    underTest.setTimer(eventTime1);    underTest.setTimer(synchronizedProcessingTime1);    underTest.setTimer(processingTime2);    underTest.setTimer(eventTime2);    underTest.setTimer(synchronizedProcessingTime2);    assertThat(underTest.removeNextEventTimer(), nullValue());    underTest.advanceInputWatermark(new Instant(30));    assertThat(underTest.removeNextEventTimer(), equalTo(eventTime1));    assertThat(underTest.removeNextEventTimer(), equalTo(eventTime2));    assertThat(underTest.removeNextEventTimer(), nullValue());    assertThat(underTest.removeNextProcessingTimer(), nullValue());    underTest.advanceProcessingTime(new Instant(30));    assertThat(underTest.removeNextProcessingTimer(), equalTo(processingTime1));    assertThat(underTest.removeNextProcessingTimer(), equalTo(processingTime2));    assertThat(underTest.removeNextProcessingTimer(), nullValue());    assertThat(underTest.removeNextSynchronizedProcessingTimer(), nullValue());    underTest.advanceSynchronizedProcessingTime(new Instant(30));    assertThat(underTest.removeNextSynchronizedProcessingTimer(), equalTo(synchronizedProcessingTime1));    assertThat(underTest.removeNextSynchronizedProcessingTimer(), equalTo(synchronizedProcessingTime2));    assertThat(underTest.removeNextProcessingTimer(), nullValue());}
public void beam_f3247_0()
{    FixedWindows windowFn = FixedWindows.of(Duration.standardMinutes(5));    WindowingStrategy<?, ?> strategy = WindowingStrategy.globalDefault().withWindowFn(windowFn);    IntervalWindow window = windowFn.assignWindow(new Instant(BoundedWindow.TIMESTAMP_MAX_VALUE));    assertThat(window.maxTimestamp(), equalTo(GlobalWindow.INSTANCE.maxTimestamp()));    assertThat(LateDataUtils.garbageCollectionTime(window, strategy), equalTo(GlobalWindow.INSTANCE.maxTimestamp()));}
public void beam_f3248_0()
{    FixedWindows windowFn = FixedWindows.of(Duration.standardMinutes(5));    Duration allowedLateness = Duration.millis(Long.MAX_VALUE);    WindowingStrategy<?, ?> strategy = WindowingStrategy.globalDefault().withWindowFn(windowFn).withAllowedLateness(allowedLateness);    IntervalWindow window = windowFn.assignWindow(new Instant(-100));    assertThat(window.maxTimestamp().plus(allowedLateness), Matchers.greaterThan(GlobalWindow.INSTANCE.maxTimestamp()));    assertThat(LateDataUtils.garbageCollectionTime(window, strategy), equalTo(GlobalWindow.INSTANCE.maxTimestamp()));}
private void beam_f3257_0()
{    System.out.println("CLEANUP");    set.cleanupTemporaryWindows();    set.checkInvariants();    System.out.println(set);    set.persist();    MergingActiveWindowSet<IntervalWindow> reloaded = new MergingActiveWindowSet<>(windowFn, state);    reloaded.checkInvariants();    assertEquals(set, reloaded);}
private IntervalWindow beam_f3258_0(long start, long size)
{    return new IntervalWindow(new Instant(start), new Duration(size));}
public void beam_f3267_0()
{    DirtyState dirtyState = new DirtyState();    Assert.assertNotEquals(dirtyState, new Object());    DirtyState differentState = new DirtyState();    differentState.beforeCommit();    Assert.assertNotEquals(dirtyState, differentState);    Assert.assertNotEquals(dirtyState.hashCode(), differentState.hashCode());}
public void beam_f3268_0()
{    cell.update(5);    cell.update(7);    assertThat(cell.getCumulative(), equalTo(DistributionData.create(12, 2, 5, 7)));    assertThat("getCumulative is idempotent", cell.getCumulative(), equalTo(DistributionData.create(12, 2, 5, 7)));    assertThat(cell.getDirty().beforeCommit(), equalTo(true));    cell.getDirty().afterCommit();    assertThat(cell.getDirty().beforeCommit(), equalTo(false));    cell.update(30);    assertThat(cell.getCumulative(), equalTo(DistributionData.create(42, 3, 5, 30)));    assertThat("Adding a new value made the cell dirty", cell.getDirty().beforeCommit(), equalTo(true));}
private ExecutionStateTracker beam_f3277_0()
{    return new ExecutionStateTracker(sampler);}
public void beam_f3278_0()
{    cell.set(5);    cell.set(7);    assertThat(cell.getCumulative().value(), equalTo(GaugeData.create(7).value()));    assertThat("getCumulative is idempotent", cell.getCumulative().value(), equalTo(7L));    assertThat(cell.getDirty().beforeCommit(), equalTo(true));    cell.getDirty().afterCommit();    assertThat(cell.getDirty().beforeCommit(), equalTo(false));    cell.set(30);    assertThat(cell.getCumulative().value(), equalTo(30L));    assertThat("Adding a new value made the cell dirty", cell.getDirty().beforeCommit(), equalTo(true));}
public void beam_f3287_0()
{    MetricsContainerImpl testObject = new MetricsContainerImpl("step1");    DistributionCell c1 = testObject.getDistribution(MetricName.named("ns", "name1"));    DistributionCell c2 = testObject.getDistribution(MetricName.named("ns", "name2"));    c1.update(5L);    c2.update(4L);    SimpleMonitoringInfoBuilder builder1 = new SimpleMonitoringInfoBuilder();    builder1.setUrn(MonitoringInfoConstants.Urns.USER_DISTRIBUTION_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, "ns").setLabel(MonitoringInfoConstants.Labels.NAME, "name1").setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "step1").setInt64DistributionValue(DistributionData.create(5, 1, 5, 5));    SimpleMonitoringInfoBuilder builder2 = new SimpleMonitoringInfoBuilder();    builder2.setUrn(MonitoringInfoConstants.Urns.USER_DISTRIBUTION_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, "ns").setLabel(MonitoringInfoConstants.Labels.NAME, "name2").setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "step1").setInt64DistributionValue(DistributionData.create(4, 1, 4, 4));    ArrayList<MonitoringInfo> actualMonitoringInfos = new ArrayList<MonitoringInfo>();    for (MonitoringInfo mi : testObject.getMonitoringInfos()) {        actualMonitoringInfos.add(SimpleMonitoringInfoBuilder.copyAndClearTimestamp(mi));    }    assertThat(actualMonitoringInfos, containsInAnyOrder(builder1.build(), builder2.build()));}
public void beam_f3288_0()
{    MetricsContainerImpl testObject = new MetricsContainerImpl("step1");    HashMap<String, String> labels = new HashMap<>();    labels.put(MonitoringInfoConstants.Labels.PCOLLECTION, "pcoll1");    DistributionCell c1 = testObject.getDistribution(MonitoringInfoMetricName.named(MonitoringInfoConstants.Urns.SAMPLED_BYTE_SIZE, labels));    c1.update(5L);    SimpleMonitoringInfoBuilder builder1 = new SimpleMonitoringInfoBuilder();    builder1.setUrn(MonitoringInfoConstants.Urns.SAMPLED_BYTE_SIZE).setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "pcoll1").setInt64DistributionValue(DistributionData.create(5, 1, 5, 5));    ArrayList<MonitoringInfo> actualMonitoringInfos = new ArrayList<MonitoringInfo>();    for (MonitoringInfo mi : testObject.getMonitoringInfos()) {        actualMonitoringInfos.add(SimpleMonitoringInfoBuilder.copyAndClearTimestamp(mi));    }    assertThat(actualMonitoringInfos, containsInAnyOrder(builder1.build()));}
public void beam_f3297_0()
{    MetricsContainerStepMap baseMetricContainerRegistry = new MetricsContainerStepMap();    CounterCell c1 = baseMetricContainerRegistry.getContainer(STEP1).getCounter(MetricName.named("ns", "name1"));    CounterCell c2 = baseMetricContainerRegistry.getUnboundContainer().getCounter(MonitoringInfoTestUtil.testElementCountName());    c1.inc(7);    c2.inc(14);    MetricsContainerStepMap testObject = new MetricsContainerStepMap();    testObject.updateAll(baseMetricContainerRegistry);    List<MonitoringInfo> expected = new ArrayList<MonitoringInfo>();    SimpleMonitoringInfoBuilder builder = new SimpleMonitoringInfoBuilder();    builder.setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, "ns").setLabel(MonitoringInfoConstants.Labels.NAME, "name1");    builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, STEP1);    builder.setInt64Value(7);    expected.add(builder.build());    expected.add(MonitoringInfoTestUtil.testElementCountMonitoringInfo(14));    ArrayList<MonitoringInfo> actual = new ArrayList<MonitoringInfo>();    for (MonitoringInfo mi : testObject.getMonitoringInfos()) {        actual.add(SimpleMonitoringInfoBuilder.copyAndClearTimestamp(mi));    }    assertThat(actual, containsInAnyOrder(expected.toArray()));}
public void beam_f3298_0()
{    MetricsContainerStepMap attemptedMetrics = new MetricsContainerStepMap();    attemptedMetrics.update(STEP1, metricsContainer);    attemptedMetrics.update(STEP1, metricsContainer);    attemptedMetrics.update(STEP2, metricsContainer);    attemptedMetrics.update(STEP2, metricsContainer);    attemptedMetrics.update(STEP2, metricsContainer);    MetricsContainerStepMap committedMetrics = new MetricsContainerStepMap();    committedMetrics.update(STEP1, metricsContainer);    committedMetrics.update(STEP2, metricsContainer);    committedMetrics.update(STEP2, metricsContainer);    MetricResults metricResults = asMetricResults(attemptedMetrics, committedMetrics);    MetricQueryResults step1res = metricResults.queryMetrics(MetricsFilter.builder().addStep(STEP1).build());    assertIterableSize(step1res.getCounters(), 1);    assertIterableSize(step1res.getDistributions(), 1);    assertIterableSize(step1res.getGauges(), 1);    assertCounter(COUNTER_NAME, step1res, STEP1, VALUE * 2, false);    assertDistribution(DISTRIBUTION_NAME, step1res, STEP1, DistributionResult.create(VALUE * 6, 4, VALUE, VALUE * 2), false);    assertGauge(GAUGE_NAME, step1res, STEP1, GaugeResult.create(VALUE, Instant.now()), false);    assertCounter(COUNTER_NAME, step1res, STEP1, VALUE, true);    assertDistribution(DISTRIBUTION_NAME, step1res, STEP1, DistributionResult.create(VALUE * 3, 2, VALUE, VALUE * 2), true);    assertGauge(GAUGE_NAME, step1res, STEP1, GaugeResult.create(VALUE, Instant.now()), true);    MetricQueryResults step2res = metricResults.queryMetrics(MetricsFilter.builder().addStep(STEP2).build());    assertIterableSize(step2res.getCounters(), 1);    assertIterableSize(step2res.getDistributions(), 1);    assertIterableSize(step2res.getGauges(), 1);    assertCounter(COUNTER_NAME, step2res, STEP2, VALUE * 3, false);    assertDistribution(DISTRIBUTION_NAME, step2res, STEP2, DistributionResult.create(VALUE * 9, 6, VALUE, VALUE * 2), false);    assertGauge(GAUGE_NAME, step2res, STEP2, GaugeResult.create(VALUE, Instant.now()), false);    assertCounter(COUNTER_NAME, step2res, STEP2, VALUE * 2, true);    assertDistribution(DISTRIBUTION_NAME, step2res, STEP2, DistributionResult.create(VALUE * 6, 4, VALUE, VALUE * 2), true);    assertGauge(GAUGE_NAME, step2res, STEP2, GaugeResult.create(VALUE, Instant.now()), true);    MetricQueryResults allres = metricResults.queryMetrics(MetricsFilter.builder().build());    assertIterableSize(allres.getCounters(), 2);    assertIterableSize(allres.getDistributions(), 2);    assertIterableSize(allres.getGauges(), 2);}
public void beam_f3307_0()
{    assertThat(metricsMap.tryGet("foo"), nullValue(AtomicLong.class));    AtomicLong foo = metricsMap.get("foo");    assertThat(metricsMap.tryGet("foo"), sameInstance(foo));}
public void beam_f3308_0()
{    AtomicLong foo = metricsMap.get("foo");    AtomicLong bar = metricsMap.get("bar");    assertThat(metricsMap.entries(), containsInAnyOrder(hasEntry("foo", foo), hasEntry("bar", bar)));}
public static Iterable<Object[]> beam_f3317_0()
{    return ImmutableList.<Object[]>builder().add(new Object[] { ImmutableMap.builder().put(TRANSFORM1, ImmutableList.of(DISTRIBUTION1)).build() }).add(new Object[] { ImmutableMap.builder().put(TRANSFORM1, ImmutableList.of(DISTRIBUTION1, COUNTER1)).build() }).add(new Object[] { ImmutableMap.builder().put(TRANSFORM1, ImmutableList.of(DISTRIBUTION1, COUNTER1)).put(TRANSFORM2, ImmutableList.of(COUNTER2)).put(TRANSFORM3, ImmutableList.of(GAUGE1)).build() }).add(new Object[] { ImmutableMap.builder().put(TRANSFORM1, ImmutableList.of(GAUGE1, GAUGE2)).build() }).build();}
public void beam_f3318_0()
{    ImmutableMap.Builder<String, Collection<BeamFnApi.Metrics.User>> result = ImmutableMap.builder();    for (Map.Entry<String, Collection<BeamFnApi.Metrics.User>> entry : fnMetrics.entrySet()) {        MetricUpdates updates = MetricsTranslation.metricUpdatesFromProto(entry.getKey(), entry.getValue());        result.putAll(MetricsTranslation.metricUpdatesToProto(updates));    }    Map<String, Collection<BeamFnApi.Metrics.User>> backToProto = result.build();    assertThat(backToProto.keySet(), equalTo(fnMetrics.keySet()));    for (String ptransformName : backToProto.keySet()) {        assertThat(backToProto.get(ptransformName), containsInAnyOrder(fnMetrics.get(ptransformName).toArray()));    }}
public void beam_f3327_0(Description description)
{    description.appendText("URN=").appendValue(mi.getUrn()).appendText(", labels=").appendValue(mi.getLabels()).appendText(", type=").appendValue(mi.getType());    if (mi.getMetric().hasCounterData()) {        description.appendText(", value=").appendValue(mi.getMetric().getCounterData().getInt64Value());    }}
public static TypeSafeMatcher<MonitoringInfo> beam_f3328_0(final long value)
{    return new TypeSafeMatcher<MonitoringInfo>() {        @Override        protected boolean matchesSafely(MonitoringInfo item) {            if (item.getMetric().getCounterData().getInt64Value() < value) {                return false;            }            return true;        }        @Override        public void describeTo(Description description) {            description.appendText("value=").appendValue(value);        }    };}
public void beam_f3337_0()
{    HashMap<String, String> labels = new HashMap<String, String>();    thrown.expect(IllegalArgumentException.class);    MonitoringInfoMetricName.named(null, labels);}
public void beam_f3338_0()
{    HashMap<String, String> labels = new HashMap<String, String>();    thrown.expect(IllegalArgumentException.class);    MonitoringInfoMetricName.named("", labels);}
public void beam_f3347_0()
{    SimpleMonitoringInfoBuilder builder = new SimpleMonitoringInfoBuilder();    builder.setUrn(MonitoringInfoConstants.Urns.ELEMENT_COUNT);    builder.setInt64Value(1);    builder.setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "myPcollection");        MonitoringInfo monitoringInfo = builder.build();    assertTrue(monitoringInfo != null);    assertEquals("myPcollection", monitoringInfo.getLabelsOrDefault(MonitoringInfoConstants.Labels.PCOLLECTION, null));    assertEquals(MonitoringInfoConstants.Urns.ELEMENT_COUNT, monitoringInfo.getUrn());    assertEquals(MonitoringInfoConstants.TypeUrns.SUM_INT64, monitoringInfo.getType());    assertEquals(1, monitoringInfo.getMetric().getCounterData().getInt64Value());    assertEquals("myPcollection", monitoringInfo.getLabelsMap().get(MonitoringInfoConstants.Labels.PCOLLECTION));}
public void beam_f3348_0()
{    SimpleMonitoringInfoBuilder builder = new SimpleMonitoringInfoBuilder();    builder.setUrn(MonitoringInfoConstants.Urns.USER_DISTRIBUTION_COUNTER);    builder.setLabel(MonitoringInfoConstants.Labels.NAME, "myName");    builder.setLabel(MonitoringInfoConstants.Labels.NAMESPACE, "myNamespace");    builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "myStep");    assertNull(builder.build());    builder.setInt64DistributionValue(DistributionData.create(10, 2, 1, 9));        MonitoringInfo monitoringInfo = builder.build();    assertTrue(monitoringInfo != null);    assertEquals(MonitoringInfoConstants.Urns.USER_DISTRIBUTION_COUNTER, monitoringInfo.getUrn());    assertEquals("myName", monitoringInfo.getLabelsOrDefault(MonitoringInfoConstants.Labels.NAME, ""));    assertEquals("myNamespace", monitoringInfo.getLabelsOrDefault(MonitoringInfoConstants.Labels.NAMESPACE, ""));    assertEquals(MonitoringInfoConstants.TypeUrns.DISTRIBUTION_INT64, monitoringInfo.getType());    MetricsApi.IntDistributionData distribution = monitoringInfo.getMetric().getDistributionData().getIntDistributionData();    assertEquals(10, distribution.getSum());    assertEquals(2, distribution.getCount());    assertEquals(9, distribution.getMax());    assertEquals(1, distribution.getMin());}
public ProcessContinuation beam_f3357_0(ProcessContext context, RestrictionTracker<OffsetRange, Long> tracker)
{    Uninterruptibles.sleepUninterruptibly(sleepBeforeFirstClaim.getMillis(), TimeUnit.MILLISECONDS);    for (long i = tracker.currentRestriction().getFrom(), numIterations = 1; tracker.tryClaim(i); ++i, ++numIterations) {        Uninterruptibles.sleepUninterruptibly(sleepBeforeEachOutput.getMillis(), TimeUnit.MILLISECONDS);        context.output("" + i);        if (numIterations == numOutputsPerProcessCall) {            return resume();        }    }    return stop();}
public OffsetRange beam_f3358_0(Void element)
{    throw new UnsupportedOperationException("Should not be called in this test");}
public void beam_f3369_0(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker)
{    c.output("foo");}
public OffsetRange beam_f3370_0(Void element)
{    throw new UnsupportedOperationException("Should not be called in this test");}
private void beam_f3379_0(ReduceFnTester<Integer, ?, IntervalWindow> tester, Integer... values) throws Exception
{    injectElements(tester, Arrays.asList(values));}
private void beam_f3380_0(TriggerStateMachine mockTrigger) throws Exception
{    doAnswer(invocation -> {        @SuppressWarnings("unchecked")        TriggerStateMachine.TriggerContext context = (TriggerStateMachine.TriggerContext) invocation.getArguments()[0];        context.trigger().setFinished(true);        return null;    }).when(mockTrigger).onFire(anyTriggerContext());}
public void beam_f3389_0() throws Exception
{    WindowingStrategy<?, IntervalWindow> strategy = WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(100))).withTimestampCombiner(TimestampCombiner.EARLIEST).withMode(AccumulationMode.ACCUMULATING_FIRED_PANES).withAllowedLateness(Duration.ZERO).withTrigger(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(10))));    ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());    tester.advanceProcessingTime(new Instant(5000));        injectElement(tester, 2);    injectElement(tester, 5);            tester.advanceInputWatermarkNoTimers(new Instant(100));        tester.advanceProcessingTime(new Instant(6000));    assertThat(tester.extractOutput(), emptyIterable());}
public void beam_f3390_0() throws Exception
{    WindowingStrategy<?, IntervalWindow> strategy = WindowingStrategy.of((WindowFn<?, IntervalWindow>) FixedWindows.of(Duration.millis(100))).withTimestampCombiner(TimestampCombiner.EARLIEST).withMode(AccumulationMode.ACCUMULATING_FIRED_PANES).withAllowedLateness(Duration.ZERO).withTrigger(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(10))));    ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(strategy, Sum.ofIntegers(), VarIntCoder.of());    tester.advanceProcessingTime(new Instant(5000));        injectElement(tester, 2);    injectElement(tester, 5);    tester.advanceInputWatermarkNoTimers(new Instant(100));    tester.advanceProcessingTimeNoTimers(new Instant(5010));        tester.fireTimers(new IntervalWindow(new Instant(0), new Instant(100)), TimestampedValue.of(TimeDomain.EVENT_TIME, new Instant(100)), TimestampedValue.of(TimeDomain.PROCESSING_TIME, new Instant(5010)));    assertThat(tester.extractOutput(), contains(isSingleWindowedValue(equalTo(7), 2, 0, 100, PaneInfo.createPane(true, true, Timing.ON_TIME, 0, 0))));}
public void beam_f3399_0() throws Exception
{    MetricsContainerImpl container = new MetricsContainerImpl("any");    MetricsEnvironment.setCurrentContainer(container);        Duration allowedLateness = Duration.millis(10);    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(FixedWindows.of(Duration.millis(10)), mockTriggerStateMachine, AccumulationMode.ACCUMULATING_FIRED_PANES, allowedLateness, ClosingBehavior.FIRE_IF_NON_EMPTY);        assertEquals(null, tester.getWatermarkHold());    assertEquals(null, tester.getOutputWatermark());        IntervalWindow expectedWindow = new IntervalWindow(new Instant(0), new Instant(10));    injectElement(tester, 1);    injectElement(tester, 3);    assertEquals(new Instant(1), tester.getWatermarkHold());    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    injectElement(tester, 2);    List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();    assertThat(output, contains(isSingleWindowedValue(containsInAnyOrder(1, 2, 3), equalTo(new Instant(1)), equalTo((BoundedWindow) expectedWindow))));    assertThat(output.get(0).getPane(), equalTo(PaneInfo.createPane(true, false, Timing.EARLY, 0, -1)));        assertThat(tester.getWatermarkHold(), nullValue());        long droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative();    assertEquals(0, droppedElements);        tester.advanceInputWatermark(new Instant(4));    assertEquals(new Instant(4), tester.getOutputWatermark());        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(false);    tester.advanceInputWatermark(new Instant(4));    injectElement(tester, 2);    injectElement(tester, 3);            assertThat(tester.getWatermarkHold(), equalTo(expectedWindow.maxTimestamp().plus(allowedLateness)));        injectElement(tester, 5);    assertEquals(new Instant(5), tester.getWatermarkHold());    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    injectElement(tester, 4);    output = tester.extractOutput();    assertThat(output, contains(isSingleWindowedValue(containsInAnyOrder(    1,     2,     3, 2, 3, 4,     5),     4,     0,     10)));    assertThat(output.get(0).getPane(), equalTo(PaneInfo.createPane(false, false, Timing.EARLY, 1, -1)));        assertThat(tester.getWatermarkHold(), nullValue());            when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(false);    tester.advanceInputWatermark(new Instant(8));    injectElement(tester, 6);    injectElement(tester, 5);    assertThat(tester.getWatermarkHold(), equalTo(expectedWindow.maxTimestamp().plus(allowedLateness)));    injectElement(tester, 4);        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);            tester.setAutoAdvanceOutputWatermark(false);    tester.advanceInputWatermark(expectedWindow.maxTimestamp().plus(1));    tester.fireTimer(expectedWindow, expectedWindow.maxTimestamp(), TimeDomain.EVENT_TIME);            output = tester.extractOutput();    assertThat(output, contains(isSingleWindowedValue(containsInAnyOrder(    1,     2,     3,     2,     3,     4,     5, 4, 5,     6),     9,     0,     10)));    assertThat(output.get(0).getPane(), equalTo(PaneInfo.createPane(false, false, Timing.ON_TIME, 2, 0)));    tester.setAutoAdvanceOutputWatermark(true);            when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(false);    injectElement(tester, 8);    droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative();    assertEquals(0, droppedElements);        tester.advanceInputWatermark(new Instant(50));    output = tester.extractOutput();            assertThat(output, contains(isSingleWindowedValue(containsInAnyOrder(    1,     2,     3,     2,     3,     4,     5,     4,     5,     6,     8),     9,     0,     10)));    assertThat(output.get(0).getPane(), equalTo(PaneInfo.createPane(false, true, Timing.LATE, 3, 1)));    assertEquals(new Instant(50), tester.getOutputWatermark());    assertEquals(null, tester.getWatermarkHold());        tester.fireTimer(new IntervalWindow(new Instant(0), new Instant(10)), new Instant(12), TimeDomain.EVENT_TIME);        assertFalse(tester.isMarkedFinished(firstWindow));    tester.assertHasOnlyGlobalAndFinishedSetsFor();}
public void beam_f3400_0() throws Exception
{    Duration allowedLateness = Duration.standardMinutes(1);    Duration gapDuration = Duration.millis(10);    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(Sessions.withGapDuration(gapDuration)).withMode(AccumulationMode.DISCARDING_FIRED_PANES).withTrigger(Repeatedly.forever(AfterWatermark.pastEndOfWindow().withLateFirings(AfterPane.elementCountAtLeast(1)))).withAllowedLateness(allowedLateness));    tester.setAutoAdvanceOutputWatermark(false);    assertEquals(null, tester.getWatermarkHold());    assertEquals(null, tester.getOutputWatermark());    tester.advanceInputWatermark(new Instant(40));    injectElements(tester, 1);    assertThat(tester.getWatermarkHold(), nullValue());    injectElements(tester, 10);    assertThat(tester.getWatermarkHold(), nullValue());}
public void beam_f3409_0() throws Exception
{    Duration allowedLateness = Duration.standardDays(1);    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(FixedWindows.of(Duration.millis(10))).withTrigger(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardSeconds(5)))).withAllowedLateness(allowedLateness));            IntervalWindow expectedWindow = new IntervalWindow(new Instant(0), new Instant(10));    tester.advanceInputWatermark(new Instant(0));    tester.advanceProcessingTime(new Instant(0));    tester.injectElements(TimestampedValue.of(1, new Instant(1)));        assertThat(tester.getWatermarkHold(), equalTo(expectedWindow.maxTimestamp()));    tester.advanceProcessingTime(new Instant(6000));        assertThat(tester.getOutputSize(), equalTo(1));        assertThat(tester.getWatermarkHold(), nullValue());        tester.advanceInputWatermark(new Instant(expectedWindow.maxTimestamp().plus(Duration.standardHours(1))));        tester.injectElements(TimestampedValue.of(3, new Instant(3)));            assertThat(tester.getWatermarkHold(), equalTo(expectedWindow.maxTimestamp().plus(allowedLateness)));}
public void beam_f3410_0() throws Exception
{    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(FixedWindows.of(Duration.millis(10))).withTrigger(Repeatedly.forever(AfterFirst.of(AfterPane.elementCountAtLeast(2), AfterWatermark.pastEndOfWindow()))).withMode(AccumulationMode.ACCUMULATING_FIRED_PANES).withAllowedLateness(Duration.millis(100)).withTimestampCombiner(TimestampCombiner.EARLIEST).withClosingBehavior(ClosingBehavior.FIRE_ALWAYS));    tester.advanceInputWatermark(new Instant(0));    tester.injectElements(TimestampedValue.of(1, new Instant(1)), TimestampedValue.of(2, new Instant(2)));    List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();    assertThat(output, contains(WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(true, false, Timing.EARLY, 0, -1))));    assertThat(output, contains(isSingleWindowedValue(containsInAnyOrder(1, 2), 1, 0, 10)));    tester.advanceInputWatermark(new Instant(50));            output = tester.extractOutput();    assertThat(output, contains(WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(false, false, Timing.ON_TIME, 1, 0))));    assertThat(output, contains(isSingleWindowedValue(containsInAnyOrder(1, 2), 9, 0, 10)));        tester.advanceInputWatermark(new Instant(150));    output = tester.extractOutput();    assertThat(output, contains(WindowMatchers.valueWithPaneInfo(PaneInfo.createPane(false, true, Timing.LATE, 2, 1))));    assertThat(output, contains(isSingleWindowedValue(containsInAnyOrder(1, 2), 9, 0, 10)));}
public void beam_f3419_0() throws Exception
{    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(Sessions.withGapDuration(Duration.millis(10)), mockTriggerStateMachine, AccumulationMode.DISCARDING_FIRED_PANES, Duration.millis(50), ClosingBehavior.FIRE_IF_NON_EMPTY);        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);    triggerShouldFinish(mockTriggerStateMachine);    tester.injectElements(TimestampedValue.of(2, new Instant(2)));        when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(false);    tester.injectElements(TimestampedValue.of(1, new Instant(1)), TimestampedValue.of(2, new Instant(2)), TimestampedValue.of(3, new Instant(3)));    tester.advanceInputWatermark(new Instant(100));    List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();    assertThat(output.size(), equalTo(2));    assertThat(output.get(0), isSingleWindowedValue(containsInAnyOrder(2),     2,     2,     12));    assertThat(output.get(0).getPane(), equalTo(PaneInfo.createPane(true, true, Timing.EARLY, 0, 0)));    assertThat(output.get(1), isSingleWindowedValue(containsInAnyOrder(1, 2, 3),     1,     1,     13));    assertThat(output.get(1).getPane(), equalTo(PaneInfo.createPane(true, true, Timing.ON_TIME, 0, 0)));}
public void beam_f3420_0() throws Exception
{    MetricsContainerImpl container = new MetricsContainerImpl("any");    MetricsEnvironment.setCurrentContainer(container);    ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(WindowingStrategy.of(SlidingWindows.of(Duration.millis(100)).every(Duration.millis(30))).withTrigger(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.millis(1000)), Sum.ofIntegers(), VarIntCoder.of());    tester.injectElements(    TimestampedValue.of(10, new Instant(23)),     TimestampedValue.of(12, new Instant(40)));    long droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative();    assertEquals(0, droppedElements);    tester.advanceInputWatermark(new Instant(70));    tester.injectElements(    TimestampedValue.of(14, new Instant(60)));    droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative();    assertEquals(1, droppedElements);    tester.advanceInputWatermark(new Instant(130));            tester.injectElements(TimestampedValue.of(16, new Instant(40)));    droppedElements = container.getCounter(MetricName.named(ReduceFnRunner.class, ReduceFnRunner.DROPPED_DUE_TO_CLOSED_WINDOW)).getCumulative();    assertEquals(4, droppedElements);}
public void beam_f3429_0() throws Exception
{    ReduceFnTester<Integer, Iterable<Integer>, GlobalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(new GlobalWindows()).withTrigger(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(new Duration(3)))).withMode(AccumulationMode.DISCARDING_FIRED_PANES));    final int n = 20;    for (int i = 0; i < n; i++) {        tester.advanceProcessingTime(new Instant(i));        tester.injectElements(TimestampedValue.of(i, new Instant(i)));    }    tester.advanceProcessingTime(new Instant(n + 4));    List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();    assertEquals((n + 3) / 4, output.size());    for (int i = 0; i < output.size(); i++) {        assertEquals(Timing.EARLY, output.get(i).getPane().getTiming());        assertEquals(i, output.get(i).getPane().getIndex());        assertEquals(4, Iterables.size(output.get(i).getValue()));    }    tester.advanceInputWatermark(BoundedWindow.TIMESTAMP_MAX_VALUE);    output = tester.extractOutput();    assertEquals(1, output.size());    assertEquals(Timing.ON_TIME, output.get(0).getPane().getTiming());    assertEquals((n + 3) / 4, output.get(0).getPane().getIndex());    assertEquals(0, Iterables.size(output.get(0).getValue()));}
public void beam_f3430_0() throws Exception
{    ReduceFnTester<Integer, Iterable<Integer>, IntervalWindow> tester = ReduceFnTester.nonCombining(WindowingStrategy.of(FixedWindows.of(Duration.millis(10))).withTrigger(AfterWatermark.pastEndOfWindow().withLateFirings(AfterPane.elementCountAtLeast(2))).withMode(AccumulationMode.DISCARDING_FIRED_PANES).withAllowedLateness(Duration.millis(100)).withClosingBehavior(ClosingBehavior.FIRE_IF_NON_EMPTY));    tester.advanceInputWatermark(new Instant(0));    tester.advanceOutputWatermark(new Instant(0));    tester.injectElements(TimestampedValue.of(1, new Instant(1)));        tester.advanceInputWatermark(new Instant(109));    tester.advanceOutputWatermark(new Instant(109));    tester.injectElements(TimestampedValue.of(2, new Instant(2)));        Instant hold = tester.getWatermarkHold();    assertEquals(new Instant(109), hold);    tester.advanceInputWatermark(new Instant(110));    tester.advanceOutputWatermark(new Instant(110));        List<WindowedValue<Iterable<Integer>>> output = tester.extractOutput();    assertEquals(2, output.size());}
public static ReduceFnTester<Integer, OutputT, W> beam_f3439_0(WindowingStrategy<?, W> strategy, CombineFn<Integer, AccumT, OutputT> combineFn, Coder<OutputT> outputCoder) throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();        AppliedCombineFn.withInputCoder(combineFn, registry, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));    return combining(strategy, TriggerStateMachines.stateMachineForTrigger(TriggerTranslation.toProto(strategy.getTrigger())), combineFn, outputCoder);}
public static ReduceFnTester<Integer, OutputT, W> beam_f3440_0(WindowingStrategy<?, W> strategy, TriggerStateMachine triggerStateMachine, CombineFn<Integer, AccumT, OutputT> combineFn, Coder<OutputT> outputCoder) throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    AppliedCombineFn<String, Integer, AccumT, OutputT> fn = AppliedCombineFn.withInputCoder(combineFn, registry, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));    return new ReduceFnTester<>(strategy, triggerStateMachine, SystemReduceFn.combining(StringUtf8Coder.of(), fn), outputCoder, PipelineOptionsFactory.create(), NullSideInputReader.empty());}
public final void beam_f3449_0(W... expectedWindows)
{    assertHasOnlyGlobalAndAllowedTags(ImmutableSet.copyOf(expectedWindows), ImmutableSet.of(((SystemReduceFn<?, ?, ?, ?, ?>) reduceFn).getBufferTag(), TriggerStateMachineRunner.FINISHED_BITS_TAG, PaneInfoTracker.PANE_INFO_TAG, WatermarkHold.watermarkHoldTagForTimestampCombiner(objectStrategy.getTimestampCombiner()), WatermarkHold.EXTRA_HOLD_TAG));}
public final void beam_f3450_0(W... expectedWindows)
{    assertHasOnlyGlobalAndAllowedTags(ImmutableSet.copyOf(expectedWindows), ImmutableSet.of(TriggerStateMachineRunner.FINISHED_BITS_TAG, PaneInfoTracker.PANE_INFO_TAG, WatermarkHold.watermarkHoldTagForTimestampCombiner(objectStrategy.getTimestampCombiner()), WatermarkHold.EXTRA_HOLD_TAG));}
public void beam_f3459_0(Instant newInputWatermark) throws Exception
{    timerInternals.advanceInputWatermark(newInputWatermark);}
public void beam_f3460_0(Instant newInputWatermark) throws Exception
{    timerInternals.advanceInputWatermark(newInputWatermark);    ReduceFnRunner<String, InputT, OutputT, W> runner = createRunner();    while (true) {        TimerData timer;        List<TimerInternals.TimerData> timers = new ArrayList<>();        while ((timer = timerInternals.removeNextEventTimer()) != null) {            timers.add(timer);        }        if (timers.isEmpty()) {            break;        }        runner.onTimers(timers);    }    if (autoAdvanceOutputWatermark) {        Instant hold = stateInternals.earliestWatermarkHold();        if (hold == null) {            WindowTracing.trace("TestInMemoryTimerInternals.advanceInputWatermark: no holds, " + "so output watermark = input watermark");            hold = timerInternals.currentInputWatermarkTime();        }        advanceOutputWatermark(hold);    }    runner.persist();}
public void beam_f3469_0(KV<String, OutputT> output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{        KV<String, OutputT> copy = SerializableUtils.ensureSerializableByCoder(KvCoder.of(StringUtf8Coder.of(), outputCoder), output, "outputForWindow");    WindowedValue<KV<String, OutputT>> value = WindowedValue.of(copy, timestamp, windows, pane);    outputs.add(value);}
public void beam_f3470_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    throw new UnsupportedOperationException("GroupAlsoByWindow should not use tagged outputs");}
public void beam_f3479_0()
{    SideInputHandler sideInputHandler = new SideInputHandler(ImmutableList.of(view1), InMemoryStateInternals.<Void>forKey(null));        IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(WINDOW_MSECS_1));    IntervalWindow secondWindow = new IntervalWindow(new Instant(1000), new Instant(1000 + WINDOW_MSECS_2));        sideInputHandler.addSideInputValue(view1, valuesInWindow(materializeValuesFor(View.asIterable(), "Hello"), new Instant(0), firstWindow));    assertThat(sideInputHandler.get(view1, firstWindow), contains("Hello"));        sideInputHandler.addSideInputValue(view1, valuesInWindow(materializeValuesFor(View.asIterable(), "Arrivederci"), new Instant(0), secondWindow));    assertThat(sideInputHandler.get(view1, secondWindow), contains("Arrivederci"));        assertThat(sideInputHandler.get(view1, firstWindow), contains("Hello"));}
public void beam_f3480_0()
{    SideInputHandler sideInputHandler = new SideInputHandler(ImmutableList.of(view1, view2), InMemoryStateInternals.<Void>forKey(null));        IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(WINDOW_MSECS_1));        sideInputHandler.addSideInputValue(view1, valuesInWindow(materializeValuesFor(View.asIterable(), "Hello"), new Instant(0), firstWindow));    assertThat(sideInputHandler.get(view1, firstWindow), contains("Hello"));        assertFalse(sideInputHandler.isReady(view2, firstWindow));        sideInputHandler.addSideInputValue(view2, valuesInWindow(materializeValuesFor(View.asIterable(), "Salut"), new Instant(0), firstWindow));    assertTrue(sideInputHandler.isReady(view2, firstWindow));    assertThat(sideInputHandler.get(view2, firstWindow), contains("Salut"));        assertThat(sideInputHandler.get(view1, firstWindow), contains("Hello"));}
public void beam_f3489_0()
{    SkewingDoFn fn = new SkewingDoFn(Duration.ZERO);    DoFnRunner<Duration, Duration> runner = new SimpleDoFnRunner<>(null, fn, NullSideInputReader.empty(), new ListOutputManager(), new TupleTag<>(), Collections.emptyList(), mockStepContext, null, Collections.emptyMap(), WindowingStrategy.of(new GlobalWindows()), DoFnSchemaInformation.create(), Collections.emptyMap());    runner.startBundle();        runner.processElement(WindowedValue.timestampedValueInGlobalWindow(Duration.ZERO, new Instant(0)));    thrown.expect(UserCodeException.class);    thrown.expectCause(isA(IllegalArgumentException.class));    thrown.expectMessage("must be no earlier");    thrown.expectMessage(String.format("timestamp of the current input (%s)", new Instant(0).toString()));    thrown.expectMessage(String.format("the allowed skew (%s)", PeriodFormat.getDefault().print(Duration.ZERO.toPeriod())));        runner.processElement(WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), new Instant(0)));}
public void beam_f3490_0()
{    SkewingDoFn fn = new SkewingDoFn(Duration.standardMinutes(10L));    DoFnRunner<Duration, Duration> runner = new SimpleDoFnRunner<>(null, fn, NullSideInputReader.empty(), new ListOutputManager(), new TupleTag<>(), Collections.emptyList(), mockStepContext, null, Collections.emptyMap(), WindowingStrategy.of(new GlobalWindows()), DoFnSchemaInformation.create(), Collections.emptyMap());    runner.startBundle();        runner.processElement(WindowedValue.timestampedValueInGlobalWindow(Duration.standardMinutes(5L), new Instant(0)));    thrown.expect(UserCodeException.class);    thrown.expectCause(isA(IllegalArgumentException.class));    thrown.expectMessage("must be no earlier");    thrown.expectMessage(String.format("timestamp of the current input (%s)", new Instant(0).toString()));    thrown.expectMessage(String.format("the allowed skew (%s)", PeriodFormat.getDefault().print(Duration.standardMinutes(10L).toPeriod())));        runner.processElement(WindowedValue.timestampedValueInGlobalWindow(Duration.standardHours(1L), new Instant(0)));}
public Duration beam_f3499_0()
{    return allowedSkew;}
public void beam_f3500_0(TupleTag<T> tag, WindowedValue<T> output)
{    outputs.put(tag, output);}
public void beam_f3509_0()
{    PushbackSideInputDoFnRunner<Integer, Integer> runner = createRunner(ImmutableList.of());    String timerId = "fooTimer";    IntervalWindow window = new IntervalWindow(new Instant(4), new Instant(16));    Instant timestamp = new Instant(72);            runner.onTimer(timerId, window, new Instant(timestamp), TimeDomain.EVENT_TIME);    assertThat(underlying.firedTimers, contains(TimerData.of(timerId, StateNamespaces.window(IntervalWindow.getCoder(), window), timestamp, TimeDomain.EVENT_TIME)));}
public DoFn<InputT, OutputT> beam_f3510_0()
{    return null;}
public T beam_f3520_0(PCollectionView<T> view, BoundedWindow window)
{    throw new NoSuchElementException();}
public boolean beam_f3521_0(PCollectionView<T> view)
{    return false;}
public void beam_f3530_0(OutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    outputWindowedValue(tester.getMainOutputTag(), output, timestamp, windows, pane);}
public void beam_f3531_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    for (BoundedWindow window : windows) {        tester.getMutableOutput(tag).add(ValueInSingleWindow.of(output, timestamp, window, pane));    }}
public SomeRestriction beam_f3540_0(Integer elem)
{    return new SomeRestriction();}
public void beam_f3541_0() throws Exception
{    DoFn<Integer, String> fn = new SelfInitiatedResumeFn();    Instant base = Instant.now();    dateTimeProvider.setDateTimeFixed(base.getMillis());    ProcessFnTester<Integer, String, SomeRestriction, Void> tester = new ProcessFnTester<>(base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeRestriction.class), MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);    tester.startElement(42, new SomeRestriction());    assertThat(tester.takeOutputElements(), contains("42"));        assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));    assertTrue(tester.takeOutputElements().isEmpty());        assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));    assertThat(tester.takeOutputElements(), contains("42"));        assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));    assertTrue(tester.takeOutputElements().isEmpty());        assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));    assertThat(tester.takeOutputElements(), contains("42"));}
public void beam_f3550_0()
{    assertEquals(State.OUTSIDE_BUNDLE, state);    state = State.TORN_DOWN;}
public void beam_f3551_0()
{    assertEquals(State.OUTSIDE_BUNDLE, state);    state = State.INSIDE_BUNDLE;}
public void beam_f3560_0(ProcessContext c, @StateId(stateId) ValueState<Integer> state)
{    Integer currentValue = MoreObjects.firstNonNull(state.read(), 0);    state.write(currentValue + 1);}
public void beam_f3561_0()
{    this.underTest = createStateInternals();}
public void beam_f3570_0() throws Exception
{    SetState<String> set1 = underTest.state(NAMESPACE_1, STRING_SET_ADDR);    SetState<String> set2 = underTest.state(NAMESPACE_2, STRING_SET_ADDR);    SetState<String> set3 = underTest.state(NAMESPACE_3, STRING_SET_ADDR);    set1.add("Hello");    set2.add("Hello");    set2.add("World");    set1.add("!");    StateMerging.mergeSets(Arrays.asList(set1, set2, set3), set3);        assertThat(set3.read(), containsInAnyOrder("Hello", "World", "!"));    assertThat(set1.read(), Matchers.emptyIterable());    assertThat(set2.read(), Matchers.emptyIterable());}
 static Map.Entry<K, V> beam_f3571_0(K k, V v)
{    return new MapEntry<>(k, v);}
public void beam_f3580_0() throws Exception
{    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);    assertThat(value.isEmpty().read(), Matchers.is(true));    ReadableState<Boolean> readFuture = value.isEmpty();    value.add(5);    assertThat(readFuture.read(), Matchers.is(false));    value.clear();    assertThat(readFuture.read(), Matchers.is(true));}
public void beam_f3581_0() throws Exception
{    CombiningState<Integer, int[], Integer> value1 = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);    CombiningState<Integer, int[], Integer> value2 = underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);    assertThat(value1.getAccum(), Matchers.is(notNullValue()));    assertThat(value2.getAccum(), Matchers.is(notNullValue()));    value1.add(5);    value2.add(10);    value1.add(6);    assertThat(value1.read(), equalTo(11));    assertThat(value2.read(), equalTo(10));        StateMerging.mergeCombiningValues(Arrays.asList(value1, value2), value1);    assertThat(value1.read(), equalTo(21));    assertThat(value2.read(), equalTo(0));}
public void beam_f3590_0() throws Exception
{    MapState<String, Integer> value = underTest.state(NAMESPACE_1, STRING_MAP_ADDR);            ReadableState<Iterable<String>> keys = value.keys();    ReadableState<Iterable<Integer>> values = value.values();    ReadableState<Iterable<Map.Entry<String, Integer>>> entries = value.entries();    value.put("A", 1);    assertFalse(Iterables.isEmpty(keys.read()));    assertFalse(Iterables.isEmpty(values.read()));    assertFalse(Iterables.isEmpty(entries.read()));        ReadableState<Integer> get = value.get("B");    value.put("B", 2);    assertNull(get.read());        value.putIfAbsent("C", 3);    assertThat(value.get("C").read(), equalTo(3));}
public void beam_f3591_0() throws Exception
{            assertThat(new StringCoderWithIdentityEquality(), not(equalTo(new StringCoderWithIdentityEquality())));    BagState<String> state1 = underTest.state(NAMESPACE_1, STRING_BAG_ADDR1);    state1.add("hello");    BagState<String> state2 = underTest.state(NAMESPACE_1, STRING_BAG_ADDR2);    assertThat(state2.read(), containsInAnyOrder("hello"));}
public Integer beam_f3601_0(Integer accumulator, CombineWithContext.Context c)
{    return accumulator;}
private IntervalWindow beam_f3602_0(long start, long end)
{    return new IntervalWindow(new Instant(start), new Instant(end));}
public void beam_f3611_0()
{    StateTag<?> fooVarInt1 = StateTags.bag("foo", VarIntCoder.of());    StateTag<?> fooVarInt2 = StateTags.bag("foo", VarIntCoder.of());    StateTag<?> fooBigEndian = StateTags.bag("foo", BigEndianIntegerCoder.of());    StateTag<?> barVarInt = StateTags.bag("bar", VarIntCoder.of());    assertEquals(fooVarInt1, fooVarInt2);    assertEquals(fooVarInt1, fooBigEndian);    assertNotEquals(fooVarInt1, barVarInt);}
public void beam_f3612_0()
{    StateTag<?> fooVarInt1 = StateTags.set("foo", VarIntCoder.of());    StateTag<?> fooVarInt2 = StateTags.set("foo", VarIntCoder.of());    StateTag<?> fooBigEndian = StateTags.set("foo", BigEndianIntegerCoder.of());    StateTag<?> barVarInt = StateTags.set("bar", VarIntCoder.of());    assertEquals(fooVarInt1, fooVarInt2);    assertEquals(fooVarInt1, fooBigEndian);    assertNotEquals(fooVarInt1, barVarInt);}
public void beam_f3621_0()
{    Instant timestamp = new Instant(100);    StateNamespace namespace = StateNamespaces.global();    TimerData eventTimer = TimerData.of(namespace, timestamp, TimeDomain.EVENT_TIME);    TimerData procTimer = TimerData.of(namespace, timestamp, TimeDomain.PROCESSING_TIME);    TimerData synchronizedProcTimer = TimerData.of(namespace, timestamp, TimeDomain.SYNCHRONIZED_PROCESSING_TIME);    assertThat(eventTimer, lessThan(procTimer));    assertThat(eventTimer, lessThan(synchronizedProcTimer));    assertThat(procTimer, lessThan(synchronizedProcTimer));}
public void beam_f3622_0()
{    Instant timestamp = new Instant(100);    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), timestamp);    IntervalWindow secondWindow = new IntervalWindow(timestamp, new Instant(200));    Coder<IntervalWindow> windowCoder = IntervalWindow.getCoder();    StateNamespace firstWindowNs = StateNamespaces.window(windowCoder, firstWindow);    StateNamespace secondWindowNs = StateNamespaces.window(windowCoder, secondWindow);    TimerData secondEventTime = TimerData.of(firstWindowNs, timestamp, TimeDomain.EVENT_TIME);    TimerData thirdEventTime = TimerData.of(secondWindowNs, timestamp, TimeDomain.EVENT_TIME);    assertThat(secondEventTime, lessThan(thirdEventTime));}
private static TriggerStateMachine.TriggerContext beam_f3631_0()
{    return Mockito.any();}
public void beam_f3632_0()
{    MockitoAnnotations.initMocks(this);}
public void beam_f3641_0()
{    TriggerStateMachine trigger = AfterPaneStateMachine.elementCountAtLeast(5);    assertEquals("AfterPane.elementCountAtLeast(5)", trigger.toString());}
public void beam_f3642_0() throws Exception
{    Duration windowDuration = Duration.millis(10);    SimpleTriggerStateMachineTester<IntervalWindow> tester = TriggerStateMachineTester.forTrigger(AfterProcessingTimeStateMachine.pastFirstElementInPane().plusDelayOf(Duration.millis(5)), FixedWindows.of(windowDuration));    tester.advanceProcessingTime(new Instant(10));        tester.injectElements(1);    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(10));    tester.advanceProcessingTime(new Instant(12));    assertFalse(tester.shouldFire(firstWindow));        tester.injectElements(11, 12, 13);    IntervalWindow secondWindow = new IntervalWindow(new Instant(10), new Instant(20));    assertFalse(tester.shouldFire(secondWindow));        tester.advanceProcessingTime(new Instant(14));    assertFalse(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));        tester.injectElements(2, 3);        tester.advanceProcessingTime(new Instant(16));    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    tester.fireIfShouldFire(firstWindow);    assertTrue(tester.isMarkedFinished(firstWindow));        tester.advanceProcessingTime(new Instant(18));    assertTrue(tester.shouldFire(secondWindow));    tester.fireIfShouldFire(secondWindow);    assertTrue(tester.isMarkedFinished(secondWindow));}
private static TriggerStateMachine.TriggerContext beam_f3651_0()
{    return Mockito.any();}
private static TriggerStateMachine.OnElementContext beam_f3652_0()
{    return Mockito.any();}
public void beam_f3661_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterEachStateMachine.inOrder(AfterWatermarkStateMachine.pastEndOfWindow(), RepeatedlyStateMachine.forever(AfterPaneStateMachine.elementCountAtLeast(1))), Sessions.withGapDuration(Duration.millis(10)));    tester.injectElements(1);    tester.injectElements(5);    IntervalWindow firstWindow = new IntervalWindow(new Instant(1), new Instant(11));    IntervalWindow secondWindow = new IntervalWindow(new Instant(5), new Instant(15));    IntervalWindow mergedWindow = new IntervalWindow(new Instant(1), new Instant(15));        tester.advanceInputWatermark(new Instant(15));    assertTrue(tester.shouldFire(firstWindow));    assertTrue(tester.shouldFire(secondWindow));    tester.fireIfShouldFire(firstWindow);    tester.fireIfShouldFire(secondWindow);        assertFalse(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    tester.injectElements(1);    tester.injectElements(5);    assertTrue(tester.shouldFire(firstWindow));    assertTrue(tester.shouldFire(secondWindow));    tester.fireIfShouldFire(firstWindow);    tester.fireIfShouldFire(secondWindow);        tester.mergeWindows();        assertFalse(tester.shouldFire(mergedWindow));    tester.injectElements(1);    assertTrue(tester.shouldFire(mergedWindow));}
public void beam_f3662_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(AfterEachStateMachine.inOrder(AfterWatermarkStateMachine.pastEndOfWindow(), RepeatedlyStateMachine.forever(AfterPaneStateMachine.elementCountAtLeast(1))), Sessions.withGapDuration(Duration.millis(10)));    tester.injectElements(1);    tester.injectElements(5);    IntervalWindow firstWindow = new IntervalWindow(new Instant(1), new Instant(11));    IntervalWindow secondWindow = new IntervalWindow(new Instant(5), new Instant(15));    IntervalWindow mergedWindow = new IntervalWindow(new Instant(1), new Instant(15));        tester.advanceInputWatermark(new Instant(11));    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    tester.fireIfShouldFire(firstWindow);        assertFalse(tester.shouldFire(firstWindow));    tester.injectElements(1);    assertTrue(tester.shouldFire(firstWindow));    tester.fireIfShouldFire(firstWindow);        tester.mergeWindows();        assertFalse(tester.shouldFire(mergedWindow));    tester.injectElements(1);    assertFalse(tester.shouldFire(mergedWindow));        tester.advanceInputWatermark(new Instant(15));    assertTrue(tester.shouldFire(mergedWindow));}
public void beam_f3671_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(DefaultTriggerStateMachine.of(), FixedWindows.of(Duration.millis(100)));    tester.injectElements(    1,     101);    IntervalWindow firstWindow = new IntervalWindow(new Instant(0), new Instant(100));    IntervalWindow secondWindow = new IntervalWindow(new Instant(100), new Instant(200));        tester.advanceInputWatermark(new Instant(99));    assertFalse(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));        tester.advanceInputWatermark(new Instant(100));    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));        tester.fireIfShouldFire(firstWindow);    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));        tester.advanceInputWatermark(new Instant(200));    assertTrue(tester.shouldFire(firstWindow));    assertTrue(tester.shouldFire(secondWindow));    assertFalse(tester.isMarkedFinished(firstWindow));    assertFalse(tester.isMarkedFinished(secondWindow));}
public void beam_f3672_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(DefaultTriggerStateMachine.of(), SlidingWindows.of(Duration.millis(100)).every(Duration.millis(50)));    tester.injectElements(    1,     50);    IntervalWindow firstWindow = new IntervalWindow(new Instant(-50), new Instant(50));    IntervalWindow secondWindow = new IntervalWindow(new Instant(0), new Instant(100));    IntervalWindow thirdWindow = new IntervalWindow(new Instant(50), new Instant(150));    assertFalse(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    assertFalse(tester.shouldFire(thirdWindow));        tester.advanceInputWatermark(new Instant(50));    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    assertFalse(tester.shouldFire(thirdWindow));    tester.fireIfShouldFire(firstWindow);    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    assertFalse(tester.shouldFire(thirdWindow));        tester.advanceInputWatermark(new Instant(99));    assertTrue(tester.shouldFire(firstWindow));    assertFalse(tester.shouldFire(secondWindow));    assertFalse(tester.shouldFire(thirdWindow));        tester.advanceInputWatermark(new Instant(100));    assertTrue(tester.shouldFire(firstWindow));    assertTrue(tester.shouldFire(secondWindow));    assertFalse(tester.shouldFire(thirdWindow));    tester.fireIfShouldFire(firstWindow);    assertFalse(tester.isMarkedFinished(firstWindow));    assertFalse(tester.isMarkedFinished(secondWindow));    assertFalse(tester.isMarkedFinished(thirdWindow));}
public static void beam_f3685_0(FinishedTriggers finishedSet, ExecutableTriggerStateMachine trigger)
{    assertFalse(finishedSet.isFinished(trigger));    finishedSet.setFinished(trigger, true);    assertTrue(finishedSet.isFinished(trigger));}
public static void beam_f3686_0(FinishedTriggers finishedSet)
{    ExecutableTriggerStateMachine trigger = ExecutableTriggerStateMachine.create(AfterAllStateMachine.of(AfterFirstStateMachine.of(AfterPaneStateMachine.elementCountAtLeast(3), AfterWatermarkStateMachine.pastEndOfWindow()), AfterAllStateMachine.of(AfterPaneStateMachine.elementCountAtLeast(10), AfterProcessingTimeStateMachine.pastFirstElementInPane())));    verifyGetAfterSet(finishedSet, trigger);    verifyGetAfterSet(finishedSet, trigger.subTriggers().get(0).subTriggers().get(1));    verifyGetAfterSet(finishedSet, trigger.subTriggers().get(0));    verifyGetAfterSet(finishedSet, trigger.subTriggers().get(1));    verifyGetAfterSet(finishedSet, trigger.subTriggers().get(1).subTriggers().get(1));    verifyGetAfterSet(finishedSet, trigger.subTriggers().get(1).subTriggers().get(0));}
public void beam_f3695_0() throws Exception
{    triggerTester.injectElements(TimestampedValue.of(1, new Instant(1)));    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(0).plus(Duration.standardMinutes(5)));    assertThat(triggerTester.shouldFire(window), is(false));    triggerTester.advanceInputWatermark(BoundedWindow.TIMESTAMP_MAX_VALUE);    assertThat(triggerTester.shouldFire(window), is(false));}
public void beam_f3696_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(new OrFinallyStateMachine(AfterPaneStateMachine.elementCountAtLeast(2), AfterPaneStateMachine.elementCountAtLeast(100)), FixedWindows.of(Duration.millis(100)));    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(100));        tester.injectElements(1);    assertFalse(tester.shouldFire(window));    assertFalse(tester.isMarkedFinished(window));        tester.injectElements(2);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertTrue(tester.isMarkedFinished(window));}
public void beam_f3705_0() throws Exception
{    tester = TriggerStateMachineTester.forTrigger(RepeatedlyStateMachine.forever(AfterPaneStateMachine.elementCountAtLeast(2)), Sessions.withGapDuration(Duration.millis(10)));    tester.injectElements(1);    IntervalWindow firstWindow = new IntervalWindow(new Instant(1), new Instant(11));    assertFalse(tester.shouldFire(firstWindow));    tester.injectElements(5);    IntervalWindow secondWindow = new IntervalWindow(new Instant(5), new Instant(15));    assertFalse(tester.shouldFire(secondWindow));        tester.mergeWindows();    IntervalWindow mergedWindow = new IntervalWindow(new Instant(1), new Instant(15));    assertTrue(tester.shouldFire(mergedWindow));}
public void beam_f3706_0() throws Exception
{    SimpleTriggerStateMachineTester<GlobalWindow> tester = TriggerStateMachineTester.forTrigger(RepeatedlyStateMachine.forever(AfterFirstStateMachine.of(AfterProcessingTimeStateMachine.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(15)), AfterPaneStateMachine.elementCountAtLeast(5))), new GlobalWindows());    GlobalWindow window = GlobalWindow.INSTANCE;    tester.injectElements(1);    assertFalse(tester.shouldFire(window));    tester.injectElements(2, 3, 4, 5);    assertTrue(tester.shouldFire(window));    tester.fireIfShouldFire(window);    assertFalse(tester.shouldFire(window));}
public void beam_f3715_0()
{    TriggerStateMachine trigger = ReshuffleTriggerStateMachine.create();    assertEquals("ReshuffleTriggerStateMachine()", trigger.toString());}
 static StubTriggerStateMachine beam_f3716_0(final String name)
{    return new StubTriggerStateMachine() {        @Override        public String toString() {            return name;        }    };}
public void beam_f3728_0()
{    RunnerApi.Trigger trigger = RunnerApi.Trigger.newBuilder().setAfterAny(RunnerApi.Trigger.AfterAny.newBuilder().addSubtriggers(subtrigger1).addSubtriggers(subtrigger2)).build();    AfterFirstStateMachine machine = (AfterFirstStateMachine) TriggerStateMachines.stateMachineForTrigger(trigger);    assertThat(machine, equalTo(AfterFirstStateMachine.of(submachine1, submachine2)));}
public void beam_f3729_0()
{    RunnerApi.Trigger trigger = RunnerApi.Trigger.newBuilder().setAfterAll(RunnerApi.Trigger.AfterAll.newBuilder().addSubtriggers(subtrigger1).addSubtriggers(subtrigger2)).build();    AfterAllStateMachine machine = (AfterAllStateMachine) TriggerStateMachines.stateMachineForTrigger(trigger);    assertThat(machine, equalTo(AfterAllStateMachine.of(submachine1, submachine2)));}
public void beam_f3744_0(int... values) throws Exception
{    List<TimestampedValue<Integer>> timestampedValues = Lists.newArrayListWithCapacity(values.length);    for (int value : values) {        timestampedValues.add(TimestampedValue.of(value, new Instant(value)));    }    injectElements(timestampedValues);}
public SimpleTriggerStateMachineTester<W> beam_f3745_0(Duration allowedLateness) throws Exception
{    return new SimpleTriggerStateMachineTester<>(executableTrigger, windowFn, allowedLateness);}
public void beam_f3754_0(Instant newProcessingTime) throws Exception
{    timerInternals.advanceProcessingTime(newProcessingTime);    while (timerInternals.removeNextProcessingTimer() != null) {        }    timerInternals.advanceSynchronizedProcessingTime(newProcessingTime);    while (timerInternals.removeNextSynchronizedProcessingTimer() != null) {        }}
public final void beam_f3755_0(TimestampedValue<InputT>... values) throws Exception
{    injectElements(Arrays.asList(values));}
public Object beam_f3765_0()
{    return element;}
public Instant beam_f3766_0()
{    return timestamp;}
public static Matcher<WindowedValue<? extends T>> beam_f3775_0(Matcher<? super T> valueMatcher, Matcher<? super Instant> timestampMatcher, Matcher<? super Collection<? extends BoundedWindow>> windowsMatcher)
{    return new WindowedValueMatcher<>(valueMatcher, timestampMatcher, windowsMatcher, Matchers.anything());}
public static Matcher<WindowedValue<? extends T>> beam_f3776_0(Matcher<? super T> valueMatcher, Matcher<? super Instant> timestampMatcher)
{    return new WindowedValueMatcher<>(valueMatcher, timestampMatcher, Matchers.anything(), Matchers.anything());}
public static Matcher<IntervalWindow> beam_f3785_0(long start, long end)
{    return Matchers.equalTo(new IntervalWindow(new Instant(start), new Instant(end)));}
public static Matcher<WindowedValue<? extends T>> beam_f3786_0(final PaneInfo paneInfo)
{    return new TypeSafeMatcher<WindowedValue<? extends T>>() {        @Override        public void describeTo(Description description) {            description.appendText("WindowedValue(paneInfo = ").appendValue(paneInfo).appendText(")");        }        @Override        protected boolean matchesSafely(WindowedValue<? extends T> item) {            return Objects.equals(item.getPane(), paneInfo);        }        @Override        protected void describeMismatchSafely(WindowedValue<? extends T> item, Description mismatchDescription) {            mismatchDescription.appendValue(item.getPane());        }    };}
public TransformEvaluator<InputT> beam_f3798_0(AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle) throws IOException
{    return createEvaluator((AppliedPTransform) application);}
private TransformEvaluator<?> beam_f3799_0(final AppliedPTransform<?, PCollection<OutputT>, ?> transform)
{    return new BoundedReadEvaluator<>(transform, evaluationContext, options, minimumDynamicSplitSize, executor);}
public UncommittedBundle<T> beam_f3808_0()
{        return underlying.createRootBundle();}
public UncommittedBundle<T> beam_f3809_0(PCollection<T> output)
{    return new CloningBundle<>(underlying.createBundle(output));}
public Instant beam_f3818_0()
{            checkState(table.earliestWatermarkHold.isPresent(), "Can't get the earliest watermark hold in a %s before it is committed", getClass().getSimpleName());    return table.earliestWatermarkHold.get();}
public T beam_f3819_0(StateNamespace namespace, StateTag<T> address, StateContext<?> c)
{    return table.get(namespace, address, c);}
public WatermarkHoldState beam_f3828_0(StateTag<WatermarkHoldState> address, TimestampCombiner timestampCombiner)
{    if (containedInUnderlying(namespace, address)) {        @SuppressWarnings("unchecked")        InMemoryState<? extends WatermarkHoldState> existingState = (InMemoryState<? extends WatermarkHoldState>) underlying.get().get(namespace, address, c);        return existingState.copy();    } else {        return new InMemoryWatermarkHold<>(timestampCombiner);    }}
public ValueState<T> beam_f3829_0(StateTag<ValueState<T>> address, Coder<T> coder)
{    if (containedInUnderlying(namespace, address)) {        @SuppressWarnings("unchecked")        InMemoryState<? extends ValueState<T>> existingState = (InMemoryState<? extends ValueState<T>>) underlying.get().get(namespace, address, c);        return existingState.copy();    } else {        return new InMemoryValue<>(coder);    }}
public ValueState<T> beam_f3838_0(StateTag<ValueState<T>> address, Coder<T> coder)
{    return underlying.get(namespace, address, c);}
public CombiningState<InputT, AccumT, OutputT> beam_f3839_0(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn)
{    return underlying.get(namespace, address, c);}
public DirectTimerInternals beam_f3848_0()
{    if (timerInternals == null) {        timerInternals = DirectTimerInternals.create(clock, watermarks, TimerUpdate.builder(key));    }    return timerInternals;}
public CopyOnAccessInMemoryStateInternals beam_f3849_0()
{    if (stateInternals != null) {        return stateInternals.commit();    }    return null;}
public Set<AppliedPTransform<?, ?, ?>> beam_f3858_0()
{    return rootTransforms;}
public Collection<AppliedPTransform<?, ?, ?>> beam_f3859_0()
{    return stepNames.keySet();}
public DirectGraph beam_f3868_0()
{    checkState(finalized, "Can't get a graph before the Pipeline has been completely traversed");    return DirectGraph.create(producers, viewWriters, perElementConsumers, rootTransforms, stepNames);}
public PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> beam_f3869_0()
{    return original;}
public void beam_f3878_0(final CommittedBundle<?> bundle, final UpdateT finalCumulative)
{                        inflightAttempted.put(bundle, finalCumulative);    executor.execute(() -> {        synchronized (attemptedLock) {            finishedAttempted = aggregation.combine(asList(finishedAttempted, finalCumulative));            inflightAttempted.remove(bundle);        }    });}
public ResultT beam_f3879_0()
{    ArrayList<UpdateT> updates = new ArrayList<>(inflightAttempted.size() + 1);        synchronized (attemptedLock) {        updates.add(finishedAttempted);        updates.addAll(inflightAttempted.values());    }    return aggregation.extract(aggregation.combine(updates));}
public GaugeData beam_f3888_0()
{    return GaugeData.empty();}
public GaugeData beam_f3889_0(Iterable<GaugeData> updates)
{    GaugeData result = GaugeData.empty();    for (GaugeData update : updates) {        result = result.combine(update);    }    return result;}
public Iterable<Class<? extends PipelineOptions>> beam_f3898_0()
{    return ImmutableList.of(DirectOptions.class, DirectTestOptions.class);}
 static Set<Enforcement> beam_f3899_0(DirectOptions options)
{    EnumSet<Enforcement> enabled = EnumSet.noneOf(Enforcement.class);    if (options.isEnforceEncodability()) {        enabled.add(ENCODABILITY);    }    if (options.isEnforceImmutability()) {        enabled.add(IMMUTABILITY);    }    return Collections.unmodifiableSet(enabled);}
 List<PTransformOverride> beam_f3908_0()
{    DirectTestOptions testOptions = options.as(DirectTestOptions.class);    ImmutableList.Builder<PTransformOverride> builder = ImmutableList.builder();    if (testOptions.isRunnerDeterminedSharding()) {        builder.add(PTransformOverride.of(PTransformMatchers.writeWithRunnerDeterminedSharding(), new WriteWithShardingFactory()));    /* Uses a view internally. */    }    builder = builder.add(PTransformOverride.of(MultiStepCombine.matcher(), MultiStepCombine.Factory.create())).add(PTransformOverride.of(PTransformMatchers.urnEqualTo(PTransformTranslation.CREATE_VIEW_TRANSFORM_URN), new ViewOverrideFactory())).add(PTransformOverride.of(PTransformMatchers.urnEqualTo(PTransformTranslation.TEST_STREAM_TRANSFORM_URN), new DirectTestStreamFactory(this))).add(PTransformOverride.of(PTransformMatchers.splittableParDo(), new ParDoMultiOverrideFactory())).add(PTransformOverride.of(PTransformMatchers.stateOrTimerParDo(), new ParDoMultiOverrideFactory())).add(PTransformOverride.of(PTransformMatchers.urnEqualTo(PTransformTranslation.SPLITTABLE_PROCESS_KEYED_URN), new SplittableParDoViaKeyedWorkItems.OverrideFactory())).add(PTransformOverride.of(PTransformMatchers.urnEqualTo(SplittableParDo.SPLITTABLE_GBKIKWI_URN), new DirectGBKIntoKeyedWorkItemsOverrideFactory())).add(PTransformOverride.of(PTransformMatchers.urnEqualTo(PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN), new DirectGroupByKeyOverrideFactory()));    /* returns two chained primitives. */    return builder.build();}
public State beam_f3909_0()
{    return state;}
public void beam_f3918_0(StateNamespace namespace, String timerId, TimeDomain timeDomain)
{    throw new UnsupportedOperationException("Canceling of timer by ID is not yet supported.");}
public void beam_f3919_0(StateNamespace namespace, String timerId)
{    throw new UnsupportedOperationException("Canceling of timer by ID is not yet supported.");}
private void beam_f3928_0(TransformEvaluator<T> evaluator, MetricsContainerImpl metricsContainer, Collection<ModelEnforcement<T>> enforcements) throws Exception
{    if (inputBundle != null) {        for (WindowedValue<T> value : inputBundle.getElements()) {            for (ModelEnforcement<T> enforcement : enforcements) {                enforcement.beforeElement(value);            }            evaluator.processElement(value);                        MetricUpdates deltas = metricsContainer.getUpdates();            if (deltas != null) {                context.getMetrics().updatePhysical(inputBundle, deltas);                metricsContainer.commitUpdates();            }            for (ModelEnforcement<T> enforcement : enforcements) {                enforcement.afterElement(value);            }        }    }}
private TransformResult<T> beam_f3929_0(TransformEvaluator<T> evaluator, MetricsContainerImpl metricsContainer, Collection<ModelEnforcement<T>> enforcements) throws Exception
{    TransformResult<T> result = evaluator.finishBundle().withLogicalMetricUpdates(metricsContainer.getCumulative());    CommittedResult outputs = onComplete.handleResult(inputBundle, result);    for (ModelEnforcement<T> enforcement : enforcements) {        enforcement.afterFinish(inputBundle, result, outputs.getOutputs());    }    return result;}
public void beam_f3938_0() throws Exception
{    Thread currentThread = Thread.currentThread();    outstanding.invalidate(currentThread);        outstanding.cleanUp();            Exception thrown = thrownOnTeardown.remove(currentThread);    if (thrown != null) {        throw thrown;    }}
public Collection<Exception> beam_f3939_0() throws Exception
{    outstanding.invalidateAll();        outstanding.cleanUp();    return thrownOnTeardown.values();}
 static void beam_f3948_0(Iterable<DoFnLifecycleManager> managers) throws Exception
{    Collection<Exception> thrown = new ArrayList<>();    for (DoFnLifecycleManager manager : managers) {        thrown.addAll(manager.removeAll());    }    if (!thrown.isEmpty()) {        Exception overallException = new Exception("Exceptions thrown while tearing down DoFns");        for (Exception e : thrown) {            overallException.addSuppressed(e);        }        throw overallException;    }}
public Collection<CommittedBundle<Void>> beam_f3949_0(AppliedPTransform<PCollectionList<T>, PCollection<T>, PTransform<PCollectionList<T>, PCollection<T>>> transform, int targetParallelism)
{    return Collections.emptyList();}
public UncommittedBundle<T> beam_f3958_0(PCollection<T> output)
{    return bundleFactory.createBundle(output);}
public UncommittedBundle<T> beam_f3959_0(StructuralKey<K> key, PCollection<T> output)
{    return bundleFactory.createKeyedBundle(key, output);}
public ReadyCheckingSideInputReader beam_f3968_0(final List<PCollectionView<?>> sideInputs)
{    return sideInputContainer.createReaderForViews(sideInputs);}
public DirectMetrics beam_f3969_0()
{    return metrics;}
public TransformExecutorService beam_f3978_0(StepAndKey stepAndKey) throws Exception
{    return TransformExecutorServices.serial(executorService);}
private RemovalListener<StepAndKey, TransformExecutorService> beam_f3979_0()
{    return notification -> {        TransformExecutorService service = notification.getValue();        if (service != null) {            service.shutdown();        }    };}
public void beam_f3988_0()
{    shutdownIfNecessary(State.CANCELLED);    visibleUpdates.cancelled();}
private void beam_f3989_1(State newState)
{    if (!newState.isTerminal()) {        return;    }        final Collection<Exception> errors = new ArrayList<>();        try {        serialExecutorServices.invalidateAll();    } catch (final RuntimeException re) {        errors.add(re);    }    try {        serialExecutorServices.cleanUp();    } catch (final RuntimeException re) {        errors.add(re);    }    try {        parallelExecutorService.shutdown();    } catch (final RuntimeException re) {        errors.add(re);    }    try {        executorService.shutdown();    } catch (final RuntimeException re) {        errors.add(re);    }    try {        metricsExecutor.shutdown();    } catch (final RuntimeException re) {        errors.add(re);    }    try {        registry.cleanup();    } catch (final Exception e) {        errors.add(e);    }        pipelineState.compareAndSet(State.RUNNING, newState);    if (!errors.isEmpty()) {        final IllegalStateException exception = new IllegalStateException("Error" + (errors.size() == 1 ? "" : "s") + " during executor shutdown:\n" + errors.stream().map(Exception::getMessage).collect(Collectors.joining("\n- ", "- ", "")));        visibleUpdates.failed(exception);        throw exception;    }}
public void beam_f3998_0()
{    updates.offer(VisibleExecutorUpdate.finished());}
private VisibleExecutorUpdate beam_f3999_0(Duration timeout) throws InterruptedException
{    return updates.poll(timeout.getMillis(), TimeUnit.MILLISECONDS);}
 Iterable<WindowedValue<V>> beam_f4010_0(final K key, Iterable<WindowedValue<V>> elements, final TimerInternals timerInternals)
{    return StreamSupport.stream(elements.spliterator(), false).flatMap(wv -> StreamSupport.stream(wv.explodeWindows().spliterator(), false)).filter(input -> {        BoundedWindow window = Iterables.getOnlyElement(input.getWindows());        boolean expired = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness()).isBefore(timerInternals.currentInputWatermarkTime());        if (expired) {                        droppedDueToLateness.inc();            WindowTracing.debug("{}: Dropping element at {} for key: {}; " + "window: {} since it is too far behind inputWatermark: {}", DirectGroupAlsoByWindow.class.getSimpleName(), input.getTimestamp(), key, window, timerInternals.currentInputWatermarkTime());        }                return !expired;    }).collect(Collectors.toList());}
public void beam_f4011_0(KV<K, Iterable<V>> output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    bundle.add(WindowedValue.of(output, timestamp, windows, pane));}
public UncommittedBundle<T> beam_f4021_0(PCollection<T> output)
{    if (Enforcement.IMMUTABILITY.appliesTo(output, graph)) {        return new ImmutabilityEnforcingBundle<>(underlying.createBundle(output));    }    return underlying.createBundle(output);}
public UncommittedBundle<T> beam_f4022_0(StructuralKey<K> key, PCollection<T> output)
{    if (Enforcement.IMMUTABILITY.appliesTo(output, graph)) {        return new ImmutabilityEnforcingBundle<>(underlying.createKeyedBundle(key, output));    }    return underlying.createKeyedBundle(key, output);}
private void beam_f4031_0(MutationDetector detector)
{    try {        detector.verifyUnmodified();    } catch (IllegalMutationException e) {        throw new IllegalMutationException(String.format("PTransform %s illegaly mutated value %s of class %s." + " Input values must not be mutated in any way.", transform.getFullName(), e.getSavedValue(), e.getSavedValue().getClass()), e.getSavedValue(), e.getNewValue());    }}
public static ImmutableListBundleFactory beam_f4032_0()
{    return FACTORY;}
public static CommittedImmutableListBundle<T> beam_f4041_0(@Nullable PCollection<T> pcollection, StructuralKey<?> key, Iterable<WindowedValue<T>> committedElements, Instant minElementTimestamp, Instant synchronizedCompletionTime)
{    return new AutoValue_ImmutableListBundleFactory_CommittedImmutableListBundle<>(pcollection, key, committedElements, minElementTimestamp, synchronizedCompletionTime);}
public Iterator<WindowedValue<T>> beam_f4042_0()
{    return getElements().iterator();}
public static KeyedPValueTrackingVisitor beam_f4052_0()
{    return new KeyedPValueTrackingVisitor();}
public CompositeBehavior beam_f4053_0(TransformHierarchy.Node node)
{    checkState(!finalized, "Attempted to use a %s that has already been finalized on a pipeline (visiting node %s)", KeyedPValueTrackingVisitor.class.getSimpleName(), node);    return CompositeBehavior.ENTER_TRANSFORM;}
public PTransformReplacement<PCollection<KV<K, InputT>>, PCollection<KV<K, OutputT>>> beam_f4062_0(AppliedPTransform<PCollection<KV<K, InputT>>, PCollection<KV<K, OutputT>>, PTransform<PCollection<KV<K, InputT>>, PCollection<KV<K, OutputT>>>> transform)
{    GlobalCombineFn<?, ?, ?> globalFn = ((Combine.PerKey) transform.getTransform()).getFn();    checkState(globalFn instanceof CombineFn, "%s.matcher() should only match %s instances using %s, got %s", MultiStepCombine.class.getSimpleName(), PerKey.class.getSimpleName(), CombineFn.class.getSimpleName(), globalFn.getClass().getName());    @SuppressWarnings("unchecked")    CombineFn<InputT, AccumT, OutputT> fn = (CombineFn<InputT, AccumT, OutputT>) globalFn;    @SuppressWarnings("unchecked")    PCollection<KV<K, InputT>> input = (PCollection<KV<K, InputT>>) Iterables.getOnlyElement(transform.getInputs().values());    @SuppressWarnings("unchecked")    PCollection<KV<K, OutputT>> output = (PCollection<KV<K, OutputT>>) Iterables.getOnlyElement(transform.getOutputs().values());    return PTransformReplacement.of(input, new MultiStepCombine<>(fn, output.getCoder()));}
public static MultiStepCombine<K, InputT, AccumT, OutputT> beam_f4063_0(CombineFn<InputT, AccumT, OutputT> combineFn, Coder<KV<K, OutputT>> outputCoder)
{    return new MultiStepCombine<>(combineFn, outputCoder);}
public BoundedWindow beam_f4072_0()
{    return window;}
public boolean beam_f4073_0(Object other)
{    if (!(other instanceof MultiStepCombine.WindowedStructuralKey)) {        return false;    }    WindowedStructuralKey that = (WindowedStructuralKey<?>) other;    return this.window.equals(that.window) && this.key.equals(that.key);}
public TransformResult<KV<K, Iterable<AccumT>>> beam_f4083_0() throws Exception
{    return StepTransformResult.<KV<K, Iterable<AccumT>>>withoutHold(application).addOutput(output).build();}
public static NanosOffsetClock beam_f4084_0()
{    return new NanosOffsetClock();}
public void beam_f4093_0(WindowedValue<InputT> element)
{    try {        Iterable<WindowedValue<InputT>> unprocessed = fnRunner.processElementInReadyWindows(element);        unprocessedElements.addAll(unprocessed);    } catch (Exception e) {        throw UserCodeException.wrap(e);    }}
public void beam_f4094_0(TimerData timer, BoundedWindow window)
{    try {        fnRunner.onTimer(timer.getTimerId(), window, timer.getTimestamp(), timer.getDomain());    } catch (Exception e) {        throw UserCodeException.wrap(e);    }}
 ParDoEvaluator<InputT> beam_f4103_1(AppliedPTransform<PCollection<InputT>, PCollectionTuple, ?> application, StructuralKey<?> key, PCollection<InputT> mainInput, List<PCollectionView<?>> sideInputs, TupleTag<OutputT> mainOutputTag, List<TupleTag<?>> additionalOutputTags, DirectStepContext stepContext, DoFn<InputT, OutputT> fn, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping, DoFnLifecycleManager fnManager) throws Exception
{    try {        return ParDoEvaluator.create(evaluationContext, options, stepContext, application, mainInput.getCoder(), mainInput.getWindowingStrategy(), fn, key, sideInputs, mainOutputTag, additionalOutputTags, pcollections(application.getOutputs()), doFnSchemaInformation, sideInputMapping, runnerFactory);    } catch (Exception e) {        try {            fnManager.remove();        } catch (Exception removalException) {                        e.addSuppressed(removalException);        }        throw e;    }}
 static Map<TupleTag<?>, PCollection<?>> beam_f4104_0(Map<TupleTag<?>, PValue> outputs)
{    Map<TupleTag<?>, PCollection<?>> pcs = new HashMap<>();    for (Map.Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {        pcs.put(output.getKey(), (PCollection<?>) output.getValue());    }    return pcs;}
public TupleTagList beam_f4113_0()
{    return additionalOutputTags;}
public DoFnSchemaInformation beam_f4114_0()
{    return doFnSchemaInformation;}
public static PCollectionViewWindow<T> beam_f4123_0(PCollectionView<T> view, BoundedWindow window)
{    return new PCollectionViewWindow<>(view, window);}
public PCollectionView<T> beam_f4124_0()
{    return view;}
public final CommittedResult beam_f4133_0(CommittedBundle<?> inputBundle, TransformResult<?> result)
{    CommittedResult<AppliedPTransform<?, ?, ?>> committedResult = evaluationContext.handleResult(inputBundle, timers, result);    for (CommittedBundle<?> outputBundle : committedResult.getOutputs()) {        pendingWork.offer(WorkUpdate.fromBundle(outputBundle, graph.getPerElementConsumers(outputBundle.getPCollection())));    }    Optional<? extends CommittedBundle<?>> unprocessedInputs = committedResult.getUnprocessedInputs();    if (unprocessedInputs.isPresent()) {        if (inputBundle.getPCollection() == null) {                        pendingRootBundles.get(result.getTransform()).offer(unprocessedInputs.get());        } else {            pendingWork.offer(WorkUpdate.fromBundle(unprocessedInputs.get(), Collections.singleton(committedResult.getExecutable())));        }    }    if (!committedResult.getProducedOutputTypes().isEmpty()) {        state.set(ExecutorState.ACTIVE);    }    outstandingWork.decrementAndGet();    return committedResult;}
public void beam_f4134_0(AppliedPTransform<?, ?, ?> transform)
{    outstandingWork.decrementAndGet();}
public static RootProviderRegistry beam_f4143_0(EvaluationContext context, PipelineOptions options)
{    return new RootProviderRegistry(ImmutableMap.<String, RootInputProvider<?, ?, ?>>builder().put(IMPULSE_TRANSFORM_URN, new ImpulseEvaluatorFactory.ImpulseRootProvider(context)).put(PTransformTranslation.READ_TRANSFORM_URN, ReadEvaluatorFactory.inputProvider(context, options)).put(DIRECT_TEST_STREAM_URN, new TestStreamEvaluatorFactory.InputProvider(context)).put(FLATTEN_TRANSFORM_URN, new EmptyInputProvider()).build());}
public static RootProviderRegistry beam_f4144_0(EvaluationContext context)
{    return new RootProviderRegistry(ImmutableMap.<String, RootInputProvider<?, ?, ?>>builder().put(IMPULSE_TRANSFORM_URN, new ImpulseEvaluatorFactory.ImpulseRootProvider(context)).build());}
public String beam_f4153_0()
{    return MoreObjects.toStringHelper(this).add("view", view).add("window", window).toString();}
public boolean beam_f4154_0(final PCollectionView<?> view, final BoundedWindow window)
{    checkArgument(readerViews.contains(view), "Tried to check if view %s was ready in a SideInputReader that does not contain it. " + "Contained views; %s", view, readerViews);    return viewContents.getUnchecked(PCollectionViewWindow.of(view, window)).isPresent();}
public void beam_f4163_0(OutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    outputManager.output(transform.getMainOutputTag(), WindowedValue.of(output, timestamp, windows, pane));}
public void beam_f4164_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    outputManager.output(tag, WindowedValue.of(output, timestamp, windows, pane));}
public TransformResult<KeyedWorkItem<K, KV<K, InputT>>> beam_f4173_0() throws Exception
{    TransformResult<KV<K, InputT>> delegateResult = delegateEvaluator.finishBundle();    StepTransformResult.Builder<KeyedWorkItem<K, KV<K, InputT>>> regroupedResult = StepTransformResult.<KeyedWorkItem<K, KV<K, InputT>>>withHold(delegateResult.getTransform(), delegateResult.getWatermarkHold()).withTimerUpdate(delegateResult.getTimerUpdate()).withState(delegateResult.getState()).withMetricUpdates(delegateResult.getLogicalMetricUpdates()).addOutput(Lists.newArrayList(delegateResult.getOutputBundles()));        for (WindowedValue<?> untypedUnprocessed : delegateResult.getUnprocessedElements()) {        WindowedValue<KV<K, InputT>> windowedKv = (WindowedValue<KV<K, InputT>>) untypedUnprocessed;        WindowedValue<KeyedWorkItem<K, KV<K, InputT>>> pushedBack = windowedKv.withValue(KeyedWorkItems.elementsWorkItem(windowedKv.getValue().getKey(), Collections.singleton(windowedKv)));        regroupedResult.addUnprocessedElements(pushedBack);    }    return regroupedResult.build();}
public static StepAndKey beam_f4174_0(AppliedPTransform<?, ?, ?> step, StructuralKey<?> key)
{    return new StepAndKey(step, key);}
public Builder<InputT> beam_f4183_0(CopyOnAccessInMemoryStateInternals state)
{    this.state = state;    return this;}
public Builder<InputT> beam_f4184_0(TimerUpdate timerUpdate)
{    this.timerUpdate = timerUpdate;    return this;}
public TransformResult<TestStreamIndex<T>> beam_f4194_0() throws Exception
{    return resultBuilder.build();}
public void beam_f4195_0(Duration amount)
{    Instant now = currentTime.get();    currentTime.compareAndSet(now, now.plus(amount));}
public static TransformEvaluatorRegistry beam_f4204_0(EvaluationContext ctxt, PipelineOptions options)
{    ImmutableMap<String, TransformEvaluatorFactory> primitives = ImmutableMap.<String, TransformEvaluatorFactory>builder().put(READ_TRANSFORM_URN, new ReadEvaluatorFactory(ctxt, options)).put(PAR_DO_TRANSFORM_URN, new ParDoEvaluatorFactory<>(ctxt, ParDoEvaluator.defaultRunnerFactory(), ParDoEvaluatorFactory.basicDoFnCacheLoader(), options)).put(FLATTEN_TRANSFORM_URN, new FlattenEvaluatorFactory(ctxt)).put(ASSIGN_WINDOWS_TRANSFORM_URN, new WindowEvaluatorFactory(ctxt)).put(IMPULSE_TRANSFORM_URN, new ImpulseEvaluatorFactory(ctxt)).put(DIRECT_WRITE_VIEW_URN, new ViewEvaluatorFactory(ctxt)).put(DIRECT_STATEFUL_PAR_DO_URN, new StatefulParDoEvaluatorFactory<>(ctxt, options)).put(DIRECT_GBKO_URN, new GroupByKeyOnlyEvaluatorFactory(ctxt)).put(DIRECT_GABW_URN, new GroupAlsoByWindowEvaluatorFactory(ctxt, options)).put(DIRECT_TEST_STREAM_URN, new TestStreamEvaluatorFactory(ctxt)).put(DIRECT_MERGE_ACCUMULATORS_EXTRACT_OUTPUT_URN, new MultiStepCombine.MergeAndExtractAccumulatorOutputEvaluatorFactory(ctxt)).put(SPLITTABLE_PROCESS_URN, new SplittableProcessElementsEvaluatorFactory<>(ctxt, options)).build();    return new TransformEvaluatorRegistry(primitives);}
public Map<? extends Class<? extends PTransform>, ? extends PTransformTranslation.TransformPayloadTranslator> beam_f4205_0()
{    return ImmutableMap.<Class<? extends PTransform>, PTransformTranslation.TransformPayloadTranslator>builder().put(DirectGroupByKey.DirectGroupByKeyOnly.class, TransformPayloadTranslator.NotSerializable.forUrn(DIRECT_GBKO_URN)).put(DirectGroupByKey.DirectGroupAlsoByWindow.class, TransformPayloadTranslator.NotSerializable.forUrn(DIRECT_GABW_URN)).put(ParDoMultiOverrideFactory.StatefulParDo.class, TransformPayloadTranslator.NotSerializable.forUrn(DIRECT_STATEFUL_PAR_DO_URN)).put(ViewOverrideFactory.WriteView.class, TransformPayloadTranslator.NotSerializable.forUrn(DIRECT_WRITE_VIEW_URN)).put(DirectTestStream.class, TransformPayloadTranslator.NotSerializable.forUrn(DIRECT_TEST_STREAM_URN)).put(SplittableParDoViaKeyedWorkItems.ProcessElements.class, TransformPayloadTranslator.NotSerializable.forUrn(SPLITTABLE_PROCESS_URN)).build();}
public void beam_f4215_0(TransformExecutor completed)
{    if (!currentlyEvaluating.compareAndSet(completed, null)) {        throw new IllegalStateException("Finished work " + completed + " but could not complete due to unexpected currently executing " + currentlyEvaluating.get());    }    updateCurrentlyEvaluating();}
public void beam_f4216_0()
{    synchronized (this) {        active = false;    }    workQueue.clear();}
private TransformEvaluator<?> beam_f4225_0(AppliedPTransform<PBegin, PCollection<OutputT>, Read.Unbounded<OutputT>> application)
{    return new UnboundedReadEvaluator<>(application, evaluationContext, options, readerReuseChance);}
public void beam_f4227_0(WindowedValue<UnboundedSourceShard<OutputT, CheckpointMarkT>> element) throws IOException
{    UncommittedBundle<OutputT> output = evaluationContext.createBundle((PCollection<OutputT>) getOnlyElement(transform.getOutputs().values()));    UnboundedSourceShard<OutputT, CheckpointMarkT> shard = element.getValue();    UnboundedReader<OutputT> reader = null;    try {        reader = getReader(shard);        boolean elementAvailable = startReader(reader, shard);        if (elementAvailable) {            UnboundedReadDeduplicator deduplicator = shard.getDeduplicator();            int numElements = 0;            do {                if (deduplicator.shouldOutput(reader.getCurrentRecordId())) {                    output.add(WindowedValue.timestampedValueInGlobalWindow(reader.getCurrent(), reader.getCurrentTimestamp()));                }                numElements++;            } while (numElements < ARBITRARY_MAX_ELEMENTS && reader.advance());            Instant watermark = reader.getWatermark();            CheckpointMarkT finishedCheckpoint = finishRead(reader, watermark, shard);                        if (ThreadLocalRandom.current().nextDouble(1.0) >= readerReuseChance) {                UnboundedReader<OutputT> toClose = reader;                                                                reader = null;                toClose.close();            }            UnboundedSourceShard<OutputT, CheckpointMarkT> residual = UnboundedSourceShard.of(shard.getSource(), shard.getDeduplicator(), reader, finishedCheckpoint);            resultBuilder.addOutput(output).addUnprocessedElements(Collections.singleton(WindowedValue.timestampedValueInGlobalWindow(residual, watermark)));        } else {            Instant watermark = reader.getWatermark();            if (watermark.isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE)) {                                                resultBuilder.addUnprocessedElements(Collections.<WindowedValue<?>>singleton(WindowedValue.timestampedValueInGlobalWindow(UnboundedSourceShard.of(shard.getSource(), shard.getDeduplicator(), reader, shard.getCheckpoint()), watermark)));            } else {                                                final CheckpointMarkT checkpoint = shard.getCheckpoint();                IOException ioe = null;                try {                    if (checkpoint != null) {                        checkpoint.finalizeCheckpoint();                    }                } catch (final IOException finalizeCheckpointException) {                    ioe = finalizeCheckpointException;                } finally {                    try {                        UnboundedReader<?> toClose = reader;                                                reader = null;                        toClose.close();                    } catch (final IOException closeEx) {                        if (ioe != null) {                            ioe.addSuppressed(closeEx);                        } else {                            throw closeEx;                        }                    }                }                if (ioe != null) {                    throw ioe;                }            }        }    } catch (IOException e) {        if (reader != null) {            reader.close();        }        throw e;    }}
private TransformEvaluator<Iterable<InT>> beam_f4237_0(final AppliedPTransform<PCollection<Iterable<InT>>, PCollection<Iterable<InT>>, WriteView<InT, OuT>> application)
{    PCollection<Iterable<InT>> input = (PCollection<Iterable<InT>>) Iterables.getOnlyElement(application.getInputs().values());    final PCollectionViewWriter<InT, OuT> writer = context.createPCollectionViewWriter(input, application.getTransform().getView());    return new TransformEvaluator<Iterable<InT>>() {        private final List<WindowedValue<InT>> elements = new ArrayList<>();        @Override        public void processElement(WindowedValue<Iterable<InT>> element) {            for (InT input : element.getValue()) {                elements.add(element.withValue(input));            }        }        @Override        public TransformResult<Iterable<InT>> finishBundle() {            writer.add(elements);            Builder resultBuilder = StepTransformResult.withoutHold(application);            if (!elements.isEmpty()) {                resultBuilder = resultBuilder.withAdditionalOutput(OutputType.PCOLLECTION_VIEW);            }            return resultBuilder.build();        }    };}
public void beam_f4238_0(WindowedValue<Iterable<InT>> element)
{    for (InT input : element.getValue()) {        elements.add(element.withValue(input));    }}
public void beam_f4247_0(AppliedPTransform<?, ?, ?> step, BoundedWindow window, WindowingStrategy<?, ?> windowingStrategy, Runnable runnable)
{    WatermarkCallback callback = WatermarkCallback.afterWindowExpiration(window, windowingStrategy, runnable);    PriorityQueue<WatermarkCallback> callbackQueue = callbacks.get(step);    if (callbackQueue == null) {        callbackQueue = new PriorityQueue<>(11, new CallbackOrdering());        if (callbacks.putIfAbsent(step, callbackQueue) != null) {            callbackQueue = callbacks.get(step);        }    }    synchronized (callbackQueue) {        callbackQueue.offer(callback);    }}
public void beam_f4248_0(AppliedPTransform<?, ?, ?> step, Instant watermark)
{    PriorityQueue<WatermarkCallback> callbackQueue = callbacks.get(step);    if (callbackQueue == null) {        return;    }    synchronized (callbackQueue) {        while (!callbackQueue.isEmpty() && callbackQueue.peek().shouldFire(watermark)) {            executor.execute(callbackQueue.poll().getCallback());        }    }}
private static WatermarkUpdate beam_f4257_0(String name, Instant oldTime, Instant currentTime)
{    WatermarkUpdate res = WatermarkUpdate.fromTimestamps(oldTime, currentTime);    if (res.isAdvanced()) {        WindowTracing.debug("Watermark {} advanced from {} to {}", name, oldTime, currentTime);    }    return res;}
public String beam_f4258_0()
{    return name;}
public synchronized void beam_f4267_0(Object key, Instant newHold)
{    if (newHold == null) {        holds.removeHold(key);    } else {        holds.updateHold(key, newHold);    }}
public String beam_f4268_0()
{    return name;}
public synchronized Instant beam_f4277_0()
{    Instant earliest = THE_END_OF_TIME.get();    for (NavigableSet<TimerData> timers : processingTimers.values()) {        if (!timers.isEmpty()) {            earliest = INSTANT_ORDERING.min(timers.first().getTimestamp(), earliest);        }    }    for (NavigableSet<TimerData> timers : synchronizedProcessingTimers.values()) {        if (!timers.isEmpty()) {            earliest = INSTANT_ORDERING.min(timers.first().getTimestamp(), earliest);        }    }    if (!pendingTimers.isEmpty()) {        earliest = INSTANT_ORDERING.min(pendingTimers.first().getTimestamp(), earliest);    }    return earliest;}
private synchronized void beam_f4278_0(TimerUpdate update)
{    Map<TimeDomain, NavigableSet<TimerData>> timerMap = timerMap(update.key);    Table<StateNamespace, String, TimerData> existingTimersForKey = existingTimers.computeIfAbsent(update.key, k -> HashBasedTable.create());    for (TimerData addedTimer : update.setTimers) {        NavigableSet<TimerData> timerQueue = timerMap.get(addedTimer.getDomain());        if (timerQueue == null) {            continue;        }        @Nullable        TimerData existingTimer = existingTimersForKey.get(addedTimer.getNamespace(), addedTimer.getTimerId());        if (existingTimer == null) {            timerQueue.add(addedTimer);        } else if (!existingTimer.equals(addedTimer)) {            timerQueue.remove(existingTimer);            timerQueue.add(addedTimer);        }                existingTimersForKey.put(addedTimer.getNamespace(), addedTimer.getTimerId(), addedTimer);    }    for (TimerData deletedTimer : update.deletedTimers) {        NavigableSet<TimerData> timerQueue = timerMap.get(deletedTimer.getDomain());        if (timerQueue == null) {            continue;        }        @Nullable        TimerData existingTimer = existingTimersForKey.get(deletedTimer.getNamespace(), deletedTimer.getTimerId());        if (existingTimer != null) {            pendingTimers.remove(deletedTimer);            timerQueue.remove(deletedTimer);            existingTimersForKey.remove(existingTimer.getNamespace(), existingTimer.getTimerId());        }    }    for (TimerData completedTimer : update.completedTimers) {        pendingTimers.remove(completedTimer);    }}
public WatermarkUpdate beam_f4287_0()
{        return WatermarkUpdate.NO_CHANGE;}
public Instant beam_f4288_0()
{    return BoundedWindow.TIMESTAMP_MAX_VALUE;}
public void beam_f4297_0(@Nullable Bundle<?, ? extends CollectionT> completed, TimerUpdate timerUpdate, ExecutableT executable, @Nullable Bundle<?, ? extends CollectionT> unprocessedInputs, Iterable<? extends Bundle<?, ? extends CollectionT>> outputs, Instant earliestHold)
{    pendingUpdates.offer(PendingWatermarkUpdate.create(executable, completed, timerUpdate, unprocessedInputs, outputs, earliestHold));    tryApplyPendingUpdates();}
private void beam_f4298_0()
{    if (refreshLock.tryLock()) {        try {            applyNUpdates(MAX_INCREMENTAL_UPDATES);        } finally {            refreshLock.unlock();        }    }}
public static KeyedHold beam_f4307_0(Object key, Instant timestamp)
{    return new KeyedHold(key, MoreObjects.firstNonNull(timestamp, THE_END_OF_TIME.get()));}
public int beam_f4308_0(KeyedHold that)
{    return ComparisonChain.start().compare(this.timestamp, that.timestamp).compare(this.key, that.key, KEY_ORDERING).result();}
public Instant beam_f4317_0()
{    return outputWatermark.get();}
public synchronized Instant beam_f4318_0()
{    latestSynchronizedInputWm = INSTANT_ORDERING.max(latestSynchronizedInputWm, INSTANT_ORDERING.min(clock.now(), synchronizedProcessingInputWatermark.get()));    return latestSynchronizedInputWm;}
public String beam_f4327_0()
{    return MoreObjects.toStringHelper(TransformWatermarks.class).add("inputWatermark", inputWatermark).add("outputWatermark", outputWatermark).add("inputProcessingTime", synchronizedProcessingInputWatermark).add("outputProcessingTime", synchronizedProcessingOutputWatermark).toString();}
public static TimerUpdate beam_f4328_0()
{    return new TimerUpdate(null, Collections.emptyList(), Collections.emptyList(), Collections.emptyList());}
public Iterable<? extends TimerData> beam_f4337_0()
{    return deletedTimers;}
public TimerUpdate beam_f4338_0(Iterable<TimerData> completedTimers)
{    return new TimerUpdate(this.key, completedTimers, setTimers, deletedTimers);}
public TransformEvaluator<InputT> beam_f4347_0(AppliedPTransform<?, ?, ?> application, @Nullable CommittedBundle<?> inputBundle) throws Exception
{    return createTransformEvaluator((AppliedPTransform) application);}
private TransformEvaluator<InputT> beam_f4348_0(AppliedPTransform<PCollection<InputT>, PCollection<InputT>, Window.Assign<InputT>> transform)
{    WindowFn<? super InputT, ?> fn = (WindowFn) WindowIntoTranslation.getWindowFn(transform);    UncommittedBundle<InputT> outputBundle = evaluationContext.createBundle((PCollection<InputT>) Iterables.getOnlyElement(transform.getOutputs().values()));    if (fn == null) {        return PassthroughTransformEvaluator.create(transform, outputBundle);    }    return new WindowIntoEvaluator<>(transform, fn, outputBundle);}
public PCollectionView<Integer> beam_f4358_0(PCollection<T> records)
{    return records.apply(Window.into(new GlobalWindows())).apply("CountRecords", Count.globally()).apply("GenerateShardCount", ParDo.of(new CalculateShardsFn())).apply(View.asSingleton());}
public void beam_f4359_0(ProcessContext ctxt)
{    ctxt.output(calculateShards(ctxt.element()));}
public void beam_f4368_0() throws Exception
{    TestSource<Long> source = new TestSource<>(BigEndianLongCoder.of(), 1L, 2L, 3L);    PCollection<Long> pcollection = p.apply(Read.from(source));    AppliedPTransform<?, ?, ?> sourceTransform = DirectGraphs.getProducer(pcollection);    UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);    when(context.createBundle(pcollection)).thenReturn(output);    TransformEvaluator<BoundedSourceShard<Long>> evaluator = factory.forApplication(sourceTransform, bundleFactory.createRootBundle().commit(Instant.now()));    evaluator.processElement(WindowedValue.valueInGlobalWindow(BoundedSourceShard.of(source)));    evaluator.finishBundle();    CommittedBundle<Long> committed = output.commit(Instant.now());    assertThat(committed.getElements(), containsInAnyOrder(gw(2L), gw(3L), gw(1L)));    assertThat(TestSource.readerClosed, is(true));}
public void beam_f4369_0() throws Exception
{    TestSource<Long> source = new TestSource<>(BigEndianLongCoder.of());    PCollection<Long> pcollection = p.apply(Read.from(source));    AppliedPTransform<?, ?, ?> sourceTransform = DirectGraphs.getProducer(pcollection);    UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);    when(context.createBundle(pcollection)).thenReturn(output);    TransformEvaluator<BoundedSourceShard<Long>> evaluator = factory.forApplication(sourceTransform, bundleFactory.createRootBundle().commit(Instant.now()));    evaluator.processElement(WindowedValue.valueInGlobalWindow(BoundedSourceShard.of(source)));    evaluator.finishBundle();    CommittedBundle<Long> committed = output.commit(Instant.now());    assertThat(committed.getElements(), emptyIterable());    assertThat(TestSource.readerClosed, is(true));}
protected long beam_f4378_0() throws NoSuchElementException
{    return (long) index;}
public boolean beam_f4379_0() throws IOException
{    return advanceImpl();}
public void beam_f4388_0()
{    PCollection<Record> pc = p.apply(Create.empty(new RecordNoDecodeCoder()));    UncommittedBundle<Record> bundle = factory.createBundle(pc);    thrown.expect(UserCodeException.class);    thrown.expectCause(isA(CoderException.class));    thrown.expectMessage("Decode not allowed");    bundle.add(WindowedValue.valueInGlobalWindow(new Record()));}
public void beam_f4389_0()
{    PCollection<Record> pc = p.apply(Create.empty(new RecordNoEncodeCoder()));    UncommittedBundle<Record> bundle = factory.createKeyedBundle(StructuralKey.of("foo", StringUtf8Coder.of()), pc);    thrown.expect(UserCodeException.class);    thrown.expectCause(isA(CoderException.class));    thrown.expectMessage("Encode not allowed");    bundle.add(WindowedValue.valueInGlobalWindow(new Record()));}
public Record beam_f4401_0(InputStream inStream) throws CoderException, IOException
{    return new Record() {        @Override        public String toString() {            return "DecodedRecord";        }    };}
public String beam_f4402_0()
{    return "DecodedRecord";}
public void beam_f4411_0()
{    List<? extends CommittedBundle<Integer>> outputs = ImmutableList.of(bundleFactory.createBundle(PCollection.createPrimitiveOutputInternal(p, WindowingStrategy.globalDefault(), PCollection.IsBounded.BOUNDED, VarIntCoder.of())).commit(Instant.now()), bundleFactory.createBundle(PCollection.createPrimitiveOutputInternal(p, WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED, VarIntCoder.of())).commit(Instant.now()));    CommittedResult<AppliedPTransform<?, ?, ?>> result = CommittedResult.create(StepTransformResult.withoutHold(transform).build(), Optional.absent(), outputs, EnumSet.of(OutputType.BUNDLE, OutputType.PCOLLECTION_VIEW));    assertThat(result.getOutputs(), Matchers.containsInAnyOrder(outputs.toArray()));}
public void beam_f4412_0()
{    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    StateNamespace namespace = new StateNamespaceForTest("foo");    StateTag<BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of());    BagState<String> stringBag = internals.state(namespace, bagTag);    assertThat(stringBag.read(), emptyIterable());    stringBag.add("bar");    stringBag.add("baz");    assertThat(stringBag.read(), containsInAnyOrder("baz", "bar"));    BagState<String> reReadStringBag = internals.state(namespace, bagTag);    assertThat(reReadStringBag.read(), containsInAnyOrder("baz", "bar"));}
public void beam_f4421_0()
{    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);    StateNamespace namespace = new StateNamespaceForTest("foo");    StateTag<BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of());    BagState<String> stringBag = underlying.state(namespace, bagTag);    assertThat(stringBag.read(), emptyIterable());    stringBag.add("bar");    stringBag.add("baz");    internals.commit();    BagState<String> reReadStringBag = internals.state(namespace, bagTag);    assertThat(reReadStringBag.read(), containsInAnyOrder("baz", "bar"));    reReadStringBag.add("spam");    BagState<String> underlyingState = underlying.state(namespace, bagTag);    assertThat(underlyingState.read(), containsInAnyOrder("spam", "bar", "baz"));    assertThat(underlyingState, is(theInstance(stringBag)));    assertThat(internals.isEmpty(), is(false));}
public void beam_f4422_0()
{    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying(key, null);    CopyOnAccessInMemoryStateInternals<String> secondUnderlying = spy(CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying));    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying(key, secondUnderlying);    StateNamespace namespace = new StateNamespaceForTest("foo");    StateTag<BagState<String>> bagTag = StateTags.bag("foo", StringUtf8Coder.of());    BagState<String> stringBag = underlying.state(namespace, bagTag);    assertThat(stringBag.read(), emptyIterable());    stringBag.add("bar");    stringBag.add("baz");    stringBag.clear();        secondUnderlying.commit();        stringBag.add("foo");    internals.commit();    BagState<String> internalsStringBag = internals.state(namespace, bagTag);    assertThat(internalsStringBag.read(), emptyIterable());    verify(secondUnderlying, never()).state(namespace, bagTag);    assertThat(internals.isEmpty(), is(false));}
public void beam_f4431_0()
{    BoundedWindow first = new BoundedWindow() {        @Override        public Instant maxTimestamp() {            return new Instant(2048L);        }    };    BoundedWindow second = new BoundedWindow() {        @Override        public Instant maxTimestamp() {            return new Instant(689743L);        }    };    CopyOnAccessInMemoryStateInternals<String> underlying = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", null);    StateTag<WatermarkHoldState> firstHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST);    WatermarkHoldState firstHold = underlying.state(StateNamespaces.window(null, first), firstHoldAddress);    firstHold.add(new Instant(22L));    CopyOnAccessInMemoryStateInternals<String> internals = CopyOnAccessInMemoryStateInternals.withUnderlying("foo", underlying.commit());    StateTag<WatermarkHoldState> secondHoldAddress = StateTags.watermarkStateInternal("foo", TimestampCombiner.EARLIEST);    WatermarkHoldState secondHold = internals.state(StateNamespaces.window(null, second), secondHoldAddress);    secondHold.add(new Instant(244L));    internals.commit();    assertThat(internals.getEarliestWatermarkHold(), equalTo(new Instant(22L)));}
public Instant beam_f4432_0()
{    return new Instant(2048L);}
public void beam_f4441_0()
{    PCollectionView<List<String>> listView = p.apply("listCreate", Create.of("foo", "bar")).apply(ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(DoFn<String, String>.ProcessContext c) throws Exception {            c.output(Integer.toString(c.element().length()));        }    })).apply(View.asList());    PCollectionView<Object> singletonView = p.apply("singletonCreate", Create.<Object>of(1, 2, 3)).apply(View.asSingleton());    p.replaceAll(DirectRunner.fromOptions(TestPipeline.testingPipelineOptions()).defaultTransformOverrides());    p.traverseTopologically(visitor);    assertThat(visitor.getGraph().getViews(), Matchers.containsInAnyOrder(listView, singletonView));}
public void beam_f4442_0(DoFn<String, String>.ProcessContext c) throws Exception
{    c.output(Integer.toString(c.element().length()));}
public void beam_f4451_0()
{    p.apply(Create.of(1, 2, 3));    p.traverseTopologically(visitor);    thrown.expect(IllegalStateException.class);    thrown.expectMessage(DirectGraphVisitor.class.getSimpleName());    thrown.expectMessage("is finalized");    p.traverseTopologically(visitor);}
public void beam_f4452_0()
{    p.apply("left", Create.of(1, 2, 3));    p.apply("right", Create.of("foo", "bar", "baz"));    p.traverseTopologically(visitor);}
public void beam_f4461_0()
{    assertEquals(ImmutableList.of(DirectOptions.class, DirectTestOptions.class), new Options().getPipelineOptions());}
public void beam_f4462_0()
{    assertEquals(ImmutableList.of(org.apache.beam.runners.direct.DirectRunner.class), new Runner().getPipelineRunners());}
public void beam_f4471_0() throws Throwable
{    Pipeline p = getPipeline();    changed = new AtomicInteger(0);    PCollection<KV<String, Long>> counts = p.apply(Create.of("foo", "bar", "foo", "baz", "bar", "foo")).apply(MapElements.via(new SimpleFunction<String, String>() {        @Override        public String apply(String input) {            return input;        }    })).apply(Count.perElement());    PCollection<String> countStrs = counts.apply(MapElements.via(new SimpleFunction<KV<String, Long>, String>() {        @Override        public String apply(KV<String, Long> input) {            return String.format("%s: %s", input.getKey(), input.getValue());        }    }));    counts.apply(ParDo.of(new DoFn<KV<String, Long>, Void>() {        @ProcessElement        public void updateChanged(ProcessContext c) {            changed.getAndIncrement();        }    }));    PAssert.that(countStrs).containsInAnyOrder("baz: 1", "bar: 2", "foo: 3");    DirectPipelineResult result = (DirectPipelineResult) p.run();    result.waitUntilFinish();    DirectPipelineResult otherResult = (DirectPipelineResult) p.run();    otherResult.waitUntilFinish();    assertThat("Each element should have been processed twice", changed.get(), equalTo(6));}
public String beam_f4472_0(String input)
{    return input;}
public void beam_f4481_0()
{    TEARDOWN_CALL.set(-1);    final Pipeline pipeline = getPipeline();    pipeline.apply(Create.of("a")).apply(ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void onElement(final ProcessContext ctx) {                }        @Teardown        public void teardown() {                        try {                Thread.sleep(1000);            } catch (final InterruptedException e) {                throw new AssertionError(e);            }            TEARDOWN_CALL.set(System.nanoTime());        }    }));    final PipelineResult pipelineResult = pipeline.run();    pipelineResult.waitUntilFinish();    final long doneTs = System.nanoTime();    final long tearDownTs = TEARDOWN_CALL.get();    assertThat(tearDownTs, greaterThan(0L));    assertThat(doneTs, greaterThan(tearDownTs));}
public void beam_f4483_0()
{        try {        Thread.sleep(1000);    } catch (final InterruptedException e) {        throw new AssertionError(e);    }    TEARDOWN_CALL.set(System.nanoTime());}
public void beam_f4493_0() throws Exception
{    Pipeline pipeline = getPipeline();    pipeline.apply(Create.of(42)).apply(ParDo.of(new DoFn<Integer, byte[]>() {        @ProcessElement        public void processElement(ProcessContext c) {            byte[] outputArray = new byte[] { 0x1, 0x2, 0x3 };            c.output(outputArray);            outputArray[0] = 0xa;            c.output(outputArray);        }    }));    thrown.expect(IllegalMutationException.class);    thrown.expectMessage("output");    thrown.expectMessage("must not be mutated");    pipeline.run();}
public void beam_f4494_0(ProcessContext c)
{    byte[] outputArray = new byte[] { 0x1, 0x2, 0x3 };    c.output(outputArray);    outputArray[0] = 0xa;    c.output(outputArray);}
public void beam_f4503_0()
{    Pipeline p = getPipeline();    p.apply(GenerateSequence.from(0)).setCoder(new LongNoDecodeCoder());    thrown.expectCause(isA(CoderException.class));    thrown.expectMessage("Cannot decode a long");    p.run();}
public void beam_f4504_0()
{    TestSerializationOfOptions options = PipelineOptionsFactory.fromArgs("--foo=testValue", "--ignoredField=overridden", "--runner=DirectRunner").as(TestSerializationOfOptions.class);    assertEquals("testValue", options.getFoo());    assertEquals("overridden", options.getIgnoredField());    Pipeline p = Pipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of("1")).apply(ParDo.of(new DoFn<String, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            TestSerializationOfOptions options = c.getPipelineOptions().as(TestSerializationOfOptions.class);            assertEquals("testValue", options.getFoo());            assertEquals("not overridden", options.getIgnoredField());            c.output(Integer.parseInt(c.element()));        }    }));    PAssert.that(pc).containsInAnyOrder(1);    p.run();}
public void beam_f4514_0()
{    MockitoAnnotations.initMocks(this);    clock = MockClock.fromInstant(new Instant(0));    timerUpdateBuilder = TimerUpdate.builder(StructuralKey.of(1234, VarIntCoder.of()));    internals = DirectTimerInternals.create(clock, watermarks, timerUpdateBuilder);}
public void beam_f4515_0()
{    TimerData eventTimer = TimerData.of(StateNamespaces.global(), new Instant(20145L), TimeDomain.EVENT_TIME);    TimerData processingTimer = TimerData.of(StateNamespaces.global(), new Instant(125555555L), TimeDomain.PROCESSING_TIME);    TimerData synchronizedProcessingTimer = TimerData.of(StateNamespaces.global(), new Instant(98745632189L), TimeDomain.SYNCHRONIZED_PROCESSING_TIME);    internals.setTimer(eventTimer);    internals.setTimer(processingTimer);    internals.setTimer(synchronizedProcessingTimer);    assertThat(internals.getTimerUpdate().getSetTimers(), containsInAnyOrder(eventTimer, synchronizedProcessingTimer, processingTimer));}
public TransformResult<Object> beam_f4524_0() throws Exception
{    finishCalled.set(true);    return result;}
public void beam_f4525_0() throws Exception
{    when(registry.forApplication(createdProducer, null)).thenReturn(null);    DirectTransformExecutor<Object> executor = new DirectTransformExecutor<>(evaluationContext, registry, Collections.emptyList(), null, createdProducer, completionCallback, transformEvaluationState);    executor.run();    assertThat(completionCallback.handledResult, is(nullValue()));    assertThat(completionCallback.handledEmpty, equalTo(true));    assertThat(completionCallback.handledException, is(nullValue()));}
public void beam_f4535_0() throws Exception
{    final TransformResult<Object> result = StepTransformResult.withoutHold(downstreamProducer).build();    TransformEvaluator<Object> evaluator = new TransformEvaluator<Object>() {        @Override        public void processElement(WindowedValue<Object> element) throws Exception {        }        @Override        public TransformResult<Object> finishBundle() throws Exception {            return result;        }    };    WindowedValue<String> fooElem = WindowedValue.valueInGlobalWindow("foo");    WindowedValue<String> barElem = WindowedValue.valueInGlobalWindow("bar");    CommittedBundle<String> inputBundle = bundleFactory.createBundle(created).add(fooElem).add(barElem).commit(Instant.now());    when(registry.forApplication(downstreamProducer, inputBundle)).thenReturn(evaluator);    TestEnforcementFactory enforcement = new TestEnforcementFactory();    DirectTransformExecutor<String> executor = new DirectTransformExecutor<>(evaluationContext, registry, Collections.<ModelEnforcementFactory>singleton(enforcement), inputBundle, downstreamProducer, completionCallback, transformEvaluationState);    executor.run();    TestEnforcement<?> testEnforcement = enforcement.instance;    assertThat(testEnforcement.beforeElements, containsInAnyOrder(barElem, fooElem));    assertThat(testEnforcement.afterElements, containsInAnyOrder(barElem, fooElem));    assertThat(testEnforcement.finishedBundles, Matchers.contains(result));}
public TransformResult<Object> beam_f4537_0() throws Exception
{    return result;}
public TestEnforcement<T> beam_f4548_0(CommittedBundle<T> input, AppliedPTransform<?, ?, ?> consumer)
{    TestEnforcement<T> newEnforcement = new TestEnforcement<>();    instance = newEnforcement;    return newEnforcement;}
public void beam_f4549_0(WindowedValue<T> element)
{    beforeElements.add(element);}
public void beam_f4558_0() throws Exception
{    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);    doThrow(IllegalArgumentException.class).when(underlying).processElement(any(WindowedValue.class));    DoFn<?, ?> original = lifecycleManager.get();    assertThat(original, not(nullValue()));    TransformEvaluator<Object> evaluator = DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(underlying, lifecycleManager);    try {        evaluator.processElement(WindowedValue.valueInGlobalWindow(new Object()));    } catch (Exception e) {        assertThat(lifecycleManager.get(), not(Matchers.theInstance(original)));        return;    }    fail("Expected underlying evaluator to throw on method call");}
public void beam_f4559_0() throws Exception
{    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);    doThrow(IllegalArgumentException.class).when(underlying).onTimer(any(TimerData.class), any(BoundedWindow.class));    DoFn<?, ?> original = lifecycleManager.get();    assertThat(original, not(nullValue()));    DoFnLifecycleManagerRemovingTransformEvaluator<Object> evaluator = DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(underlying, lifecycleManager);    try {        evaluator.onTimer(TimerData.of("foo", StateNamespaces.global(), new Instant(0), TimeDomain.EVENT_TIME), GlobalWindow.INSTANCE);    } catch (Exception e) {        assertThat(lifecycleManager.get(), not(Matchers.theInstance(original)));        return;    }    fail("Expected underlying evaluator to throw on method call");}
public boolean beam_f4570_0(Object item)
{    if (!(item instanceof UserCodeException)) {        return false;    }    UserCodeException that = (UserCodeException) item;    return causeMatcher.matches(that.getCause());}
public void beam_f4571_0(Description description)
{    description.appendText("a throwable with a cause ").appendDescriptionOf(causeMatcher);}
public Void beam_f4581_0() throws Exception
{    startSignal.await();        mgr.remove();    return null;}
public void beam_f4582_0()
{    checkState(!setupCalled, "Cannot call setup: already set up");    checkState(!teardownCalled, "Cannot call setup: already torn down");    setupCalled = true;}
public void beam_f4592_0() throws Exception
{    TransformResult<?> finishedResult = StepTransformResult.withoutHold(createdProducer).build();    context.handleResult(null, ImmutableList.of(), finishedResult);    final CountDownLatch callLatch = new CountDownLatch(1);    context.extractFiredTimers();    Runnable callback = callLatch::countDown;    context.scheduleAfterOutputWouldBeProduced(downstream, GlobalWindow.INSTANCE, WindowingStrategy.globalDefault(), callback);    assertThat(callLatch.await(1, TimeUnit.SECONDS), is(true));}
public void beam_f4593_0()
{    TransformResult<?> holdResult = StepTransformResult.withHold(createdProducer, new Instant(0)).build();    context.handleResult(null, ImmutableList.of(), holdResult);    StructuralKey<?> key = StructuralKey.of("foo".length(), VarIntCoder.of());    TimerData toFire = TimerData.of(StateNamespaces.global(), new Instant(100L), TimeDomain.EVENT_TIME);    TransformResult<?> timerResult = StepTransformResult.withoutHold(downstreamProducer).withState(CopyOnAccessInMemoryStateInternals.withUnderlying(key, null)).withTimerUpdate(TimerUpdate.builder(key).setTimer(toFire).build()).build();        assertThat(context.extractFiredTimers(), emptyIterable());    context.handleResult(context.createKeyedBundle(key, created).commit(Instant.now()), ImmutableList.of(), timerResult);        assertThat(context.extractFiredTimers(), emptyIterable());    TransformResult<?> advanceResult = StepTransformResult.withoutHold(createdProducer).build();        context.handleResult(null, ImmutableList.of(), advanceResult);    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> fired = context.extractFiredTimers();    assertThat(Iterables.getOnlyElement(fired).getKey(), equalTo(key));    FiredTimers<AppliedPTransform<?, ?, ?>> firedForKey = Iterables.getOnlyElement(fired);        assertThat(firedForKey.getTimers(), contains(toFire));        assertThat(context.extractFiredTimers(), emptyIterable());}
public void beam_f4602_0() throws Exception
{    PCollectionList<Integer> list = PCollectionList.empty(p);    PCollection<Integer> flattened = list.apply(Flatten.pCollections());    flattened.setCoder(VarIntCoder.of());    EvaluationContext evaluationContext = mock(EvaluationContext.class);    when(evaluationContext.createBundle(flattened)).thenReturn(bundleFactory.createBundle(flattened));    FlattenEvaluatorFactory factory = new FlattenEvaluatorFactory(evaluationContext);    AppliedPTransform<?, ?, ?> flattendProducer = DirectGraphs.getProducer(flattened);    TransformEvaluator<Integer> emptyEvaluator = factory.forApplication(flattendProducer, bundleFactory.createRootBundle().commit(BoundedWindow.TIMESTAMP_MAX_VALUE));    TransformResult<Integer> leftSideResult = emptyEvaluator.finishBundle();    CommittedBundle<?> outputBundle = Iterables.getOnlyElement(leftSideResult.getOutputBundles()).commit(Instant.now());    assertThat(outputBundle.getElements(), emptyIterable());    assertThat(leftSideResult.getTransform(), Matchers.<AppliedPTransform<?, ?, ?>>equalTo(flattendProducer));}
public void beam_f4603_0() throws Exception
{    KV<String, Integer> firstFoo = KV.of("foo", -1);    KV<String, Integer> secondFoo = KV.of("foo", 1);    KV<String, Integer> thirdFoo = KV.of("foo", 3);    KV<String, Integer> firstBar = KV.of("bar", 22);    KV<String, Integer> secondBar = KV.of("bar", 12);    KV<String, Integer> firstBaz = KV.of("baz", Integer.MAX_VALUE);    PCollection<KV<String, Integer>> values = p.apply(Create.of(firstFoo, firstBar, secondFoo, firstBaz, secondBar, thirdFoo));    PCollection<KeyedWorkItem<String, Integer>> groupedKvs = values.apply(new DirectGroupByKeyOnly<>());    CommittedBundle<KV<String, Integer>> inputBundle = bundleFactory.createBundle(values).commit(Instant.now());    EvaluationContext evaluationContext = mock(EvaluationContext.class);    StructuralKey<String> fooKey = StructuralKey.of("foo", StringUtf8Coder.of());    UncommittedBundle<KeyedWorkItem<String, Integer>> fooBundle = bundleFactory.createKeyedBundle(fooKey, groupedKvs);    StructuralKey<String> barKey = StructuralKey.of("bar", StringUtf8Coder.of());    UncommittedBundle<KeyedWorkItem<String, Integer>> barBundle = bundleFactory.createKeyedBundle(barKey, groupedKvs);    StructuralKey<String> bazKey = StructuralKey.of("baz", StringUtf8Coder.of());    UncommittedBundle<KeyedWorkItem<String, Integer>> bazBundle = bundleFactory.createKeyedBundle(bazKey, groupedKvs);    when(evaluationContext.createKeyedBundle(fooKey, groupedKvs)).thenReturn(fooBundle);    when(evaluationContext.createKeyedBundle(barKey, groupedKvs)).thenReturn(barBundle);    when(evaluationContext.createKeyedBundle(bazKey, groupedKvs)).thenReturn(bazBundle);        @SuppressWarnings("unchecked")    Coder<String> keyCoder = ((KvCoder<String, Integer>) values.getCoder()).getKeyCoder();    TransformEvaluator<KV<String, Integer>> evaluator = new GroupByKeyOnlyEvaluatorFactory(evaluationContext).forApplication(DirectGraphs.getProducer(groupedKvs), inputBundle);    evaluator.processElement(WindowedValue.valueInGlobalWindow(firstFoo));    evaluator.processElement(WindowedValue.valueInGlobalWindow(secondFoo));    evaluator.processElement(WindowedValue.valueInGlobalWindow(thirdFoo));    evaluator.processElement(WindowedValue.valueInGlobalWindow(firstBar));    evaluator.processElement(WindowedValue.valueInGlobalWindow(secondBar));    evaluator.processElement(WindowedValue.valueInGlobalWindow(firstBaz));    evaluator.finishBundle();    assertThat(fooBundle.commit(Instant.now()).getElements(), contains(new KeyedWorkItemMatcher<>(KeyedWorkItems.elementsWorkItem("foo", ImmutableSet.of(WindowedValue.valueInGlobalWindow(-1), WindowedValue.valueInGlobalWindow(1), WindowedValue.valueInGlobalWindow(3))), keyCoder)));    assertThat(barBundle.commit(Instant.now()).getElements(), contains(new KeyedWorkItemMatcher<>(KeyedWorkItems.elementsWorkItem("bar", ImmutableSet.of(WindowedValue.valueInGlobalWindow(12), WindowedValue.valueInGlobalWindow(22))), keyCoder)));    assertThat(bazBundle.commit(Instant.now()).getElements(), contains(new KeyedWorkItemMatcher<>(KeyedWorkItems.elementsWorkItem("baz", ImmutableSet.of(WindowedValue.valueInGlobalWindow(Integer.MAX_VALUE))), keyCoder)));}
public void beam_f4612_0()
{    UncommittedBundle<byte[]> keyed = factory.createKeyedBundle(StructuralKey.of("mykey", StringUtf8Coder.of()), transformed);    WindowedValue<byte[]> windowedArray = WindowedValue.of(new byte[] { 4, 8, 12 }, new Instant(891L), new IntervalWindow(new Instant(0), new Instant(1000)), PaneInfo.ON_TIME_AND_ONLY_FIRING);    keyed.add(windowedArray);    CommittedBundle<byte[]> committed = keyed.commit(Instant.now());    assertThat(committed.getElements(), containsInAnyOrder(windowedArray));}
public void beam_f4613_0()
{    UncommittedBundle<byte[]> intermediate = factory.createBundle(transformed);    WindowedValue<byte[]> windowedArray = WindowedValue.of(new byte[] { 4, 8, 12 }, new Instant(891L), new IntervalWindow(new Instant(0), new Instant(1000)), PaneInfo.ON_TIME_AND_ONLY_FIRING);    intermediate.add(windowedArray);    CommittedBundle<byte[]> committed = intermediate.commit(Instant.now());    assertThat(committed.getElements(), containsInAnyOrder(windowedArray));}
public void beam_f4622_0()
{    WindowedValue<byte[]> element = WindowedValue.valueInGlobalWindow("bar".getBytes(UTF_8));    CommittedBundle<byte[]> elements = bundleFactory.createBundle(pcollection).add(element).commit(Instant.now());    ModelEnforcement<byte[]> enforcement = factory.forBundle(elements, consumer);    enforcement.beforeElement(element);    element.getValue()[0] = 'f';    thrown.expect(IllegalMutationException.class);    thrown.expectMessage(consumer.getFullName());    thrown.expectMessage("illegaly mutated");    thrown.expectMessage("Input values must not be mutated");    enforcement.afterElement(element);    enforcement.afterFinish(elements, StepTransformResult.<byte[]>withoutHold(consumer).build(), Collections.emptyList());}
public void beam_f4623_0()
{    WindowedValue<byte[]> element = WindowedValue.valueInGlobalWindow("bar".getBytes(UTF_8));    CommittedBundle<byte[]> elements = bundleFactory.createBundle(pcollection).add(element).commit(Instant.now());    ModelEnforcement<byte[]> enforcement = factory.forBundle(elements, consumer);    enforcement.beforeElement(element);    enforcement.afterElement(element);    element.getValue()[0] = 'f';    thrown.expect(IllegalMutationException.class);    thrown.expectMessage(consumer.getFullName());    thrown.expectMessage("illegaly mutated");    thrown.expectMessage("Input values must not be mutated");    enforcement.afterFinish(elements, StepTransformResult.<byte[]>withoutHold(consumer).build(), Collections.emptyList());}
public void beam_f4632_0()
{    WindowedValue<Integer> firstValue = WindowedValue.valueInGlobalWindow(1);    WindowedValue<Integer> secondValue = WindowedValue.timestampedValueInGlobalWindow(2, new Instant(1000L));    afterCommitGetElementsShouldHaveAddedElements(ImmutableList.of(firstValue, secondValue));}
public void beam_f4633_0()
{    Instant timestamp = BoundedWindow.TIMESTAMP_MAX_VALUE;    WindowedValue<Integer> value = WindowedValue.timestampedValueInGlobalWindow(1, timestamp);    UncommittedBundle<Integer> bundle = bundleFactory.createRootBundle();    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(timestamp.toString());    bundle.add(value);}
public void beam_f4642_0()
{    p = TestPipeline.create();    visitor = KeyedPValueTrackingVisitor.create();}
public void beam_f4643_0()
{    PCollection<KV<String, Iterable<Integer>>> keyed = p.apply(Create.of(KV.of("foo", 3))).apply(new DirectGroupByKeyOnly<>()).apply(new DirectGroupAlsoByWindow<>(WindowingStrategy.globalDefault(), WindowingStrategy.globalDefault()));    p.traverseTopologically(visitor);    assertThat(visitor.getKeyedPValues(), hasItem(keyed));}
public void beam_f4652_0(Instant newNow)
{    checkArgument(!newNow.isBefore(now), "Cannot move MockClock backwards in time from %s to %s", now, newNow);    this.now = newNow;}
public void beam_f4653_0(Duration duration)
{    checkArgument(duration.getMillis() > 0, "Cannot move MockClock backwards in time by duration %s", duration);    set(now.plus(duration));}
public MultiStepAccumulator beam_f4662_0(Iterable<MultiStepAccumulator> accumulators)
{    MultiStepAccumulator result = MultiStepAccumulator.of(0L, false);    for (MultiStepAccumulator accumulator : accumulators) {        result = result.merge(accumulator);    }    return result;}
public Long beam_f4663_0(MultiStepAccumulator accumulator)
{    assertThat("Accumulators should have been serialized and deserialized within the Pipeline", accumulator.isDeserialized(), is(true));    return accumulator.getValue();}
public T beam_f4672_0(PCollectionView<T> view, BoundedWindow window)
{    if (window.equals(GlobalWindow.INSTANCE)) {        return (T) (Integer) 5;    }    fail("Should only call get in the Global Window, others are not ready");    throw new AssertionError("Unreachable");}
public boolean beam_f4673_0(PCollectionView<T> view)
{    return true;}
public void beam_f4682_0() throws Exception
{    ImmutableList.Builder<WindowedValue<?>> valuesBuilder = ImmutableList.builder();    for (Object materializedValue : materializeValuesFor(View.asMap(), KV.of("one", 1))) {        valuesBuilder.add(WindowedValue.of(materializedValue, new Instant(1L), SECOND_WINDOW, PaneInfo.createPane(true, false, Timing.EARLY)));    }    for (Object materializedValue : materializeValuesFor(View.asMap(), KV.of("two", 2))) {        valuesBuilder.add(WindowedValue.of(materializedValue, new Instant(20L), SECOND_WINDOW, PaneInfo.createPane(true, false, Timing.EARLY)));    }    container.write(mapView, valuesBuilder.build());    Map<String, Integer> viewContents = container.createReaderForViews(ImmutableList.of(mapView)).get(mapView, SECOND_WINDOW);    assertThat(viewContents, hasEntry("one", 1));    assertThat(viewContents, hasEntry("two", 2));    assertThat(viewContents.size(), is(2));    ImmutableList.Builder<WindowedValue<?>> overwriteValuesBuilder = ImmutableList.builder();    for (Object materializedValue : materializeValuesFor(View.asMap(), KV.of("three", 3))) {        overwriteValuesBuilder.add(WindowedValue.of(materializedValue, new Instant(300L), SECOND_WINDOW, PaneInfo.createPane(false, false, Timing.EARLY, 1, -1)));    }    container.write(mapView, overwriteValuesBuilder.build());    Map<String, Integer> overwrittenViewContents = container.createReaderForViews(ImmutableList.of(mapView)).get(mapView, SECOND_WINDOW);    assertThat(overwrittenViewContents, hasEntry("three", 3));    assertThat(overwrittenViewContents.size(), is(1));}
public void beam_f4683_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("not ready");    container.createReaderForViews(ImmutableList.of(mapView)).get(mapView, GlobalWindow.INSTANCE);}
public void beam_f4692_0()
{    ImmutableList.Builder<WindowedValue<?>> mapValuesBuilder = ImmutableList.builder();    for (Object materializedValue : materializeValuesFor(View.asMap(), KV.of("one", 1))) {        mapValuesBuilder.add(WindowedValue.of(materializedValue, SECOND_WINDOW.maxTimestamp().minus(100L), SECOND_WINDOW, PaneInfo.ON_TIME_AND_ONLY_FIRING));    }    container.write(mapView, mapValuesBuilder.build());    ReadyCheckingSideInputReader reader = container.createReaderForViews(ImmutableList.of(mapView, singletonView));    assertThat(reader.isReady(mapView, FIRST_WINDOW), is(false));    assertThat(reader.isReady(mapView, SECOND_WINDOW), is(true));    assertThat(reader.isReady(singletonView, SECOND_WINDOW), is(false));    ImmutableList.Builder<WindowedValue<?>> newMapValuesBuilder = ImmutableList.builder();    for (Object materializedValue : materializeValuesFor(View.asMap(), KV.of("too", 2))) {        newMapValuesBuilder.add(WindowedValue.of(materializedValue, FIRST_WINDOW.maxTimestamp().minus(100L), FIRST_WINDOW, PaneInfo.ON_TIME_AND_ONLY_FIRING));    }    container.write(mapView, newMapValuesBuilder.build());        assertThat(reader.isReady(mapView, FIRST_WINDOW), is(false));    ImmutableList.Builder<WindowedValue<?>> singletonValuesBuilder = ImmutableList.builder();    for (Object materializedValue : materializeValuesFor(View.asSingleton(), 1.25)) {        singletonValuesBuilder.add(WindowedValue.of(materializedValue, SECOND_WINDOW.maxTimestamp().minus(100L), SECOND_WINDOW, PaneInfo.ON_TIME_AND_ONLY_FIRING));    }    container.write(singletonView, singletonValuesBuilder.build());    assertThat(reader.isReady(mapView, SECOND_WINDOW), is(true));    assertThat(reader.isReady(singletonView, SECOND_WINDOW), is(false));    assertThat(reader.isReady(mapView, GlobalWindow.INSTANCE), is(false));    assertThat(reader.isReady(singletonView, GlobalWindow.INSTANCE), is(false));    reader = container.createReaderForViews(ImmutableList.of(mapView, singletonView));    assertThat(reader.isReady(mapView, SECOND_WINDOW), is(true));    assertThat(reader.isReady(singletonView, SECOND_WINDOW), is(true));    assertThat(reader.isReady(mapView, FIRST_WINDOW), is(true));}
public void beam_f4693_0() throws Exception
{    CountDownLatch onComplete = new CountDownLatch(1);    immediatelyInvokeCallback(mapView, GlobalWindow.INSTANCE);    CountDownLatch latch = invokeLatchedCallback(singletonView, GlobalWindow.INSTANCE, onComplete);    ReadyCheckingSideInputReader reader = container.createReaderForViews(ImmutableList.of(mapView, singletonView));    assertThat(reader.isReady(mapView, GlobalWindow.INSTANCE), is(true));    assertThat(reader.isReady(singletonView, GlobalWindow.INSTANCE), is(false));    latch.countDown();    if (!onComplete.await(1500L, TimeUnit.MILLISECONDS)) {        fail("Callback to set empty values did not complete!");    }        assertThat(reader.isReady(singletonView, GlobalWindow.INSTANCE), is(false));        reader = container.createReaderForViews(ImmutableList.of(mapView, singletonView));    assertThat(reader.isReady(singletonView, GlobalWindow.INSTANCE), is(true));}
public void beam_f4704_0()
{    TransformResult<Integer> result = StepTransformResult.<Integer>withoutHold(transform).withAdditionalOutput(OutputType.PCOLLECTION_VIEW).build();    assertThat(result.getOutputTypes(), containsInAnyOrder(OutputType.PCOLLECTION_VIEW));}
public void beam_f4705_0()
{    TransformResult<Integer> result = StepTransformResult.<Integer>withoutHold(transform).addOutput(bundleFactory.createBundle(pc)).withAdditionalOutput(OutputType.PCOLLECTION_VIEW).build();    assertThat(result.getOutputTypes(), hasItem(OutputType.PCOLLECTION_VIEW));}
public void beam_f4714_0()
{    @SuppressWarnings("unchecked")    DirectTransformExecutor<Object> first = mock(DirectTransformExecutor.class);    @SuppressWarnings("unchecked")    DirectTransformExecutor<Object> second = mock(DirectTransformExecutor.class);    TransformExecutorService serial = TransformExecutorServices.serial(executorService);    serial.schedule(first);    thrown.expect(IllegalStateException.class);    thrown.expectMessage("unexpected currently executing");    serial.complete(second);}
public void beam_f4715_0()
{    @SuppressWarnings("unchecked")    DirectTransformExecutor<Object> first = mock(DirectTransformExecutor.class);    @SuppressWarnings("unchecked")    DirectTransformExecutor<Object> second = mock(DirectTransformExecutor.class);    TransformExecutorService serial = TransformExecutorServices.serial(executorService);    serial.schedule(first);    verify(first).run();    serial.schedule(second);    verify(second, never()).run();    serial.shutdown();    serial.complete(first);    verify(second, never()).run();}
public void beam_f4724_0() throws Exception
{        PCollection<Long> pcollection = p.apply(Read.from(new TestUnboundedSource<>(VarLongCoder.of(), 1L)));    AppliedPTransform<?, ?, ?> sourceTransform = getProducer(pcollection);    when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());    Collection<CommittedBundle<?>> initialInputs = new UnboundedReadEvaluatorFactory.InputProvider(context, options).getInitialInputs(sourceTransform, 1);            when(context.createBundle(pcollection)).thenReturn(bundleFactory.createBundle(pcollection));    CommittedBundle<?> inputBundle = Iterables.getOnlyElement(initialInputs);    TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> evaluator = factory.forApplication(sourceTransform, inputBundle);    for (WindowedValue<?> value : inputBundle.getElements()) {        evaluator.processElement((WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>>) value);    }    TransformResult<UnboundedSourceShard<Long, TestCheckpointMark>> result = evaluator.finishBundle();            UncommittedBundle<Long> secondOutput = bundleFactory.createBundle(longs);    when(context.createBundle(longs)).thenReturn(secondOutput);    TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> secondEvaluator = factory.forApplication(sourceTransform, inputBundle);    WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>> residual = (WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>>) Iterables.getOnlyElement(result.getUnprocessedElements());    secondEvaluator.processElement(residual);    TransformResult<UnboundedSourceShard<Long, TestCheckpointMark>> secondResult = secondEvaluator.finishBundle();            assertThat(secondOutput.commit(Instant.now()).getElements(), Matchers.emptyIterable());            WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>> unprocessed = (WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>>) Iterables.getOnlyElement(secondResult.getUnprocessedElements());    assertThat(unprocessed.getTimestamp(), Matchers.greaterThan(residual.getTimestamp()));    assertThat(unprocessed.getValue().getExistingReader(), not(nullValue()));}
public void beam_f4725_0() throws Exception
{    int numElements = 1000;    ContiguousSet<Long> elems = ContiguousSet.create(Range.openClosed(0L, (long) numElements), DiscreteDomain.longs());    TestUnboundedSource<Long> source = new TestUnboundedSource<>(BigEndianLongCoder.of(), elems.toArray(new Long[0]));    source.advanceWatermarkToInfinity = true;    PCollection<Long> pcollection = p.apply(Read.from(source));    DirectGraph graph = DirectGraphs.getGraph(p);    AppliedPTransform<?, ?, ?> sourceTransform = graph.getProducer(pcollection);    when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());    UncommittedBundle<Long> output = mock(UncommittedBundle.class);    when(context.createBundle(pcollection)).thenReturn(output);    WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>> shard = WindowedValue.valueInGlobalWindow(UnboundedSourceShard.unstarted(source, NeverDeduplicator.create()));    CommittedBundle<UnboundedSourceShard<Long, TestCheckpointMark>> inputBundle = bundleFactory.<UnboundedSourceShard<Long, TestCheckpointMark>>createRootBundle().add(shard).commit(Instant.now());    UnboundedReadEvaluatorFactory factory = new UnboundedReadEvaluatorFactory(context, options, 1.0);    new UnboundedReadEvaluatorFactory.InputProvider(context, options).getInitialInputs(sourceTransform, 1);    CommittedBundle<UnboundedSourceShard<Long, TestCheckpointMark>> residual = inputBundle;    do {        TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> evaluator = factory.forApplication(sourceTransform, residual);        evaluator.processElement(Iterables.getOnlyElement(residual.getElements()));        TransformResult<UnboundedSourceShard<Long, TestCheckpointMark>> result = evaluator.finishBundle();        residual = inputBundle.withElements((Iterable<WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>>>) result.getUnprocessedElements());    } while (!Iterables.isEmpty(residual.getElements()));    verify(output, times(numElements)).add(any());    assertThat(TestUnboundedSource.readerCreatedCount, equalTo(1));    assertThat(TestUnboundedSource.readerClosedCount, equalTo(1));}
public UnboundedSource.UnboundedReader<T> beam_f4734_0(PipelineOptions options, @Nullable TestCheckpointMark checkpointMark)
{    checkState(checkpointMark == null || checkpointMark.decoded, "Cannot resume from a checkpoint that has not been decoded");    readerCreatedCount++;    return new TestUnboundedReader(elems, checkpointMark == null ? -1 : checkpointMark.index);}
public Coder<TestCheckpointMark> beam_f4735_0()
{    return new TestCheckpointMark.Coder();}
public T beam_f4744_0() throws NoSuchElementException
{    return elems.get(index);}
public Instant beam_f4745_0() throws NoSuchElementException
{    return new Instant(index);}
public void beam_f4754_0()
{    final PCollection<Integer> ints = p.apply("CreateContents", Create.of(1, 2, 3));    final PCollectionView<List<Integer>> view = ints.apply(View.asList());    PTransformReplacement<PCollection<Integer>, PCollection<Integer>> replacement = factory.getReplacementTransform(AppliedPTransform.of("foo", ints.expand(), view.expand(), CreatePCollectionView.of(view), p));    ints.apply(replacement.getTransform());    final AtomicBoolean writeViewVisited = new AtomicBoolean();    p.traverseTopologically(new PipelineVisitor.Defaults() {        @Override        public void visitPrimitiveTransform(Node node) {            if (node.getTransform() instanceof WriteView) {                assertThat("There should only be one WriteView primitive in the graph", writeViewVisited.getAndSet(true), is(false));                PCollectionView<?> replacementView = ((WriteView) node.getTransform()).getView();                                                assertThat(replacementView.getTagInternal(), equalTo((TupleTag) view.getTagInternal()));                assertThat(replacementView.getViewFn(), equalTo(view.getViewFn()));                assertThat(replacementView.getWindowMappingFn(), equalTo(view.getWindowMappingFn()));                assertThat(node.getInputs().entrySet(), hasSize(1));            }        }    });    assertThat(writeViewVisited.get(), is(true));}
public void beam_f4755_0(Node node)
{    if (node.getTransform() instanceof WriteView) {        assertThat("There should only be one WriteView primitive in the graph", writeViewVisited.getAndSet(true), is(false));        PCollectionView<?> replacementView = ((WriteView) node.getTransform()).getView();                        assertThat(replacementView.getTagInternal(), equalTo((TupleTag) view.getTagInternal()));        assertThat(replacementView.getViewFn(), equalTo(view.getViewFn()));        assertThat(replacementView.getWindowMappingFn(), equalTo(view.getWindowMappingFn()));        assertThat(node.getInputs().entrySet(), hasSize(1));    }}
public void beam_f4764_0()
{    TransformWatermarks watermarks = manager.getWatermarks(graph.getProducer(createdInts));    assertThat(watermarks.getInputWatermark(), equalTo(BoundedWindow.TIMESTAMP_MIN_VALUE));    assertThat(watermarks.getOutputWatermark(), equalTo(BoundedWindow.TIMESTAMP_MIN_VALUE));}
public void beam_f4765_0()
{    CommittedBundle<Integer> output = multiWindowedBundle(createdInts, 1);    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.singleton(output), new Instant(8000L));    manager.refreshAll();    TransformWatermarks updatedSourceWatermark = manager.getWatermarks(graph.getProducer(createdInts));    assertThat(updatedSourceWatermark.getOutputWatermark(), equalTo(new Instant(8000L)));}
public void beam_f4774_0()
{    WindowedValue<Integer> first = WindowedValue.timestampedValueInGlobalWindow(1, new Instant(22));    CommittedBundle<Integer> createdBundle = bundleFactory.createBundle(createdInts).add(first).commit(clock.now());    WindowedValue<Integer> second = WindowedValue.timestampedValueInGlobalWindow(2, new Instant(22));    CommittedBundle<Integer> neverCreatedBundle = bundleFactory.createBundle(createdInts).add(second).commit(clock.now());    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.<CommittedBundle<?>>singleton(createdBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.updateWatermarks(neverCreatedBundle, TimerUpdate.empty(), graph.getProducer(filtered), neverCreatedBundle.withElements(Collections.emptyList()), Collections.emptyList(), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    TransformWatermarks filteredWms = manager.getWatermarks(graph.getProducer(filtered));    assertThat(filteredWms.getInputWatermark(), equalTo(new Instant(22L)));}
public void beam_f4775_0()
{    Instant sourceWatermark = new Instant(1_000_000L);    CommittedBundle<Integer> createdBundle = timestampedBundle(createdInts, TimestampedValue.of(1, sourceWatermark), TimestampedValue.of(2, new Instant(1234L)));    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.<CommittedBundle<?>>singleton(createdBundle), sourceWatermark);    CommittedBundle<KV<String, Integer>> keyBundle = timestampedBundle(keyed, TimestampedValue.of(KV.of("MyKey", 1), sourceWatermark), TimestampedValue.of(KV.of("MyKey", 2), new Instant(1234L)));        manager.updateWatermarks(createdBundle, TimerUpdate.empty(), graph.getProducer(keyed), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(keyBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();    TransformWatermarks onTimeWatermarks = manager.getWatermarks(graph.getProducer(keyed));    assertThat(onTimeWatermarks.getInputWatermark(), equalTo(sourceWatermark));    assertThat(onTimeWatermarks.getOutputWatermark(), equalTo(sourceWatermark));    CommittedBundle<Integer> lateDataBundle = timestampedBundle(createdInts, TimestampedValue.of(3, new Instant(-1000L)));            manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(lateDataBundle), new Instant(2_000_000L));    manager.refreshAll();    TransformWatermarks bufferedLateWm = manager.getWatermarks(graph.getProducer(createdInts));    assertThat(bufferedLateWm.getOutputWatermark(), equalTo(new Instant(2_000_000L)));            TransformWatermarks lateDataBufferedWatermark = manager.getWatermarks(graph.getProducer(keyed));    assertThat(lateDataBufferedWatermark.getInputWatermark(), not(lessThan(sourceWatermark)));    assertThat(lateDataBufferedWatermark.getOutputWatermark(), not(lessThan(sourceWatermark)));    CommittedBundle<KV<String, Integer>> lateKeyedBundle = timestampedBundle(keyed, TimestampedValue.of(KV.of("MyKey", 3), new Instant(-1000L)));    manager.updateWatermarks(lateDataBundle, TimerUpdate.empty(), graph.getProducer(keyed), lateDataBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(lateKeyedBundle), BoundedWindow.TIMESTAMP_MAX_VALUE);    manager.refreshAll();}
public void beam_f4784_0()
{    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> initialTimers = manager.extractFiredTimers();        assertThat(initialTimers, emptyIterable());        CommittedBundle<Integer> createdBundle = multiWindowedBundle(filtered);    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.singleton(createdBundle), new Instant(1500L));    manager.refreshAll();    TimerData earliestTimer = TimerData.of(StateNamespaces.global(), new Instant(1000), TimeDomain.EVENT_TIME);    TimerData middleTimer = TimerData.of(StateNamespaces.global(), new Instant(5000L), TimeDomain.EVENT_TIME);    TimerData lastTimer = TimerData.of(StateNamespaces.global(), new Instant(10000L), TimeDomain.EVENT_TIME);    StructuralKey<byte[]> key = StructuralKey.of(new byte[] { 1, 4, 9 }, ByteArrayCoder.of());    TimerUpdate update = TimerUpdate.builder(key).setTimer(earliestTimer).setTimer(middleTimer).setTimer(lastTimer).build();    manager.updateWatermarks(createdBundle, update, graph.getProducer(filtered), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(multiWindowedBundle(intsToFlatten)), new Instant(1000L));    manager.refreshAll();    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> firstFiredTimers = manager.extractFiredTimers();    assertThat(firstFiredTimers, not(emptyIterable()));    FiredTimers<AppliedPTransform<?, ?, ?>> firstFired = Iterables.getOnlyElement(firstFiredTimers);    assertThat(firstFired.getTimers(), contains(earliestTimer));    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.emptyList(), new Instant(50_000L));    manager.refreshAll();    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> secondFiredTimers = manager.extractFiredTimers();    assertThat(secondFiredTimers, not(emptyIterable()));    FiredTimers<AppliedPTransform<?, ?, ?>> secondFired = Iterables.getOnlyElement(secondFiredTimers);        assertThat(secondFired.getTimers(), contains(middleTimer, lastTimer));}
public void beam_f4785_0()
{    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> initialTimers = manager.extractFiredTimers();        assertThat(initialTimers, emptyIterable());        CommittedBundle<Integer> createdBundle = multiWindowedBundle(filtered);    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.singleton(createdBundle), new Instant(1500L));    TimerData earliestTimer = TimerData.of(StateNamespaces.global(), new Instant(999L), TimeDomain.PROCESSING_TIME);    TimerData middleTimer = TimerData.of(StateNamespaces.global(), new Instant(5000L), TimeDomain.PROCESSING_TIME);    TimerData lastTimer = TimerData.of(StateNamespaces.global(), new Instant(10000L), TimeDomain.PROCESSING_TIME);    StructuralKey<?> key = StructuralKey.of(-12L, VarLongCoder.of());    TimerUpdate update = TimerUpdate.builder(key).setTimer(lastTimer).setTimer(earliestTimer).setTimer(middleTimer).build();    manager.updateWatermarks(createdBundle, update, graph.getProducer(filtered), createdBundle.withElements(Collections.emptyList()), Collections.<CommittedBundle<?>>singleton(multiWindowedBundle(intsToFlatten)), new Instant(1000L));    manager.refreshAll();    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> firstFiredTimers = manager.extractFiredTimers();    assertThat(firstFiredTimers, not(emptyIterable()));    FiredTimers<AppliedPTransform<?, ?, ?>> firstFired = Iterables.getOnlyElement(firstFiredTimers);    assertThat(firstFired.getTimers(), contains(earliestTimer));    clock.set(new Instant(50_000L));    manager.updateWatermarks(null, TimerUpdate.empty(), graph.getProducer(createdInts), null, Collections.emptyList(), new Instant(50_000L));    manager.refreshAll();    Collection<FiredTimers<AppliedPTransform<?, ?, ?>>> secondFiredTimers = manager.extractFiredTimers();    assertThat(secondFiredTimers, not(emptyIterable()));    FiredTimers<AppliedPTransform<?, ?, ?>> secondFired = Iterables.getOnlyElement(secondFiredTimers);        assertThat(secondFired.getTimers(), contains(middleTimer, lastTimer));}
public void beam_f4794_0()
{    TimerUpdateBuilder builder = TimerUpdate.builder(null);    TimerData timer = TimerData.of(StateNamespaces.global(), Instant.now(), TimeDomain.EVENT_TIME);    TimerUpdate built = builder.deletedTimer(timer).setTimer(timer).build();    assertThat(built.getSetTimers(), contains(timer));    assertThat(built.getDeletedTimers(), emptyIterable());}
public void beam_f4795_0()
{    TimerUpdateBuilder builder = TimerUpdate.builder(null);    TimerData timer = TimerData.of(StateNamespaces.global(), Instant.now(), TimeDomain.EVENT_TIME);    TimerUpdate built = builder.build();    builder.setTimer(timer);    assertThat(built.getSetTimers(), emptyIterable());    builder.build();    assertThat(built.getSetTimers(), emptyIterable());}
public void beam_f4804_0() throws Exception
{    Window<Long> transform = Window.into(new EvaluatorTestWindowFn());    PCollection<Long> windowed = input.apply(transform);    CommittedBundle<Long> inputBundle = createInputBundle();    UncommittedBundle<Long> outputBundle = createOutputBundle(windowed, inputBundle);    TransformResult<Long> result = runEvaluator(windowed, inputBundle);    assertThat(Iterables.getOnlyElement(result.getOutputBundles()), Matchers.equalTo(outputBundle));    CommittedBundle<Long> committed = outputBundle.commit(Instant.now());    assertThat(committed.getElements(), containsInAnyOrder(    isSingleWindowedValue(valueInGlobalWindow.getValue(), valueInGlobalWindow.getTimestamp(), new IntervalWindow(valueInGlobalWindow.getTimestamp(), valueInGlobalWindow.getTimestamp().plus(1L)), valueInGlobalWindow.getPane()),     isWindowedValue(valueInIntervalWindow.getValue(), valueInIntervalWindow.getTimestamp(), valueInIntervalWindow.getWindows(), valueInIntervalWindow.getPane()),     isSingleWindowedValue(valueInGlobalAndTwoIntervalWindows.getValue(), valueInGlobalAndTwoIntervalWindows.getTimestamp(), new IntervalWindow(valueInGlobalAndTwoIntervalWindows.getTimestamp(), valueInGlobalAndTwoIntervalWindows.getTimestamp().plus(1L)), valueInGlobalAndTwoIntervalWindows.getPane()), isSingleWindowedValue(valueInGlobalAndTwoIntervalWindows.getValue(), valueInGlobalAndTwoIntervalWindows.getTimestamp(), intervalWindow1, valueInGlobalAndTwoIntervalWindows.getPane()), isSingleWindowedValue(valueInGlobalAndTwoIntervalWindows.getValue(), valueInGlobalAndTwoIntervalWindows.getTimestamp(), intervalWindow2, valueInGlobalAndTwoIntervalWindows.getPane())));}
private CommittedBundle<Long> beam_f4805_0()
{    CommittedBundle<Long> inputBundle = bundleFactory.createBundle(input).add(valueInGlobalWindow).add(valueInGlobalAndTwoIntervalWindows).add(valueInIntervalWindow).commit(Instant.now());    return inputBundle;}
public void beam_f4814_0()
{    ResourceId outputDirectory = LocalResources.fromString("/foo", true);    PTransform<PCollection<Object>, WriteFilesResult<Void>> original = WriteFiles.to(new FileBasedSink<Object, Void, Object>(StaticValueProvider.of(outputDirectory), DynamicFileDestinations.constant(new FakeFilenamePolicy())) {        @Override        public WriteOperation<Void, Object> createWriteOperation() {            throw new IllegalArgumentException("Should not be used");        }    });    @SuppressWarnings("unchecked")    PCollection<Object> objs = (PCollection) p.apply(Create.empty(VoidCoder.of()));    AppliedPTransform<PCollection<Object>, WriteFilesResult<Void>, PTransform<PCollection<Object>, WriteFilesResult<Void>>> originalApplication = AppliedPTransform.of("write", objs.expand(), Collections.emptyMap(), original, p);    assertThat(factory.getReplacementTransform(originalApplication).getTransform(), not(equalTo((Object) original)));}
public WriteOperation<Void, Object> beam_f4815_0()
{    throw new IllegalArgumentException("Should not be used");}
public ResourceId beam_f4824_0(int shardNumber, int numShards, FileBasedSink.OutputFileHints outputFileHints)
{    throw new IllegalArgumentException("Should not be used");}
public void beam_f4825_0(MetricQueryResults metricQueryResults) throws Exception
{    final long metricTimestamp = System.currentTimeMillis() / 1000L;    Socket socket = new Socket(InetAddress.getByName(address), port);    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(socket.getOutputStream(), charset));    StringBuilder messagePayload = new StringBuilder();    Iterable<MetricResult<Long>> counters = metricQueryResults.getCounters();    Iterable<MetricResult<GaugeResult>> gauges = metricQueryResults.getGauges();    Iterable<MetricResult<DistributionResult>> distributions = metricQueryResults.getDistributions();    for (MetricResult<Long> counter : counters) {        messagePayload.append(new CounterMetricMessage(counter, "value", metricTimestamp).toString());    }    for (MetricResult<GaugeResult> gauge : gauges) {        messagePayload.append(new GaugeMetricMessage(gauge, "value").toString());    }    for (MetricResult<DistributionResult> distribution : distributions) {        messagePayload.append(new DistributionMetricMessage(distribution, "min", metricTimestamp).toString());        messagePayload.append(new DistributionMetricMessage(distribution, "max", metricTimestamp).toString());        messagePayload.append(new DistributionMetricMessage(distribution, "count", metricTimestamp).toString());        messagePayload.append(new DistributionMetricMessage(distribution, "sum", metricTimestamp).toString());        messagePayload.append(new DistributionMetricMessage(distribution, "mean", metricTimestamp).toString());    }    writer.write(messagePayload.toString());    writer.flush();    writer.close();    socket.close();}
public String beam_f4834_0()
{    return committedOrAttempted;}
public void beam_f4835_0(MetricQueryResults metricQueryResults) throws Exception
{    URL url = new URL(urlString);    String metrics = serializeMetrics(metricQueryResults);    byte[] postData = metrics.getBytes(StandardCharsets.UTF_8);    HttpURLConnection connection = (HttpURLConnection) url.openConnection();    connection.setDoOutput(true);    connection.setInstanceFollowRedirects(false);    connection.setRequestMethod("POST");    connection.setRequestProperty("Content-Type", "application/json");    connection.setRequestProperty("charset", "utf-8");    connection.setRequestProperty("Content-Length", Integer.toString(postData.length));    connection.setUseCaches(false);    try (DataOutputStream connectionOuputStream = new DataOutputStream(connection.getOutputStream())) {        connectionOuputStream.write(postData);    }    int responseCode = connection.getResponseCode();    if (responseCode != 200) {        throw new HTTPException(responseCode);    }}
public List<MetricResult<GaugeResult>> beam_f4844_0()
{    return makeResults("s3", "n3", GaugeResult.create(100L, new Instant(345862800L)), GaugeResult.create(120L, new Instant(345862800L)));}
public static void beam_f4845_0() throws IOException, InterruptedException
{        ServerSocket serverSocket = new ServerSocket(0);    port = serverSocket.getLocalPort();    serverSocket.close();    graphiteServer = new NetworkMockServer(port);    graphiteServer.clear();    graphiteServer.start();}
public static void beam_f4854_0()
{    httpServer.stop(0);}
public void beam_f4855_0(CountDownLatch countDownLatch)
{    this.countDownLatch = countDownLatch;}
public T beam_f4864_0()
{    return null;}
public T beam_f4865_0(T t)
{    try {        return CoderUtils.clone(coder, t);    } catch (CoderException e) {        throw new RuntimeException("Could not clone.", e);    }}
public int beam_f4874_0()
{    return coder.hashCode();}
public TypeSerializerConfigSnapshot beam_f4875_0()
{    return new CoderTypeSerializerConfigSnapshot<>(coder);}
public byte[] beam_f4884_0()
{    return EMPTY;}
public byte[] beam_f4885_0(byte[] from)
{    return from;}
protected StateInternals beam_f4905_0()
{    try {        KeyedStateBackend<ByteBuffer> keyedStateBackend = createStateBackend();        return new FlinkStateInternals<>(keyedStateBackend, StringUtf8Coder.of());    } catch (Exception e) {        throw new RuntimeException(e);    }}
public void beam_f4906_0() throws Exception
{    KeyedStateBackend<ByteBuffer> keyedStateBackend = createStateBackend();    FlinkStateInternals stateInternals = new FlinkStateInternals<>(keyedStateBackend, StringUtf8Coder.of());    StateTag<WatermarkHoldState> stateTag = StateTags.watermarkStateInternal("hold", TimestampCombiner.EARLIEST);    WatermarkHoldState globalWindow = stateInternals.state(StateNamespaces.global(), stateTag);    WatermarkHoldState fixedWindow = stateInternals.state(StateNamespaces.window(IntervalWindow.getCoder(), new IntervalWindow(new Instant(0), new Instant(10))), stateTag);    Instant noHold = new Instant(Long.MAX_VALUE);    assertThat(stateInternals.watermarkHold(), is(noHold));    Instant high = new Instant(10);    globalWindow.add(high);    assertThat(stateInternals.watermarkHold(), is(high));    Instant middle = new Instant(5);    fixedWindow.add(middle);    assertThat(stateInternals.watermarkHold(), is(middle));    Instant low = new Instant(1);    globalWindow.add(low);    assertThat(stateInternals.watermarkHold(), is(low));        globalWindow.add(high);    assertThat(stateInternals.watermarkHold(), is(low));    fixedWindow.add(high);    assertThat(stateInternals.watermarkHold(), is(low));    changeKey(keyedStateBackend);        stateInternals = new FlinkStateInternals<>(keyedStateBackend, StringUtf8Coder.of());    globalWindow = stateInternals.state(StateNamespaces.global(), stateTag);    fixedWindow = stateInternals.state(StateNamespaces.window(IntervalWindow.getCoder(), new IntervalWindow(new Instant(0), new Instant(10))), stateTag);    assertThat(stateInternals.watermarkHold(), is(low));    fixedWindow.clear();    assertThat(stateInternals.watermarkHold(), is(low));    globalWindow.clear();    assertThat(stateInternals.watermarkHold(), is(noHold));}
public T beam_f4916_0()
{    return null;}
public T beam_f4917_0(T t)
{    try {        return CoderUtils.clone(coder, t);    } catch (CoderException e) {        throw new RuntimeException("Could not clone.", e);    }}
public TypeSerializerSnapshot<T> beam_f4926_0()
{    return new LegacySnapshot<>(this);}
public int beam_f4927_0()
{        return 1;}
public byte[] beam_f4936_0(DataInputView source) throws IOException
{    final int len = source.readInt();    byte[] result = new byte[len];    source.readFully(result);    return result;}
public byte[] beam_f4937_0(byte[] reuse, DataInputView source) throws IOException
{    return deserialize(source);}
public static KeyedStateBackend<ByteBuffer> beam_f4959_0() throws Exception
{    MemoryStateBackend backend = new MemoryStateBackend();    AbstractKeyedStateBackend<ByteBuffer> keyedStateBackend = backend.createKeyedStateBackend(new DummyEnvironment("test", 1, 0), new JobID(), "test_op", new GenericTypeInfo<>(ByteBuffer.class).createSerializer(new ExecutionConfig()), 2, new KeyGroupRange(0, 1), new KvStateRegistry().createTaskRegistry(new JobID(), new JobVertexID()), TtlTimeProvider.DEFAULT, null, Collections.emptyList(), new CloseableRegistry());    changeKey(keyedStateBackend);    return keyedStateBackend;}
public static void beam_f4960_0(KeyedStateBackend<ByteBuffer> keyedStateBackend) throws CoderException
{    keyedStateBackend.setCurrentKey(ByteBuffer.wrap(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), UUID.randomUUID().toString())));}
public List<T> beam_f4970_0(List<T> accumulator)
{    return accumulator;}
public Coder<List<T>> beam_f4971_0(CoderRegistry registry, Coder<T> inputCoder)
{    return ListCoder.of(inputCoder);}
public void beam_f4980_1(TransformHierarchy.Node node)
{    this.depth--;    }
public void beam_f4981_1(TransformHierarchy.Node node)
{                PTransform<?, ?> transform = node.getTransform();    BatchTransformTranslator<?> translator = FlinkBatchTransformTranslators.getTranslator(transform);    if (translator == null) {        String transformUrn = PTransformTranslation.urnForTransform(transform);        throw new UnsupportedOperationException("The transform " + transformUrn + " is currently not supported.");    }    applyBatchTransform(transform, node, translator);}
public void beam_f4990_0(String pCollectionId, DataSet<T> dataSet)
{    checkArgument(!dataSets.containsKey(pCollectionId));    dataSets.put(pCollectionId, dataSet);    danglingDataSets.add(pCollectionId);}
public DataSet<T> beam_f4991_0(String pCollectionId)
{    DataSet<T> dataSet = (DataSet<T>) dataSets.get(pCollectionId);    if (dataSet == null) {        throw new IllegalArgumentException(String.format("Unknown dataset for id %s.", pCollectionId));    }            danglingDataSets.remove(pCollectionId);    return dataSet;}
private static void beam_f5000_0(PTransformNode transform, RunnerApi.Pipeline pipeline, BatchTranslationContext context)
{    TypeInformation<WindowedValue<byte[]>> typeInformation = new CoderTypeInformation<>(WindowedValue.getFullCoder(ByteArrayCoder.of(), GlobalWindow.Coder.INSTANCE));    DataSource<WindowedValue<byte[]>> dataSource = new DataSource<>(context.getExecutionEnvironment(), new ImpulseInputFormat(), typeInformation, transform.getTransform().getUniqueName()).name("Impulse");    context.addDataSet(Iterables.getOnlyElement(transform.getTransform().getOutputsMap().values()), dataSource);}
public List<T> beam_f5001_0()
{    return new ArrayList<>();}
private static String beam_f5010_0(FlinkBatchTranslationContext context)
{    return context.getCurrentTransform().getFullName();}
public void beam_f5011_0(PTransform<PBegin, PCollection<byte[]>> transform, FlinkBatchTranslationContext context)
{    String name = transform.getName();    PCollection<byte[]> output = context.getOutput(transform);    TypeInformation<WindowedValue<byte[]>> typeInformation = context.getTypeInfo(output);    DataSource<WindowedValue<byte[]>> dataSource = new DataSource<>(context.getExecutionEnvironment(), new ImpulseInputFormat(), typeInformation, name);    context.setOutputDataSet(output, dataSource);}
public Coder<List<T>> beam_f5020_0(CoderRegistry registry, Coder<T> inputCoder)
{    return ListCoder.of(inputCoder);}
public Coder<List<T>> beam_f5021_0(CoderRegistry registry, Coder<T> inputCoder)
{    return ListCoder.of(inputCoder);}
public PipelineOptions beam_f5030_0()
{    return options;}
public DataSet<WindowedValue<T>> beam_f5031_0(PValue value)
{        danglingDataSets.remove(value);    return (DataSet<WindowedValue<T>>) dataSets.get(value);}
 Map<TupleTag<?>, PValue> beam_f5040_0(PTransform<?, ?> transform)
{    return currentTransform.getInputs();}
 T beam_f5041_0(PTransform<T, ?> transform)
{    return (T) Iterables.getOnlyElement(TransformInputs.nonAdditionalInputs(currentTransform));}
public static ExecutionEnvironment beam_f5050_0(FlinkPipelineOptions options, List<String> filesToStage)
{    return createBatchExecutionEnvironment(options, filesToStage, null);}
 static ExecutionEnvironment beam_f5051_1(FlinkPipelineOptions options, List<String> filesToStage, @Nullable String confDir)
{        String masterUrl = options.getFlinkMaster();    Configuration flinkConfiguration = getFlinkConfiguration(confDir);    ExecutionEnvironment flinkBatchEnv;        if ("[local]".equals(masterUrl)) {        flinkBatchEnv = ExecutionEnvironment.createLocalEnvironment(flinkConfiguration);    } else if ("[collection]".equals(masterUrl)) {        flinkBatchEnv = new CollectionEnvironment();    } else if ("[auto]".equals(masterUrl)) {        flinkBatchEnv = ExecutionEnvironment.getExecutionEnvironment();    } else {        int defaultPort = flinkConfiguration.getInteger(RestOptions.PORT);        HostAndPort hostAndPort = HostAndPort.fromString(masterUrl).withDefaultPort(defaultPort);        flinkConfiguration.setInteger(RestOptions.PORT, hostAndPort.getPort());        flinkBatchEnv = ExecutionEnvironment.createRemoteEnvironment(hostAndPort.getHost(), hostAndPort.getPort(), flinkConfiguration, filesToStage.toArray(new String[filesToStage.size()]));            }        flinkBatchEnv.getConfig().setExecutionMode(ExecutionMode.valueOf(options.getExecutionModeForBatch()));        if (options.getParallelism() != -1 && !(flinkBatchEnv instanceof CollectionEnvironment)) {        flinkBatchEnv.setParallelism(options.getParallelism());    }            final int parallelism;    if (flinkBatchEnv instanceof CollectionEnvironment) {        parallelism = 1;    } else {        parallelism = determineParallelism(options.getParallelism(), flinkBatchEnv.getParallelism(), flinkConfiguration);    }    flinkBatchEnv.setParallelism(parallelism);        options.setParallelism(parallelism);    if (options.getObjectReuse()) {        flinkBatchEnv.getConfig().enableObjectReuse();    } else {        flinkBatchEnv.getConfig().disableObjectReuse();    }    applyLatencyTrackingInterval(flinkBatchEnv.getConfig(), options);    return flinkBatchEnv;}
 static JobInvocation beam_f5060_0(String invocationId, String retrievalToken, ListeningExecutorService executorService, RunnerApi.Pipeline pipeline, FlinkPipelineOptions flinkOptions, PortablePipelineRunner pipelineRunner)
{    JobInfo jobInfo = JobInfo.create(invocationId, flinkOptions.getJobName(), retrievalToken, PipelineOptionsTranslation.toProto(flinkOptions));    return new JobInvocation(jobInfo, executorService, pipeline, pipelineRunner);}
 String beam_f5061_0()
{    return this.flinkMasterUrl;}
private static void beam_f5070_0(FlinkPipelineOptions options)
{    if (!options.getFlinkMaster().matches("\\[auto\\]|\\[collection\\]|\\[local\\]")) {        options.setFilesToStage(PipelineResources.prepareFilesForStaging(options.getFilesToStage(), MoreObjects.firstNonNull(options.getTempLocation(), System.getProperty("java.io.tmpdir"))));    }}
public JobExecutionResult beam_f5071_0() throws Exception
{    final String jobName = options.getJobName();    if (flinkBatchEnv != null) {        return flinkBatchEnv.execute(jobName);    } else if (flinkStreamEnv != null) {        return flinkStreamEnv.execute(jobName);    } else {        throw new IllegalStateException("The Pipeline has not yet been translated.");    }}
public void beam_f5080_0(Pipeline pipeline)
{    pipeline.traverseTopologically(this);}
protected static String beam_f5081_0(int n)
{    StringBuilder builder = new StringBuilder();    for (int i = 0; i < n; i++) {        builder.append("|   ");    }    return builder.toString();}
private void beam_f5090_1(Pipeline pipeline)
{        if (!ptransformViewsWithNonDeterministicKeyCoders.isEmpty()) {        final SortedSet<String> ptransformViewNamesWithNonDeterministicKeyCoders = new TreeSet<>();        pipeline.traverseTopologically(new Pipeline.PipelineVisitor.Defaults() {            @Override            public void visitPrimitiveTransform(TransformHierarchy.Node node) {                if (ptransformViewsWithNonDeterministicKeyCoders.contains(node.getTransform())) {                    ptransformViewNamesWithNonDeterministicKeyCoders.add(node.getFullName());                }            }            @Override            public CompositeBehavior enterCompositeTransform(TransformHierarchy.Node node) {                if (ptransformViewsWithNonDeterministicKeyCoders.contains(node.getTransform())) {                    ptransformViewNamesWithNonDeterministicKeyCoders.add(node.getFullName());                }                return CompositeBehavior.ENTER_TRANSFORM;            }        });            }}
public void beam_f5091_0(TransformHierarchy.Node node)
{    if (ptransformViewsWithNonDeterministicKeyCoders.contains(node.getTransform())) {        ptransformViewNamesWithNonDeterministicKeyCoders.add(node.getFullName());    }}
public State beam_f5100_0(Duration duration)
{    return State.DONE;}
public MetricResults beam_f5101_0()
{    return asAttemptedOnlyMetricResults(getMetricsContainerStepMap());}
 static PTransformMatcher beam_f5111_0()
{    return application -> {        if (WRITE_FILES_TRANSFORM_URN.equals(PTransformTranslation.urnForTransformOrNull(application.getTransform()))) {            try {                FlinkPipelineOptions options = application.getPipeline().getOptions().as(FlinkPipelineOptions.class);                ShardingFunction shardingFn = ((WriteFiles<?, ?, ?>) application.getTransform()).getShardingFunction();                return WriteFilesTranslation.isRunnerDeterminedSharding((AppliedPTransform) application) || (options.isAutoBalanceWriteFilesShardingEnabled() && shardingFn == null);            } catch (IOException exc) {                throw new RuntimeException(String.format("Transform with URN %s failed to parse: %s", WRITE_FILES_TRANSFORM_URN, application.getTransform()), exc);            }        }        return false;    };}
public PTransformReplacement<PCollection<UserT>, WriteFilesResult<DestinationT>> beam_f5112_0(AppliedPTransform<PCollection<UserT>, WriteFilesResult<DestinationT>, WriteFiles<UserT, DestinationT, OutputT>> transform)
{                Integer jobParallelism = options.getParallelism();    Preconditions.checkArgument(jobParallelism > 0, "Parallelism of a job should be greater than 0. Currently set: %s", jobParallelism);    int numShards = jobParallelism * 2;    try {        List<PCollectionView<?>> sideInputs = WriteFilesTranslation.getDynamicDestinationSideInputs(transform);        FileBasedSink sink = WriteFilesTranslation.getSink(transform);        @SuppressWarnings("unchecked")        WriteFiles<UserT, DestinationT, OutputT> replacement = WriteFiles.to(sink).withSideInputs(sideInputs);        if (WriteFilesTranslation.isWindowedWrites(transform)) {            replacement = replacement.withWindowedWrites();        }        if (WriteFilesTranslation.isRunnerDeterminedSharding(transform)) {            replacement = replacement.withNumShards(numShards);        } else {            if (transform.getTransform().getNumShardsProvider() != null) {                replacement = replacement.withNumShards(transform.getTransform().getNumShardsProvider());            }            if (transform.getTransform().getComputeNumShards() != null) {                replacement = replacement.withSharding(transform.getTransform().getComputeNumShards());            }        }        if (options.isAutoBalanceWriteFilesShardingEnabled()) {            replacement = replacement.withShardingFunction(new FlinkAutoBalancedShardKeyShardingFunction<>(jobParallelism, options.getMaxParallelism(), sink.getDynamicDestinations().getDestinationCoder()));        }        return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), replacement);    } catch (Exception e) {        throw new RuntimeException(e);    }}
public JobExecutionResult beam_f5121_0(String jobName) throws Exception
{    return getExecutionEnvironment().execute(jobName);}
public StreamExecutionEnvironment beam_f5122_0()
{    return executionEnvironment;}
private void beam_f5131_0(String id, RunnerApi.Pipeline pipeline, StreamingTranslationContext context)
{    RunnerApi.PTransform pTransform = pipeline.getComponents().getTransformsOrThrow(id);    String inputPCollectionId = Iterables.getOnlyElement(pTransform.getInputsMap().values());    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(pipeline.getComponents());    RunnerApi.WindowingStrategy windowingStrategyProto = pipeline.getComponents().getWindowingStrategiesOrThrow(pipeline.getComponents().getPcollectionsOrThrow(inputPCollectionId).getWindowingStrategyId());    WindowingStrategy<?, ?> windowingStrategy;    try {        windowingStrategy = WindowingStrategyTranslation.fromProto(windowingStrategyProto, rehydratedComponents);    } catch (InvalidProtocolBufferException e) {        throw new IllegalStateException(String.format("Unable to hydrate GroupByKey windowing strategy %s.", windowingStrategyProto), e);    }    WindowedValueCoder<KV<K, V>> windowedInputCoder = (WindowedValueCoder) instantiateCoder(inputPCollectionId, pipeline.getComponents());    DataStream<WindowedValue<KV<K, V>>> inputDataStream = context.getDataStreamOrThrow(inputPCollectionId);    SingleOutputStreamOperator<WindowedValue<KV<K, Iterable<V>>>> outputDataStream = addGBK(inputDataStream, windowingStrategy, windowedInputCoder, pTransform.getUniqueName(), context);        outputDataStream.uid(pTransform.getUniqueName());    context.addDataStream(Iterables.getOnlyElement(pTransform.getOutputsMap().values()), outputDataStream);}
private SingleOutputStreamOperator<WindowedValue<KV<K, Iterable<V>>>> beam_f5132_0(DataStream<WindowedValue<KV<K, V>>> inputDataStream, WindowingStrategy<?, ?> windowingStrategy, WindowedValueCoder<KV<K, V>> windowedInputCoder, String operatorName, StreamingTranslationContext context)
{    KvCoder<K, V> inputElementCoder = (KvCoder<K, V>) windowedInputCoder.getValueCoder();    SingletonKeyedWorkItemCoder<K, V> workItemCoder = SingletonKeyedWorkItemCoder.of(inputElementCoder.getKeyCoder(), inputElementCoder.getValueCoder(), windowingStrategy.getWindowFn().windowCoder());    WindowedValue.FullWindowedValueCoder<SingletonKeyedWorkItem<K, V>> windowedWorkItemCoder = WindowedValue.getFullCoder(workItemCoder, windowingStrategy.getWindowFn().windowCoder());    CoderTypeInformation<WindowedValue<SingletonKeyedWorkItem<K, V>>> workItemTypeInfo = new CoderTypeInformation<>(windowedWorkItemCoder);    DataStream<WindowedValue<SingletonKeyedWorkItem<K, V>>> workItemStream = inputDataStream.flatMap(new FlinkStreamingTransformTranslators.ToKeyedWorkItem<>()).returns(workItemTypeInfo).name("ToKeyedWorkItem");    WorkItemKeySelector<K, V> keySelector = new WorkItemKeySelector<>(inputElementCoder.getKeyCoder());    KeyedStream<WindowedValue<SingletonKeyedWorkItem<K, V>>, ByteBuffer> keyedWorkItemStream = workItemStream.keyBy(keySelector);    SystemReduceFn<K, V, Iterable<V>, Iterable<V>, BoundedWindow> reduceFn = SystemReduceFn.buffering(inputElementCoder.getValueCoder());    Coder<Iterable<V>> accumulatorCoder = IterableCoder.of(inputElementCoder.getValueCoder());    Coder<WindowedValue<KV<K, Iterable<V>>>> outputCoder = WindowedValue.getFullCoder(KvCoder.of(inputElementCoder.getKeyCoder(), accumulatorCoder), windowingStrategy.getWindowFn().windowCoder());    TypeInformation<WindowedValue<KV<K, Iterable<V>>>> outputTypeInfo = new CoderTypeInformation<>(outputCoder);    TupleTag<KV<K, Iterable<V>>> mainTag = new TupleTag<>("main output");    WindowDoFnOperator<K, V, Iterable<V>> doFnOperator = new WindowDoFnOperator<>(reduceFn, operatorName, (Coder) windowedWorkItemCoder, mainTag, Collections.emptyList(), new DoFnOperator.MultiOutputOutputManagerFactory(mainTag, outputCoder), windowingStrategy, new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    context.getPipelineOptions(), inputElementCoder.getKeyCoder(), (KeySelector) keySelector);    SingleOutputStreamOperator<WindowedValue<KV<K, Iterable<V>>>> outputDataStream = keyedWorkItemStream.transform(operatorName, outputTypeInfo, (OneInputStreamOperator) doFnOperator);    return outputDataStream;}
private TransformedSideInputs beam_f5141_0(RunnerApi.ExecutableStagePayload stagePayload, RunnerApi.Components components, StreamingTranslationContext context)
{    LinkedHashMap<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>> sideInputs = getSideInputIdToPCollectionViewMap(stagePayload, components);    Map<TupleTag<?>, Integer> tagToIntMapping = new HashMap<>();    Map<Integer, PCollectionView<?>> intToViewMapping = new HashMap<>();    List<WindowedValueCoder<KV<Void, Object>>> kvCoders = new ArrayList<>();    List<Coder<?>> viewCoders = new ArrayList<>();    int count = 0;    for (Map.Entry<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>> sideInput : sideInputs.entrySet()) {        TupleTag<?> tag = sideInput.getValue().getTagInternal();        intToViewMapping.put(count, sideInput.getValue());        tagToIntMapping.put(tag, count);        count++;        String collectionId = components.getTransformsOrThrow(sideInput.getKey().getTransformId()).getInputsOrThrow(sideInput.getKey().getLocalName());        DataStream<Object> sideInputStream = context.getDataStreamOrThrow(collectionId);        TypeInformation<Object> tpe = sideInputStream.getType();        if (!(tpe instanceof CoderTypeInformation)) {            throw new IllegalStateException("Input Stream TypeInformation is no CoderTypeInformation.");        }        WindowedValueCoder<Object> coder = (WindowedValueCoder) ((CoderTypeInformation) tpe).getCoder();        Coder<KV<Void, Object>> kvCoder = KvCoder.of(VoidCoder.of(), coder.getValueCoder());        kvCoders.add(coder.withValueCoder(kvCoder));                WindowedValueCoder<KV<Void, Iterable<Object>>> viewCoder = coder.withValueCoder(KvCoder.of(VoidCoder.of(), IterableCoder.of(coder.getValueCoder())));        viewCoders.add(viewCoder);    }        UnionCoder unionCoder = UnionCoder.of(viewCoders);    CoderTypeInformation<RawUnionValue> unionTypeInformation = new CoderTypeInformation<>(unionCoder);        DataStream<RawUnionValue> sideInputUnion = null;    for (Map.Entry<RunnerApi.ExecutableStagePayload.SideInputId, PCollectionView<?>> sideInput : sideInputs.entrySet()) {        TupleTag<?> tag = sideInput.getValue().getTagInternal();        final int intTag = tagToIntMapping.get(tag);        RunnerApi.PTransform pTransform = components.getTransformsOrThrow(sideInput.getKey().getTransformId());        String collectionId = pTransform.getInputsOrThrow(sideInput.getKey().getLocalName());        DataStream<WindowedValue<?>> sideInputStream = context.getDataStreamOrThrow(collectionId);                String viewName = sideInput.getKey().getTransformId() + "-" + sideInput.getKey().getLocalName();        WindowedValueCoder<KV<Void, Object>> kvCoder = kvCoders.get(intTag);        DataStream<WindowedValue<KV<Void, Object>>> keyedSideInputStream = sideInputStream.map(new ToVoidKeyValue());        SingleOutputStreamOperator<WindowedValue<KV<Void, Iterable<Object>>>> viewStream = addGBK(keyedSideInputStream, sideInput.getValue().getWindowingStrategyInternal(), kvCoder, viewName, context);                viewStream.uid(pTransform.getUniqueName() + "-" + sideInput.getKey().getLocalName());        DataStream<RawUnionValue> unionValueStream = viewStream.map(new FlinkStreamingTransformTranslators.ToRawUnion<>(intTag)).returns(unionTypeInformation);        if (sideInputUnion == null) {            sideInputUnion = unionValueStream;        } else {            sideInputUnion = sideInputUnion.union(unionValueStream);        }    }    return new TransformedSideInputs(intToViewMapping, sideInputUnion);}
public WindowedValue<KV<Void, T>> beam_f5142_0(WindowedValue<T> value)
{    return value.withValue(KV.of(null, value.getValue()));}
public RawUnionValue beam_f5151_0(T o) throws Exception
{    return new RawUnionValue(intTag, o);}
private static Tuple2<Map<Integer, PCollectionView<?>>, DataStream<RawUnionValue>> beam_f5152_0(Collection<PCollectionView<?>> sideInputs, FlinkStreamingTranslationContext context)
{        Map<TupleTag<?>, Integer> tagToIntMapping = new HashMap<>();    Map<Integer, PCollectionView<?>> intToViewMapping = new HashMap<>();    int count = 0;    for (PCollectionView<?> sideInput : sideInputs) {        TupleTag<?> tag = sideInput.getTagInternal();        intToViewMapping.put(count, sideInput);        tagToIntMapping.put(tag, count);        count++;    }    List<Coder<?>> inputCoders = new ArrayList<>();    for (PCollectionView<?> sideInput : sideInputs) {        DataStream<Object> sideInputStream = context.getInputDataStream(sideInput);        TypeInformation<Object> tpe = sideInputStream.getType();        if (!(tpe instanceof CoderTypeInformation)) {            throw new IllegalStateException("Input Stream TypeInformation is no CoderTypeInformation.");        }        Coder<?> coder = ((CoderTypeInformation) tpe).getCoder();        inputCoders.add(coder);    }    UnionCoder unionCoder = UnionCoder.of(inputCoders);    CoderTypeInformation<RawUnionValue> unionTypeInformation = new CoderTypeInformation<>(unionCoder);        DataStream<RawUnionValue> sideInputUnion = null;    for (PCollectionView<?> sideInput : sideInputs) {        TupleTag<?> tag = sideInput.getTagInternal();        final int intTag = tagToIntMapping.get(tag);        DataStream<Object> sideInputStream = context.getInputDataStream(sideInput);        DataStream<RawUnionValue> unionValueStream = sideInputStream.map(new ToRawUnion<>(intTag)).returns(unionTypeInformation);        if (sideInputUnion == null) {            sideInputUnion = unionValueStream;        } else {            sideInputUnion = sideInputUnion.union(unionValueStream);        }    }    if (sideInputUnion == null) {        throw new IllegalStateException("No unioned side inputs, this indicates a bug.");    }    return new Tuple2<>(intToViewMapping, sideInputUnion);}
public void beam_f5161_0(PTransform<PCollection<KV<K, InputT>>, PCollection<KV<K, OutputT>>> transform, FlinkStreamingTranslationContext context)
{    String fullName = getCurrentTransformName(context);    PCollection<KV<K, InputT>> input = context.getInput(transform);    @SuppressWarnings("unchecked")    WindowingStrategy<?, BoundedWindow> windowingStrategy = (WindowingStrategy<?, BoundedWindow>) input.getWindowingStrategy();    KvCoder<K, InputT> inputKvCoder = (KvCoder<K, InputT>) input.getCoder();    SingletonKeyedWorkItemCoder<K, InputT> workItemCoder = SingletonKeyedWorkItemCoder.of(inputKvCoder.getKeyCoder(), inputKvCoder.getValueCoder(), input.getWindowingStrategy().getWindowFn().windowCoder());    DataStream<WindowedValue<KV<K, InputT>>> inputDataStream = context.getInputDataStream(input);    WindowedValue.FullWindowedValueCoder<SingletonKeyedWorkItem<K, InputT>> windowedWorkItemCoder = WindowedValue.getFullCoder(workItemCoder, input.getWindowingStrategy().getWindowFn().windowCoder());    CoderTypeInformation<WindowedValue<SingletonKeyedWorkItem<K, InputT>>> workItemTypeInfo = new CoderTypeInformation<>(windowedWorkItemCoder);    DataStream<WindowedValue<SingletonKeyedWorkItem<K, InputT>>> workItemStream = inputDataStream.flatMap(new ToKeyedWorkItem<>()).returns(workItemTypeInfo).name("ToKeyedWorkItem");    WorkItemKeySelector keySelector = new WorkItemKeySelector<>(inputKvCoder.getKeyCoder());    KeyedStream<WindowedValue<SingletonKeyedWorkItem<K, InputT>>, ByteBuffer> keyedWorkItemStream = workItemStream.keyBy(keySelector);    GlobalCombineFn<? super InputT, ?, OutputT> combineFn = ((Combine.PerKey) transform).getFn();    SystemReduceFn<K, InputT, ?, OutputT, BoundedWindow> reduceFn = SystemReduceFn.combining(inputKvCoder.getKeyCoder(), AppliedCombineFn.withInputCoder(combineFn, input.getPipeline().getCoderRegistry(), inputKvCoder));    Coder<WindowedValue<KV<K, OutputT>>> outputCoder = context.getWindowedInputCoder(context.getOutput(transform));    TypeInformation<WindowedValue<KV<K, OutputT>>> outputTypeInfo = context.getTypeInfo(context.getOutput(transform));    List<PCollectionView<?>> sideInputs = ((Combine.PerKey) transform).getSideInputs();    if (sideInputs.isEmpty()) {        TupleTag<KV<K, OutputT>> mainTag = new TupleTag<>("main output");        WindowDoFnOperator<K, InputT, OutputT> doFnOperator = new WindowDoFnOperator<>(reduceFn, fullName, (Coder) windowedWorkItemCoder, mainTag, Collections.emptyList(), new DoFnOperator.MultiOutputOutputManagerFactory<>(mainTag, outputCoder), windowingStrategy, new HashMap<>(), /* side-input mapping */        Collections.emptyList(), /* side inputs */        context.getPipelineOptions(), inputKvCoder.getKeyCoder(), keySelector);                        @SuppressWarnings("unchecked")        SingleOutputStreamOperator<WindowedValue<KV<K, OutputT>>> outDataStream = keyedWorkItemStream.transform(fullName, outputTypeInfo, (OneInputStreamOperator) doFnOperator).uid(fullName);        context.setOutputDataStream(context.getOutput(transform), outDataStream);    } else {        Tuple2<Map<Integer, PCollectionView<?>>, DataStream<RawUnionValue>> transformSideInputs = transformSideInputs(sideInputs, context);        TupleTag<KV<K, OutputT>> mainTag = new TupleTag<>("main output");        WindowDoFnOperator<K, InputT, OutputT> doFnOperator = new WindowDoFnOperator<>(reduceFn, fullName, (Coder) windowedWorkItemCoder, mainTag, Collections.emptyList(), new DoFnOperator.MultiOutputOutputManagerFactory<>(mainTag, outputCoder), windowingStrategy, transformSideInputs.f0, sideInputs, context.getPipelineOptions(), inputKvCoder.getKeyCoder(), keySelector);                        TwoInputTransformation<WindowedValue<SingletonKeyedWorkItem<K, InputT>>, RawUnionValue, WindowedValue<KV<K, OutputT>>> rawFlinkTransform = new TwoInputTransformation<>(keyedWorkItemStream.getTransformation(), transformSideInputs.f1.broadcast().getTransformation(), transform.getName(), (TwoInputStreamOperator) doFnOperator, outputTypeInfo, keyedWorkItemStream.getParallelism());        rawFlinkTransform.setStateKeyType(keyedWorkItemStream.getKeyType());        rawFlinkTransform.setStateKeySelectors(keyedWorkItemStream.getKeySelector(), null);        @SuppressWarnings({ "unchecked", "rawtypes" })        SingleOutputStreamOperator<WindowedValue<KV<K, OutputT>>> outDataStream = new SingleOutputStreamOperator(keyedWorkItemStream.getExecutionEnvironment(),         rawFlinkTransform) {        };        keyedWorkItemStream.getExecutionEnvironment().addOperator(rawFlinkTransform);        context.setOutputDataStream(context.getOutput(transform), outDataStream);    }}
 boolean beam_f5162_0(PTransform<PCollection<KV<K, InputT>>, PCollection<KeyedWorkItem<K, InputT>>> transform, FlinkStreamingTranslationContext context)
{    return true;}
public String beam_f5171_0(SplittableParDoViaKeyedWorkItems.GBKIntoKeyedWorkItems<?, ?> transform)
{    return SplittableParDo.SPLITTABLE_GBKIKWI_URN;}
public String beam_f5172_0(CreateStreamingFlinkView.CreateFlinkPCollectionView<?, ?> transform)
{    return CreateStreamingFlinkView.CREATE_STREAMING_FLINK_VIEW_URN;}
public void beam_f5181_0()
{    unboundedSourceWrapper.cancel();}
public void beam_f5182_0(long timestamp) throws Exception
{    unboundedSourceWrapper.onProcessingTime(timestamp);}
public DataStream<T> beam_f5191_0(PValue value)
{    return (DataStream<T>) dataStreams.get(value);}
public void beam_f5192_0(PValue value, DataStream<?> set)
{    if (!dataStreams.containsKey(value)) {        dataStreams.put(value, set);    }}
public T beam_f5201_0(PTransform<?, T> transform)
{    return (T) Iterables.getOnlyElement(currentTransform.getOutputs().values());}
public Map<TupleTag<?>, PValue> beam_f5202_0(PTransform<?, OutputT> transform)
{    return currentTransform.getOutputs();}
public void beam_f5211_0(Metric metric, String metricName, MetricGroup group)
{    final String name = group.getMetricIdentifier(metricName, this);    super.notifyOfRemovedMetric(metric, metricName, group);    synchronized (this) {        ps.printf("%s: %s%n", name, Metrics.toString(metric));    }}
public void beam_f5212_1()
{    ps.close();    }
public DistributionResult beam_f5221_0()
{    return data;}
 void beam_f5222_0(GaugeResult update)
{    this.data = update;}
public boolean beam_f5231_0(ReaderT reader) throws IOException
{    if (enableMetrics) {        try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(container.getMetricsContainer(stepName))) {            boolean result = reader.advance();            container.updateMetrics(stepName);            return result;        }    } else {        return reader.advance();    }}
 static boolean beam_f5232_0(Pipeline p)
{    PipelineTranslationModeOptimizer optimizer = new PipelineTranslationModeOptimizer();    optimizer.translate(p);    return optimizer.hasUnboundedCollections;}
public AccumT beam_f5243_0(K key, InputT value, PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    AccumT accumulator = combineFnRunner.createAccumulator(options, sideInputReader, windows);    return combineFnRunner.addInput(accumulator, value, options, sideInputReader, windows);}
public AccumT beam_f5244_0(K key, AccumT accumulator, InputT value, PipelineOptions options, SideInputReader sideInputReader, Collection<? extends BoundedWindow> windows)
{    return combineFnRunner.addInput(accumulator, value, options, sideInputReader, windows);}
public Instant beam_f5253_0()
{    return value.getTimestamp();}
public BoundedWindow beam_f5254_0()
{    return Iterables.getOnlyElement(value.getWindows());}
public void beam_f5263_0(Configuration parameters) throws Exception
{            FileSystems.setDefaultPipelineOptions(PipelineOptionsFactory.create());    executableStage = ExecutableStage.fromPayload(stagePayload);    runtimeContext = getRuntimeContext();    container = new FlinkMetricContainer(getRuntimeContext());        stageContext = contextFactory.get(jobInfo);    stageBundleFactory = stageContext.getStageBundleFactory(executableStage);                stateRequestHandler = getStateRequestHandler(executableStage, stageBundleFactory.getProcessBundleDescriptor(), runtimeContext);    progressHandler = new BundleProgressHandler() {        @Override        public void onProgress(ProcessBundleProgressResponse progress) {            container.updateMetrics(stageName, progress.getMonitoringInfosList());        }        @Override        public void onCompleted(ProcessBundleResponse response) {            container.updateMetrics(stageName, response.getMonitoringInfosList());        }    };}
public void beam_f5264_0(ProcessBundleProgressResponse progress)
{    container.updateMetrics(stageName, progress.getMonitoringInfosList());}
public void beam_f5273_0(Iterable<WindowedValue<KV<K, InputT>>> elements, Collector<WindowedValue<KV<K, OutputT>>> out) throws Exception
{    PipelineOptions options = serializedOptions.get();    FlinkSideInputReader sideInputReader = new FlinkSideInputReader(sideInputs, getRuntimeContext());    AbstractFlinkCombineRunner<K, InputT, AccumT, OutputT, W> reduceRunner;    if (windowingStrategy.getWindowFn().windowCoder().equals(IntervalWindow.getCoder())) {        reduceRunner = new SortingFlinkCombineRunner<>();    } else {        reduceRunner = new HashingFlinkCombineRunner<>();    }    reduceRunner.combine(new AbstractFlinkCombineRunner.CompleteFlinkCombiner<>(combineFn), windowingStrategy, sideInputReader, options, elements, out);}
public void beam_f5274_0(WindowedValue<RawUnionValue> windowedValue, Collector<WindowedValue<T>> collector) throws Exception
{    int unionTag = windowedValue.getValue().getUnionTag();    if (unionTag == ourOutputTag) {        collector.collect((WindowedValue<T>) windowedValue.withValue(windowedValue.getValue().getValue()));    }}
public StateInternals beam_f5283_0()
{    return stateInternals;}
public TimerInternals beam_f5284_0()
{    return timerInternals;}
public Coder<T> beam_f5293_0()
{    return elementCoder;}
private SideInputHandler<V, W> beam_f5294_0(PCollectionView<?> collection, Coder<K> keyCoder, Coder<V> valueCoder)
{    return new SideInputHandler<V, W>() {        @Override        public Iterable<V> get(byte[] key, W window) {            Iterable<KV<K, V>> values = (Iterable<KV<K, V>>) runnerHandler.getIterable(collection, window);            ArrayList<V> result = new ArrayList<>();                        for (KV<K, V> kv : values) {                ByteArrayOutputStream bos = new ByteArrayOutputStream();                try {                    keyCoder.encode(kv.getKey(), bos);                    if (Arrays.equals(key, bos.toByteArray())) {                        result.add(kv.getValue());                    }                } catch (IOException ex) {                    throw new RuntimeException(ex);                }            }            return result;        }        @Override        public Coder<V> resultCoder() {            return valueCoder;        }    };}
public void beam_f5303_0()
{    this.running = false;}
public void beam_f5305_0(FunctionInitializationContext context) throws Exception
{    impulseEmitted = context.getOperatorStateStore().getListState(new ListStateDescriptor<>("impulse-emitted", BooleanSerializer.INSTANCE));}
public boolean beam_f5314_0()
{    return true;}
public TypeSerializer<T> beam_f5315_0(ExecutionConfig config)
{    return new CoderTypeSerializer<>(coder);}
public boolean beam_f5324_0(byte[] candidate)
{    if (encodedReferenceKey.length != candidate.length) {        return false;    }    int len = candidate.length;    for (int i = 0; i < len; i++) {        if (encodedReferenceKey[i] != candidate[i]) {            return false;        }    }    return true;}
public int beam_f5325_0(TypeComparator<byte[]> other)
{            EncodedValueComparator otherEncodedValueComparator = (EncodedValueComparator) other;    int len = Math.min(encodedReferenceKey.length, otherEncodedValueComparator.encodedReferenceKey.length);    for (int i = 0; i < len; i++) {        byte b1 = encodedReferenceKey[i];        byte b2 = otherEncodedValueComparator.encodedReferenceKey[i];        int result = (b1 < b2 ? -1 : (b1 == b2 ? 0 : 1));        if (result != 0) {            return ascending ? -result : result;        }    }    int result = encodedReferenceKey.length - otherEncodedValueComparator.encodedReferenceKey.length;    return ascending ? -result : result;}
public byte[] beam_f5334_0(byte[] reuse, DataInputView source) throws IOException
{    throw new UnsupportedOperationException();}
public boolean beam_f5335_0()
{    return !ascending;}
public boolean beam_f5344_0()
{    return true;}
public TypeSerializer<byte[]> beam_f5345_0(ExecutionConfig executionConfig)
{    return new EncodedValueSerializer();}
public static NoopLock beam_f5354_0()
{    if (instance == null) {        instance = new NoopLock();    }    return instance;}
public boolean beam_f5357_0()
{    return true;}
public long beam_f5368_0()
{    return 1;}
public float beam_f5369_0()
{    return 1;}
public long beam_f5379_0()
{    return estimatedSize;}
public long beam_f5380_0()
{    return BaseStatistics.NUM_RECORDS_UNKNOWN;}
protected DoFn<InputT, OutputT> beam_f5389_0()
{    return doFn;}
protected DoFnRunner<InputT, OutputT> beam_f5390_0(DoFnRunner<InputT, OutputT> wrappedRunner)
{    if (keyCoder != null) {        StatefulDoFnRunner.CleanupTimer cleanupTimer = new StatefulDoFnRunner.TimeInternalsCleanupTimer(timerInternals, windowingStrategy);                @SuppressWarnings({ "unchecked", "rawtypes" })        Coder windowCoder = windowingStrategy.getWindowFn().windowCoder();        @SuppressWarnings({ "unchecked", "rawtypes" })        StatefulDoFnRunner.StateCleaner<?> stateCleaner = new StatefulDoFnRunner.StateInternalsStateCleaner<>(doFn, keyedStateInternals, windowCoder);        return DoFnRunners.defaultStatefulDoFnRunner(doFn, wrappedRunner, windowingStrategy, cleanupTimer, stateCleaner);    } else {        return doFnRunner;    }}
protected void beam_f5399_0(Runnable callback)
{    this.bundleFinishedCallback = callback;}
public final void beam_f5400_0(StreamRecord<WindowedValue<InputT>> streamRecord) throws Exception
{    checkInvokeStartBundle();    doFnRunner.processElement(streamRecord.getValue());    checkInvokeFinishBundleByCount();}
private void beam_f5409_0()
{    if (bundleStarted.compareAndSet(false, true)) {        outputManager.flushBuffer();        pushbackDoFnRunner.startBundle();    }}
private void beam_f5410_0()
{    elementCount++;    if (elementCount >= maxBundleSize) {        invokeFinishBundle();    }}
private void beam_f5419_0(long currentInputWatermark)
{    this.currentSideInputWatermark = currentInputWatermark;}
private void beam_f5420_0(long currentOutputWatermark)
{    this.currentOutputWatermark = currentOutputWatermark;}
public List<? extends Coder<?>> beam_f5429_0()
{    return new ArrayList<>(idsToCoders.values());}
public void beam_f5430_0() throws NonDeterministicException
{    for (Coder<?> coder : idsToCoders.values()) {        verifyDeterministic(this, "Coder must be deterministic", coder);    }}
 void beam_f5439_0(TimerData timer)
{    try {        pendingTimersById.remove(getContextTimerId(timer.getTimerId(), timer.getNamespace()));    } catch (Exception e) {        throw new RuntimeException("Failed to cleanup state with pending timers", e);    }}
private String beam_f5440_0(String timerId, StateNamespace namespace)
{    return timerId + namespace.stringKey();}
protected Lock beam_f5449_0()
{    return stateBackendLock;}
public void beam_f5450_0() throws Exception
{    executableStage = ExecutableStage.fromPayload(payload);    initializeUserState(executableStage, getKeyedStateBackend());                        stageContext = contextFactory.get(jobInfo);    flinkMetricContainer = new FlinkMetricContainer(getRuntimeContext());    stageBundleFactory = stageContext.getStageBundleFactory(executableStage);    stateRequestHandler = getStateRequestHandler(executableStage);    progressHandler = new BundleProgressHandler() {        @Override        public void onProgress(ProcessBundleProgressResponse progress) {            flinkMetricContainer.updateMetrics(stepName, progress.getMonitoringInfosList());        }        @Override        public void onCompleted(ProcessBundleResponse response) {            flinkMetricContainer.updateMetrics(stepName, response.getMonitoringInfosList());        }    };        super.open();}
private void beam_f5459_0(K key)
{        ByteBuffer encodedKey = FlinkKeyUtils.fromEncodedKey(key);    keyedStateBackend.setCurrentKey(encodedKey);}
public ByteBuffer beam_f5462_0()
{            Preconditions.checkState(stateBackendLock.isLocked(), "State backend must be locked when retrieving the current key.");    return this.<ByteBuffer>getKeyedStateBackend().getCurrentKey();}
public void beam_f5471_1(WindowedValue<InputT> element)
{    try {                mainInputReceiver.accept(element);    } catch (Exception e) {        throw new RuntimeException("Failed to process element with SDK harness.", e);    }    emitResults();}
public void beam_f5472_1(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain)
{    Object timerKey = keyForTimer.get();    Preconditions.checkNotNull(timerKey, "Key for timer needs to be set before calling onTimer");    Preconditions.checkNotNull(remoteBundle, "Call to onTimer outside of a bundle");        FnDataReceiver<WindowedValue<?>> timerReceiver = Preconditions.checkNotNull(remoteBundle.getInputReceivers().get(timerId), "No receiver found for timer %s", timerId);    WindowedValue<KV<Object, Timer>> timerValue = WindowedValue.of(KV.of(timerKey, Timer.of(timestamp, new byte[0])), timestamp, Collections.singleton(window), PaneInfo.NO_FIRING);    try {        timerReceiver.accept(timerValue);    } catch (Exception e) {        throw new RuntimeException(String.format(Locale.ENGLISH, "Failed to process timer %s", timerReceiver), e);    }}
public boolean beam_f5481_0(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain)
{    boolean isEventTimer = timeDomain.equals(TimeDomain.EVENT_TIME);    Instant gcTime = LateDataUtils.garbageCollectionTime(window, windowingStrategy).plus(1);    return isEventTimer && GC_TIMER_ID.equals(timerId) && gcTime.equals(timestamp);}
public void beam_f5482_0(BoundedWindow window)
{        cleanupQueue.add(KV.of(keyedStateBackend.get(), window));}
public List<? extends Coder<?>> beam_f5492_0()
{    return Collections.emptyList();}
public void beam_f5494_0(StateInitializationContext context) throws Exception
{    super.initializeState(context);    timerService = getInternalTimerService("dedup-cleanup-timer", VoidNamespaceSerializer.INSTANCE, this);}
private void beam_f5504_0()
{    if (!shutdownOnFinalWatermark) {                while (isRunning) {            try {                                Thread.sleep(1000);            } catch (InterruptedException e) {                if (!isRunning) {                                        Thread.currentThread().interrupt();                }            }        }    }}
private void beam_f5505_0(SourceContext<WindowedValue<ValueWithRecordId<OutputT>>> ctx, UnboundedSource.UnboundedReader<OutputT> reader)
{            OutputT item = reader.getCurrent();    byte[] recordId = reader.getCurrentRecordId();    Instant timestamp = reader.getCurrentTimestamp();    WindowedValue<ValueWithRecordId<OutputT>> windowedValue = WindowedValue.of(new ValueWithRecordId<>(item, recordId), timestamp, GlobalWindow.INSTANCE, PaneInfo.NO_FIRING);    ctx.collect(windowedValue);}
 List<? extends UnboundedSource<OutputT, CheckpointMarkT>> beam_f5514_0()
{    return localSplitSources;}
 List<UnboundedSource.UnboundedReader<OutputT>> beam_f5515_0()
{    return localReaders;}
public ByteBuffer beam_f5524_0(WindowedValue<KV<K, V>> value)
{    K key = value.getValue().getKey();    return FlinkKeyUtils.encodeKey(key, keyCoder);}
public TypeInformation<ByteBuffer> beam_f5525_0()
{    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of());}
public Iterable<WindowedValue<ElemT>> beam_f5534_0()
{    return Collections.singletonList(value);}
public static SingletonKeyedWorkItemCoder<K, ElemT> beam_f5535_0(Coder<K> keyCoder, Coder<ElemT> elemCoder, Coder<? extends BoundedWindow> windowCoder)
{    return new SingletonKeyedWorkItemCoder<>(keyCoder, elemCoder, windowCoder);}
public boolean beam_f5544_0()
{    return false;}
protected DoFnRunner<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> beam_f5545_0(DoFnRunner<KeyedWorkItem<byte[], KV<InputT, RestrictionT>>, OutputT> wrappedRunner)
{        return wrappedRunner;}
public void beam_f5554_0(DoFnRunner doFnRunner)
{    doFnRunner.onTimer(timerId, window, timestamp, timeDomain);}
public boolean beam_f5555_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    Timer timer = (Timer) o;    return timerId.equals(timer.timerId) && window.equals(timer.window) && timestamp.equals(timer.timestamp) && timeDomain == timer.timeDomain;}
public void beam_f5567_0(long checkpointId) throws Exception
{                addToBeAcknowledgedCheckpoint(checkpointId, currentStateId);    currentStateId = generateNewId();    currentBufferingElementsHandler = bufferingElementsHandlerFactory.get(currentStateId);}
public void beam_f5568_0(long checkpointId) throws Exception
{    List<CheckpointElement> toAck = removeToBeAcknowledgedCheckpoints(checkpointId);    for (CheckpointElement toBeAcked : toAck) {        BufferingElementsHandler bufferingElementsHandler = bufferingElementsHandlerFactory.get(toBeAcked.internalId);        Iterator<BufferedElement> iterator = bufferingElementsHandler.getElements().iterator();        boolean hasElements = iterator.hasNext();        if (hasElements) {            underlying.startBundle();        }        while (iterator.hasNext()) {            BufferedElement bufferedElement = iterator.next();            bufferedElement.processWith(underlying);        }        if (hasElements) {            underlying.finishBundle();        }        bufferingElementsHandler.clear();    }}
public Stream<BufferedElement> beam_f5577_0()
{    try {        return StreamSupport.stream(elementState.get().spliterator(), false);    } catch (Exception e) {        throw new RuntimeException("Failed to retrieve buffered element from state backend.", e);    }}
public void beam_f5578_0(BufferedElement element)
{    try {        elementState.add(element);    } catch (Exception e) {        throw new RuntimeException("Failed to buffer element in state backend.", e);    }}
public CombiningState<InputT, AccumT, OutputT> beam_f5587_0(StateTag<CombiningState<InputT, AccumT, OutputT>> address, Coder<AccumT> accumCoder, CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn)
{    return new FlinkCombiningStateWithContext<>(stateBackend, address, combineFn, namespace, accumCoder, FlinkBroadcastStateInternals.this, CombineContextFactory.createFromStateContext(context));}
public WatermarkHoldState beam_f5588_0(StateTag<WatermarkHoldState> address, TimestampCombiner timestampCombiner)
{    throw new UnsupportedOperationException(String.format("%s is not supported", WatermarkHoldState.class.getSimpleName()));}
public T beam_f5597_0()
{    return readInternal();}
public boolean beam_f5598_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    FlinkBroadcastValueState<?> that = (FlinkBroadcastValueState<?>) o;    return namespace.equals(that.namespace) && address.equals(that.address);}
public void beam_f5607_0()
{    clearInternal();}
public boolean beam_f5608_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    FlinkBroadcastBagState<?> that = (FlinkBroadcastBagState<?>) o;    return namespace.equals(that.namespace) && address.equals(that.address);}
public Boolean beam_f5617_0()
{    try {        return readInternal() == null;    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ReadableState<Boolean> beam_f5618_0()
{    return this;}
public OutputT beam_f5627_0()
{    try {        AccumT accum = readInternal();        if (accum != null) {            return combineFn.extractOutput(accum);        } else {            return combineFn.extractOutput(combineFn.createAccumulator());        }    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ReadableState<Boolean> beam_f5628_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            try {                return readInternal() == null;            } catch (Exception e) {                throw new RuntimeException("Error reading state.", e);            }        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public AccumT beam_f5637_0()
{    try {        AccumT accum = readInternal();        return accum != null ? accum : combineFn.createAccumulator(context);    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public AccumT beam_f5638_0(Iterable<AccumT> accumulators)
{    return combineFn.mergeAccumulators(accumulators, context);}
public K beam_f5647_0()
{    ByteBuffer keyBytes = flinkStateBackend.getCurrentKey();    return FlinkKeyUtils.decodeKey(keyBytes, keyCoder);}
public T beam_f5648_0(StateNamespace namespace, StateTag<T> address, StateContext<?> context)
{    return address.getSpec().bind(address.getId(), new FlinkStateBinder(namespace, context, flinkStateBackend, watermarkHolds, watermarkHoldStateDescriptor));}
public ValueState<T> beam_f5657_0()
{    return this;}
public T beam_f5658_0()
{    try {        return flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).value();    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public ReadableState<Boolean> beam_f5667_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            try {                Iterable<T> result = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).get();                return result == null;            } catch (Exception e) {                throw new RuntimeException("Error reading state.", e);            }        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public Boolean beam_f5668_0()
{    try {        Iterable<T> result = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).get();        return result == null;    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public AccumT beam_f5677_0(Iterable<AccumT> accumulators)
{    return combineFn.mergeAccumulators(accumulators);}
public OutputT beam_f5678_0()
{    try {        org.apache.flink.api.common.state.ValueState<AccumT> state = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor);        AccumT accum = state.value();        if (accum != null) {            return combineFn.extractOutput(accum);        } else {            return combineFn.extractOutput(combineFn.createAccumulator());        }    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public void beam_f5687_0(AccumT accum)
{    try {        org.apache.flink.api.common.state.ValueState<AccumT> state = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor);        AccumT current = state.value();        if (current == null) {            state.update(accum);        } else {            current = combineFn.mergeAccumulators(Lists.newArrayList(current, accum), context);            state.update(current);        }    } catch (Exception e) {        throw new RuntimeException("Error adding to state.", e);    }}
public AccumT beam_f5688_0()
{    try {        AccumT accum = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).value();        return accum != null ? accum : combineFn.createAccumulator(context);    } catch (Exception e) {        throw new RuntimeException("Error reading state.", e);    }}
public TimestampCombiner beam_f5697_0()
{    return timestampCombiner;}
public WatermarkHoldState beam_f5698_0()
{    return this;}
public ReadableState<ValueT> beam_f5707_0(final KeyT input)
{    try {        return ReadableStates.immediate(flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).get(input));    } catch (Exception e) {        throw new RuntimeException("Error get from state.", e);    }}
public void beam_f5708_0(KeyT key, ValueT value)
{    try {        flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).put(key, value);    } catch (Exception e) {        throw new RuntimeException("Error put kv to state.", e);    }}
public ReadableState<Iterable<Map.Entry<KeyT, ValueT>>> beam_f5717_0()
{    return new ReadableState<Iterable<Map.Entry<KeyT, ValueT>>>() {        @Override        public Iterable<Map.Entry<KeyT, ValueT>> read() {            try {                Iterable<Map.Entry<KeyT, ValueT>> result = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).entries();                return result != null ? ImmutableList.copyOf(result) : Collections.emptyList();            } catch (Exception e) {                throw new RuntimeException("Error get map state entries.", e);            }        }        @Override        public ReadableState<Iterable<Map.Entry<KeyT, ValueT>>> readLater() {            return this;        }    };}
public Iterable<Map.Entry<KeyT, ValueT>> beam_f5718_0()
{    try {        Iterable<Map.Entry<KeyT, ValueT>> result = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).entries();        return result != null ? ImmutableList.copyOf(result) : Collections.emptyList();    } catch (Exception e) {        throw new RuntimeException("Error get map state entries.", e);    }}
public void beam_f5727_0(T value)
{    try {        flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).put(value, true);    } catch (Exception e) {        throw new RuntimeException("Error add value to state.", e);    }}
public ReadableState<Boolean> beam_f5728_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            try {                Iterable<T> result = flinkStateBackend.getPartitionedState(namespace.stringKey(), StringSerializer.INSTANCE, flinkStateDescriptor).keys();                return result == null || Iterables.isEmpty(result);            } catch (Exception e) {                throw new RuntimeException("Error isEmpty from state.", e);            }        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public BagState<T> beam_f5737_0(String id, StateSpec<BagState<T>> spec, Coder<T> elemCoder)
{    try {        keyedStateBackend.getOrCreateKeyedState(StringSerializer.INSTANCE, new ListStateDescriptor<>(id, new CoderTypeSerializer<>(elemCoder)));    } catch (Exception e) {        throw new RuntimeException(e);    }    return null;}
public SetState<T> beam_f5738_0(String id, StateSpec<SetState<T>> spec, Coder<T> elemCoder)
{    try {        keyedStateBackend.getOrCreateKeyedState(StringSerializer.INSTANCE, new MapStateDescriptor<>(id, new CoderTypeSerializer<>(elemCoder), VoidSerializer.INSTANCE));    } catch (Exception e) {        throw new RuntimeException(e);    }    return null;}
public TypeInformation<ByteBuffer> beam_f5747_0()
{    return new GenericTypeInfo<>(ByteBuffer.class);}
protected TypeComparator<byte[]> beam_f5748_0(boolean ascending)
{    return new EncodedValueTypeInformation().createComparator(ascending, new ExecutionConfig());}
public void beam_f5757_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    options.setFlinkMaster("host:80");    ExecutionEnvironment bev = FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());    assertThat(options.getParallelism(), is(1));    assertThat(bev.getParallelism(), is(1));}
public void beam_f5758_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    options.setFlinkMaster("host:80");    StreamExecutionEnvironment sev = FlinkExecutionEnvironments.createStreamExecutionEnvironment(options, Collections.emptyList());    assertThat(options.getParallelism(), is(1));    assertThat(sev.getParallelism(), is(1));}
public void beam_f5767_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(FlinkRunner.class);    options.setFlinkMaster("host:p0rt");    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Unparseable port number");    FlinkExecutionEnvironments.createBatchExecutionEnvironment(options, Collections.emptyList());}
public void beam_f5768_0()
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(FlinkRunner.class);    options.setFlinkMaster("host:p0rt");    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Unparseable port number");    FlinkExecutionEnvironments.createStreamExecutionEnvironment(options, Collections.emptyList());}
public void beam_f5777_0()
{    FlinkJobServerDriver.FlinkServerConfiguration config = new FlinkJobServerDriver.FlinkServerConfiguration();    FlinkJobServerDriver driver = FlinkJobServerDriver.fromConfig(config);    assertThat(driver.configuration, is(config));}
public void beam_f5778_0() throws Exception
{    FlinkJobServerDriver driver = null;    Thread driverThread = null;    final PrintStream oldErr = System.err;    ByteArrayOutputStream baos = new ByteArrayOutputStream();    PrintStream newErr = new PrintStream(baos);    try {        System.setErr(newErr);        driver = FlinkJobServerDriver.fromParams(new String[] { "--job-port=0", "--artifact-port=0", "--expansion-port=0" });        driverThread = new Thread(driver);        driverThread.start();        boolean success = false;        while (!success) {            newErr.flush();            String output = baos.toString(Charsets.UTF_8.name());            if (output.contains("JobService started on localhost:") && output.contains("ArtifactStagingService started on localhost:") && output.contains("ExpansionService started on localhost:")) {                success = true;            } else {                Thread.sleep(100);            }        }        assertThat(driverThread.isAlive(), is(true));    } catch (Throwable t) {                System.setErr(oldErr);        throw t;    } finally {        System.setErr(oldErr);        if (driver != null) {            driver.stop();        }        if (driverThread != null) {            driverThread.interrupt();            driverThread.join();        }    }}
public void beam_f5787_0() throws Exception
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    options.setFlinkMaster("clusterAddress");    FlinkPipelineExecutionEnvironment flinkEnv = new FlinkPipelineExecutionEnvironment(options);    Pipeline pipeline = Pipeline.create(options);    flinkEnv.translate(pipeline);    ExecutionEnvironment executionEnvironment = flinkEnv.getBatchExecutionEnvironment();    assertThat(executionEnvironment, instanceOf(RemoteEnvironment.class));    @SuppressWarnings("unchecked")    List<URL> jarFiles = (List<URL>) Whitebox.getInternalState(executionEnvironment, "jarFiles");    List<URL> urlConvertedStagedFiles = convertFilesToURLs(options.getFilesToStage());    assertThat(jarFiles, is(urlConvertedStagedFiles));}
public void beam_f5788_0() throws Exception
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setRunner(TestFlinkRunner.class);    options.setFlinkMaster("clusterAddress");    options.setStreaming(true);    FlinkPipelineExecutionEnvironment flinkEnv = new FlinkPipelineExecutionEnvironment(options);    Pipeline pipeline = Pipeline.create(options);    flinkEnv.translate(pipeline);    StreamExecutionEnvironment streamExecutionEnvironment = flinkEnv.getStreamExecutionEnvironment();    assertThat(streamExecutionEnvironment, instanceOf(RemoteStreamEnvironment.class));    @SuppressWarnings("unchecked")    List<URL> jarFiles = (List<URL>) Whitebox.getInternalState(streamExecutionEnvironment, "jarFiles");    List<URL> urlConvertedStagedFiles = convertFilesToURLs(options.getFilesToStage());    assertThat(jarFiles, is(urlConvertedStagedFiles));}
private FlinkPipelineOptions beam_f5798_0(String flinkMaster) throws IOException
{    return testPreparingResourcesToStage(flinkMaster, true);}
private FlinkPipelineOptions beam_f5799_0(String flinkMaster, boolean includeNonExisting) throws IOException
{    Pipeline pipeline = Pipeline.create();    String tempLocation = tmpFolder.newFolder().getAbsolutePath();    List<String> filesToStage = new ArrayList<>();    File stagingDir = tmpFolder.newFolder();    stagingDir.createNewFile();    filesToStage.add(stagingDir.getAbsolutePath());    File individualStagingFile = tmpFolder.newFile();    filesToStage.add(individualStagingFile.getAbsolutePath());    if (includeNonExisting) {        filesToStage.add("/path/to/not/existing/dir");    }    FlinkPipelineOptions options = setPipelineOptions(flinkMaster, tempLocation, filesToStage);    FlinkPipelineExecutionEnvironment flinkEnv = new FlinkPipelineExecutionEnvironment(options);    flinkEnv.translate(pipeline);    return options;}
public void beam_f5808_0() throws Exception
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setParallelism(1);            options.setCheckpointingInterval(Long.MAX_VALUE);    options.setRunner(FlinkRunner.class);    options.setStreaming(true);    ResourceId outputDir = FileSystems.matchNewResource(tempFolder.getRoot().getAbsolutePath(), true).resolve(String.format("requires-stable-input-%tF-%<tH-%<tM-%<tS-%<tL", new Date()), ResolveOptions.StandardResolveOptions.RESOLVE_DIRECTORY);    String singleOutputPrefix = outputDir.resolve("pardo-single-output", ResolveOptions.StandardResolveOptions.RESOLVE_DIRECTORY).resolve("key-", ResolveOptions.StandardResolveOptions.RESOLVE_FILE).toString();    String multiOutputPrefix = outputDir.resolve("pardo-multi-output", ResolveOptions.StandardResolveOptions.RESOLVE_DIRECTORY).resolve("key-", ResolveOptions.StandardResolveOptions.RESOLVE_FILE).toString();    Pipeline p = createPipeline(options, singleOutputPrefix, multiOutputPrefix);        latch = new CountDownLatch(2);    JobID jobID = executePipeline(p);    String savepointDir;    do {                        savepointDir = takeSavepoint(jobID);    } while (!latch.await(100, TimeUnit.MILLISECONDS));    flinkCluster.cancelJob(jobID).get();    options.setShutdownSourcesOnFinalWatermark(true);    restoreFromSavepoint(p, savepointDir);    waitUntilJobIsDone();    assertThat(new FlinkRunnerResult(Collections.emptyMap(), 1L), SerializableMatchers.allOf(new FileChecksumMatcher(VALUE_CHECKSUM, new FilePatternMatchingShardedFile(singleOutputPrefix + "*")), new FileChecksumMatcher(VALUE_CHECKSUM, new FilePatternMatchingShardedFile(multiOutputPrefix + "*"))));}
private JobGraph beam_f5809_0(Pipeline pipeline)
{    FlinkRunner flinkRunner = FlinkRunner.fromOptions(pipeline.getOptions());    return flinkRunner.getJobGraph(pipeline);}
public void beam_f5818_0()
{    FlinkRunnerResult result = new FlinkRunnerResult(Collections.emptyMap(), 100);    assertThat(result.waitUntilFinish(), is(PipelineResult.State.DONE));    assertThat(result.waitUntilFinish(Duration.millis(100)), is(PipelineResult.State.DONE));}
public void beam_f5819_0()
{    FlinkRunnerResult result = new FlinkRunnerResult(Collections.emptyMap(), 100);    result.cancel();    assertThat(result.getState(), is(PipelineResult.State.DONE));}
private String beam_f5828_0() throws Exception
{    final URI uri;    Method getRestAddress = flinkCluster.getClass().getMethod("getRestAddress");    if (getRestAddress.getReturnType().equals(URI.class)) {                uri = (URI) getRestAddress.invoke(flinkCluster);    } else if (getRestAddress.getReturnType().equals(CompletableFuture.class)) {        @SuppressWarnings("unchecked")        CompletableFuture<URI> future = (CompletableFuture<URI>) getRestAddress.invoke(flinkCluster);        uri = future.get();    } else {        throw new RuntimeException("Could not determine Rest address for this Flink version.");    }    return uri.getHost() + ":" + uri.getPort();}
private JobID beam_f5829_0() throws InterruptedException, ExecutionException
{    while (true) {        JobStatusMessage jobStatus = Iterables.getFirst(flinkCluster.listJobs().get(), null);        if (jobStatus != null && jobStatus.getJobState() == JobStatus.RUNNING) {            return jobStatus.getJobId();        }        Thread.sleep(100);    }}
public void beam_f5838_0(ProcessContext context)
{    context.output(KV.of("key", context.element()));}
public void beam_f5839_0(ProcessContext context, @StateId("valueState") ValueState<Integer> intValueState, @StateId("bagState") BagState<Integer> intBagState)
{    assertThat(intValueState.read(), Matchers.is(42));    assertThat(intBagState.read(), IsIterableContaining.hasItems(40, 1, 1));    oneShotLatch.countDown();}
public void beam_f5848_0()
{    final int parallelism = 2;    Read.Unbounded transform = Read.from(new TestUnboundedSource());    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();    env.setParallelism(parallelism);    StreamTransformation<?> sourceTransform = applyReadSourceTransform(transform, PCollection.IsBounded.UNBOUNDED, env);    UnboundedSourceWrapper source = (UnboundedSourceWrapper) ((SourceTransformation) ((OneInputTransformation) sourceTransform).getInput()).getOperator().getUserFunction();    assertEquals(parallelism, source.getSplitSources().size());}
private StreamTransformation<?> beam_f5849_0(PTransform<?, ?> transform, PCollection.IsBounded isBounded, StreamExecutionEnvironment env)
{    FlinkStreamingPipelineTranslator.StreamTransformTranslator<PTransform<?, ?>> translator = getReadSourceTranslator();    FlinkStreamingTranslationContext ctx = new FlinkStreamingTranslationContext(env, PipelineOptionsFactory.create());    Pipeline pipeline = Pipeline.create();    PCollection<String> pc = PCollection.createPrimitiveOutputInternal(pipeline, WindowingStrategy.globalDefault(), isBounded, StringUtf8Coder.of());    pc.setName("output");    Map<TupleTag<?>, PValue> outputs = new HashMap<>();    outputs.put(new TupleTag<>(), pc);    AppliedPTransform<?, ?, ?> appliedTransform = AppliedPTransform.of("test-transform", Collections.emptyMap(), outputs, transform, Pipeline.create());    ctx.setCurrentTransform(appliedTransform);    translator.translateNode(transform, ctx);    return ctx.getInputDataStream(pc).getTransformation();}
public static FlinkTestPipeline beam_f5858_0()
{    return create(false);}
public static FlinkTestPipeline beam_f5859_0()
{    return create(true);}
public void beam_f5869_0()
{    FlinkMetricContainer.FlinkGauge flinkGauge = new FlinkMetricContainer.FlinkGauge(GaugeResult.empty());    when(metricGroup.gauge(eq("namespace.name"), anyObject())).thenReturn(flinkGauge);    FlinkMetricContainer container = new FlinkMetricContainer(runtimeContext);    MetricsContainer step = container.getMetricsContainer("step");    MetricName metricName = MetricName.named("namespace", "name");    Gauge gauge = step.getGauge(metricName);    assertThat(flinkGauge.getValue(), is(GaugeResult.empty()));        container.updateMetrics("step");    gauge.set(1);    gauge.set(42);    container.updateMetrics("step");    assertThat(flinkGauge.getValue().getValue(), is(42L));}
public void beam_f5870_0()
{    FlinkMetricContainer container = new FlinkMetricContainer(runtimeContext);    MetricsContainer step = container.getMetricsContainer("step");    SimpleCounter userCounter = new SimpleCounter();    when(metricGroup.counter("ns1.metric1")).thenReturn(userCounter);    SimpleCounter elemCounter = new SimpleCounter();    when(metricGroup.counter("beam.metric:element_count:v1")).thenReturn(elemCounter);    MonitoringInfo userCountMonitoringInfo = new SimpleMonitoringInfoBuilder().setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, "ns1").setLabel(MonitoringInfoConstants.Labels.NAME, "metric1").setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "anyPTransform").setInt64Value(111).build();    assertNotNull(userCountMonitoringInfo);    MonitoringInfo elemCountMonitoringInfo = new SimpleMonitoringInfoBuilder().setUrn(MonitoringInfoConstants.Urns.ELEMENT_COUNT).setInt64Value(222).setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "step").setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "pcoll").setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "anyPTransform").build();    assertNotNull(elemCountMonitoringInfo);    assertThat(userCounter.getCount(), is(0L));    assertThat(elemCounter.getCount(), is(0L));    container.updateMetrics("step", ImmutableList.of(userCountMonitoringInfo, elemCountMonitoringInfo));    assertThat(userCounter.getCount(), is(111L));    assertThat(elemCounter.getCount(), is(222L));}
public void beam_f5879_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    options.setRunner(CrashingRunner.class);    options.as(FlinkPipelineOptions.class).setFlinkMaster("[local]");    options.as(FlinkPipelineOptions.class).setStreaming(isStreaming);    options.as(FlinkPipelineOptions.class).setParallelism(2);    options.as(FlinkPipelineOptions.class).setShutdownSourcesOnFinalWatermark(true);    options.as(PortablePipelineOptions.class).setDefaultEnvironmentType(Environments.ENVIRONMENT_EMBEDDED);    Pipeline p = Pipeline.create(options);    PCollection<KV<String, Iterable<Long>>> result = p.apply("impulse", Impulse.create()).apply("create", ParDo.of(new DoFn<byte[], String>() {        @ProcessElement        public void process(ProcessContext ctxt) {            ctxt.output("zero");            ctxt.output("one");            ctxt.output("two");        }    })).apply("len", ParDo.of(new DoFn<String, Long>() {        @ProcessElement        public void process(ProcessContext ctxt) {            ctxt.output((long) ctxt.element().length());        }    })).apply("addKeys", WithKeys.of("foo")).setCoder(KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of())).apply("gbk", GroupByKey.create());    PAssert.that(result).containsInAnyOrder(KV.of("foo", ImmutableList.of(4L, 3L, 3L)));            p.replaceAll(Collections.singletonList(JavaReadViaImpulse.boundedOverride()));    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);        JobInvocation jobInvocation = FlinkJobInvoker.createJobInvocation("fakeId", "fakeRetrievalToken", flinkJobExecutor, pipelineProto, options.as(FlinkPipelineOptions.class), new FlinkPipelineRunner(options.as(FlinkPipelineOptions.class), null, Collections.emptyList()));    jobInvocation.start();    while (jobInvocation.getState() != Enum.DONE) {        Thread.sleep(1000);    }}
public void beam_f5880_0(ProcessContext ctxt)
{    ctxt.output("zero");    ctxt.output("one");    ctxt.output("two");}
public static Object[] beam_f5889_0()
{    return new Object[] { true, false };}
public static void beam_f5890_0()
{            flinkJobExecutor = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(1));}
public void beam_f5899_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    options.setRunner(CrashingRunner.class);    options.as(FlinkPipelineOptions.class).setFlinkMaster("[local]");    options.as(FlinkPipelineOptions.class).setStreaming(isStreaming);    options.as(FlinkPipelineOptions.class).setParallelism(2);    options.as(FlinkPipelineOptions.class).setShutdownSourcesOnFinalWatermark(true);    options.as(PortablePipelineOptions.class).setDefaultEnvironmentType(Environments.ENVIRONMENT_EMBEDDED);    Pipeline p = Pipeline.create(options);    PCollection<Long> result = p.apply(GenerateSequence.from(0L).to(10L));    PAssert.that(result).containsInAnyOrder(ImmutableList.of(0L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L));            p.replaceAll(Collections.singletonList(JavaReadViaImpulse.boundedOverride()));    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);        JobInvocation jobInvocation = FlinkJobInvoker.createJobInvocation("fakeId", "fakeRetrievalToken", flinkJobExecutor, pipelineProto, options.as(FlinkPipelineOptions.class), new FlinkPipelineRunner(options.as(FlinkPipelineOptions.class), null, Collections.emptyList()));    jobInvocation.start();    while (jobInvocation.getState() != Enum.DONE) {        Thread.sleep(100);    }}
public void beam_f5900_0() throws Exception
{            File resultParent = createAndRegisterTempFile("result");    resultDir = resultParent.toURI().toString();    resultPath = new File(resultParent, "file.txt").getAbsolutePath();}
public void beam_f5909_0(ProcessContext c) throws Exception
{    c.output(c.element().toString());}
public static Collection<Object[]> beam_f5910_0()
{    /* Parameters for initializing the tests: {numTasks, numSplits} */    return Arrays.asList(new Object[][] { { 1, 1 }, { 1, 2 }, { 1, 4 } });}
public void beam_f5922_0() throws Exception
{    Pipeline p = FlinkTestPipeline.createForStreaming();    p.getOptions().as(FlinkPipelineOptions.class).setParallelism(1);    PCollection<String> output = p.apply(Create.of(Arrays.asList(KV.of(0, "user1"), KV.of(1, "user1"), KV.of(2, "user1"), KV.of(10, "user2"), KV.of(1, "user2"), KV.of(15000, "user2"), KV.of(12000, "user2"), KV.of(25000, "user3")))).apply(ParDo.of(new ExtractUserAndTimestamp())).apply(Window.<String>into(FixedWindows.of(Duration.standardHours(1))).triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO).discardingFiredPanes()).apply(ParDo.of(new DoFn<String, KV<Void, String>>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            String elem = c.element();            c.output(KV.of(null, elem));        }    })).apply(GroupByKey.create()).apply(ParDo.of(new DoFn<KV<Void, Iterable<String>>, String>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            KV<Void, Iterable<String>> elem = c.element();            StringBuilder str = new StringBuilder();            str.append("k: " + elem.getKey() + " v:");            for (String v : elem.getValue()) {                str.append(" " + v);            }            c.output(str.toString());        }    }));    output.apply(TextIO.write().to(resultPath));    p.run();}
public void beam_f5923_0(ProcessContext c) throws Exception
{    String elem = c.element();    c.output(KV.of(null, elem));}
public void beam_f5932_0(ProcessContext c) throws Exception
{    TableRow row = c.element();    long timestamp = (Integer) row.get("timestamp");    String userName = (String) row.get("contributor_username");    if (userName != null) {                c.outputWithTimestamp(userName, new Instant(timestamp * 1000L));    }}
public void beam_f5933_0(ProcessContext c) throws Exception
{    KV<String, Long> el = c.element();    String out = "user: " + el.getKey() + " value:" + el.getValue();    c.output(out);}
public void beam_f5942_0() throws Exception
{    if (once) {        return;    }        receiverFactory.create("one").accept(three);    receiverFactory.create("two").accept(four);    receiverFactory.create("three").accept(five);    once = true;}
public ProcessBundleDescriptors.ExecutableProcessBundleDescriptor beam_f5943_0()
{    return processBundleDescriptor;}
private static OperatorStateStore beam_f5953_0(ListState<T> listState) throws Exception
{    OperatorStateStore mock = Mockito.mock(OperatorStateStore.class);    when(mock.getListState(Matchers.any(ListStateDescriptor.class))).thenReturn(listState);    return mock;}
private static ListState<T> beam_f5954_0(List<T> initialState) throws Exception
{    ListState mock = Mockito.mock(ListState.class);    when(mock.get()).thenReturn(initialState);    return mock;}
public void beam_f5963_0(OnTimerContext context)
{    assertEquals("Timer timestamp must match set timestamp.", timerTimestamp, context.timestamp());    context.outputWithTimestamp(eventTimeMessage, context.timestamp());}
public void beam_f5964_0(OnTimerContext context)
{    assertEquals(    "Timer timestamp must match current input watermark", timerTimestamp.plus(1), context.timestamp());    context.outputWithTimestamp(processingTimeMessage, context.timestamp());}
public void beam_f5973_0() throws Exception
{    DoFn<KV<String, Long>, KV<String, Long>> filterElementsEqualToCountFn = new DoFn<KV<String, Long>, KV<String, Long>>() {        @StateId("counter")        private final StateSpec<ValueState<Long>> counterSpec = StateSpecs.value(VarLongCoder.of());        @ProcessElement        public void processElement(ProcessContext context, @StateId("counter") ValueState<Long> count) {            long currentCount = Optional.ofNullable(count.read()).orElse(0L);            currentCount = currentCount + 1;            count.write(currentCount);            KV<String, Long> currentElement = context.element();            if (currentCount == currentElement.getValue()) {                context.output(currentElement);            }        }    };    WindowingStrategy<Object, GlobalWindow> windowingStrategy = WindowingStrategy.globalDefault();    TupleTag<KV<String, Long>> outputTag = new TupleTag<>("main-output");    FullWindowedValueCoder<KV<String, Long>> kvCoder = WindowedValue.getFullCoder(KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of()), windowingStrategy.getWindowFn().windowCoder());    CoderTypeInformation<String> keyCoderInfo = new CoderTypeInformation<>(StringUtf8Coder.of());    KeySelector<WindowedValue<KV<String, Long>>, String> keySelector = e -> e.getValue().getKey();    OneInputStreamOperatorTestHarness<WindowedValue<KV<String, Long>>, WindowedValue<KV<String, Long>>> testHarness = createTestHarness(windowingStrategy, filterElementsEqualToCountFn, kvCoder, kvCoder, outputTag, keyCoderInfo, keySelector);    testHarness.open();    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("a", 100L))));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("a", 100L))));    OperatorSubtaskState snapshot = testHarness.snapshot(0, 0);    testHarness.close();    testHarness = createTestHarness(windowingStrategy, filterElementsEqualToCountFn, kvCoder, kvCoder, outputTag, keyCoderInfo, keySelector);    testHarness.initializeState(snapshot);    testHarness.open();        testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("a", 100L))));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("a", 4L))));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("a", 5L))));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("a", 100L))));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow(KV.of("a", 4L)), WindowedValue.valueInGlobalWindow(KV.of("a", 5L))));    testHarness.close();}
public void beam_f5974_0(ProcessContext context, @StateId("counter") ValueState<Long> count)
{    long currentCount = Optional.ofNullable(count.read()).orElse(0L);    currentCount = currentCount + 1;    count.write(currentCount);    KV<String, Long> currentElement = context.element();    if (currentCount == currentElement.getValue()) {        context.output(currentElement);    }}
public void beam_f5983_0(OnTimerContext context)
{    assertEquals("Timer timestamp must match set timestamp.", timerTimestamp, context.timestamp());    context.outputWithTimestamp(outputMessage, context.timestamp());}
private OneInputStreamOperatorTestHarness<WindowedValue<InT>, WindowedValue<OutT>> beam_f5984_0(WindowingStrategy<Object, ?> windowingStrategy, DoFn<InT, OutT> fn, FullWindowedValueCoder<InT> inputCoder, FullWindowedValueCoder<OutT> outputCoder, TupleTag<OutT> outputTag, TypeInformation<K> keyCoderInfo, KeySelector<WindowedValue<InT>, K> keySelector) throws Exception
{    DoFnOperator<InT, OutT> doFnOperator = new DoFnOperator<>(fn, "stepName", inputCoder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), new DoFnOperator.MultiOutputOutputManagerFactory<>(outputTag, outputCoder), windowingStrategy, new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    PipelineOptionsFactory.as(FlinkPipelineOptions.class), VarIntCoder.of(), /* key coder */    keySelector, DoFnSchemaInformation.create(), Collections.emptyMap());    return new KeyedOneInputStreamOperatorTestHarness<>(doFnOperator, keySelector, keyCoderInfo);}
public void beam_f5993_0() throws Exception
{    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    options.setMaxBundleSize(2L);    options.setCheckpointingInterval(1L);    TupleTag<String> outputTag = new TupleTag<>("main-output");    StringUtf8Coder keyCoder = StringUtf8Coder.of();    KvToByteBufferKeySelector keySelector = new KvToByteBufferKeySelector<>(keyCoder);    KvCoder<String, String> kvCoder = KvCoder.of(keyCoder, StringUtf8Coder.of());    WindowedValue.ValueOnlyWindowedValueCoder<KV<String, String>> windowedValueCoder = WindowedValue.getValueOnlyCoder(kvCoder);    DoFn<KV<String, String>, KV<String, String>> doFn = new DoFn<KV<String, String>, KV<String, String>>() {        @StartBundle        public void startBundle(StartBundleContext context) {            numStartBundleCalled++;        }        @ProcessElement                @RequiresStableInput        public void processElement(ProcessContext context) {            context.output(context.element());        }        @FinishBundle        public void finishBundle(FinishBundleContext context) {            context.output(KV.of("key3", "finishBundle"), BoundedWindow.TIMESTAMP_MIN_VALUE, GlobalWindow.INSTANCE);        }    };    DoFnOperator.MultiOutputOutputManagerFactory<String> outputManagerFactory = new DoFnOperator.MultiOutputOutputManagerFactory(outputTag, WindowedValue.getFullCoder(StringUtf8Coder.of(), GlobalWindow.Coder.INSTANCE));    Supplier<DoFnOperator<KV<String, String>, KV<String, String>>> doFnOperatorSupplier = () -> new DoFnOperator(doFn, "stepName", windowedValueCoder, null, Collections.emptyMap(), outputTag, Collections.emptyList(), outputManagerFactory, WindowingStrategy.globalDefault(), new HashMap<>(), /* side-input mapping */    Collections.emptyList(), /* side inputs */    options, keyCoder, keySelector, DoFnSchemaInformation.create(), Collections.emptyMap());    DoFnOperator<KV<String, String>, KV<String, String>> doFnOperator = doFnOperatorSupplier.get();    OneInputStreamOperatorTestHarness<WindowedValue<KV<String, String>>, WindowedValue<KV<String, String>>> testHarness = new KeyedOneInputStreamOperatorTestHarness(doFnOperator, keySelector, keySelector.getProducedType());    testHarness.open();    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("key", "a"))));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("key", "b"))));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("key2", "c"))));    testHarness.processElement(new StreamRecord<>(WindowedValue.valueInGlobalWindow(KV.of("key2", "d"))));    assertThat(Iterables.size(testHarness.getOutput()), is(0));    OperatorSubtaskState backup = testHarness.snapshot(0, 0);    doFnOperator.notifyCheckpointComplete(0L);    assertThat(numStartBundleCalled, is(1));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow(KV.of("key", "a")), WindowedValue.valueInGlobalWindow(KV.of("key", "b")), WindowedValue.valueInGlobalWindow(KV.of("key2", "c")), WindowedValue.valueInGlobalWindow(KV.of("key2", "d")), WindowedValue.valueInGlobalWindow(KV.of("key3", "finishBundle"))));    doFnOperator = doFnOperatorSupplier.get();    testHarness = new KeyedOneInputStreamOperatorTestHarness(doFnOperator, keySelector, keySelector.getProducedType());        testHarness.initializeState(backup);    testHarness.open();    doFnOperator.notifyCheckpointComplete(0L);    assertThat(numStartBundleCalled, is(2));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow(KV.of("key", "a")), WindowedValue.valueInGlobalWindow(KV.of("key", "b")), WindowedValue.valueInGlobalWindow(KV.of("key2", "c")), WindowedValue.valueInGlobalWindow(KV.of("key2", "d")), WindowedValue.valueInGlobalWindow(KV.of("key3", "finishBundle"))));        doFnOperator.notifyCheckpointComplete(1L);    assertThat(numStartBundleCalled, is(2));    assertThat(stripStreamRecordFromWindowedValue(testHarness.getOutput()), contains(WindowedValue.valueInGlobalWindow(KV.of("key", "a")), WindowedValue.valueInGlobalWindow(KV.of("key", "b")), WindowedValue.valueInGlobalWindow(KV.of("key2", "c")), WindowedValue.valueInGlobalWindow(KV.of("key2", "d")), WindowedValue.valueInGlobalWindow(KV.of("key3", "finishBundle"))));}
public void beam_f5994_0(StartBundleContext context)
{    numStartBundleCalled++;}
public void beam_f6003_0(ProcessContext c) throws Exception
{    if ("one".equals(c.element())) {        c.output(additionalOutput1, "extra: one");    } else if ("two".equals(c.element())) {        c.output(additionalOutput2, "extra: two");    } else {        c.output("got: " + c.element());        c.output(additionalOutput1, "got: " + c.element());        c.output(additionalOutput2, "got: " + c.element());    }}
public void beam_f6004_0(ProcessContext c) throws Exception
{    c.output(c.element());}
public Map<String, FnDataReceiver> beam_f6013_0()
{    return ImmutableMap.of("input", input -> {    /* Ignore input*/    });}
public void beam_f6014_0() throws Exception
{    if (onceEmitted) {        return;    }        receiverFactory.create(mainOutput.getId()).accept(three);    receiverFactory.create(additionalOutput1.getId()).accept(four);    receiverFactory.create(additionalOutput2.getId()).accept(five);    onceEmitted = true;}
private static BeamFnApi.StateRequest beam_f6024_0(ByteString key, String userStateId) throws Exception
{    BeamFnApi.StateRequest.Builder builder = stateRequest(key, userStateId);    builder.setGet(BeamFnApi.StateGetRequest.newBuilder().build());    return builder.build();}
private static BeamFnApi.StateRequest beam_f6025_0(ByteString key, String userStateId) throws Exception
{    BeamFnApi.StateRequest.Builder builder = stateRequest(key, userStateId);    builder.setAppend(BeamFnApi.StateAppendRequest.newBuilder().build());    return builder.build();}
public void beam_f6034_0()
{    Void key = null;    VoidCoder coder = VoidCoder.of();    ByteBuffer byteBuffer = FlinkKeyUtils.encodeKey(key, coder);    assertThat(FlinkKeyUtils.decodeKey(byteBuffer, coder), is(nullValue()));}
public void beam_f6035_0() throws Exception
{    String input = "hello world";    Coder<String> coder = StringUtf8Coder.of();    ByteBuffer encoded = FlinkKeyUtils.encodeKey(input, coder);        assertThat(encoded.array(), is(CoderUtils.encodeToByteArray(coder, input, Coder.Context.NESTED)));}
 void beam_f6044_0()
{    haltEmission = false;}
public List<TestCountingSource> beam_f6045_0(int desiredNumSplits, PipelineOptions options)
{    List<TestCountingSource> splits = new ArrayList<>();    int actualNumSplits = (fixedNumSplits == -1) ? desiredNumSplits : fixedNumSplits;    for (int i = 0; i < actualNumSplits; i++) {        splits.add(withShardNumber(i));    }    return splits;}
public TestCountingSource beam_f6055_0()
{    return TestCountingSource.this;}
public Instant beam_f6056_0()
{    if (current >= numMessagesPerShard - 1) {                return new Instant(BoundedWindow.TIMESTAMP_MAX_VALUE);    }        return new Instant(current + 1);}
public int beam_f6065_0()
{    return ToCounterMark.class.hashCode();}
public boolean beam_f6066_0(Object obj)
{    return obj instanceof ToCounterMark;}
public void beam_f6079_0(StreamRecord<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> windowedValueStreamRecord)
{    minElementsCountdown.countDown();}
public void beam_f6081_0() throws Exception
{    final int numElements = 20;    final Object checkpointLock = new Object();    PipelineOptions options = PipelineOptionsFactory.create();                TestCountingSource source = new TestCountingSource(numElements);    UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark> flinkWrapper = new UnboundedSourceWrapper<>("stepName", options, source, numSplits);    assertEquals(numSplits, flinkWrapper.getSplitSources().size());    StreamSource<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> sourceOperator = new StreamSource<>(flinkWrapper);    AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> testHarness = new AbstractStreamOperatorTestHarness<>(sourceOperator, numTasks, /* max parallelism */    numTasks, /* parallelism */    0);    testHarness.setTimeCharacteristic(TimeCharacteristic.EventTime);    final Set<KV<Integer, Integer>> emittedElements = new HashSet<>();    boolean readFirstBatchOfElements = false;    try {        testHarness.open();        sourceOperator.run(checkpointLock, new TestStreamStatusMaintainer(), new Output<StreamRecord<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>>>() {            private int count = 0;            @Override            public void emitWatermark(Watermark watermark) {            }            @Override            public <X> void collect(OutputTag<X> outputTag, StreamRecord<X> streamRecord) {                collect((StreamRecord) streamRecord);            }            @Override            public void emitLatencyMarker(LatencyMarker latencyMarker) {            }            @Override            public void collect(StreamRecord<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> windowedValueStreamRecord) {                emittedElements.add(windowedValueStreamRecord.getValue().getValue().getValue());                count++;                if (count >= numElements / 2) {                    throw new SuccessException();                }            }            @Override            public void close() {            }        });    } catch (SuccessException e) {                readFirstBatchOfElements = true;    }    assertTrue("Did not successfully read first batch of elements.", readFirstBatchOfElements);        OperatorSubtaskState snapshot = testHarness.snapshot(0, 0);        final ArrayList<Integer> finalizeList = new ArrayList<>();    TestCountingSource.setFinalizeTracker(finalizeList);    testHarness.notifyOfCompletedCheckpoint(0);    assertEquals(flinkWrapper.getLocalSplitSources().size(), finalizeList.size());        TestCountingSource restoredSource = new TestCountingSource(numElements);    UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark> restoredFlinkWrapper = new UnboundedSourceWrapper<>("stepName", options, restoredSource, numSplits);    assertEquals(numSplits, restoredFlinkWrapper.getSplitSources().size());    StreamSource<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>, UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark>> restoredSourceOperator = new StreamSource<>(restoredFlinkWrapper);        AbstractStreamOperatorTestHarness<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> restoredTestHarness = new AbstractStreamOperatorTestHarness<>(restoredSourceOperator, numTasks, /* max parallelism */    1, /* parallelism */    0);    restoredTestHarness.setTimeCharacteristic(TimeCharacteristic.EventTime);        restoredTestHarness.initializeState(snapshot);    boolean readSecondBatchOfElements = false;        try {        restoredTestHarness.open();        restoredSourceOperator.run(checkpointLock, new TestStreamStatusMaintainer(), new Output<StreamRecord<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>>>() {            private int count = 0;            @Override            public void emitWatermark(Watermark watermark) {            }            @Override            public <X> void collect(OutputTag<X> outputTag, StreamRecord<X> streamRecord) {                collect((StreamRecord) streamRecord);            }            @Override            public void emitLatencyMarker(LatencyMarker latencyMarker) {            }            @Override            public void collect(StreamRecord<WindowedValue<ValueWithRecordId<KV<Integer, Integer>>>> windowedValueStreamRecord) {                emittedElements.add(windowedValueStreamRecord.getValue().getValue().getValue());                count++;                if (count >= numElements / 2) {                    throw new SuccessException();                }            }            @Override            public void close() {            }        });    } catch (SuccessException e) {                readSecondBatchOfElements = true;    }    assertEquals(Math.max(1, numSplits / numTasks), restoredFlinkWrapper.getLocalSplitSources().size());    assertTrue("Did not successfully read second batch of elements.", readSecondBatchOfElements);        assertTrue(emittedElements.size() == numElements);}
public void beam_f6096_0() throws Exception
{    testSourceDoesNotShutdown(true);}
private static void beam_f6097_1(boolean shouldHaveReaders) throws Exception
{    final int parallelism = 2;    FlinkPipelineOptions options = PipelineOptionsFactory.as(FlinkPipelineOptions.class);    TestCountingSource source = new TestCountingSource(20).withoutSplitting();    UnboundedSourceWrapper<KV<Integer, Integer>, TestCountingSource.CounterMark> sourceWrapper = new UnboundedSourceWrapper<>("noReader", options, source, parallelism);    StreamingRuntimeContext mock = Mockito.mock(StreamingRuntimeContext.class);    if (shouldHaveReaders) {                Mockito.when(mock.getIndexOfThisSubtask()).thenReturn(0);    } else {                Mockito.when(mock.getIndexOfThisSubtask()).thenReturn(parallelism - 1);    }    Mockito.when(mock.getNumberOfParallelSubtasks()).thenReturn(parallelism);    Mockito.when(mock.getExecutionConfig()).thenReturn(new ExecutionConfig());    ProcessingTimeService timerService = Mockito.mock(ProcessingTimeService.class);    Mockito.when(timerService.getCurrentProcessingTime()).thenReturn(Long.MAX_VALUE);    Mockito.when(mock.getProcessingTimeService()).thenReturn(timerService);    sourceWrapper.setRuntimeContext(mock);    sourceWrapper.open(new Configuration());    SourceFunction.SourceContext sourceContext = Mockito.mock(SourceFunction.SourceContext.class);    Object checkpointLock = new Object();    Mockito.when(sourceContext.getCheckpointLock()).thenReturn(checkpointLock);                sourceWrapper.setSourceContext(sourceContext);    sourceWrapper.open(new Configuration());    assertThat(sourceWrapper.getLocalReaders().isEmpty(), is(!shouldHaveReaders));    Thread thread = new Thread(() -> {        try {            sourceWrapper.run(sourceContext);        } catch (Exception e) {                    }    });    try {        thread.start();                if (!shouldHaveReaders) {                        while (true) {                StackTraceElement[] callStack = thread.getStackTrace();                if (callStack.length >= 2 && "sleep".equals(callStack[0].getMethodName()) && "finalizeSource".equals(callStack[1].getMethodName())) {                    break;                }                Thread.sleep(10);            }        }                assertThat(sourceWrapper.isRunning(), is(true));        synchronized (checkpointLock) {                                    sourceWrapper.onProcessingTime(42);        }                assertThat(sourceWrapper.isRunning(), is(true));        assertThat(thread.isAlive(), is(true));        sourceWrapper.cancel();    } finally {        thread.interrupt();                thread.join(1000);    }    assertThat(thread.isAlive(), is(false));}
 static Iterable<WindowedValue<T>> beam_f6109_0(Iterable<Object> input)
{    return FluentIterable.from(input).filter(o -> o instanceof StreamRecord && ((StreamRecord) o).getValue() instanceof WindowedValue).transform(new Function<Object, WindowedValue<T>>() {        @Nullable        @Override        @SuppressWarnings({ "unchecked", "rawtypes" })        public WindowedValue<T> apply(@Nullable Object o) {            if (o instanceof StreamRecord && ((StreamRecord) o).getValue() instanceof WindowedValue) {                return (WindowedValue) ((StreamRecord) o).getValue();            }            throw new RuntimeException("unreachable");        }    });}
public WindowedValue<T> beam_f6110_0(@Nullable Object o)
{    if (o instanceof StreamRecord && ((StreamRecord) o).getValue() instanceof WindowedValue) {        return (WindowedValue) ((StreamRecord) o).getValue();    }    throw new RuntimeException("unreachable");}
 ItemBuilder beam_f6119_0(long timestamp)
{    this.timestamp = timestamp;    return this;}
 ItemBuilder beam_f6120_0(IntervalWindow window)
{    this.window = window;    return this;}
public static GearpumpRunner beam_f6129_0(PipelineOptions options)
{    GearpumpPipelineOptions pipelineOptions = PipelineOptionsValidator.validate(GearpumpPipelineOptions.class, options);    return new GearpumpRunner(pipelineOptions);}
public GearpumpPipelineResult beam_f6130_0(Pipeline pipeline)
{    String appName = options.getApplicationName();    if (null == appName) {        appName = DEFAULT_APPNAME;    }    Config config = registerSerializers(ClusterConfig.defaultConfig(), options.getSerializers());    if (options.getRemote()) {        RuntimeEnvironment.setRuntimeEnv(new RemoteRuntimeEnvironment());    } else {        RuntimeEnvironment.setRuntimeEnv(new EmbeddedRuntimeEnvironment());        config = config.withValue(Constants.APPLICATION_TOTAL_RETRIES(), ConfigValueFactory.fromAnyRef(0));    }    ClientContext clientContext = ClientContext.apply(config);    options.setClientContext(clientContext);    UserConfig userConfig = UserConfig.empty();    JavaStreamApp streamApp = new JavaStreamApp(appName, clientContext, userConfig);    TranslationContext translationContext = new TranslationContext(streamApp, options);    GearpumpPipelineTranslator translator = new GearpumpPipelineTranslator(translationContext);    translator.translate(pipeline);    RunningApplication app = streamApp.submit();    return new GearpumpPipelineResult(clientContext, app);}
public List<T> beam_f6139_0(List<T> accumulator, T input)
{    accumulator.add(input);    return accumulator;}
public List<T> beam_f6140_0(Iterable<List<T>> accumulators)
{    List<T> result = createAccumulator();    for (List<T> accumulator : accumulators) {        result.addAll(accumulator);    }    return result;}
public void beam_f6149_0(Flatten.PCollections<T> transform, TranslationContext context)
{    JavaStream<T> merged = null;    Set<PCollection<T>> unique = new HashSet<>();    for (PValue input : context.getInputs().values()) {        PCollection<T> collection = (PCollection<T>) input;        JavaStream<T> inputStream = context.getInputStream(collection);        if (null == merged) {            merged = inputStream;        } else {                        if (unique.contains(collection)) {                inputStream = inputStream.map(new DummyFunction<>(), "dummy");            }            merged = merged.merge(inputStream, 1, transform.getName());        }        unique.add(collection);    }    if (null == merged) {        UnboundedSourceWrapper<String, ?> unboundedSourceWrapper = new UnboundedSourceWrapper<>(new ValuesSource<>(Lists.newArrayList("dummy"), StringUtf8Coder.of()), context.getPipelineOptions());        merged = context.getSourceStream(unboundedSourceWrapper);    }    context.setOutputStream(context.getOutput(), merged);}
public T beam_f6150_0(T t)
{    return t;}
public CompositeBehavior beam_f6159_1(TransformHierarchy.Node node)
{        return CompositeBehavior.ENTER_TRANSFORM;}
public void beam_f6160_1(TransformHierarchy.Node node)
{    }
public ByteBuffer beam_f6169_0(WindowedValue<KV<K, V>> wv)
{    try {        return ByteBuffer.wrap(CoderUtils.encodeToByteArray(keyCoder, wv.getValue().getKey()));    } catch (CoderException e) {        throw new RuntimeException(e);    }}
public KV<Instant, WindowedValue<KV<K, V>>> beam_f6170_0(WindowedValue<KV<K, V>> wv)
{    BoundedWindow window = Iterables.getOnlyElement(wv.getWindows());    Instant timestamp = timestampCombiner.assign(window, windowFn.getOutputTime(wv.getTimestamp(), window));    return KV.of(timestamp, wv);}
public void beam_f6179_0()
{    try {        if (reader != null) {            reader.close();        }    } catch (IOException e) {        throw new RuntimeException(e);    }}
public Instant beam_f6180_0()
{    if (reader instanceof UnboundedSource.UnboundedReader) {        org.joda.time.Instant watermark = ((UnboundedSource.UnboundedReader) reader).getWatermark();        if (watermark.equals(BoundedWindow.TIMESTAMP_MAX_VALUE)) {            return Watermark.MAX();        } else {            return TranslatorUtils.jodaTimeToJava8Time(watermark);        }    } else {        if (available) {            return Watermark.MIN();        } else {            return Watermark.MAX();        }    }}
public boolean beam_f6189_0() throws IOException
{    if (iterator.hasNext()) {        current = iterator.next();        return true;    } else {        return false;    }}
public T beam_f6190_0() throws NoSuchElementException
{    return current;}
public void beam_f6200_0(Read.Unbounded<T> transform, TranslationContext context)
{    UnboundedSource<T, ?> unboundedSource = transform.getSource();    UnboundedSourceWrapper<T, ?> unboundedSourceWrapper = new UnboundedSourceWrapper<>(unboundedSource, context.getPipelineOptions());    JavaStream<WindowedValue<T>> sourceStream = context.getSourceStream(unboundedSourceWrapper);    context.setOutputStream(context.getOutput(), sourceStream);}
public void beam_f6201_0(TransformHierarchy.Node treeNode, Pipeline pipeline)
{    this.currentTransform = treeNode.toAppliedPTransform(pipeline);}
public JavaStream<T> beam_f6210_0(DataSource dataSource)
{    return streamApp.source(dataSource, pipelineOptions.getParallelism(), UserConfig.empty(), "source");}
public PushbackSideInputDoFnRunner<InputT, OutputT> beam_f6211_0(ReadyCheckingSideInputReader sideInputReader)
{    PipelineOptions options = serializedOptions.get();    DoFnRunner<InputT, OutputT> underlying = DoFnRunners.simpleRunner(options, fn, sideInputReader, outputManager, mainOutputTag, sideOutputTags, stepContext, null, outputCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping);    return SimplePushbackSideInputDoFnRunner.create(underlying, sideInputs, sideInputReader);}
public List<RawUnionValue> beam_f6220_0()
{    return Lists.newArrayList();}
public List<RawUnionValue> beam_f6221_0(List<RawUnionValue> accumulator, RawUnionValue rawUnionValue)
{    accumulator.add(rawUnionValue);    return accumulator;}
public Iterator<WindowedValue<T>> beam_f6230_0(final WindowedValue<T> value)
{    try {        Collection<BoundedWindow> windows = windowFn.assignWindows(windowFn.new AssignContext() {            @Override            public T element() {                return value.getValue();            }            @Override            public Instant timestamp() {                return value.getTimestamp();            }            @Override            public BoundedWindow window() {                return Iterables.getOnlyElement(value.getWindows());            }        });        List<WindowedValue<T>> values = new ArrayList<>(windows.size());        for (BoundedWindow win : windows) {            values.add(WindowedValue.of(value.getValue(), value.getTimestamp(), win, value.getPane()));        }        return values.iterator();    } catch (Exception e) {        throw new RuntimeException(e);    }}
public T beam_f6231_0()
{    return value.getValue();}
public boolean beam_f6240_0(DataSource o)
{    return o instanceof UnboundedSourceWrapper;}
public void beam_f6241_0()
{    PCollection mockOutput = mock(PCollection.class);    TranslationContext translationContext = mock(TranslationContext.class);    when(translationContext.getInputs()).thenReturn(Collections.EMPTY_MAP);    when(translationContext.getOutput()).thenReturn(mockOutput);    when(translationContext.getPipelineOptions()).thenReturn(PipelineOptionsFactory.as(GearpumpPipelineOptions.class));    translator.translate(transform, translationContext);    verify(translationContext).getSourceStream(argThat(new UnboundedSourceWrapperMatcher()));}
public void beam_f6250_0()
{    WindowFn slidingWindows = Sessions.withGapDuration(Duration.millis(10));    GroupByKeyTranslator.Merge merge = new GroupByKeyTranslator.Merge(slidingWindows, timestampCombiner);    org.joda.time.Instant key1 = new org.joda.time.Instant(5);    WindowedValue<KV<String, String>> value1 = WindowedValue.of(KV.of("key1", "value1"), key1, new IntervalWindow(new org.joda.time.Instant(5), new org.joda.time.Instant(10)), PaneInfo.NO_FIRING);    org.joda.time.Instant key2 = new org.joda.time.Instant(10);    WindowedValue<KV<String, String>> value2 = WindowedValue.of(KV.of("key2", "value2"), key2, new IntervalWindow(new org.joda.time.Instant(9), new org.joda.time.Instant(14)), PaneInfo.NO_FIRING);    KV<org.joda.time.Instant, WindowedValue<KV<String, List<String>>>> result1 = merge.fold(KV.<org.joda.time.Instant, WindowedValue<KV<String, List<String>>>>of(null, null), KV.of(key1, value1));    assertThat(result1.getKey(), equalTo(key1));    assertThat(result1.getValue().getValue().getValue(), equalTo(Lists.newArrayList("value1")));    KV<org.joda.time.Instant, WindowedValue<KV<String, List<String>>>> result2 = merge.fold(result1, KV.of(key2, value2));    assertThat(result2.getKey(), equalTo(timestampCombiner.combine(key1, key2)));    Collection<? extends BoundedWindow> resultWindows = result2.getValue().getWindows();    assertThat(resultWindows.size(), equalTo(1));    IntervalWindow expectedWindow = new IntervalWindow(new org.joda.time.Instant(5), new org.joda.time.Instant(14));    assertThat(resultWindows.toArray()[0], equalTo(expectedWindow));    assertThat(result2.getValue().getValue().getValue(), equalTo(Lists.newArrayList("value1", "value2")));}
protected Source.Reader<T> beam_f6251_0(PipelineOptions options) throws IOException
{    return this.valuesSource.createReader(options, null);}
public void beam_f6260_0()
{    assertThat(TranslatorUtils.boundedWindowToGearpumpWindow(new IntervalWindow(new org.joda.time.Instant(0), new org.joda.time.Instant(Long.MAX_VALUE))), equalTo(Window.apply(Instant.EPOCH, Instant.ofEpochMilli(Long.MAX_VALUE))));    assertThat(TranslatorUtils.boundedWindowToGearpumpWindow(new IntervalWindow(new org.joda.time.Instant(Long.MIN_VALUE), new org.joda.time.Instant(Long.MAX_VALUE))), equalTo(Window.apply(Instant.ofEpochMilli(Long.MIN_VALUE), Instant.ofEpochMilli(Long.MAX_VALUE))));    BoundedWindow globalWindow = GlobalWindow.INSTANCE;    assertThat(TranslatorUtils.boundedWindowToGearpumpWindow(globalWindow), equalTo(Window.apply(Instant.ofEpochMilli(BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis()), Instant.ofEpochMilli(globalWindow.maxTimestamp().getMillis() + 1))));}
public void beam_f6261_0()
{    WindowFn slidingWindows = SlidingWindows.of(Duration.millis(10)).every(Duration.millis(5));    WindowAssignTranslator.AssignWindows<String> assignWindows = new WindowAssignTranslator.AssignWindows(slidingWindows);    String value = "v1";    Instant timestamp = new Instant(1);    WindowedValue<String> windowedValue = WindowedValue.timestampedValueInGlobalWindow(value, timestamp);    ArrayList<WindowedValue<String>> expected = new ArrayList<>();    expected.add(WindowedValue.of(value, timestamp, new IntervalWindow(new Instant(0), new Instant(10)), PaneInfo.NO_FIRING));    expected.add(WindowedValue.of(value, timestamp, new IntervalWindow(new Instant(-5), new Instant(5)), PaneInfo.NO_FIRING));    Iterator<WindowedValue<String>> result = assignWindows.flatMap(windowedValue);    assertThat(expected, equalTo(Lists.newArrayList(result)));}
public Map<PValue, ReplacementOutput> beam_f6270_0(Map<TupleTag<?>, PValue> outputs, PCollectionTuple newOutput)
{    return ReplacementOutputs.tagged(outputs, newOutput);}
 ParDo.SingleOutput<KV<K, InputT>, OutputT> beam_f6271_0()
{    return originalParDo;}
public void beam_f6280_0(final ProcessContext c, final BoundedWindow window)
{    throw new UnsupportedOperationException("BatchStatefulDoFn.ProcessElement should never be invoked");}
public void beam_f6281_0()
{    DoFnInvokers.invokerFor(underlyingDoFn).invokeTeardown();}
public PCollection<KV<Integer, Iterable<KV<KV<K, W>, WindowedValue<V>>>>> beam_f6290_0(PCollection<KV<K, V>> input)
{    @SuppressWarnings("unchecked")    Coder<W> windowCoder = (Coder<W>) input.getWindowingStrategy().getWindowFn().windowCoder();    @SuppressWarnings("unchecked")    KvCoder<K, V> inputCoder = (KvCoder<K, V>) input.getCoder();    PCollection<KV<Integer, KV<KV<K, W>, WindowedValue<V>>>> keyedByHash;    keyedByHash = input.apply(ParDo.of(new GroupByKeyHashAndSortByKeyAndWindowDoFn<K, V, W>(coder)));    keyedByHash.setCoder(KvCoder.of(VarIntCoder.of(), KvCoder.of(KvCoder.of(inputCoder.getKeyCoder(), windowCoder), FullWindowedValueCoder.of(inputCoder.getValueCoder(), windowCoder))));    return keyedByHash.apply(new GroupByKeyAndSortValuesOnly<>());}
public void beam_f6291_0(ProcessContext c) throws Exception
{    long currentKeyIndex = 0;        long currentUniqueKeyCounter = 1;    Iterator<KV<KV<K, W>, WindowedValue<V>>> iterator = c.element().getValue().iterator();    KV<KV<K, W>, WindowedValue<V>> currentValue = iterator.next();    Object currentKeyStructuralValue = keyCoder.structuralValue(currentValue.getKey().getKey());    Object currentWindowStructuralValue = windowCoder.structuralValue(currentValue.getKey().getValue());    while (iterator.hasNext()) {        KV<KV<K, W>, WindowedValue<V>> nextValue = iterator.next();        Object nextKeyStructuralValue = keyCoder.structuralValue(nextValue.getKey().getKey());        Object nextWindowStructuralValue = windowCoder.structuralValue(nextValue.getKey().getValue());        outputDataRecord(c, currentValue, currentKeyIndex);        final long nextKeyIndex;        final long nextUniqueKeyCounter;                if (!currentWindowStructuralValue.equals(nextWindowStructuralValue)) {                                                outputMetadataRecordForSize(c, currentValue, currentUniqueKeyCounter);            outputMetadataRecordForEntrySet(c, currentValue);            nextKeyIndex = 0;            nextUniqueKeyCounter = 1;        } else if (!currentKeyStructuralValue.equals(nextKeyStructuralValue)) {                                    outputMetadataRecordForEntrySet(c, currentValue);            nextKeyIndex = 0;            nextUniqueKeyCounter = currentUniqueKeyCounter + 1;        } else if (!uniqueKeysExpected) {                                    nextKeyIndex = currentKeyIndex + 1;            nextUniqueKeyCounter = currentUniqueKeyCounter;        } else {            throw new IllegalStateException(String.format("Unique keys are expected but found key %s with values %s and %s in window %s.", currentValue.getKey().getKey(), currentValue.getValue().getValue(), nextValue.getValue().getValue(), currentValue.getKey().getValue()));        }        currentValue = nextValue;        currentWindowStructuralValue = nextWindowStructuralValue;        currentKeyStructuralValue = nextKeyStructuralValue;        currentKeyIndex = nextKeyIndex;        currentUniqueKeyCounter = nextUniqueKeyCounter;    }    outputDataRecord(c, currentValue, currentKeyIndex);    outputMetadataRecordForSize(c, currentValue, currentUniqueKeyCounter);            outputMetadataRecordForEntrySet(c, currentValue);}
private PCollection<?> beam_f6300_0(PCollection<KV<K, V>> input)
{    @SuppressWarnings("unchecked")    Coder<W> windowCoder = (Coder<W>) input.getWindowingStrategy().getWindowFn().windowCoder();    @SuppressWarnings({ "rawtypes", "unchecked" })    KvCoder<K, V> inputCoder = (KvCoder) input.getCoder();    @SuppressWarnings({ "unchecked", "rawtypes" })    Coder<Function<Iterable<WindowedValue<V>>, Iterable<V>>> transformCoder = (Coder) SerializableCoder.of(IterableWithWindowedValuesToIterable.class);    Coder<TransformedMap<K, Iterable<WindowedValue<V>>, Iterable<V>>> finalValueCoder = TransformedMapCoder.of(transformCoder, MapCoder.of(inputCoder.getKeyCoder(), IterableCoder.of(FullWindowedValueCoder.of(inputCoder.getValueCoder(), windowCoder))));    return BatchViewAsSingleton.applyForSingleton(runner, input, new ToMultimapDoFn<>(windowCoder), finalValueCoder, view);}
private static PCollection<?> beam_f6301_0(DataflowRunner runner, PCollection<KV<K, V>> input, PCollectionView<ViewT> view, boolean uniqueKeysExpected) throws NonDeterministicException
{    @SuppressWarnings("unchecked")    Coder<W> windowCoder = (Coder<W>) input.getWindowingStrategy().getWindowFn().windowCoder();    @SuppressWarnings({ "rawtypes", "unchecked" })    KvCoder<K, V> inputCoder = (KvCoder) input.getCoder();            inputCoder.getKeyCoder().verifyDeterministic();    IsmRecordCoder<WindowedValue<V>> ismCoder = coderForMapLike(windowCoder, inputCoder.getKeyCoder(), inputCoder.getValueCoder());            TupleTag<IsmRecord<WindowedValue<V>>> mainOutputTag = new TupleTag<>();    TupleTag<KV<Integer, KV<W, Long>>> outputForSizeTag = new TupleTag<>();    TupleTag<KV<Integer, KV<W, K>>> outputForEntrySetTag = new TupleTag<>();            PCollectionTuple outputTuple = input.apply("GBKaSVForData", new GroupByKeyHashAndSortByKeyAndWindow<K, V, W>(ismCoder)).apply(ParDo.of(new ToIsmRecordForMapLikeDoFn<>(outputForSizeTag, outputForEntrySetTag, windowCoder, inputCoder.getKeyCoder(), ismCoder, uniqueKeysExpected)).withOutputTags(mainOutputTag, TupleTagList.of(ImmutableList.of(outputForSizeTag, outputForEntrySetTag))));        PCollection<IsmRecord<WindowedValue<V>>> perHashWithReifiedWindows = outputTuple.get(mainOutputTag);    perHashWithReifiedWindows.setCoder(ismCoder);                PCollection<KV<Integer, KV<W, Long>>> outputForSize = outputTuple.get(outputForSizeTag);    outputForSize.setCoder(KvCoder.of(VarIntCoder.of(), KvCoder.of(windowCoder, VarLongCoder.of())));    PCollection<IsmRecord<WindowedValue<V>>> windowMapSizeMetadata = outputForSize.apply("GBKaSVForSize", new GroupByKeyAndSortValuesOnly<>()).apply(ParDo.of(new ToIsmMetadataRecordForSizeDoFn<K, V, W>(windowCoder)));    windowMapSizeMetadata.setCoder(ismCoder);            PCollection<KV<Integer, KV<W, K>>> outputForEntrySet = outputTuple.get(outputForEntrySetTag);    outputForEntrySet.setCoder(KvCoder.of(VarIntCoder.of(), KvCoder.of(windowCoder, inputCoder.getKeyCoder())));    PCollection<IsmRecord<WindowedValue<V>>> windowMapKeysMetadata = outputForEntrySet.apply("GBKaSVForKeys", new GroupByKeyAndSortValuesOnly<>()).apply(ParDo.of(new ToIsmMetadataRecordForKeyDoFn<K, V, W>(inputCoder.getKeyCoder(), windowCoder)));    windowMapKeysMetadata.setCoder(ismCoder);        runner.addPCollectionRequiringIndexedFormat(perHashWithReifiedWindows);    runner.addPCollectionRequiringIndexedFormat(windowMapSizeMetadata);    runner.addPCollectionRequiringIndexedFormat(windowMapKeysMetadata);    PCollectionList<IsmRecord<WindowedValue<V>>> outputs = PCollectionList.of(ImmutableList.of(perHashWithReifiedWindows, windowMapSizeMetadata, windowMapKeysMetadata));    PCollection<IsmRecord<WindowedValue<V>>> flattenedOutputs = Pipeline.applyTransform(outputs, Flatten.pCollections());    flattenedOutputs.apply(CreateDataflowView.forBatch(view));    return flattenedOutputs;}
public void beam_f6310_0(ProcessContext c) throws Exception
{    c.output(IsmRecord.of(ImmutableList.of(GlobalWindow.INSTANCE, indexInBundle), WindowedValue.of(c.element(), c.timestamp(), GlobalWindow.INSTANCE, c.pane())));    indexInBundle += 1;}
public void beam_f6311_0(ProcessContext c) throws Exception
{    long elementsInWindow = 0;    Optional<Object> previousWindowStructuralValue = Optional.absent();    for (KV<W, WindowedValue<T>> value : c.element().getValue()) {        Object currentWindowStructuralValue = windowCoder.structuralValue(value.getKey());                if (previousWindowStructuralValue.isPresent() && !previousWindowStructuralValue.get().equals(currentWindowStructuralValue)) {                        elementsInWindow = 0;        }        c.output(IsmRecord.of(ImmutableList.of(value.getKey(), elementsInWindow), value.getValue()));        previousWindowStructuralValue = Optional.of(currentWindowStructuralValue);        elementsInWindow += 1;    }}
public Iterable<V> beam_f6320_0(@Nonnull Iterable<WindowedValue<V>> input)
{    return Iterables.transform(input, WindowedValueToValue.of());}
public void beam_f6321_0(ProcessContext c, BoundedWindow untypedWindow) throws Exception
{    @SuppressWarnings("unchecked")    W window = (W) untypedWindow;    c.output(KV.of(ismCoderForHash.hash(ImmutableList.of(window)), KV.of(window, WindowedValue.of(c.element(), c.timestamp(), window, c.pane()))));}
private static WindowedValue<T> beam_f6330_0(T value)
{    return new ValueInEmptyWindows<>(value);}
public WindowedValue<NewT> beam_f6331_0(NewT value)
{    return new ValueInEmptyWindows<>(value);}
public static CreateDataflowView<ElemT, ViewT> beam_f6340_0(PCollectionView<ViewT> view)
{    return new CreateDataflowView<>(view, true);}
public PCollection<ElemT> beam_f6341_0(PCollection<ElemT> input)
{    if (streaming) {        return PCollection.createPrimitiveOutputInternal(input.getPipeline(), input.getWindowingStrategy(), input.isBounded(), input.getCoder());    }    return (PCollection) view.getPCollection();}
public LeaseWorkItemResponse beam_f6350_0(@Nonnull String jobId, @Nonnull LeaseWorkItemRequest request) throws IOException
{    checkNotNull(jobId, "jobId");    checkNotNull(request, "request");    Jobs.WorkItems.Lease jobWorkItemsLease = dataflow.projects().locations().jobs().workItems().lease(options.getProject(), options.getRegion(), jobId, request);    return jobWorkItemsLease.execute();}
public ReportWorkItemStatusResponse beam_f6351_0(@Nonnull String jobId, @Nonnull ReportWorkItemStatusRequest request) throws IOException
{    checkNotNull(jobId, "jobId");    checkNotNull(request, "request");    Jobs.WorkItems.ReportStatus jobWorkItemsReportStatus = dataflow.projects().locations().jobs().workItems().reportStatus(options.getProject(), options.getRegion(), jobId, request);    return jobWorkItemsReportStatus.execute();}
public Iterable<MetricResult<Long>> beam_f6360_0()
{    return counterResults.build();}
public Iterable<MetricResult<GaugeResult>> beam_f6361_0()
{    return gaugeResults.build();}
public String beam_f6370_0()
{    return dataflowOptions.getRegion();}
public DataflowPipelineJob beam_f6371_0()
{    if (terminalState == null) {        throw new IllegalStateException("getReplacedByJob() called before job terminated");    }    if (replacedByJob == null) {        throw new IllegalStateException("getReplacedByJob() called for job that was not replaced");    }    return replacedByJob;}
public State beam_f6381_1() throws IOException
{                                            FutureTask<State> tentativeCancelTask = new FutureTask<>(() -> {        Job content = new Job();        content.setProjectId(getProjectId());        String currentJobId = getJobId();        content.setId(currentJobId);        content.setRequestedState("JOB_STATE_CANCELLED");        try {            Job job = dataflowClient.updateJob(currentJobId, content);            return MonitoringUtil.toState(job.getCurrentState());        } catch (IOException e) {            State state = getState();            if (state.isTerminal()) {                                return state;            } else if (e.getMessage().contains("has terminated")) {                                                                                                                                                                                                return state;            } else {                String errorMsg = String.format("Failed to cancel job in state %s, " + "please go to the Developers Console to cancel it manually: %s", state, MonitoringUtil.getJobMonitoringPageURL(getProjectId(), getRegion(), getJobId()));                                throw new IOException(errorMsg, e);            }        }    });    if (cancelState.compareAndSet(null, tentativeCancelTask)) {                        cancelState.get().run();    }    try {        return cancelState.get().get();    } catch (InterruptedException | ExecutionException e) {        throw new IOException(e);    }}
public State beam_f6382_0()
{    if (terminalState != null) {        return terminalState;    }    return getStateWithRetriesOrUnknownOnException(BackOffAdapter.toGcpBackOff(STATUS_BACKOFF_FACTORY.backoff()), Sleeper.DEFAULT);}
public static DataflowPipelineTranslator beam_f6391_0(DataflowPipelineOptions options)
{    return new DataflowPipelineTranslator(options);}
public JobSpecification beam_f6392_1(Pipeline pipeline, DataflowRunner runner, List<DataflowPackage> packages)
{        SdkComponents sdkComponents = SdkComponents.create();    String workerHarnessContainerImageURL = DataflowRunner.getContainerImageForJob(options.as(DataflowPipelineOptions.class));    RunnerApi.Environment defaultEnvironmentForDataflow = Environments.createDockerEnvironment(workerHarnessContainerImageURL);    sdkComponents.registerEnvironment(defaultEnvironmentForDataflow);    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(pipeline, sdkComponents, true);        Translator translator = new Translator(pipeline, runner, sdkComponents);    Job result = translator.translate(packages);    return new JobSpecification(result, pipelineProto, Collections.unmodifiableMap(translator.stepNames));}
public DataflowPipelineOptions beam_f6401_0()
{    return options;}
public Map<TupleTag<?>, PValue> beam_f6402_0(PTransform<InputT, ?> transform)
{    return getCurrentTransform(transform).getInputs();}
public StepTranslator beam_f6411_1(PTransform<?, ?> transform, String type)
{    String stepName = genStepName();    if (stepNames.put(getCurrentTransform(transform), stepName) != null) {        throw new IllegalArgumentException(transform + " already has a name specified");    }        List<Step> steps = job.getSteps();    if (steps == null) {        steps = new ArrayList<>();        job.setSteps(steps);    }    Step step = new Step();    step.setName(stepName);    step.setKind(type);    steps.add(step);    StepTranslator stepContext = new StepTranslator(this, step);    stepContext.addInput(PropertyNames.USER_NAME, getFullName(transform));    stepContext.addDisplayData(step, stepName, transform);        return stepContext;}
public OutputReference beam_f6412_0(PValue value, AppliedPTransform<?, ?, ?> producer)
{    String stepName = stepNames.get(producer);    checkArgument(stepName != null, "%s doesn't have a name specified", producer);    String outputName = outputNames.get(value);    checkArgument(outputName != null, "output %s doesn't have a name specified", value);    return new OutputReference(stepName, outputName);}
public void beam_f6421_0(Coder<?> coder)
{    CloudObject encoding = translateCoder(coder, translator);    addObject(getProperties(), PropertyNames.ENCODING, encoding);}
public void beam_f6422_0(String name, Boolean value)
{    addBoolean(getProperties(), name, value);}
private void beam_f6431_0(Step step, String stepName, HasDisplayData hasDisplayData)
{    DisplayData displayData = DisplayData.from(hasDisplayData);    List<Map<String, Object>> list = MAPPER.convertValue(displayData, List.class);    addList(getProperties(), PropertyNames.DISPLAY_DATA, list);}
public String beam_f6432_0()
{    return "DataflowPipelineTranslator#" + hashCode();}
private void beam_f6441_0(Flatten.PCollections<T> transform, TranslationContext context)
{    StepTranslationContext stepContext = context.addStep(transform, "Flatten");    List<OutputReference> inputs = new ArrayList<>();    for (PValue input : context.getInputs(transform).values()) {        inputs.add(context.asOutputReference(input, context.getProducer(input)));    }    stepContext.addInput(PropertyNames.INPUTS, inputs);    stepContext.addOutput(PropertyNames.OUTPUT, context.getOutput(transform));}
public void beam_f6442_0(GroupByKeyAndSortValuesOnly transform, TranslationContext context)
{    groupByKeyAndSortValuesHelper(transform, context);}
private void beam_f6451_0(Window.Assign<T> transform, TranslationContext context)
{    StepTranslationContext stepContext = context.addStep(transform, "Bucket");    PCollection<T> input = context.getInput(transform);    stepContext.addInput(PropertyNames.PARALLEL_INPUT, input);    stepContext.addOutput(PropertyNames.OUTPUT, context.getOutput(transform));    WindowingStrategy<?, ?> strategy = context.getOutput(transform).getWindowingStrategy();    byte[] serializedBytes = serializeWindowingStrategy(strategy, context.getPipelineOptions());    String serializedJson = byteArrayToJsonString(serializedBytes);    stepContext.addInput(PropertyNames.SERIALIZED_FN, serializedJson);}
public void beam_f6452_0(SplittableParDo.ProcessKeyedElements transform, TranslationContext context)
{    translateTyped(transform, context);}
public boolean beam_f6461_0(AppliedPTransform<?, ?, ?> application)
{    return application.getTransform().getClass().equals(Combine.GroupedValues.class) && ((Combine.GroupedValues<?, ?, ?>) application.getTransform()).getSideInputs().isEmpty() && parentIsCombinePerKey(application);}
private boolean beam_f6462_0(AppliedPTransform<?, ?, ?> application)
{            final TransformHierarchy.Node[] parent = new TransformHierarchy.Node[1];                Pipeline pipeline = application.getPipeline();    pipeline.traverseTopologically(new Pipeline.PipelineVisitor.Defaults() {        private ArrayDeque<TransformHierarchy.Node> parents = new ArrayDeque<>();        @Override        public CompositeBehavior enterCompositeTransform(TransformHierarchy.Node node) {            CompositeBehavior behavior = CompositeBehavior.ENTER_TRANSFORM;                        if (!node.isRootNode() && node.toAppliedPTransform(getPipeline()).equals(application)) {                                if (parents.isEmpty()) {                    parent[0] = null;                } else {                    parent[0] = parents.peekFirst();                }                behavior = CompositeBehavior.DO_NOT_ENTER_TRANSFORM;            }                                    parents.addFirst(node);            return behavior;        }        @Override        public void leaveCompositeTransform(TransformHierarchy.Node node) {            if (!node.isRootNode()) {                parents.removeFirst();            }        }    });    if (parent[0] == null) {        return false;    }            AppliedPTransform<?, ?, ?> appliedParent;    try {        appliedParent = parent[0].toAppliedPTransform(pipeline);    } catch (NullPointerException e) {        return false;    }    return appliedParent.getTransform().getClass().equals(Combine.PerKey.class);}
public void beam_f6472_0(Node node)
{    if (transform.getTransform() == node.getTransform()) {        tracking = false;    }}
public PTransformReplacement<PCollection<InputT>, PValue> beam_f6473_0(final AppliedPTransform<PCollection<InputT>, PValue, PTransform<PCollection<InputT>, PValue>> transform)
{    PTransform<PCollection<InputT>, PValue> rep = InstanceBuilder.ofType(replacement).withArg(DataflowRunner.class, runner).withArg(CreatePCollectionView.class, findCreatePCollectionView(transform)).build();    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), (PTransform) rep);}
private static Map<String, Object> beam_f6482_0(DataflowPipelineOptions options)
{    DataflowRunnerInfo runnerInfo = DataflowRunnerInfo.getDataflowRunnerInfo();    String majorVersion;    String jobType;    if (hasExperiment(options, "beam_fn_api")) {        majorVersion = runnerInfo.getFnApiEnvironmentMajorVersion();        jobType = options.isStreaming() ? "FNAPI_STREAMING" : "FNAPI_BATCH";    } else {        majorVersion = runnerInfo.getLegacyEnvironmentMajorVersion();        jobType = options.isStreaming() ? "STREAMING" : "JAVA_BATCH_AUTOSCALING";    }    return ImmutableMap.of(PropertyNames.ENVIRONMENT_VERSION_MAJOR_KEY, majorVersion, PropertyNames.ENVIRONMENT_VERSION_JOB_TYPE_KEY, jobType);}
protected void beam_f6483_0(Pipeline pipeline)
{    boolean streaming = options.isStreaming() || containsUnboundedPCollection(pipeline);            UnconsumedReads.ensureAllReadsConsumed(pipeline);    pipeline.replaceAll(getOverrides(streaming));}
 void beam_f6494_0(PCollection<?> pcol)
{    pcollectionsRequiringIndexedFormat.add(pcol);}
 void beam_f6495_0(PTransform<?, ?> ptransform)
{    ptransformViewsWithNonDeterministicKeyCoders.add(ptransform);}
public PDone beam_f6504_0(PCollection<PubsubMessage> input)
{    return PDone.in(input.getPipeline());}
protected String beam_f6505_0()
{    return "StreamingPubsubIOWrite";}
public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> beam_f6514_0(AppliedPTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>> transform)
{    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), new BatchGroupIntoBatches(transform.getTransform().getBatchSize()));}
public Map<PValue, ReplacementOutput> beam_f6515_0(Map<TupleTag<?>, PValue> outputs, PCollection<KV<K, Iterable<V>>> newOutput)
{    return ReplacementOutputs.singleton(outputs, newOutput);}
public String beam_f6524_0()
{    return String.format("Read(%s)", NameUtils.approximateSimpleName(source));}
public void beam_f6525_0(ReadWithIds<?> transform, TranslationContext context)
{    ReadTranslator.translateReadHelper(transform.getSource(), transform, context);}
public PCollectionView<?> beam_f6534_0()
{    return view;}
public Coder<T> beam_f6535_0()
{    return dataCoder;}
public Map<PValue, ReplacementOutput> beam_f6544_0(Map<TupleTag<?>, PValue> outputs, PDone newOutput)
{    return Collections.emptyMap();}
public PTransformReplacement<PCollection<UserT>, WriteFilesResult<DestinationT>> beam_f6545_0(AppliedPTransform<PCollection<UserT>, WriteFilesResult<DestinationT>, WriteFiles<UserT, DestinationT, OutputT>> transform)
{                            int numShards;    if (options.getMaxNumWorkers() > 0) {        numShards = options.getMaxNumWorkers() * 2;    } else if (options.getNumWorkers() > 0) {        numShards = options.getNumWorkers() * 2;    } else {        numShards = DEFAULT_NUM_SHARDS;    }    try {        List<PCollectionView<?>> sideInputs = WriteFilesTranslation.getDynamicDestinationSideInputs(transform);        FileBasedSink sink = WriteFilesTranslation.getSink(transform);        WriteFiles<UserT, DestinationT, OutputT> replacement = WriteFiles.to(sink).withSideInputs(sideInputs);        if (WriteFilesTranslation.isWindowedWrites(transform)) {            replacement = replacement.withWindowedWrites();        }        return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), replacement.withNumShards(numShards));    } catch (Exception e) {        throw new RuntimeException(e);    }}
public Map<String, String> beam_f6555_0()
{    return ImmutableMap.copyOf((Map) properties);}
private static Properties beam_f6556_0(String path) throws IOException
{    Properties properties = new Properties();    try (InputStream in = DataflowRunnerInfo.class.getResourceAsStream(path)) {        if (in == null) {            return null;        }        properties.load(in);    }    return properties;}
public byte[] beam_f6565_0()
{    checkState(isMetadataKey(keyComponents()), "This is a value record and not a metadata record.");    return metadata();}
public static IsmRecordCoder<V> beam_f6566_0(int numberOfShardKeyCoders, int numberOfMetadataShardKeyCoders, List<Coder<?>> keyComponentCoders, Coder<V> valueCoder)
{    checkNotNull(keyComponentCoders);    checkArgument(keyComponentCoders.size() > 0);    checkArgument(numberOfShardKeyCoders > 0);    checkArgument(numberOfShardKeyCoders <= keyComponentCoders.size());    checkArgument(numberOfMetadataShardKeyCoders <= keyComponentCoders.size());    return new IsmRecordCoder<>(numberOfShardKeyCoders, numberOfMetadataShardKeyCoders, keyComponentCoders, valueCoder);}
public int beam_f6575_0(List<?> keyComponents, RandomAccessData keyBytesToMutate, List<Integer> keyComponentByteOffsetsToMutate)
{    checkNotNull(keyComponents);    checkArgument(keyComponents.size() <= keyComponentCoders.size(), "Expected at most %s key component(s) but received %s.", keyComponentCoders.size(), keyComponents);    final int numberOfKeyCodersToUse;    final int shardOffset;    if (isMetadataKey(keyComponents)) {        numberOfKeyCodersToUse = numberOfMetadataShardKeyCoders;        shardOffset = SHARD_BITS + 1;    } else {        numberOfKeyCodersToUse = numberOfShardKeyCoders;        shardOffset = 0;    }    checkArgument(numberOfKeyCodersToUse <= keyComponents.size(), "Expected at least %s key component(s) but received %s.", numberOfShardKeyCoders, keyComponents);    try {                for (int i = 0; i < numberOfKeyCodersToUse; ++i) {            getKeyComponentCoder(i).encode(keyComponents.get(i), keyBytesToMutate.asOutputStream(), Context.NESTED);            keyComponentByteOffsetsToMutate.add(keyBytesToMutate.size());        }        int rval = HASH_FUNCTION.hashBytes(keyBytesToMutate.array(), 0, keyBytesToMutate.size()).asInt() & SHARD_BITS;        rval += shardOffset;                for (int i = numberOfKeyCodersToUse; i < keyComponents.size(); ++i) {            getKeyComponentCoder(i).encode(keyComponents.get(i), keyBytesToMutate.asOutputStream(), Context.NESTED);            keyComponentByteOffsetsToMutate.add(keyBytesToMutate.size());        }        return rval;    } catch (IOException e) {        throw new IllegalStateException(String.format("Failed to hash %s with coder %s", keyComponents, this), e);    }}
public List<Coder<?>> beam_f6576_0()
{    return ImmutableList.<Coder<?>>builder().addAll(keyComponentCoders).add(valueCoder).build();}
public boolean beam_f6585_0(Object obj)
{    return this == obj;}
public int beam_f6586_0()
{    return -1248902349;}
public static IsmShard beam_f6595_0(int id, long blockOffset, long indexOffset)
{    IsmShard ismShard = new AutoValue_IsmFormat_IsmShard(id, blockOffset, indexOffset);    checkState(id >= 0, "%s attempting to be written with negative shard id.", ismShard);    checkState(blockOffset >= 0, "%s attempting to be written with negative block offset.", ismShard);    checkState(indexOffset >= 0, "%s attempting to be written with negative index offset.", ismShard);    return ismShard;}
public int beam_f6596_0()
{    return id();}
public static KeyPrefix beam_f6605_0(int sharedKeySize, int unsharedKeySize)
{    return new AutoValue_IsmFormat_KeyPrefix(sharedKeySize, unsharedKeySize);}
public static KeyPrefixCoder beam_f6606_0()
{    return INSTANCE;}
public Footer beam_f6616_0(InputStream inStream) throws CoderException, IOException
{    DataInputStream dataIn = new DataInputStream(inStream);    Footer footer = Footer.of(dataIn.readLong(), dataIn.readLong(), dataIn.readLong());    int version = dataIn.read();    if (version != Footer.VERSION) {        throw new IOException("Unknown version " + version + ". " + "Only version 2 is currently supported.");    }    return footer;}
public boolean beam_f6618_0()
{    return true;}
public WorkerLogLevelOverrides beam_f6627_0(Package pkg, Level level)
{    checkNotNull(pkg, "Expected package to be not null.");    addOverrideForName(pkg.getName(), level);    return this;}
public WorkerLogLevelOverrides beam_f6628_0(String name, Level level)
{    checkNotNull(name, "Expected name to be not null.");    checkNotNull(level, "Expected level to be one of %s.", Arrays.toString(Level.values()));    put(name, level);    return this;}
public static PTransformTranslation.TransformPayloadTranslator beam_f6637_0()
{    return new PayloadTranslator();}
public String beam_f6638_0(ParDoSingle<?, ?> transform)
{    return PAR_DO_TRANSFORM_URN;}
public String beam_f6647_0(SdkComponents newComponents)
{    if (signature.processElement().isSplittable()) {        Coder<?> restrictionCoder = DoFnInvokers.invokerFor(doFn).invokeGetRestrictionCoder(transform.getPipeline().getCoderRegistry());        try {            return newComponents.registerCoder(restrictionCoder);        } catch (IOException e) {            throw new IllegalStateException(String.format("Unable to register restriction coder for %s.", transform.getFullName()), e);        }    }    return "";}
public Map<? extends Class<? extends PTransform>, ? extends PTransformTranslation.TransformPayloadTranslator> beam_f6648_0()
{    return Collections.singletonMap(ParDoSingle.class, new PayloadTranslator());}
public Map<PValue, ReplacementOutput> beam_f6657_0(Map<TupleTag<?>, PValue> outputs, PCollection<OutputT> newOutput)
{    return ReplacementOutputs.singleton(outputs, newOutput);}
public PTransformReplacement<PCollection<InputT>, PCollectionTuple> beam_f6658_0(AppliedPTransform<PCollection<InputT>, PCollectionTuple, ParDo.MultiOutput<InputT, OutputT>> appliedTransform)
{    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(appliedTransform), new PTransform<PCollection<InputT>, PCollectionTuple>() {        @Override        public PCollectionTuple expand(PCollection<InputT> input) {            return input.apply("Materialize input", Reshuffle.viaRandomKey()).apply("ParDo with stable input", appliedTransform.getTransform());        }    });}
public Map<PValue, ReplacementOutput> beam_f6667_0(Map<TupleTag<?>, PValue> outputs, PCollectionTuple newOutput)
{    return ReplacementOutputs.tagged(outputs, newOutput);}
public PTransformReplacement<PCollection<ElemT>, PCollection<ElemT>> beam_f6668_0(AppliedPTransform<PCollection<ElemT>, PCollection<ElemT>, CreatePCollectionView<ElemT, ViewT>> transform)
{    StreamingCreatePCollectionView<ElemT, ViewT> streamingView = new StreamingCreatePCollectionView<>(transform.getTransform().getView());    return PTransformReplacement.of(PTransformReplacements.getSingletonMainInput(transform), streamingView);}
 static TestDataflowRunner beam_f6677_0(TestDataflowPipelineOptions options, DataflowClient client)
{    return new TestDataflowRunner(options, client);}
public DataflowPipelineJob beam_f6678_0(Pipeline pipeline)
{    return run(pipeline, runner);}
public void beam_f6687_1(List<JobMessage> messages)
{    messageHandler.process(messages);    for (JobMessage message : messages) {        if ("JOB_MESSAGE_ERROR".equals(message.getMessageImportance())) {                        errorMessage.append(message.getMessageText());            hasSeenError = true;        }    }}
 boolean beam_f6688_0()
{    return hasSeenError;}
public Class<?> beam_f6697_0()
{    return classes.get(0);}
private static Map<String, CloudKnownType> beam_f6698_0()
{    Map<String, CloudKnownType> result = new HashMap<>();    for (CloudKnownType ty : CloudKnownType.values()) {        result.put(ty.getUri(), ty);    }    return result;}
public static CloudObject beam_f6707_0(Class<?> cls)
{    CloudObject result = new CloudObject();    result.className = checkNotNull(cls).getName();    return result;}
public static CloudObject beam_f6708_0(String className)
{    CloudObject result = new CloudObject();    result.className = checkNotNull(className);    return result;}
public CloudObject beam_f6717_0()
{    return (CloudObject) super.clone();}
private static Map<Class<? extends Coder>, CloudObjectTranslator<? extends Coder>> beam_f6718_0()
{    ImmutableMap.Builder<Class<? extends Coder>, CloudObjectTranslator<? extends Coder>> builder = ImmutableMap.builder();    for (CoderCloudObjectTranslatorRegistrar coderRegistrar : ServiceLoader.load(CoderCloudObjectTranslatorRegistrar.class)) {        builder.putAll(coderRegistrar.classesToTranslators());    }    return builder.build();}
public Class<KvCoder> beam_f6727_0()
{    return KvCoder.class;}
public String beam_f6728_0()
{    return CloudObjectKinds.KIND_PAIR;}
public Class<? extends LengthPrefixCoder> beam_f6737_0()
{    return LengthPrefixCoder.class;}
public String beam_f6738_0()
{    return CloudObjectKinds.KIND_LENGTH_PREFIX;}
public Class<? extends IntervalWindowCoder> beam_f6747_0()
{    return IntervalWindowCoder.class;}
public String beam_f6748_0()
{    return CloudObjectKinds.KIND_INTERVAL_WINDOW;}
public Class<? extends ByteArrayCoder> beam_f6757_0()
{    return ByteArrayCoder.class;}
public String beam_f6758_0()
{    return CloudObjectKinds.KIND_BYTES;}
public Class<? extends CustomCoder> beam_f6767_0()
{    return CustomCoder.class;}
public String beam_f6768_0()
{    return CloudObject.forClass(CustomCoder.class).getClassName();}
public Class<? extends IterableLikeCoder> beam_f6777_0()
{    return clazz;}
public String beam_f6778_0()
{    return CloudObject.forClass(clazz).getClassName();}
public Class<? extends NullableCoder> beam_f6787_0()
{    return NullableCoder.class;}
public String beam_f6788_0()
{    return CloudObject.forClass(NullableCoder.class).getClassName();}
public CoGbkResultCoder beam_f6797_0(CloudObject cloudObject)
{    List<Coder<?>> components = getComponents(cloudObject);    checkArgument(components.size() == 1, "Expected 1 component for %s, got %s", CoGbkResultCoder.class.getSimpleName(), components.size());    checkArgument(components.get(0) instanceof UnionCoder, "Expected only component to be a %s, got %s", UnionCoder.class.getSimpleName(), components.get(0).getClass().getName());    return CoGbkResultCoder.of(schemaFromCloudObject(CloudObject.fromSpec(Structs.getObject(cloudObject, PropertyNames.CO_GBK_RESULT_SCHEMA))), (UnionCoder) components.get(0));}
public Class<? extends CoGbkResultCoder> beam_f6798_0()
{    return CoGbkResultCoder.class;}
private static ApiComponents beam_f6807_0(String urlString)
{    try {        URL url = new URL(urlString);        String rootUrl = url.getProtocol() + "://" + url.getHost() + (url.getPort() > 0 ? ":" + url.getPort() : "");        return new ApiComponents(rootUrl, url.getPath());    } catch (MalformedURLException e) {        throw new RuntimeException("Invalid URL: " + urlString);    }}
public static Dataflow.Builder beam_f6808_0(DataflowPipelineOptions options)
{    String servicePath = options.getDataflowEndpoint();    ApiComponents components;    if (servicePath.contains("://")) {        components = apiComponentsFromUrl(servicePath);    } else {        components = new ApiComponents(options.getApiRootUrl(), servicePath);    }    return new Dataflow.Builder(getTransport(), getJsonFactory(), chainHttpRequestInitializer(options.getGcpCredential(),     new RetryHttpRequestInitializer(ImmutableList.of(404)))).setApplicationName(options.getAppName()).setRootUrl(components.rootUrl).setServicePath(components.servicePath).setGoogleClientRequestInitializer(options.getGoogleApiTrace());}
private GcsCreateOptions beam_f6817_0()
{    int uploadSizeBytes = firstNonNull(options.getGcsUploadBufferSizeBytes(), 1024 * 1024);    checkArgument(uploadSizeBytes > 0, "gcsUploadBufferSizeBytes must be > 0");    uploadSizeBytes = Math.min(uploadSizeBytes, 1024 * 1024);    return GcsCreateOptions.builder().setGcsUploadBufferSizeBytes(uploadSizeBytes).setMimeType(MimeTypes.BINARY).build();}
public void beam_f6818_1(List<JobMessage> messages)
{    for (JobMessage message : messages) {        if (Strings.isNullOrEmpty(message.getMessageText())) {            continue;        }        @Nullable        Instant time = fromCloudTime(message.getTime());        String logMessage = (time == null ? "UNKNOWN TIMESTAMP: " : time + ": ") + message.getMessageText();        switch(message.getMessageImportance()) {            case JOB_MESSAGE_ERROR:                                break;            case JOB_MESSAGE_WARNING:                                break;            case JOB_MESSAGE_BASIC:            case JOB_MESSAGE_DETAILED:                                break;            case JOB_MESSAGE_DEBUG:                                break;            default:                LOG.trace(logMessage);        }    }}
public void beam_f6827_0()
{    executorService.shutdown();}
public int beam_f6828_0(PackageAttributes o1, PackageAttributes o2)
{        long sizeDiff = o2.getSize() - o1.getSize();    if (sizeDiff != 0) {                return Long.signum(sizeDiff);    }        return o1.getHash().compareTo(o2.getHash());}
public DataflowPackage beam_f6837_0(byte[] bytes, String target, String stagingPath, CreateOptions createOptions)
{    try {        return MoreFutures.get(stagePackage(PackageAttributes.forBytesToStage(bytes, target, stagingPath), DEFAULT_SLEEPER, createOptions)).getPackageAttributes().getDestination();    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException("Interrupted while staging pipeline", e);    } catch (ExecutionException e) {        throw new RuntimeException("Error while staging pipeline", e.getCause());    }}
 List<DataflowPackage> beam_f6838_1(Collection<String> classpathElements, final String stagingPath, final Sleeper retrySleeper, final CreateOptions createOptions)
{        Instant start = Instant.now();    if (classpathElements.size() > SANE_CLASSPATH_SIZE) {            }    checkArgument(stagingPath != null, "Can't stage classpath elements because no staging location has been provided");    final AtomicInteger numUploaded = new AtomicInteger(0);    final AtomicInteger numCached = new AtomicInteger(0);    List<CompletionStage<DataflowPackage>> destinationPackages = new ArrayList<>();    for (String classpathElement : classpathElements) {        DataflowPackage sourcePackage = new DataflowPackage();        if (classpathElement.contains("=")) {            String[] components = classpathElement.split("=", 2);            sourcePackage.setName(components[0]);            sourcePackage.setLocation(components[1]);        } else {            sourcePackage.setName(null);            sourcePackage.setLocation(classpathElement);        }        File sourceFile = new File(sourcePackage.getLocation());        if (!sourceFile.exists()) {                        continue;        }        CompletionStage<StagingResult> stagingResult = computePackageAttributes(sourcePackage, stagingPath).thenComposeAsync(packageAttributes -> stagePackage(packageAttributes, retrySleeper, createOptions));        CompletionStage<DataflowPackage> stagedPackage = stagingResult.thenApply(stagingResult1 -> {            if (stagingResult1.alreadyStaged()) {                numCached.incrementAndGet();            } else {                numUploaded.incrementAndGet();            }            return stagingResult1.getPackageAttributes().getDestination();        });        destinationPackages.add(stagedPackage);    }    try {        CompletionStage<List<DataflowPackage>> stagingFutures = MoreFutures.allAsList(destinationPackages);        boolean finished = false;        do {            try {                MoreFutures.get(stagingFutures, 3L, TimeUnit.MINUTES);                finished = true;            } catch (TimeoutException e) {                                            }        } while (!finished);        List<DataflowPackage> stagedPackages = MoreFutures.get(stagingFutures);        Instant done = Instant.now();                return stagedPackages;    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException("Interrupted while staging packages", e);    } catch (ExecutionException e) {        throw new RuntimeException("Error while staging packages", e.getCause());    }}
public void beam_f6847_0(RandomAccessData value, OutputStream outStream) throws CoderException, IOException
{    encode(value, outStream, Coder.Context.NESTED);}
public void beam_f6848_0(RandomAccessData value, OutputStream outStream, Coder.Context context) throws CoderException, IOException
{    if (Objects.equals(value, POSITIVE_INFINITY)) {        throw new CoderException("Positive infinity can not be encoded.");    }    if (!context.isWholeStream) {        VarInt.encode(value.size, outStream);    }    value.writeTo(outStream, 0, value.size);}
public RandomAccessData beam_f6858_0() throws IOException
{    RandomAccessData copy = copy();    for (int i = copy.size - 1; i >= 0; --i) {        if (copy.buffer[i] != UnsignedBytes.MAX_VALUE) {            copy.buffer[i] = UnsignedBytes.checkedCast(UnsignedBytes.toInt(copy.buffer[i]) + 1L);            return copy;        }    }    return POSITIVE_INFINITY;}
public byte[] beam_f6859_0()
{    return buffer;}
public RandomAccessData beam_f6868_0() throws IOException
{    RandomAccessData copy = new RandomAccessData(size);    writeTo(copy.asOutputStream(), 0, size);    return copy;}
public boolean beam_f6869_0(Object other)
{    if (other == this) {        return true;    }    if (!(other instanceof RandomAccessData)) {        return false;    }    return UNSIGNED_LEXICOGRAPHICAL_COMPARATOR.compare(this, (RandomAccessData) other) == 0;}
public SerializableCoder<?> beam_f6878_0(CloudObject cloudObject)
{    String className = Structs.getString(cloudObject, TYPE_FIELD);    try {        Class<? extends Serializable> targetClass = (Class<? extends Serializable>) Class.forName(className);        checkArgument(Serializable.class.isAssignableFrom(targetClass), "Target class %s does not extend %s", targetClass.getName(), Serializable.class.getSimpleName());        return SerializableCoder.of(targetClass);    } catch (ClassNotFoundException e) {        throw new IllegalArgumentException(e);    }}
public Class<SerializableCoder> beam_f6879_0()
{    return SerializableCoder.class;}
public static Long beam_f6888_0(Map<String, Object> map, String name, @Nullable Long defaultValue)
{    return getValue(map, name, Long.class, "a long", defaultValue);}
public static Integer beam_f6889_0(Map<String, Object> map, String name)
{    return getValue(map, name, Integer.class, "an int");}
public static void beam_f6898_0(Map<String, Object> map, String name, String value)
{    addObject(map, name, CloudObject.forString(value));}
public static void beam_f6899_0(Map<String, Object> map, String name, boolean value)
{    addObject(map, name, CloudObject.forBoolean(value));}
public static void beam_f6908_0(Map<String, Object> map, String name, Double value)
{    addObject(map, name, CloudObject.forFloat(value));}
private static T beam_f6909_0(Map<String, Object> map, String name, Class<T> clazz, String type)
{    @Nullable    T result = getValue(map, name, clazz, type, null);    if (result == null) {        throw new ParameterNotFoundException(name, map);    }    return result;}
public void beam_f6918_0() throws Exception
{    DataflowPipelineOptions options = buildPipelineOptions("--experiments=beam_fn_api");    options.setRunner(DataflowRunner.class);    Pipeline pipeline = Pipeline.create(options);    DummyStatefulDoFn fn = new DummyStatefulDoFn();    pipeline.apply(Create.of(KV.of(1, 2))).apply(ParDo.of(fn));    DataflowRunner runner = DataflowRunner.fromOptions(options);    runner.replaceTransforms(pipeline);    assertThat(findBatchStatefulDoFn(pipeline), equalTo((DoFn) fn));}
public void beam_f6919_0() throws Exception
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setRunner(DataflowRunner.class);    Pipeline pipeline = Pipeline.create(options);    TupleTag<Integer> mainOutputTag = new TupleTag<Integer>() {    };    TupleTag<Integer> sideOutputTag = new TupleTag<Integer>() {    };    DummyStatefulDoFn fn = new DummyStatefulDoFn();    pipeline.apply(Create.of(KV.of(1, 2))).apply(ParDo.of(fn).withOutputTags(mainOutputTag, TupleTagList.of(sideOutputTag)));    DataflowRunner runner = DataflowRunner.fromOptions(options);    runner.replaceTransforms(pipeline);    assertThat(findBatchStatefulDoFn(pipeline), equalTo((DoFn) fn));}
public void beam_f6929_0() throws Exception
{    DoFnTester<KV<Integer, Iterable<KV<GlobalWindow, WindowedValue<String>>>>, IsmRecord<WindowedValue<String>>> doFnTester = DoFnTester.of(new BatchViewOverrides.BatchViewAsSingleton.IsmRecordForSingularValuePerWindowDoFn<String, GlobalWindow>(GlobalWindow.Coder.INSTANCE));    thrown.expect(IllegalStateException.class);    thrown.expectMessage("found for singleton within window");    doFnTester.processBundle(ImmutableList.of(KV.of(0, ImmutableList.of(KV.of(GlobalWindow.INSTANCE, valueInGlobalWindow("a")), KV.of(GlobalWindow.INSTANCE, valueInGlobalWindow("b"))))));}
public void beam_f6930_0() throws Exception
{    DoFnTester<String, IsmRecord<WindowedValue<String>>> doFnTester = DoFnTester.of(new BatchViewOverrides.BatchViewAsList.ToIsmRecordForGlobalWindowDoFn<String>());        assertThat(doFnTester.processBundle(ImmutableList.of("a", "b", "c")), contains(IsmRecord.of(ImmutableList.of(GlobalWindow.INSTANCE, 0L), valueInGlobalWindow("a")), IsmRecord.of(ImmutableList.of(GlobalWindow.INSTANCE, 1L), valueInGlobalWindow("b")), IsmRecord.of(ImmutableList.of(GlobalWindow.INSTANCE, 2L), valueInGlobalWindow("c"))));}
public void beam_f6939_0() throws IOException
{    Job modelJob = new Job();    modelJob.setCurrentState(State.RUNNING.toString());    DataflowPipelineJob job = mock(DataflowPipelineJob.class);    DataflowPipelineOptions options = mock(DataflowPipelineOptions.class);    when(options.isStreaming()).thenReturn(false);    when(job.getDataflowOptions()).thenReturn(options);    when(job.getState()).thenReturn(State.RUNNING);    job.jobId = JOB_ID;    JobMetrics jobMetrics = new JobMetrics();    jobMetrics.setMetrics(null);    DataflowClient dataflowClient = mock(DataflowClient.class);    when(dataflowClient.getJobMetrics(JOB_ID)).thenReturn(jobMetrics);    DataflowMetrics dataflowMetrics = new DataflowMetrics(job, dataflowClient);    MetricQueryResults result = dataflowMetrics.allMetrics();    assertThat(ImmutableList.copyOf(result.getCounters()), is(empty()));    assertThat(ImmutableList.copyOf(result.getDistributions()), is(empty()));}
public void beam_f6940_0() throws IOException
{    Job modelJob = new Job();    modelJob.setCurrentState(State.RUNNING.toString());    DataflowPipelineJob job = mock(DataflowPipelineJob.class);    DataflowPipelineOptions options = mock(DataflowPipelineOptions.class);    when(options.isStreaming()).thenReturn(false);    when(job.getDataflowOptions()).thenReturn(options);    when(job.getState()).thenReturn(State.DONE);    job.jobId = JOB_ID;    JobMetrics jobMetrics = new JobMetrics();    jobMetrics.setMetrics(ImmutableList.of());    DataflowClient dataflowClient = mock(DataflowClient.class);    when(dataflowClient.getJobMetrics(JOB_ID)).thenReturn(jobMetrics);    DataflowMetrics dataflowMetrics = new DataflowMetrics(job, dataflowClient);    verify(dataflowClient, times(0)).getJobMetrics(JOB_ID);    dataflowMetrics.allMetrics();    verify(dataflowClient, times(1)).getJobMetrics(JOB_ID);    dataflowMetrics.allMetrics();    verify(dataflowClient, times(1)).getJobMetrics(JOB_ID);}
public void beam_f6949_0() throws IOException
{    JobMetrics jobMetrics = new JobMetrics();    DataflowClient dataflowClient = mock(DataflowClient.class);    when(dataflowClient.getJobMetrics(JOB_ID)).thenReturn(jobMetrics);    DataflowPipelineJob job = mock(DataflowPipelineJob.class);    DataflowPipelineOptions options = mock(DataflowPipelineOptions.class);    when(options.isStreaming()).thenReturn(true);    when(job.getDataflowOptions()).thenReturn(options);    when(job.getState()).thenReturn(State.RUNNING);    job.jobId = JOB_ID;    AppliedPTransform<?, ?, ?> myStep2 = mock(AppliedPTransform.class);    when(myStep2.getFullName()).thenReturn("myStepName");    job.transformStepNames = HashBiMap.create();    job.transformStepNames.put(myStep2, "s2");    AppliedPTransform<?, ?, ?> myStep3 = mock(AppliedPTransform.class);    when(myStep3.getFullName()).thenReturn("myStepName3");    job.transformStepNames.put(myStep3, "s3");    AppliedPTransform<?, ?, ?> myStep4 = mock(AppliedPTransform.class);    when(myStep4.getFullName()).thenReturn("myStepName4");    job.transformStepNames.put(myStep4, "s4");            jobMetrics.setMetrics(ImmutableList.of(makeCounterMetricUpdate("counterName", "counterNamespace", "s2", 1233L, false), makeCounterMetricUpdate("counterName", "counterNamespace", "s2", 1234L, true), makeCounterMetricUpdate("otherCounter", "otherNamespace", "s3", 12L, false), makeCounterMetricUpdate("otherCounter", "otherNamespace", "s3", 12L, true), makeCounterMetricUpdate("counterName", "otherNamespace", "s4", 1200L, false), makeCounterMetricUpdate("counterName", "otherNamespace", "s4", 1233L, true)));    DataflowMetrics dataflowMetrics = new DataflowMetrics(job, dataflowClient);    MetricQueryResults result = dataflowMetrics.allMetrics();    try {        result.getCounters().iterator().next().getCommitted();        fail("Expected UnsupportedOperationException");    } catch (UnsupportedOperationException expected) {        assertThat(expected.getMessage(), containsString("This runner does not currently support committed" + " metrics results. Please use 'attempted' instead."));    }    assertThat(result.getCounters(), containsInAnyOrder(attemptedMetricsResult("counterNamespace", "counterName", "myStepName", 1233L), attemptedMetricsResult("otherNamespace", "otherCounter", "myStepName3", 12L), attemptedMetricsResult("otherNamespace", "counterName", "myStepName4", 1200L)));}
public void beam_f6950_0()
{    MockitoAnnotations.initMocks(this);    when(mockWorkflowClient.projects()).thenReturn(mockProjects);    when(mockProjects.locations()).thenReturn(mockLocations);    when(mockLocations.jobs()).thenReturn(mockJobs);    options = PipelineOptionsFactory.as(TestDataflowPipelineOptions.class);    options.setDataflowClient(mockWorkflowClient);    options.setProject(PROJECT_ID);    options.setRegion(REGION_ID);    options.setRunner(DataflowRunner.class);    options.setTempLocation("gs://fakebucket/temp");    options.setPathValidatorClass(NoopPathValidator.class);    options.setGcpCredential(new TestCredential());}
public void beam_f6959_0() throws Exception
{    assertEquals(null, mockWaitToFinishInState(State.UNKNOWN));    expectedLogs.verifyWarn("No terminal state was returned within allotted timeout. State value UNKNOWN");}
public void beam_f6960_0() throws Exception
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    when(mockJobs.get(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID))).thenReturn(statusRequest);    when(statusRequest.execute()).thenThrow(IOException.class);    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, ImmutableMap.of());    long startTime = fastClock.nanoTime();    State state = job.waitUntilFinish(Duration.standardMinutes(5), null, fastClock, fastClock);    assertEquals(null, state);    long timeDiff = TimeUnit.NANOSECONDS.toMillis(fastClock.nanoTime() - startTime);    checkValidInterval(DataflowPipelineJob.MESSAGES_POLLING_INTERVAL, DataflowPipelineJob.MESSAGES_POLLING_RETRIES, timeDiff);}
public void beam_f6969_0() throws IOException
{    Dataflow.Projects.Locations.Jobs.Update update = mock(Dataflow.Projects.Locations.Jobs.Update.class);    when(mockJobs.update(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID), any(Job.class))).thenReturn(update);    when(update.execute()).thenReturn(new Job().setCurrentState("JOB_STATE_CANCELLED"));    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, null);    assertEquals(State.CANCELLED, job.cancel());    Job content = new Job();    content.setProjectId(PROJECT_ID);    content.setId(JOB_ID);    content.setRequestedState("JOB_STATE_CANCELLED");    verify(mockJobs).update(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID), eq(content));    verifyNoMoreInteractions(mockJobs);}
public void beam_f6970_0() throws IOException
{    Dataflow.Projects.Locations.Jobs.Get statusRequest = mock(Dataflow.Projects.Locations.Jobs.Get.class);    Job statusResponse = new Job();    statusResponse.setCurrentState("JOB_STATE_RUNNING");    when(mockJobs.get(PROJECT_ID, REGION_ID, JOB_ID)).thenReturn(statusRequest);    when(statusRequest.execute()).thenReturn(statusResponse);    Dataflow.Projects.Locations.Jobs.Update update = mock(Dataflow.Projects.Locations.Jobs.Update.class);    when(mockJobs.update(eq(PROJECT_ID), eq(REGION_ID), eq(JOB_ID), any(Job.class))).thenReturn(update);    when(update.execute()).thenThrow(new IOException("Some random IOException"));    DataflowPipelineJob job = new DataflowPipelineJob(DataflowClient.create(options), JOB_ID, options, null);    thrown.expect(IOException.class);    thrown.expectMessage("Failed to cancel job in state RUNNING, " + "please go to the Developers Console to cancel it manually:");    job.cancel();}
public void beam_f6980_0()
{    for (PipelineRunnerRegistrar registrar : Lists.newArrayList(ServiceLoader.load(PipelineRunnerRegistrar.class).iterator())) {        if (registrar instanceof DataflowPipelineRegistrar.Runner) {            return;        }    }    fail("Expected to find " + DataflowPipelineRegistrar.Runner.class);}
public boolean beam_f6981_0(Job o)
{    Job job = (Job) o;    return job.getId() == null && job.getProjectId() == null && job.getName() != null && job.getType() != null && job.getEnvironment() != null && job.getSteps() != null && job.getCurrentState() == null && job.getCurrentStateTime() == null && job.getExecutionInfo() == null && job.getCreateTime() == null;}
public void beam_f6990_0() throws IOException
{    final DataflowPipelineWorkerPoolOptions.AutoscalingAlgorithmType noScaling = DataflowPipelineWorkerPoolOptions.AutoscalingAlgorithmType.NONE;    DataflowPipelineOptions options = buildPipelineOptions();    options.setAutoscalingAlgorithm(noScaling);    Pipeline p = buildPipeline(options);    p.traverseTopologically(new RecordingPipelineVisitor());    Job job = DataflowPipelineTranslator.fromOptions(options).translate(p, DataflowRunner.fromOptions(options), Collections.emptyList()).getJob();    assertEquals(1, job.getEnvironment().getWorkerPools().size());    assertEquals("AUTOSCALING_ALGORITHM_NONE", job.getEnvironment().getWorkerPools().get(0).getAutoscalingSettings().getAlgorithm());    assertEquals(0, job.getEnvironment().getWorkerPools().get(0).getAutoscalingSettings().getMaxNumWorkers().intValue());}
public void beam_f6991_0() throws IOException
{    final DataflowPipelineWorkerPoolOptions.AutoscalingAlgorithmType noScaling = null;    DataflowPipelineOptions options = buildPipelineOptions();    options.setMaxNumWorkers(42);    options.setAutoscalingAlgorithm(noScaling);    Pipeline p = buildPipeline(options);    p.traverseTopologically(new RecordingPipelineVisitor());    Job job = DataflowPipelineTranslator.fromOptions(options).translate(p, DataflowRunner.fromOptions(options), Collections.emptyList()).getJob();    assertEquals(1, job.getEnvironment().getWorkerPools().size());    assertNull(job.getEnvironment().getWorkerPools().get(0).getAutoscalingSettings().getAlgorithm());    assertEquals(42, job.getEnvironment().getWorkerPools().get(0).getAutoscalingSettings().getMaxNumWorkers().intValue());}
public void beam_f7000_0() throws Exception
{    DataflowPipelineOptions options = buildPipelineOptions();    Pipeline pipeline = Pipeline.create(options);    DataflowPipelineTranslator t = DataflowPipelineTranslator.fromOptions(options);    applyRead(pipeline, "gs://bucket/foo");    applyRead(pipeline, "gs://bucket/foo/");    applyRead(pipeline, "gs://bucket/foo/*");    applyRead(pipeline, "gs://bucket/foo/?");    applyRead(pipeline, "gs://bucket/foo/[0-9]");    applyRead(pipeline, "gs://bucket/foo/*baz*");    applyRead(pipeline, "gs://bucket/foo/*baz?");    applyRead(pipeline, "gs://bucket/foo/[0-9]baz?");    applyRead(pipeline, "gs://bucket/foo/baz/*");    applyRead(pipeline, "gs://bucket/foo/baz/*wonka*");    applyRead(pipeline, "gs://bucket/foo/*baz/wonka*");    applyRead(pipeline, "gs://bucket/foo*/baz");    applyRead(pipeline, "gs://bucket/foo?/baz");    applyRead(pipeline, "gs://bucket/foo[0-9]/baz");        JobSpecification jobSpecification = t.translate(pipeline, DataflowRunner.fromOptions(options), Collections.emptyList());    assertAllStepOutputsHaveUniqueIds(jobSpecification.getJob());}
private void beam_f7001_0(Pipeline pipeline, String path)
{    pipeline.apply("Read(" + path + ")", TextIO.read().from(path));}
public void beam_f7012_0() throws Exception
{                DataflowPipelineOptions options = buildPipelineOptions();    DataflowPipelineTranslator translator = DataflowPipelineTranslator.fromOptions(options);    Pipeline pipeline = Pipeline.create(options);    pipeline.apply(Create.of(1)).apply(View.asSingleton());    DataflowRunner runner = DataflowRunner.fromOptions(options);    runner.replaceTransforms(pipeline);    Job job = translator.translate(pipeline, runner, Collections.emptyList()).getJob();    assertAllStepOutputsHaveUniqueIds(job);    List<Step> steps = job.getSteps();    assertEquals(9, steps.size());    @SuppressWarnings("unchecked")    List<Map<String, Object>> toIsmRecordOutputs = (List<Map<String, Object>>) steps.get(7).getProperties().get(PropertyNames.OUTPUT_INFO);    assertTrue(Structs.getBoolean(Iterables.getOnlyElement(toIsmRecordOutputs), "use_indexed_format"));    Step collectionToSingletonStep = steps.get(8);    assertEquals("CollectionToSingleton", collectionToSingletonStep.getKind());}
public void beam_f7013_0() throws Exception
{                DataflowPipelineOptions options = buildPipelineOptions();    DataflowPipelineTranslator translator = DataflowPipelineTranslator.fromOptions(options);    Pipeline pipeline = Pipeline.create(options);    pipeline.apply(Create.of(1, 2, 3)).apply(View.asIterable());    DataflowRunner runner = DataflowRunner.fromOptions(options);    runner.replaceTransforms(pipeline);    Job job = translator.translate(pipeline, runner, Collections.emptyList()).getJob();    assertAllStepOutputsHaveUniqueIds(job);    List<Step> steps = job.getSteps();    assertEquals(3, steps.size());    @SuppressWarnings("unchecked")    List<Map<String, Object>> toIsmRecordOutputs = (List<Map<String, Object>>) steps.get(1).getProperties().get(PropertyNames.OUTPUT_INFO);    assertTrue(Structs.getBoolean(Iterables.getOnlyElement(toIsmRecordOutputs), "use_indexed_format"));    Step collectionToSingletonStep = steps.get(2);    assertEquals("CollectionToSingleton", collectionToSingletonStep.getKind());}
public void beam_f7023_0()
{    PTransformMatcher matcher = new DataflowPTransformMatchers.CombineValuesWithoutSideInputsPTransformMatcher();    AppliedPTransform<?, ?, ?> groupedValues;    groupedValues = getCombineGroupedValuesFrom(createCombineGroupedValuesPipeline());    assertThat(matcher.matches(groupedValues), is(true));    groupedValues = getCombineGroupedValuesFrom(createCombinePerKeyPipeline());    assertThat(matcher.matches(groupedValues), is(true));}
public void beam_f7024_0()
{    PTransformMatcher matcher = new DataflowPTransformMatchers.CombineValuesWithoutSideInputsPTransformMatcher();    AppliedPTransform<?, ?, ?> groupedValues;    groupedValues = getCombineGroupedValuesFrom(createCombineGroupedValuesWithSideInputsPipeline());    assertThat(matcher.matches(groupedValues), is(false));    groupedValues = getCombineGroupedValuesFrom(createCombinePerKeyWithSideInputsPipeline());    assertThat(matcher.matches(groupedValues), is(false));}
public Integer beam_f7033_0()
{    return 0;}
public Integer beam_f7034_0(Integer accum, Integer input)
{    return accum + input;}
public void beam_f7043_0() throws IOException
{    this.mockGcsUtil = mock(GcsUtil.class);    when(mockGcsUtil.create(any(GcsPath.class), anyString())).then(invocation -> FileChannel.open(Files.createTempFile("channel-", ".tmp"), StandardOpenOption.CREATE, StandardOpenOption.WRITE, StandardOpenOption.DELETE_ON_CLOSE));    when(mockGcsUtil.create(any(GcsPath.class), anyString(), anyInt())).then(invocation -> FileChannel.open(Files.createTempFile("channel-", ".tmp"), StandardOpenOption.CREATE, StandardOpenOption.WRITE, StandardOpenOption.DELETE_ON_CLOSE));    when(mockGcsUtil.expand(any(GcsPath.class))).then(invocation -> ImmutableList.of((GcsPath) invocation.getArguments()[0]));    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_STAGING_BUCKET))).thenReturn(true);    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_TEMP_BUCKET))).thenReturn(true);    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_TEMP_BUCKET + "/staging/"))).thenReturn(true);    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_PROFILE_BUCKET))).thenReturn(true);    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(NON_EXISTENT_BUCKET))).thenReturn(false);        when(mockGcsUtil.getObjects(anyListOf(GcsPath.class))).thenAnswer(invocationOnMock -> {        List<GcsPath> gcsPaths = (List<GcsPath>) invocationOnMock.getArguments()[0];        List<GcsUtil.StorageObjectOrIOException> results = new ArrayList<>();        for (GcsPath gcsPath : gcsPaths) {            if (gcsPath.getBucket().equals(VALID_BUCKET)) {                StorageObject resultObject = new StorageObject();                resultObject.setBucket(gcsPath.getBucket());                resultObject.setName(gcsPath.getObject());                results.add(GcsUtil.StorageObjectOrIOException.create(resultObject));            }        }        return results;    });        when(mockGcsUtil.bucketAccessible(GcsPath.fromUri("gs://bucket/object"))).thenReturn(true);    mockJobs = mock(Dataflow.Projects.Locations.Jobs.class);}
private Pipeline beam_f7044_0(DataflowPipelineOptions options)
{    options.setStableUniqueNames(CheckEnabled.ERROR);    options.setRunner(DataflowRunner.class);    Pipeline p = Pipeline.create(options);    p.apply("ReadMyFile", TextIO.read().from("gs://bucket/object")).apply("WriteMyFile", TextIO.write().to("gs://bucket/object"));        FileSystems.setDefaultPipelineOptions(options);    return p;}
public void beam_f7053_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();        options.setAppName(DataflowRunnerTest.class.getSimpleName());    options.setJobName("some-job-name");    Pipeline p = Pipeline.create(options);    p.run();    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);    Mockito.verify(mockJobs).create(eq(PROJECT_ID), eq(REGION_ID), jobCaptor.capture());    Map<String, Object> sdkPipelineOptions = jobCaptor.getValue().getEnvironment().getSdkPipelineOptions();    assertThat(sdkPipelineOptions, hasKey("options"));    Map<String, Object> optionsMap = (Map<String, Object>) sdkPipelineOptions.get("options");    assertThat(optionsMap, hasEntry("appName", (Object) options.getAppName()));    assertThat(optionsMap, hasEntry("project", (Object) options.getProject()));    assertThat(optionsMap, hasEntry("pathValidatorClass", (Object) options.getPathValidatorClass().getName()));    assertThat(optionsMap, hasEntry("runner", (Object) options.getRunner().getName()));    assertThat(optionsMap, hasEntry("jobName", (Object) options.getJobName()));    assertThat(optionsMap, hasEntry("tempLocation", (Object) options.getTempLocation()));    assertThat(optionsMap, hasEntry("stagingLocation", (Object) options.getStagingLocation()));    assertThat(optionsMap, hasEntry("stableUniqueNames", (Object) options.getStableUniqueNames().toString()));    assertThat(optionsMap, hasEntry("streaming", (Object) options.isStreaming()));    assertThat(optionsMap, hasEntry("numberOfWorkerHarnessThreads", (Object) options.getNumberOfWorkerHarnessThreads()));}
public void beam_f7054_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setFlexRSGoal(DataflowPipelineOptions.FlexResourceSchedulingGoal.COST_OPTIMIZED);    Pipeline p = Pipeline.create(options);    p.run();    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);    Mockito.verify(mockJobs).create(eq(PROJECT_ID), eq(REGION_ID), jobCaptor.capture());    assertEquals("FLEXRS_COST_OPTIMIZED", jobCaptor.getValue().getEnvironment().getFlexResourceSchedulingGoal());}
public void beam_f7063_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setUpdate(true);    options.setJobName("oldJobName");    Pipeline p = buildDataflowPipeline(options);    DataflowPipelineJob job = (DataflowPipelineJob) p.run();    assertEquals("newid", job.getJobId());    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);    Mockito.verify(mockJobs).create(eq(PROJECT_ID), eq(REGION_ID), jobCaptor.capture());    assertValidJob(jobCaptor.getValue());}
public void beam_f7064_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setExperiments(Arrays.asList("upload_graph"));    Pipeline p = buildDataflowPipeline(options);    DataflowPipelineJob job = (DataflowPipelineJob) p.run();    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);    Mockito.verify(mockJobs).create(eq(PROJECT_ID), eq(REGION_ID), jobCaptor.capture());    assertValidJob(jobCaptor.getValue());    assertTrue(jobCaptor.getValue().getSteps().isEmpty());    assertTrue(jobCaptor.getValue().getStepsLocation().startsWith("gs://valid-bucket/temp/staging/dataflow_graph"));}
public void beam_f7073_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setTempLocation("file://temp/location");    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("DataflowRunner requires gcpTempLocation, " + "but failed to retrieve a value from PipelineOptions");    DataflowRunner.fromOptions(options);}
public void beam_f7074_0() throws IOException
{    DataflowPipelineOptions options = buildPipelineOptions();    options.setStagingLocation("file://my/staging/location");    try {        DataflowRunner.fromOptions(options);        fail("fromOptions should have failed");    } catch (IllegalArgumentException e) {        assertThat(e.getMessage(), containsString("Expected a valid 'gs://' path but was given"));    }    options.setStagingLocation("my/staging/location");    try {        DataflowRunner.fromOptions(options);        fail("fromOptions should have failed");    } catch (IllegalArgumentException e) {        assertThat(e.getMessage(), containsString("Expected a valid 'gs://' path but was given"));    }}
public void beam_f7083_0() throws IOException
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    options.setRunner(DataflowRunner.class);    options.setProject("some project");    options.setGcpTempLocation(VALID_TEMP_BUCKET);    options.setGcsUtil(mockGcsUtil);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Project ID");    thrown.expectMessage("project description");    DataflowRunner.fromOptions(options);}
public void beam_f7084_0() throws IOException
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    FileSystems.setDefaultPipelineOptions(options);    options.setRunner(DataflowRunner.class);    options.setProject("foo-12345");    options.setGcpTempLocation(VALID_TEMP_BUCKET);    options.setGcsUtil(mockGcsUtil);    options.as(DataflowPipelineDebugOptions.class).setNumberOfWorkerHarnessThreads(-1);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Number of worker harness threads");    thrown.expectMessage("Please make sure the value is non-negative.");    DataflowRunner.fromOptions(options);}
public void beam_f7093_0() throws IOException
{    int gcsUploadBufferSizeBytes = 12345678;    DataflowPipelineOptions batchOptions = buildPipelineOptions();    batchOptions.setGcsUploadBufferSizeBytes(gcsUploadBufferSizeBytes);    batchOptions.setRunner(DataflowRunner.class);    Pipeline.create(batchOptions);    assertEquals(gcsUploadBufferSizeBytes, batchOptions.getGcsUploadBufferSizeBytes().intValue());    DataflowPipelineOptions streamingOptions = buildPipelineOptions();    streamingOptions.setStreaming(true);    streamingOptions.setGcsUploadBufferSizeBytes(gcsUploadBufferSizeBytes);    streamingOptions.setRunner(DataflowRunner.class);    Pipeline.create(streamingOptions);    assertEquals(gcsUploadBufferSizeBytes, streamingOptions.getGcsUploadBufferSizeBytes().intValue());}
public PCollection<Integer> beam_f7094_0(PCollection<Integer> input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), input.isBounded(), input.getCoder());}
public CompositeBehavior beam_f7105_0(TransformHierarchy.Node node)
{    if (node.getTransform() != null) {        transforms.add(node.getTransform());    }    return CompositeBehavior.ENTER_TRANSFORM;}
public List<PTransform<?, ?>> beam_f7106_0()
{    return transforms;}
private void beam_f7115_0(PipelineOptions options) throws Exception
{    Pipeline p = Pipeline.create(options);    p.apply(Create.of(KV.of(13, 42))).apply(Window.into(Sessions.withGapDuration(Duration.millis(1)))).apply(ParDo.of(new DoFn<KV<Integer, Integer>, Void>() {        @StateId("fizzle")        private final StateSpec<ValueState<Void>> voidState = StateSpecs.value();        @ProcessElement        public void process() {        }    }));    thrown.expectMessage("merging");    thrown.expect(UnsupportedOperationException.class);    p.run();}
public void beam_f7117_0() throws Exception
{    PipelineOptions options = buildPipelineOptions();    options.as(StreamingOptions.class).setStreaming(true);    verifyMergingStatefulParDoRejected(options);}
public Writer<Void, Object> beam_f7127_0()
{    throw new UnsupportedOperationException();}
public void beam_f7128_0() throws Exception
{    DataflowPipelineDebugOptions options = PipelineOptionsFactory.fromArgs("--transformNameMapping={\"a\":\"b\",\"foo\":\"\",\"bar\":\"baz\"}").as(DataflowPipelineDebugOptions.class);    assertEquals(3, options.getTransformNameMapping().size());    assertThat(options.getTransformNameMapping(), hasEntry("a", "b"));    assertThat(options.getTransformNameMapping(), hasEntry("foo", ""));    assertThat(options.getTransformNameMapping(), hasEntry("bar", "baz"));}
public void beam_f7137_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    FileSystems.setDefaultPipelineOptions(options);    options.setPathValidatorClass(NoopPathValidator.class);    options.setTempLocation("gs://temp_location/");    options.setGcpTempLocation("gs://gcp_temp_location/");    assertEquals("gs://gcp_temp_location/staging/", options.getStagingLocation());}
public void beam_f7138_0()
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    assertEquals(DataflowPipelineOptions.FlexResourceSchedulingGoal.UNSPECIFIED, options.getFlexRSGoal());    options.setFlexRSGoal(DataflowPipelineOptions.FlexResourceSchedulingGoal.COST_OPTIMIZED);    assertEquals(DataflowPipelineOptions.FlexResourceSchedulingGoal.COST_OPTIMIZED, options.getFlexRSGoal());}
public void beam_f7147_0() throws Exception
{    String testValue = "{\"A\":\"WARN\"}";    assertEquals(testValue, MAPPER.writeValueAsString(MAPPER.readValue(testValue, WorkerLogLevelOverrides.class)));}
public void beam_f7148_0()
{    ParDo.SingleOutput<Integer, Long> originalTransform = ParDo.of(new ToLongFn());    DisplayData originalDisplayData = DisplayData.from(originalTransform);    PCollection<? extends Integer> input = pipeline.apply(Create.of(1, 2, 3));    AppliedPTransform<PCollection<? extends Integer>, PCollection<Long>, ParDo.SingleOutput<Integer, Long>> application = AppliedPTransform.of("original", input.expand(), input.apply(originalTransform).expand(), originalTransform, pipeline);    PTransformReplacement<PCollection<? extends Integer>, PCollection<Long>> replacement = factory.getReplacementTransform(application);    DisplayData replacementDisplayData = DisplayData.from(replacement.getTransform());    assertThat(replacementDisplayData, equalTo(originalDisplayData));    DisplayData primitiveDisplayData = Iterables.getOnlyElement(DisplayDataEvaluator.create().displayDataForPrimitiveTransforms(replacement.getTransform(), VarIntCoder.of()));    assertThat(primitiveDisplayData, equalTo(replacementDisplayData));}
public void beam_f7157_0()
{    assertEquals("TestDataflowRunner#TestAppName", TestDataflowRunner.fromOptions(options).toString());}
public void beam_f7158_0() throws Exception
{    Pipeline p = Pipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.DONE);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    when(mockClient.getJobMetrics(anyString())).thenReturn(generateMockMetricResponse(true, /* success */    true));    assertEquals(mockJob, runner.run(p, mockRunner));}
private JobMetrics beam_f7167_0(Map<String, BigDecimal> metricMap) throws IOException
{    return buildJobMetrics(generateMockStreamingMetrics(metricMap));}
private List<MetricUpdate> beam_f7168_0(Map<String, BigDecimal> metricMap)
{    List<MetricUpdate> metrics = Lists.newArrayList();    for (Map.Entry<String, BigDecimal> entry : metricMap.entrySet()) {        MetricStructuredName name = new MetricStructuredName();        name.setName(entry.getKey());        MetricUpdate metric = new MetricUpdate();        metric.setName(name);        metric.setScalar(entry.getValue());        metrics.add(metric);    }    return metrics;}
public void beam_f7177_0() throws Exception
{    options.setStreaming(true);    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    final DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.DONE);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    options.as(TestPipelineOptions.class).setOnCreateMatcher(new TestSuccessMatcher(mockJob, 0));    when(mockJob.waitUntilFinish(any(Duration.class), any(JobMessagesHandler.class))).thenReturn(State.DONE);    when(mockClient.getJobMetrics(anyString())).thenReturn(generateMockMetricResponse(true, /* success */    true));    runner.run(p, mockRunner);}
public void beam_f7178_0() throws Exception
{    Pipeline p = TestPipeline.create(options);    PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));    PAssert.that(pc).containsInAnyOrder(1, 2, 3);    final DataflowPipelineJob mockJob = Mockito.mock(DataflowPipelineJob.class);    when(mockJob.getState()).thenReturn(State.DONE);    when(mockJob.getProjectId()).thenReturn("test-project");    when(mockJob.getJobId()).thenReturn("test-job");    DataflowRunner mockRunner = Mockito.mock(DataflowRunner.class);    when(mockRunner.run(any(Pipeline.class))).thenReturn(mockJob);    TestDataflowRunner runner = TestDataflowRunner.fromOptionsAndClient(options, mockClient);    options.as(TestPipelineOptions.class).setOnSuccessMatcher(new TestSuccessMatcher(mockJob, 1));    when(mockClient.getJobMetrics(anyString())).thenReturn(generateMockMetricResponse(true, /* success */    true));    runner.run(p, mockRunner);}
public void beam_f7189_0()
{    Pipeline p = createTestServiceRunner();    PCollection<KV<String, Integer>> input = p.apply(new PTransform<PBegin, PCollection<KV<String, Integer>>>() {        @Override        public PCollection<KV<String, Integer>> expand(PBegin input) {            return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));        }    });    thrown.expect(IllegalStateException.class);    thrown.expectMessage("GroupByKey cannot be applied to non-bounded PCollection in the GlobalWindow without " + "a trigger. Use a Window.into or Window.triggering transform prior to GroupByKey.");    input.apply("GroupByKey", GroupByKey.create());}
public PCollection<KV<String, Integer>> beam_f7190_0(PBegin input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));}
public void beam_f7199_0()
{    testViewUnbounded(createTestBatchRunner(), View.asIterable());}
public void beam_f7200_0()
{    testViewUnbounded(createTestStreamingRunner(), View.asIterable());}
public void beam_f7209_0()
{    testViewNonmerging(createTestBatchRunner(), View.asIterable());}
public void beam_f7210_0()
{    testViewNonmerging(createTestStreamingRunner(), View.asIterable());}
public static Iterable<Coder<?>> beam_f7219_0()
{    Builder<Coder<?>> dataBuilder = ImmutableList.<Coder<?>>builder().add(new ArbitraryCoder()).add(new ObjectCoder()).add(GlobalWindow.Coder.INSTANCE).add(IntervalWindow.getCoder()).add(LengthPrefixCoder.of(VarLongCoder.of())).add(IterableCoder.of(VarLongCoder.of())).add(KvCoder.of(VarLongCoder.of(), ByteArrayCoder.of())).add(WindowedValue.getFullCoder(KvCoder.of(VarLongCoder.of(), ByteArrayCoder.of()), IntervalWindow.getCoder())).add(ByteArrayCoder.of()).add(VarLongCoder.of()).add(SerializableCoder.of(Record.class)).add(AvroCoder.of(Record.class)).add(CollectionCoder.of(VarLongCoder.of())).add(ListCoder.of(VarLongCoder.of())).add(SetCoder.of(VarLongCoder.of())).add(MapCoder.of(VarLongCoder.of(), ByteArrayCoder.of())).add(NullableCoder.of(IntervalWindow.getCoder())).add(UnionCoder.of(ImmutableList.of(VarLongCoder.of(), ByteArrayCoder.of(), KvCoder.of(VarLongCoder.of(), ByteArrayCoder.of())))).add(CoGbkResultCoder.of(CoGbkResultSchema.of(ImmutableList.of(new TupleTag<Long>(), new TupleTag<byte[]>())), UnionCoder.of(ImmutableList.of(VarLongCoder.of(), ByteArrayCoder.of())))).add(SchemaCoder.of(Schema.builder().build()));    for (Class<? extends Coder> atomicCoder : DefaultCoderCloudObjectTranslatorRegistrar.KNOWN_ATOMIC_CODERS) {        dataBuilder.add(InstanceBuilder.ofType(atomicCoder).fromFactoryMethod("of").build());    }    return dataBuilder.build();}
public void beam_f7220_0() throws Exception
{    CloudObject cloudObject = CloudObjects.asCloudObject(coder, /*sdkComponents=*/    null);    Coder<?> fromCloudObject = CloudObjects.coderFromCloudObject(cloudObject);    assertEquals(coder.getClass(), fromCloudObject.getClass());    assertEquals(coder, fromCloudObject);}
public void beam_f7232_0() throws IOException
{    DataflowClient dataflowClient = mock(DataflowClient.class);    ListJobMessagesResponse firstResponse = new ListJobMessagesResponse();    firstResponse.setJobMessages(new ArrayList<>());    for (long i = 0; i < 100; ++i) {        JobMessage message = new JobMessage();        message.setId("message_" + i);        message.setTime(TimeUtil.toCloudTime(new Instant(i)));        firstResponse.getJobMessages().add(message);    }    String pageToken = "page_token";    firstResponse.setNextPageToken(pageToken);    ListJobMessagesResponse secondResponse = new ListJobMessagesResponse();    secondResponse.setJobMessages(new ArrayList<>());    for (long i = 100; i < 150; ++i) {        JobMessage message = new JobMessage();        message.setId("message_" + i);        message.setTime(TimeUtil.toCloudTime(new Instant(i)));        secondResponse.getJobMessages().add(message);    }    when(dataflowClient.listJobMessages(JOB_ID, null)).thenReturn(firstResponse);    when(dataflowClient.listJobMessages(JOB_ID, pageToken)).thenReturn(secondResponse);    MonitoringUtil util = new MonitoringUtil(dataflowClient);    List<JobMessage> messages = util.getJobMessages(JOB_ID, -1);    assertEquals(150, messages.size());}
public void beam_f7233_0()
{        assertEquals(State.UNKNOWN, MonitoringUtil.toState("JOB_STATE_UNKNOWN"));    assertEquals(State.STOPPED, MonitoringUtil.toState("JOB_STATE_STOPPED"));    assertEquals(State.RUNNING, MonitoringUtil.toState("JOB_STATE_RUNNING"));    assertEquals(State.DONE, MonitoringUtil.toState("JOB_STATE_DONE"));    assertEquals(State.FAILED, MonitoringUtil.toState("JOB_STATE_FAILED"));    assertEquals(State.CANCELLED, MonitoringUtil.toState("JOB_STATE_CANCELLED"));    assertEquals(State.UPDATED, MonitoringUtil.toState("JOB_STATE_UPDATED"));        assertEquals(State.RUNNING, MonitoringUtil.toState("JOB_STATE_DRAINING"));    assertEquals(State.DONE, MonitoringUtil.toState("JOB_STATE_DRAINED"));}
private static PackageAttributes beam_f7242_0(File file, @Nullable String overridePackageName) throws IOException
{    PackageAttributes attributes = PackageUtil.PackageAttributes.forFileToStage(file, STAGING_PATH);    if (overridePackageName != null) {        attributes = attributes.withPackageName(overridePackageName);    }    return attributes;}
public void beam_f7243_0() throws Exception
{    String contents = "This is a test!";    File tmpFile = makeFileWithContents("file.txt", contents);    PackageAttributes attr = makePackageAttributes(tmpFile, null);    DataflowPackage target = attr.getDestination();    assertThat(target.getName(), RegexMatcher.matches("file-" + HASH_PATTERN + ".txt"));    assertThat(target.getLocation(), equalTo(STAGING_PATH + target.getName()));    assertThat(attr.getSize(), equalTo((long) contents.length()));}
public void beam_f7252_0() throws Exception
{    Pipe pipe = Pipe.open();    File tmpDirectory = tmpFolder.newFolder("folder");    tmpFolder.newFolder("folder", "empty_directory");    tmpFolder.newFolder("folder", "directory");    makeFileWithContents("folder/file.txt", "This is a test!");    makeFileWithContents("folder/directory/file.txt", "This is also a test!");    when(mockGcsUtil.getObjects(anyListOf(GcsPath.class))).thenReturn(ImmutableList.of(StorageObjectOrIOException.create(new FileNotFoundException("some/path"))));    when(mockGcsUtil.create(any(GcsPath.class), anyString())).thenReturn(pipe.sink());    defaultPackageUtil.stageClasspathElements(ImmutableList.of(tmpDirectory.getAbsolutePath()), STAGING_PATH, createOptions);    verify(mockGcsUtil).getObjects(anyListOf(GcsPath.class));    verify(mockGcsUtil).create(any(GcsPath.class), anyString());    verifyNoMoreInteractions(mockGcsUtil);    List<String> zipEntryNames = new ArrayList<>();    try (ZipInputStream inputStream = new ZipInputStream(Channels.newInputStream(pipe.source()))) {        for (ZipEntry entry = inputStream.getNextEntry(); entry != null; entry = inputStream.getNextEntry()) {            zipEntryNames.add(entry.getName());        }    }    assertThat(zipEntryNames, containsInAnyOrder("directory/file.txt", "empty_directory/", "file.txt"));}
public void beam_f7253_0() throws Exception
{    Pipe pipe = Pipe.open();    File tmpDirectory = tmpFolder.newFolder("folder");    when(mockGcsUtil.getObjects(anyListOf(GcsPath.class))).thenReturn(ImmutableList.of(StorageObjectOrIOException.create(new FileNotFoundException("some/path"))));    when(mockGcsUtil.create(any(GcsPath.class), anyString())).thenReturn(pipe.sink());    List<DataflowPackage> targets = defaultPackageUtil.stageClasspathElements(ImmutableList.of(tmpDirectory.getAbsolutePath()), STAGING_PATH, createOptions);    DataflowPackage target = Iterables.getOnlyElement(targets);    verify(mockGcsUtil).getObjects(anyListOf(GcsPath.class));    verify(mockGcsUtil).create(any(GcsPath.class), anyString());    verifyNoMoreInteractions(mockGcsUtil);    assertThat(target.getName(), RegexMatcher.matches("folder-" + HASH_PATTERN + ".jar"));    assertThat(target.getLocation(), equalTo(STAGING_PATH + target.getName()));    try (ZipInputStream zipInputStream = new ZipInputStream(Channels.newInputStream(pipe.source()))) {        assertNull(zipInputStream.getNextEntry());    }}
public LowLevelHttpRequest beam_f7262_0(String method, String url) throws IOException
{    ErrorInfo errorInfo = new ErrorInfo();    errorInfo.setReason(reason);    errorInfo.setMessage(message);    errorInfo.setFactory(jsonFactory);    GenericJson error = new GenericJson();    error.set("code", status);    error.set("errors", Arrays.asList(errorInfo));    error.setFactory(jsonFactory);    GenericJson errorResponse = new GenericJson();    errorResponse.set("error", error);    errorResponse.setFactory(jsonFactory);    return new MockLowLevelHttpRequest().setResponse(new MockLowLevelHttpResponse().setContent(errorResponse.toPrettyString()).setContentType(Json.MEDIA_TYPE).setStatusCode(status));}
private StorageObject beam_f7263_0(String gcsFilename, long fileSize)
{    GcsPath gcsPath = GcsPath.fromUri(gcsFilename);    return new StorageObject().setBucket(gcsPath.getBucket()).setName(gcsPath.getObject()).setSize(BigInteger.valueOf(fileSize));}
public void beam_f7272_0() throws Exception
{    RandomAccessData stream = new RandomAccessData(0);    assertArrayEquals(new byte[0], stream.array());        stream.resetTo(3);    assertArrayEquals(new byte[] { 0x00, 0x00, 0x00 }, stream.array());}
public void beam_f7273_0() throws Exception
{    RandomAccessData stream = new RandomAccessData(0);    assertArrayEquals(new byte[0], stream.array());    stream.readFrom(new ByteArrayInputStream(TEST_DATA_A), 0, TEST_DATA_A.length);    assertArrayEquals(TEST_DATA_A, Arrays.copyOf(stream.array(), TEST_DATA_A.length));}
public void beam_f7282_0()
{    assertEquals(new Instant(0), fromCloudTime("1970-01-01T00:00:00Z"));    assertEquals(new Instant(1), fromCloudTime("1970-01-01T00:00:00.001Z"));    assertEquals(new Instant(1), fromCloudTime("1970-01-01T00:00:00.001000Z"));    assertEquals(new Instant(1), fromCloudTime("1970-01-01T00:00:00.001001Z"));    assertEquals(new Instant(1), fromCloudTime("1970-01-01T00:00:00.001000000Z"));    assertEquals(new Instant(1), fromCloudTime("1970-01-01T00:00:00.001000001Z"));    assertEquals(new Instant(0), fromCloudTime("1970-01-01T00:00:00.0Z"));    assertEquals(new Instant(0), fromCloudTime("1970-01-01T00:00:00.00Z"));    assertEquals(new Instant(420), fromCloudTime("1970-01-01T00:00:00.42Z"));    assertEquals(new Instant(300), fromCloudTime("1970-01-01T00:00:00.3Z"));    assertEquals(new Instant(20), fromCloudTime("1970-01-01T00:00:00.02Z"));    assertNull(fromCloudTime(""));    assertNull(fromCloudTime("1970-01-01T00:00:00"));    assertNull(fromCloudTime("1970-01-01T00:00:00.1e3Z"));}
public void beam_f7283_0()
{    assertEquals("0s", toCloudDuration(Duration.ZERO));    assertEquals("4s", toCloudDuration(Duration.millis(4000)));    assertEquals("4.001s", toCloudDuration(Duration.millis(4001)));}
public String beam_f7292_0()
{    return applianceShuffleReader.getDatasetId();}
public void beam_f7293_0()
{    destroySynchronized();}
public BoundedWindow beam_f7302_0()
{    return Iterables.getOnlyElement(elem.getWindows());}
public void beam_f7304_0() throws Exception
{    receiver = null;}
public DynamicSplitResult beam_f7313_1(DynamicSplitRequest splitRequest)
{    ApproximateSplitRequest splitProgress = SourceTranslationUtils.splitRequestToApproximateSplitRequest(splitRequest);    double splitAtFraction = splitProgress.getFractionConsumed();        OffsetBasedSource<ByteBuffer> residual = reader.splitAtFraction(splitAtFraction);    if (residual == null) {                return null;    }    com.google.api.services.dataflow.model.Position acceptedPosition = new com.google.api.services.dataflow.model.Position();    acceptedPosition.setByteOffset(residual.getStartOffset());        return new DynamicSplitResultWithPosition(SourceTranslationUtils.cloudPositionToReaderPosition(acceptedPosition));}
public Map<String, ReaderFactory> beam_f7314_0()
{    return ImmutableMap.of("AvroSource", new AvroByteReaderFactory());}
 static BatchDataflowWorker beam_f7323_0(WorkUnitClient workUnitClient, DataflowWorkerHarnessOptions options)
{    return new BatchDataflowWorker(null, SdkHarnessRegistries.emptySdkHarnessRegistry(), workUnitClient, IntrinsicMapTaskExecutorFactory.defaultFactory(), options);}
 static BatchDataflowWorker beam_f7324_0(@Nullable RunnerApi.Pipeline pipeline, SdkHarnessRegistry sdkHarnessRegistry, WorkUnitClient workUnitClient, DataflowWorkerHarnessOptions options)
{    return new BatchDataflowWorker(pipeline, sdkHarnessRegistry, workUnitClient, BeamFnMapTaskExecutorFactory.defaultFactory(), options);}
private static MetricsContainerRegistry<MetricsContainerImpl> beam_f7333_0()
{    return new MetricsContainerRegistry<MetricsContainerImpl>() {        @Override        protected MetricsContainerImpl createContainer(String stepName) {            return new MetricsContainerImpl(stepName);        }    };}
protected MetricsContainerImpl beam_f7334_0(String stepName)
{    return new MetricsContainerImpl(stepName);}
protected void beam_f7343_0(Object newKey)
{    for (StepContext stepContext : getAllStepContexts()) {        stepContext.setKey(newKey);    }}
public Object beam_f7344_0()
{    return key;}
public TimerInternals beam_f7353_0()
{    throw new UnsupportedOperationException("System timerInternals() are not supported in Batch mode." + " Perhaps you meant stepContext.namespacedToUser().timerInternals()");}
public TimerData beam_f7354_0(Coder<W> windowCoder)
{        return null;}
public Iterable<CounterUpdate> beam_f7364_0(boolean isFinalUpdate)
{    return executionStateRegistry.extractUpdates(isFinalUpdate);}
public Long beam_f7365_0()
{    Long totalThrottleTime = 0L;    for (MetricsContainerImpl container : containerRegistry.getContainers()) {                CounterCell dataStoreThrottlingTime = container.tryGetCounter(MetricName.named(BatchModeExecutionContext.DATASTORE_THROTTLE_TIME_NAMESPACE, "cumulativeThrottlingSeconds"));        if (dataStoreThrottlingTime != null) {            totalThrottleTime += dataStoreThrottlingTime.getCumulative();        }        CounterCell httpClientApiThrottlingTime = container.tryGetCounter(MetricName.named(BatchModeExecutionContext.HTTP_CLIENT_API_THROTTLE_TIME_NAMESPACE, "cumulativeThrottlingSeconds"));        if (httpClientApiThrottlingTime != null) {            totalThrottleTime += httpClientApiThrottlingTime.getCumulative();        }        CounterCell throttlingMsecs = container.tryGetCounter(DataflowSystemMetrics.THROTTLING_MSECS_METRIC_NAME);        if (throttlingMsecs != null) {            totalThrottleTime += TimeUnit.MILLISECONDS.toSeconds(throttlingMsecs.getCumulative());        }    }    return totalThrottleTime;}
private Function<Node, Node> beam_f7375_0(final Network<Node, Edge> network, final FnDataService beamFnDataService, final OperationContext context)
{    return new TypeSafeNodeFunction<RemoteGrpcPortNode>(RemoteGrpcPortNode.class) {        @Override        public Node typedApply(RemoteGrpcPortNode input) {            RegisterAndProcessBundleOperation registerFnOperation = (RegisterAndProcessBundleOperation) Iterables.getOnlyElement(Iterables.filter(network.adjacentNodes(input), OperationNode.class)).getOperation();                        Coder<?> coder = Iterables.getOnlyElement(Iterables.filter(network.adjacentNodes(input), OutputReceiverNode.class)).getCoder();                                    Iterable<OutputReceiverNode> outputReceiverNodes = Iterables.filter(network.successors(input), OutputReceiverNode.class);            Operation operation;            if (outputReceiverNodes.iterator().hasNext()) {                OutputReceiver[] outputReceivers = new OutputReceiver[] { Iterables.getOnlyElement(outputReceiverNodes).getOutputReceiver() };                operation = new RemoteGrpcPortReadOperation<>(beamFnDataService, input.getPrimitiveTransformId(), registerFnOperation::getProcessBundleInstructionId, (Coder) coder, outputReceivers, context);            } else {                operation = new RemoteGrpcPortWriteOperation<>(beamFnDataService, input.getPrimitiveTransformId(), registerFnOperation::getProcessBundleInstructionId, (Coder) coder, context);            }            return OperationNode.create(operation);        }    };}
public Node beam_f7376_0(RemoteGrpcPortNode input)
{    RegisterAndProcessBundleOperation registerFnOperation = (RegisterAndProcessBundleOperation) Iterables.getOnlyElement(Iterables.filter(network.adjacentNodes(input), OperationNode.class)).getOperation();        Coder<?> coder = Iterables.getOnlyElement(Iterables.filter(network.adjacentNodes(input), OutputReceiverNode.class)).getCoder();            Iterable<OutputReceiverNode> outputReceiverNodes = Iterables.filter(network.successors(input), OutputReceiverNode.class);    Operation operation;    if (outputReceiverNodes.iterator().hasNext()) {        OutputReceiver[] outputReceivers = new OutputReceiver[] { Iterables.getOnlyElement(outputReceiverNodes).getOutputReceiver() };        operation = new RemoteGrpcPortReadOperation<>(beamFnDataService, input.getPrimitiveTransformId(), registerFnOperation::getProcessBundleInstructionId, (Coder) coder, outputReceivers, context);    } else {        operation = new RemoteGrpcPortWriteOperation<>(beamFnDataService, input.getPrimitiveTransformId(), registerFnOperation::getProcessBundleInstructionId, (Coder) coder, context);    }    return OperationNode.create(operation);}
 Function<Node, Node> beam_f7385_0(final String stageName, final Network<Node, Edge> network, final PipelineOptions options, final ReaderFactory readerFactory, final SinkFactory sinkFactory, final DataflowExecutionContext<?> executionContext)
{    return new TypeSafeNodeFunction<ParallelInstructionNode>(ParallelInstructionNode.class) {        @Override        public Node typedApply(ParallelInstructionNode node) {            ParallelInstruction instruction = node.getParallelInstruction();            NameContext nameContext = NameContext.create(stageName, instruction.getOriginalName(), instruction.getSystemName(), instruction.getName());            try {                DataflowOperationContext context = executionContext.createOperationContext(nameContext);                if (instruction.getRead() != null) {                    return createReadOperation(network, node, options, readerFactory, executionContext, context);                } else if (instruction.getWrite() != null) {                    return createWriteOperation(node, options, sinkFactory, executionContext, context);                } else if (instruction.getParDo() != null) {                    return createParDoOperation(network, node, options, executionContext, context);                } else if (instruction.getPartialGroupByKey() != null) {                    return createPartialGroupByKeyOperation(network, node, options, executionContext, context);                } else if (instruction.getFlatten() != null) {                    return createFlattenOperation(network, node, context);                } else {                    throw new IllegalArgumentException(String.format("Unexpected instruction: %s", instruction));                }            } catch (Exception e) {                throw new RuntimeException(e);            }        }    };}
public Node beam_f7386_0(ParallelInstructionNode node)
{    ParallelInstruction instruction = node.getParallelInstruction();    NameContext nameContext = NameContext.create(stageName, instruction.getOriginalName(), instruction.getSystemName(), instruction.getName());    try {        DataflowOperationContext context = executionContext.createOperationContext(nameContext);        if (instruction.getRead() != null) {            return createReadOperation(network, node, options, readerFactory, executionContext, context);        } else if (instruction.getWrite() != null) {            return createWriteOperation(node, options, sinkFactory, executionContext, context);        } else if (instruction.getParDo() != null) {            return createParDoOperation(network, node, options, executionContext, context);        } else if (instruction.getPartialGroupByKey() != null) {            return createPartialGroupByKeyOperation(network, node, options, executionContext, context);        } else if (instruction.getFlatten() != null) {            return createFlattenOperation(network, node, context);        } else {            throw new IllegalArgumentException(String.format("Unexpected instruction: %s", instruction));        }    } catch (Exception e) {        throw new RuntimeException(e);    }}
 static OutputReceiver[] beam_f7395_0(Network<Node, Edge> network, Node node)
{    int outDegree = network.outDegree(node);    if (outDegree == 0) {        return EMPTY_OUTPUT_RECEIVER_ARRAY;    }    OutputReceiver[] receivers = new OutputReceiver[outDegree];    Iterator<Node> receiverNodes = network.successors(node).iterator();    int i = 0;    do {        receivers[i] = ((OutputReceiverNode) receiverNodes.next()).getOutputReceiver();        i += 1;    } while (receiverNodes.hasNext());    return receivers;}
public boolean beam_f7396_0(T value)
{    return coder.isRegisterByteSizeObserverCheap(value);}
private static DoFnInfo<?, ?> beam_f7405_0(AppliedCombineFn<K, InputT, AccumT, OutputT> combineFn, SideInputReader sideInputReader, String phase)
{    switch(phase) {        case CombinePhase.ALL:            return CombineValuesDoFn.createDoFnInfo(combineFn, sideInputReader);        case CombinePhase.ADD:            return AddInputsDoFn.createDoFnInfo(combineFn, sideInputReader);        case CombinePhase.MERGE:            return MergeAccumulatorsDoFn.createDoFnInfo(combineFn, sideInputReader);        case CombinePhase.EXTRACT:            return ExtractOutputDoFn.createDoFnInfo(combineFn, sideInputReader);        default:            throw new IllegalArgumentException("phase must be one of 'all', 'add', 'merge', 'extract'");    }}
private static DoFnInfo<?, ?> beam_f7406_0(AppliedCombineFn<K, InputT, AccumT, OutputT> combineFn, SideInputReader sideInputReader)
{    GlobalCombineFnRunner<InputT, AccumT, OutputT> combineFnRunner = GlobalCombineFnRunners.create(combineFn.getFn());    DoFn<KV<K, Iterable<InputT>>, KV<K, OutputT>> doFn = new CombineValuesDoFn<>(combineFnRunner, sideInputReader);    Coder<KV<K, Iterable<InputT>>> inputCoder = null;    if (combineFn.getKvCoder() != null) {        inputCoder = KvCoder.of(combineFn.getKvCoder().getKeyCoder(), IterableCoder.of(combineFn.getKvCoder().getValueCoder()));    }    return DoFnInfo.forFn(doFn, combineFn.getWindowingStrategy(), combineFn.getSideInputViews(), inputCoder,     Collections.emptyMap(), new TupleTag<>(PropertyNames.OUTPUT), DoFnSchemaInformation.create(), Collections.emptyMap());}
public Iterator<Source> beam_f7415_0()
{    return sources.iterator();}
public ConcatIterator<T> beam_f7416_0() throws IOException
{    return new ConcatIterator<T>(readerFactory, options, executionContext, operationContext, sources);}
public NativeReader<?> beam_f7425_0(CloudObject spec, @Nullable Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{    @SuppressWarnings("unchecked")    Coder<Object> typedCoder = (Coder<Object>) coder;    return createTyped(spec, typedCoder, options, executionContext, operationContext);}
public NativeReader<T> beam_f7426_0(CloudObject spec, @Nullable Coder<T> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{    List<Source> sources = getSubSources(spec);    return new ConcatReader<T>(registry, options, executionContext, operationContext, sources);}
public String beam_f7435_0()
{    return name.getFlatName();}
public AccumT beam_f7436_0()
{    return internal.getAggregate();}
protected void beam_f7445_0(long elementByteSize)
{    counter.addValue(elementByteSize);}
protected Counter<InputT, AccumT> beam_f7446_0(CounterName name, AtomicCounterValue<InputT, AccumT> counterValue)
{    return new Counter<>(name, counterValue);}
public Counter<Double, Double> beam_f7455_0(CounterName name)
{    return createCounter(name, new DoubleSumCounterValue());}
public Counter<Double, Double> beam_f7456_0(CounterName name)
{    return createCounter(name, new DoubleMinCounterValue());}
public static CounterDistribution beam_f7465_0()
{    return EMPTY;}
public final CounterDistribution beam_f7466_0(long value)
{    Preconditions.checkArgument(value >= 0, "Distribution counters support only non-negative numbers.");    long min = Math.min(this.getMin(), value);    long max = Math.max(this.getMax(), value);    long count = this.getCount() + 1;    long sum = this.getSum() + value;        double sumOfSquares = this.getSumOfSquares() + Math.pow(value, 2);    int bucketIndex = calculateBucket(value);    List<Long> buckets = incrementBucket(bucketIndex);    int firstBucketOffset = this.getBuckets().isEmpty() ? bucketIndex : Math.min(this.getFirstBucketOffset(), bucketIndex);    return CounterDistribution.builder().minMax(min, max).count(count).sum(sum).sumOfSquares(sumOfSquares).buckets(firstBucketOffset, buckets).build();}
public Long beam_f7475_0()
{    return aggregate.getAndSet(Long.MIN_VALUE);}
public UpdateT beam_f7476_0(CounterName name, boolean delta, CounterUpdateExtractor<UpdateT> updateExtractor)
{    return updateExtractor.longMax(name, delta, extractValue(delta));}
public Integer beam_f7485_0()
{    return aggregate.get();}
public void beam_f7486_0(Integer value)
{    int current;    int update;    do {        current = aggregate.get();        update = Math.min(value, current);    } while (update < current && !aggregate.compareAndSet(current, update));}
protected CounterMean<Integer> beam_f7495_0()
{    return IntegerCounterMean.ZERO;}
public UpdateT beam_f7496_0(CounterName name, boolean delta, CounterUpdateExtractor<UpdateT> updateExtractor)
{    return updateExtractor.intMean(name, delta, extractValue(delta));}
public Double beam_f7505_0()
{    return aggregate.getAndSet(0);}
public UpdateT beam_f7506_0(CounterName name, boolean delta, CounterUpdateExtractor<UpdateT> updateExtractor)
{    return updateExtractor.doubleSum(name, delta, extractValue(delta));}
public Boolean beam_f7515_0()
{    return aggregate.get();}
public UpdateT beam_f7516_0(CounterName name, boolean delta, CounterUpdateExtractor<UpdateT> updateExtractor)
{    return updateExtractor.boolOr(name, delta, extractValue(delta));}
public long beam_f7525_0()
{    return count;}
public CounterMean<Integer> beam_f7526_0(Integer value)
{    return new IntegerCounterMean(aggregate + value, count + 1);}
public boolean beam_f7535_0(Object obj)
{    if (obj == this) {        return true;    } else if (!(obj instanceof DoubleCounterMean)) {        return false;    }    DoubleCounterMean that = (DoubleCounterMean) obj;    return this.aggregate == that.aggregate && this.count == that.count;}
public int beam_f7536_0()
{    return Objects.hash(aggregate, count);}
public CounterName beam_f7545_0(String stepName)
{    return create(name(), origin(), stepName, prefix(), contextOriginalName(), contextSystemName(), originalRequestingStepName(), inputIndex());}
public CounterName beam_f7546_0(String prefix)
{    return create(name(), origin(), stepName(), prefix, contextOriginalName(), contextSystemName(), originalRequestingStepName(), inputIndex());}
public String beam_f7555_0()
{    if (prettyName == null) {        synchronized (this) {            if (prettyName == null) {                prettyName = toPrettyString();            }        }    }    return prettyName;}
public boolean beam_f7556_0()
{    return contextOriginalName() != null;}
private List<UpdateT> beam_f7565_0(boolean delta, boolean dirtyOnly, CounterUpdateExtractor<UpdateT> extractors)
{                List<UpdateT> updates = new ArrayList<>(counters.size());    for (Counter<?, ?> counter : counters.values()) {                        boolean shouldCommit = delta && dirtyOnly && counter.isDirty();        if (shouldCommit) {            counter.committing();        }        if (!dirtyOnly || shouldCommit) {            UpdateT update = extractUpdate(counter, delta, extractors);            if (update != null) {                updates.add(update);            }        }        if (shouldCommit) {            counter.committed();        }    }    return Collections.unmodifiableList(updates);}
public List<UpdateT> beam_f7566_0(boolean delta, CounterUpdateExtractor<UpdateT> extractors)
{    return extractUpdatesImpl(delta, false, extractors);}
public CounterUpdate beam_f7575_0(CounterName name, boolean delta, Long value)
{    return longUpdate(name, delta, "SUM", value);}
public CounterUpdate beam_f7576_0(CounterName name, boolean delta, Long value)
{    return longUpdate(name, delta, "MIN", value);}
public CounterUpdate beam_f7585_0(CounterName name, boolean delta, Double value)
{    return doubleUpdate(name, delta, "MAX", value);}
public CounterUpdate beam_f7586_0(CounterName name, boolean delta, CounterMean<Double> value)
{    if (value.getCount() <= 0) {        return null;    }    return initUpdate(name, delta, "MEAN").setFloatingPointMean(new FloatingPointMean().setSum(value.getAggregate()).setCount(longToSplitInt(value.getCount())));}
public void beam_f7595_0(@Nullable java.util.List<CounterUpdate> counters)
{    if (counters == null) {        return;    }    for (CounterUpdate update : counters) {        cache.shortenIdsIfAvailable(update);    }}
public void beam_f7596_0(CounterUpdate update)
{    Long shortId;    if (update.getNameAndKind() != null) {        String name = checkNotNull(update.getNameAndKind().getName(), "Counter update name should be non-null");        shortId = unstructuredShortIdMap.get(name);    } else if (update.getStructuredNameAndMetadata() != null) {        CounterStructuredName name = checkNotNull(update.getStructuredNameAndMetadata().getName(), "Counter update structured-name should be non-null");        shortId = structuredShortIdMap.get(name);    } else {        throw new IllegalArgumentException("CounterUpdate should have nameAndKind or structuredNameAndmetadata");    }    if (shortId != null) {        update.setNameAndKind(null);        update.setStructuredNameAndMetadata(null);        update.setShortId(shortId);    }}
private static BackOff beam_f7608_0()
{    return FluentBackoff.DEFAULT.withInitialBackoff(Duration.millis(BACKOFF_INITIAL_INTERVAL_MILLIS)).withMaxBackoff(Duration.millis(BACKOFF_MAX_INTERVAL_MILLIS)).backoff();}
private static int beam_f7609_0(DataflowWorkerHarnessOptions pipelineOptions)
{    if (pipelineOptions.getNumberOfWorkerHarnessThreads() != 0) {        return pipelineOptions.getNumberOfWorkerHarnessThreads();    }    return Math.max(Runtime.getRuntime().availableProcessors(), 1);}
public void beam_f7618_0(long millisSinceLastSample)
{    executionReader.takeSample(Duration.ofMillis(millisSinceLastSample));}
public void beam_f7619_0(NameContext step)
{    ElementExecution execution = new ElementExecution(step);    addExecution(execution);}
public Iterable<T> beam_f7628_0(long snapshot)
{                    PeekingIterator<SnapshottedItem<T>> iter = Iterators.peekingIterator(queue.iterator());    return () -> new Iterator<T>() {        @Override        public boolean hasNext() {            return iter.hasNext() && iter.peek().snapshot <= snapshot;        }        @Override        public T next() {            SnapshottedItem<T> next = iter.next();            checkValidSnapshot(next);            return next.item;        }        @Override        public void remove() {            iter.remove();        }        private void checkValidSnapshot(SnapshottedItem<T> next) {            if (next.snapshot > snapshot) {                throw new NoSuchElementException();            }        }    };}
public boolean beam_f7629_0()
{    return iter.hasNext() && iter.peek().snapshot <= snapshot;}
 void beam_f7638_0(long bytes)
{    bytesSinked += bytes;}
protected void beam_f7639_0()
{        bytesSinked = 0;}
public Closeable beam_f7648_0(ExecutionState newState)
{    Closeable baseCloseable = super.enterState(newState);    final boolean isDataflowProcessElementState = newState.isProcessElementState && newState instanceof DataflowExecutionState;    if (isDataflowProcessElementState) {        elementExecutionTracker.enter(((DataflowExecutionState) newState).getStepName());    }    return () -> {        if (isDataflowProcessElementState) {            elementExecutionTracker.exit();        }        baseCloseable.close();    };}
public String beam_f7649_0()
{    return this.workItemId;}
public Distribution beam_f7658_0(MetricName metricName)
{    return getCurrentContainer().getDistribution(metricName);}
public Gauge beam_f7659_0(MetricName metricName)
{    return getCurrentContainer().getGauge(metricName);}
public MetricsContainer beam_f7668_0()
{    return metricsContainer;}
private Closeable beam_f7669_0(ExecutionState state)
{    return executionStateTracker.enterState(state);}
 static String beam_f7678_0(Duration duration)
{    return DURATION_FORMATTER.print(duration.toPeriod());}
public void beam_f7679_0(Object elem) throws Exception
{    objectAndByteCounter.update(elem);    long windowsSize = ((WindowedValue<?>) elem).getWindows().size();    if (windowsSize == 0) {                                elementCount.addValue(1L);    } else {        elementCount.addValue(windowsSize);    }}
public ViewFn<?, MultimapView<K, V>> beam_f7688_0()
{    return (ViewFn) PortabilityViewFn.INSTANCE;}
public Materialization<MultimapView<K, V>> beam_f7689_0()
{    return Materializations.multimap();}
public WindowMappingFn<W> beam_f7698_0()
{    throw new UnsupportedOperationException();}
public Coder<?> beam_f7699_0()
{    return coder.getValueCoder();}
private static void beam_f7708_0(WindowedValue<KeyedWorkItem<byte[], T>> windowedKWI)
{        Collection<? extends BoundedWindow> outerWindows = windowedKWI.getWindows();    if (!outerWindows.isEmpty()) {        checkArgument(outerWindows.size() == 1, "The KeyedWorkItem itself must not be in multiple windows, but was in: %s", outerWindows);        BoundedWindow onlyWindow = Iterables.getOnlyElement(outerWindows);        checkArgument(onlyWindow instanceof GlobalWindow, "KeyedWorkItem must be in the Global window, but was in: %s", onlyWindow);    }}
private static BoundedWindow beam_f7709_0(KeyedWorkItem<byte[], T> kwi)
{    if (Iterables.isEmpty(kwi.elementsIterable())) {                TimerData timer = Iterables.getOnlyElement(kwi.timersIterable());        return ((WindowNamespace) timer.getNamespace()).getWindow();    } else {                                        WindowedValue<T> value = Iterables.getOnlyElement(kwi.elementsIterable());        return Iterables.getOnlyElement(value.getWindows());    }}
public String beam_f7718_0()
{    return MoreObjects.toStringHelper(this).add("sideInputIndex", sideInputIndex).add("declaringStep", declaringOperationContext.nameContext()).toString();}
public CounterName beam_f7719_0()
{    return CounterName.named(name);}
protected String beam_f7728_0()
{    return workItemStatusClient.uniqueWorkId();}
protected long beam_f7729_0()
{    return getLeaseExpirationTimestamp(workItem);}
public WorkItemServiceState beam_f7738_1(WorkItemStatus workItemStatus) throws IOException
{    DateTime endTime = DateTime.now();    workItemStatus.setFactory(Transport.getJsonFactory());            if (firstNonNull(workItemStatus.getCompleted(), Boolean.FALSE) && DataflowWorkerLoggingMDC.getStageName() != null) {        DateTime startTime = stageStartTime.get();        if (startTime != null) {                        Interval elapsed = new Interval(startTime, endTime);            int numErrors = workItemStatus.getErrors() == null ? 0 : workItemStatus.getErrors().size();                    }    }    shortIdCache.shortenIdsIfAvailable(workItemStatus.getCounterUpdates());    ReportWorkItemStatusRequest request = new ReportWorkItemStatusRequest().setWorkerId(options.getWorkerId()).setWorkItemStatuses(Collections.singletonList(workItemStatus)).setCurrentWorkerTime(toCloudTime(endTime));    ReportWorkItemStatusResponse result = dataflow.projects().locations().jobs().workItems().reportStatus(options.getProject(), options.getRegion(), options.getJobId(), request).execute();    if (result == null) {                throw new IOException("Got null work item status response");    }    if (result.getWorkItemServiceStates() == null) {                throw new IOException("Report work item status contained no work item service states");    }    if (result.getWorkItemServiceStates().size() != 1) {                throw new IOException("This version of the SDK expects exactly one work item service state from the service " + "but got " + result.getWorkItemServiceStates().size() + " states");    }    shortIdCache.storeNewShortIds(request, result);    WorkItemServiceState state = result.getWorkItemServiceStates().get(0);        return state;}
public ParDoFn beam_f7739_0(PipelineOptions options, CloudObject cloudUserFn, List<SideInputInfo> sideInputInfos, TupleTag<?> mainOutputTag, Map<TupleTag<?>, Integer> outputTupleTagsToReceiverIndices, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    String className = cloudUserFn.getClassName();    ParDoFnFactory factory = defaultFactories.get(className);    if (factory == null) {        throw new Exception("No known ParDoFnFactory for " + className);    }    return factory.create(options, cloudUserFn, sideInputInfos, mainOutputTag, outputTupleTagsToReceiverIndices, executionContext, operationContext);}
public Long beam_f7748_0()
{    return value.getAndSet(0);}
public void beam_f7749_0(long n)
{    update(DistributionData.singleton(n));}
public DoFnInfo<?, ?> beam_f7758_0() throws Exception
{    DoFnInfo<?, ?> fn = fns.peek();    if (fn == null) {        fn = deserializeCopy();        fns.offer(fn);    }    return fn;}
public DoFnInfo<?, ?> beam_f7759_0() throws Exception
{    DoFnInfo<?, ?> fn = fns.poll();    if (fn == null) {        fn = deserializeCopy();    }    return fn;}
 static ExperimentContext beam_f7770_0(Iterable<String> experimentNames)
{    return new ExperimentContext(experimentNames);}
public boolean beam_f7771_0(Experiment exp)
{    return enabledExperiments.contains(exp);}
public Map<TupleTag<?>, PValue> beam_f7780_0()
{    return delegate.expand();}
public void beam_f7781_0(String transformName, PInput input, PTransform<?, ?> transform)
{    delegate.finishSpecifyingOutput(transformName, input, transform);}
public boolean beam_f7790_0()
{    return true;}
public static Iterable<String> beam_f7791_0(String filepattern)
{    ImmutableList.Builder<String> builder = ImmutableList.builder();    Matcher match = AT_N_SPEC.matcher(filepattern);    if (!match.find()) {        builder.add(filepattern);    } else {        int numShards = Integer.parseInt(match.group("N"));        String formatString = "-%0" + getShardWidth(numShards, filepattern) + "d-of-%05d";        for (int i = 0; i < numShards; ++i) {            builder.add(AT_N_SPEC.matcher(filepattern).replaceAll(String.format(formatString, i, numShards)));        }        if (match.find()) {            throw new IllegalArgumentException("More than one @N wildcard found in filepattern: " + filepattern);        }    }    return builder.build();}
public Progress beam_f7801_0() throws Exception
{    return progressTracker.getWorkerProgress();}
public Iterable<CounterUpdate> beam_f7802_0()
{    List<CounterUpdate> result = progressTracker.extractCounterUpdates();    if ((result != null) && (result.size() > 0)) {        return result;    }        MetricUpdates updates = progressTracker.extractMetricUpdates();    Iterable<CounterUpdate> deprecatedMetrics = Iterables.concat(StreamSupport.stream(updates.counterUpdates().spliterator(), false).map(update -> MetricsToCounterUpdateConverter.fromCounter(update.getKey(), true, update.getUpdate())).collect(Collectors.toList()), StreamSupport.stream(updates.distributionUpdates().spliterator(), false).map(update -> MetricsToCounterUpdateConverter.fromDistribution(update.getKey(), true, update.getUpdate())).collect(Collectors.toList()));    return deprecatedMetrics;}
public Progress beam_f7813_0() throws Exception
{    return readOperation.getProgress();}
public MetricUpdates beam_f7814_0()
{    return MetricUpdates.EMPTY;}
public Progress beam_f7823_0() throws Exception
{    return latestProgress.get();}
public List<CounterUpdate> beam_f7824_0()
{    return counterUpdates;}
public T beam_f7833_0(double x)
{    purgeUpTo(x);    return interpolate(x);}
public void beam_f7834_0()
{        Arrays.fill(xs, Double.NaN);        Arrays.fill(ys, null);    numPoints = 0;}
public CounterUpdate beam_f7843_1(MonitoringInfo monitoringInfo)
{    Optional<String> validationResult = validate(monitoringInfo);    if (validationResult.isPresent()) {                return null;    }    long value = monitoringInfo.getMetric().getCounterData().getInt64Value();    final String pcollectionId = monitoringInfo.getLabelsMap().get(MonitoringInfoConstants.Labels.PCOLLECTION);    final String pcollectionName = pcollectionIdToNameContext.get(pcollectionId).userName();    String counterName = pcollectionName + "-ElementCount";    NameAndKind name = new NameAndKind();    name.setName(counterName).setKind("SUM");    return new CounterUpdate().setNameAndKind(name).setCumulative(true).setInteger(DataflowCounterUpdateExtractor.longToSplitInt(value));}
public static String beam_f7844_0()
{    return SUPPORTED_URN;}
public FnDataReceiver<?> beam_f7853_0(String pCollectionId)
{    return receivedElement -> receive(pCollectionId, receivedElement);}
private StateRequestHandler beam_f7854_0(ExecutableStage executableStage, StateRequestHandlers.SideInputHandlerFactory sideInputHandlerFactory)
{    final StateRequestHandler sideInputHandler;    try {        sideInputHandler = StateRequestHandlers.forSideInputHandlerFactory(ProcessBundleDescriptors.getSideInputs(executableStage), sideInputHandlerFactory);    } catch (IOException e) {        throw new RuntimeException("Failed to setup state handler", e);    }    EnumMap<BeamFnApi.StateKey.TypeCase, StateRequestHandler> handlerMap = new EnumMap<BeamFnApi.StateKey.TypeCase, StateRequestHandler>(BeamFnApi.StateKey.TypeCase.class);    handlerMap.put(BeamFnApi.StateKey.TypeCase.MULTIMAP_SIDE_INPUT, sideInputHandler);    return StateRequestHandlers.delegateBasedUponType(handlerMap);}
public String beam_f7863_0()
{    return processBundleId;}
public void beam_f7864_0() throws Exception
{    try (Closeable scope = context.enterStart()) {        super.start();                if (registerFuture == null) {            InstructionRequest request = InstructionRequest.newBuilder().setInstructionId(idGenerator.getId()).setRegister(registerRequest).build();            registerFuture = instructionRequestHandler.handle(request);            getRegisterResponse(registerFuture);        }        checkState(registerRequest.getProcessBundleDescriptorCount() == 1, "Only one bundle registration at a time currently supported.");        InstructionRequest processBundleRequest = InstructionRequest.newBuilder().setInstructionId(getProcessBundleInstructionId()).setProcessBundle(ProcessBundleRequest.newBuilder().setProcessBundleDescriptorId(registerRequest.getProcessBundleDescriptor(0).getId())).build();        deregisterStateHandler = beamFnStateDelegator.registerForProcessBundleInstructionId(getProcessBundleInstructionId(), this::delegateByStateKeyType);        processBundleResponse = instructionRequestHandler.handle(processBundleRequest);    }}
public List<MonitoringInfo> beam_f7873_0(Iterable<MonitoringInfo> monitoringInfos)
{    List<MonitoringInfo> result = new ArrayList<MonitoringInfo>();    if (grpcReadTransformReadWritePCollectionNames.isEmpty()) {        return result;    }    for (MonitoringInfo mi : monitoringInfos) {        if (mi.getUrn().equals(MonitoringInfoConstants.Urns.ELEMENT_COUNT)) {            String pcollection = mi.getLabelsOrDefault(MonitoringInfoConstants.Labels.PCOLLECTION, null);            if ((pcollection != null) && (grpcReadTransformReadWritePCollectionNames.contains(pcollection))) {                result.add(mi);            }        }    }    return result;}
 long beam_f7874_0(final Iterable<MonitoringInfo> monitoringInfos)
{    if (grpcReadTransformId == null) {        return 0;    }    for (MonitoringInfo mi : monitoringInfos) {        if (mi.getUrn().equals(MonitoringInfoConstants.Urns.ELEMENT_COUNT)) {            String pcollection = mi.getLabelsOrDefault(MonitoringInfoConstants.Labels.PCOLLECTION, null);            if (pcollection != null && pcollection.equals(grpcReadTransformOutputPCollectionName)) {                return mi.getMetric().getCounterData().getInt64Value();            }        }    }    return 0;}
private static void beam_f7883_0(CompletionStage<?> future)
{    if (future != null) {                future.toCompletableFuture().cancel(true);    }}
private ByteString beam_f7884_0(Iterable<ByteString> values)
{    ByteString rval = ByteString.EMPTY;    if (values != null) {        for (ByteString value : values) {            rval = rval.concat(value);        }    }    return rval;}
private static Map<String, ProcessBundleDescriptors.TimerSpec> beam_f7893_0(ProcessBundleDescriptors.ExecutableProcessBundleDescriptor processBundleDescriptor)
{    Map<String, ProcessBundleDescriptors.TimerSpec> timerIdToTimerSpecMap = new HashMap<>();    processBundleDescriptor.getTimerSpecs().values().forEach(transformTimerMap -> {        for (ProcessBundleDescriptors.TimerSpec timerSpec : transformTimerMap.values()) {            timerIdToTimerSpecMap.put(timerSpec.timerId(), timerSpec);        }    });    return timerIdToTimerSpecMap;}
private static Map<String, Coder<BoundedWindow>> beam_f7894_1(ProcessBundleDescriptor processBundleDescriptor, Map<String, ProcessBundleDescriptors.TimerSpec> timerIdToTimerSpecMap, RunnerApi.Components components)
{    Map<String, Coder<BoundedWindow>> timerWindowCodersMap = new HashMap<>();    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(components);        for (RunnerApi.PTransform pTransform : processBundleDescriptor.getTransformsMap().values()) {        for (String timerId : timerIdToTimerSpecMap.keySet()) {            if (!pTransform.getInputsMap().containsKey(timerId)) {                continue;            }            RunnerApi.Coder windowingCoder = getTimerWindowingCoder(pTransform, timerId, processBundleDescriptor);            try {                timerWindowCodersMap.put(timerId, (Coder<BoundedWindow>) CoderTranslation.fromProto(windowingCoder, rehydratedComponents));            } catch (IOException e) {                String err = String.format("Failed to retrieve window coder for timerId %s. Failed with error: %s", timerId, e.getMessage());                                throw new RuntimeException(err, e);            }        }    }    return timerWindowCodersMap;}
public Endpoints.ApiServiceDescriptor beam_f7903_0()
{    return apiServiceDescriptor;}
public void beam_f7905_0() throws Exception
{    future.get().awaitCompletion();}
private CompletableFuture<BeamFnDataGrpcMultiplexer> beam_f7915_0(String sdkWorkerId)
{    Preconditions.checkNotNull(sdkWorkerId);    return connectedClients.computeIfAbsent(sdkWorkerId, clientId -> new CompletableFuture<>());}
public void beam_f7916_0() throws Exception
{    try (Closeable scope = context.enterStart()) {        bundleId = bundleIdSupplier.getId();        super.start();        inboundDataClient = beamFnDataService.receive(LogicalEndpoint.of(bundleId, ptransformId), coder, this::consumeOutput);    }}
private void beam_f7925_1() throws Exception
{    if (shouldWait()) {        try {            lock.lock();            while (shouldWait()) {                                condition.await();            }        } finally {            lock.unlock();        }    }}
public void beam_f7926_0()
{    usingElementsProcessed = false;    try {        lock.lock();        condition.signal();    } finally {        lock.unlock();    }}
public StreamObserver<BeamFnApi.LogEntry.List> beam_f7935_1(StreamObserver<BeamFnApi.LogControl> outboundObserver)
{        InboundObserver inboundObserver = new InboundObserver(headerAccessor.getSdkWorkerId());    connectedClients.put(inboundObserver, streamObserverFactory.apply(outboundObserver));    return inboundObserver;}
private void beam_f7936_1(StreamObserver<BeamFnApi.LogControl> outboundObserver)
{    if (outboundObserver != null) {        try {            outboundObserver.onCompleted();        } catch (RuntimeException ignored) {                                }    }}
private TargetWindowT beam_f7945_1(SdkFunctionSpec windowMappingFn, BoundedWindow mainWindow)
{    try {        String processRequestInstructionId = idGenerator.getId();        InstructionRequest processRequest = InstructionRequest.newBuilder().setInstructionId(processRequestInstructionId).setProcessBundle(ProcessBundleRequest.newBuilder().setProcessBundleDescriptorId(registerIfRequired())).build();        ConcurrentLinkedQueue<WindowedValue<KV<byte[], TargetWindowT>>> outputValue = new ConcurrentLinkedQueue<>();                InboundDataClient waitForInboundTermination = beamFnDataService.receive(LogicalEndpoint.of(processRequestInstructionId, "write"), inboundCoder, outputValue::add);        CompletionStage<InstructionResponse> processResponse = instructionRequestHandler.handle(processRequest);                try (CloseableFnDataReceiver<WindowedValue<KV<byte[], BoundedWindow>>> outboundConsumer = beamFnDataService.send(LogicalEndpoint.of(processRequestInstructionId, "read"), outboundCoder)) {            outboundConsumer.accept(WindowedValue.valueInGlobalWindow(KV.of(EMPTY_ARRAY, mainWindow)));        }                throwIfFailure(processResponse);        waitForInboundTermination.awaitCompletion();        WindowedValue<KV<byte[], TargetWindowT>> sideInputWindow = outputValue.poll();        checkState(sideInputWindow != null, "Expected side input window to have been emitted by SDK harness.");        checkState(sideInputWindow.getValue() != null, "Side input window emitted by SDK harness was a WindowedValue with no value in it.");        checkState(sideInputWindow.getValue().getValue() != null, "Side input window emitted by SDK harness was a WindowedValue<KV<...>> with a null V.");        checkState(outputValue.isEmpty(), "Expected only a single side input window to have been emitted by " + "the SDK harness but also received %s", outputValue);        return sideInputWindow.getValue().getValue();    } catch (Throwable e) {                throw new IllegalStateException(e);    }}
private synchronized String beam_f7946_0() throws ExecutionException, InterruptedException
{    if (processBundleDescriptorId == null) {        String descriptorId = idGenerator.getId();        CompletionStage<InstructionResponse> response = instructionRequestHandler.handle(InstructionRequest.newBuilder().setInstructionId(idGenerator.getId()).setRegister(RegisterRequest.newBuilder().addProcessBundleDescriptor(processBundleDescriptor.toBuilder().setId(descriptorId).build()).build()).build());        throwIfFailure(response);        processBundleDescriptorId = descriptorId;    }    return processBundleDescriptorId;}
private static boolean beam_f7955_0(Node node)
{    return node instanceof ParallelInstructionNode && ((ParallelInstructionNode) node).getExecutionLocation() == Nodes.ExecutionLocation.SDK_HARNESS;}
public Node beam_f7956_0(MutableNetwork<Node, Edge> input)
{    for (Node node : input.nodes()) {        if (node instanceof RemoteGrpcPortNode || node instanceof ParallelInstructionNode || node instanceof InstructionOutputNode) {            continue;        }        throw new IllegalArgumentException(String.format("Network contains unknown type of node: %s", input));    }        for (Node node : input.nodes()) {        if (node instanceof InstructionOutputNode) {            continue;        }        for (Node successor : input.successors(node)) {            for (Edge edge : input.edgesConnecting(node, successor)) {                if (edge instanceof DefaultEdge) {                    input.removeEdge(edge);                    input.addEdge(node, successor, MultiOutputInfoEdge.create(new MultiOutputInfo().setTag(idGenerator.getId())));                }            }        }    }    RunnerApi.Components.Builder componentsBuilder = RunnerApi.Components.newBuilder();    componentsBuilder.mergeFrom(this.pipeline.getComponents());        if (pipeline.getComponents().getEnvironmentsMap().isEmpty()) {        String envId = Environments.JAVA_SDK_HARNESS_ENVIRONMENT.getUrn() + idGenerator.getId();        componentsBuilder.putEnvironments(envId, Environments.JAVA_SDK_HARNESS_ENVIRONMENT);    }                String globalWindowingStrategyId = "generatedGlobalWindowingStrategy" + idGenerator.getId();    String intervalWindowEncodingWindowingStrategyId = "generatedIntervalWindowEncodingWindowingStrategy" + idGenerator.getId();    SdkComponents sdkComponents = SdkComponents.create(pipeline.getComponents());    try {        registerWindowingStrategy(globalWindowingStrategyId, WindowingStrategy.globalDefault(), componentsBuilder, sdkComponents);        registerWindowingStrategy(intervalWindowEncodingWindowingStrategyId, WindowingStrategy.of(FixedWindows.of(Duration.standardSeconds(1))), componentsBuilder, sdkComponents);    } catch (IOException exc) {        throw new RuntimeException("Could not convert default windowing stratey to proto", exc);    }    Map<Node, String> nodesToPCollections = new HashMap<>();    ImmutableMap.Builder<String, NameContext> ptransformIdToNameContexts = ImmutableMap.builder();    ImmutableMap.Builder<String, Iterable<SideInputInfo>> ptransformIdToSideInputInfos = ImmutableMap.builder();    ImmutableMap.Builder<String, Iterable<PCollectionView<?>>> ptransformIdToPCollectionViews = ImmutableMap.builder();        Set<PCollectionNode> executableStageOutputs = new HashSet<>();        Set<PCollectionNode> executableStageInputs = new HashSet<>();    for (InstructionOutputNode node : Iterables.filter(input.nodes(), InstructionOutputNode.class)) {        InstructionOutput instructionOutput = node.getInstructionOutput();        String coderId = "generatedCoder" + idGenerator.getId();        String windowingStrategyId;        try (ByteString.Output output = ByteString.newOutput()) {            try {                Coder<?> javaCoder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(instructionOutput.getCodec()));                Coder<?> elementCoder = ((WindowedValueCoder<?>) javaCoder).getValueCoder();                sdkComponents.registerCoder(elementCoder);                RunnerApi.Coder coderProto = CoderTranslation.toProto(elementCoder, sdkComponents);                componentsBuilder.putCoders(coderId, coderProto);                                if (javaCoder instanceof FullWindowedValueCoder) {                    FullWindowedValueCoder<?> windowedValueCoder = (FullWindowedValueCoder<?>) javaCoder;                    Coder<?> windowCoder = windowedValueCoder.getWindowCoder();                    if (windowCoder instanceof IntervalWindowCoder) {                        windowingStrategyId = intervalWindowEncodingWindowingStrategyId;                    } else if (windowCoder instanceof GlobalWindow.Coder) {                        windowingStrategyId = globalWindowingStrategyId;                    } else {                        throw new UnsupportedOperationException(String.format("Dataflow portable runner harness doesn't support windowing with %s", windowCoder));                    }                } else {                    throw new UnsupportedOperationException("Dataflow portable runner harness only supports FullWindowedValueCoder");                }            } catch (IOException e) {                throw new IllegalArgumentException(String.format("Unable to encode coder %s for output %s", instructionOutput.getCodec(), instructionOutput), e);            } catch (Exception e) {                                OBJECT_MAPPER.writeValue(output, instructionOutput.getCodec());                componentsBuilder.putCoders(coderId, RunnerApi.Coder.newBuilder().setSpec(RunnerApi.FunctionSpec.newBuilder().setPayload(output.toByteString())).build());                                                windowingStrategyId = globalWindowingStrategyId;            }        } catch (IOException e) {            throw new IllegalArgumentException(String.format("Unable to encode coder %s for output %s", instructionOutput.getCodec(), instructionOutput), e);        }                String pcollectionId = node.getPcollectionId();        RunnerApi.PCollection pCollection = RunnerApi.PCollection.newBuilder().setCoderId(coderId).setWindowingStrategyId(windowingStrategyId).setIsBounded(RunnerApi.IsBounded.Enum.BOUNDED).build();        nodesToPCollections.put(node, pcollectionId);        componentsBuilder.putPcollections(pcollectionId, pCollection);                if (isExecutableStageOutputPCollection(input, node)) {            executableStageOutputs.add(PipelineNode.pCollection(pcollectionId, pCollection));        }        if (isExecutableStageInputPCollection(input, node)) {            executableStageInputs.add(PipelineNode.pCollection(pcollectionId, pCollection));        }    }    componentsBuilder.putAllCoders(sdkComponents.toComponents().getCodersMap());    Set<PTransformNode> executableStageTransforms = new HashSet<>();    Set<TimerReference> executableStageTimers = new HashSet<>();    List<UserStateId> userStateIds = new ArrayList<>();    Set<SideInputReference> executableStageSideInputs = new HashSet<>();    for (ParallelInstructionNode node : Iterables.filter(input.nodes(), ParallelInstructionNode.class)) {        ImmutableMap.Builder<String, PCollectionNode> sideInputIds = ImmutableMap.builder();        ParallelInstruction parallelInstruction = node.getParallelInstruction();        String ptransformId = "generatedPtransform" + idGenerator.getId();        ptransformIdToNameContexts.put(ptransformId, NameContext.create(null, parallelInstruction.getOriginalName(), parallelInstruction.getSystemName(), parallelInstruction.getName()));        RunnerApi.PTransform.Builder pTransform = RunnerApi.PTransform.newBuilder();        RunnerApi.FunctionSpec.Builder transformSpec = RunnerApi.FunctionSpec.newBuilder();        List<String> timerIds = new ArrayList<>();        if (parallelInstruction.getParDo() != null) {            ParDoInstruction parDoInstruction = parallelInstruction.getParDo();            CloudObject userFnSpec = CloudObject.fromSpec(parDoInstruction.getUserFn());            String userFnClassName = userFnSpec.getClassName();            if (userFnClassName.equals("CombineValuesFn") || userFnClassName.equals("KeyedCombineFn")) {                transformSpec = transformCombineValuesFnToFunctionSpec(userFnSpec);                ptransformIdToPCollectionViews.put(ptransformId, Collections.emptyList());            } else {                String parDoPTransformId = getString(userFnSpec, PropertyNames.SERIALIZED_FN);                RunnerApi.PTransform parDoPTransform = pipeline.getComponents().getTransformsOrDefault(parDoPTransformId, null);                                if (parDoPTransform != null) {                    checkArgument(parDoPTransform.getSpec().getUrn().equals(PTransformTranslation.PAR_DO_TRANSFORM_URN), "Found transform \"%s\" for ParallelDo instruction, " + " but that transform had unexpected URN \"%s\" (expected \"%s\")", parDoPTransformId, parDoPTransform.getSpec().getUrn(), PTransformTranslation.PAR_DO_TRANSFORM_URN);                    RunnerApi.ParDoPayload parDoPayload;                    try {                        parDoPayload = RunnerApi.ParDoPayload.parseFrom(parDoPTransform.getSpec().getPayload());                    } catch (InvalidProtocolBufferException exc) {                        throw new RuntimeException("ParDo did not have a ParDoPayload", exc);                    }                                        for (Map.Entry<String, RunnerApi.TimerSpec> entry : parDoPayload.getTimerSpecsMap().entrySet()) {                        timerIds.add(entry.getKey());                    }                    for (Map.Entry<String, RunnerApi.StateSpec> entry : parDoPayload.getStateSpecsMap().entrySet()) {                        UserStateId.Builder builder = UserStateId.newBuilder();                        builder.setTransformId(parDoPTransformId);                        builder.setLocalName(entry.getKey());                        userStateIds.add(builder.build());                    }                                        for (String sideInputTag : parDoPayload.getSideInputsMap().keySet()) {                        String sideInputPCollectionId = parDoPTransform.getInputsOrThrow(sideInputTag);                        RunnerApi.PCollection sideInputPCollection = pipeline.getComponents().getPcollectionsOrThrow(sideInputPCollectionId);                        pTransform.putInputs(sideInputTag, sideInputPCollectionId);                        PCollectionNode pCollectionNode = PipelineNode.pCollection(sideInputPCollectionId, sideInputPCollection);                        sideInputIds.put(sideInputTag, pCollectionNode);                    }                                                            ImmutableList.Builder<PCollectionView<?>> pcollectionViews = ImmutableList.builder();                    for (Map.Entry<String, RunnerApi.SideInput> sideInputEntry : parDoPayload.getSideInputsMap().entrySet()) {                        pcollectionViews.add(RegisterNodeFunction.transformSideInputForRunner(pipeline, parDoPTransform, sideInputEntry.getKey(), sideInputEntry.getValue()));                    }                    ptransformIdToPCollectionViews.put(ptransformId, pcollectionViews.build());                    transformSpec.setUrn(PTransformTranslation.PAR_DO_TRANSFORM_URN).setPayload(parDoPayload.toByteString());                } else {                                                            byte[] userFnBytes = getBytes(userFnSpec, PropertyNames.SERIALIZED_FN);                    transformSpec.setUrn(ParDoTranslation.CUSTOM_JAVA_DO_FN_URN).setPayload(ByteString.copyFrom(userFnBytes));                }                if (parDoInstruction.getSideInputs() != null) {                    ptransformIdToSideInputInfos.put(ptransformId, forSideInputInfos(parDoInstruction.getSideInputs(), true));                }            }        } else if (parallelInstruction.getRead() != null) {            ReadInstruction readInstruction = parallelInstruction.getRead();            CloudObject sourceSpec = CloudObject.fromSpec(CloudSourceUtils.flattenBaseSpecs(readInstruction.getSource()).getSpec());                        transformSpec.setUrn(JAVA_SOURCE_URN);            try {                byte[] serializedSource = Base64.getDecoder().decode(getString(sourceSpec, SERIALIZED_SOURCE));                ByteString sourceByteString = ByteString.copyFrom(serializedSource);                transformSpec.setPayload(sourceByteString);            } catch (Exception e) {                throw new IllegalArgumentException(String.format("Unable to process Read %s", parallelInstruction), e);            }        } else if (parallelInstruction.getFlatten() != null) {            transformSpec.setUrn(PTransformTranslation.FLATTEN_TRANSFORM_URN);        } else {            throw new IllegalArgumentException(String.format("Unknown type of ParallelInstruction %s", parallelInstruction));        }                for (Node predecessorOutput : input.predecessors(node)) {            pTransform.putInputs("generatedInput" + idGenerator.getId(), nodesToPCollections.get(predecessorOutput));        }        for (Edge edge : input.outEdges(node)) {            Node nodeOutput = input.incidentNodes(edge).target();            MultiOutputInfoEdge edge2 = (MultiOutputInfoEdge) edge;            pTransform.putOutputs(edge2.getMultiOutputInfo().getTag(), nodesToPCollections.get(nodeOutput));        }        pTransform.setSpec(transformSpec);        PTransformNode pTransformNode = PipelineNode.pTransform(ptransformId, pTransform.build());        executableStageTransforms.add(pTransformNode);        for (String timerId : timerIds) {            executableStageTimers.add(TimerReference.of(pTransformNode, timerId));        }        ImmutableMap<String, PCollectionNode> sideInputIdToPCollectionNodes = sideInputIds.build();        for (String sideInputTag : sideInputIdToPCollectionNodes.keySet()) {            SideInputReference sideInputReference = SideInputReference.of(pTransformNode, sideInputTag, sideInputIdToPCollectionNodes.get(sideInputTag));            executableStageSideInputs.add(sideInputReference);        }        executableStageTransforms.add(pTransformNode);    }    if (executableStageInputs.size() != 1) {        throw new UnsupportedOperationException("ExecutableStage only support one input PCollection");    }    PCollectionNode executableInput = executableStageInputs.iterator().next();    RunnerApi.Components executableStageComponents = componentsBuilder.build();        Environment executableStageEnv = getEnvironmentFromPTransform(executableStageComponents, executableStageTransforms);    if (executableStageEnv == null) {        executableStageEnv = Environments.JAVA_SDK_HARNESS_ENVIRONMENT;    }    Set<UserStateReference> executableStageUserStateReference = new HashSet<>();    for (UserStateId userStateId : userStateIds) {        executableStageUserStateReference.add(UserStateReference.fromUserStateId(userStateId, executableStageComponents));    }    ExecutableStage executableStage = ImmutableExecutableStage.ofFullComponents(executableStageComponents, executableStageEnv, executableInput, executableStageSideInputs, executableStageUserStateReference, executableStageTimers, executableStageTransforms, executableStageOutputs);    return ExecutableStageNode.create(executableStage, ptransformIdToNameContexts.build(), ptransformIdToSideInputInfos.build(), ptransformIdToPCollectionViews.build());}
public MutableNetwork<Node, Edge> beam_f7965_0(MutableNetwork<Node, Edge> network)
{    Map<Node, AggregatedLocation> predecessorLocationsMap = new HashMap<>();    Map<Node, AggregatedLocation> successorLocationsMap = new HashMap<>();    Map<Node, ExecutionLocation> deducedLocationsMap = new HashMap<>();    ImmutableList<Node> flattens = ImmutableList.copyOf(Iterables.filter(network.nodes(), IsFlatten.INSTANCE));        for (Node flatten : flattens) {        AggregatedLocation predecessorLocations = AggregatedLocation.NEITHER;        AggregatedLocation successorLocations = AggregatedLocation.NEITHER;        predecessorLocations = getPredecessorLocations(flatten, network, predecessorLocationsMap);        successorLocations = getSuccessorLocations(flatten, network, successorLocationsMap);        deducedLocationsMap.put(flatten, DEDUCTION_TABLE.get(predecessorLocations, successorLocations));    }        Networks.replaceDirectedNetworkNodes(network, (Node node) -> {        if (!deducedLocationsMap.containsKey(node)) {            return node;        }        ParallelInstructionNode castNode = ((ParallelInstructionNode) node);        ExecutionLocation deducedLocation = deducedLocationsMap.get(node);        return ParallelInstructionNode.create(castNode.getParallelInstruction(), deducedLocation);    });    return network;}
private AggregatedLocation beam_f7966_0(Node node, MutableNetwork<Node, Edge> network, Map<Node, AggregatedLocation> predecessorLocationsMap)
{    return getConnectedNodeLocations(node, network, predecessorLocationsMap, SearchDirection.PREDECESSORS);}
public Edge beam_f7975_0()
{    try {        return (Edge) super.clone();    } catch (CloneNotSupportedException e) {        throw new IllegalStateException(e);    }}
public static DefaultEdge beam_f7976_0()
{    return new AutoValue_Edges_DefaultEdge();}
public static MutableNetwork<Node, Edge> beam_f7985_0(MutableNetwork<Node, Edge> network)
{    Networks.replaceDirectedNetworkNodes(network, forInstructionOutputNode(network));    return network;}
public static List<SideInputInfo> beam_f7986_0(List<SideInputInfo> sideInputInfos, boolean replaceWithByteArrayCoder)
{    ImmutableList.Builder<SideInputInfo> updatedSideInputInfos = ImmutableList.builder();    for (SideInputInfo sideInputInfo : sideInputInfos) {        try {            SideInputInfo updatedSideInputInfo = clone(sideInputInfo, SideInputInfo.class);            for (Source source : updatedSideInputInfo.getSources()) {                source.setCodec(forCodec(source.getCodec(), replaceWithByteArrayCoder));            }            updatedSideInputInfos.add(updatedSideInputInfo);        } catch (IOException e) {            throw new RuntimeException(String.format("Failed to replace unknown coder with " + "LengthPrefixCoder for : {%s}", sideInputInfo), e);        }    }    return updatedSideInputInfos.build();}
 static ParallelInstruction beam_f7995_0(ParallelInstruction input, boolean replaceWithByteArrayCoder) throws Exception
{    try {        ParallelInstruction instruction = clone(input, ParallelInstruction.class);        if (instruction.getRead() != null) {            Source cloudSource = instruction.getRead().getSource();            cloudSource.setCodec(forCodec(cloudSource.getCodec(), replaceWithByteArrayCoder));        } else if (instruction.getWrite() != null) {            com.google.api.services.dataflow.model.Sink cloudSink = instruction.getWrite().getSink();            cloudSink.setCodec(forCodec(cloudSink.getCodec(), replaceWithByteArrayCoder));        } else if (instruction.getParDo() != null) {            instruction.setParDo(forParDoInstruction(instruction.getParDo(), replaceWithByteArrayCoder));        } else if (instruction.getPartialGroupByKey() != null) {            PartialGroupByKeyInstruction pgbk = instruction.getPartialGroupByKey();            pgbk.setInputElementCodec(forCodec(pgbk.getInputElementCodec(), replaceWithByteArrayCoder));        } else if (instruction.getFlatten() != null) {                } else {            throw new RuntimeException("Unknown parallel instruction: " + input);        }        return instruction;    } catch (IOException e) {        throw new RuntimeException(String.format("Failed to replace unknown coder with " + "LengthPrefixCoder for : {%s}", input), e);    }}
private static ParDoInstruction beam_f7996_0(ParDoInstruction input, boolean replaceWithByteArrayCoder) throws Exception
{    ParDoInstruction parDo = input;    CloudObject userFn = CloudObject.fromSpec(parDo.getUserFn());    String parDoFnType = userFn.getClassName();    if (MERGE_WINDOWS_DO_FN.contains(parDoFnType)) {        parDo = clone(input, ParDoInstruction.class);        Map<String, Object> inputCoderObject = Structs.getObject(parDo.getUserFn(), WorkerPropertyNames.INPUT_CODER);        parDo.getUserFn().put(WorkerPropertyNames.INPUT_CODER, forCodec(inputCoderObject, replaceWithByteArrayCoder));    } else if ("CreateIsmShardKeyAndSortKeyDoFn".equals(parDoFnType) || "ToIsmRecordForMultimapDoFn".equals(parDoFnType) || "StreamingPCollectionViewWriterDoFn".equals(parDoFnType)) {        parDo = clone(input, ParDoInstruction.class);        Map<String, Object> inputCoderObject = Structs.getObject(parDo.getUserFn(), PropertyNames.ENCODING);        parDo.getUserFn().put(PropertyNames.ENCODING, forCodec(inputCoderObject, replaceWithByteArrayCoder));    }        return parDo;}
public static String beam_f8005_0(Network<N, E> network)
{    StringBuilder builder = new StringBuilder();    builder.append("digraph network {\n");    Map<N, String> nodeName = Maps.newIdentityHashMap();    network.nodes().forEach(node -> nodeName.put(node, "n" + nodeName.size()));    for (Entry<N, String> nodeEntry : nodeName.entrySet()) {        builder.append(String.format("  %s [fontname=\"Courier New\" label=\"%s\"];%n", nodeEntry.getValue(), escapeDot(nodeEntry.getKey().toString())));    }    for (E edge : network.edges()) {        EndpointPair<N> endpoints = network.incidentNodes(edge);        builder.append(String.format("  %s -> %s [fontname=\"Courier New\" label=\"%s\"];%n", nodeName.get(endpoints.source()), nodeName.get(endpoints.target()), escapeDot(edge.toString())));    }    builder.append("}");    return builder.toString();}
private static String beam_f8006_0(String s)
{    return s.replace("\\", "\\\\").replace("\"", "\\\"").replace("\n", "\\l");}
public void beam_f8015_0() throws IOException
{    baseGenerator.writeStartArray();}
public void beam_f8016_0() throws IOException
{    baseGenerator.writeEndArray();}
public void beam_f8025_0(float v) throws IOException
{    baseGenerator.writeNumber(v);}
public void beam_f8026_0(double v) throws IOException
{    baseGenerator.writeNumber(v);}
public static OperationNode beam_f8035_0(Operation operation)
{    checkNotNull(operation);    return new AutoValue_Nodes_OperationNode(operation);}
public static RemoteGrpcPortNode beam_f8036_0(BeamFnApi.RemoteGrpcPort port, String primitiveTransformId)
{    checkNotNull(port);    return new AutoValue_Nodes_RemoteGrpcPortNode(port, primitiveTransformId);}
private RunnerApi.FunctionSpec.Builder beam_f8045_0(CloudObject userFn)
{            String combinePTransformId = getString(userFn, PropertyNames.SERIALIZED_FN);    RunnerApi.PTransform combinePerKeyPTransform = pipeline.getComponents().getTransformsOrDefault(combinePTransformId, null);    checkArgument(combinePerKeyPTransform != null, "Transform with id \"%s\" not found in pipeline.", combinePTransformId);    checkArgument(combinePerKeyPTransform.getSpec().getUrn().equals(COMBINE_PER_KEY_URN), "Found transform \"%s\" for Combine instruction, " + "but that transform had unexpected URN \"%s\" (expected \"%s\")", combinePerKeyPTransform, combinePerKeyPTransform.getSpec().getUrn(), COMBINE_PER_KEY_URN);    RunnerApi.CombinePayload combinePayload;    try {        combinePayload = RunnerApi.CombinePayload.parseFrom(combinePerKeyPTransform.getSpec().getPayload());    } catch (InvalidProtocolBufferException exc) {        throw new RuntimeException("Combine did not have a CombinePayload", exc);    }    String phase = getString(userFn, WorkerPropertyNames.PHASE, CombinePhase.ALL);    String urn;    switch(phase) {        case CombinePhase.ALL:            urn = COMBINE_GROUPED_VALUES_URN;            break;        case CombinePhase.ADD:            urn = COMBINE_PRECOMBINE_URN;            break;        case CombinePhase.MERGE:            urn = COMBINE_MERGE_URN;            break;        case CombinePhase.EXTRACT:            urn = COMBINE_EXTRACT_URN;            break;        default:            throw new RuntimeException("Encountered unknown Combine Phase: " + phase);    }    return RunnerApi.FunctionSpec.newBuilder().setUrn(urn).setPayload(combinePayload.toByteString());}
public static final PCollectionView<?> beam_f8046_0(RunnerApi.Pipeline pipeline, RunnerApi.PTransform parDoPTransform, String sideInputTag, RunnerApi.SideInput sideInput)
{    checkArgument(Materializations.MULTIMAP_MATERIALIZATION_URN.equals(sideInput.getAccessPattern().getUrn()), "This handler is only capable of dealing with %s materializations " + "but was asked to handle %s for PCollectionView with tag %s.", Materializations.MULTIMAP_MATERIALIZATION_URN, sideInput.getAccessPattern().getUrn(), sideInputTag);    String sideInputPCollectionId = parDoPTransform.getInputsOrThrow(sideInputTag);    RunnerApi.PCollection sideInputPCollection = pipeline.getComponents().getPcollectionsOrThrow(sideInputPCollectionId);    try {        FullWindowedValueCoder<KV<Object, Object>> runnerSideInputCoder = (FullWindowedValueCoder) WireCoders.instantiateRunnerWireCoder(PipelineNode.pCollection(sideInputPCollectionId, sideInputPCollection), pipeline.getComponents());        return DataflowPortabilityPCollectionView.with(new TupleTag<>(sideInputTag), runnerSideInputCoder);    } catch (IOException e) {        throw new IllegalStateException("Unable to translate proto to coder", e);    }}
public void beam_f8056_0(OutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    WindowedValue<OutputT> windowed = WindowedValue.of(output, timestamp, windows, pane);    outputManager.output(mainOutputTag, windowed);}
public void beam_f8057_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    throw new UnsupportedOperationException();}
public AccumT beam_f8067_0(List<AccumT> accumulator)
{    if (accumulator.isEmpty()) {        return combineFn.createAccumulator();    } else {        return combineFn.mergeAccumulators(accumulator);    }}
private List<AccumT> beam_f8068_0(Iterable<AccumT> accumulators)
{    List<AccumT> singleton = new ArrayList<>();    singleton.add(combineFn.mergeAccumulators(accumulators));    return singleton;}
public void beam_f8077_0(Receiver... receivers) throws Exception
{    checkState(fnRunner == null, "bundle already started (or not properly finished)");    checkState(receivers.length == 1, "%s.startBundle() called with %s receivers, expected exactly 1. " + "This is a bug in the Dataflow service", getClass().getSimpleName(), receivers.length);    receiver = receivers[0];    fnRunner = createRunner();    fnRunner.startBundle();}
public void beam_f8078_0(Object untypedElem) throws Exception
{    checkState(fnRunner != null);        if (!acceptsKeyedWorkItems) {                fnRunner.processElement((WindowedValue<InputT>) untypedElem);    } else {                Coder<W> windowCoder = windowingStrategy.getWindowFn().windowCoder();        TimerInternals.TimerData timer;        List<TimerInternals.TimerData> firedTimers = new ArrayList<>();        while ((timer = stepContext.getNextFiredTimer(windowCoder)) != null) {            firedTimers.add(timer);        }                WindowedValue<KeyedWorkItem<K, V>> typedElem = (WindowedValue<KeyedWorkItem<K, V>>) untypedElem;        KeyedWorkItem<K, V> keyedWorkItemWithAllTheTimers = typedElem.getValue();        KeyedWorkItem<K, V> keyedWorkItemWithJustMyTimers = KeyedWorkItems.workItem(keyedWorkItemWithAllTheTimers.key(), firedTimers, keyedWorkItemWithAllTheTimers.elementsIterable());        fnRunner.processElement((WindowedValue<InputT>) typedElem.withValue(keyedWorkItemWithJustMyTimers));    }}
protected void beam_f8088_0(long byteSize)
{            currentGroupSize.addAndGet(byteSize);    parentReader.notifyElementRead(byteSize);}
protected void beam_f8089_0(long bytes)
{    if (shuffleReadCounter != null) {        shuffleReadCounter.addBytesRead(bytes);    }}
public boolean beam_f8098_0()
{    try (Closeable read = tracker.enterState(readState)) {        return base.hasNext();    } catch (IOException e) {        throw new RuntimeException(e);    }}
public V beam_f8099_0()
{    try (Closeable read = tracker.enterState(readState)) {        ShuffleEntry entry = base.next();        checkNotNull(entry);                        notifyValueReturned(currentGroupSize.getAndSet(0L));        try {            if (parentReader.secondaryKeyCoder != null) {                ByteArrayInputStream bais = new ByteArrayInputStream(entry.getSecondaryKey());                @SuppressWarnings("unchecked")                V value = (V) KV.of(                parentReader.secondaryKeyCoder.decode(bais), CoderUtils.decodeFromByteArray(parentReader.valueCoder, entry.getValue()));                return value;            } else {                @SuppressWarnings("unchecked")                V value = (V) CoderUtils.decodeFromByteArray(parentReader.valueCoder, entry.getValue());                return value;            }        } catch (IOException exn) {            throw new RuntimeException(exn);        }    } catch (IOException e) {        throw new RuntimeException(e);    }}
protected boolean beam_f8108_0()
{        long nowMs = clock.currentTimeMillis();    if (nowMs - prevHotKeyDetectionLogMs < loggingPeriod.getMillis()) {        return true;    }    prevHotKeyDetectionLogMs = nowMs;    return false;}
protected String beam_f8109_0(String userStepName, String hotKeyAge)
{    return MessageFormat.format("A hot key was detected in step ''{0}'' with age of ''{1}''. This is" + " a symptom of key distribution being skewed. To fix, please inspect your data and " + "pipeline to ensure that elements are evenly distributed across your key space.", userStepName, hotKeyAge);}
public DynamicSplitResult beam_f8118_0()
{    if (!tracker.trySplitAtPosition(lastReturnedIndex + 1)) {        return null;    }    com.google.api.services.dataflow.model.Position splitPosition = new com.google.api.services.dataflow.model.Position();    splitPosition.setRecordIndex((long) lastReturnedIndex + 1);    return new DynamicSplitResultWithPosition(SourceTranslationUtils.cloudPositionToReaderPosition(splitPosition));}
public Map<String, ReaderFactory> beam_f8119_0()
{    return ImmutableMap.of("InMemorySource", new InMemoryReaderFactory());}
public Node beam_f8128_0(ParallelInstructionNode node)
{    ParallelInstruction instruction = node.getParallelInstruction();    NameContext nameContext = NameContext.create(stageName, instruction.getOriginalName(), instruction.getSystemName(), instruction.getName());    try {        DataflowOperationContext context = executionContext.createOperationContext(nameContext);        if (instruction.getRead() != null) {            return createReadOperation(network, node, options, readerFactory, executionContext, context);        } else if (instruction.getWrite() != null) {            return createWriteOperation(node, options, sinkFactory, executionContext, context);        } else if (instruction.getParDo() != null) {            return createParDoOperation(network, node, options, executionContext, context);        } else if (instruction.getPartialGroupByKey() != null) {            return createPartialGroupByKeyOperation(network, node, options, executionContext, context);        } else if (instruction.getFlatten() != null) {            return createFlattenOperation(network, node, context);        } else {            throw new IllegalArgumentException(String.format("Unexpected instruction: %s", instruction));        }    } catch (Exception e) {        throw new RuntimeException(e);    }}
 OperationNode beam_f8129_0(Network<Node, Edge> network, ParallelInstructionNode node, PipelineOptions options, ReaderFactory readerFactory, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    ParallelInstruction instruction = node.getParallelInstruction();    ReadInstruction read = instruction.getRead();    Source cloudSource = CloudSourceUtils.flattenBaseSpecs(read.getSource());    CloudObject sourceSpec = CloudObject.fromSpec(cloudSource.getSpec());    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(cloudSource.getCodec()));    NativeReader<?> reader = readerFactory.create(sourceSpec, coder, options, executionContext, operationContext);    OutputReceiver[] receivers = getOutputReceivers(network, node);    return OperationNode.create(ReadOperation.create(reader, receivers, operationContext));}
public boolean beam_f8138_0(T value)
{    return coder.isRegisterByteSizeObserverCheap(value);}
public void beam_f8139_0(T value, ElementByteSizeObserver observer) throws Exception
{    coder.registerByteSizeObserver(value, observer);}
public boolean beam_f8148_0(Object obj)
{    if (!(obj instanceof IsmReaderKey)) {        return false;    }    IsmReaderKey key = (IsmReaderKey) obj;    return Objects.equals(filename, key.filename);}
public int beam_f8149_0()
{    return filename.hashCode();}
 boolean beam_f8158_0(RandomAccessData keyBytes)
{    return bloomFilter.mightContain(keyBytes.array(), 0, keyBytes.size());}
private synchronized Optional<SeekableByteChannel> beam_f8159_0(Optional<SeekableByteChannel> inChannel, SideInputReadCounter readCounter) throws IOException
{    if (footer != null) {        checkState(shardIdToShardMap != null, "Expected shard id to shard map to have been initialized.");        checkState(shardOffsetToShardMap != null, "Expected shard offset to shard map to have been initialized.");        return inChannel;    }    checkState(shardIdToShardMap == null, "Expected shard id to shard map to not have been initialized.");    checkState(shardOffsetToShardMap == null, "Expected shard offset to shard map to not have been initialized.");    SeekableByteChannel rawChannel;    RandomAccessData data;    long startPosition;    try (Closeable closeReadCounter = readCounter.enter()) {        rawChannel = openIfNeeded(inChannel);        this.length = rawChannel.size();                                startPosition = Math.max(length - MAX_SHARD_INDEX_AND_FOOTER_SIZE, 0);        position(rawChannel, startPosition);        data = new RandomAccessData(ByteStreams.toByteArray(Channels.newInputStream(rawChannel)));    }    readCounter.addBytesRead(data.size());        this.footer = FooterCoder.of().decode(data.asInputStream(data.size() - Footer.FIXED_LENGTH, Footer.FIXED_LENGTH));    checkState(startPosition < footer.getIndexPosition(), "Malformed file, expected to have been able to read entire shard index.");    int offsetWithinReadData = (int) (footer.getIndexPosition() - startPosition);        List<IsmShard> ismShards = IsmFormat.ISM_SHARD_INDEX_CODER.decode(data.asInputStream(offsetWithinReadData, data.size() - offsetWithinReadData));        ImmutableSortedMap.Builder<Integer, IsmShard> shardIdToShardMapBuilder = ImmutableSortedMap.orderedBy(Ordering.<Integer>natural());    for (IsmShard ismShard : ismShards) {        shardIdToShardMapBuilder.put(ismShard.getId(), ismShard);    }    shardIdToShardMap = shardIdToShardMapBuilder.build();        ImmutableSortedMap.Builder<Long, IsmShard> shardOffsetToShardMapBuilder = ImmutableSortedMap.orderedBy(Ordering.<Long>natural());    for (IsmShard ismShard : ismShards) {        shardOffsetToShardMapBuilder.put(ismShard.getBlockOffset(), ismShard);    }    shardOffsetToShardMap = shardOffsetToShardMapBuilder.build();        if (startPosition < footer.getBloomFilterPosition()) {        Optional<SeekableByteChannel> cachedDataChannel = Optional.<SeekableByteChannel>of(new CachedTailSeekableByteChannel(startPosition, data.array()));        initializeBloomFilterAndIndexPerShard(cachedDataChannel);                if (cache != null && startPosition == 0) {            for (IsmShard ismShard : ismShards) {                initializeForKeyedRead(ismShard.getId(), cachedDataChannel, readCounter);            }            for (SortedMap<RandomAccessData, IsmShardKey> shards : indexPerShard.values()) {                for (Map.Entry<RandomAccessData, IsmShardKey> block : shards.entrySet()) {                    cache.put(block.getValue(), new IsmCacheLoader(block.getValue()).call(cachedDataChannel.get()));                }            }        }    }    return Optional.of(rawChannel);}
protected List<?> beam_f8168_0()
{    return keyComponents;}
public final WindowedValue<IsmRecord<V>> beam_f8169_0(List<?> additionalKeyComponents) throws IOException
{    RandomAccessData keyBytes = new RandomAccessData();    try (Closeable readerCloser = readCounter.enter()) {        int shardId = coder.encodeAndHash(ImmutableList.builder().addAll(keyComponents).addAll(additionalKeyComponents).build(), keyBytes);        return getBlock(keyBytes, shardId, readCounter).get(keyBytes);    }}
private NavigableMap<RandomAccessData, WindowedValue<IsmRecord<V>>> beam_f8178_0(RandomAccessData keyBytes, int shardId, SideInputReadCounter readCounter) throws IOException
{    Optional<SeekableByteChannel> inChannel = initializeFooterAndShardIndex(Optional.<SeekableByteChannel>absent(), readCounter);        if (!shardIdToShardMap.containsKey(shardId) || !bloomFilterMightContain(keyBytes)) {        return ImmutableSortedMap.<RandomAccessData, WindowedValue<IsmRecord<V>>>orderedBy(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR).build();    }    inChannel = initializeForKeyedRead(shardId, inChannel, readCounter);    closeIfPresent(inChannel);    final NavigableMap<RandomAccessData, IsmShardKey> indexInShard = indexPerShard.get(shardId);    final IsmShardKey cacheEntry = indexInShard.floorEntry(keyBytes).getValue();    try (Closeable readerCloseable = IsmReader.setSideInputReadContext(readCounter)) {        return fetch(cacheEntry);    }}
public boolean beam_f8179_0() throws IOException
{    return advance();}
public boolean beam_f8188_0() throws IOException, NoSuchElementException
{    checkState(position <= readLimit, "Read past end of stream");    if (position == readLimit) {        current = Optional.absent();        return false;    }    final IsmRecord<V> ismRecord;    readKey(inStream, keyBytes);    InputStream keyInputStream = keyBytes.asInputStream(0, keyBytes.size());    List<Object> keyComponents = new ArrayList<>(coder.getKeyComponentCoders().size());    for (int i = 0; i < coder.getKeyComponentCoders().size(); ++i) {        keyComponents.add(coder.getKeyComponentCoder(i).decode(keyInputStream));    }    if (IsmFormat.isMetadataKey(keyComponents)) {        byte[] metadata = ByteArrayCoder.of().decode(inStream);        ismRecord = IsmRecord.<V>meta(keyComponents, metadata);    } else {        V value = coder.getValueCoder().decode(inStream);        ismRecord = IsmRecord.<V>of(keyComponents, value);    }    long newPosition = rawChannel.position();    IsmReaderImpl.this.notifyElementRead(newPosition - position);    readCounter.addBytesRead(newPosition - position);    position = newPosition;    current = Optional.of(new ValueInEmptyWindows<>(ismRecord));    return true;}
public WindowedValue<IsmRecord<V>> beam_f8189_0()
{    if (!current.isPresent()) {        throw new NoSuchElementException();    }    return current.get();}
public int beam_f8199_0(ByteBuffer src) throws IOException
{    throw new NonWritableChannelException();}
public long beam_f8200_0() throws IOException
{        return offset + localPosition;}
public ViewT beam_f8209_0(final PCollectionView<ViewT> view, final BoundedWindow window)
{    @SuppressWarnings("rawtypes")    final TupleTag tag = view.getTagInternal();    checkArgument(tagToIsmReaderMap.containsKey(tag), "calling getSideInput() with unknown view");        try {        ViewFn<?, ?> viewFn = view.getViewFn();                if (viewFn instanceof SingletonViewFn) {            ViewT rval = executionContext.<PCollectionViewWindow<ViewT>, ViewT>getLogicalReferenceCache().get(PCollectionViewWindow.of(view, window), () -> {                @SuppressWarnings("unchecked")                ViewT viewT = getSingletonForWindow(tag, (SingletonViewFn<ViewT>) viewFn, window);                @SuppressWarnings("unchecked")                ViewT nullPlaceHolder = (ViewT) NULL_PLACE_HOLDER;                return viewT == null ? nullPlaceHolder : viewT;            });            return rval == NULL_PLACE_HOLDER ? null : rval;        } else if (singletonMaterializedTags.contains(tag)) {            checkArgument(viewFn instanceof MapViewFn || viewFn instanceof MultimapViewFn, "Unknown view type stored as singleton. Expected one of %s, got %s", KNOWN_SINGLETON_VIEW_TYPES, viewFn.getClass().getName());            return executionContext.<PCollectionViewWindow<ViewT>, ViewT>getLogicalReferenceCache().get(PCollectionViewWindow.of(view, window), () -> {                return getMapSingletonForViewAndWindow(tag, window);            });        } else {            return executionContext.<PCollectionViewWindow<ViewT>, ViewT>getLogicalReferenceCache().get(PCollectionViewWindow.of(view, window), () -> {                if (viewFn instanceof IterableViewFn || viewFn instanceof ListViewFn) {                    @SuppressWarnings("unchecked")                    ViewT viewT = (ViewT) getListForWindow(tag, window);                    return viewT;                } else if (viewFn instanceof MapViewFn) {                    @SuppressWarnings("unchecked")                    ViewT viewT = (ViewT) getMapForWindow(tag, window);                    return viewT;                } else if (viewFn instanceof MultimapViewFn) {                    @SuppressWarnings("unchecked")                    ViewT viewT = (ViewT) getMultimapForWindow(tag, window);                    return viewT;                } else if (viewFn instanceof DataflowPortabilityPCollectionView.PortabilityViewFn) {                    @SuppressWarnings("unchecked")                    ViewT viewT = (ViewT) getPortabilityMultimapForWindow(tag, window);                    return viewT;                }                throw new IllegalArgumentException("Unknown type of view requested: " + view);            });        }    } catch (ExecutionException e) {        throw new IllegalStateException(String.format("Failed to materialize view %s for window %s.", view, window), e.getCause());    }}
private T beam_f8210_0(TupleTag<?> viewTag, SingletonViewFn<T> viewFn, W window) throws IOException
{    @SuppressWarnings({ "rawtypes", "unchecked" })    List<IsmReader<WindowedValue<T>>> readers = (List) tagToIsmReaderMap.get(viewTag);    List<IsmReader<WindowedValue<T>>.IsmPrefixReaderIterator> readerIterators = findAndStartReaders(readers, ImmutableList.of(window));        if (readerIterators.isEmpty()) {        return viewFn.getDefaultValue();    }    checkState(readerIterators.size() == 1, "Expected only one Ism file to contain the singleton record for window %s", window);    return readerIterators.get(0).getCurrent().getValue().getValue().getValue();}
private T beam_f8219_0(List<IsmReader<WindowedValue<V>>> readers, List<?> keyComponents, Coder<T> metadataCoder) throws IOException
{        List<IsmReader<WindowedValue<V>>.IsmPrefixReaderIterator> readerIterators = findAndStartReaders(readers, keyComponents);    if (readerIterators.isEmpty()) {        return null;    }        IsmReader<WindowedValue<V>>.IsmPrefixReaderIterator readerIterator = Iterables.getOnlyElement(readerIterators);        return CoderUtils.decodeFromByteArray(metadataCoder, readerIterator.getCurrent().getValue().getMetadata());}
public V beam_f8220_0(int index)
{    return getUsingLong(index);}
public V beam_f8229_0()
{    if (!hasPrevious()) {        throw new NoSuchElementException();    }    position -= 1;    return getUsingLong(position);}
public int beam_f8230_0()
{    return Ints.checkedCast(position);}
public boolean beam_f8239_0(Object o)
{    if (!(o instanceof Entry)) {        return false;    }    @SuppressWarnings("unchecked")    Entry<K, ?> entry = (Entry<K, ?>) o;    try {                                List<IsmReader<WindowedValue<V1>>.IsmPrefixReaderIterator> readerIterators = findAndStartReaders(readers, ImmutableList.of(entry.getKey(), window));        if (readerIterators.isEmpty()) {            return false;        }                IsmReader<WindowedValue<V1>>.IsmPrefixReaderIterator readerIterator = Iterables.getOnlyElement(readerIterators);        return Objects.equal(entry.getValue(), transform.apply(KV.of(entry.getKey(), readerIterator)));    } catch (IOException e) {        throw new IllegalStateException(e);    }}
public Iterator<Entry<K, V2>> beam_f8240_0()
{    return new EntrySetIterator();}
public String beam_f8249_0()
{    return MoreObjects.toStringHelper(StructuralMapEntry.class).add("key", getKey()).add("value", getValue()).add("keyCoder", keyCoder).toString();}
public SinkWriter<WindowedValue<IsmRecord<V>>> beam_f8250_0() throws IOException
{    return new IsmSinkWriter(FileSystems.create(resourceId, MimeTypes.BINARY));}
public Sink<?> beam_f8259_0(CloudObject spec, Coder<?> coder, PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{        @SuppressWarnings("unchecked")    Coder<WindowedValue<IsmRecord<Object>>> typedCoder = (Coder<WindowedValue<IsmRecord<Object>>>) coder;    String filename = getString(spec, WorkerPropertyNames.FILENAME);    checkArgument(typedCoder instanceof WindowedValueCoder, "%s only supports using %s but got %s.", IsmSink.class, WindowedValueCoder.class, typedCoder);    WindowedValueCoder<IsmRecord<Object>> windowedCoder = (WindowedValueCoder<IsmRecord<Object>>) typedCoder;    checkArgument(windowedCoder.getValueCoder() instanceof IsmRecordCoder, "%s only supports using %s but got %s.", IsmSink.class, IsmRecordCoder.class, windowedCoder.getValueCoder());    @SuppressWarnings("unchecked")    IsmRecordCoder<Object> ismCoder = (IsmRecordCoder<Object>) windowedCoder.getValueCoder();    long bloomFilterSizeLimitBytes = Math.max(MIN_BLOOM_FILTER_SIZE_BYTES, DoubleMath.roundToLong(BLOOM_FILTER_SIZE_LIMIT_MULTIPLIER * options.as(DataflowWorkerHarnessOptions.class).getWorkerCacheMb() *     1024 * 1024, RoundingMode.DOWN));    return new IsmSink<>(FileSystems.matchNewResource(filename, false), ismCoder, bloomFilterSizeLimitBytes);}
public static boolean beam_f8260_0(Throwable t)
{    while (t != null) {        if (t instanceof KeyTokenInvalidException) {            return true;        }        t = t.getCause();    }    return false;}
public synchronized void beam_f8269_0()
{    try {        if (generator != null) {            generator.flush();        }    } catch (IOException | RuntimeException e) {        reportFailure("Unable to flush", e, ErrorManager.FLUSH_FAILURE);    }}
public synchronized void beam_f8270_0()
{            flush();        try {        if (generator != null) {            generator.close();        }    } catch (IOException | RuntimeException e) {        reportFailure("Unable to close", e, ErrorManager.CLOSE_FAILURE);    } finally {        generator = null;        counter = null;    }}
public static synchronized void beam_f8279_0(DataflowWorkerLoggingOptions options)
{    if (!initialized) {        throw new RuntimeException("configure() called before initialize()");    }    if (options.getDefaultWorkerLogLevel() != null) {        Level defaultLevel = getJulLevel(options.getDefaultWorkerLogLevel());        LogManager.getLogManager().getLogger(ROOT_LOGGER_NAME).setLevel(defaultLevel);    }    if (options.getWorkerLogLevelOverrides() != null) {        for (Map.Entry<String, DataflowWorkerLoggingOptions.Level> loggerOverride : options.getWorkerLogLevelOverrides().entrySet()) {            Logger logger = Logger.getLogger(loggerOverride.getKey());            logger.setLevel(getJulLevel(loggerOverride.getValue()));            configuredLoggers.add(logger);        }    }        if (options.getWorkerSystemOutMessageLevel() != null) {        System.out.close();        System.setOut(JulHandlerPrintStreamAdapterFactory.create(loggingHandler, SYSTEM_OUT_LOG_NAME, getJulLevel(options.getWorkerSystemOutMessageLevel())));    }    if (options.getWorkerSystemErrMessageLevel() != null) {        System.err.close();        System.setErr(JulHandlerPrintStreamAdapterFactory.create(loggingHandler, SYSTEM_ERR_LOG_NAME, getJulLevel(options.getWorkerSystemErrMessageLevel())));    }}
public static DataflowWorkerLoggingHandler beam_f8280_0()
{    if (!initialized) {        throw new RuntimeException("getLoggingHandler() called before initialize()");    }    return loggingHandler;}
public static String beam_f8289_0()
{    return jobId.get();}
public static String beam_f8290_0()
{    return stageName.get();}
private void beam_f8299_0(Level level, String message)
{    if (logger.isLoggable(level)) {        LogRecord log = new LogRecord(level, message);        log.setLoggerName(loggerName);        handler.publish(log);    }}
 static void beam_f8300_0()
{    outputWarning.set(false);}
public static CounterUpdate beam_f8310_0(MetricKey key, boolean isCumulative, DistributionData update)
{    CounterStructuredNameAndMetadata name = structuredNameAndMetadata(key, Kind.DISTRIBUTION);    DistributionUpdate distributionUpdateProto = new DistributionUpdate();    distributionUpdateProto.setMin(longToSplitInt(update.min())).setMax(longToSplitInt(update.max())).setCount(longToSplitInt(update.count())).setSum(longToSplitInt(update.sum()));    return new CounterUpdate().setStructuredNameAndMetadata(name).setCumulative(isCumulative).setDistribution(distributionUpdateProto);}
private static CounterStructuredNameAndMetadata beam_f8311_0(MetricKey metricKey, Kind kind)
{    MetricName metricName = metricKey.metricName();    CounterStructuredNameAndMetadata name = new CounterStructuredNameAndMetadata();    name.setMetadata(new CounterMetadata().setKind(kind.toString()));    name.setName(new CounterStructuredName().setName(metricName.getName()).setOriginalStepName(metricKey.stepName()).setOrigin(Origin.USER.toString()).setOriginNamespace(metricName.getNamespace()));    return name;}
public void beam_f8320_0(PrintWriter writer)
{    writer.println("Active Fetches:");    writer.println("  Side Inputs: " + activeSideInputs.get());    writer.println("  State Reads: " + activeStateReads.get());    writer.println("Heartbeat Keys Active: " + activeHeartbeats.get());}
public Closeable beam_f8322_0()
{    return () -> {    };}
public String beam_f8331_0(PipelineOptions options)
{    return System.getProperty("windmill.periodic_status_page_directory");}
public WindmillServerStub beam_f8332_0(PipelineOptions options)
{    StreamingDataflowWorkerOptions streamingOptions = options.as(StreamingDataflowWorkerOptions.class);    if (streamingOptions.getWindmillServiceEndpoint() != null || streamingOptions.isEnableStreamingEngine() || streamingOptions.getLocalWindmillHostport().startsWith("grpc:")) {        try {            return new GrpcWindmillServer(streamingOptions);        } catch (IOException e) {            throw new RuntimeException("Failed to create GrpcWindmillServer: ", e);        }    } else {        return new WindmillServer(streamingOptions.getLocalWindmillHostport());    }}
public byte[] beam_f8341_0()
{    if ((encodedArrays == null) || (encodedArrays.isEmpty()) || ((encodedArrays.get(0)).length - firstArrayPosition <= 0)) {        throw new IllegalArgumentException("Invalid encoded byte array");    }                byte[] store = encodedArrays.get(0);    int decodedLength = 0;    boolean valid = false;    int i = firstArrayPosition;    while (i < store.length - 1) {        byte b = store[i++];        if (b == ESCAPE1) {            b = store[i++];            if (b == SEPARATOR) {                valid = true;                break;            } else if (b == NULL_CHARACTER) {                decodedLength++;            } else {                throw new IllegalArgumentException("Invalid encoded byte array");            }        } else if (b == ESCAPE2) {            b = store[i++];            if (b == FF_CHARACTER) {                decodedLength++;            } else {                throw new IllegalArgumentException("Invalid encoded byte array");            }        } else {            decodedLength++;        }    }    if (!valid) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    byte[] decodedArray = new byte[decodedLength];    int copyStart = firstArrayPosition;    int outIndex = 0;    int j = firstArrayPosition;    while (j < store.length - 1) {                byte b = store[j++];        if (b == ESCAPE1) {            System.arraycopy(store, copyStart, decodedArray, outIndex, j - copyStart - 1);            outIndex += j - copyStart - 1;                                    b = store[j++];            if (b == SEPARATOR) {                if ((store.length - j) == 0) {                                        encodedArrays.remove(0);                    firstArrayPosition = 0;                } else {                    firstArrayPosition = j;                }                return decodedArray;            } else if (b == NULL_CHARACTER) {                decodedArray[outIndex++] = 0x00;            }                        copyStart = j;        } else if (b == ESCAPE2) {            System.arraycopy(store, copyStart, decodedArray, outIndex, j - copyStart - 1);            outIndex += j - copyStart - 1;                                    b = store[j++];            if (b == FF_CHARACTER) {                decodedArray[outIndex++] = (byte) 0xff;            }                        copyStart = j;        }    }        throw new IllegalArgumentException("Invalid encoded byte array");}
public long beam_f8342_0()
{    if ((encodedArrays == null) || (encodedArrays.isEmpty()) || ((encodedArrays.get(0)).length - firstArrayPosition < 1)) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    byte[] store = encodedArrays.get(0);        int len = store[firstArrayPosition];    if ((firstArrayPosition + len + 1 > store.length) || len > 8) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    long result = 0;    for (int i = 0; i < len; i++) {        result <<= 8;        result |= (store[firstArrayPosition + i + 1] & 0xff);    }    if ((store.length - firstArrayPosition - len - 1) == 0) {                encodedArrays.remove(0);        firstArrayPosition = 0;    } else {        firstArrayPosition = firstArrayPosition + len + 1;    }    return result;}
public void beam_f8351_0(Object untypedElem) throws Exception
{    @SuppressWarnings("unchecked")    WindowedValue<Object> elem = (WindowedValue) untypedElem;    receiver.process(elem.withValue(KV.of(key, elem.getValue())));}
public static ParDoFn beam_f8355_0(PipelineOptions options, KvCoder<K, ?> inputElementCoder, @Nullable CloudObject cloudUserFn, @Nullable List<SideInputInfo> sideInputInfos, List<Receiver> receivers, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    AppliedCombineFn<K, InputT, AccumT, ?> combineFn;    SideInputReader sideInputReader;    StepContext stepContext;    if (cloudUserFn == null) {        combineFn = null;        sideInputReader = NullSideInputReader.empty();        stepContext = null;    } else {        Object deserializedFn = SerializableUtils.deserializeFromByteArray(getBytes(cloudUserFn, PropertyNames.SERIALIZED_FN), "serialized combine fn");        @SuppressWarnings("unchecked")        AppliedCombineFn<K, InputT, AccumT, ?> combineFnUnchecked = ((AppliedCombineFn<K, InputT, AccumT, ?>) deserializedFn);        combineFn = combineFnUnchecked;        sideInputReader = executionContext.getSideInputReader(sideInputInfos, combineFn.getSideInputViews(), operationContext);        stepContext = executionContext.getStepContext(operationContext);    }    return create(options, inputElementCoder, combineFn, sideInputReader, receivers.get(0), stepContext);}
public Object beam_f8364_0(Object pair)
{    @SuppressWarnings("unchecked")    WindowedValue<KV<?, ?>> windowedKv = (WindowedValue<KV<?, ?>>) pair;    return windowedKv.getValue().getValue();}
public Object beam_f8365_0(Object key, Object values)
{    WindowedValue<?> windowedKey = (WindowedValue<?>) key;    return windowedKey.withValue(KV.of(windowedKey.getValue(), values));}
private void beam_f8379_0(Coder<WindowedValue<KV<K, V>>> coder) throws Exception
{    if (!(coder instanceof WindowedValueCoder)) {        throw new Exception("unexpected kind of coder for WindowedValue: " + coder);    }    WindowedValueCoder<KV<K, V>> windowedElemCoder = ((WindowedValueCoder<KV<K, V>>) coder);    Coder<KV<K, V>> elemCoder = windowedElemCoder.getValueCoder();    if (!(elemCoder instanceof KvCoder)) {        throw new Exception("unexpected kind of coder for elements read from " + "a key-partitioning shuffle: " + elemCoder);    }    @SuppressWarnings("unchecked")    KvCoder<K, V> kvCoder = (KvCoder<K, V>) elemCoder;    this.keyCoder = kvCoder.getKeyCoder();    windowedValueCoder = windowedElemCoder.withValueCoder(kvCoder.getValueCoder());}
public NativeReaderIterator<WindowedValue<KV<K, V>>> beam_f8380_0() throws IOException
{    return iterator(new ApplianceShuffleEntryReader(shuffleReaderConfig, executionContext, operationContext, false));}
public static PCollectionViewWindow<T> beam_f8389_0(PCollectionView<T> view, BoundedWindow window)
{    return new PCollectionViewWindow<>(view, window);}
public PCollectionView<T> beam_f8390_0()
{    return view;}
public ProfileScope beam_f8399_0(ProfilerWrapper profiler)
{    return new TaggedProfileScope(profiler, 0);}
public ProfileScope beam_f8400_0(ProfilerWrapper profiler, String attributeTag)
{    return NoopProfileScope.NOOP;}
public NativeReader<?> beam_f8410_0(CloudObject cloudSourceSpec, Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{    checkArgument(coder != null, "coder must not be null");    @SuppressWarnings("unchecked")    Coder<WindowedValue<Object>> typedCoder = (Coder<WindowedValue<Object>>) coder;    SimpleFunction<PubsubMessage, Object> parseFn = null;    byte[] attributesFnBytes = getBytes(cloudSourceSpec, PropertyNames.PUBSUB_SERIALIZED_ATTRIBUTES_FN, null);        if (attributesFnBytes != null && attributesFnBytes.length > 0) {        parseFn = (SimpleFunction<PubsubMessage, Object>) SerializableUtils.deserializeFromByteArray(attributesFnBytes, "serialized fn info");    }    return new PubsubReader<>(typedCoder, (StreamingModeExecutionContext) executionContext, parseFn);}
public NativeReaderIterator<WindowedValue<T>> beam_f8411_0() throws IOException
{    return new PubsubReaderIterator(context.getWork());}
public boolean beam_f8420_0()
{    return true;}
private static String beam_f8421_0(KV<String, ByteString> key)
{    return key.getKey() + "-" + key.getValue().toStringUtf8();}
public static List<T> beam_f8430_0(NativeReader<T> reader) throws IOException
{    try (NativeReader.NativeReaderIterator<T> iterator = reader.iterator()) {        return readRemainingFromIterator(iterator, false);    }}
public static List<T> beam_f8431_0(NativeReader.NativeReaderIterator<T> reader, int n, boolean started) throws IOException
{    List<T> res = new ArrayList<>();    for (long i = 0; n == Integer.MAX_VALUE || i < n; i++) {        if (!((i == 0 && !started) ? reader.start() : reader.advance())) {            break;        }        res.add(reader.getCurrent());    }    return res;}
public IsmRecordCoder<?> beam_f8443_0(CloudObject cloudObject)
{    List<Coder<?>> coders = getComponents(cloudObject);    return IsmRecordCoder.of(Structs.getLong(cloudObject, "num_shard_key_coders").intValue(), 0, coders.subList(0, coders.size() - 1), coders.get(coders.size() - 1));}
public Class beam_f8444_0()
{    return IsmRecordCoder.class;}
public static SdkHarnessRegistry beam_f8453_0(ApiServiceDescriptor stateApiServiceDescriptor, GrpcStateService beamFnStateService, BeamFnDataGrpcService beamFnDataGrpcService)
{    return new WorkBalancingSdkHarnessRegistry(stateApiServiceDescriptor, beamFnStateService, beamFnDataGrpcService);}
private boolean beam_f8454_0(WorkCountingSdkWorkerHarness worker)
{        if (worker.closed.get()) {        workers.remove(worker);        return false;    }    return true;}
public String beam_f8463_0()
{    return controlClientHandler.getWorkerId();}
public GrpcFnServer<GrpcDataService> beam_f8464_0()
{    return GrpcFnServer.create(beamFnDataGrpcService.getDataService(getWorkerId()), beamFnDataApiServiceDescriptor());}
public SdkWorkerHarness beam_f8473_0()
{    return sdkWorkerHarness;}
public ApiServiceDescriptor beam_f8475_0()
{    return null;}
private void beam_f8485_0() throws IOException
{    writer.write(Arrays.copyOf(chunk.array(), chunk.size()));    chunk.resetTo(0);}
private int beam_f8486_0(Coder<EncodeT> coder, EncodeT value) throws IOException
{        int initialChunkSize = chunk.size();    chunk.resetTo(initialChunkSize + Ints.BYTES);    coder.encode(value, chunk.asOutputStream(), Context.OUTER);    int elementSize = chunk.size() - initialChunkSize - Ints.BYTES;    byte[] internalBytes = chunk.array();    internalBytes[initialChunkSize] = (byte) ((elementSize >>> 24) & 0xFF);    internalBytes[initialChunkSize + 1] = (byte) ((elementSize >>> 16) & 0xFF);    internalBytes[initialChunkSize + 2] = (byte) ((elementSize >>> 8) & 0xFF);    internalBytes[initialChunkSize + 3] = (byte) ((elementSize >>> 0) & 0xFF);    return elementSize;}
public IsmPrefixReaderIterator beam_f8495_0(List<?> keyComponents) throws IOException
{    try (Closeable counterCloser = IsmReader.setSideInputReadContext(readCounter)) {        return delegate.overKeyComponents(keyComponents);    }}
public IsmPrefixReaderIterator beam_f8496_0(List<?> keyComponents, int shardId, RandomAccessData keyBytes) throws IOException
{    try (Closeable counterCloser = IsmReader.setSideInputReadContext(readCounter)) {        return delegate.overKeyComponents(keyComponents, shardId, keyBytes);    }}
private void beam_f8508_0() throws Exception
{    checkState(fnRunner == null, "bundle already started (or not properly finished)");    OutputManager outputManager = new OutputManager() {        final Map<TupleTag<?>, OutputReceiver> undeclaredOutputs = new HashMap<>();        @Nullable        private Receiver getReceiverOrNull(TupleTag<?> tag) {            Integer receiverIndex = outputTupleTagsToReceiverIndices.get(tag);            if (receiverIndex != null) {                return receivers[receiverIndex];            } else {                return undeclaredOutputs.get(tag);            }        }        @Override        public <T> void output(TupleTag<T> tag, WindowedValue<T> output) {            outputsPerElementTracker.onOutput();            Receiver receiver = getReceiverOrNull(tag);            if (receiver == null) {                                                                String outputName = "implicit-" + tag.getId();                                                                                OutputReceiver undeclaredReceiver = new OutputReceiver();                ElementCounter outputCounter = new DataflowOutputCounter(outputName, counterFactory, stepContext.getNameContext());                undeclaredReceiver.addOutputCounter(outputCounter);                undeclaredOutputs.put(tag, undeclaredReceiver);                receiver = undeclaredReceiver;            }            try {                receiver.process(output);            } catch (RuntimeException | Error e) {                                throw e;            } catch (Exception e) {                                throw new RuntimeException(e);            }        }    };    fnInfo = (DoFnInfo) doFnInstanceManager.get();    fnSignature = DoFnSignatures.getSignature(fnInfo.getDoFn().getClass());    fnRunner = runnerFactory.createRunner(fnInfo.getDoFn(), options, mainOutputTag, sideOutputTags, fnInfo.getSideInputViews(), sideInputReader, fnInfo.getInputCoder(), fnInfo.getOutputCoders(), fnInfo.getWindowingStrategy(), stepContext, userStepContext, outputManager, doFnSchemaInformation, sideInputMapping);    fnRunner.startBundle();}
private Receiver beam_f8509_0(TupleTag<?> tag)
{    Integer receiverIndex = outputTupleTagsToReceiverIndices.get(tag);    if (receiverIndex != null) {        return receivers[receiverIndex];    } else {        return undeclaredOutputs.get(tag);    }}
public void beam_f8518_0(SimpleParDoFn doFn, TimerData timer) throws Exception
{    doFn.processSystemTimer(timer);}
private void beam_f8519_0(TimerType mode, DataflowExecutionContext.DataflowStepContext context, Coder<BoundedWindow> windowCoder) throws Exception
{    TimerData timer = context.getNextFiredTimer(windowCoder);    if (timer != null && fnRunner == null) {                try (Closeable start = operationContext.enterStart()) {            reallyStartBundle();        }    }    while (timer != null) {        mode.processTimer(this, timer);        timer = context.getNextFiredTimer(windowCoder);    }}
public boolean beam_f8528_0()
{    return underlyingSink.supportsRestart();}
public long beam_f8529_0(T value) throws IOException
{    long size = underlyingWriter.add(value);    executionContext.reportBytesSinked(size);    return size;}
public static NativeReader.DynamicSplitRequest beam_f8538_0(@Nullable ApproximateSplitRequest splitRequest)
{    return (splitRequest == null) ? null : new DataflowDynamicSplitRequest(splitRequest);}
public String beam_f8539_0()
{    return String.valueOf(cloudProgress);}
public boolean beam_f8548_0(Object obj)
{    if (this == obj) {        return true;    } else if (!(obj instanceof DataflowDynamicSplitRequest)) {        return false;    } else {        return Objects.equals(splitRequest, ((DataflowDynamicSplitRequest) obj).splitRequest);    }}
 static final ParDoFnFactory beam_f8549_0()
{    return new UserParDoFnFactory(new ProcessFnExtractor(), new SplittableDoFnRunnerFactory());}
public int beam_f8558_0()
{    return Objects.hash(tag, window);}
public static SideInputCacheEntry beam_f8559_0(Object value, int encodedSize)
{    return new SideInputCacheEntry(true, value, encodedSize);}
 void beam_f8568_1()
{    try {        GetDebugConfigRequest request = new GetDebugConfigRequest().setComponentId(COMPONENT).setWorkerId(host);        GetDebugConfigResponse response = this.client.projects().locations().jobs().debug().getConfig(project, region, job, request).execute();        Config config = mapper.readValue(response.getConfig(), Config.class);        synchronized (lock) {            captureConfig = config;        }    } catch (Exception e) {            }}
private boolean beam_f8569_0()
{    synchronized (lock) {        long nowUsec = DateTime.now().getMillis() * 1000;        if (captureConfig.expireTimestampUsec < nowUsec) {                        return false;        }        if (captureConfig.captureFrequencyUsec == 0) {                        return false;        }        return captureConfig.captureFrequencyUsec < (nowUsec - lastCaptureUsec);    }}
public String beam_f8578_0()
{    return "/statusz";}
public void beam_f8579_0(PrintWriter writer)
{    writer.println("<html>");    writer.println("<h1>Worker Harness</h1>");    for (DataProviderInfo info : dataProviders.values()) {        writer.print("<h2>");        writer.print(info.longName);        writer.println("</h2>");        info.dataProvider.appendSummaryHtml(writer);    }    writer.println("</html>");}
public void beam_f8588_1()
{    try {        statusServer.stop();    } catch (Exception e) {            }}
public void beam_f8589_0(BaseStatusServlet servlet)
{    ServletHolder holder = new ServletHolder();    holder.setServlet(servlet);    servletHandler.addServletWithMapping(holder, servlet.getPath());}
public V beam_f8598_0()
{    V result = queue.poll();    if (result != null) {        limit.release(weigher.apply(result));    }    return result;}
public V beam_f8599_0(long timeout, TimeUnit unit) throws InterruptedException
{    V result = queue.poll(timeout, unit);    if (result != null) {        limit.release(weigher.apply(result));    }    return result;}
private void beam_f8608_0(CounterUpdate stepCounterUpdate)
{    CounterStructuredName structuredName = stepCounterUpdate.getStructuredNameAndMetadata().getName();    if (THROTTLING_MSECS_METRIC_NAME.getNamespace().equals(structuredName.getOriginNamespace()) && THROTTLING_MSECS_METRIC_NAME.getName().equals(structuredName.getName())) {        long msecs = DataflowCounterUpdateExtractor.splitIntToLong(stepCounterUpdate.getInteger());        if (msecs > 0) {            throttledMsecs.addValue(msecs);        }    }}
public static StreamingDataflowWorker beam_f8609_0(List<MapTask> mapTasks, WorkUnitClient workUnitClient, DataflowWorkerHarnessOptions options, @Nullable RunnerApi.Pipeline pipeline, SdkHarnessRegistry sdkHarnessRegistry) throws IOException
{    return new StreamingDataflowWorker(mapTasks, BeamFnMapTaskExecutorFactory.defaultFactory(), workUnitClient, options.as(StreamingDataflowWorkerOptions.class), pipeline, sdkHarnessRegistry, true, new HotKeyLogger());}
public boolean beam_f8618_0()
{    return workUnitExecutor.getQueue().isEmpty();}
public void beam_f8619_1()
{    running.set(true);    if (windmillServiceEnabled) {                schedulePeriodicGlobalConfigRequests();    }    memoryMonitorThread.start();    dispatchThread.start();    commitThread.start();    ExecutionStateSampler.instance().start();        globalWorkerUpdatesTimer = new Timer("GlobalWorkerUpdatesTimer");    globalWorkerUpdatesTimer.schedule(new TimerTask() {        @Override        public void run() {            reportPeriodicWorkerUpdates();        }    }, 0, options.getWindmillHarnessUpdateReportingPeriod().getMillis());    if (options.getActiveWorkRefreshPeriodMillis() > 0) {        refreshActiveWorkTimer = new Timer("RefreshActiveWork");        refreshActiveWorkTimer.schedule(new TimerTask() {            @Override            public void run() {                refreshActiveWork();            }        }, options.getActiveWorkRefreshPeriodMillis(), options.getActiveWorkRefreshPeriodMillis());    }    if (options.getPeriodicStatusPageOutputDirectory() != null) {        statusPageTimer = new Timer("DumpStatusPages");        statusPageTimer.schedule(new TimerTask() {            @Override            public void run() {                List<Capturable> pages = statusPages.getDebugCapturePages();                if (pages.isEmpty()) {                                    }                Long timestamp = Instant.now().getMillis();                for (Capturable page : pages) {                    PrintWriter writer = null;                    try {                        File outputFile = new File(options.getPeriodicStatusPageOutputDirectory(), ("StreamingDataflowWorker" + options.getWorkerId() + "_" + page.pageName() + timestamp.toString()).replaceAll("/", "_"));                        writer = new PrintWriter(outputFile, UTF_8.name());                        page.captureData(writer);                    } catch (IOException e) {                                            } finally {                        if (writer != null) {                            writer.close();                        }                    }                }            }        }, 60 * 1000, 60 * 1000);    }    reportHarnessStartup();}
private static void beam_f8628_0(int millis)
{    Uninterruptibles.sleepUninterruptibly(millis, TimeUnit.MILLISECONDS);}
private ComputationState beam_f8629_0(String computationId)
{    ComputationState state = computationMap.get(computationId);    if (state == null) {        getConfig(computationId);        state = computationMap.get(computationId);    }    return state;}
private Coder<?> beam_f8638_0(Coder<?> readCoder)
{    if (!(readCoder instanceof WindowedValueCoder)) {        throw new RuntimeException(String.format("Expected coder for streaming read to be %s, but received %s", WindowedValueCoder.class.getSimpleName(), readCoder));    }            Coder<?> valueCoder = ((WindowedValueCoder<?>) readCoder).getValueCoder();    if (!(valueCoder instanceof WindmillKeyedWorkItem.FakeKeyedWorkItemCoder<?, ?>)) {        return null;    }    return ((WindmillKeyedWorkItem.FakeKeyedWorkItemCoder<?, ?>) valueCoder).getKeyCoder();}
private void beam_f8639_1(Windmill.WorkItem work)
{    for (Long callbackId : work.getSourceState().getFinalizeIdsList()) {        final Runnable callback = commitCallbacks.remove(callbackId);                if (callback != null) {            workUnitExecutor.forceExecute(() -> {                try {                    callback.run();                } catch (Throwable t) {                                    }            });        }    }}
public void beam_f8648_0()
{    getGlobalConfig();}
private void beam_f8649_0()
{        getConfig(null);}
public void beam_f8658_1()
{    updateVMMetrics();    try {        sendWorkerUpdatesToDataflowService(pendingDeltaCounters, pendingCumulativeCounters);    } catch (IOException e) {            } catch (Exception e) {            }}
private Object beam_f8659_0(CounterUpdate counterUpdate)
{    Object key = null;    if (counterUpdate.getNameAndKind() != null) {        key = counterUpdate.getNameAndKind().getName();    } else if (counterUpdate.getStructuredNameAndMetadata() != null) {        key = counterUpdate.getStructuredNameAndMetadata().getName();    }    checkArgument(key != null, "Could not find name for CounterUpdate: %s", counterUpdate);    return key;}
public List<Windmill.KeyedGetDataRequest> beam_f8668_0(Instant refreshDeadline)
{    List<Windmill.KeyedGetDataRequest> result = new ArrayList<>();    synchronized (activeWork) {        for (Map.Entry<ByteString, Queue<Work>> entry : activeWork.entrySet()) {            ByteString key = entry.getKey();            for (Work work : entry.getValue()) {                if (work.getStartTime().isBefore(refreshDeadline)) {                    result.add(Windmill.KeyedGetDataRequest.newBuilder().setKey(key).setShardingKey(work.getWorkItem().getShardingKey()).setWorkToken(work.getWorkItem().getWorkToken()).build());                }            }        }    }    return result;}
public void beam_f8669_0(PrintWriter writer)
{    final Instant now = Instant.now();        final int maxCommitPending = 50;    int commitPendingCount = 0;    writer.println("<table border=\"1\" " + "style=\"border-collapse:collapse;padding:5px;border-spacing:5px;border:1px\">");    writer.println("<tr><th>Key</th><th>Token</th><th>Queued</th><th>Active For</th><th>State</th></tr>");            StringBuilder builder = new StringBuilder();    synchronized (activeWork) {        for (Map.Entry<ByteString, Queue<Work>> entry : activeWork.entrySet()) {            Queue<Work> queue = entry.getValue();            Work work = queue.peek();            if (work == null) {                continue;            }            Windmill.WorkItem workItem = work.getWorkItem();            State state = work.getState();            if (state == State.COMMITTING || state == State.COMMIT_QUEUED) {                if (++commitPendingCount >= maxCommitPending) {                    continue;                }            }            builder.append("<tr>");            builder.append("<td>");            builder.append(String.format("%016x", workItem.getShardingKey()));            builder.append("</td><td>");            builder.append(workItem.getWorkToken());            builder.append("</td><td>");            builder.append(queue.size() - 1);            builder.append("</td><td>");            Duration activeFor = new Duration(work.getStartTime(), now);                        builder.append(activeFor.toString().substring(2));            builder.append("</td><td>");            builder.append(state);            builder.append("</td></tr>\n");        }    }    writer.print(builder.toString());    writer.println("</table>");    if (commitPendingCount >= maxCommitPending) {        writer.println("<br>");        writer.print("Skipped keys in COMMITTING/COMMIT_QUEUED: ");        writer.println(maxCommitPending - commitPendingCount);        writer.println("<br>");    }}
public static boolean beam_f8678_0(WindowingStrategy<?, ?> strategy)
{    return strategy.getTrigger() instanceof ReshuffleTrigger;}
public void beam_f8679_0(KeyedWorkItem<K, T> element, PipelineOptions options, StepContext stepContext, SideInputReader sideInputReader, OutputWindowedValue<KV<K, Iterable<T>>> output) throws Exception
{    @SuppressWarnings("unchecked")    K key = element.key();    for (WindowedValue<T> item : element.elementsIterable()) {        output.outputWindowedValue(KV.of(key, Collections.singletonList(item.getValue())), item.getTimestamp(), item.getWindows(), item.getPane());    }}
public void beam_f8688_0()
{    simpleDoFnRunner.finishBundle();    sideInputFetcher.persist();}
 ValueState<K> beam_f8689_0()
{    return stepContext.stateInternals().state(StateNamespaces.global(), keyAddr);}
public StepContext beam_f8698_0(DataflowOperationContext operationContext)
{    return new StepContext(operationContext);}
protected SideInputReader beam_f8699_0(Iterable<? extends SideInputInfo> sideInputInfos, DataflowOperationContext operationContext)
{    throw new UnsupportedOperationException("Cannot call getSideInputReader for StreamingDataflowWorker: " + "the MapTask specification should not have had any SideInputInfo descriptors " + "since the streaming runner does not yet support them.");}
public UnboundedSource.UnboundedReader<?> beam_f8708_0()
{    return readerCache.acquireReader(computationId, getSerializedKey(), getWork().getCacheToken());}
public void beam_f8709_0(UnboundedSource.UnboundedReader<?> reader)
{    checkState(activeReader == null, "not expected to be overwritten");    activeReader = reader;}
public TimerData beam_f8718_0(Coder<W> windowCoder)
{    if (cachedFiredUserTimers == null) {        cachedFiredUserTimers = FluentIterable.<Timer>from(StreamingModeExecutionContext.this.getFiredTimers()).filter(timer -> WindmillTimerInternals.isUserTimer(timer) && timer.getStateFamily().equals(stateFamily)).transform(timer -> WindmillTimerInternals.windmillTimerToTimerData(WindmillNamespacePrefix.USER_NAMESPACE_PREFIX, timer, windowCoder)).iterator();    }    if (!cachedFiredUserTimers.hasNext()) {        return null;    }    TimerData nextTimer = cachedFiredUserTimers.next();        userTimerInternals.deleteTimer(nextTimer);    return nextTimer;}
public void beam_f8719_0(String timerId, W window, Coder<W> windowCoder, Instant cleanupTime)
{    timerInternals().setTimer(StateNamespaces.window(windowCoder, window), timerId, cleanupTime, TimeDomain.EVENT_TIME);}
public TimerInternals beam_f8728_0()
{    checkState(stateFamily != null, "Tried to access user timers for stateless step: " + getNameContext());    return checkNotNull(userTimerInternals);}
public boolean beam_f8729_0(PCollectionView<?> view, BoundedWindow w, StateFetcher.SideInputState s)
{    return wrapped.issueSideInputFetch(view, w, s);}
public DataflowStepContext beam_f8738_0()
{    return this;}
public static StreamingModeSideInputReader beam_f8739_0(Iterable<? extends PCollectionView<?>> views, StreamingModeExecutionContext context)
{    return new StreamingModeSideInputReader(views, context);}
public void beam_f8751_0(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain)
{    throw new UnsupportedOperationException("Attempt to deliver a timer to a DoFn, but timers are not supported in Dataflow.");}
public void beam_f8752_0()
{    simpleDoFnRunner.finishBundle();    sideInputFetcher.persist();}
public void beam_f8761_0()
{    if (blockedMap == null) {        return;    }    ValueState<Map<W, Set<Windmill.GlobalDataRequest>>> mapState = stepContext.stateInternals().state(StateNamespaces.global(), blockedMapAddr);    if (blockedMap.isEmpty()) {                        mapState.clear();    } else {        mapState.write(blockedMap);    }    blockedMap = null;}
private Map<W, Set<Windmill.GlobalDataRequest>> beam_f8762_0()
{    if (blockedMap == null) {        blockedMap = stepContext.stateInternals().state(StateNamespaces.global(), blockedMapAddr).read();        if (blockedMap == null) {            blockedMap = new HashMap<>();        }    }    return blockedMap;}
public void beam_f8771_0(Windmill.GlobalDataRequest value, OutputStream outStream) throws IOException
{    encode(value, outStream, Context.NESTED);}
public void beam_f8772_0(Windmill.GlobalDataRequest value, OutputStream outStream, Context context) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null " + protoMessageClass.getSimpleName());    }    if (context.isWholeStream) {        value.writeTo(outStream);    } else {        value.writeDelimitedTo(outStream);    }}
public CounterUpdate beam_f8781_0(@Nonnull Map.Entry<MetricName, DeltaCounterCell> entry)
{    long value = entry.getValue().getSumAndReset();    if (value == 0) {        return null;    }    return MetricsToCounterUpdateConverter.fromCounter(MetricKey.create(stepName, entry.getKey()), false, value);}
private FluentIterable<CounterUpdate> beam_f8782_0()
{    return FluentIterable.from(distributions.entries()).transform(new Function<Entry<MetricName, DeltaDistributionCell>, CounterUpdate>() {        @SuppressFBWarnings(value = "NP_METHOD_PARAMETER_TIGHTENS_ANNOTATION", justification = "https://github.com/google/guava/issues/920")        @Override        @Nullable        public CounterUpdate apply(@Nonnull Map.Entry<MetricName, DeltaDistributionCell> entry) {            DistributionData value = entry.getValue().getAndReset();            if (value.count() == 0) {                return null;            }            return MetricsToCounterUpdateConverter.fromDistribution(MetricKey.create(stepName, entry.getKey()), false, value);        }    }).filter(Predicates.notNull());}
public boolean beam_f8794_0() throws IOException
{    if (!iterator.hasNext()) {        current = null;        return false;    }    ShuffleEntry record = iterator.next();        byte[] value = record.getValue();    shuffleReader.notifyElementRead(record.length());    current = CoderUtils.decodeFromByteArray(shuffleReader.coder, value);    return true;}
public T beam_f8795_0() throws NoSuchElementException
{    if (current == null) {        throw new NoSuchElementException();    }    return current;}
private X beam_f8804_0(Coder<X> coder, InputStream input) throws IOException
{    return coder.decode(input, Coder.Context.OUTER);}
public boolean beam_f8805_0()
{    return true;}
public AccumT beam_f8814_0(AccumT accumulator, InputT input, BoundedWindow w)
{    return keyedCombineFn.addInput(accumulator, input);}
public AccumT beam_f8815_0(Iterable<AccumT> accumulators, BoundedWindow w)
{    return keyedCombineFn.mergeAccumulators(accumulators);}
public static boolean beam_f8824_0(WindowingStrategy<?, ?> strategy)
{    return strategy.getTrigger() instanceof ReshuffleTrigger;}
public void beam_f8825_0(KV<K, Iterable<WindowedValue<V>>> element, PipelineOptions options, StepContext stepContext, SideInputReader sideInputReader, OutputWindowedValue<KV<K, Iterable<V>>> output) throws Exception
{    K key = element.getKey();    for (WindowedValue<V> item : element.getValue()) {        output.outputWindowedValue(KV.<K, Iterable<V>>of(key, Collections.singletonList(item.getValue())), item.getTimestamp(), item.getWindows(), item.getPane());    }}
public V beam_f8834_0()
{    skipToValidElement();    WindowedValue<V> next = iterator.next();    if (!next.getWindows().contains(window)) {        throw new NoSuchElementException("No next item in window");    }    return next.getValue();}
public void beam_f8835_0()
{    throw new UnsupportedOperationException();}
public void beam_f8844_0(Runnable r)
{    semaphore.acquireUninterruptibly();    super.execute(r);}
public void beam_f8845_0(Runnable r)
{    semaphore.reducePermits(1);    super.execute(r);}
public int beam_f8854_0()
{    if (size == -1) {        throw new UnsupportedOperationException();    } else {        return size;    }}
private PeekingReiterator<Object> beam_f8855_0(int tag)
{    if (tag >= starts.size()) {        PeekingReiterator<Object> start = getStart(tag - 1);        while (start.hasNext() && extractor.getTag(start.peek()) < tag) {            start.next();        }        starts.add(start);    }        return starts.set(tag, starts.get(tag).copy());}
public Reiterator<ShuffleEntry> beam_f8864_0(@Nullable ShufflePosition startPosition, @Nullable ShufflePosition endPosition)
{    return new ShuffleReadIterator(startPosition, endPosition);}
public boolean beam_f8865_0()
{    fillEntriesIfNeeded();        return entries.hasNext();}
public byte[] beam_f8875_0()
{    return position;}
public String beam_f8876_0()
{    return encodeBase64URLSafeString(position);}
public int beam_f8885_0()
{    return Objects.hashCode(startPosition, endPosition);}
public static ElementExecutionTracker beam_f8886_0()
{    return NO_OP_INSTANCE;}
public GroupingShuffleRangeTracker beam_f8898_0()
{    return rangeTracker;}
public ValuesIterator beam_f8899_0()
{    return new ValuesIterator(parent, baseValuesIterator.copy(), currentKeyBytes);}
public synchronized ShufflePosition beam_f8908_0()
{    return lastGroupStart;}
public synchronized boolean beam_f8909_0()
{    return getLastGroupStart() != null;}
public static GroupingTable<K, V, List<V>> beam_f8918_0(Long maxTableSizeBytes, GroupingKeyCreator<? super K> groupingKeyCreator, PairInfo pairInfo, SizeEstimator<? super K> keySizer, SizeEstimator<? super V> valueSizer)
{    return new BufferingGroupingTable<>(maxTableSizeBytes, groupingKeyCreator, pairInfo, keySizer, valueSizer);}
public static GroupingTable<K, V, List<V>> beam_f8919_0(GroupingKeyCreator<? super K> groupingKeyCreator, PairInfo pairInfo, SizeEstimator<? super K> keySizer, SizeEstimator<? super V> valueSizer, double sizeEstimatorSampleRate)
{    return new BufferingGroupingTable<>(DEFAULT_MAX_GROUPING_TABLE_BYTES, groupingKeyCreator, pairInfo, new SamplingSizeEstimator<>(keySizer, sizeEstimatorSampleRate, 1.0), new SamplingSizeEstimator<>(valueSizer, sizeEstimatorSampleRate, 1.0));}
public GroupingTableEntry<K, V, List<V>> beam_f8928_0(final K key) throws Exception
{    return new GroupingTableEntry<K, V, List<V>>() {        long size = keySizer.estimateSize(key);        final List<V> values = new ArrayList<>();        @Override        public K getKey() {            return key;        }        @Override        public List<V> getValue() {            return values;        }        @Override        public long getSize() {            return size;        }        @Override        public void compact() {        }        @Override        public void add(V value) throws Exception {            values.add(value);            size += BYTES_PER_JVM_WORD + valueSizer.estimateSize(value);        }    };}
public K beam_f8929_0()
{    return key;}
public void beam_f8939_0(InputT value) throws Exception
{    accumulator = combiner.add(key, accumulator, value);    accumulatorSize = accumulatorSizer.estimateSize(accumulator);}
private static int beam_f8940_0()
{    String wordSizeInBits = System.getProperty("sun.arch.data.model");    try {        return Integer.parseInt(wordSizeInBits) / 8;    } catch (NumberFormatException e) {                return 8;    }}
public NativeReader.Progress beam_f8949_0() throws Exception
{    return getReadOperation().getProgress();}
public NativeReader.DynamicSplitResult beam_f8950_0() throws Exception
{    return getReadOperation().requestCheckpoint();}
public double beam_f8959_0()
{    return REMAINING_PARALLELISM_FROM_PROGRESS_FRACTION;}
public void beam_f8961_0() throws IOException
{        close();}
 void beam_f8970_0()
{    if (initializationState != InitializationState.FINISHED) {        throw new AssertionError("expecting this instruction to be finished");    }}
 boolean beam_f8971_0()
{    return (initializationState == InitializationState.FINISHED);}
public OutputObjectAndByteCounter beam_f8980_0(String meanByteCounterName)
{    if (elementByteSizeObservable != null) {        meanByteCount = counterFactory.longMean(getCounterName(meanByteCounterName));        meanByteCountObserver = new CounterBackedElementByteSizeObserver(meanByteCount);    }    return this;}
public OutputObjectAndByteCounter beam_f8981_0(int period)
{    this.samplingTokenUpperBound = period * SAMPLING_CUTOFF;    return this;}
public void beam_f8990_0(Receiver receiver)
{    outputs.add(receiver);}
public void beam_f8991_0(ElementCounter outputCounter)
{    outputCounters.add(outputCounter);}
public ParDoFn beam_f9000_0() throws Exception
{    return fn;}
public final ProgressTracker<T> beam_f9001_0()
{    return new Tracker(0);}
protected static CounterName beam_f9010_0(OperationContext context)
{    return CounterName.named(String.format("%s-ByteCount", context.nameContext().systemName()));}
public NativeReader<?> beam_f9011_0()
{    return reader;}
public void beam_f9020_0()
{    abortRead.set(true);}
public NativeReader.Progress beam_f9021_0()
{    return progress.get();}
public NativeReader.DynamicSplitResult beam_f9032_0()
{    NativeReader.DynamicSplitResult result = readerIterator.requestCheckpoint();    if (result != null) {                setProgressFromIteratorConcurrent();    }    return result;}
public NativeReader.DynamicSplitResult beam_f9033_0(NativeReader.DynamicSplitRequest splitRequest)
{    NativeReader.DynamicSplitResult result = readerIterator.requestDynamicSplit(splitRequest);    if (result != null) {                setProgressFromIteratorConcurrent();    }    return result;}
public static String beam_f9042_0(byte[] bytes)
{        return Arrays.toString(bytes);}
public boolean beam_f9043_0(Object o)
{    if (this == o) {        return true;    }    if (o instanceof ShuffleEntry) {        ShuffleEntry that = (ShuffleEntry) o;        return (this.position == null ? that.position == null : this.position.equals(that.position)) && (this.key == null ? that.key == null : Arrays.equals(this.key, that.key)) && (this.secondaryKey == null ? that.secondaryKey == null : Arrays.equals(this.secondaryKey, that.secondaryKey)) && (this.value == null ? that.value == null : Arrays.equals(this.value, that.value));    }    return false;}
public boolean beam_f9055_0()
{    return false;}
 NativeReader.Progress beam_f9056_0() throws Exception
{        return null;}
private void beam_f9067_1()
{    if (executor.isShutdown()) {        return;    }    long delay = Math.min(progressReportIntervalMs, nextPeriodicCheckpointTimeMs - clock.currentTimeMillis());    executor.schedule(new Runnable() {        @Override        public void run() {            doNextUpdate();        }    }, delay, TimeUnit.MILLISECONDS);    }
public void beam_f9068_0()
{    doNextUpdate();}
protected long beam_f9077_0()
{    return DEFAULT_MAX_REPORTING_INTERVAL_MILLIS;}
protected long beam_f9078_0()
{    return DEFAULT_LEASE_RENEWAL_LATENCY_MARGIN;}
public void beam_f9088_0() throws Exception
{    if (writer == null) {                super.abort();        return;    }    try (Closeable scope = context.enterAbort()) {        writer.abort();    } finally {        super.abort();        writer = null;    }}
public boolean beam_f9089_0()
{    return sink.supportsRestart();}
private File beam_f9098_0()
{    return new File(localDumpFolder, "heap_dump.hprof");}
 boolean beam_f9099_1()
{    if (uploadToGCSPath == null) {        return false;    }    boolean uploadedHeapDump = false;    File localSource = getDefaultHeapDumpPath();        if (localSource.exists()) {                String remoteDest = String.format("%s/heap_dump%s.hprof", uploadToGCSPath, UUID.randomUUID().toString());        ResourceId resource = FileSystems.matchNewResource(remoteDest, false);        try {            uploadFileToGCS(localSource, resource);            uploadedHeapDump = true;                    } catch (IOException e) {                    }        try {            Files.delete(localSource.toPath());                    } catch (IOException e) {                    }    }    return uploadedHeapDump;}
private void beam_f9108_1(int thrashingCount)
{    File heapDumpFile = tryToDumpHeap();        System.exit(1);}
public void beam_f9109_1()
{    synchronized (waitingForStateChange) {        Preconditions.checkState(!isRunning.getAndSet(true), "already running");        waitingForStateChange.notifyAll();    }            tryUploadHeapDumpIfItExists();    try {        long lastTimeWokeUp = System.currentTimeMillis();        long lastLog = -1;        int currentThrashingCount = 0;        while (true) {            synchronized (waitingForStateChange) {                waitingForStateChange.wait(sleepTimeMillis);            }            if (!isRunning.get()) {                break;            }            long now = System.currentTimeMillis();            updateData(now, lastTimeWokeUp);            updateIsThrashing();            if (lastLog < 0 || lastLog + NORMAL_LOGGING_PERIOD_MILLIS < now) {                                lastLog = now;            }            if (isThrashing.get()) {                currentThrashingCount++;                if (shutDownAfterNumGCThrashing > 0 && (currentThrashingCount >= shutDownAfterNumGCThrashing)) {                    shutDownDueToGcThrashing(currentThrashingCount);                }            } else {                                currentThrashingCount = 0;            }            lastTimeWokeUp = now;        }    } catch (InterruptedException e) {        Thread.currentThread().interrupt();                            }}
public ScalableBloomFilter beam_f9118_0(InputStream inStream) throws CoderException, IOException
{    return new ScalableBloomFilter(BloomFilter.readFrom(inStream, ByteBufferFunnel.INSTANCE));}
public boolean beam_f9120_0()
{    return true;}
public boolean beam_f9129_0(final byte[] buf, final int off, final int len)
{    ByteBuffer buffer = ByteBuffer.wrap(buf, off, len);    return put(buffer);}
public boolean beam_f9130_0(final ByteBuffer byteBuffer)
{    if (bloomFilters.get(bloomFilters.size() - 1).mightContain(byteBuffer)) {                return false;    }    int bloomFilterToStartWith = Math.min(Long.SIZE - Long.numberOfLeadingZeros(numberOfInsertions), bloomFilters.size() - 1);    for (int i = bloomFilterToStartWith; i < bloomFilters.size(); ++i) {        bloomFilters.get(i).put(byteBuffer);    }    numberOfInsertions += 1;    return true;}
public Map<Class<? extends Coder>, CloudObjectTranslator<? extends Coder>> beam_f9139_0()
{    return ImmutableMap.of();}
public Map<String, CloudObjectTranslator<? extends Coder>> beam_f9140_0()
{    return Collections.singletonMap(TRANSLATOR.cloudObjectClassName(), TRANSLATOR);}
public ParDoFn beam_f9149_0(PipelineOptions options, CloudObject cloudUserFn, List<SideInputInfo> sideInputInfos, TupleTag<?> mainOutputTag, Map<TupleTag<?>, Integer> outputTupleTagsToReceiverIndices, DataflowExecutionContext<?> executionContext, DataflowOperationContext operationContext) throws Exception
{    return new ValuesParDoFn<>();}
public void beam_f9150_0(Receiver... receivers) throws Exception
{    checkState(receivers.length == 1, "%s.startBundle() called with %s receivers, expected exactly 1. " + "This is a bug in the Dataflow service", getClass().getSimpleName(), receivers.length);    this.receiver = receivers[0];}
public void beam_f9162_0()
{    onDoneHandler.run();    inboundObserver.onCompleted();}
public void beam_f9163_0(ClientCallStreamObserver<RespT> stream)
{    stream.setOnReadyHandler(onReadyHandler);}
public void beam_f9172_0(final URI uri, Executor executor, final org.apache.beam.vendor.grpc.v1p21p0.com.google.auth.RequestMetadataCallback callback)
{    credentials.getRequestMetadata(uri, executor, new VendoredRequestMetadataCallbackAdapter(callback));}
public Map<String, List<String>> beam_f9173_0(URI uri) throws IOException
{    return credentials.getRequestMetadata(uri);}
private synchronized CloudWindmillServiceV1Alpha1Grpc.CloudWindmillServiceV1Alpha1Stub beam_f9182_0()
{    if (stubList.isEmpty()) {        throw new RuntimeException("windmillServiceEndpoint has not been set");    }    if (stubList.size() == 1) {        return stubList.get(0);    }    return stubList.get(rand.nextInt(stubList.size()));}
private synchronized CloudWindmillServiceV1Alpha1Grpc.CloudWindmillServiceV1Alpha1BlockingStub beam_f9183_0()
{    if (syncStubList.isEmpty()) {        throw new RuntimeException("windmillServiceEndpoint has not been set");    }    if (syncStubList.size() == 1) {        return syncStubList.get(0);    }    return syncStubList.get(rand.nextInt(syncStubList.size()));}
public CommitWorkStream beam_f9192_0()
{    return new GrpcCommitWorkStream();}
public GetConfigResponse beam_f9193_0(GetConfigRequest request)
{    if (syncApplianceStub == null) {        throw new RpcException(new UnsupportedOperationException("GetConfig not supported with windmill service."));    } else {        return callWithBackoff(() -> syncApplianceStub.withDeadlineAfter(unaryDeadlineSeconds, TimeUnit.SECONDS).getConfig(request));    }}
public void beam_f9202_0(ResponseT response)
{    try {        backoff.reset();    } catch (IOException e) {        }    onResponse(response);}
public void beam_f9203_0(Throwable t)
{    onStreamFinished(t);}
public void beam_f9212_0(PrintWriter writer)
{        writer.format("GetWorkStream: %d buffers, %d inflight messages allowed, %d inflight bytes allowed", buffers.size(), inflightMessages.intValue(), inflightBytes.intValue());}
protected void beam_f9213_0(StreamingGetWorkResponseChunk chunk)
{    getWorkThrottleTimer.stop();    long id = chunk.getStreamId();    WorkItemBuffer buffer = buffers.computeIfAbsent(id, (Long l) -> new WorkItemBuffer());    buffer.append(chunk);    if (chunk.getRemainingBytesForWorkItem() == 0) {        long size = buffer.bufferedSize();        buffer.runAndReset();                long numInflight = inflightMessages.decrementAndGet();        long bytesInflight = inflightBytes.addAndGet(-size);                if (numInflight < request.getMaxItems() / 2 || bytesInflight < request.getMaxBytes() / 2) {            long moreItems = request.getMaxItems() - numInflight;            long moreBytes = request.getMaxBytes() - bytesInflight;            inflightMessages.getAndAdd(moreItems);            inflightBytes.getAndAdd(moreBytes);            final StreamingGetWorkRequest extension = StreamingGetWorkRequest.newBuilder().setRequestExtension(StreamingGetWorkRequestExtension.newBuilder().setMaxItems(moreItems).setMaxBytes(moreBytes)).build();            executor().execute(() -> {                try {                    send(extension);                } catch (IllegalStateException e) {                                }            });        }    }}
protected void beam_f9222_0(StreamingGetDataResponse chunk)
{    Preconditions.checkArgument(chunk.getRequestIdCount() == chunk.getSerializedResponseCount());    Preconditions.checkArgument(chunk.getRemainingBytesForResponse() == 0 || chunk.getRequestIdCount() == 1);    getDataThrottleTimer.stop();    for (int i = 0; i < chunk.getRequestIdCount(); ++i) {        AppendableInputStream responseStream = pending.get(chunk.getRequestId(i));        Verify.verify(responseStream != null, "No pending response stream");        responseStream.append(chunk.getSerializedResponse(i).newInput());        if (chunk.getRemainingBytesForResponse() == 0) {            responseStream.complete();        }    }}
protected void beam_f9223_0()
{    getDataThrottleTimer.start();}
 boolean beam_f9232_0(PendingRequest request)
{    return queue.isEmpty() || (queue.size() < streamingRpcBatchLimit && (request.getBytes() + queuedBytes) < COMMIT_STREAM_CHUNK_SIZE);}
 void beam_f9233_0(long id, PendingRequest request)
{    assert (canAccept(request));    queuedBytes += request.getBytes();    queue.put(id, request);}
private final void beam_f9242_0(Map<Long, PendingRequest> requests)
{    if (requests.isEmpty()) {        return;    }    if (requests.size() == 1) {        Map.Entry<Long, PendingRequest> elem = requests.entrySet().iterator().next();        if (elem.getValue().request.getSerializedSize() > COMMIT_STREAM_CHUNK_SIZE) {            issueMultiChunkRequest(elem.getKey(), elem.getValue());        } else {            issueSingleRequest(elem.getKey(), elem.getValue());        }    } else {        issueBatchedRequest(requests);    }    requests.clear();}
private void beam_f9243_0(final long id, PendingRequest pendingRequest)
{    StreamingCommitWorkRequest.Builder requestBuilder = StreamingCommitWorkRequest.newBuilder();    requestBuilder.addCommitChunkBuilder().setComputationId(pendingRequest.computation).setRequestId(id).setShardingKey(pendingRequest.request.getShardingKey()).setSerializedWorkItemCommit(pendingRequest.request.toByteString());    StreamingCommitWorkRequest chunk = requestBuilder.build();    try {        synchronized (this) {            pending.put(id, pendingRequest);            send(chunk);        }    } catch (IllegalStateException e) {        }}
public int beam_f9252_0(byte[] b, int off, int len) throws IOException
{    if (cancelled.get()) {        throw new CancellationException();    }    return stream.read(b, off, len);}
public int beam_f9253_0() throws IOException
{    if (cancelled.get()) {        throw new CancellationException();    }    return stream.available();}
public boolean beam_f9263_0()
{    return true;}
public Windmill.GetWorkResponse beam_f9264_0(Windmill.GetWorkRequest workRequest)
{    try {        byte[] requestBytes = workRequest.toByteArray();        return Windmill.GetWorkResponse.newBuilder().mergeFrom(getWorkImpl(nativePointer, requestBytes, requestBytes.length)).build();    } catch (IOException e) {        throw new RuntimeException("Proto deserialization failed: " + e);    }}
public S beam_f9274_0()
{    int index = ThreadLocalRandom.current().nextInt(streams.size());    S result;    S closeStream = null;    synchronized (this) {        StreamData streamData = streams.get(index);        if (streamData == null || streamData.stream.startTime().isBefore(Instant.now().minus(streamTimeout))) {            if (streamData != null && --streamData.holds == 0) {                holds.remove(streamData.stream);                closeStream = streamData.stream;            }            streamData = new StreamData();            streams.set(index, streamData);            holds.put(streamData.stream, streamData);        }        streamData.holds++;        result = streamData.stream;    }    if (closeStream != null) {        closeStream.close();    }    return result;}
public void beam_f9275_0(S stream)
{    boolean closeStream = false;    synchronized (this) {        if (--holds.get(stream).holds == 0) {            closeStream = true;            holds.remove(stream);        }    }    if (closeStream) {        stream.close();    }}
public void beam_f9284_0(KeyedWorkItem<K, ElemT> value, OutputStream outStream)
{    throw new UnsupportedOperationException();}
public KeyedWorkItem<K, ElemT> beam_f9285_0(InputStream inStream)
{    throw new UnsupportedOperationException();}
public WindowedValue<T> beam_f9295_0() throws NoSuchElementException
{    if (!current.isPresent()) {        throw new NoSuchElementException();    }    return current.get();}
public static ByteString beam_f9296_0(Coder<Collection<? extends BoundedWindow>> windowsCoder, Collection<? extends BoundedWindow> windows, PaneInfo pane) throws IOException
{    ByteString.Output stream = ByteString.newOutput();    PaneInfoCoder.INSTANCE.encode(pane, stream);    windowsCoder.encode(windows, stream, Coder.Context.OUTER);    return stream.toByteString();}
public void beam_f9305_0() throws IOException
{    close();}
public boolean beam_f9306_0()
{    return true;}
public ByteString beam_f9315_0()
{    return key;}
public boolean beam_f9316_0(Object that)
{    if (that instanceof ComputationKey) {        ComputationKey other = (ComputationKey) that;        return computation.equals(other.computation) && key.equals(other.key);    }    return false;}
public long beam_f9325_0()
{    return weight + PER_CACHE_ENTRY_OVERHEAD;}
public long beam_f9326_0()
{    return cacheToken;}
public BagState<T> beam_f9335_0(StateTag<BagState<T>> address, Coder<T> elemCoder)
{    WindmillBag<T> result = (WindmillBag<T>) cache.get(namespace, address);    if (result == null) {        result = new WindmillBag<>(namespace, address, stateFamily, elemCoder, isNewKey);    }    result.initializeForWorkItem(reader, scopedReadStateSupplier);    return result;}
public SetState<T> beam_f9336_0(StateTag<SetState<T>> spec, Coder<T> elemCoder)
{    throw new UnsupportedOperationException(String.format("%s is not supported", SetState.class.getSimpleName()));}
 void beam_f9345_0()
{    this.reader = null;    this.scopedReadStateSupplier = null;}
 Closeable beam_f9346_0()
{    return scopedReadStateSupplier.get();}
private Future<T> beam_f9355_0()
{        return valueIsKnown ? Futures.immediateFuture(value) : reader.valueFuture(stateKey, stateFamily, coder);}
public void beam_f9356_0()
{    cleared = true;    cachedValues = new ConcatIterables<>();    localAdditions = new ArrayList<>();    encodedSize = 0;}
public WorkItemCommitRequest beam_f9365_0(WindmillStateCache.ForKey cache) throws IOException
{    WorkItemCommitRequest.Builder commitBuilder = WorkItemCommitRequest.newBuilder();    Windmill.TagBag.Builder bagUpdatesBuilder = null;    if (cleared) {        bagUpdatesBuilder = commitBuilder.addBagUpdatesBuilder();        bagUpdatesBuilder.setDeleteAll(true);        cleared = false;    }    if (!localAdditions.isEmpty()) {                if (bagUpdatesBuilder == null) {            bagUpdatesBuilder = commitBuilder.addBagUpdatesBuilder();        }        for (T value : localAdditions) {            ByteString.Output stream = ByteString.newOutput();                        elemCoder.encode(value, stream, Coder.Context.OUTER);            ByteString encoded = stream.toByteString();            if (cachedValues != null) {                                                encodedSize += encoded.size();            }            bagUpdatesBuilder.addValues(encoded);        }    }    if (bagUpdatesBuilder != null) {        bagUpdatesBuilder.setTag(stateKey).setStateFamily(stateFamily);    }    if (cachedValues != null) {        if (!localAdditions.isEmpty()) {                                    cachedValues.extendWith(localAdditions);        }                        cache.put(namespace, address, this, encodedSize);    }            localAdditions = new ArrayList<>();    return commitBuilder.buildPartial();}
private Future<Iterable<T>> beam_f9366_0()
{    return cachedValues != null ? null : reader.bagFuture(stateKey, stateFamily, elemCoder);}
public TimestampCombiner beam_f9375_0()
{    return timestampCombiner;}
public Future<WorkItemCommitRequest> beam_f9376_0(final WindmillStateCache.ForKey cache)
{    Future<WorkItemCommitRequest> result;    if (!cleared && localAdditions == null) {                return Futures.immediateFuture(WorkItemCommitRequest.newBuilder().buildPartial());    }    if (cleared && localAdditions == null) {                WorkItemCommitRequest.Builder commitBuilder = WorkItemCommitRequest.newBuilder();        commitBuilder.addWatermarkHoldsBuilder().setTag(stateKey).setStateFamily(stateFamily).setReset(true);        result = Futures.immediateFuture(commitBuilder.buildPartial());    } else if (cleared && localAdditions != null) {                WorkItemCommitRequest.Builder commitBuilder = WorkItemCommitRequest.newBuilder();        commitBuilder.addWatermarkHoldsBuilder().setTag(stateKey).setStateFamily(stateFamily).setReset(true).addTimestamps(WindmillTimeUtils.harnessToWindmillTimestamp(localAdditions));        cachedValue = Optional.of(localAdditions);        result = Futures.immediateFuture(commitBuilder.buildPartial());    } else if (!cleared && localAdditions != null) {                result = combineWithPersisted();    } else {        throw new IllegalStateException("Unreachable condition");    }    return Futures.lazyTransform(result, result1 -> {        cleared = false;        localAdditions = null;        if (cachedValue != null) {            cache.put(namespace, address, WindmillWatermarkHold.this, ENCODED_SIZE);        }        return result1;    });}
public Future<WorkItemCommitRequest> beam_f9385_0(WindmillStateCache.ForKey cache) throws IOException
{    if (hasLocalAdditions) {        if (COMPACT_NOW.get().get() || bag.valuesAreCached()) {                        localAdditionsAccum = getAccum();        }        bag.add(combineFn.compact(localAdditionsAccum));        localAdditionsAccum = combineFn.createAccumulator();        hasLocalAdditions = false;    }    return bag.persist(cache);}
public AccumT beam_f9386_0()
{    Iterable<AccumT> accums = Iterables.concat(bag.read(), Collections.singleton(localAdditionsAccum));        AccumT merged = combineFn.mergeAccumulators(accums);    bag.clear();    localAdditionsAccum = merged;    hasLocalAdditions = true;    return merged;}
public int beam_f9395_0()
{    return Objects.hashCode(kind, tag, stateFamily, requestPosition);}
public String beam_f9396_0()
{    return "Tag(" + kind + "," + tag.toStringUtf8() + "," + stateFamily + (requestPosition == null ? "" : ("," + requestPosition.toString())) + ")";}
public Future<Iterable<T>> beam_f9405_0(ByteString encodedTag, String stateFamily, Coder<T> elemCoder)
{        StateTag stateTag = new StateTag(StateTag.Kind.BAG, encodedTag, stateFamily);        return valuesToPagingIterableFuture(stateTag, elemCoder, this.<T, ValuesAndContPosition<T>>stateFuture(stateTag, elemCoder));}
private Future<ValuesAndContPosition<T>> beam_f9406_0(StateTag contStateTag, Coder<T> elemCoder)
{    if (contStateTag.requestPosition == null) {                return null;    }    return stateFuture(contStateTag, elemCoder);}
private void beam_f9415_0(Windmill.KeyedGetDataRequest request, Windmill.KeyedGetDataResponse response, Set<StateTag> toFetch)
{    bytesRead += response.getSerializedSize();    if (response.getFailed()) {                KeyTokenInvalidException keyTokenInvalidException = new KeyTokenInvalidException(key.toStringUtf8());        for (StateTag stateTag : toFetch) {            waiting.get(stateTag).future.setException(keyTokenInvalidException);        }        return;    }    if (!key.equals(response.getKey())) {        throw new RuntimeException("Expected data for key " + key + " but was " + response.getKey());    }    for (Windmill.TagBag bag : response.getBagsList()) {        StateTag stateTag = new StateTag(StateTag.Kind.BAG, bag.getTag(), bag.getStateFamily(), bag.hasRequestPosition() ? bag.getRequestPosition() : null);        if (!toFetch.remove(stateTag)) {            throw new IllegalStateException("Received response for unrequested tag " + stateTag + ". Pending tags: " + toFetch);        }        consumeBag(bag, stateTag);    }    for (Windmill.WatermarkHold hold : response.getWatermarkHoldsList()) {        StateTag stateTag = new StateTag(StateTag.Kind.WATERMARK, hold.getTag(), hold.getStateFamily());        if (!toFetch.remove(stateTag)) {            throw new IllegalStateException("Received response for unrequested tag " + stateTag + ". Pending tags: " + toFetch);        }        consumeWatermark(hold, stateTag);    }    for (Windmill.TagValue value : response.getValuesList()) {        StateTag stateTag = new StateTag(StateTag.Kind.VALUE, value.getTag(), value.getStateFamily());        if (!toFetch.remove(stateTag)) {            throw new IllegalStateException("Received response for unrequested tag " + stateTag + ". Pending tags: " + toFetch);        }        consumeTagValue(value, stateTag);    }    if (!toFetch.isEmpty()) {        throw new IllegalStateException("Didn't receive responses for all pending fetches. Missing: " + toFetch);    }}
protected List<T> beam_f9416_0()
{    return delegate;}
protected T beam_f9425_0()
{    while (true) {        if (currentPage.hasNext()) {            return currentPage.next();        }        if (pendingNextPage == null) {            return endOfData();        }        ValuesAndContPosition<T> valuesAndContPosition;        try {            valuesAndContPosition = pendingNextPage.get();        } catch (InterruptedException | ExecutionException e) {            if (e instanceof InterruptedException) {                Thread.currentThread().interrupt();            }            throw new RuntimeException("Unable to read value from state", e);        }        currentPage = valuesAndContPosition.values.iterator();        nextPagePos = new StateTag(nextPagePos.kind, nextPagePos.tag, nextPagePos.stateFamily, valuesAndContPosition.continuationPosition);        pendingNextPage =         reader.continuationBagFuture(nextPagePos, elemCoder);    }}
public WindmillTimerInternals beam_f9426_0(WindmillNamespacePrefix prefix)
{    return new WindmillTimerInternals(stateFamily, prefix, inputDataWatermark, processingTime, outputDataWatermark, synchronizedProcessingTime);}
public Instant beam_f9435_0()
{    return outputDataWatermark;}
public void beam_f9436_0(Windmill.WorkItemCommitRequest.Builder outputBuilder)
{    for (Cell<String, StateNamespace, Boolean> cell : timerStillPresent.cellSet()) {                        TimerData timerData = timers.get(cell.getRowKey(), cell.getColumnKey());        Timer.Builder timer = buildWindmillTimerFromTimerData(stateFamily, prefix, timerData, outputBuilder.addOutputTimersBuilder());        if (cell.getValue()) {                        if (WindmillNamespacePrefix.USER_NAMESPACE_PREFIX.equals(prefix)) {                                outputBuilder.addWatermarkHoldsBuilder().setTag(timerHoldTag(prefix, timerData)).setStateFamily(stateFamily).setReset(true).addTimestamps(WindmillTimeUtils.harnessToWindmillTimestamp(timerData.getTimestamp()));            }        } else {                        timer.clearTimestamp();            if (WindmillNamespacePrefix.USER_NAMESPACE_PREFIX.equals(prefix)) {                                outputBuilder.addWatermarkHoldsBuilder().setTag(timerHoldTag(prefix, timerData)).setStateFamily(stateFamily).setReset(true);            }        }    }        timers.clear();}
 static TimeDomain beam_f9445_0(Windmill.Timer.Type type)
{    switch(type) {        case REALTIME:            return TimeDomain.PROCESSING_TIME;        case DEPENDENT_REALTIME:            return TimeDomain.SYNCHRONIZED_PROCESSING_TIME;        case WATERMARK:            return TimeDomain.EVENT_TIME;        default:            throw new IllegalArgumentException("Unsupported timer type " + type);    }}
public static Instant beam_f9446_0(long watermarkUs)
{    if (watermarkUs == Long.MIN_VALUE) {                return null;    } else {        return windmillToHarnessTimestamp(watermarkUs);    }}
public boolean beam_f9455_0() throws IOException
{    return false;}
public WindowedValue<KeyedWorkItem<K, T>> beam_f9456_0()
{    throw new NoSuchElementException();}
public Iterable<CounterUpdate> beam_f9465_0()
{    return Collections.emptyList();}
public String beam_f9466_0()
{    return String.format("<primary: %s; residual: %s>", primary, residual);}
public NativeReader<?> beam_f9475_0(CloudObject spec, @Nullable Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{        return WorkerCustomSources.create(spec, options, executionContext);}
public static NativeReader<WindowedValue<?>> beam_f9476_0(final CloudObject spec, final PipelineOptions options, DataflowExecutionContext executionContext) throws Exception
{    @SuppressWarnings("unchecked")    final Source<Object> source = (Source<Object>) deserializeFromCloudSource(spec);    if (source instanceof BoundedSource) {        @SuppressWarnings({ "unchecked", "rawtypes" })        NativeReader<WindowedValue<?>> reader = (NativeReader) new NativeReader<WindowedValue<Object>>() {            @Override            public NativeReaderIterator<WindowedValue<Object>> iterator() throws IOException {                return new BoundedReaderIterator<>(((BoundedSource<Object>) source).createReader(options));            }        };        return reader;    } else if (source instanceof UnboundedSource) {        @SuppressWarnings({ "unchecked", "rawtypes" })        NativeReader<WindowedValue<?>> reader = (NativeReader) new UnboundedReader<Object>(options, spec, (StreamingModeExecutionContext) executionContext);        return reader;    } else {        throw new IllegalArgumentException("Unexpected source kind: " + source.getClass());    }}
public void beam_f9485_0()
{    for (BoundedSource<T> boundedSource : boundedSources) {        boundedSource.validate();    }}
public Coder<T> beam_f9486_0()
{    return boundedSources.get(0).getDefaultOutputCoder();}
public NativeReader.Progress beam_f9495_0()
{        return SourceTranslationUtils.cloudProgressToReaderProgress(getReaderProgress(reader));}
public NativeReader.DynamicSplitResult beam_f9496_1(NativeReader.DynamicSplitRequest request)
{    ApproximateSplitRequest stopPosition = SourceTranslationUtils.splitRequestToApproximateSplitRequest(request);    Double fractionConsumed = stopPosition.getFractionConsumed();    if (fractionConsumed == null) {                        return null;    }    BoundedSource<T> original = reader.getCurrentSource();    BoundedSource<T> residual = reader.splitAtFraction(fractionConsumed);    if (residual == null) {                return null;    }        BoundedSource<T> primary = reader.getCurrentSource();    if (original == primary) {        throw new IllegalStateException("Successful split did not change the current source: primary is identical to original" + " (Source objects MUST be immutable): " + primary);    }    if (original == residual) {        throw new IllegalStateException("Successful split did not change the current source: residual is identical to original" + " (Source objects MUST be immutable): " + residual);    }    try {        primary.validate();    } catch (Exception e) {        throw new IllegalStateException("Successful split produced an illegal primary source. " + "\nOriginal: " + original + "\nPrimary: " + primary + "\nResidual: " + residual);    }    try {        residual.validate();    } catch (Exception e) {        throw new IllegalStateException("Successful split produced an illegal residual source. " + "\nOriginal: " + original + "\nPrimary: " + primary + "\nResidual: " + residual);    }    return new BoundedSourceSplit<T>(primary, residual);}
public void beam_f9506_1(Thread thread, Throwable e)
{    try {                System.err.println("Uncaught exception in main thread. Exiting with status code 1.");        e.printStackTrace();    } catch (Throwable t) {        PrintStream originalStdErr = DataflowWorkerLoggingInitializer.getOriginalStdErr();        if (originalStdErr != null) {            originalStdErr.println("Uncaught exception in main thread. Exiting with status code 1.");            e.printStackTrace(originalStdErr);            originalStdErr.println("UncaughtExceptionHandler caused another exception to be thrown, as follows:");            t.printStackTrace(originalStdErr);        }    } finally {        runtime.halt(1);    }}
public String beam_f9507_0()
{    if (uniqueWorkId == null) {        uniqueWorkId = String.format("%s;%s;%s", workItem.getProjectId(), workItem.getJobId(), workItem.getId());    }    return uniqueWorkId;}
 synchronized void beam_f9516_0(WorkItemStatus status)
{    List<MetricUpdate> updates = new ArrayList<>();    if (executionContext != null && executionContext.getExecutionStateTracker() != null) {        ExecutionStateTracker tracker = executionContext.getExecutionStateTracker();        MetricUpdate update = new MetricUpdate();        update.setKind("internal");        MetricStructuredName name = new MetricStructuredName();        name.setName("state-sampler");        update.setName(name);        Map<String, Object> metric = new HashMap<>();        ExecutionState state = tracker.getCurrentState();        if (state != null) {            metric.put("last-state-name", state.getDescription());        }        metric.put("num-transitions", tracker.getNumTransitions());        metric.put("last-state-duration-ms", tracker.getMillisSinceLastTransition());        update.setInternal(metric);        updates.add(update);    }    status.setMetricUpdates(updates);}
private synchronized WorkItemStatus beam_f9517_0(boolean isFinal)
{    WorkItemStatus status = new WorkItemStatus();    status.setWorkItemId(Long.toString(workItem.getId()));    status.setCompleted(isFinal);    status.setReportIndex(checkNotNull(nextReportIndex, "nextReportIndex should be non-null when sending an update"));    if (worker != null) {        populateMetricUpdates(status);        populateCounterUpdates(status);    }    double throttleTime = extractThrottleTime();    status.setTotalThrottlerWaitTimeSeconds(throttleTime);    return status;}
public void beam_f9526_0() throws Throwable
{    Throwable thrown = null;    delegate = executorServiceSupplier.get();    try {        statement.evaluate();    } catch (Throwable t) {        thrown = t;    }    shutdown();    if (!awaitTermination(5, TimeUnit.SECONDS)) {        shutdownNow();        IllegalStateException e = new IllegalStateException("Test executor failed to shutdown cleanly.");        if (thrown != null) {            thrown.addSuppressed(e);        } else {            thrown = e;        }    }    if (thrown != null) {        throw thrown;    }}
protected ExecutorService beam_f9527_0()
{    return delegate;}
public void beam_f9536_0() throws Throwable
{    testService.submit(this::taskToRun);    throw exceptionToThrow;}
private void beam_f9537_0()
{    taskStarted.set(true);    try {        while (true) {            Thread.sleep(10000);        }    } catch (InterruptedException e) {        taskWasInterrupted.set(true);        return;    }}
public void beam_f9548_0(Throwable t)
{    onError.accept(t);}
public void beam_f9549_0()
{    onCompleted.run();}
public void beam_f9562_0()
{    FixMultiOutputInfosOnParDoInstructions function = new FixMultiOutputInfosOnParDoInstructions(IdGenerators.decrementingLongs());    MapTask output = function.apply(createMapTaskWithParDo(2, "5", "6"));    assertEquals(createMapTaskWithParDo(2, "5", "6"), output);}
public void beam_f9563_0()
{    FixMultiOutputInfosOnParDoInstructions function = new FixMultiOutputInfosOnParDoInstructions(IdGenerators.decrementingLongs());    MapTask output = function.apply(createMapTaskWithParDo(1));    assertEquals(createMapTaskWithParDo(1, "-1"), output);}
public void beam_f9572_0() throws Exception
{    String[] names = { "sum_counter", "max_counter", "min_counter" };    String[] kinds = { "sum", "max", "min" };    long[] deltas = { 100, 200, 300 };    counters.importCounters(names, kinds, deltas);    Counter<Long, Long> sumCounter = getCounter("sum_counter");    assertNotNull(sumCounter);    counterSet.extractUpdates(false, mockUpdateExtractor);    verify(mockUpdateExtractor).longSum(named("stageName-systemName-dataset-sum_counter"), false, 100L);    verify(mockUpdateExtractor).longMax(named("stageName-systemName-dataset-max_counter"), false, 200L);    verify(mockUpdateExtractor).longMin(named("stageName-systemName-dataset-min_counter"), false, 300L);    verifyNoMoreInteractions(mockUpdateExtractor);}
public void beam_f9573_0() throws Exception
{    Counter<Long, Long> sumCounter = counterSet.longSum(CounterName.named("stageName-systemName-dataset-sum_counter"));    sumCounter.addValue(1000L);    String[] names = { "sum_counter" };    String[] kinds = { "sum" };    long[] deltas = { 122 };    counters.importCounters(names, kinds, deltas);    counterSet.extractUpdates(false, mockUpdateExtractor);    verify(mockUpdateExtractor).longSum(named("stageName-systemName-dataset-sum_counter"), false, 1122L);    verifyNoMoreInteractions(mockUpdateExtractor);}
public void beam_f9582_0() throws Exception
{    runTestRead(Collections.singletonList(TestUtils.INTS), BigEndianIntegerCoder.of(), true);}
public void beam_f9583_0() throws Exception
{    runTestRead(Collections.singletonList(TestUtils.NO_INTS), BigEndianIntegerCoder.of(), true);}
public void beam_f9592_0() throws Exception
{    Coder<?> coder = WindowedValue.getFullCoder(BigEndianIntegerCoder.of(), GlobalWindow.Coder.INSTANCE);    Sink<?> sink = runTestCreateAvroSink(pathToAvroFile, coder);    Assert.assertThat(sink, new IsInstanceOf(AvroByteSink.class));    AvroByteSink<?> avroSink = (AvroByteSink<?>) sink;    Assert.assertEquals(pathToAvroFile, avroSink.resourceId.toString());    Assert.assertEquals(coder, avroSink.coder);}
 void beam_f9593_0(List<T> elems, Coder<T> coder) throws Exception
{    File tmpFile = tmpFolder.newFile("file.avro");    String filename = tmpFile.getPath();        AvroByteSink<T> avroSink = new AvroByteSink<>(FileSystems.matchNewResource(filename, false), coder);    List<Long> actualSizes = new ArrayList<>();    try (Sink.SinkWriter<T> writer = avroSink.writer()) {        for (T elem : elems) {            actualSizes.add(writer.add(elem));        }    }        AvroByteReader<T> reader = new AvroByteReader<>(filename, 0L, Long.MAX_VALUE, coder, PipelineOptionsFactory.create());    List<T> actual = readAllFromReader(reader);    List<Long> expectedSizes = new ArrayList<>();    for (T value : actual) {        expectedSizes.add((long) CoderUtils.encodeToByteArray(coder, value).length);    }        Assert.assertEquals(elems, actual);    Assert.assertEquals(expectedSizes, actualSizes);}
public void beam_f9603_0() throws IOException
{    SourceSplitResponse splitResponse = new SourceSplitResponse();    splitResponse.setShards(ImmutableList.<SourceSplitShard>of(new SourceSplitShard(), new SourceSplitShard()));    assertThat(DataflowApiUtils.computeSerializedSizeBytes(splitResponse), greaterThan(0L));}
public void beam_f9604_0()
{    BatchModeExecutionContext executionContext = BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "testStage");    DataflowOperationContext operationContext = executionContext.createOperationContext(NameContextsForTests.nameContextForTest());    Counter counter = operationContext.metricsContainer().getCounter(MetricName.named("namespace", "some-counter"));    counter.inc(1);    counter.inc(41);    counter.inc(1);    counter.inc(-1);    final CounterUpdate expected = new CounterUpdate().setStructuredNameAndMetadata(new CounterStructuredNameAndMetadata().setName(new CounterStructuredName().setOrigin("USER").setOriginNamespace("namespace").setName("some-counter").setOriginalStepName("originalName")).setMetadata(new CounterMetadata().setKind("SUM"))).setCumulative(true).setInteger(longToSplitInt(42));    assertThat(executionContext.extractMetricUpdates(false), containsInAnyOrder(expected));    executionContext.commitMetricUpdates();    Counter counterUncommitted = operationContext.metricsContainer().getCounter(MetricName.named("namespace", "uncommitted-counter"));    counterUncommitted.inc(64);    final CounterUpdate expectedUncommitted = new CounterUpdate().setStructuredNameAndMetadata(new CounterStructuredNameAndMetadata().setName(new CounterStructuredName().setOrigin("USER").setOriginNamespace("namespace").setName("uncommitted-counter").setOriginalStepName("originalName")).setMetadata(new CounterMetadata().setKind("SUM"))).setCumulative(true).setInteger(longToSplitInt(64));        assertThat(executionContext.extractMetricUpdates(false), containsInAnyOrder(expectedUncommitted));    assertThat(executionContext.extractMetricUpdates(true), containsInAnyOrder(expected, expectedUncommitted));    executionContext.commitMetricUpdates();        assertThat(executionContext.extractMetricUpdates(false), emptyIterable());    assertThat(executionContext.extractMetricUpdates(true), containsInAnyOrder(expected, expectedUncommitted));}
public void beam_f9613_0(Integer element)
{    count++;    sum += element.doubleValue();}
public void beam_f9614_0(CountSum accumulator)
{    count += accumulator.count;    sum += accumulator.sum;}
public void beam_f9624_0(Object outputElem)
{    receivedElems.add(outputElem);}
private ParDoFn beam_f9625_0(String phase, Combine.CombineFn<InputT, AccumT, OutputT> combineFn, Coder<K> keyCoder, Coder<InputT> inputCoder, Coder<AccumT> accumCoder, WindowingStrategy<?, ?> windowingStrategy) throws Exception
{                    CloudObject spec = CloudObject.forClassName("CombineValuesFn");    @SuppressWarnings("unchecked")    AppliedCombineFn appliedCombineFn = AppliedCombineFn.withAccumulatorCoder(combineFn, accumCoder, Collections.emptyList(), KvCoder.of(keyCoder, inputCoder), windowingStrategy);    addString(spec, PropertyNames.SERIALIZED_FN, byteArrayToJsonString(serializeToByteArray(appliedCombineFn)));    addString(spec, WorkerPropertyNames.PHASE, phase);    return parDoFnFactory.create(PipelineOptionsFactory.create(), spec, ImmutableList.<SideInputInfo>of(), MAIN_OUTPUT, ImmutableMap.<TupleTag<?>, Integer>of(MAIN_OUTPUT, 0), BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "testStage"), TestOperationContext.create());}
public void beam_f9634_0() throws Exception
{    List<List<String>> allData = createInMemorySourceData(15, 10);    Source source = createSourcesWithInMemorySources(allData);    @SuppressWarnings("unchecked")    NativeReader<String> reader = (NativeReader<String>) ReaderRegistry.defaultRegistry().create(source, null, null, null);    assertNotNull(reader);    List<String> expected = new ArrayList<>();    for (List<String> data : allData) {        expected.addAll(data);    }    assertThat(readAllFromReader(reader), containsInAnyOrder(expected.toArray()));}
public void beam_f9635_0()
{    recordedReaders.clear();    registry = ReaderRegistry.defaultRegistry().register(TestReader.class.getName(), new TestReaderFactory());}
public double beam_f9644_0()
{    return Double.NaN;}
public NativeReader<?> beam_f9645_0(CloudObject spec, @Nullable Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext) throws Exception
{    return (NativeReader<?>) spec.get(READER_OBJECT);}
public void beam_f9654_0() throws Exception
{    testReadersOfSizes(10, 5, 20, 40);}
public void beam_f9655_0() throws Exception
{    testReadersOfSizes(0, 5, 20, 40);}
public void beam_f9664_0() throws Exception
{    runProgressTest(10, 30, 20, 15, 7);}
public void beam_f9665_0(int... readerSizes) throws Exception
{    ConcatReader<String> concatReader = createConcatReadersOfSizes(new ArrayList<String>(), readerSizes);        for (int indexToSplit = 0; indexToSplit <= readerSizes.length; indexToSplit++) {                int recordsToRead = -1;        for (int readerIndex = 0; readerIndex < readerSizes.length; readerIndex++) {            for (int recordIndex = 0; recordIndex <= readerSizes[readerIndex]; recordIndex++) {                if (readerIndex > 0 && recordIndex == 0) {                                        continue;                }                recordsToRead++;                NativeReader.NativeReaderIterator<String> iterator = concatReader.iterator();                for (int i = 0; i < recordsToRead; i++) {                    if (i == 0) {                        iterator.start();                    } else {                        iterator.advance();                    }                }                DynamicSplitResult splitResult = iterator.requestDynamicSplit(ReaderTestUtils.splitRequestAtConcatPosition(indexToSplit, null));                if ((recordsToRead == 0) || (readerIndex >= indexToSplit) || (indexToSplit < 0 || indexToSplit >= readerSizes.length)) {                    assertNull(splitResult);                } else {                    Assert.assertEquals(indexToSplit, ReaderTestUtils.positionFromSplitResult(splitResult).getConcatPosition().getIndex().intValue());                }            }        }    }}
private void beam_f9675_0(long value, int expectedBucket)
{    int actualBucket = CounterDistribution.calculateBucket(value);    assertEquals(String.format("Bucket value for %d", value), expectedBucket, actualBucket);}
public void beam_f9676_0()
{    CounterDistribution counter = CounterDistribution.empty();    List<Long> expectedBuckets = ImmutableList.of(1L, 3L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L);    for (long value : new long[] { 1, 500, 2, 3, 1000, 4 }) {        counter = counter.addValue(value);    }    assertEquals(expectedBuckets, counter.getBuckets());    assertEquals(1250030.0, counter.getSumOfSquares(), 0);    assertEquals(1510, counter.getSum());    assertEquals(1, counter.getFirstBucketOffset());    assertEquals(6, counter.getCount());    assertEquals(1, counter.getMin());    assertEquals(1000, counter.getMax());}
public void beam_f9685_0()
{    Counter<Double, Double> c = counters.doubleSum(name);    c.addValue(Math.E).addValue(Math.PI).addValue(0.0);    assertEquals(Math.E + Math.PI, c.getAggregate(), EPSILON);    c.getAndReset();    c.addValue(Math.sqrt(2)).addValue(2 * Math.PI).addValue(3 * Math.E);    assertEquals(Math.sqrt(2) + 2 * Math.PI + 3 * Math.E, c.getAggregate(), EPSILON);    assertEquals("getAndReset should return previous value", Math.sqrt(2) + 2 * Math.PI + 3 * Math.E, c.getAndReset(), EPSILON);    assertEquals("getAndReset should have reset value", 0.0, c.getAggregate(), EPSILON);}
public void beam_f9686_0()
{    Counter<Long, Long> c = counters.longMax(name);    assertEquals(Long.MIN_VALUE, (long) c.getAggregate());    c.addValue(13L).addValue(42L).addValue(0L);    assertEquals(42L, (long) c.getAggregate());    c.getAndReset();    c.addValue(120L).addValue(17L).addValue(37L);    assertEquals(120L, (long) c.getAggregate());    c.addValue(15L).addValue(42L);    assertEquals(120L, (long) c.getAggregate());    c.addValue(137L);    assertEquals(137L, (long) c.getAggregate());    c.getAndReset();    c.addValue(100L).addValue(17L).addValue(49L);    assertEquals(100L, (long) c.getAggregate());    assertEquals("getAndReset should return previous value", 100L, (long) c.getAndReset());    assertEquals("getAndReset should have reset value", Long.MIN_VALUE, (long) c.getAggregate());}
public void beam_f9695_0()
{    Counter<Boolean, Boolean> c = counters.booleanAnd(name);    assertTrue(c.getAggregate());    c.addValue(true);    assertTrue(c.getAggregate());    c.addValue(false);    assertFalse(c.getAggregate());    c.getAndReset();    c.addValue(true).addValue(true);    assertTrue(c.getAggregate());    c.addValue(false);    assertFalse(c.getAggregate());    assertFalse(c.getAndReset());    assertTrue(c.getAggregate());    c.addValue(false);    assertFalse(c.getAggregate());}
public void beam_f9696_0()
{    Counter<Boolean, Boolean> c = counters.booleanOr(name);    assertFalse(c.getAggregate());    c.addValue(false);    assertFalse(c.getAggregate());    c.addValue(true);    assertTrue(c.getAggregate());    c.getAndReset();    c.addValue(false).addValue(false);    assertFalse(c.getAggregate());    c.addValue(true);    assertTrue(c.getAggregate());    assertTrue(c.getAndReset());    assertFalse(c.getAggregate());    c.addValue(true);    assertTrue(c.getAggregate());}
private List<WorkItemServiceState> beam_f9705_0(Long[]... counterIds)
{    List<WorkItemServiceState> states = new ArrayList<>();    for (Long[] ids : counterIds) {        WorkItemServiceState state = new WorkItemServiceState();        List<MetricShortId> shortIds = new ArrayList<>();        for (int i = 0; i < ids.length; i++) {            shortIds.add(createMetricShortId(i, ids[i]));        }        state.setMetricShortId(shortIds);        states.add(state);    }    return states;}
private List<WorkItemServiceState> beam_f9706_0(MetricShortId[]... counterIds)
{    List<WorkItemServiceState> states = new ArrayList<>();    for (MetricShortId[] ids : counterIds) {        WorkItemServiceState state = new WorkItemServiceState();        List<MetricShortId> shortIds = new ArrayList<>();        for (int i = 0; i < ids.length; i++) {            shortIds.add(ids[i]);        }        state.setMetricShortId(shortIds);        states.add(state);    }    return states;}
private Map<String, Object> beam_f9715_0()
{    return ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "kind:ism_record", "num_shard_key_coders", 1L, PropertyNames.COMPONENT_ENCODINGS, ImmutableList.of(ImmutableMap.of("@type", "kind:var_int32"), ImmutableMap.of("@type", "kind:global_window"), ImmutableMap.of("@type", "kind:fixed_big_endian_int32"), ImmutableMap.of("@type", "kind:fixed_big_endian_int32")));}
public void beam_f9716_0() throws Exception
{    MockitoAnnotations.initMocks(this);    when(transport.buildRequest(anyString(), anyString())).thenReturn(request);    doCallRealMethod().when(request).getContentAsString();    Dataflow service = new Dataflow(transport, Transport.getJsonFactory(), null);    pipelineOptions = PipelineOptionsFactory.as(DataflowWorkerHarnessOptions.class);    pipelineOptions.setProject(PROJECT_ID);    pipelineOptions.setJobId(JOB_ID);    pipelineOptions.setWorkerId(WORKER_ID);    pipelineOptions.setGcpCredential(new TestCredential());    pipelineOptions.setDataflowClient(service);}
public void beam_f9725_0() throws IOException
{    NameContext stepA = createStep("A");    NameContext stepB = createStep("B");    NameContext stepC = createStep("C");    NameContext stepD = createStep("D");            tracker.enter(stepA);        tracker.enter(stepB);        tracker.exit();        tracker.takeSample(40);    assertThat(getCounterValue(stepB), equalTo(distribution(10)));        tracker.enter(stepB);        tracker.exit();        tracker.enter(stepC);        tracker.enter(stepD);        tracker.takeSample(50);    assertThat(getCounterValue(stepB), equalTo(distribution(10, 10)));        tracker.exit();        tracker.exit();        tracker.enter(stepC);        tracker.takeSample(40);    assertThat(getCounterValue(stepC), equalTo(distribution(20)));    assertThat(getCounterValue(stepD), equalTo(distribution(20)));        tracker.exit();        tracker.exit();        tracker.takeSample(30);    assertThat(getCounterValue(stepA), equalTo(distribution(60)));    assertThat(getCounterValue(stepB), equalTo(distribution(10, 10)));    assertThat(getCounterValue(stepC), equalTo(distribution(20, 20)));    assertThat(getCounterValue(stepD), equalTo(distribution(20)));}
public void beam_f9726_0() throws IOException
{    NameContext step = createStep("A");    tracker.enter(step);        tracker.takeSample(10);    assertThat(getCounter(step), nullValue());    tracker.exit();        tracker.takeSample(10);    assertThat(getCounterValue(step), equalTo(distribution(10)));}
private Counter<Long, CounterDistribution> beam_f9735_0(NameContext step)
{    CounterName counterName = CounterName.named("per-element-processing-time").withOriginalName(step);    return (Counter<Long, CounterDistribution>) counters.getExistingCounter(counterName);}
private CounterDistribution beam_f9736_0(long... values)
{    CounterDistribution dist = CounterDistribution.empty();    for (long value : values) {        dist = dist.addValue(value);    }    return dist;}
public void beam_f9745_0() throws Exception
{    BatchModeExecutionContext executionContext = BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "testStage");    Closeable c = executionContext.getExecutionStateTracker().activate();    c.close();        assertTrue(AutoRegistrationClass.WAS_CALLED);        assertFalse(AutoRegistrationClassNotActive.WAS_CALLED);}
public void beam_f9746_0()
{    options = PipelineOptionsFactory.create();    clock = mock(MillisProvider.class);    sampler = ExecutionStateSampler.newForTest(clock);    counterSet = new CounterSet();}
protected boolean beam_f9755_0(ByteString actual)
{    return actual.equals(expected);}
public void beam_f9756_0()
{    MockitoAnnotations.initMocks(this);    otherState = stateRegistry.getState(NameContext.forStage("STAGE"), "other", null, emptyScope);    startState = stateRegistry.getState(NameContextsForTests.nameContextForTest(), START_STATE_NAME, metricsContainer, profileScope);    processState = stateRegistry.getState(NameContextsForTests.nameContextForTest(), PROCESS_STATE_NAME, metricsContainer, profileScope);    abortState = stateRegistry.getState(NameContextsForTests.nameContextForTest(), ABORT_STATE_NAME, metricsContainer, profileScope);    stateRegistry.getState(NameContextsForTests.nameContextForTest(), FINISH_STATE_NAME, metricsContainer, profileScope);    operationContext = new DataflowOperationContext(counterFactory, NameContextsForTests.nameContextForTest(), metricsContainer, stateTracker, stateRegistry);        stateTracker.enterState(otherState);        Mockito.reset(emptyScope);}
public void beam_f9766_0()
{    assertThat(DataflowOperationContext.formatDuration(getDuration(0, 0, 5, 10, 500)), equalTo("05m10s"));    assertThat(DataflowOperationContext.formatDuration(getDuration(0, 2, 10, 23, 500)), equalTo("02h10m23s"));    assertThat(DataflowOperationContext.formatDuration(getDuration(1, 0, 0, 23, 500)), equalTo("24h00m23s"));    assertThat(DataflowOperationContext.formatDuration(getDuration(1, 0, 10, 23, 500)), equalTo("24h10m23s"));    assertThat(DataflowOperationContext.formatDuration(getDuration(2, 0, 0, 23, 500)), equalTo("48h00m23s"));}
private Duration beam_f9767_0(int days, int hours, int minutes, int seconds, int millis)
{    return Duration.standardDays(days).plus(Duration.standardHours(hours)).plus(Duration.standardMinutes(minutes)).plus(Duration.standardSeconds(seconds)).plus(Duration.millis(millis));}
public void beam_f9776_0()
{    DataflowExecutionContext mockedExecutionContext = mock(DataflowExecutionContext.class);    DataflowOperationContext mockedOperationContext = mock(DataflowOperationContext.class);    final int siIndexId = 3;    ExecutionStateTracker mockedExecutionStateTracker = mock(ExecutionStateTracker.class);    when(mockedExecutionContext.getExecutionStateTracker()).thenReturn(mockedExecutionStateTracker);    Thread mockedThreadObject = mock(Thread.class);    when(mockedExecutionStateTracker.getTrackedThread()).thenReturn(mockedThreadObject);    DataflowExecutionState mockedExecutionState = mock(DataflowExecutionState.class);    when(mockedExecutionStateTracker.getCurrentState()).thenReturn(mockedExecutionState);    NameContext mockedNameContext = mock(NameContext.class);    when(mockedExecutionState.getStepName()).thenReturn(mockedNameContext);    when(mockedNameContext.originalName()).thenReturn("DummyName");    NameContext mockedDeclaringNameContext = mock(NameContext.class);    when(mockedOperationContext.nameContext()).thenReturn(mockedDeclaringNameContext);    when(mockedDeclaringNameContext.originalName()).thenReturn("DummyDeclaringName");    CounterFactory mockedCounterFactory = mock(CounterFactory.class);    when(mockedExecutionContext.getCounterFactory()).thenReturn(mockedCounterFactory);    Counter<Long, Long> mockedCounter = mock(Counter.class);    when(mockedCounterFactory.longSum(any())).thenReturn(mockedCounter);    DataflowExecutionStateRegistry mockedExecutionStateRegistry = mock(DataflowExecutionStateRegistry.class);    when(mockedExecutionContext.getExecutionStateRegistry()).thenReturn(mockedExecutionStateRegistry);    DataflowExecutionState mockedCounterExecutionState = mock(DataflowExecutionState.class);    when(mockedExecutionStateRegistry.getIOState(any(), any(), any(), any(), any(), any())).thenReturn(mockedCounterExecutionState);    DataflowSideInputReadCounter testObject = new DataflowSideInputReadCounter(mockedExecutionContext, mockedOperationContext, siIndexId);    testObject.enter();    verify(mockedExecutionStateTracker, never()).enterState(any());}
public void beam_f9777_0() throws Exception
{    DataflowWorkerHarnessOptions pipelineOptions = PipelineOptionsFactory.as(DataflowWorkerHarnessOptions.class);    pipelineOptions.setJobId(JOB_ID);    pipelineOptions.setWorkerId(WORKER_ID);    String serializedOptions = new ObjectMapper().writeValueAsString(pipelineOptions);    File file = tmpFolder.newFile();    Files.write(Paths.get(file.getPath()), serializedOptions.getBytes(StandardCharsets.UTF_8));    System.setProperty("sdk_pipeline_options_file", file.getPath());    DataflowWorkerHarnessOptions generatedOptions = DataflowWorkerHarnessHelper.initializeGlobalStateAndPipelineOptions(DataflowBatchWorkerHarnessTest.class);        assertThat(generatedOptions.getJobId(), equalTo(JOB_ID));    assertThat(generatedOptions.getWorkerId(), equalTo(WORKER_ID));        assertThat(DataflowWorkerLoggingMDC.getJobId(), equalTo(JOB_ID));    assertThat(DataflowWorkerLoggingMDC.getWorkerId(), equalTo(WORKER_ID));}
public void beam_f9786_0() throws Exception
{            WorkItemServiceState firstResponse = generateServiceState(ReaderTestUtils.positionAtIndex(2L), 1000);    when(workItemStatusClient.reportUpdate(isNull(DynamicSplitResult.class), isA(Duration.class))).thenReturn(firstResponse);    when(worker.getWorkerProgress()).thenReturn(cloudProgressToReaderProgress(ReaderTestUtils.approximateProgressAtIndex(1L)));    when(worker.requestDynamicSplit(toDynamicSplitRequest(firstResponse.getSplitRequest()))).thenReturn(new NativeReader.DynamicSplitResultWithPosition(cloudPositionToReaderPosition(firstResponse.getSplitRequest().getPosition())));    progressUpdater.startReportingProgress();    executor.runNextRunnable();        assertEquals(clock.currentTimeMillis(), startTime + 300);    verify(workItemStatusClient).reportUpdate(isNull(DynamicSplitResult.class), isA(Duration.class));    verify(worker).requestDynamicSplit(isA(DynamicSplitRequest.class));    clock.setTime(clock.currentTimeMillis() + 100);        progressUpdater.stopReportingProgress();    assertEquals(clock.currentTimeMillis(), startTime + 400);        verify(workItemStatusClient, atLeastOnce()).reportUpdate(splitResultCaptor.capture(), isA(Duration.class));    assertEquals("Final update is sent and contains the latest split result", splitResultCaptor.getValue(), new NativeReader.DynamicSplitResultWithPosition(cloudPositionToReaderPosition(ReaderTestUtils.positionAtIndex(2L))));        verify(workItemStatusClient, Mockito.atLeastOnce()).uniqueWorkId();    verifyNoMoreInteractions(workItemStatusClient);}
private WorkItemServiceState beam_f9787_0(@Nullable Position suggestedStopPosition, long millisToNextUpdate)
{    WorkItemServiceState responseState = new WorkItemServiceState();    responseState.setFactory(Transport.getJsonFactory());    responseState.setLeaseExpireTime(toCloudTime(new Instant(clock.currentTimeMillis() + LEASE_MS)));    responseState.setReportStatusInterval(toCloudDuration(Duration.millis(millisToNextUpdate)));    if (suggestedStopPosition != null) {        responseState.setSplitRequest(ReaderTestUtils.approximateSplitRequestAtPosition(suggestedStopPosition));    }    HotKeyDetection hotKeyDetection = new HotKeyDetection();    hotKeyDetection.setUserStepName(STEP_ID);    hotKeyDetection.setHotKeyAge(toCloudDuration(HOT_KEY_AGE));    responseState.setHotKeyDetection(hotKeyDetection);    return responseState;}
private LowLevelHttpResponse beam_f9796_0(WorkItem... workItems) throws Exception
{    MockLowLevelHttpResponse response = new MockLowLevelHttpResponse();    response.setContentType(Json.MEDIA_TYPE);    LeaseWorkItemResponse lease = new LeaseWorkItemResponse();    lease.setWorkItems(Lists.newArrayList(workItems));        lease.setFactory(Transport.getJsonFactory());    response.setContent(lease.toPrettyString());    return response;}
private WorkItem beam_f9797_0(String projectId, String jobId)
{    WorkItem workItem = new WorkItem();    workItem.setFactory(Transport.getJsonFactory());    workItem.setProjectId(projectId);    workItem.setJobId(jobId);            workItem.setId(1234L);    return workItem;}
public void beam_f9808_0() throws Exception
{    DoFnInfo<?, ?> info = DoFnInfo.forFn(initialFn, WindowingStrategy.globalDefault(), null, /* side input views */    null, /* input coder */    new TupleTag<>(PropertyNames.OUTPUT), /* main output id */    DoFnSchemaInformation.create(), Collections.emptyMap());    DoFnInstanceManager mgr = DoFnInstanceManagers.cloningPool(info);    DoFnInfo<?, ?> retrievedInfo = mgr.get();    assertThat(retrievedInfo, not(Matchers.<DoFnInfo<?, ?>>theInstance(info)));    assertThat(retrievedInfo.getDoFn(), not(theInstance(info.getDoFn())));    mgr.complete(retrievedInfo);    DoFnInfo<?, ?> afterCompleteInfo = mgr.get();    assertThat(afterCompleteInfo, Matchers.<DoFnInfo<?, ?>>theInstance(retrievedInfo));    assertThat(afterCompleteInfo.getDoFn(), theInstance(retrievedInfo.getDoFn()));}
public void beam_f9809_0() throws Exception
{    DoFnInfo<?, ?> info = DoFnInfo.forFn(initialFn, WindowingStrategy.globalDefault(), null, /* side input views */    null, /* input coder */    new TupleTag<>(PropertyNames.OUTPUT), /* main output id */    DoFnSchemaInformation.create(), Collections.emptyMap());    DoFnInstanceManager mgr = DoFnInstanceManagers.cloningPool(info);    DoFnInfo<?, ?> retrievedInfo = mgr.get();    mgr.abort(retrievedInfo);    TestFn fn = (TestFn) retrievedInfo.getDoFn();    assertThat(fn.tornDown, is(true));    DoFnInfo<?, ?> afterAbortInfo = mgr.get();    assertThat(afterAbortInfo, not(Matchers.<DoFnInfo<?, ?>>theInstance(retrievedInfo)));    assertThat(afterAbortInfo.getDoFn(), not(theInstance(retrievedInfo.getDoFn())));    assertThat(((TestFn) afterAbortInfo.getDoFn()).tornDown, is(false));}
private void beam_f9818_0(Windmill.GetDataRequest request)
{    for (ComputationGetDataRequest computationRequest : request.getRequestsList()) {        for (KeyedGetDataRequest keyRequest : computationRequest.getRequestsList()) {            errorCollector.checkThat(keyRequest.hasWorkToken(), equalTo(true));            errorCollector.checkThat(keyRequest.getShardingKey(), allOf(greaterThan(0L), lessThan(Long.MAX_VALUE)));            errorCollector.checkThat(keyRequest.getMaxBytes(), greaterThanOrEqualTo(0L));        }    }}
public Windmill.GetDataResponse beam_f9819_1(Windmill.GetDataRequest request)
{        validateGetDataRequest(request);    ++numGetDataRequests;    GetDataResponse response;    Function<GetDataRequest, GetDataResponse> responseFn = dataToOffer.poll();    if (responseFn == null) {        response = Windmill.GetDataResponse.newBuilder().build();    } else {        response = responseFn.apply(request);        try {                                    sleepMillis(500);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();        }    }        return response;}
public boolean beam_f9828_0(int time, TimeUnit unit) throws InterruptedException
{    return done.await(time, unit);}
public Instant beam_f9829_0()
{    return startTime;}
public Instant beam_f9844_0()
{    return startTime;}
public void beam_f9845_0()
{    while (!workToOffer.isEmpty()) {        Uninterruptibles.sleepUninterruptibly(1000, TimeUnit.MILLISECONDS);    }}
public void beam_f9854_0() throws Exception
{    assertThat(Filepatterns.expandAtNFilepattern("gs://bucket/object@google.ism"), contains("gs://bucket/object@google.ism"));}
public void beam_f9855_0() throws Exception
{    exception.expect(IllegalArgumentException.class);    exception.expectMessage("More than one @N wildcard found in filepattern: gs://bucket/object@10.@20.ism");    Filepatterns.expandAtNFilepattern("gs://bucket/object@10.@20.ism");}
public void beam_f9864_0() throws Exception
{    final String stepName = "fakeStepNameWithUserMetrics";    final String namespace = "sdk/whatever";    final String name = "someCounter";    final int counterValue = 42;    final CountDownLatch progressSentLatch = new CountDownLatch(1);    final CountDownLatch processBundleLatch = new CountDownLatch(1);    final Metrics.User.MetricName metricName = Metrics.User.MetricName.newBuilder().setNamespace(namespace).setName(name).build();    InstructionRequestHandler instructionRequestHandler = new InstructionRequestHandler() {        @Override        public CompletionStage<InstructionResponse> handle(InstructionRequest request) {            switch(request.getRequestCase()) {                case REGISTER:                    return CompletableFuture.completedFuture(responseFor(request).build());                case PROCESS_BUNDLE:                    return MoreFutures.supplyAsync(() -> {                        processBundleLatch.await();                        return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build();                    });                case PROCESS_BUNDLE_PROGRESS:                    progressSentLatch.countDown();                    return CompletableFuture.completedFuture(responseFor(request).setProcessBundleProgress(BeamFnApi.ProcessBundleProgressResponse.newBuilder().setMetrics(Metrics.newBuilder().putPtransforms(GRPC_READ_ID, FAKE_ELEMENT_COUNT_METRICS).putPtransforms(stepName, Metrics.PTransform.newBuilder().addUser(Metrics.User.newBuilder().setMetricName(metricName).setCounterData(Metrics.User.CounterData.newBuilder().setValue(counterValue))).build()))).build());                default:                                        return new CompletableFuture<>();            }        }        @Override        public void close() {        }    };    RegisterAndProcessBundleOperation processOperation = new RegisterAndProcessBundleOperation(IdGenerators.decrementingLongs(), instructionRequestHandler, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of(), ImmutableMap.of(), ImmutableTable.of(), ImmutableMap.of(), mockContext);    BeamFnMapTaskExecutor mapTaskExecutor = BeamFnMapTaskExecutor.forOperations(ImmutableList.of(readOperation, grpcPortWriteOperation, processOperation), executionStateTracker);            CompletionStage<Void> doneFuture = MoreFutures.runAsync(mapTaskExecutor::execute);    progressSentLatch.await();    Iterable<CounterUpdate> metricsCounterUpdates = Collections.emptyList();    while (Iterables.size(metricsCounterUpdates) == 0) {        Thread.sleep(ReadOperation.DEFAULT_PROGRESS_UPDATE_PERIOD_MS);        metricsCounterUpdates = mapTaskExecutor.extractMetricUpdates();    }    assertThat(metricsCounterUpdates, contains(new CounterHamcrestMatchers.CounterUpdateIntegerValueMatcher(counterValue)));        processBundleLatch.countDown();    MoreFutures.get(doneFuture);}
public CompletionStage<InstructionResponse> beam_f9865_0(InstructionRequest request)
{    switch(request.getRequestCase()) {        case REGISTER:            return CompletableFuture.completedFuture(responseFor(request).build());        case PROCESS_BUNDLE:            return MoreFutures.supplyAsync(() -> {                processBundleLatch.await();                return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build();            });        case PROCESS_BUNDLE_PROGRESS:            progressSentLatch.countDown();            return CompletableFuture.completedFuture(responseFor(request).setProcessBundleProgress(BeamFnApi.ProcessBundleProgressResponse.newBuilder().setMetrics(Metrics.newBuilder().putPtransforms(GRPC_READ_ID, FAKE_ELEMENT_COUNT_METRICS).putPtransforms(stepName, Metrics.PTransform.newBuilder().addUser(Metrics.User.newBuilder().setMetricName(metricName).setCounterData(Metrics.User.CounterData.newBuilder().setValue(counterValue))).build()))).build());        default:                        return new CompletableFuture<>();    }}
public String beam_f9878_0()
{    return valuesPrefix + "OriginalName";}
public String beam_f9879_0()
{    return valuesPrefix + "SystemName";}
public boolean beam_f9889_0(PCollectionView<T> view)
{    return SIDE_INPUT_NAME.equals(view.getTagInternal().getId());}
public boolean beam_f9890_0()
{    return false;}
public void beam_f9899_0()
{    Map<String, NameContext> pcollectionNameMapping = new HashMap<>();    pcollectionNameMapping.put("anyValue", NameContext.create("anyStageName", "anyOriginName", "anySystemName", "transformedValue"));    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("beam:metric:element_count:v1").putLabels(MonitoringInfoConstants.Labels.PCOLLECTION, "anyValue").build();    ElementCountMonitoringInfoToCounterUpdateTransformer testObject = new ElementCountMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, pcollectionNameMapping);    when(mockSpecValidator.validate(any())).thenReturn(Optional.empty());    CounterUpdate result = testObject.transform(monitoringInfo);    assertNotEquals(null, result);    assertEquals("{cumulative=true, integer={highBits=0, lowBits=0}, " + "nameAndKind={kind=SUM, " + "name=transformedValue-ElementCount}}", result.toString());}
public void beam_f9900_0()
{    MockitoAnnotations.initMocks(this);}
public void beam_f9909_0()
{    Map<String, String> counterNameMapping = new HashMap<>();    counterNameMapping.put("beam:counter:supported", "supportedCounter");    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    MSecMonitoringInfoToCounterUpdateTransformer testObject = new MSecMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping, counterNameMapping);    Optional<String> error = Optional.of("Error text");    when(mockSpecValidator.validate(any())).thenReturn(error);    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("beam:metric:pardo_execution_time:start_bundle_msecs:v1:invalid").build();    assertEquals(null, testObject.transform(monitoringInfo));}
public void beam_f9910_0()
{    Map<String, String> counterNameMapping = new HashMap<>();    counterNameMapping.put("beam:counter:supported", "supportedCounter");    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("beam:metric:pardo_execution_time:start_bundle_msecs:v1:invalid").build();    MSecMonitoringInfoToCounterUpdateTransformer testObject = new MSecMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping, counterNameMapping);    when(mockSpecValidator.validate(any())).thenReturn(Optional.empty());    exception.expect(RuntimeException.class);    testObject.transform(monitoringInfo);}
public String beam_f9919_0()
{    return Long.toString(longs.getAndIncrement());}
public void beam_f9920_0()
{    new RegisterAndProcessBundleOperation(IdGenerators.decrementingLongs(), new InstructionRequestHandler() {        @Override        public CompletionStage<InstructionResponse> handle(InstructionRequest request) {            CompletableFuture<InstructionResponse> responseFuture = new CompletableFuture<>();            completeFuture(request, responseFuture);            return responseFuture;        }        @Override        public void close() {        }    }, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of(), ImmutableMap.of(), ImmutableTable.of(), ImmutableMap.of(), mockContext).supportsRestart();}
public CompletionStage<InstructionResponse> beam_f9933_0(InstructionRequest request)
{    requests.add(request);    switch(request.getRequestCase()) {        case REGISTER:            return CompletableFuture.completedFuture(InstructionResponse.newBuilder().setInstructionId(request.getInstructionId()).build());        case PROCESS_BUNDLE:            CompletableFuture<InstructionResponse> responseFuture = new CompletableFuture<>();            executorService.submit(() -> {                                Thread.sleep(100);                responseFuture.complete(InstructionResponse.newBuilder().setInstructionId(request.getInstructionId()).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build());                completeFuture(request, responseFuture);                return null;            });            return responseFuture;        default:                        return new CompletableFuture<>();    }}
public void beam_f9935_0() throws Exception
{    IdGenerator idGenerator = makeIdGeneratorStartingFrom(777L);    ExecutorService executorService = Executors.newCachedThreadPool();    InMemoryStateInternals<ByteString> stateInternals = InMemoryStateInternals.forKey(ByteString.EMPTY);    DataflowStepContext mockStepContext = mock(DataflowStepContext.class);    DataflowStepContext mockUserStepContext = mock(DataflowStepContext.class);    when(mockStepContext.namespacedToUser()).thenReturn(mockUserStepContext);    when(mockUserStepContext.stateInternals()).thenReturn(stateInternals);    InstructionRequestHandler instructionRequestHandler = new InstructionRequestHandler() {        @Override        public CompletionStage<InstructionResponse> handle(InstructionRequest request) {            switch(request.getRequestCase()) {                case REGISTER:                    return CompletableFuture.completedFuture(responseFor(request).build());                case PROCESS_BUNDLE:                    return MoreFutures.supplyAsync(() -> {                        StateRequest partialRequest = StateRequest.newBuilder().setStateKey(StateKey.newBuilder().setBagUserState(StateKey.BagUserState.newBuilder().setTransformId("testPTransformId").setWindow(ByteString.EMPTY).setUserStateId("testUserStateId"))).buildPartial();                        StateRequest get = partialRequest.toBuilder().setGet(StateGetRequest.getDefaultInstance()).build();                        StateRequest clear = partialRequest.toBuilder().setClear(StateClearRequest.getDefaultInstance()).build();                        StateRequest append = partialRequest.toBuilder().setAppend(StateAppendRequest.newBuilder().setData(ByteString.copyFromUtf8("ABC"))).build();                        StateRequestHandler stateHandler = stateHandlerCaptor.getValue();                        StateResponse.Builder getWhenEmptyResponse = MoreFutures.get(stateHandler.handle(get));                        assertEquals(ByteString.EMPTY, getWhenEmptyResponse.getGet().getData());                        StateResponse.Builder appendWhenEmptyResponse = MoreFutures.get(stateHandler.handle(append));                        assertNotNull(appendWhenEmptyResponse);                        StateResponse.Builder appendWhenEmptyResponse2 = MoreFutures.get(stateHandler.handle(append));                        assertNotNull(appendWhenEmptyResponse2);                        StateResponse.Builder getWhenHasValueResponse = MoreFutures.get(stateHandler.handle(get));                        assertEquals(ByteString.copyFromUtf8("ABC").concat(ByteString.copyFromUtf8("ABC")), getWhenHasValueResponse.getGet().getData());                        StateResponse.Builder clearResponse = MoreFutures.get(stateHandler.handle(clear));                        assertNotNull(clearResponse);                        return responseFor(request).setProcessBundle(BeamFnApi.ProcessBundleResponse.getDefaultInstance()).build();                    });                default:                                        return new CompletableFuture<>();            }        }        @Override        public void close() {        }    };    RegisterAndProcessBundleOperation operation = new RegisterAndProcessBundleOperation(idGenerator, instructionRequestHandler, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of("testPTransformId", mockStepContext), ImmutableMap.of(), ImmutableTable.of(), ImmutableMap.of(), mockContext);    operation.start();    verify(mockBeamFnStateDelegator).registerForProcessBundleInstructionId(eq("778"), stateHandlerCaptor.capture());        operation.finish();        assertEquals(stateServiceRegisterCounter.get(), stateServiceDeregisterCounter.get());    assertEquals(0, stateServiceAbortCounter.get());}
public void beam_f9947_0() throws Exception
{    IdGenerator idGenerator = makeIdGeneratorStartingFrom(777L);    ExecutorService executorService = Executors.newCachedThreadPool();    CountDownLatch waitForAbortToComplete = new CountDownLatch(1);    AtomicReference<ThrowingRunnable> abortReference = new AtomicReference<>();    RegisterAndProcessBundleOperation operation = new RegisterAndProcessBundleOperation(idGenerator, new InstructionRequestHandler() {        @Override        public CompletionStage<InstructionResponse> handle(InstructionRequest request) {            CompletableFuture<InstructionResponse> responseFuture = new CompletableFuture<>();            if (request.getRequestCase() == RequestCase.PROCESS_BUNDLE) {                executorService.submit((Callable<Void>) () -> {                    abortReference.get().run();                    waitForAbortToComplete.countDown();                    return null;                });            } else {                completeFuture(request, responseFuture);            }            return responseFuture;        }        @Override        public void close() {        }    }, mockBeamFnStateDelegator, REGISTER_REQUEST, ImmutableMap.of(), ImmutableMap.of(), ImmutableMap.of(), ImmutableTable.of(), ImmutableMap.of(), mockContext);    abortReference.set(operation::abort);    operation.start();    waitForAbortToComplete.await();        assertEquals(stateServiceRegisterCounter.get(), stateServiceAbortCounter.get());    assertEquals(0, stateServiceDeregisterCounter.get());}
public CompletionStage<InstructionResponse> beam_f9948_0(InstructionRequest request)
{    CompletableFuture<InstructionResponse> responseFuture = new CompletableFuture<>();    if (request.getRequestCase() == RequestCase.PROCESS_BUNDLE) {        executorService.submit((Callable<Void>) () -> {            abortReference.get().run();            waitForAbortToComplete.countDown();            return null;        });    } else {        completeFuture(request, responseFuture);    }    return responseFuture;}
public void beam_f9958_0()
{    Interpolator<Double> interpolator = new Interpolator<Double>(10) {        @Override        protected Double interpolate(Double prev, Double next, double fraction) {                        return prev * (1 - fraction) + next * fraction;        }    };        for (int i = 0; i < 100; i++) {        interpolator.addPoint(i, Math.sqrt(i));        assertEquals(Math.sqrt(i), interpolator.interpolate(i), 0);        if (i > 5) {            assertEquals(Math.sqrt(i - 1), interpolator.interpolate(i - 1), 0);            assertEquals(Math.sqrt(i - 5), interpolator.interpolate(i - 5), 0);        }        if (i % 3 == 0) {            interpolator.purgeUpTo(i - 5);        }    }        for (int i = 100; i < 200; i++) {        interpolator.addPoint(i, Math.sqrt(i));        assertEquals(Math.sqrt(i), interpolator.interpolate(i), 0);        if (i > 110) {            assertNotEquals(10.0, interpolator.interpolate(100), 0);            assertEquals(10.0, interpolator.interpolate(100), .02);        }    }}
protected Double beam_f9959_0(Double prev, Double next, double fraction)
{        return prev * (1 - fraction) + next * fraction;}
public void beam_f9970_0(@TimerId(timerId2) Timer timer, TimeDomain timeDomain, OutputReceiver<Integer> r)
{    r.output(2);}
public RemoteBundle beam_f9971_0(OutputReceiverFactory outputReceiverFactory, StateRequestHandler stateRequestHandler, BundleProgressHandler progressHandler) throws Exception
{    ImmutableMap.Builder<String, RemoteOutputReceiver<?>> outputReceivers = ImmutableMap.builder();    for (Map.Entry<String, Coder> remoteOutputCoder : processBundleDescriptor.getRemoteOutputCoders().entrySet()) {        String bundleOutputPCollection = Iterables.getOnlyElement(processBundleDescriptor.getProcessBundleDescriptor().getTransformsOrThrow(remoteOutputCoder.getKey()).getInputsMap().values());        FnDataReceiver<WindowedValue<?>> outputReceiver = outputReceiverFactory.create(bundleOutputPCollection);        outputReceivers.put(remoteOutputCoder.getKey(), RemoteOutputReceiver.of(remoteOutputCoder.getValue(), outputReceiver));    }    return processor.newBundle(outputReceivers.build(), stateRequestHandler, progressHandler);}
private static StageBundleFactory beam_f9981_0(SdkHarnessClient client, ProcessBundleDescriptors.ExecutableProcessBundleDescriptor processBundleDescriptor, GrpcStateService stateDelegator)
{    return new SimpleStageBundleFactory(client, client.getProcessor(processBundleDescriptor.getProcessBundleDescriptor(), processBundleDescriptor.getRemoteInputDestinations(), stateDelegator), processBundleDescriptor);}
private static JobInfo beam_f9982_0()
{    return JobInfo.create("job_id", "job_name", "", Struct.newBuilder().build());}
public void beam_f9991_0()
{    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("beam:metric:user").putLabels(MonitoringInfoConstants.Labels.NAME, "anyName").putLabels(MonitoringInfoConstants.Labels.NAMESPACE, "anyNamespace").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "anyValue").build();    UserMonitoringInfoToCounterUpdateTransformer testObject = new UserMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping);    when(mockSpecValidator.validate(any())).thenReturn(Optional.empty());    assertEquals(null, testObject.transform(monitoringInfo));}
public void beam_f9992_0()
{    Map<String, DataflowStepContext> stepContextMapping = new HashMap<>();    NameContext nc = NameContext.create("anyStageName", "anyOriginalName", "anySystemName", "anyUserName");    DataflowStepContext dsc = mock(DataflowStepContext.class);    when(dsc.getNameContext()).thenReturn(nc);    stepContextMapping.put("anyValue", dsc);    MonitoringInfo monitoringInfo = MonitoringInfo.newBuilder().setUrn("beam:metric:user").putLabels(MonitoringInfoConstants.Labels.NAME, "anyName").putLabels(MonitoringInfoConstants.Labels.NAMESPACE, "anyNamespace").putLabels(MonitoringInfoConstants.Labels.PTRANSFORM, "anyValue").build();    UserMonitoringInfoToCounterUpdateTransformer testObject = new UserMonitoringInfoToCounterUpdateTransformer(mockSpecValidator, stepContextMapping);    when(mockSpecValidator.validate(any())).thenReturn(Optional.empty());    CounterUpdate result = testObject.transform(monitoringInfo);    assertNotEquals(null, result);    assertEquals("{cumulative=true, integer={highBits=0, lowBits=0}, " + "structuredNameAndMetadata={metadata={kind=SUM}, " + "name={name=anyName, origin=USER, originNamespace=anyNamespace, " + "originalStepName=anyOriginalName}}}", result.toString());}
private Server beam_f10001_0(BindableService service, Endpoints.ApiServiceDescriptor descriptor) throws Exception
{    String serverName = descriptor.getUrl();    Server server = InProcessServerBuilder.forName(serverName).addService(ServerInterceptors.intercept(service, GrpcContextHeaderAccessorProvider.interceptor())).build();    server.start();    return server;}
public void beam_f10002_0()
{    MockitoAnnotations.initMocks(this);    testReceiver = new TestOutputReceiver(CODER, NameContextsForTests.nameContextForTest());    operation = new RemoteGrpcPortReadOperation<>(beamFnDataService, TRANSFORM_ID, bundleIdSupplier, CODER, new OutputReceiver[] { testReceiver }, operationContext);}
public void beam_f10011_0() throws Exception
{    closed = true;}
public synchronized void beam_f10013_0(T t) throws Exception
{    if (closed) {        throw new IllegalStateException("Consumer is closed but attempting to consume " + t);    }    add(t);}
public void beam_f10023_0()
{    MockitoAnnotations.initMocks(this);}
public void beam_f10024_0()
{    StreamObserver<String> observer = ServerStreamObserverFactory.fromOptions(PipelineOptionsFactory.create()).from(mockResponseObserver);    assertThat(observer, instanceOf(DirectStreamObserver.class));}
public void beam_f10036_0() throws Exception
{    assertEquals(createEmptyNetwork(), new CloneAmbiguousFlattensFunction().apply(createEmptyNetwork()));}
public void beam_f10037_0() throws Exception
{                MutableNetwork<Node, Edge> network = createEmptyNetwork();    Node sdkPredecessor = createSdkNode("sdk_predecessor");    Node runnerPredecessor = createRunnerNode("runner_predecessor");    Node sdkPredecessorOutput = createPCollection("sdk_predecessor.out");    Node runnerPredecessorOutput = createPCollection("runner_predecessor.out");    Node ambiguousFlatten = createFlatten("ambiguous_flatten", ExecutionLocation.AMBIGUOUS);    Node ambiguousFlattenOutput = createPCollection("ambiguous_flatten.out");    Node sdkSuccessor = createSdkNode("sdk_successor");    Node runnerSuccessor = createRunnerNode("runner_successor");    Node noLocationSuccessor = createNoLocationNode();    Node sdkSuccessorOutput = createPCollection("sdk_successor.out");    Node runnerSuccessorOutput = createPCollection("runner_successor.out");    Node noLocationSuccessorOutput = createPCollection("no_location_successor.out");    network.addNode(sdkPredecessor);    network.addNode(runnerPredecessor);    network.addNode(sdkPredecessorOutput);    network.addNode(runnerPredecessorOutput);    network.addNode(ambiguousFlatten);    network.addNode(ambiguousFlattenOutput);    network.addNode(sdkSuccessor);    network.addNode(runnerSuccessor);    network.addNode(noLocationSuccessor);    network.addNode(sdkSuccessorOutput);    network.addNode(runnerSuccessorOutput);    network.addNode(noLocationSuccessorOutput);    network.addEdge(sdkPredecessor, sdkPredecessorOutput, DefaultEdge.create());    network.addEdge(runnerPredecessor, runnerPredecessorOutput, DefaultEdge.create());    network.addEdge(sdkPredecessorOutput, ambiguousFlatten, DefaultEdge.create());    network.addEdge(runnerPredecessorOutput, ambiguousFlatten, DefaultEdge.create());    network.addEdge(ambiguousFlatten, ambiguousFlattenOutput, DefaultEdge.create());    network.addEdge(ambiguousFlattenOutput, sdkSuccessor, DefaultEdge.create());    network.addEdge(ambiguousFlattenOutput, runnerSuccessor, DefaultEdge.create());    network.addEdge(ambiguousFlattenOutput, noLocationSuccessor, DefaultEdge.create());    network.addEdge(sdkSuccessor, sdkSuccessorOutput, DefaultEdge.create());    network.addEdge(runnerSuccessor, runnerSuccessorOutput, DefaultEdge.create());    network.addEdge(noLocationSuccessor, noLocationSuccessorOutput, DefaultEdge.create());                        List<List<Node>> originalPaths = Networks.allPathsFromRootsToLeaves(network);    network = new CloneAmbiguousFlattensFunction().apply(network);        ParallelInstructionNode sdkFlatten = null;    ParallelInstructionNode runnerFlatten = null;    for (Node node : network.nodes()) {        if (node instanceof ParallelInstructionNode && ((ParallelInstructionNode) node).getParallelInstruction().getFlatten() != null) {            ParallelInstructionNode castNode = ((ParallelInstructionNode) node);            if (castNode.getExecutionLocation() == ExecutionLocation.SDK_HARNESS) {                sdkFlatten = castNode;            } else if (castNode.getExecutionLocation() == ExecutionLocation.RUNNER_HARNESS) {                runnerFlatten = castNode;            } else {                assertTrue("Ambiguous flatten not removed from network.", castNode.getExecutionLocation() != ExecutionLocation.AMBIGUOUS);            }        }    }    assertNotNull("Ambiguous flatten was not cloned into sdk flatten.", sdkFlatten);    assertNotNull("Ambiguous flatten was not cloned into runner flatten.", runnerFlatten);    Node sdkFlattenOutput = Iterables.getOnlyElement(network.successors(sdkFlatten));    Node runnerFlattenOutput = Iterables.getOnlyElement(network.successors(runnerFlatten));    assertEquals(2, network.predecessors(sdkFlatten).size());    assertEquals(2, network.predecessors(runnerFlatten).size());    assertEquals(1, network.successors(sdkFlattenOutput).size());    assertEquals(2, network.successors(runnerFlattenOutput).size());    assertSame(sdkSuccessor, Iterables.getOnlyElement(network.successors(sdkFlattenOutput)));    assertThat(network.successors(runnerFlattenOutput), hasItems(runnerSuccessor, noLocationSuccessor));    assertEquals(originalPaths.size(), Networks.allPathsFromRootsToLeaves(network).size());}
public void beam_f10046_0()
{    MockitoAnnotations.initMocks(this);    createRegisterFnOperation = new CreateRegisterFnOperationFunction(IdGenerators.decrementingLongs(), portSupplier, registerFnOperationFunction, false);}
public void beam_f10047_0()
{    MutableNetwork<Node, Edge> appliedNetwork = createRegisterFnOperation.apply(createEmptyNetwork());    MutableNetwork<Node, Edge> expectedNetwork = createEmptyNetwork();    assertNetworkMaintainsBipartiteStructure(appliedNetwork);    assertEquals(String.format("Expected network %s but got network %s", expectedNetwork, appliedNetwork), expectedNetwork, appliedNetwork);}
private static ParallelInstructionNode beam_f10056_0(String name, Nodes.ExecutionLocation location)
{    return ParallelInstructionNode.create(new ParallelInstruction().setName(name).setParDo(new ParDoInstruction()), location);}
private static InstructionOutputNode beam_f10057_0(String name)
{    return InstructionOutputNode.create(new InstructionOutput().setName(name), "fakeId");}
public void beam_f10066_0() throws Exception
{        assertSingleFlattenLocationDeduction(ExecutionLocation.RUNNER_HARNESS, ExecutionLocation.RUNNER_HARNESS, ExecutionLocation.RUNNER_HARNESS);}
public void beam_f10067_0() throws Exception
{            assertSingleFlattenLocationDeduction(ExecutionLocation.RUNNER_HARNESS, ExecutionLocation.AMBIGUOUS, ExecutionLocation.RUNNER_HARNESS);}
public void beam_f10076_0() throws Exception
{            assertSingleFlattenLocationDeduction(ExecutionLocation.UNKNOWN, ExecutionLocation.UNKNOWN, ExecutionLocation.RUNNER_HARNESS);}
public void beam_f10077_0() throws Exception
{                        MutableNetwork<Node, Edge> network = createEmptyNetwork();    Node sdkNode1 = createSdkNode("sdk_node1");    Node sdkNode1Output = createPCollection("sdk_node1.out");    Node sdkNode2 = createSdkNode("sdk_node2");    Node sdkNode2Output = createPCollection("sdk_node2.out");    Node sdkNode3 = createSdkNode("sdk_node3");    Node sdkNode3Output = createPCollection("sdk_node3.out");    Node runnerNode1 = createRunnerNode("runner_node1");    Node runnerNode1Output = createPCollection("runner_node1.out");    Node runnerNode2 = createRunnerNode("runner_node2");    Node runnerNode2Output = createPCollection("runner_node2.out");    Node runnerNode3 = createRunnerNode("runner_node3");    Node runnerNode3Output = createPCollection("runner_node3.out");    Node flatten1 = createFlatten("flatten1");    Node flatten1Output = createPCollection("flatten1.out");    Node flatten2 = createFlatten("flatten2");    Node flatten2Output = createPCollection("flatten2.out");    Node flatten3 = createFlatten("flatten3");    Node flatten3Output = createPCollection("flatten3.out");    network.addNode(sdkNode1);    network.addNode(sdkNode2);    network.addNode(sdkNode3);    network.addNode(runnerNode1);    network.addNode(runnerNode2);    network.addNode(runnerNode3);    network.addNode(flatten1);    network.addNode(flatten1Output);    network.addNode(flatten2);    network.addNode(flatten2Output);    network.addNode(flatten3);    network.addNode(flatten3Output);    network.addEdge(sdkNode1, sdkNode1Output, DefaultEdge.create());    network.addEdge(sdkNode2, sdkNode2Output, DefaultEdge.create());    network.addEdge(runnerNode1, runnerNode1Output, DefaultEdge.create());    network.addEdge(runnerNode2, runnerNode2Output, DefaultEdge.create());    network.addEdge(sdkNode1Output, flatten1, DefaultEdge.create());    network.addEdge(sdkNode2Output, flatten1, DefaultEdge.create());    network.addEdge(runnerNode1Output, flatten2, DefaultEdge.create());    network.addEdge(runnerNode2Output, flatten2, DefaultEdge.create());    network.addEdge(flatten1, flatten1Output, DefaultEdge.create());    network.addEdge(flatten2, flatten2Output, DefaultEdge.create());    network.addEdge(flatten1Output, flatten3, DefaultEdge.create());    network.addEdge(flatten2Output, flatten3, DefaultEdge.create());    network.addEdge(flatten3, flatten3Output, DefaultEdge.create());    network.addEdge(flatten3Output, sdkNode3, DefaultEdge.create());    network.addEdge(flatten3Output, runnerNode3, DefaultEdge.create());    network.addEdge(sdkNode3, sdkNode3Output, DefaultEdge.create());    network.addEdge(runnerNode3, runnerNode3Output, DefaultEdge.create());    network = new DeduceFlattenLocationsFunction().apply(network);    ExecutionLocation flatten1Location = getExecutionLocationOf("flatten1", network);    assertEquals(flatten1Location, ExecutionLocation.SDK_HARNESS);    ExecutionLocation flatten2Location = getExecutionLocationOf("flatten2", network);    assertEquals(flatten2Location, ExecutionLocation.RUNNER_HARNESS);    ExecutionLocation flatten3Location = getExecutionLocationOf("flatten3", network);    assertEquals(flatten3Location, ExecutionLocation.AMBIGUOUS);}
protected boolean beam_f10086_0(Node a, Node b)
{    if (a instanceof ParallelInstructionNode && b instanceof ParallelInstructionNode) {        ParallelInstruction contentsA = ((ParallelInstructionNode) a).getParallelInstruction();        ParallelInstruction contentsB = ((ParallelInstructionNode) b).getParallelInstruction();        return contentsA.equals(contentsB);    } else {                return a.equals(b);    }}
protected int beam_f10087_0(Node n)
{    return n.hashCode();}
private void beam_f10096_0(Node node, ExecutionLocation expectedLocation)
{    assertThat(node, instanceOf(ParallelInstructionNode.class));    ExecutionLocation location = ((ParallelInstructionNode) node).getExecutionLocation();    assertEquals(location, expectedLocation);}
private void beam_f10097_0(Node expected, Node actual)
{    assertThat(expected, instanceOf(ParallelInstructionNode.class));    assertThat(actual, instanceOf(ParallelInstructionNode.class));    ParallelInstruction expectedContents = ((ParallelInstructionNode) expected).getParallelInstruction();    ParallelInstruction actualContents = ((ParallelInstructionNode) actual).getParallelInstruction();    assertEquals(expectedContents, actualContents);}
public void beam_f10106_0()
{    HappensBeforeEdge edge = HappensBeforeEdge.create();    assertNotEquals(edge, HappensBeforeEdge.create());    assertNotEquals(edge, edge.clone());}
public void beam_f10107_0() throws Exception
{    Node unknown = createParDoNode("parDoId");    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(unknown);    Network<Node, Edge> inputNetwork = ImmutableNetwork.copyOf(network);    network = InsertFetchAndFilterStreamingSideInputNodes.with(null).forNetwork(network);    assertThatNetworksAreIdentical(inputNetwork, network);}
private static ParallelInstructionNode beam_f10117_0(String parDoId)
{    CloudObject userFn = CloudObject.forClassName("DoFn");    userFn.put(PropertyNames.SERIALIZED_FN, parDoId);    return ParallelInstructionNode.create(new ParallelInstruction().setParDo(new ParDoInstruction().setUserFn(userFn)), ExecutionLocation.SDK_HARNESS);}
public void beam_f10118_0()
{    MockitoAnnotations.initMocks(this);    instruction = new ParallelInstruction();    instruction.setFactory(new JacksonFactory());    instructionOutputNode = createInstructionOutputNode("parDo.out", windowedValueCoder);}
public void beam_f10127_0()
{    Node readNode = createReadNode("Read", "Source", windowedValueCoder);    Edge readNodeEdge = DefaultEdge.create();    Node readNodeOut = createInstructionOutputNode("Read.out", windowedValueCoder);    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(readNode);    network.addNode(readNodeOut);    network.addEdge(readNode, readNodeOut, readNodeEdge);    ParallelInstructionNode prefixedReadNode = createReadNode("Read", "Source", prefixedAndReplacedWindowedValueCoder);    InstructionOutputNode prefixedReadNodeOut = createInstructionOutputNode("Read.out", prefixedAndReplacedWindowedValueCoder);    MutableNetwork<Node, Edge> prefixedNetwork = andReplaceForRunnerNetwork(network);    Set prefixedInstructions = new HashSet<>();    for (Node node : prefixedNetwork.nodes()) {        if (node instanceof ParallelInstructionNode) {            prefixedInstructions.add(((ParallelInstructionNode) node).getParallelInstruction());        } else if (node instanceof InstructionOutputNode) {            prefixedInstructions.add(((InstructionOutputNode) node).getInstructionOutput());        }    }    Set expectedInstructions = ImmutableSet.of(prefixedReadNode.getParallelInstruction(), prefixedReadNodeOut.getInstructionOutput());    assertEquals(expectedInstructions, prefixedInstructions);}
public void beam_f10128_0()
{    MutableNetwork<Node, Edge> network = createEmptyNetwork();    network.addNode(instructionOutputNode);    network.addNode(grpcPortNode);    network.addEdge(grpcPortNode, instructionOutputNode, DefaultEdge.create());    assertEquals(CloudObjects.asCloudObject(prefixedWindowedValueCoder, /*sdkComponents=*/    null), ((InstructionOutputNode) forInstructionOutputNode(network).apply(instructionOutputNode)).getInstructionOutput().getCodec());}
public void beam_f10137_0()
{    InstructionOutput readOutput = createInstructionOutput("Read.out");    ParallelInstruction read = createParallelInstruction("Read", readOutput);    read.setRead(new ReadInstruction());    MapTask mapTask = new MapTask();    mapTask.setInstructions(ImmutableList.of(read));    mapTask.setFactory(Transport.getJsonFactory());    Network<Node, Edge> network = new MapTaskToNetworkFunction(IdGenerators.decrementingLongs()).apply(mapTask);    assertNetworkProperties(network);    assertEquals(2, network.nodes().size());    assertEquals(1, network.edges().size());    ParallelInstructionNode readNode = get(network, read);    InstructionOutputNode readOutputNode = getOnlySuccessor(network, readNode);    assertEquals(readOutput, readOutputNode.getInstructionOutput());}
public void beam_f10138_0()
{    InstructionOutput readOutput = createInstructionOutput("Read.out");    ParallelInstruction read = createParallelInstruction("Read", readOutput);    read.setRead(new ReadInstruction());    MultiOutputInfo parDoMultiOutput = createMultiOutputInfo("output");    ParDoInstruction parDoInstruction = new ParDoInstruction();        parDoInstruction.setInput(createInstructionInput(0, 0));    parDoInstruction.setMultiOutputInfos(ImmutableList.of(parDoMultiOutput));    InstructionOutput parDoOutput = createInstructionOutput("ParDo.out");    ParallelInstruction parDo = createParallelInstruction("ParDo", parDoOutput);    parDo.setParDo(parDoInstruction);    MapTask mapTask = new MapTask();    mapTask.setInstructions(ImmutableList.of(read, parDo));    mapTask.setFactory(Transport.getJsonFactory());    Network<Node, Edge> network = new MapTaskToNetworkFunction(IdGenerators.decrementingLongs()).apply(mapTask);    assertNetworkProperties(network);    assertEquals(4, network.nodes().size());    assertEquals(3, network.edges().size());    ParallelInstructionNode readNode = get(network, read);    InstructionOutputNode readOutputNode = getOnlySuccessor(network, readNode);    assertEquals(readOutput, readOutputNode.getInstructionOutput());    ParallelInstructionNode parDoNode = getOnlySuccessor(network, readOutputNode);    InstructionOutputNode parDoOutputNode = getOnlySuccessor(network, parDoNode);    assertEquals(parDoOutput, parDoOutputNode.getInstructionOutput());    assertEquals(parDoMultiOutput, ((MultiOutputInfoEdge) Iterables.getOnlyElement(network.edgesConnecting(parDoNode, parDoOutputNode))).getMultiOutputInfo());}
private static InstructionOutputNode beam_f10147_0(Network<Node, Edge> network, ParallelInstructionNode node)
{    return (InstructionOutputNode) Iterables.getOnlyElement(network.successors(node));}
private static InstructionOutputNode beam_f10148_0(Network<Node, Edge> network, ParallelInstructionNode node)
{    return (InstructionOutputNode) Iterables.getOnlyElement(network.predecessors(node));}
public void beam_f10157_0()
{    assertEquals(ImmutableSet.of("A", "D", "I", "M", "O"), Networks.reachableNodes(createNetwork(), ImmutableSet.of("A", "D", "I", "M", "O"), ImmutableSet.of("A", "D", "I", "M", "O")));}
public void beam_f10158_0()
{        assertEquals(ImmutableSet.of("I", "J", "E", "G", "H", "K", "L"), Networks.reachableNodes(createNetwork(), ImmutableSet.of("I"), ImmutableSet.of("J")));}
public void beam_f10167_0()
{    InstructionOutput param = new InstructionOutput();    assertSame(param, InstructionOutputNode.create(param, PCOLLECTION_ID).getInstructionOutput());    assertNotEquals(InstructionOutputNode.create(param, PCOLLECTION_ID), InstructionOutputNode.create(param, PCOLLECTION_ID));}
public void beam_f10168_0()
{    OutputReceiver receiver = new OutputReceiver();    Coder<?> coder = StringUtf8Coder.of();    assertSame(receiver, OutputReceiverNode.create(receiver, coder, PCOLLECTION_ID).getOutputReceiver());    assertSame(coder, OutputReceiverNode.create(receiver, coder, PCOLLECTION_ID).getCoder());    assertNotEquals(OutputReceiverNode.create(receiver, coder, PCOLLECTION_ID), OutputReceiverNode.create(receiver, coder, PCOLLECTION_ID));}
private void beam_f10177_0(MutableNetwork<Node, Edge> network)
{    Network<Node, Edge> originalNetwork = ImmutableNetwork.copyOf(network);    network = new RemoveFlattenInstructionsFunction().apply(network);        for (Node node : network.nodes()) {        assertFalse(isFlatten(node));    }        List<List<Node>> originalNetworkPathsWithoutFlatten = Networks.allPathsFromRootsToLeaves(originalNetwork);    for (List<Node> path : originalNetworkPathsWithoutFlatten) {        Iterator<Node> nodeIterator = path.iterator();        while (nodeIterator.hasNext()) {            Node node = nodeIterator.next();                        if (isFlatten(node)) {                nodeIterator.remove();                nodeIterator.next();                nodeIterator.remove();            }        }    }        assertThat(originalNetworkPathsWithoutFlatten, containsInAnyOrder(Networks.allPathsFromRootsToLeaves(network).toArray()));}
private boolean beam_f10178_0(Node node)
{    return node instanceof ParallelInstructionNode && ((ParallelInstructionNode) node).getParallelInstruction().getFlatten() != null;}
private void beam_f10187_0(String mockOriginalName)
{    DataflowExecutionState state = new TestDataflowExecutionState(NameContext.create(MOCK_STAGE_NAME, mockOriginalName, MOCK_SYSTEM_NAME, MOCK_USER_NAME), "activity");    tracker.enterState(state);}
public void beam_f10188_0()
{    trackerCleanup = tracker.activate();    setCurrentExecutionState(MOCK_ORIGINAL_NAME_FOR_EXECUTING_STEP1);}
public void beam_f10197_0() throws Exception
{    runTestReadFromShuffle(KVS, false, /* do not sort values */    ValuesToRead.READ_ONE_VALUE);    runTestReadFromShuffle(KVS, true, /* sort values */    ValuesToRead.READ_ONE_VALUE);}
public void beam_f10198_0() throws Exception
{    runTestReadFromShuffle(KVS, false, /* do not sort values */    ValuesToRead.READ_NO_VALUES);    runTestReadFromShuffle(KVS, true, /* sort values */    ValuesToRead.READ_NO_VALUES);}
public void beam_f10207_0() throws Exception
{    runTestBytesReadCounter(KVS, true, /* sort values */    ValuesToRead.READ_ALL_VALUES_TWICE, 200L);}
public void beam_f10208_0() throws Exception
{    runTestBytesReadCounter(KVS, false, /* do not sort values */    ValuesToRead.READ_ONE_VALUE, 200L);}
public void beam_f10217_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    BatchModeExecutionContext context = BatchModeExecutionContext.forTesting(options, "testStage");    final int kFirstShard = 0;    TestShuffleReader shuffleReader = new TestShuffleReader();    final int kNumRecords = 5;    for (int i = 0; i < kNumRecords; ++i) {        byte[] key = CoderUtils.encodeToByteArray(BigEndianIntegerCoder.of(), i);        ShuffleEntry entry = new ShuffleEntry(fabricatePosition(kFirstShard, i), key, EMPTY_BYTE_ARRAY, key);        shuffleReader.addEntry(entry);    }    TestOperationContext operationContext = TestOperationContext.create();    GroupingShuffleReader<Integer, Integer> groupingShuffleReader = new GroupingShuffleReader<>(options, null, null, null, WindowedValue.getFullCoder(KvCoder.of(BigEndianIntegerCoder.of(), IterableCoder.of(BigEndianIntegerCoder.of())), IntervalWindow.getCoder()), context, operationContext, ShuffleReadCounterFactory.INSTANCE, false);    assertFalse(shuffleReader.isClosed());    try (GroupingShuffleReaderIterator<Integer, Integer> iter = groupingShuffleReader.iterator(shuffleReader)) {                assertEquals(0.0, consumedParallelismFromProgress(iter.getProgress()), 0);                                assertTrue(iter.start());                assertEquals(0.0, readerProgressToCloudProgress(iter.getProgress()).getConsumedParallelism().getValue(), 0);        assertNotNull(iter.requestDynamicSplit(splitRequestAtPosition(makeShufflePosition(fabricatePosition(kFirstShard, 2).immediateSuccessor().getPosition()))));                assertEquals(0.0, consumedParallelismFromProgress(iter.getProgress()), 0);                assertTrue(iter.advance());        assertEquals(1.0, consumedParallelismFromProgress(iter.getProgress()), 0);                        assertTrue(iter.advance());        assertEquals(2.0, consumedParallelismFromProgress(iter.getProgress()), 0);                assertFalse(iter.advance());        assertEquals(3.0, consumedParallelismFromProgress(iter.getProgress()), 0);    }    assertTrue(shuffleReader.isClosed());}
private Position beam_f10218_0(int shard, byte[] key) throws Exception
{    return new Position().setShufflePosition(encodeBase64URLSafeString(fabricatePosition(shard, key).getPosition()));}
 static Source beam_f10227_0(List<T> elements, Long start, Long end, Coder<T> coder) throws Exception
{    List<String> encodedElements = InMemoryReaderTest.encodedElements(elements, coder);    CloudObject spec = CloudObject.forClassName("InMemorySource");    addStringList(spec, WorkerPropertyNames.ELEMENTS, encodedElements);    if (start != null) {        addLong(spec, WorkerPropertyNames.START_INDEX, start);    }    if (end != null) {        addLong(spec, WorkerPropertyNames.END_INDEX, end);    }    Source cloudSource = new Source();    cloudSource.setSpec(spec);    cloudSource.setCodec(CloudObjects.asCloudObject(coder, /*sdkComponents=*/    null));    return cloudSource;}
 void beam_f10228_0(List<T> elements, Long start, Long end, int expectedStart, int expectedEnd, Coder<T> coder) throws Exception
{    Source cloudSource = createInMemoryCloudSource(elements, start, end, coder);    NativeReader<?> reader = ReaderRegistry.defaultRegistry().create(cloudSource, PipelineOptionsFactory.create(), BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "testStage"), TestOperationContext.create());    Assert.assertThat(reader, new IsInstanceOf(InMemoryReader.class));    InMemoryReader<?> inMemoryReader = (InMemoryReader<?>) reader;    Assert.assertEquals(InMemoryReaderTest.encodedElements(elements, coder), inMemoryReader.encodedElements);    Assert.assertEquals(expectedStart, inMemoryReader.startIndex);    Assert.assertEquals(expectedEnd, inMemoryReader.endIndex);    Assert.assertEquals(coder, inMemoryReader.coder);}
public void beam_f10237_0() throws Exception
{    runTestReadInMemory(Arrays.asList(33, 44, 55, 66, 77, 88), null, 30, Arrays.asList(33, 44, 55, 66, 77, 88), Arrays.asList(4, 4, 4, 4, 4, 4), BigEndianIntegerCoder.of());}
public void beam_f10238_0() throws Exception
{    runTestReadInMemory(Arrays.asList(33, 44, 55, 66, 77, 88), 20, null, Arrays.<Integer>asList(), Arrays.<Integer>asList(), BigEndianIntegerCoder.of());}
private static void beam_f10247_0(CounterUpdateExtractor<?> updateExtractor, String... outputNames)
{    for (String outputName : outputNames) {        verify(updateExtractor).longSum(eq(named(getElementCounterName(outputName))), anyBoolean(), anyLong());        verify(updateExtractor).longSum(eq(named(getObjectCounterName(outputName))), anyBoolean(), anyLong());        verify(updateExtractor).longMean(eq(named(getMeanByteCounterName(outputName))), anyBoolean(), Mockito.<CounterMean<Long>>any());    }}
public void beam_f10248_0() throws Exception
{    List<ParallelInstruction> instructions = Arrays.asList(createReadInstruction("Read", ReaderFactoryTest.SingletonTestReaderFactory.class), createParDoInstruction(0, 0, "DoFn1", "DoFnUserName"), createParDoInstruction(1, 0, "DoFnWithContext", "DoFnWithContextUserName"));    MapTask mapTask = new MapTask();    mapTask.setStageName(STAGE);    mapTask.setInstructions(instructions);    mapTask.setFactory(Transport.getJsonFactory());    BatchModeExecutionContext context = BatchModeExecutionContext.forTesting(options, counterSet, "testStage");    try (DataflowMapTaskExecutor executor = mapTaskExecutorFactory.create(null, /* beamFnControlClientHandler */    null, /* beamFnDataService */    null, /* beamFnStateService */    null, mapTaskToNetwork.apply(mapTask), options, STAGE, readerRegistry, sinkRegistry, context, counterSet, idGenerator)) {        executor.execute();    }    List<String> stepNames = new ArrayList<>();    for (BatchModeExecutionContext.StepContext stepContext : context.getAllStepContexts()) {        stepNames.add(stepContext.getNameContext().systemName());    }    assertThat(stepNames, hasItems("DoFn1", "DoFnWithContext"));}
public void beam_f10258_0() throws IOException
{    close();}
public TestSink beam_f10259_0(CloudObject o, @Nullable Coder<?> coder, @Nullable PipelineOptions options, @Nullable DataflowExecutionContext executionContext, DataflowOperationContext operationContext)
{    return new TestSink();}
public void beam_f10268_0() throws Exception
{    aborted = true;    super.abort();}
public NativeReader.Progress beam_f10269_0()
{    return cloudProgressToReaderProgress(progress);}
private NameContext beam_f10283_0(String originalName)
{    return NameContext.create("test-stage", originalName, "system-" + originalName, "step-" + originalName);}
public void beam_f10284_0() throws Exception
{    ExecutionStateTracker stateTracker = new DataflowExecutionStateTracker(ExecutionStateSampler.newForTest(), new TestDataflowExecutionState(NameContext.forStage("testStage"), "other", null, /* requestingStepName */    null, /* sideInputIndex */    null, /* metricsContainer */    NoopProfileScope.NOOP), new CounterSet(), PipelineOptionsFactory.create(), "test-work-item-id");    final String o1 = "o1";    TestOperationContext context1 = createContext(o1, stateTracker);    final String o2 = "o2";    TestOperationContext context2 = createContext(o2, stateTracker);    final String o3 = "o3";    TestOperationContext context3 = createContext(o3, stateTracker);    List<Operation> operations = Arrays.asList(new Operation(new OutputReceiver[] {}, context1) {        @Override        public void start() throws Exception {            super.start();            try (Closeable scope = context.enterStart()) {                Metrics.counter("TestMetric", "MetricCounter").inc(1L);            }        }    }, new Operation(new OutputReceiver[] {}, context2) {        @Override        public void start() throws Exception {            super.start();            try (Closeable scope = context.enterStart()) {                Metrics.counter("TestMetric", "MetricCounter").inc(2L);            }        }    }, new Operation(new OutputReceiver[] {}, context3) {        @Override        public void start() throws Exception {            super.start();            try (Closeable scope = context.enterStart()) {                Metrics.counter("TestMetric", "MetricCounter").inc(3L);            }        }    });    try (IntrinsicMapTaskExecutor executor = IntrinsicMapTaskExecutor.withSharedCounterSet(operations, counterSet, stateTracker)) {                executor.execute();        assertThat(context1.metricsContainer().getUpdates().counterUpdates(), contains(metricUpdate("TestMetric", "MetricCounter", o1, 1L)));        assertThat(context2.metricsContainer().getUpdates().counterUpdates(), contains(metricUpdate("TestMetric", "MetricCounter", o2, 2L)));        assertThat(context3.metricsContainer().getUpdates().counterUpdates(), contains(metricUpdate("TestMetric", "MetricCounter", o3, 3L)));    }}
public void beam_f10293_0() throws Exception
{    Operation o1 = Mockito.mock(Operation.class);    Operation o2 = Mockito.mock(Operation.class);    Operation o3 = Mockito.mock(Operation.class);    Mockito.doThrow(new Exception("in finish")).when(o2).finish();    ExecutionStateTracker stateTracker = ExecutionStateTracker.newForTest();    try (IntrinsicMapTaskExecutor executor = IntrinsicMapTaskExecutor.withSharedCounterSet(Arrays.<Operation>asList(o1, o2, o3), counterSet, stateTracker)) {        executor.execute();        fail("Should have thrown");    } catch (Exception e) {        InOrder inOrder = Mockito.inOrder(o1, o2, o3);        inOrder.verify(o3).start();        inOrder.verify(o2).start();        inOrder.verify(o1).start();        inOrder.verify(o1).finish();        inOrder.verify(o2).finish();                Mockito.verify(o1).abort();        Mockito.verify(o2).abort();        Mockito.verify(o3).abort();        Mockito.verifyNoMoreInteractions(o1, o2, o3);    }}
public void beam_f10294_0() throws Exception
{    Operation o1 = Mockito.mock(Operation.class);    Operation o2 = Mockito.mock(Operation.class);    Operation o3 = Mockito.mock(Operation.class);    Operation o4 = Mockito.mock(Operation.class);    Mockito.doThrow(new Exception("in finish")).when(o2).finish();    Mockito.doThrow(new Exception("suppressed in abort")).when(o3).abort();    ExecutionStateTracker stateTracker = ExecutionStateTracker.newForTest();    try (IntrinsicMapTaskExecutor executor = IntrinsicMapTaskExecutor.withSharedCounterSet(Arrays.<Operation>asList(o1, o2, o3, o4), counterSet, stateTracker)) {        executor.execute();        fail("Should have thrown");    } catch (Exception e) {        InOrder inOrder = Mockito.inOrder(o1, o2, o3, o4);        inOrder.verify(o4).start();        inOrder.verify(o3).start();        inOrder.verify(o2).start();        inOrder.verify(o1).start();        inOrder.verify(o1).finish();                inOrder.verify(o2).finish();                Mockito.verify(o1).abort();        Mockito.verify(o2).abort();                Mockito.verify(o3).abort();        Mockito.verify(o4).abort();        Mockito.verifyNoMoreInteractions(o1, o2, o3, o4);                assertThat(e.getMessage(), equalTo("in finish"));        assertThat(e.getSuppressed(), arrayWithSize(1));        assertThat(e.getSuppressed()[0].getMessage(), equalTo("suppressed in abort"));    }}
public void beam_f10303_0() throws Exception
{    IsmRecordCoder<String> coder = IsmRecordCoder.of(2, 0, ImmutableList.<Coder<?>>of(StringUtf8Coder.of(), StringUtf8Coder.of()), StringUtf8Coder.of());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Expected at most");    coder.hash(ImmutableList.of("A", "B", "C"));}
public void beam_f10304_0() throws Exception
{    IsmRecordCoder<String> coder = IsmRecordCoder.of(2, 0, ImmutableList.<Coder<?>>of(StringUtf8Coder.of(), StringUtf8Coder.of()), StringUtf8Coder.of());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Expected at least");    coder.hash(ImmutableList.of("A"));}
public void beam_f10313_0() throws Exception
{    WindowedValueCoder<?> coder = WindowedValue.getFullCoder(IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(StringUtf8Coder.of()), VarLongCoder.of()), GlobalWindow.Coder.INSTANCE);    String tmpFile = tmpFolder.newFile().getPath();    ResourceId tmpResourceId = FileSystems.matchSingleFileSpec(tmpFile).resourceId();    @SuppressWarnings("rawtypes")    IsmReader<?> ismReader = (IsmReader) new IsmReaderFactory().create(createSpecForFilename(tmpFile), coder, options, executionContext, operationContext);    assertEquals(coder.getValueCoder(), ismReader.getCoder());    assertEquals(tmpResourceId, ismReader.getResourceId());}
public void beam_f10314_0() throws Exception
{    Coder<?> coder = WindowedValue.getFullCoder(IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(StringUtf8Coder.of()), VarLongCoder.of()), GlobalWindow.Coder.INSTANCE);    String tmpFile = tmpFolder.newFile().getPath();    String anotherTmpFile = tmpFolder.newFile().getPath();    @SuppressWarnings("rawtypes")    IsmReader<?> ismReader = (IsmReader) new IsmReaderFactory().create(createSpecForFilename(tmpFile), coder, options, executionContext, operationContext);    assertSame(ismReader, new IsmReaderFactory().create(createSpecForFilename(tmpFile), coder, options, executionContext, operationContext));    assertNotSame(ismReader, new IsmReaderFactory().create(createSpecForFilename(anotherTmpFile), coder, options, executionContext, operationContext));}
public void beam_f10323_0() throws Exception
{    Random random = new Random(23498323891L);    int minElements = (int) Math.pow(2, 6);    int valueSize = 128;            checkState(minElements * valueSize > 2 * TEST_BLOCK_SIZE);    writeElementsToFileAndReadInOrder(dataGenerator(8, /* number of primary keys */    minElements + random.nextInt(minElements), /* number of secondary keys */    8, /* max key size */    valueSize));}
public void beam_f10324_0() throws Exception
{    Random random = new Random(2348238943L);    for (int i : Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8)) {        int minElements = (int) Math.pow(2, i);        int valueSize = 128;                writeElementsToFileAndReadInRandomOrder(dataGenerator(7, /* number of primary keys */        minElements + random.nextInt(minElements), /* number of secondary keys */        8, /* max key size */        valueSize));    }}
 long beam_f10333_0()
{    return TEST_BLOCK_SIZE;}
private void beam_f10334_0(Iterable<IsmRecord<byte[]>> elements) throws Exception
{    File tmpFile = tmpFolder.newFile();    writeElementsToFile(elements, tmpFile);    IsmReader<byte[]> reader = new IsmReaderImpl<>(FileSystems.matchSingleFileSpec(tmpFile.getAbsolutePath()).resourceId(), CODER, cache);    assertFalse(reader.isInitialized());    TestReaderObserver observer = new TestReaderObserver(reader);    reader.addObserver(observer);    Iterator<IsmRecord<byte[]>> elementsIterator = elements.iterator();    try (NativeReader.NativeReaderIterator<WindowedValue<IsmRecord<byte[]>>> iterator = reader.iterator()) {        boolean more = iterator.start();        assertTrue(reader.isInitialized());        for (; more; more = iterator.advance()) {            if (!elementsIterator.hasNext()) {                break;            }            IsmRecord<byte[]> expected = elementsIterator.next();            IsmRecord<byte[]> actual = iterator.getCurrent().getValue();            assertIsmEquals(actual, expected);            final int expectedLength;            if (IsmFormat.isMetadataKey(expected.getKeyComponents())) {                expectedLength = expected.getMetadata().length;            } else {                expectedLength = expected.getValue().length;            }                        assertTrue(expectedLength <= observer.getActualSizes().get(observer.getActualSizes().size() - 1));        }        if (iterator.advance()) {            fail("Read more elements then expected, did not expect: " + iterator.getCurrent());        } else if (elementsIterator.hasNext()) {            fail("Read less elements then expected, expected: " + elementsIterator.next());        }                try {            iterator.getCurrent();            fail("Expected a NoSuchElementException to have been thrown.");        } catch (NoSuchElementException expected) {        }    }}
public void beam_f10343_0() throws Exception
{    try (SeekableByteChannel channel = new CachedTailSeekableByteChannel(0, new byte[0])) {        expectedException.expect(NonWritableChannelException.class);        channel.truncate(0);    }}
public void beam_f10344_0() throws Exception
{    try (SeekableByteChannel channel = new CachedTailSeekableByteChannel(0, new byte[0])) {        expectedException.expect(NonWritableChannelException.class);        channel.write(ByteBuffer.wrap(new byte[0]));    }}
public void beam_f10353_0() throws Exception
{    IntervalWindow firstWindow = new IntervalWindow(new Instant(0L), new Instant(100L));    IntervalWindow secondWindow = new IntervalWindow(new Instant(50L), new Instant(150L));    IntervalWindow emptyWindow = new IntervalWindow(new Instant(75L), new Instant(175L));    final Map<IntervalWindow, WindowedValue<Map<String, Long>>> elements = ImmutableMap.<IntervalWindow, WindowedValue<Map<String, Long>>>builder().put(firstWindow, WindowedValue.of(ImmutableMap.<String, Long>builder().put("foo", 0L).put("bar", -1L).build(), new Instant(7), firstWindow, PaneInfo.NO_FIRING)).put(secondWindow, WindowedValue.of(ImmutableMap.<String, Long>builder().put("bar", -1L).put("baz", 1L).build(), new Instant(53L), secondWindow, PaneInfo.NO_FIRING)).build();    Coder<Map<String, Long>> mapCoder = MapCoder.of(StringUtf8Coder.of(), VarLongCoder.of());    final PCollectionView<Map<String, Long>> view = Pipeline.create().apply(Create.empty(KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of()))).apply(Window.into(SlidingWindows.of(Duration.millis(100L)).every(Duration.millis(50L)))).apply(View.asMap());    IsmRecordCoder<WindowedValue<Map<String, Long>>> recordCoder = IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(INTERVAL_WINDOW_CODER), WindowedValue.getFullCoder(mapCoder, INTERVAL_WINDOW_CODER));    final Source source = initInputFile(fromValues(elements.values()), recordCoder);    final IsmSideInputReader reader = sideInputReader(view.getTagInternal().getId(), source);    List<Callable<Map<BoundedWindow, Map<String, Long>>>> tasks = new ArrayList<>();    for (int i = 0; i < NUM_THREADS; ++i) {        tasks.add(() -> {                                    Map<String, Long> value = reader.get(view, firstWindow);            assertEquals(elements.get(firstWindow).getValue(), value);                        assertSame(value, reader.get(view, firstWindow));            Map<String, Long> secondValue = reader.get(view, secondWindow);            assertEquals(elements.get(secondWindow).getValue(), secondValue);                        assertSame(secondValue, reader.get(view, secondWindow));            Map<String, Long> emptyValue = reader.get(view, emptyWindow);            assertThat(emptyValue.keySet(), empty());            Map<BoundedWindow, Map<String, Long>> result = ImmutableMap.<BoundedWindow, Map<String, Long>>builder().put(firstWindow, value).put(secondWindow, secondValue).put(emptyWindow, emptyValue).build();            return result;        });    }    List<Future<Map<BoundedWindow, Map<String, Long>>>> results = pipelineOptions.getExecutorService().invokeAll(tasks);        Map<BoundedWindow, Map<String, Long>> value = results.get(0).get();    for (Future<Map<BoundedWindow, Map<String, Long>>> result : results) {        assertEquals(value, result.get());        for (Map.Entry<BoundedWindow, Map<String, Long>> entry : result.get().entrySet()) {            assertSame(value.get(entry.getKey()), entry.getValue());        }    }}
public void beam_f10354_0() throws Exception
{    IntervalWindow firstWindow = new IntervalWindow(new Instant(0L), new Instant(100L));    IntervalWindow secondWindow = new IntervalWindow(new Instant(50L), new Instant(150L));    IntervalWindow emptyWindow = new IntervalWindow(new Instant(75L), new Instant(175L));        @SuppressWarnings({ "unchecked", "rawtypes" })    final Map<IntervalWindow, WindowedValue<Map<String, Iterable<Long>>>> elements = ImmutableMap.<IntervalWindow, WindowedValue<Map<String, Iterable<Long>>>>builder().put(firstWindow, WindowedValue.of((Map) ImmutableListMultimap.<String, Long>builder().put("foo", 0L).put("foo", 2L).put("bar", -1L).build().asMap(), new Instant(7), firstWindow, PaneInfo.NO_FIRING)).put(secondWindow, WindowedValue.of((Map) ImmutableListMultimap.<String, Long>builder().put("bar", -1L).put("baz", 1L).put("baz", 3L).build().asMap(), new Instant(53L), secondWindow, PaneInfo.NO_FIRING)).build();    StringUtf8Coder strCoder = StringUtf8Coder.of();    Coder<Map<String, Iterable<Long>>> mapCoder = MapCoder.of(strCoder, IterableCoder.of(VarLongCoder.of()));    final PCollectionView<Map<String, Iterable<Long>>> view = Pipeline.create().apply(Create.empty(KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of()))).apply(Window.into(FixedWindows.of(Duration.millis(100L)))).apply(View.asMultimap());    IsmRecordCoder<WindowedValue<Map<String, Iterable<Long>>>> recordCoder = IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(INTERVAL_WINDOW_CODER), WindowedValue.getFullCoder(mapCoder, INTERVAL_WINDOW_CODER));    final Source source = initInputFile(fromValues(elements.values()), recordCoder);    final IsmSideInputReader reader = sideInputReader(view.getTagInternal().getId(), source);    List<Callable<Map<BoundedWindow, Map<String, Iterable<Long>>>>> tasks = new ArrayList<>();    for (int i = 0; i < NUM_THREADS; ++i) {        tasks.add(() -> {                                    Map<String, Iterable<Long>> value = reader.get(view, firstWindow);            assertEquals(elements.get(firstWindow).getValue(), value);                        assertSame(value, reader.get(view, firstWindow));            Map<String, Iterable<Long>> secondValue = reader.get(view, secondWindow);            assertEquals(elements.get(secondWindow).getValue(), secondValue);                        assertSame(secondValue, reader.get(view, secondWindow));            Map<String, Iterable<Long>> emptyValue = reader.get(view, emptyWindow);            assertThat(emptyValue.keySet(), empty());            Map<BoundedWindow, Map<String, Iterable<Long>>> result = ImmutableMap.<BoundedWindow, Map<String, Iterable<Long>>>builder().put(firstWindow, value).put(secondWindow, secondValue).put(emptyWindow, emptyValue).build();            return result;        });    }    List<Future<Map<BoundedWindow, Map<String, Iterable<Long>>>>> results = pipelineOptions.getExecutorService().invokeAll(tasks);    Map<BoundedWindow, Map<String, Iterable<Long>>> value = results.get(0).get();    for (Future<Map<BoundedWindow, Map<String, Iterable<Long>>>> result : results) {        assertEquals(value, result.get());        for (Map.Entry<BoundedWindow, Map<String, Iterable<Long>>> entry : result.get().entrySet()) {            assertSame(value.get(entry.getKey()), entry.getValue());        }    }}
public void beam_f10363_0() throws Exception
{            Coder<WindowedValue<Long>> valueCoder = WindowedValue.getFullCoder(VarLongCoder.of(), INTERVAL_WINDOW_CODER);    final ListMultimap<byte[], WindowedValue<Long>> firstWindow = ImmutableListMultimap.<byte[], WindowedValue<Long>>builder().put(new byte[] { 0x00 }, valueInIntervalWindow(12L, 10)).put(new byte[] { 0x01 }, valueInIntervalWindow(22L, 10)).put(new byte[] { 0x02 }, valueInIntervalWindow(32L, 10)).build();    final ListMultimap<byte[], WindowedValue<Long>> secondWindow = ImmutableListMultimap.<byte[], WindowedValue<Long>>builder().put(new byte[] { 0x00 }, valueInIntervalWindow(42L, 20)).put(new byte[] { 0x03 }, valueInIntervalWindow(52L, 20)).put(new byte[] { 0x02 }, valueInIntervalWindow(62L, 20)).build();    final ListMultimap<byte[], WindowedValue<Long>> thirdWindow = ImmutableListMultimap.<byte[], WindowedValue<Long>>builder().put(new byte[] { 0x02 }, valueInIntervalWindow(72L, 30)).put(new byte[] { 0x04 }, valueInIntervalWindow(82L, 30)).put(new byte[] { 0x05 }, valueInIntervalWindow(92L, 30)).build();    final PCollectionView<Map<byte[], Iterable<Long>>> view = Pipeline.create().apply(Create.empty(KvCoder.of(ByteArrayCoder.of(), VarLongCoder.of()))).apply(Window.into(FixedWindows.of(Duration.millis(10)))).apply(View.asMultimap());    IsmRecordCoder<WindowedValue<Long>> ismCoder = IsmRecordCoder.of(1, 2, ImmutableList.of(MetadataKeyCoder.of(ByteArrayCoder.of()), INTERVAL_WINDOW_CODER, BigEndianLongCoder.of()), valueCoder);    Multimap<Integer, IsmRecord<WindowedValue<Long>>> elementsPerShard = forMap(ismCoder, firstWindow);    elementsPerShard.putAll(forMap(ismCoder, secondWindow));    elementsPerShard.putAll(forMap(ismCoder, thirdWindow));    List<IsmRecord<WindowedValue<Long>>> firstElements = new ArrayList<>();    List<IsmRecord<WindowedValue<Long>>> secondElements = new ArrayList<>();    for (Map.Entry<Integer, Collection<IsmRecord<WindowedValue<Long>>>> entry : elementsPerShard.asMap().entrySet()) {        if (entry.getKey() % 2 == 0) {            firstElements.addAll(entry.getValue());        } else {            secondElements.addAll(entry.getValue());        }    }        checkState(!firstElements.isEmpty());    checkState(!secondElements.isEmpty());    Source sourceA = initInputFile(firstElements, ismCoder);    Source sourceB = initInputFile(secondElements, ismCoder);    List<IsmRecord<WindowedValue<Long>>> firstWindowMapMetadata = forMapMetadata(ByteArrayCoder.of(), firstWindow.keySet(), intervalWindow(10));    List<IsmRecord<WindowedValue<Long>>> secondWindowMapMetadata = forMapMetadata(ByteArrayCoder.of(), secondWindow.keySet(), intervalWindow(20));    List<IsmRecord<WindowedValue<Long>>> thirdWindowMapMetadata = forMapMetadata(ByteArrayCoder.of(), thirdWindow.keySet(), intervalWindow(30));    Source sourceMetaA = initInputFile(firstWindowMapMetadata, ismCoder);    Source sourceMetaB = initInputFile(concat(secondWindowMapMetadata, thirdWindowMapMetadata), ismCoder);    final IsmSideInputReader reader = sideInputReader(view.getTagInternal().getId(), sourceA, sourceB, sourceMetaA, sourceMetaB);    List<Callable<Map<BoundedWindow, Map<byte[], Iterable<Long>>>>> tasks = new ArrayList<>();    for (int i = 0; i < NUM_THREADS; ++i) {        tasks.add(() -> {                                    Map<byte[], Iterable<Long>> firstValues = reader.get(view, intervalWindow(10));            Map<byte[], Iterable<Long>> secondValues = reader.get(view, intervalWindow(20));            Map<byte[], Iterable<Long>> thirdValues = reader.get(view, intervalWindow(30));            verifyMap(Maps.transformValues(firstWindow.asMap(), new TransformForMultimap<Long>()), firstValues, new ComparatorForMultimap<Long>());            verifyMap(Maps.transformValues(secondWindow.asMap(), new TransformForMultimap<Long>()), secondValues, new ComparatorForMultimap<Long>());            verifyMap(Maps.transformValues(thirdWindow.asMap(), new TransformForMultimap<Long>()), thirdValues, new ComparatorForMultimap<Long>());                        assertSame(firstValues, reader.get(view, intervalWindow(10)));            assertSame(secondValues, reader.get(view, intervalWindow(20)));            assertSame(thirdValues, reader.get(view, intervalWindow(30)));                        assertEquals(Collections.EMPTY_MAP, reader.get(view, intervalWindow(40)));            return ImmutableMap.<BoundedWindow, Map<byte[], Iterable<Long>>>of(intervalWindow(10), firstValues, intervalWindow(20), secondValues, intervalWindow(30), thirdValues);        });    }    List<Future<Map<BoundedWindow, Map<byte[], Iterable<Long>>>>> results = pipelineOptions.getExecutorService().invokeAll(tasks);    Map<BoundedWindow, Map<byte[], Iterable<Long>>> value = results.get(0).get();        for (Future<Map<BoundedWindow, Map<byte[], Iterable<Long>>>> result : results) {        assertEquals(value, result.get());        for (Map.Entry<BoundedWindow, Map<byte[], Iterable<Long>>> entry : result.get().entrySet()) {            assertSame(value.get(entry.getKey()), entry.getValue());        }    }}
public void beam_f10364_0() throws Exception
{                byte[] duplicateKey = new byte[] { 0x01 };    Coder<WindowedValue<Long>> valueCoder = WindowedValue.getFullCoder(VarLongCoder.of(), INTERVAL_WINDOW_CODER);    final ListMultimap<byte[], WindowedValue<Long>> firstWindow = ImmutableListMultimap.<byte[], WindowedValue<Long>>builder().put(new byte[] { 0x00 }, valueInIntervalWindow(12L, 10)).put(duplicateKey, valueInIntervalWindow(22L, 10)).put(duplicateKey, valueInIntervalWindow(23L, 10)).put(new byte[] { 0x02 }, valueInIntervalWindow(32L, 10)).build();    final ListMultimap<byte[], WindowedValue<Long>> secondWindow = ImmutableListMultimap.<byte[], WindowedValue<Long>>builder().put(new byte[] { 0x00 }, valueInIntervalWindow(42L, 20)).put(new byte[] { 0x03 }, valueInIntervalWindow(52L, 20)).put(new byte[] { 0x02 }, valueInIntervalWindow(62L, 20)).build();    final ListMultimap<byte[], WindowedValue<Long>> thirdWindow = ImmutableListMultimap.<byte[], WindowedValue<Long>>builder().put(new byte[] { 0x02 }, valueInIntervalWindow(73L, 30)).put(new byte[] { 0x04 }, valueInIntervalWindow(82L, 30)).put(new byte[] { 0x05 }, valueInIntervalWindow(92L, 30)).build();    final PCollectionView<MultimapView<byte[], WindowedValue<Long>>> view = DataflowPortabilityPCollectionView.with(new TupleTag<>(), FullWindowedValueCoder.of(KvCoder.of(ByteArrayCoder.of(), valueCoder), INTERVAL_WINDOW_CODER));    IsmRecordCoder<WindowedValue<Long>> ismCoder = IsmRecordCoder.of(1, 0, ImmutableList.of(ByteArrayCoder.of(), INTERVAL_WINDOW_CODER, BigEndianLongCoder.of()), valueCoder);    Multimap<Integer, IsmRecord<WindowedValue<Long>>> elementsPerShard = forMap(ismCoder, firstWindow);    elementsPerShard.putAll(forMap(ismCoder, secondWindow));    elementsPerShard.putAll(forMap(ismCoder, thirdWindow));    List<IsmRecord<WindowedValue<Long>>> firstElements = new ArrayList<>();    List<IsmRecord<WindowedValue<Long>>> secondElements = new ArrayList<>();    for (Map.Entry<Integer, Collection<IsmRecord<WindowedValue<Long>>>> entry : elementsPerShard.asMap().entrySet()) {        if (entry.getKey() % 2 == 0) {            firstElements.addAll(entry.getValue());        } else {            secondElements.addAll(entry.getValue());        }    }        checkState(!firstElements.isEmpty());    checkState(!secondElements.isEmpty());    Source sourceA = initInputFile(firstElements, ismCoder);    Source sourceB = initInputFile(secondElements, ismCoder);    final IsmSideInputReader reader = sideInputReader(view.getTagInternal().getId(), sourceA, sourceB);    List<Callable<Map<BoundedWindow, MultimapView<byte[], WindowedValue<Long>>>>> tasks = new ArrayList<>();    for (int i = 0; i < 3; ++i) {        tasks.add(() -> {                                    MultimapView<byte[], WindowedValue<Long>> firstValues = reader.get(view, intervalWindow(10));            MultimapView<byte[], WindowedValue<Long>> secondValues = reader.get(view, intervalWindow(20));            MultimapView<byte[], WindowedValue<Long>> thirdValues = reader.get(view, intervalWindow(30));            for (Map.Entry<byte[], Collection<WindowedValue<Long>>> entry : firstWindow.asMap().entrySet()) {                verifyIterable(entry.getValue(), firstValues.get(entry.getKey()));            }            for (Map.Entry<byte[], Collection<WindowedValue<Long>>> entry : secondWindow.asMap().entrySet()) {                verifyIterable(entry.getValue(), secondValues.get(entry.getKey()));            }            for (Map.Entry<byte[], Collection<WindowedValue<Long>>> entry : thirdWindow.asMap().entrySet()) {                verifyIterable(entry.getValue(), thirdValues.get(entry.getKey()));            }                        assertSame(firstValues, reader.get(view, intervalWindow(10)));            assertSame(secondValues, reader.get(view, intervalWindow(20)));            assertSame(thirdValues, reader.get(view, intervalWindow(30)));            return ImmutableMap.of(intervalWindow(10), firstValues, intervalWindow(20), secondValues, intervalWindow(30), thirdValues);        });    }    List<Future<Map<BoundedWindow, MultimapView<byte[], WindowedValue<Long>>>>> results = pipelineOptions.getExecutorService().invokeAll(tasks);    Map<BoundedWindow, MultimapView<byte[], WindowedValue<Long>>> value = results.get(0).get();        for (Future<Map<BoundedWindow, MultimapView<byte[], WindowedValue<Long>>>> result : results) {        assertEquals(value, result.get());        for (Map.Entry<BoundedWindow, MultimapView<byte[], WindowedValue<Long>>> entry : result.get().entrySet()) {            assertSame(value.get(entry.getKey()), entry.getValue());        }    }}
private static void beam_f10373_0(Map<byte[], T> expectedMap, Map<byte[], T> mapView, Comparator<T> valueComparator)
{    List<Entry<byte[], T>> expectedElements = new ArrayList<>(expectedMap.entrySet());    Random random = new Random(1237812387L);        assertEquals(expectedMap.size(), mapView.size());        Collections.shuffle(expectedElements, random);    for (Entry<byte[], T> expected : expectedElements) {        assertTrue(valueComparator.compare(expected.getValue(), mapView.get(expected.getKey())) == 0);    }        Collections.shuffle(expectedElements, random);    for (Entry<byte[], T> expected : expectedElements) {        assertTrue(mapView.containsKey(expected.getKey()));    }        Collections.shuffle(expectedElements, random);    Set<byte[]> mapViewKeySet = mapView.keySet();    for (Entry<byte[], T> expected : expectedElements) {        mapViewKeySet.contains(expected.getKey());    }        Iterator<byte[]> mapViewKeySetIterator = mapView.keySet().iterator();    assertEquals(expectedElements.size(), Iterators.size(mapViewKeySetIterator));    try {        mapViewKeySetIterator.next();        fail("Expected to have thrown NoSuchElementException");    } catch (NoSuchElementException expected) {    }        Collections.shuffle(expectedElements, random);    Set<Map.Entry<byte[], T>> mapViewEntrySet = mapView.entrySet();    for (Entry<byte[], T> expected : expectedElements) {        mapViewEntrySet.contains(new SimpleImmutableEntry<>(expected.getKey(), expected.getValue()));    }        Iterator<Map.Entry<byte[], T>> mapViewEntrySetIterator = mapView.entrySet().iterator();    assertEquals(expectedElements.size(), Iterators.size(mapViewEntrySetIterator));    try {        mapViewEntrySetIterator.next();        fail("Expected to have thrown NoSuchElementException");    } catch (NoSuchElementException expected) {    }        Collections.shuffle(expectedElements, random);    Collection<T> mapViewValues = mapView.values();    for (Entry<byte[], T> expected : expectedElements) {        mapViewValues.contains(expected.getValue());    }        Iterator<T> mapViewValuesIterator = mapView.values().iterator();    assertEquals(expectedElements.size(), Iterators.size(mapViewValuesIterator));    try {        mapViewValuesIterator.next();        fail("Expected to have thrown NoSuchElementException");    } catch (NoSuchElementException expected) {    }}
 WindowedValue<Long> beam_f10374_0(long value, long startOfWindow)
{    return WindowedValue.of(value, new Instant(startOfWindow), intervalWindow(startOfWindow), PaneInfo.NO_FIRING);}
private Source beam_f10383_0(Iterable<IsmRecord<WindowedValue<V>>> elements, IsmRecordCoder<WindowedValue<V>> coder) throws Exception
{    return initInputFile(elements, coder, tmpFolder.newFile().getPath());}
private Source beam_f10384_0(Iterable<IsmRecord<WindowedValue<V>>> elements, IsmRecordCoder<WindowedValue<V>> coder, String tmpFilePath) throws Exception
{        Map<Integer, SortedMap<RandomAccessData, IsmRecord<WindowedValue<V>>>> writeOrder = new HashMap<>();    for (IsmRecord<WindowedValue<V>> element : elements) {        int shardId = coder.hash(element.getKeyComponents());        if (!writeOrder.containsKey(shardId)) {            writeOrder.put(shardId, new TreeMap<RandomAccessData, IsmRecord<WindowedValue<V>>>(RandomAccessData.UNSIGNED_LEXICOGRAPHICAL_COMPARATOR));        }        RandomAccessData data = encodeKeyPortion(coder, element);        writeOrder.get(shardId).put(data, element);    }    IsmSink<WindowedValue<V>> sink = new IsmSink<>(FileSystems.matchNewResource(tmpFilePath, false), coder, BLOOM_FILTER_SIZE_LIMIT);    try (SinkWriter<WindowedValue<IsmRecord<WindowedValue<V>>>> writer = sink.writer()) {        for (Entry<Integer, SortedMap<RandomAccessData, IsmRecord<WindowedValue<V>>>> entry : writeOrder.entrySet()) {            for (IsmRecord<WindowedValue<V>> record : entry.getValue().values()) {                writer.add(new ValueInEmptyWindows<>(record));            }        }    }    return newIsmSource(coder, tmpFilePath);}
public void beam_f10393_0() throws Throwable
{    IsmSink<byte[]> sink = new IsmSink<>(FileSystems.matchNewResource(tmpFolder.newFile().getPath(), false), CODER, BLOOM_FILTER_SIZE_LIMIT);    SinkWriter<WindowedValue<IsmRecord<byte[]>>> sinkWriter = sink.writer();    sinkWriter.add(new ValueInEmptyWindows<>(IsmRecord.of(ImmutableList.of(EMPTY, new byte[] { 0x00, 0x00 }), EMPTY)));    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("expects keys to be written in strictly increasing order");    sinkWriter.add(new ValueInEmptyWindows<>(IsmRecord.of(ImmutableList.of(EMPTY, new byte[] { 0x00 }), EMPTY)));}
public void beam_f10394_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("is expected to be deterministic");    new IsmSink<>(FileSystems.matchNewResource(tmpFolder.newFile().getPath(), false), IsmRecordCoder.of(1, 0, ImmutableList.<Coder<?>>of(NON_DETERMINISTIC_CODER, ByteArrayCoder.of()), ByteArrayCoder.of()), BLOOM_FILTER_SIZE_LIMIT);}
public void beam_f10403_0()
{    trackerCleanup = tracker.activate();}
public void beam_f10404_0() throws IOException
{    trackerCleanup.close();}
public void beam_f10413_0() throws IOException
{    DataflowWorkerLoggingMDC.setJobId("testJobId");    DataflowWorkerLoggingMDC.setWorkerId("testWorkerId");    DataflowWorkerLoggingMDC.setWorkId("testWorkId");    assertEquals("{\"timestamp\":{\"seconds\":0,\"nanos\":1000000},\"severity\":\"INFO\"," + "\"message\":\"test.message\",\"thread\":\"2\",\"job\":\"testJobId\"," + "\"worker\":\"testWorkerId\",\"work\":\"1\",\"logger\":\"LoggerName\"}" + System.lineSeparator(), createJson(createLogEntry("test.message")));}
public void beam_f10414_0() throws IOException
{    DataflowWorkerLoggingMDC.setJobId("testJobId");    DataflowWorkerLoggingMDC.setWorkerId("testWorkerId");    DataflowWorkerLoggingMDC.setWorkId("testWorkId");    assertEquals("{\"timestamp\":{\"seconds\":0,\"nanos\":1000000},\"severity\":\"INFO\"," + "\"message\":\"test.message\",\"thread\":\"2\",\"job\":\"testJobId\"," + "\"worker\":\"testWorkerId\",\"work\":\"1\",\"logger\":\"LoggerName\"," + "\"exception\":\"testTrace\"}" + System.lineSeparator(), createJson(createLogEntry("test.message").toBuilder().setTrace("testTrace").build()));}
private void beam_f10423_0(Handler handler, Level level)
{    assertThat(handler, instanceOf(DataflowWorkerLoggingHandler.class));    assertEquals(level, handler.getLevel());}
public void beam_f10424_0() throws IOException
{    DataflowWorkerLoggingOptions options = PipelineOptionsFactory.as(DataflowWorkerLoggingOptions.class);    options.setDefaultWorkerLogLevel(DataflowWorkerLoggingOptions.Level.ERROR);    options.setWorkerLogLevelOverrides(new WorkerLogLevelOverrides().addOverrideForName("A", DataflowWorkerLoggingOptions.Level.INFO));    DataflowWorkerLoggingInitializer.configure(options);    LogManager.getLogManager().getLogger("A").info("foobar");    verifyLogOutput("foobar");}
public void beam_f10433_0(LogRecord record)
{    System.out.println(record.getMessage());}
private void beam_f10436_0(Handler handler)
{    Logger rootLogger = LogManager.getLogManager().getLogger("");    rootLogger.addHandler(handler);}
public void beam_f10445_0()
{    PrintStream printStream = createPrintStreamAdapter();    printStream.print("blah");    assertThat(handler.getLogs(), not(hasLogItem("blah")));}
public void beam_f10446_0()
{    PrintStream printStream = createPrintStreamAdapter();    printStream.print("blah");    printStream.flush();    assertThat(handler.getLogs(), hasLogItem("blah"));}
public void beam_f10455_0()
{    String msg = "any message";    LogRecord record = new LogRecord(Level.INFO, msg);    assertThat(record, LogRecordMatcher.hasLog(msg));    assertThat(record, LogRecordMatcher.hasLog(Level.INFO, msg));}
public void beam_f10456_0()
{    LogRecord record = new LogRecord(Level.INFO, "hello world");    assertThat(record, LogRecordMatcher.hasLog("hello"));    assertThat(record, LogRecordMatcher.hasLog("world"));    assertThat("Should match empty substring", record, LogRecordMatcher.hasLog(""));}
public void beam_f10467_1()
{    Logger log = Logger.getLogger("any.logger");    LogSaver saver = new LogSaver();    log.addHandler(saver);        assertThat(saver.getLogs(), Matchers.hasItem(LogRecordMatcher.hasLog(Level.WARNING, "foobar")));}
public void beam_f10468_0() throws Exception
{    ContextActivationObserverRegistry registry = ContextActivationObserverRegistry.createDefault();    List<ContextActivationObserver> contextActivationObservers = registry.getContextActivationObservers();        assertThat(contextActivationObservers, hasItem(instanceOf(MetricsEnvironmentContextActivationObserverRegistration.MetricsEnvironmentContextActivationObserver.class)));}
public void beam_f10477_0()
{    assertSignedNumIncreasingEncodingEquals("003f8000000000000000", Long.MIN_VALUE);    assertSignedNumIncreasingEncodingEquals("003f8000000000000001", Long.MIN_VALUE + 1);    assertSignedNumIncreasingEncodingEquals("077fffffff", Integer.MIN_VALUE - 1L);    assertSignedNumIncreasingEncodingEquals("0780000000", Integer.MIN_VALUE);    assertSignedNumIncreasingEncodingEquals("0780000001", Integer.MIN_VALUE + 1);    assertSignedNumIncreasingEncodingEquals("3fbf", -65);    assertSignedNumIncreasingEncodingEquals("40", -64);    assertSignedNumIncreasingEncodingEquals("41", -63);    assertSignedNumIncreasingEncodingEquals("7d", -3);    assertSignedNumIncreasingEncodingEquals("7e", -2);    assertSignedNumIncreasingEncodingEquals("7f", -1);    assertSignedNumIncreasingEncodingEquals("80", 0);    assertSignedNumIncreasingEncodingEquals("81", 1);    assertSignedNumIncreasingEncodingEquals("82", 2);    assertSignedNumIncreasingEncodingEquals("83", 3);    assertSignedNumIncreasingEncodingEquals("bf", 63);    assertSignedNumIncreasingEncodingEquals("c040", 64);    assertSignedNumIncreasingEncodingEquals("c041", 65);    assertSignedNumIncreasingEncodingEquals("f87ffffffe", Integer.MAX_VALUE - 1);    assertSignedNumIncreasingEncodingEquals("f87fffffff", Integer.MAX_VALUE);    assertSignedNumIncreasingEncodingEquals("f880000000", Integer.MAX_VALUE + 1L);    assertSignedNumIncreasingEncodingEquals("ffc07ffffffffffffffe", Long.MAX_VALUE - 1);    assertSignedNumIncreasingEncodingEquals("ffc07fffffffffffffff", Long.MAX_VALUE);}
private static byte[] beam_f10478_0(String hexDigits)
{    return BaseEncoding.base16().lowerCase().decode(hexDigits);}
public void beam_f10487_0()
{    byte[] escapeChars = new byte[] { OrderedCode.ESCAPE1, OrderedCode.NULL_CHARACTER, OrderedCode.SEPARATOR, OrderedCode.ESCAPE2, OrderedCode.INFINITY, OrderedCode.FF_CHARACTER };    byte[] anotherArray = new byte[] { 'a', 'b', 'c', 'd', 'e' };    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeTrailingBytes(escapeChars);    assertArrayEquals(orderedCode.getEncodedBytes(), escapeChars);    assertArrayEquals(orderedCode.readTrailingBytes(), escapeChars);    try {        orderedCode.readInfinity();        fail("Expected IllegalArgumentException.");    } catch (IllegalArgumentException e) {        }    orderedCode = new OrderedCode();    orderedCode.writeTrailingBytes(anotherArray);    assertArrayEquals(orderedCode.getEncodedBytes(), anotherArray);    assertArrayEquals(orderedCode.readTrailingBytes(), anotherArray);}
public void beam_f10488_0()
{    byte[] first = { 'a', 'b', 'c' };    byte[] second = { 'd', 'e', 'f' };    byte[] last = { 'x', 'y', 'z' };    byte[] escapeChars = new byte[] { OrderedCode.ESCAPE1, OrderedCode.NULL_CHARACTER, OrderedCode.SEPARATOR, OrderedCode.ESCAPE2, OrderedCode.INFINITY, OrderedCode.FF_CHARACTER };    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeBytes(first);    orderedCode.writeBytes(second);    orderedCode.writeBytes(last);    orderedCode.writeInfinity();    orderedCode.writeNumIncreasing(0);    orderedCode.writeNumIncreasing(1);    orderedCode.writeNumIncreasing(Long.MIN_VALUE);    orderedCode.writeNumIncreasing(Long.MAX_VALUE);    orderedCode.writeSignedNumIncreasing(0);    orderedCode.writeSignedNumIncreasing(1);    orderedCode.writeSignedNumIncreasing(Long.MIN_VALUE);    orderedCode.writeSignedNumIncreasing(Long.MAX_VALUE);    orderedCode.writeTrailingBytes(escapeChars);    byte[] allEncoded = orderedCode.getEncodedBytes();    assertArrayEquals(orderedCode.readBytes(), first);    assertArrayEquals(orderedCode.readBytes(), second);    assertFalse(orderedCode.readInfinity());    assertArrayEquals(orderedCode.readBytes(), last);    assertTrue(orderedCode.readInfinity());    assertEquals(0, orderedCode.readNumIncreasing());    assertEquals(1, orderedCode.readNumIncreasing());    assertFalse(orderedCode.readInfinity());    assertEquals(Long.MIN_VALUE, orderedCode.readNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readNumIncreasing());    assertEquals(0, orderedCode.readSignedNumIncreasing());    assertEquals(1, orderedCode.readSignedNumIncreasing());    assertFalse(orderedCode.readInfinity());    assertEquals(Long.MIN_VALUE, orderedCode.readSignedNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readSignedNumIncreasing());    assertArrayEquals(orderedCode.getEncodedBytes(), escapeChars);    assertArrayEquals(orderedCode.readTrailingBytes(), escapeChars);    orderedCode = new OrderedCode(allEncoded);    assertArrayEquals(orderedCode.readBytes(), first);    assertArrayEquals(orderedCode.readBytes(), second);    assertFalse(orderedCode.readInfinity());    assertArrayEquals(orderedCode.readBytes(), last);    assertTrue(orderedCode.readInfinity());    assertEquals(0, orderedCode.readNumIncreasing());    assertEquals(1, orderedCode.readNumIncreasing());    assertFalse(orderedCode.readInfinity());    assertEquals(Long.MIN_VALUE, orderedCode.readNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readNumIncreasing());    assertEquals(0, orderedCode.readSignedNumIncreasing());    assertEquals(1, orderedCode.readSignedNumIncreasing());    assertFalse(orderedCode.readInfinity());    assertEquals(Long.MIN_VALUE, orderedCode.readSignedNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readSignedNumIncreasing());    assertArrayEquals(orderedCode.getEncodedBytes(), escapeChars);    assertArrayEquals(orderedCode.readTrailingBytes(), escapeChars);}
public void beam_f10497_0() throws Exception
{    StreamingOptions options = PipelineOptionsFactory.as(StreamingOptions.class);    options.setStreaming(true);    Coder keyCoder = StringUtf8Coder.of();    Coder valueCoder = BigEndianIntegerCoder.of();    KvCoder<String, Integer> kvCoder = KvCoder.of(keyCoder, valueCoder);    TestOutputReceiver receiver = new TestOutputReceiver(new ElementByteSizeObservableCoder(WindowedValue.getValueOnlyCoder(kvCoder)), counterSet, NameContextsForTests.nameContextForTest());    ParDoFn pgbk = PartialGroupByKeyParDoFns.create(options, kvCoder, AppliedCombineFn.withInputCoder(Sum.ofIntegers(), CoderRegistry.createDefault(), kvCoder), NullSideInputReader.empty(), receiver, null);    assertTrue(pgbk instanceof SimplePartialGroupByKeyParDoFn);}
public void beam_f10498_0() throws Exception
{    StreamingOptions options = PipelineOptionsFactory.as(StreamingOptions.class);    options.setStreaming(true);    Coder keyCoder = StringUtf8Coder.of();    Coder valueCoder = BigEndianIntegerCoder.of();    KvCoder<String, Integer> kvCoder = KvCoder.of(keyCoder, valueCoder);    TestOutputReceiver receiver = new TestOutputReceiver(new ElementByteSizeObservableCoder(WindowedValue.getValueOnlyCoder(kvCoder)), counterSet, NameContextsForTests.nameContextForTest());    when(mockSideInputReader.isEmpty()).thenReturn(false);    when(mockStreamingStepContext.stateInternals()).thenReturn((StateInternals) mockStateInternals);    when(mockStateInternals.state(Matchers.<StateNamespace>any(), Matchers.<StateTag>any())).thenReturn(mockState);    when(mockState.read()).thenReturn(Maps.newHashMap());    ParDoFn pgbk = PartialGroupByKeyParDoFns.create(options, kvCoder, AppliedCombineFn.withInputCoder(Sum.ofIntegers(), CoderRegistry.createDefault(), kvCoder, ImmutableList.<PCollectionView<?>>of(), WindowingStrategy.globalDefault()), mockSideInputReader, receiver, mockStreamingStepContext);    assertTrue(pgbk instanceof StreamingSideInputPGBKParDoFn);}
public void beam_f10507_0() throws Exception
{    runTestReadFromShuffle(NO_KVS);}
public void beam_f10508_0() throws Exception
{    runTestReadFromShuffle(KVS);}
public void beam_f10517_0() throws Exception
{    MockitoAnnotations.initMocks(this);}
private void beam_f10518_0(String parseFn) throws Exception
{    when(mockContext.getWork()).thenReturn(Windmill.WorkItem.newBuilder().setKey(ByteString.copyFromUtf8("key")).setWorkToken(0).addMessageBundles(Windmill.InputMessageBundle.newBuilder().setSourceComputationId("pubsub").addMessages(Windmill.Message.newBuilder().setTimestamp(0).setData(ByteString.copyFromUtf8("e0"))).addMessages(Windmill.Message.newBuilder().setTimestamp(1000).setData(ByteString.copyFromUtf8("e1"))).addMessages(Windmill.Message.newBuilder().setTimestamp(2000).setData(ByteString.copyFromUtf8("e2")))).build());    Map<String, Object> spec = new HashMap<>();    spec.put(PropertyNames.OBJECT_TYPE_NAME, "");    if (parseFn != null) {        spec.put(PropertyNames.PUBSUB_SERIALIZED_ATTRIBUTES_FN, parseFn);    }    CloudObject cloudSourceSpec = CloudObject.fromSpec(spec);    PubsubReader.Factory factory = new PubsubReader.Factory();    PubsubReader<String> reader = (PubsubReader<String>) factory.create(cloudSourceSpec, WindowedValue.getFullCoder(StringUtf8Coder.of(), IntervalWindow.getCoder()), null, mockContext, null);    NativeReader.NativeReaderIterator<WindowedValue<String>> iter = reader.iterator();    assertTrue(iter.start());    assertEquals(iter.getCurrent(), WindowedValue.timestampedValueInGlobalWindow("e0", new Instant(0)));    assertTrue(iter.advance());    assertEquals(iter.getCurrent(), WindowedValue.timestampedValueInGlobalWindow("e1", new Instant(1)));    assertTrue(iter.advance());    assertEquals(iter.getCurrent(), WindowedValue.timestampedValueInGlobalWindow("e2", new Instant(2)));    assertFalse(iter.advance());}
public void beam_f10527_0() throws IOException
{    readerCache.cacheReader(C_ID, KEY_1, 1, reader1);    readerCache.cacheReader(C_ID, KEY_2, 2, reader2);    readerCache.cacheReader(C_ID, KEY_3, 2, reader3);    readerCache.invalidateReader(C_ID, KEY_1);    verify(reader1).close();        doThrow(new IOException("swallowed")).when(reader2).close();    readerCache.invalidateReader(C_ID, KEY_2);        doThrow(new RuntimeException("expected")).when(reader3).close();    try {        readerCache.invalidateReader(C_ID, KEY_3);        fail("Exception should have been thrown");    } catch (RuntimeException e) {        assertEquals("expected", e.getMessage());    }}
public void beam_f10528_0() throws IOException, InterruptedException
{        Duration cacheDuration = Duration.millis(10);        ReaderCache readerCache = new ReaderCache(cacheDuration);    readerCache.cacheReader(C_ID, KEY_1, 1, reader1);    readerCache.cacheReader(C_ID, KEY_2, 2, reader2);    Stopwatch stopwatch = Stopwatch.createStarted();    while (stopwatch.elapsed(TimeUnit.MILLISECONDS) < 2 * cacheDuration.getMillis()) {        Thread.sleep(1);    }        readerCache.acquireReader(C_ID, KEY_3, 3);    verify(reader1).close();}
public boolean beam_f10537_0() throws IOException
{    return false;}
public WindowedValue<String> beam_f10538_0()
{    return WindowedValue.valueInGlobalWindow("something");}
public static ApproximateSplitRequest beam_f10547_0(@Nullable Long index)
{    return approximateSplitRequestAtPosition(positionAtIndex(index));}
public static ApproximateReportedProgress beam_f10548_0(@Nullable Long byteOffset)
{    return approximateProgressAtPosition(positionAtByteOffset(byteOffset));}
public static NativeReader.DynamicSplitRequest beam_f10557_0(@Nullable Long byteOffset)
{    return toDynamicSplitRequest(approximateSplitRequestAtByteOffset(byteOffset));}
public static NativeReader.DynamicSplitRequest beam_f10558_0(@Nullable Integer index, @Nullable Position innerPosition)
{    return toDynamicSplitRequest(approximateSplitRequestAtConcatPosition(index, innerPosition));}
public void beam_f10567_0()
{    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "kind:var_int32")));    assertThat(coder, instanceOf(VarIntCoder.class));}
public void beam_f10568_0()
{    Coder<?> coder = CloudObjects.coderFromCloudObject(CloudObject.fromSpec(ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "kind:fixed_big_endian_int32")));    assertThat(coder, instanceOf(BigEndianIntegerCoder.class));}
 void beam_f10577_0(byte[] shuffleReaderConfig, @Nullable String start, @Nullable String end, Coder<?> keyCoder, WindowedValueCoder<?> windowedValueCoder) throws Exception
{    PartitioningShuffleReader partitioningShuffleReader = runTestCreateShuffleReader(shuffleReaderConfig, start, end, CloudObjects.asCloudObject(FullWindowedValueCoder.of(KvCoder.of(keyCoder, windowedValueCoder.getValueCoder()), IntervalWindowCoder.of()), /*sdkComponents=*/    null), BatchModeExecutionContext.forTesting(PipelineOptionsFactory.create(), "testStage"), PartitioningShuffleReader.class, "PartitioningShuffleSource");    Assert.assertArrayEquals(shuffleReaderConfig, partitioningShuffleReader.shuffleReaderConfig);    Assert.assertEquals(start, partitioningShuffleReader.startShufflePosition);    Assert.assertEquals(end, partitioningShuffleReader.stopShufflePosition);    Assert.assertEquals(keyCoder, partitioningShuffleReader.keyCoder);    Assert.assertEquals(windowedValueCoder, partitioningShuffleReader.windowedValueCoder);}
public void beam_f10578_0() throws Exception
{    runTestCreateUngroupedShuffleReader(new byte[] { (byte) 0xE1 }, null, null, StringUtf8Coder.of());}
 void beam_f10587_0(byte[] shuffleWriterConfig, Coder<?> keyCoder, Coder<?> valueCoder) throws Exception
{    FullWindowedValueCoder<?> coder = WindowedValue.getFullCoder(KvCoder.of(keyCoder, valueCoder), IntervalWindow.getCoder());    ShuffleSink shuffleSink = runTestCreateShuffleSinkHelper(shuffleWriterConfig, "group_keys", coder, coder);    Assert.assertEquals(ShuffleSink.ShuffleKind.GROUP_KEYS, shuffleSink.shuffleKind);    Assert.assertTrue(shuffleSink.shardByKey);    Assert.assertTrue(shuffleSink.groupValues);    Assert.assertFalse(shuffleSink.sortValues);    Assert.assertEquals(keyCoder, shuffleSink.keyCoder);    Assert.assertEquals(valueCoder, shuffleSink.valueCoder);    Assert.assertNull(shuffleSink.windowedValueCoder);    Assert.assertNull(shuffleSink.sortKeyCoder);    Assert.assertNull(shuffleSink.sortValueCoder);}
 void beam_f10588_0(byte[] shuffleWriterConfig, Coder<?> keyCoder, Coder<?> sortKeyCoder, Coder<?> sortValueCoder) throws Exception
{    FullWindowedValueCoder<?> coder = WindowedValue.getFullCoder(KvCoder.of(keyCoder, KvCoder.of(sortKeyCoder, sortValueCoder)), IntervalWindow.getCoder());    ShuffleSink shuffleSink = runTestCreateShuffleSinkHelper(shuffleWriterConfig, "group_keys_and_sort_values", coder, coder);    Assert.assertEquals(ShuffleSink.ShuffleKind.GROUP_KEYS_AND_SORT_VALUES, shuffleSink.shuffleKind);    Assert.assertTrue(shuffleSink.shardByKey);    Assert.assertTrue(shuffleSink.groupValues);    Assert.assertTrue(shuffleSink.sortValues);    Assert.assertEquals(keyCoder, shuffleSink.keyCoder);    Assert.assertEquals(KvCoder.of(sortKeyCoder, sortValueCoder), shuffleSink.valueCoder);    Assert.assertEquals(sortKeyCoder, shuffleSink.sortKeyCoder);    Assert.assertEquals(sortValueCoder, shuffleSink.sortValueCoder);    Assert.assertNull(shuffleSink.windowedValueCoder);}
public void beam_f10597_0() throws Exception
{    runTestWriteUngroupingShuffleSink(TestUtils.INTS);}
public void beam_f10598_0() throws Exception
{    runTestWriteGroupingShuffleSink(NO_KVS);}
public void beam_f10607_0()
{    assertThat(state, not(equalTo(State.TORN_DOWN)));    state = State.TORN_DOWN;}
private void beam_f10608_0(String s)
{    throw new RuntimeException(s);}
public void beam_f10617_0() throws Exception
{    TestErrorDoFn fn = new TestErrorDoFn();    DoFnInfo<?, ?> fnInfo = DoFnInfo.forFn(fn, WindowingStrategy.globalDefault(), null, /* side input views */    null, /* input coder */    MAIN_OUTPUT, DoFnSchemaInformation.create(), Collections.emptyMap());    TestReceiver receiver = new TestReceiver();    ParDoFn userParDoFn = new SimpleParDoFn<>(options, DoFnInstanceManagers.singleInstance(fnInfo), new EmptySideInputReader(), MAIN_OUTPUT, ImmutableMap.of(MAIN_OUTPUT, 0), BatchModeExecutionContext.forTesting(options, "testStage").getStepContext(operationContext), operationContext, DoFnSchemaInformation.create(), Collections.emptyMap(), SimpleDoFnRunnerFactory.INSTANCE);    try {        userParDoFn.startBundle(receiver);        userParDoFn.processElement(null);        fail("should have failed");    } catch (Exception exn) {                                                        assertThat(exn, instanceOf(UserCodeException.class));                                        assertThat(stackTraceFrameStrings(exn.getCause()), contains(containsString("TestErrorDoFn.nestedFunctionBeta"), containsString("TestErrorDoFn.nestedFunctionAlpha"), containsString("TestErrorDoFn.startBundle")));        assertThat(exn.toString(), containsString("test error in initialize"));    }    try {        userParDoFn.processElement(WindowedValue.valueInGlobalWindow(3));        fail("should have failed");    } catch (Exception exn) {                        assertThat(exn, instanceOf(UserCodeException.class));                                assertThat(stackTraceFrameStrings(exn.getCause()), contains(containsString("TestErrorDoFn.nestedFunctionBeta"), containsString("TestErrorDoFn.processElement")));        assertThat(exn.toString(), containsString("test error in process"));    }    try {        userParDoFn.finishBundle();        fail("should have failed");    } catch (Exception exn) {                        assertThat(exn, instanceOf(UserCodeException.class));                        assertThat(stackTraceFrameStrings(exn.getCause()), contains(containsString("TestErrorDoFn.finishBundle")));        assertThat(exn.toString(), containsString("test error in finalize"));    }}
public void beam_f10618_0() throws Exception
{    TestDoFn fn = new TestDoFn(ImmutableList.of(new TupleTag<>("declared"), new TupleTag<>("undecl1"), new TupleTag<>("undecl2"), new TupleTag<>("undecl3")));    DoFnInfo<?, ?> fnInfo = DoFnInfo.forFn(fn, WindowingStrategy.globalDefault(), null, /* side input views */    null, /* input coder */    MAIN_OUTPUT, DoFnSchemaInformation.create(), Collections.emptyMap());    CounterSet counters = new CounterSet();    TestOperationContext operationContext = TestOperationContext.create(counters);    ParDoFn userParDoFn = new SimpleParDoFn<>(options, DoFnInstanceManagers.cloningPool(fnInfo), NullSideInputReader.empty(), MAIN_OUTPUT, ImmutableMap.of(MAIN_OUTPUT, 0, new TupleTag<String>("declared"), 1), BatchModeExecutionContext.forTesting(options, "testStage").getStepContext(operationContext), operationContext, DoFnSchemaInformation.create(), Collections.emptyMap(), SimpleDoFnRunnerFactory.INSTANCE);    userParDoFn.startBundle(new TestReceiver(), new TestReceiver());    thrown.expect(UserCodeException.class);    thrown.expectCause(instanceOf(IllegalArgumentException.class));    thrown.expectMessage("Unknown output tag");    userParDoFn.processElement(WindowedValue.valueInGlobalWindow(5));}
public boolean beam_f10627_0()
{    return true;}
public T beam_f10628_0(PCollectionView<T> view, final BoundedWindow window)
{    throw new IllegalArgumentException("calling getSideInput() with unknown view");}
public void beam_f10637_0() throws Exception
{    Coder<List<String>> coder = ListCoder.of(StringUtf8Coder.of());    ByteString.Output stream = ByteString.newOutput();    coder.encode(Arrays.asList("data1"), stream, Coder.Context.OUTER);    ByteString encodedIterable1 = stream.toByteString();    stream = ByteString.newOutput();    coder.encode(Arrays.asList("data2"), stream, Coder.Context.OUTER);    ByteString encodedIterable2 = stream.toByteString();    Cache<StateFetcher.SideInputId, StateFetcher.SideInputCacheEntry> cache = CacheBuilder.newBuilder().build();    StateFetcher fetcher = new StateFetcher(server, cache);    PCollectionView<String> view1 = TestPipeline.create().apply(Create.empty(StringUtf8Coder.of())).apply(View.<String>asSingleton());    PCollectionView<String> view2 = TestPipeline.create().apply(Create.empty(StringUtf8Coder.of())).apply(View.<String>asSingleton());    String tag1 = view1.getTagInternal().getId();    String tag2 = view2.getTagInternal().getId();            when(server.getSideInputData(any(Windmill.GlobalDataRequest.class))).thenReturn(buildGlobalDataResponse(tag1, ByteString.EMPTY, true, encodedIterable1), buildGlobalDataResponse(tag2, ByteString.EMPTY, true, encodedIterable2), buildGlobalDataResponse(tag1, ByteString.EMPTY, true, encodedIterable1));    assertEquals("data1", fetcher.fetchSideInput(view1, GlobalWindow.INSTANCE, STATE_FAMILY, SideInputState.UNKNOWN, readStateSupplier).orNull());    assertEquals("data2", fetcher.fetchSideInput(view2, GlobalWindow.INSTANCE, STATE_FAMILY, SideInputState.UNKNOWN, readStateSupplier).orNull());    cache.invalidateAll();    assertEquals("data1", fetcher.fetchSideInput(view1, GlobalWindow.INSTANCE, STATE_FAMILY, SideInputState.UNKNOWN, readStateSupplier).orNull());    assertEquals("data1", fetcher.fetchSideInput(view1, GlobalWindow.INSTANCE, STATE_FAMILY, SideInputState.UNKNOWN, readStateSupplier).orNull());    ArgumentCaptor<Windmill.GlobalDataRequest> captor = ArgumentCaptor.forClass(Windmill.GlobalDataRequest.class);    verify(server, times(3)).getSideInputData(captor.capture());    verifyNoMoreInteractions(server);    assertThat(captor.getAllValues(), contains(buildGlobalDataRequest(tag1, ByteString.EMPTY), buildGlobalDataRequest(tag2, ByteString.EMPTY), buildGlobalDataRequest(tag1, ByteString.EMPTY)));}
public void beam_f10638_0() throws Exception
{    StateFetcher fetcher = new StateFetcher(server);    ByteString encodedIterable = ByteString.EMPTY;    PCollectionView<Long> view = TestPipeline.create().apply(Create.empty(VarLongCoder.of())).apply(Sum.longsGlobally().asSingletonView());    String tag = view.getTagInternal().getId();            when(server.getSideInputData(any(Windmill.GlobalDataRequest.class))).thenReturn(buildGlobalDataResponse(tag, ByteString.EMPTY, true, encodedIterable));    assertEquals(0L, (long) fetcher.fetchSideInput(view, GlobalWindow.INSTANCE, STATE_FAMILY, SideInputState.UNKNOWN, readStateSupplier).orNull());    verify(server).getSideInputData(buildGlobalDataRequest(tag, ByteString.EMPTY));    verifyNoMoreInteractions(server);}
public void beam_f10647_0() throws Exception
{    MockitoAnnotations.initMocks(this);    wsp = new WorkerStatusPages(server, mockMemoryMonitor, mockHealthyIndicator);    server.addConnector(connector);    wsp.start();}
public void beam_f10648_0() throws Exception
{    wsp.stop();}
public Long beam_f10657_0()
{    return idGenerator.getAndIncrement();}
private String beam_f10658_0(int index)
{    return DEFAULT_KEY_STRING + index;}
private MapTask beam_f10667_0(List<ParallelInstruction> instructions)
{    MapTask mapTask = new MapTask().setStageName(DEFAULT_MAP_STAGE_NAME).setSystemName(DEFAULT_MAP_SYSTEM_NAME).setInstructions(instructions);    mapTask.setFactory(Transport.getJsonFactory());    return mapTask;}
private Windmill.GetWorkResponse beam_f10668_0(String input, byte[] metadata) throws Exception
{    Windmill.GetWorkResponse.Builder builder = Windmill.GetWorkResponse.newBuilder();    TextFormat.merge(input, builder);    if (metadata != null) {        Windmill.InputMessageBundle.Builder messageBundleBuilder = builder.getWorkBuilder(0).getWorkBuilder(0).getMessageBundlesBuilder(0);        for (Windmill.Message.Builder messageBuilder : messageBundleBuilder.getMessagesBuilderList()) {            messageBuilder.setMetadata(addPaneTag(PaneInfo.NO_FIRING, metadata));        }    }    return builder.build();}
private StreamingComputationConfig beam_f10677_0(List<ParallelInstruction> instructions)
{    StreamingComputationConfig config = new StreamingComputationConfig();    config.setComputationId(DEFAULT_COMPUTATION_ID);    config.setSystemName(DEFAULT_MAP_SYSTEM_NAME);    config.setStageName(DEFAULT_MAP_STAGE_NAME);    config.setInstructions(instructions);    return config;}
private ByteString beam_f10678_0(PaneInfo pane, byte[] windowBytes) throws CoderException, IOException
{    Output output = ByteString.newOutput();    PaneInfo.PaneInfoCoder.INSTANCE.encode(pane, output, Context.OUTER);    output.write(windowBytes);    return output.toByteString();}
public void beam_f10687_0() throws Exception
{    int expectedNumberOfThreads = 5;    List<ParallelInstruction> instructions = Arrays.asList(makeSourceInstruction(StringUtf8Coder.of()), makeDoFnInstruction(blockingFn, 0, StringUtf8Coder.of()), makeSinkInstruction(StringUtf8Coder.of(), 0));    FakeWindmillServer server = new FakeWindmillServer(errorCollector);    StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server);    options.setNumberOfWorkerHarnessThreads(expectedNumberOfThreads);    StreamingDataflowWorker worker = makeWorker(instructions, options, true);    worker.start();    for (int i = 0; i < expectedNumberOfThreads * 2; ++i) {        server.addWorkToOffer(makeInput(i, TimeUnit.MILLISECONDS.toMicros(i)));    }            BlockingFn.counter.acquire(expectedNumberOfThreads);        if (BlockingFn.counter.tryAcquire(500, TimeUnit.MILLISECONDS)) {        fail("Expected number of threads " + expectedNumberOfThreads + " does not match actual " + "number of work items processed concurrently " + BlockingFn.callCounter.get() + ".");    }    BlockingFn.blocker.countDown();}
public void beam_f10688_0(ProcessContext c)
{    if (!thrown) {        thrown = true;        throw new KeyTokenInvalidException("key");    } else {        c.output(c.element());    }}
public void beam_f10697_0() throws Exception
{    Duration gapDuration = Duration.standardSeconds(1);    CloudObject spec = CloudObject.forClassName("AssignWindowsDoFn");    SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environments.JAVA_SDK_HARNESS_ENVIRONMENT);    addString(spec, PropertyNames.SERIALIZED_FN, StringUtils.byteArrayToJsonString(WindowingStrategyTranslation.toMessageProto(WindowingStrategy.of(FixedWindows.of(gapDuration)), sdkComponents).toByteArray()));    ParallelInstruction addWindowsInstruction = new ParallelInstruction().setSystemName("AssignWindows").setName("AssignWindows").setOriginalName("AssignWindowsOriginal").setParDo(new ParDoInstruction().setInput(new InstructionInput().setProducerInstructionIndex(0).setOutputNum(0)).setNumOutputs(1).setUserFn(spec)).setOutputs(Arrays.asList(new InstructionOutput().setOriginalName(DEFAULT_OUTPUT_ORIGINAL_NAME).setSystemName(DEFAULT_OUTPUT_SYSTEM_NAME).setName("output").setCodec(CloudObjects.asCloudObject(WindowedValue.getFullCoder(StringUtf8Coder.of(), IntervalWindow.getCoder()), /*sdkComponents=*/    null))));    List<ParallelInstruction> instructions = Arrays.asList(makeSourceInstruction(StringUtf8Coder.of()), addWindowsInstruction, makeSinkInstruction(StringUtf8Coder.of(), 1));    FakeWindmillServer server = new FakeWindmillServer(errorCollector);    int timestamp1 = 0;    int timestamp2 = 1000000;    server.addWorkToOffer(makeInput(timestamp1, timestamp1));    server.addWorkToOffer(makeInput(timestamp2, timestamp2));    StreamingDataflowWorker worker = makeWorker(instructions, createTestingPipelineOptions(server), false);    worker.start();    Map<Long, Windmill.WorkItemCommitRequest> result = server.waitForAndGetCommits(2);    assertThat(result.get((long) timestamp1), equalTo(setMessagesMetadata(PaneInfo.NO_FIRING, intervalWindowBytes(WINDOW_AT_ZERO), makeExpectedOutput(timestamp1, timestamp1)).build()));    assertThat(result.get((long) timestamp2), equalTo(setMessagesMetadata(PaneInfo.NO_FIRING, intervalWindowBytes(WINDOW_AT_ONE_SECOND), makeExpectedOutput(timestamp2, timestamp2)).build()));}
private void beam_f10698_0(WorkItemCommitRequest commit, Timer... timers)
{    assertThat(commit.getOutputTimersList(), Matchers.containsInAnyOrder(timers));}
public void beam_f10707_0() throws Exception
{        runMergeSessionsActions(Arrays.asList(new Action(buildSessionInput(1, 40, 0, Arrays.asList(1L), Collections.EMPTY_LIST)).withHolds(buildHold("/gAAAAAAAAAsK/+uhold", -1, true), buildHold("/gAAAAAAAAAsK/+uextra", -1, true)).withTimers(buildWatermarkTimer("/s/gAAAAAAAAAsK/+0", 3600010))));                            runMergeSessionsActions(Arrays.asList(new Action(buildSessionInput(1, 0, 0, Arrays.asList(1L), Collections.EMPTY_LIST)).withHolds(buildHold("/gAAAAAAAAAsK/+uhold", 10, false)).withTimers(buildWatermarkTimer("/s/gAAAAAAAAAsK/+0", 10), buildWatermarkTimer("/s/gAAAAAAAAAsK/+0", 3600010)), new Action(buildSessionInput(2, 30, 0, Collections.EMPTY_LIST, Arrays.asList(buildWatermarkTimer("/s/gAAAAAAAAAsK/+0", 10)))).withTimers(buildWatermarkTimer("/s/gAAAAAAAAAsK/+0", 3600010)).withHolds(buildHold("/gAAAAAAAAAsK/+uhold", -1, true), buildHold("/gAAAAAAAAAsK/+uextra", -1, true)), new Action(buildSessionInput(3, 30, 0, Arrays.asList(8L), Collections.EMPTY_LIST)).withTimers(buildWatermarkTimer("/s/gAAAAAAAABIR/+0", 3600017), buildWatermarkTimer("/s/gAAAAAAAAAsK/+0", 10, true), buildWatermarkTimer("/s/gAAAAAAAAAsK/+0", 3600010, true)).withHolds(buildHold("/gAAAAAAAAAsK/+uhold", -1, true), buildHold("/gAAAAAAAAAsK/+uextra", -1, true)), new Action(buildSessionInput(4, 30, 0, Arrays.asList(31L), Collections.EMPTY_LIST)).withTimers(buildWatermarkTimer("/s/gAAAAAAAACkK/+0", 3600040), buildWatermarkTimer("/s/gAAAAAAAACkK/+0", 40)).withHolds(buildHold("/gAAAAAAAACkK/+uhold", 40, false)), new Action(buildSessionInput(5, 30, 0, Arrays.asList(17L, 23L), Collections.EMPTY_LIST)).withTimers(buildWatermarkTimer("/s/gAAAAAAAACkK/+0", 3600040, true), buildWatermarkTimer("/s/gAAAAAAAACkK/+0", 40, true), buildWatermarkTimer("/s/gAAAAAAAABIR/+0", 3600017, true), buildWatermarkTimer("/s/gAAAAAAAABIR/+0", 17, true), buildWatermarkTimer("/s/gAAAAAAAACko/+0", 40), buildWatermarkTimer("/s/gAAAAAAAACko/+0", 3600040)).withHolds(buildHold("/gAAAAAAAACkK/+uhold", -1, true), buildHold("/gAAAAAAAACkK/+uextra", -1, true), buildHold("/gAAAAAAAAAsK/+uhold", 40, true), buildHold("/gAAAAAAAAAsK/+uextra", 3600040, true)), new Action(buildSessionInput(6, 50, 0, Collections.EMPTY_LIST, Arrays.asList(buildWatermarkTimer("/s/gAAAAAAAACko/+0", 40)))).withTimers(buildWatermarkTimer("/s/gAAAAAAAACko/+0", 3600040)).withHolds(buildHold("/gAAAAAAAAAsK/+uhold", -1, true), buildHold("/gAAAAAAAAAsK/+uextra", -1, true))));}
private static CounterUpdate beam_f10708_0(Iterable<CounterUpdate> counters, String name)
{    for (CounterUpdate counter : counters) {        if (counter.getNameAndKind().getName().equals(name)) {            return counter;        }    }    return null;}
public void beam_f10718_0(ProcessContext c)
{    StringBuilder builder = new StringBuilder(1000000);    for (int i = 0; i < 1000000; i++) {        builder.append(' ');    }    String largeString = builder.toString();    for (int i = 0; i < 3000; i++) {        c.output(largeString);    }}
public void beam_f10719_0() throws Exception
{    List<ParallelInstruction> instructions = Arrays.asList(makeSourceInstruction(StringUtf8Coder.of()), makeDoFnInstruction(new FanoutFn(), 0, StringUtf8Coder.of()), makeSinkInstruction(StringUtf8Coder.of(), 0));    FakeWindmillServer server = new FakeWindmillServer(errorCollector);    StreamingDataflowWorkerOptions options = createTestingPipelineOptions(server);    StreamingDataflowWorker worker = makeWorker(instructions, options, true);    worker.start();    server.addWorkToOffer(makeInput(0, TimeUnit.MILLISECONDS.toMicros(0)));    server.waitForAndGetCommits(0);    worker.stop();}
private void beam_f10728_0(WorkItem.Builder workItem, IntervalWindow window, Instant timestamp, Windmill.Timer.Type type)
{    StateNamespace namespace = StateNamespaces.window(windowCoder, window);    workItem.getTimersBuilder().addTimersBuilder().setTag(WindmillTimerInternals.timerTag(WindmillNamespacePrefix.SYSTEM_NAMESPACE_PREFIX, TimerData.of(namespace, timestamp, type == Windmill.Timer.Type.WATERMARK ? TimeDomain.EVENT_TIME : TimeDomain.PROCESSING_TIME))).setTimestamp(WindmillTimeUtils.harnessToWindmillTimestamp(timestamp)).setType(type).setStateFamily(STATE_FAMILY);}
private void beam_f10729_0(InputMessageBundle.Builder messageBundle, Collection<IntervalWindow> windows, Instant timestamp, Coder<V> valueCoder, V value) throws IOException
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Coder<Collection<? extends BoundedWindow>> windowsCoder = (Coder) CollectionCoder.of(windowCoder);    ByteString.Output dataOutput = ByteString.newOutput();    valueCoder.encode(value, dataOutput, Context.OUTER);    messageBundle.addMessagesBuilder().setMetadata(WindmillSink.encodeMetadata(windowsCoder, windows, PaneInfo.NO_FIRING)).setData(dataOutput.toByteString()).setTimestamp(WindmillTimeUtils.harnessToWindmillTimestamp(timestamp));}
public Long beam_f10738_0(Long accumulator, Long input)
{    return accumulator + input;}
public Long beam_f10739_0(Iterable<Long> accumulators)
{    Long sum = 0L;    for (Long value : accumulators) {        sum += value;    }    return sum;}
public StateInternals beam_f10748_0()
{    return null;}
public void beam_f10749_0() throws Exception
{    TupleTag<KV<String, Iterable<String>>> outputTag = new TupleTag<>();    ListOutputManager outputManager = new ListOutputManager();    DoFnRunner<KeyedWorkItem<String, String>, KV<String, Iterable<String>>> runner = makeRunner(outputTag, outputManager, WindowingStrategy.of(FixedWindows.of(Duration.millis(10))));    runner.startBundle();    runner.finishBundle();    List<?> result = outputManager.getOutput(outputTag);    assertEquals(0, result.size());}
public void beam_f10758_0() throws Exception
{    ListOutputManager outputManager = new ListOutputManager();    StreamingKeyedWorkItemSideInputDoFnRunner<String, Integer, KV<String, Integer>, IntervalWindow> runner = createRunner(outputManager);    runner.keyValue().write("a");    Set<IntervalWindow> readyWindows = ImmutableSet.of(window(10, 20));    when(sideInputFetcher.getReadyWindows()).thenReturn(readyWindows);    when(sideInputFetcher.prefetchElements(readyWindows)).thenReturn(ImmutableList.of(elemsBag));    when(sideInputFetcher.prefetchTimers(readyWindows)).thenReturn(ImmutableList.of(timersBag));    when(elemsBag.read()).thenReturn(ImmutableList.of(createDatum(13, 13L), createDatum(18, 18L)));    when(timersBag.read()).thenReturn(ImmutableList.of(timerData(window(10, 20), new Instant(19), Timer.Type.WATERMARK)));    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(new Instant(20));    runner.startBundle();    List<WindowedValue<KV<String, Integer>>> result = outputManager.getOutput(mainOutputTag);    assertEquals(1, result.size());    WindowedValue<KV<String, Integer>> item0 = result.get(0);    assertEquals("a", item0.getValue().getKey());    assertEquals(31, item0.getValue().getValue().intValue());}
private WindowedValue<T> beam_f10759_0(T element, long timestampMillis)
{    Instant timestamp = new Instant(timestampMillis);    return WindowedValue.of(element, timestamp, Arrays.asList(WINDOW_FN.assignWindow(timestamp)), PaneInfo.NO_FIRING);}
public void beam_f10768_0()
{    MetricsContainer metricsContainer = Mockito.mock(MetricsContainer.class);    ProfileScope profileScope = Mockito.mock(ProfileScope.class);    ExecutionState start1 = executionContext.executionStateRegistry.getState(NameContext.create("stage", "original-1", "system-1", "user-1"), ExecutionStateTracker.START_STATE_NAME, metricsContainer, profileScope);    ExecutionState process1 = executionContext.executionStateRegistry.getState(NameContext.create("stage", "original-1", "system-1", "user-1"), ExecutionStateTracker.PROCESS_STATE_NAME, metricsContainer, profileScope);    ExecutionState start2 = executionContext.executionStateRegistry.getState(NameContext.create("stage", "original-2", "system-2", "user-2"), ExecutionStateTracker.START_STATE_NAME, metricsContainer, profileScope);    ExecutionState other = executionContext.executionStateRegistry.getState(NameContext.forStage("stage"), "other", null, NoopProfileScope.NOOP);    other.takeSample(120);    start1.takeSample(100);    process1.takeSample(500);    assertThat(executionStateRegistry.extractUpdates(false), containsInAnyOrder(msecStage("other-msecs", "stage", 120), msec("start-msecs", "stage", "original-1", 100), msec("process-msecs", "stage", "original-1", 500)));    process1.takeSample(200);    start2.takeSample(200);    assertThat(executionStateRegistry.extractUpdates(false), containsInAnyOrder(msec("process-msecs", "stage", "original-1", 200), msec("start-msecs", "stage", "original-2", 200)));    process1.takeSample(300);    assertThat(executionStateRegistry.extractUpdates(false), containsInAnyOrder(msec("process-msecs", "stage", "original-1", 300)));}
private CounterUpdate beam_f10769_0(String counterName, String stageName, String originalStepName, long value)
{    return new CounterUpdate().setStructuredNameAndMetadata(new CounterStructuredNameAndMetadata().setName(new CounterStructuredName().setOrigin("SYSTEM").setName(counterName).setOriginalStepName(originalStepName).setExecutionStepName(stageName)).setMetadata(new CounterMetadata().setKind("SUM"))).setCumulative(false).setInteger(longToSplitInt(value));}
public void beam_f10778_0() throws Exception
{    PCollectionView<String> view = createView();    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10));    Windmill.GlobalDataId id = Windmill.GlobalDataId.newBuilder().setTag(view.getTagInternal().getId()).setVersion(ByteString.copyFrom(CoderUtils.encodeToByteArray(IntervalWindow.getCoder(), window))).build();    Set<Windmill.GlobalDataRequest> requestSet = new HashSet<>();    requestSet.add(Windmill.GlobalDataRequest.newBuilder().setDataId(id).build());    Map<IntervalWindow, Set<Windmill.GlobalDataRequest>> blockedMap = new HashMap<>();    blockedMap.put(window, requestSet);    ValueState<Map<IntervalWindow, Set<GlobalDataRequest>>> blockedMapState = state.state(StateNamespaces.global(), StreamingSideInputFetcher.blockedMapAddr(WINDOW_FN.windowCoder()));    blockedMapState.write(blockedMap);    ListOutputManager outputManager = new ListOutputManager();    List<PCollectionView<String>> views = Arrays.asList(view);    StreamingSideInputFetcher<String, IntervalWindow> sideInputFetcher = createFetcher(views);    StreamingSideInputDoFnRunner<String, String, IntervalWindow> runner = createRunner(outputManager, views, sideInputFetcher);    sideInputFetcher.watermarkHold(createWindow(0)).add(new Instant(0));    sideInputFetcher.elementBag(createWindow(0)).add(createDatum("e", 0));    when(stepContext.getSideInputNotifications()).thenReturn(Arrays.asList(id));    when(stepContext.issueSideInputFetch(eq(view), any(BoundedWindow.class), eq(SideInputState.UNKNOWN))).thenReturn(false);    when(stepContext.issueSideInputFetch(eq(view), any(BoundedWindow.class), eq(SideInputState.KNOWN_READY))).thenReturn(true);    when(execContext.getSideInputReaderForViews(Mockito.<Iterable<? extends PCollectionView<?>>>any())).thenReturn(mockSideInputReader);    when(mockSideInputReader.contains(eq(view))).thenReturn(true);    when(mockSideInputReader.get(eq(view), any(BoundedWindow.class))).thenReturn("data");    runner.startBundle();    runner.finishBundle();    assertThat(outputManager.getOutput(mainOutputTag), contains(createDatum("e:data", 0)));    assertThat(blockedMapState.read(), Matchers.nullValue());    assertThat(sideInputFetcher.watermarkHold(createWindow(0)).read(), Matchers.nullValue());    assertThat(sideInputFetcher.elementBag(createWindow(0)).read(), Matchers.emptyIterable());}
public void beam_f10779_0() throws Exception
{    PCollectionView<String> view1 = createView();    PCollectionView<String> view2 = createView();    IntervalWindow window = new IntervalWindow(new Instant(0), new Instant(10));    Windmill.GlobalDataId id = Windmill.GlobalDataId.newBuilder().setTag(view1.getTagInternal().getId()).setVersion(ByteString.copyFrom(CoderUtils.encodeToByteArray(IntervalWindow.getCoder(), window))).build();    Set<Windmill.GlobalDataRequest> requestSet = new HashSet<>();    requestSet.add(Windmill.GlobalDataRequest.newBuilder().setDataId(id).build());    Map<IntervalWindow, Set<Windmill.GlobalDataRequest>> blockedMap = new HashMap<>();    blockedMap.put(window, requestSet);    ValueState<Map<IntervalWindow, Set<GlobalDataRequest>>> blockedMapState = state.state(StateNamespaces.global(), StreamingSideInputFetcher.blockedMapAddr(WINDOW_FN.windowCoder()));    blockedMapState.write(blockedMap);    when(stepContext.getSideInputNotifications()).thenReturn(Arrays.asList(id));    when(stepContext.issueSideInputFetch(any(PCollectionView.class), any(BoundedWindow.class), any(SideInputState.class))).thenReturn(true);    when(execContext.getSideInputReaderForViews(Mockito.<Iterable<? extends PCollectionView<?>>>any())).thenReturn(mockSideInputReader);    when(mockSideInputReader.contains(eq(view1))).thenReturn(true);    when(mockSideInputReader.contains(eq(view2))).thenReturn(true);    when(mockSideInputReader.get(eq(view1), any(BoundedWindow.class))).thenReturn("data1");    when(mockSideInputReader.get(eq(view2), any(BoundedWindow.class))).thenReturn("data2");    ListOutputManager outputManager = new ListOutputManager();    List<PCollectionView<String>> views = Arrays.asList(view1, view2);    StreamingSideInputFetcher<String, IntervalWindow> sideInputFetcher = createFetcher(views);    StreamingSideInputDoFnRunner<String, String, IntervalWindow> runner = createRunner(outputManager, views, sideInputFetcher);    sideInputFetcher.watermarkHold(createWindow(0)).add(new Instant(0));    sideInputFetcher.elementBag(createWindow(0)).add(createDatum("e1", 0));    runner.startBundle();    runner.processElement(createDatum("e2", 2));    runner.finishBundle();    assertThat(outputManager.getOutput(mainOutputTag), contains(createDatum("e1:data1:data2", 0), createDatum("e2:data1:data2", 2)));    assertThat(blockedMapState.read(), Matchers.nullValue());    assertThat(sideInputFetcher.watermarkHold(createWindow(0)).read(), Matchers.nullValue());    assertThat(sideInputFetcher.elementBag(createWindow(0)).read(), Matchers.emptyIterable());}
public void beam_f10788_0() throws Exception
{    PCollectionView<String> view = createView();    IntervalWindow readyWindow = createWindow(0);    Windmill.GlobalDataId id = Windmill.GlobalDataId.newBuilder().setTag(view.getTagInternal().getId()).setVersion(ByteString.copyFrom(CoderUtils.encodeToByteArray(IntervalWindow.getCoder(), readyWindow))).build();    when(stepContext.getSideInputNotifications()).thenReturn(Arrays.<Windmill.GlobalDataId>asList(id));    when(stepContext.issueSideInputFetch(eq(view), any(BoundedWindow.class), eq(SideInputState.UNKNOWN))).thenReturn(false);    when(stepContext.issueSideInputFetch(eq(view), any(BoundedWindow.class), eq(SideInputState.KNOWN_READY))).thenReturn(true);    StreamingSideInputFetcher<String, IntervalWindow> fetcher = createFetcher(Arrays.asList(view));        WindowedValue<String> datum1 = createDatum("e1", 0);    assertTrue(fetcher.storeIfBlocked(datum1));    assertThat(fetcher.getBlockedWindows(), Matchers.contains(createWindow(0)));    WindowedValue<String> datum2 = createDatum("e2", 0);    assertTrue(fetcher.storeIfBlocked(datum2));    assertThat(fetcher.getBlockedWindows(), Matchers.contains(createWindow(0)));    WindowedValue<String> datum3 = createDatum("e3", 10);    assertTrue(fetcher.storeIfBlocked(datum3));    assertThat(fetcher.getBlockedWindows(), Matchers.containsInAnyOrder(createWindow(0), createWindow(10)));    TimerData timer1 = createTimer(0);    assertTrue(fetcher.storeIfBlocked(timer1));    TimerData timer2 = createTimer(15);    assertTrue(fetcher.storeIfBlocked(timer2));        assertThat(fetcher.getReadyWindows(), Matchers.contains(readyWindow));    Set<WindowedValue<String>> actualElements = Sets.newHashSet();    for (BagState<WindowedValue<String>> bag : fetcher.prefetchElements(ImmutableList.of(readyWindow))) {        for (WindowedValue<String> elem : bag.read()) {            actualElements.add(elem);        }        bag.clear();    }    assertThat(actualElements, Matchers.containsInAnyOrder(datum1, datum2));    Set<TimerData> actualTimers = Sets.newHashSet();    for (BagState<TimerData> bag : fetcher.prefetchTimers(ImmutableList.of(readyWindow))) {        for (TimerData timer : bag.read()) {            actualTimers.add(timer);        }        bag.clear();    }    assertThat(actualTimers, Matchers.contains(timer1));        fetcher.releaseBlockedWindows(ImmutableList.of(readyWindow));    assertThat(fetcher.getBlockedWindows(), Matchers.contains(createWindow(10)));        Set<WindowedValue<String>> restElements = Sets.newHashSet();    for (BagState<WindowedValue<String>> bag : fetcher.prefetchElements(ImmutableList.of(createWindow(10), createWindow(15)))) {        for (WindowedValue<String> elem : bag.read()) {            restElements.add(elem);        }    }    assertThat(restElements, Matchers.contains(datum3));    Set<TimerData> restTimers = Sets.newHashSet();    for (BagState<TimerData> bag : fetcher.prefetchTimers(ImmutableList.of(createWindow(10), createWindow(15)))) {        for (TimerData timer : bag.read()) {            restTimers.add(timer);        }    }    assertThat(restTimers, Matchers.contains(timer2));}
private StreamingSideInputFetcher<String, IntervalWindow> beam_f10789_0(List<PCollectionView<String>> views) throws Exception
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Iterable<PCollectionView<?>> typedViews = (Iterable) views;    return new StreamingSideInputFetcher<String, IntervalWindow>(typedViews, StringUtf8Coder.of(), WindowingStrategy.of(WINDOW_FN), stepContext);}
protected void beam_f10798_0()
{    DataflowWorkerLoggingMDC.setJobId(previousJobId);    DataflowWorkerLoggingMDC.setStageName(previousStageName);    DataflowWorkerLoggingMDC.setWorkerId(previousWorkerId);    DataflowWorkerLoggingMDC.setWorkId(previousWorkId);}
public void beam_f10799_0() throws Throwable
{        TestRule restoreMDC = new RestoreDataflowLoggingMDC();    final boolean[] evaluateRan = new boolean[1];    DataflowWorkerLoggingMDC.setJobId("oldJobId");    DataflowWorkerLoggingMDC.setStageName("oldStageName");    DataflowWorkerLoggingMDC.setWorkerId("oldWorkerId");    DataflowWorkerLoggingMDC.setWorkId("oldWorkId");    restoreMDC.apply(new Statement() {        @Override        public void evaluate() {            evaluateRan[0] = true;                        assertNull("null JobId", DataflowWorkerLoggingMDC.getJobId());            assertNull("null StageName", DataflowWorkerLoggingMDC.getStageName());            assertNull("null WorkerId", DataflowWorkerLoggingMDC.getWorkerId());            assertNull("null WorkId", DataflowWorkerLoggingMDC.getWorkId());                        DataflowWorkerLoggingMDC.setJobId("newJobId");            DataflowWorkerLoggingMDC.setStageName("newStageName");            DataflowWorkerLoggingMDC.setWorkerId("newWorkerId");            DataflowWorkerLoggingMDC.setWorkId("newWorkId");                        assertEquals("newJobId", DataflowWorkerLoggingMDC.getJobId());            assertEquals("newStageName", DataflowWorkerLoggingMDC.getStageName());            assertEquals("newWorkerId", DataflowWorkerLoggingMDC.getWorkerId());            assertEquals("newWorkId", DataflowWorkerLoggingMDC.getWorkId());        }    }, Description.EMPTY).evaluate();        assertTrue(evaluateRan[0]);    assertEquals("oldJobId", DataflowWorkerLoggingMDC.getJobId());    assertEquals("oldStageName", DataflowWorkerLoggingMDC.getStageName());    assertEquals("oldWorkerId", DataflowWorkerLoggingMDC.getWorkerId());    assertEquals("oldWorkId", DataflowWorkerLoggingMDC.getWorkId());}
public void beam_f10808_0()
{    if (finalizeTracker != null) {        finalizeTracker.add(current);    }}
public Coder<CounterMark> beam_f10809_0()
{    return DelegateCoder.of(VarIntCoder.of(), input -> input.current, CounterMark::new);}
public CounterMark beam_f10819_1()
{    if (throwOnFirstSnapshot && !thrown) {        thrown = true;                throw new RuntimeException("failed during checkpoint");    }        return new CounterMark(current);}
public long beam_f10820_0()
{    return 7L;}
public static TestOperationContext beam_f10831_0(CounterSet counterSet)
{    return new TestOperationContext(counterSet, NameContextsForTests.nameContextForTest(), new MetricsContainerImpl(NameContextsForTests.SYSTEM_NAME), ExecutionStateTracker.newForTest());}
public static TestOperationContext beam_f10832_0(CounterSet counterSet, NameContext nameContext)
{    return new TestOperationContext(counterSet, nameContext, new MetricsContainerImpl(nameContext.systemName()), ExecutionStateTracker.newForTest());}
public Map<String, ShuffleReadCounter> beam_f10841_0()
{    return originalShuffleStepToCounter;}
public void beam_f10842_0(String key, String secondaryKey, String value)
{    addEntry(new ShuffleEntry(key.getBytes(StandardCharsets.UTF_8), secondaryKey.getBytes(StandardCharsets.UTF_8), value.getBytes(StandardCharsets.UTF_8)));}
public boolean beam_f10851_0()
{    return lastReturnedIndex < items.size() - 1;}
public T beam_f10852_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    lastReturnedIndex++;    return items.get(lastReturnedIndex);}
public void beam_f10861_0()
{    runTest(IN_RANGE_ENTRIES, NO_ENTRIES, START_KEY, END_KEY);}
public void beam_f10862_0()
{    runTest(IN_RANGE_ENTRIES, OUT_OF_RANGE_ENTRIES, START_KEY, END_KEY);}
public void beam_f10871_0() throws Exception
{    runTestReadFromShuffle(TestUtils.NO_INTS);}
public void beam_f10872_0() throws Exception
{    runTestReadFromShuffle(TestUtils.INTS);}
private CloudObject beam_f10882_0(DoFn<?, ?> fn)
{    return getCloudObject(fn, WindowingStrategy.globalDefault());}
private CloudObject beam_f10883_0(DoFn<?, ?> fn, WindowingStrategy<?, ?> windowingStrategy)
{    CloudObject object = CloudObject.forClassName("DoFn");    @SuppressWarnings({ "rawtypes", "unchecked" })    DoFnInfo<?, ?> info = DoFnInfo.forFn(fn, windowingStrategy, null, /* side input views */    null, /* input coder */    new TupleTag<>(PropertyNames.OUTPUT), /* main output id */    DoFnSchemaInformation.create(), Collections.emptyMap());    object.set(PropertyNames.SERIALIZED_FN, StringUtils.byteArrayToJsonString(SerializableUtils.serializeToByteArray(info)));    return object;}
public void beam_f10892_0() throws Exception
{    AppliedCombineFn<String, Long, ?, Long> appliedFn = AppliedCombineFn.withInputCoder(Sum.ofLongs(), CoderRegistry.createDefault(), KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of()));    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy.of(Sessions.withGapDuration(Duration.millis(10))).withTrigger(AfterPane.elementCountAtLeast(1));    assertThat(BatchGroupAlsoByWindowsDoFns.create(windowingStrategy, appliedFn), instanceOf(BatchGroupAlsoByWindowAndCombineFn.class));}
public void beam_f10893_0() throws Exception
{    Coder<Long> inputCoder = VarLongCoder.of();    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy.of(FixedWindows.of(Duration.millis(10))).withTrigger(new ReshuffleTrigger<>());    assertThat(BatchGroupAlsoByWindowsDoFns.createForIterable(windowingStrategy, new InMemoryStateInternalsFactory<Long>(), inputCoder), instanceOf(BatchGroupAlsoByWindowReshuffleFn.class));}
public void beam_f10902_0() throws Exception
{        CloudObject grandparent = CloudObject.forClassName("text");    addString(grandparent, "G", "g_g");    addString(grandparent, "GP", "gp_g");    addString(grandparent, "GC", "gc_g");    addString(grandparent, "GPC", "gpc_g");    CloudObject parent = CloudObject.forClassName("text");    addString(parent, "P", "p_p");    addString(parent, "PC", "pc_p");    addString(parent, "GP", "gp_p");    addString(parent, "GPC", "gpc_p");    CloudObject child = CloudObject.forClassName("text");    addString(child, "C", "c_c");    addString(child, "PC", "pc_c");    addString(child, "GC", "gc_c");    addString(child, "GPC", "gpc_c");    Source source = new Source();    source.setBaseSpecs(new ArrayList<Map<String, Object>>());    source.getBaseSpecs().add(grandparent);    source.getBaseSpecs().add(parent);    source.setSpec(child);    source.setCodec(CloudObjects.asCloudObject(StringUtf8Coder.of(), /*sdkComponents=*/    null));    Source flat = CloudSourceUtils.flattenBaseSpecs(source);    assertNull(flat.getBaseSpecs());    assertEquals(StringUtf8Coder.class.getName(), getString(flat.getCodec(), PropertyNames.OBJECT_TYPE_NAME));    CloudObject flatSpec = CloudObject.fromSpec(flat.getSpec());    assertEquals("g_g", getString(flatSpec, "G"));    assertEquals("p_p", getString(flatSpec, "P"));    assertEquals("c_c", getString(flatSpec, "C"));    assertEquals("gp_p", getString(flatSpec, "GP"));    assertEquals("gc_c", getString(flatSpec, "GC"));    assertEquals("pc_c", getString(flatSpec, "PC"));    assertEquals("gpc_c", getString(flatSpec, "GPC"));}
public BatchGroupAlsoByWindowFn<K, InputT, OutputT> beam_f10903_0(WindowingStrategy<?, W> windowingStrategy, StateInternalsFactory<K> stateInternalsFactory)
{    return new BatchGroupAlsoByWindowViaOutputBufferFn<>(windowingStrategy, stateInternalsFactory, SystemReduceFn.<K, InputT, AccumT, OutputT, W>combining(keyCoder, combineFn));}
public void beam_f10912_0()
{    TaggedReiteratorList iter = create(new String[] {}, new String[] { "a", "b", "c" }, new String[] {}, new String[] {}, new String[] { "d" });    assertEquals(iter.get(2));    assertEquals(iter.get(1), "a", "b", "c");    assertEquals(iter.get(2));    assertEquals(iter.get(0));    assertEquals(iter.get(2));    assertEquals(iter.get(4), "d");    assertEquals(iter.get(3));}
private TaggedReiteratorList beam_f10913_0(String[]... values)
{    ArrayList<TaggedValue> taggedValues = new ArrayList<>();    for (int tag = 0; tag < values.length; tag++) {        for (String value : values[tag]) {            taggedValues.add(new TaggedValue(tag, value));        }    }    return new TaggedReiteratorList(new TestReiterator(taggedValues.toArray(new TaggedValue[0])), new TaggedValueExtractor());}
public TestReiterator beam_f10922_0()
{    return new TestReiterator(values, pos);}
public int beam_f10923_0(TaggedValue elem)
{    return elem.tag;}
public void beam_f10932_0() throws IOException, ExecutionException
{    ShuffleBatchReader base = mock(ShuffleBatchReader.class);    CachingShuffleBatchReader reader = new CachingShuffleBatchReader(base);    when(base.read(null, null)).thenReturn(testBatch);    ShuffleBatchReader.Batch read = reader.read(null, null);    assertThat(read, equalTo(testBatch));    verify(base, times(1)).read(null, null);    CachingShuffleBatchReader.BatchRange range = new CachingShuffleBatchReader.BatchRange(null, null);    CachingShuffleBatchReader.Batch batch = reader.cache.get(range);    assertThat(batch, notNullValue());    reader.cache.invalidateAll();    read = reader.read(null, null);    assertThat(read, equalTo(testBatch));    verify(base, times(2)).read(null, null);}
private static OutputReceiver[] beam_f10933_0(int numOutputs, CounterSet counterSet)
{    OutputReceiver[] receivers = new OutputReceiver[numOutputs];    for (int i = 0; i < numOutputs; i++) {        receivers[i] = new TestOutputReceiver("out_" + i, new ElementByteSizeObservableCoder(StringUtf8Coder.of()), counterSet);    }    return receivers;}
public SinkWriter<String> beam_f10942_0()
{    return new TestSinkWriter();}
public long beam_f10943_0(String outputElem)
{    outputElems.add(outputElem);    return outputElem.length();}
public T beam_f10952_0()
{    T res = entries.get(nextIndex);    nextIndex++;    return res;}
public void beam_f10953_0()
{    throw new UnsupportedOperationException();}
private static ByteArrayShufflePosition beam_f10966_0(int... bytes)
{    byte[] b = new byte[bytes.length];    for (int i = 0; i < bytes.length; ++i) {        b[i] = (byte) bytes[i];    }    return ByteArrayShufflePosition.of(b);}
public void beam_f10967_0() throws Exception
{    GroupingShuffleRangeTracker tracker = new GroupingShuffleRangeTracker(null, null);    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(1, 2, 3)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(1, 2, 5)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(3, 6, 8, 10)));}
public void beam_f10976_0() throws Exception
{    GroupingShuffleRangeTracker tracker = new GroupingShuffleRangeTracker(ofBytes(0, 0, 0), ofBytes(10, 20, 30));        assertFalse(tracker.trySplitAtPosition(ofBytes(0, 0, 0)));    assertFalse(tracker.trySplitAtPosition(ofBytes(3, 4, 5, 6)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(1, 2, 3)));        assertTrue(tracker.trySplitAtPosition(ofBytes(3, 4, 5, 6)));        assertFalse(tracker.trySplitAtPosition(ofBytes(3, 4, 5, 6)));    assertFalse(tracker.trySplitAtPosition(ofBytes(3, 4, 5, 6, 7)));    assertFalse(tracker.trySplitAtPosition(ofBytes(4, 5, 6, 7)));        assertTrue(tracker.trySplitAtPosition(ofBytes(3, 2, 1)));        assertFalse(tracker.trySplitAtPosition(ofBytes(1, 2, 3)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(2, 3, 4)));    assertTrue(tracker.tryReturnRecordAt(true, ofBytes(3, 2, 0)));    assertFalse(tracker.tryReturnRecordAt(true, ofBytes(3, 2, 1)));}
public void beam_f10977_0() throws Exception
{    GroupingTableBase<String, String, List<String>> table = (GroupingTableBase<String, String, List<String>>) GroupingTables.buffering(new IdentityGroupingKeyCreator(), new KvPairInfo(), new StringPowerSizeEstimator(), new StringPowerSizeEstimator());    table.setMaxSize(1000);    TestOutputReceiver receiver = new TestOutputReceiver(KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(StringUtf8Coder.of())), NameContextsForTests.nameContextForTest());    table.put("A", "a", receiver);    table.put("B", "b1", receiver);    table.put("B", "b2", receiver);    table.put("C", "c", receiver);    assertThat(receiver.outputElems, empty());    table.put("C", "cccc", receiver);    assertThat(receiver.outputElems, hasItem((Object) KV.of("C", Arrays.asList("c", "cccc"))));    table.put("DDDD", "d", receiver);    assertThat(receiver.outputElems, hasItem((Object) KV.of("DDDD", Arrays.asList("d"))));    table.flush(receiver);    assertThat(receiver.outputElems, IsIterableContainingInAnyOrder.<Object>containsInAnyOrder(KV.of("A", Arrays.asList("a")), KV.of("B", Arrays.asList("b1", "b2")), KV.of("C", Arrays.asList("c", "cccc")), KV.of("DDDD", Arrays.asList("d"))));}
public void beam_f10986_0() throws Exception
{        List<Long> sizes = Arrays.asList(1L, 10L, 100L, 1000L);    IdentitySizeEstimator underlying = new IdentitySizeEstimator();    SizeEstimator<Long> estimator = new SamplingSizeEstimator<Long>(underlying, 0.1, 0.2, 10, new Random(1));        for (int k = 0; k < 10; k++) {        long size = sizes.get(k % sizes.size());        assertEquals(size, estimator.estimateSize(size));        assertEquals(k + 1, underlying.calls);    }        for (int k = 10; k < 20; k++) {        long size = sizes.get(k % sizes.size());        assertEquals(size, estimator.estimateSize(size));        assertEquals(k + 1, underlying.calls);    }        for (int k = 20; k < 500; k++) {        estimator.estimateSize(sizes.get(k % sizes.size()));    }        int initialCalls = underlying.calls;    for (int k = 500; k < 1500; k++) {        long size = sizes.get(k % sizes.size());        assertThat(estimator.estimateSize(size), anyOf(is(in(sizes)), between(250L, 350L)));    }    assertThat(underlying.calls - initialCalls, between(180, 220));        for (int k = 1500; k < 3000; k++) {        estimator.estimateSize(sizes.get(k % sizes.size()));    }        initialCalls = underlying.calls;    for (int k = 3000; k < 4000; k++) {        long size = sizes.get(k % sizes.size());        assertThat(estimator.estimateSize(size), anyOf(is(in(sizes)), between(250L, 350L)));    }    assertThat(underlying.calls - initialCalls, between(90, 110));}
public void beam_f10987_0() throws Exception
{    IdentitySizeEstimator underlying = new IdentitySizeEstimator();    SizeEstimator<Long> estimator = new SamplingSizeEstimator<Long>(underlying, 0.05, 1.0, 10, new Random(1));        for (int k = 0; k < 10; k++) {        assertEquals(100, estimator.estimateSize(100L));        assertEquals(k + 1, underlying.calls);    }        for (int k = 10; k < 20; k++) {        assertEquals(100, estimator.estimateSize(100L));    }    assertThat(underlying.calls, between(11, 19));    int initialCalls = underlying.calls;        for (int k = 20; k < 1020; k++) {        assertEquals(100, estimator.estimateSize(100L));    }    assertThat(underlying.calls - initialCalls, between(40, 60));        while (estimator.estimateSize(1000000L) == 100) {    }        assertEquals(99, estimator.estimateSize(99L));}
public Object beam_f10996_0(Object key, Object value)
{    return KV.of(key, value);}
public void beam_f10997_0() throws Exception
{    aborted = true;    super.abort();}
public void beam_f11011_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    options.as(DataflowPipelineDebugOptions.class).setExperiments(Lists.newArrayList(DataflowElementExecutionTracker.TIME_PER_ELEMENT_EXPERIMENT));    ExecutionStateSampler stateSampler = ExecutionStateSampler.newForTest();    DataflowExecutionStateTracker stateTracker = new DataflowExecutionStateTracker(stateSampler, new TestDataflowExecutionState(NameContext.forStage("test-stage"), "other", null, /* requestingStepName */    null, /* sideInputIndex */    null, /* metricsContainer */    NoopProfileScope.NOOP), counterSet, options, "test-work-item-id");    NameContext parDoName = nameForStep("s1");            ReadOperation read = ReadOperation.forTest(new TestReader("a", "b", "c"), new OutputReceiver(), TestOperationContext.create(counterSet, nameForStep("s0"), null, stateTracker));    ParDoOperation parDo = new ParDoOperation(new NoopParDoFn(), new OutputReceiver[0], TestOperationContext.create(counterSet, parDoName, null, stateTracker));    parDo.attachInput(read, 0);    List<Operation> operations = Lists.newArrayList(read, parDo);    try (MapTaskExecutor executor = new MapTaskExecutor(operations, counterSet, stateTracker)) {        executor.execute();    }    stateSampler.doSampling(100L);    CounterName counterName = CounterName.named("per-element-processing-time").withOriginalName(parDoName);    Counter<Long, CounterDistribution> counter = (Counter<Long, CounterDistribution>) counterSet.getExistingCounter(counterName);    assertThat(counter.getAggregate().getCount(), equalTo(3L));}
private NameContext beam_f11012_0(String originalName)
{    return NameContext.create("test-stage", originalName, "system-" + originalName, "step-" + originalName);}
public void beam_f11021_0() throws Exception
{    Operation o1 = Mockito.mock(Operation.class);    Operation o2 = Mockito.mock(Operation.class);    Operation o3 = Mockito.mock(Operation.class);    Mockito.doThrow(new Exception("in start")).when(o2).start();    ExecutionStateTracker stateTracker = ExecutionStateTracker.newForTest();    try (MapTaskExecutor executor = new MapTaskExecutor(Arrays.<Operation>asList(o1, o2, o3), counterSet, stateTracker)) {        executor.execute();        fail("Should have thrown");    } catch (Exception e) {        InOrder inOrder = Mockito.inOrder(o1, o2, o3);        inOrder.verify(o3).start();        inOrder.verify(o2).start();                Mockito.verify(o1).abort();        Mockito.verify(o2).abort();        Mockito.verify(o3).abort();        Mockito.verifyNoMoreInteractions(o1, o2, o3);    }}
public void beam_f11022_0() throws Exception
{    Operation o1 = Mockito.mock(Operation.class);    Operation o2 = Mockito.mock(Operation.class);    Operation o3 = Mockito.mock(Operation.class);    Mockito.doThrow(new Exception("in finish")).when(o2).finish();    ExecutionStateTracker stateTracker = ExecutionStateTracker.newForTest();    try (MapTaskExecutor executor = new MapTaskExecutor(Arrays.<Operation>asList(o1, o2, o3), counterSet, stateTracker)) {        executor.execute();        fail("Should have thrown");    } catch (Exception e) {        InOrder inOrder = Mockito.inOrder(o1, o2, o3);        inOrder.verify(o3).start();        inOrder.verify(o2).start();        inOrder.verify(o1).start();        inOrder.verify(o1).finish();        inOrder.verify(o2).finish();                Mockito.verify(o1).abort();        Mockito.verify(o2).abort();        Mockito.verify(o3).abort();        Mockito.verifyNoMoreInteractions(o1, o2, o3);    }}
public void beam_f11031_0() throws Exception
{    List<Long> expected = Arrays.asList(4009645L, 3979997L, 4014060L);    for (int n = 0; n < 3; n++) {        OutputObjectAndByteCounter counter = makeCounter("byte_count_" + n, 100, n);        for (int i = 0; i < 1000000; i++) {            counter.update("foo");        }        assertEquals(expected.get(n), counter.getByteCount().getAggregate());    }}
public void beam_f11032_0() throws Exception
{    OutputReceiver fanOut = new OutputReceiver();    TestOutputCounter outputCounter = new TestOutputCounter(nameContextForTest());    fanOut.addOutputCounter(outputCounter);    fanOut.process("hi");    fanOut.process("bob");    CounterMean<Long> meanByteCount = outputCounter.getMeanByteCount().getAggregate();    Assert.assertEquals(7, (long) meanByteCount.getAggregate());    Assert.assertEquals(2, meanByteCount.getCount());}
public void beam_f11041_0() throws Exception
{    TestReader reader = new TestReader("hi", "there", "", "bob");    TestOutputReceiver receiver = new TestOutputReceiver(counterSet, NameContext.create("test", "test_receiver", "test_receiver", "test_receiver"));    ReadOperation readOperation = ReadOperation.forTest(reader, receiver, context);    readOperation.start();    readOperation.finish();    assertThat(receiver.outputElems, containsInAnyOrder((Object) "hi", "there", "", "bob"));    CounterUpdateExtractor<?> updateExtractor = mock(CounterUpdateExtractor.class);    counterSet.extractUpdates(false, updateExtractor);    verify(updateExtractor).longSum(eq(getObjectCounterName("test_receiver_out")), anyBoolean(), eq(4L));    verify(updateExtractor).longMean(eq(getMeanByteCounterName("test_receiver_out")), anyBoolean(), eq(LongCounterMean.ZERO.addValue(14L, 4)));    verify(updateExtractor).longSum(eq(named("ReadOperation-ByteCount")), anyBoolean(), eq(10L));    verifyNoMoreInteractions(updateExtractor);}
public ScheduledExecutorService beam_f11042_0()
{    return executor;}
public void beam_f11051_0() throws Exception
{    InMemoryReader<String> reader = new InMemoryReader<>(Arrays.asList("0", "1", "2", "3", "4", "5", "6", "7", "8", "9"), 0, 10, StringUtf8Coder.of());    final ReadOperation[] operationHolder = new ReadOperation[1];    OutputReceiver receiver = new OutputReceiver() {        @Override        public void process(Object elem) throws Exception {            ReadOperation readOperation = operationHolder[0];            if ("1".equals(elem)) {                NativeReader.DynamicSplitResultWithPosition split = (NativeReader.DynamicSplitResultWithPosition) readOperation.requestDynamicSplit(splitRequestAtIndex(8L));                assertNotNull(split);                assertEquals(positionAtIndex(8L), toCloudPosition(split.getAcceptedPosition()));                                ApproximateReportedProgress progress = readerProgressToCloudProgress(readOperation.getProgress());                assertEquals(1, progress.getPosition().getRecordIndex().longValue());            } else if ("3".equals(elem)) {                                                                                                NativeReader.DynamicSplitResultWithPosition split = (NativeReader.DynamicSplitResultWithPosition) readOperation.requestDynamicSplit(splitRequestAtIndex(6L));                assertNotNull(split);                assertEquals(positionAtIndex(6L), toCloudPosition(split.getAcceptedPosition()));                split = (NativeReader.DynamicSplitResultWithPosition) readOperation.requestDynamicSplit(splitRequestAtIndex(6L));                assertNull(split);            }        }    };    ReadOperation readOperation = ReadOperation.forTest(reader, receiver, context);    readOperation.setProgressUpdatePeriodMs(ReadOperation.UPDATE_ON_EACH_ITERATION);    operationHolder[0] = readOperation;        assertNull(readOperation.requestDynamicSplit(splitRequestAtIndex(8L)));    readOperation.start();    readOperation.finish();        assertNull(readOperation.requestDynamicSplit(splitRequestAtIndex(5L)));}
public void beam_f11052_0(Object elem) throws Exception
{    ReadOperation readOperation = operationHolder[0];    if ("1".equals(elem)) {        NativeReader.DynamicSplitResultWithPosition split = (NativeReader.DynamicSplitResultWithPosition) readOperation.requestDynamicSplit(splitRequestAtIndex(8L));        assertNotNull(split);        assertEquals(positionAtIndex(8L), toCloudPosition(split.getAcceptedPosition()));                ApproximateReportedProgress progress = readerProgressToCloudProgress(readOperation.getProgress());        assertEquals(1, progress.getPosition().getRecordIndex().longValue());    } else if ("3".equals(elem)) {                                                NativeReader.DynamicSplitResultWithPosition split = (NativeReader.DynamicSplitResultWithPosition) readOperation.requestDynamicSplit(splitRequestAtIndex(6L));        assertNotNull(split);        assertEquals(positionAtIndex(6L), toCloudPosition(split.getAcceptedPosition()));        split = (NativeReader.DynamicSplitResultWithPosition) readOperation.requestDynamicSplit(splitRequestAtIndex(6L));        assertNull(split);    }}
public Integer beam_f11061_0()
{    return current;}
public NativeReader.Progress beam_f11062_0()
{    Preconditions.checkState(!isClosed);    return cloudProgressToReaderProgress(new ApproximateReportedProgress().setPosition(new Position().setRecordIndex((long) current)).setFractionConsumed(tracker.getFractionConsumed()));}
public void beam_f11071_0()
{    try {        exchanger.exchange(null);    } catch (InterruptedException e) {        throw new RuntimeException(e);    }}
public void beam_f11072_0()
{    ShuffleEntry entry = new ShuffleEntry(KEY, SKEY, VALUE);    assertThat(entry.getKey(), equalTo(KEY));    assertThat(entry.getSecondaryKey(), equalTo(SKEY));    assertThat(entry.getValue(), equalTo(VALUE));}
public void beam_f11081_0()
{    ShuffleEntry entry0 = new ShuffleEntry(KEY, SKEY, VALUE);    ShuffleEntry entry1 = new ShuffleEntry(KEY, SKEY, null);    assertFalse(entry0.equals(entry1));    assertFalse(entry1.equals(entry0));    assertThat(entry0.hashCode(), not(equalTo(entry1.hashCode())));}
public void beam_f11082_0()
{    final byte[] empty = {};    ShuffleEntry entry0 = new ShuffleEntry(null, null, null);    ShuffleEntry entry1 = new ShuffleEntry(empty, empty, empty);    assertFalse(entry0.equals(entry1));    assertFalse(entry1.equals(entry0));    assertThat(entry0.hashCode(), not(equalTo(entry1.hashCode())));}
protected long beam_f11091_0()
{    return initialLeaseExpirationMs;}
protected String beam_f11092_0()
{    return "wi123";}
public void beam_f11101_0() throws Exception
{    Sink<Object> mockSink = mock(Sink.class);    Sink.SinkWriter<Object> mockWriter = mock(Sink.SinkWriter.class);    when(mockSink.writer()).thenReturn(mockWriter);    WriteOperation writeOperation = WriteOperation.forTest(mockSink, context);    writeOperation.start();    writeOperation.abort();    assertThat(writeOperation.initializationState, equalTo(InitializationState.ABORTED));    verify(mockWriter).abort();}
public void beam_f11102_0() throws Exception
{    Sink<Object> mockSink = mock(Sink.class);    Sink.SinkWriter<Object> mockWriter = mock(Sink.SinkWriter.class);    when(mockSink.writer()).thenReturn(mockWriter);    WriteOperation writeOperation = WriteOperation.forTest(mockSink, context);    writeOperation.start();    writeOperation.finish();    assertThat(writeOperation.initializationState, equalTo(InitializationState.FINISHED));    writeOperation.abort();    assertThat(writeOperation.initializationState, equalTo(InitializationState.ABORTED));        verify(mockWriter).close();        verify(mockWriter, never()).abort();}
public static TypeSafeMatcher<CounterUpdate> beam_f11111_0()
{    return new CounterStructuredNameMatcher(null, null);}
public static TypeSafeMatcher<CounterUpdate> beam_f11112_0(CounterName name, String kind)
{    checkArgument(name.isStructured(), "Expected CounterName must be structured");    return new CounterStructuredNameMatcher(name, kind);}
public void beam_f11121_0(Description description)
{    description.appendText("CounterDoubleValue " + value);}
public static TypeSafeMatcher<CounterUpdate> beam_f11122_0(Double value)
{    return new CounterUpdateDoubleValueMatcher(value);}
public static TypeSafeMatcher<CounterUpdate> beam_f11131_0(Integer value)
{    return new CounterUpdateIntegerCountMatcher(value);}
public boolean beam_f11132_0(CounterUpdate counterUpdate)
{    return value.equals(counterUpdate.getFloatingPointMean().getSum());}
public void beam_f11141_0()
{    Counter<Long, ?> c1 = counterFactory.longSum(CounterName.named("c1"));    Counter<Double, ?> c2 = counterFactory.doubleMax(CounterName.named("c2"));    Counter<Double, ?> c3 = counterFactory.doubleMin(CounterName.named("c3"));    Counter<Double, ?> c4 = counterFactory.doubleMean(CounterName.named("c4"));    Counter<Integer, ?> c5 = counterFactory.intMin(CounterName.named("c5"));    Counter<Boolean, ?> c6 = counterFactory.booleanAnd(CounterName.named("c6"));    Counter<Boolean, ?> c7 = counterFactory.booleanOr(CounterName.named("c7"));    Counter<Integer, ?> c8 = counterFactory.intMean(CounterName.named("c8"));    Counter<Long, ?> c9 = counterFactory.distribution(CounterName.named("c9"));    assertThat(c1.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c1"), hasKind("SUM"), hasIntegerValue(0)));    c1.addValue(123L).addValue(-13L);    assertThat(c1.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(110));    assertThat(c2.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c2"), hasKind("MAX"), hasDoubleValue(Double.NEGATIVE_INFINITY)));    c2.getAndReset();    c2.addValue(Math.PI).addValue(Math.E);    assertThat(c2.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasDoubleValue(Math.PI));    c3.addValue(Math.PI).addValue(-Math.PI).addValue(-Math.sqrt(2));    assertThat(c3.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c3"), hasKind("MIN"), hasDoubleValue(-Math.PI)));        assertThat(c4.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), is(nullValue()));    c4.addValue(Math.PI).addValue(Math.E).addValue(Math.sqrt(2));    assertThat(c4.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c4"), hasKind("MEAN"), hasDoubleSum(Math.PI + Math.E + Math.sqrt(2)), hasDoubleCount(3)));    c4.addValue(2.0).addValue(5.0);    assertThat(c4.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasDoubleSum(7.0), hasDoubleCount(2)));    assertThat(c5.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c5"), hasKind("MIN"), hasIntegerValue(Integer.MAX_VALUE)));    c5.addValue(123).addValue(-13);    assertThat(c5.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasIntegerValue(-13));    assertThat(c6.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c6"), hasKind("AND"), hasBooleanValue(true)));    c6.addValue(false);    assertThat(c6.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasBooleanValue(false));    assertThat(c7.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c7"), hasKind("OR"), hasBooleanValue(false)));    c7.addValue(true);    assertThat(c7.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), hasBooleanValue(true));    c8.addValue(1).addValue(2).addValue(3).addValue(4).addValue(5);    assertThat(c8.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c8"), hasKind("MEAN"), hasIntegerSum(15), hasIntegerCount(5)));    c9.addValue(1L).addValue(0L).addValue(1L).addValue(9L).addValue(19L);    assertThat(c9.extractUpdate(true, DataflowCounterUpdateExtractor.INSTANCE), allOf(hasName("c9"), hasKind("DISTRIBUTION"), hasDistribution(CounterFactory.CounterDistribution.builder().minMax(0L, 19L).count(5L).sum(30L).sumOfSquares(444f).firstBucketOffset(0).buckets(Lists.newArrayList(1L, 2L, 0L, 1L, 1L)).build())));}
private SplitInt64 beam_f11142_0(int highBits, long lowBits)
{    SplitInt64 splitInt = new SplitInt64();        splitInt.setHighBits(highBits);    splitInt.setLowBits(lowBits);    return splitInt;}
public static void beam_f11151_0(GroupAlsoByWindowDoFnFactory<String, String, Iterable<String>> gabwFactory) throws Exception
{    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy.of(FixedWindows.of(Duration.millis(10)));    List<WindowedValue<KV<String, Iterable<String>>>> result = runGABW(gabwFactory, windowingStrategy, "key", WindowedValue.of("v1", new Instant(1), Arrays.asList(window(0, 10)), PaneInfo.NO_FIRING), WindowedValue.of("v2", new Instant(2), Arrays.asList(window(0, 10)), PaneInfo.NO_FIRING), WindowedValue.of("v3", new Instant(13), Arrays.asList(window(10, 20)), PaneInfo.NO_FIRING));    assertThat(result, hasSize(2));    TimestampedValue<KV<String, Iterable<String>>> item0 = getOnlyElementInWindow(result, window(0, 10));    assertThat(item0.getValue().getValue(), containsInAnyOrder("v1", "v2"));    assertThat(item0.getTimestamp(), equalTo(window(0, 10).maxTimestamp()));    TimestampedValue<KV<String, Iterable<String>>> item1 = getOnlyElementInWindow(result, window(10, 20));    assertThat(item1.getValue().getValue(), contains("v3"));    assertThat(item1.getTimestamp(), equalTo(window(10, 20).maxTimestamp()));}
public static void beam_f11152_0(GroupAlsoByWindowDoFnFactory<String, String, Iterable<String>> gabwFactory) throws Exception
{    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy.of(SlidingWindows.of(Duration.millis(20)).every(Duration.millis(10))).withTimestampCombiner(TimestampCombiner.EARLIEST);    List<WindowedValue<KV<String, Iterable<String>>>> result = runGABW(gabwFactory, windowingStrategy, "key", WindowedValue.of("v1", new Instant(5), Arrays.asList(window(-10, 10), window(0, 20)), PaneInfo.NO_FIRING), WindowedValue.of("v2", new Instant(15), Arrays.asList(window(0, 20), window(10, 30)), PaneInfo.NO_FIRING));    assertThat(result, hasSize(3));    TimestampedValue<KV<String, Iterable<String>>> item0 = getOnlyElementInWindow(result, window(-10, 10));    assertThat(item0.getValue().getValue(), contains("v1"));    assertThat(item0.getTimestamp(), equalTo(new Instant(5)));    TimestampedValue<KV<String, Iterable<String>>> item1 = getOnlyElementInWindow(result, window(0, 20));    assertThat(item1.getValue().getValue(), containsInAnyOrder("v1", "v2"));        assertThat(item1.getTimestamp(), equalTo(new Instant(10)));    TimestampedValue<KV<String, Iterable<String>>> item2 = getOnlyElementInWindow(result, window(10, 30));    assertThat(item2.getValue().getValue(), contains("v2"));        assertThat(item2.getTimestamp(), equalTo(new Instant(20)));}
public static void beam_f11161_0(GroupAlsoByWindowDoFnFactory<String, Long, Long> gabwFactory, CombineFn<Long, ?, Long> combineFn) throws Exception
{    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy.of(Sessions.withGapDuration(Duration.millis(10))).withTimestampCombiner(TimestampCombiner.END_OF_WINDOW);    BoundedWindow secondWindow = window(15, 25);    List<WindowedValue<KV<String, Long>>> result = runGABW(gabwFactory, windowingStrategy, "k", WindowedValue.of(1L, new Instant(0), Arrays.asList(window(0, 10)), PaneInfo.NO_FIRING), WindowedValue.of(2L, new Instant(5), Arrays.asList(window(5, 15)), PaneInfo.NO_FIRING), WindowedValue.of(4L, new Instant(15), Arrays.asList(secondWindow), PaneInfo.NO_FIRING));    assertThat(result, hasSize(2));    BoundedWindow firstResultWindow = window(0, 15);    TimestampedValue<KV<String, Long>> item0 = getOnlyElementInWindow(result, firstResultWindow);    assertThat(item0.getValue().getValue(), equalTo(combineFn.apply(ImmutableList.of(1L, 2L))));    assertThat(item0.getTimestamp(), equalTo(firstResultWindow.maxTimestamp()));    TimestampedValue<KV<String, Long>> item1 = getOnlyElementInWindow(result, secondWindow);    assertThat(item1.getValue().getValue(), equalTo(combineFn.apply(ImmutableList.of(4L))));    assertThat(item1.getTimestamp(), equalTo(secondWindow.maxTimestamp()));}
private static List<WindowedValue<KV<K, OutputT>>> beam_f11162_0(GroupAlsoByWindowDoFnFactory<K, InputT, OutputT> gabwFactory, WindowingStrategy<?, W> windowingStrategy, K key, WindowedValue<InputT>... values) throws Exception
{    return runGABW(gabwFactory, windowingStrategy, key, Arrays.asList(values));}
public void beam_f11171_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    throw new UnsupportedOperationException();}
public BatchGroupAlsoByWindowFn<K, InputT, OutputT> beam_f11172_0(WindowingStrategy<?, W> windowingStrategy, StateInternalsFactory<K> stateInternals)
{    return new BatchGroupAlsoByWindowAndCombineFn<>(windowingStrategy, combineFn);}
public void beam_f11181_0() throws Exception
{    GroupAlsoByWindowProperties.groupsIntoOverlappingNonmergingWindows(new GABWViaIteratorsDoFnFactory<String, String>());}
public void beam_f11182_0() throws Exception
{    GroupAlsoByWindowProperties.groupsElementsIntoFixedWindowsWithEndOfWindowTimestamp(new GABWViaIteratorsDoFnFactory<String, String>());}
public void beam_f11191_0() throws Exception
{    GroupAlsoByWindowProperties.groupsElementsIntoFixedWindowsWithEndOfWindowTimestamp(new BufferingGABWViaOutputBufferDoFnFactory<String, String>(StringUtf8Coder.of()));}
public void beam_f11192_0() throws Exception
{    GroupAlsoByWindowProperties.groupsElementsIntoFixedWindowsWithLatestTimestamp(new BufferingGABWViaOutputBufferDoFnFactory<String, String>(StringUtf8Coder.of()));}
public void beam_f11201_0() throws Exception
{    File folder = tempFolder.newFolder();    File dump1 = MemoryMonitor.dumpHeap(folder);    assertNotNull(dump1);    assertTrue(dump1.exists());    assertThat(dump1.getParentFile(), Matchers.equalTo(folder));    File dump2 = MemoryMonitor.dumpHeap(folder);    assertNotNull(dump2);    assertTrue(dump2.exists());    assertThat(dump2.getParentFile(), Matchers.equalTo(folder));}
public void beam_f11202_0() throws Exception
{    File remoteFolder = tempFolder.newFolder();    monitor = MemoryMonitor.forTest(provider, 10, 0, true, remoteFolder.getPath(), localDumpFolder);        monitor.dumpHeap();        assertTrue(monitor.tryUploadHeapDumpIfItExists());    File[] files = remoteFolder.listFiles();    assertThat(files, Matchers.arrayWithSize(1));    assertThat(files[0].getAbsolutePath(), Matchers.containsString("heap_dump"));    assertThat(files[0].getAbsolutePath(), Matchers.containsString("hprof"));}
public void beam_f11211_0()
{    CloudObject cloudObject = CloudObject.forClassName("com.google.cloud.dataflow.sdk.util.TimerOrElement$TimerOrElementCoder");    List<CloudObject> component = Collections.singletonList(CloudObjects.asCloudObject(KvCoder.of(VarLongCoder.of(), ByteArrayCoder.of()), /*sdkComponents=*/    null));    Structs.addList(cloudObject, PropertyNames.COMPONENT_ENCODINGS, component);    Coder<?> decoded = CloudObjects.coderFromCloudObject(cloudObject);    assertThat(decoded, instanceOf(TimerOrElementCoder.class));    TimerOrElementCoder<?> decodedCoder = (TimerOrElementCoder<?>) decoded;    assertThat(decodedCoder.getKeyCoder(), equalTo(VarLongCoder.of()));    assertThat(decodedCoder.getElementCoder(), equalTo(ByteArrayCoder.of()));}
public void beam_f11212_0() throws Exception
{    ParDoFn parDoFn = new ValuesDoFnFactory().create(null, /* pipeline options */    CloudObject.fromSpec(ImmutableMap.of(PropertyNames.OBJECT_TYPE_NAME, "ValuesDoFn")), null, /* side input infos */    null, /* main output tag */    null, /* output tag to receiver index */    null, /* exection context */    null);    List<Object> outputReceiver = new ArrayList<>();    parDoFn.startBundle(outputReceiver::add);    parDoFn.processElement(valueInGlobalWindow(KV.of(42, 43)));    assertThat(outputReceiver, contains(valueInGlobalWindow(43)));}
public void beam_f11222_0()
{    injector.cancel();    responseObserver.onCompleted();}
public void beam_f11223_1() throws Exception
{        serviceRegistry.addService(new CloudWindmillServiceV1Alpha1ImplBase() {        @Override        public StreamObserver<StreamingGetDataRequest> getDataStream(StreamObserver<StreamingGetDataResponse> responseObserver) {            return new StreamObserver<StreamingGetDataRequest>() {                boolean sawHeader = false;                HashSet<Long> seenIds = new HashSet<>();                ResponseErrorInjector injector = new ResponseErrorInjector(responseObserver);                StreamingGetDataResponse.Builder responseBuilder = StreamingGetDataResponse.newBuilder();                @Override                public void onNext(StreamingGetDataRequest chunk) {                    maybeInjectError(responseObserver);                    try {                        if (!sawHeader) {                                                        errorCollector.checkThat(chunk.getHeader(), Matchers.equalTo(JobHeader.newBuilder().setJobId("job").setProjectId("project").setWorkerId("worker").build()));                            sawHeader = true;                        } else {                                                        errorCollector.checkThat(chunk.getSerializedSize(), Matchers.lessThanOrEqualTo(STREAM_CHUNK_SIZE));                            int i = 0;                            for (GlobalDataRequest request : chunk.getGlobalDataRequestList()) {                                long requestId = chunk.getRequestId(i++);                                errorCollector.checkThat(seenIds.add(requestId), Matchers.is(true));                                sendResponse(requestId, processGlobalDataRequest(request));                            }                            for (ComputationGetDataRequest request : chunk.getStateRequestList()) {                                long requestId = chunk.getRequestId(i++);                                errorCollector.checkThat(seenIds.add(requestId), Matchers.is(true));                                sendResponse(requestId, processStateRequest(request));                            }                            flushResponse();                        }                    } catch (Exception e) {                        errorCollector.addError(e);                    }                }                @Override                public void onError(Throwable throwable) {                }                @Override                public void onCompleted() {                    injector.cancel();                    responseObserver.onCompleted();                }                private ByteString processGlobalDataRequest(GlobalDataRequest request) {                    errorCollector.checkThat(request.getStateFamily(), Matchers.is("family"));                    return GlobalData.newBuilder().setDataId(request.getDataId()).setStateFamily("family").setData(ByteString.copyFromUtf8(request.getDataId().getTag())).build().toByteString();                }                private ByteString processStateRequest(ComputationGetDataRequest compRequest) {                    errorCollector.checkThat(compRequest.getRequestsCount(), Matchers.is(1));                    errorCollector.checkThat(compRequest.getComputationId(), Matchers.is("computation"));                    KeyedGetDataRequest request = compRequest.getRequests(0);                    KeyedGetDataResponse response = makeGetDataResponse(request.getKey().toStringUtf8(), request.getValuesToFetch(0).getTag().toStringUtf8());                    return response.toByteString();                }                private void sendResponse(long id, ByteString serializedResponse) {                    if (ThreadLocalRandom.current().nextInt(4) == 0) {                        sendChunkedResponse(id, serializedResponse);                    } else {                        responseBuilder.addRequestId(id).addSerializedResponse(serializedResponse);                        if (responseBuilder.getRequestIdCount() > 10) {                            flushResponse();                        }                    }                }                private void sendChunkedResponse(long id, ByteString serializedResponse) {                                        for (int i = 0; i < serializedResponse.size(); i += 10) {                        int end = Math.min(serializedResponse.size(), i + 10);                        try {                            responseObserver.onNext(StreamingGetDataResponse.newBuilder().addRequestId(id).addSerializedResponse(serializedResponse.substring(i, end)).setRemainingBytesForResponse(serializedResponse.size() - end).build());                        } catch (IllegalStateException e) {                                                }                    }                }                private void flushResponse() {                    if (responseBuilder.getRequestIdCount() > 0) {                                                try {                            responseObserver.onNext(responseBuilder.build());                        } catch (IllegalStateException e) {                                                }                        responseBuilder.clear();                    }                }            };        }    });    GetDataStream stream = client.getDataStream();        ExecutorService executor = Executors.newFixedThreadPool(50);    final CountDownLatch done = new CountDownLatch(200);    for (int i = 0; i < 100; ++i) {        final String key = "key" + i;        final String s = i % 5 == 0 ? largeString(i) : "tag";        executor.submit(() -> {            errorCollector.checkThat(stream.requestKeyedData("computation", makeGetDataRequest(key, s)), Matchers.equalTo(makeGetDataResponse(key, s)));            done.countDown();        });        executor.execute(() -> {            errorCollector.checkThat(stream.requestGlobalData(makeGlobalDataRequest(key)), Matchers.equalTo(makeGlobalDataResponse(key)));            done.countDown();        });    }    done.await();    stream.close();    assertTrue(stream.awaitTermination(60, TimeUnit.SECONDS));    executor.shutdown();}
private String beam_f11233_0(int length)
{    return String.join("", Collections.nCopies(length, "."));}
private KeyedGetDataRequest beam_f11234_0(String key, String tag)
{    return KeyedGetDataRequest.newBuilder().setKey(ByteString.copyFromUtf8(key)).setWorkToken(17).addValuesToFetch(TagValue.newBuilder().setTag(ByteString.copyFromUtf8(tag))).build();}
private List<KeyedGetDataRequest> beam_f11244_0(List<String> keys)
{    List<KeyedGetDataRequest> result = new ArrayList<>();    for (String key : keys) {        result.add(Windmill.KeyedGetDataRequest.newBuilder().setKey(ByteString.copyFromUtf8(key)).setWorkToken(0).build());    }    return result;}
public void beam_f11245_1() throws Exception
{        final Map<String, List<KeyedGetDataRequest>> heartbeats = new HashMap<>();    serviceRegistry.addService(new CloudWindmillServiceV1Alpha1ImplBase() {        @Override        public StreamObserver<StreamingGetDataRequest> getDataStream(StreamObserver<StreamingGetDataResponse> responseObserver) {            return new StreamObserver<StreamingGetDataRequest>() {                boolean sawHeader = false;                @Override                public void onNext(StreamingGetDataRequest chunk) {                    try {                        if (!sawHeader) {                                                        errorCollector.checkThat(chunk.getHeader(), Matchers.equalTo(JobHeader.newBuilder().setJobId("job").setProjectId("project").setWorkerId("worker").build()));                            sawHeader = true;                        } else {                                                        errorCollector.checkThat(chunk.getSerializedSize(), Matchers.lessThanOrEqualTo(STREAM_CHUNK_SIZE));                            errorCollector.checkThat(chunk.getRequestIdCount(), Matchers.is(0));                            synchronized (heartbeats) {                                for (ComputationGetDataRequest request : chunk.getStateRequestList()) {                                    errorCollector.checkThat(request.getRequestsCount(), Matchers.is(1));                                    heartbeats.putIfAbsent(request.getComputationId(), new ArrayList<>());                                    heartbeats.get(request.getComputationId()).add(request.getRequestsList().get(0));                                }                            }                        }                    } catch (Exception e) {                        errorCollector.addError(e);                    }                }                @Override                public void onError(Throwable throwable) {                }                @Override                public void onCompleted() {                    responseObserver.onCompleted();                }            };        }    });    Map<String, List<KeyedGetDataRequest>> activeMap = new HashMap<>();    List<String> computation1Keys = new ArrayList<>();    List<String> computation2Keys = new ArrayList<>();    for (int i = 0; i < 100; ++i) {        computation1Keys.add("Computation1Key" + i);        computation2Keys.add("Computation2Key" + largeString(i * 20));    }    activeMap.put("Computation1", makeHeartbeatRequest(computation1Keys));    activeMap.put("Computation2", makeHeartbeatRequest(computation2Keys));    GetDataStream stream = client.getDataStream();    stream.refreshActiveWork(activeMap);    stream.close();    assertTrue(stream.awaitTermination(60, TimeUnit.SECONDS));    while (true) {        Thread.sleep(100);        synchronized (heartbeats) {            if (heartbeats.size() != activeMap.size()) {                continue;            }            assertEquals(heartbeats, activeMap);            break;        }    }}
public void beam_f11256_0() throws Exception
{    Windmill.WorkItem.Builder workItem = Windmill.WorkItem.newBuilder().setKey(SERIALIZED_KEY).setWorkToken(17);    Windmill.InputMessageBundle.Builder chunk1 = workItem.addMessageBundlesBuilder();    chunk1.setSourceComputationId("computation");    addElement(chunk1, 5, "hello", WINDOW_1, paneInfo(0));    addElement(chunk1, 7, "world", WINDOW_2, paneInfo(2));    Windmill.InputMessageBundle.Builder chunk2 = workItem.addMessageBundlesBuilder();    chunk2.setSourceComputationId("computation");    addElement(chunk2, 6, "earth", WINDOW_1, paneInfo(1));    KeyedWorkItem<String, String> keyedWorkItem = new WindmillKeyedWorkItem<>(KEY, workItem.build(), WINDOW_CODER, WINDOWS_CODER, VALUE_CODER);    assertThat(keyedWorkItem.elementsIterable(), Matchers.contains(WindowedValue.of("hello", new Instant(5), WINDOW_1, paneInfo(0)), WindowedValue.of("world", new Instant(7), WINDOW_2, paneInfo(2)), WindowedValue.of("earth", new Instant(6), WINDOW_1, paneInfo(1))));}
private void beam_f11257_0(Windmill.InputMessageBundle.Builder chunk, long timestamp, String value, IntervalWindow window, PaneInfo pane) throws IOException
{    ByteString encodedMetadata = WindmillSink.encodeMetadata(WINDOWS_CODER, Collections.singletonList(window), pane);    chunk.addMessagesBuilder().setTimestamp(WindmillTimeUtils.harnessToWindmillTimestamp(new Instant(timestamp))).setData(ByteString.copyFromUtf8(value)).setMetadata(encodedMetadata);}
public void beam_f11266_0(Appendable appendable) throws IOException
{    appendable.append(id);}
public String beam_f11267_0()
{    return id;}
public String beam_f11276_0()
{    return "State(" + value + ")";}
private static StateNamespace beam_f11277_0(long start)
{    return StateNamespaces.window(IntervalWindow.getCoder(), new IntervalWindow(new Instant(start), new Instant(start + 1)));}
public boolean beam_f11286_0(Object other)
{    return this == other;}
public int beam_f11287_0()
{    return System.identityHashCode(this);}
public void beam_f11296_0() throws Exception
{    StateTag<BagState<String>> addr = StateTags.bag("bag", StringUtf8Coder.of());    BagState<String> bag = underTest.state(NAMESPACE, addr);    SettableFuture<Iterable<String>> future = SettableFuture.create();    when(mockReader.bagFuture(key(NAMESPACE, "bag"), STATE_FAMILY, StringUtf8Coder.of())).thenReturn(future);    bag.readLater();    bag.add("hello");    waitAndSet(future, Arrays.asList("world"), 200);    assertThat(bag.read(), Matchers.containsInAnyOrder("hello", "world"));    bag.add("goodbye");    assertThat(bag.read(), Matchers.containsInAnyOrder("hello", "world", "goodbye"));}
public void beam_f11297_0() throws Exception
{    StateTag<BagState<String>> addr = StateTags.bag("bag", StringUtf8Coder.of());    BagState<String> bag = underTest.state(NAMESPACE, addr);    bag.clear();    bag.add("hello");    assertThat(bag.read(), Matchers.containsInAnyOrder("hello"));        Mockito.verifyZeroInteractions(mockReader);}
public void beam_f11306_0() throws Exception
{    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE, COMBINING_ADDR);    value.clear();    value.readLater();    value.add(5);    value.add(6);    assertThat(value.read(), Matchers.equalTo(11));    value.add(2);    assertThat(value.read(), Matchers.equalTo(13));        Mockito.verifyZeroInteractions(mockReader);}
public void beam_f11307_0() throws Exception
{    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE, COMBINING_ADDR);    SettableFuture<Iterable<int[]>> future = SettableFuture.create();    when(mockReader.bagFuture(eq(COMBINING_KEY), eq(STATE_FAMILY), Mockito.<Coder<int[]>>any())).thenReturn(future);    ReadableState<Boolean> result = value.isEmpty().readLater();    ArgumentCaptor<ByteString> byteString = ArgumentCaptor.forClass(ByteString.class);                        Mockito.verify(mockReader).bagFuture(byteString.capture(), eq(STATE_FAMILY), Mockito.<Coder<int[]>>any());    assertThat(byteString.getValue(), byteStringEq(COMBINING_KEY));    waitAndSet(future, Arrays.asList(new int[] { 29 }), 200);    assertThat(result.read(), Matchers.is(false));}
public void beam_f11316_0() throws Exception
{    StateTag<WatermarkHoldState> addr = StateTags.watermarkStateInternal("watermark", TimestampCombiner.EARLIEST);    WatermarkHoldState bag = underTest.state(NAMESPACE, addr);    bag.clear();    assertThat(bag.read(), Matchers.nullValue());    bag.add(new Instant(300));    assertThat(bag.read(), Matchers.equalTo(new Instant(300)));        Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11317_0() throws Exception
{    StateTag<WatermarkHoldState> addr = StateTags.watermarkStateInternal("watermark", TimestampCombiner.EARLIEST);    WatermarkHoldState bag = underTest.state(NAMESPACE, addr);    bag.add(new Instant(1000));    bag.add(new Instant(2000));    Windmill.WorkItemCommitRequest.Builder commitBuilder = Windmill.WorkItemCommitRequest.newBuilder();    underTest.persist(commitBuilder);    assertEquals(1, commitBuilder.getWatermarkHoldsCount());    Windmill.WatermarkHold watermarkHold = commitBuilder.getWatermarkHolds(0);    assertEquals(key(NAMESPACE, "watermark"), watermarkHold.getTag());    assertEquals(TimeUnit.MILLISECONDS.toMicros(1000), watermarkHold.getTimestamps(0));    Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11326_0() throws Exception
{    StateTag<ValueState<String>> addr = StateTags.value("value", StringUtf8Coder.of());    ValueState<String> value = underTest.state(NAMESPACE, addr);    value.clear();    assertEquals(null, value.read());    Mockito.verifyNoMoreInteractions(mockReader);}
public void beam_f11327_0() throws Exception
{    StateTag<ValueState<String>> addr = StateTags.value("value", StringUtf8Coder.of());    ValueState<String> value = underTest.state(NAMESPACE, addr);    SettableFuture<String> future = SettableFuture.create();    when(mockReader.valueFuture(key(NAMESPACE, "value"), STATE_FAMILY, StringUtf8Coder.of())).thenReturn(future);    waitAndSet(future, "World", 200);    assertEquals("World", value.read());}
private void beam_f11336_0()
{    WindmillStateInternals.COMPACT_NOW.set(() -> false);}
private void beam_f11337_0()
{    WindmillStateInternals.COMPACT_NOW.set(() -> true);}
public void beam_f11346_0() throws Exception
{        Future<Instant> watermarkFuture = underTest.watermarkFuture(STATE_KEY_2, STATE_FAMILY);    Future<Iterable<Integer>> bagFuture = underTest.bagFuture(STATE_KEY_1, STATE_FAMILY, INT_CODER);    Mockito.verifyNoMoreInteractions(mockWindmill);    ArgumentCaptor<Windmill.KeyedGetDataRequest> request = ArgumentCaptor.forClass(Windmill.KeyedGetDataRequest.class);    Windmill.KeyedGetDataResponse.Builder response = Windmill.KeyedGetDataResponse.newBuilder().setKey(DATA_KEY).addWatermarkHolds(Windmill.WatermarkHold.newBuilder().setTag(STATE_KEY_2).setStateFamily(STATE_FAMILY).addTimestamps(5000000).addTimestamps(6000000)).addBags(Windmill.TagBag.newBuilder().setTag(STATE_KEY_1).setStateFamily(STATE_FAMILY).addValues(intData(5)).addValues(intData(100)));    Mockito.when(mockWindmill.getStateData(Mockito.eq(COMPUTATION), Mockito.isA(Windmill.KeyedGetDataRequest.class))).thenReturn(response.build());    Instant result = watermarkFuture.get();    Mockito.verify(mockWindmill).getStateData(Mockito.eq(COMPUTATION), request.capture());        KeyedGetDataRequest keyedRequest = request.getValue();    assertThat(keyedRequest.getKey(), Matchers.equalTo(DATA_KEY));    assertThat(keyedRequest.getWorkToken(), Matchers.equalTo(WORK_TOKEN));    assertThat(keyedRequest.getBagsToFetchCount(), Matchers.equalTo(1));    assertThat(keyedRequest.getBagsToFetch(0).getDeleteAll(), Matchers.equalTo(false));    assertThat(keyedRequest.getBagsToFetch(0).getTag(), Matchers.equalTo(STATE_KEY_1));    assertThat(keyedRequest.getWatermarkHoldsToFetchCount(), Matchers.equalTo(1));    assertThat(keyedRequest.getWatermarkHoldsToFetch(0).getTag(), Matchers.equalTo(STATE_KEY_2));        assertThat(result, Matchers.equalTo(new Instant(5000)));    Mockito.verifyNoMoreInteractions(mockWindmill);    assertThat(bagFuture.get(), Matchers.contains(5, 100));    Mockito.verifyNoMoreInteractions(mockWindmill);        Future<Instant> watermarkFuture2 = underTest.watermarkFuture(STATE_KEY_2, STATE_FAMILY);    assertTrue(watermarkFuture2.isDone());    assertNoReader(watermarkFuture);    assertNoReader(watermarkFuture2);}
public void beam_f11347_0() throws Exception
{    Future<Integer> future = underTest.valueFuture(STATE_KEY_1, "", INT_CODER);    Mockito.verifyNoMoreInteractions(mockWindmill);    Windmill.KeyedGetDataRequest.Builder expectedRequest = Windmill.KeyedGetDataRequest.newBuilder().setKey(DATA_KEY).setShardingKey(SHARDING_KEY).setMaxBytes(WindmillStateReader.MAX_KEY_BYTES).setWorkToken(WORK_TOKEN).addValuesToFetch(Windmill.TagValue.newBuilder().setTag(STATE_KEY_1).setStateFamily("").build());    Windmill.KeyedGetDataResponse.Builder response = Windmill.KeyedGetDataResponse.newBuilder().setKey(DATA_KEY).addValues(Windmill.TagValue.newBuilder().setTag(STATE_KEY_1).setStateFamily("").setValue(intValue(8)));    Mockito.when(mockWindmill.getStateData(COMPUTATION, expectedRequest.build())).thenReturn(response.build());    Integer result = future.get();    Mockito.verify(mockWindmill).getStateData(COMPUTATION, expectedRequest.build());    Mockito.verifyNoMoreInteractions(mockWindmill);    assertThat(result, Matchers.equalTo(8));    assertNoReader(future);}
public static Iterable<Object[]> beam_f11356_0()
{    ImmutableList.Builder<Object[]> ret = ImmutableList.builder();    for (int i = 1; i <= 1000; i += 137) {        ret.add(new Object[] { i });    }    return ret.build();}
public void beam_f11357_0() throws Exception
{    final long apiSizeLimitForTest = 500 * 1024;    DataflowPipelineOptions options = PipelineOptionsFactory.create().as(DataflowPipelineOptions.class);                com.google.api.services.dataflow.model.Source source = WorkerCustomSourcesTest.translateIOToCloudSource(CountingSource.upTo(numberOfSplits), options);    SourceSplitResponse split = WorkerCustomSourcesTest.performSplit(source, options, 1L, null, /* numBundles limit */    apiSizeLimitForTest);    assertThat(split.getBundles().size(), lessThanOrEqualTo(WorkerCustomSources.DEFAULT_NUM_BUNDLES_LIMIT));    List<OffsetBasedSource<?>> originalSplits = new ArrayList<>(numberOfSplits);        for (DerivedSource derivedSource : split.getBundles()) {        Object deserializedSource = WorkerCustomSources.deserializeFromCloudSource(derivedSource.getSource().getSpec());        if (deserializedSource instanceof SplittableOnlyBoundedSource) {            SplittableOnlyBoundedSource<?> splittableOnlySource = (SplittableOnlyBoundedSource<?>) deserializedSource;            originalSplits.addAll((List) splittableOnlySource.split(1L, options));        } else {            originalSplits.add((OffsetBasedSource<?>) deserializedSource);        }    }    assertEquals(numberOfSplits, originalSplits.size());    for (int i = 0; i < originalSplits.size(); i++) {        OffsetBasedSource<?> offsetBasedSource = (OffsetBasedSource<?>) originalSplits.get(i);        assertEquals(i, offsetBasedSource.getStartOffset());        assertEquals(i + 1, offsetBasedSource.getEndOffset());    }}
public void beam_f11367_0()
{    Preconditions.checkState(errorMessage == null, errorMessage);}
public String beam_f11368_0()
{    return description;}
public BoundedReader<Integer> beam_f11378_0(PipelineOptions options) throws IOException
{    return new FailingReader(this);}
public String beam_f11379_0()
{    return "Some description";}
public void beam_f11388_0() throws Exception
{                DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    com.google.api.services.dataflow.model.Source source = translateIOToCloudSource(CountingSource.upTo(8000), options);                        SourceSplitResponse bundledWithOnlyNumBundlesLimit = performSplit(source, options, 8L, 100, /* numBundles limit */    10000 * 1024L);    assertEquals(100, bundledWithOnlyNumBundlesLimit.getBundles().size());    assertThat(DataflowApiUtils.computeSerializedSizeBytes(bundledWithOnlyNumBundlesLimit), greaterThan(10 * 1024L));    SourceSplitResponse bundledWithOnlySizeLimit = performSplit(source, options, 8L, 1000000, /* numBundles limit */    10 * 1024L);    int numBundlesWithOnlySizeLimit = bundledWithOnlySizeLimit.getBundles().size();    assertThat(numBundlesWithOnlySizeLimit, lessThan(100));    SourceSplitResponse bundledWithSizeLimit = performSplit(source, options, 8L, 100, 10 * 1024L);    assertEquals(numBundlesWithOnlySizeLimit, bundledWithSizeLimit.getBundles().size());}
public void beam_f11389_0() throws Exception
{    DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);    com.google.api.services.dataflow.model.Source source = translateIOToCloudSource(CountingSource.upTo(1000), options);    expectedException.expectMessage("[0, 1000)");    expectedException.expectMessage("larger than the limit 100");    performSplit(source, options, 8L, 10, 100L);}
public long beam_f11398_0()
{    if (splitPointsRemaining instanceof Number) {        return ((Number) splitPointsRemaining).longValue();    } else {        throw (RuntimeException) splitPointsRemaining;    }}
public void beam_f11399_0()
{        assertNull(longToParallelism(-10));    assertNull(longToParallelism(-1));        ReportedParallelism p = longToParallelism(0);    assertEquals(p.getValue(), 0.0, 1e-6);    p = longToParallelism(100);    assertEquals(p.getValue(), 100.0, 1e-6);    p = longToParallelism(Long.MAX_VALUE);    assertEquals(p.getValue(), Long.MAX_VALUE, 1e-6);}
public void beam_f11408_0() throws IOException
{    RuntimeException error = new RuntimeException();    error.fillInStackTrace();    when(worker.extractMetricUpdates()).thenReturn(Collections.emptyList());    statusClient.setWorker(worker, executionContext);    statusClient.reportError(error);    verify(workUnitClient).reportWorkItemStatus(statusCaptor.capture());    WorkItemStatus workStatus = statusCaptor.getValue();    assertThat(workStatus.getWorkItemId(), equalTo(Long.toString(WORK_ID)));    assertThat(workStatus.getCompleted(), equalTo(true));    assertThat(workStatus.getReportIndex(), equalTo(INITIAL_REPORT_INDEX));    assertThat(workStatus.getErrors(), hasSize(1));    Status status = workStatus.getErrors().get(0);    assertThat(status.getCode(), equalTo(2));    assertThat(status.getMessage(), containsString("WorkItemStatusClientTest"));}
public void beam_f11409_0() throws IOException
{    OutOfMemoryError error = new OutOfMemoryError();    error.fillInStackTrace();    when(worker.extractMetricUpdates()).thenReturn(Collections.emptyList());    statusClient.setWorker(worker, executionContext);    statusClient.reportError(error);    verify(workUnitClient).reportWorkItemStatus(statusCaptor.capture());    WorkItemStatus workStatus = statusCaptor.getValue();    assertThat(workStatus.getWorkItemId(), equalTo(Long.toString(WORK_ID)));    assertThat(workStatus.getCompleted(), equalTo(true));    assertThat(workStatus.getReportIndex(), equalTo(INITIAL_REPORT_INDEX));    assertThat(workStatus.getErrors(), hasSize(1));    Status status = workStatus.getErrors().get(0);    assertThat(status.getCode(), equalTo(2));    assertThat(status.getMessage(), containsString("WorkItemStatusClientTest"));    assertThat(status.getMessage(), containsString("An OutOfMemoryException occurred."));}
public void beam_f11418_0() throws Exception
{        WorkItemStatus status = new WorkItemStatus();    BatchModeExecutionContext executionContext = mock(BatchModeExecutionContext.class);    when(executionContext.getExecutionStateTracker()).thenReturn(null);    statusClient.setWorker(worker, executionContext);    statusClient.populateMetricUpdates(status);    assertThat(status.getMetricUpdates(), empty());}
public void beam_f11419_0() throws Exception
{        WorkItemStatus status = new WorkItemStatus();    BatchModeExecutionContext executionContext = mock(BatchModeExecutionContext.class);    ExecutionStateTracker executionStateTracker = mock(ExecutionStateTracker.class);    ExecutionState executionState = mock(ExecutionState.class);    when(executionState.getDescription()).thenReturn("stageName-systemName-some-state");    when(executionContext.getExecutionStateTracker()).thenReturn(executionStateTracker);    when(executionStateTracker.getMillisSinceLastTransition()).thenReturn(20L);    when(executionStateTracker.getNumTransitions()).thenReturn(10L);    when(executionStateTracker.getCurrentState()).thenReturn(executionState);    statusClient.setWorker(worker, executionContext);    statusClient.populateMetricUpdates(status);    assertThat(status.getMetricUpdates(), hasSize(1));    MetricUpdate update = status.getMetricUpdates().get(0);    assertThat(update.getName().getName(), equalTo("state-sampler"));    assertThat(update.getKind(), equalTo("internal"));    Map<String, Object> samplerMetrics = (Map<String, Object>) update.getInternal();    assertThat(samplerMetrics, hasEntry("last-state-name", "stageName-systemName-some-state"));    assertThat(samplerMetrics, hasEntry("num-transitions", 10L));    assertThat(samplerMetrics, hasEntry("last-state-duration-ms", 20L));}
public void beam_f11428_0() throws Exception
{    WorkItemStatus status = new WorkItemStatus();    statusClient.setWorker(worker, executionContext);    statusClient.populateSplitResult(status, null);    assertThat(status.getDynamicSourceSplit(), nullValue());    assertThat(status.getStopPosition(), nullValue());}
public void beam_f11429_0() throws Exception
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("setWorker");    thrown.expectMessage("reportUpdate");    statusClient.reportUpdate(null, null);}
public void beam_f11438_1(ArtifactApi.GetManifestRequest request, StreamObserver<ArtifactApi.GetManifestResponse> responseObserver)
{    final String token = request.getRetrievalToken();    if (Strings.isNullOrEmpty(token)) {        throw new StatusRuntimeException(Status.INVALID_ARGUMENT.withDescription("Empty artifact token"));    }        try {        ArtifactApi.ProxyManifest proxyManifest = MANIFEST_CACHE.get(token);        ArtifactApi.GetManifestResponse response = ArtifactApi.GetManifestResponse.newBuilder().setManifest(proxyManifest.getManifest()).build();                responseObserver.onNext(response);        responseObserver.onCompleted();    } catch (Exception e) {                responseObserver.onError(e);    }}
public void beam_f11439_1(ArtifactApi.GetArtifactRequest request, StreamObserver<ArtifactApi.ArtifactChunk> responseObserver)
{        String name = request.getName();    try {        ArtifactApi.ProxyManifest proxyManifest = MANIFEST_CACHE.get(request.getRetrievalToken());                ArtifactApi.ProxyManifest.Location location = proxyManifest.getLocationList().stream().filter(loc -> loc.getName().equals(name)).findFirst().orElseThrow(() -> new StatusRuntimeException(Status.NOT_FOUND.withDescription(String.format("Artifact location not found in manifest: %s", name))));        List<ArtifactMetadata> existingArtifacts = proxyManifest.getManifest().getArtifactList();        ArtifactMetadata metadata = existingArtifacts.stream().filter(meta -> meta.getName().equals(name)).findFirst().orElseThrow(() -> new StatusRuntimeException(Status.NOT_FOUND.withDescription(String.format("Artifact metadata not found in manifest: %s", name))));        ResourceId artifactResourceId = FileSystems.matchNewResource(location.getUri(), false);                Hasher hasher = Hashing.sha256().newHasher();        byte[] data = new byte[ARTIFACT_CHUNK_SIZE_BYTES];        try (InputStream stream = Channels.newInputStream(FileSystems.open(artifactResourceId))) {            int len;            while ((len = stream.read(data)) != -1) {                hasher.putBytes(data, 0, len);                responseObserver.onNext(ArtifactApi.ArtifactChunk.newBuilder().setData(ByteString.copyFrom(data, 0, len)).build());            }        }        if (metadata.getSha256() != null && !metadata.getSha256().isEmpty()) {            String expected = metadata.getSha256();            String actual = hasher.hash().toString();            if (!actual.equals(expected)) {                throw new StatusRuntimeException(Status.DATA_LOSS.withDescription(String.format("Artifact %s is corrupt: expected sha256 %s, actual %s", name, expected, actual)));            }        }        responseObserver.onCompleted();    } catch (IOException | ExecutionException e) {                responseObserver.onError(e);    }}
public void beam_f11450_1(String stagingSessionToken) throws Exception
{    StagingSessionToken parsedToken = StagingSessionToken.decode(stagingSessionToken);    ResourceId dir = getJobDirResourceId(parsedToken);    ResourceId manifestResourceId = dir.resolve(MANIFEST, StandardResolveOptions.RESOLVE_FILE);        ProxyManifest proxyManifest = BeamFileSystemArtifactRetrievalService.loadManifest(manifestResourceId);    for (Location location : proxyManifest.getLocationList()) {        String uri = location.getUri();                FileSystems.delete(Collections.singletonList(FileSystems.matchNewResource(uri, false)));    }    ResourceId artifactsResourceId = dir.resolve(ARTIFACTS, StandardResolveOptions.RESOLVE_DIRECTORY);    if (!proxyManifest.getLocationList().isEmpty()) {                        FileSystems.delete(Collections.singletonList(artifactsResourceId));    }        FileSystems.delete(Collections.singletonList(manifestResourceId));        FileSystems.delete(Collections.singletonList(dir));    }
private ResourceId beam_f11451_0(StagingSessionToken stagingSessionToken)
{    ResourceId baseResourceId;        baseResourceId = FileSystems.matchNewResource(stagingSessionToken.getBasePath(), true);        return baseResourceId.resolve(stagingSessionToken.getSessionId(), StandardResolveOptions.RESOLVE_DIRECTORY);}
private void beam_f11460_0(String basePath)
{    this.basePath = basePath;}
public String beam_f11461_0()
{    try {        return MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        String message = String.format("Error %s occurred while serializing %s", e.getMessage(), this);        throw new StatusRuntimeException(Status.INVALID_ARGUMENT.withDescription(message));    }}
public static DefaultJobBundleFactory beam_f11472_0(JobInfo jobInfo)
{    PipelineOptions pipelineOptions = PipelineOptionsTranslation.fromProto(jobInfo.pipelineOptions());    Map<String, EnvironmentFactory.Provider> environmentFactoryProviderMap = ImmutableMap.of(BeamUrns.getUrn(StandardEnvironments.Environments.DOCKER), new DockerEnvironmentFactory.Provider(pipelineOptions), BeamUrns.getUrn(StandardEnvironments.Environments.PROCESS), new ProcessEnvironmentFactory.Provider(pipelineOptions), BeamUrns.getUrn(StandardEnvironments.Environments.EXTERNAL), new ExternalEnvironmentFactory.Provider(),     Environments.ENVIRONMENT_EMBEDDED, new EmbeddedEnvironmentFactory.Provider(pipelineOptions));    return new DefaultJobBundleFactory(jobInfo, environmentFactoryProviderMap);}
public static DefaultJobBundleFactory beam_f11473_0(JobInfo jobInfo, Map<String, EnvironmentFactory.Provider> environmentFactoryProviderMap)
{    return new DefaultJobBundleFactory(jobInfo, environmentFactoryProviderMap);}
public Map<String, FnDataReceiver> beam_f11482_0()
{    return bundle.getInputReceivers();}
public void beam_f11483_0() throws Exception
{    bundle.close();    client.unref();}
private ServerInfo beam_f11492_0(JobInfo jobInfo, ServerFactory serverFactory) throws IOException
{    Preconditions.checkNotNull(serverFactory, "serverFactory can not be null");    GrpcFnServer<FnApiControlClientPoolService> controlServer = GrpcFnServer.allocatePortAndCreateFor(FnApiControlClientPoolService.offeringClientsToPool(clientPool.getSink(), GrpcContextHeaderAccessorProvider.getHeaderAccessor()), serverFactory);    GrpcFnServer<GrpcLoggingService> loggingServer = GrpcFnServer.allocatePortAndCreateFor(GrpcLoggingService.forWriter(Slf4jLogWriter.getDefault()), serverFactory);    GrpcFnServer<ArtifactRetrievalService> retrievalServer = GrpcFnServer.allocatePortAndCreateFor(BeamFileSystemArtifactRetrievalService.create(), serverFactory);    GrpcFnServer<StaticGrpcProvisionService> provisioningServer = GrpcFnServer.allocatePortAndCreateFor(StaticGrpcProvisionService.create(jobInfo.toProvisionInfo()), serverFactory);    GrpcFnServer<GrpcDataService> dataServer = GrpcFnServer.allocatePortAndCreateFor(GrpcDataService.create(executor, OutboundObserverFactory.serverDirect()), serverFactory);    GrpcFnServer<GrpcStateService> stateServer = GrpcFnServer.allocatePortAndCreateFor(GrpcStateService.create(), serverFactory);    ServerInfo serverInfo = new AutoValue_DefaultJobBundleFactory_ServerInfo.Builder().setControlServer(controlServer).setLoggingServer(loggingServer).setRetrievalServer(retrievalServer).setProvisioningServer(provisioningServer).setDataServer(dataServer).setStateServer(stateServer).build();    return serverInfo;}
public static FnApiControlClient beam_f11493_0(String workerId, StreamObserver<BeamFnApi.InstructionRequest> requestObserver)
{    return new FnApiControlClient(workerId, requestObserver);}
public void beam_f11502_1(Throwable cause)
{        closeAndTerminateOutstandingRequests(cause);}
public static FnApiControlClientPoolService beam_f11503_0(ControlClientPool.Sink clientPool, HeaderAccessor headerAccessor)
{    return new FnApiControlClientPoolService(clientPool, headerAccessor);}
public static ExecutableProcessBundleDescriptor beam_f11512_0(String id, ExecutableStage stage, ApiServiceDescriptor dataEndpoint, ApiServiceDescriptor stateEndpoint) throws IOException
{    checkState(id != null, "id must be specified.");    checkState(stage != null, "stage must be specified.");    checkState(dataEndpoint != null, "dataEndpoint must be specified.");    checkState(stateEndpoint != null, "stateEndpoint must be specified.");    return fromExecutableStageInternal(id, stage, dataEndpoint, stateEndpoint);}
public static ExecutableProcessBundleDescriptor beam_f11513_0(String id, ExecutableStage stage, ApiServiceDescriptor dataEndpoint) throws IOException
{    checkState(id != null, "id must be specified.");    checkState(stage != null, "stage must be specified.");    checkState(dataEndpoint != null, "dateEndpoint must be specified.");    return fromExecutableStageInternal(id, stage, dataEndpoint, null);}
private static Map<String, Map<String, TimerSpec>> beam_f11522_0(ApiServiceDescriptor dataEndpoint, ExecutableStage stage, Components.Builder components, ImmutableMap.Builder<String, RemoteInputDestination> remoteInputsBuilder, ImmutableMap.Builder<String, Coder> outputTransformCodersBuilder) throws IOException
{    ImmutableTable.Builder<String, String, TimerSpec> idsToSpec = ImmutableTable.builder();    for (TimerReference timerReference : stage.getTimers()) {        RunnerApi.ParDoPayload payload = RunnerApi.ParDoPayload.parseFrom(timerReference.transform().getTransform().getSpec().getPayload());        RunnerApi.TimeDomain.Enum timeDomain = payload.getTimerSpecsOrThrow(timerReference.localName()).getTimeDomain();        org.apache.beam.sdk.state.TimerSpec spec;        switch(timeDomain) {            case EVENT_TIME:                spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);                break;            case PROCESSING_TIME:                spec = TimerSpecs.timer(TimeDomain.PROCESSING_TIME);                break;            case SYNCHRONIZED_PROCESSING_TIME:                spec = TimerSpecs.timer(TimeDomain.SYNCHRONIZED_PROCESSING_TIME);                break;            default:                throw new IllegalArgumentException(String.format("Unknown time domain %s", timeDomain));        }        String mainInputName = timerReference.transform().getTransform().getInputsOrThrow(Iterables.getOnlyElement(Sets.difference(timerReference.transform().getTransform().getInputsMap().keySet(), Sets.union(payload.getSideInputsMap().keySet(), payload.getTimerSpecsMap().keySet()))));        String timerCoderId = keyValueCoderId(components.getCodersOrThrow(components.getPcollectionsOrThrow(mainInputName).getCoderId()).getComponentCoderIds(0), payload.getTimerSpecsOrThrow(timerReference.localName()).getTimerCoderId(), components);        RunnerApi.PCollection timerCollectionSpec = components.getPcollectionsOrThrow(mainInputName).toBuilder().setCoderId(timerCoderId).build();                String inputTimerPCollectionId = SyntheticComponents.uniqueId(String.format("%s.timer.%s.in", timerReference.transform().getId(), timerReference.localName()), components.getPcollectionsMap()::containsKey);        components.putPcollections(inputTimerPCollectionId, timerCollectionSpec);        remoteInputsBuilder.put(inputTimerPCollectionId, addStageInput(dataEndpoint, PipelineNode.pCollection(inputTimerPCollectionId, timerCollectionSpec), components));        String outputTimerPCollectionId = SyntheticComponents.uniqueId(String.format("%s.timer.%s.out", timerReference.transform().getId(), timerReference.localName()), components.getPcollectionsMap()::containsKey);        components.putPcollections(outputTimerPCollectionId, timerCollectionSpec);        OutputEncoding outputEncoding = addStageOutput(dataEndpoint, components, PipelineNode.pCollection(outputTimerPCollectionId, timerCollectionSpec));        outputTransformCodersBuilder.put(outputEncoding.getPTransformId(), outputEncoding.getCoder());        components.putTransforms(timerReference.transform().getId(),         components.getTransformsOrThrow(timerReference.transform().getId()).toBuilder().putInputs(timerReference.localName(), inputTimerPCollectionId).putOutputs(timerReference.localName(), outputTimerPCollectionId).build());        idsToSpec.put(timerReference.transform().getId(), timerReference.localName(), TimerSpec.of(timerReference.transform().getId(), timerReference.localName(), inputTimerPCollectionId, outputTimerPCollectionId, outputEncoding.getPTransformId(), spec));    }    return idsToSpec.build().rowMap();}
private static String beam_f11523_0(String keyCoderId, String valueCoderId, Components.Builder components)
{    String id = uniqueId(String.format("kv-%s-%s", keyCoderId, valueCoderId), components.getCodersMap()::containsKey);    RunnerApi.Coder.Builder coder;    components.putCoders(id, RunnerApi.Coder.newBuilder().setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn(ModelCoders.KV_CODER_URN)).addComponentCoderIds(keyCoderId).addComponentCoderIds(valueCoderId).build());    return id;}
private ScheduledExecutorService beam_f11532_0()
{        if (executor != null) {        return executor;    }    synchronized (this) {        if (executor == null) {            executor = Executors.newScheduledThreadPool(1, new ThreadFactoryBuilder().setDaemon(true).build());        }        return executor;    }}
 void beam_f11533_1(ExecutableStageContext context)
{    @SuppressWarnings({ "unchecked", "Not exected to be called from outside." })    WrappedContext wrapper = (WrappedContext) context;    synchronized (wrapper) {        if (wrapper.referenceCount.decrementAndGet() == 0) {                        wrapper.referenceCount = null;            if (getCache().remove(wrapper.jobInfo.jobId(), wrapper)) {                try {                    wrapper.closeActual();                } catch (Throwable t) {                                    }            }        }    }}
public ActiveBundle beam_f11542_0(Map<String, RemoteOutputReceiver<?>> outputReceivers, BundleProgressHandler progressHandler)
{    return newBundle(outputReceivers, request -> {        throw new UnsupportedOperationException(String.format("The %s does not have a registered state handler.", ActiveBundle.class.getSimpleName()));    }, progressHandler);}
public ActiveBundle beam_f11543_1(Map<String, RemoteOutputReceiver<?>> outputReceivers, StateRequestHandler stateRequestHandler, BundleProgressHandler progressHandler)
{    String bundleId = idGenerator.getId();    final CompletionStage<BeamFnApi.InstructionResponse> genericResponse = fnApiControlClient.handle(BeamFnApi.InstructionRequest.newBuilder().setInstructionId(bundleId).setProcessBundle(BeamFnApi.ProcessBundleRequest.newBuilder().setProcessBundleDescriptorId(processBundleDescriptor.getId()).addAllCacheTokens(stateRequestHandler.getCacheTokens())).build());        CompletionStage<BeamFnApi.ProcessBundleResponse> specificResponse = genericResponse.thenApply(InstructionResponse::getProcessBundle);    Map<String, InboundDataClient> outputClients = new HashMap<>();    for (Map.Entry<String, RemoteOutputReceiver<?>> receiver : outputReceivers.entrySet()) {        InboundDataClient outputClient = attachReceiver(bundleId, receiver.getKey(), (RemoteOutputReceiver) receiver.getValue());        outputClients.put(receiver.getKey(), outputClient);    }    ImmutableMap.Builder<String, CloseableFnDataReceiver> dataReceiversBuilder = ImmutableMap.builder();    for (Map.Entry<String, RemoteInputDestination> remoteInput : remoteInputs.entrySet()) {        dataReceiversBuilder.put(remoteInput.getKey(), fnApiDataService.send(LogicalEndpoint.of(bundleId, remoteInput.getValue().getPTransformId()), (Coder) remoteInput.getValue().getCoder()));    }    return new ActiveBundle(bundleId, specificResponse, dataReceiversBuilder.build(), outputClients, stateDelegator.registerForProcessBundleInstructionId(bundleId, stateRequestHandler), progressHandler);}
public Registration beam_f11552_0(String processBundleInstructionId, StateRequestHandler handler)
{    return Registration.INSTANCE;}
private BundleProcessor beam_f11555_1(BeamFnApi.ProcessBundleDescriptor processBundleDescriptor, Map<String, RemoteInputDestination> remoteInputDestinations, StateDelegator stateDelegator)
{            CompletionStage<BeamFnApi.InstructionResponse> genericResponse = fnApiControlClient.handle(BeamFnApi.InstructionRequest.newBuilder().setInstructionId(idGenerator.getId()).setRegister(BeamFnApi.RegisterRequest.newBuilder().addProcessBundleDescriptor(processBundleDescriptor).build()).build());    CompletionStage<RegisterResponse> registerResponseFuture = genericResponse.thenApply(InstructionResponse::getRegister);    BundleProcessor bundleProcessor = new BundleProcessor(processBundleDescriptor, registerResponseFuture, remoteInputDestinations, stateDelegator);    return bundleProcessor;}
public StreamObserver<BeamFnApi.Elements> beam_f11566_1(final StreamObserver<BeamFnApi.Elements> outboundElementObserver)
{        BeamFnDataGrpcMultiplexer multiplexer = new BeamFnDataGrpcMultiplexer(null, outboundObserverFactory, inbound -> outboundElementObserver);        if (!connectedClient.set(multiplexer)) {        additionalMultiplexers.offer(multiplexer);    }    try {                return connectedClient.get().getInboundObserver();    } catch (InterruptedException | ExecutionException e) {        throw new RuntimeException(e);    }}
public void beam_f11567_0() throws Exception
{            connectedClient.cancel(true);        for (BeamFnDataGrpcMultiplexer additional : additionalMultiplexers) {        try {            additional.close();        } catch (Exception ignored) {                }    }    if (!connectedClient.isCancelled()) {        connectedClient.get().close();    }}
public void beam_f11576_0(String containerId) throws IOException, TimeoutException, InterruptedException
{    checkArgument(containerId != null);    checkArgument(CONTAINER_ID_PATTERN.matcher(containerId).matches(), "Container ID must be a 64-character hexadecimal string");    runShortCommand(Arrays.asList(dockerExecutable, "kill", containerId));}
public void beam_f11577_0(String containerId) throws IOException, TimeoutException, InterruptedException
{    checkArgument(containerId != null);    checkArgument(CONTAINER_ID_PATTERN.matcher(containerId).matches(), "Container ID must be a 64-character hexadecimal string");    runShortCommand(Arrays.asList(dockerExecutable, "rm", containerId));}
private List<String> beam_f11586_0()
{    String dockerGcloudConfig = "/root/.config/gcloud";    String localGcloudConfig = firstNonNull(System.getenv("CLOUDSDK_CONFIG"), Paths.get(System.getProperty("user.home"), ".config", "gcloud").toString());        if (Files.exists(Paths.get(localGcloudConfig))) {        return ImmutableList.of("--mount", String.format("type=bind,src=%s,dst=%s", localGcloudConfig, dockerGcloudConfig));    } else {        return ImmutableList.of();    }}
 static ServerFactory beam_f11587_0()
{    ServerFactory.UrlFactory dockerUrlFactory = (host, port) -> HostAndPort.fromParts(DOCKER_FOR_MAC_HOST, port).toString();    if (RUNNING_INSIDE_DOCKER_ON_MAC) {                return ServerFactory.createWithUrlFactoryAndPortSupplier(dockerUrlFactory,         () -> MAC_PORT.getAndUpdate(val -> val == MAC_PORT_END ? MAC_PORT_START : val + 1));    } else {        return ServerFactory.createWithUrlFactory(dockerUrlFactory);    }}
public static ExternalEnvironmentFactory beam_f11596_0(GrpcFnServer<FnApiControlClientPoolService> controlServiceServer, GrpcFnServer<GrpcLoggingService> loggingServiceServer, GrpcFnServer<ArtifactRetrievalService> retrievalServiceServer, GrpcFnServer<StaticGrpcProvisionService> provisioningServiceServer, ControlClientPool.Source clientSource, IdGenerator idGenerator)
{    return new ExternalEnvironmentFactory(controlServiceServer, loggingServiceServer, retrievalServiceServer, provisioningServiceServer, idGenerator, clientSource);}
public RemoteEnvironment beam_f11597_1(Environment environment) throws Exception
{    Preconditions.checkState(environment.getUrn().equals(BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.EXTERNAL)), "The passed environment does not contain an ExternalPayload.");    final RunnerApi.ExternalPayload externalPayload = RunnerApi.ExternalPayload.parseFrom(environment.getPayload());    final String workerId = idGenerator.getId();    BeamFnApi.StartWorkerRequest startWorkerRequest = BeamFnApi.StartWorkerRequest.newBuilder().setWorkerId(workerId).setControlEndpoint(controlServiceServer.getApiServiceDescriptor()).setLoggingEndpoint(loggingServiceServer.getApiServiceDescriptor()).setArtifactEndpoint(retrievalServiceServer.getApiServiceDescriptor()).setProvisionEndpoint(provisioningServiceServer.getApiServiceDescriptor()).putAllParams(externalPayload.getParamsMap()).build();        BeamFnApi.StartWorkerResponse startWorkerResponse = BeamFnExternalWorkerPoolGrpc.newBlockingStub(ManagedChannelFactory.createDefault().forDescriptor(externalPayload.getEndpoint())).startWorker(startWorkerRequest);    if (!startWorkerResponse.getError().isEmpty()) {        throw new RuntimeException(startWorkerResponse.getError());    }        InstructionRequestHandler instructionHandler = null;    while (instructionHandler == null) {        try {            instructionHandler = clientSource.take(workerId, Duration.ofMinutes(2));        } catch (TimeoutException timeoutEx) {                    } catch (InterruptedException interruptEx) {            Thread.currentThread().interrupt();            throw new RuntimeException(interruptEx);        }    }    final InstructionRequestHandler finalInstructionHandler = instructionHandler;    return new RemoteEnvironment() {        @Override        public Environment getEnvironment() {            return environment;        }        @Override        public InstructionRequestHandler getInstructionRequestHandler() {            return finalInstructionHandler;        }        @Override        public void close() throws Exception {            finalInstructionHandler.close();            BeamFnApi.StopWorkerRequest stopWorkerRequest = BeamFnApi.StopWorkerRequest.newBuilder().setWorkerId(workerId).build();                        BeamFnApi.StopWorkerResponse stopWorkerResponse = BeamFnExternalWorkerPoolGrpc.newBlockingStub(ManagedChannelFactory.createDefault().forDescriptor(externalPayload.getEndpoint())).stopWorker(stopWorkerRequest);            if (!stopWorkerResponse.getError().isEmpty()) {                throw new RuntimeException(stopWorkerResponse.getError());            }        }    };}
public void beam_f11606_0() throws Exception
{    synchronized (lock) {        if (!isClosed) {            instructionHandler.close();            processManager.stopProcess(workerId);            isClosed = true;        }    }}
public static ProcessEnvironmentFactory beam_f11607_0(ProcessManager processManager, GrpcFnServer<FnApiControlClientPoolService> controlServiceServer, GrpcFnServer<GrpcLoggingService> loggingServiceServer, GrpcFnServer<ArtifactRetrievalService> retrievalServiceServer, GrpcFnServer<StaticGrpcProvisionService> provisioningServiceServer, ControlClientPool.Source clientSource, IdGenerator idGenerator, PipelineOptions pipelineOptions)
{    return new ProcessEnvironmentFactory(processManager, controlServiceServer, loggingServiceServer, retrievalServiceServer, provisioningServiceServer, idGenerator, clientSource, pipelineOptions);}
private void beam_f11616_1(String id, Process process)
{    if (process.isAlive()) {                        process.destroy();        long maxTimeToWait = 2000;        if (waitForProcessToDie(process, maxTimeToWait)) {                    } else {                        process.destroyForcibly();            if (waitForProcessToDie(process, maxTimeToWait)) {                            } else {                            }        }    }}
private static boolean beam_f11617_0(Process process, long maxWaitTimeMillis)
{    final long startTime = System.currentTimeMillis();    while (process.isAlive() && System.currentTimeMillis() - startTime < maxWaitTimeMillis) {        try {            Thread.sleep(100);        } catch (InterruptedException e) {            Thread.currentThread().interrupt();            throw new RuntimeException("Interrupted while waiting on process", e);        }    }    return !process.isAlive();}
public InstructionRequestHandler beam_f11626_0()
{    return this.instructionRequestHandler;}
public synchronized void beam_f11627_0() throws Exception
{        if (!isClosed) {        return;    }    isClosed = true;    this.instructionRequestHandler.close();}
public static GrpcFnServer<ServiceT> beam_f11636_0(ServiceT service, ApiServiceDescriptor endpoint, ServerFactory factory) throws IOException
{    return new GrpcFnServer<>(factory.create(ImmutableList.of(service), endpoint), service, endpoint);}
public static GrpcFnServer<ServiceT> beam_f11637_0(ServiceT service, ApiServiceDescriptor endpoint)
{    return new GrpcFnServer(null, service, endpoint) {        @Override        public void close() throws Exception {        }    };}
public void beam_f11647_1(PrepareJobRequest request, StreamObserver<PrepareJobResponse> responseObserver)
{    try {        LOG.trace("{} {}", PrepareJobRequest.class.getSimpleName(), request);                String preparationId = String.format("%s_%s", request.getJobName(), UUID.randomUUID().toString());        Struct pipelineOptions = request.getPipelineOptions();        if (pipelineOptions == null) {            throw new NullPointerException("Encountered null pipeline options.");        }        LOG.trace("PIPELINE OPTIONS {} {}", pipelineOptions.getClass(), pipelineOptions);        JobPreparation preparation = JobPreparation.builder().setId(preparationId).setPipeline(request.getPipeline()).setOptions(pipelineOptions).build();        JobPreparation previous = preparations.putIfAbsent(preparationId, preparation);        if (previous != null) {                        String errMessage = String.format("A job with the preparation ID \"%s\" already exists.", preparationId);            StatusException exception = Status.NOT_FOUND.withDescription(errMessage).asException();            responseObserver.onError(exception);            return;        }        String stagingSessionToken = stagingServiceTokenProvider.apply(preparationId);        stagingSessionTokens.putIfAbsent(preparationId, stagingSessionToken);                PrepareJobResponse response = PrepareJobResponse.newBuilder().setPreparationId(preparationId).setArtifactStagingEndpoint(stagingServiceDescriptor).setStagingSessionToken(stagingSessionToken).build();        responseObserver.onNext(response);        responseObserver.onCompleted();    } catch (Exception e) {                responseObserver.onError(Status.INTERNAL.withCause(e).asException());    }}
public void beam_f11648_1(RunJobRequest request, StreamObserver<RunJobResponse> responseObserver)
{    LOG.trace("{} {}", RunJobRequest.class.getSimpleName(), request);    String preparationId = request.getPreparationId();    try {                JobPreparation preparation = preparations.get(preparationId);        if (preparation == null) {            String errMessage = String.format("Unknown Preparation Id \"%s\".", preparationId);            StatusException exception = Status.NOT_FOUND.withDescription(errMessage).asException();            responseObserver.onError(exception);            return;        }        try {            PipelineValidator.validate(preparation.pipeline());        } catch (Exception e) {                        responseObserver.onError(new StatusRuntimeException(Status.INVALID_ARGUMENT.withCause(e)));            return;        }                JobInvocation invocation = invoker.invoke(preparation.pipeline(), preparation.options(), request.getRetrievalToken());        String invocationId = invocation.getId();        invocation.addStateListener(state -> {            if (!JobInvocation.isTerminated(state)) {                return;            }            String stagingSessionToken = stagingSessionTokens.get(preparationId);            stagingSessionTokens.remove(preparationId);            if (cleanupJobFn != null) {                try {                    cleanupJobFn.accept(stagingSessionToken);                } catch (Exception e) {                                    }            }        });        invocation.start();        invocations.put(invocationId, invocation);        RunJobResponse response = RunJobResponse.newBuilder().setJobId(invocationId).build();        responseObserver.onNext(response);        responseObserver.onCompleted();    } catch (StatusRuntimeException e) {                responseObserver.onError(e);    } catch (Exception e) {        String errMessage = String.format("Encountered Unexpected Exception for Preparation %s", preparationId);                responseObserver.onError(Status.INTERNAL.withCause(e).asException());    }}
private JobInvocation beam_f11658_0(String invocationId) throws StatusException
{    JobInvocation invocation = invocations.get(invocationId);    if (invocation == null) {        throw Status.NOT_FOUND.asException();    }    return invocation;}
private PortablePipelineResult beam_f11659_0() throws Exception
{    return pipelineRunner.run(pipeline, jobInfo);}
public RunnerApi.Pipeline beam_f11669_0()
{    return this.pipeline;}
public synchronized void beam_f11670_0(Consumer<JobState.Enum> stateStreamObserver)
{    stateStreamObserver.accept(getState());    stateObservers.add(stateStreamObserver);}
protected InMemoryJobService beam_f11679_0() throws IOException
{    artifactStagingServer = createArtifactStagingService();    expansionServer = createExpansionService();    JobInvoker invoker = createJobInvoker();    return InMemoryJobService.create(artifactStagingServer.getApiServiceDescriptor(), this::createSessionToken, (String stagingSessionToken) -> {        if (configuration.cleanArtifactsPerJob) {            artifactStagingServer.getService().removeArtifacts(stagingSessionToken);        }    }, invoker);}
public String beam_f11680_0()
{    return host;}
public String beam_f11689_0() throws IOException
{    jobServer = createJobServer();    return jobServer.getApiServiceDescriptor().getUrl();}
public void beam_f11690_1()
{    try {        jobServer = createJobServer();        jobServer.getServer().awaitTermination();    } catch (InterruptedException e) {            } catch (Exception e) {            } finally {        stop();    }}
protected void beam_f11699_1(JarFile inputJar) throws IOException
{    Enumeration<JarEntry> inputJarEntries = inputJar.entries();                Set<String> previousEntryNames = new HashSet<>(ImmutableList.of(JarFile.MANIFEST_NAME, PortablePipelineJarUtils.ARTIFACT_MANIFEST_PATH, PortablePipelineJarUtils.PIPELINE_PATH, PortablePipelineJarUtils.PIPELINE_OPTIONS_PATH));    while (inputJarEntries.hasMoreElements()) {        JarEntry inputJarEntry = inputJarEntries.nextElement();        InputStream inputStream = inputJar.getInputStream(inputJarEntry);        String entryName = inputJarEntry.getName();        if (previousEntryNames.contains(entryName)) {                    } else {            JarEntry outputJarEntry = new JarEntry(inputJarEntry);            outputStream.putNextEntry(outputJarEntry);            LOG.trace("Copying jar entry {}", inputJarEntry);            IOUtils.copy(inputStream, outputStream);            previousEntryNames.add(entryName);        }    }}
 ProxyManifest beam_f11700_0(String retrievalToken, ArtifactRetriever retrievalServiceStub) throws IOException
{    GetManifestRequest manifestRequest = GetManifestRequest.newBuilder().setRetrievalToken(retrievalToken).build();    ArtifactApi.Manifest manifest = retrievalServiceStub.getManifest(manifestRequest).getManifest();        ProxyManifest.Builder proxyManifestBuilder = ProxyManifest.newBuilder().setManifest(manifest);    for (ArtifactMetadata artifact : manifest.getArtifactList()) {        String outputPath = PortablePipelineJarUtils.ARTIFACT_FOLDER_PATH + "/" + UUID.randomUUID().toString();        LOG.trace("Copying artifact {} to {}", artifact.getName(), outputPath);        proxyManifestBuilder.addLocation(Location.newBuilder().setName(artifact.getName()).setUri("/" + outputPath).build());        outputStream.putNextEntry(new JarEntry(outputPath));        GetArtifactRequest artifactRequest = GetArtifactRequest.newBuilder().setRetrievalToken(retrievalToken).setName(artifact.getName()).build();        Iterator<ArtifactChunk> artifactResponse = retrievalServiceStub.getArtifact(artifactRequest);        while (artifactResponse.hasNext()) {            artifactResponse.next().getData().writeTo(outputStream);        }    }    return proxyManifestBuilder.build();}
public MetricResults beam_f11709_0()
{    throw new UnsupportedOperationException("Jar creation does not yield metrics.");}
public JobApi.MetricResults beam_f11710_0() throws UnsupportedOperationException
{    return JobApi.MetricResults.getDefaultInstance();}
public void beam_f11719_1() throws Exception
{    Set<InboundObserver> remainingClients = ImmutableSet.copyOf(connectedClients.keySet());    if (!remainingClients.isEmpty()) {                        for (InboundObserver client : remainingClients) {                                                completeIfNotNull(connectedClients.remove(client));        }    }}
public StreamObserver<BeamFnApi.LogEntry.List> beam_f11720_1(StreamObserver<BeamFnApi.LogControl> outboundObserver)
{        InboundObserver inboundObserver = new InboundObserver();    connectedClients.put(inboundObserver, outboundObserver);    return inboundObserver;}
public static StaticGrpcProvisionService beam_f11729_0(ProvisionInfo info)
{    return new StaticGrpcProvisionService(info);}
public void beam_f11730_0(ProvisionApi.GetProvisionInfoRequest request, StreamObserver<GetProvisionInfoResponse> responseObserver)
{    responseObserver.onNext(GetProvisionInfoResponse.newBuilder().setInfo(info).build());    responseObserver.onCompleted();}
private static Server beam_f11740_0(List<BindableService> services, InetSocketAddress socket) throws IOException
{    NettyServerBuilder builder = NettyServerBuilder.forPort(socket.getPort()).maxMessageSize(Integer.MAX_VALUE).permitKeepAliveTime(KEEP_ALIVE_TIME_SEC, TimeUnit.SECONDS);    services.stream().forEach(service -> builder.addService(ServerInterceptors.intercept(service, GrpcContextHeaderAccessorProvider.interceptor())));    return builder.build().start();}
private static File beam_f11741_0(int port)
{    return new File(System.getProperty("java.io.tmpdir"), String.format("fnapi%d.sock", port));}
public WindowedValue<KV<InputT, RestrictionT>> beam_f11750_0(TimerData timer)
{    initState(timer.getNamespace());    WindowedValue<KV<InputT, RestrictionT>> seed = seedState.read();    inputTimestamp = seed.getTimestamp();    return seed.withValue(KV.of(seed.getValue().getKey(), restrictionState.read()));}
public void beam_f11751_0() throws IOException
{    if (primaryRoots == null) {                seedState.clear();        restrictionState.clear();        holdState.clear();        return;    }        checkArgument(residualRoots.size() == 1, "More than 1 residual is unsupported for now");    DelayedBundleApplication residual = residualRoots.get(0);    ByteString encodedResidual = residual.getApplication().getElement();    WindowedValue<KV<InputT, RestrictionT>> decodedResidual = elementRestrictionWireCoder.decode(encodedResidual.newInput());    restrictionState.write(decodedResidual.getValue().getValue());    Instant watermarkHold = residual.getApplication().getOutputWatermarksMap().isEmpty() ? inputTimestamp : new Instant(Iterables.getOnlyElement(residual.getApplication().getOutputWatermarksMap().values()));    checkArgument(!watermarkHold.isBefore(inputTimestamp), "Watermark hold %s can not be before input timestamp %s", watermarkHold, inputTimestamp);    holdState.add(watermarkHold);    Instant requestedWakeupTime = new Instant(Timestamps.toMillis(residual.getRequestedExecutionTime()));    Instant wakeupTime = timerInternals.currentProcessingTime().isBefore(requestedWakeupTime) ? requestedWakeupTime : timerInternals.currentProcessingTime();        timerInternals.setTimer(stateNamespace, "sdfContinuation", wakeupTime, TimeDomain.PROCESSING_TIME);}
public void beam_f11760_0(StateRequest request)
{    StateRequestHandler handler = requestHandlers.getOrDefault(request.getInstructionId(), this::handlerNotFound);    try {        CompletionStage<StateResponse.Builder> result = handler.handle(request);        result.whenComplete((StateResponse.Builder responseBuilder, Throwable t) -> outboundObserver.onNext(t == null ? responseBuilder.setId(request.getId()).build() : createErrorResponse(request.getId(), t)));    } catch (Exception e) {        outboundObserver.onNext(createErrorResponse(request.getId(), e));    }}
public void beam_f11761_0(Throwable t)
{    outboundObserver.onCompleted();}
public Optional<ByteString> beam_f11770_0()
{    return Optional.of(cacheToken);}
private void beam_f11771_0(K key)
{    if (stateInternals == null) {        stateInternals = InMemoryStateInternals.forKey(key);    }}
public CompletionStage<StateResponse.Builder> beam_f11780_0(StateRequest request) throws Exception
{    return handlers.getOrDefault(request.getStateKey().getTypeCase(), this::handlerNotFound).handle(request);}
public Iterable<BeamFnApi.ProcessBundleRequest.CacheToken> beam_f11781_0()
{            Set<BeamFnApi.ProcessBundleRequest.CacheToken> cacheTokens = new HashSet<>();    for (StateRequestHandler handler : handlers.values()) {        for (BeamFnApi.ProcessBundleRequest.CacheToken cacheToken : handler.getCacheTokens()) {            cacheTokens.add(cacheToken);        }    }    return cacheTokens;}
private static CompletionStage<StateResponse.Builder> beam_f11790_0(StateRequest request, ByteString key, W window, BagUserStateHandler<ByteString, ByteString, W> handler)
{            checkState(request.getGet().getContinuationToken().isEmpty(), "Continuation tokens are unsupported.");    return CompletableFuture.completedFuture(StateResponse.newBuilder().setId(request.getId()).setGet(StateGetResponse.newBuilder().setData(ByteString.copyFrom(handler.get(key, window)))));}
private static CompletionStage<StateResponse.Builder> beam_f11791_0(StateRequest request, ByteString key, W window, BagUserStateHandler<ByteString, ByteString, W> handler)
{    handler.append(key, window, ImmutableList.of(request.getAppend().getData()).iterator());    return CompletableFuture.completedFuture(StateResponse.newBuilder().setId(request.getId()).setAppend(StateAppendResponse.getDefaultInstance()));}
public Iterable<V> beam_f11800_0(byte[] keyBytes, W window)
{    K key;    try {                key = keyCoder.decode(new ByteArrayInputStream(keyBytes));    } catch (IOException e) {        throw new RuntimeException(e);    }    return collection.get(SideInputKey.of(keyCoder.structuralValue(key), windowCoder.structuralValue(window)));}
public Coder<V> beam_f11801_0()
{    return valueCoder;}
private static String beam_f11810_0(String coderId, RunnerApi.Components.Builder components, boolean replaceWithByteArrayCoder)
{    Coder coder = components.getCodersOrThrow(coderId);    RunnerApi.Coder.Builder builder = coder.toBuilder().clearComponentCoderIds();    for (String componentCoderId : coder.getComponentCoderIdsList()) {        builder.addComponentCoderIds(addLengthPrefixedCoder(componentCoderId, components, replaceWithByteArrayCoder));    }    return addCoder(builder.build(), components, coderId + "-length_prefix");}
private static String beam_f11811_0(String coderId, RunnerApi.Components.Builder components)
{    Coder.Builder lengthPrefixed = Coder.newBuilder().addComponentCoderIds(coderId);    lengthPrefixed.getSpecBuilder().setUrn(ModelCoders.LENGTH_PREFIX_CODER_URN).build();    return addCoder(lengthPrefixed.build(), components, coderId + "-length_prefix");}
public void beam_f11820_0() throws Exception
{    if (stagingServer != null) {        stagingServer.close();    }    if (stagingService != null) {        stagingService.close();    }    if (retrievalServer != null) {        retrievalServer.close();    }    if (retrievalService != null) {        retrievalService.close();    }}
private void beam_f11821_0(String stagingSessionToken, final String filePath, final String fileName) throws Exception
{    CompletableFuture<Boolean> complete = new CompletableFuture<>();    StreamObserver<PutArtifactRequest> outputStreamObserver = stagingStub.putArtifact(new StreamObserver<PutArtifactResponse>() {        @Override        public void onNext(PutArtifactResponse putArtifactResponse) {                }        @Override        public void onError(Throwable throwable) {            throwable.printStackTrace();            Assert.fail("OnError should never be called.");        }        @Override        public void onCompleted() {            complete.complete(Boolean.TRUE);        }    });    outputStreamObserver.onNext(PutArtifactRequest.newBuilder().setMetadata(PutArtifactMetadata.newBuilder().setMetadata(ArtifactMetadata.newBuilder().setName(fileName).build()).setStagingSessionToken(stagingSessionToken)).build());    try (FileInputStream fileInputStream = new FileInputStream(filePath)) {        byte[] buffer = new byte[DATA_1KB];        int len;        while ((len = fileInputStream.read(buffer)) != -1) {            outputStreamObserver.onNext(PutArtifactRequest.newBuilder().setData(ArtifactChunk.newBuilder().setData(ByteString.copyFrom(buffer, 0, len)).build()).build());        }        outputStreamObserver.onCompleted();        complete.get(10, TimeUnit.SECONDS);    }}
public void beam_f11831_0() throws Exception
{    String stagingSession1 = "123";    String stagingSession2 = "abc";    Map<String, Integer> files1 = ImmutableMap.<String, Integer>builder().put("file5cb", (DATA_1KB / 2)).put("file1kb", DATA_1KB).put("file15cb", (DATA_1KB * 3) / 2).build();    Map<String, Integer> files2 = ImmutableMap.<String, Integer>builder().put("nested/file1kb", DATA_1KB).put("file10kb", 10 * DATA_1KB).put("file100kb", 100 * DATA_1KB).build();    final String text = "abcdefghinklmop\n";    ImmutableMap.<String, Integer>builder().putAll(files1).putAll(files2).build().forEach((fileName, size) -> {        Path filePath = Paths.get(originalDir.toString(), fileName).toAbsolutePath();        try {            Files.createDirectories(filePath.getParent());            Files.write(filePath, Strings.repeat(text, Double.valueOf(Math.ceil(size * 1.0 / text.length())).intValue()).getBytes(StandardCharsets.UTF_8));        } catch (IOException ignored) {        }    });    String stagingSessionToken1 = BeamFileSystemArtifactStagingService.generateStagingSessionToken(stagingSession1, stagingDir.toUri().getPath());    String stagingSessionToken2 = BeamFileSystemArtifactStagingService.generateStagingSessionToken(stagingSession2, stagingDir.toUri().getPath());    List<ArtifactMetadata> metadata1 = Collections.synchronizedList(new ArrayList<>());    List<ArtifactMetadata> metadata2 = Collections.synchronizedList(new ArrayList<>());    ExecutorService executorService = Executors.newFixedThreadPool(8);    try {        Iterator<String> iterator1 = files1.keySet().iterator();        Iterator<String> iterator2 = files2.keySet().iterator();        while (iterator1.hasNext() && iterator2.hasNext()) {            String fileName1 = iterator1.next();            String fileName2 = iterator2.next();            executorService.execute(() -> {                try {                    putArtifact(stagingSessionToken1, Paths.get(originalDir.toString(), fileName1).toAbsolutePath().toString(), fileName1);                    putArtifact(stagingSessionToken2, Paths.get(originalDir.toString(), fileName2).toAbsolutePath().toString(), fileName2);                } catch (Exception e) {                    Assert.fail(e.getMessage());                }                metadata1.add(ArtifactMetadata.newBuilder().setName(fileName1).build());                metadata2.add(ArtifactMetadata.newBuilder().setName(fileName2).build());            });        }    } finally {        executorService.shutdown();        executorService.awaitTermination(2, TimeUnit.SECONDS);    }    String retrievalToken1 = commitManifest(stagingSessionToken1, metadata1);    String retrievalToken2 = commitManifest(stagingSessionToken2, metadata2);    Assert.assertEquals(Paths.get(stagingDir.toAbsolutePath().toString(), stagingSession1, "MANIFEST").toString(), retrievalToken1);    Assert.assertEquals(Paths.get(stagingDir.toAbsolutePath().toString(), stagingSession2, "MANIFEST").toString(), retrievalToken2);    assertFiles(files1.keySet(), retrievalToken1);    assertFiles(files2.keySet(), retrievalToken2);    checkCleanup(stagingSessionToken1, stagingSession1);    checkCleanup(stagingSessionToken2, stagingSession2);}
private void beam_f11832_0(Set<String> files, String retrievalToken) throws Exception
{    ProxyManifest proxyManifest = BeamFileSystemArtifactRetrievalService.loadManifest(retrievalToken);    GetManifestResponse retrievedManifest = retrievalBlockingStub.getManifest(GetManifestRequest.newBuilder().setRetrievalToken(retrievalToken).build());    Assert.assertEquals("Manifest in proxy manifest file doesn't match the retrieved manifest", proxyManifest.getManifest(), retrievedManifest.getManifest());    Assert.assertEquals("Files in locations does not match actual file list.", files, proxyManifest.getLocationList().stream().map(Location::getName).collect(Collectors.toSet()));    Assert.assertEquals("Duplicate file entries in locations.", files.size(), proxyManifest.getLocationCount());    for (Location location : proxyManifest.getLocationList()) {        String expectedContent = readFile(Paths.get(originalDir.toString(), location.getName()).toAbsolutePath().toString());        String actualStagedContent = readFile(location.getUri());        Assert.assertEquals("Staged content doesn't match expected content for " + location.getName(), expectedContent, actualStagedContent);        String retrievedContent = retrieveArtifact(location.getName(), retrievalToken);        Assert.assertEquals("Retrieved content doesn't match expected content for " + location.getName(), expectedContent, retrievedContent);    }}
public void beam_f11841_0() throws Exception
{    try (DefaultJobBundleFactory bundleFactory = createDefaultJobBundleFactory(envFactoryProviderMap)) {        bundleFactory.forStage(getExecutableStage(environment));        verify(envFactory).createEnvironment(environment);    }}
public void beam_f11842_0() throws Exception
{    ServerFactory serverFactory = ServerFactory.createDefault();    Environment environmentA = Environment.newBuilder().setUrn("env:urn:a").setPayload(ByteString.copyFrom(new byte[1])).build();    Environment environmentAA = Environment.newBuilder().setUrn("env:urn:a").setPayload(ByteString.copyFrom(new byte[2])).build();    EnvironmentFactory envFactoryA = mock(EnvironmentFactory.class);    when(envFactoryA.createEnvironment(environmentA)).thenReturn(remoteEnvironment);    when(envFactoryA.createEnvironment(environmentAA)).thenReturn(remoteEnvironment);    EnvironmentFactory.Provider environmentProviderFactoryA = mock(EnvironmentFactory.Provider.class);    when(environmentProviderFactoryA.createEnvironmentFactory(any(), any(), any(), any(), any(), any())).thenReturn(envFactoryA);    when(environmentProviderFactoryA.getServerFactory()).thenReturn(serverFactory);    Environment environmentB = Environment.newBuilder().setUrn("env:urn:b").build();    EnvironmentFactory envFactoryB = mock(EnvironmentFactory.class);    when(envFactoryB.createEnvironment(environmentB)).thenReturn(remoteEnvironment);    EnvironmentFactory.Provider environmentProviderFactoryB = mock(EnvironmentFactory.Provider.class);    when(environmentProviderFactoryB.createEnvironmentFactory(any(), any(), any(), any(), any(), any())).thenReturn(envFactoryB);    when(environmentProviderFactoryB.getServerFactory()).thenReturn(serverFactory);    Map<String, Provider> environmentFactoryProviderMap = ImmutableMap.of(environmentA.getUrn(), environmentProviderFactoryA, environmentB.getUrn(), environmentProviderFactoryB);    try (DefaultJobBundleFactory bundleFactory = createDefaultJobBundleFactory(environmentFactoryProviderMap)) {        bundleFactory.forStage(getExecutableStage(environmentA));        verify(environmentProviderFactoryA, Mockito.times(1)).createEnvironmentFactory(any(), any(), any(), any(), any(), any());        verify(environmentProviderFactoryB, Mockito.times(0)).createEnvironmentFactory(any(), any(), any(), any(), any(), any());        verify(envFactoryA, Mockito.times(1)).createEnvironment(environmentA);        verify(envFactoryA, Mockito.times(0)).createEnvironment(environmentAA);        bundleFactory.forStage(getExecutableStage(environmentAA));        verify(environmentProviderFactoryA, Mockito.times(2)).createEnvironmentFactory(any(), any(), any(), any(), any(), any());        verify(environmentProviderFactoryB, Mockito.times(0)).createEnvironmentFactory(any(), any(), any(), any(), any(), any());        verify(envFactoryA, Mockito.times(1)).createEnvironment(environmentA);        verify(envFactoryA, Mockito.times(1)).createEnvironment(environmentAA);    }}
public void beam_f11851_0() throws Exception
{    server.close();}
public void beam_f11852_0() throws Exception
{    StreamObserver<BeamFnApi.InstructionRequest> requestObserver = mock(StreamObserver.class);    StreamObserver<BeamFnApi.InstructionResponse> responseObserver = controlService.control(requestObserver);        InstructionRequestHandler client = pool.getSource().take("", Duration.ofSeconds(2));        String id = "fakeInstruction";    CompletionStage<BeamFnApi.InstructionResponse> responseFuture = client.handle(BeamFnApi.InstructionRequest.newBuilder().setInstructionId(id).build());    verify(requestObserver).onNext(any(BeamFnApi.InstructionRequest.class));    assertThat(MoreFutures.isDone(responseFuture), is(false));        responseObserver.onNext(BeamFnApi.InstructionResponse.newBuilder().setInstructionId(id).build());    MoreFutures.get(responseFuture);}
public void beam_f11861_0() throws Exception
{    String id = "actualInstruction";    String unknownId = "unknownInstruction";    CompletionStage<BeamFnApi.InstructionResponse> responseFuture = client.handle(BeamFnApi.InstructionRequest.newBuilder().setInstructionId(id).build());    client.asResponseObserver().onNext(BeamFnApi.InstructionResponse.newBuilder().setInstructionId(unknownId).build());    assertThat(MoreFutures.isDone(responseFuture), is(false));    assertThat(MoreFutures.isCancelled(responseFuture), is(false));}
public void beam_f11862_0() throws Exception
{    String id = "clientHangUpInstruction";    CompletionStage<InstructionResponse> responseFuture = client.handle(BeamFnApi.InstructionRequest.newBuilder().setInstructionId(id).build());    client.asResponseObserver().onCompleted();    thrown.expect(ExecutionException.class);    thrown.expectCause(isA(IllegalStateException.class));    thrown.expectMessage("closed");    MoreFutures.get(responseFuture);}
public void beam_f11871_0(ProcessContext ctxt)
{    ctxt.output("zero");    ctxt.output("one");    ctxt.output("two");}
public void beam_f11872_0(ProcessContext ctxt)
{    ctxt.output((long) ctxt.element().length());}
public void beam_f11881_0() throws Exception
{    final String processUserCounterName = "processUserCounter";    final String startUserCounterName = "startUserCounter";    final String finishUserCounterName = "finishUserCounter";    final String processUserDistributionName = "processUserDistribution";    final String startUserDistributionName = "startUserDistribution";    final String finishUserDistributionName = "finishUserDistribution";    Pipeline p = Pipeline.create();                PCollection<String> input = p.apply("impulse", Impulse.create()).apply("create", ParDo.of(new DoFn<byte[], String>() {        private boolean emitted = false;        private Counter startCounter = Metrics.counter(RemoteExecutionTest.class, startUserCounterName);        @StartBundle        public void startBundle() throws InterruptedException {            Thread.sleep(1000);            startCounter.inc(10);            Metrics.distribution(RemoteExecutionTest.class, startUserDistributionName).update(10);        }        @SuppressWarnings("unused")        @ProcessElement        public void processElement(ProcessContext ctxt) throws InterruptedException {                        if (!emitted) {                ctxt.output("zero");                ctxt.output("one");                ctxt.output("two");                Thread.sleep(1000);                Metrics.counter(RemoteExecutionTest.class, processUserCounterName).inc();                Metrics.distribution(RemoteExecutionTest.class, processUserDistributionName).update(1);            }            emitted = true;        }        @DoFn.FinishBundle        public void finishBundle() throws InterruptedException {            Thread.sleep(1000);            Metrics.counter(RemoteExecutionTest.class, finishUserCounterName).inc(100);            Metrics.distribution(RemoteExecutionTest.class, finishUserDistributionName).update(100);        }    })).setCoder(StringUtf8Coder.of());    SingleOutput<String, String> pardo = ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void process(ProcessContext ctxt) {                        ctxt.output(ctxt.element());            ctxt.output(ctxt.element());        }    });    input.apply("processA", pardo).setCoder(StringUtf8Coder.of());    input.apply("processB", pardo).setCoder(StringUtf8Coder.of());    RunnerApi.Pipeline pipelineProto = PipelineTranslation.toProto(p);    FusedPipeline fused = GreedyPipelineFuser.fuse(pipelineProto);    Optional<ExecutableStage> optionalStage = Iterables.tryFind(fused.getFusedStages(), (ExecutableStage stage) -> true);    checkState(optionalStage.isPresent(), "Expected a stage with side inputs.");    ExecutableStage stage = optionalStage.get();    ExecutableProcessBundleDescriptor descriptor = ProcessBundleDescriptors.fromExecutableStage("test_stage", stage, dataServer.getApiServiceDescriptor(), stateServer.getApiServiceDescriptor());    BundleProcessor processor = controlClient.getProcessor(descriptor.getProcessBundleDescriptor(), descriptor.getRemoteInputDestinations(), stateDelegator);    Map<String, Coder> remoteOutputCoders = descriptor.getRemoteOutputCoders();    Map<String, Collection<WindowedValue<?>>> outputValues = new HashMap<>();    Map<String, RemoteOutputReceiver<?>> outputReceivers = new HashMap<>();    for (Entry<String, Coder> remoteOutputCoder : remoteOutputCoders.entrySet()) {        List<WindowedValue<?>> outputContents = Collections.synchronizedList(new ArrayList<>());        outputValues.put(remoteOutputCoder.getKey(), outputContents);        outputReceivers.put(remoteOutputCoder.getKey(), RemoteOutputReceiver.of((Coder<WindowedValue<?>>) remoteOutputCoder.getValue(), outputContents::add));    }    Iterable<String> sideInputData = Arrays.asList("A", "B", "C");    StateRequestHandler stateRequestHandler = StateRequestHandlers.forSideInputHandlerFactory(descriptor.getSideInputSpecs(), new SideInputHandlerFactory() {        @Override        public <T, V, W extends BoundedWindow> SideInputHandler<V, W> forSideInput(String pTransformId, String sideInputId, RunnerApi.FunctionSpec accessPattern, Coder<T> elementCoder, Coder<W> windowCoder) {            return new SideInputHandler<V, W>() {                @Override                public Iterable<V> get(byte[] key, W window) {                    return (Iterable) sideInputData;                }                @Override                public Coder<V> resultCoder() {                    return ((KvCoder) elementCoder).getValueCoder();                }            };        }    });    String testPTransformId = "create/ParMultiDo(Anonymous)";    BundleProgressHandler progressHandler = new BundleProgressHandler() {        @Override        public void onProgress(ProcessBundleProgressResponse progress) {        }        @Override        public void onCompleted(ProcessBundleResponse response) {            List<Matcher<MonitoringInfo>> matchers = new ArrayList<Matcher<MonitoringInfo>>();                        SimpleMonitoringInfoBuilder builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, RemoteExecutionTest.class.getName()).setLabel(MonitoringInfoConstants.Labels.NAME, processUserCounterName);            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "create/ParMultiDo(Anonymous)");            builder.setInt64Value(1);            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));            builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, RemoteExecutionTest.class.getName()).setLabel(MonitoringInfoConstants.Labels.NAME, startUserCounterName);            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "create/ParMultiDo(Anonymous)");            builder.setInt64Value(10);            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));            builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.USER_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, RemoteExecutionTest.class.getName()).setLabel(MonitoringInfoConstants.Labels.NAME, finishUserCounterName);            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "create/ParMultiDo(Anonymous)");            builder.setInt64Value(100);            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));                        builder.setUrn(MonitoringInfoConstants.Urns.USER_DISTRIBUTION_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, RemoteExecutionTest.class.getName()).setLabel(MonitoringInfoConstants.Labels.NAME, processUserDistributionName);            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "create/ParMultiDo(Anonymous)");            builder.setInt64DistributionValue(DistributionData.create(1, 1, 1, 1));            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));            builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.USER_DISTRIBUTION_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, RemoteExecutionTest.class.getName()).setLabel(MonitoringInfoConstants.Labels.NAME, startUserDistributionName);            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "create/ParMultiDo(Anonymous)");            builder.setInt64DistributionValue(DistributionData.create(10, 1, 10, 10));            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));            builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.USER_DISTRIBUTION_COUNTER).setLabel(MonitoringInfoConstants.Labels.NAMESPACE, RemoteExecutionTest.class.getName()).setLabel(MonitoringInfoConstants.Labels.NAME, finishUserDistributionName);            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, "create/ParMultiDo(Anonymous)");            builder.setInt64DistributionValue(DistributionData.create(100, 1, 100, 100));            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));                                    builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.ELEMENT_COUNT);            builder.setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "impulse.out");            builder.setInt64Value(2);            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));            builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.ELEMENT_COUNT);            builder.setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "create/ParMultiDo(Anonymous).output");            builder.setInt64Value(3);            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));                        builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.ELEMENT_COUNT);            builder.setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "processA/ParMultiDo(Anonymous).output");            builder.setInt64Value(6);            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));            builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.ELEMENT_COUNT);            builder.setLabel(MonitoringInfoConstants.Labels.PCOLLECTION, "processB/ParMultiDo(Anonymous).output");            builder.setInt64Value(6);            matchers.add(MonitoringInfoMatchers.matchSetFields(builder.build()));                        builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(MonitoringInfoConstants.Urns.START_BUNDLE_MSECS);            builder.setInt64TypeUrn();            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, testPTransformId);            matchers.add(allOf(MonitoringInfoMatchers.matchSetFields(builder.build()), MonitoringInfoMatchers.valueGreaterThan(0)));                        builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(Urns.PROCESS_BUNDLE_MSECS);            builder.setInt64TypeUrn();            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, testPTransformId);            matchers.add(allOf(MonitoringInfoMatchers.matchSetFields(builder.build()), MonitoringInfoMatchers.valueGreaterThan(0)));            builder = new SimpleMonitoringInfoBuilder();            builder.setUrn(Urns.FINISH_BUNDLE_MSECS);            builder.setInt64TypeUrn();            builder.setLabel(MonitoringInfoConstants.Labels.PTRANSFORM, testPTransformId);            matchers.add(allOf(MonitoringInfoMatchers.matchSetFields(builder.build()), MonitoringInfoMatchers.valueGreaterThan(0)));            for (Matcher<MonitoringInfo> matcher : matchers) {                assertThat(response.getMonitoringInfosList(), Matchers.hasItem(matcher));            }        }    };    try (ActiveBundle bundle = processor.newBundle(outputReceivers, stateRequestHandler, progressHandler)) {        Iterables.getOnlyElement(bundle.getInputReceivers().values()).accept(WindowedValue.valueInGlobalWindow(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), "X")));        Iterables.getOnlyElement(bundle.getInputReceivers().values()).accept(WindowedValue.valueInGlobalWindow(CoderUtils.encodeToByteArray(StringUtf8Coder.of(), "Y")));    }}
public void beam_f11882_0() throws InterruptedException
{    Thread.sleep(1000);    startCounter.inc(10);    Metrics.distribution(RemoteExecutionTest.class, startUserDistributionName).update(10);}
public void beam_f11893_0(@Element KV<String, String> element, @StateId(stateId) BagState<String> state, @StateId(stateId2) BagState<String> state2, OutputReceiver<KV<String, String>> r)
{    ReadableState<Boolean> isEmpty = state.isEmpty();    for (String value : state.read()) {        r.output(KV.of(element.getKey(), value));    }    state.add(element.getValue());    state2.clear();}
public BagUserStateHandler<ByteString, Object, BoundedWindow> beam_f11894_0(String pTransformId, String userStateId, Coder<ByteString> keyCoder, Coder<Object> valueCoder, Coder<BoundedWindow> windowCoder)
{    return new BagUserStateHandler<ByteString, Object, BoundedWindow>() {        @Override        public Iterable<Object> get(ByteString key, BoundedWindow window) {            return (Iterable) userStateData.get(userStateId);        }        @Override        public void append(ByteString key, BoundedWindow window, Iterator<Object> values) {            Iterators.addAll(userStateData.get(userStateId), (Iterator) values);        }        @Override        public void clear(ByteString key, BoundedWindow window) {            userStateData.get(userStateId).clear();        }    };}
public void beam_f11904_0(ProcessContext c)
{    try {        c.output(CoderUtils.decodeFromByteArray(StringUtf8Coder.of(), c.element()));    } catch (CoderException e) {        throw new RuntimeException(e);    }}
public void beam_f11905_0(ProcessContext c)
{    c.output("stream" + suffix + c.element());}
public void beam_f11914_0() throws Exception
{    CompletableFuture<InstructionResponse> processBundleResponseFuture = new CompletableFuture<>();    when(fnApiControlClient.handle(any(BeamFnApi.InstructionRequest.class))).thenReturn(new CompletableFuture<>()).thenReturn(processBundleResponseFuture);    FullWindowedValueCoder<String> coder = FullWindowedValueCoder.of(StringUtf8Coder.of(), Coder.INSTANCE);    BundleProcessor processor = sdkHarnessClient.getProcessor(descriptor, Collections.singletonMap("inputPC", RemoteInputDestination.of((FullWindowedValueCoder) coder, SDK_GRPC_READ_TRANSFORM)));    when(dataService.send(any(), eq(coder))).thenReturn(mock(CloseableFnDataReceiver.class));    try (ActiveBundle activeBundle = processor.newBundle(Collections.emptyMap(), BundleProgressHandler.ignored())) {                                                        BeamFnApi.ProcessBundleResponse response = ProcessBundleResponse.getDefaultInstance();        processBundleResponseFuture.complete(BeamFnApi.InstructionResponse.newBuilder().setProcessBundle(response).build());    }}
public void beam_f11915_0() throws Exception
{    SdkHarnessClient client = harness.client();    BundleProcessor processor = client.getProcessor(descriptor, Collections.singletonMap("inputPC", RemoteInputDestination.of((FullWindowedValueCoder) FullWindowedValueCoder.of(StringUtf8Coder.of(), Coder.INSTANCE), SDK_GRPC_READ_TRANSFORM)));    Collection<WindowedValue<String>> outputs = new ArrayList<>();    try (ActiveBundle activeBundle = processor.newBundle(Collections.singletonMap(SDK_GRPC_WRITE_TRANSFORM, RemoteOutputReceiver.of(FullWindowedValueCoder.of(LengthPrefixCoder.of(StringUtf8Coder.of()), Coder.INSTANCE), outputs::add)), BundleProgressHandler.ignored())) {        FnDataReceiver<WindowedValue<?>> bundleInputReceiver = Iterables.getOnlyElement(activeBundle.getInputReceivers().values());        bundleInputReceiver.accept(WindowedValue.valueInGlobalWindow("foo"));        bundleInputReceiver.accept(WindowedValue.valueInGlobalWindow("bar"));        bundleInputReceiver.accept(WindowedValue.valueInGlobalWindow("baz"));    }        assertThat(outputs, containsInAnyOrder(WindowedValue.valueInGlobalWindow("spam"), WindowedValue.valueInGlobalWindow("ham"), WindowedValue.valueInGlobalWindow("eggs")));}
public void beam_f11924_0() throws Exception
{    MockitoAnnotations.initMocks(this);    when(instructionRequestHandler.handle(any(InstructionRequest.class))).thenReturn(CompletableFuture.completedFuture(InstructionResponse.getDefaultInstance()));    InProcessServerFactory serverFactory = InProcessServerFactory.create();    dataServer = GrpcFnServer.allocatePortAndCreateFor(GrpcDataService.create(executor, OutboundObserverFactory.serverDirect()), serverFactory);    stateServer = GrpcFnServer.allocatePortAndCreateFor(GrpcStateService.create(), serverFactory);    factory = SingleEnvironmentInstanceJobBundleFactory.create(environmentFactory, dataServer, stateServer, IdGenerators.incrementingLongs());}
public void beam_f11925_0() throws Exception
{    try (AutoCloseable data = dataServer;        AutoCloseable state = stateServer) {        executor.shutdownNow();    }}
public ApiServiceDescriptor beam_f11934_0()
{    return dataServer.getApiServiceDescriptor();}
protected void beam_f11935_0() throws Exception
{    InProcessServerFactory serverFactory = InProcessServerFactory.create();    executor = Executors.newCachedThreadPool(new ThreadFactoryBuilder().setDaemon(true).build());    ControlClientPool clientPool = MapControlClientPool.create();    FnApiControlClientPoolService clientPoolService = FnApiControlClientPoolService.offeringClientsToPool(clientPool.getSink(), GrpcContextHeaderAccessorProvider.getHeaderAccessor());    loggingServer = GrpcFnServer.allocatePortAndCreateFor(GrpcLoggingService.forWriter(Slf4jLogWriter.getDefault()), serverFactory);    dataServer = GrpcFnServer.allocatePortAndCreateFor(GrpcDataService.create(executor, OutboundObserverFactory.serverDirect()), serverFactory);    controlServer = GrpcFnServer.allocatePortAndCreateFor(clientPoolService, serverFactory);    InstructionRequestHandler requestHandler = EmbeddedEnvironmentFactory.create(PipelineOptionsFactory.create(), loggingServer, controlServer, clientPool.getSource()).createEnvironment(Environment.getDefaultInstance()).getInstructionRequestHandler();            client = SdkHarnessClient.usingFnApiClient(requestHandler, dataServer.getService());}
public void beam_f11944_0() throws Exception
{    when(docker.runImage(Mockito.eq(IMAGE_NAME), Mockito.any(), Mockito.any())).thenReturn(CONTAINER_ID);    when(docker.isContainerRunning(Mockito.eq(CONTAINER_ID))).thenReturn(true);    DockerEnvironmentFactory factory = getFactory((workerId, timeout) -> {        throw new TimeoutException();    });    factory.createEnvironment(ENVIRONMENT);    verify(docker).getContainerLogs(CONTAINER_ID);}
public void beam_f11945_0() throws Exception
{    when(docker.runImage(Mockito.eq(IMAGE_NAME), Mockito.any(), Mockito.any())).thenReturn(CONTAINER_ID);    when(docker.isContainerRunning(Mockito.eq(CONTAINER_ID))).thenReturn(true);    DockerEnvironmentFactory factory = getFactory((workerId, timeout) -> client);    RemoteEnvironment handle = factory.createEnvironment(ENVIRONMENT);    handle.close();    verify(docker).getContainerLogs(CONTAINER_ID);}
public void beam_f11954_0() throws IOException
{    ProcessManager processManager = ProcessManager.create();    processManager.startProcess("1", "bash", Collections.emptyList());    processManager.stopProcess("1");    processManager.startProcess("2", "bash", Arrays.asList("-c", "ls"));    processManager.stopProcess("2");    processManager.startProcess("1", "bash", Arrays.asList("-c", "ls", "-l", "-a"));    processManager.stopProcess("1");}
public void beam_f11955_0() throws IOException
{    ProcessManager processManager = ProcessManager.create();    try {        processManager.startProcess("1", "asfasfls", Collections.emptyList());        fail();    } catch (IOException e) {        assertThat(e.getMessage(), containsString("Cannot run program \"asfasfls\""));    }}
public void beam_f11964_0() throws Exception
{    final String worker1 = "worker1";    CompletableFuture<String> workerId = new CompletableFuture<>();    Consumer<StreamObserver<Elements>> consumer = elementsStreamObserver -> workerId.complete(GrpcContextHeaderAccessorProvider.getHeaderAccessor().getSdkWorkerId());    TestDataService testService = new TestDataService(Mockito.mock(StreamObserver.class), consumer);    ApiServiceDescriptor serviceDescriptor = ApiServiceDescriptor.newBuilder().setUrl("testServer").build();    Server server = InProcessServerFactory.create().create(ImmutableList.of(testService), serviceDescriptor);    final Metadata.Key<String> workerIdKey = Metadata.Key.of("worker_id", Metadata.ASCII_STRING_MARSHALLER);    Channel channel = InProcessChannelBuilder.forName(serviceDescriptor.getUrl()).intercept(new ClientInterceptor() {        @Override        public <ReqT, RespT> ClientCall<ReqT, RespT> interceptCall(MethodDescriptor<ReqT, RespT> method, CallOptions callOptions, Channel next) {            ClientCall<ReqT, RespT> call = next.newCall(method, callOptions);            return new SimpleForwardingClientCall<ReqT, RespT>(call) {                @Override                public void start(ClientCall.Listener<RespT> responseListener, Metadata headers) {                    headers.put(workerIdKey, worker1);                    super.start(responseListener, headers);                }            };        }    }).build();    BeamFnDataGrpc.BeamFnDataStub stub = BeamFnDataGrpc.newStub(channel);    stub.data(Mockito.mock(StreamObserver.class));    server.shutdown();    Assert.assertEquals(worker1, workerId.get());}
public ClientCall<ReqT, RespT> beam_f11965_0(MethodDescriptor<ReqT, RespT> method, CallOptions callOptions, Channel next)
{    ClientCall<ReqT, RespT> call = next.newCall(method, callOptions);    return new SimpleForwardingClientCall<ReqT, RespT>(call) {        @Override        public void start(ClientCall.Listener<RespT> responseListener, Metadata headers) {            headers.put(workerIdKey, worker1);            super.start(responseListener, headers);        }    };}
public void beam_f11974_0()
{    prepareJob();    JobApi.GetJobPipelineRequest request = JobApi.GetJobPipelineRequest.newBuilder().setJobId(TEST_JOB_ID).build();    RecordingObserver<JobApi.GetJobPipelineResponse> recorder = new RecordingObserver<>();    service.getPipeline(request, recorder);        assertThat(recorder.isSuccessful(), is(false));    assertThat(recorder.error, isA(StatusException.class));}
public void beam_f11975_0() throws Exception
{    prepareAndRunJob();    JobApi.GetJobPipelineRequest request = JobApi.GetJobPipelineRequest.newBuilder().setJobId(TEST_JOB_ID).build();    RecordingObserver<JobApi.GetJobPipelineResponse> recorder = new RecordingObserver<>();    service.getPipeline(request, recorder);    assertThat(recorder.isSuccessful(), is(true));    assertThat(recorder.values, hasSize(1));    JobApi.GetJobPipelineResponse response = recorder.values.get(0);    assertThat(response.getPipeline(), is(TEST_PIPELINE));}
public void beam_f11984_0() throws Exception
{    jobInvocation.start();    assertThat(jobInvocation.getState(), is(JobApi.JobState.Enum.RUNNING));        runner.setResult(null);    awaitJobState(jobInvocation, JobApi.JobState.Enum.UNSPECIFIED);}
public void beam_f11985_0() throws Exception
{    jobInvocation.start();    assertThat(jobInvocation.getState(), is(JobApi.JobState.Enum.RUNNING));    jobInvocation.cancel();    awaitJobState(jobInvocation, JobApi.JobState.Enum.CANCELLED);}
public State beam_f11994_0(Duration duration)
{    return null;}
public State beam_f11995_0()
{    return null;}
public static void beam_f12004_0(String[] args)
{    System.out.println("Hello world");}
public void beam_f12005_0()
{    Manifest manifest = jarCreator.createManifest(FakePipelineRunnner.class);    assertEquals(FakePipelineRunnner.class.getName(), manifest.getMainAttributes().getValue(Name.MAIN_CLASS));}
public void beam_f12014_0(LogEntry entry)
{    entries.add(entry);}
public void beam_f12015_0()
{    latch.countDown();}
public StreamObserver<BeamFnApi.Elements> beam_f12024_0(StreamObserver<BeamFnApi.Elements> outboundObserver)
{    Uninterruptibles.putUninterruptibly(outboundObservers, outboundObserver);    return inboundObserver;}
public void beam_f12025_0() throws Exception
{    MockitoAnnotations.initMocks(this);    stateService = GrpcStateService.create();}
public void beam_f12034_0()
{    when(context.getSideInput(COLLECTION_ID)).thenReturn(Arrays.asList(WindowedValue.valueInGlobalWindow(KV.of("foo", 2)), WindowedValue.valueInGlobalWindow(KV.of("bar", 3)), WindowedValue.valueInGlobalWindow(KV.of("foo", 5))));    BatchSideInputHandlerFactory factory = BatchSideInputHandlerFactory.forStage(EXECUTABLE_STAGE, context);    SideInputHandler<Integer, GlobalWindow> handler = factory.forSideInput(TRANSFORM_ID, SIDE_INPUT_NAME, MULTIMAP_ACCESS, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()), GlobalWindow.Coder.INSTANCE);    Iterable<Integer> result = handler.get(ENCODED_FOO, GlobalWindow.INSTANCE);    assertThat(result, containsInAnyOrder(2, 5));}
public void beam_f12035_0()
{    Instant instantA = new DateTime(2018, 1, 1, 1, 1, DateTimeZone.UTC).toInstant();    Instant instantB = new DateTime(2018, 1, 1, 1, 2, DateTimeZone.UTC).toInstant();    Instant instantC = new DateTime(2018, 1, 1, 1, 3, DateTimeZone.UTC).toInstant();    IntervalWindow windowA = new IntervalWindow(instantA, instantB);    IntervalWindow windowB = new IntervalWindow(instantB, instantC);    when(context.getSideInput(COLLECTION_ID)).thenReturn(Arrays.asList(WindowedValue.of(KV.of("foo", 1), instantA, windowA, PaneInfo.NO_FIRING), WindowedValue.of(KV.of("bar", 2), instantA, windowA, PaneInfo.NO_FIRING), WindowedValue.of(KV.of("foo", 3), instantA, windowA, PaneInfo.NO_FIRING), WindowedValue.of(KV.of("foo", 4), instantB, windowB, PaneInfo.NO_FIRING), WindowedValue.of(KV.of("bar", 5), instantB, windowB, PaneInfo.NO_FIRING), WindowedValue.of(KV.of("foo", 6), instantB, windowB, PaneInfo.NO_FIRING)));    BatchSideInputHandlerFactory factory = BatchSideInputHandlerFactory.forStage(EXECUTABLE_STAGE, context);    SideInputHandler<Integer, IntervalWindow> handler = factory.forSideInput(TRANSFORM_ID, SIDE_INPUT_NAME, MULTIMAP_ACCESS, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()), IntervalWindowCoder.of());    Iterable<Integer> resultA = handler.get(ENCODED_FOO, windowA);    Iterable<Integer> resultB = handler.get(ENCODED_FOO, windowB);    assertThat(resultA, containsInAnyOrder(1, 3));    assertThat(resultB, containsInAnyOrder(4, 6));}
public void beam_f12045_0() throws IOException
{    SdkComponents sdkComponents = SdkComponents.create();    sdkComponents.registerEnvironment(Environments.createDockerEnvironment("java"));    String coderId = sdkComponents.registerCoder(original);    Components.Builder components = sdkComponents.toComponents().toBuilder();    String updatedCoderId = LengthPrefixUnknownCoders.addLengthPrefixedCoder(coderId, components, replaceWithByteArray);    assertEquals(expected, RehydratedComponents.forComponents(components.build()).getCoder(updatedCoderId));}
 DAG beam_f12046_0()
{    wireUp();    return dag;}
private void beam_f12055_0()
{    new WiringInstaller().wireUp();}
 void beam_f12056_0()
{    Collection<String> edgeIds = new HashSet<>();    edgeIds.addAll(edgeStartPoints.keySet());    edgeIds.addAll(edgeEndPoints.keySet());    for (String edgeId : edgeIds) {        String pCollId = pCollsOfEdges.get(edgeId);        if (pCollId == null) {            throw new RuntimeException("Oops!");        }        Vertex sourceVertex = edgeStartPoints.get(edgeId);        if (sourceVertex == null) {            throw new RuntimeException("Oops!");        }        Coder edgeCoder = edgeCoders.get(edgeId);        if (edgeCoder == null) {            throw new RuntimeException("Oops!");        }        List<Vertex> destinationVertices = edgeEndPoints.getOrDefault(edgeId, Collections.emptyList());        boolean sideInputEdge = sideInputCollections.contains(pCollId);        for (Vertex destinationVertex : destinationVertices) {            addEdge(sourceVertex, destinationVertex, edgeCoder, edgeId, pCollId, sideInputEdge);        }    }}
public MetricResults beam_f12065_0()
{    return new MetricResults() {        @Override        public MetricQueryResults queryMetrics(@Nullable MetricsFilter filter) {            return new MetricQueryResults() {                @Override                public Iterable<MetricResult<Long>> getCounters() {                    return Collections.emptyList();                }                @Override                public Iterable<MetricResult<DistributionResult>> getDistributions() {                    return Collections.emptyList();                }                @Override                public Iterable<MetricResult<GaugeResult>> getGauges() {                    return Collections.emptyList();                }            };        }    };}
public MetricQueryResults beam_f12066_0(@Nullable MetricsFilter filter)
{    return new MetricQueryResults() {        @Override        public Iterable<MetricResult<Long>> getCounters() {            return Collections.emptyList();        }        @Override        public Iterable<MetricResult<DistributionResult>> getDistributions() {            return Collections.emptyList();        }        @Override        public Iterable<MetricResult<GaugeResult>> getGauges() {            return Collections.emptyList();        }    };}
 void beam_f12076_0(CompletableFuture<Void> completionFuture)
{    this.completionFuture = completionFuture;}
 void beam_f12077_0(Throwable throwable)
{    metricResults.freeze();    terminalState = throwable != null ? State.FAILED : State.DONE;}
 void beam_f12086_0(Function<PTransform<?, ?>, JetTransformTranslator<?>> extraTranslatorProvider)
{    Function<PTransform<?, ?>, JetTransformTranslator<?>> initialTranslatorProvider = this.translatorProvider;    this.translatorProvider = transform -> {        JetTransformTranslator<?> translator = initialTranslatorProvider.apply(transform);        if (translator == null) {            translator = extraTranslatorProvider.apply(transform);        }        return translator;    };}
private void beam_f12087_0(Pipeline pipeline)
{    pipeline.replaceAll(getDefaultOverrides());    UnconsumedReads.ensureAllReadsConsumed(pipeline);}
public Iterable<Class<? extends PipelineRunner<?>>> beam_f12096_0()
{    return ImmutableList.of(JetRunner.class);}
public Iterable<Class<? extends PipelineOptions>> beam_f12097_0()
{    return ImmutableList.of(JetPipelineOptions.class);}
public Vertex beam_f12106_0(Pipeline pipeline, AppliedPTransform<?, ?, ?> appliedTransform, Node node, JetTranslationContext context)
{    String transformName = appliedTransform.getFullName();    DAGBuilder dagBuilder = context.getDagBuilder();    String vertexId = dagBuilder.newVertexId(transformName);    Map.Entry<TupleTag<?>, PValue> output = Utils.getOutput(appliedTransform);    Coder outputCoder = Utils.getCoder((PCollection) Utils.getOutput(appliedTransform).getValue());    Vertex vertex = dagBuilder.addVertex(vertexId, ImpulseP.supplier(outputCoder, vertexId));    String outputEdgeId = Utils.getTupleTagId(output.getValue());    dagBuilder.registerCollectionOfEdge(outputEdgeId, output.getKey().getId());    dagBuilder.registerEdgeStartPoint(outputEdgeId, vertex, outputCoder);    return vertex;}
 SerializablePipelineOptions beam_f12107_0()
{    return options;}
public void beam_f12116_0(long n)
{    count -= n;}
 DistributionData beam_f12117_0()
{    return distributionData;}
private static Predicate<Map.Entry<MetricKey, ?>> beam_f12126_0(final MetricsFilter filter)
{    return entry -> MetricFiltering.matches(filter, entry.getKey());}
public Iterable<MetricResult<Long>> beam_f12127_0()
{    return counters;}
 Iterable<MetricResult<DistributionResult>> beam_f12136_0(MetricsFilter filter)
{    return FluentIterable.from(distributions.entrySet()).filter(matchesFilter(filter)).transform(this::toUpdateResult).toList();}
private MetricResult<DistributionResult> beam_f12137_0(Map.Entry<MetricKey, DistributionData> entry)
{    MetricKey key = entry.getKey();    DistributionResult distributionResult = entry.getValue().extractResult();    return MetricResult.create(key, distributionResult, distributionResult);}
public void beam_f12146_0(boolean async)
{    if (counters.isEmpty() && distributions.isEmpty() && gauges.isEmpty()) {        return;    }    ImmutableList<MetricUpdates.MetricUpdate<Long>> counters = extractUpdates(this.counters);    ImmutableList<MetricUpdates.MetricUpdate<DistributionData>> distributions = extractUpdates(this.distributions);    ImmutableList<MetricUpdates.MetricUpdate<GaugeData>> gauges = extractUpdates(this.gauges);    MetricUpdates updates = new MetricUpdatesImpl(counters, distributions, gauges);    if (async) {        accumulator.setAsync(metricsKey, updates);    } else {        accumulator.set(metricsKey, updates);    }}
private ImmutableList<MetricUpdates.MetricUpdate<UpdateT>> beam_f12147_0(Map<MetricName, CellT> cells)
{    ImmutableList.Builder<MetricUpdates.MetricUpdate<UpdateT>> updates = ImmutableList.builder();    for (CellT cell : cells.values()) {        UpdateT value = cell.getValue();        if (value != null) {            MetricKey key = MetricKey.create(stepName, cell.getName());            MetricUpdates.MetricUpdate<UpdateT> update = MetricUpdates.MetricUpdate.create(key, value);            updates.add(update);        }    }    return updates.build();}
private void beam_f12156_0(Inbox inbox)
{    startRunnerBundle(doFnRunner);    for (byte[] value; (value = (byte[]) inbox.poll()) != null; ) {        WindowedValue<InputT> windowedValue = Utils.decodeWindowedValue(value, inputCoder);        processElementWithRunner(doFnRunner, windowedValue);        if (!outputManager.tryFlush()) {            break;        }    }    finishRunnerBundle(doFnRunner);}
protected void beam_f12157_0(DoFnRunner<InputT, OutputT> runner)
{    runner.startBundle();}
private static Boolean beam_f12166_0(SerializablePipelineOptions serializablePipelineOptions)
{    PipelineOptions pipelineOptions = serializablePipelineOptions.get();    JetPipelineOptions jetPipelineOptions = pipelineOptions.as(JetPipelineOptions.class);    return jetPipelineOptions.getJetProcessorsCooperative();}
public void beam_f12167_0(TupleTag<T> tag, WindowedValue<T> outputValue)
{    assert currentBucket == 0 && currentItem == 0 : "adding output while flushing";    Coder coder = outputCoders.get(tag);    byte[] output = Utils.encode(outputValue, coder);    for (int ordinal : outputCollToOrdinals.get(tag)) {        outputBuckets[ordinal].add(output);    }}
public void beam_f12176_0()
{    items.remove();}
public static SupplierEx<Processor> beam_f12177_0(Coder inputCoder, Coder outputCoder, WindowingStrategy<T, BoundedWindow> windowingStrategy, String ownerId)
{    return () -> new AssignWindowP<>(inputCoder, outputCoder, windowingStrategy, ownerId);}
public boolean beam_f12186_0()
{    return emitFromTraverser(this);}
public boolean beam_f12187_0()
{    return false;}
public void beam_f12197_0(Edge edge, String edgeId, String pCollId, String vertexId)
{    if (ownerId.equals(vertexId)) {        Coder coder = inputCollectionCoders.get(edgeId);        inputOrdinalCoders.put(edge.getDestOrdinal(), coder);    }}
public boolean beam_f12198_0()
{    if (active) {        return tryEmit(Utils.encode(WindowedValue.valueInGlobalWindow(new byte[0]), outputCoder));    } else {        return true;    }}
private static void beam_f12207_0(TimerInternals.TimerData timer, DoFnRunner<KV<?, ?>, ?> doFnRunner)
{    StateNamespace namespace = timer.getNamespace();    BoundedWindow window = ((StateNamespaces.WindowNamespace) namespace).getWindow();    doFnRunner.onTimer(timer.getTimerId(), window, timer.getTimestamp(), timer.getDomain());}
protected DoFnRunner<KV<?, ?>, OutputT> beam_f12208_0(PipelineOptions pipelineOptions, DoFn<KV<?, ?>, OutputT> doFn, SideInputReader sideInputReader, JetOutputManager outputManager, TupleTag<OutputT> mainOutputTag, List<TupleTag<?>> additionalOutputTags, Coder<KV<?, ?>> inputValueCoder, Map<TupleTag<?>, Coder<?>> outputValueCoders, WindowingStrategy<?, ?> windowingStrategy, DoFnSchemaInformation doFnSchemaInformation, Map<String, PCollectionView<?>> sideInputMapping)
{    timerInternals = new InMemoryTimerInternals();    keyedStepContext = new KeyedStepContext(timerInternals);    return DoFnRunners.simpleRunner(pipelineOptions, doFn, sideInputReader, outputManager, mainOutputTag, additionalOutputTags, keyedStepContext, inputValueCoder, outputValueCoders, windowingStrategy, doFnSchemaInformation, sideInputMapping);}
public StateInternals beam_f12217_0()
{    return currentStateInternals;}
public TimerInternals beam_f12218_0()
{    return timerInternals;}
private static long beam_f12227_0(long[] instants)
{    long min = instants[0];    for (int i = 1; i < instants.length; i++) {        if (instants[i] < min) {            min = instants[i];        }    }    return min;}
public static ProcessorMetaSupplier beam_f12228_0(UnboundedSource<T, CmT> unboundedSource, SerializablePipelineOptions options, Coder outputCoder, String ownerId)
{    return new UnboundedSourceProcessorMetaSupplier<>(unboundedSource, options, outputCoder, ownerId);}
 TimestampAndValues beam_f12237_0(TimestampCombiner timestampCombiner, TimestampAndValues other)
{    pane = other.pane;    timestamp = timestampCombiner.combine(timestamp, other.timestamp);    values.addAll(other.values);    return this;}
public static SupplierEx<Processor> beam_f12238_0(SerializablePipelineOptions pipelineOptions, WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder, Coder outputCoder, WindowingStrategy windowingStrategy, String ownerId)
{    return () -> new WindowGroupP<>(pipelineOptions, inputCoder, outputCoder, windowingStrategy, ownerId);}
public void beam_f12247_0(TupleTag<AdditionalOutputT> tag, AdditionalOutputT output, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    throw new UnsupportedOperationException("Grouping should not use side outputs");}
 void beam_f12248_0(Instant watermark, Instant now)
{    try {        timerInternals.advanceProcessingTime(now);        advanceInputWatermark(watermark);        Instant hold = stateInternals.earliestWatermarkHold();        if (hold == null) {            WindowTracing.trace("TestInMemoryTimerInternals.advanceInputWatermark: no holds, " + "so output watermark = input watermark");            hold = timerInternals.currentInputWatermarkTime();        }        advanceOutputWatermark(hold);        reduceFnRunner.persist();    } catch (Exception e) {        throw ExceptionUtil.rethrow(e);    }}
 static Collection<PValue> beam_f12257_0(Pipeline pipeline, TransformHierarchy.Node node)
{    if (node.getTransform() == null) {        return null;    }    return TransformInputs.nonAdditionalInputs(node.toAppliedPTransform(pipeline));}
 static Map<TupleTag<?>, PValue> beam_f12258_0(AppliedPTransform<?, ?, ?> appliedTransform)
{    return appliedTransform.getInputs();}
 static Map<T, Coder> beam_f12267_0(Map<TupleTag<?>, PValue> pCollections, Function<Map.Entry<TupleTag<?>, PValue>, T> tupleTagExtractor)
{    return pCollections.entrySet().stream().collect(Collectors.toMap(tupleTagExtractor, e -> getCoder((PCollection) e.getValue())));}
 static Map<TupleTag<?>, Coder<?>> beam_f12268_0(AppliedPTransform<?, ?, ?> appliedTransform)
{    return appliedTransform.getOutputs().entrySet().stream().filter(e -> e.getValue() instanceof PCollection).collect(Collectors.toMap(Map.Entry::getKey, e -> ((PCollection) e.getValue()).getCoder()));}
public static WindowedValue.FullWindowedValueCoder beam_f12277_0(WindowedValue.FullWindowedValueCoder elementCoder)
{    return WindowedValue.FullWindowedValueCoder.of(ListCoder.of(elementCoder.getValueCoder()), elementCoder.getWindowCoder());}
public boolean beam_f12278_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    ByteArrayKey that = (ByteArrayKey) o;    return Arrays.equals(value, that.value);}
public Vertex beam_f12287_0(Pipeline pipeline, AppliedPTransform<?, ?, ?> appliedTransform, TransformHierarchy.Node node, JetTranslationContext context)
{    String transformName = appliedTransform.getFullName();    DAGBuilder dagBuilder = context.getDagBuilder();    String vertexId = dagBuilder.newVertexId(transformName);    TestStream<T> testStream = (TestStream<T>) appliedTransform.getTransform();            Map.Entry<TupleTag<?>, PValue> output = Utils.getOutput(appliedTransform);    Coder outputCoder = Utils.getCoder((PCollection) output.getValue());    TestStream.TestStreamCoder<T> payloadCoder = TestStream.TestStreamCoder.of(testStream.getValueCoder());    byte[] encodedPayload = getEncodedPayload(testStream, payloadCoder);    Vertex vertex = dagBuilder.addVertex(vertexId, TestStreamP.supplier(encodedPayload, payloadCoder, outputCoder));    String outputEdgeId = Utils.getTupleTagId(output.getValue());    dagBuilder.registerCollectionOfEdge(outputEdgeId, output.getKey().getId());    dagBuilder.registerEdgeStartPoint(outputEdgeId, vertex, outputCoder);    return vertex;}
private static byte[] beam_f12288_0(TestStream<T> testStream, TestStream.TestStreamCoder<T> coder)
{    try {        return CoderUtils.encodeToByteArray(coder, testStream);    } catch (CoderException e) {        throw ExceptionUtil.rethrow(e);    }}
public boolean beam_f12297_0(Object other)
{    if (other == this) {        return true;    }    if (other instanceof CoderStructuralKey) {        CoderStructuralKey<?> that = (CoderStructuralKey<?>) other;        return structuralValue.equals(that.structuralValue);    }    return false;}
public int beam_f12298_0()
{    return structuralValue.hashCode();}
public CloseableResource<T> beam_f12307_0()
{    checkState(closer != null, "%s has transferred ownership", CloseableResource.class.getName());    checkState(!isClosed, "% is closed", CloseableResource.class.getName());    CloseableResource<T> other = CloseableResource.of(resource, closer);    this.closer = null;    return other;}
public void beam_f12308_0() throws CloseException
{    if (closer != null && !isClosed) {        try {            closer.close(resource);        } catch (Exception e) {            throw new CloseException(e);        } finally {                        isClosed = true;        }    }}
private static State beam_f12318_1(JobApi.JobState.Enum protoState)
{    switch(protoState) {        case UNSPECIFIED:            return State.UNKNOWN;        case STOPPED:            return State.STOPPED;        case RUNNING:            return State.RUNNING;        case DONE:            return State.DONE;        case FAILED:            return State.FAILED;        case CANCELLED:            return State.CANCELLED;        case UPDATED:            return State.UPDATED;        case DRAINING:                        return State.UNKNOWN;        case DRAINED:            return State.UNKNOWN;        case STARTING:            return State.RUNNING;        case CANCELLING:            return State.CANCELLED;        default:                        return State.UNKNOWN;    }}
public static PortableRunner beam_f12319_0(PipelineOptions options)
{    return create(options, ManagedChannelFactory.createDefault());}
public void beam_f12328_0(RunJobRequest request, StreamObserver<RunJobResponse> responseObserver)
{    responseObserver.onNext(RunJobResponse.newBuilder().setJobId(jobId).build());    responseObserver.onCompleted();}
public void beam_f12329_0(GetJobStateRequest request, StreamObserver<GetJobStateResponse> responseObserver)
{    responseObserver.onNext(GetJobStateResponse.newBuilder().setState(jobState).build());    responseObserver.onCompleted();}
public void beam_f12338_0() throws Exception
{    AtomicBoolean closed = new AtomicBoolean(false);    CloseableResource<Foo> original = CloseableResource.of(new Foo(), unused -> closed.set(true));    CloseableResource<Foo> transferred = original.transfer();    transferred.close();    assertThat(closed.get(), is(true));}
public void beam_f12339_0() throws Exception
{    CloseableResource<Foo> foo = CloseableResource.of(new Foo(), unused -> {    });    foo.close();    thrown.expect(IllegalStateException.class);    foo.transfer();}
public void beam_f12348_0()
{    if (this.readerToSsp.isEmpty()) {        throw new IllegalArgumentException("Attempted to call start without assigned system stream partitions");    }    final int capacity = pipelineOptions.getSystemBufferSize();    final FnWithMetricsWrapper metricsWrapper = pipelineOptions.getEnableMetrics() ? new FnWithMetricsWrapper(metricsContainer, stepName) : null;    readerTask = new ReaderTask<>(readerToSsp, capacity, metricsWrapper);    final Thread thread = new Thread(readerTask, "bounded-source-system-consumer-" + NEXT_ID.getAndIncrement());    thread.start();}
public void beam_f12349_0()
{        if (readerTask != null) {        readerTask.stop();    }}
private void beam_f12358_0(Exception exception)
{    this.lastException = exception;            readerToSsp.values().forEach(ssp -> {        final IncomingMessageEnvelope checkLastExceptionEvelope = new IncomingMessageEnvelope(ssp, null, null, null);        enqueueUninterruptibly(checkLastExceptionEvelope);    });}
private void beam_f12359_0(IncomingMessageEnvelope envelope)
{    final BlockingQueue<IncomingMessageEnvelope> queue = queues.get(envelope.getSystemStreamPartition());    while (true) {        try {            queue.put(envelope);            return;        } catch (InterruptedException e) {                                        }    }}
public Map<String, SystemStreamMetadata> beam_f12368_0(Set<String> streamNames)
{    return streamNames.stream().collect(Collectors.toMap(Function.<String>identity(), streamName -> {        try {            final List<UnboundedSource<T, CheckpointMarkT>> splits = split(source, pipelineOptions);            final Map<Partition, SystemStreamPartitionMetadata> partitionMetaData = new HashMap<>();                        for (int i = 0; i < splits.size(); i++) {                partitionMetaData.put(new Partition(i), new SystemStreamPartitionMetadata(null, null, null));            }            return new SystemStreamMetadata(streamName, partitionMetaData);        } catch (Exception e) {            throw new SamzaException("Fail to read stream metadata", e);        }    }));}
public Integer beam_f12369_0(String offset1, String offset2)
{        return null;}
private void beam_f12378_0(UnboundedReader reader) throws InterruptedException
{    @SuppressWarnings("unchecked")    final T value = (T) reader.getCurrent();    final Instant time = reader.getCurrentTimestamp();    final SystemStreamPartition ssp = readerToSsp.get(reader);    final WindowedValue<T> windowedValue = WindowedValue.timestampedValueInGlobalWindow(value, time);    final OpMessage<T> opMessage = OpMessage.ofElement(windowedValue);    final IncomingMessageEnvelope envelope = new IncomingMessageEnvelope(ssp, getOffset(reader), null, opMessage);    queues.get(ssp).put(envelope);}
 void beam_f12379_0()
{    running = false;}
public void beam_f12388_1(ExternalContext externalContext)
{    Thread.setDefaultUncaughtExceptionHandler(new SamzaUncaughtExceptionHandler(() -> {                System.exit(1);    }));    ContainerLaunchUtil.run(appDesc, System.getenv(ShellCommandConfig.ENV_CONTAINER_ID()), ContainerCfgFactory.jobModel);}
public ApplicationStatus beam_f12390_0()
{        return ApplicationStatus.Running;}
public DoFn<InT, OutT> beam_f12399_0()
{    return underlying.getFn();}
private void beam_f12400_0(Runnable runnable)
{    try {        metricsWrapper.wrap(() -> {            runnable.run();            return (Void) null;        });    } catch (Exception e) {        throw new RuntimeException(e);    }}
public void beam_f12409_0(Config config, Context context, Scheduler<KeyedTimerData<Void>> timerRegistry, OpEmitter<OutT> emitter)
{    this.inputWatermark = BoundedWindow.TIMESTAMP_MIN_VALUE;    this.sideInputWatermark = BoundedWindow.TIMESTAMP_MIN_VALUE;    this.pushbackWatermarkHold = BoundedWindow.TIMESTAMP_MAX_VALUE;    final DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());    final SamzaPipelineOptions pipelineOptions = Base64Serializer.deserializeUnchecked(config.get("beamPipelineOptions"), SerializablePipelineOptions.class).get().as(SamzaPipelineOptions.class);    final String stateId = "pardo-" + stepId;    final SamzaStoreStateInternals.Factory<?> nonKeyedStateInternalsFactory = SamzaStoreStateInternals.createStateInternalFactory(stateId, null, context.getTaskContext(), pipelineOptions, signature);    this.timerInternalsFactory = SamzaTimerInternalsFactory.createTimerInternalFactory(keyCoder, (Scheduler) timerRegistry, getTimerStateId(signature), nonKeyedStateInternalsFactory, windowingStrategy, isBounded, pipelineOptions);    this.sideInputHandler = new SideInputHandler(sideInputs, nonKeyedStateInternalsFactory.stateInternalsForKey(null));    if (isPortable) {        SamzaExecutionContext samzaExecutionContext = (SamzaExecutionContext) context.getApplicationContainerContext();        ExecutableStage executableStage = ExecutableStage.fromPayload(stagePayload);        stageBundleFactory = samzaExecutionContext.getJobBundleFactory().forStage(executableStage);        this.fnRunner = SamzaDoFnRunners.createPortable(outputManagerFactory.create(emitter), stageBundleFactory, mainOutputTag, idToTupleTagMap, context, stepName);    } else {        this.fnRunner = SamzaDoFnRunners.create(pipelineOptions, doFn, windowingStrategy, stepName, stateId, context, mainOutputTag, sideInputHandler, timerInternalsFactory, keyCoder, outputManagerFactory.create(emitter), inputCoder, sideOutputTags, outputCoders, doFnSchemaInformation, sideInputMapping);    }    this.pushbackFnRunner = SimplePushbackSideInputDoFnRunner.create(fnRunner, sideInputs, sideInputHandler);    this.pushbackValues = new ArrayList<>();    final Iterator<SamzaDoFnInvokerRegistrar> invokerReg = ServiceLoader.load(SamzaDoFnInvokerRegistrar.class).iterator();    if (!invokerReg.hasNext()) {                doFnInvoker = DoFnInvokers.invokerFor(doFn);    } else {        doFnInvoker = Iterators.getOnlyElement(invokerReg).invokerFor(doFn, context);    }    doFnInvoker.invokeSetup();}
private String beam_f12410_0(DoFnSignature signature)
{    final StringBuilder builder = new StringBuilder("timer");    if (signature.usesTimers()) {        signature.timerDeclarations().keySet().forEach(key -> builder.append(key));    }    return builder.toString();}
public DoFnRunners.OutputManager beam_f12419_0(OpEmitter<OutT> emitter)
{    return new DoFnRunners.OutputManager() {        @Override        public <T> void output(TupleTag<T> tupleTag, WindowedValue<T> windowedValue) {                        @SuppressWarnings("unchecked")            final WindowedValue<OutT> retypedWindowedValue = (WindowedValue<OutT>) windowedValue;            emitter.emitElement(retypedWindowedValue);        }    };}
public void beam_f12420_0(TupleTag<T> tupleTag, WindowedValue<T> windowedValue)
{        @SuppressWarnings("unchecked")    final WindowedValue<OutT> retypedWindowedValue = (WindowedValue<OutT>) windowedValue;    emitter.emitElement(retypedWindowedValue);}
private void beam_f12429_0(Object value)
{    if (value instanceof KeyedWorkItem) {        keyedInternals.setKey(((KeyedWorkItem<?, ?>) value).key());    } else if (value instanceof KeyedTimerData) {        final Object key = ((KeyedTimerData) value).getKey();        if (key != null) {            keyedInternals.setKey(key);        }    } else {        keyedInternals.setKey(((KV<?, ?>) value).getKey());    }}
private void beam_f12430_0()
{    keyedInternals.clearKey();}
 TimerInternals beam_f12439_0()
{    return new KeyedTimerInternals();}
 void beam_f12440_0(K key)
{    checkState(threadLocalKeyedStates.get() == null, "States for key %s is not cleared before processing", key);    threadLocalKeyedStates.set(new KeyedStates<K>(key));}
public void beam_f12449_0(StateNamespace namespace, String timerId)
{    getInternals().deleteTimer(namespace, timerId);}
public void beam_f12450_0(TimerData timerKey)
{    getInternals().deleteTimer(timerKey);}
public boolean beam_f12459_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final KeyedTimerData<?> that = (KeyedTimerData<?>) o;    return Arrays.equals(keyBytes, that.keyBytes) && timerData.equals(that.timerData);}
public int beam_f12460_0()
{    int result = Arrays.hashCode(keyBytes);    result = 31 * result + timerData.hashCode();    return result;}
public final void beam_f12474_0(Scheduler<KeyedTimerData<K>> timerRegistry)
{    assert context != null;    op.open(config, context, timerRegistry, emitter);}
public Collection<OpMessage<OutT>> beam_f12475_1(OpMessage<InT> message)
{    assert outputList.isEmpty();    try {        switch(message.getType()) {            case ELEMENT:                op.processElement(message.getElement(), emitter);                break;            case SIDE_INPUT:                op.processSideInput(message.getViewId(), message.getViewElements(), emitter);                break;            case SIDE_INPUT_WATERMARK:                op.processSideInputWatermark(message.getSideInputWatermark(), emitter);                break;            default:                throw new IllegalArgumentException(String.format("Unexpected input type: %s", message.getType()));        }    } catch (Exception e) {                throw UserCodeException.wrap(e);    }    final List<OpMessage<OutT>> results = new ArrayList<>(outputList);    outputList.clear();    return results;}
public static OpMessage<T> beam_f12484_0(String viewId, WindowedValue<? extends Iterable<ElemT>> elements)
{    return new OpMessage<>(Type.SIDE_INPUT, null, viewId, elements, null);}
public static OpMessage<T> beam_f12485_0(Instant watermark)
{    return new OpMessage<>(Type.SIDE_INPUT_WATERMARK, null, null, null, watermark);}
public String beam_f12494_0()
{    return "OpMessage{" + "type=" + type + ", element=" + element + ", viewId='" + viewId + '\'' + ", viewElements=" + viewElements + '}';}
public InT beam_f12495_0()
{    return value.getValue();}
public void beam_f12505_0(WindowedValue<InT> elem)
{    try {        OutputReceiverFactory receiverFactory = new OutputReceiverFactory() {            @Override            public FnDataReceiver<FnOutT> create(String pCollectionId) {                return (receivedElement) -> {                                        outputQueue.put(KV.of(pCollectionId, receivedElement));                };            }        };        try (RemoteBundle bundle = stageBundleFactory.getBundle(receiverFactory, StateRequestHandler.unsupported(), BundleProgressHandler.ignored())) {            Iterables.getOnlyElement(bundle.getInputReceivers().values()).accept(elem);        }                KV<String, FnOutT> result;        while ((result = outputQueue.poll()) != null) {            outputManager.output(idToTupleTagMap.get(result.getKey()), (WindowedValue) result.getValue());        }    } catch (Exception e) {        throw new RuntimeException(e);    }}
public FnDataReceiver<FnOutT> beam_f12506_0(String pCollectionId)
{    return (receivedElement) -> {                outputQueue.put(KV.of(pCollectionId, receivedElement));    };}
public SetState<T> beam_f12517_0(StateTag<SetState<T>> spec, Coder<T> elemCoder)
{    return new SamzaSetStateImpl<>(namespace, address, elemCoder);}
public MapState<KeyT, ValueT> beam_f12518_0(StateTag<MapState<KeyT, ValueT>> spec, Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder)
{    return new SamzaMapStateImpl<>(namespace, address, mapKeyCoder, mapValueCoder);}
protected ReadableState<Boolean> beam_f12527_0()
{    return new ReadableState<Boolean>() {        @Override        public Boolean read() {            return store.get(getEncodedStoreKey()) == null;        }        @Override        public ReadableState<Boolean> readLater() {            return this;        }    };}
public Boolean beam_f12528_0()
{    return store.get(getEncodedStoreKey()) == null;}
public ValueState<T> beam_f12537_0()
{    return this;}
public void beam_f12538_0()
{    clearInternal();}
public ReadableState<Boolean> beam_f12547_0(T t)
{    return mapState.putIfAbsent(t, true);}
public void beam_f12548_0(T t)
{    mapState.remove(t);}
public Iterator<T> beam_f12557_0()
{    return new Iterator<T>() {        @Override        public boolean hasNext() {            return iter.hasNext();        }        @Override        public T next() {            return iter.next().getKey();        }    };}
public boolean beam_f12558_0()
{    return iter.hasNext();}
public Iterable<KeyT> beam_f12567_0()
{    return createIterable(entry -> decodeKey(entry.getKey()));}
public ReadableState<Iterable<KeyT>> beam_f12568_0()
{    return this;}
public boolean beam_f12577_0()
{    boolean hasNext = kvIter.hasNext();    if (!hasNext) {        kvIter.close();        openIterators.remove(kvIter);    }    return hasNext;}
public Map.Entry<KeyT, ValueT> beam_f12578_0()
{    Entry<byte[], byte[]> entry = kvIter.next();    return new AbstractMap.SimpleEntry<>(decodeKey(entry.getKey()), decodeValue(entry.getValue()));}
private byte[] beam_f12587_0()
{    byte[] maxKey = new byte[maxKeySize];    Arrays.fill(maxKey, (byte) 0xff);    final byte[] encodedKey = getEncodedStoreKey();    System.arraycopy(encodedKey, 0, maxKey, 0, encodedKey.length);    return maxKey;}
public void beam_f12588_0()
{    openIterators.forEach(KeyValueIterator::close);    openIterators.clear();}
public void beam_f12597_0(Instant value)
{    final Instant currentValue = readInternal();    final Instant combinedValue = currentValue == null ? value : timestampCombiner.combine(currentValue, value);    if (!combinedValue.equals(currentValue)) {        writeInternal(combinedValue);    }}
public ReadableState<Boolean> beam_f12598_0()
{    return isEmptyInternal();}
public Collection<KeyedTimerData<K>> beam_f12607_0()
{    final Collection<KeyedTimerData<K>> readyTimers = new ArrayList<>();    while (!eventTimeTimers.isEmpty() && eventTimeTimers.first().getTimerData().getTimestamp().isBefore(inputWatermark)) {        final KeyedTimerData<K> keyedTimerData = eventTimeTimers.pollFirst();        readyTimers.add(keyedTimerData);        state.deletePersisted(keyedTimerData);                if (eventTimeTimers.isEmpty()) {            state.loadEventTimeTimers();        }    }    return readyTimers;}
public void beam_f12608_0(KeyedTimerData<K> keyedTimerData)
{    state.deletePersisted(keyedTimerData);}
public Instant beam_f12617_0()
{    throw new UnsupportedOperationException(String.format("%s does not currently support synchronized processing time", SamzaRunner.class));}
public Instant beam_f12618_0()
{    return inputWatermark;}
public Iterable<WindowedValue<V>> beam_f12627_0()
{    return Collections.singletonList(value);}
public void beam_f12628_0(WindowedValue<T> inputElement, OpEmitter<T> emitter)
{    final Collection<W> windows;    try {        windows = windowFn.assignWindows(new SamzaAssignContext<>(windowFn, inputElement));    } catch (Exception e) {        throw new RuntimeException(e);    }    windows.stream().map(window -> WindowedValue.of(inputElement.getValue(), inputElement.getTimestamp(), window, inputElement.getPane())).forEach(outputElement -> emitter.emitElement(outputElement));}
public SamzaExecutionContext beam_f12637_0(ExternalContext externalContext, JobContext jobContext, ContainerContext containerContext)
{    final MetricsRegistryMap metricsRegistry = (MetricsRegistryMap) containerContext.getContainerMetricsRegistry();    SamzaExecutionContext.this.setMetricsContainer(new SamzaMetricsContainer(metricsRegistry));    return SamzaExecutionContext.this;}
public static void beam_f12638_0(String[] args) throws Exception
{    SamzaPortablePipelineOptions pipelineOptions = PipelineOptionsFactory.fromArgs(args).as(SamzaPortablePipelineOptions.class);    fromOptions(pipelineOptions).run();}
public State beam_f12647_0()
{    return waitUntilFinish(null);}
public MetricResults beam_f12648_0()
{    return asAttemptedOnlyMetricResults(executionContext.getMetricsContainer().getContainers());}
private ApplicationRunner beam_f12657_0(StreamApplication app, Config config)
{    final ApplicationRunner runner = ApplicationRunners.getApplicationRunner(app, config);    ExternalContext externalContext = null;    if (listener != null) {        externalContext = listener.onStart();    }    runner.run(externalContext);    if (listener != null && options.getSamzaExecutionEnvironment() == SamzaExecutionEnvironment.YARN) {        listener.onSubmit();    }    return runner;}
private static boolean beam_f12658_0(SamzaPipelineOptions options, String configKey)
{    if (options == null || options.getConfigOverride() == null) {        return false;    }    return options.getConfigOverride().containsKey(configKey);}
public static GroupWithoutRepartition<InputT, OutputT> beam_f12667_0(PTransform<InputT, OutputT> transform)
{    return new GroupWithoutRepartition<>(transform);}
public OutputT beam_f12668_0(InputT input)
{    if (input instanceof PCollection) {        return (OutputT) ((PCollection) input).apply(transform);    } else if (input instanceof KeyedPCollectionTuple) {        return (OutputT) ((KeyedPCollectionTuple) input).apply(transform);    } else {        throw new RuntimeException(transform.getName() + " is not supported with " + GroupWithoutRepartition.class.getSimpleName());    }}
public static Map<String, String> beam_f12677_0()
{        return ImmutableMap.<String, String>builder().put(APP_RUNNER_CLASS, LocalApplicationRunner.class.getName()).put(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, ZkJobCoordinatorFactory.class.getName()).build();}
private static Map<String, String> beam_f12678_1(SamzaPipelineOptions options)
{    ImmutableMap.Builder<String, String> configBuilder = ImmutableMap.<String, String>builder().put("stores.beamStore.factory", "org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory").put("stores.beamStore.key.serde", "byteSerde").put("stores.beamStore.msg.serde", "byteSerde").put("serializers.registry.byteSerde.class", ByteSerdeFactory.class.getName());    if (options.getStateDurable()) {                configBuilder.put("stores.beamStore.changelog", getChangelogTopic(options, "beamStore"));        configBuilder.put("job.host-affinity.enabled", "true");    }        switch(options.getSamzaExecutionEnvironment()) {        case YARN:            configBuilder.putAll(yarnRunConfig());            break;        case STANDALONE:            configBuilder.putAll(standAloneRunConfig());            break;        default:                        configBuilder.putAll(localRunConfig());            break;    }        configBuilder.put("samza.li.task.wrapper.enabled", "false");    return configBuilder.build();}
public void beam_f12687_0(Flatten.PCollections<T> transform, TransformHierarchy.Node node, TranslationContext ctx)
{    doTranslate(transform, node, ctx);}
private static void beam_f12688_0(Flatten.PCollections<T> transform, TransformHierarchy.Node node, TranslationContext ctx)
{    final PCollection<T> output = ctx.getOutput(transform);    final List<MessageStream<OpMessage<T>>> inputStreams = new ArrayList<>();    for (Map.Entry<TupleTag<?>, PValue> taggedPValue : node.getInputs().entrySet()) {        if (!(taggedPValue.getValue() instanceof PCollection)) {            throw new IllegalArgumentException(String.format("Got non-PCollection input for flatten. Tag: %s. Input: %s. Type: %s", taggedPValue.getKey(), taggedPValue.getValue(), taggedPValue.getValue().getClass()));        }        @SuppressWarnings("unchecked")        final PCollection<T> input = (PCollection<T>) taggedPValue.getValue();        inputStreams.add(ctx.getMessageStream(input));    }    if (inputStreams.isEmpty()) {                final MessageStream<OpMessage<T>> noOpStream = ctx.getDummyStream().flatMap(OpAdapter.adapt((Op<String, T, Void>) (inputElement, emitter) -> {        }));        ctx.registerMessageStream(output, noOpStream);        return;    }    ctx.registerMessageStream(output, mergeInputStreams(inputStreams));}
private static SystemReduceFn<K, InputT, ?, OutputT, BoundedWindow> beam_f12697_0(PTransform<PCollection<KV<K, InputT>>, PCollection<KV<K, OutputT>>> transform, Pipeline pipeline, KvCoder<K, InputT> kvInputCoder)
{    if (transform instanceof GroupByKey) {        return (SystemReduceFn<K, InputT, ?, OutputT, BoundedWindow>) SystemReduceFn.buffering(kvInputCoder.getValueCoder());    } else if (transform instanceof Combine.PerKey) {        final CombineFnBase.GlobalCombineFn<? super InputT, ?, OutputT> combineFn = ((Combine.PerKey) transform).getFn();        return SystemReduceFn.combining(kvInputCoder.getKeyCoder(), AppliedCombineFn.withInputCoder(combineFn, pipeline.getCoderRegistry(), kvInputCoder));    } else {        throw new RuntimeException("Transform " + transform + " cannot be translated as GroupByKey.");    }}
private static boolean beam_f12698_0(TransformHierarchy.Node node, TranslationContext ctx)
{    if (ctx.getPipelineOptions().getMaxSourceParallelism() == 1) {                return false;    }    if (node == null) {        return true;    }    if (node.getTransform() instanceof GroupWithoutRepartition) {        return false;    } else {        return needRepartition(node.getEnclosingNode(), ctx);    }}
public Long beam_f12707_0()
{        return Long.MAX_VALUE;}
public void beam_f12708_0(WindowedValue<RawUnionValue> inputElement, OpEmitter<OutT> emitter)
{    @SuppressWarnings("unchecked")    final OutT value = (OutT) inputElement.getValue().getValue();    emitter.emitElement(inputElement.withValue(value));}
public void beam_f12718_0(String id, MessageStream<OpMessage<T>> stream)
{    if (messsageStreams.containsKey(id)) {        throw new IllegalArgumentException("Stream already registered for id: " + id);    }    messsageStreams.put(id, stream);}
public OutputStream<OutT> beam_f12719_0(OutputDescriptor<OutT, ?> outputDescriptor)
{    return appDescriptor.getOutputStream(outputDescriptor);}
public SystemConsumer beam_f12728_0(String systemName, Config config, MetricsRegistry metricsRegistry)
{    return new SamzaImpulseSystemConsumer();}
public SystemProducer beam_f12729_0(String systemName, Config config, MetricsRegistry metricsRegistry)
{    throw new UnsupportedOperationException("SamzaImpulseSystem doesn't support producing");}
public void beam_f12741_0(T transform, TransformHierarchy.Node node, Pipeline pipeline, TransformTranslator<T> translator)
{    ctx.setCurrentTransform(node.toAppliedPTransform(pipeline));    ctx.setCurrentTopologicalId(topologicalId++);    translator.translate(transform, node, ctx);    ctx.clearCurrentTransform();}
public static void beam_f12742_0(Pipeline pipeline, SamzaPipelineOptions options, Map<PValue, String> idMap, ConfigBuilder configBuilder)
{    final ConfigContext ctx = new ConfigContext(idMap, options);    final TransformVisitorFn configFn = new TransformVisitorFn() {        @Override        public <T extends PTransform<?, ?>> void apply(T transform, TransformHierarchy.Node node, Pipeline pipeline, TransformTranslator<T> translator) {            ctx.setCurrentTransform(node.toAppliedPTransform(pipeline));            if (translator instanceof TransformConfigGenerator) {                TransformConfigGenerator<T> configGenerator = (TransformConfigGenerator<T>) translator;                configBuilder.putAll(configGenerator.createConfig(transform, node, ctx));            }            ctx.clearCurrentTransform();        }    };    final SamzaPipelineVisitor visitor = new SamzaPipelineVisitor(configFn);    pipeline.traverseTopologically(visitor);}
private static Map<String, TransformTranslator<?>> beam_f12751_1()
{    Map<String, TransformTranslator<?>> translators = new HashMap<>();    for (SamzaTranslatorRegistrar registrar : ServiceLoader.load(SamzaTranslatorRegistrar.class)) {        translators.putAll(registrar.getTransformTranslators());    }        return ImmutableMap.copyOf(translators);}
public static void beam_f12752_1(RunnerApi.Pipeline pipeline, PortableTranslationContext ctx)
{    QueryablePipeline queryablePipeline = QueryablePipeline.forTransforms(pipeline.getRootTransformIdsList(), pipeline.getComponents());    int topologicalId = 0;    for (PipelineNode.PTransformNode transform : queryablePipeline.getTopologicallyOrderedTransforms()) {        ctx.setCurrentTopologicalId(topologicalId++);                TRANSLATORS.get(transform.getTransform().getSpec().getUrn()).translatePortable(transform, queryablePipeline, ctx);    }}
public List<T> beam_f12761_0(List<T> accumulator, T input)
{    accumulator.add(input);    return accumulator;}
public List<T> beam_f12762_0(Iterable<List<T>> accumulators)
{    List<T> result = createAccumulator();    for (List<T> accumulator : accumulators) {        result.addAll(accumulator);    }    return result;}
 void beam_f12771_0(T transform, TransformHierarchy.Node node, TranslationContext ctx)
{    throw new UnsupportedOperationException("Java translation is not supported for " + this.getClass().getSimpleName());}
 void beam_f12772_0(PipelineNode.PTransformNode transform, QueryablePipeline pipeline, PortableTranslationContext ctx)
{    throw new UnsupportedOperationException("Portable translation is not supported for " + this.getClass().getSimpleName());}
public void beam_f12781_0()
{    this.currentTransform = null;}
public AppliedPTransform<?, ?, ?> beam_f12782_0()
{    return currentTransform;}
private static MessageStream<T> beam_f12791_0(MessageStream<org.apache.samza.operators.KV<?, T>> input)
{    return input.map(org.apache.samza.operators.KV::getValue);}
public String beam_f12792_0(PValue pvalue)
{    final String id = idMap.get(pvalue);    if (id == null) {        throw new IllegalArgumentException("No id mapping for value: " + pvalue);    }    return id;}
public static WindowingStrategy<?, BoundedWindow> beam_f12801_0(PipelineNode.PTransformNode transform, QueryablePipeline pipeline)
{    String inputId = Iterables.getOnlyElement(transform.getTransform().getInputsMap().values());    RehydratedComponents rehydratedComponents = RehydratedComponents.forComponents(pipeline.getComponents());    RunnerApi.WindowingStrategy windowingStrategyProto = pipeline.getComponents().getWindowingStrategiesOrThrow(pipeline.getComponents().getPcollectionsOrThrow(inputId).getWindowingStrategyId());    WindowingStrategy<?, ?> windowingStrategy;    try {        windowingStrategy = WindowingStrategyTranslation.fromProto(windowingStrategyProto, rehydratedComponents);    } catch (InvalidProtocolBufferException e) {        throw new IllegalStateException(String.format("Unable to hydrate GroupByKey windowing strategy %s.", windowingStrategyProto), e);    }    @SuppressWarnings("unchecked")    WindowingStrategy<?, BoundedWindow> ret = (WindowingStrategy<?, BoundedWindow>) windowingStrategy;    return ret;}
public static String beam_f12802_0(String name)
{    return name.replaceAll("[\\.(/]", "-").replaceAll("[^A-Za-z0-9-_]", "");}
public void beam_f12811_0() throws IOException, InterruptedException
{    final TestBoundedSource.SplittableBuilder<String> builder = TestBoundedSource.<String>createSplits(3);    builder.forSplit(0).addElements("split-0");    builder.forSplit(1).addElements("split-1");    builder.forSplit(2).addElements("split-2");    final TestBoundedSource<String> source = builder.build();    final BoundedSourceSystem.Consumer<String> consumer = createConsumer(source, 3);    consumer.register(ssp(0), NULL_STRING);    consumer.register(ssp(1), NULL_STRING);    consumer.register(ssp(2), NULL_STRING);    consumer.start();    final Set<String> offsets = new HashSet<>();        List<IncomingMessageEnvelope> envelopes = consumeUntilTimeoutOrEos(consumer, ssp(0), DEFAULT_TIMEOUT_MILLIS);    assertEquals(Arrays.asList(createElementMessage(ssp(0), envelopes.get(0).getOffset(), "split-0", BoundedWindow.TIMESTAMP_MIN_VALUE), createWatermarkMessage(ssp(0), BoundedWindow.TIMESTAMP_MAX_VALUE), createEndOfStreamMessage(ssp(0))), envelopes);    offsets.add(envelopes.get(0).getOffset());        envelopes = consumeUntilTimeoutOrEos(consumer, ssp(1), DEFAULT_TIMEOUT_MILLIS);    assertEquals(Arrays.asList(createElementMessage(ssp(1), envelopes.get(0).getOffset(), "split-1", BoundedWindow.TIMESTAMP_MIN_VALUE), createWatermarkMessage(ssp(1), BoundedWindow.TIMESTAMP_MAX_VALUE), createEndOfStreamMessage(ssp(1))), envelopes);    offsets.add(envelopes.get(0).getOffset());        envelopes = consumeUntilTimeoutOrEos(consumer, ssp(2), DEFAULT_TIMEOUT_MILLIS);    assertEquals(Arrays.asList(createElementMessage(ssp(2), envelopes.get(0).getOffset(), "split-2", BoundedWindow.TIMESTAMP_MIN_VALUE), createWatermarkMessage(ssp(2), BoundedWindow.TIMESTAMP_MAX_VALUE), createEndOfStreamMessage(ssp(2))), envelopes);    offsets.add(envelopes.get(0).getOffset());        assertEquals(Sets.newHashSet("0", "1", "2"), offsets);    consumer.stop();}
private static List<IncomingMessageEnvelope> beam_f12812_0(SystemConsumer consumer, SystemStreamPartition ssp, long timeoutMillis) throws InterruptedException
{    assertTrue("Expected timeoutMillis (" + timeoutMillis + ") >= 0", timeoutMillis >= 0);    final List<IncomingMessageEnvelope> accumulator = new ArrayList<>();    final long start = System.currentTimeMillis();    long now = start;    while (timeoutMillis + start >= now) {        accumulator.addAll(pollOnce(consumer, ssp, now - start - timeoutMillis));        if (!accumulator.isEmpty() && accumulator.get(accumulator.size() - 1).isEndOfStream()) {            break;        }        now = System.currentTimeMillis();    }    return accumulator;}
public BoundedReader<T> beam_f12821_0(PipelineOptions options) throws IOException
{    assert events.size() == 1;    return new Reader(events.get(0));}
public TestBoundedSource<T> beam_f12823_0()
{    return new TestBoundedSource<>(Collections.singletonList(getEvents()));}
public final SourceBuilder<T, W> beam_f12834_0(T... elements)
{    for (T element : elements) {        events.add(new ElementEvent<>(element, currentTimestamp));    }    return this;}
public SourceBuilder<T, W> beam_f12835_0(IOException exception)
{    events.add(new ExceptionEvent<>(exception));    return this;}
 static void beam_f12844_0(Exception expectedException, Callable<T> callable) throws Exception
{    try {        callable.call();        fail("Expected exception (" + expectedException + "), but no exception was thrown");    } catch (Exception e) {        Throwable currentException = e;        while (currentException != null) {            if (currentException.equals(expectedException)) {                return;            }            currentException = currentException.getCause();        }        assertEquals(expectedException, e);    }}
public static Builder<T> beam_f12845_0()
{    return new Builder<>();}
public boolean beam_f12855_0() throws IOException
{    if (!started) {        throw new IllegalStateException("Advance called when reader was not started");    }    for (++index; index < events.size(); ++index) {        final Event<T> event = events.get(index);        if (event instanceof ExceptionEvent) {            throw ((ExceptionEvent<T>) event).exception;        } else if (event instanceof LatchEvent) {            try {                ((LatchEvent) event).latch.await();            } catch (InterruptedException e) {                                Thread.currentThread().interrupt();            }        } else if (event instanceof WatermarkEvent) {            watermark = ((WatermarkEvent) event).watermark;        } else if (event instanceof NoElementEvent) {            return false;        } else {            curTime = ((ElementEvent<T>) event).timestamp;            ++offset;            return true;        }    }    return false;}
public T beam_f12856_0() throws NoSuchElementException
{    if (!started) {        throw new NoSuchElementException();    }    final Event<T> event = events.get(index);    assert event instanceof ElementEvent;    return ((ElementEvent<T>) event).element;}
public void beam_f12866_0() throws IOException, InterruptedException
{    final Instant now = Instant.now();    final Instant nowPlusOne = now.plus(1L);    final TestUnboundedSource<String> source = TestUnboundedSource.<String>createBuilder().setTimestamp(now).addElements("first").setTimestamp(nowPlusOne).addElements("second").advanceWatermarkTo(now).build();    final UnboundedSourceSystem.Consumer<String, TestCheckpointMark> consumer = createConsumer(source);    consumer.register(DEFAULT_SSP, NULL_STRING);    consumer.start();    assertEquals(Arrays.asList(createElementMessage(DEFAULT_SSP, offset(0), "first", now), createElementMessage(DEFAULT_SSP, offset(1), "second", nowPlusOne), createWatermarkMessage(DEFAULT_SSP, now)), consumeUntilTimeoutOrWatermark(consumer, DEFAULT_SSP, DEFAULT_WATERMARK_TIMEOUT_MILLIS));    consumer.stop();}
public void beam_f12867_0() throws IOException, InterruptedException
{    final Instant now = Instant.now();    final Instant nowPlusOne = now.plus(1L);    final Instant nowPlusTwo = now.plus(2L);    final TestUnboundedSource<String> source = TestUnboundedSource.<String>createBuilder().setTimestamp(now).addElements("first").advanceWatermarkTo(now).noElements().setTimestamp(nowPlusOne).addElements("second").setTimestamp(nowPlusTwo).addElements("third").advanceWatermarkTo(nowPlusOne).build();    final UnboundedSourceSystem.Consumer<String, TestCheckpointMark> consumer = createConsumer(source);    consumer.register(DEFAULT_SSP, NULL_STRING);    consumer.start();        assertEquals(Arrays.asList(createElementMessage(DEFAULT_SSP, offset(0), "first", now), createWatermarkMessage(DEFAULT_SSP, now)), consumeUntilTimeoutOrWatermark(consumer, DEFAULT_SSP, DEFAULT_WATERMARK_TIMEOUT_MILLIS));        assertEquals(Arrays.asList(createElementMessage(DEFAULT_SSP, offset(1), "second", nowPlusOne), createElementMessage(DEFAULT_SSP, offset(2), "third", nowPlusTwo), createWatermarkMessage(DEFAULT_SSP, nowPlusOne)), consumeUntilTimeoutOrWatermark(consumer, DEFAULT_SSP, DEFAULT_WATERMARK_TIMEOUT_MILLIS));    consumer.stop();}
private static List<IncomingMessageEnvelope> beam_f12876_0(SystemConsumer consumer, SystemStreamPartition ssp, long timeoutMillis) throws InterruptedException
{    final Set<SystemStreamPartition> sspSet = Collections.singleton(ssp);    final Map<SystemStreamPartition, List<IncomingMessageEnvelope>> pollResult = consumer.poll(sspSet, timeoutMillis);    assertEquals(sspSet, pollResult.keySet());    assertNotNull(pollResult.get(ssp));    return pollResult.get(ssp);}
private static String beam_f12877_0(int offset) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    CHECKPOINT_MARK_CODER.encode(TestCheckpointMark.of(offset), baos);    return Base64.getEncoder().encodeToString(baos.toByteArray());}
public KeyValueIterator<byte[], byte[]> beam_f12886_0(byte[] from, byte[] to)
{    TestKeyValueIteraor iter = new TestKeyValueIteraor(super.range(from, to));    iterators.add(iter);    return iter;}
public void beam_f12887_0()
{    iter.close();    closed = true;}
public void beam_f12896_0(KeyedTimerData<String> key)
{    timers.remove(key);}
public void beam_f12897_0()
{    final SamzaPipelineOptions pipelineOptions = PipelineOptionsFactory.create().as(SamzaPipelineOptions.class);    pipelineOptions.setTimerBufferSize(1);    final RocksDbKeyValueStore store = createStore("store1");    final SamzaTimerInternalsFactory<String> timerInternalsFactory = createTimerInternalsFactory(null, "timer", pipelineOptions, store);    final StateNamespace nameSpace = StateNamespaces.global();    final TimerInternals timerInternals = timerInternalsFactory.timerInternalsForKey("testKey");    final TimerInternals.TimerData timer1 = TimerInternals.TimerData.of("timer1", nameSpace, new Instant(10), TimeDomain.EVENT_TIME);    timerInternals.setTimer(timer1);    final TimerInternals.TimerData timer2 = TimerInternals.TimerData.of("timer2", nameSpace, new Instant(100), TimeDomain.EVENT_TIME);    timerInternals.setTimer(timer2);    timerInternalsFactory.setInputWatermark(new Instant(5));    Collection<KeyedTimerData<String>> readyTimers = timerInternalsFactory.removeReadyTimers();    assertTrue(readyTimers.isEmpty());    timerInternalsFactory.setInputWatermark(new Instant(20));    readyTimers = timerInternalsFactory.removeReadyTimers();    assertEquals(1, readyTimers.size());    assertEquals(timer1, readyTimers.iterator().next().getTimerData());    timerInternalsFactory.setInputWatermark(new Instant(150));    readyTimers = timerInternalsFactory.removeReadyTimers();    assertEquals(1, readyTimers.size());    assertEquals(timer2, readyTimers.iterator().next().getTimerData());    store.close();}
private SystemStreamPartition beam_f12907_0(int i)
{    return new SystemStreamPartition("default-system", "default-stream", new Partition(i));}
public static void beam_f12908_1(SparkPipelineOptions opts, JavaSparkContext jsc)
{    if (instance == null) {        synchronized (AggregatorsAccumulator.class) {            if (instance == null) {                Optional<CheckpointDir> maybeCheckpointDir = opts.isStreaming() ? Optional.of(new CheckpointDir(opts.getCheckpointDir())) : Optional.absent();                NamedAggregators namedAggregators = new NamedAggregators();                NamedAggregatorsAccumulator accumulator = new NamedAggregatorsAccumulator(namedAggregators);                jsc.sc().register(accumulator, ACCUMULATOR_NAME);                if (maybeCheckpointDir.isPresent()) {                    Optional<NamedAggregators> maybeRecoveredValue = recoverValueFromCheckpoint(jsc, maybeCheckpointDir.get());                    if (maybeRecoveredValue.isPresent()) {                        accumulator = new NamedAggregatorsAccumulator(maybeRecoveredValue.get());                    }                }                instance = accumulator;            }        }            }}
private static State<InputT, InterT, OutputT> beam_f12917_0(State<?, ?, ?> s1, State<?, ?, ?> s2)
{    return ((State<InputT, InterT, OutputT>) s1).merge((State<InputT, InterT, OutputT>) s2);}
public String beam_f12918_0()
{    StringBuilder sb = new StringBuilder();    for (Map.Entry<String, State<?, ?, ?>> e : mNamedAggregators.entrySet()) {        sb.append(e.getKey()).append(": ").append(e.getValue().render()).append(" ");    }    return sb.toString();}
public static T beam_f12927_0(byte[] serialized, Coder<T> coder)
{    ByteArrayInputStream bais = new ByteArrayInputStream(serialized);    try {        return coder.decode(bais);    } catch (IOException e) {        throw new IllegalStateException("Error decoding bytes for coder: " + coder, e);    }}
public static Iterable<T> beam_f12928_0(Collection<byte[]> serialized, final Coder<T> coder)
{    return serialized.stream().map(bytes -> fromByteArray(checkNotNull(bytes, "Cannot decode null values."), coder)).collect(Collectors.toList());}
public Object beam_f12937_0(Kryo kryo, Input input, Class type)
{    try {        return new ObjectInputStreamWithClassLoader(input, kryo.getClassLoader()).readObject();    } catch (Exception e) {        throw new KryoException("Error during Java deserialization.", e);    }}
protected Class<?> beam_f12938_0(ObjectStreamClass desc)
{    try {        return Class.forName(desc.getName(), false, classLoader);    } catch (ClassNotFoundException e) {        throw new RuntimeException("Could not find class: " + desc.getName(), e);    }}
public static CreateStream<T> beam_f12947_0(Coder<T> coder, Duration batchDuration, boolean forceWatermarkSync)
{    return new CreateStream<>(batchDuration, new Instant(0), coder, forceWatermarkSync);}
public static CreateStream<T> beam_f12948_0(Coder<T> coder, Duration batchDuration)
{    return of(coder, batchDuration, true);}
public Queue<Iterable<TimestampedValue<T>>> beam_f12957_0()
{    return batches;}
public Queue<SparkWatermarks> beam_f12958_0()
{    return times;}
public Source.Reader<T> beam_f12968_0(final PipelineOptions options, final CheckpointMarkT checkpointMark) throws IOException
{    try {        initReaderCache((long) readerCacheInterval);        return (Source.Reader<T>) readerCache.get(this, new ReaderLoader(options, checkpointMark));    } catch (final ExecutionException e) {        throw new RuntimeException("Failed to get or create reader", e);    }}
public void beam_f12969_0()
{    source.validate();}
public boolean beam_f12978_0() throws IOException
{    if (recordsRead >= maxNumRecords) {        finalizeCheckpoint();        return false;    } else {        return advanceWithBackoff();    }}
private boolean beam_f12979_0() throws IOException
{        final BackOff backoff = backoffFactory.backoff();    long nextSleep = backoff.nextBackOffMillis();    while (nextSleep != BackOff.STOP) {        if (readEndTime != null && Instant.now().isAfter(readEndTime)) {            finalizeCheckpoint();            return false;        }        if (unboundedReader.advance()) {            recordsRead++;            return true;        }        Uninterruptibles.sleepUninterruptibly(nextSleep, TimeUnit.MILLISECONDS);        nextSleep = backoff.nextBackOffMillis();    }    finalizeCheckpoint();    return false;}
public void beam_f12988_0(final RemovalNotification<MicrobatchSource<?, ?>, Source.Reader<?>> notification)
{    try {        notification.getValue().close();    } catch (final IOException e) {        throw new RuntimeException(e);    }}
public static void beam_f12989_0()
{    synchronized (MicrobatchSource.class) {        readerCache.invalidateAll();    }}
public Partition[] beam_f13001_1()
{    try {        long desiredSizeBytes = (bundleSize > 0) ? bundleSize : DEFAULT_BUNDLE_SIZE;        if (bundleSize == 0) {            try {                desiredSizeBytes = source.getEstimatedSizeBytes(options.get()) / numPartitions;            } catch (Exception e) {                            }        }        List<? extends Source<T>> partitionedSources = source.split(desiredSizeBytes, options.get());        Partition[] partitions = new SourcePartition[partitionedSources.size()];        for (int i = 0; i < partitionedSources.size(); i++) {            partitions[i] = new SourcePartition<>(id(), i, partitionedSources.get(i));        }        return partitions;    } catch (Exception e) {        throw new RuntimeException("Failed to create partitions for source " + source.getClass().getSimpleName(), e);    }}
private BoundedSource.BoundedReader<T> beam_f13002_0(SourcePartition<T> partition)
{    try {        return ((BoundedSource<T>) partition.source).createReader(options.get());    } catch (IOException e) {        throw new RuntimeException("Failed to create reader from a BoundedSource.", e);    }}
public void beam_f13011_0()
{    throw new UnsupportedOperationException();}
public int beam_f13012_0()
{    return index;}
public Duration beam_f13021_0()
{    return parent.slideDuration();}
public scala.collection.immutable.List<DStream<?>> beam_f13022_0()
{    return scala.collection.JavaConversions.asScalaBuffer(Collections.<DStream<?>>singletonList(parent)).toList();}
public Iterator<byte[]> beam_f13031_0(Tuple2<Iterable<byte[]>, Metadata> t2) throws Exception
{    return t2._1().iterator();}
public static AggregatorMetric beam_f13032_0(final NamedAggregators namedAggregators)
{    return new AggregatorMetric(namedAggregators);}
public static void beam_f13041_0()
{    synchronized (MetricsAccumulator.class) {        instance = null;    }}
private static void beam_f13042_0() throws IOException
{    if (checkpointFilePath != null) {        Checkpoint.writeObject(fileSystem, checkpointFilePath, instance.value());    }}
 String beam_f13051_0(MetricResult<?> metricResult)
{    MetricKey key = metricResult.getKey();    MetricName name = key.metricName();    String step = key.stepName();    ArrayList<String> pieces = new ArrayList<>();    if (step != null) {        step = step.replaceAll(ILLEGAL_CHARACTERS, "_");        if (step.endsWith("_")) {            step = step.substring(0, step.length() - 1);        }        pieces.add(step);    }    pieces.addAll(ImmutableList.of(name.getNamespace(), name.getName()).stream().map(str -> str.replaceAll(ILLEGAL_CHARACTERS, "_")).collect(toList()));    return String.join(".", pieces);}
public String beam_f13052_0()
{    return name;}
public SortedMap<String, Counter> beam_f13061_0(final MetricFilter filter)
{    return internalMetricRegistry.getCounters(filter);}
public SortedMap<String, Gauge> beam_f13062_0(final MetricFilter filter)
{    return new ImmutableSortedMap.Builder<String, Gauge>(Ordering.from(String.CASE_INSENSITIVE_ORDER)).putAll(internalMetricRegistry.getGauges(filter)).putAll(extractGauges(internalMetricRegistry, filter)).build();}
public static SparkJobInvoker beam_f13071_0(SparkJobServerDriver.SparkServerConfiguration configuration)
{    return new SparkJobInvoker(configuration);}
protected JobInvocation beam_f13072_1(Pipeline pipeline, Struct options, @Nullable String retrievalToken, ListeningExecutorService executorService)
{    LOG.trace("Parsing pipeline options");    SparkPipelineOptions sparkOptions = PipelineOptionsTranslation.fromProto(options).as(SparkPipelineOptions.class);    String invocationId = String.format("%s_%s", sparkOptions.getJobName(), UUID.randomUUID().toString());        if (sparkOptions.getSparkMaster().equals(SparkPipelineOptions.DEFAULT_MASTER_URL)) {        sparkOptions.setSparkMaster(configuration.getSparkMasterUrl());    }        sparkOptions.setRunner(null);    if (sparkOptions.getAppName() == null) {                sparkOptions.setAppName(invocationId);    }    return createJobInvocation(invocationId, retrievalToken, executorService, pipeline, sparkOptions);}
public CompositeBehavior beam_f13081_0(TransformHierarchy.Node node)
{    CompositeBehavior compositeBehavior = super.enterCompositeTransform(node);    PTransform<?, ?> transform = node.getTransform();    if (transform != null) {        @SuppressWarnings("unchecked")        final Class<PTransform<?, ?>> transformClass = (Class<PTransform<?, ?>>) transform.getClass();        if (compositeBehavior == CompositeBehavior.ENTER_TRANSFORM && !knownComposite(transformClass) && shouldDebug(node)) {            transforms.add(new NativeTransform(node, null, transform, true));        }    }    return compositeBehavior;}
private boolean beam_f13082_0(Class<PTransform<?, ?>> transform)
{    String transformPackage = transform.getPackage().getName();    for (String knownCompositePackage : knownCompositesPackages) {        if (transformPackage.startsWith(knownCompositePackage)) {            return true;        }    }    return false;}
 static void beam_f13091_0(SparkPipelineOptions options)
{    List<String> filesToStage = options.getFilesToStage().stream().map(File::new).filter(File::exists).map(file -> {        return file.getAbsolutePath();    }).collect(Collectors.toList());    options.setFilesToStage(PipelineResources.prepareFilesForStaging(filesToStage, MoreObjects.firstNonNull(options.getTempLocation(), System.getProperty("java.io.tmpdir"))));}
private static RuntimeException beam_f13092_0(final Throwable e)
{    return (e instanceof RuntimeException) ? (RuntimeException) e : new RuntimeException(e);}
public JobApi.MetricResults beam_f13101_1() throws UnsupportedOperationException
{        return JobApi.MetricResults.newBuilder().build();}
protected void beam_f13102_0()
{    javaStreamingContext.stop(false, true);        try {        javaStreamingContext.awaitTerminationOrTimeout(0);    } catch (Exception e) {        throw beamExceptionFrom(e);    } finally {        SparkContextFactory.stopSparkContext(javaSparkContext);        if (Objects.equals(state, State.RUNNING)) {            this.state = State.STOPPED;        }    }}
public static void beam_f13111_0(SparkPipelineOptions opts, JavaSparkContext jsc)
{        MetricsAccumulator.init(opts, jsc);    AggregatorsAccumulator.init(opts, jsc);}
private void beam_f13112_0(Pipeline pipeline)
{    TranslationModeDetector detector = new TranslationModeDetector();    pipeline.traverseTopologically(detector);    if (detector.getTranslationMode().equals(TranslationMode.STREAMING)) {                this.mOptions.setStreaming(true);    }}
protected TransformEvaluator<TransformT> beam_f13121_1(TransformHierarchy.Node node, TransformT transform)
{                Map<TupleTag<?>, PValue> pValues;    if (node.getInputs().isEmpty()) {                pValues = node.getOutputs();    } else {        pValues = node.getInputs();    }    PCollection.IsBounded isNodeBounded = isBoundedCollection(pValues.values());            return isNodeBounded.equals(PCollection.IsBounded.BOUNDED) ? translator.translateBounded(transform) : translator.translateUnbounded(transform);}
protected PCollection.IsBounded beam_f13122_0(Collection<PValue> pValues)
{                    PCollection.IsBounded isBounded = PCollection.IsBounded.BOUNDED;    for (PValue pValue : pValues) {        if (pValue instanceof PCollection) {            isBounded = isBounded.and(((PCollection) pValue).isBounded());        } else {            isBounded = isBounded.and(PCollection.IsBounded.BOUNDED);        }    }    return isBounded;}
 Collection<byte[]> beam_f13132_0()
{    return serTimers;}
public void beam_f13133_0(final KV<K, Iterable<V>> output, final Instant timestamp, final Collection<? extends BoundedWindow> windows, final PaneInfo pane)
{    windowedValues.add(WindowedValue.of(output, timestamp, windows, pane));}
private static void beam_f13142_0(final DStream<Tuple2<ByteArray, Tuple2<StateAndTimers, List<byte[]>>>> firedStream, final SerializablePipelineOptions options)
{    final Long checkpointDurationMillis = getBatchDuration(options);    if (checkpointDurationMillis > 0) {        firedStream.checkpoint(new Duration(checkpointDurationMillis));    }}
private static Long beam_f13143_0(final SerializablePipelineOptions options)
{    return options.get().as(SparkPipelineOptions.class).getCheckpointDurationMillis();}
public T beam_f13152_0(StateNamespace namespace, StateTag<T> address, StateContext<?> c)
{    return address.bind(new SparkStateBinder(namespace, c));}
public ValueState<T> beam_f13153_0(StateTag<ValueState<T>> address, Coder<T> coder)
{    return new SparkValueState<>(namespace, address, coder);}
public void beam_f13162_0()
{    stateTable.remove(namespace.stringKey(), address.getId());}
public boolean beam_f13163_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    @SuppressWarnings("unchecked")    AbstractState<?> that = (AbstractState<?>) o;    return namespace.equals(that.namespace) && address.equals(that.address);}
public ReadableState<Boolean> beam_f13172_0()
{    return this;}
public Boolean beam_f13173_0()
{    return stateTable.get(namespace.stringKey(), address.getId()) == null;}
public void beam_f13182_0(AccumT accum)
{    accum = combineFn.mergeAccumulators(Arrays.asList(getAccum(), accum));    writeValue(accum);}
public AccumT beam_f13183_0(Iterable<AccumT> accumulators)
{    return combineFn.mergeAccumulators(accumulators);}
 Collection<TimerData> beam_f13192_0()
{    return timers;}
 void beam_f13193_0(Iterator<TimerData> timers)
{    while (timers.hasNext()) {        TimerData timer = timers.next();        this.timers.add(timer);    }}
public void beam_f13202_0(StateNamespace namespace, String timerId, Instant target, TimeDomain timeDomain)
{    throw new UnsupportedOperationException("Setting a timer by ID not yet supported.");}
public void beam_f13203_0(StateNamespace namespace, String timerId)
{    throw new UnsupportedOperationException("Deleting a timer by ID is not yet supported.");}
private static void beam_f13212_0(TestSparkPipelineOptions testSparkPipelineOptions, SparkPipelineResult result)
{    Long timeoutMillis = Duration.standardSeconds(checkNotNull(testSparkPipelineOptions.getTestTimeoutSeconds())).getMillis();    Long batchDurationMillis = testSparkPipelineOptions.getBatchIntervalMillis();    Instant stopPipelineWatermark = new Instant(testSparkPipelineOptions.getStopPipelineWatermark());                Instant globalWatermark;    result.waitUntilFinish(Duration.millis(batchDurationMillis));    do {        SparkTimerInternals sparkTimerInternals = SparkTimerInternals.global(GlobalWatermarkHolder.get(batchDurationMillis));        sparkTimerInternals.advanceWatermark();        globalWatermark = sparkTimerInternals.currentInputWatermarkTime();                Uninterruptibles.sleepUninterruptibly(batchDurationMillis, TimeUnit.MILLISECONDS);    } while ((timeoutMillis -= batchDurationMillis) > 0 && globalWatermark.isBefore(stopPipelineWatermark));}
public JavaRDD<WindowedValue<T>> beam_f13213_0()
{    if (rdd == null) {        WindowedValue.ValueOnlyWindowedValueCoder<T> windowCoder = WindowedValue.getValueOnlyCoder(coder);        rdd = jsc.parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder)).map(CoderHelpers.fromByteFunction(windowCoder));    }    return rdd;}
public void beam_f13222_0(final String timerId, final BoundedWindow window, final Instant timestamp, final TimeDomain timeDomain)
{    try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(metricsContainer())) {        delegate.onTimer(timerId, window, timestamp, timeDomain);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void beam_f13223_0()
{    try (Closeable ignored = MetricsEnvironment.scopedMetricsContainer(metricsContainer())) {        delegate.finishBundle();    } catch (IOException e) {        throw new RuntimeException(e);    }}
public T beam_f13232_0(PTransform<T, ?> transform)
{    @SuppressWarnings("unchecked")    T input = (T) Iterables.getOnlyElement(TransformInputs.nonAdditionalInputs(getCurrentTransform()));    return input;}
public Map<TupleTag<?>, PValue> beam_f13233_0(PTransform<?, ?> transform)
{    checkArgument(currentTransform != null, "can only be called with non-null currentTransform");    checkArgument(currentTransform.getTransform() == transform, "can only be called with current transform");    return currentTransform.getInputs();}
public Dataset beam_f13242_0(PValue pvalue)
{    Dataset dataset = datasets.get(pvalue);    leaves.remove(dataset);    return dataset;}
public void beam_f13243_0()
{    for (Dataset dataset : leaves) {                dataset.action();    }}
public static SparkCombineFn.WindowedAccumulator<InputT, InputT, AccumT, ?> beam_f13252_0(JavaRDD<WindowedValue<InputT>> rdd, final SparkCombineFn<InputT, InputT, AccumT, OutputT> sparkCombineFn, final Coder<AccumT> aCoder, final WindowingStrategy<?, ?> windowingStrategy)
{    @SuppressWarnings("unchecked")    final Coder<BoundedWindow> windowCoder = (Coder) windowingStrategy.getWindowFn().windowCoder();    final SparkCombineFn.WindowedAccumulatorCoder<InputT, InputT, AccumT> waCoder = sparkCombineFn.accumulatorCoder(windowCoder, aCoder, windowingStrategy);    ValueAndCoderLazySerializable<SparkCombineFn.WindowedAccumulator<InputT, InputT, AccumT, ?>> accumulatedResult = rdd.aggregate(ValueAndCoderLazySerializable.of(sparkCombineFn.createCombiner(), waCoder), (ab, ib) -> {        SparkCombineFn.WindowedAccumulator<InputT, InputT, AccumT, ?> merged = sparkCombineFn.mergeValue(ab.getOrDecode(waCoder), ib);        return ValueAndCoderLazySerializable.of(merged, waCoder);    }, (a1b, a2b) -> {        SparkCombineFn.WindowedAccumulator<InputT, InputT, AccumT, ?> merged = sparkCombineFn.mergeCombiners(a1b.getOrDecode(waCoder), a2b.getOrDecode(waCoder));        return ValueAndCoderLazySerializable.of(merged, waCoder);    });    return accumulatedResult.getOrDecode(waCoder);}
public static JavaPairRDD<K, SparkCombineFn.WindowedAccumulator<KV<K, V>, V, AccumT, ?>> beam_f13253_0(JavaRDD<WindowedValue<KV<K, V>>> rdd, final SparkCombineFn<KV<K, V>, V, AccumT, ?> sparkCombineFn, final Coder<K> keyCoder, final Coder<V> valueCoder, final Coder<AccumT> aCoder, final WindowingStrategy<?, ?> windowingStrategy)
{    boolean mustBringWindowToKey = sparkCombineFn.mustBringWindowToKey();    @SuppressWarnings("unchecked")    Coder<BoundedWindow> windowCoder = (Coder) windowingStrategy.getWindowFn().windowCoder();    final SparkCombineFn.WindowedAccumulatorCoder<KV<K, V>, V, AccumT> waCoder = sparkCombineFn.accumulatorCoder(windowCoder, aCoder, windowingStrategy);                                final JavaPairRDD<ByteArray, WindowedValue<KV<K, V>>> inRddDuplicatedKeyPair;    if (!mustBringWindowToKey) {        inRddDuplicatedKeyPair = rdd.mapToPair(TranslationUtils.toPairByKeyInWindowedValue(keyCoder));    } else {        inRddDuplicatedKeyPair = GroupNonMergingWindowsFunctions.bringWindowToKey(rdd, keyCoder, windowCoder);    }    JavaPairRDD<ByteArray, ValueAndCoderLazySerializable<SparkCombineFn.WindowedAccumulator<KV<K, V>, V, AccumT, ?>>> accumulatedResult = inRddDuplicatedKeyPair.combineByKey(input -> ValueAndCoderLazySerializable.of(sparkCombineFn.createCombiner(input), waCoder), (acc, input) -> ValueAndCoderLazySerializable.of(sparkCombineFn.mergeValue(acc.getOrDecode(waCoder), input), waCoder), (acc1, acc2) -> ValueAndCoderLazySerializable.of(sparkCombineFn.mergeCombiners(acc1.getOrDecode(waCoder), acc2.getOrDecode(waCoder)), waCoder));    return accumulatedResult.mapToPair(i -> new Tuple2<>(CoderHelpers.fromByteArray(i._1.getValue(), keyCoder), i._2.getOrDecode(waCoder)));}
public WindowedValue<KV<K, Iterable<V>>> beam_f13262_0()
{    while (inner.hasNext()) {        final ByteArray nextKey = inner.peek()._1;        if (nextKey.equals(currentKey)) {                        inner.next();            continue;        }        currentKey = nextKey;        final WindowedValue<KV<K, V>> decodedItem = decodeItem(inner.peek());        return decodedItem.withValue(KV.of(decodedItem.getValue().getKey(), new ValueIterator(inner, currentKey)));    }    hasNext = false;    return null;}
public Iterator<V> beam_f13263_0()
{    if (consumed) {        throw new IllegalStateException("ValueIterator can't be iterated more than once," + "otherwise there could be data lost");    }    consumed = true;    return new AbstractIterator<V>() {        @Override        protected V computeNext() {            if (inner.hasNext() && currentKey.equals(inner.peek()._1)) {                return decodeValue(inner.next()._2);            }            return endOfData();        }    };}
public void beam_f13272_0()
{    throw new RuntimeException("TimerDataIterator not support remove!");}
public void beam_f13273_0()
{    outputs.clear();}
public Set<String> beam_f13282_0()
{    return urnToTransformTranslator.keySet();}
public void beam_f13283_0(final RunnerApi.Pipeline pipeline, SparkTranslationContext context)
{    QueryablePipeline p = QueryablePipeline.forTransforms(pipeline.getRootTransformIdsList(), pipeline.getComponents());    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {                for (String inputId : transformNode.getTransform().getInputsMap().values()) {            context.incrementConsumptionCountBy(inputId, 1);        }                if (transformNode.getTransform().getSpec().getUrn().equals(ExecutableStage.URN)) {            context.incrementConsumptionCountBy(getExecutableStageIntermediateId(transformNode), transformNode.getTransform().getOutputsMap().size());        }        for (String outputId : transformNode.getTransform().getOutputsMap().values()) {            WindowedValueCoder outputCoder = getWindowedValueCoder(outputId, pipeline.getComponents());            context.putCoder(outputId, outputCoder);        }    }    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {        urnToTransformTranslator.getOrDefault(transformNode.getTransform().getSpec().getUrn(), SparkBatchPortablePipelineTranslator::urnNotFound).translate(transformNode, pipeline, context);    }}
private static Tuple2<Broadcast<List<byte[]>>, WindowedValueCoder<T>> beam_f13292_0(String collectionId, RunnerApi.Components components, SparkTranslationContext context)
{    @SuppressWarnings("unchecked")    BoundedDataset<T> dataset = (BoundedDataset<T>) context.popDataset(collectionId);    WindowedValueCoder<T> coder = getWindowedValueCoder(collectionId, components);    List<byte[]> bytes = dataset.getBytes(coder);    Broadcast<List<byte[]>> broadcast = context.getSparkContext().broadcast(bytes);    return new Tuple2<>(broadcast, coder);}
private static void beam_f13293_0(PTransformNode transformNode, RunnerApi.Pipeline pipeline, SparkTranslationContext context)
{    Map<String, String> inputsMap = transformNode.getTransform().getInputsMap();    JavaRDD<WindowedValue<T>> unionRDD;    if (inputsMap.isEmpty()) {        unionRDD = context.getSparkContext().emptyRDD();    } else {        JavaRDD<WindowedValue<T>>[] rdds = new JavaRDD[inputsMap.size()];        int index = 0;        for (String inputId : inputsMap.values()) {            rdds[index] = ((BoundedDataset<T>) context.popDataset(inputId)).getRDD();            index++;        }        unionRDD = context.getSparkContext().union(rdds);    }    context.pushDataset(getOutputId(transformNode), new BoundedDataset<>(unionRDD));}
 static WindowedAccumulator<InputT, ValueT, AccumT, ?> beam_f13302_0(SparkCombineFn<InputT, ValueT, AccumT, ?> context, Function<InputT, ValueT> toValue, WindowingStrategy<?, ?> windowingStrategy, Comparator<BoundedWindow> windowComparator)
{    return create(toValue, context.getType(windowingStrategy), windowComparator);}
 static WindowedAccumulator<InputT, ValueT, AccumT, ?> beam_f13303_0(Function<InputT, ValueT> toValue, Type type, Comparator<BoundedWindow> windowComparator)
{    switch(type) {        case MERGING:            return MergingWindowedAccumulator.create(toValue, windowComparator);        case NON_MERGING:            return NonMergingWindowedAccumulator.create(toValue);        case SINGLE_WINDOW:        case EXPLODE_WINDOWS:            return SingleWindowWindowedAccumulator.create(toValue);        default:            throw new IllegalArgumentException("Unknown type: " + type);    }}
public void beam_f13312_0(WindowedValue<InputT> value, SparkCombineFn<InputT, ValueT, AccumT, ?> context) throws Exception
{    for (WindowedValue<InputT> v : value.explodeWindows()) {        SparkCombineContext ctx = context.ctxtForValue(v);        BoundedWindow window = getWindow(v);        TimestampCombiner combiner = context.windowingStrategy.getTimestampCombiner();        Instant windowTimestamp = combiner.assign(window, context.windowingStrategy.getWindowFn().getOutputTime(v.getTimestamp(), window));        map.compute(window, (w, windowAccumulator) -> {            final AccumT acc;            final Instant timestamp;            if (windowAccumulator == null) {                acc = context.combineFn.createAccumulator(ctx);                timestamp = windowTimestamp;            } else {                acc = windowAccumulator.getValue();                timestamp = windowAccumulator.getTimestamp();            }            AccumT result = context.combineFn.addInput(acc, toValue(v), ctx);            Instant timestampCombined = combiner.combine(windowTimestamp, timestamp);            return WindowedValue.of(result, timestampCombined, window, PaneInfo.NO_FIRING);        });    }    mergeWindows(context);}
public void beam_f13313_0(ImplT other, SparkCombineFn<?, ?, AccumT, ?> context) throws Exception
{    other.map.forEach((window, acc) -> {        WindowedValue<AccumT> thisAcc = this.map.get(window);        if (thisAcc == null) {                        this.map.put(window, acc);        } else {                        this.map.put(window, WindowedValue.of(context.combineFn.mergeAccumulators(Lists.newArrayList(thisAcc.getValue(), acc.getValue()), context.ctxtForValue(acc)), context.windowingStrategy.getTimestampCombiner().combine(acc.getTimestamp(), thisAcc.getTimestamp()), window, PaneInfo.NO_FIRING));        }    });    mergeWindows(context);}
private WindowFn<Object, BoundedWindow>.MergeContext beam_f13323_0(WindowFn<Object, BoundedWindow> windowFn, BiFunction<AccumT, AccumT, AccumT> mergeFn, BiConsumer<Collection<BoundedWindow>, KV<BoundedWindow, AccumT>> afterMerge, Map<BoundedWindow, WindowedValue<AccumT>> map)
{    return windowFn.new MergeContext() {        @Override        public Collection<BoundedWindow> windows() {            return map.keySet();        }        @Override        public void merge(Collection<BoundedWindow> toBeMerged, BoundedWindow mergeResult) {            AccumT accumulator = null;            for (BoundedWindow w : toBeMerged) {                WindowedValue<AccumT> windowAccumulator = Objects.requireNonNull(map.get(w));                if (accumulator == null) {                    accumulator = windowAccumulator.getValue();                } else {                    accumulator = mergeFn.apply(accumulator, windowAccumulator.getValue());                }            }            afterMerge.accept(toBeMerged, KV.of(mergeResult, accumulator));        }    };}
public Collection<BoundedWindow> beam_f13324_0()
{    return map.keySet();}
 static SparkCombineFn<KV<K, V>, V, AccumT, OutputT> beam_f13334_0(CombineWithContext.CombineFnWithContext<V, AccumT, OutputT> combineFn, SerializablePipelineOptions options, Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>> sideInputs, WindowingStrategy<?, ?> windowingStrategy, WindowedAccumulator.Type nonMergingStrategy)
{    return new SparkCombineFn<>(false, KV::getValue, combineFn, options, sideInputs, windowingStrategy, nonMergingStrategy);}
public static SparkCombineFn<KV<K, V>, V, AccumT, OutputT> beam_f13335_0(CombineWithContext.CombineFnWithContext<V, AccumT, OutputT> combineFn, SerializablePipelineOptions options, Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>> sideInputs, WindowingStrategy<?, ?> windowingStrategy)
{    return new SparkCombineFn<>(false, KV::getValue, combineFn, options, sideInputs, windowingStrategy);}
public Stream<WindowedValue<OutputT>> beam_f13344_0(WindowedAccumulator<?, ?, AccumT, ?> accumulator)
{    return accumulator.extractOutput().stream().filter(Objects::nonNull).map(windowAcc -> windowAcc.withValue(combineFn.extractOutput(windowAcc.getValue(), ctxtForValue(windowAcc))));}
 WindowedAccumulatorCoder<InputT, ValueT, AccumT> beam_f13345_0(Coder<BoundedWindow> windowCoder, Coder<AccumT> accumulatorCoder, WindowingStrategy<?, ?> windowingStrategy)
{    return new WindowedAccumulatorCoder<>(toValue, windowCoder, windowComparator, accumulatorCoder, getType(windowingStrategy));}
private static JavaSparkContext beam_f13354_1(SparkContextOptions contextOptions)
{    if (usesProvidedSparkContext) {                JavaSparkContext jsc = contextOptions.getProvidedSparkContext();        if (jsc == null || jsc.sc().isStopped()) {                        throw new RuntimeException("The provided Spark context was not created or was stopped");        }        return jsc;    } else {                SparkConf conf = new SparkConf();        if (!conf.contains("spark.master")) {                        conf.setMaster(contextOptions.getSparkMaster());        }        if (contextOptions.getFilesToStage() != null && !contextOptions.getFilesToStage().isEmpty()) {            conf.setJars(contextOptions.getFilesToStage().toArray(new String[0]));        }        conf.setAppName(contextOptions.getAppName());                conf.set("spark.kryo.registrator", SparkRunnerKryoRegistrator.class.getName());        return new JavaSparkContext(conf);    }}
public static SparkExecutableStageContextFactory beam_f13355_0()
{    return instance;}
private StateRequestHandler beam_f13364_0(ExecutableStage executableStage, ProcessBundleDescriptors.ExecutableProcessBundleDescriptor processBundleDescriptor)
{    EnumMap<TypeCase, StateRequestHandler> handlerMap = new EnumMap<>(StateKey.TypeCase.class);    final StateRequestHandler sideInputHandler;    StateRequestHandlers.SideInputHandlerFactory sideInputHandlerFactory = BatchSideInputHandlerFactory.forStage(executableStage, new BatchSideInputHandlerFactory.SideInputGetter() {        @Override        public <T> List<T> getSideInput(String pCollectionId) {            Tuple2<Broadcast<List<byte[]>>, WindowedValueCoder<SideInputT>> tuple2 = sideInputs.get(pCollectionId);            Broadcast<List<byte[]>> broadcast = tuple2._1;            WindowedValueCoder<SideInputT> coder = tuple2._2;            return (List<T>) broadcast.value().stream().map(bytes -> CoderHelpers.fromByteArray(bytes, coder)).collect(Collectors.toList());        }    });    try {        sideInputHandler = StateRequestHandlers.forSideInputHandlerFactory(ProcessBundleDescriptors.getSideInputs(executableStage), sideInputHandlerFactory);    } catch (IOException e) {        throw new RuntimeException("Failed to setup state handler", e);    }    if (bagUserStateHandlerFactory == null) {        bagUserStateHandlerFactory = new InMemoryBagUserStateFactory();    }    final StateRequestHandler userStateHandler;    if (executableStage.getUserStates().size() > 0) {                bagUserStateHandlerFactory.resetForNewKey();        userStateHandler = StateRequestHandlers.forBagUserStateHandlerFactory(processBundleDescriptor, bagUserStateHandlerFactory);    } else {        userStateHandler = StateRequestHandler.unsupported();    }    handlerMap.put(StateKey.TypeCase.MULTIMAP_SIDE_INPUT, sideInputHandler);    handlerMap.put(StateKey.TypeCase.BAG_USER_STATE, userStateHandler);    return StateRequestHandlers.delegateBasedUponType(handlerMap);}
public List<T> beam_f13365_0(String pCollectionId)
{    Tuple2<Broadcast<List<byte[]>>, WindowedValueCoder<SideInputT>> tuple2 = sideInputs.get(pCollectionId);    Broadcast<List<byte[]>> broadcast = tuple2._1;    WindowedValueCoder<SideInputT> coder = tuple2._2;    return (List<T>) broadcast.value().stream().map(bytes -> CoderHelpers.fromByteArray(bytes, coder)).collect(Collectors.toList());}
private SideInputBroadcast beam_f13374_1(PCollectionView<?> view, JavaSparkContext context)
{    Tuple2<byte[], Coder<Iterable<WindowedValue<?>>>> tuple2 = pviews.get(view);    SideInputBroadcast helper = SideInputBroadcast.create(tuple2._1, tuple2._2);    String pCollectionName = view.getPCollection() != null ? view.getPCollection().getName() : "UNKNOWN";        helper.broadcast(context);    broadcastHelperMap.put(view, helper);    return helper;}
 Iterable<OutputT> beam_f13375_0(Iterator<WindowedValue<FnInputT>> partition) throws Exception
{        if (!partition.hasNext()) {        return new ArrayList<>();    }        return this.getOutputIterable(partition, doFnRunner);}
public void beam_f13384_0(String pCollectionId, Dataset dataset)
{    dataset.setName(pCollectionId);    SparkPipelineOptions sparkOptions = serializablePipelineOptions.get().as(SparkPipelineOptions.class);    if (!sparkOptions.isCacheDisabled() && consumptionCount.getOrDefault(pCollectionId, 0) > 1) {        String storageLevel = sparkOptions.getStorageLevel();        @Nullable        Coder coder = coderMap.get(pCollectionId);        dataset.cache(storageLevel, coder);    }    datasets.put(pCollectionId, dataset);    leaves.add(dataset);}
public Dataset beam_f13385_0(String pCollectionId)
{    Dataset dataset = datasets.get(pCollectionId);    leaves.remove(dataset);    return dataset;}
public Path beam_f13394_0()
{    return rootCheckpointDir;}
public Path beam_f13395_0()
{    return sparkCheckpointDir;}
public String beam_f13404_0()
{    return "streamingContext.<readFrom(<source>)>()";}
private static TransformEvaluator<CreateStream<T>> beam_f13405_0()
{    return new TransformEvaluator<CreateStream<T>>() {        @Override        public void evaluate(CreateStream<T> transform, EvaluationContext context) {            final Queue<JavaRDD<WindowedValue<T>>> rddQueue = buildRdds(transform.getBatches(), context.getStreamingContext(), context.getOutput(transform).getCoder());            final JavaInputDStream<WindowedValue<T>> javaInputDStream = buildInputStream(rddQueue, transform, context);            final UnboundedDataset<T> unboundedDataset = new UnboundedDataset<>(javaInputDStream, Collections.singletonList(javaInputDStream.inputDStream().id()));                        GlobalWatermarkHolder.addAll(ImmutableMap.of(unboundedDataset.getStreamSources().get(0), transform.getTimes()));            context.putDataset(transform, unboundedDataset);        }        private Queue<JavaRDD<WindowedValue<T>>> buildRdds(Queue<Iterable<TimestampedValue<T>>> batches, JavaStreamingContext jssc, Coder<T> coder) {            final WindowedValue.FullWindowedValueCoder<T> windowCoder = WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);            final Queue<JavaRDD<WindowedValue<T>>> rddQueue = new LinkedBlockingQueue<>();            for (final Iterable<TimestampedValue<T>> timestampedValues : batches) {                final Iterable<WindowedValue<T>> windowedValues = StreamSupport.stream(timestampedValues.spliterator(), false).map(timestampedValue -> WindowedValue.of(timestampedValue.getValue(), timestampedValue.getTimestamp(), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING)).collect(Collectors.toList());                final JavaRDD<WindowedValue<T>> rdd = jssc.sparkContext().parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder)).map(CoderHelpers.fromByteFunction(windowCoder));                rddQueue.offer(rdd);            }            return rddQueue;        }        private JavaInputDStream<WindowedValue<T>> buildInputStream(Queue<JavaRDD<WindowedValue<T>>> rddQueue, CreateStream<T> transform, EvaluationContext context) {            return transform.isForceWatermarkSync() ? new JavaInputDStream<>(new WatermarkSyncedDStream<>(rddQueue, transform.getBatchDuration(), context.getStreamingContext().ssc()), JavaSparkContext$.MODULE$.fakeClassTag()) : context.getStreamingContext().queueStream(rddQueue, true);        }        @Override        public String toNativeString() {            return "streamingContext.queueStream(...)";        }    };}
public void beam_f13414_0(final Window.Assign<T> transform, EvaluationContext context)
{    @SuppressWarnings("unchecked")    UnboundedDataset<T> unboundedDataset = (UnboundedDataset<T>) context.borrowDataset(transform);    JavaDStream<WindowedValue<T>> dStream = unboundedDataset.getDStream();    JavaDStream<WindowedValue<T>> outputStream;    if (TranslationUtils.skipAssignWindows(transform, context)) {                outputStream = dStream;    } else {        outputStream = dStream.transform(rdd -> rdd.map(new SparkAssignWindowFn<>(transform.getWindowFn())));    }    context.putDataset(transform, new UnboundedDataset<>(outputStream, unboundedDataset.getStreamSources()));}
public String beam_f13415_0()
{    return "map(new <windowFn>())";}
public String beam_f13424_0()
{    return "mapPartitions(new <fn>())";}
private static TransformEvaluator<Reshuffle<K, V>> beam_f13425_0()
{    return new TransformEvaluator<Reshuffle<K, V>>() {        @Override        public void evaluate(Reshuffle<K, V> transform, EvaluationContext context) {            @SuppressWarnings("unchecked")            UnboundedDataset<KV<K, V>> inputDataset = (UnboundedDataset<KV<K, V>>) context.borrowDataset(transform);            List<Integer> streamSources = inputDataset.getStreamSources();            JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();            final KvCoder<K, V> coder = (KvCoder<K, V>) context.getInput(transform).getCoder();            @SuppressWarnings("unchecked")            final WindowingStrategy<?, W> windowingStrategy = (WindowingStrategy<?, W>) context.getInput(transform).getWindowingStrategy();            @SuppressWarnings("unchecked")            final WindowFn<Object, W> windowFn = (WindowFn<Object, W>) windowingStrategy.getWindowFn();            final WindowedValue.WindowedValueCoder<KV<K, V>> wvCoder = WindowedValue.FullWindowedValueCoder.of(coder, windowFn.windowCoder());            JavaDStream<WindowedValue<KV<K, V>>> reshuffledStream = dStream.transform(rdd -> GroupCombineFunctions.reshuffle(rdd, wvCoder));            context.putDataset(transform, new UnboundedDataset<>(reshuffledStream, streamSources));        }        @Override        public String toNativeString() {            return "repartition(...)";        }    };}
public String beam_f13434_0(CreateStream<?> transform)
{    return CreateStream.TRANSFORM_URN;}
 JavaDStream<WindowedValue<T>> beam_f13435_0()
{    return dStream;}
public scala.Option<RDD<WindowedValue<T>>> beam_f13445_1(final Time validTime)
{    final long batchTime = validTime.milliseconds();    LOG.trace("BEFORE waiting for watermark sync, " + "LastWatermarkedBatchTime: {}, current batch time: {}", GlobalWatermarkHolder.getLastWatermarkedBatchTime(), batchTime);    final Stopwatch stopwatch = Stopwatch.createStarted();    awaitWatermarkSyncWith(batchTime);    stopwatch.stop();            LOG.trace("AFTER waiting for watermark sync, " + "LastWatermarkedBatchTime: {}, current batch time: {}", GlobalWatermarkHolder.getLastWatermarkedBatchTime(), batchTime);    final RDD<WindowedValue<T>> rdd = generateRdd();    isFirst = false;    return scala.Option.apply(rdd);}
private static TransformEvaluator<Flatten.PCollections<T>> beam_f13448_0()
{    return new TransformEvaluator<Flatten.PCollections<T>>() {        @SuppressWarnings("unchecked")        @Override        public void evaluate(Flatten.PCollections<T> transform, EvaluationContext context) {            Collection<PValue> pcs = context.getInputs(transform).values();            JavaRDD<WindowedValue<T>> unionRDD;            if (pcs.isEmpty()) {                unionRDD = context.getSparkContext().emptyRDD();            } else {                JavaRDD<WindowedValue<T>>[] rdds = new JavaRDD[pcs.size()];                int index = 0;                for (PValue pc : pcs) {                    checkArgument(pc instanceof PCollection, "Flatten had non-PCollection value in input: %s of type %s", pc, pc.getClass().getSimpleName());                    rdds[index] = ((BoundedDataset<T>) context.borrowDataset(pc)).getRDD();                    index++;                }                unionRDD = context.getSparkContext().union(rdds);            }            context.putDataset(transform, new BoundedDataset<>(unionRDD));        }        @Override        public String toNativeString() {            return "sparkContext.union(...)";        }    };}
private static TransformEvaluator<Combine.Globally<InputT, OutputT>> beam_f13457_0()
{    return new TransformEvaluator<Combine.Globally<InputT, OutputT>>() {        @Override        public void evaluate(Combine.Globally<InputT, OutputT> transform, EvaluationContext context) {            final PCollection<InputT> input = context.getInput(transform);            final Coder<InputT> iCoder = context.getInput(transform).getCoder();            final Coder<OutputT> oCoder = context.getOutput(transform).getCoder();            final WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();            @SuppressWarnings("unchecked")            final CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn = (CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT>) CombineFnUtil.toFnWithContext(transform.getFn());            final WindowedValue.FullWindowedValueCoder<OutputT> wvoCoder = WindowedValue.FullWindowedValueCoder.of(oCoder, windowingStrategy.getWindowFn().windowCoder());            final boolean hasDefault = transform.isInsertDefault();            final SparkCombineFn<InputT, InputT, AccumT, OutputT> sparkCombineFn = SparkCombineFn.globally(combineFn, context.getSerializableOptions(), TranslationUtils.getSideInputs(transform.getSideInputs(), context), windowingStrategy);            final Coder<AccumT> aCoder;            try {                aCoder = combineFn.getAccumulatorCoder(context.getPipeline().getCoderRegistry(), iCoder);            } catch (CannotProvideCoderException e) {                throw new IllegalStateException("Could not determine coder for accumulator", e);            }            @SuppressWarnings("unchecked")            JavaRDD<WindowedValue<InputT>> inRdd = ((BoundedDataset<InputT>) context.borrowDataset(transform)).getRDD();            JavaRDD<WindowedValue<OutputT>> outRdd;            SparkCombineFn.WindowedAccumulator<InputT, InputT, AccumT, ?> accumulated = GroupCombineFunctions.combineGlobally(inRdd, sparkCombineFn, aCoder, windowingStrategy);            if (!accumulated.isEmpty()) {                Iterable<WindowedValue<OutputT>> output = sparkCombineFn.extractOutput(accumulated);                outRdd = context.getSparkContext().parallelize(CoderHelpers.toByteArrays(output, wvoCoder)).map(CoderHelpers.fromByteFunction(wvoCoder));            } else {                                                JavaSparkContext jsc = new JavaSparkContext(inRdd.context());                if (hasDefault) {                    OutputT defaultValue = combineFn.defaultValue();                    outRdd = jsc.parallelize(Lists.newArrayList(CoderHelpers.toByteArray(defaultValue, oCoder))).map(CoderHelpers.fromByteFunction(oCoder)).map(WindowedValue::valueInGlobalWindow);                } else {                    outRdd = jsc.emptyRDD();                }            }            context.putDataset(transform, new BoundedDataset<>(outRdd));        }        @Override        public String toNativeString() {            return "aggregate(..., new <fn>(), ...)";        }    };}
public void beam_f13458_0(Combine.Globally<InputT, OutputT> transform, EvaluationContext context)
{    final PCollection<InputT> input = context.getInput(transform);    final Coder<InputT> iCoder = context.getInput(transform).getCoder();    final Coder<OutputT> oCoder = context.getOutput(transform).getCoder();    final WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();    @SuppressWarnings("unchecked")    final CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT> combineFn = (CombineWithContext.CombineFnWithContext<InputT, AccumT, OutputT>) CombineFnUtil.toFnWithContext(transform.getFn());    final WindowedValue.FullWindowedValueCoder<OutputT> wvoCoder = WindowedValue.FullWindowedValueCoder.of(oCoder, windowingStrategy.getWindowFn().windowCoder());    final boolean hasDefault = transform.isInsertDefault();    final SparkCombineFn<InputT, InputT, AccumT, OutputT> sparkCombineFn = SparkCombineFn.globally(combineFn, context.getSerializableOptions(), TranslationUtils.getSideInputs(transform.getSideInputs(), context), windowingStrategy);    final Coder<AccumT> aCoder;    try {        aCoder = combineFn.getAccumulatorCoder(context.getPipeline().getCoderRegistry(), iCoder);    } catch (CannotProvideCoderException e) {        throw new IllegalStateException("Could not determine coder for accumulator", e);    }    @SuppressWarnings("unchecked")    JavaRDD<WindowedValue<InputT>> inRdd = ((BoundedDataset<InputT>) context.borrowDataset(transform)).getRDD();    JavaRDD<WindowedValue<OutputT>> outRdd;    SparkCombineFn.WindowedAccumulator<InputT, InputT, AccumT, ?> accumulated = GroupCombineFunctions.combineGlobally(inRdd, sparkCombineFn, aCoder, windowingStrategy);    if (!accumulated.isEmpty()) {        Iterable<WindowedValue<OutputT>> output = sparkCombineFn.extractOutput(accumulated);        outRdd = context.getSparkContext().parallelize(CoderHelpers.toByteArrays(output, wvoCoder)).map(CoderHelpers.fromByteFunction(wvoCoder));    } else {                        JavaSparkContext jsc = new JavaSparkContext(inRdd.context());        if (hasDefault) {            OutputT defaultValue = combineFn.defaultValue();            outRdd = jsc.parallelize(Lists.newArrayList(CoderHelpers.toByteArray(defaultValue, oCoder))).map(CoderHelpers.fromByteFunction(oCoder)).map(WindowedValue::valueInGlobalWindow);        } else {            outRdd = jsc.emptyRDD();        }    }    context.putDataset(transform, new BoundedDataset<>(outRdd));}
private static TransformEvaluator<Read.Bounded<T>> beam_f13467_0()
{    return new TransformEvaluator<Read.Bounded<T>>() {        @Override        public void evaluate(Read.Bounded<T> transform, EvaluationContext context) {            String stepName = context.getCurrentTransform().getFullName();            final JavaSparkContext jsc = context.getSparkContext();                        JavaRDD<WindowedValue<T>> input = new SourceRDD.Bounded<>(jsc.sc(), transform.getSource(), context.getSerializableOptions(), stepName).toJavaRDD();            context.putDataset(transform, new BoundedDataset<>(input));        }        @Override        public String toNativeString() {            return "sparkContext.<readFrom(<source>)>()";        }    };}
public void beam_f13468_0(Read.Bounded<T> transform, EvaluationContext context)
{    String stepName = context.getCurrentTransform().getFullName();    final JavaSparkContext jsc = context.getSparkContext();        JavaRDD<WindowedValue<T>> input = new SourceRDD.Bounded<>(jsc.sc(), transform.getSource(), context.getSerializableOptions(), stepName).toJavaRDD();    context.putDataset(transform, new BoundedDataset<>(input));}
public void beam_f13477_0(Reshuffle<K, V> transform, EvaluationContext context)
{    @SuppressWarnings("unchecked")    JavaRDD<WindowedValue<KV<K, V>>> inRDD = ((BoundedDataset<KV<K, V>>) context.borrowDataset(transform)).getRDD();    @SuppressWarnings("unchecked")    final WindowingStrategy<?, W> windowingStrategy = (WindowingStrategy<?, W>) context.getInput(transform).getWindowingStrategy();    final KvCoder<K, V> coder = (KvCoder<K, V>) context.getInput(transform).getCoder();    @SuppressWarnings("unchecked")    final WindowFn<Object, W> windowFn = (WindowFn<Object, W>) windowingStrategy.getWindowFn();    final WindowedValue.WindowedValueCoder<KV<K, V>> wvCoder = WindowedValue.FullWindowedValueCoder.of(coder, windowFn.windowCoder());    JavaRDD<WindowedValue<KV<K, V>>> reshuffled = GroupCombineFunctions.reshuffle(inRDD, wvCoder);    context.putDataset(transform, new BoundedDataset<>(reshuffled));}
public String beam_f13478_0()
{    return "repartition(...)";}
public static JavaDStream<T2> beam_f13487_0(JavaPairDStream<T1, T2> pairDStream)
{    return pairDStream.map(Tuple2::_2);}
public static PairFunction<KV<K, V>, K, V> beam_f13488_0()
{    return kv -> new Tuple2<>(kv.getKey(), kv.getValue());}
 static Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>> beam_f13497_0(Iterable<PCollectionView<?>> views, EvaluationContext context)
{    return getSideInputs(views, context.getSparkContext(), context.getPViews());}
public static Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>> beam_f13498_0(Iterable<PCollectionView<?>> views, JavaSparkContext context, SparkPCollectionView pviews)
{    if (views == null) {        return ImmutableMap.of();    } else {        Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>> sideInputs = Maps.newHashMap();        for (PCollectionView<?> view : views) {            SideInputBroadcast helper = pviews.getPCollectionView(view, context);            WindowingStrategy<?, ?> windowingStrategy = view.getWindowingStrategyInternal();            sideInputs.put(view.getTagInternal(), KV.of(windowingStrategy, helper));        }        return sideInputs;    }}
public void beam_f13507_0(Kryo kryo, Output output, ValueAndCoderLazySerializable<T> item)
{    try {        item.writeCommon(output);    } catch (IOException e) {        throw new KryoException(e);    }}
public ValueAndCoderLazySerializable<T> beam_f13508_0(Kryo kryo, Input input, Class<ValueAndCoderLazySerializable<T>> type)
{    ValueAndCoderLazySerializable<T> value = new ValueAndCoderLazySerializable<>();    try {        value.readCommon(input);    } catch (IOException e) {        throw new KryoException(e);    }    return value;}
public boolean beam_f13517_0(Object o)
{    if (o == null || getClass() != o.getClass()) {        return false;    }    ByteArray byteArray = (ByteArray) o;    return Arrays.equals(value, byteArray.value);}
public int beam_f13518_0()
{    return value != null ? Arrays.hashCode(value) : 0;}
public static Map<Integer, SparkWatermarks> beam_f13527_0(Long cacheInterval)
{    if (canBypassRemoteWatermarkFetching()) {        /*      driverNodeWatermarks != null =>      => advance() was called      => WatermarkAdvancingStreamingListener#onBatchCompleted() was called      => we are currently running on the driver node      => we can get the watermarks from the driver local copy instead of fetching their block      remotely using block manger      /------------------------------------------------------------------------------------------/      In test mode, the system is running inside a single JVM, and thus both driver and executors      "canBypassWatermarkBlockFetching" by using the static driverNodeWatermarks copy.      This allows tests to avoid the asynchronous nature of using the BlockManager directly.      */        return getLocalWatermarkCopy();    } else {        if (watermarkCache == null) {            watermarkCache = createWatermarkCache(cacheInterval);        }        try {            return watermarkCache.get("SINGLETON");        } catch (ExecutionException e) {            throw new RuntimeException(e);        }    }}
private static boolean beam_f13528_0()
{    return driverNodeWatermarks != null;}
private static Map<Integer, SparkWatermarks> beam_f13537_0(BlockManager blockManager)
{    final Option<BlockResult> blockResultOption = blockManager.get(WATERMARKS_BLOCK_ID, WATERMARKS_TAG);    if (blockResultOption.isDefined()) {        Iterator<Object> data = blockResultOption.get().data();        Map<Integer, SparkWatermarks> next = (Map<Integer, SparkWatermarks>) data.next();                while (data.hasNext()) {                }        return next;    } else {        return null;    }}
public Map<Integer, SparkWatermarks> beam_f13538_0(@Nonnull String key) throws Exception
{    final BlockManager blockManager = SparkEnv.get().blockManager();    final Map<Integer, SparkWatermarks> watermarks = fetchSparkWatermarks(blockManager);    return watermarks != null ? watermarks : Maps.newHashMap();}
public static SideInputBroadcast<T> beam_f13547_0(byte[] bytes, Coder<T> coder)
{    return new SideInputBroadcast<>(bytes, coder);}
public synchronized T beam_f13548_0()
{    if (value == null) {        value = deserialize();    }    return value;}
public T beam_f13557_0()
{    return value;}
public static JavaDStream<WindowedValue<T>> beam_f13558_0(JavaStreamingContext streamingContext, List<JavaDStream<WindowedValue<T>>> dStreams)
{    try {        if (streamingContext.sparkContext().version().startsWith("3")) {                                                Method method = streamingContext.getClass().getDeclaredMethod("union", JavaDStream[].class);            Object result = method.invoke(streamingContext, new Object[] { dStreams.toArray(new JavaDStream[0]) });            return (JavaDStream<WindowedValue<T>>) result;        }                        Method method = streamingContext.getClass().getDeclaredMethod("union", JavaDStream.class, List.class);        Object result = method.invoke(streamingContext, dStreams.remove(0), dStreams);        return (JavaDStream<WindowedValue<T>>) result;    } catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {        throw new RuntimeException("Error invoking Spark union", e);    }}
public void beam_f13570_0() throws Exception
{    assertThat(InMemoryMetrics.valueOf("emptyLines"), is(nullValue()));    Instant instant = new Instant(0);    CreateStream<String> source = CreateStream.of(StringUtf8Coder.of(), Duration.millis((pipeline.getOptions().as(SparkPipelineOptions.class)).getBatchIntervalMillis())).emptyBatch().advanceWatermarkForNextBatch(instant).nextBatch(TimestampedValue.of(WORDS.get(0), instant), TimestampedValue.of(WORDS.get(1), instant), TimestampedValue.of(WORDS.get(2), instant)).advanceWatermarkForNextBatch(instant.plus(Duration.standardSeconds(2L))).nextBatch(TimestampedValue.of(WORDS.get(3), instant.plus(Duration.standardSeconds(1L))), TimestampedValue.of(WORDS.get(4), instant.plus(Duration.standardSeconds(1L))), TimestampedValue.of(WORDS.get(5), instant.plus(Duration.standardSeconds(1L)))).advanceNextBatchWatermarkToInfinity();    PCollection<String> output = pipeline.apply(source).apply(Window.<String>into(FixedWindows.of(Duration.standardSeconds(3L))).withAllowedLateness(Duration.ZERO)).apply(new WordCount.CountWords()).apply(MapElements.via(new WordCount.FormatAsTextFn()));    PAssert.that(output).containsInAnyOrder(EXPECTED_COUNTS);    pipeline.run();    assertThat(InMemoryMetrics.<Double>valueOf("emptyLines"), is(1d));}
public void beam_f13571_0()
{    SparkPipelineOptions options = createOptions();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> pCollection = pipeline.apply(Create.of("foo", "bar"));        pCollection.apply(Count.globally());        PCollectionView<List<String>> view = pCollection.apply(View.asList());                pipeline.apply(Create.of("foo", "baz")).apply(ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(ProcessContext processContext) {            if (processContext.sideInput(view).contains(processContext.element())) {                processContext.output(processContext.element());            }        }    }).withSideInputs(view));    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);    EvaluationContext ctxt = new EvaluationContext(jsc, pipeline, options);    SparkRunner.CacheVisitor cacheVisitor = new SparkRunner.CacheVisitor(new TransformTranslator.Translator(), ctxt);    pipeline.traverseTopologically(cacheVisitor);    assertEquals(2L, (long) ctxt.getCacheCandidates().get(pCollection));    assertEquals(1L, ctxt.getCacheCandidates().values().stream().filter(l -> l > 1).count());}
public void beam_f13580_0(Kryo kryo)
{    fail("Default spark.serializer is JavaSerializer" + " so spark.kryo.registrator shouldn't be called");}
public void beam_f13581_0(Kryo kryo)
{    super.registerClasses(kryo);    Registration registration = kryo.getRegistration(MicrobatchSource.class);    com.esotericsoftware.kryo.Serializer kryoSerializer = registration.getSerializer();    assertTrue(kryoSerializer instanceof StatelessJavaSerializer);}
public void beam_f13590_0() throws Exception
{    PCollection<String> inputWords = p.apply(Create.of(WORDS).withCoder(StringUtf8Coder.of()));    PCollection<String> output = inputWords.apply(new WordCount.CountWords()).apply(MapElements.via(new WordCount.FormatAsTextFn()));    output.apply(TextIO.write().to(outputDir.getAbsolutePath()).withNumShards(3).withSuffix(".txt"));    p.run().waitUntilFinish();    int count = 0;    Set<String> expected = Sets.newHashSet("hi: 5", "there: 1", "sue: 2", "bob: 2");    for (File f : tmpDir.getRoot().listFiles(pathname -> pathname.getName().matches("out-.*\\.txt"))) {        count++;        for (String line : Files.readLines(f, StandardCharsets.UTF_8)) {            assertTrue(line + " not found", expected.remove(line));        }    }    assertEquals(3, count);    assertTrue(expected.toString(), expected.isEmpty());}
 boolean beam_f13591_0()
{    return closed;}
public void beam_f13600_0() throws Exception
{    assertReaderRange(TestReader.START, TestReader.LIMIT);    exception.expect(NoSuchElementException.class);    readerIterator.next();}
public void beam_f13601_0() throws Exception
{    assertThat(readerIterator.hasNext(), is(true));    assertThat(readerIterator.hasNext(), is(true));    assertThat(readerIterator.next().getValue(), is(1));    assertThat(readerIterator.hasNext(), is(true));    assertThat(readerIterator.hasNext(), is(true));    assertThat(readerIterator.hasNext(), is(true));    assertThat(readerIterator.next().getValue(), is(2));    assertThat(readerIterator.next().getValue(), is(3));        assertThat(readerIterator.hasNext(), is(false));    assertThat(readerIterator.hasNext(), is(false));        exception.expect(NoSuchElementException.class);    readerIterator.next();}
public void beam_f13610_0() throws Exception
{    JavaSparkContext jsc = new JavaSparkContext("local[*]", "Existing_Context");        jsc.stop();    testWithInvalidContext(jsc);}
private void beam_f13611_0(JavaSparkContext jsc) throws Exception
{    SparkContextOptions options = getSparkContextOptions(jsc);    Pipeline p = Pipeline.create(options);    PCollection<String> inputWords = p.apply(Create.of(WORDS).withCoder(StringUtf8Coder.of()));    PCollection<String> output = inputWords.apply(new WordCount.CountWords()).apply(MapElements.via(new WordCount.FormatAsTextFn()));    PAssert.that(output).containsInAnyOrder(EXPECTED_COUNT_SET);        PipelineResult result = p.run();    TestPipeline.verifyPAssertsSucceeded(p, result);}
private SparkPipelineOptions beam_f13620_0()
{    options.setRunner(SparkRunner.class);    options.setStreaming(true);    return options;}
private SparkPipelineOptions beam_f13621_0()
{    options.setRunner(SparkRunner.class);        options.setStreaming(false);    return options;}
public void beam_f13630_0() throws Exception
{    testCanceledPipeline(getStreamingOptions());}
public void beam_f13631_0() throws Exception
{    testCanceledPipeline(getBatchOptions());}
public void beam_f13640_0(ProcessContext context)
{    context.output("zero");    context.output("one");    context.output("two");}
public void beam_f13641_0(ProcessContext context)
{    context.output((long) context.element().length());}
public KV<String, Long> beam_f13650_0(KV<String, Long> input)
{    return KV.of(input.getKey(), input.getValue() + 1);}
public void beam_f13651_0()
{    assertEquals(ImmutableList.of(SparkPipelineOptions.class), new SparkRunnerRegistrar.Options().getPipelineOptions());}
private GroupByKeyIterator<String, Integer, GlobalWindow> beam_f13667_0() throws Coder.NonDeterministicException
{    return createGbkIterator(GlobalWindow.INSTANCE, GlobalWindow.Coder.INSTANCE, WindowingStrategy.globalDefault());}
private GroupByKeyIterator<String, Integer, W> beam_f13668_0(W window, Coder<W> winCoder, WindowingStrategy<Object, W> winStrategy) throws Coder.NonDeterministicException
{    StringUtf8Coder keyCoder = StringUtf8Coder.of();    final WindowedValue.FullWindowedValueCoder<KV<String, Integer>> winValCoder = WindowedValue.getFullCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()), winStrategy.getWindowFn().windowCoder());    ItemFactory<String, Integer, W> factory = ItemFactory.forWindow(keyCoder, winValCoder, winCoder, window);    List<Tuple2<ByteArray, byte[]>> items = Arrays.asList(factory.create("k1", 1), factory.create("k1", 2), factory.create("k2", 3), factory.create("k2", 4), factory.create("k2", 5));    return new GroupByKeyIterator<>(items.iterator(), keyCoder, winStrategy, winValCoder);}
public void beam_f13677_0() throws Exception
{    WindowingStrategy<Object, IntervalWindow> strategy = WindowingStrategy.of(SlidingWindows.of(Duration.millis(3000)).every(Duration.millis(1000)));    SparkCombineFn<KV<String, Integer>, Integer, Long, Long> sparkCombineFn = SparkCombineFn.keyed(combineFn, opts, Collections.emptyMap(), strategy, SparkCombineFn.WindowedAccumulator.Type.EXPLODE_WINDOWS);    Instant now = Instant.ofEpochMilli(0);    WindowedValue<KV<String, Integer>> first = input("key", 1, now.plus(5000), strategy.getWindowFn());    WindowedValue<KV<String, Integer>> second = input("key", 2, now.plus(1500), strategy.getWindowFn());    WindowedValue<KV<String, Integer>> third = input("key", 3, now.plus(500), strategy.getWindowFn());    Map<KV<String, BoundedWindow>, List<WindowedValue<KV<String, Integer>>>> groupByKeyAndWindow;    groupByKeyAndWindow = Stream.of(first, second, third).flatMap(e -> StreamSupport.stream(e.explodeWindows().spliterator(), false)).collect(Collectors.groupingBy(e -> KV.of(e.getValue().getKey(), Iterables.getOnlyElement(e.getWindows()))));    List<String> result = new ArrayList<>();    for (Map.Entry<KV<String, BoundedWindow>, List<WindowedValue<KV<String, Integer>>>> e : groupByKeyAndWindow.entrySet()) {        SparkCombineFn.WindowedAccumulator<KV<String, Integer>, Integer, Long, ?> combiner = null;        for (WindowedValue<KV<String, Integer>> v : e.getValue()) {            if (combiner == null) {                combiner = sparkCombineFn.createCombiner(v);            } else {                combiner.add(v, sparkCombineFn);            }        }        WindowedValue<Long> combined = Iterables.getOnlyElement(combiner.extractOutput());        result.add(combined.getValue() + ":" + combined.getTimestamp().getMillis());    }    assertUnorderedEquals(Lists.newArrayList("3:999", "5:1999", "5:2999", "2:3999", "1:5999", "1:6999", "1:7999"), result);}
private static Combine.CombineFn<Integer, Long, Long> beam_f13678_0()
{    return new Combine.CombineFn<Integer, Long, Long>() {        @Override        public Long createAccumulator() {            return 0L;        }        @Override        public Long addInput(Long mutableAccumulator, Integer input) {            return mutableAccumulator + input;        }        @Override        public Long mergeAccumulators(Iterable<Long> accumulators) {            return StreamSupport.stream(accumulators.spliterator(), false).mapToLong(e -> e).sum();        }        @Override        public Long extractOutput(Long accumulator) {            return accumulator;        }    };}
 WindowFn<V, BoundedWindow>.AssignContext beam_f13687_0(WindowFn<V, BoundedWindow> windowFn, V value, Instant timestamp)
{    return windowFn.new AssignContext() {        @Override        public V element() {            return value;        }        @Override        public Instant timestamp() {            return timestamp;        }        @Override        public BoundedWindow window() {            return GlobalWindow.INSTANCE;        }    };}
public V beam_f13688_0()
{    return value;}
public String beam_f13697_0()
{    return "bundle-id";}
public Map<String, FnDataReceiver> beam_f13698_0()
{    return ImmutableMap.of("input", input -> {    /* Ignore input*/    });}
public void beam_f13708_0() throws IOException
{    CreateStream<String> source = CreateStream.of(StringUtf8Coder.of(), batchDuration()).nextBatch("foo", "bar").advanceNextBatchWatermarkToInfinity();    CreateStream<Integer> other = CreateStream.of(VarIntCoder.of(), batchDuration()).nextBatch(1, 2, 3, 4).advanceNextBatchWatermarkToInfinity();    PCollection<String> createStrings = p.apply("CreateStrings", source).apply("WindowStrings", Window.<String>configure().triggering(AfterPane.elementCountAtLeast(2)).withAllowedLateness(Duration.ZERO).accumulatingFiredPanes());    PAssert.that(createStrings).containsInAnyOrder("foo", "bar");    PCollection<Integer> createInts = p.apply("CreateInts", other).apply("WindowInts", Window.<Integer>configure().triggering(AfterPane.elementCountAtLeast(4)).withAllowedLateness(Duration.ZERO).accumulatingFiredPanes());    PAssert.that(createInts).containsInAnyOrder(1, 2, 3, 4);    p.run();}
public void beam_f13709_0() throws IOException
{    Instant instant = new Instant(0);    CreateStream<Integer> source1 = CreateStream.of(VarIntCoder.of(), batchDuration()).emptyBatch().advanceWatermarkForNextBatch(instant.plus(Duration.standardMinutes(5))).nextBatch(TimestampedValue.of(1, instant), TimestampedValue.of(2, instant), TimestampedValue.of(3, instant)).advanceWatermarkForNextBatch(instant.plus(Duration.standardMinutes(10)));    CreateStream<Integer> source2 = CreateStream.of(VarIntCoder.of(), batchDuration()).emptyBatch().advanceWatermarkForNextBatch(instant.plus(Duration.standardMinutes(1))).nextBatch(TimestampedValue.of(4, instant)).advanceWatermarkForNextBatch(instant.plus(Duration.standardMinutes(2))).nextBatch(TimestampedValue.of(5, instant)).advanceWatermarkForNextBatch(instant.plus(Duration.standardMinutes(5))).emptyBatch().advanceNextBatchWatermarkToInfinity();    PCollection<Integer> windowed1 = p.apply("CreateStream1", source1).apply("Window1", Window.<Integer>into(FixedWindows.of(Duration.standardMinutes(5))).triggering(AfterWatermark.pastEndOfWindow()).accumulatingFiredPanes().withAllowedLateness(Duration.ZERO));    PCollection<Integer> windowed2 = p.apply("CreateStream2", source2).apply("Window2", Window.<Integer>into(FixedWindows.of(Duration.standardMinutes(5))).triggering(AfterWatermark.pastEndOfWindow()).accumulatingFiredPanes().withAllowedLateness(Duration.ZERO));    PCollectionList<Integer> pCollectionList = PCollectionList.of(windowed1).and(windowed2);    PCollection<Integer> flattened = pCollectionList.apply(Flatten.pCollections());    PCollection<Integer> triggered = flattened.apply(WithKeys.of(1)).apply(GroupByKey.create()).apply(Values.create()).apply(Flatten.iterables());    IntervalWindow window = new IntervalWindow(instant, instant.plus(Duration.standardMinutes(5L)));    PAssert.that(triggered).inOnTimePane(window).containsInAnyOrder(1, 2, 3, 4, 5);    p.run();}
public void beam_f13718_0()
{    teardownCalls.incrementAndGet();}
public void beam_f13719_0(ProcessContext context)
{    Integer element = context.element();    context.output(element);}
public void beam_f13728_0(ProcessContext c)
{    String element = c.element();        assertThat(c.sideInput(view), containsInAnyOrder("side1", "side2"));    counter.inc();    if (!"EOF".equals(element)) {        aggregator.inc();        c.output(c.element());    }}
public PDone beam_f13729_0(PCollection<Iterable<T>> input)
{    input.apply(ParDo.of(new AssertDoFn<>(expected)));    return PDone.in(input.getPipeline());}
private void beam_f13738_0(List<Integer> streamingSources)
{    numAssertions++;    assertThat(streamingSources, containsInAnyOrder(expected));}
public void beam_f13739_0(Pipeline p)
{    super.enterPipeline(p);    evaluator.enterPipeline(p);}
public Properties beam_f13748_0()
{    Properties props = new Properties();    props.putAll(baseProperties);    props.put("bootstrap.servers", brokerList);    return props;}
public String beam_f13749_0()
{    return brokerList;}
public void beam_f13758_0(int port)
{    this.port = port;}
public void beam_f13759_0(int tickTime)
{    this.tickTime = tickTime;}
private WindowedValue<T> beam_f13768_0(T val)
{    return WindowedValue.of(val, Instant.now(), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING);}
public static MyClass beam_f13769_0(List<?>... sideInputs)
{    return new MyClass().myMethod(sideInputs);}
public static AvroCoder<T> beam_f13779_0(Class<T> type, Schema schema)
{    return new AvroCoder<>(type, schema);}
public static CoderProvider beam_f13780_0()
{    return new AvroCoderProvider();}
public void beam_f13789_0(T value, OutputStream outStream) throws IOException
{        BinaryEncoder encoderInstance = ENCODER_FACTORY.directBinaryEncoder(outStream, encoder.get());        encoder.set(encoderInstance);    writer.get().write(value, encoderInstance);}
public T beam_f13790_0(InputStream inStream) throws IOException
{        BinaryDecoder decoderInstance = DECODER_FACTORY.directBinaryDecoder(inStream, decoder.get());        decoder.set(decoderInstance);    return reader.get().read(null, decoderInstance);}
private void beam_f13799_0(String context, TypeDescriptor<?> type)
{        if (!DETERMINISTIC_STRINGABLE_CLASSES.contains(type.getRawType())) {        reportError(context, "%s may not have deterministic #toString()", type);    }}
private void beam_f13800_0(String context, TypeDescriptor<?> type, Schema schema)
{    final List<Schema> unionTypes = schema.getTypes();    if (!type.getRawType().isAnnotationPresent(Union.class)) {                if (unionTypes.size() == 2 && unionTypes.contains(AVRO_NULL_SCHEMA)) {                        Schema nullableFieldSchema = unionTypes.get(0).equals(AVRO_NULL_SCHEMA) ? unionTypes.get(1) : unionTypes.get(0);            doCheck(context, type, nullableFieldSchema);            return;        }                reportError(context, "Expected type %s to have @Union annotation", type);        return;    }        String baseClassContext = type.getRawType().getName();        for (Schema concrete : unionTypes) {        @SuppressWarnings("unchecked")        TypeDescriptor<?> unionType = TypeDescriptor.of(ReflectData.get().getClass(concrete));        recurse(baseClassContext, unionType, concrete);    }}
public void beam_f13809_0(BigDecimal value, OutputStream outStream) throws IOException, CoderException
{    encode(value, outStream, Context.NESTED);}
public void beam_f13810_0(BigDecimal value, OutputStream outStream, Context context) throws IOException, CoderException
{    checkNotNull(value, String.format("cannot encode a null %s", BigDecimal.class.getSimpleName()));    VAR_INT_CODER.encode(value.scale(), outStream);    BIG_INT_CODER.encode(value.unscaledValue(), outStream, context);}
public Integer beam_f13819_0(InputStream inStream) throws IOException, CoderException
{    try {        return new DataInputStream(inStream).readInt();    } catch (EOFException | UTFDataFormatException exn) {                throw new CoderException(exn);    }}
public boolean beam_f13821_0()
{    return true;}
public TypeDescriptor<Long> beam_f13831_0()
{    return TYPE_DESCRIPTOR;}
protected long beam_f13832_0(Long value) throws Exception
{    if (value == null) {        throw new CoderException("cannot encode a null Long");    }    return 8;}
public void beam_f13842_0(BigInteger value, OutputStream outStream) throws IOException, CoderException
{    encode(value, outStream, Context.NESTED);}
public void beam_f13843_0(BigInteger value, OutputStream outStream, Context context) throws IOException, CoderException
{    checkNotNull(value, String.format("cannot encode a null %s", BigInteger.class.getSimpleName()));    BYTE_ARRAY_CODER.encode(value.toByteArray(), outStream, context);}
public void beam_f13852_0(BitSet value, OutputStream outStream, Context context) throws CoderException, IOException
{    if (value == null) {        throw new CoderException("cannot encode a null BitSet");    }    BYTE_ARRAY_CODER.encodeAndOwn(value.toByteArray(), outStream, context);}
public BitSet beam_f13853_0(InputStream inStream) throws CoderException, IOException
{    return decode(inStream, Context.NESTED);}
protected long beam_f13862_0(Boolean value) throws Exception
{    return 1;}
public static ByteArrayCoder beam_f13863_0()
{    return INSTANCE;}
protected long beam_f13873_0(byte[] value) throws Exception
{    if (value == null) {        throw new CoderException("cannot encode a null byte[]");    }    return (long) VarInt.getLength(value.length) + value.length;}
public static ByteCoder beam_f13874_0()
{    return INSTANCE;}
public Context beam_f13884_0()
{    return NESTED;}
public boolean beam_f13885_0(Object obj)
{    if (!(obj instanceof Context)) {        return false;    }    return Objects.equal(isWholeStream, ((Context) obj).isWholeStream);}
public boolean beam_f13894_0(T value)
{    return false;}
public void beam_f13895_0(T value, ElementByteSizeObserver observer) throws Exception
{    observer.update(getEncodedElementByteSize(value));}
public String beam_f13904_0()
{    return MoreObjects.toStringHelper(getClass()).add("rawType", rawType).add("factoryMethod", factoryMethod).toString();}
public Coder<T> beam_f13905_0(TypeDescriptor<T> type, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    if (!this.type.equals(type)) {        throw new CannotProvideCoderException(String.format("Unable to provide coder for %s, this factory can only provide coders for %s", type, this.type));    }    return (Coder) coder;}
public Coder<OutputT> beam_f13914_0(TypeDescriptor<OutputT> typeDescriptor, TypeDescriptor<InputT> inputTypeDescriptor, Coder<InputT> inputCoder) throws CannotProvideCoderException
{    checkArgument(typeDescriptor != null);    checkArgument(inputTypeDescriptor != null);    checkArgument(inputCoder != null);    return getCoderFromTypeDescriptor(typeDescriptor, getTypeToCoderBindings(inputTypeDescriptor.getType(), inputCoder));}
public Coder<OutputT> beam_f13915_0(SerializableFunction<InputT, OutputT> fn, Coder<InputT> inputCoder) throws CannotProvideCoderException
{    ParameterizedType fnType = (ParameterizedType) TypeDescriptor.of(fn.getClass()).getSupertype(SerializableFunction.class).getType();    return getCoder(fn.getClass(), SerializableFunction.class, ImmutableMap.of(fnType.getActualTypeArguments()[0], inputCoder), SerializableFunction.class.getTypeParameters()[1]);}
private Coder<?> beam_f13924_0(ParameterizedType type, SetMultimap<Type, Coder<?>> typeCoderBindings) throws CannotProvideCoderException
{    List<Coder<?>> typeArgumentCoders = new ArrayList<>();    for (Type typeArgument : type.getActualTypeArguments()) {        try {            Coder<?> typeArgumentCoder = getCoderFromTypeDescriptor(TypeDescriptor.of(typeArgument), typeCoderBindings);            typeArgumentCoders.add(typeArgumentCoder);        } catch (CannotProvideCoderException exc) {            throw new CannotProvideCoderException(String.format("Cannot provide coder for parameterized type %s: %s", type, exc.getMessage()), exc);        }    }    return getCoderFromFactories(TypeDescriptor.of(type), typeArgumentCoders);}
private Coder<?> beam_f13925_0(TypeDescriptor<?> typeDescriptor, List<Coder<?>> typeArgumentCoders) throws CannotProvideCoderException
{    List<CannotProvideCoderException> suppressedExceptions = new ArrayList<>();    for (CoderProvider coderProvider : coderProviders) {        try {            return coderProvider.coderFor(typeDescriptor, typeArgumentCoders);        } catch (CannotProvideCoderException e) {                        suppressedExceptions.add(e);        }    }        StringBuilder messageBuilder = new StringBuilder().append("Unable to provide a Coder for ").append(typeDescriptor).append(".\n").append("  Building a Coder using a registered CoderProvider failed.\n").append("  See suppressed exceptions for detailed failures.");    CannotProvideCoderException exceptionOnFailure = new CannotProvideCoderException(messageBuilder.toString());    for (CannotProvideCoderException suppressedException : suppressedExceptions) {        exceptionOnFailure.addSuppressed(suppressedException);    }    throw exceptionOnFailure;}
public Coder<T> beam_f13934_1(TypeDescriptor<T> typeDescriptor, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    Class<?> clazz = typeDescriptor.getRawType();    DefaultCoder defaultAnnotation = clazz.getAnnotation(DefaultCoder.class);    if (defaultAnnotation == null) {        throw new CannotProvideCoderException(String.format("Class %s does not have a @DefaultCoder annotation.", clazz.getName()));    }    Class<? extends Coder> defaultAnnotationValue = defaultAnnotation.value();    if (defaultAnnotationValue == null) {        throw new CannotProvideCoderException(String.format("Class %s has a @DefaultCoder annotation with a null value.", clazz.getName()));    }        Method coderProviderMethod;    try {        coderProviderMethod = defaultAnnotationValue.getMethod("getCoderProvider");    } catch (NoSuchMethodException e) {        throw new CannotProvideCoderException(String.format("Unable to find 'public static CoderProvider getCoderProvider()' on %s", defaultAnnotationValue), e);    }    CoderProvider coderProvider;    try {        coderProvider = (CoderProvider) coderProviderMethod.invoke(null);    } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException | NullPointerException | ExceptionInInitializerError e) {        throw new CannotProvideCoderException(String.format("Unable to invoke 'public static CoderProvider getCoderProvider()' on %s", defaultAnnotationValue), e);    }    return coderProvider.coderFor(typeDescriptor, componentCoders);}
public static DelegateCoder<T, IntermediateT> beam_f13935_0(Coder<IntermediateT> coder, CodingFunction<T, IntermediateT> toFn, CodingFunction<IntermediateT, T> fromFn)
{    return of(coder, toFn, fromFn, null);}
public boolean beam_f13944_0(Object o)
{    if (o == null || this.getClass() != o.getClass()) {        return false;    }    DelegateCoder<?, ?> that = (DelegateCoder<?, ?>) o;    return Objects.equal(this.coder, that.coder) && Objects.equal(this.toFn, that.toFn) && Objects.equal(this.fromFn, that.fromFn);}
public int beam_f13945_0()
{    return Objects.hashCode(this.coder, this.toFn, this.fromFn);}
public boolean beam_f13954_0()
{    return true;}
public boolean beam_f13955_0(Double value)
{    return true;}
public boolean beam_f13964_0()
{    return true;}
public boolean beam_f13965_0(ReadableDuration value)
{    return LONG_CODER.isRegisterByteSizeObserverCheap(toLong(value));}
public TypeDescriptor<Float> beam_f13974_0()
{    return TYPE_DESCRIPTOR;}
protected long beam_f13975_0(Float value) throws Exception
{    if (value == null) {        throw new CoderException("cannot encode a null Float");    }    return 4;}
public Object beam_f13985_0(Iterable<T> value)
{    ArrayList<Object> result = new ArrayList<>();    for (T elem : value) {        result.add(getElemCoder().structuralValue(elem));    }    return result;}
protected final Iterable<T> beam_f13986_0(List<T> decodedElements)
{    return decodedElements;}
public void beam_f13995_0(Observable obs, Object obj)
{    if (!(obj instanceof Long)) {        throw new AssertionError("unexpected parameter object");    }    if (countable) {        outerObserver.update(obs, obj);    } else {                outerObserver.update(obs, 1 + (long) obj);    }}
public static KvCoder<K, V> beam_f13996_0(Coder<K> keyCoder, Coder<V> valueCoder)
{    return new KvCoder<>(keyCoder, valueCoder);}
public boolean beam_f14005_0()
{    return keyCoder.consistentWithEquals() && valueCoder.consistentWithEquals();}
public Object beam_f14006_0(KV<K, V> kv)
{    if (consistentWithEquals()) {        return kv;    } else {        return KV.of(getKeyCoder().structuralValue(kv.getKey()), getValueCoder().structuralValue(kv.getValue()));    }}
public void beam_f14015_0() throws NonDeterministicException
{    valueCoder.verifyDeterministic();}
public boolean beam_f14016_0()
{    return valueCoder.consistentWithEquals();}
public static MapCoder<K, V> beam_f14025_0(Coder<K> keyCoder, Coder<V> valueCoder)
{    return new MapCoder<>(keyCoder, valueCoder);}
public Coder<K> beam_f14026_0()
{    return keyCoder;}
public Object beam_f14035_0(Map<K, V> value)
{    if (consistentWithEquals()) {        return value;    } else {        Map<Object, Object> ret = Maps.newHashMapWithExpectedSize(value.size());        for (Map.Entry<K, V> entry : value.entrySet()) {            ret.put(keyCoder.structuralValue(entry.getKey()), valueCoder.structuralValue(entry.getValue()));        }        return ret;    }}
public void beam_f14036_0(Map<K, V> map, ElementByteSizeObserver observer) throws Exception
{    observer.update(4L);    if (map.isEmpty()) {        return;    }    Iterator<Entry<K, V>> entries = map.entrySet().iterator();    Entry<K, V> entry = entries.next();    while (entries.hasNext()) {        keyCoder.registerByteSizeObserver(entry.getKey(), observer);        valueCoder.registerByteSizeObserver(entry.getValue(), observer);        entry = entries.next();    }    keyCoder.registerByteSizeObserver(entry.getKey(), observer);    valueCoder.registerByteSizeObserver(entry.getValue(), observer);}
public void beam_f14045_0() throws NonDeterministicException
{    verifyDeterministic(this, "Value coder must be deterministic", valueCoder);}
public boolean beam_f14046_0()
{    return valueCoder.consistentWithEquals();}
private void beam_f14055_0(FieldType fieldType)
{    switch(fieldType.getTypeName()) {        case ROW:            setSchemaIds(fieldType.getRowSchema(), UUID.randomUUID());            return;        case MAP:            setSchemaIds(fieldType.getMapKeyType());            setSchemaIds(fieldType.getMapValueType());            return;        case LOGICAL_TYPE:            setSchemaIds(fieldType.getLogicalType().getBaseType());            return;        case ARRAY:            setSchemaIds(fieldType.getCollectionElementType());            return;        default:            return;    }}
private Coder<Row> beam_f14056_0()
{    if (delegateCoder == null) {                        delegateCoder = RowCoderGenerator.generate(schema);    }    return delegateCoder;}
private static long beam_f14065_0(FieldType typeDescriptor, Object value)
{    switch(typeDescriptor.getTypeName()) {        case LOGICAL_TYPE:            return estimatedSizeBytes(typeDescriptor.getLogicalType().getBaseType(), value);        case ROW:            return estimatedSizeBytes((Row) value);        case ARRAY:            List list = (List) value;            long listSizeBytes = 0;            for (Object elem : list) {                listSizeBytes += estimatedSizeBytes(typeDescriptor.getCollectionElementType(), elem);            }            return 4 + listSizeBytes;        case BYTES:            byte[] bytes = (byte[]) value;            return 4L + bytes.length;        case MAP:            Map<Object, Object> map = (Map<Object, Object>) value;            long mapSizeBytes = 0;            for (Map.Entry<Object, Object> elem : map.entrySet()) {                mapSizeBytes += typeDescriptor.getMapKeyType().getTypeName().equals(TypeName.STRING) ? ((String) elem.getKey()).length() : ESTIMATED_FIELD_SIZES.get(typeDescriptor.getMapKeyType().getTypeName());                mapSizeBytes += estimatedSizeBytes(typeDescriptor.getMapValueType(), elem.getValue());            }            return 4 + mapSizeBytes;        case STRING:                        return ((String) value).length();        default:            return ESTIMATED_FIELD_SIZES.get(typeDescriptor.getTypeName());    }}
public String beam_f14066_0()
{    String string = "Schema: " + schema + "  UUID: " + id + " delegateCoder: " + getDelegateCoder();    return string;}
 static Row beam_f14075_0(Schema schema, Coder[] coders, InputStream inputStream) throws IOException
{    int fieldCount = VAR_INT_CODER.decode(inputStream);    BitSet nullFields = NULL_LIST_CODER.decode(inputStream);    List<Object> fieldValues = Lists.newArrayListWithCapacity(coders.length);    for (int i = 0; i < fieldCount; ++i) {                if (i < coders.length) {            if (nullFields.get(i)) {                fieldValues.add(null);            } else {                fieldValues.add(coders[i].decode(inputStream));            }        }    }        for (int i = fieldCount; i < coders.length; i++) {        fieldValues.add(null);    }        return Row.withSchema(schema).attachValues(fieldValues).build();}
private static DynamicType.Builder<Coder> beam_f14076_0(Schema schema, DynamicType.Builder<Coder> builder)
{    List<StackManipulation> componentCoders = Lists.newArrayListWithCapacity(schema.getFieldCount());    for (int i = 0; i < schema.getFieldCount(); i++) {                        componentCoders.add(getCoder(schema.getField(i).getType().withNullable(false)));    }    return builder.defineField(CODERS_FIELD_NAME, Coder[].class, Visibility.PRIVATE, Ownership.STATIC, FieldManifestation.FINAL).initializer((methodVisitor, implementationContext, instrumentedMethod) -> {        StackManipulation manipulation = new StackManipulation.Compound(        ArrayFactory.forType(CODER_TYPE.asGenericType()).withValues(componentCoders), FieldAccess.forField(implementationContext.getInstrumentedType().getDeclaredFields().filter(ElementMatchers.named(CODERS_FIELD_NAME)).getOnly()).write());        StackManipulation.Size size = manipulation.apply(methodVisitor, implementationContext);        return new ByteCodeAppender.Size(size.getMaximalSize(), instrumentedMethod.getStackSize());    });}
public static SerializableCoder<T> beam_f14085_0(Class<T> clazz)
{    checkEqualsMethodDefined(clazz);    return new SerializableCoder<>(clazz, TypeDescriptor.of(clazz));}
private static void beam_f14086_1(Class<T> clazz)
{    boolean warn = true;    if (!clazz.isInterface()) {        Method method;        try {            method = clazz.getMethod("equals", Object.class);        } catch (NoSuchMethodException e) {                        throw new AssertionError(String.format("Concrete class %s has no equals method", clazz));        }                warn = Object.class.equals(method.getDeclaringClass());    }        if (warn && MISSING_EQUALS_METHOD.add(clazz)) {            }}
public int beam_f14095_0()
{    return type.hashCode();}
public TypeDescriptor<T> beam_f14096_0()
{    if (typeDescriptor == null) {        typeDescriptor = TypeDescriptor.of(type);    }    return typeDescriptor;}
public void beam_f14105_0() throws NonDeterministicException
{    keyCoder.verifyDeterministic();}
public static SnappyCoder<T> beam_f14106_0(Coder<T> innerCoder)
{    return new SnappyCoder<>(innerCoder);}
public int beam_f14115_0()
{    return this.clazz.hashCode();}
public void beam_f14116_0(T value, OutputStream outStream) throws CoderException, IOException
{    encode(value, outStream, Context.NESTED);}
private static String beam_f14125_0(InputStream dis) throws IOException
{    int len = VarInt.decodeInt(dis);    if (len < 0) {        throw new CoderException("Invalid encoded string length: " + len);    }    byte[] bytes = new byte[len];    ByteStreams.readFully(dis, bytes);    return new String(bytes, StandardCharsets.UTF_8);}
public void beam_f14126_0(String value, OutputStream outStream) throws IOException
{    encode(value, outStream, Context.NESTED);}
public int beam_f14136_0()
{    return Arrays.hashCode(value);}
public String beam_f14137_0()
{    return "base64:" + BaseEncoding.base64().encode(value);}
public Integer beam_f14146_0(InputStream inStream, Context context) throws IOException, CoderException
{    String textualValue = StringUtf8Coder.of().decode(inStream, context);    try {        return Integer.valueOf(textualValue);    } catch (NumberFormatException exn) {        throw new CoderException("error when decoding a textual integer", exn);    }}
public void beam_f14147_0()
{    StringUtf8Coder.of().verifyDeterministic();}
protected long beam_f14157_0(Integer value) throws Exception
{    if (value == null) {        throw new CoderException("cannot encode a null Integer");    }    return VarInt.getLength(value.longValue());}
public static VarLongCoder beam_f14158_0()
{    return INSTANCE;}
public Void beam_f14169_0(InputStream inStream)
{        return null;}
public Object beam_f14171_0(Void value)
{    return STRUCTURAL_VOID_VALUE;}
public static ReadAll<GenericRecord> beam_f14182_0(Schema schema)
{    return new AutoValue_AvroIO_ReadAll.Builder<GenericRecord>().setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.ALLOW_IF_WILDCARD)).setRecordClass(GenericRecord.class).setSchema(schema).setInferBeamSchema(false).setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES).build();}
public static Read<GenericRecord> beam_f14183_0(String schema)
{    return readGenericRecords(new Schema.Parser().parse(schema));}
public static TypedWrite<UserT, Void, GenericRecord> beam_f14192_0()
{    return AvroIO.<UserT, GenericRecord>defaultWriteBuilder().setGenericRecords(true).build();}
public static Write<GenericRecord> beam_f14193_0(String schema)
{    return writeGenericRecords(new Schema.Parser().parse(schema));}
public Read<T> beam_f14202_0(boolean withBeamSchemas)
{    return toBuilder().setInferBeamSchema(withBeamSchemas).build();}
public PCollection<T> beam_f14203_0(PBegin input)
{    checkNotNull(getFilepattern(), "filepattern");    checkNotNull(getSchema(), "schema");    if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {        PCollection<T> read = input.apply("Read", org.apache.beam.sdk.io.Read.from(createSource(getFilepattern(), getMatchConfiguration().getEmptyMatchTreatment(), getRecordClass(), getSchema())));        return getInferBeamSchema() ? setBeamSchema(read, getRecordClass(), getSchema()) : read;    }        ReadFiles<T> readFiles = (getRecordClass() == GenericRecord.class) ? (ReadFiles<T>) readFilesGenericRecords(getSchema()) : readFiles(getRecordClass());    return input.apply("Create filepattern", Create.ofProvider(getFilepattern(), StringUtf8Coder.of())).apply("Match All", FileIO.matchAll().withConfiguration(getMatchConfiguration())).apply("Read Matches", FileIO.readMatches().withDirectoryTreatment(DirectoryTreatment.PROHIBIT)).apply("Via ReadFiles", readFiles);}
public ReadAll<T> beam_f14212_0(Duration pollInterval, TerminationCondition<String, ?> terminationCondition)
{    return withMatchConfiguration(getMatchConfiguration().continuously(pollInterval, terminationCondition));}
 ReadAll<T> beam_f14213_0(long desiredBundleSizeBytes)
{    return toBuilder().setDesiredBundleSizeBytes(desiredBundleSizeBytes).build();}
public Parse<T> beam_f14222_0(EmptyMatchTreatment treatment)
{    return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));}
public Parse<T> beam_f14223_0(Duration pollInterval, TerminationCondition<String, ?> terminationCondition)
{    return withMatchConfiguration(getMatchConfiguration().continuously(pollInterval, terminationCondition));}
public void beam_f14232_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("parseFn", getParseFn().getClass()).withLabel("Parse function"));}
public FileBasedSource<T> beam_f14233_0(String input)
{    return AvroSource.from(input).withParseFn(parseFn, coder);}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14242_0(ResourceId outputPrefix)
{    return toResource(StaticValueProvider.of(outputPrefix));}
public ResourceId beam_f14243_0(String input)
{    return FileBasedSink.convertToFileResourceIfPossible(input);}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14252_0(String shardTemplate)
{    return toBuilder().setShardTemplate(shardTemplate).build();}
public TypedWrite<UserT, DestinationT, OutputT> beam_f14253_0(String filenameSuffix)
{    return toBuilder().setFilenameSuffix(filenameSuffix).build();}
public void beam_f14262_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    resolveDynamicDestinations().populateDisplayData(builder);    builder.addIfNotDefault(DisplayData.item("numShards", getNumShards()).withLabel("Maximum Output Shards"), 0).addIfNotNull(DisplayData.item("tempDirectory", getTempDirectory()).withLabel("Directory for temporary files"));}
public Write<T> beam_f14263_0(String outputPrefix)
{    return new Write<>(inner.to(FileBasedSink.convertToFileResourceIfPossible(outputPrefix)).withFormatFunction(SerializableFunctions.identity()));}
public Write<T> beam_f14272_0(String shardTemplate)
{    return new Write<>(inner.withShardNameTemplate(shardTemplate));}
public Write<T> beam_f14273_0(String filenameSuffix)
{    return new Write<>(inner.withSuffix(filenameSuffix));}
public static DynamicAvroDestinations<UserT, Void, OutputT> beam_f14282_0(FilenamePolicy filenamePolicy, Schema schema, Map<String, Object> metadata, CodecFactory codec, SerializableFunction<UserT, OutputT> formatFunction)
{    return new ConstantAvroDestination<>(filenamePolicy, schema, metadata, codec, formatFunction);}
public static Sink<ElementT> beam_f14283_0(final Class<ElementT> clazz)
{    return new AutoValue_AvroIO_Sink.Builder<ElementT>().setJsonSchema(ReflectData.get().getSchema(clazz).toString()).setMetadata(ImmutableMap.of()).setCodec(TypedWrite.DEFAULT_SERIALIZABLE_CODEC).build();}
public DynamicAvroDestinations<UserT, DestinationT, OutputT> beam_f14292_0()
{    return (DynamicAvroDestinations<UserT, DestinationT, OutputT>) super.getDynamicDestinations();}
public WriteOperation<DestinationT, OutputT> beam_f14293_0()
{    return new AvroWriteOperation<>(this, genericRecords);}
private static Mode<T> beam_f14302_0(Class<T> clazz)
{    return new Mode<>(clazz, ReflectData.get().getSchema(clazz).toString(), null, null);}
private static Mode<T> beam_f14303_0(SerializableFunction<GenericRecord, T> parseFn, Coder<T> outputCoder)
{    return new Mode<>(GenericRecord.class, null, parseFn, outputCoder);}
public AvroSource<T> beam_f14312_0(long minBundleSize)
{    if (getMode() == SINGLE_FILE_OR_SUBRANGE) {        return new AvroSource<>(getSingleFileMetadata(), minBundleSize, getStartOffset(), getEndOffset(), mode);    }    return new AvroSource<>(getFileOrPatternSpecProvider(), getEmptyMatchTreatment(), minBundleSize, mode);}
public void beam_f14313_0()
{    super.validate();    mode.validate();}
 static AvroMetadata beam_f14322_0(ResourceId fileResource) throws IOException
{    String codec = null;    String schemaString = null;    byte[] syncMarker;    try (InputStream stream = Channels.newInputStream(FileSystems.open(fileResource))) {        BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(stream, null);                                                        byte[] magic = new byte[DataFileConstants.MAGIC.length];        decoder.readFixed(magic);        if (!Arrays.equals(magic, DataFileConstants.MAGIC)) {            throw new IOException("Missing Avro file signature: " + fileResource);        }                ByteBuffer valueBuffer = ByteBuffer.allocate(512);        long numRecords = decoder.readMapStart();        while (numRecords > 0) {            for (long recordIndex = 0; recordIndex < numRecords; recordIndex++) {                String key = decoder.readString();                                                                valueBuffer = decoder.readBytes(valueBuffer);                byte[] bytes = new byte[valueBuffer.remaining()];                valueBuffer.get(bytes);                if (key.equals(DataFileConstants.CODEC)) {                    codec = new String(bytes, StandardCharsets.UTF_8);                } else if (key.equals(DataFileConstants.SCHEMA)) {                    schemaString = new String(bytes, StandardCharsets.UTF_8);                }            }            numRecords = decoder.mapNext();        }        if (codec == null) {            codec = DataFileConstants.NULL_CODEC;        }                syncMarker = new byte[DataFileConstants.SYNC_SIZE];        decoder.readFixed(syncMarker);    }    checkState(schemaString != null, "No schema present in Avro file metadata %s", fileResource);    return new AvroMetadata(syncMarker, codec, schemaString);}
private static synchronized String beam_f14323_0(String schema)
{    String internSchema = schemaStringLogicalReferenceCache.get(schema);    if (internSchema != null) {        return internSchema;    }    schemaStringLogicalReferenceCache.put(schema, schema);    return schema;}
public AvroBlock<T> beam_f14332_0()
{    return currentBlock;}
public long beam_f14333_0()
{    synchronized (progressLock) {        return currentBlockOffset;    }}
protected final boolean beam_f14342_0() throws IOException
{    atSplitPoint = false;    while (getCurrentBlock() == null || !getCurrentBlock().readNextRecord()) {        if (!readNextBlock()) {            return false;        }                atSplitPoint = true;    }    return true;}
public Double beam_f14343_0()
{    if (!isStarted()) {        return 0.0;    }    if (isDone()) {        return 1.0;    }    FileBasedSource<T> source = getCurrentSource();    if (source.getEndOffset() == Long.MAX_VALUE) {                return null;    }    long currentBlockOffset = getCurrentBlockOffset();    long startOffset = source.getStartOffset();    long endOffset = source.getEndOffset();    double fractionAtBlockStart = ((double) (currentBlockOffset - startOffset)) / (endOffset - startOffset);    double fractionAtBlockEnd = (double) (currentBlockOffset + getCurrentBlockSize() - startOffset) / (endOffset - startOffset);    double blockFraction = getCurrentBlock().getFractionOfBlockConsumed();    return Math.min(1.0, fractionAtBlockStart + blockFraction * (fractionAtBlockEnd - fractionAtBlockStart));}
public void beam_f14352_0(@Element Shard<T> shard, OutputReceiver<Shard<T>> out, PipelineOptions options) throws Exception
{    int numInitialSplits = numInitialSplits(shard.getMaxNumRecords());    List<? extends UnboundedSource<T, ?>> splits = shard.getSource().split(numInitialSplits, options);    int numSplits = splits.size();    long[] numRecords = splitNumRecords(shard.getMaxNumRecords(), numSplits);    for (int i = 0; i < numSplits; i++) {        out.output(shard.toBuilder().setSource(splits.get(i)).setMaxNumRecords(numRecords[i]).setMaxReadTime(shard.getMaxReadTime()).build());    }}
public void beam_f14353_0(@Element Shard<T> shard, OutputReceiver<ValueWithRecordId<T>> out, PipelineOptions options) throws Exception
{    Instant endTime = shard.getMaxReadTime() == null ? null : Instant.now().plus(shard.getMaxReadTime());    if (shard.getMaxNumRecords() <= 0 || (shard.getMaxReadTime() != null && shard.getMaxReadTime().getMillis() == 0)) {        return;    }    try (UnboundedSource.UnboundedReader<T> reader = SerializableUtils.clone(shard.getSource()).createReader(options, null)) {        for (long i = 0L; i < shard.getMaxNumRecords(); ++i) {            boolean available = (i == 0) ? reader.start() : reader.advance();            if (!available && !advanceWithBackoff(reader, endTime)) {                break;            }            out.outputWithTimestamp(new ValueWithRecordId<T>(reader.getCurrent(), reader.getCurrentRecordId()), reader.getCurrentTimestamp());        }        reader.getCheckpointMark().finalizeCheckpoint();    }}
public static boolean beam_f14362_0(String filename)
{    return Compression.AUTO.isCompressed(filename);}
 static DecompressingChannelFactory beam_f14363_0(Compression compression)
{    switch(compression) {        case AUTO:            return AUTO;        case UNCOMPRESSED:            return UNCOMPRESSED;        case GZIP:            return GZIP;        case BZIP2:            return BZIP2;        case ZIP:            return ZIP;        case ZSTD:            return ZSTD;        case DEFLATE:            return DEFLATE;        default:            throw new IllegalArgumentException("Unsupported compression type: " + compression);    }}
public final Coder<T> beam_f14372_0()
{    return sourceDelegate.getOutputCoder();}
public final DecompressingChannelFactory beam_f14373_0()
{    return channelFactory;}
public void beam_f14382_0() throws IOException
{    inner.close();}
protected final void beam_f14383_0(ReadableByteChannel channel) throws IOException
{    synchronized (progressLock) {        this.channel = new CountingChannel(channel, getCurrentSource().getStartOffset());        channel = this.channel;    }    if (channelFactory == CompressionMode.AUTO) {        readerDelegate.startReading(Compression.detect(getCurrentSource().getFileOrPatternSpec()).readDecompressed(channel));    } else {        readerDelegate.startReading(channelFactory.createDecompressingChannel(channel));    }}
public int beam_f14392_0(byte[] b, int off, int len) throws IOException
{    int result = zipInputStream.read(b, off, len);    while (result == -1) {        currentEntry = zipInputStream.getNextEntry();        if (currentEntry == null) {            return -1;        } else {            result = zipInputStream.read(b, off, len);        }    }    return result;}
public ReadableByteChannel beam_f14393_0(ReadableByteChannel channel)
{    throw new UnsupportedOperationException("Must resolve compression into a concrete value before calling readDecompressed()");}
public WritableByteChannel beam_f14402_0(WritableByteChannel channel) throws IOException
{    throw new UnsupportedOperationException("Writing ZIP files is currently unsupported");}
public ReadableByteChannel beam_f14403_0(ReadableByteChannel channel) throws IOException
{    return Channels.newChannel(new ZstdCompressorInputStream(Channels.newInputStream(channel)));}
public FilenamePolicy beam_f14412_0(Void destination)
{    return filenamePolicy;}
public Schema beam_f14413_0(Void destination)
{    return schema.get();}
public Instant beam_f14422_0(Long input)
{    return Instant.now();}
public boolean beam_f14423_0(Object other)
{    return other instanceof NowTimestampFn;}
protected long beam_f14432_0() throws NoSuchElementException
{    return current;}
public synchronized long beam_f14433_0()
{    return Math.max(0, getCurrentSource().getEndOffset() - current);}
public Coder<CountingSource.CounterMark> beam_f14443_0()
{    return AvroCoder.of(CountingSource.CounterMark.class);}
public Coder<Long> beam_f14444_0()
{    return VarLongCoder.of();}
public Long beam_f14453_0() throws NoSuchElementException
{    return current;}
public Instant beam_f14454_0() throws NoSuchElementException
{    return currentTimestamp;}
public int beam_f14465_0()
{    return Objects.hashCode(baseFilename.get(), shardTemplate, suffix);}
public boolean beam_f14466_0(Object o)
{    if (!(o instanceof Params)) {        return false;    }    Params other = (Params) o;    return baseFilename.get().equals(other.baseFilename.get()) && shardTemplate.equals(other.shardTemplate) && suffix.equals(other.suffix);}
public ResourceId beam_f14475_0(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, OutputFileHints outputFileHints)
{    String paneStr = paneInfoToString(paneInfo);    String windowStr = windowToString(window);    return constructName(params.baseFilename.get(), params.shardTemplate, params.suffix + outputFileHints.getSuggestedFilenameSuffix(), shardNumber, numShards, paneStr, windowStr);}
private String beam_f14476_0(BoundedWindow window)
{    if (window instanceof GlobalWindow) {        return "GlobalWindow";    }    if (window instanceof IntervalWindow) {        IntervalWindow iw = (IntervalWindow) window;        return String.format("%s-%s", iw.start().toString(), iw.end().toString());    }    return window.toString();}
public Void beam_f14485_0()
{    return (Void) null;}
public FilenamePolicy beam_f14486_0(Void destination)
{    return filenamePolicy;}
public static DynamicDestinations<UserT, Params, OutputT> beam_f14495_0(SerializableFunction<UserT, Params> destinationFunction, Params emptyDestination, SerializableFunction<UserT, OutputT> formatFunction)
{    return new DefaultPolicyDestinations<>(destinationFunction, emptyDestination, formatFunction);}
public String beam_f14496_0()
{    return canonical.getSuggestedSuffix();}
 final void beam_f14505_0(DoFn<?, ?>.ProcessContext context)
{    this.sideInputAccessor = new SideInputAccessorViaProcessContext(context);}
public Coder<DestinationT> beam_f14506_0()
{    return null;}
public void beam_f14518_0(Collection<ResourceId> filenames) throws IOException
{    removeTemporaryFiles(filenames, !windowedWrites);}
protected final List<KV<FileResult<DestinationT>, ResourceId>> beam_f14519_0(@Nullable DestinationT dest, @Nullable BoundedWindow window, @Nullable Integer numShards, Collection<FileResult<DestinationT>> existingResults) throws Exception
{    Collection<FileResult<DestinationT>> completeResults = windowedWrites ? existingResults : createMissingEmptyShards(dest, numShards, existingResults);    for (FileResult<DestinationT> res : completeResults) {        checkArgument(Objects.equals(dest, res.getDestination()), "File result has wrong destination: expected %s, got %s", dest, res.getDestination());        checkArgument(Objects.equals(window, res.getWindow()), "File result has wrong window: expected %s, got %s", window, res.getWindow());    }    List<KV<FileResult<DestinationT>, ResourceId>> outputFilenames = Lists.newArrayList();    final int effectiveNumShards;    if (numShards != null) {        effectiveNumShards = numShards;        for (FileResult<DestinationT> res : completeResults) {            checkArgument(res.getShard() != UNKNOWN_SHARDNUM, "Fixed sharding into %s shards was specified, " + "but file result %s does not specify a shard", numShards, res);        }    } else {        effectiveNumShards = Iterables.size(completeResults);        for (FileResult<DestinationT> res : completeResults) {            checkArgument(res.getShard() == UNKNOWN_SHARDNUM, "Runner-chosen sharding was specified, " + "but file result %s explicitly specifies a shard", res);        }    }    List<FileResult<DestinationT>> resultsWithShardNumbers = Lists.newArrayList();    if (numShards != null) {        resultsWithShardNumbers = Lists.newArrayList(completeResults);    } else {        int i = 0;        for (FileResult<DestinationT> res : completeResults) {            resultsWithShardNumbers.add(res.withShard(i++));        }    }    Map<ResourceId, FileResult<DestinationT>> distinctFilenames = Maps.newHashMap();    for (FileResult<DestinationT> result : resultsWithShardNumbers) {        checkArgument(result.getShard() != UNKNOWN_SHARDNUM, "Should have set shard number on %s", result);        ResourceId finalFilename = result.getDestinationFile(windowedWrites, getSink().getDynamicDestinations(), effectiveNumShards, getSink().getWritableByteChannelFactory());        checkArgument(!distinctFilenames.containsKey(finalFilename), "Filename policy must generate unique filenames, but generated the same name %s " + "for file results %s and %s", finalFilename, result, distinctFilenames.get(finalFilename));        distinctFilenames.put(finalFilename, result);        outputFilenames.add(KV.of(result, finalFilename));    }    return outputFilenames;}
private static void beam_f14531_1(WritableByteChannel channel, ResourceId filename, Exception prior) throws Exception
{    try {        channel.close();    } catch (Exception e) {                prior.addSuppressed(e);        throw prior;    }}
public final void beam_f14532_1() throws Exception
{    if (outputFile != null) {                        FileSystems.delete(Collections.singletonList(outputFile), StandardMoveOptions.IGNORE_MISSING_FILES);    }}
public PaneInfo beam_f14541_0()
{    return paneInfo;}
public DestinationT beam_f14542_0()
{    return destination;}
public final String beam_f14551_0()
{    return fileOrPatternSpec.get();}
public final ValueProvider<String> beam_f14552_0()
{    return fileOrPatternSpec;}
public String beam_f14561_0()
{    switch(mode) {        case FILEPATTERN:            return fileOrPatternSpec.toString();        case SINGLE_FILE_OR_SUBRANGE:            return fileOrPatternSpec + " range " + super.toString();        default:            throw new IllegalStateException("Unexpected mode: " + mode);    }}
public void beam_f14562_0()
{    super.validate();    switch(mode) {        case FILEPATTERN:            checkArgument(getStartOffset() == 0, "FileBasedSource is based on a file pattern or a full single file " + "but the starting offset proposed %s is not zero", getStartOffset());            checkArgument(getEndOffset() == Long.MAX_VALUE, "FileBasedSource is based on a file pattern or a full single file " + "but the ending offset proposed %s is not Long.MAX_VALUE", getEndOffset());            break;        case SINGLE_FILE_OR_SUBRANGE:                        break;        default:            throw new IllegalStateException("Unknown mode: " + mode);    }}
private boolean beam_f14571_0() throws IOException
{    while (fileReadersIterator.hasNext()) {        currentReader = fileReadersIterator.next();        if (currentReader.start()) {            return true;        }        currentReader.close();    }    return false;}
public T beam_f14572_0() throws NoSuchElementException
{        return currentReader.getCurrent();}
public static Write<Void, InputT> beam_f14581_0()
{    return new AutoValue_FileIO_Write.Builder<Void, InputT>().setDynamic(false).setCompression(Compression.UNCOMPRESSED).setIgnoreWindowing(false).setNoSpilling(false).build();}
public static Write<DestT, InputT> beam_f14582_0()
{    return new AutoValue_FileIO_Write.Builder<DestT, InputT>().setDynamic(true).setCompression(Compression.UNCOMPRESSED).setIgnoreWindowing(false).setNoSpilling(false).build();}
public int beam_f14591_0()
{    return Objects.hashCode(metadata, compression);}
public static MatchConfiguration beam_f14592_0(EmptyMatchTreatment emptyMatchTreatment)
{    return new AutoValue_FileIO_MatchConfiguration.Builder().setEmptyMatchTreatment(emptyMatchTreatment).build();}
public PCollection<MatchResult.Metadata> beam_f14601_0(PBegin input)
{    return input.apply("Create filepattern", Create.ofProvider(getFilepattern(), StringUtf8Coder.of())).apply("Via MatchAll", matchAll().withConfiguration(getConfiguration()));}
public void beam_f14602_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("filePattern", getFilepattern()).withLabel("Input File Pattern")).include("configuration", getConfiguration());}
public ReadMatches beam_f14611_0(Compression compression)
{    checkArgument(compression != null, "compression can not be null");    return toBuilder().setCompression(compression).build();}
public ReadMatches beam_f14612_0(DirectoryTreatment directoryTreatment)
{    checkArgument(directoryTreatment != null, "directoryTreatment can not be null");    return toBuilder().setDirectoryTreatment(directoryTreatment).build();}
public Write<DestinationT, UserT> beam_f14621_0(SerializableFunction<UserT, DestinationT> destinationFn)
{    checkArgument(destinationFn != null, "destinationFn can not be null");    return by(fn(destinationFn));}
public Write<DestinationT, UserT> beam_f14622_0(Contextful<Fn<UserT, DestinationT>> destinationFn)
{    checkArgument(destinationFn != null, "destinationFn can not be null");    return toBuilder().setDestinationFn(destinationFn).build();}
public Write<DestinationT, UserT> beam_f14631_0(String suffix)
{    checkArgument(suffix != null, "suffix can not be null");    return withSuffix(StaticValueProvider.of(suffix));}
public Write<DestinationT, UserT> beam_f14632_0(ValueProvider<String> suffix)
{    checkArgument(suffix != null, "suffix can not be null");    return toBuilder().setFilenameSuffix(suffix).build();}
public Write<DestinationT, UserT> beam_f14641_0(int numShards)
{    checkArgument(numShards >= 0, "numShards must be non-negative, but was: %s", numShards);    if (numShards == 0) {        return withNumShards(null);    }    return withNumShards(StaticValueProvider.of(numShards));}
public Write<DestinationT, UserT> beam_f14642_0(@Nullable ValueProvider<Integer> numShards)
{    return toBuilder().setNumShards(numShards).build();}
public Writer<DestinationT, OutputT> beam_f14651_0() throws Exception
{    return new Writer<DestinationT, OutputT>(this, "") {        @Nullable        private Sink<OutputT> sink;        @Override        protected void prepareWrite(WritableByteChannel channel) throws Exception {            Fn<DestinationT, Sink<OutputT>> sinkFn = (Fn) spec.getSinkFn().getClosure();            sink = sinkFn.apply(getDestination(), new Fn.Context() {                @Override                public <T> T sideInput(PCollectionView<T> view) {                    return getWriteOperation().getSink().getDynamicDestinations().sideInput(view);                }            });            sink.open(channel);        }        @Override        public void write(OutputT value) throws Exception {            sink.write(value);        }        @Override        protected void finishWrite() throws Exception {            sink.flush();        }    };}
protected void beam_f14652_0(WritableByteChannel channel) throws Exception
{    Fn<DestinationT, Sink<OutputT>> sinkFn = (Fn) spec.getSinkFn().getClosure();    sink = sinkFn.apply(getDestination(), new Fn.Context() {        @Override        public <T> T sideInput(PCollectionView<T> view) {            return getWriteOperation().getSink().getDynamicDestinations().sideInput(view);        }    });    sink.open(channel);}
public FilenamePolicy beam_f14661_0(final DestinationT destination)
{    final FileNaming namingFn;    try {        namingFn = spec.getFileNamingFn().getClosure().apply(destination, getContext());    } catch (Exception e) {        throw new RuntimeException(e);    }    return new FilenamePolicy() {        @Override        public ResourceId windowedFilename(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, OutputFileHints outputFileHints) {                        return FileSystems.matchNewResource(namingFn.getFilename(window, paneInfo, numShards, shardNumber, spec.getCompression()), false);        }        @Nullable        @Override        public ResourceId unwindowedFilename(int shardNumber, int numShards, OutputFileHints outputFileHints) {            return FileSystems.matchNewResource(namingFn.getFilename(GlobalWindow.INSTANCE, PaneInfo.NO_FIRING, numShards, shardNumber, spec.getCompression()), false);        }    };}
public ResourceId beam_f14662_0(int shardNumber, int numShards, BoundedWindow window, PaneInfo paneInfo, OutputFileHints outputFileHints)
{        return FileSystems.matchNewResource(namingFn.getFilename(window, paneInfo, numShards, shardNumber, spec.getCompression()), false);}
private static MatchResult beam_f14671_0(String spec, MatchResult res, EmptyMatchTreatment emptyMatchTreatment) throws IOException
{    if (res.status() == Status.NOT_FOUND || (res.status() == Status.OK && res.metadata().isEmpty())) {        boolean notFoundAllowed = emptyMatchTreatment == EmptyMatchTreatment.ALLOW || (hasGlobWildcard(spec) && emptyMatchTreatment == EmptyMatchTreatment.ALLOW_IF_WILDCARD);        return notFoundAllowed ? MatchResult.create(Status.OK, Collections.emptyList()) : MatchResult.create(Status.NOT_FOUND, new FileNotFoundException("No files matched spec: " + spec));    }    return res;}
public static Metadata beam_f14672_0(String spec) throws IOException
{    List<MatchResult> matches = FileSystems.match(Collections.singletonList(spec));    MatchResult matchResult = Iterables.getOnlyElement(matches);    if (matchResult.status() == Status.NOT_FOUND) {        throw new FileNotFoundException(String.format("File spec %s not found", spec));    } else if (matchResult.status() != Status.OK) {        throw new IOException(String.format("Error matching file spec %s: status %s", spec, matchResult.status()));    } else {        List<Metadata> metadata = matchResult.metadata();        if (metadata.size() != 1) {            throw new IOException(String.format("Expecting spec %s to match exactly one file, but matched %s: %s", spec, metadata.size(), metadata));        }        return metadata.get(0);    }}
public ResourceId beam_f14681_0(@Nonnull Metadata input)
{    return input.resourceId();}
private static KV<List<ResourceId>, List<ResourceId>> beam_f14682_0(List<ResourceId> srcResourceIds, List<ResourceId> destResourceIds) throws IOException
{    validateSrcDestLists(srcResourceIds, destResourceIds);    if (srcResourceIds.isEmpty()) {                return KV.of(Collections.<ResourceId>emptyList(), Collections.<ResourceId>emptyList());    }    List<ResourceId> srcToHandle = new ArrayList<>();    List<ResourceId> destToHandle = new ArrayList<>();    List<MatchResult> matchResults = matchResources(srcResourceIds);    for (int i = 0; i < matchResults.size(); ++i) {        if (!matchResults.get(i).status().equals(Status.NOT_FOUND)) {            srcToHandle.add(srcResourceIds.get(i));            destToHandle.add(destResourceIds.get(i));        }    }    return KV.of(srcToHandle, destToHandle);}
public static MatchResult beam_f14691_0(Status status, List<Metadata> metadata)
{    return new AutoValue_MatchResult_Success(status, metadata);}
public List<Metadata> beam_f14692_0() throws IOException
{    return getMetadata();}
public boolean beam_f14701_0()
{    return true;}
public static MetadataCoderV2 beam_f14702_0()
{    return INSTANCE;}
private static void beam_f14711_0(ResourceId baseDirectory, List<ResourceId> allResourceIds)
{    ResourceId file1 = baseDirectory.resolve("child1", RESOLVE_FILE);    ResourceId file2 = baseDirectory.resolve("child2", RESOLVE_FILE);    ResourceId file2a = baseDirectory.resolve("child2", RESOLVE_FILE);    allResourceIds.add(file1);    allResourceIds.add(file2);    assertThat("Resolved file isDirectory()", file1.isDirectory(), is(false));    assertThat("Resolved file isDirectory()", file2.isDirectory(), is(false));    assertThat("Resolved file isDirectory()", file2a.isDirectory(), is(false));    ResourceId dir1 = baseDirectory.resolve("child1", RESOLVE_DIRECTORY);    ResourceId dir2 = baseDirectory.resolve("child2", RESOLVE_DIRECTORY);    ResourceId dir2a = baseDirectory.resolve("child2", RESOLVE_DIRECTORY);    assertThat("Resolved directory isDirectory()", dir1.isDirectory(), is(true));    assertThat("Resolved directory isDirectory()", dir2.isDirectory(), is(true));    assertThat("Resolved directory isDirectory()", dir2a.isDirectory(), is(true));    allResourceIds.add(dir1);    allResourceIds.add(dir2);        assertEqualityGroups(Arrays.asList(Arrays.asList(file1), Arrays.asList(file2, file2a), Arrays.asList(dir1, dir1.getCurrentDirectory()), Arrays.asList(dir2, dir2a, dir2.getCurrentDirectory()), Arrays.asList(baseDirectory, file1.getCurrentDirectory(), file2.getCurrentDirectory())));        assertEqualityGroups(Arrays.asList(Arrays.asList(file1.toString()), Arrays.asList(file2.toString(), file2a.toString()), Arrays.asList(dir1.toString(), dir1.getCurrentDirectory().toString()), Arrays.asList(dir2.toString(), dir2a.toString(), dir2.getCurrentDirectory().toString()), Arrays.asList(baseDirectory.toString(), file1.getCurrentDirectory().toString(), file2.getCurrentDirectory().toString())));}
private static void beam_f14712_0(List<List<T>> equalityGroups)
{    for (int i = 0; i < equalityGroups.size(); ++i) {        List<T> current = equalityGroups.get(i);        for (int j = 0; j < current.size(); ++j) {            for (int k = 0; k < current.size(); ++k) {                assertEquals("Value at " + j + " should equal value at " + k + " in equality group " + i, current.get(j), current.get(k));            }        }        for (int j = 0; j < equalityGroups.size(); ++j) {            if (i == j) {                continue;            }            assertTrue(current + " should not match any in " + equalityGroups.get(j), Collections.disjoint(current, equalityGroups.get(j)));        }    }}
public void beam_f14721_0(@Nullable Long elementsPerPeriod)
{    this.elementsPerPeriod = elementsPerPeriod;}
public static GenerateSequence beam_f14722_0(long from)
{    checkArgument(from >= 0, "Value of from must be non-negative, but was: %s", from);    return new AutoValue_GenerateSequence.Builder().setFrom(from).setTo(-1).setElementsPerPeriod(0).build();}
protected ReadableByteChannel beam_f14731_1(LocalResourceId resourceId) throws IOException
{            @SuppressWarnings("resource")    FileInputStream inputStream = new FileInputStream(resourceId.getPath().toFile());        return inputStream.getChannel();}
protected void beam_f14732_1(List<LocalResourceId> srcResourceIds, List<LocalResourceId> destResourceIds) throws IOException
{    checkArgument(srcResourceIds.size() == destResourceIds.size(), "Number of source files %s must equal number of destination files %s", srcResourceIds.size(), destResourceIds.size());    int numFiles = srcResourceIds.size();    for (int i = 0; i < numFiles; i++) {        LocalResourceId src = srcResourceIds.get(i);        LocalResourceId dst = destResourceIds.get(i);                File parent = dst.getCurrentDirectory().getPath().toFile();        if (!parent.exists()) {            checkArgument(parent.mkdirs() || parent.exists(), "Unable to make output directory %s in order to copy into file %s", parent, dst.getPath());        }                        Files.copy(src.getPath(), dst.getPath(), StandardCopyOption.REPLACE_EXISTING, StandardCopyOption.COPY_ATTRIBUTES);    }}
public Iterable<FileSystem> beam_f14741_0(@Nullable PipelineOptions options)
{    return ImmutableList.of(new LocalFileSystem());}
 static LocalResourceId beam_f14742_0(Path path, boolean isDirectory)
{    checkNotNull(path, "path");    return new LocalResourceId(path, isDirectory);}
public String beam_f14751_0()
{    return pathString;}
public boolean beam_f14752_0(Object obj)
{    if (!(obj instanceof LocalResourceId)) {        return false;    }    LocalResourceId other = (LocalResourceId) obj;    return this.pathString.equals(other.pathString) && this.isDirectory == other.isDirectory;}
public long beam_f14761_0(PipelineOptions options) throws Exception
{    long trueEndOffset = (endOffset == Long.MAX_VALUE) ? getMaxEndOffset(options) : endOffset;    return getBytesPerOffset() * (trueEndOffset - getStartOffset());}
public List<? extends OffsetBasedSource<T>> beam_f14762_0(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{            long desiredBundleSizeOffsetUnits = Math.max(Math.max(1, desiredBundleSizeBytes / getBytesPerOffset()), minBundleSize);    List<OffsetBasedSource<T>> subSources = new ArrayList<>();    for (OffsetRange range : new OffsetRange(startOffset, Math.min(endOffset, getMaxEndOffset(options))).split(desiredBundleSizeOffsetUnits, minBundleSize)) {        subSources.add(createSourceForSubrange(range.getFrom(), range.getTo()));    }    return subSources;}
public final boolean beam_f14771_0() throws IOException
{    return (advanceImpl() && rangeTracker.tryReturnRecordAt(isAtSplitPoint(), getCurrentOffset())) || rangeTracker.markDone();}
public synchronized OffsetBasedSource<T> beam_f14772_0()
{    return source;}
public ByteBuffer beam_f14781_0()
{    return value.asReadOnlyByteBuffer();}
public byte[] beam_f14782_0()
{    return value.toByteArray();}
public Boolean beam_f14791_0(ByteKey key)
{    return key.compareTo(startKey) >= 0 && endsAfterKey(key);}
public Boolean beam_f14792_0(ByteKeyRange other)
{        return endsAfterKey(other.startKey) && other.endsAfterKey(startKey);}
private static byte[] beam_f14801_0(byte[] array, byte b)
{    byte[] ret = new byte[array.length + 1];    ret[0] = b;    System.arraycopy(array, 0, ret, 1, array.length);    return ret;}
private static byte[] beam_f14802_0(byte[] array, int size)
{    int padding = size - array.length;    if (padding == 0) {        return array;    }    if (padding < 0) {                        verify(padding == -1, "key %s: expected length %d with exactly one byte of padding, found %d", ByteKey.copyFrom(array), size, -padding);        verify((array[0] == 0) && ((array[1] & 0x80) == 0x80), "key %s: is 1 byte longer than expected, indicating BigInteger padding. Expect first byte" + " to be zero with set MSB in second byte.", ByteKey.copyFrom(array));        return Arrays.copyOfRange(array, 1, array.length);    }    byte[] ret = new byte[size];    System.arraycopy(array, 0, ret, padding, array.length);    return ret;}
public synchronized boolean beam_f14811_1(boolean isAtSplitPoint, ByteKey recordStart)
{    if (done) {        return false;    }    checkState(!(position == null && !isAtSplitPoint), "The first record must be at a split point");    checkState(!(recordStart.compareTo(range.getStartKey()) < 0), "Trying to return record which is before the start key");    checkState(!(position != null && recordStart.compareTo(position) < 0), "Trying to return record which is before the last-returned record");    if (position == null) {                range = range.withStartKey(recordStart);    }    position = recordStart;    if (isAtSplitPoint) {        if (!range.containsKey(recordStart)) {            done = true;            return false;        }        ++splitPointsSeen;    }    return true;}
public synchronized boolean beam_f14812_1(ByteKey splitPosition)
{        if (!range.containsKey(splitPosition)) {                return false;    }        if (position == null) {                return false;    }        if (splitPosition.compareTo(position) <= 0) {                return false;    }    range = range.withEndKey(splitPosition);    return true;}
public boolean beam_f14821_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    OffsetRange that = (OffsetRange) o;    if (from != that.from) {        return false;    }    return to == that.to;}
public int beam_f14822_0()
{    int result = (int) (from ^ (from >>> 32));    result = 31 * result + (int) (to ^ (to >>> 32));    return result;}
public synchronized boolean beam_f14831_0()
{        return (offsetOfLastSplitPoint != -1) || done;}
public synchronized boolean beam_f14832_0()
{    return done;}
public synchronized long beam_f14841_0()
{    if (!isStarted()) {        return 0;    } else if (isDone()) {        return splitPointsSeen;    } else {                checkState(splitPointsSeen > 0, "A started rangeTracker should have seen > 0 split points (is %s)", splitPointsSeen);        return splitPointsSeen - 1;    }}
public synchronized boolean beam_f14842_0()
{    done = true;    return false;}
public String beam_f14851_0()
{    return String.format("Read(%s)", NameUtils.approximateSimpleName(source));}
public void beam_f14852_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("source", source.getClass()).withLabel("Read Source")).include("source", source);}
public FileIO.ReadableFile beam_f14861_0(InputStream is) throws IOException
{    MatchResult.Metadata metadata = MetadataCoder.of().decode(is);    Compression compression = Compression.values()[VarIntCoder.of().decode(is)];    return new FileIO.ReadableFile(metadata, compression);}
public PCollection<T> beam_f14862_0(PCollection<ReadableFile> input)
{    return input.apply("Split into ranges", ParDo.of(new SplitIntoRangesFn(desiredBundleSizeBytes))).apply("Reshuffle", Reshuffle.viaRandomKey()).apply("Read ranges", ParDo.of(new ReadFileRangesFn<>(createSource))).setCoder(coder);}
public Coder<T> beam_f14872_0()
{        return getDefaultOutputCoder();}
public static Read beam_f14874_0()
{    return new AutoValue_TextIO_Read.Builder().setCompression(Compression.AUTO).setHintMatchesManyFiles(false).setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW)).build();}
public Read beam_f14883_0(Compression compression)
{    return toBuilder().setCompression(compression).build();}
public Read beam_f14884_0(Duration pollInterval, TerminationCondition<String, ?> terminationCondition)
{    return withMatchConfiguration(getMatchConfiguration().continuously(pollInterval, terminationCondition));}
public ReadAll beam_f14893_0(TextIO.CompressionType compressionType)
{    return withCompression(compressionType.canonical);}
public ReadAll beam_f14894_0(Compression compression)
{    return toBuilder().setCompression(compression).build();}
public void beam_f14903_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("delimiter", Arrays.toString(getDelimiter())).withLabel("Custom delimiter to split records"));}
public FileBasedSource<String> beam_f14904_0(String input)
{    return new TextSource(StaticValueProvider.of(input), EmptyMatchTreatment.DISALLOW, delimiter);}
public TypedWrite<UserT, DestinationT> beam_f14913_0(ValueProvider<ResourceId> tempDirectory)
{    return toBuilder().setTempDirectory(tempDirectory).build();}
public TypedWrite<UserT, DestinationT> beam_f14914_0(ResourceId tempDirectory)
{    return withTempDirectory(StaticValueProvider.of(tempDirectory));}
public TypedWrite<UserT, DestinationT> beam_f14923_0(Compression compression)
{    checkArgument(compression != null, "compression can not be null");    return withWritableByteChannelFactory(FileBasedSink.CompressionType.fromCanonical(compression));}
public TypedWrite<UserT, DestinationT> beam_f14924_0()
{    return toBuilder().setWindowedWrites(true).build();}
public Write beam_f14933_0(FilenamePolicy filenamePolicy)
{    return new Write(inner.to(filenamePolicy).withFormatFunction(SerializableFunctions.identity()));}
public Write beam_f14934_0(DynamicDestinations<String, ?, String> dynamicDestinations)
{    return new Write(inner.to((DynamicDestinations) dynamicDestinations).withFormatFunction(null));}
public Write beam_f14943_0(@Nullable String header)
{    return new Write(inner.withHeader(header));}
public Write beam_f14944_0(@Nullable String footer)
{    return new Write(inner.withFooter(footer));}
public static Sink beam_f14953_0()
{    return new AutoValue_TextIO_Sink.Builder().build();}
public Sink beam_f14954_0(String header)
{    checkArgument(header != null, "header can not be null");    return toBuilder().setHeader(header).build();}
public boolean beam_f14963_0(int numberOfFiles, long totalReadBytes)
{    return totalReadBytes > limit;}
public WriteOperation<DestinationT, String> beam_f14964_0()
{    return new TextWriteOperation<>(this, delimiter, header, footer);}
protected FileBasedSource<String> beam_f14973_0(MatchResult.Metadata metadata, long start, long end)
{    return new TextSource(metadata, start, end, delimiter);}
protected FileBasedReader<String> beam_f14974_0(PipelineOptions options)
{    return new TextBasedReader(this, delimiter);}
private boolean beam_f14983_0(int minCapacity) throws IOException
{        while (buffer.size() <= minCapacity && !eof) {        eof = inChannel.read(readBuffer) == -1;        readBuffer.flip();        buffer = buffer.concat(ByteString.copyFrom(readBuffer));        readBuffer.clear();    }        return buffer.size() >= minCapacity;}
public static Read beam_f14984_0()
{    return new AutoValue_TFRecordIO_Read.Builder().setValidate(true).setCompression(Compression.AUTO).build();}
public PCollection<byte[]> beam_f14993_0(PBegin input)
{    if (getFilepattern() == null) {        throw new IllegalStateException("Need to set the filepattern of a TFRecordIO.Read transform");    }    if (getValidate()) {        checkState(getFilepattern().isAccessible(), "Cannot validate with a RVP.");        try {            MatchResult matches = FileSystems.match(getFilepattern().get());            checkState(!matches.metadata().isEmpty(), "Unable to find any files matching %s", getFilepattern().get());        } catch (IOException e) {            throw new IllegalStateException(String.format("Failed to validate %s", getFilepattern().get()), e);        }    }    return input.apply("Read", org.apache.beam.sdk.io.Read.from(getSource()));}
protected FileBasedSource<byte[]> beam_f14994_0()
{    return CompressedSource.from(new TFRecordSource(getFilepattern())).withCompression(getCompression());}
public Write beam_f15003_0(String shardTemplate)
{    return toBuilder().setShardTemplate(shardTemplate).build();}
public Write beam_f15004_0()
{    return withNumShards(1).withShardNameTemplate("");}
protected FileBasedSource<byte[]> beam_f15014_0(Metadata metadata, long start, long end)
{    checkArgument(start == 0, "TFRecordSource is not splittable");    return new TFRecordSource(metadata, start, end);}
protected FileBasedReader<byte[]> beam_f15015_0(PipelineOptions options)
{    return new TFRecordReader(this);}
public Writer<Void, byte[]> beam_f15024_0() throws Exception
{    return new TFRecordWriter(this);}
protected void beam_f15025_0(WritableByteChannel channel) throws Exception
{    this.outChannel = channel;    this.codec = new TFRecordCodec();}
public byte[] beam_f15035_0() throws NoSuchElementException
{    if (getCurrentSource().requiresDeduping()) {        throw new IllegalStateException("getCurrentRecordId() must be overridden if requiresDeduping returns true()");    }    return EMPTY;}
public long beam_f15036_0()
{    return BACKLOG_UNKNOWN;}
public WriteFiles<UserT, DestinationT, OutputT> beam_f15045_0()
{    return toBuilder().setComputeNumShards(null).setNumShardsProvider(null).build();}
public WriteFiles<UserT, DestinationT, OutputT> beam_f15046_0(ShardingFunction<UserT, DestinationT> shardingFunction)
{    return toBuilder().setShardingFunction(shardingFunction).build();}
public void beam_f15055_0(ProcessContext c)
{    c.output(c.element().withShard(UNKNOWN_SHARDNUM));}
public void beam_f15056_0(StartBundleContext c)
{        writers = Maps.newHashMap();}
public void beam_f15065_0(ProcessContext context) throws Exception
{    getDynamicDestinations().setSideInputAccessorFromProcessContext(context);    final int shardCount;    if (numShardsView != null) {        shardCount = context.sideInput(numShardsView);    } else {        checkNotNull(getNumShardsProvider());        shardCount = getNumShardsProvider().get();    }    checkArgument(shardCount > 0, "Must have a positive number of shards specified for non-runner-determined sharding." + " Got %s", shardCount);    DestinationT destination = getDynamicDestinations().getDestination(context.element());    ShardedKey<Integer> shardKey = shardingFn.assignShardKey(destination, context.element(), shardCount);    context.output(KV.of(shardKey, context.element()));}
public void beam_f15066_1(ProcessContext c, BoundedWindow window) throws Exception
{    getDynamicDestinations().setSideInputAccessorFromProcessContext(c);                Map<DestinationT, Writer<DestinationT, OutputT>> writers = Maps.newHashMap();    for (UserT input : c.element().getValue()) {        DestinationT destination = getDynamicDestinations().getDestination(input);        Writer<DestinationT, OutputT> writer = writers.get(destination);        if (writer == null) {            String uuid = UUID.randomUUID().toString();                        writer = writeOperation.createWriter();            writer.setDestination(destination);            writer.open(uuid);            writers.put(destination, writer);        }        writeOrClose(writer, getDynamicDestinations().formatRecord(input));    }        for (Map.Entry<DestinationT, Writer<DestinationT, OutputT>> entry : writers.entrySet()) {        Writer<DestinationT, OutputT> writer = entry.getValue();        try {                        writer.close();        } catch (Exception e) {                        writer.cleanup();            throw e;        }        int shard = c.element().getKey().getShardNumber();        checkArgument(shard != UNKNOWN_SHARDNUM, "Shard should have been set, but is unset for element %s", c.element());        c.output(new FileResult<>(writer.getOutputFile(), shard, window, c.pane(), entry.getKey()));    }}
public Pipeline beam_f15075_0()
{    return pipeline;}
public PCollection<KV<DestinationT, String>> beam_f15077_0()
{    return perDestinationOutputFilenames;}
public static GaugeResult beam_f15086_0()
{    return EmptyGaugeResult.INSTANCE;}
public long beam_f15087_0()
{    return -1L;}
public String beam_f15096_0()
{    return String.format("%s:%s", getNamespace(), getName());}
public String beam_f15097_0()
{    return getName();}
public static MetricQueryResults beam_f15106_0(Iterable<MetricResult<Long>> counters, Iterable<MetricResult<DistributionResult>> distributions, Iterable<MetricResult<GaugeResult>> gauges)
{    return new AutoValue_MetricQueryResults(counters, distributions, gauges);}
public MetricName beam_f15107_0()
{    return getKey().metricName();}
public MetricQueryResults beam_f15116_0()
{    return queryMetrics(MetricsFilter.builder().build());}
public String beam_f15117_0()
{    return allMetrics().toString();}
public MetricName beam_f15126_0()
{    return name;}
public void beam_f15127_0(long value)
{    MetricsContainer container = MetricsEnvironment.getCurrentContainer();    if (container != null) {        container.getGauge(name).set(value);    }}
public Set<MetricNameFilter> beam_f15136_0()
{    return immutableNames();}
public static Builder beam_f15137_0()
{    return new AutoValue_MetricsFilter.Builder();}
public static Counter beam_f15146_0(String splitId)
{    return Metrics.counter(SOURCE_SPLITS_NAMESPACE, renderName(splitId, BYTES_READ));}
public static Gauge beam_f15147_0()
{    return BACKLOG_BYTES_GAUGE;}
public Class<? extends PipelineRunner<?>> beam_f15156_0(PipelineOptions options)
{    try {        @SuppressWarnings({ "unchecked", "rawtypes" })        Class<? extends PipelineRunner<?>> direct = (Class<? extends PipelineRunner<?>>) Class.forName("org.apache.beam.runners.direct.DirectRunner", true, ReflectHelpers.findClassLoader());        return direct;    } catch (ClassNotFoundException e) {        throw new IllegalArgumentException(String.format("No Runner was specified and the DirectRunner was not found on the classpath.%n" + "Specify a runner by either:%n" + "    Explicitly specifying a runner by providing the 'runner' property%n" + "    Adding the DirectRunner to the classpath%n" + "    Calling 'PipelineOptions.setRunner(PipelineRunner)' directly"));    }}
public String beam_f15157_0(PipelineOptions options)
{    String appName = options.as(ApplicationNameOptions.class).getAppName();    String normalizedAppName = appName == null || appName.length() == 0 ? "BeamApp" : appName.toLowerCase().replaceAll("[^a-z0-9]", "0").replaceAll("^[^a-z]", "a");    String userName = MoreObjects.firstNonNull(System.getProperty("user.name"), "");    String normalizedUserName = userName.toLowerCase().replaceAll("[^a-z0-9]", "0");    String datePart = FORMATTER.print(DateTimeUtils.currentTimeMillis());    String randomPart = Integer.toHexString(ThreadLocalRandom.current().nextInt());    return String.format("%s-%s-%s-%s", normalizedAppName, normalizedUserName, datePart, randomPart);}
public Builder beam_f15166_0()
{    return new Builder(args, validation, false, isCli);}
public PipelineOptions beam_f15167_0()
{    return as(PipelineOptions.class);}
public static void beam_f15176_0(PrintStream out)
{    checkNotNull(out);    out.println("The set of registered options are:");    Set<Class<? extends PipelineOptions>> sortedOptions = new TreeSet<>(ClassNameComparator.INSTANCE);    sortedOptions.addAll(CACHE.get().registeredOptions);    for (Class<? extends PipelineOptions> kls : sortedOptions) {        out.format("  %s%n", kls.getName());    }    out.format("%nUse --help=<OptionsName> for detailed help. For example:%n" + "  --help=DataflowPipelineOptions <short names valid for registered options>%n" + "  --help=org.apache.beam.sdk.options.DataflowPipelineOptions%n");}
public static void beam_f15177_0(PrintStream out, Class<? extends PipelineOptions> iface)
{    checkNotNull(out);    checkNotNull(iface);    CACHE.get().validateWellFormed(iface);    Set<PipelineOptionSpec> properties = PipelineOptionsReflector.getOptionSpecs(iface, true);    RowSortedTable<Class<?>, String, Method> ifacePropGetterTable = TreeBasedTable.create(ClassNameComparator.INSTANCE, Ordering.natural());    for (PipelineOptionSpec prop : properties) {        ifacePropGetterTable.put(prop.getDefiningInterface(), prop.getName(), prop.getGetterMethod());    }    for (Map.Entry<Class<?>, Map<String, Method>> ifaceToPropertyMap : ifacePropGetterTable.rowMap().entrySet()) {        Class<?> currentIface = ifaceToPropertyMap.getKey();        Map<String, Method> propertyNamesToGetters = ifaceToPropertyMap.getValue();        SortedSetMultimap<String, String> requiredGroupNameToProperties = getRequiredGroupNamesToProperties(propertyNamesToGetters);        out.format("%s:%n", currentIface.getName());        prettyPrintDescription(out, currentIface.getAnnotation(Description.class));        out.println();        List<String> lists = Lists.newArrayList(propertyNamesToGetters.keySet());        lists.sort(String.CASE_INSENSITIVE_ORDER);        for (String propertyName : lists) {            Method method = propertyNamesToGetters.get(propertyName);            String printableType = method.getReturnType().getSimpleName();            if (method.getReturnType().isEnum()) {                printableType = Joiner.on(" | ").join(method.getReturnType().getEnumConstants());            }            out.format("  --%s=<%s>%n", propertyName, printableType);            Optional<String> defaultValue = getDefaultValueFromAnnotation(method);            if (defaultValue.isPresent()) {                out.format("    Default: %s%n", defaultValue.get());            }            prettyPrintDescription(out, method.getAnnotation(Description.class));            prettyPrintRequiredGroups(out, method.getAnnotation(Validation.Required.class), requiredGroupNameToProperties);        }        out.println();    }}
private static SortedSetMultimap<String, String> beam_f15186_0(Map<String, Method> propertyNamesToGetters)
{    SortedSetMultimap<String, String> result = TreeMultimap.create();    for (Map.Entry<String, Method> propertyEntry : propertyNamesToGetters.entrySet()) {        Required requiredAnnotation = propertyEntry.getValue().getAnnotation(Validation.Required.class);        if (requiredAnnotation != null) {            for (String groupName : requiredAnnotation.groups()) {                result.put(groupName, propertyEntry.getKey());            }        }    }    return result;}
private static List<PropertyDescriptor> beam_f15187_0(Class<? extends PipelineOptions> iface, Set<Class<? extends PipelineOptions>> validatedPipelineOptionsInterfaces, Class<? extends PipelineOptions> klass) throws IntrospectionException
{    checkArgument(Modifier.isPublic(iface.getModifiers()), "Please mark non-public interface %s as public. The JVM requires that " + "all non-public interfaces to be in the same package which will prevent the " + "PipelineOptions proxy class to implement all of the interfaces.", iface.getName());        validateReturnType(iface);    SortedSet<Method> allInterfaceMethods = FluentIterable.from(ReflectHelpers.getClosureOfMethodsOnInterfaces(validatedPipelineOptionsInterfaces)).append(ReflectHelpers.getClosureOfMethodsOnInterface(iface)).filter(NOT_SYNTHETIC_PREDICATE).filter(NOT_STATIC_PREDICATE).toSortedSet(MethodComparator.INSTANCE);    List<PropertyDescriptor> descriptors = getPropertyDescriptors(allInterfaceMethods, iface);        validateMethodAnnotations(allInterfaceMethods, descriptors);        validateGettersSetters(iface, descriptors);        validateMethodsAreEitherBeanMethodOrKnownMethod(iface, klass, descriptors);    return descriptors;}
private static void beam_f15196_0(Class<? extends PipelineOptions> iface, Class<? extends PipelineOptions> klass, List<PropertyDescriptor> descriptors)
{    Set<Method> knownMethods = Sets.newHashSet(IGNORED_METHODS);        for (Method method : klass.getMethods()) {        if (Modifier.isStatic(method.getModifiers()) || method.isSynthetic()) {            knownMethods.add(method);        }    }        try {        knownMethods.add(iface.getMethod("as", Class.class));        knownMethods.add(iface.getMethod("outputRuntimeOptions"));        knownMethods.add(iface.getMethod("populateDisplayData", DisplayData.Builder.class));    } catch (NoSuchMethodException | SecurityException e) {        throw new RuntimeException(e);    }    for (PropertyDescriptor descriptor : descriptors) {        knownMethods.add(descriptor.getReadMethod());        knownMethods.add(descriptor.getWriteMethod());    }    final Set<String> knownMethodsNames = Sets.newHashSet();    for (Method method : knownMethods) {        knownMethodsNames.add(method.getName());    }                SortedSet<Method> unknownMethods = new TreeSet<>(MethodComparator.INSTANCE);    unknownMethods.addAll(Sets.filter(Sets.difference(Sets.newHashSet(iface.getMethods()), knownMethods), Predicates.and(NOT_SYNTHETIC_PREDICATE, input -> !knownMethodsNames.contains(input.getName()), NOT_STATIC_PREDICATE)));    checkArgument(unknownMethods.isEmpty(), "Methods %s on [%s] do not conform to being bean properties.", FluentIterable.from(unknownMethods).transform(ReflectHelpers.METHOD_FORMATTER), iface.getName());}
private static void beam_f15197_0(Class<?> checkClass, Class fromClass, Set<Class<?>> nonPipelineOptions)
{    if (checkClass.equals(fromClass)) {        return;    }    if (checkClass.getInterfaces().length == 0) {        nonPipelineOptions.add(checkClass);        return;    }    for (Class<?> klass : checkClass.getInterfaces()) {        checkInheritedFrom(klass, fromClass, nonPipelineOptions);    }}
public int beam_f15206_0(Method o1, Method o2)
{    return o1.getName().compareTo(o2.getName());}
public Class<?> beam_f15207_0(@Nonnull Method input)
{    return input.getReturnType();}
private Registration<T> beam_f15216_0(Class<T> iface)
{    return validateWellFormed(iface, registeredOptions);}
 Set<String> beam_f15217_0()
{    ImmutableSortedSet.Builder<String> supportedRunners = ImmutableSortedSet.naturalOrder();    for (Class<? extends PipelineRunner<?>> runner : supportedPipelineRunners.values()) {        supportedRunners.add(runner.getSimpleName());    }    return supportedRunners.build();}
public static T beam_f15226_0(Class<T> klass, PipelineOptions options)
{    return validate(klass, options, false);}
public static T beam_f15227_0(Class<T> klass, PipelineOptions options)
{    return validate(klass, options, true);}
 static BoundValue beam_f15236_0(@Nullable Object value)
{    return BoundValue.of(value, false);}
 static BoundValue beam_f15237_0(@Nullable Object value)
{    return BoundValue.of(value, true);}
public synchronized String beam_f15246_0()
{    SortedMap<String, Object> sortedOptions = new TreeMap<>();        sortedOptions.putAll(jsonOptions);        for (Map.Entry<String, BoundValue> entry : options.entrySet()) {        sortedOptions.put(entry.getKey(), entry.getValue().getValue());    }    StringBuilder b = new StringBuilder();    b.append("Current Settings:\n");    for (Map.Entry<String, Object> entry : sortedOptions.entrySet()) {        b.append("  " + entry.getKey() + ": " + entry.getValue() + "\n");    }    return b.toString();}
private Object beam_f15247_0(String propertyName, Method method)
{    try {        JavaType type = PipelineOptionsFactory.MAPPER.getTypeFactory().constructType(method.getGenericReturnType());        JsonNode jsonNode = jsonOptions.get(propertyName);        return PipelineOptionsFactory.MAPPER.readValue(jsonNode.toString(), type);    } catch (IOException e) {        throw new RuntimeException("Unable to parse representation", e);    }}
public Iterable<Class<? extends PipelineOptions>> beam_f15256_0()
{    return ImmutableList.of(RemoteEnvironmentOptions.class);}
public SdkHarnessLogLevelOverrides beam_f15257_0(Class<?> klass, LogLevel logLevel)
{    checkNotNull(klass, "Expected class to be not null.");    addOverrideForName(klass.getName(), logLevel);    return this;}
public T beam_f15266_0()
{    if (cachedValue == null) {        cachedValue = translator.apply(value.get());    }    return cachedValue;}
public boolean beam_f15267_0()
{    return value.isAccessible();}
public JsonDeserializer<?> beam_f15276_0(DeserializationContext ctxt, BeanProperty property) throws JsonMappingException
{    checkNotNull(ctxt, "Null DeserializationContext.");    JavaType type = checkNotNull(ctxt.getContextualType(), "Invalid type: %s", getClass());    JavaType[] params = type.findTypeParameters(ValueProvider.class);    if (params.length != 1) {        throw new RuntimeException("Unable to derive type for ValueProvider: " + type.toString());    }    JavaType param = params[0];    return new Deserializer(param);}
public ValueProvider<?> beam_f15277_0(JsonParser jp, DeserializationContext ctxt) throws IOException, JsonProcessingException
{    JsonDeserializer dser = ctxt.findRootValueDeserializer(checkNotNull(innerType, "Invalid %s: innerType is null. Serialization error?", getClass()));    Object o = dser.deserialize(jp, ctxt);    return StaticValueProvider.of(o);}
public void beam_f15286_0(List<PTransformOverride> overrides)
{    for (PTransformOverride override : overrides) {        replace(override);    }    checkNoMoreMatches(overrides);}
private void beam_f15287_0(final List<PTransformOverride> overrides)
{    traverseTopologically(new PipelineVisitor.Defaults() {        SetMultimap<Node, PTransformOverride> matched = HashMultimap.create();        @Override        public CompositeBehavior enterCompositeTransform(Node node) {            if (!node.isRootNode()) {                checkForMatches(node);            }            if (matched.containsKey(node)) {                return CompositeBehavior.DO_NOT_ENTER_TRANSFORM;            } else {                return CompositeBehavior.ENTER_TRANSFORM;            }        }        @Override        public void leaveCompositeTransform(Node node) {            if (node.isRootNode()) {                checkState(matched.isEmpty(), "Found nodes that matched overrides. Matches: %s", matched);            }        }        @Override        public void visitPrimitiveTransform(Node node) {            checkForMatches(node);        }        private void checkForMatches(Node node) {            for (PTransformOverride override : overrides) {                if (override.getMatcher().matchesDuringValidation(node.toAppliedPTransform(getPipeline()))) {                    matched.put(node, override);                }            }        }    });}
public PipelineResult beam_f15296_1(PipelineOptions options)
{    PipelineRunner<? extends PipelineResult> runner = PipelineRunner.fromOptions(options);                try {        validate(options);        return runner.run(this);    } catch (UserCodeException e) {                throw new PipelineExecutionException(e.getCause());    }}
public CoderRegistry beam_f15297_0()
{    if (coderRegistry == null) {        coderRegistry = CoderRegistry.createDefault();    }    return coderRegistry;}
public static OutputT beam_f15309_0(String name, InputT input, PTransform<? super InputT, OutputT> transform)
{    return input.getPipeline().applyInternal(name, input, transform);}
public String beam_f15310_0()
{    return "Pipeline#" + hashCode();}
public String beam_f15319_0(@Nonnull final Map.Entry<String, Collection<PTransform<?, ?>>> input)
{    final Collection<PTransform<?, ?>> values = instances.get(input.getKey());    return "- name=" + input.getKey() + ":\n" + Joiner.on("\n").join(transform(values, new TransformToMessage()));}
public String beam_f15320_0(@Nonnull final Map.Entry<String, Collection<PTransform<?, ?>>> input)
{    return input.getKey();}
 boolean beam_f15329_0(AppliedPTransform<?, ?, ?> application)
{    return matches(application);}
 PTransformMatcher beam_f15330_0(PTransformMatcher matcher)
{    return application -> this.matches(application) && matcher.matches(application);}
public void beam_f15339_0()
{        for (PValue inputValue : current.getInputs().values()) {        PInput input = producerInput.remove(inputValue);        Node producerNode = maybeGetProducer(inputValue);        if (producerNode != null) {            inputValue.finishSpecifying(input, producerNode.getTransform());        }    }}
public void beam_f15340_0(POutput output)
{    for (PCollection<?> value : fullyExpand(output).values()) {        if (!producers.containsKey(value)) {            producers.put(value, current);            value.finishSpecifyingOutput(current.getFullName(), unexpandedInputs.get(current), current.transform);        }        producerInput.put(value, unexpandedInputs.get(current));    }    output.finishSpecifyingOutput(current.getFullName(), unexpandedInputs.get(current), current.transform);    current.setOutput(output);}
public PTransform<?, ?> beam_f15349_0()
{    return transform;}
public Node beam_f15350_0()
{    return enclosingNode;}
 void beam_f15359_1(Map<PValue, ReplacementOutput> originalToReplacement)
{    checkNotNull(this.outputs, "Outputs haven't been specified for node %s yet", getFullName());    for (Node component : this.parts) {                component.replaceOutputs(originalToReplacement);    }    ImmutableMap.Builder<TupleTag<?>, PValue> newOutputsBuilder = ImmutableMap.builder();    for (Map.Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {        ReplacementOutput mapping = originalToReplacement.get(output.getValue());        if (mapping != null) {            if (this.equals(producers.get(mapping.getReplacement().getValue()))) {                                                                producerInput.remove(mapping.getReplacement().getValue());                                producers.remove(mapping.getReplacement().getValue());                producers.put(mapping.getOriginal().getValue(), this);            }                        newOutputsBuilder.put(output.getKey(), mapping.getOriginal().getValue());        } else {            newOutputsBuilder.put(output);        }    }    ImmutableMap<TupleTag<?>, PValue> newOutputs = newOutputsBuilder.build();    checkState(outputs.size() == newOutputs.size(), "Number of outputs must be stable across replacement");    this.outputs = newOutputs;}
public Map<TupleTag<?>, PValue> beam_f15360_0()
{    return outputs == null ? Collections.emptyMap() : outputs;}
public SerializableFunction<Row, T> beam_f15369_0(TypeDescriptor<T> typeDescriptor)
{    ProviderAndDescriptor providerAndDescriptor = getSchemaProvider(typeDescriptor);    return (providerAndDescriptor != null) ? providerAndDescriptor.schemaProvider.fromRowFunction((TypeDescriptor<T>) providerAndDescriptor.typeDescriptor) : null;}
public List<SchemaProvider> beam_f15370_0()
{    return ImmutableList.of(new DefaultSchemaProvider());}
public FieldValueTypeInformationFactory beam_f15379_0()
{    return AvroUtils::getFieldTypes;}
public CreatedT beam_f15380_0(Class<?> clazz, Schema schema)
{    if (cache == null) {        cache = new ConcurrentHashMap<>();    }    CreatedT cached = cache.get(clazz);    if (cached != null) {        return cached;    }    cached = innerFactory.create(clazz, schema);    cache.put(clazz, cached);    return cached;}
public static FieldAccessDescriptor beam_f15389_0(Iterable<Integer> ids)
{    List<FieldDescriptor> fields = StreamSupport.stream(ids.spliterator(), false).map(n -> FieldDescriptor.builder().setFieldId(n).build()).collect(Collectors.toList());    return withFields(fields);}
public static FieldAccessDescriptor beam_f15390_0(FieldDescriptor... fields)
{    return withFields(Arrays.asList(fields));}
public Set<String> beam_f15399_0()
{    return getFieldsAccessed().stream().map(FieldDescriptor::getFieldName).collect(Collectors.toSet());}
public Map<Integer, FieldAccessDescriptor> beam_f15400_0()
{    return getNestedFieldsAccessed().entrySet().stream().collect(Collectors.toMap(f -> f.getKey().getFieldId(), f -> f.getValue()));}
private static void beam_f15409_0(Schema schema, FieldDescriptor fieldDescriptor)
{    Integer fieldId = fieldDescriptor.getFieldId();    if (fieldId != null) {        if (fieldId < 0 || fieldId >= schema.getFieldCount()) {            throw new IllegalArgumentException("Invalid field id " + fieldId + " for schema " + schema);        }    }            Field field = (fieldId != null) ? schema.getField(fieldId) : schema.getField(fieldDescriptor.getFieldName());    FieldType fieldType = field.getType();    for (Qualifier qualifier : fieldDescriptor.getQualifiers()) {        switch(qualifier.getKind()) {            case LIST:                checkArgument(qualifier.getList().equals(ListQualifier.ALL));                checkArgument(fieldType.getTypeName().equals(TypeName.ARRAY));                fieldType = fieldType.getCollectionElementType();                break;            case MAP:                checkArgument(qualifier.getMap().equals(MapQualifier.ALL));                checkArgument(fieldType.getTypeName().equals(TypeName.MAP));                fieldType = fieldType.getMapValueType();                break;            default:                throw new IllegalStateException("Unexpected qualifier type " + qualifier.getKind());        }    }}
public String beam_f15410_0()
{    if (getAllFields()) {        return "*";    }    List<String> singleSelectors = getFieldsAccessed().stream().map(FieldDescriptor::getFieldName).collect(Collectors.toList());    List<String> nestedSelectors = getNestedFieldsAccessed().entrySet().stream().map(e -> e.getKey().getFieldName() + "." + e.getValue().toString()).collect(Collectors.toList());    ;    return String.join(", ", Iterables.concat(singleSelectors, nestedSelectors));}
private static FieldValueTypeInformation beam_f15419_0(Field field)
{    return getArrayComponentType(TypeDescriptor.of(field.getGenericType()));}
private static FieldValueTypeInformation beam_f15420_0(TypeDescriptor valueType)
{        TypeDescriptor componentType = null;    if (valueType.isArray()) {        Type component = valueType.getComponentType().getType();        if (!component.equals(byte.class)) {            componentType = TypeDescriptor.of(component);        }    } else if (valueType.isSubtypeOf(TypeDescriptor.of(Collection.class))) {        TypeDescriptor<Collection<?>> collection = valueType.getSupertype(Collection.class);        if (collection.getType() instanceof ParameterizedType) {            ParameterizedType ptype = (ParameterizedType) collection.getType();            java.lang.reflect.Type[] params = ptype.getActualTypeArguments();            checkArgument(params.length == 1);            componentType = TypeDescriptor.of(params[0]);        } else {            throw new RuntimeException("Collection parameter is not parameterized!");        }    }    if (componentType == null) {        return null;    }    return new AutoValue_FieldValueTypeInformation.Builder().setName("").setNullable(false).setType(componentType).setRawType(componentType.getRawType()).setElementType(getArrayComponentType(componentType)).setMapKeyType(getMapKeyType(componentType)).setMapValueType(getMapValueType(componentType)).build();}
private List beam_f15429_0(FieldType elementType, List<ElementT> rowList, FieldValueTypeInformation elementTypeInformation, Factory<List<FieldValueTypeInformation>> typeFactory)
{    List list = Lists.newArrayList();    for (ElementT element : rowList) {        list.add(fromValue(elementType, element, elementTypeInformation.getType().getType(), elementTypeInformation.getElementType(), elementTypeInformation.getMapKeyType(), elementTypeInformation.getMapValueType(), typeFactory));    }    return list;}
private Map<?, ?> beam_f15430_0(FieldType keyType, FieldType valueType, Map<?, ?> map, FieldValueTypeInformation keyTypeInformation, FieldValueTypeInformation valueTypeInformation, Factory<List<FieldValueTypeInformation>> typeFactory)
{    Map newMap = Maps.newHashMap();    for (Map.Entry<?, ?> entry : map.entrySet()) {        Object key = fromValue(keyType, entry.getKey(), keyTypeInformation.getType().getType(), keyTypeInformation.getElementType(), keyTypeInformation.getMapKeyType(), keyTypeInformation.getMapValueType(), typeFactory);        Object value = fromValue(valueType, entry.getValue(), valueTypeInformation.getType().getType(), valueTypeInformation.getElementType(), valueTypeInformation.getMapKeyType(), valueTypeInformation.getMapValueType(), typeFactory);        newMap.put(key, value);    }    return newMap;}
public List<FieldValueSetter> beam_f15439_0(Class<?> targetClass, Schema schema)
{    return JavaBeanUtils.getSetters(targetClass, schema, SetterTypeSupplier.INSTANCE);}
public List<FieldValueTypeInformation> beam_f15440_0(Class<?> clazz)
{    List<FieldValueTypeInformation> types = ReflectUtils.getFields(clazz).stream().filter(f -> !f.isAnnotationPresent(SchemaIgnore.class)).map(FieldValueTypeInformation::forField).map(t -> {        SchemaFieldName fieldName = t.getField().getAnnotation(SchemaFieldName.class);        return (fieldName != null) ? t.withName(fieldName.value()) : t;    }).collect(Collectors.toList());        if (ReflectUtils.getAnnotatedCreateMethod(clazz) == null && ReflectUtils.getAnnotatedConstructor(clazz) == null) {        Optional<Field> finalField = types.stream().map(FieldValueTypeInformation::getField).filter(f -> Modifier.isFinal(f.getModifiers())).findAny();        if (finalField.isPresent()) {            throw new IllegalArgumentException("Class " + clazz + " has final fields and no " + "registered creator. Cannot use as schema, as we don't know how to create this " + "object automatically");        }    }    return types;}
public T beam_f15449_0(T base)
{    return base;}
public static FixedBytes beam_f15450_0(int byteArraySize)
{    return new FixedBytes(byteArraySize);}
public FieldAccessDescriptor beam_f15459_0(DotExpressionContext ctx)
{    List<FieldAccessDescriptor> components = ctx.dotExpressionComponent().stream().map(dotE -> dotE.accept(this)).collect(Collectors.toList());        checkArgument(!components.isEmpty());    FieldAccessDescriptor fieldAccessDescriptor = components.get(components.size() - 1);    for (int i = components.size() - 2; i >= 0; --i) {        FieldAccessDescriptor component = components.get(i);        if (component.getAllFields()) {            throw new IllegalArgumentException("We currently only support wildcards at terminal" + " parts of selectors. 'x.*' is allowed, but x.*.y is not currently allowed.");                }        FieldDescriptor fieldAccessed = component.getFieldsAccessed().stream().findFirst().orElseThrow(IllegalArgumentException::new);        fieldAccessDescriptor = FieldAccessDescriptor.withFields().withNestedField(fieldAccessed, fieldAccessDescriptor);    }    return fieldAccessDescriptor;}
public FieldAccessDescriptor beam_f15460_0(QualifyComponentContext ctx)
{    return ctx.qualifiedComponent().accept(this);}
public int beam_f15469_0()
{    return Arrays.hashCode(array);}
public Builder beam_f15470_0(List<Field> fields)
{    this.fields.addAll(fields);    return this;}
public Builder beam_f15479_0(String name)
{    fields.add(Field.of(name, FieldType.INT64));    return this;}
public Builder beam_f15480_0(String name)
{    fields.add(Field.of(name, FieldType.DECIMAL));    return this;}
public Builder beam_f15489_0(String name, FieldType keyType, FieldType valueType)
{    fields.add(Field.of(name, FieldType.map(keyType, valueType)));    return this;}
public int beam_f15490_0()
{    return fields.size() - 1;}
public boolean beam_f15499_0(Schema other)
{    if (uuid != null && other.uuid != null && Objects.equals(uuid, other.uuid)) {        return true;    }    if (getFieldCount() != other.getFieldCount()) {        return false;    }    if (!Objects.equals(fieldIndices.values(), other.fieldIndices.values())) {        return false;    }    for (int i = 0; i < getFieldCount(); ++i) {        if (!getField(i).typesEqual(other.getField(i))) {            return false;        }    }    return true;}
public boolean beam_f15500_0(Schema other)
{    return equivalent(other, EquivalenceNullablePolicy.SAME);}
public boolean beam_f15509_0()
{    return STRING_TYPES.contains(this);}
public boolean beam_f15510_0()
{    return DATE_TYPES.contains(this);}
public static FieldType.Builder beam_f15519_0(TypeName typeName)
{    return new AutoValue_Schema_FieldType.Builder().setTypeName(typeName).setNullable(false).setMetadata(Collections.emptyMap());}
public static FieldType beam_f15520_0(TypeName typeName)
{    return forTypeName(typeName).build();}
public FieldType beam_f15529_0(String key, String metadata)
{    return withMetadata(key, metadata.getBytes(StandardCharsets.UTF_8));}
public byte[] beam_f15530_0(String key)
{    ByteArrayWrapper metadata = getMetadata().get(key);    return (metadata != null) ? metadata.array : null;}
public Field beam_f15539_0(String name)
{    return toBuilder().setName(name).build();}
public Field beam_f15540_0(String description)
{    return toBuilder().setDescription(description).build();}
public List<String> beam_f15549_0()
{    return getFields().stream().map(Schema.Field::getName).collect(Collectors.toList());}
public Field beam_f15550_0(int index)
{    return getFields().get(index);}
public SerializableFunction<Row, T> beam_f15559_0()
{    return fromRowFunction;}
public SerializableFunction<T, Row> beam_f15560_0()
{    return toRowFunction;}
public SerializableFunction<Row, T> beam_f15569_0(TypeDescriptor<T> typeDescriptor)
{    TypeDescriptor<?> type = typeDescriptor;    do {        SchemaProvider schemaProvider = providers.get(type);        if (schemaProvider != null) {            return (SerializableFunction<Row, T>) schemaProvider.fromRowFunction(type);        }        Class<?> superClass = type.getRawType().getSuperclass();        if (superClass == null || superClass.equals(Object.class)) {            return null;        }        type = TypeDescriptor.of(superClass);    } while (true);}
public static SchemaRegistry beam_f15570_0()
{    return new SchemaRegistry();}
public void beam_f15579_0(TypeDescriptor<T> typeDescriptor)
{    registerSchemaProvider(typeDescriptor, new JavaBeanSchema());}
public Schema beam_f15580_0(Class<T> clazz) throws NoSuchSchemaException
{    return getSchema(TypeDescriptor.of(clazz));}
public Object beam_f15589_0(Object... params)
{    Object object;    try {        object = clazz.getDeclaredConstructor().newInstance();    } catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException | InstantiationException e) {        throw new RuntimeException("Failed to instantiate object ", e);    }    for (int i = 0; i < params.length; ++i) {        FieldValueSetter setter = setters.get(i);        setter.set(object, params[i]);    }    return object;}
public static Inner<T> beam_f15590_0()
{    return new Inner<>();}
private static AddFieldsInformation beam_f15599_0(Schema.FieldType inputFieldType, Collection<NewField> nestedFields)
{    AddFieldsInformation addFieldsInformation;    Schema.FieldType fieldType;    switch(inputFieldType.getTypeName()) {        case ROW:            addFieldsInformation = getAddFieldsInformation(inputFieldType.getRowSchema(), nestedFields);            fieldType = addFieldsInformation.getOutputFieldType();            break;        case ARRAY:            addFieldsInformation = getAddFieldsInformation(inputFieldType.getCollectionElementType(), nestedFields);            fieldType = Schema.FieldType.array(addFieldsInformation.getOutputFieldType());            break;        case MAP:            addFieldsInformation = getAddFieldsInformation(inputFieldType.getMapValueType(), nestedFields);            fieldType = Schema.FieldType.map(inputFieldType.getMapKeyType(), addFieldsInformation.getOutputFieldType());            break;        default:            throw new RuntimeException("Cannot select a subfield of a non-composite type.");    }    fieldType = fieldType.withNullable(inputFieldType.getNullable());    return addFieldsInformation.toBuilder().setOutputFieldType(fieldType).build();}
private static Row beam_f15600_0(Row row, AddFieldsInformation addFieldsInformation)
{    Schema outputSchema = checkNotNull(addFieldsInformation.getOutputFieldType().getRowSchema());    List<Object> newValues = Lists.newArrayListWithCapacity(outputSchema.getFieldCount());    for (int i = 0; i < row.getFieldCount(); ++i) {        AddFieldsInformation nested = addFieldsInformation.getNestedNewValues().get(i);        if (nested != null) {                                    Object newValue = fillNewFields(row.getValue(i), nested.getOutputFieldType(), nested);            newValues.add(newValue);        } else {                        newValues.add(row.getValue(i));        }    }            newValues.addAll(addFieldsInformation.getDefaultValues());        for (int i = newValues.size(); i < addFieldsInformation.getNestedNewValues().size(); ++i) {        AddFieldsInformation newNestedField = addFieldsInformation.getNestedNewValues().get(i);        if (newNestedField != null) {            newValues.add(fillNewFields(null, newNestedField.getOutputFieldType(), newNestedField));        }    }    return Row.withSchema(outputSchema).attachValues(newValues).build();}
public String beam_f15609_0()
{    return "Cast.Widening";}
public List<CompatibilityError> beam_f15610_0(final Schema input, final Schema output)
{    return fold.apply(input, output);}
public List<CompatibilityError> beam_f15619_0(Context context, FieldType input, FieldType output)
{    TypeName inputType = input.getTypeName();    TypeName outputType = output.getTypeName();    boolean supertype = outputType.isSupertypeOf(inputType);    boolean subtype = outputType.isSubtypeOf(inputType);    if (isDecimal(inputType) && isIntegral(outputType)) {        return Collections.emptyList();    } else if (!supertype && !subtype) {        return Collections.singletonList(CompatibilityError.create(context.path(), "Can't cast '" + inputType + "' to '" + outputType + "'"));    }    return Collections.emptyList();}
public static boolean beam_f15620_0(TypeName type)
{    return type == TypeName.BYTE || type == TypeName.INT16 || type == TypeName.INT32 || type == TypeName.INT64;}
public static By beam_f15629_0(Integer... fieldIds)
{    return fieldAccessDescriptor(FieldAccessDescriptor.withFieldIds(fieldIds));}
public static By beam_f15630_0(FieldAccessDescriptor fieldAccessDescriptor)
{    return new AutoValue_CoGroup_By.Builder().setFieldAccessDescriptor(fieldAccessDescriptor).setOptionalParticipation(false).build();}
public void beam_f15639_0(@Element Row row, OutputReceiver<KV<Row, Row>> o)
{    o.output(KV.of(SelectHelpers.selectRow(row, keyFields, schema, keySchema), row));}
 static void beam_f15640_0(PCollectionTuple input, JoinArguments joinArgs)
{    if (joinArgs.allInputsJoinArgs == null) {                Set<String> inputTags = input.getAll().keySet().stream().map(TupleTag::getId).collect(Collectors.toSet());        Set<String> joinTags = joinArgs.joinArgsMap.keySet();        if (!inputTags.equals(joinTags)) {            throw new IllegalArgumentException("The input PCollectionTuple has tags: " + inputTags + " and the join was specified for tags " + joinTags + ". These do not match.");        }    }}
public void beam_f15649_0(@Element CoGbkResult gbkResult, OutputReceiver<Row> o)
{    List<Iterable> allIterables = extractIterables(gbkResult);    List<Row> accumulatedRows = Lists.newArrayListWithCapacity(sortedTags.size());    crossProduct(0, accumulatedRows, allIterables, o);}
private List<Iterable> beam_f15650_0(CoGbkResult gbkResult)
{    List<Iterable> iterables = Lists.newArrayListWithCapacity(sortedTags.size());    for (int i = 0; i < sortedTags.size(); ++i) {        String tag = sortedTags.get(i);        Iterable items = gbkResult.getAll(tagToKeyedTag.get(i));        if (!items.iterator().hasNext() && joinArgs.getOptionalParticipation(tag)) {                                    items = () -> NULL_LIST.iterator();        }        iterables.add(items);    }    return iterables;}
private static Schema beam_f15659_0(Schema schema)
{    if (schema.getFieldCount() != 1) {        return null;    }    FieldType fieldType = schema.getField(0).getType();    if (!fieldType.getTypeName().isCompositeType()) {        return null;    }    return fieldType.getRowSchema();}
public PCollection<OutputT> beam_f15660_0(PCollection<InputT> input)
{    if (!input.hasSchema()) {        throw new RuntimeException("Convert requires a schema on the input.");    }    SchemaRegistry registry = input.getPipeline().getSchemaRegistry();    ConvertHelpers.ConvertedSchemaInformation<OutputT> converted = ConvertHelpers.getConvertedSchemaInformation(input.getSchema(), outputTypeDescriptor, registry);    boolean unbox = converted.unboxedType != null;    PCollection<OutputT> output;    if (converted.outputSchemaCoder != null) {        output = input.apply(ParDo.of(new DoFn<InputT, OutputT>() {            @ProcessElement            public void processElement(@Element Row row, OutputReceiver<OutputT> o) {                                Object input = unbox ? row.getValue(0) : row;                                o.output(converted.outputSchemaCoder.getFromRowFunction().apply((Row) input));            }        }));        output = output.setSchema(converted.outputSchemaCoder.getSchema(), converted.outputSchemaCoder.getToRowFunction(), converted.outputSchemaCoder.getFromRowFunction());    } else {        SerializableFunction<?, OutputT> convertPrimitive = ConvertHelpers.getConvertPrimitive(converted.unboxedType, outputTypeDescriptor);        output = input.apply(ParDo.of(new DoFn<InputT, OutputT>() {            @ProcessElement            public void processElement(@Element Row row, OutputReceiver<OutputT> o) {                o.output(convertPrimitive.apply(row.getValue(0)));            }        }));        output.setTypeDescriptor(outputTypeDescriptor);        }    return output;}
public Inner<T> beam_f15669_0(String fieldName, SerializableFunction<FieldT, Boolean> predicate)
{    filters.add(new AutoValue_Filter_Inner_FilterDescription.Builder<FieldT>().setFieldAccessDescriptor(FieldAccessDescriptor.withFieldNames(fieldName)).setPredicate(predicate).setSelectsSingleField(true).build());    return this;}
public Inner<T> beam_f15670_0(int fieldId, SerializableFunction<FieldT, Boolean> predicate)
{    filters.add(new AutoValue_Filter_Inner_FilterDescription.Builder<FieldT>().setFieldAccessDescriptor(FieldAccessDescriptor.withFieldIds(fieldId)).setPredicate(predicate).setSelectsSingleField(true).build());    return this;}
public static ByFields<T> beam_f15679_0(Iterable<Integer> fieldIds)
{    return new ByFields<>(FieldAccessDescriptor.withFieldIds(fieldIds));}
public static ByFields<T> beam_f15680_0(FieldAccessDescriptor fieldAccess)
{    return new ByFields<>(fieldAccess);}
public CombineFieldsGlobally<InputT> beam_f15689_0(List<String> inputFieldNames, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return aggregateFields(FieldAccessDescriptor.withFieldNames(inputFieldNames), fn, outputField);}
public CombineFieldsGlobally<InputT> beam_f15690_0(List<Integer> inputFieldIds, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return aggregateFields(FieldAccessDescriptor.withFieldIds(inputFieldIds), fn, outputField);}
public CombineFieldsGlobally<InputT> beam_f15699_0(List<Integer> inputFieldIds, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return aggregateFields(FieldAccessDescriptor.withFieldIds(inputFieldIds), fn, outputFieldName);}
public CombineFieldsGlobally<InputT> beam_f15700_0(FieldAccessDescriptor fieldAccessDescriptor, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return new CombineFieldsGlobally<>(schemaAggregateFn.aggregateFields(fieldAccessDescriptor, fn, outputFieldName));}
public CombineFieldsByFields<InputT> beam_f15709_0(String inputFieldName, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return new CombineFieldsByFields<>(this, SchemaAggregateFn.<InputT>create().aggregateFields(FieldAccessDescriptor.withFieldNames(inputFieldName), fn, outputField));}
public CombineFieldsByFields<InputT> beam_f15710_0(int inputFieldId, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, Field outputField)
{    return new CombineFieldsByFields<>(this, SchemaAggregateFn.<InputT>create().aggregateFields(FieldAccessDescriptor.withFieldIds(inputFieldId), fn, outputField));}
public PCollection<KV<Row, OutputT>> beam_f15719_0(PCollection<InputT> input)
{    return input.apply(byFields).apply(Combine.groupedValues(combineFn));}
public CombineFieldsByFields<InputT> beam_f15720_0(String inputFieldName, CombineFn<CombineInputT, AccumT, CombineOutputT> fn, String outputFieldName)
{    return new CombineFieldsByFields<>(byFields, schemaAggregateFn.aggregateFields(FieldAccessDescriptor.withFieldNames(inputFieldName), fn, outputFieldName));}
public PCollection<KV<Row, Row>> beam_f15729_0(PCollection<InputT> input)
{    SchemaAggregateFn.Inner<InputT> fn = schemaAggregateFn.withSchema(input.getSchema(), input.getToRowFunction());    return input.apply(byFields).apply(Combine.groupedValues(fn));}
public static Impl beam_f15730_0(String... fieldNames)
{    return new Impl(FieldAccessDescriptor.withFieldNames(fieldNames), FieldAccessDescriptor.create());}
public Impl beam_f15739_0(String... fieldNames)
{    return new Impl(lhs, FieldAccessDescriptor.withFieldNames(fieldNames));}
public Impl beam_f15740_0(Integer... fieldIds)
{    return new Impl(lhs, FieldAccessDescriptor.withFieldIds(fieldIds));}
public Impl<LhsT, RhsT> beam_f15749_0(FieldAccessDescriptor fieldAccessDescriptor)
{    return new Impl<>(joinType, rhs, FieldsEqual.left(fieldAccessDescriptor).right(fieldAccessDescriptor));}
public Impl<LhsT, RhsT> beam_f15750_0(FieldsEqual.Impl predicate)
{    return new Impl<>(joinType, rhs, predicate);}
public void beam_f15759_0(@Element Row row, OutputReceiver<Row> o)
{    o.output(Row.withSchema(outputSchema).attachValues(row.getValues()).build());}
 static Inner<T> beam_f15760_0()
{    return new AutoValue_SchemaAggregateFn_Inner.Builder<T>().setFieldAggregations(Lists.newArrayList()).build();}
public Object[] beam_f15769_0(Object[] accumulator, T input)
{    return getComposedCombineFn().addInput(accumulator, input);}
public Object[] beam_f15770_0(Iterable<Object[]> accumulator)
{    return getComposedCombineFn().mergeAccumulators(accumulator);}
public static Inner<T> beam_f15779_0()
{    return new AutoValue_Unnest_Inner.Builder<T>().setFieldNameFunction(CONCAT_FIELD_NAMES).build();}
 static Schema beam_f15780_0(Schema schema)
{    List<String> nameComponents = Lists.newArrayList();    return getUnnestedSchema(schema, nameComponents, CONCAT_FIELD_NAMES);}
private static Class beam_f15789_0(Class<?> clazz)
{    String generatedClassName = getAutoValueGeneratedName(clazz.getName());    try {        return Class.forName(generatedClassName);    } catch (ClassNotFoundException e) {        throw new IllegalStateException("AutoValue generated class not found: " + generatedClassName);    }}
private static Class beam_f15790_0(Class<?> clazz)
{        String builderName = getAutoValueGeneratedName(clazz.getName()) + "$Builder";    try {        return Class.forName(builderName);    } catch (ClassNotFoundException e) {        return null;    }}
private static SchemaUserTypeCreator beam_f15799_0(Class<T> clazz, Schema schema)
{    Constructor baseConstructor = null;    Constructor[] constructors = clazz.getDeclaredConstructors();    for (Constructor constructor : constructors) {                if (constructor.getParameterCount() == schema.getFieldCount()) {            baseConstructor = constructor;        }    }    if (baseConstructor == null) {        throw new RuntimeException("No matching constructor found for class " + clazz);    }        MethodCall construct = MethodCall.construct(baseConstructor);    for (int i = 0; i < baseConstructor.getParameterTypes().length; ++i) {        Class<?> baseType = baseConstructor.getParameterTypes()[i];        construct = construct.with(readAndConvertParameter(baseType, i), baseType);    }    try {        DynamicType.Builder<SchemaUserTypeCreator> builder = BYTE_BUDDY.with(new InjectPackageStrategy(clazz)).subclass(SchemaUserTypeCreator.class).method(ElementMatchers.named("create")).intercept(construct);        return builder.make().load(ReflectHelpers.findClassLoader(clazz.getClassLoader()), ClassLoadingStrategy.Default.INJECTION).getLoaded().getDeclaredConstructor().newInstance();    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {        throw new RuntimeException("Unable to generate a getter for class " + clazz + " with schema " + schema);    }}
private static StackManipulation beam_f15800_0(Class<?> constructorParameterType, int index)
{                    ConvertType convertType = new ConvertType(true);        ForLoadedType convertedType = new ForLoadedType((Class) convertType.convert(TypeDescriptor.of(constructorParameterType)));            StackManipulation readParameter = new StackManipulation.Compound(MethodVariableAccess.REFERENCE.loadFrom(1), IntegerConstant.forValue(index), ArrayAccess.REFERENCE.load(), TypeCasting.to(convertedType));        return new ByteBuddyUtils.ConvertValueForSetter(readParameter).convert(TypeDescriptor.of(constructorParameterType));}
public static Schema beam_f15809_0(org.apache.avro.Schema schema)
{    Schema.Builder builder = Schema.builder();    for (org.apache.avro.Schema.Field field : schema.getFields()) {        Field beamField = toBeamField(field);        if (field.doc() != null) {            beamField = beamField.withDescription(field.doc());        }        builder.addField(beamField);    }    return builder.build();}
public static org.apache.avro.Schema beam_f15810_0(Schema beamSchema, @Nullable String name, @Nullable String namespace)
{    final String schemaName = Strings.isNullOrEmpty(name) ? "topLevelRecord" : name;    final String schemaNamespace = namespace == null ? "" : namespace;    String childNamespace = !"".equals(schemaNamespace) ? schemaNamespace + "." + schemaName : schemaName;    List<org.apache.avro.Schema.Field> fields = Lists.newArrayList();    for (Schema.Field field : beamSchema.getFields()) {        org.apache.avro.Schema.Field recordField = toAvroField(field, childNamespace);        fields.add(recordField);    }    return org.apache.avro.Schema.createRecord(schemaName, null, schemaNamespace, false, fields);}
public static SchemaCoder<T> beam_f15819_0(TypeDescriptor<T> type)
{    @SuppressWarnings("unchecked")    Class<T> clazz = (Class<T>) type.getRawType();    return schemaCoder(clazz);}
public static SchemaCoder<T> beam_f15820_0(Class<T> clazz)
{    return schemaCoder(clazz, new ReflectData(clazz.getClassLoader()).getSchema(clazz));}
public static List<FieldValueGetter> beam_f15829_0(Class<T> clazz, Schema schema)
{    if (TypeDescriptor.of(clazz).isSubtypeOf(TypeDescriptor.of(SpecificRecord.class))) {        return JavaBeanUtils.getGetters(clazz, schema, new AvroSpecificRecordFieldValueTypeSupplier());    } else {        return POJOUtils.getGetters(clazz, schema, new AvroPojoFieldValueTypeSupplier());    }}
public static SchemaUserTypeCreator beam_f15830_0(Class<T> clazz, Schema schema)
{    if (TypeDescriptor.of(clazz).isSubtypeOf(TypeDescriptor.of(SpecificRecord.class))) {        return AvroByteBuddyUtils.getCreator((Class<? extends SpecificRecord>) clazz, schema);    } else {        return POJOUtils.getSetFieldCreator(clazz, schema, new AvroPojoFieldValueTypeSupplier());    }}
private static Object beam_f15839_0(Integer value, Schema.FieldType fieldType)
{    checkTypeName(fieldType.getTypeName(), Schema.TypeName.INT32, "int");    return value;}
private static Object beam_f15840_0(Long value, Schema.FieldType fieldType)
{    checkTypeName(fieldType.getTypeName(), Schema.TypeName.INT64, "long");    return value;}
private static Object beam_f15849_0(Map<CharSequence, Object> values, org.apache.avro.Schema valueAvroSchema, Schema.FieldType fieldType)
{    checkTypeName(fieldType.getTypeName(), Schema.TypeName.MAP, "map");    checkNotNull(fieldType.getMapKeyType());    checkNotNull(fieldType.getMapValueType());    if (!fieldType.getMapKeyType().equals(Schema.FieldType.STRING)) {        throw new IllegalArgumentException("Can't convert 'string' map keys to " + fieldType.getMapKeyType());    }    Map<Object, Object> ret = new HashMap<>();    for (Map.Entry<CharSequence, Object> value : values.entrySet()) {        ret.put(convertStringStrict(value.getKey(), fieldType.getMapKeyType()), convertAvroFieldStrict(value.getValue(), valueAvroSchema, fieldType.getMapValueType()));    }    return ret;}
private static void beam_f15850_0(Schema.TypeName got, Schema.TypeName expected, String label)
{    checkArgument(got.equals(expected), "Can't convert '" + label + "' to " + got + ", expected: " + expected);}
protected Type beam_f15859_0(TypeDescriptor<?> type)
{    return byte[].class;}
protected Type beam_f15860_0(TypeDescriptor<?> type)
{    return byte[].class;}
protected StackManipulation beam_f15869_0(TypeDescriptor<?> type)
{        if (Instant.class.isAssignableFrom(type.getRawType())) {        return readValue;    }                                List<StackManipulation> stackManipulations = new ArrayList<>();        stackManipulations.add(TypeCreation.of(INSTANT_TYPE));    stackManipulations.add(Duplication.SINGLE);        if (ReadablePartial.class.isAssignableFrom(type.getRawType())) {                        stackManipulations.add(readValue);        stackManipulations.add(TypeCasting.to(READABLE_PARTIAL_TYPE));                stackManipulations.add(FieldAccess.forField(INSTANT_TYPE.getDeclaredFields().filter(ElementMatchers.named("EPOCH")).getOnly()).read());                stackManipulations.add(MethodInvocation.invoke(READABLE_PARTIAL_TYPE.getDeclaredMethods().filter(ElementMatchers.named("toDateTime").and(ElementMatchers.takesArguments(READABLE_INSTANT_TYPE))).getOnly()));    } else {                        stackManipulations.add(readValue);        stackManipulations.add(TypeCasting.to(READABLE_INSTANT_TYPE));    }        stackManipulations.add(MethodInvocation.invoke(READABLE_INSTANT_TYPE.getDeclaredMethods().filter(ElementMatchers.named("getMillis")).getOnly()));        stackManipulations.add(MethodInvocation.invoke(INSTANT_TYPE.getDeclaredMethods().filter(ElementMatchers.isConstructor().and(ElementMatchers.takesArguments(ForLoadedType.of(long.class)))).getOnly()));    return new StackManipulation.Compound(stackManipulations);}
protected StackManipulation beam_f15870_0(TypeDescriptor<?> type)
{        return new Compound(readValue, MethodInvocation.invoke(BYTE_BUFFER_TYPE.getDeclaredMethods().filter(ElementMatchers.named("array").and(ElementMatchers.returns(BYTE_ARRAY_TYPE))).getOnly()));}
protected StackManipulation beam_f15879_0(TypeDescriptor<?> type)
{                        ForLoadedType loadedType = new ForLoadedType(type.getRawType());    return new Compound(    TypeCreation.of(loadedType), Duplication.SINGLE,     readValue, TypeCasting.to(READABLE_INSTANT_TYPE),     MethodInvocation.invoke(READABLE_INSTANT_TYPE.getDeclaredMethods().filter(ElementMatchers.named("getMillis")).getOnly()),     MethodInvocation.invoke(loadedType.getDeclaredMethods().filter(ElementMatchers.isConstructor().and(ElementMatchers.takesArguments(ForLoadedType.of(long.class)))).getOnly()));}
protected StackManipulation beam_f15880_0(TypeDescriptor<?> type)
{        return new Compound(readValue, TypeCasting.to(BYTE_ARRAY_TYPE),     MethodInvocation.invoke(BYTE_BUFFER_TYPE.getDeclaredMethods().filter(ElementMatchers.named("wrap").and(ElementMatchers.takesArguments(BYTE_ARRAY_TYPE))).getOnly()));}
public InstrumentedType beam_f15889_0(InstrumentedType instrumentedType)
{    return instrumentedType;}
protected StackManipulation beam_f15890_0()
{    return MethodInvocation.invoke(new ForLoadedMethod(creator));}
 List<FieldValueTypeInformation> beam_f15899_0(Class<?> clazz, Schema schema)
{    return StaticSchemaInference.sortBySchema(get(clazz), schema);}
public static Schema beam_f15900_0(Class<?> clazz, FieldValueTypeSupplier fieldValueTypeSupplier)
{    return StaticSchemaInference.schemaFromClass(clazz, fieldValueTypeSupplier);}
public static SchemaUserTypeCreator beam_f15909_0(Class clazz, Constructor constructor, Schema schema, FieldValueTypeSupplier fieldValueTypeSupplier)
{    return CACHED_CREATORS.computeIfAbsent(new ClassWithSchema(clazz, schema), c -> {        List<FieldValueTypeInformation> types = fieldValueTypeSupplier.get(clazz, schema);        return createConstructorCreator(clazz, constructor, schema, types);    });}
public static SchemaUserTypeCreator beam_f15910_0(Class<T> clazz, Constructor<T> constructor, Schema schema, List<FieldValueTypeInformation> types)
{    try {        DynamicType.Builder<SchemaUserTypeCreator> builder = BYTE_BUDDY.with(new InjectPackageStrategy(clazz)).subclass(SchemaUserTypeCreator.class).method(ElementMatchers.named("create")).intercept(new ConstructorCreateInstruction(types, clazz, constructor));        return builder.make().load(ReflectHelpers.findClassLoader(clazz.getClassLoader()), ClassLoadingStrategy.Default.INJECTION).getLoaded().getDeclaredConstructor().newInstance();    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {        throw new RuntimeException("Unable to generate a creator for class " + clazz + " with schema " + schema);    }}
public static List<FieldValueGetter> beam_f15919_0(Class<?> clazz, Schema schema, FieldValueTypeSupplier fieldValueTypeSupplier)
{        return CACHED_GETTERS.computeIfAbsent(new ClassWithSchema(clazz, schema), c -> {        List<FieldValueTypeInformation> types = fieldValueTypeSupplier.get(clazz, schema);        List<FieldValueGetter> getters = types.stream().map(POJOUtils::createGetter).collect(Collectors.toList());        if (getters.size() != schema.getFieldCount()) {            throw new RuntimeException("Was not able to generate getters for schema: " + schema + " class: " + clazz);        }        return getters;    });}
public static SchemaUserTypeCreator beam_f15920_0(Class<T> clazz, Schema schema, FieldValueTypeSupplier fieldValueTypeSupplier)
{    return CACHED_CREATORS.computeIfAbsent(new ClassWithSchema(clazz, schema), c -> {        List<FieldValueTypeInformation> types = fieldValueTypeSupplier.get(clazz, schema);        return createSetFieldCreator(clazz, schema, types);    });}
private static FieldValueSetter<ObjectT, ValueT> beam_f15929_0(FieldValueTypeInformation typeInformation)
{    Field field = typeInformation.getField();    DynamicType.Builder<FieldValueSetter> builder = ByteBuddyUtils.subclassSetterInterface(BYTE_BUDDY, field.getDeclaringClass(), new ConvertType(false).convert(TypeDescriptor.of(field.getType())));    builder = implementSetterMethods(builder, field);    try {        return builder.make().load(ReflectHelpers.findClassLoader(field.getDeclaringClass().getClassLoader()), ClassLoadingStrategy.Default.INJECTION).getLoaded().getDeclaredConstructor().newInstance();    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {        throw new RuntimeException("Unable to generate a getter for field '" + field + "'.", e);    }}
private static DynamicType.Builder<FieldValueSetter> beam_f15930_0(DynamicType.Builder<FieldValueSetter> builder, Field field)
{    return builder.method(ElementMatchers.named("name")).intercept(FixedValue.reference(field.getName())).method(ElementMatchers.named("set")).intercept(new SetFieldInstruction(field));}
public static List<Method> beam_f15939_0(Class clazz)
{    return DECLARED_METHODS.computeIfAbsent(clazz, c -> {        return Arrays.stream(c.getDeclaredMethods()).filter(m -> !Modifier.isPrivate(m.getModifiers())).filter(m -> !Modifier.isProtected(m.getModifiers())).filter(m -> !Modifier.isStatic(m.getModifiers())).collect(Collectors.toList());    });}
public static Constructor beam_f15940_0(Class clazz)
{    return Arrays.stream(clazz.getDeclaredConstructors()).filter(m -> !Modifier.isPrivate(m.getModifiers())).filter(m -> !Modifier.isProtected(m.getModifiers())).filter(m -> m.getAnnotation(SchemaCreate.class) != null).findFirst().orElse(null);}
public Context beam_f15949_0(String part)
{    return create(ImmutableList.<String>builder().addAll(path()).add(part).build(), parent());}
public Context beam_f15950_0(TypeName parent)
{    return create(path(), Optional.of(parent));}
private static void beam_f15959_0(List<Qualifier> qualifiers, Object value, Row.Builder output, FieldAccessDescriptor fieldAccessDescriptor, FieldType inputType, FieldType outputType)
{    if (qualifiers.isEmpty()) {        Row row = (Row) value;        selectIntoRow(row, output, fieldAccessDescriptor);        return;    }            selectIntoRowWithQualifiers(qualifiers, 0, value, output, fieldAccessDescriptor, inputType, outputType);}
private static void beam_f15960_0(List<Qualifier> qualifiers, int qualifierPosition, Object value, Row.Builder output, FieldAccessDescriptor fieldAccessDescriptor, FieldType inputType, FieldType outputType)
{    if (qualifierPosition >= qualifiers.size()) {                Row row = (Row) value;        selectIntoRow(row, output, fieldAccessDescriptor);        return;    }    Qualifier qualifier = qualifiers.get(qualifierPosition);    switch(qualifier.getKind()) {        case LIST:            {                FieldType nestedInputType = checkNotNull(inputType.getCollectionElementType());                FieldType nestedOutputType = checkNotNull(outputType.getCollectionElementType());                List<Object> list = (List) value;                                                                                                Schema tempSchema = Schema.builder().addField("a", nestedInputType).build();                FieldAccessDescriptor tempAccessDescriptor = FieldAccessDescriptor.create().withNestedField("a", fieldAccessDescriptor).resolve(tempSchema);                                                Schema nestedSchema = getOutputSchema(tempSchema, tempAccessDescriptor);                List<List<Object>> selectedLists = Lists.newArrayListWithCapacity(nestedSchema.getFieldCount());                for (int i = 0; i < nestedSchema.getFieldCount(); i++) {                    selectedLists.add(Lists.newArrayListWithCapacity(list.size()));                }                for (Object o : list) {                    Row.Builder selectElementBuilder = Row.withSchema(nestedSchema);                    selectIntoRowWithQualifiers(qualifiers, qualifierPosition + 1, o, selectElementBuilder, fieldAccessDescriptor, nestedInputType, nestedOutputType);                    Row elementBeforeDistribution = selectElementBuilder.build();                    for (int i = 0; i < nestedSchema.getFieldCount(); ++i) {                        selectedLists.get(i).add(elementBeforeDistribution.getValue(i));                    }                }                for (List aList : selectedLists) {                    output.addValue(aList);                }                break;            }        case MAP:            {                FieldType nestedInputType = checkNotNull(inputType.getMapValueType());                FieldType nestedOutputType = checkNotNull(outputType.getMapValueType());                                                                Schema tempSchema = Schema.builder().addField("a", nestedInputType).build();                FieldAccessDescriptor tempAccessDescriptor = FieldAccessDescriptor.create().withNestedField("a", fieldAccessDescriptor).resolve(tempSchema);                Schema nestedSchema = getOutputSchema(tempSchema, tempAccessDescriptor);                List<Map> selectedMaps = Lists.newArrayListWithExpectedSize(nestedSchema.getFieldCount());                for (int i = 0; i < nestedSchema.getFieldCount(); ++i) {                    selectedMaps.add(Maps.newHashMap());                }                Map<Object, Object> map = (Map) value;                for (Map.Entry<Object, Object> entry : map.entrySet()) {                    Row.Builder selectValueBuilder = Row.withSchema(nestedSchema);                    selectIntoRowWithQualifiers(qualifiers, qualifierPosition + 1, entry.getValue(), selectValueBuilder, fieldAccessDescriptor, nestedInputType, nestedOutputType);                    Row valueBeforeDistribution = selectValueBuilder.build();                    for (int i = 0; i < nestedSchema.getFieldCount(); ++i) {                        selectedMaps.get(i).put(entry.getKey(), valueBeforeDistribution.getValue(i));                    }                }                for (Map aMap : selectedMaps) {                    output.addValue(aMap);                }                break;            }        default:            throw new RuntimeException("Unexpected type " + qualifier.getKind());    }}
public BoundedWindow beam_f15969_0()
{    throw new IllegalArgumentException("cannot call window() in a null context");}
public static StateContext<W> beam_f15970_0()
{    return (StateContext<W>) NULL_CONTEXT;}
public ResultT beam_f15979_0(Coder<?> elementCoder)
{    return dispatchDefault();}
public static StateSpec<ValueState<T>> beam_f15980_0()
{    return new ValueStateSpec<>(null);}
public static StateSpec<SetState<T>> beam_f15989_0(Coder<T> elemCoder)
{    return new SetStateSpec<>(elemCoder);}
public static StateSpec<MapState<K, V>> beam_f15990_0()
{    return new MapStateSpec<>(null, null);}
public void beam_f15999_0(Coder[] coders)
{    if (this.coder == null && coders[0] != null) {        this.coder = (Coder<T>) coders[0];    }}
public void beam_f16000_0()
{    if (coder == null) {        throw new IllegalStateException("Unable to infer a coder for ValueState and no Coder" + " was specified. Please set a coder by either invoking" + " StateSpecs.value(Coder<T> valueCoder) or by registering the coder in the" + " Pipeline's CoderRegistry.");    }}
private StateSpec<BagState<AccumT>> beam_f16009_0()
{    return new BagStateSpec<>(accumCoder);}
public CombiningState<InputT, AccumT, OutputT> beam_f16010_0(String id, StateBinder visitor)
{    return visitor.bindCombiningWithContext(id, this, accumCoder, combineFn);}
public void beam_f16019_0(Coder[] coders)
{    if (this.elemCoder == null && coders[0] != null) {        this.elemCoder = (Coder<T>) coders[0];    }}
public void beam_f16020_0()
{    if (elemCoder == null) {        throw new IllegalStateException("Unable to infer a coder for BagState and no Coder" + " was specified. Please set a coder by either invoking" + " StateSpecs.bag(Coder<T> elemCoder) or by registering the coder in the" + " Pipeline's CoderRegistry.");    }}
public SetState<T> beam_f16029_0(String id, StateBinder visitor)
{    return visitor.bindSet(id, this, elemCoder);}
public ResultT beam_f16030_0(Cases<ResultT> cases)
{    return cases.dispatchSet(elemCoder);}
public static TimerSpec beam_f16041_0(TimeDomain timeDomain)
{    return new AutoValue_TimerSpecs_SimpleTimerSpec(timeDomain);}
 static Predicate<Annotation> beam_f16042_0(final Class<? extends Annotation> clazz)
{    return annotation -> annotation.annotationType() != null && annotation.annotationType().equals(clazz);}
public static void beam_f16051_0(Coder<IterableT> coder, Coder.Context context, IterableT value) throws Exception
{    Iterable<T> result = decodeEncode(coder, context, value);        if (Iterables.isEmpty(value)) {        assertThat(result, emptyIterable());    } else {                assertThat(result, contains((T[]) Iterables.toArray(value, Object.class)));    }}
public static void beam_f16052_0(Coder<T> coder)
{    SerializableUtils.ensureSerializable(coder);}
public static void beam_f16061_0(Coder<T> coder, T value, String base64Encoding) throws Exception
{    assertThat(ENCODING_WIRE_FORMAT_MESSAGE, CoderUtils.encodeToBase64(coder, value), equalTo(base64Encoding));}
public static void beam_f16062_0(Coder<T> coder, List<T> values, List<String> base64Encodings) throws Exception
{    assertThat("List of base64 encodings has different size than List of values", base64Encodings.size(), equalTo(values.size()));    for (int i = 0; i < base64Encodings.size(); i++) {        coderEncodesBase64(coder, values.get(i), base64Encodings.get(i));    }}
public long beam_f16071_0()
{    return currentSum;}
public long beam_f16072_0()
{    return count;}
private static List<AccumT> beam_f16081_0(CombineFn<InputT, AccumT, OutputT> fn, Iterable<? extends Iterable<InputT>> shards)
{    List<AccumT> accumulators = new ArrayList<>();    int maybeCompact = 0;    for (Iterable<InputT> shard : shards) {        AccumT accumulator = fn.createAccumulator();        for (InputT elem : shard) {            accumulator = fn.addInput(accumulator, elem);        }        if (maybeCompact++ % 2 == 0) {            accumulator = fn.compact(accumulator);        }        accumulators.add(accumulator);    }    return accumulators;}
private static List<List<T>> beam_f16082_0(List<T> input, int numShards)
{    List<List<T>> shards = new ArrayList<>(numShards);    for (int i = 0; i < numShards; i++) {        shards.add(input.subList(i * input.size() / numShards, (i + 1) * input.size() / numShards));    }    return shards;}
public static GatherAllPanes<T> beam_f16091_0()
{    return new GatherAllPanes<>();}
public PCollection<Iterable<ValueInSingleWindow<T>>> beam_f16092_0(PCollection<T> input)
{    WindowFn<?, ?> originalWindowFn = input.getWindowingStrategy().getWindowFn();    return input.apply(Reify.windows()).apply(WithKeys.<Integer, ValueInSingleWindow<T>>of(0).withKeyType(new TypeDescriptor<Integer>() {    })).apply(Window.into(new IdentityWindowFn<KV<Integer, ValueInSingleWindow<T>>>(originalWindowFn.windowCoder())).triggering(Never.ever()).withAllowedLateness(input.getWindowingStrategy().getAllowedLateness()).discardingFiredPanes()).apply(GroupByKey.create()).apply(Values.create()).setWindowingStrategyInternal(input.getWindowingStrategy());}
 static SimpleFunction<Iterable<ValueInSingleWindow<T>>, Iterable<T>> beam_f16101_0()
{    return new ExtractAllPanes<>();}
public Iterable<T> beam_f16102_0(Iterable<ValueInSingleWindow<T>> input)
{    List<T> outputs = new ArrayList<>();    for (ValueInSingleWindow<T> value : input) {        if (!value.getPane().isFirst() || !value.getPane().isLast()) {            throw site.wrap(String.format("Expected elements to be produced by a trigger that fires at most once, but got " + "a value %s in a pane that is %s.", value, value.getPane().isFirst() ? "not the last pane" : "not the first pane"));        }        outputs.add(value.getValue());    }    return outputs;}
public PCollection<Void> beam_f16111_0(PCollection<SuccessOrFailure> input)
{    return input.apply(ParDo.of(new DefaultConcludeFn()));}
 static PAssertionSite beam_f16112_0(String message)
{    return new PAssertionSite(message, new Throwable().getStackTrace());}
public static SingletonAssert<T> beam_f16121_0(PCollection<T> actual)
{    return thatSingleton(actual.getName(), actual);}
public static SingletonAssert<T> beam_f16122_0(String reason, PCollection<T> actual)
{    return new PCollectionSingletonAssert<>(actual, PAssertionSite.capture(reason));}
public PCollectionContentsAssert<T> beam_f16131_0(BoundedWindow window)
{    return withPane(window, PaneExtractors.latePanes());}
public PCollectionContentsAssert<T> beam_f16132_0(BoundedWindow window)
{    return withPane(window, PaneExtractors.nonLatePanes());}
 PCollectionContentsAssert<T> beam_f16141_0(final SerializableMatcher<Iterable<? extends T>> matcher)
{            @SuppressWarnings({ "rawtypes", "unchecked" })    SerializableFunction<Iterable<T>, Void> checkerFn = (SerializableFunction) new MatcherCheckerFn<>(matcher);    actual.apply("PAssert$" + (assertCount++), new GroupThenAssert<>(checkerFn, rewindowingStrategy, paneExtractor, site));    return this;}
public Void beam_f16142_0(T actual)
{    assertThat(actual, matcher);    return null;}
public IterableAssert<T> beam_f16151_0()
{    return withPanes(GlobalWindow.INSTANCE, PaneExtractors.earlyPanes());}
private PCollectionSingletonIterableAssert<T> beam_f16152_0(BoundedWindow window, SimpleFunction<Iterable<ValueInSingleWindow<Iterable<T>>>, Iterable<Iterable<T>>> paneExtractor)
{    @SuppressWarnings({ "unchecked", "rawtypes" })    Coder<BoundedWindow> windowCoder = (Coder) actual.getWindowingStrategy().getWindowFn().windowCoder();    return new PCollectionSingletonIterableAssert<>(actual, IntoStaticWindows.of(windowCoder, window), paneExtractor, site);}
public PCollectionSingletonAssert<T> beam_f16161_0(BoundedWindow window)
{    return withPanes(window, PaneExtractors.latePanes());}
public SingletonAssert<T> beam_f16162_0(T expected)
{    return satisfies(new AssertIsEqualToRelation<>(), expected);}
public PCollectionViewAssert<ElemT, ViewT> beam_f16171_0(BoundedWindow window)
{    return inPane(window, PaneExtractors.finalPane());}
public PCollectionViewAssert<ElemT, ViewT> beam_f16172_0(BoundedWindow window)
{    return inPane(window, PaneExtractors.onTimePane());}
public int beam_f16181_0()
{    throw new UnsupportedOperationException(String.format("%s.hashCode() is not supported.", SingletonAssert.class.getSimpleName()));}
public static CreateActual<T, ActualT> beam_f16182_0(PCollection<T> actual, AssertionWindows rewindowActuals, SimpleFunction<Iterable<ValueInSingleWindow<T>>, Iterable<T>> extractPane, PTransform<PCollection<T>, PCollectionView<ActualT>> actualView)
{    return new CreateActual<>(actual, rewindowActuals, extractPane, actualView);}
public void beam_f16191_0(ProcessContext c)
{    ActualT actualContents = c.sideInput(actual);    c.output(doChecks(site, actualContents, checkerFn));}
public void beam_f16192_0(ProcessContext c)
{    c.output(doChecks(site, c.element(), checkerFn));}
public static AssertionWindows beam_f16201_0()
{    return new IntoGlobalWindow();}
private PTransform<PCollection<T>, PCollection<T>> beam_f16202_0()
{    return Window.into(new GlobalWindows());}
public void beam_f16211_0(ProcessContext c, BoundedWindow window) throws Exception
{    if (windows.getWindows().contains(window)) {        c.output(c.element());    }}
public static int beam_f16212_0(Pipeline pipeline)
{    AssertionCountingVisitor visitor = new AssertionCountingVisitor();    pipeline.traverseTopologically(visitor);    return visitor.getPAssertCount();}
public static SerializableMatcher<T> beam_f16221_0(final SerializableMatcher<T>... matchers)
{    return fromSupplier(() -> Matchers.allOf(matchers));}
public static SerializableMatcher<T> beam_f16222_0(Iterable<SerializableMatcher<? super T>> serializableMatchers)
{        @SuppressWarnings({ "rawtypes", "unchecked" })    final Iterable<Matcher<? super T>> matchers = (Iterable) serializableMatchers;    return fromSupplier(() -> Matchers.anyOf(matchers));}
public static SerializableMatcher<T[]> beam_f16231_0(final SerializableMatcher<? super T>... matchers)
{    return fromSupplier(() -> Matchers.arrayContainingInAnyOrder(matchers));}
public static SerializableMatcher<T[]> beam_f16232_0(Collection<SerializableMatcher<? super T>> serializableMatchers)
{        @SuppressWarnings({ "rawtypes", "unchecked" })    final Collection<Matcher<? super T>> matchers = (Collection) serializableMatchers;    return fromSupplier(() -> Matchers.arrayContainingInAnyOrder(matchers));}
public static SerializableMatcher<Iterable<? extends T>> beam_f16241_0(Coder<T> coder, T... items)
{    final SerializableSupplier<T[]> itemsSupplier = new SerializableArrayViaCoder<>(coder, items);    return fromSupplier(() -> Matchers.containsInAnyOrder(itemsSupplier.get()));}
public static SerializableMatcher<Iterable<? extends T>> beam_f16242_0(final SerializableMatcher<? super T>... matchers)
{    return fromSupplier(() -> Matchers.containsInAnyOrder(matchers));}
public static SerializableMatcher<T> beam_f16251_0(final T target)
{    return fromSupplier(() -> Matchers.greaterThan(target));}
public static SerializableMatcher<T> beam_f16252_0(final Coder<T> coder, T target)
{    final SerializableSupplier<T> targetSupplier = new SerializableViaCoder<>(coder, target);    return fromSupplier(() -> Matchers.greaterThan(targetSupplier.get()));}
public static SerializableMatcher<Iterable<T>> beam_f16261_0(final SerializableMatcher<? super Integer> sizeMatcher)
{    return fromSupplier(() -> Matchers.iterableWithSize(sizeMatcher));}
public static SerializableMatcher<T> beam_f16262_0(final Collection<T> collection)
{    return fromSupplier(() -> is(in(collection)));}
public static SerializableMatcher<KV<? extends K, ? extends V>> beam_f16271_0(V value)
{    return new KvValueMatcher<>(equalTo(value));}
public static SerializableMatcher<KV<? extends K, ? extends V>> beam_f16272_0(Coder<V> coder, V value)
{    return new KvValueMatcher<>(equalTo(coder, value));}
public static SerializableMatcher<String> beam_f16281_0(final String substring)
{    return fromSupplier(() -> Matchers.startsWith(substring));}
public boolean beam_f16282_0(Object item)
{    @SuppressWarnings("unchecked")    KV<K, ?> kvItem = (KV<K, ?>) item;    return keyMatcher.matches(kvItem.getKey());}
public void beam_f16291_0(Description description)
{    supplier.get().describeTo(description);}
public boolean beam_f16292_0(Object item)
{    return supplier.get().matches(item);}
public static List<T> beam_f16301_0(BoundedSource<T> source, long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{    List<T> res = Lists.newArrayList();    for (BoundedSource<T> split : source.split(desiredBundleSizeBytes, options)) {        res.addAll(readFromSource(split, options));    }    return res;}
public static List<T> beam_f16302_0(Source.Reader<T> reader) throws IOException
{    return readRemainingFromReader(reader, false);}
private static void beam_f16311_0(String message, String expectedLabel, List<T> expected, String actualLabel, List<T> actual)
{    int i = 0;    for (; i < expected.size() && i < actual.size(); ++i) {        if (!Objects.equals(expected.get(i), actual.get(i))) {            Assert.fail(String.format("%s: %s and %s have %d items in common and then differ. " + "Item in %s (%d more): %s, item in %s (%d more): %s", message, expectedLabel, actualLabel, i, expectedLabel, expected.size() - i - 1, expected.get(i), actualLabel, actual.size() - i - 1, actual.get(i)));        }    }    if (i < expected.size()) /* but i == actual.size() */    {        Assert.fail(String.format("%s: %s has %d more items after matching all %d from %s. First 5: %s", message, expectedLabel, expected.size() - actual.size(), actual.size(), actualLabel, expected.subList(actual.size(), Math.min(expected.size(), actual.size() + 5))));    } else if (i < actual.size()) /* but i == expected.size() */    {        Assert.fail(String.format("%s: %s has %d more items after matching all %d from %s. First 5: %s", message, actualLabel, actual.size() - expected.size(), expected.size(), expectedLabel, actual.subList(expected.size(), Math.min(actual.size(), expected.size() + 5))));    } else {        }}
private static SourceTestUtils.SplitAtFractionResult beam_f16312_0(BoundedSource<T> source, List<T> expectedItems, int numItemsToReadBeforeSplit, double splitFraction, ExpectedSplitOutcome expectedOutcome, PipelineOptions options) throws Exception
{    try (BoundedSource.BoundedReader<T> reader = source.createReader(options)) {        BoundedSource<T> originalSource = reader.getCurrentSource();        List<T> currentItems = readNItemsFromUnstartedReader(reader, numItemsToReadBeforeSplit);        BoundedSource<T> residual = reader.splitAtFraction(splitFraction);        if (residual != null) {            assertFalse(String.format("Primary source didn't change after a successful split of %s at %f " + "after reading %d items. " + "Was the source object mutated instead of creating a new one? " + "Source objects MUST be immutable.", source, splitFraction, numItemsToReadBeforeSplit), reader.getCurrentSource() == originalSource);            assertFalse(String.format("Residual source equal to original source after a successful split of %s at %f " + "after reading %d items. " + "Was the source object mutated instead of creating a new one? " + "Source objects MUST be immutable.", source, splitFraction, numItemsToReadBeforeSplit), reader.getCurrentSource() == residual);        }                switch(expectedOutcome) {            case MUST_SUCCEED_AND_BE_CONSISTENT:                assertNotNull("Failed to split reader of source: " + source + " at " + splitFraction + " after reading " + numItemsToReadBeforeSplit + " items", residual);                break;            case MUST_FAIL:                assertEquals(null, residual);                break;            case MUST_BE_CONSISTENT_IF_SUCCEEDS:                                break;        }        currentItems.addAll(readRemainingFromReader(reader, numItemsToReadBeforeSplit > 0));        BoundedSource<T> primary = reader.getCurrentSource();        return verifySingleSplitAtFractionResult(source, expectedItems, currentItems, primary, residual, numItemsToReadBeforeSplit, splitFraction, options);    }}
public List<? extends BoundedSource<T>> beam_f16321_0(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{    return ImmutableList.of(this);}
public long beam_f16322_0(PipelineOptions options) throws Exception
{    return boundedSource.getEstimatedSizeBytes(options);}
public BoundedSource<T> beam_f16331_0(double fraction)
{    return null;}
public Double beam_f16332_0()
{    return boundedReader.getFractionConsumed();}
public boolean beam_f16341_0(WindowFn<?, ?> other)
{    if (!(other instanceof StaticWindows)) {        return false;    }    StaticWindows that = (StaticWindows) other;    return Objects.equals(this.windows.get(), that.windows.get());}
public void beam_f16342_0(WindowFn<?, ?> other) throws IncompatibleWindowException
{    if (!this.isCompatible(other)) {        throw new IncompatibleWindowException(other, String.format("Only %s objects with the same window supplier are compatible.", StaticWindows.class.getSimpleName()));    }}
public String beam_f16351_0()
{    return MoreObjects.toStringHelper(this).add("isSuccess", isSuccess()).addValue(throwable).omitNullValues().toString();}
public boolean beam_f16352_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    SuccessOrFailure that = (SuccessOrFailure) o;    return isSuccess == that.isSuccess && Objects.equal(site, that.site) && Objects.equal(throwable, that.throwable);}
private void beam_f16362_0()
{    if (!isEmptyPipeline(pipeline)) {        if (!runAttempted && !enableAutoRunIfMissing) {            throw new PipelineRunMissingException("The pipeline has not been run.");        } else {            final List<TransformHierarchy.Node> pipelineNodes = recordPipelineNodes(pipeline);            if (pipelineRunSucceeded() && !visitedAll(pipelineNodes)) {                final boolean hasDanglingPAssert = FluentIterable.from(pipelineNodes).filter(Predicates.not(Predicates.in(runVisitedNodes))).anyMatch(isPAssertNode);                if (hasDanglingPAssert) {                    throw new AbandonedNodeException("The pipeline contains abandoned PAssert(s).");                } else {                    throw new AbandonedNodeException("The pipeline contains abandoned PTransform(s).");                }            }        }    }}
private boolean beam_f16363_0(final List<TransformHierarchy.Node> pipelineNodes)
{    return runVisitedNodes.equals(pipelineNodes);}
public void beam_f16372_0() throws Throwable
{    options.as(ApplicationNameOptions.class).setAppName(getAppName(description));    setDeducedEnforcementLevel();                                    statement.evaluate();    enforcement.get().afterUserCodeFinished();}
public PipelineResult beam_f16373_0()
{    return run(getOptions());}
public static void beam_f16382_0(Pipeline pipeline, PipelineResult pipelineResult)
{    if (MetricsEnvironment.isMetricsSupported()) {        long expectedNumberOfAssertions = (long) PAssert.countAsserts(pipeline);        long successfulAssertions = 0;        Iterable<MetricResult<Long>> successCounterResults = pipelineResult.metrics().queryMetrics(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(PAssert.class, PAssert.SUCCESS_COUNTER)).build()).getCounters();        for (MetricResult<Long> counter : successCounterResults) {            if (counter.getAttempted() > 0) {                successfulAssertions++;            }        }        assertThat(String.format("Expected %d successful assertions, but found %d.", expectedNumberOfAssertions, successfulAssertions), successfulAssertions, is(expectedNumberOfAssertions));    }}
public boolean beam_f16383_0()
{    return empty;}
public Builder<T> beam_f16393_0(Instant newWatermark)
{    checkArgument(!newWatermark.isBefore(currentWatermark), "The watermark must monotonically advance");    checkArgument(newWatermark.isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE), "The Watermark cannot progress beyond the maximum. Got: %s. Maximum: %s", newWatermark, BoundedWindow.TIMESTAMP_MAX_VALUE);    ImmutableList<Event<T>> newEvents = ImmutableList.<Event<T>>builder().addAll(events).add(WatermarkEvent.advanceTo(newWatermark)).build();    return new Builder<>(coder, newEvents, newWatermark);}
public Builder<T> beam_f16394_0(Duration amount)
{    checkArgument(amount.getMillis() > 0, "Must advance the processing time by a positive amount. Got: ", amount);    ImmutableList<Event<T>> newEvents = ImmutableList.<Event<T>>builder().addAll(events).add(ProcessingTimeEvent.advanceBy(amount)).build();    return new Builder<>(coder, newEvents, currentWatermark);}
public static TestStream<T> beam_f16403_0(Coder<T> coder, List<Event<T>> events)
{    return new TestStream<>(coder, events);}
public boolean beam_f16404_0(Object other)
{    if (!(other instanceof TestStream)) {        return false;    }    TestStream<?> that = (TestStream<?>) other;    return getValueCoder().equals(that.getValueCoder()) && getEvents().equals(that.getEvents());}
public static Collection<W> beam_f16414_0(WindowFn<T, W> windowFn, long timestamp) throws Exception
{    return assignedWindowsWithValue(windowFn, TimestampedValue.of((T) null, new Instant(timestamp)));}
public static Collection<W> beam_f16415_0(WindowFn<T, W> windowFn, TimestampedValue<T> timestampedValue) throws Exception
{    TestAssignContext<T, W> assignContext = new TestAssignContext<>(timestampedValue, windowFn);    return windowFn.assignWindows(assignContext);}
public Collection<W> beam_f16424_0()
{    return elements.keySet();}
public Set<V> beam_f16425_0(W window)
{    return elements.get(window);}
private static Instant beam_f16434_0(TimestampCombiner timestampCombiner, Instant inputTimestamp, BoundedWindow window)
{    switch(timestampCombiner) {        case EARLIEST:        case LATEST:            return inputTimestamp;        case END_OF_WINDOW:            return window.maxTimestamp();        default:            throw new IllegalArgumentException(String.format("Unknown %s: %s", TimestampCombiner.class, timestampCombiner));    }}
private static Instant beam_f16435_0(TimestampCombiner timestampCombiner, Iterable<Instant> outputInstants)
{    checkArgument(!Iterables.isEmpty(outputInstants), "Cannot combine zero instants with %s", timestampCombiner);    switch(timestampCombiner) {        case EARLIEST:            return Ordering.natural().min(outputInstants);        case LATEST:            return Ordering.natural().max(outputInstants);        case END_OF_WINDOW:            return outputInstants.iterator().next();        default:            throw new IllegalArgumentException(String.format("Unknown %s: %s", TimestampCombiner.class, timestampCombiner));    }}
public static ApproximateQuantilesCombineFn<T, Top.Natural<T>> beam_f16444_0(int numQuantiles)
{    return create(numQuantiles, new Top.Natural<T>());}
public ApproximateQuantilesCombineFn<T, ComparatorT> beam_f16445_0(double epsilon)
{    return create(numQuantiles, compareFn, maxNumElements, epsilon);}
public static QuantileState<T, ComparatorT> beam_f16454_0(ComparatorT compareFn, int numQuantiles, T elem, int numBuffers, int bufferSize)
{    return new QuantileState<>(compareFn, numQuantiles, elem, /* min */    elem, /* max */    numBuffers, bufferSize, Collections.singletonList(elem), Collections.emptyList());}
public void beam_f16455_0(T elem)
{    if (isEmpty()) {        min = max = elem;    } else if (compareFn.compare(elem, min) < 0) {        min = elem;    } else if (compareFn.compare(elem, max) > 0) {        max = elem;    }    addUnbuffered(elem);}
public String beam_f16464_0()
{    return "QuantileBuffer[" + "level=" + level + ", weight=" + weight + ", elements=" + elements + "]";}
public Iterator<WeightedValue<T>> beam_f16465_0()
{    return new UnmodifiableIterator<WeightedValue<T>>() {        Iterator<T> iter = elements.iterator();        @Override        public boolean hasNext() {            return iter.hasNext();        }        @Override        public WeightedValue<T> next() {            return WeightedValue.of(iter.next(), weight);        }    };}
public int beam_f16474_0()
{    return Objects.hash(elementCoder, compareFn);}
public void beam_f16475_0() throws NonDeterministicException
{    verifyDeterministic(this, "QuantileState.ElementCoder must be deterministic", elementCoder);    verifyDeterministic(this, "QuantileState.ElementListCoder must be deterministic", elementListCoder);}
public boolean beam_f16484_0(long value)
{    if (heap.size() >= sampleSize && value < minHash) {                return false;    }    if (heap.add(value)) {        if (heap.size() > sampleSize) {            heap.remove(minHash);            minHash = heap.first();        } else if (value < minHash) {            minHash = value;        }    }    return true;}
 long beam_f16485_0()
{    if (heap.size() < sampleSize) {        return (long) heap.size();    } else {        double sampleSpaceSize = Long.MAX_VALUE - (double) minHash;                                                        double estimate = Math.log1p(-sampleSize / sampleSpaceSize) / Math.log1p(-1 / sampleSpaceSize) * HASH_SPACE_SIZE / sampleSpaceSize;        return Math.round(estimate);    }}
 static long beam_f16494_0(double estimationError)
{    return Math.round(Math.ceil(4.0 / Math.pow(estimationError, 2.0)));}
private static void beam_f16495_0(DisplayData.Builder builder, long sampleSize, @Nullable Double maxEstimationError)
{    builder.add(DisplayData.item("sampleSize", sampleSize).withLabel("Sample Size")).addIfNotNull(DisplayData.item("maximumEstimationError", maxEstimationError).withLabel("Maximum Estimation Error"));}
private static PerKey<K, InputT, OutputT> beam_f16504_0(GlobalCombineFn<? super InputT, ?, OutputT> fn, DisplayData.ItemSpec<? extends Class<?>> fnDisplayData)
{    return new PerKey<>(fn, fnDisplayData, false);}
private static PerKey<K, InputT, OutputT> beam_f16505_0(GlobalCombineFn<? super InputT, ?, OutputT> fn, DisplayData.ItemSpec<? extends Class<?>> fnDisplayData)
{    return new PerKey<>(fn, fnDisplayData, true);}
public TypeDescriptor<InputT> beam_f16514_0()
{    return new TypeDescriptor<InputT>(getClass()) {    };}
public static BinaryCombineFn<V> beam_f16515_0(SerializableBiFunction<V, V, V> combiner)
{    return new BinaryCombineFn<V>() {        @Override        public V apply(V left, V right) {            return combiner.apply(left, right);        }    };}
private void beam_f16524_0(V value)
{    this.present = true;    this.value = value;}
public String beam_f16525_0()
{    return "Combine.Holder(value=" + value + ", present=" + present + ")";}
public int[] beam_f16534_0(Iterable<int[]> accumulators)
{    Iterator<int[]> iter = accumulators.iterator();    if (!iter.hasNext()) {        return createAccumulator();    } else {        int[] running = iter.next();        while (iter.hasNext()) {            running[0] = apply(running[0], iter.next()[0]);        }        return running;    }}
public Integer beam_f16535_0(int[] accumulator)
{    return accumulator[0];}
public int beam_f16544_0()
{    return this.getClass().hashCode();}
public long[] beam_f16545_0()
{    return wrap(identity());}
public int beam_f16554_0()
{    return this.getClass().hashCode();}
public long[] beam_f16555_0(Long value)
{    return wrap(value);}
private static double[] beam_f16564_0(double value)
{    return new double[] { value };}
public Double beam_f16565_0(double[] accumulator)
{    return accumulator[0];}
protected String beam_f16574_0()
{    return String.format("Combine.globally(%s)", NameUtils.approximateSimpleName(fn));}
public GloballyAsSingletonView<InputT, OutputT> beam_f16575_0()
{    return new GloballyAsSingletonView<>(fn, fnDisplayData, insertDefault, fanout);}
public PCollection<OutputT> beam_f16584_0(PCollection<InputT> input)
{    PCollection<KV<Void, InputT>> withKeys = input.apply(WithKeys.of((Void) null)).setCoder(KvCoder.of(VoidCoder.of(), input.getCoder()));    Combine.PerKey<Void, InputT, OutputT> combine = Combine.fewKeys(fn, fnDisplayData);    if (!sideInputs.isEmpty()) {        combine = combine.withSideInputs(sideInputs);    }    PCollection<KV<Void, OutputT>> combined;    if (fanout >= 2) {        combined = withKeys.apply(combine.withHotKeyFanout(fanout));    } else {        combined = withKeys.apply(combine);    }    PCollection<OutputT> output = combined.apply(Values.create());    if (insertDefault) {        if (!output.getWindowingStrategy().getWindowFn().isCompatible(new GlobalWindows())) {            throw new IllegalStateException(fn.getIncompatibleGlobalWindowErrorMessage());        }        return insertDefaultValueIfEmpty(output);    } else {        return output;    }}
public void beam_f16585_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    Combine.populateDisplayData(builder, fn, fnDisplayData);    Combine.populateGlobalDisplayData(builder, fanout, insertDefault);}
public void beam_f16594_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    Combine.populateDisplayData(builder, fn, fnDisplayData);    Combine.populateGlobalDisplayData(builder, fanout, insertDefault);}
public static IterableCombineFn<V> beam_f16595_0(SerializableFunction<Iterable<V>, V> combiner)
{    return of(combiner, DEFAULT_BUFFER_SIZE);}
public String beam_f16604_0()
{    return NameUtils.approximateSimpleName(combiner);}
public static SimpleCombineFn<V> beam_f16605_0(SerializableFunction<Iterable<V>, V> combiner)
{    return new SimpleCombineFn<>(combiner);}
public List<PCollectionView<?>> beam_f16614_0()
{    return sideInputs;}
public Map<TupleTag<?>, PValue> beam_f16615_0()
{    return PCollectionViews.toAdditionalInputs(sideInputs);}
public AccumT beam_f16624_0(AccumT accumulator)
{    return fn.compact(accumulator);}
public AccumT beam_f16625_0(AccumT accumulator)
{    return accumulator;}
public Coder<AccumT> beam_f16634_0(CoderRegistry registry, Coder<InputOrAccum<InputT, AccumT>> inputCoder) throws CannotProvideCoderException
{    return accumCoder;}
public void beam_f16635_0(DisplayData.Builder builder)
{    builder.delegate(PerKeyWithHotKeyFanout.this);}
public AccumT beam_f16644_0(AccumT accumulator, InputOrAccum<InputT, AccumT> value, Context c)
{    if (value.accum == null) {        return fnWithContext.addInput(accumulator, value.input, c);    } else {        return fnWithContext.mergeAccumulators(ImmutableList.of(accumulator, value.accum), c);    }}
public AccumT beam_f16645_0(Iterable<AccumT> accumulators, Context c)
{    return fnWithContext.mergeAccumulators(accumulators, c);}
public KV<K, InputOrAccum<InputT, AccumT>> beam_f16654_0(KV<K, InputT> element)
{    return KV.of(element.getKey(), InputOrAccum.<InputT, AccumT>input(element.getValue()));}
public void beam_f16655_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    Combine.populateDisplayData(builder, fn, fnDisplayData);    if (hotKeyFanout instanceof HasDisplayData) {        builder.include("hotKeyFanout", (HasDisplayData) hotKeyFanout);    }    builder.add(DisplayData.item("fanoutFn", hotKeyFanout.getClass()).withLabel("Fanout Function"));}
public GroupedValues<K, InputT, OutputT> beam_f16664_0(PCollectionView<?>... sideInputs)
{    return withSideInputs(Arrays.asList(sideInputs));}
public GroupedValues<K, InputT, OutputT> beam_f16665_0(Iterable<? extends PCollectionView<?>> sideInputs)
{    return new GroupedValues<>(fn, fnDisplayData, ImmutableList.copyOf(sideInputs));}
private KvCoder<K, InputT> beam_f16674_0(Coder<? extends KV<K, ? extends Iterable<InputT>>> inputCoder)
{    if (!(inputCoder instanceof KvCoder)) {        throw new IllegalStateException("Combine.GroupedValues requires its input to use KvCoder");    }    @SuppressWarnings({ "unchecked", "rawtypes" })    KvCoder<K, ? extends Iterable<InputT>> kvCoder = (KvCoder) inputCoder;    Coder<K> keyCoder = kvCoder.getKeyCoder();    Coder<? extends Iterable<InputT>> kvValueCoder = kvCoder.getValueCoder();    if (!(kvValueCoder instanceof IterableCoder)) {        throw new IllegalStateException("Combine.GroupedValues requires its input values to use " + "IterableCoder");    }    @SuppressWarnings("unchecked")    IterableCoder<InputT> inputValuesCoder = (IterableCoder<InputT>) kvValueCoder;    Coder<InputT> inputValueCoder = inputValuesCoder.getElemCoder();    return KvCoder.of(keyCoder, inputValueCoder);}
public void beam_f16675_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    Combine.populateDisplayData(builder, fn, fnDisplayData);}
public ComposedCombineFn<DataT> beam_f16685_0(SimpleFunction<DataT, InputT> extractInputFn, Coder combineInputCoder, CombineFn<InputT, ?, OutputT> combineFn, TupleTag<OutputT> outputTag)
{    return new ComposedCombineFn<DataT>().with(extractInputFn, combineInputCoder, combineFn, outputTag);}
public ComposedCombineFnWithContext<DataT> beam_f16686_0(SimpleFunction<DataT, InputT> extractInputFn, CombineFnWithContext<InputT, ?, OutputT> combineFnWithContext, TupleTag<OutputT> outputTag)
{    return new ComposedCombineFnWithContext<DataT>().with(extractInputFn, combineFnWithContext, outputTag);}
public Object[] beam_f16695_0()
{    Object[] accumsArray = new Object[combineFnCount];    for (int i = 0; i < combineFnCount; ++i) {        accumsArray[i] = combineFns.get(i).createAccumulator();    }    return accumsArray;}
public Object[] beam_f16696_0(Object[] accumulator, DataT value)
{    for (int i = 0; i < combineFnCount; ++i) {        Object input = extractInputFns.get(i).apply(value);        accumulator[i] = combineFns.get(i).addInput(accumulator[i], input);    }    return accumulator;}
public Object[] beam_f16705_0(Object[] accumulator, DataT value, Context c)
{    for (int i = 0; i < combineFnCount; ++i) {        Object input = extractInputFns.get(i).apply(value);        accumulator[i] = combineFnWithContexts.get(i).addInput(accumulator[i], input, c);    }    return accumulator;}
public Object[] beam_f16706_0(Iterable<Object[]> accumulators, Context c)
{    Iterator<Object[]> iter = accumulators.iterator();    if (!iter.hasNext()) {        return createAccumulator(c);    } else {                                Object[] accum = iter.next();        for (int i = 0; i < combineFnCount; ++i) {            accum[i] = combineFnWithContexts.get(i).mergeAccumulators(new ProjectionIterable(accumulators, i), c);        }        return accum;    }}
public void beam_f16715_0(Object[] value, OutputStream outStream) throws CoderException, IOException
{    encode(value, outStream, Context.NESTED);}
public void beam_f16716_0(Object[] value, OutputStream outStream, Context context) throws CoderException, IOException
{    checkArgument(value.length == codersCount);    if (value.length == 0) {        return;    }    int lastIndex = codersCount - 1;    for (int i = 0; i < lastIndex; ++i) {        coders.get(i).encode(value[i], outStream);    }    coders.get(lastIndex).encode(value[lastIndex], outStream, context);}
public OutputT beam_f16725_0()
{    throw new UnsupportedOperationException("Override this function to provide the default value.");}
public ClosureT beam_f16726_0()
{    return closure;}
public static Contextful<Fn<InputT, OutputT>> beam_f16735_0(final Fn<InputT, OutputT> fn, Requirements requirements)
{    return of(fn, requirements);}
public static CombineFn<T, ?, Long> beam_f16736_0()
{    return new CountFn<>();}
public Long beam_f16745_0(long[] accumulator)
{    return accumulator[0];}
public Coder<long[]> beam_f16746_0(CoderRegistry registry, Coder<T> inputCoder)
{    return new AtomicCoder<long[]>() {        @Override        public void encode(long[] value, OutputStream outStream) throws IOException {            VarInt.encode(value[0], outStream);        }        @Override        public long[] decode(InputStream inStream) throws IOException, CoderException {            try {                return new long[] { VarInt.decodeLong(inStream) };            } catch (EOFException | UTFDataFormatException exn) {                throw new CoderException(exn);            }        }        @Override        public boolean isRegisterByteSizeObserverCheap(long[] value) {            return true;        }        @Override        protected long getEncodedElementByteSize(long[] value) {            return VarInt.getLength(value[0]);        }    };}
public static Values<T> beam_f16755_0(@Nullable T elem, @Nullable T... elems)
{        List<T> input = new ArrayList<>(elems.length + 1);    input.add(elem);    input.addAll(Arrays.asList(elems));    return of(input);}
public static Values<Row> beam_f16756_0(Schema schema)
{    return new Values<Row>(new ArrayList<>(), Optional.of(SchemaCoder.of(schema, SerializableFunctions.identity(), SerializableFunctions.identity())), Optional.absent());}
public Values<T> beam_f16765_0(Schema schema, SerializableFunction<T, Row> toRowFunction, SerializableFunction<Row, T> fromRowFunction)
{    return withCoder(SchemaCoder.of(schema, toRowFunction, fromRowFunction));}
public Values<T> beam_f16766_0(Schema schema)
{    return withCoder(SchemaCoder.of(schema, (SerializableFunction<T, Row>) SerializableFunctions.<Row>identity(), (SerializableFunction<Row, T>) SerializableFunctions.<Row>identity()));}
public OffsetBasedSource<T> beam_f16775_0(long start, long end)
{    List<byte[]> primaryElems = allElementsBytes.subList((int) start, (int) end);    long primarySizeEstimate = (long) (totalSize * primaryElems.size() / (double) allElementsBytes.size());    return new CreateSource<>(primaryElems, primarySizeEstimate, coder);}
public long beam_f16776_0()
{    if (allElementsBytes.isEmpty()) {        return 1L;    }    return Math.max(1, totalSize / allElementsBytes.size());}
public TimestampedValues<T> beam_f16786_0(Schema schema, SerializableFunction<T, Row> toRowFunction, SerializableFunction<Row, T> fromRowFunction)
{    return withCoder(SchemaCoder.of(schema, toRowFunction, fromRowFunction));}
public TimestampedValues<T> beam_f16787_0(TypeDescriptor<T> type)
{    return new TimestampedValues<>(timestampedElements, elementCoder, Optional.of(type));}
public Collection<Item> beam_f16796_0()
{    return entries.values();}
public Map<Identifier, Item> beam_f16797_0()
{    return entries;}
public ItemSpec<T> beam_f16806_0(@Nullable String url)
{    return toBuilder().setLinkUrl(url).build();}
private ItemSpec<T> beam_f16807_0(T value)
{    return toBuilder().setRawValue(value).build();}
public Path beam_f16816_0(String path)
{    validatePathElement(path);    return new Path(ImmutableList.<String>builder().addAll(components.iterator()).add(path).build());}
private static void beam_f16817_0(String path)
{    checkNotNull(path);    checkArgument(!"".equals(path), "path cannot be empty");}
 FormattedItemValue beam_f16826_0(Object value)
{    return new FormattedItemValue(checkType(value, Number.class, FLOAT));}
 FormattedItemValue beam_f16827_0(Object value)
{    return new FormattedItemValue(checkType(value, Boolean.class, BOOLEAN));}
public Builder beam_f16836_0(ItemSpec<?> item)
{    checkNotNull(item, "Input display item cannot be null");    return addItemIf(true, item);}
public Builder beam_f16837_0(ItemSpec<?> item)
{    checkNotNull(item, "Input display item cannot be null");    return addItemIf(item.getValue() != null, item);}
public static ItemSpec<Double> beam_f16846_0(String key, @Nullable Double value)
{    return item(key, Type.FLOAT, value);}
public static ItemSpec<Boolean> beam_f16847_0(String key, @Nullable Boolean value)
{    return item(key, Type.BOOLEAN, value);}
public KV<T, Void> beam_f16856_0(T element)
{    return KV.of(element, (Void) null);}
public Void beam_f16857_0(Iterable<Void> iter)
{        return null;}
public static ProcessContinuation beam_f16866_0()
{    return PROCESS_CONTINUATION_STOP;}
public static ProcessContinuation beam_f16867_0()
{    return new AutoValue_DoFn_ProcessContinuation(true, Duration.ZERO);}
public static MultiOutputReceiver beam_f16878_0(DoFn<?, ?>.WindowedContext context, @Nullable Map<TupleTag<?>, Coder<?>> outputCoders)
{    return new WindowedContextMultiOutputReceiver(context, outputCoders);}
public static MultiOutputReceiver beam_f16879_0(DoFn<?, ?>.WindowedContext context)
{    return new WindowedContextMultiOutputReceiver(context);}
private SerializableFunction<InputT, OutputT> beam_f16888_0()
{    if (conversionFunction == null) {        conversionFunction = (SerializableFunction<InputT, OutputT>) ConvertHelpers.getConvertPrimitive(primitiveType, primitiveOutputType);    }    return conversionFunction;}
public static DoFnTester<InputT, OutputT> beam_f16889_1(DoFn<InputT, OutputT> fn)
{    checkNotNull(fn, "fn can't be null");        return new DoFnTester<>(fn);}
private static void beam_f16898_0(UserCodeException e) throws Exception
{    if (e.getCause() instanceof Exception) {        throw (Exception) e.getCause();    } else if (e.getCause() instanceof Error) {        throw (Error) e.getCause();    } else {        throw e;    }}
public void beam_f16899_0(InputT element) throws Exception
{    processTimestampedElement(TimestampedValue.atMinimumTimestamp(element));}
public InputT beam_f16908_0(DoFn<InputT, OutputT> doFn)
{    return processContext.element();}
public InputT beam_f16909_0(String sideInputTag)
{    throw new UnsupportedOperationException("SideInputs are not supported by DoFnTester");}
public org.apache.beam.sdk.state.State beam_f16918_0(String stateId)
{    throw new UnsupportedOperationException("DoFnTester doesn't support state yet");}
public Timer beam_f16919_0(String timerId)
{    throw new UnsupportedOperationException("DoFnTester doesn't support timers yet");}
public List<T> beam_f16928_0(TupleTag<T> tag)
{        return getImmutableOutput(tag).stream().map(ValueInSingleWindow::getValue).collect(Collectors.toList());}
public void beam_f16929_0(TupleTag<T> tag)
{    getMutableOutput(tag).clear();}
public DoFn<InputT, OutputT>.ProcessContext beam_f16938_0(ValueInSingleWindow<InputT> element)
{    return new TestProcessContext(element);}
public InputT beam_f16939_0()
{    return element.getValue();}
public void beam_f16948_0(TupleTag<T> tag, T output, Instant timestamp)
{    getMutableOutput(tag).add(ValueInSingleWindow.of(output, timestamp, element.getWindow(), element.getPane()));}
public void beam_f16949_0() throws Exception
{    if (state == State.BUNDLE_STARTED) {        finishBundle();    }    if (state == State.BUNDLE_FINISHED) {        fnInvoker.invokeTeardown();        fn = null;        fnInvoker = null;    }    state = State.TORN_DOWN;}
protected Void beam_f16958_0(DoFnSignature.Parameter p)
{    throw new UnsupportedOperationException("Parameter " + p + " not supported by DoFnTester");}
private void beam_f16959_0() throws Exception
{    checkState(state == State.UNINITIALIZED, "Already initialized");    checkState(fn == null, "Uninitialized but fn != null");    if (cloningBehavior.equals(CloningBehavior.DO_NOT_CLONE)) {        fn = origFn;    } else {        fn = (DoFn<InputT, OutputT>) SerializableUtils.deserializeFromByteArray(SerializableUtils.serializeToByteArray(origFn), origFn.toString());    }    fnInvoker = DoFnInvokers.invokerFor(fn);    fnInvoker.invokeSetup();}
 Filter<T> beam_f16968_0(String description)
{    return new Filter<>(predicate, description);}
public PCollection<T> beam_f16969_0(PCollection<T> input)
{    return input.apply(ParDo.of(new DoFn<T, T>() {        @ProcessElement        public void processElement(@Element T element, OutputReceiver<T> r) throws Exception {            if (predicate.apply(element)) {                r.output(element);            }        }    })).setCoder(input.getCoder());}
public PCollection<OutputT> beam_f16978_0(PCollection<? extends InputT> input)
{    checkArgument(fn != null, ".via() is required");    return input.apply("FlatMap", ParDo.of(new DoFn<InputT, OutputT>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            Iterable<OutputT> res = fn.getClosure().apply(c.element(), Fn.Context.wrapProcessContext(c));            for (OutputT output : res) {                c.output(output);            }        }        @Override        public TypeDescriptor<InputT> getInputTypeDescriptor() {            return inputType;        }        @Override        public TypeDescriptor<OutputT> getOutputTypeDescriptor() {            checkState(outputType != null, "%s output type descriptor was null; " + "this probably means that getOutputTypeDescriptor() was called after " + "serialization/deserialization, but it is only available prior to " + "serialization, for constructing a pipeline and inferring coders", FlatMapElements.class.getSimpleName());            return outputType;        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.delegate(FlatMapElements.this);        }    }).withSideInputs(fn.getRequirements().getSideInputs()));}
public void beam_f16979_0(ProcessContext c) throws Exception
{    Iterable<OutputT> res = fn.getClosure().apply(c.element(), Fn.Context.wrapProcessContext(c));    for (OutputT output : res) {        c.output(output);    }}
public void beam_f16988_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("class", originalFnForDisplayData.getClass()));    if (originalFnForDisplayData instanceof HasDisplayData) {        builder.include("fn", (HasDisplayData) originalFnForDisplayData);    }    builder.add(DisplayData.item("exceptionHandler.class", exceptionHandler.getClass()));    if (exceptionHandler instanceof HasDisplayData) {        builder.include("exceptionHandler", (HasDisplayData) exceptionHandler);    }}
public TypeDescriptor<FailureT> beam_f16989_0()
{    return failureType;}
public Iterable<T> beam_f16998_0(Iterable<T> element)
{    return element;}
public static GroupByKey<K, V> beam_f16999_0()
{    return new GroupByKey<>(false);}
 static Coder<Iterable<V>> beam_f17008_0(Coder<KV<K, V>> inputCoder)
{    return IterableCoder.of(getInputValueCoder(inputCoder));}
public static KvCoder<K, Iterable<V>> beam_f17009_0(Coder<KV<K, V>> inputCoder)
{    return KvCoder.of(getKeyCoder(inputCoder), getOutputValueCoder(inputCoder));}
private void beam_f17018_1(OutputReceiver<KV<K, Iterable<InputT>>> receiver, ValueState<K> key, BagState<InputT> batch, CombiningState<Long, long[], Long> numElementsInBatch)
{    Iterable<InputT> values = batch.read();        if (!Iterables.isEmpty(values)) {        receiver.output(KV.of(key.read(), values));    }    batch.clear();        numElementsInBatch.clear();}
public static Impulse beam_f17019_0()
{    return new Impulse();}
public CoGbkResultSchema beam_f17029_0()
{    return schema;}
public String beam_f17030_0()
{    return valueMap.toString();}
public CoGbkResultSchema beam_f17039_0()
{    return schema;}
public UnionCoder beam_f17040_0()
{    return unionCoder;}
public static CoGbkResult beam_f17049_0()
{    return new CoGbkResult(new CoGbkResultSchema(TupleTagList.empty()), new ArrayList<Iterable<?>>());}
private V beam_f17050_0(TupleTag<V> tag, @Nullable V defaultValue, boolean useDefault)
{    int index = schema.getIndex(tag);    if (index < 0) {        throw new IllegalArgumentException("TupleTag " + tag + " is not in the schema");    }    @SuppressWarnings("unchecked")    Iterator<V> unions = (Iterator<V>) valueMap.get(index).iterator();    if (!unions.hasNext()) {        if (useDefault) {            return defaultValue;        } else {            throw new IllegalArgumentException("TupleTag " + tag + " corresponds to an empty result, and no default was provided");        }    }    V value = unions.next();    if (unions.hasNext()) {        throw new IllegalArgumentException("TupleTag " + tag + " corresponds to a non-singleton result");    }    return value;}
public TupleTagList beam_f17059_0()
{    return tupleTagList;}
public boolean beam_f17060_0(Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof CoGbkResultSchema)) {        return false;    }    CoGbkResultSchema other = (CoGbkResultSchema) obj;    return tupleTagList.getAll().equals(other.tupleTagList.getAll());}
public static KeyedPCollectionTuple<K> beam_f17069_0(Pipeline pipeline)
{    return new KeyedPCollectionTuple<>(pipeline);}
public static KeyedPCollectionTuple<K> beam_f17070_0(TupleTag<InputT> tag, PCollection<KV<K, InputT>> pc)
{    return new KeyedPCollectionTuple<K>(pc.getPipeline()).and(tag, pc);}
public Coder<K> beam_f17079_0()
{    if (keyCoder == null) {        throw new IllegalStateException("cannot return null keyCoder");    }    return keyCoder;}
public CoGbkResultSchema beam_f17080_0()
{    return schema;}
public boolean beam_f17089_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    RawUnionValue that = (RawUnionValue) o;    if (unionTag != that.unionTag) {        return false;    }    return value != null ? value.equals(that.value) : that.value == null;}
public int beam_f17090_0()
{    int result = unionTag;    result = 31 * result + (value != null ? value.hashCode() : 0);    return result;}
public List<? extends Coder<?>> beam_f17099_0()
{    return elementCoders;}
public boolean beam_f17100_0(RawUnionValue union)
{    int index = getIndexForEncoding(union);    @SuppressWarnings("unchecked")    Coder<Object> coder = (Coder<Object>) elementCoders.get(index);    return coder.isRegisterByteSizeObserverCheap(union.getValue());}
public PCollection<K> beam_f17109_0(PCollection<? extends KV<K, ?>> in)
{    return in.apply("Keys", MapElements.via(new SimpleFunction<KV<K, ?>, K>() {        @Override        public K apply(KV<K, ?> kv) {            return kv.getKey();        }    }));}
public K beam_f17110_0(KV<K, ?> kv)
{    return kv.getKey();}
public Coder<TimestampedValue<T>> beam_f17119_0(CoderRegistry registry, Coder<TimestampedValue<T>> inputCoder) throws CannotProvideCoderException
{    return NullableCoder.of(inputCoder);}
public Coder<T> beam_f17120_0(CoderRegistry registry, Coder<TimestampedValue<T>> inputCoder) throws CannotProvideCoderException
{    checkState(inputCoder instanceof TimestampedValue.TimestampedValueCoder, "inputCoder must be a TimestampedValueCoder, but was %s", inputCoder);    TimestampedValue.TimestampedValueCoder<T> inputTVCoder = (TimestampedValue.TimestampedValueCoder<T>) inputCoder;    return NullableCoder.of(inputTVCoder.getValueCoder());}
public MapElements<NewInputT, OutputT> beam_f17129_0(SerializableFunction<NewInputT, OutputT> fn)
{    return via((ProcessFunction<NewInputT, OutputT>) fn);}
public MapElements<NewInputT, OutputT> beam_f17130_0(Contextful<Fn<NewInputT, OutputT>> fn)
{    return new MapElements<>(fn, fn.getClosure(), TypeDescriptors.inputOf(fn.getClosure()), outputType);}
public MapWithFailures<InputT, OutputT, FailureT> beam_f17139_0(ProcessFunction<ExceptionElement<InputT>, FailureT> exceptionHandler)
{    return new MapWithFailures<>(fn, originalFnForDisplayData, inputType, outputType, exceptionHandler, failureType);}
public WithFailures.Result<PCollection<OutputT>, FailureT> beam_f17140_0(PCollection<InputT> input)
{    checkArgument(exceptionHandler != null, ".exceptionsVia() is required");    MapFn doFn = new MapFn();    PCollectionTuple tuple = input.apply(MapWithFailures.class.getSimpleName(), ParDo.of(doFn).withOutputTags(doFn.outputTag, TupleTagList.of(doFn.failureTag)).withSideInputs(this.fn.getRequirements().getSideInputs()));    return WithFailures.Result.of(tuple, doFn.outputTag, doFn.failureTag);}
public static Materialization<IterableView<V>> beam_f17149_0()
{    return new Materialization<IterableView<V>>() {        @Override        public String getUrn() {            return ITERABLE_MATERIALIZATION_URN;        }    };}
public String beam_f17150_0()
{    return ITERABLE_MATERIALIZATION_URN;}
public static Combine.BinaryCombineDoubleFn beam_f17159_0()
{    return new Max.MaxDoubleFn();}
public static BinaryCombineFn<T> beam_f17160_0(final T identity, final ComparatorT comparator)
{    return new MaxFn<>(identity, comparator);}
public T beam_f17169_0(T left, T right)
{    return comparator.compare(left, right) >= 0 ? left : right;}
public void beam_f17170_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("comparer", comparator.getClass()).withLabel("Record Comparer"));}
public static Combine.AccumulatingCombineFn<NumT, CountSum<NumT>, Double> beam_f17179_0()
{    return new MeanFn<>();}
public CountSum<NumT> beam_f17180_0()
{    return new CountSum<>();}
public CountSum<NumT> beam_f17189_0(InputStream inStream) throws CoderException, IOException
{    return new CountSum<>(LONG_CODER.decode(inStream), DOUBLE_CODER.decode(inStream));}
public void beam_f17190_0() throws NonDeterministicException
{    LONG_CODER.verifyDeterministic();    DOUBLE_CODER.verifyDeterministic();}
public static Combine.BinaryCombineDoubleFn beam_f17199_0()
{    return new Min.MinDoubleFn();}
public static BinaryCombineFn<T> beam_f17200_0(T identity, ComparatorT comparator)
{    return new MinFn<>(identity, comparator);}
public T beam_f17209_0(T left, T right)
{    return comparator.compare(left, right) <= 0 ? left : right;}
public void beam_f17210_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("comparer", comparator.getClass()).withLabel("Record Comparer"));}
private static void beam_f17219_0(DoFn<?, ?> fn, CoderRegistry coderRegistry, Coder<?> inputCoder)
{    DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());    Map<String, DoFnSignature.StateDeclaration> stateDeclarations = signature.stateDeclarations();    for (DoFnSignature.StateDeclaration stateDeclaration : stateDeclarations.values()) {        try {            StateSpec<?> stateSpec = (StateSpec<?>) stateDeclaration.field().get(fn);            stateSpec.offerCoders(codersForStateSpecTypes(stateDeclaration, coderRegistry, inputCoder));            stateSpec.finishSpecifying();        } catch (IllegalAccessException e) {            throw new RuntimeException(e);        }    }}
private static void beam_f17220_0(DoFn<?, ?> fn, PCollection<?> input)
{    Coder<?> inputCoder = input.getCoder();    checkArgument(inputCoder instanceof KvCoder, "%s requires its input to use %s in order to use state and timers.", ParDo.class.getSimpleName(), KvCoder.class.getSimpleName());    KvCoder<?, ?> kvCoder = (KvCoder<?, ?>) inputCoder;    try {        kvCoder.getKeyCoder().verifyDeterministic();    } catch (Coder.NonDeterministicException exc) {        throw new IllegalArgumentException(String.format("%s requires a deterministic key coder in order to use state and timers", ParDo.class.getSimpleName()));    }}
public SingleOutput<InputT, OutputT> beam_f17229_0(Iterable<? extends PCollectionView<?>> sideInputs)
{    Map<String, PCollectionView<?>> mappedInputs = StreamSupport.stream(sideInputs.spliterator(), false).collect(Collectors.toMap(v -> v.getTagInternal().getId(), v -> v));    return withSideInputs(mappedInputs);}
public SingleOutput<InputT, OutputT> beam_f17230_0(Map<String, PCollectionView<?>> sideInputs)
{    return new SingleOutput<>(fn, ImmutableMap.<String, PCollectionView<?>>builder().putAll(this.sideInputs).putAll(sideInputs).build(), fnDisplayData);}
public String beam_f17239_0()
{    return fn.toString();}
public MultiOutput<InputT, OutputT> beam_f17240_0(PCollectionView<?>... sideInputs)
{    return withSideInputs(Arrays.asList(sideInputs));}
public TupleTagList beam_f17249_0()
{    return additionalOutputTags;}
public Map<String, PCollectionView<?>> beam_f17250_0()
{    return sideInputs;}
public void beam_f17259_0(@Element X input, MultiOutputReceiver r)
{    int partition = partitionFn.partitionFor(input, numPartitions);    if (0 <= partition && partition < numPartitions) {        @SuppressWarnings("unchecked")        TupleTag<X> typedTag = (TupleTag<X>) outputTags.get(partition);        r.get(typedTag).output(input);    } else {        throw new IndexOutOfBoundsException("Partition function returned out of bounds index: " + partition + " not in [0.." + numPartitions + ")");    }}
public void beam_f17260_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("numPartitions", numPartitions).withLabel("Partition Count")).add(DisplayData.item("partitionFn", partitionFn.getClass()).withLabel("Partition Function"));}
public OutputT beam_f17273_0(InputT input)
{    return fn.apply(input);}
public static PTransform<InputT, OutputT> beam_f17274_0(String name, SerializableFunction<InputT, OutputT> fn)
{    return new PTransform<InputT, OutputT>(name) {        @Override        public OutputT expand(InputT input) {            return fn.apply(input);        }    };}
private synchronized Constructor<?> beam_f17283_0(DoFnSignature signature)
{    Class<? extends DoFn<?, ?>> fnClass = signature.fnClass();    Constructor<?> constructor = byteBuddyInvokerConstructorCache.get(fnClass);    if (constructor == null) {        Class<? extends DoFnInvoker<?, ?>> invokerClass = generateInvokerClass(signature);        try {            constructor = invokerClass.getConstructor(fnClass);        } catch (IllegalArgumentException | NoSuchMethodException | SecurityException e) {            throw new RuntimeException(e);        }        byteBuddyInvokerConstructorCache.put(fnClass, constructor);    }    return constructor;}
public static void beam_f17284_0(InputT element, RestrictionT restriction, DoFn.OutputReceiver<RestrictionT> receiver)
{    receiver.output(restriction);}
private static Implementation beam_f17293_0(TypeDescription doFnType, DoFnSignature.DoFnMethod method)
{    return (method == null) ? ExceptionMethod.throwing(UnsupportedOperationException.class) : new DowncastingParametersMethodDelegation(doFnType, method.targetMethod());}
public InstrumentedType beam_f17294_0(InstrumentedType instrumentedType)
{        delegateField = instrumentedType.getSuperClass().getDeclaredFields().filter(ElementMatchers.named(FN_DELEGATE_FIELD_NAME)).getOnly();        return instrumentedType;}
 static StackManipulation beam_f17303_0(DoFnSignature.Parameter parameter, final StackManipulation pushDelegate)
{    return parameter.match(new Cases<StackManipulation>() {        @Override        public StackManipulation dispatch(StartBundleContextParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(START_BUNDLE_CONTEXT_PARAMETER_METHOD, DoFn.class)));        }        @Override        public StackManipulation dispatch(FinishBundleContextParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(FINISH_BUNDLE_CONTEXT_PARAMETER_METHOD, DoFn.class)));        }        @Override        public StackManipulation dispatch(ProcessContextParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(PROCESS_CONTEXT_PARAMETER_METHOD, DoFn.class)));        }        @Override        public StackManipulation dispatch(ElementParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(ELEMENT_PARAMETER_METHOD, DoFn.class)), TypeCasting.to(new TypeDescription.ForLoadedType(p.elementT().getRawType())));        }        @Override        public StackManipulation dispatch(SchemaElementParameter p) {            ForLoadedType elementType = new ForLoadedType(p.elementT().getRawType());            ForLoadedType castType = elementType.isPrimitive() ? new ForLoadedType(Primitives.wrap(p.elementT().getRawType())) : elementType;            StackManipulation stackManipulation = new StackManipulation.Compound(IntegerConstant.forValue(p.index()), MethodInvocation.invoke(getExtraContextFactoryMethodDescription(SCHEMA_ELEMENT_PARAMETER_METHOD, int.class)), TypeCasting.to(castType));            if (elementType.isPrimitive()) {                stackManipulation = new Compound(stackManipulation, Assigner.DEFAULT.assign(elementType.asBoxed().asGenericType(), elementType.asUnboxed().asGenericType(), Typing.STATIC));            }            return stackManipulation;        }        @Override        public StackManipulation dispatch(TimestampParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(TIMESTAMP_PARAMETER_METHOD, DoFn.class)));        }        @Override        public StackManipulation dispatch(TimeDomainParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(TIME_DOMAIN_PARAMETER_METHOD, DoFn.class)));        }        @Override        public StackManipulation dispatch(OutputReceiverParameter p) {            String method = p.isRowReceiver() ? OUTPUT_ROW_RECEIVER_METHOD : OUTPUT_PARAMETER_METHOD;            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(method, DoFn.class)));        }        @Override        public StackManipulation dispatch(TaggedOutputReceiverParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(TAGGED_OUTPUT_PARAMETER_METHOD, DoFn.class)));        }        @Override        public StackManipulation dispatch(OnTimerContextParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(ON_TIMER_CONTEXT_PARAMETER_METHOD, DoFn.class)));        }        @Override        public StackManipulation dispatch(WindowParameter p) {            return new StackManipulation.Compound(simpleExtraContextParameter(WINDOW_PARAMETER_METHOD), TypeCasting.to(new TypeDescription.ForLoadedType(p.windowT().getRawType())));        }        @Override        public StackManipulation dispatch(PaneInfoParameter p) {            return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(PANE_INFO_PARAMETER_METHOD, DoFn.class)));        }        @Override        public StackManipulation dispatch(RestrictionTrackerParameter p) {                        return new StackManipulation.Compound(simpleExtraContextParameter(RESTRICTION_TRACKER_PARAMETER_METHOD), TypeCasting.to(new TypeDescription.ForLoadedType(p.trackerT().getRawType())));        }        @Override        public StackManipulation dispatch(StateParameter p) {            return new StackManipulation.Compound(new TextConstant(p.referent().id()), MethodInvocation.invoke(getExtraContextFactoryMethodDescription(STATE_PARAMETER_METHOD, String.class)), TypeCasting.to(new TypeDescription.ForLoadedType(p.referent().stateType().getRawType())));        }        @Override        public StackManipulation dispatch(TimerParameter p) {            return new StackManipulation.Compound(new TextConstant(p.referent().id()), MethodInvocation.invoke(getExtraContextFactoryMethodDescription(TIMER_PARAMETER_METHOD, String.class)), TypeCasting.to(new TypeDescription.ForLoadedType(Timer.class)));        }        @Override        public StackManipulation dispatch(DoFnSignature.Parameter.PipelineOptionsParameter p) {            return simpleExtraContextParameter(PIPELINE_OPTIONS_PARAMETER_METHOD);        }        @Override        public StackManipulation dispatch(SideInputParameter p) {            return new StackManipulation.Compound(new TextConstant(p.sideInputId()), MethodInvocation.invoke(getExtraContextFactoryMethodDescription(SIDE_INPUT_PARAMETER_METHOD, String.class)), TypeCasting.to(new TypeDescription.ForLoadedType(p.elementT().getRawType())));        }    });}
public StackManipulation beam_f17304_0(StartBundleContextParameter p)
{    return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(START_BUNDLE_CONTEXT_PARAMETER_METHOD, DoFn.class)));}
public StackManipulation beam_f17313_0(OnTimerContextParameter p)
{    return new StackManipulation.Compound(pushDelegate, MethodInvocation.invoke(getExtraContextFactoryMethodDescription(ON_TIMER_CONTEXT_PARAMETER_METHOD, DoFn.class)));}
public StackManipulation beam_f17314_0(WindowParameter p)
{    return new StackManipulation.Compound(simpleExtraContextParameter(WINDOW_PARAMETER_METHOD), TypeCasting.to(new TypeDescription.ForLoadedType(p.windowT().getRawType())));}
private Object beam_f17323_0(Type type)
{    switch(type.getSort()) {        case Type.OBJECT:            return type.getInternalName();        case Type.INT:        case Type.BYTE:        case Type.BOOLEAN:        case Type.SHORT:            return Opcodes.INTEGER;        case Type.LONG:            return Opcodes.LONG;        case Type.DOUBLE:            return Opcodes.DOUBLE;        case Type.FLOAT:            return Opcodes.FLOAT;        default:            throw new IllegalArgumentException("Unhandled type as method argument: " + type);    }}
private void beam_f17324_0(MethodVisitor mv, boolean localsIncludeReturn, @Nullable String stackTop)
{    boolean hasReturnLocal = (returnVarIndex != null) && localsIncludeReturn;    Type[] localTypes = Type.getArgumentTypes(instrumentedMethod.getDescriptor());    Object[] locals = new Object[1 + localTypes.length + (hasReturnLocal ? 1 : 0)];    locals[0] = instrumentedMethod.getReceiverType().asErasure().getInternalName();    for (int i = 0; i < localTypes.length; i++) {        locals[i + 1] = describeType(localTypes[i]);    }    if (hasReturnLocal) {        locals[locals.length - 1] = returnType.getInternalName();    }    Object[] stack = stackTop == null ? new Object[] {} : new Object[] { stackTop };    mv.visitFrame(Opcodes.F_NEW, locals.length, locals, stack.length, stack);}
public InstrumentedType beam_f17333_0(InstrumentedType instrumentedType)
{    return instrumentedType;}
public ByteCodeAppender beam_f17334_0(final Target implementationTarget)
{    return (methodVisitor, implementationContext, instrumentedMethod) -> {        StackManipulation.Size size = new StackManipulation.Compound(        MethodVariableAccess.REFERENCE.loadFrom(0),         MethodInvocation.invoke(new TypeDescription.ForLoadedType(Object.class).getDeclaredMethods().filter(ElementMatchers.isConstructor().and(ElementMatchers.takesArguments(0))).getOnly()),         MethodVariableAccess.REFERENCE.loadFrom(0),         MethodVariableAccess.REFERENCE.loadFrom(1),         FieldAccess.forField(implementationTarget.getInstrumentedType().getDeclaredFields().filter(ElementMatchers.named(FN_DELEGATE_FIELD_NAME)).getOnly()).write(),         MethodReturn.VOID).apply(methodVisitor, implementationContext);        return new ByteCodeAppender.Size(size.getMaximalSize(), instrumentedMethod.getStackSize());    };}
public MultiOutputReceiver beam_f17343_0(DoFn<InputT, OutputT> doFn)
{    throw new UnsupportedOperationException(String.format("Should never call non-overridden methods of %s", FakeArgumentProvider.class.getSimpleName()));}
public BoundedWindow beam_f17344_0()
{    throw new UnsupportedOperationException(String.format("Should never call non-overridden methods of %s", FakeArgumentProvider.class.getSimpleName()));}
public static DoFnInvoker<InputT, OutputT> beam_f17353_0(DoFn<InputT, OutputT> fn)
{    return ByteBuddyDoFnInvokerFactory.only().newByteBuddyInvoker(fn);}
public static DoFnInvoker<InputT, OutputT> beam_f17354_0(DoFn<InputT, OutputT> fn)
{    DoFnInvoker<InputT, OutputT> doFnInvoker = invokerFor(fn);    try {        doFnInvoker.invokeSetup();    } catch (Exception e) {        try {            doFnInvoker.invokeTeardown();        } catch (Exception suppressed) {            e.addSuppressed(suppressed);        }        throw e;    }    return doFnInvoker;}
public ResultT beam_f17363_0(ElementParameter p)
{    return dispatchDefault(p);}
public ResultT beam_f17364_0(SchemaElementParameter p)
{    return dispatchDefault(p);}
public ResultT beam_f17373_0(StateParameter p)
{    return dispatchDefault(p);}
public ResultT beam_f17374_0(TimerParameter p)
{    return dispatchDefault(p);}
public static OutputReceiverParameter beam_f17383_0(boolean rowReceiver)
{    return new AutoValue_DoFnSignature_Parameter_OutputReceiverParameter(rowReceiver);}
public static TaggedOutputReceiverParameter beam_f17384_0()
{    return TAGGED_OUTPUT_RECEIVER_PARAMETER;}
public boolean beam_f17393_0()
{    return extraParameters().stream().anyMatch(Predicates.or(Predicates.instanceOf(WindowParameter.class), Predicates.instanceOf(TimerParameter.class), Predicates.instanceOf(StateParameter.class))::apply);}
public List<SchemaElementParameter> beam_f17394_0()
{    return extraParameters().stream().filter(Predicates.instanceOf(SchemaElementParameter.class)::apply).map(SchemaElementParameter.class::cast).collect(Collectors.toList());}
 static FieldAccessDeclaration beam_f17403_0(String id, Field field)
{    field.setAccessible(true);    return new AutoValue_DoFnSignature_FieldAccessDeclaration(id, field);}
 static LifecycleMethod beam_f17404_0(Method targetMethod)
{    return new AutoValue_DoFnSignature_LifecycleMethod(targetMethod);}
public Map<String, TimerDeclaration> beam_f17413_0()
{    return Collections.unmodifiableMap(timerDeclarations);}
public Map<String, FieldAccessDeclaration> beam_f17414_0()
{    return fieldAccessDeclarations;}
public boolean beam_f17423_0()
{    return extraParameters.stream().anyMatch(Predicates.instanceOf(Parameter.PipelineOptionsParameter.class)::apply);}
public TypeDescriptor<? extends BoundedWindow> beam_f17424_0()
{    return windowT;}
private static DoFnSignature beam_f17433_0(Class<? extends DoFn<?, ?>> fnClass)
{    DoFnSignature.Builder signatureBuilder = DoFnSignature.builder();    ErrorReporter errors = new ErrorReporter(null, fnClass.getName());    errors.checkArgument(DoFn.class.isAssignableFrom(fnClass), "Must be subtype of DoFn");    signatureBuilder.setFnClass(fnClass);    TypeDescriptor<? extends DoFn<?, ?>> fnT = TypeDescriptor.of(fnClass);        TypeDescriptor<?> inputT = null;    TypeDescriptor<?> outputT = null;    for (TypeDescriptor<?> supertype : fnT.getTypes()) {        if (!supertype.getRawType().equals(DoFn.class)) {            continue;        }        Type[] args = ((ParameterizedType) supertype.getType()).getActualTypeArguments();        inputT = TypeDescriptor.of(args[0]);        outputT = TypeDescriptor.of(args[1]);    }    errors.checkNotNull(inputT, "Unable to determine input type");            FnAnalysisContext fnContext = FnAnalysisContext.create();    fnContext.addStateDeclarations(analyzeStateDeclarations(errors, fnClass).values());    fnContext.addTimerDeclarations(analyzeTimerDeclarations(errors, fnClass).values());    fnContext.addFieldAccessDeclarations(analyzeFieldAccessDeclaration(errors, fnClass).values());    Method processElementMethod = findAnnotatedMethod(errors, DoFn.ProcessElement.class, fnClass, true);    Method startBundleMethod = findAnnotatedMethod(errors, DoFn.StartBundle.class, fnClass, false);    Method finishBundleMethod = findAnnotatedMethod(errors, DoFn.FinishBundle.class, fnClass, false);    Method setupMethod = findAnnotatedMethod(errors, DoFn.Setup.class, fnClass, false);    Method teardownMethod = findAnnotatedMethod(errors, DoFn.Teardown.class, fnClass, false);    Method onWindowExpirationMethod = findAnnotatedMethod(errors, DoFn.OnWindowExpiration.class, fnClass, false);    Method getInitialRestrictionMethod = findAnnotatedMethod(errors, DoFn.GetInitialRestriction.class, fnClass, false);    Method splitRestrictionMethod = findAnnotatedMethod(errors, DoFn.SplitRestriction.class, fnClass, false);    Method getRestrictionCoderMethod = findAnnotatedMethod(errors, DoFn.GetRestrictionCoder.class, fnClass, false);    Method newTrackerMethod = findAnnotatedMethod(errors, DoFn.NewTracker.class, fnClass, false);    Collection<Method> onTimerMethods = declaredMethodsWithAnnotation(DoFn.OnTimer.class, fnClass, DoFn.class);    HashMap<String, DoFnSignature.OnTimerMethod> onTimerMethodMap = Maps.newHashMapWithExpectedSize(onTimerMethods.size());    for (Method onTimerMethod : onTimerMethods) {        String id = onTimerMethod.getAnnotation(DoFn.OnTimer.class).value();        errors.checkArgument(fnContext.getTimerDeclarations().containsKey(id), "Callback %s is for undeclared timer %s", onTimerMethod, id);        TimerDeclaration timerDecl = fnContext.getTimerDeclarations().get(id);        errors.checkArgument(timerDecl.field().getDeclaringClass().equals(getDeclaringClass(onTimerMethod)), "Callback %s is for timer %s declared in a different class %s." + " Timer callbacks must be declared in the same lexical scope as their timer", onTimerMethod, id, timerDecl.field().getDeclaringClass().getCanonicalName());        onTimerMethodMap.put(id, analyzeOnTimerMethod(errors, fnT, onTimerMethod, id, inputT, outputT, fnContext));    }    signatureBuilder.setOnTimerMethods(onTimerMethodMap);        for (TimerDeclaration decl : fnContext.getTimerDeclarations().values()) {        errors.checkArgument(onTimerMethodMap.containsKey(decl.id()), "No callback registered via %s for timer %s", DoFn.OnTimer.class.getSimpleName(), decl.id());    }    ErrorReporter processElementErrors = errors.forMethod(DoFn.ProcessElement.class, processElementMethod);    DoFnSignature.ProcessElementMethod processElement = analyzeProcessElementMethod(processElementErrors, fnT, processElementMethod, inputT, outputT, fnContext);    signatureBuilder.setProcessElement(processElement);    if (startBundleMethod != null) {        ErrorReporter startBundleErrors = errors.forMethod(DoFn.StartBundle.class, startBundleMethod);        signatureBuilder.setStartBundle(analyzeStartBundleMethod(startBundleErrors, fnT, startBundleMethod, inputT, outputT));    }    if (finishBundleMethod != null) {        ErrorReporter finishBundleErrors = errors.forMethod(DoFn.FinishBundle.class, finishBundleMethod);        signatureBuilder.setFinishBundle(analyzeFinishBundleMethod(finishBundleErrors, fnT, finishBundleMethod, inputT, outputT));    }    if (setupMethod != null) {        signatureBuilder.setSetup(analyzeLifecycleMethod(errors.forMethod(DoFn.Setup.class, setupMethod), setupMethod));    }    if (teardownMethod != null) {        signatureBuilder.setTeardown(analyzeLifecycleMethod(errors.forMethod(DoFn.Teardown.class, teardownMethod), teardownMethod));    }    if (onWindowExpirationMethod != null) {        signatureBuilder.setOnWindowExpiration(analyzeOnWindowExpirationMethod(errors, fnT, onWindowExpirationMethod, inputT, outputT, fnContext));    }    ErrorReporter getInitialRestrictionErrors;    if (getInitialRestrictionMethod != null) {        getInitialRestrictionErrors = errors.forMethod(DoFn.GetInitialRestriction.class, getInitialRestrictionMethod);        signatureBuilder.setGetInitialRestriction(analyzeGetInitialRestrictionMethod(getInitialRestrictionErrors, fnT, getInitialRestrictionMethod, inputT));    }    if (splitRestrictionMethod != null) {        ErrorReporter splitRestrictionErrors = errors.forMethod(DoFn.SplitRestriction.class, splitRestrictionMethod);        signatureBuilder.setSplitRestriction(analyzeSplitRestrictionMethod(splitRestrictionErrors, fnT, splitRestrictionMethod, inputT));    }    if (getRestrictionCoderMethod != null) {        ErrorReporter getRestrictionCoderErrors = errors.forMethod(DoFn.GetRestrictionCoder.class, getRestrictionCoderMethod);        signatureBuilder.setGetRestrictionCoder(analyzeGetRestrictionCoderMethod(getRestrictionCoderErrors, fnT, getRestrictionCoderMethod));    }    if (newTrackerMethod != null) {        ErrorReporter newTrackerErrors = errors.forMethod(DoFn.NewTracker.class, newTrackerMethod);        signatureBuilder.setNewTracker(analyzeNewTrackerMethod(newTrackerErrors, fnT, newTrackerMethod));    }    signatureBuilder.setIsBoundedPerElement(inferBoundedness(fnT, processElement, errors));    signatureBuilder.setStateDeclarations(fnContext.getStateDeclarations());    signatureBuilder.setTimerDeclarations(fnContext.getTimerDeclarations());    signatureBuilder.setFieldAccessDeclarations(fnContext.getFieldAccessDeclarations());    DoFnSignature signature = signatureBuilder.build();        if (processElement.isSplittable()) {        verifySplittableMethods(signature, errors);    } else {        verifyUnsplittableMethods(errors, signature);    }    return signature;}
private static Class<?> beam_f17434_0(Method onTimerMethod)
{    Class<?> declaringClass = onTimerMethod.getDeclaringClass();    if (declaringClass.getName().contains("$MockitoMock$")) {        declaringClass = declaringClass.getSuperclass();    }    return declaringClass;}
 static DoFnSignature.OnWindowExpirationMethod beam_f17443_0(ErrorReporter errors, TypeDescriptor<? extends DoFn<?, ?>> fnClass, Method m, TypeDescriptor<?> inputT, TypeDescriptor<?> outputT, FnAnalysisContext fnContext)
{    errors.checkArgument(void.class.equals(m.getReturnType()), "Must return void");    Type[] params = m.getGenericParameterTypes();    MethodAnalysisContext methodContext = MethodAnalysisContext.create();    boolean requiresStableInput = m.isAnnotationPresent(DoFn.RequiresStableInput.class);    @Nullable    TypeDescriptor<? extends BoundedWindow> windowT = getWindowType(fnClass, m);    List<DoFnSignature.Parameter> extraParameters = new ArrayList<>();    ErrorReporter onWindowExpirationErrors = errors.forMethod(DoFn.OnWindowExpiration.class, m);    for (int i = 0; i < params.length; ++i) {        Parameter parameter = analyzeExtraParameter(onWindowExpirationErrors, fnContext, methodContext, fnClass, ParameterDescription.of(m, i, fnClass.resolveType(params[i]), Arrays.asList(m.getParameterAnnotations()[i])), inputT, outputT);        checkParameterOneOf(errors, parameter, ALLOWED_ON_WINDOW_EXPIRATION_PARAMETERS);        extraParameters.add(parameter);    }    return DoFnSignature.OnWindowExpirationMethod.create(m, requiresStableInput, windowT, extraParameters);}
 static DoFnSignature.ProcessElementMethod beam_f17444_0(ErrorReporter errors, TypeDescriptor<? extends DoFn<?, ?>> fnClass, Method m, TypeDescriptor<?> inputT, TypeDescriptor<?> outputT, FnAnalysisContext fnContext)
{    errors.checkArgument(void.class.equals(m.getReturnType()) || DoFn.ProcessContinuation.class.equals(m.getReturnType()), "Must return void or %s", DoFn.ProcessContinuation.class.getSimpleName());    MethodAnalysisContext methodContext = MethodAnalysisContext.create();    boolean requiresStableInput = m.isAnnotationPresent(DoFn.RequiresStableInput.class);    Type[] params = m.getGenericParameterTypes();    TypeDescriptor<?> trackerT = getTrackerType(fnClass, m);    TypeDescriptor<? extends BoundedWindow> windowT = getWindowType(fnClass, m);    for (int i = 0; i < params.length; ++i) {        Parameter extraParam = analyzeExtraParameter(errors.forMethod(DoFn.ProcessElement.class, m), fnContext, methodContext, fnClass, ParameterDescription.of(m, i, fnClass.resolveType(params[i]), Arrays.asList(m.getParameterAnnotations()[i])), inputT, outputT);        methodContext.addParameter(extraParam);    }    int schemaElementIndex = 0;    for (int i = 0; i < methodContext.getExtraParameters().size(); ++i) {        Parameter parameter = methodContext.getExtraParameters().get(i);        if (parameter instanceof SchemaElementParameter) {            SchemaElementParameter schemaParameter = (SchemaElementParameter) parameter;            schemaParameter = schemaParameter.toBuilder().setIndex(schemaElementIndex).build();            methodContext.setParameter(i, schemaParameter);            ++schemaElementIndex;        }    }        if (methodContext.hasRestrictionTrackerParameter()) {        for (Parameter parameter : methodContext.getExtraParameters()) {            checkParameterOneOf(errors, parameter, ALLOWED_SPLITTABLE_PROCESS_ELEMENT_PARAMETERS);        }    } else {        for (Parameter parameter : methodContext.getExtraParameters()) {            checkParameterOneOf(errors, parameter, ALLOWED_NON_SPLITTABLE_PROCESS_ELEMENT_PARAMETERS);        }    }    return DoFnSignature.ProcessElementMethod.create(m, methodContext.getExtraParameters(), requiresStableInput, trackerT, windowT, DoFn.ProcessContinuation.class.equals(m.getReturnType()));}
private static boolean beam_f17453_0(List<Annotation> annotations)
{    return annotations.stream().anyMatch(a -> a.annotationType().equals(DoFn.Timestamp.class));}
private static boolean beam_f17454_0(List<Annotation> annotations)
{    return annotations.stream().anyMatch(a -> a.annotationType().equals(DoFn.SideInput.class));}
private static ImmutableMap<String, TimerDeclaration> beam_f17463_0(ErrorReporter errors, Class<?> fnClazz)
{    Map<String, DoFnSignature.TimerDeclaration> declarations = new HashMap<>();    for (Field field : declaredFieldsWithAnnotation(DoFn.TimerId.class, fnClazz, DoFn.class)) {                field.setAccessible(true);        String id = field.getAnnotation(DoFn.TimerId.class).value();        validateTimerField(errors, declarations, id, field);        declarations.put(id, DoFnSignature.TimerDeclaration.create(id, field));    }    return ImmutableMap.copyOf(declarations);}
private static void beam_f17464_0(ErrorReporter errors, Map<String, TimerDeclaration> declarations, String id, Field field)
{    if (declarations.containsKey(id)) {        errors.throwIllegalArgument("Duplicate %s \"%s\", used on both of [%s] and [%s]", DoFn.TimerId.class.getSimpleName(), id, field.toString(), declarations.get(id).field().toString());    }    Class<?> timerSpecRawType = field.getType();    if (!(timerSpecRawType.equals(TimerSpec.class))) {        errors.throwIllegalArgument("%s annotation on non-%s field [%s]", DoFn.TimerId.class.getSimpleName(), TimerSpec.class.getSimpleName(), field.toString());    }    if (!Modifier.isFinal(field.getModifiers())) {        errors.throwIllegalArgument("Non-final field %s annotated with %s. Timer declarations must be final.", field.toString(), DoFn.TimerId.class.getSimpleName());    }}
private static Map<String, DoFnSignature.StateDeclaration> beam_f17473_0(ErrorReporter errors, Class<?> fnClazz)
{    Map<String, DoFnSignature.StateDeclaration> declarations = new HashMap<>();    for (Field field : declaredFieldsWithAnnotation(DoFn.StateId.class, fnClazz, DoFn.class)) {                field.setAccessible(true);        String id = field.getAnnotation(DoFn.StateId.class).value();        if (declarations.containsKey(id)) {            errors.throwIllegalArgument("Duplicate %s \"%s\", used on both of [%s] and [%s]", DoFn.StateId.class.getSimpleName(), id, field.toString(), declarations.get(id).field().toString());            continue;        }        Class<?> stateSpecRawType = field.getType();        if (!(TypeDescriptor.of(stateSpecRawType).isSubtypeOf(TypeDescriptor.of(StateSpec.class)))) {            errors.throwIllegalArgument("%s annotation on non-%s field [%s] that has class %s", DoFn.StateId.class.getSimpleName(), StateSpec.class.getSimpleName(), field.toString(), stateSpecRawType.getName());            continue;        }        if (!Modifier.isFinal(field.getModifiers())) {            errors.throwIllegalArgument("Non-final field %s annotated with %s. State declarations must be final.", field.toString(), DoFn.StateId.class.getSimpleName());            continue;        }        Type stateSpecType = field.getGenericType();                        TypeDescriptor<? extends StateSpec<?>> stateSpecSubclassTypeDescriptor = (TypeDescriptor) TypeDescriptor.of(stateSpecType);                        TypeDescriptor<StateSpec<?>> stateSpecTypeDescriptor = (TypeDescriptor) stateSpecSubclassTypeDescriptor.getSupertype(StateSpec.class);                        Type unresolvedStateType = ((ParameterizedType) stateSpecTypeDescriptor.getType()).getActualTypeArguments()[0];                TypeDescriptor<? extends State> stateType = (TypeDescriptor<? extends State>) TypeDescriptor.of(fnClazz).resolveType(unresolvedStateType);        declarations.put(id, DoFnSignature.StateDeclaration.create(id, field, stateType));    }    return ImmutableMap.copyOf(declarations);}
private static Method beam_f17474_0(ErrorReporter errors, Class<? extends Annotation> anno, Class<?> fnClazz, boolean required)
{    Collection<Method> matches = declaredMethodsWithAnnotation(anno, fnClazz, DoFn.class);    if (matches.isEmpty()) {        errors.checkArgument(!required, "No method annotated with @%s found", anno.getSimpleName());        return null;    }                Method first = matches.iterator().next();    for (Method other : matches) {        errors.checkArgument(first.getName().equals(other.getName()) && Arrays.equals(first.getParameterTypes(), other.getParameterTypes()), "Found multiple methods annotated with @%s. [%s] and [%s]", anno.getSimpleName(), format(first), format(other));    }    ErrorReporter methodErrors = errors.forMethod(anno, first);        methodErrors.checkArgument((first.getModifiers() & Modifier.PUBLIC) != 0, "Must be public");        methodErrors.checkArgument((first.getModifiers() & Modifier.STATIC) == 0, "Must not be static");    return first;}
public static TimerSpec beam_f17483_0(TimerDeclaration timerDeclaration, DoFn<?, ?> target)
{    try {        Object fieldValue = timerDeclaration.field().get(target);        checkState(fieldValue instanceof TimerSpec, "Malformed %s class %s: timer declaration field %s does not have type %s.", DoFn.class.getSimpleName(), target.getClass().getName(), timerDeclaration.field().getName(), TimerSpec.class);        return (TimerSpec) timerDeclaration.field().get(target);    } catch (IllegalAccessException exc) {        throw new RuntimeException(String.format("Malformed %s class %s: timer declaration field %s is not accessible.", DoFn.class.getSimpleName(), target.getClass().getName(), timerDeclaration.field().getName()));    }}
public static OnTimerInvoker<InputT, OutputT> beam_f17484_0(DoFn<InputT, OutputT> fn, String timerId)
{    return ByteBuddyOnTimerInvokerFactory.only().forTimer(fn, timerId);}
public static MatchesName beam_f17493_0(String regex, String groupName)
{    return matches(Pattern.compile(regex), groupName);}
public static MatchesName beam_f17494_0(Pattern pattern, String groupName)
{    return new MatchesName(pattern, groupName);}
public static Find beam_f17503_0(String regex, int group)
{    return find(Pattern.compile(regex), group);}
public static Find beam_f17504_0(Pattern pattern, int group)
{    return new Find(pattern, group);}
public static ReplaceAll beam_f17513_0(String regex, String replacement)
{    return replaceAll(Pattern.compile(regex), replacement);}
public static ReplaceAll beam_f17514_0(Pattern pattern, String replacement)
{    return new ReplaceAll(pattern, replacement);}
public PCollection<String> beam_f17523_0(PCollection<String> in)
{    return in.apply(ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(@Element String element, OutputReceiver<String> r) throws Exception {            Matcher m = pattern.matcher(element);            if (m.matches()) {                r.output(m.group(groupName));            }        }    }));}
public void beam_f17524_0(@Element String element, OutputReceiver<String> r) throws Exception
{    Matcher m = pattern.matcher(element);    if (m.matches()) {        r.output(m.group(groupName));    }}
public PCollection<String> beam_f17533_0(PCollection<String> in)
{    return in.apply(ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(@Element String element, OutputReceiver<String> r) throws Exception {            Matcher m = pattern.matcher(element);            if (m.find()) {                r.output(m.group(groupName));            }        }    }));}
public void beam_f17534_0(@Element String element, OutputReceiver<String> r) throws Exception
{    Matcher m = pattern.matcher(element);    if (m.find()) {        r.output(m.group(groupName));    }}
public PCollection<String> beam_f17543_0(PCollection<String> in)
{    return in.apply(ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(@Element String element, OutputReceiver<String> r) throws Exception {            Matcher m = pattern.matcher(element);            r.output(m.replaceFirst(replacement));        }    }));}
public void beam_f17544_0(@Element String element, OutputReceiver<String> r) throws Exception
{    Matcher m = pattern.matcher(element);    r.output(m.replaceFirst(replacement));}
public void beam_f17553_0(@Element T element, @Timestamp Instant timestamp, OutputReceiver<TimestampedValue<T>> r)
{    r.output(TimestampedValue.of(element, timestamp));}
public PCollection<KV<K, ValueInSingleWindow<V>>> beam_f17554_0(PCollection<KV<K, V>> input)
{    KvCoder<K, V> coder = (KvCoder<K, V>) input.getCoder();    return input.apply(ParDo.of(new DoFn<KV<K, V>, KV<K, ValueInSingleWindow<V>>>() {        @ProcessElement        public void processElement(@Element KV<K, V> element, @Timestamp Instant timestamp, BoundedWindow window, PaneInfo pane, OutputReceiver<KV<K, ValueInSingleWindow<V>>> r) {            r.output(KV.of(element.getKey(), ValueInSingleWindow.of(element.getValue(), timestamp, window, pane)));        }    })).setCoder(KvCoder.of(coder.getKeyCoder(), ValueInSingleWindow.Coder.of(coder.getValueCoder(), input.getWindowingStrategy().getWindowFn().windowCoder())));}
public static PTransform<PCollection<T>, PCollection<ValueInSingleWindow<T>>> beam_f17563_0()
{    return new Window<>();}
public static PTransform<PCollection<KV<K, V>>, PCollection<KV<K, ValueInSingleWindow<V>>>> beam_f17564_0()
{    return new WindowInValue<>();}
public PCollection<KV<K, V>> beam_f17573_0(PCollection<? extends KV<K, TimestampedValue<V>>> input)
{    return input.apply(new RemoveWildcard<KV<K, TimestampedValue<V>>>()).apply(Reify.extractTimestampsFromValues());}
public Collection<PCollectionView<?>> beam_f17574_0()
{    return sideInputs;}
public void beam_f17583_0(@Element KV<K, Iterable<TimestampedValue<V>>> element, OutputReceiver<KV<K, TimestampedValue<V>>> r)
{    K key = element.getKey();    for (TimestampedValue<V> value : element.getValue()) {        r.output(KV.of(key, value));    }}
public PCollection<T> beam_f17584_0(PCollection<T> input)
{    return input.apply("Pair with random key", ParDo.of(new AssignShardFn<>())).apply(Reshuffle.of()).apply(Values.create());}
public void beam_f17593_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("sampleSize", limit).withLabel("Sample Size"));}
public PCollection<Iterable<T>> beam_f17594_0(PCollection<T> input)
{    return input.apply(Combine.globally(new FixedSizedSampleFn<>(sampleSize)));}
public Top.BoundedHeap<KV<Integer, T>, SerializableComparator<KV<Integer, T>>> beam_f17603_0(Top.BoundedHeap<KV<Integer, T>, SerializableComparator<KV<Integer, T>>> accumulator, T input)
{    accumulator.addInput(KV.of(rand.nextInt(), input));    return accumulator;}
public Top.BoundedHeap<KV<Integer, T>, SerializableComparator<KV<Integer, T>>> beam_f17604_0(Iterable<Top.BoundedHeap<KV<Integer, T>, SerializableComparator<KV<Integer, T>>>> accumulators)
{    return topCombineFn.mergeAccumulators(accumulators);}
public static SerializableFunction<InT, OutT> beam_f17613_0(@Nullable OutT value)
{    return new Constant<>(value);}
public OutputT beam_f17614_0(InputT input)
{    return fn.apply(input);}
 static ByteKey beam_f17623_0(ByteKey key)
{    return ByteKey.copyFrom(Bytes.concat(key.getBytes(), ZERO_BYTE_ARRAY));}
public double beam_f17624_0()
{        if (NO_KEYS.equals(range)) {        return 0;    }        if (lastAttemptedKey == null) {        return 1;    }        if (lastAttemptedKey.isEmpty() || !(range.getEndKey().isEmpty() || range.getEndKey().compareTo(lastAttemptedKey) > 0)) {        return 0;    }    return range.estimateFractionForKey(lastAttemptedKey);}
public static Combine.Globally<Long, Long> beam_f17633_0()
{    return Combine.globally(Sum.ofLongs());}
public static Combine.PerKey<K, Long, Long> beam_f17634_0()
{    return Combine.perKey(Sum.ofLongs());}
public int beam_f17643_0()
{    return getClass().hashCode();}
public long beam_f17644_0(long a, long b)
{    return a + b;}
public static Combine.Globally<T, List<T>> beam_f17653_0(int count)
{    return Combine.globally(new TopCombineFn<>(count, new Reversed<T>()));}
public static Combine.Globally<T, List<T>> beam_f17654_0(int count)
{    return Combine.globally(largestFn(count));}
public static PTransform<PCollection<KV<K, V>>, PCollection<KV<K, List<V>>>> beam_f17663_0(int count, ComparatorT compareFn)
{    return Combine.perKey(new TopCombineFn<>(count, compareFn));}
public static PTransform<PCollection<KV<K, V>>, PCollection<KV<K, List<V>>>> beam_f17664_0(int count)
{    return Combine.perKey(smallestFn(count));}
public void beam_f17673_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("count", count).withLabel("Top Count")).add(DisplayData.item("comparer", compareFn.getClass()).withLabel("Record Comparer"));}
public String beam_f17674_0()
{    return "Default values are not supported in Top.[of, smallest, largest]() if the input " + "PCollection is not windowed by GlobalWindows. Instead, use " + "Top.[of, smallest, largest]().withoutDefaults() to output an empty PCollection if the " + "input PCollection is empty, or Top.[of, smallest, largest]().asSingletonView() to " + "get a PCollection containing the empty list if the input PCollection is empty.";}
public boolean beam_f17683_0(BoundedHeap<T, ComparatorT> value)
{    return listCoder.isRegisterByteSizeObserverCheap(value.asList());}
public void beam_f17684_0(BoundedHeap<T, ComparatorT> value, ElementByteSizeObserver observer) throws Exception
{    listCoder.registerByteSizeObserver(value.asList(), observer);}
public String beam_f17693_0(Object input)
{    return input.toString();}
public PCollection<String> beam_f17694_0(PCollection<? extends KV<?, ?>> input)
{    return input.apply(MapElements.via(new SimpleFunction<KV<?, ?>, String>() {        @Override        public String apply(KV<?, ?> input) {            return input.getKey().toString() + delimiter + input.getValue().toString();        }    }));}
public static AsIterable<T> beam_f17703_0()
{    return new AsIterable<>();}
public static AsMap<K, V> beam_f17704_0()
{    return new AsMap<>();}
public T beam_f17713_0()
{    if (hasDefault) {        if (defaultValue == null) {            return null;        }        try {            return CoderUtils.decodeFromByteArray(valueCoder, defaultValue);        } catch (CoderException e) {            throw new IllegalArgumentException(String.format("Could not decode the default value with the provided coder %s", valueCoder));        }    } else {        throw new IllegalArgumentException("Empty PCollection accessed as a singleton view. " + "Consider setting withDefault to provide a default value");    }}
public PCollectionView<Map<K, Iterable<V>>> beam_f17714_0(PCollection<KV<K, V>> input)
{    try {        GroupByKey.applicableTo(input);    } catch (IllegalStateException e) {        throw new IllegalStateException("Unable to create a side-input view from input", e);    }    KvCoder<K, V> kvCoder = (KvCoder<K, V>) input.getCoder();    Coder<K> keyCoder = kvCoder.getKeyCoder();    Coder<V> valueCoder = kvCoder.getValueCoder();    PCollection<KV<Void, KV<K, V>>> materializationInput = input.apply(new VoidKeyToMultimapMaterialization<>());    PCollectionView<Map<K, Iterable<V>>> view = PCollectionViews.multimapView(materializationInput, (TypeDescriptorSupplier<K>) keyCoder::getEncodedTypeDescriptor, (TypeDescriptorSupplier<V>) valueCoder::getEncodedTypeDescriptor, materializationInput.getWindowingStrategy());    materializationInput.apply(CreatePCollectionView.of(view));    return view;}
public static OnSignal<T> beam_f17723_0(List<PCollection<?>> signals)
{    return new OnSignal<>(signals);}
public PCollection<T> beam_f17724_0(PCollection<T> input)
{    List<PCollectionView<?>> views = Lists.newArrayList();    for (int i = 0; i < signals.size(); ++i) {        views.add(signals.get(i).apply("To wait view " + i, new ToWaitView()));    }    return input.apply("Wait", MapElements.into(input.getCoder().getEncodedTypeDescriptor()).via(fn((t, c) -> t, requiresSideInputs(views))));}
 List<TimestampedValue<OutputT>> beam_f17733_0()
{    return outputs;}
 Instant beam_f17734_0()
{    return watermark;}
public boolean beam_f17743_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    PollResult<?> that = (PollResult<?>) o;    return Objects.equals(outputs, that.outputs) && Objects.equals(watermark, that.watermark);}
public int beam_f17744_0()
{    return Objects.hash(outputs, watermark);}
public Coder<Integer> beam_f17753_0()
{    return VarIntCoder.of();}
public Integer beam_f17754_0(Instant now, InputT input)
{    return 0;}
public Coder<KV<Instant, ReadableDuration>> beam_f17763_0()
{    return KvCoder.of(InstantCoder.of(), DurationCoder.of());}
public KV<Instant, ReadableDuration> beam_f17764_0(Instant now, InputT input)
{    return KV.of(now, maxTimeSinceInput.apply(input));}
public Coder<KV<FirstStateT, SecondStateT>> beam_f17773_0()
{    return KvCoder.of(first.getStateCoder(), second.getStateCoder());}
public KV<FirstStateT, SecondStateT> beam_f17774_0(Instant now, InputT input)
{    return KV.of(first.forNewInput(now, input), second.forNewInput(now, input));}
public void beam_f17783_0(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker)
{    long position = tracker.currentRestriction().getFrom();    while (tracker.tryClaim(position)) {        TimestampedValue<OutputT> value = c.element().getValue().get((int) position);        c.outputWithTimestamp(KV.of(c.element().getKey(), value.getValue()), value.getTimestamp());        c.updateWatermark(value.getTimestamp());        position += 1L;    }}
public OffsetRange beam_f17784_0(KV<InputT, List<TimestampedValue<OutputT>>> element)
{    return new OffsetRange(0, element.getValue().size());}
public Coder<GrowthState> beam_f17793_0()
{    return SnappyCoder.of(GrowthStateCoder.of(outputCoder, (Coder) spec.getTerminationPerInput().getStateCoder()));}
public static NonPollingGrowthState<OutputT> beam_f17794_0(Growth.PollResult<OutputT> pending)
{    return new AutoValue_Watch_NonPollingGrowthState(pending);}
public static HashCode128Coder beam_f17803_0()
{    return INSTANCE;}
public void beam_f17804_0(HashCode value, OutputStream os) throws IOException
{    checkArgument(value.bits() == 128, "Expected a 128-bit hash code, but got %s bits", value.bits());    byte[] res = new byte[16];    value.writeBytesTo(res, 0, 16);    os.write(res);}
public List<? extends Coder<?>> beam_f17813_0()
{    return Arrays.asList(outputCoder, terminationStateCoder);}
public void beam_f17814_0() throws NonDeterministicException
{    outputCoder.verifyDeterministic();}
protected Trigger beam_f17823_0(List<Trigger> continuationTriggers)
{    return Repeatedly.forever(new AfterFirst(continuationTriggers));}
public String beam_f17824_0()
{    StringBuilder builder = new StringBuilder("AfterEach.inOrder(");    Joiner.on(", ").appendTo(builder, subTriggers);    builder.append(")");    return builder.toString();}
public Instant beam_f17833_0(BoundedWindow window)
{    return BoundedWindow.TIMESTAMP_MAX_VALUE;}
protected OnceTrigger beam_f17834_0(List<Trigger> continuationTriggers)
{    return AfterPane.elementCountAtLeast(1);}
public boolean beam_f17843_0(Trigger other)
{    return this.equals(other);}
public Instant beam_f17844_0(BoundedWindow window)
{    return BoundedWindow.TIMESTAMP_MAX_VALUE;}
public boolean beam_f17853_0(Object obj)
{    return this == obj || obj instanceof AfterSynchronizedProcessingTime;}
public int beam_f17854_0()
{    return Objects.hashCode(AfterSynchronizedProcessingTime.class);}
public String beam_f17863_0()
{    StringBuilder builder = new StringBuilder(TO_STRING);    if (!(earlyTrigger instanceof Never.NeverTrigger)) {        builder.append(".withEarlyFirings(").append(earlyTrigger).append(")");    }    if (lateTrigger != null && !(lateTrigger instanceof Never.NeverTrigger)) {        builder.append(".withLateFirings(").append(lateTrigger).append(")");    }    return builder.toString();}
public AfterWatermarkEarlyAndLate beam_f17864_0(OnceTrigger earlyFirings)
{    checkNotNull(earlyFirings, "Must specify the trigger to use for early firings");    return new AfterWatermarkEarlyAndLate(earlyFirings, null);}
public static DaysWindows beam_f17873_0(int number)
{    return new DaysWindows(number, DEFAULT_START_DATE, DateTimeZone.UTC);}
public static DaysWindows beam_f17874_0(int number, int startDayOfWeek)
{    return new DaysWindows(7 * number, DEFAULT_START_DATE.withDayOfWeek(startDayOfWeek), DateTimeZone.UTC);}
public void beam_f17883_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("numDays", number).withLabel("Windows Days")).addIfNotDefault(DisplayData.item("startDate", new DateTime(startDate, timeZone).toInstant()).withLabel("Window Start Date"), new DateTime(DEFAULT_START_DATE, DateTimeZone.UTC).toInstant());}
public int beam_f17884_0()
{    return number;}
public void beam_f17893_0(WindowFn<?, ?> other) throws IncompatibleWindowException
{    if (!this.isCompatible(other)) {        throw new IncompatibleWindowException(other, String.format("Only %s objects with the same number of months, " + "day of month, start date and time zone are compatible.", MonthsWindows.class.getSimpleName()));    }}
public void beam_f17894_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("numMonths", number).withLabel("Window Months")).addIfNotDefault(DisplayData.item("startDate", new DateTime(startDate, timeZone).toInstant()).withLabel("Window Start Date"), new DateTime(DEFAULT_START_DATE, DateTimeZone.UTC).toInstant());}
public Coder<IntervalWindow> beam_f17903_0()
{    return IntervalWindow.getCoder();}
public boolean beam_f17904_0(WindowFn<?, ?> other)
{    if (!(other instanceof YearsWindows)) {        return false;    }    YearsWindows that = (YearsWindows) other;    return number == that.number && monthOfYear == that.monthOfYear && dayOfMonth == that.dayOfMonth && Objects.equals(startDate, that.startDate) && Objects.equals(timeZone, that.timeZone);}
public Instant beam_f17913_0(BoundedWindow window)
{    return window.maxTimestamp();}
public boolean beam_f17914_0(Trigger other)
{        return other instanceof DefaultTrigger;}
public Duration beam_f17923_0()
{    return size;}
public Duration beam_f17924_0()
{    return offset;}
private static Instant beam_f17935_0()
{    return new Instant(Long.parseLong(RunnerApi.BeamConstants.Constants.GLOBAL_WINDOW_MAX_TIMESTAMP_MILLIS.getValueDescriptor().getOptions().getExtension(RunnerApi.beamConstant)));}
public Collection<GlobalWindow> beam_f17936_0(AssignContext c)
{    return GLOBAL_WINDOWS;}
public int beam_f17945_0()
{        return GlobalWindows.class.hashCode();}
public String beam_f17946_0()
{    return getClass().getCanonicalName();}
public boolean beam_f17955_0(Object o)
{    return (o instanceof IntervalWindow) && ((IntervalWindow) o).end.isEqual(end) && ((IntervalWindow) o).start.isEqual(start);}
public int beam_f17956_0()
{        return (int) (start.getMillis() + modInverse((int) (end.getMillis() << 1) + 1));}
public boolean beam_f17965_0()
{    return instantCoder.consistentWithEquals() && durationCoder.consistentWithEquals();}
public List<? extends Coder<?>> beam_f17966_0()
{    return Collections.emptyList();}
public Instant beam_f17975_0(Instant inputTimestamp, W window)
{    return inputTimestamp;}
public static void beam_f17976_0(WindowFn<?, IntervalWindow>.MergeContext c) throws Exception
{                    List<IntervalWindow> sortedWindows = new ArrayList<>();    for (IntervalWindow window : c.windows()) {        sortedWindows.add(window);    }    Collections.sort(sortedWindows);    List<MergeCandidate> merges = new ArrayList<>();    MergeCandidate current = new MergeCandidate();    for (IntervalWindow window : sortedWindows) {        if (current.intersects(window)) {            current.add(window);        } else {            merges.add(current);            current = new MergeCandidate(window);        }    }    merges.add(current);    for (MergeCandidate merge : merges) {        merge.apply(c);    }}
public Trigger beam_f17986_0()
{    return subTriggers().get(ACTUAL);}
public OnceTrigger beam_f17987_0()
{    return (OnceTrigger) subTriggers().get(UNTIL);}
public boolean beam_f17996_0()
{    return Timing.UNKNOWN.equals(timing);}
public boolean beam_f17997_0()
{    return isFirst;}
public static Encoding beam_f18006_0(byte b)
{    return Encoding.values()[b >> 4];}
private Encoding beam_f18007_0(PaneInfo value)
{    if ((value.index == 0 && value.nonSpeculativeIndex == 0) || value.timing == Timing.UNKNOWN) {        return Encoding.FIRST;    } else if (value.index == value.nonSpeculativeIndex || value.timing == Timing.EARLY) {        return Encoding.ONE_INDEX;    } else {        return Encoding.TWO_INDICES;    }}
public static Repeatedly beam_f18017_0(Trigger repeated)
{    return new Repeatedly(repeated);}
public Trigger beam_f18018_0()
{    return repeatedTrigger;}
public void beam_f18027_0(MergeContext c) throws Exception
{    MergeOverlappingIntervalWindows.mergeWindows(c);}
public Coder<IntervalWindow> beam_f18028_0()
{    return IntervalWindow.getCoder();}
public static SlidingWindows beam_f18037_0(Duration size)
{    return new SlidingWindows(getDefaultPeriod(size), size, Duration.ZERO);}
public SlidingWindows beam_f18038_0(Duration period)
{    return new SlidingWindows(period, size, offset);}
public void beam_f18047_0(WindowFn<?, ?> other) throws IncompatibleWindowException
{    if (!this.isCompatible(other)) {        throw new IncompatibleWindowException(other, String.format("Only %s objects with the same size, period and offset are compatible.", SlidingWindows.class.getSimpleName()));    }}
public void beam_f18048_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("size", size).withLabel("Window Size")).add(DisplayData.item("period", period).withLabel("Window Period")).add(DisplayData.item("offset", offset).withLabel("Window Start Offset"));}
public final Instant beam_f18057_0(BoundedWindow intoWindow, Instant timestamp)
{    return merge(intoWindow, Collections.singleton(timestamp));}
public final Instant beam_f18058_0(Instant... timestamps)
{    return combine(Arrays.asList(timestamps));}
public boolean beam_f18067_0()
{    return false;}
public Instant beam_f18068_0(Iterable<? extends Instant> timestamps)
{    checkArgument(Iterables.size(timestamps) > 0);    return Iterables.get(timestamps, 0);}
public boolean beam_f18077_0(Trigger other)
{    if (!getClass().equals(other.getClass())) {        return false;    }    if (subTriggers == null) {        return other.subTriggers == null;    } else if (other.subTriggers == null) {        return false;    } else if (subTriggers.size() != other.subTriggers.size()) {        return false;    }    for (int i = 0; i < subTriggers.size(); i++) {        if (!subTriggers.get(i).isCompatible(other.subTriggers.get(i))) {            return false;        }    }    return true;}
public String beam_f18078_0()
{    String simpleName = getClass().getSimpleName();    if (getClass().getEnclosingClass() != null) {        simpleName = getClass().getEnclosingClass().getSimpleName() + "." + simpleName;    }    if (subTriggers == null || subTriggers.isEmpty()) {        return simpleName;    } else {        return simpleName + "(" + Joiner.on(", ").join(subTriggers) + ")";    }}
public Window<T> beam_f18087_0()
{    return toBuilder().setAccumulationMode(AccumulationMode.DISCARDING_FIRED_PANES).build();}
public Window<T> beam_f18088_0()
{    return toBuilder().setAccumulationMode(AccumulationMode.ACCUMULATING_FIRED_PANES).build();}
public void beam_f18097_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    if (getWindowFn() != null) {        builder.add(DisplayData.item("windowFn", getWindowFn().getClass()).withLabel("Windowing Function")).include("windowFn", getWindowFn());    }    if (getAllowedLateness() != null) {        builder.addIfNotDefault(DisplayData.item("allowedLateness", getAllowedLateness()).withLabel("Allowed Lateness"), Duration.millis(BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis()));    }    if (getTrigger() != null && !(getTrigger() instanceof DefaultTrigger)) {        builder.add(DisplayData.item("trigger", getTrigger().toString()).withLabel("Trigger"));    }    if (getAccumulationMode() != null) {        builder.add(DisplayData.item("accumulationMode", getAccumulationMode().toString()).withLabel("Accumulation Mode"));    }    if (getClosingBehavior() != null) {        builder.add(DisplayData.item("closingBehavior", getClosingBehavior().toString()).withLabel("Window Closing Behavior"));    }    if (getTimestampCombiner() != null) {        builder.add(DisplayData.item("timestampCombiner", getTimestampCombiner().toString()).withLabel("Timestamp Combiner"));    }}
protected String beam_f18098_0()
{    return "Window.Into()";}
public Instant beam_f18107_0(Instant inputTimestamp, W window)
{    return inputTimestamp;}
public boolean beam_f18108_0()
{    return false;}
public OutputT beam_f18118_0(List<PCollection<FailureElementT>> failureCollections)
{    failureCollections.add(failures());    return output();}
public Pipeline beam_f18119_0()
{    return output().getPipeline();}
public Duration beam_f18129_0()
{    return allowedTimestampSkew;}
public PCollection<T> beam_f18130_0(PCollection<T> input)
{    return input.apply("AddTimestamps", ParDo.of(new AddTimestampsDoFn<>(fn, allowedTimestampSkew)));}
protected boolean beam_f18139_0(final Class<?> clazz, final Description mismatchDescription)
{    return clazz.getName().startsWith(packageName + ".");}
private boolean beam_f18140_0(final ApiSurface checkedApiSurface, final Set<Matcher<Class<?>>> allowedClasses, final Description mismatchDescription)
{        final Function<Matcher<Class<?>>, String> toMessage = abandonedClassMacther -> {        final StringDescription description = new StringDescription();        description.appendText("No ");        abandonedClassMacther.describeTo(description);        return description.toString();    };    final Predicate<Matcher<Class<?>>> matchedByExposedClasses = classMatcher -> FluentIterable.from(checkedApiSurface.getExposedClasses()).anyMatch(classMatcher::matches);        final ImmutableSet<Matcher<Class<?>>> matchedClassMatchers = FluentIterable.from(allowedClasses).filter(matchedByExposedClasses).toSet();    final Sets.SetView<Matcher<Class<?>>> abandonedClassMatchers = Sets.difference(allowedClasses, matchedClassMatchers);    final ImmutableList<String> messages = FluentIterable.from(abandonedClassMatchers).transform(toMessage).toSortedList(Ordering.natural());    if (!messages.isEmpty()) {        mismatchDescription.appendText("The following white-listed scopes did not have matching classes on the API surface:" + "\n\t" + Joiner.on("\n\t").join(messages));    }    return messages.isEmpty();}
public ApiSurface beam_f18149_1(String packageName, ClassLoader classLoader) throws IOException
{    ClassPath classPath = ClassPath.from(classLoader);    Set<Class<?>> newRootClasses = Sets.newHashSet();    for (ClassPath.ClassInfo classInfo : classPath.getTopLevelClassesRecursive(packageName)) {        Class clazz = null;        try {            clazz = classInfo.load();        } catch (NoClassDefFoundError e) {                                    continue;        }        if (exposed(clazz.getModifiers())) {            newRootClasses.add(clazz);        }    }        newRootClasses.addAll(rootClasses);    return new ApiSurface(newRootClasses, patternsToPrune);}
public ApiSurface beam_f18150_1(Class<?> clazz)
{    Set<Class<?>> newRootClasses = Sets.newHashSet();        newRootClasses.add(clazz);    newRootClasses.addAll(rootClasses);    return new ApiSurface(newRootClasses, patternsToPrune);}
private List<Class<?>> beam_f18159_0(Class<?> exposedClass, Set<Class<?>> excluded)
{    List<Class<?>> exposurePath = Lists.newArrayList();    exposurePath.add(exposedClass);    Collection<Class<?>> exposers = getExposedToExposers().get(exposedClass);    if (exposers.isEmpty()) {        throw new IllegalArgumentException("Class " + exposedClass + " is not exposed.");    }    for (Class<?> exposer : exposers) {        if (excluded.contains(exposer)) {            continue;        }                if (exposer == null) {            return exposurePath;        }        List<Class<?>> restOfPath = getAnyExposurePath(exposer, Sets.union(excluded, Sets.newHashSet(exposer)));        if (restOfPath != null) {            exposurePath.addAll(restOfPath);            return exposurePath;        }    }    return null;}
private Multimap<Class<?>, Class<?>> beam_f18160_0()
{    if (exposedToExposers == null) {        constructExposedToExposers();    }    return exposedToExposers;}
private void beam_f18169_0(Type type)
{    visited.add(type);}
private void beam_f18170_1(TypeToken type, Class<?> cause)
{        addExposedTypes(type.getType(), cause);}
private void beam_f18179_1(Field field, Class<?> cause)
{    addExposedTypes(field.getGenericType(), cause);    for (Annotation annotation : field.getDeclaredAnnotations()) {                addExposedTypes(annotation.annotationType(), cause);    }}
private Set<Invokable> beam_f18180_0(TypeToken<?> type)
{    Set<Invokable> invokables = Sets.newHashSet();    for (Constructor constructor : type.getRawType().getConstructors()) {        if (0 != (constructor.getModifiers() & (Modifier.PUBLIC | Modifier.PROTECTED))) {            invokables.add(type.constructor(constructor));        }    }    for (Method method : type.getRawType().getMethods()) {        if (0 != (method.getModifiers() & (Modifier.PUBLIC | Modifier.PROTECTED))) {            invokables.add(type.method(method));        }    }    return invokables;}
public Coder<AccumT> beam_f18189_0()
{    return accumulatorCoder;}
public KvCoder<K, InputT> beam_f18190_0()
{    return kvCoder;}
public BitSet beam_f18201_0(InputStream inStream, Context context) throws CoderException, IOException
{    return BitSet.valueOf(BYTE_ARRAY_CODER.decode(inStream, context));}
public void beam_f18202_0() throws NonDeterministicException
{    BYTE_ARRAY_CODER.verifyDeterministic();}
public void beam_f18211_0() throws IOException
{    if (finished) {        return;    }    flush();        VarInt.encode(0, os);    if (!BUFFER_POOL.offer(buffer)) {        }    finished = true;}
public void beam_f18212_0() throws IOException
{    if (finished) {        throw new IOException("Stream has been finished. Can not add any more elements.");    }    count++;}
public ImmutableSet<ClassInfo> beam_f18221_0()
{    return FluentIterable.from(resources).filter(ClassInfo.class).filter(IS_TOP_LEVEL).toSet();}
public ImmutableSet<ClassInfo> beam_f18222_0(String packageName)
{    checkNotNull(packageName);    ImmutableSet.Builder<ClassInfo> builder = ImmutableSet.builder();    for (ClassInfo classInfo : getTopLevelClasses()) {        if (classInfo.getPackageName().equals(packageName)) {            builder.add(classInfo);        }    }    return builder.build();}
public String beam_f18231_0()
{    return resourceName;}
public String beam_f18232_0()
{    return Reflection.getPackageName(className);}
 static ImmutableSet<File> beam_f18241_1(File jarFile, @Nullable Manifest manifest)
{    if (manifest == null) {        return ImmutableSet.of();    }    ImmutableSet.Builder<File> builder = ImmutableSet.builder();    String classpathAttribute = manifest.getMainAttributes().getValue(Attributes.Name.CLASS_PATH.toString());    if (classpathAttribute != null) {        for (String path : CLASS_PATH_ATTRIBUTE_SEPARATOR.split(classpathAttribute)) {            URL url;            try {                url = getClassPathEntry(jarFile, path);            } catch (MalformedURLException e) {                                                continue;            }            if ("file".equals(url.getProtocol())) {                builder.add(toFile(url));            }        }    }    return builder.build();}
 static ImmutableMap<File, ClassLoader> beam_f18242_0(ClassLoader classloader)
{    LinkedHashMap<File, ClassLoader> entries = Maps.newLinkedHashMap();        ClassLoader parent = classloader.getParent();    if (parent != null) {        entries.putAll(getClassPathEntries(parent));    }    if (classloader instanceof URLClassLoader) {        URLClassLoader urlClassLoader = (URLClassLoader) classloader;        for (URL entry : urlClassLoader.getURLs()) {            if ("file".equals(entry.getProtocol())) {                File file = toFile(entry);                if (!entries.containsKey(file)) {                    entries.put(file, classloader);                }            }        }    }    return ImmutableMap.copyOf(entries);}
public static byte[] beam_f18251_0(Coder<T> coder, T value, Coder.Context context) throws CoderException
{    if (threadLocalOutputStreamInUse.get()) {                        ByteArrayOutputStream stream = new ExposedByteArrayOutputStream();        encodeToSafeStream(coder, value, stream, context);        return stream.toByteArray();    } else {        threadLocalOutputStreamInUse.set(true);        try {            ByteArrayOutputStream stream = getThreadLocalOutputStream();            encodeToSafeStream(coder, value, stream, context);            return stream.toByteArray();        } finally {            threadLocalOutputStreamInUse.set(false);        }    }}
private static void beam_f18252_0(Coder<T> coder, T value, OutputStream stream, Coder.Context context) throws CoderException
{    try {        coder.encode(value, new UnownedOutputStream(stream), context);    } catch (IOException exn) {        Throwables.propagateIfPossible(exn, CoderException.class);        throw new IllegalArgumentException("Forbidden IOException when writing to OutputStream", exn);    }}
public PipelineOptions beam_f18261_0()
{    throw new IllegalArgumentException("cannot call getPipelineOptions() in a null context");}
public T beam_f18262_0(PCollectionView<T> view)
{    throw new IllegalArgumentException("cannot call sideInput() in a null context");}
public AccumT beam_f18271_0(Iterable<AccumT> accumulators, Context c)
{    return combineFn.mergeAccumulators(accumulators);}
public OutputT beam_f18272_0(AccumT accumulator, Context c)
{    return combineFn.extractOutput(accumulator);}
public OutputT beam_f18281_0(AccumT accumulator)
{    return combineFn.extractOutput(accumulator, context);}
public AccumT beam_f18282_0(AccumT accumulator)
{    return combineFn.compact(accumulator, context);}
public boolean beam_f18291_0()
{    return isLazy;}
public void beam_f18292_0(Object obj)
{    update(null, obj);}
private void beam_f18301_0(StringBuilder builder, TypeVariable<?> t)
{    builder.append(t.getName());}
private void beam_f18302_0(StringBuilder builder, WildcardType t)
{    builder.append("?");    for (Type lowerBound : t.getLowerBounds()) {        builder.append(" super ");        format(builder, lowerBound);    }    for (Type upperBound : t.getUpperBounds()) {        if (!Object.class.equals(upperBound)) {            builder.append(" extends ");            format(builder, upperBound);        }    }}
public static ClassLoader beam_f18311_0(final Class<?>... classes)
{    if (classes == null || classes.length == 0) {        throw new IllegalArgumentException("set of classes can't be null");    }    ClassLoader current = null;    for (final Class<?> clazz : classes) {        final ClassLoader proposed = clazz.getClassLoader();        if (proposed == null) {            continue;        }        if (current == null) {            current = proposed;        } else if (proposed != current && isParent(current, proposed)) {            current = proposed;        }    }    return current == null ? ClassLoader.getSystemClassLoader() : current;}
public static ClassLoader beam_f18312_0()
{    return findClassLoader(Thread.currentThread().getContextClassLoader());}
public Map<TupleTag<?>, Coder<?>> beam_f18321_0()
{    return outputCoders;}
public TupleTag<OutputT> beam_f18322_0()
{    return mainOutput;}
public void beam_f18334_0()
{    try {        super.close();    } catch (IOException exn) {        throw new RuntimeException("Unexpected IOException closing ByteArrayInputStream", exn);    }}
private void beam_f18335_0()
{    isFallback = true;    if (swappedBuffer != null) {                                byte[] tempBuffer = buf;        count = 0;        buf = swappedBuffer;        super.write(tempBuffer, 0, tempBuffer.length);        swappedBuffer = null;    }}
 List<String> beam_f18344_1(Collection<Metadata> files) throws IOException
{    List<String> allLines = Lists.newArrayList();    int i = 1;    for (Metadata file : files) {        try (Reader reader = Channels.newReader(FileSystems.open(file.resourceId()), StandardCharsets.UTF_8.name())) {            List<String> lines = CharStreams.readLines(reader);            allLines.addAll(lines);                    }        i++;    }    return allLines;}
public BackOff beam_f18345_0()
{    return new BackoffImpl(this);}
public String beam_f18354_0()
{    return MoreObjects.toStringHelper(BackoffImpl.class).add("backoffConfig", backoffConfig).add("currentRetry", currentRetry).add("currentCumulativeBackoff", currentCumulativeBackoff).toString();}
public Collection<BoundedWindow> beam_f18355_0(WindowFn<T, BoundedWindow>.AssignContext c) throws Exception
{        return Collections.singleton(c.window());}
public static InstanceBuilder<T> beam_f18364_0(Class<T> type)
{    return new InstanceBuilder<>(type);}
public static InstanceBuilder<T> beam_f18365_0(TypeDescriptor<T> token)
{    @SuppressWarnings("unchecked")    Class<T> type = (Class<T>) token.getRawType();    return new InstanceBuilder<>(type);}
public static Row beam_f18374_0(ObjectMapper objectMapper, String jsonString)
{    try {        return objectMapper.readValue(jsonString, Row.class);    } catch (JsonParseException | JsonMappingException jsonException) {        throw new UnsupportedRowJsonException("Unable to parse Row", jsonException);    } catch (IOException e) {        throw new IllegalArgumentException("Unable to parse json object: " + jsonString, e);    }}
public static T beam_f18375_0(CompletionStage<T> future) throws InterruptedException, ExecutionException
{    return future.toCompletableFuture().get();}
public static ExceptionOrResult<T> beam_f18384_0(Throwable throwable)
{    return new AutoValue_MoreFutures_ExceptionOrResult(IsException.EXCEPTION, null, throwable);}
public static ExceptionOrResult<T> beam_f18385_0(T result)
{    return new AutoValue_MoreFutures_ExceptionOrResult(IsException.EXCEPTION, result, null);}
public void beam_f18396_0()
{    try {        verifyUnmodifiedThrowingCheckedExceptions();    } catch (CoderException exn) {        throw new RuntimeException(exn);    }}
private void beam_f18397_1() throws CoderException
{                    T possiblyModifiedClonedValue = CoderUtils.clone(coder, possiblyModifiedObject);    Object newStructuralValue = coder.structuralValue(possiblyModifiedClonedValue);    if (originalStructuralValue.equals(newStructuralValue)) {        return;    } else if (Objects.deepEquals(encodedOriginalObject, CoderUtils.encodeToByteArray(coder, possiblyModifiedObject))) {                return;    }    illegalMutation(clonedOriginalObject, possiblyModifiedClonedValue);}
public String beam_f18406_0()
{    return filePattern;}
public List<String> beam_f18407_1(Sleeper sleeper, BackOff backOff) throws IOException, InterruptedException
{    IOException lastException = null;    do {        try {                        Collection<Metadata> files = Iterables.getOnlyElement(FileSystems.match(Collections.singletonList(filePattern))).metadata();                        if (files.isEmpty() || !checkTotalNumOfFiles(files)) {                continue;            }                        return readLines(files);        } catch (IOException e) {                        lastException = e;                    }    } while (BackOffUtils.next(sleeper, backOff));        throw new IOException(String.format("Unable to read file(s) after retrying %d times", MAX_READ_RETRIES), lastException);}
public Row beam_f18416_0(JsonParser jsonParser, DeserializationContext deserializationContext) throws IOException
{        return (Row) extractJsonNodeValue(FieldValue.of("root", FieldType.row(schema), jsonParser.readValueAsTree()));}
private static Object beam_f18417_0(FieldValue fieldValue)
{    if (!fieldValue.isJsonValuePresent()) {        throw new UnsupportedRowJsonException("Field '" + fieldValue.name() + "' is not present in the JSON object");    }    if (fieldValue.isJsonNull()) {        return null;    }    if (fieldValue.isRowType()) {        return jsonObjectToRow(fieldValue);    }    if (fieldValue.isArrayType()) {        return jsonArrayToList(fieldValue);    }    return extractJsonPrimitiveValue(fieldValue);}
 Stream<JsonNode> beam_f18426_0()
{    return StreamSupport.stream(jsonValue().spliterator(), SEQUENTIAL);}
 boolean beam_f18427_0()
{    return TypeName.ARRAY.equals(type().getTypeName());}
 static ValueExtractor<Byte> beam_f18436_0()
{    return ValidatingValueExtractor.<Byte>builder().setExtractor(jsonNode -> (byte) jsonNode.intValue()).setValidator(jsonNode -> jsonNode.isIntegralNumber() && jsonNode.canConvertToInt() && jsonNode.intValue() >= Byte.MIN_VALUE && jsonNode.intValue() <= Byte.MAX_VALUE).build();}
 static ValueExtractor<Short> beam_f18437_0()
{    return ValidatingValueExtractor.<Short>builder().setExtractor(jsonNode -> (short) jsonNode.intValue()).setValidator(jsonNode -> jsonNode.isIntegralNumber() && jsonNode.canConvertToInt() && jsonNode.intValue() >= Short.MIN_VALUE && jsonNode.intValue() <= Short.MAX_VALUE).build();}
public W beam_f18446_0(JsonNode value)
{    if (!validator().test(value)) {        throw new UnsupportedRowJsonException("Value \"" + value.asText() + "\" " + "is out of range for the type of the field defined in the row schema.");    }    return extractor().apply(value);}
public Throwable beam_f18447_0()
{    return throwable;}
public static T beam_f18456_0(Coder<T> coder, T value, String errorContext)
{    byte[] encodedValue;    try {        encodedValue = encodeToByteArray(coder, value);    } catch (CoderException exn) {                throw new IllegalArgumentException(errorContext + ": unable to encode value " + value + " using " + coder, exn);    }    try {        return decodeFromByteArray(coder, encodedValue);    } catch (CoderException exn) {                throw new IllegalArgumentException(errorContext + ": unable to decode " + Arrays.toString(encodedValue) + ", encoding of value " + value + ", using " + coder, exn);    }}
protected Class<?> beam_f18457_0(final ObjectStreamClass classDesc) throws IOException, ClassNotFoundException
{        final String n = classDesc.getName();    final ClassLoader classloader = ReflectHelpers.findClassLoader();    try {        return Class.forName(n, false, classloader);    } catch (final ClassNotFoundException e) {        return super.resolveClass(classDesc);    }}
public static byte[] beam_f18466_0(InputStream stream) throws IOException
{    if (stream instanceof ExposedByteArrayInputStream) {                return ((ExposedByteArrayInputStream) stream).readAll();    } else if (stream instanceof ByteArrayInputStream) {                byte[] ret = new byte[stream.available()];        stream.read(ret);        return ret;    }        SoftReference<byte[]> refBuffer = threadLocalBuffer.get();    byte[] buffer = refBuffer == null ? null : refBuffer.get();    if (buffer == null) {        buffer = new byte[BUF_SIZE];        threadLocalBuffer.set(new SoftReference<>(buffer));    }    ByteArrayOutputStream outStream = new ByteArrayOutputStream();    while (true) {        int r = stream.read(buffer);        if (r == -1) {            break;        }        outStream.write(buffer, 0, r);    }    return outStream.toByteArray();}
public static String beam_f18467_0(byte[] bytes)
{    StringBuilder sb = new StringBuilder(bytes.length * 2);    for (byte b : bytes) {        if (b >= 32 && b < 127) {                        char c = (char) b;            if (c != '%' && c != '\\' && c != '\"') {                                                sb.append(c);                continue;            }        }                sb.append(String.format("%%%02x", b));    }    return sb.toString();}
public String beam_f18476_0()
{    return MoreObjects.toStringHelper(UnownedInputStream.class).add("in", in).toString();}
public void beam_f18477_0() throws IOException
{    throw new UnsupportedOperationException("Caller does not own the underlying output stream " + " and should not call close().");}
public static void beam_f18486_0(int v, OutputStream stream) throws IOException
{    encode(convertIntToLongNoSignExtend(v), stream);}
public static void beam_f18487_0(long v, OutputStream stream) throws IOException
{    do {                long bits = v & 0x7F;        v >>>= 7;        byte b = (byte) (bits | ((v != 0) ? 0x80 : 0));        stream.write(b);    } while (v != 0);}
 static WindowedValue<T> beam_f18496_0(T value, Instant timestamp, Collection<? extends BoundedWindow> windows, PaneInfo pane)
{    if (windows.size() == 1) {        return of(value, timestamp, windows.iterator().next(), pane);    } else {        return new TimestampedValueInMultipleWindows<>(value, timestamp, windows, pane);    }}
public static WindowedValue<T> beam_f18497_0(T value, Instant timestamp, BoundedWindow window, PaneInfo pane)
{    checkArgument(pane != null, "WindowedValue requires PaneInfo, but it was null");    boolean isGlobal = GlobalWindow.INSTANCE.equals(window);    if (isGlobal && BoundedWindow.TIMESTAMP_MIN_VALUE.equals(timestamp)) {        return valueInGlobalWindow(value, pane);    } else if (isGlobal) {        return new TimestampedValueInGlobalWindow<>(value, timestamp, pane);    } else {        return new TimestampedValueInSingleWindow<>(value, timestamp, window, pane);    }}
public T beam_f18506_0()
{    return value;}
public Instant beam_f18507_0()
{    return BoundedWindow.TIMESTAMP_MIN_VALUE;}
public WindowedValue<NewT> beam_f18516_0(NewT newValue)
{    return new TimestampedValueInGlobalWindow<>(newValue, getTimestamp(), getPane());}
public Collection<? extends BoundedWindow> beam_f18517_0()
{    return GLOBAL_WINDOWS;}
public BoundedWindow beam_f18526_0()
{    return window;}
public boolean beam_f18527_0(Object o)
{    if (o instanceof TimestampedValueInSingleWindow) {        TimestampedValueInSingleWindow<?> that = (TimestampedValueInSingleWindow<?>) o;                return this.getTimestamp().isEqual(that.getTimestamp()) && Objects.equals(that.getValue(), this.getValue()) && Objects.equals(that.getPane(), this.getPane()) && Objects.equals(that.window, this.window);    } else {        return super.equals(o);    }}
public static FullWindowedValueCoder<T> beam_f18536_0(Coder<T> valueCoder, Coder<? extends BoundedWindow> windowCoder)
{    return FullWindowedValueCoder.of(valueCoder, windowCoder);}
public static ValueOnlyWindowedValueCoder<T> beam_f18537_0(Coder<T> valueCoder)
{    return ValueOnlyWindowedValueCoder.of(valueCoder);}
public WindowedValue<T> beam_f18546_0(InputStream inStream, Context context) throws CoderException, IOException
{    Instant timestamp = InstantCoder.of().decode(inStream);    Collection<? extends BoundedWindow> windows = windowsCoder.decode(inStream);    PaneInfo pane = PaneInfoCoder.INSTANCE.decode(inStream);    T value = valueCoder.decode(inStream, context);        return WindowedValue.createWithoutValidation(value, timestamp, windows, pane);}
public void beam_f18547_0() throws NonDeterministicException
{    verifyDeterministic(this, "FullWindowedValueCoder requires a deterministic valueCoder", valueCoder);    verifyDeterministic(this, "FullWindowedValueCoder requires a deterministic windowCoder", windowCoder);}
public WindowedValue<T> beam_f18556_0(InputStream inStream, Context context) throws CoderException, IOException
{    T value = valueCoder.decode(inStream, context);    return WindowedValue.valueInGlobalWindow(value);}
public void beam_f18557_0() throws NonDeterministicException
{    verifyDeterministic(this, "ValueOnlyWindowedValueCoder requires a deterministic valueCoder", valueCoder);}
 static FluentIterable<ZipEntry> beam_f18566_0(final ZipFile file)
{    checkNotNull(file);    return new FluentIterable<ZipEntry>() {        @Override        public Iterator<ZipEntry> iterator() {            return (Iterator<ZipEntry>) Iterators.forEnumeration(file.entries());        }    };}
public Iterator<ZipEntry> beam_f18567_0()
{    return (Iterator<ZipEntry>) Iterators.forEnumeration(file.entries());}
public boolean beam_f18576_0(Object other)
{    if (this == other) {        return true;    }    if (!(other instanceof KV)) {        return false;    }    KV<?, ?> otherKv = (KV<?, ?>) other;        return Objects.deepEquals(this.key, otherKv.key) && Objects.deepEquals(this.value, otherKv.value);}
public int beam_f18577_0(KV<K, V> a, KV<K, V> b)
{    if (a.key == null) {        return b.key == null ? 0 : -1;    } else if (b.key == null) {        return 1;    } else {        return a.key.compareTo(b.key);    }}
public void beam_f18586_0(String transformName, PInput input, PTransform<?, ?> transform)
{    this.coderOrFailure = inferCoderOrFail(input, transform, getPipeline().getCoderRegistry(), getPipeline().getSchemaRegistry());    super.finishSpecifyingOutput(transformName, input, transform);}
public void beam_f18587_0(PInput input, PTransform<?, ?> transform)
{    if (isFinishedSpecifying()) {        return;    }    this.coderOrFailure = inferCoderOrFail(input, transform, getPipeline().getCoderRegistry(), getPipeline().getSchemaRegistry());            getCoder();    super.finishSpecifying(input, transform);}
public PCollection<T> beam_f18596_0(Schema schema)
{    return setSchema(schema, (SerializableFunction<T, Row>) SerializableFunctions.<Row>identity(), (SerializableFunction<Row, T>) SerializableFunctions.<Row>identity());}
public PCollection<T> beam_f18597_0(Schema schema, SerializableFunction<T, Row> toRowFunction, SerializableFunction<Row, T> fromRowFunction)
{    return setCoder(SchemaCoder.of(schema, toRowFunction, fromRowFunction));}
public PCollection<T> beam_f18606_0(TypeDescriptor<T> typeDescriptor)
{    this.typeDescriptor = typeDescriptor;    return this;}
public PCollection<T> beam_f18607_0(WindowingStrategy<?, ?> windowingStrategy)
{    this.windowingStrategy = windowingStrategy;    return this;}
public int beam_f18616_0()
{    return pcollections.size();}
public PCollection<T> beam_f18617_0(int index)
{        @SuppressWarnings("unchecked")    PCollection<T> value = (PCollection<T>) pcollections.get(index).getValue();    return value;}
public static PCollectionTuple beam_f18626_0(Pipeline pipeline)
{    return new PCollectionTuple(pipeline);}
public static PCollectionTuple beam_f18627_0(TupleTag<T> tag, PCollection<T> pc)
{    return empty(pc.getPipeline()).and(tag, pc);}
public boolean beam_f18636_0(String tag)
{    return has(new TupleTag<>(tag));}
public PCollection<T> beam_f18637_0(TupleTag<T> tag)
{    @SuppressWarnings("unchecked")    PCollection<T> pcollection = (PCollection<T>) pcollectionMap.get(tag);    if (pcollection == null) {        throw new IllegalArgumentException("TupleTag not found in this PCollectionTuple tuple");    }    return pcollection;}
public boolean beam_f18646_0(Object other)
{    if (!(other instanceof PCollectionTuple)) {        return false;    }    PCollectionTuple that = (PCollectionTuple) other;    return this.pipeline.equals(that.pipeline) && this.pcollectionMap.equals(that.pcollectionMap);}
public int beam_f18647_0()
{    return Objects.hash(this.pipeline, this.pcollectionMap);}
public Materialization<MultimapView<Void, T>> beam_f18656_0()
{    return Materializations.multimap();}
public T beam_f18657_0(MultimapView<Void, T> primitiveViewT)
{    try {        return Iterables.getOnlyElement(primitiveViewT.get(null));    } catch (NoSuchElementException exc) {        return getDefaultValue();    } catch (IllegalArgumentException exc) {        throw new IllegalArgumentException("PCollection with more than one element accessed as a singleton view.");    }}
public int beam_f18666_0()
{    return ListViewFn.class.hashCode();}
public Materialization<MultimapView<Void, KV<K, V>>> beam_f18667_0()
{    return Materializations.multimap();}
public TupleTag<?> beam_f18676_0()
{    return tag;}
public WindowingStrategy<?, ?> beam_f18677_0()
{    return windowingStrategy;}
public String beam_f18687_0()
{    if (name == null) {        throw new IllegalStateException("name not set");    }    return name;}
public PValueBase beam_f18688_0(String name)
{    checkState(!finishedSpecifying, "cannot change the name of %s once it's been used", this);    this.name = name;    return this;}
public Byte beam_f18697_0(String fieldName)
{    return getByte(getSchema().indexOf(fieldName));}
public byte[] beam_f18698_0(String fieldName)
{    return getBytes(getSchema().indexOf(fieldName));}
public Boolean beam_f18707_0(String fieldName)
{    return getBoolean(getSchema().indexOf(fieldName));}
public List<T> beam_f18708_0(String fieldName)
{    return getArray(getSchema().indexOf(fieldName));}
public Long beam_f18717_0(int idx)
{    return getValue(idx);}
public String beam_f18718_0(int idx)
{    return getValue(idx);}
public int beam_f18727_0()
{    int h = 1;    for (int i = 0; i < getFieldCount(); i++) {        h = 31 * h + Equals.deepHashCode(getValue(i), getSchema().getField(i).getType());    }    return h;}
 static boolean beam_f18728_0(Object a, Object b, Schema.FieldType fieldType)
{    if (a == null || b == null) {        return a == b;    } else if (fieldType.getTypeName() == TypeName.LOGICAL_TYPE) {        return deepEquals(a, b, fieldType.getLogicalType().getBaseType());    } else if (fieldType.getTypeName() == Schema.TypeName.BYTES) {        return Arrays.equals((byte[]) a, (byte[]) b);    } else if (fieldType.getTypeName() == Schema.TypeName.ARRAY) {        return deepEqualsForList((List<Object>) a, (List<Object>) b, fieldType.getCollectionElementType());    } else if (fieldType.getTypeName() == Schema.TypeName.MAP) {        return deepEqualsForMap((Map<Object, Object>) a, (Map<Object, Object>) b, fieldType.getMapValueType());    } else {        return Objects.equals(a, b);    }}
public Schema beam_f18737_0()
{    return schema;}
public Builder beam_f18738_0(@Nullable Object values)
{    this.values.add(values);    return this;}
private Object beam_f18747_0(Object value, LogicalType logicalType, String fieldName)
{    return verify(logicalType.toBaseType(value), logicalType.getBaseType(), fieldName);}
private List<Object> beam_f18748_0(Object value, FieldType collectionElementType, String fieldName)
{    boolean collectionElementTypeNullable = collectionElementType.getNullable();    if (!(value instanceof List)) {        throw new IllegalArgumentException(String.format("For field name %s and array type expected List class. Instead " + "class type was %s.", fieldName, value.getClass()));    }    List<Object> valueList = (List<Object>) value;    List<Object> verifiedList = Lists.newArrayListWithCapacity(valueList.size());    for (Object listValue : valueList) {        if (listValue == null) {            if (!collectionElementTypeNullable) {                throw new IllegalArgumentException(String.format("%s is not nullable in Array field %s", collectionElementType, fieldName));            }            verifiedList.add(null);        } else {            verifiedList.add(verify(listValue, collectionElementType, fieldName));        }    }    return verifiedList;}
private List beam_f18757_0(FieldType elementType, Object fieldValue)
{    Iterable iterable = (Iterable) fieldValue;    List<Object> list = Lists.newArrayList();    for (Object o : iterable) {        list.add(getValue(elementType, o, null));    }    return list;}
private Map<?, ?> beam_f18758_0(FieldType keyType, FieldType valueType, Map<?, ?> fieldValue)
{    Map returnMap = Maps.newHashMap();    for (Map.Entry<?, ?> entry : fieldValue.entrySet()) {        returnMap.put(getValue(keyType, entry.getKey(), null), getValue(valueType, entry.getValue(), null));    }    return returnMap;}
public List<Object> beam_f18767_0()
{    return values;}
public int beam_f18768_0()
{    return values.size();}
public static TimestampedValue<V> beam_f18777_0(@Nullable V value)
{    return of(value, BoundedWindow.TIMESTAMP_MIN_VALUE);}
public static TimestampedValue<V> beam_f18778_0(@Nullable V value, Instant timestamp)
{    return new TimestampedValue<>(value, timestamp);}
public TimestampedValue<T> beam_f18787_0(InputStream inStream) throws IOException
{    T value = valueCoder.decode(inStream);    Instant timestamp = InstantCoder.of().decode(inStream);    return TimestampedValue.of(value, timestamp);}
public void beam_f18788_0() throws NonDeterministicException
{    verifyDeterministic(this, "TimestampedValueCoder requires a deterministic valueCoder", valueCoder);}
public boolean beam_f18797_0(Object that)
{    if (that instanceof TupleTag) {        return this.id.equals(((TupleTag<?>) that).id);    } else {        return false;    }}
public int beam_f18798_0()
{    return id.hashCode();}
public List<TupleTag<?>> beam_f18807_0()
{    return tupleTags;}
public String beam_f18808_0()
{    return MoreObjects.toStringHelper(TupleTagList.class).add("tupleTags", tupleTags).toString();}
public final boolean beam_f18817_0()
{    return token.isArray();}
public final TypeVariable<Class<? super T>> beam_f18818_0(String paramName)
{                Class<?> rawType = getRawType();    for (TypeVariable<?> param : rawType.getTypeParameters()) {        if (param.getName().equals(paramName)) {            @SuppressWarnings("unchecked")            TypeVariable<Class<? super T>> typedParam = (TypeVariable<Class<? super T>>) param;            return typedParam;        }    }    throw new IllegalArgumentException("No type parameter named " + paramName + " found on " + getRawType());}
public TypeDescriptor<T> beam_f18827_0(Type formal, Type actual)
{    TypeResolver resolver = new TypeResolver().where(formal, actual);    return (TypeDescriptor<T>) TypeDescriptor.of(resolver.resolveType(token.getType()));}
public boolean beam_f18828_0()
{    return hasUnresolvedParameters(getType());}
public static TypeDescriptor<Short> beam_f18837_0()
{    return new TypeDescriptor<Short>() {    };}
public static TypeDescriptor<BigDecimal> beam_f18838_0()
{    return new TypeDescriptor<BigDecimal>() {    };}
public static TypeDescriptor<Map<K, V>> beam_f18847_0(TypeDescriptor<K> keyType, TypeDescriptor<V> valueType)
{    return new TypeDescriptor<Map<K, V>>() {    }.where(new TypeParameter<K>() {    }, keyType).where(new TypeParameter<V>() {    }, valueType);}
public static TypeDescriptor<List<T>> beam_f18848_0(TypeDescriptor<T> element)
{    return new TypeDescriptor<List<T>>() {    }.where(new TypeParameter<T>() {    }, element);}
public static TypeDescriptor<InputT> beam_f18857_0(Contextful.Fn<InputT, OutputT> fn)
{    return TypeDescriptors.extractFromTypeParameters(fn, Contextful.Fn.class, new TypeDescriptors.TypeVariableExtractor<Contextful.Fn<InputT, OutputT>, InputT>() {    });}
public static TypeDescriptor<OutputT> beam_f18858_0(Contextful.Fn<InputT, OutputT> fn)
{    return TypeDescriptors.extractFromTypeParameters(fn, Contextful.Fn.class, new TypeDescriptors.TypeVariableExtractor<Contextful.Fn<InputT, OutputT>, OutputT>() {    });}
public ValueInSingleWindow<T> beam_f18867_0(InputStream inStream, Context context) throws IOException
{    Instant timestamp = InstantCoder.of().decode(inStream);    BoundedWindow window = windowCoder.decode(inStream);    PaneInfo pane = PaneInfo.PaneInfoCoder.INSTANCE.decode(inStream);    T value = valueCoder.decode(inStream, context);    return new AutoValue_ValueInSingleWindow<>(value, timestamp, window, pane);}
public List<? extends org.apache.beam.sdk.coders.Coder<?>> beam_f18868_0()
{        return ImmutableList.of(valueCoder);}
public List<? extends Coder<?>> beam_f18877_0()
{    return Arrays.asList(valueCoder);}
public void beam_f18878_0(ValueWithRecordId<ValueT> value, OutputStream outStream) throws IOException
{    encode(value, outStream, Context.NESTED);}
public WindowFn<T, W> beam_f18887_0()
{    return windowFn;}
public Trigger beam_f18888_0()
{    return trigger;}
public boolean beam_f18897_0()
{    return timestampCombinerSpecified;}
public WindowingStrategy<T, W> beam_f18898_0(Trigger trigger)
{    return new WindowingStrategy<>(windowFn, trigger, true, mode, modeSpecified, allowedLateness, allowedLatenessSpecified, timestampCombiner, timestampCombinerSpecified, closingBehavior, onTimeBehavior);}
public int beam_f18907_0()
{    return Objects.hash(allowedLatenessSpecified, modeSpecified, timestampCombinerSpecified, mode, allowedLateness, closingBehavior, trigger, timestampCombiner, windowFn);}
public WindowingStrategy<T, W> beam_f18908_0()
{    return new WindowingStrategy<>(windowFn, trigger, true, mode, true, allowedLateness, true, timestampCombiner, true, closingBehavior, onTimeBehavior);}
public void beam_f18917_0() throws Exception
{    Pojo value = new Pojo("Hello", 42);    AvroCoder<Pojo> coder = AvroCoder.of(Pojo.class);    CoderProperties.coderDecodeEncodeEqual(coder, value);}
public void beam_f18918_0() throws Exception
{    String schemaString = "{\"namespace\": \"example.avro\",\n" + " \"type\": \"record\",\n" + " \"name\": \"User\",\n" + " \"fields\": [\n" + "     {\"name\": \"name\", \"type\": \"string\"},\n" + "     {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n" + "     {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n" + " ]\n" + "}";    Schema schema = (new Schema.Parser()).parse(schemaString);    GenericRecord before = new GenericData.Record(schema);    before.put("name", "Bob");    before.put("favorite_number", 256);        AvroCoder<GenericRecord> coder = AvroCoder.of(GenericRecord.class, schema);    CoderProperties.coderDecodeEncodeEqual(coder, before);    Assert.assertEquals(schema, coder.getSchema());}
private Matcher<String> beam_f18927_0(final String prefix, final String messagePart)
{    return new TypeSafeMatcher<String>(String.class) {        @Override        public void describeTo(Description description) {            description.appendText(String.format("Reason starting with '%s:' containing '%s'", prefix, messagePart));        }        @Override        protected boolean matchesSafely(String item) {            return item.startsWith(prefix + ":") && item.contains(messagePart);        }    };}
public void beam_f18928_0(Description description)
{    description.appendText(String.format("Reason starting with '%s:' containing '%s'", prefix, messagePart));}
public void beam_f18937_0()
{    assertDeterministic(AvroCoder.of(StringSortedMapField.class));}
public void beam_f18938_0()
{        assertNonDeterministic(AvroCoder.of(TreeMapNonDetValue.class), reasonField(UnorderedMapClass.class, "mapField", "java.util.Map<java.lang.String, java.lang.String> " + "may not be deterministically ordered"));}
public void beam_f18947_0()
{    assertDeterministic(AvroCoder.of(HasGenericRecord.class));}
public void beam_f18948_0()
{    assertNonDeterministic(AvroCoder.of(HasCustomSchema.class), reasonField(HasCustomSchema.class, "withCustomSchema", "Custom schemas are only supported for subtypes of IndexedRecord."));}
public boolean beam_f18957_0(Object other)
{    return other instanceof GenericWithAnnotation && onlySomeTypesAllowed.equals(((GenericWithAnnotation<?>) other).onlySomeTypesAllowed);}
public int beam_f18958_0()
{    return Objects.hash(getClass(), onlySomeTypesAllowed);}
public void beam_f18967_0() throws Exception
{    thrown.expect(NullPointerException.class);    thrown.expectMessage("cannot encode a null BigDecimal");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f18968_0() throws Exception
{    for (Integer value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f18977_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f18978_0() throws Exception
{    TestElementByteSizeObserver observer = new TestElementByteSizeObserver();    for (BigInteger value : TEST_VALUES) {        TEST_CODER.registerByteSizeObserver(value, observer);        observer.advance();        assertThat(observer.getSumAndReset(), equalTo((long) CoderUtils.encodeToByteArray(TEST_CODER, value, Coder.Context.NESTED).length));    }}
public void beam_f18987_0() throws Exception
{    CoderProperties.testByteCount(ByteArrayCoder.of(), Coder.Context.OUTER, new byte[][] { { 0xa, 0xb, 0xc } });    CoderProperties.testByteCount(ByteArrayCoder.of(), Coder.Context.NESTED, new byte[][] { { 0xa, 0xb, 0xc }, {}, {}, { 0xd, 0xe }, {} });}
public void beam_f18988_0() throws Exception
{        for (byte[] value1 : TEST_VALUES) {        for (byte[] value2 : TEST_VALUES) {            CoderProperties.structuralValueConsistentWithEquals(TEST_CODER, value1, value2);        }    }}
public void beam_f18997_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f18998_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null Byte");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f19007_0() throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    TypeDescriptor<List<Integer>> listToken = new TypeDescriptor<List<Integer>>() {    };    assertEquals(ListCoder.of(VarIntCoder.of()), registry.getCoder(listToken));    registry.registerCoderProvider(CoderProviders.fromStaticMethods(MyValue.class, MyValueCoder.class));    TypeDescriptor<KV<String, List<MyValue>>> kvToken = new TypeDescriptor<KV<String, List<MyValue>>>() {    };    assertEquals(KvCoder.of(StringUtf8Coder.of(), ListCoder.of(MyValueCoder.of())), registry.getCoder(kvToken));}
public void beam_f19008_0() throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    TypeDescriptor<Map<Integer, String>> mapToken = new TypeDescriptor<Map<Integer, String>>() {    };    assertEquals(MapCoder.of(VarIntCoder.of(), StringUtf8Coder.of()), registry.getCoder(mapToken));}
public void beam_f19017_0() throws Exception
{    thrown.expect(IncompatibleCoderException.class);    thrown.expectMessage("not assignable");    CoderRegistry.verifyCompatible(BigEndianIntegerCoder.of(), String.class);}
public void beam_f19018_0() throws Exception
{    thrown.expect(CannotProvideCoderException.class);    thrown.expectMessage("type is over specified");    CoderRegistry.createDefault().getCoder(new TypeDescriptor<Integer>() {    }, new TypeDescriptor<KV<Integer, Integer>>() {    }, KvCoder.of(BigEndianIntegerCoder.of(), VarIntCoder.of()));}
public void beam_f19028_0() throws Exception
{    pipeline.apply(Create.of("hello", "goodbye")).apply(new PTransformOutputingMySerializableGeneric());    pipeline.run();}
public PCollection<KV<String, MySerializableGeneric<T>>> beam_f19030_0(PCollection<String> input)
{    return input.apply(ParDo.of(new OutputDoFn()));}
public AutoRegistrationClass beam_f19042_0(InputStream inStream)
{    return null;}
public void beam_f19043_0() throws Exception
{    assertEquals(AutoRegistrationClassCoder.INSTANCE, CoderRegistry.createDefault().getCoder(AutoRegistrationClass.class));}
public void beam_f19054_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Reasons must not be empty");    new NonDeterministicException(VoidCoder.of(), Collections.emptyList());}
public void beam_f19055_0()
{    NonDeterministicException rootCause = new NonDeterministicException(VoidCoder.of(), "Root Cause");    NonDeterministicException exception = new NonDeterministicException(StringUtf8Coder.of(), "Problem", rootCause);    assertEquals(rootCause, exception.getCause());    assertThat(exception.getReasons(), contains("Problem"));    assertThat(exception.toString(), containsString("Problem"));    assertThat(exception.toString(), containsString("is not deterministic"));}
public KV<String, Long> beam_f19064_0(InputStream inStream) throws IOException
{    return KV.of(key, new DataInputStream(inStream).readLong());}
public boolean beam_f19065_0(Object other)
{    return other instanceof MyCustomCoder && key.equals(((MyCustomCoder) other).key);}
public Coder<T> beam_f19074_0(TypeDescriptor<T> typeDescriptor, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    return OldCustomSerializableCoder.of((Class) typeDescriptor.getRawType());}
public void beam_f19075_0() throws Exception
{    CoderRegistry registry = CoderRegistry.createDefault();    registry.registerCoderProvider(new DefaultCoderProvider());    assertThat(registry.getCoder(AvroRecord.class), instanceOf(AvroCoder.class));    assertThat(registry.getCoder(SerializableRecord.class), instanceOf(SerializableCoder.class));    assertThat(registry.getCoder(CustomRecord.class), instanceOf(CustomSerializableCoder.class));    assertThat(registry.getCoder(OldCustomRecord.class), instanceOf(OldCustomSerializableCoder.class));}
public void beam_f19084_0() throws Exception
{    for (Set<Integer> value : TEST_VALUES) {        CoderProperties.coderDeterministic(TEST_CODER, value, Sets.newHashSet(value));    }}
public void beam_f19085_0() throws Exception
{    for (Set<Integer> value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f19094_0() throws Exception
{    for (ReadableDuration value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f19095_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f19104_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f19105_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null Instant");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f19114_0() throws Exception
{    for (AnyCoderAndData keyCoderAndData : TEST_DATA) {        Coder keyCoder = keyCoderAndData.coderAndData.coder;        for (Object key : keyCoderAndData.coderAndData.data) {            for (AnyCoderAndData valueCoderAndData : TEST_DATA) {                Coder valueCoder = valueCoderAndData.coderAndData.coder;                for (Object value : valueCoderAndData.coderAndData.data) {                    CoderProperties.coderDecodeEncodeEqual(KvCoder.of(keyCoder, valueCoder), KV.of(key, value));                }            }        }    }}
public void beam_f19115_0() throws Exception
{    CoderProperties.coderSerializable(KvCoder.of(GlobalWindow.Coder.INSTANCE, GlobalWindow.Coder.INSTANCE));}
public void beam_f19124_0() throws Exception
{    for (byte[] value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f19125_0() throws Exception
{    CoderProperties.testByteCount(LengthPrefixCoder.of(VarIntCoder.of()), Coder.Context.NESTED, new Integer[] { 0, 10, 1000 });}
public void beam_f19134_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null List");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f19135_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null Integer");    List<Integer> list = Arrays.asList(1, 2, 3, null, 4);    Coder<List<Integer>> coder = ListCoder.of(VarIntCoder.of());    CoderProperties.coderDecodeEncodeEqual(coder, list);}
public void beam_f19144_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null Map");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f19145_0() throws Exception
{    MapCoder<byte[], Integer> coder = MapCoder.of(ByteArrayCoder.of(), VarIntCoder.of());    Map<byte[], Integer> value = Collections.singletonMap(new byte[] { 1, 2, 3, 4 }, 1);    CoderProperties.structuralValueDecodeEncodeEqual(coder, value);}
public void beam_f19154_0() throws Exception
{    NullableCoder<String> varLenCoder = NullableCoder.of(StringUtf8Coder.of());    assertEquals(1, varLenCoder.getEncodedElementByteSize(null));    assertEquals(6, varLenCoder.getEncodedElementByteSize("spam"));}
public void beam_f19155_0() throws Exception
{    NullableCoder<Double> coder = NullableCoder.of(DoubleCoder.of());    assertTrue(coder.isRegisterByteSizeObserverCheap(5.0));}
public void beam_f19164_0(String value, OutputStream outStream, Context context) throws IOException
{    checkArgument(context.isWholeStream, "Expected to get entire stream");    StringUtf8Coder.of().encode(value, outStream, context);}
public String beam_f19165_0(InputStream inStream) throws CoderException, IOException
{    return decode(inStream, Context.NESTED);}
public void beam_f19175_0() throws Exception
{    Pipeline p = pipelineWith(new CustomTestCoder(NULL_POINTER_EXCEPTION, null, null, null, EXCEPTION_MESSAGE));    thrown.expect(Exception.class);    thrown.expect(new ExceptionMatcher("java.lang.NullPointerException: Super Unique Message!!!"));    p.run().waitUntilFinish();}
public void beam_f19176_0() throws Exception
{    Pipeline p = pipelineWith(new CustomTestCoder(null, IO_EXCEPTION, null, null, EXCEPTION_MESSAGE));    thrown.expect(Exception.class);    thrown.expect(new ExceptionMatcher("java.io.IOException: Super Unique Message!!!"));    p.run().waitUntilFinish();}
public static ContentReader beam_f19185_0(Iterable<String> expected)
{    return new ContentReader(expected);}
public Void beam_f19186_0(Iterable<String> contents)
{    assertThat(contents, containsInAnyOrder(expected));    return null;}
public void beam_f19195_0() throws Exception
{    Schema nestedSchema = Schema.builder().addInt32Field("f1_int").addStringField("f1_str").build();    FieldType collectionElementType = FieldType.row(nestedSchema);    Schema schema = Schema.builder().addArrayField("f_array", collectionElementType).build();    Row row = Row.withSchema(schema).addArray(Row.withSchema(nestedSchema).addValues(1, "one").build(), Row.withSchema(nestedSchema).addValues(2, "two").build(), Row.withSchema(nestedSchema).addValues(3, "three").build()).build();    CoderProperties.coderDecodeEncodeEqual(RowCoder.of(schema), row);}
public void beam_f19196_0() throws Exception
{    FieldType arrayType = FieldType.array(FieldType.array(FieldType.INT32));    Schema schema = Schema.builder().addField("f_array", arrayType).build();    Row row = Row.withSchema(schema).addArray(Lists.newArrayList(1, 2, 3, 4), Lists.newArrayList(5, 6, 7, 8), Lists.newArrayList(9, 10, 11, 12)).build();    CoderProperties.coderDecodeEncodeEqual(RowCoder.of(schema), row);}
public boolean beam_f19205_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    MyRecord myRecord = (MyRecord) o;    return value.equals(myRecord.value);}
public int beam_f19206_0()
{    return value.hashCode();}
public void beam_f19215_0() throws Exception
{    Coder<String> coder = SerializableCoder.of(String.class);    byte[] encodedBytes = CoderUtils.encodeToByteArray(coder, null);    assertNull(CoderUtils.decodeFromByteArray(coder, encodedBytes));}
public void beam_f19216_0() throws Exception
{    Coder<String> coder = SerializableCoder.of(String.class);    byte[] encodedBytes;    try (ByteArrayOutputStream os = new ByteArrayOutputStream()) {        coder.encode(null, os);        coder.encode("TestValue", os);        coder.encode(null, os);        coder.encode("TestValue2", os);        coder.encode(null, os);        encodedBytes = os.toByteArray();    }    try (ByteArrayInputStream is = new ByteArrayInputStream(encodedBytes)) {        assertNull(coder.decode(is));        assertEquals("TestValue", coder.decode(is));        assertNull(coder.decode(is));        assertEquals("TestValue2", coder.decode(is));        assertNull(coder.decode(is));        assertEquals(0, is.available());    }}
public boolean beam_f19225_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    ProperEquals that = (ProperEquals) o;    return x == that.x;}
public int beam_f19226_0()
{    return x;}
public void beam_f19235_0() throws Exception
{    for (String uriString : TEST_URI_STRINGS) {        CoderProperties.coderDecodeEncodeEqual(uriCoder, new URI(uriString));    }}
public void beam_f19236_0() throws Exception
{    CoderProperties.coderSerializable(uriCoder);}
public List<? extends Coder<?>> beam_f19245_0()
{    return Collections.emptyList();}
public boolean beam_f19247_0()
{    return true;}
public void beam_f19257_0() throws Exception
{    Assert.assertThat(new Foo<String>().getEncodedTypeDescriptor().getType(), CoreMatchers.not(TypeDescriptor.of(String.class).getType()));}
public void beam_f19258_0() throws Exception
{    Assert.assertThat(new FooTwo().getEncodedTypeDescriptor(), CoreMatchers.equalTo(TypeDescriptor.of(String.class)));}
public void beam_f19268_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f19269_0() throws Exception
{    thrown.expect(CoderException.class);    thrown.expectMessage("cannot encode a null Integer");    CoderUtils.encodeToBase64(TEST_CODER, null);}
public void beam_f19278_0()
{    AvroIO.Write<String> write = AvroIO.write(String.class).to("/tmp/foo/baz");    assertEquals(CodecFactory.snappyCodec().toString(), write.inner.getCodec().toString());}
public void beam_f19279_0()
{    AvroIO.Write<String> write = AvroIO.write(String.class).to("/tmp/foo/baz").withCodec(CodecFactory.snappyCodec());    assertEquals(SNAPPY_CODEC, write.inner.getCodec().toString());}
public void beam_f19288_0()
{    AvroIO.Write<GenericClass> write = AvroIO.write(GenericClass.class).to("/foo").withShardNameTemplate("-SS-of-NN-").withSuffix("bar").withNumShards(100).withCodec(CodecFactory.deflateCodec(6));    DisplayData displayData = DisplayData.from(write);    assertThat(displayData, hasDisplayItem("filePrefix", "/foo"));    assertThat(displayData, hasDisplayItem("shardNameTemplate", "-SS-of-NN-"));    assertThat(displayData, hasDisplayItem("fileSuffix", "bar"));    assertThat(displayData, hasDisplayItem("schema", "{\"type\":\"record\",\"name\":\"GenericClass\",\"namespace\":\"org.apache.beam.sdk.io" + ".AvroIOTest$\",\"fields\":[{\"name\":\"intField\",\"type\":\"int\"}," + "{\"name\":\"stringField\",\"type\":\"string\"}]}"));    assertThat(displayData, hasDisplayItem("numShards", 100));    assertThat(displayData, hasDisplayItem("codec", CodecFactory.deflateCodec(6).toString()));}
public void beam_f19289_0() throws Throwable
{    List<GenericClass> values = ImmutableList.of(new GenericClass(3, "hi"), new GenericClass(5, "bar"));    File outputFile = tmpFolder.newFile("output.avro");    writePipeline.apply(Create.of(values)).apply(AvroIO.write(GenericClass.class).to(writePipeline.newProvider(outputFile.getAbsolutePath())).withoutSharding());    writePipeline.run();    PAssert.that(readPipeline.apply("Read", AvroIO.read(GenericClass.class).withBeamSchemas(withBeamSchemas).from(readPipeline.newProvider(outputFile.getAbsolutePath())))).containsInAnyOrder(values);    readPipeline.run();}
public GenericClass beam_f19298_0(Long i)
{    return new GenericClass(i.intValue(), "value" + i);}
public void beam_f19299_0()
{    SimpleFunction<Long, GenericClass> mapFn = new CreateGenericClass();    List<GenericClass> firstValues = new ArrayList<>();    List<GenericClass> secondValues = new ArrayList<>();    for (int i = 0; i < 7; ++i) {        (i < 3 ? firstValues : secondValues).add(mapFn.apply((long) i));    }            Window<Long> window = Window.<Long>into(FixedWindows.of(Duration.millis(100))).withAllowedLateness(Duration.ZERO).triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))).discardingFiredPanes();    readPipeline.apply("Sequence first", GenerateSequence.from(0).to(3).withRate(1, Duration.millis(300))).apply("Window first", window).apply("Map first", MapElements.via(mapFn)).apply("Write first", AvroIO.write(GenericClass.class).to(tmpFolder.getRoot().getAbsolutePath() + "/first").withNumShards(2).withWindowedWrites());    readPipeline.apply("Sequence second", GenerateSequence.from(3).to(7).withRate(1, Duration.millis(300))).apply("Window second", window).apply("Map second", MapElements.via(mapFn)).apply("Write second", AvroIO.write(GenericClass.class).to(tmpFolder.getRoot().getAbsolutePath() + "/second").withNumShards(3).withWindowedWrites());            PAssert.that(readPipeline.apply("Read", AvroIO.read(GenericClass.class).withBeamSchemas(withBeamSchemas).from(tmpFolder.getRoot().getAbsolutePath() + "/first*").watchForNewFiles(Duration.millis(100), Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))).containsInAnyOrder(firstValues);    PAssert.that(readPipeline.apply("Parse", AvroIO.parseGenericRecords(new ParseGenericClass()).from(tmpFolder.getRoot().getAbsolutePath() + "/first*").watchForNewFiles(Duration.millis(100), Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))).containsInAnyOrder(firstValues);    PCollection<String> paths = readPipeline.apply("Create paths", Create.of(tmpFolder.getRoot().getAbsolutePath() + "/first*", tmpFolder.getRoot().getAbsolutePath() + "/second*"));    PAssert.that(paths.apply("Match All Read files", FileIO.matchAll().continuously(Duration.millis(100), Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3)))).apply("Read Matches Read files", FileIO.readMatches().withDirectoryTreatment(FileIO.ReadMatches.DirectoryTreatment.PROHIBIT)).apply("Read files", AvroIO.readFiles(GenericClass.class).withBeamSchemas(withBeamSchemas).withDesiredBundleSizeBytes(10))).containsInAnyOrder(Iterables.concat(firstValues, secondValues));    PAssert.that(paths.apply("Read all", AvroIO.readAll(GenericClass.class).withBeamSchemas(withBeamSchemas).watchForNewFiles(Duration.millis(100), Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))).withDesiredBundleSizeBytes(10))).containsInAnyOrder(Iterables.concat(firstValues, secondValues));    PAssert.that(paths.apply("Match All ParseFilesGenericRecords", FileIO.matchAll().continuously(Duration.millis(100), Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3)))).apply("Match Matches ParseFilesGenericRecords", FileIO.readMatches().withDirectoryTreatment(FileIO.ReadMatches.DirectoryTreatment.PROHIBIT)).apply("ParseFilesGenericRecords", AvroIO.parseFilesGenericRecords(new ParseGenericClass()).withCoder(AvroCoder.of(GenericClass.class)).withDesiredBundleSizeBytes(10))).containsInAnyOrder(Iterables.concat(firstValues, secondValues));    PAssert.that(paths.apply("ParseAllGenericRecords", AvroIO.parseAllGenericRecords(new ParseGenericClass()).withCoder(AvroCoder.of(GenericClass.class)).watchForNewFiles(Duration.millis(100), Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))).withDesiredBundleSizeBytes(10))).containsInAnyOrder(Iterables.concat(firstValues, secondValues));    readPipeline.run();}
public void beam_f19308_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("fileNamePrefix", outputFilePrefix.toString()).withLabel("File Name Prefix"));}
public void beam_f19309_0() throws Throwable
{    testWindowedAvroIOWriteUsingMethod(WriteMethod.AVROIO_WRITE);}
public String beam_f19318_0()
{    return "";}
public FilenamePolicy beam_f19319_0(String destination)
{    return DefaultFilenamePolicy.fromStandardParameters(StaticValueProvider.of(baseDir.resolve("file_" + destination, RESOLVE_FILE)), "-SSSSS-of-NNNNN", ".avro", false);}
public void beam_f19328_0() throws Exception
{    testDynamicDestinationsUnwindowedWithSharding(WriteMethod.AVROIO_WRITE, Sharding.FIXED_3_SHARDS);}
public void beam_f19329_0() throws Exception
{    testDynamicDestinationsUnwindowedWithSharding(WriteMethod.AVROIO_SINK_WITH_SCHEMA, Sharding.RUNNER_DETERMINED);}
public void beam_f19338_0() throws Exception
{    String[] expectedElements = new String[] { "first", "second", "third" };    runTestWrite(expectedElements, 1);}
public void beam_f19339_0() throws Exception
{    String[] expectedElements = new String[] { "first", "second", "third", "fourth", "fifth" };    runTestWrite(expectedElements, 4);}
public void beam_f19348_0() throws Exception
{        List<FixedRecord> expected = createFixedRecords(20);    String filename = generateTestFile("tmp.avro", expected, SyncBehavior.SYNC_REGULAR, 5, AvroCoder.of(FixedRecord.class), DataFileConstants.NULL_CODEC);    AvroSource<FixedRecord> source = AvroSource.from(filename).withSchema(FixedRecord.class);    SourceTestUtils.assertSplitAtFractionExhaustive(source, null);}
public void beam_f19349_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();            List<Bird> expected = createRandomRecords(DEFAULT_RECORD_COUNT);    String filename = generateTestFile("tmp.avro", expected, SyncBehavior.SYNC_RANDOM, DEFAULT_RECORD_COUNT / 20, /* max records/block */    AvroCoder.of(Bird.class), DataFileConstants.NULL_CODEC);    File file = new File(filename);        AvroSource<Bird> source = AvroSource.from(filename).withSchema(Bird.class).withMinBundleSize(100L);        assertEquals(expected, SourceTestUtils.readFromSource(source, options));    List<? extends BoundedSource<Bird>> splits;    int nonEmptySplits;        splits = source.split(100L, options);    assertTrue(splits.size() > 2);    SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);    nonEmptySplits = 0;    for (BoundedSource<Bird> subSource : splits) {        if (SourceTestUtils.readFromSource(subSource, options).size() > 0) {            nonEmptySplits += 1;        }    }    assertTrue(nonEmptySplits > 2);        splits = source.split(file.length() / 4, options);    assertTrue(splits.size() > 2);    SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);    nonEmptySplits = 0;    for (BoundedSource<Bird> subSource : splits) {        if (SourceTestUtils.readFromSource(subSource, options).size() > 0) {            nonEmptySplits += 1;        }    }    assertTrue(nonEmptySplits > 2);        splits = source.split(file.length(), options);    assertTrue(splits.size() == 1);    SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);}
public void beam_f19358_0() throws IOException
{        for (int i = 0; i <= 16; i++) {        testAdvancePastNextSyncMarkerAt(i, 1000);        testAdvancePastNextSyncMarkerAt(160 + i, 1000);    }        testAdvancePastNextSyncMarkerAt(983, 1000);        testAdvancePastNextSyncMarkerAt(984, 1000);    testAdvancePastNextSyncMarkerAt(985, 1000);    testAdvancePastNextSyncMarkerAt(999, 1000);        testAdvancePastNextSyncMarkerAt(1000, 1000);}
public void beam_f19359_0()
{    byte[] marker = { 0, 1, 2, 3 };    byte[] buffer;    Seeker s;    s = new Seeker(marker);    buffer = new byte[] { 0, 1, 2, 3, 4, 5, 6, 7 };    assertEquals(3, s.find(buffer, buffer.length));    buffer = new byte[] { 0, 0, 0, 0, 0, 1, 2, 3 };    assertEquals(7, s.find(buffer, buffer.length));    buffer = new byte[] { 0, 1, 2, 0, 0, 1, 2, 3 };    assertEquals(7, s.find(buffer, buffer.length));    buffer = new byte[] { 0, 1, 2, 3 };    assertEquals(3, s.find(buffer, buffer.length));}
public int beam_f19368_0()
{    return value[0] | (value[1] << 8) | (value[2] << 16) | (value[3] << 24);}
public boolean beam_f19369_0(Object o)
{    if (o instanceof FixedRecord) {        FixedRecord other = (FixedRecord) o;        return this.asInt() == other.asInt();    }    return false;}
public int beam_f19378_0()
{    return Objects.hash(number, species, quality, quantity, habitat, fancinessLevel);}
private static List<Bird> beam_f19379_0(long n)
{    String[] qualities = { "miserable", "forelorn", "fidgity", "squirrelly", "fanciful", "chipper", "lazy" };    String[] species = { "pigeons", "owls", "gulls", "hawks", "robins", "jays" };    Random random = new Random(0);    List<Bird> records = new ArrayList<>();    for (long i = 0; i < n; i++) {        Bird bird = new Bird();        bird.quality = qualities[random.nextInt(qualities.length)];        bird.species = species[random.nextInt(species.length)];        bird.number = i;        bird.quantity = random.nextLong();        records.add(bird);    }    return records;}
public TestCountingSource beam_f19388_0()
{    return new TestCountingSource(numMessagesPerShard, shardNumber, true, throwOnFirstSnapshot, true);}
private TestCountingSource beam_f19389_0(int shardNumber)
{    return new TestCountingSource(numMessagesPerShard, shardNumber, dedup, throwOnFirstSnapshot, true);}
public boolean beam_f19398_0()
{    if (current >= numMessagesPerShard - 1) {        return false;    }        if (current >= 0 && dedup && ThreadLocalRandom.current().nextInt(5) == 0) {        return true;    }    current++;    return true;}
public KV<Integer, Integer> beam_f19399_0()
{    return KV.of(shardNumber, current);}
public void beam_f19409_0() throws IOException
{    BoundedReadFromUnboundedSourceTest.TestCountingSource source = new BoundedReadFromUnboundedSourceTest.TestCountingSource(3);    PipelineOptions options = PipelineOptionsFactory.create();    BoundedReadFromUnboundedSourceTest.TestCountingSource.CountingSourceReader reader = source.createReader(options, null);    assertTrue(reader.start());    assertEquals(0L, (long) reader.getCurrent().getValue());    assertTrue(reader.advance());    assertEquals(1L, (long) reader.getCurrent().getValue());    BoundedReadFromUnboundedSourceTest.TestCountingSource.CounterMark checkpoint = reader.getCheckpointMark();    checkpoint.finalizeCheckpoint();    reader = source.createReader(options, checkpoint);    assertTrue(reader.start());    assertEquals(2L, (long) reader.getCurrent().getValue());    assertFalse(reader.advance());}
public void beam_f19410_0() throws IOException
{    BoundedReadFromUnboundedSourceTest.TestCountingSource source = new BoundedReadFromUnboundedSourceTest.TestCountingSource(1);    PipelineOptions options = PipelineOptionsFactory.create();    BoundedReadFromUnboundedSourceTest.TestCountingSource.CountingSourceReader reader = source.createReader(options, null);    assertTrue(reader.start());    assertEquals(0L, (long) reader.getCurrent().getValue());    assertFalse(reader.advance());    BoundedReadFromUnboundedSourceTest.TestCountingSource.CounterMark checkpoint = reader.getCheckpointMark();    checkpoint.finalizeCheckpoint();    source = new BoundedReadFromUnboundedSourceTest.TestCountingSource(2);    reader = source.createReader(options, checkpoint);    assertTrue(reader.start());    assertEquals(1L, (long) reader.getCurrent().getValue());    assertFalse(reader.advance());}
private static byte[] beam_f19419_0(byte[] input) throws IOException
{    ByteArrayOutputStream res = new ByteArrayOutputStream();    try (GZIPOutputStream gzipStream = new GZIPOutputStream(res)) {        gzipStream.write(input);    }    return res.toByteArray();}
private static byte[] beam_f19420_0(byte[] first, byte[] second)
{    byte[] res = new byte[first.length + second.length];    System.arraycopy(first, 0, res, 0, first.length);    System.arraycopy(second, 0, res, first.length, second.length);    return res;}
public void beam_f19429_0() throws Exception
{    String baseName = "test-input";    File uncompressedFile = tmpFolder.newFile(baseName + ".bin");    Files.write(generateInput(10), uncompressedFile);    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(uncompressedFile.getPath(), 1));    assertTrue(source.isSplittable());    SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());}
public void beam_f19430_0() throws Exception
{    String baseName = "test-input";    File uncompressedFile = tmpFolder.newFile(baseName + ".bin");    Files.write(generateInput(10), uncompressedFile);    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(uncompressedFile.getPath(), 1)).withDecompression(CompressionMode.UNCOMPRESSED);    assertTrue(source.isSplittable());    SourceTestUtils.assertSplitAtFractionExhaustive(source, PipelineOptionsFactory.create());}
public void beam_f19439_0() throws Exception
{    int numFiles = 3;    String baseName = "test_input-";    String filePattern = new File(tmpFolder.getRoot().toString(), baseName + "*").toString();    List<Byte> expected = new ArrayList<>();    for (int i = 0; i < numFiles; i++) {        byte[] generated = generateInput(100);        File tmpFile = tmpFolder.newFile(baseName + i);        writeFile(tmpFile, generated, CompressionMode.GZIP);        expected.addAll(Bytes.asList(generated));    }    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(filePattern, 1)).withDecompression(CompressionMode.GZIP);    List<Byte> actual = SourceTestUtils.readFromSource(source, PipelineOptionsFactory.create());    assertEquals(HashMultiset.create(expected), HashMultiset.create(actual));}
public void beam_f19440_0()
{    ByteSource inputSource = new ByteSource("foobar.txt", 1) {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    };    CompressedSource<?> compressedSource = CompressedSource.from(inputSource);    CompressedSource<?> gzipSource = compressedSource.withDecompression(CompressionMode.GZIP);    DisplayData compressedSourceDisplayData = DisplayData.from(compressedSource);    DisplayData gzipDisplayData = DisplayData.from(gzipSource);    assertThat(compressedSourceDisplayData, hasDisplayItem("compressionMode"));    assertThat(gzipDisplayData, hasDisplayItem("compressionMode", CompressionMode.GZIP.toString()));    assertThat(compressedSourceDisplayData, hasDisplayItem("source", inputSource.getClass()));    assertThat(compressedSourceDisplayData, includesDisplayDataFor("source", inputSource));}
private void beam_f19449_0(byte[] expected, File inputFile, @Nullable DecompressingChannelFactory decompressionFactory) throws IOException
{    CompressedSource<Byte> source = CompressedSource.from(new ByteSource(inputFile.toPath().toString(), 1));    if (decompressionFactory != null) {        source = source.withDecompression(decompressionFactory);    }    List<KV<Long, Byte>> actualOutput = Lists.newArrayList();    try (BoundedReader<Byte> reader = source.createReader(PipelineOptionsFactory.create())) {        for (boolean more = reader.start(); more; more = reader.advance()) {            actualOutput.add(KV.of(reader.getCurrentTimestamp().getMillis(), reader.getCurrent()));        }    }    List<KV<Long, Byte>> expectedOutput = Lists.newArrayList();    for (int i = 0; i < expected.length; i++) {        expectedOutput.add(KV.of((long) i, expected[i]));    }    assertEquals(expectedOutput, actualOutput);}
private void beam_f19450_0(byte[] input, CompressionMode mode) throws IOException
{    runReadTest(input, mode, mode);}
public Instant beam_f19459_0() throws NoSuchElementException
{    return new Instant(getCurrentOffset());}
public void beam_f19460_0(ProcessContext context)
{    context.output(KV.of(context.timestamp().getMillis(), context.element()));}
public void beam_f19469_0() throws IOException
{    final int numRecords = 5;        @SuppressWarnings("deprecation")    BoundedSource<Long> source = CountingSource.upTo(numRecords);    try (BoundedReader<Long> reader = source.createReader(PipelineOptionsFactory.create())) {                        assertEquals(0.0, reader.getFractionConsumed(), 1e-6);        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(numRecords, reader.getSplitPointsRemaining());        assertTrue(reader.start());        int i = 0;        do {            assertEquals(i, reader.getSplitPointsConsumed());            assertEquals(numRecords - i, reader.getSplitPointsRemaining());            ++i;        } while (reader.advance());                assertEquals(numRecords, i);        assertEquals(1.0, reader.getFractionConsumed(), 1e-6);        assertEquals(numRecords, reader.getSplitPointsConsumed());        assertEquals(0, reader.getSplitPointsRemaining());    }}
public void beam_f19470_0()
{    long numElements = 1000;    PCollection<Long> input = p.apply(Read.from(CountingSource.unbounded()).withMaxNumRecords(numElements));    addCountingAsserts(input, numElements);    p.run();}
public void beam_f19479_0()
{    assertEquals("/path/to/output-001-of-123.txt", constructName("/path/to/output", "-SSS-of-NNN", ".txt", 1, 123, null, null));    assertEquals("/path/to/out.txt/part-00042", constructName("/path/to/out.txt", "/part-SSSSS", "", 42, 100, null, null));    assertEquals("/path/to/out.txt", constructName("/path/to/ou", "t.t", "xt", 1, 1, null, null));    assertEquals("/path/to/out0102shard.txt", constructName("/path/to/out", "SSNNshard", ".txt", 1, 2, null, null));    assertEquals("/path/to/out-2/1.part-1-of-2.txt", constructName("/path/to/out", "-N/S.part-S-of-N", ".txt", 1, 2, null, null));}
public void beam_f19480_0()
{    assertEquals("/out-100-of-5000.txt", constructName("/out", "-SS-of-NN", ".txt", 100, 5000, null, null));}
private ResourceId beam_f19489_0()
{    return LocalResources.fromFile(tmpFolder.getRoot(), /* isDirectory */    true);}
private ResourceId beam_f19490_0()
{    String baseOutputDirname = "output";    return getTemporaryFolder().resolve(baseOutputDirname, StandardResolveOptions.RESOLVE_DIRECTORY);}
public void beam_f19499_0() throws Exception
{    SimpleSink.SimpleWriteOperation writeOp = buildWriteOperation();    List<File> files = generateTemporaryFilesForFinalize(3);    runFinalize(writeOp, files);        tmpFolder.newFolder(tempDirectoryName);    tmpFolder.newFile(tempDirectoryName + "/1");    runFinalize(writeOp, files);}
private List<File> beam_f19500_0(int numFiles) throws Exception
{    List<File> temporaryFiles = new ArrayList<>();    for (int i = 0; i < numFiles; i++) {        ResourceId temporaryFile = WriteOperation.buildTemporaryFilename(getBaseTempDirectory(), "" + i);        File tmpFile = new File(tmpFolder.getRoot(), temporaryFile.toString());        tmpFile.getParentFile().mkdirs();        assertTrue(tmpFile.createNewFile());        temporaryFiles.add(tmpFile);    }    return temporaryFiles;}
public void beam_f19509_0() throws FileNotFoundException, IOException
{    final File file = writeValuesWithCompression(Compression.GZIP, "abc", "123");        assertReadValues(new BufferedReader(new InputStreamReader(new GZIPInputStream(new FileInputStream(file)), StandardCharsets.UTF_8)), "abc", "123");}
public void beam_f19510_0() throws FileNotFoundException, IOException
{    final File file = writeValuesWithCompression(Compression.DEFLATE, "abc", "123");        assertReadValues(new BufferedReader(new InputStreamReader(new DeflateCompressorInputStream(new FileInputStream(file)), StandardCharsets.UTF_8)), "abc", "123");}
protected FileBasedSource<String> beam_f19519_0(Metadata fileName, long start, long end)
{    return new TestFileBasedSource(fileName, getMinBundleSize(), start, end, splitHeader);}
protected FileBasedReader<String> beam_f19520_0(PipelineOptions options)
{    if (splitHeader == null) {        return new TestReader(this);    } else {        return new TestReaderWithSplits(this);    }}
public String beam_f19529_0() throws NoSuchElementException
{    return lineReader.getCurrent();}
protected void beam_f19530_0(ReadableByteChannel channel) throws IOException
{    this.lineReader = new LineReader(channel);}
public void beam_f19539_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    List<String> data = createStringDataset(3, 50);    String fileName = "file";    File file = createFileWithData(fileName, data);    TestFileBasedSource source = new TestFileBasedSource(file.getPath(), 64, null);    assertEquals(data, readFromSource(source, options));}
public void beam_f19540_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    List<String> data1 = createStringDataset(3, 50);    File file1 = createFileWithData("file1", data1);    List<String> data2 = createStringDataset(3, 50);    createFileWithData("file2", data2);    List<String> data3 = createStringDataset(3, 50);    createFileWithData("file3", data3);    List<String> data4 = createStringDataset(3, 50);    createFileWithData("otherfile", data4);    TestFileBasedSource source = new TestFileBasedSource(new File(file1.getParent(), "file*").getPath(), 64, null);    List<String> expectedResults = new ArrayList<>();    expectedResults.addAll(data1);    expectedResults.addAll(data2);    expectedResults.addAll(data3);    assertThat(expectedResults, containsInAnyOrder(readFromSource(source, options).toArray()));}
public void beam_f19549_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    String header = "<h>";    List<String> data = new ArrayList<>();    for (int i = 0; i < 10; i++) {        data.add(header);        data.addAll(createStringDataset(3, 9));    }    String fileName = "file";    File file = createFileWithData(fileName, data);    TestFileBasedSource source = new TestFileBasedSource(file.getPath(), 64, header);    List<String> expectedResults = new ArrayList<>();    expectedResults.addAll(data);        expectedResults.removeAll(Collections.singletonList(header));    assertEquals(expectedResults, readFromSource(source, options));}
public void beam_f19550_0() throws IOException
{    PipelineOptions options = PipelineOptionsFactory.create();    String header = "<h>";    List<String> data = new ArrayList<>();    for (int i = 0; i < 10; i++) {        data.add(header);        data.addAll(createStringDataset(3, 9));    }    String fileName = "file";    File file = createFileWithData(fileName, data);    Metadata metadata = FileSystems.matchSingleFileSpec(file.getPath());    TestFileBasedSource source1 = new TestFileBasedSource(metadata, 64, 0, 60, header);    TestFileBasedSource source2 = new TestFileBasedSource(metadata, 64, 60, Long.MAX_VALUE, header);    List<String> expectedResults = new ArrayList<>();    expectedResults.addAll(data);        expectedResults.removeAll(Arrays.asList(header));    List<String> results = new ArrayList<>();    results.addAll(readFromSource(source1, options));    results.addAll(readFromSource(source2, options));    assertThat(expectedResults, containsInAnyOrder(results.toArray()));}
public void beam_f19559_0() throws Exception
{    List<String> data = createStringDataset(3, 50);    String fileName = "file";    File file = createFileWithData(fileName, data);    TestFileBasedSource source = new TestFileBasedSource(file.getPath(), 64, null);    assertEquals(file.length(), source.getEstimatedSizeBytes(null));}
public void beam_f19560_0() throws Exception
{    List<String> data1 = createStringDataset(3, 20);    File file1 = createFileWithData("file1", data1);    List<String> data2 = createStringDataset(3, 40);    File file2 = createFileWithData("file2", data2);    List<String> data3 = createStringDataset(3, 30);    File file3 = createFileWithData("file3", data3);    List<String> data4 = createStringDataset(3, 45);    createFileWithData("otherfile", data4);    List<String> data5 = createStringDataset(3, 53);    createFileWithData("anotherfile", data5);    TestFileBasedSource source = new TestFileBasedSource(new File(file1.getParent(), "file*").getPath(), 64, null);            assertEquals(file1.length() + file2.length() + file3.length(), source.getEstimatedSizeBytes(null));}
public void beam_f19569_0() throws IOException
{    p.apply(Create.of(tmpFolder.getRoot().getAbsolutePath() + "/*")).apply(FileIO.matchAll().withEmptyMatchTreatment(EmptyMatchTreatment.DISALLOW));    thrown.expectCause(isA(FileNotFoundException.class));    p.run();}
public void beam_f19570_0() throws IOException
{    p.apply(Create.of(tmpFolder.getRoot().getAbsolutePath() + "/blah")).apply(FileIO.matchAll().withEmptyMatchTreatment(EmptyMatchTreatment.ALLOW_IF_WILDCARD));    thrown.expectCause(isA(FileNotFoundException.class));    p.run();}
public void beam_f19579_0() throws Exception
{    assertTrue(FileSystems.getFileSystemInternal(toLocalResourceId("~/home/").getScheme()) instanceof LocalFileSystem);    assertTrue(FileSystems.getFileSystemInternal(toLocalResourceId("file://home").getScheme()) instanceof LocalFileSystem);    assertTrue(FileSystems.getFileSystemInternal(toLocalResourceId("FILE://home").getScheme()) instanceof LocalFileSystem);    assertTrue(FileSystems.getFileSystemInternal(toLocalResourceId("File://home").getScheme()) instanceof LocalFileSystem);    if (SystemUtils.IS_OS_WINDOWS) {        assertTrue(FileSystems.getFileSystemInternal(toLocalResourceId("c:\\home\\").getScheme()) instanceof LocalFileSystem);    }}
public void beam_f19580_0() throws Exception
{    thrown.expect(RuntimeException.class);    thrown.expectMessage("Scheme: [file] has conflicting filesystems");    FileSystems.verifySchemesAreUnique(PipelineOptionsFactory.create(), Sets.newHashSet(new LocalFileSystemRegistrar(), new LocalFileSystemRegistrar()));}
private void beam_f19589_0(Path path, String content) throws Exception
{    try (Writer writer = Channels.newWriter(localFileSystem.create(LocalResourceId.fromPath(path, false), CreateOptions.StandardCreateOptions.builder().setMimeType(MimeTypes.TEXT).build()), StandardCharsets.UTF_8.name())) {        writer.write(content);    }}
private LocalResourceId beam_f19590_0(String str) throws Exception
{    boolean isDirectory;    if (SystemUtils.IS_OS_WINDOWS) {        isDirectory = str.endsWith("\\");    } else {        isDirectory = str.endsWith("/");    }    return LocalResourceId.fromPath(Paths.get(str), isDirectory);}
public void beam_f19599_0()
{    PCollection<Long> input = p.apply(GenerateSequence.from(0).to(0));    PAssert.that(input).empty();    p.run();}
public void beam_f19600_0()
{    PCollection<Long> input = p.apply(GenerateSequence.from(42).to(42));    PAssert.that(input).empty();    p.run();}
public Instant beam_f19609_0(Long input)
{    return new Instant(input);}
public void beam_f19610_0()
{    for (FileSystemRegistrar registrar : Lists.newArrayList(ServiceLoader.load(FileSystemRegistrar.class).iterator())) {        if (registrar instanceof LocalFileSystemRegistrar) {            Iterable<FileSystem> fileSystems = registrar.fromOptions(PipelineOptionsFactory.create());            assertThat(fileSystems, contains(instanceOf(LocalFileSystem.class)));            return;        }    }    fail("Expected to find " + LocalFileSystemRegistrar.class);}
public void beam_f19619_0() throws Exception
{    Path srcPath1 = temporaryFolder.newFile().toPath();    Path srcPath2 = temporaryFolder.newFile().toPath();    Path destPath1 = temporaryFolder.getRoot().toPath().resolve("nonexistentdir").resolve("dest1");    Path destPath2 = srcPath2.resolveSibling("dest2");    createFileWithContent(srcPath1, "content1");    createFileWithContent(srcPath2, "content2");    localFileSystem.rename(toLocalResourceIds(ImmutableList.of(srcPath1, srcPath2), false), toLocalResourceIds(ImmutableList.of(destPath1, destPath2), false));    assertContents(ImmutableList.of(destPath1, destPath2), ImmutableList.of("content1", "content2"));    assertFalse(srcPath1 + "exists", srcPath1.toFile().exists());    assertFalse(srcPath2 + "exists", srcPath2.toFile().exists());}
public void beam_f19620_0() throws Exception
{    File f1 = temporaryFolder.newFile("file1");    File f2 = temporaryFolder.newFile("file2");    File f3 = temporaryFolder.newFile("other-file");    localFileSystem.delete(toLocalResourceIds(Lists.newArrayList(f1.toPath(), f2.toPath()), false));    assertFalse(f1.exists());    assertFalse(f2.exists());    assertTrue(f3.exists());}
public void beam_f19629_0() throws Exception
{    List<String> expected = ImmutableList.of(temporaryFolder.newFile("a").toString());    temporaryFolder.newFile("aa");    temporaryFolder.newFile("ab");    String file = "file:///" + temporaryFolder.getRoot().toPath().resolve("a").toString();    List<MatchResult> results = localFileSystem.match(ImmutableList.of(file));    assertThat(toFilenames(results), containsInAnyOrder(expected.toArray(new String[expected.size()])));}
public void beam_f19630_0() throws Exception
{    File unmatchedSubDir = temporaryFolder.newFolder("aaa");    File unmatchedSubDirFile = File.createTempFile("sub-dir-file", "", unmatchedSubDir);    unmatchedSubDirFile.deleteOnExit();    List<String> expected = ImmutableList.of(temporaryFolder.newFile("a").toString(), temporaryFolder.newFile("aa").toString(), temporaryFolder.newFile("ab").toString());    temporaryFolder.newFile("ba");    temporaryFolder.newFile("bb");    List<MatchResult> matchResults = matchGlobWithPathPrefix(temporaryFolder.getRoot().toPath().resolve("a"), "*");    assertThat(toFilenames(matchResults), containsInAnyOrder(expected.toArray(new String[expected.size()])));}
private static void beam_f19639_0(File file) throws IOException
{    if (!file.getParentFile().mkdirs() || !file.createNewFile()) {        throw new IOException("Failed creating empty file " + file.getAbsolutePath());    }}
public void beam_f19640_0()
{    if (SystemUtils.IS_OS_WINDOWS) {                return;    }        assertEquals(toResourceIdentifier("/root/tmp/aa"), toResourceIdentifier("/root/tmp/").resolve("aa", StandardResolveOptions.RESOLVE_FILE));    assertEquals(toResourceIdentifier("/root/tmp/aa/bb/cc/"), toResourceIdentifier("/root/tmp/").resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("bb", StandardResolveOptions.RESOLVE_DIRECTORY).resolve("cc", StandardResolveOptions.RESOLVE_DIRECTORY));        assertEquals(toResourceIdentifier("/root/tmp/aa"), toResourceIdentifier("/root/tmp/bb/").resolve("/root/tmp/aa", StandardResolveOptions.RESOLVE_FILE));        assertEquals(toResourceIdentifier("file:/aa"), toResourceIdentifier("file:///").resolve("aa", StandardResolveOptions.RESOLVE_FILE));}
public void beam_f19649_0()
{    assertTrue(toResourceIdentifier("/").isDirectory());    assertTrue(toResourceIdentifier("/root/tmp/").isDirectory());    assertFalse(toResourceIdentifier("/root").isDirectory());}
public void beam_f19650_0() throws Exception
{    File someFile = tmpFolder.newFile("somefile");    LocalResourceId fileResource = LocalResourceId.fromPath(someFile.toPath(), /* isDirectory */    false);    assertThat(fileResource.toString(), not(endsWith(File.separator)));    assertThat(fileResource.toString(), containsString("somefile"));    assertThat(fileResource.toString(), startsWith(tmpFolder.getRoot().getAbsolutePath()));    LocalResourceId dirResource = LocalResourceId.fromPath(someFile.toPath(), /* isDirectory */    true);    assertThat(dirResource.toString(), endsWith(File.separator));    assertThat(dirResource.toString(), containsString("somefile"));    assertThat(dirResource.toString(), startsWith(tmpFolder.getRoot().getAbsolutePath()));}
protected long beam_f19659_0()
{    return current;}
public boolean beam_f19660_0()
{    current = getCurrentSource().getStartOffset();    while (current % granularity != 0) {        ++current;    }    return true;}
public void beam_f19670_0() throws Exception
{    long start = 0;    long end = 1000;    long minBundleSize = 50;    CoarseRangeSource testSource = new CoarseRangeSource(start, end, minBundleSize, 1);        long[] boundaries = { 0, 110, 220, 330, 440, 550, 660, 770, 880, 1000 };    assertSplitsAre(testSource.split(110 * testSource.getBytesPerOffset(), null), boundaries);}
public void beam_f19671_0() throws IOException
{                PipelineOptions options = PipelineOptionsFactory.create();    CoarseRangeSource source = new CoarseRangeSource(13, 35, 1, 10);    try (CoarseRangeReader reader = source.createReader(options)) {        List<Integer> items = new ArrayList<>();        assertEquals(0.0, reader.getFractionConsumed(), 1e-6);        assertTrue(reader.start());        items.add(reader.getCurrent());        while (reader.advance()) {            Double fraction = reader.getFractionConsumed();            assertNotNull(fraction);            assertTrue(fraction.toString(), fraction > 0.0);            assertTrue(fraction.toString(), fraction <= 1.0);            items.add(reader.getCurrent());        }        assertEquals(1.0, reader.getFractionConsumed(), 1e-6);        assertEquals(20, items.size());        assertEquals(20, items.get(0).intValue());        assertEquals(39, items.get(items.size() - 1).intValue());        source = new CoarseRangeSource(13, 17, 1, 10);    }    try (BoundedSource.BoundedReader<Integer> reader = source.createReader(options)) {        assertFalse(reader.start());    }}
public void beam_f19680_0()
{    double delta = 0.0000001;    double[] testFractions = new double[] { 0.01, 0.1, 0.123, 0.2, 0.3, 0.45738, 0.5, 0.6, 0.7182, 0.8, 0.95, 0.97, 0.99 };    ByteKey last = range.getStartKey();    for (double fraction : testFractions) {        String message = Double.toString(fraction);        try {            ByteKey key = range.interpolateKey(fraction);            assertThat(message, key, greaterThanOrEqualTo(last));            assertThat(message, range.estimateFractionForKey(key), closeTo(fraction, delta));            last = key;        } catch (IllegalStateException e) {            assertThat(message, e.getMessage(), containsString("near-empty ByteKeyRange"));        }    }}
private static void beam_f19681_0(ByteKeyRange left, ByteKeyRange right)
{    bidirectionalOverlapHelper(left, right, false);}
public void beam_f19690_0()
{    final double delta = 0.0000001;    /* 0x80 is halfway between [] and [] */    assertEquals(0.5, ByteKeyRange.ALL_KEYS.estimateFractionForKey(ByteKey.of(0x80)), delta);    /* 0x80 is halfway between [00] and [] */    ByteKeyRange after0 = ByteKeyRange.of(ByteKey.of(0), ByteKey.EMPTY);    assertEquals(0.5, after0.estimateFractionForKey(ByteKey.of(0x80)), delta);    /* 0x80 is halfway between [0000] and [] */    ByteKeyRange after00 = ByteKeyRange.of(ByteKey.of(0, 0), ByteKey.EMPTY);    assertEquals(0.5, after00.estimateFractionForKey(ByteKey.of(0x80)), delta);    /* 0x7f is halfway between [] and [fe] */    ByteKeyRange upToFE = ByteKeyRange.of(ByteKey.EMPTY, ByteKey.of(0xfe));    assertEquals(0.5, upToFE.estimateFractionForKey(ByteKey.of(0x7f)), delta);    /* 0x40 is one-quarter of the way between [] and [] */    assertEquals(0.25, ByteKeyRange.ALL_KEYS.estimateFractionForKey(ByteKey.of(0x40)), delta);    /* 0x40 is one-half of the way between [] and [0x80] */    ByteKeyRange upTo80 = ByteKeyRange.of(ByteKey.EMPTY, ByteKey.of(0x80));    assertEquals(0.50, upTo80.estimateFractionForKey(ByteKey.of(0x40)), delta);    /* 0x40 is one-half of the way between [0x30] and [0x50] */    ByteKeyRange range30to50 = ByteKeyRange.of(ByteKey.of(0x30), ByteKey.of(0x50));    assertEquals(0.50, range30to50.estimateFractionForKey(ByteKey.of(0x40)), delta);    /* 0x40 is one-half of the way between [0x30, 0, 1] and [0x4f, 0xff, 0xff, 0, 0] */    ByteKeyRange range31to4f = ByteKeyRange.of(ByteKey.of(0x30, 0, 1), ByteKey.of(0x4f, 0xff, 0xff, 0, 0));    assertEquals(0.50, range31to4f.estimateFractionForKey(ByteKey.of(0x40)), delta);    /* Exact fractions from 0 to 47 for a prime range. */    ByteKeyRange upTo47 = ByteKeyRange.of(ByteKey.EMPTY, ByteKey.of(47));    for (int i = 0; i <= 47; ++i) {        assertEquals("i=" + i, i / 47.0, upTo47.estimateFractionForKey(ByteKey.of(i)), delta);    }    /* Exact fractions from 0 to 83 for a prime range. */    ByteKeyRange rangeFDECtoFDEC83 = ByteKeyRange.of(ByteKey.of(0xfd, 0xec), ByteKey.of(0xfd, 0xec, 83));    for (int i = 0; i <= 83; ++i) {        assertEquals("i=" + i, i / 83.0, rangeFDECtoFDEC83.estimateFractionForKey(ByteKey.of(0xfd, 0xec, i)), delta);    }}
public void beam_f19691_0()
{    /* 0x80 is halfway between [] and [] */    assertEqualExceptPadding(ByteKey.of(0x80), ByteKeyRange.ALL_KEYS.interpolateKey(0.5));    /* 0x80 is halfway between [00] and [] */    ByteKeyRange after0 = ByteKeyRange.of(ByteKey.of(0), ByteKey.EMPTY);    assertEqualExceptPadding(ByteKey.of(0x80), after0.interpolateKey(0.5));    /* 0x80 is halfway between [0000] and [] -- padding to longest key */    ByteKeyRange after00 = ByteKeyRange.of(ByteKey.of(0, 0), ByteKey.EMPTY);    assertEqualExceptPadding(ByteKey.of(0x80), after00.interpolateKey(0.5));    /* 0x7f is halfway between [] and [fe] */    ByteKeyRange upToFE = ByteKeyRange.of(ByteKey.EMPTY, ByteKey.of(0xfe));    assertEqualExceptPadding(ByteKey.of(0x7f), upToFE.interpolateKey(0.5));    /* 0x40 is one-quarter of the way between [] and [] */    assertEqualExceptPadding(ByteKey.of(0x40), ByteKeyRange.ALL_KEYS.interpolateKey(0.25));    /* 0x40 is halfway between [] and [0x80] */    ByteKeyRange upTo80 = ByteKeyRange.of(ByteKey.EMPTY, ByteKey.of(0x80));    assertEqualExceptPadding(ByteKey.of(0x40), upTo80.interpolateKey(0.5));    /* 0x40 is halfway between [0x30] and [0x50] */    ByteKeyRange range30to50 = ByteKeyRange.of(ByteKey.of(0x30), ByteKey.of(0x50));    assertEqualExceptPadding(ByteKey.of(0x40), range30to50.interpolateKey(0.5));    /* 0x40 is halfway between [0x30, 0, 1] and [0x4f, 0xff, 0xff, 0, 0]  */    ByteKeyRange range31to4f = ByteKeyRange.of(ByteKey.of(0x30, 0, 1), ByteKey.of(0x4f, 0xff, 0xff, 0, 0));    assertEqualExceptPadding(ByteKey.of(0x40), range31to4f.interpolateKey(0.5));}
public void beam_f19700_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(INITIAL_RANGE);    tracker.tryReturnRecordAt(true, NEW_START_KEY);    String expected = String.format("ByteKeyRangeTracker{range=%s, position=%s}", NEW_RANGE, NEW_START_KEY);    assertEquals(expected, tracker.toString());}
public void beam_f19701_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(INITIAL_RANGE);    assertEquals(INITIAL_START_KEY, tracker.getStartPosition());    assertEquals(END_KEY, tracker.getStopPosition());}
public void beam_f19710_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(INITIAL_RANGE);        assertFalse(tracker.trySplitAtPosition(INITIAL_MIDDLE_KEY));        assertTrue(tracker.tryReturnRecordAt(true, INITIAL_START_KEY));    assertTrue(tracker.trySplitAtPosition(BEFORE_END_KEY));    assertEquals(BEFORE_END_KEY, tracker.getStopPosition());        assertFalse(tracker.trySplitAtPosition(END_KEY));        assertTrue(tracker.tryReturnRecordAt(true, INITIAL_MIDDLE_KEY));    assertFalse(tracker.trySplitAtPosition(INITIAL_MIDDLE_KEY));    assertTrue(tracker.tryReturnRecordAt(true, INITIAL_MIDDLE_KEY));}
public void beam_f19711_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(INITIAL_RANGE);    assertEquals(0, tracker.getSplitPointsConsumed());        assertTrue(tracker.tryReturnRecordAt(true, INITIAL_START_KEY));    assertEquals(0, tracker.getSplitPointsConsumed());        assertTrue(tracker.tryReturnRecordAt(true, AFTER_START_KEY));    assertEquals(1, tracker.getSplitPointsConsumed());        assertTrue(tracker.tryReturnRecordAt(false, INITIAL_MIDDLE_KEY));    assertEquals(1, tracker.getSplitPointsConsumed());        assertTrue(tracker.tryReturnRecordAt(true, BEFORE_END_KEY));    assertEquals(2, tracker.getSplitPointsConsumed());        tracker.markDone();    assertEquals(3, tracker.getSplitPointsConsumed());}
public void beam_f19720_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(3, 6);    assertTrue(tracker.tryReturnRecordAt(true, 3));    assertTrue(tracker.tryReturnRecordAt(true, 4));    assertTrue(tracker.tryReturnRecordAt(true, 5));    assertFalse(tracker.tryReturnRecordAt(true, 6));}
public void beam_f19721_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(9, 18);        assertTrue(tracker.tryReturnRecordAt(true, 10));    assertTrue(tracker.tryReturnRecordAt(false, 12));    assertTrue(tracker.tryReturnRecordAt(false, 14));    assertTrue(tracker.tryReturnRecordAt(true, 16));        assertTrue(tracker.tryReturnRecordAt(false, 18));    assertTrue(tracker.tryReturnRecordAt(false, 20));        assertFalse(tracker.tryReturnRecordAt(true, 22));}
public void beam_f19730_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(100, 200);    expected.expect(IllegalStateException.class);    tracker.tryReturnRecordAt(false, 120);}
public void beam_f19731_0() throws Exception
{    OffsetRangeTracker tracker = new OffsetRangeTracker(100, 200);    tracker.tryReturnRecordAt(true, 120);    expected.expect(IllegalStateException.class);    tracker.tryReturnRecordAt(true, 110);}
public long beam_f19740_0(PipelineOptions options) throws Exception
{    return 0;}
public BoundedReader<String> beam_f19741_0(PipelineOptions options) throws IOException
{    return null;}
public void beam_f19751_0() throws Exception
{    for (int i = 0; i < 10; ++i) {        SerializableAvroCodecFactory codecFactory = new SerializableAvroCodecFactory(CodecFactory.deflateCodec(i));        SerializableAvroCodecFactory serdeC = SerializableUtils.clone(codecFactory);        assertEquals(CodecFactory.deflateCodec(i).toString(), serdeC.getCodec().toString());    }}
public void beam_f19752_0() throws Exception
{    for (int i = 0; i < 10; ++i) {        SerializableAvroCodecFactory codecFactory = new SerializableAvroCodecFactory(CodecFactory.xzCodec(i));        SerializableAvroCodecFactory serdeC = SerializableUtils.clone(codecFactory);        assertEquals(CodecFactory.xzCodec(i).toString(), serdeC.getCodec().toString());    }}
protected void beam_f19761_0(WritableByteChannel channel)
{    this.channel = channel;}
protected void beam_f19762_0() throws Exception
{    channel.write(wrap(HEADER));}
private static String beam_f19771_0(Compression compression)
{    switch(compression) {        case UNCOMPRESSED:            return ".txt";        case GZIP:            return ".gz";        case BZIP2:            return ".bz2";        case ZIP:            return ".zip";        case DEFLATE:            return ".deflate";        default:            return "";    }}
public static Iterable<Object[]> beam_f19772_0()
{    return ImmutableList.<Object[]>builder().add(new Object[] { EMPTY, UNCOMPRESSED }).add(new Object[] { EMPTY, GZIP }).add(new Object[] { EMPTY, BZIP2 }).add(new Object[] { EMPTY, ZIP }).add(new Object[] { EMPTY, DEFLATE }).add(new Object[] { TINY, UNCOMPRESSED }).add(new Object[] { TINY, GZIP }).add(new Object[] { TINY, BZIP2 }).add(new Object[] { TINY, ZIP }).add(new Object[] { TINY, DEFLATE }).add(new Object[] { LARGE, UNCOMPRESSED }).add(new Object[] { LARGE, GZIP }).add(new Object[] { LARGE, BZIP2 }).add(new Object[] { LARGE, ZIP }).add(new Object[] { LARGE, DEFLATE }).build();}
private void beam_f19781_0(String[] expected) throws Exception
{    File tmpFile = tempFolder.newFile();    String filename = tmpFile.getPath();    try (PrintStream writer = new PrintStream(new FileOutputStream(tmpFile))) {        for (String elem : expected) {            byte[] encodedElem = CoderUtils.encodeToByteArray(StringUtf8Coder.of(), elem);            String line = new String(encodedElem, Charsets.UTF_8);            writer.println(line);        }    }    TextIO.Read read = TextIO.read().from(filename);    PCollection<String> output = p.apply(read);    PAssert.that(output).containsInAnyOrder(expected);    p.run();}
public void beam_f19782_0()
{    assertFalse(TextIO.Read.isSelfOverlapping(new byte[] { 'a', 'b', 'c' }));    assertFalse(TextIO.Read.isSelfOverlapping(new byte[] { 'c', 'a', 'b', 'd', 'a', 'b' }));    assertFalse(TextIO.Read.isSelfOverlapping(new byte[] { 'a', 'b', 'c', 'a', 'b', 'd' }));    assertTrue(TextIO.Read.isSelfOverlapping(new byte[] { 'a', 'b', 'a' }));    assertTrue(TextIO.Read.isSelfOverlapping(new byte[] { 'a', 'b', 'c', 'a', 'b' }));}
public void beam_f19791_0() throws Exception
{    TextIO.Read read = TextIO.read().from("/tmp/test");    assertEquals(AUTO, read.getCompression());    read = TextIO.read().from("/tmp/test").withCompression(GZIP);    assertEquals(GZIP, read.getCompression());}
public void beam_f19792_0() throws Exception
{    File smallGzNotCompressed = writeToFile(TINY, tempFolder, "tiny_uncompressed.gz", UNCOMPRESSED);        assertReadingCompressedFileMatchesExpected(smallGzNotCompressed, GZIP, TINY, p);    p.run();}
public void beam_f19801_0() throws IOException
{    String file = "line1\nline2\nline3";    BoundedSource<String> source = prepareSource(file.getBytes(Charsets.UTF_8));    BoundedSource<String> remainder;        try (BoundedSource.BoundedReader<String> readerOrig = source.createReader(PipelineOptionsFactory.create())) {                assertEquals(0.0, readerOrig.getFractionConsumed(), 1e-6);        assertEquals(0, readerOrig.getSplitPointsConsumed());        assertEquals(BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, readerOrig.getSplitPointsRemaining());                assertTrue(readerOrig.start());        assertEquals(0, readerOrig.getSplitPointsConsumed());        assertEquals(BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, readerOrig.getSplitPointsRemaining());                remainder = readerOrig.splitAtFraction(0.1);        System.err.println(readerOrig.getCurrentSource());        assertNotNull(remainder);                assertEquals(0, readerOrig.getSplitPointsConsumed());        assertEquals(1, readerOrig.getSplitPointsRemaining());                assertFalse(readerOrig.advance());        assertEquals(1.0, readerOrig.getFractionConsumed(), 1e-6);        assertEquals(1, readerOrig.getSplitPointsConsumed());        assertEquals(0, readerOrig.getSplitPointsRemaining());    }        try (BoundedSource.BoundedReader<String> reader = remainder.createReader(PipelineOptionsFactory.create())) {                assertEquals(0.0, reader.getFractionConsumed(), 1e-6);        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());                assertTrue(reader.start());        assertEquals(0, reader.getSplitPointsConsumed());        assertEquals(BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());                assertTrue(reader.advance());        assertEquals(1, reader.getSplitPointsConsumed());        assertEquals(1, reader.getSplitPointsRemaining());                assertFalse(reader.advance());        assertEquals(1.0, reader.getFractionConsumed(), 1e-6);        assertEquals(2, reader.getSplitPointsConsumed());        assertEquals(0, reader.getSplitPointsRemaining());    }}
public void beam_f19802_0() throws Exception
{    PipelineOptions options = TestPipeline.testingPipelineOptions();    long desiredBundleSize = 1000;    File largeTxt = writeToFile(LARGE, tempFolder, "large.txt", UNCOMPRESSED);        assertThat(largeTxt.length(), greaterThan(2 * desiredBundleSize));    FileBasedSource<String> source = TextIO.read().from(largeTxt.getPath()).getSource();    List<? extends FileBasedSource<String>> splits = source.split(desiredBundleSize, options);        assertThat(splits, hasSize(greaterThan(1)));    SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);}
public Coder<String> beam_f19811_0()
{    return StringUtf8Coder.of();}
public FileBasedSink.FilenamePolicy beam_f19812_0(String destination)
{    return DefaultFilenamePolicy.fromStandardParameters(ValueProvider.StaticValueProvider.of(baseDir.resolve("file_" + destination + ".txt", ResolveOptions.StandardResolveOptions.RESOLVE_FILE)), null, null, false);}
public void beam_f19821_0() throws Exception
{    ResourceId baseDir = FileSystems.matchNewResource(Files.createTempDirectory(tempFolder.getRoot().toPath(), "testDynamicDestinations").toString(), true);    List<UserWriteType> elements = Lists.newArrayList(new UserWriteType("aaaa", "first"), new UserWriteType("aaab", "second"), new UserWriteType("baaa", "third"), new UserWriteType("baab", "fourth"), new UserWriteType("caaa", "fifth"), new UserWriteType("caab", "sixth"));    PCollection<UserWriteType> input = p.apply(Create.of(elements));    input.apply(TextIO.<UserWriteType>writeCustomType().to(new UserWriteDestination(baseDir), new DefaultFilenamePolicy.Params().withBaseFilename(baseDir.resolve("empty", ResolveOptions.StandardResolveOptions.RESOLVE_FILE))).withFormatFunction(new SerializeUserWrite()).withTempDirectory(FileSystems.matchNewResource(baseDir.toString(), true)));    p.run();    String[] aElements = Iterables.toArray(StreamSupport.stream(elements.stream().filter(Predicates.compose(new StartsWith("a"), new ExtractWriteDestination())::apply).collect(Collectors.toList()).spliterator(), false).map(Functions.toStringFunction()::apply).collect(Collectors.toList()), String.class);    String[] bElements = Iterables.toArray(StreamSupport.stream(elements.stream().filter(Predicates.compose(new StartsWith("b"), new ExtractWriteDestination())::apply).collect(Collectors.toList()).spliterator(), false).map(Functions.toStringFunction()::apply).collect(Collectors.toList()), String.class);    String[] cElements = Iterables.toArray(StreamSupport.stream(elements.stream().filter(Predicates.compose(new StartsWith("c"), new ExtractWriteDestination())::apply).collect(Collectors.toList()).spliterator(), false).map(Functions.toStringFunction()::apply).collect(Collectors.toList()), String.class);    assertOutputFiles(aElements, null, null, 0, baseDir.resolve("file_a.txt", ResolveOptions.StandardResolveOptions.RESOLVE_FILE), DefaultFilenamePolicy.DEFAULT_UNWINDOWED_SHARD_TEMPLATE);    assertOutputFiles(bElements, null, null, 0, baseDir.resolve("file_b.txt", ResolveOptions.StandardResolveOptions.RESOLVE_FILE), DefaultFilenamePolicy.DEFAULT_UNWINDOWED_SHARD_TEMPLATE);    assertOutputFiles(cElements, null, null, 0, baseDir.resolve("file_c.txt", ResolveOptions.StandardResolveOptions.RESOLVE_FILE), DefaultFilenamePolicy.DEFAULT_UNWINDOWED_SHARD_TEMPLATE);}
private void beam_f19822_0(String[] elems) throws Exception
{    runTestWrite(elems, null, null, 1);}
public void beam_f19831_0() throws Exception
{    runTestWrite(LINES_ARRAY);}
public void beam_f19832_0() throws Exception
{    runTestWrite(NO_LINES_ARRAY, 0);}
public void beam_f19841_0()
{    TextIO.Write write = TextIO.write().to("foo").withFooter("myFooter");    DisplayData displayData = DisplayData.from(write);    assertThat(displayData, hasDisplayItem("fileFooter", "myFooter"));}
public void beam_f19842_0()
{    assertEquals("TextIO.Write", TextIO.write().to("somefile").getName());}
public void beam_f19851_0() throws Exception
{    TextRowCountEstimator textRowCountEstimator = TextRowCountEstimator.builder().setFilePattern(temporaryFolder.getRoot() + "/something/**").build();    Double rows = textRowCountEstimator.estimateRowCount(PipelineOptionsFactory.create());    Assert.assertNull(rows);}
public void beam_f19852_0()
{    readPipeline.enableAbandonedNodeEnforcement(false);    assertEquals("TFRecordIO.Read/Read.out", readPipeline.apply(TFRecordIO.read().from("foo.*").withoutValidation()).getName());    assertEquals("MyRead/Read.out", readPipeline.apply("MyRead", TFRecordIO.read().from("foo.*").withoutValidation()).getName());}
public void beam_f19861_0() throws Exception
{    expectedException.expectCause(instanceOf(IOException.class));    expectedException.expectCause(hasMessage(containsString("Mismatch of length mask")));    byte[] data = BaseEncoding.base64().decode(FOO_RECORD_BASE64);    data[9] += (byte) 1;    runTestRead(data, FOO_RECORDS);}
public void beam_f19862_0() throws Exception
{    expectedException.expectCause(instanceOf(IOException.class));    expectedException.expectCause(hasMessage(containsString("Mismatch of data mask")));    byte[] data = BaseEncoding.base64().decode(FOO_RECORD_BASE64);    data[16] += (byte) 1;    runTestRead(data, FOO_RECORDS);}
public void beam_f19871_0() throws IOException
{    runTestRoundTrip(LARGE, 10, ".tfrecords", DEFLATE, DEFLATE);}
public void beam_f19872_0() throws IOException
{    runTestRoundTrip(LARGE, 10, ".tfrecords", UNCOMPRESSED, AUTO);}
public String beam_f19881_0(String input)
{    return input;}
public PCollectionView<Integer> beam_f19882_0(PCollection<String> input)
{    return null;}
private ResourceId beam_f19891_0()
{    return LocalResources.fromFile(tmpFolder.getRoot(), true).resolve("output", StandardResolveOptions.RESOLVE_DIRECTORY);}
private SimpleSink<Void> beam_f19892_0()
{    FilenamePolicy filenamePolicy = new PerWindowFiles(getBaseOutputDirectory().resolve("file", StandardResolveOptions.RESOLVE_FILE), "simple");    return SimpleSink.makeSimpleSink(getBaseOutputDirectory(), filenamePolicy);}
public void beam_f19901_0() throws IOException
{    List<String> inputs = Lists.newArrayList();    for (int i = 0; i < 100; ++i) {        inputs.add("mambo_number_" + i);    }    runWrite(inputs, Window.into(FixedWindows.of(Duration.millis(1))), getBaseOutputFilename(), WriteFiles.to(makeSimpleSink()).withMaxNumWritersPerBundle(2).withWindowedWrites().withNoSpilling());}
public void beam_f19902_0()
{    SimpleSink<Void> sink = makeSimpleSink();    WriteFiles<String, ?, String> write = WriteFiles.to(sink).withNumShards(3);    assertThat((SimpleSink<Void>) write.getSink(), is(sink));    PTransform<PCollection<String>, PCollectionView<Integer>> originalSharding = write.getComputeNumShards();    assertThat(write.getComputeNumShards(), is(nullValue()));    assertThat(write.getNumShardsProvider(), instanceOf(StaticValueProvider.class));    assertThat(write.getNumShardsProvider().get(), equalTo(3));    assertThat(write.getComputeNumShards(), equalTo(originalSharding));    WriteFiles<String, ?, ?> write2 = write.withSharding(SHARDING_TRANSFORM);    assertThat((SimpleSink<Void>) write2.getSink(), is(sink));    assertThat(write2.getComputeNumShards(), equalTo(SHARDING_TRANSFORM));        WriteFiles<String, ?, ?> writeUnsharded = write2.withRunnerDeterminedSharding();    assertThat(writeUnsharded.getComputeNumShards(), nullValue());    assertThat(write.getComputeNumShards(), equalTo(originalSharding));}
public void beam_f19911_0() throws Exception
{    testDynamicDestinationsHelper(true, false);}
public void beam_f19912_0() throws Exception
{    testDynamicDestinationsHelper(false, false);}
private void beam_f19921_0(List<String> inputs, PTransform<PCollection<String>, PCollection<String>> transform, String baseName, WriteFiles<String, ?, String> write) throws IOException
{    runShardedWrite(inputs, transform, baseName, write);}
public String beam_f19922_0(IntervalWindow window)
{    String prefix = baseFilename.isDirectory() ? "" : firstNonNull(baseFilename.getFilename(), "");    return String.format("%s%s-%s", prefix, FORMATTER.print(window.start()), FORMATTER.print(window.end()));}
public ShardedKey<Integer> beam_f19931_0(Void destination, String element, int shardCount) throws Exception
{    int shard = Integer.parseInt(element);    return ShardedKey.of(0, shard);}
private boolean beam_f19932_0(String actualScope, String subPath)
{    return MetricFiltering.subPathMatches(actualScope, subPath);}
public static Matcher<MetricResult<T>> beam_f19941_0(final String namespace, final String name, final String step, final T value)
{    return metricsResult(namespace, name, step, value, true);}
public static Matcher<MetricResult<T>> beam_f19942_0(final String namespace, final String name, final String step, final Matcher<T> valueMatcher, final boolean isCommitted)
{    final String metricState = isCommitted ? "committed" : "attempted";    return new MatchNameAndKey<T>(namespace, name, step) {        @Override        protected boolean matchesSafely(MetricResult<T> item) {            final T metricValue = isCommitted ? item.getCommitted() : item.getAttempted();            return super.matchesSafely(item) && valueMatcher.matches(metricValue);        }        @Override        public void describeTo(Description description) {            super.describeTo(description);            description.appendText(String.format(", %s=", metricState));            valueMatcher.describeTo(description);            description.appendText("}");        }        @Override        protected void describeMismatchSafely(MetricResult<T> item, Description mismatchDescription) {            final T metricValue = isCommitted ? item.getCommitted() : item.getAttempted();            super.describeMismatchSafely(item, mismatchDescription);            mismatchDescription.appendText(String.format("%s: ", metricState));            valueMatcher.describeMismatch(metricValue, mismatchDescription);            mismatchDescription.appendText("}");        }    };}
 static Matcher<MetricResult<DistributionResult>> beam_f19951_0(final String namespace, final String name, final String step, final Long attemptedMin, final Long attemptedMax)
{    return distributionMinMax(namespace, name, step, attemptedMin, attemptedMax, false);}
 static Matcher<MetricResult<DistributionResult>> beam_f19952_0(final String namespace, final String name, final String step, final Long committedMin, final Long committedMax)
{    return distributionMinMax(namespace, name, step, committedMin, committedMax, true);}
public void beam_f19961_0()
{    Counter counter = Metrics.counter("ns", "name");    MetricsContainer c1 = Mockito.mock(MetricsContainer.class);    MetricsContainer c2 = Mockito.mock(MetricsContainer.class);    Counter counter1 = Mockito.mock(Counter.class);    Counter counter2 = Mockito.mock(Counter.class);    when(c1.getCounter(MetricName.named("ns", "name"))).thenReturn(counter1);    when(c2.getCounter(MetricName.named("ns", "name"))).thenReturn(counter2);    MetricsEnvironment.setCurrentContainer(c1);    counter.inc();    MetricsEnvironment.setCurrentContainer(c2);    counter.dec();    MetricsEnvironment.setCurrentContainer(null);    verify(counter1).inc(1L);    verify(counter2).inc(-1L);    verifyNoMoreInteractions(counter1, counter2);}
public void beam_f19962_0()
{    assertNull(MetricsEnvironment.getCurrentContainer());}
public void beam_f19971_0()
{    assertNull(MetricsEnvironment.getCurrentContainer());        Counter counter = Metrics.counter(NS, NAME);    counter.inc();    counter.inc(5L);    counter.dec();    counter.dec(5L);}
public void beam_f19972_0()
{    thrown.expect(IllegalArgumentException.class);    Metrics.counter(NS, "");}
public void beam_f19981_0()
{    PipelineResult result = runPipelineWithMetrics();    MetricQueryResults metrics = queryTestMetrics(result);    assertGaugeMetrics(metrics, true);}
public void beam_f19982_0()
{    long numElements = 1000;    pipeline.apply(GenerateSequence.from(0).to(numElements));    PipelineResult pipelineResult = pipeline.run();    MetricQueryResults metrics = pipelineResult.metrics().queryMetrics(MetricsFilter.builder().addNameFilter(MetricNameFilter.named(ELEMENTS_READ.getNamespace(), ELEMENTS_READ.getName())).build());    assertThat(metrics.getCounters(), hasItem(attemptedMetricsResult(ELEMENTS_READ.getNamespace(), ELEMENTS_READ.getName(), "Read(BoundedCountingSource)", 1000L)));}
private static void beam_f19991_0(MetricQueryResults metrics, boolean isCommitted)
{    assertCounterMetrics(metrics, isCommitted);    assertDistributionMetrics(metrics, isCommitted);    assertGaugeMetrics(metrics, isCommitted);}
public void beam_f19992_0()
{    ExperimentalOptions options = PipelineOptionsFactory.fromArgs("--experiments=experimentA,experimentB").as(ExperimentalOptions.class);    assertTrue(ExperimentalOptions.hasExperiment(options, "experimentA"));    assertTrue(ExperimentalOptions.hasExperiment(options, "experimentB"));    assertFalse(ExperimentalOptions.hasExperiment(options, "experimentC"));}
public void beam_f20001_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Expected getter for property [object] of type [java.lang.Object] on " + "[org.apache.beam.sdk.options.PipelineOptionsFactoryTest$MissingGetter].");    PipelineOptionsFactory.as(MissingGetter.class);}
public void beam_f20002_0()
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("missing property methods on [org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MissingMultipleGetters]");    expectedException.expectMessage("getter for property [object] of type [java.lang.Object]");    expectedException.expectMessage("getter for property [otherObject] of type [java.lang.Object]");    PipelineOptionsFactory.as(MissingMultipleGetters.class);}
public void beam_f20011_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("[org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultiReturnTypeConflict]");    expectedException.expectMessage("Methods with multiple definitions with different return types");    expectedException.expectMessage("Method [getObject] has multiple definitions");    expectedException.expectMessage("public abstract java.lang.Object " + "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$" + "MissingSetter.getObject()");    expectedException.expectMessage("public abstract java.lang.String org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultiReturnTypeConflict.getObject()");    expectedException.expectMessage("Method [getOther] has multiple definitions");    expectedException.expectMessage("public abstract java.lang.Object " + "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$" + "MultiReturnTypeConflictBase.getOther()");    expectedException.expectMessage("public abstract java.lang.Long org.apache.beam.sdk.options." + "PipelineOptionsFactoryTest$MultiReturnTypeConflict.getOther()");    PipelineOptionsFactory.as(MultiReturnTypeConflict.class);}
public void beam_f20012_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Expected setter for property [value] to not be marked with @JsonIgnore on [" + "org.apache.beam.sdk.options.PipelineOptionsFactoryTest$SetterWithJsonIgnore]");    PipelineOptionsFactory.as(SetterWithJsonIgnore.class);}
public void beam_f20021_0() throws Exception
{        GetterWithDefault options = PipelineOptionsFactory.as(GetterWithDefault.class);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Property [object] is marked with contradictory annotations. Found [" + "[Default.Integer(value=1) on org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$GetterWithDefault#getObject()], " + "[Default.Integer(value=0) on org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$GetterWithInconsistentDefaultValue#getObject()]].");        options.as(GetterWithInconsistentDefaultValue.class);}
public void beam_f20022_0() throws Exception
{        GetterWithJsonIgnore options = PipelineOptionsFactory.as(GetterWithJsonIgnore.class);    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Property [object] is marked with contradictory annotations. Found [" + "[JsonIgnore(value=false) on org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$GetterWithInconsistentJsonIgnoreValue#getObject()], " + "[JsonIgnore(value=true) on org.apache.beam.sdk.options.PipelineOptionsFactoryTest" + "$GetterWithJsonIgnore#getObject()]].");        options.as(GetterWithInconsistentJsonIgnoreValue.class);}
public void beam_f20031_0()
{    String[] args = new String[] { "--stringValue=beam" };    String[] emptyArgs = new String[] { "--stringValue=" };    Objects options = PipelineOptionsFactory.fromArgs(args).as(Objects.class);    assertEquals("beam", options.getStringValue().get());    options = PipelineOptionsFactory.fromArgs(emptyArgs).as(Objects.class);    assertEquals("", options.getStringValue().get());}
public void beam_f20032_0()
{    String[] args = new String[] { "--longValue=12345678762" };    String[] emptyArgs = new String[] { "--longValue=" };    Objects options = PipelineOptionsFactory.fromArgs(args).as(Objects.class);    assertEquals(Long.valueOf(12345678762L), options.getLongValue().get());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage(emptyStringErrorMessage());    PipelineOptionsFactory.fromArgs(emptyArgs).as(Objects.class);}
public void beam_f20041_0()
{    String[] args = new String[] { "--stringValue=abc", "--stringValue=xyz" };    String[] commaArgs = new String[] { "--stringValue=abc,xyz" };    String[] emptyArgs = new String[] { "--stringValue=", "--stringValue=" };    Arrays options = PipelineOptionsFactory.fromArgs(args).as(Arrays.class);    assertArrayEquals(new String[] { "abc", "xyz" }, options.getStringValue().get());    options = PipelineOptionsFactory.fromArgs(commaArgs).as(Arrays.class);    assertArrayEquals(new String[] { "abc", "xyz" }, options.getStringValue().get());    options = PipelineOptionsFactory.fromArgs(emptyArgs).as(Arrays.class);    assertArrayEquals(new String[] { "", "" }, options.getStringValue().get());}
public void beam_f20042_0()
{    String[] args = new String[] { "--longValue=12345678762", "--longValue=12345678763" };    String[] commaArgs = new String[] { "--longValue=12345678762,12345678763" };    String[] emptyArgs = new String[] { "--longValue=", "--longValue=" };    Arrays options = PipelineOptionsFactory.fromArgs(args).as(Arrays.class);    assertArrayEquals(new Long[] { 12345678762L, 12345678763L }, options.getLongValue().get());    options = PipelineOptionsFactory.fromArgs(commaArgs).as(Arrays.class);    assertArrayEquals(new Long[] { 12345678762L, 12345678763L }, options.getLongValue().get());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage(emptyStringErrorMessage());    PipelineOptionsFactory.fromArgs(emptyArgs).as(Arrays.class);}
public void beam_f20051_0()
{    String[] args = new String[] { "--longValue=12345678762", "--longValue=12345678763" };    String[] commaArgs = new String[] { "--longValue=12345678762,12345678763" };    String[] emptyArgs = new String[] { "--longValue=", "--longValue=" };    Lists options = PipelineOptionsFactory.fromArgs(args).as(Lists.class);    assertEquals(ImmutableList.of(12345678762L, 12345678763L), options.getLongValue().get());    options = PipelineOptionsFactory.fromArgs(commaArgs).as(Lists.class);    assertEquals(ImmutableList.of(12345678762L, 12345678763L), options.getLongValue().get());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage(emptyStringErrorMessage());    PipelineOptionsFactory.fromArgs(emptyArgs).as(Lists.class);}
public void beam_f20052_0()
{    String[] args = new String[] { "--enumValue=" + TestEnum.Value, "--enumValue=" + TestEnum.Value2 };    String[] commaArgs = new String[] { "--enumValue=" + TestEnum.Value + "," + TestEnum.Value2 };    String[] emptyArgs = new String[] { "--enumValue=" };    Lists options = PipelineOptionsFactory.fromArgs(args).as(Lists.class);    assertEquals(ImmutableList.of(TestEnum.Value, TestEnum.Value2), options.getEnumValue().get());    options = PipelineOptionsFactory.fromArgs(commaArgs).as(Lists.class);    assertEquals(ImmutableList.of(TestEnum.Value, TestEnum.Value2), options.getEnumValue().get());    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage(emptyStringErrorMessage());    PipelineOptionsFactory.fromArgs(emptyArgs).as(Lists.class);}
public PipelineResult beam_f20061_0(Pipeline pipeline)
{    return null;}
public void beam_f20062_0()
{    String[] args = new String[] { String.format("--runner=%s", ExampleTestRunner.class.getName()) };    PipelineOptions opts = PipelineOptionsFactory.fromArgs(args).create();    assertEquals(opts.getRunner(), ExampleTestRunner.class);}
public void beam_f20071_0()
{    String[] args = new String[] { "--string=100", null, null, "--runner=" + REGISTERED_RUNNER.getSimpleName() };    PipelineOptionsFactory.fromArgs(args).as(Objects.class);}
public void beam_f20072_0()
{    String[] args = new String[] { "--=100" };    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Argument '--=100' starts with '--='");    PipelineOptionsFactory.fromArgs(args).create();}
public void beam_f20081_0()
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ListMultimap<String, String> arguments = ArrayListMultimap.create();    arguments.put("help", "org.apache.beam.sdk.Pipeline");    assertTrue(PipelineOptionsFactory.printHelpUsageAndExitIfNeeded(arguments, new PrintStream(baos), false));    String output = new String(baos.toByteArray(), Charsets.UTF_8);    assertThat(output, containsString("Unable to find option org.apache.beam.sdk.Pipeline"));    assertThat(output, containsString("The set of registered options are:"));    assertThat(output, containsString("org.apache.beam.sdk.options.PipelineOptions"));}
public void beam_f20082_0()
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    ListMultimap<String, String> arguments = ArrayListMultimap.create();    arguments.put("help", "org.apache.beam.sdk.option.DataflowPipelineOptions");    assertTrue(PipelineOptionsFactory.printHelpUsageAndExitIfNeeded(arguments, new PrintStream(baos), false));    String output = new String(baos.toByteArray(), Charsets.UTF_8);        assertThat(output, not(containsString("org.apache.beam.sdk.options.DataflowPipelineDebugOptions")));        assertThat(output, not(containsString("--gcpCredential")));}
public Iterable<Class<? extends PipelineOptions>> beam_f20091_0()
{    return ImmutableList.of(RegisteredTestOptions.class);}
public void beam_f20092_0() throws Exception
{    JacksonIncompatibleOptions options = PipelineOptionsFactory.fromArgs("--jacksonIncompatible=\"testValue\"").as(JacksonIncompatibleOptions.class);    assertEquals("testValue", options.getJacksonIncompatible().value);}
public void beam_f20101_0()
{    List<PipelineOptionDescriptor> described = PipelineOptionsFactory.describe(Sets.newHashSet(PipelineOptions.class, TestDescribeOptions.class));    Map<String, PipelineOptionDescriptor> mapped = uniqueIndex(described, input -> input.getName());    assertEquals("no duplicates", described.size(), mapped.size());    Collection<PipelineOptionDescriptor> filtered = Collections2.filter(described, input -> input.getGroup().equals(TestDescribeOptions.class.getName()));    assertEquals(6, filtered.size());    mapped = uniqueIndex(filtered, input -> input.getName());    PipelineOptionDescriptor listDesc = mapped.get("list");    assertThat(listDesc, notNullValue());    assertThat(listDesc.getDescription(), isEmptyString());    assertEquals(PipelineOptionType.Enum.ARRAY, listDesc.getType());    assertThat(listDesc.getDefaultValue(), isEmptyString());    PipelineOptionDescriptor stringDesc = mapped.get("string");    assertThat(stringDesc, notNullValue());    assertThat(stringDesc.getDescription(), isEmptyString());    assertEquals(PipelineOptionType.Enum.STRING, stringDesc.getType());    assertThat(stringDesc.getDefaultValue(), isEmptyString());    PipelineOptionDescriptor integerDesc = mapped.get("integer");    assertThat(integerDesc, notNullValue());    assertEquals("integer property", integerDesc.getDescription());    assertEquals(PipelineOptionType.Enum.INTEGER, integerDesc.getType());    assertThat(integerDesc.getDefaultValue(), isEmptyString());    PipelineOptionDescriptor floatDesc = mapped.get("float");    assertThat(integerDesc, notNullValue());    assertEquals("float number property", floatDesc.getDescription());    assertEquals(PipelineOptionType.Enum.NUMBER, floatDesc.getType());    assertThat(floatDesc.getDefaultValue(), isEmptyString());    PipelineOptionDescriptor booleanSimpleDesc = mapped.get("boolean_simple");    assertThat(booleanSimpleDesc, notNullValue());    assertEquals("simple boolean property", booleanSimpleDesc.getDescription());    assertEquals(PipelineOptionType.Enum.BOOLEAN, booleanSimpleDesc.getType());    assertThat(booleanSimpleDesc.getDefaultValue(), equalTo("true"));    PipelineOptionDescriptor booleanWrapperDesc = mapped.get("boolean_wrapper");    assertThat(booleanWrapperDesc, notNullValue());    assertThat(booleanWrapperDesc.getDescription(), isEmptyString());    assertEquals(PipelineOptionType.Enum.BOOLEAN, booleanWrapperDesc.getType());    assertThat(booleanWrapperDesc.getDefaultValue(), equalTo("false"));}
public void beam_f20102_0() throws NoSuchMethodException
{    Set<PipelineOptionSpec> properties = PipelineOptionsReflector.getOptionSpecs(SimpleOptions.class, true);    assertThat(properties, Matchers.hasItems(PipelineOptionSpec.of(SimpleOptions.class, "foo", SimpleOptions.class.getDeclaredMethod("getFoo"))));}
private static Matcher<PipelineOptionSpec> beam_f20111_0(Matcher<String> matcher)
{    return new FeatureMatcher<PipelineOptionSpec, String>(matcher, "name", "name") {        @Override        protected String featureValueOf(PipelineOptionSpec actual) {            return actual.getName();        }    };}
protected String beam_f20112_0(PipelineOptionSpec actual)
{    return actual.getName();}
public void beam_f20121_0()
{    Set<Long> ids = new HashSet<>();    for (int i = 0; i < 1000; ++i) {        long id = PipelineOptionsFactory.create().getOptionsId();        if (!ids.add(id)) {            fail(String.format("Generated duplicate id %s, existing generated ids %s", id, ids));        }    }}
public void beam_f20122_0()
{    PipelineOptions options = PipelineOptionsFactory.create();    String userAgent = options.getUserAgent();    assertNotNull(userAgent);    assertTrue(userAgent.contains(DEFAULT_USER_AGENT_NAME));}
public void beam_f20131_0() throws Exception
{    expectedException.expect(IllegalArgumentException.class);    expectedException.expectMessage("Missing required value for " + "[--object, \"Fake Description\"].");    SubClassValidation required = PipelineOptionsFactory.fromArgs(new String[] {}).as(SubClassValidation.class);    PipelineOptionsValidator.validateCli(Required.class, required);}
public void beam_f20132_0()
{    GroupRequired groupRequired = PipelineOptionsFactory.as(GroupRequired.class);    groupRequired.setFoo("foo");    groupRequired.setBar(null);    groupRequired.setRunner(CrashingRunner.class);    PipelineOptionsValidator.validate(GroupRequired.class, groupRequired);        groupRequired.setFoo(null);    groupRequired.setBar("bar");    PipelineOptionsValidator.validate(GroupRequired.class, groupRequired);}
public void beam_f20141_0()
{    SubOptions subOpts = PipelineOptionsFactory.as(SubOptions.class);    subOpts.setFoo("bar");    subOpts.setBar("bar");    subOpts.setSuperclassObj("SuperDuper");    subOpts.setRunner(CrashingRunner.class);    PipelineOptionsValidator.validate(SubOptions.class, subOpts);    PipelineOptionsValidator.validate(SuperOptions.class, subOpts);}
protected void beam_f20142_0()
{    PipelineOptionsFactory.resetCache();}
public void beam_f20151_0() throws Exception
{    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    StringWithDefault proxy = handler.as(StringWithDefault.class);    Long optionsId = proxy.getOptionsId();    proxy.setString("stringValue");    DefaultAnnotations proxy2 = proxy.as(DefaultAnnotations.class);    proxy2.setLong(57L);    Simple deserializedOptions = serializeDeserialize(Simple.class, proxy2);    deserializedOptions.setString("overriddenValue");    assertEquals(String.format("Current Settings:%n" + "  long: 57%n" + "  optionsId: %d%n" + "  string: overriddenValue%n", optionsId), deserializedOptions.toString());}
public void beam_f20152_0() throws Exception
{    expectedException.expect(RuntimeException.class);    expectedException.expectMessage("Unknown method [public abstract void " + "org.apache.beam.sdk.options.ProxyInvocationHandlerTest$UnknownMethod.unknownMethod()] " + "invoked with args [null].");    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    handler.invoke(handler, UnknownMethod.class.getMethod("unknownMethod"), null);}
public void beam_f20161_0() throws Exception
{    ProxyInvocationHandler handler = new ProxyInvocationHandler(Maps.newHashMap());    PartialMethodConflict partialMethodConflict = handler.as(PartialMethodConflict.class);        partialMethodConflict.setString("conflictValue");    assertEquals("conflictValue", partialMethodConflict.getString());    assertEquals("conflictValue", partialMethodConflict.as(Simple.class).getString());        partialMethodConflict.setPrimitive(5);    assertEquals(5, partialMethodConflict.getPrimitive());    assertEquals(5, partialMethodConflict.as(Simple.class).getPrimitive());}
public void beam_f20162_0()
{    Set<Class<? extends PipelineOptions>> defaultRegistry = new HashSet<>(PipelineOptionsFactory.getRegisteredOptions());    assertThat(defaultRegistry, not(hasItem(FooOptions.class)));    PipelineOptionsFactory.register(FooOptions.class);    assertThat(PipelineOptionsFactory.getRegisteredOptions(), hasItem(FooOptions.class));    PipelineOptionsFactory.resetCache();    assertEquals(defaultRegistry, PipelineOptionsFactory.getRegisteredOptions());}
public boolean beam_f20171_0(Object obj)
{    return obj != null && getClass().equals(obj.getClass()) && Objects.equals(doubleField, ((InnerType) obj).doubleField);}
public int beam_f20172_0()
{    return 0;}
public void beam_f20181_0()
{    PipelineOptions options = PipelineOptionsFactory.create();    options.setTempLocation("myTemp");    DisplayData displayData = DisplayData.from(options);    assertThat(displayData, hasDisplayItem(allOf(hasKey("tempLocation"), hasType(DisplayData.Type.STRING), hasValue("myTemp"), hasNamespace(PipelineOptions.class))));}
public void beam_f20182_0()
{    Instant now = Instant.now();    TypedOptions options = PipelineOptionsFactory.as(TypedOptions.class);    options.setInteger(1234);    options.setTimestamp(now);    options.setJavaClass(ProxyInvocationHandlerTest.class);    options.setObject(new Serializable() {        @Override        public String toString() {            return "foobar";        }    });    DisplayData displayData = DisplayData.from(options);    assertThat(displayData, hasDisplayItem("integer", 1234));    assertThat(displayData, hasDisplayItem("timestamp", now));    assertThat(displayData, hasDisplayItem("javaClass", ProxyInvocationHandlerTest.class));    assertThat(displayData, hasDisplayItem("object", "foobar"));}
public void beam_f20191_0()
{    HasDefaults options = PipelineOptionsFactory.as(HasDefaults.class);    assertEquals("bar", options.getFoo());    DisplayData data = DisplayData.from(options);    assertThat(data, not(hasDisplayItem("foo")));}
public void beam_f20192_0()
{    HasDefaults options = PipelineOptionsFactory.as(HasDefaults.class);    String defaultValue = options.getFoo();    options.setFoo(defaultValue);    DisplayData data = DisplayData.from(options);    assertThat(data, hasDisplayItem("foo", defaultValue));}
public void beam_f20201_0()
{    IgnoredProperty options = PipelineOptionsFactory.as(IgnoredProperty.class);    options.setValue("foobar");    DisplayData data = DisplayData.from(options);    assertThat(data, not(hasDisplayItem("value")));}
public void beam_f20202_0()
{    expectedException.expectCause(Matchers.instanceOf(NotSerializableException.class));    SerializableUtils.clone(new CapturesOptions());}
public void beam_f20211_0() throws Exception
{    TestOptions submitOptions = PipelineOptionsFactory.fromArgs("--string=baz", "--otherString=quux").as(TestOptions.class);    String serializedOptions = MAPPER.writeValueAsString(submitOptions);    String updatedOptions = ValueProviders.updateSerializedOptions(serializedOptions, ImmutableMap.of("string", "bar"));    TestOptions runtime = MAPPER.readValue(updatedOptions, PipelineOptions.class).as(TestOptions.class);    assertEquals("bar", runtime.getString());    assertEquals("quux", runtime.getOtherString());}
public void beam_f20212_0() throws Exception
{    TestOptions submitOptions = PipelineOptionsFactory.as(TestOptions.class);    String serializedOptions = MAPPER.writeValueAsString(submitOptions);    String updatedOptions = ValueProviders.updateSerializedOptions(serializedOptions, ImmutableMap.of());    TestOptions runtime = MAPPER.readValue(updatedOptions, PipelineOptions.class).as(TestOptions.class);    assertNull(runtime.getString());}
public void beam_f20221_0() throws Exception
{    TestOptions runtime = MAPPER.readValue("{ \"options\": { \"bar\": \"quux\" }}", PipelineOptions.class).as(TestOptions.class);    TestOptions options = PipelineOptionsFactory.as(TestOptions.class);    runtime.setOptionsId(options.getOptionsId());    RuntimeValueProvider.setRuntimeOptions(runtime);    ValueProvider<String> provider = options.getBar();    assertTrue(provider.isAccessible());    assertEquals("quux", provider.get());}
public void beam_f20222_0() throws Exception
{    TestOptions runtime = PipelineOptionsFactory.as(TestOptions.class);    TestOptions options = PipelineOptionsFactory.as(TestOptions.class);    runtime.setOptionsId(options.getOptionsId());    RuntimeValueProvider.setRuntimeOptions(runtime);    ValueProvider<String> provider = options.getBar();    assertTrue(provider.isAccessible());    assertEquals("bar", provider.get());}
public Integer beam_f20231_0(AtomicInteger from)
{    return from.incrementAndGet();}
public void beam_f20232_0() throws Exception
{    AtomicInteger increment = new AtomicInteger();    ValueProvider<Integer> nvp = NestedValueProvider.of(StaticValueProvider.of(increment), new IncrementAtomicIntegerTranslator());    Integer originalValue = nvp.get();    Integer cachedValue = nvp.get();    Integer incrementValue = increment.incrementAndGet();    Integer secondCachedValue = nvp.get();    assertEquals(originalValue, cachedValue);    assertEquals(secondCachedValue, cachedValue);    assertNotEquals(originalValue, incrementValue);}
public void beam_f20242_0()
{    PipelineOptions options = TestPipeline.testingPipelineOptions();    options.setRunner(TestPipelineRunnerThrowingUserException.class);    Pipeline p = Pipeline.create(options);        thrown.expect(PipelineExecutionException.class);    thrown.expectCause(isA(IllegalStateException.class));    thrown.expectMessage("user code exception");    p.run();}
public void beam_f20243_0()
{    PipelineOptions options = TestPipeline.testingPipelineOptions();    options.setRunner(TestPipelineRunnerThrowingSdkException.class);    Pipeline p = Pipeline.create(options);        try {        p.run();        fail("Should have thrown an exception.");    } catch (RuntimeException exn) {                assertThat(exn, not(instanceOf(UserCodeException.class)));                assertThat(exn.getMessage(), containsString("SDK exception"));                assertThat(exn, instanceOf(IllegalStateException.class));    }}
public T beam_f20252_0(T input)
{    return input;}
public void beam_f20253_0() throws Exception
{    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3, 4));    TupleTag<Integer> tag = new TupleTag<>();    PCollectionTuple tuple = PCollectionTuple.of(tag, input);    PCollection<Integer> output = tuple.apply("ProjectTag", new TupleProjectionTransform<>(tag));    PAssert.that(output).containsInAnyOrder(1, 2, 3, 4);    pipeline.run();}
public PCollection<Integer> beam_f20262_0(PCollection<Integer> input)
{    return input.apply("custom_name", Sum.integersGlobally());}
public PCollection<Integer> beam_f20263_0(PCollection<Integer> input)
{    return input.apply("custom_name", Max.integersGlobally());}
public PTransformReplacement<PBegin, PCollection<T>> beam_f20272_0(AppliedPTransform<PBegin, PCollection<T>, Create.Values<T>> transform)
{    return PTransformReplacement.of(transform.getPipeline().begin(), new EmptyFlatten<T>());}
public Map<PValue, ReplacementOutput> beam_f20273_0(Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput)
{    Map.Entry<TupleTag<?>, PValue> original = Iterables.getOnlyElement(outputs.entrySet());    Map.Entry<TupleTag<?>, PValue> replacement = Iterables.getOnlyElement(newOutput.expand().entrySet());    return Collections.singletonMap(newOutput, ReplacementOutput.of(TaggedPValue.of(original.getKey(), original.getValue()), TaggedPValue.of(replacement.getKey(), replacement.getValue())));}
public POutput beam_f20282_0(PBegin input)
{    PCollection<Double> output = input.apply(Create.of(1, 2, 3, 4)).apply("ScaleByTwo", MapElements.via(new ScaleFn<>(2.0, counter)));    PAssert.that(output).containsInAnyOrder(2.0, 4.0, 6.0, 8.0);    return output;}
public void beam_f20283_0()
{    hierarchy = new TransformHierarchy();}
public PCollection<Long> beam_f20292_0(PBegin input)
{    return comp;}
public void beam_f20293_0()
{    PTransform<?, ?> enclosingPT = new PTransform<PInput, POutput>() {        @Override        public POutput expand(PInput input) {            return PDone.in(input.getPipeline());        }    };    TransformHierarchy.Node enclosing = hierarchy.pushNode("Enclosing", PBegin.in(pipeline), enclosingPT);    Create.Values<Long> originalTransform = Create.of(1L);    TransformHierarchy.Node original = hierarchy.pushNode("Create", PBegin.in(pipeline), originalTransform);    assertThat(hierarchy.getCurrent(), equalTo(original));    PCollection<Long> originalOutput = pipeline.apply(originalTransform);    hierarchy.setOutput(originalOutput);    hierarchy.popNode();    assertThat(original.finishedSpecifying, is(true));    hierarchy.setOutput(PDone.in(pipeline));    hierarchy.popNode();    assertThat(hierarchy.getCurrent(), not(equalTo(enclosing)));    Read.Bounded<Long> replacementTransform = Read.from(CountingSource.upTo(1L));    PCollection<Long> replacementOutput = pipeline.apply(replacementTransform);    Node replacement = hierarchy.replaceNode(original, PBegin.in(pipeline), replacementTransform);    assertThat(hierarchy.getCurrent(), equalTo(replacement));    hierarchy.setOutput(replacementOutput);    TaggedPValue taggedReplacement = TaggedPValue.ofExpandedValue(replacementOutput);    Map<PValue, ReplacementOutput> replacementOutputs = Collections.singletonMap(replacementOutput, ReplacementOutput.of(TaggedPValue.ofExpandedValue(originalOutput), taggedReplacement));    hierarchy.replaceOutputs(replacementOutputs);    assertThat(replacement.getInputs(), equalTo(original.getInputs()));    assertThat(replacement.getEnclosingNode(), equalTo(original.getEnclosingNode()));    assertThat(replacement.getEnclosingNode(), equalTo(enclosing));    assertThat(replacement.getTransform(), equalTo(replacementTransform));        assertThat(replacement.getOutputs().keySet(), Matchers.contains(taggedReplacement.getTag()));    assertThat(replacement.getOutputs().values(), Matchers.contains(originalOutput));    hierarchy.popNode();}
public void beam_f20302_0(TransformHierarchy.Node node)
{    visitedPrimitiveNodes.add(node);}
public void beam_f20303_0(PValue value, TransformHierarchy.Node producer)
{    visitedValuesInVisitor.add(value);}
public Map<TupleTag<?>, PValue> beam_f20312_0()
{    return Collections.singletonMap(twoTag, two);}
public PCollectionTuple beam_f20313_0(PBegin input)
{    return oneAndTwo;}
public PCollectionTuple beam_f20322_0(PBegin input)
{    return oneAndTwo;}
public PCollectionTuple beam_f20323_0(PBegin input)
{    return input.apply(producer);}
public void beam_f20332_0() throws Exception
{    p.enableAbandonedNodeEnforcement(false);    p.apply(new InvalidCompositeTransform());    p.traverseTopologically(new Pipeline.PipelineVisitor.Defaults() {    });}
public void beam_f20333_0()
{    PCollection<Integer> input = p.begin().apply(Create.of(1, 2, 3));    input.apply(new UnboundOutputCreator());    p.run();}
public void beam_f20342_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    SimpleAutoValue inner = new AutoValue_AutoValueSchemaTest_SimpleAutoValue("string", (byte) 1, (short) 2, (int) 3, (long) 4, true, DATE, BYTE_ARRAY, ByteBuffer.wrap(BYTE_ARRAY), DATE.toInstant(), BigDecimal.ONE, STRING_BUILDER);    AutoValueOuter outer = new AutoValue_AutoValueSchemaTest_AutoValueOuter(inner);    Row row = registry.getToRowFunction(AutoValueOuter.class).apply(outer);    verifyRow(row.getRow("inner"));}
public void beam_f20343_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    SimpleAutoValue inner = new AutoValue_AutoValueSchemaTest_SimpleAutoValue("string", (byte) 1, (short) 2, (int) 3, (long) 4, true, DATE, BYTE_ARRAY, ByteBuffer.wrap(BYTE_ARRAY), DATE.toInstant(), BigDecimal.ONE, STRING_BUILDER);    AutoValueOuterWithBuilder outer = new AutoValue_AutoValueSchemaTest_AutoValueOuterWithBuilder.Builder().setInner(inner).build();    Row row = registry.getToRowFunction(AutoValueOuterWithBuilder.class).apply(outer);    verifyRow(row.getRow("inner"));}
public boolean beam_f20352_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof AvroPojo)) {        return false;    }    AvroPojo avroPojo = (AvroPojo) o;    return boolNonNullable == avroPojo.boolNonNullable && Objects.equals(anInt, avroPojo.anInt) && Objects.equals(aLong, avroPojo.aLong) && Objects.equals(aFloat, avroPojo.aFloat) && Objects.equals(aDouble, avroPojo.aDouble) && Objects.equals(string, avroPojo.string) && Objects.equals(bytes, avroPojo.bytes) && Arrays.equals(fixed, avroPojo.fixed) && Objects.equals(date, avroPojo.date) && Objects.equals(timestampMillis, avroPojo.timestampMillis) && Objects.equals(testEnum, avroPojo.testEnum) && Objects.equals(row, avroPojo.row) && Objects.equals(array, avroPojo.array) && Objects.equals(map, avroPojo.map);}
public int beam_f20353_0()
{    return Objects.hash(boolNonNullable, anInt, aLong, aFloat, aDouble, string, bytes, Arrays.hashCode(fixed), date, timestampMillis, testEnum, row, array, map);}
public void beam_f20362_0()
{    PCollection<Row> input = pipeline.apply(Create.of(ROW_FOR_POJO)).setRowSchema(POJO_SCHEMA);    PCollection<KV<Row, Iterable<Row>>> output = input.apply(Group.byFieldNames("string"));    PAssert.that(output).containsInAnyOrder(KV.of(Row.withSchema(Schema.of(Field.of("string", FieldType.STRING))).addValue("mystring").build(), ImmutableList.of(ROW_FOR_POJO)));    pipeline.run();}
public void beam_f20363_0()
{    FieldAccessDescriptorParser.parse("a.b[][*].c.*");}
public void beam_f20372_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("*");    assertTrue(fieldAccessDescriptor.resolve(SIMPLE_SCHEMA).getAllFields());}
public void beam_f20373_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("field0", "field1.*").resolve(NESTED_SCHEMA2);    assertEquals(1, fieldAccessDescriptor.getFieldsAccessed().size());    assertEquals("field0", fieldAccessDescriptor.getFieldsAccessed().iterator().next().getFieldName());    assertEquals(1, fieldAccessDescriptor.nestedFieldsById().size());    FieldAccessDescriptor nestedAccess = fieldAccessDescriptor.nestedFieldsById().get(1);    assertTrue(nestedAccess.getAllFields());}
public void beam_f20382_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("field0[]{}.field2").resolve(NESTED_ARRAY_MAP_SCHEMA);    assertTrue(fieldAccessDescriptor.fieldIdsAccessed().isEmpty());    assertEquals(1, fieldAccessDescriptor.nestedFieldsById().size());    fieldAccessDescriptor = fieldAccessDescriptor.nestedFieldsById().get(0);    assertEquals(ImmutableList.of(2), fieldAccessDescriptor.fieldIdsAccessed());}
public void beam_f20383_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("field3", "field2", "field1", "field0").resolve(SIMPLE_SCHEMA);    assertEquals(ImmutableList.of(0, 1, 2, 3), fieldAccessDescriptor.fieldIdsAccessed());}
public void beam_f20392_0()
{    assertEquals(FieldType.map(FieldType.STRING, FieldType.INT64), FieldTypeDescriptors.fieldTypeForJavaType(TypeDescriptors.maps(TypeDescriptors.strings(), TypeDescriptors.longs())));    assertEquals(FieldType.map(FieldType.STRING, FieldType.array(FieldType.INT64)), FieldTypeDescriptors.fieldTypeForJavaType(TypeDescriptors.maps(TypeDescriptors.strings(), TypeDescriptors.lists(TypeDescriptors.longs()))));}
private SimpleBean beam_f20393_0(String name)
{    return new SimpleBean(name, (byte) 1, (short) 2, 3, 4L, true, DATE, DATE.toInstant(), BYTE_ARRAY, BigDecimal.ONE, new StringBuilder(name).append("builder"));}
public void beam_f20402_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    SchemaTestUtils.assertSchemaEquivalent(PRIMITIVE_ARRAY_BEAN_SCHEMA, registry.getSchema(PrimitiveArrayBean.class));    List<String> strList = ImmutableList.of("a", "b", "c");    int[] intArray = { 1, 2, 3, 4 };    Long[] longArray = { 42L, 43L, 44L };    PrimitiveArrayBean bean = new PrimitiveArrayBean(strList, intArray, longArray);    Row row = registry.getToRowFunction(PrimitiveArrayBean.class).apply(bean);    assertEquals(strList, row.getArray("strings"));    assertEquals(Ints.asList(intArray), row.getArray("integers"));    assertEquals(Arrays.asList(longArray), row.getArray("longs"));        assertSame(row.getArray("strings"), row.getArray("strings"));    assertSame(row.getArray("integers"), row.getArray("integers"));    assertSame(row.getArray("longs"), row.getArray("longs"));}
public void beam_f20403_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Row row = Row.withSchema(PRIMITIVE_ARRAY_BEAN_SCHEMA).addArray("a", "b", "c", "d").addArray(1, 2, 3, 4).addArray(42L, 43L, 44L, 45L).build();    PrimitiveArrayBean bean = registry.getFromRowFunction(PrimitiveArrayBean.class).apply(row);    assertEquals(row.getArray("strings"), bean.getStrings());    assertEquals(row.getArray("integers"), Ints.asList(bean.getIntegers()));    assertEquals(row.getArray("longs"), Arrays.asList(bean.getLongs()));}
private SimplePOJO beam_f20412_0(String name)
{    return new SimplePOJO(name, (byte) 1, (short) 2, 3, 4L, true, DATE, INSTANT, BYTE_ARRAY, BYTE_BUFFER, BigDecimal.ONE, new StringBuilder(name).append("builder"));}
private AnnotatedSimplePojo beam_f20413_0(String name)
{    return new AnnotatedSimplePojo(name, (byte) 1, 4L, (short) 2, 3, true, DATE, BigDecimal.ONE, INSTANT, BYTE_ARRAY, BYTE_BUFFER, new StringBuilder(name).append("builder"));}
public void beam_f20422_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    SchemaTestUtils.assertSchemaEquivalent(PRIMITIVE_ARRAY_POJO_SCHEMA, registry.getSchema(PrimitiveArrayPOJO.class));    List<String> strList = ImmutableList.of("a", "b", "c");    int[] intArray = { 1, 2, 3, 4 };    Long[] longArray = { 42L, 43L, 44L };    PrimitiveArrayPOJO pojo = new PrimitiveArrayPOJO(strList, intArray, longArray);    Row row = registry.getToRowFunction(PrimitiveArrayPOJO.class).apply(pojo);    assertEquals(strList, row.getArray("strings"));    assertEquals(Ints.asList(intArray), row.getArray("integers"));    assertEquals(Arrays.asList(longArray), row.getArray("longs"));        assertSame(row.getArray("strings"), row.getArray("strings"));    assertSame(row.getArray("integers"), row.getArray("integers"));    assertSame(row.getArray("longs"), row.getArray("longs"));}
public void beam_f20423_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Row row = Row.withSchema(PRIMITIVE_ARRAY_POJO_SCHEMA).addArray("a", "b", "c", "d").addArray(1, 2, 3, 4).addArray(42L, 43L, 44L, 45L).build();    PrimitiveArrayPOJO pojo = registry.getFromRowFunction(PrimitiveArrayPOJO.class).apply(row);    assertEquals(row.getArray("strings"), pojo.strings);    assertEquals(row.getArray("integers"), Ints.asList(pojo.integers));    assertEquals(row.getArray("longs"), Arrays.asList(pojo.longs));}
public void beam_f20432_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Row row = registry.getToRowFunction(POJOWithNestedNullable.class).apply(new POJOWithNestedNullable(null));    assertNull(row.getValue("nested"));}
public void beam_f20433_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    Row row = Row.withSchema(NESTED_NULLABLE_SCHEMA).addValue(null).build();    POJOWithNestedNullable pojo = registry.getFromRowFunction(POJOWithNestedNullable.class).apply(row);    assertNull(pojo.nested);}
public SerializableFunction<T, Row> beam_f20442_0(TypeDescriptor<T> typeDescriptor)
{    return v -> Row.withSchema(schemaFor(typeDescriptor)).addValue(v).build();}
public SerializableFunction<Row, T> beam_f20443_0(TypeDescriptor<T> typeDescriptor)
{    return r -> r.getValue(0);}
public SerializableFunction<Row, T> beam_f20452_0(TypeDescriptor<T> typeDescriptor)
{    if (typeDescriptor.equals(TypeDescriptor.of(TestDefaultSchemaClass.class))) {        return r -> (T) new TestSchemaClass();    }    return null;}
public void beam_f20453_0() throws NoSuchSchemaException
{    SchemaRegistry registry = SchemaRegistry.createDefault();    assertEquals(EMPTY_SCHEMA, registry.getSchema(TestDefaultSchemaClass.class));}
public void beam_f20462_0()
{    Schema schema = Schema.of(Field.of("f_byte", FieldType.BYTE));    thrown.expect(IndexOutOfBoundsException.class);    schema.getField(1);}
public void beam_f20463_0()
{    Schema schema = Stream.of(Schema.Field.of("f_int", FieldType.INT32), Schema.Field.of("f_string", FieldType.STRING)).collect(toSchema());    assertEquals(2, schema.getFieldCount());    assertEquals("f_int", schema.getField(0).getName());    assertEquals(FieldType.INT32, schema.getField(0).getType());    assertEquals("f_string", schema.getField(1).getName());    assertEquals(FieldType.STRING, schema.getField(1).getType());}
public void beam_f20472_0()
{    Schema schema = Schema.builder().addStringField("field1").build();    PCollection<Row> added = pipeline.apply(Create.of(Row.withSchema(schema).addValue("value").build()).withRowSchema(schema)).apply(AddFields.<Row>create().field("field2", Schema.FieldType.INT32).field("field3", Schema.FieldType.array(Schema.FieldType.STRING)));    Schema expectedSchema = Schema.builder().addStringField("field1").addNullableField("field2", Schema.FieldType.INT32).addNullableField("field3", Schema.FieldType.array(Schema.FieldType.STRING)).build();    assertEquals(expectedSchema, added.getSchema());    Row expected = Row.withSchema(expectedSchema).addValues("value", null, null).build();    PAssert.that(added).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20473_0()
{    Schema schema = Schema.builder().addStringField("field1").build();    PCollection<Row> added = pipeline.apply(Create.of(Row.withSchema(schema).addValue("value").build()).withRowSchema(schema)).apply(AddFields.<Row>create().field("field2", Schema.FieldType.INT32, 42));    Schema expectedSchema = Schema.builder().addStringField("field1").addField("field2", Schema.FieldType.INT32).build();    assertEquals(expectedSchema, added.getSchema());    Row expected = Row.withSchema(expectedSchema).addValues("value", 42).build();    PAssert.that(added).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20482_0()
{    Schema schema = Schema.builder().addStringField("field1").build();    thrown.expect(IllegalArgumentException.class);    pipeline.apply(Create.of(Row.withSchema(schema).addValue("value").build()).withRowSchema(schema)).apply(AddFields.<Row>create().field("field2", Schema.FieldType.INT32, null));    pipeline.run();}
public void beam_f20483_0()
{    Schema inputSchema = Schema.of(Schema.Field.of("f0", Schema.FieldType.INT16), Schema.Field.of("f1", Schema.FieldType.INT32), Schema.Field.of("f2", Schema.FieldType.STRING));        Schema outputSchema = Schema.of(Schema.Field.of("f2", Schema.FieldType.STRING), Schema.Field.of("f1", Schema.FieldType.INT32));    Row input = Row.withSchema(inputSchema).addValues((short) 1, 2, "3").build();    Row expected = Row.withSchema(outputSchema).addValues("3", 2).build();    PCollection<Row> output = pipeline.apply(Create.of(input).withRowSchema(inputSchema)).apply(Cast.widening(outputSchema));    PAssert.that(output).containsInAnyOrder(expected);    pipeline.run();}
public void beam_f20492_0()
{    Object output = Cast.castValue(Arrays.asList((short) 1, (short) 2, (short) 3), Schema.FieldType.array(Schema.FieldType.INT16), Schema.FieldType.array(Schema.FieldType.INT32));    assertEquals(Arrays.asList(1, 2, 3), output);}
public void beam_f20493_0()
{    Object output = Cast.castValue(ImmutableMap.of((short) 1, 1, (short) 2, 2, (short) 3, 3), Schema.FieldType.map(Schema.FieldType.INT16, Schema.FieldType.INT32), Schema.FieldType.map(Schema.FieldType.INT32, Schema.FieldType.INT64));    assertEquals(ImmutableMap.of(1, 1L, 2, 2L, 3, 3L), output);}
public void beam_f20502_0()
{    PCollection<Row> pc1 = pipeline.apply("Create1", Create.of(Row.withSchema(CG_SCHEMA_1).addValues("user1", 1, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 2, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 3, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 4, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 5, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 6, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 7, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 8, "ar").build())).setRowSchema(CG_SCHEMA_1);    PCollection<Row> pc2 = pipeline.apply("Create2", Create.of(Row.withSchema(CG_SCHEMA_2).addValues("user1", 9, "us").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 10, "us").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 11, "il").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 12, "il").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 13, "fr").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 14, "fr").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 15, "ar").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 16, "ar").build())).setRowSchema(CG_SCHEMA_2);    PCollection<Row> pc3 = pipeline.apply("Create3", Create.of(Row.withSchema(CG_SCHEMA_3).addValues("user1", 17, "us").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 18, "us").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 19, "il").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 20, "il").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 21, "fr").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 22, "fr").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 23, "ar").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 24, "ar").build())).setRowSchema(CG_SCHEMA_3);    Row key1 = Row.withSchema(SIMPLE_CG_KEY_SCHEMA).addValues("user1", "us").build();    Row key1Joined = Row.withSchema(SIMPLE_CG_OUTPUT_SCHEMA).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_1).addValues("user1", 1, "us").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 2, "us").build())).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_2).addValues("user1", 9, "us").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 10, "us").build())).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_3).addValues("user1", 17, "us").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 18, "us").build())).build();    Row key2 = Row.withSchema(SIMPLE_CG_KEY_SCHEMA).addValues("user1", "il").build();    Row key2Joined = Row.withSchema(SIMPLE_CG_OUTPUT_SCHEMA).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_1).addValues("user1", 3, "il").build(), Row.withSchema(CG_SCHEMA_1).addValues("user1", 4, "il").build())).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_2).addValues("user1", 11, "il").build(), Row.withSchema(CG_SCHEMA_2).addValues("user1", 12, "il").build())).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_3).addValues("user1", 19, "il").build(), Row.withSchema(CG_SCHEMA_3).addValues("user1", 20, "il").build())).build();    Row key3 = Row.withSchema(SIMPLE_CG_KEY_SCHEMA).addValues("user2", "fr").build();    Row key3Joined = Row.withSchema(SIMPLE_CG_OUTPUT_SCHEMA).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_1).addValues("user2", 5, "fr").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 6, "fr").build())).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_2).addValues("user2", 13, "fr").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 14, "fr").build())).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_3).addValues("user2", 21, "fr").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 22, "fr").build())).build();    Row key4 = Row.withSchema(SIMPLE_CG_KEY_SCHEMA).addValues("user2", "ar").build();    Row key4Joined = Row.withSchema(SIMPLE_CG_OUTPUT_SCHEMA).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_1).addValues("user2", 7, "ar").build(), Row.withSchema(CG_SCHEMA_1).addValues("user2", 8, "ar").build())).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_2).addValues("user2", 15, "ar").build(), Row.withSchema(CG_SCHEMA_2).addValues("user2", 16, "ar").build())).addValue(Lists.newArrayList(Row.withSchema(CG_SCHEMA_3).addValues("user2", 23, "ar").build(), Row.withSchema(CG_SCHEMA_3).addValues("user2", 24, "ar").build())).build();    PCollection<KV<Row, Row>> joined = PCollectionTuple.of("pc1", pc1, "pc2", pc2, "pc3", pc3).apply("CoGroup", CoGroup.join("pc1", By.fieldNames("user", "country")).join("pc2", By.fieldNames("user2", "country2")).join("pc3", By.fieldNames("user3", "country3")));    List<KV<Row, Row>> expected = ImmutableList.of(KV.of(key1, key1Joined), KV.of(key2, key2Joined), KV.of(key3, key3Joined), KV.of(key4, key4Joined));    PAssert.that(joined).satisfies(actual -> containsJoinedFields(expected, actual));    pipeline.run();}
public void beam_f20503_0()
{    PCollection<Row> pc1 = pipeline.apply("Create1", Create.of(Row.withSchema(CG_SCHEMA_1).addValues("user1", 1, "us").build())).setRowSchema(CG_SCHEMA_1);    PCollection<Row> pc2 = pipeline.apply("Create2", Create.of(Row.withSchema(CG_SCHEMA_2).addValues("user1", 9, "us").build())).setRowSchema(CG_SCHEMA_2);    PCollection<Row> pc3 = pipeline.apply("Create3", Create.of(Row.withSchema(CG_SCHEMA_3).addValues("user1", 17, "us").build()));    thrown.expect(IllegalArgumentException.class);    PCollection<KV<Row, Row>> joined = PCollectionTuple.of("pc1", pc1, "pc2", pc2, "pc3", pc3).apply("CoGroup", CoGroup.join("pc1", By.fieldNames("user", "country")).join("pc2", By.fieldNames("user2", "country2")));    pipeline.run();}
public boolean beam_f20512_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    POJO1 pojo1 = (POJO1) o;    return field2 == pojo1.field2 && Objects.equals(field1, pojo1.field1) && Objects.equals(field3, pojo1.field3) && Arrays.equals(field4, pojo1.field4) && Objects.equals(field5, pojo1.field5);}
public int beam_f20513_0()
{    int result = Objects.hash(field1, field2, field3, field5);    result = 31 * result + Arrays.hashCode(field4);    return result;}
public void beam_f20522_0()
{    PCollection<POJO2> pojos = pipeline.apply(Create.of(new POJO1())).apply(Convert.to(POJO2.class));    PAssert.that(pojos).containsInAnyOrder(new POJO2());    pipeline.run();}
public void beam_f20523_0()
{    PCollection<POJO1Nested> pojos = pipeline.apply(Create.of(new POJO1())).apply(Select.fieldNames("field3")).apply(Convert.to(TypeDescriptor.of(POJO1Nested.class)));    PAssert.that(pojos).containsInAnyOrder(new POJO1Nested());    pipeline.run();}
public void beam_f20532_0()
{    thrown.expect(IllegalArgumentException.class);    pipeline.apply(Create.of(new AutoValue_FilterTest_Simple("pass", 52, 2))).apply(Filter.<AutoValue_FilterTest_Simple>create().whereFieldName("missing", f -> true));    pipeline.run();}
public void beam_f20533_0()
{    thrown.expect(IllegalArgumentException.class);    pipeline.apply(Create.of(new AutoValue_FilterTest_Simple("pass", 52, 2))).apply(Filter.<AutoValue_FilterTest_Simple>create().whereFieldId(23, f -> true));    pipeline.run();}
public boolean beam_f20542_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    OuterPOJO outerPOJO = (OuterPOJO) o;    return Objects.equals(inner, outerPOJO.inner);}
public int beam_f20543_0()
{    return Objects.hash(inner);}
public void beam_f20552_0()
{    Collection<AggregatePojos> elements = ImmutableList.of(new AggregatePojos(1, 1, 2), new AggregatePojos(2, 1, 3), new AggregatePojos(3, 2, 4), new AggregatePojos(4, 2, 5));    PCollection<KV<Row, Row>> aggregations = pipeline.apply(Create.of(elements)).apply(Group.<AggregatePojos>byFieldNames("field2").aggregateField("field1", Sum.ofLongs(), "field1_sum").aggregateField("field3", Sum.ofIntegers(), "field3_sum").aggregateField("field1", Top.largestLongsFn(1), "field1_top"));    Schema keySchema = Schema.builder().addInt64Field("field2").build();    Schema valueSchema = Schema.builder().addInt64Field("field1_sum").addInt32Field("field3_sum").addArrayField("field1_top", FieldType.INT64).build();    List<KV<Row, Row>> expected = ImmutableList.of(KV.of(Row.withSchema(keySchema).addValue(1L).build(), Row.withSchema(valueSchema).addValue(3L).addValue(5).addArray(2L).build()), KV.of(Row.withSchema(keySchema).addValue(2L).build(), Row.withSchema(valueSchema).addValue(7L).addValue(9).addArray(4L).build()));    PAssert.that(aggregations).satisfies(actual -> containsKvs(expected, actual));    pipeline.run();}
public void beam_f20553_0()
{    Collection<AggregatePojos> elements = ImmutableList.of(new AggregatePojos(1, 1, 2), new AggregatePojos(2, 1, 3), new AggregatePojos(3, 2, 4), new AggregatePojos(4, 2, 5));    PCollection<Row> aggregate = pipeline.apply(Create.of(elements)).apply(Group.<AggregatePojos>globally().aggregateField("field1", Sum.ofLongs(), "field1_sum").aggregateField("field3", Sum.ofIntegers(), "field3_sum").aggregateField("field1", Top.largestLongsFn(1), "field1_top"));    Schema aggregateSchema = Schema.builder().addInt64Field("field1_sum").addInt32Field("field3_sum").addArrayField("field1_top", FieldType.INT64).build();    Row expectedRow = Row.withSchema(aggregateSchema).addValues(10L, 14).addArray(4L).build();    PAssert.that(aggregate).containsInAnyOrder(expectedRow);    pipeline.run();}
public void beam_f20562_0()
{    Collection<OuterAggregate> elements = ImmutableList.of(new OuterAggregate(new AggregatePojos(1, 1, 2)), new OuterAggregate(new AggregatePojos(2, 1, 3)), new OuterAggregate(new AggregatePojos(3, 2, 4)), new OuterAggregate(new AggregatePojos(4, 2, 5)));    PCollection<Row> aggregate = pipeline.apply(Create.of(elements)).apply(Group.<OuterAggregate>globally().aggregateField("inner.field1", Sum.ofLongs(), "field1_sum").aggregateField("inner.field3", Sum.ofIntegers(), "field3_sum").aggregateField("inner.field1", Top.largestLongsFn(1), "field1_top"));    Schema aggregateSchema = Schema.builder().addInt64Field("field1_sum").addInt32Field("field3_sum").addArrayField("field1_top", FieldType.INT64).build();    Row expectedRow = Row.withSchema(aggregateSchema).addValues(10L, 14).addArray(4L).build();    PAssert.that(aggregate).containsInAnyOrder(expectedRow);    pipeline.run();}
private static Void beam_f20563_0(List<KV<Row, Collection<T>>> expectedKvs, Iterable<KV<Row, Iterable<T>>> actualKvs, T[] emptyArray)
{    List<KV<Row, Iterable<T>>> list = Lists.newArrayList(actualKvs);    List<Matcher<? super KV<Row, Iterable<POJO>>>> matchers = new ArrayList<>();    for (KV<Row, Collection<T>> expected : expectedKvs) {        T[] values = expected.getValue().toArray(emptyArray);        matchers.add(isKv(equalTo(expected.getKey()), containsInAnyOrder(values)));    }    assertThat(actualKvs, containsInAnyOrder(matchers.toArray(new Matcher[0])));    return null;}
 static List<Row> beam_f20572_0(List<Row> inputs1, List<Row> inputs2, String[] keys1, String[] keys2, Schema expectedSchema)
{    List<Row> joined = Lists.newArrayList();    for (Row row1 : inputs1) {        for (Row row2 : inputs2) {            List key1 = Arrays.stream(keys1).map(row1::getValue).collect(Collectors.toList());            List key2 = Arrays.stream(keys2).map(row2::getValue).collect(Collectors.toList());            if (key1.equals(key2)) {                joined.add(Row.withSchema(expectedSchema).addValues(row1, row2).build());            }        }    }    return joined;}
 static List<Row> beam_f20573_0(List<Row> inputs1, List<Row> inputs2, List<Row> inputs3, String[] keys1, String[] keys2, String[] keys3, Schema expectedSchema)
{    List<Row> joined = Lists.newArrayList();    for (Row row1 : inputs1) {        for (Row row2 : inputs2) {            for (Row row3 : inputs3) {                List key1 = Arrays.stream(keys1).map(row1::getValue).collect(Collectors.toList());                List key2 = Arrays.stream(keys2).map(row2::getValue).collect(Collectors.toList());                List key3 = Arrays.stream(keys3).map(row3::getValue).collect(Collectors.toList());                if (key1.equals(key2) && key2.equals(key3)) {                    joined.add(Row.withSchema(expectedSchema).addValues(row1, row2, row3).build());                }            }        }    }    return joined;}
public boolean beam_f20582_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    POJO1Selected that = (POJO1Selected) o;    return Objects.equals(field1, that.field1) && Objects.equals(field3, that.field3);}
public int beam_f20583_0()
{    return Objects.hash(field1, field3);}
public void beam_f20592_0()
{    PCollection<POJO1Selected> pojos = pipeline.apply(Create.of(new POJO2())).apply(Select.fieldNames("field2.field1", "field2.field3")).apply(Convert.to(POJO1Selected.class));    PAssert.that(pojos).containsInAnyOrder(new POJO1Selected());    pipeline.run();}
public boolean beam_f20593_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof PrimitiveArray)) {        return false;    }    PrimitiveArray that = (PrimitiveArray) o;    return Objects.equals(field1, that.field1);}
public int beam_f20602_0()
{    return Objects.hash(field1);}
public boolean beam_f20603_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof PartialRowSingleMap)) {        return false;    }    PartialRowSingleMap that = (PartialRowSingleMap) o;    return Objects.equals(field1, that.field1) && Objects.equals(field3, that.field3);}
public int beam_f20612_0()
{    return Objects.hash(field1);}
public boolean beam_f20613_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof PartialRowMultipleMaps)) {        return false;    }    PartialRowMultipleMaps that = (PartialRowMultipleMaps) o;    return Objects.equals(field1, that.field1) && Objects.equals(field3, that.field3);}
public void beam_f20622_0()
{    List<Row> rows = IntStream.rangeClosed(0, 2).mapToObj(i -> Row.withSchema(SIMPLE_SCHEMA).addValues(i, Integer.toString(i)).build()).collect(Collectors.toList());    PCollection<Row> unnested = pipeline.apply(Create.of(rows).withRowSchema(SIMPLE_SCHEMA)).apply(Unnest.create());    PAssert.that(unnested).containsInAnyOrder(rows);    pipeline.run();}
public void beam_f20623_0()
{    List<Row> bottomRow = IntStream.rangeClosed(0, 2).mapToObj(i -> Row.withSchema(SIMPLE_SCHEMA).addValues(i, Integer.toString(i)).build()).collect(Collectors.toList());    List<Row> rows = bottomRow.stream().map(r -> Row.withSchema(NESTED_SCHEMA).addValues(r, r).build()).collect(Collectors.toList());    PCollection<Row> unnested = pipeline.apply(Create.of(rows).withRowSchema(NESTED_SCHEMA)).apply(Unnest.create());    assertEquals(UNNESTED_SCHEMA, unnested.getSchema());    List<Row> expected = bottomRow.stream().map(r -> Row.withSchema(UNNESTED_SCHEMA).addValues(r.getValue(0), r.getValue(1), r.getValue(0), r.getValue(1)).build()).collect(Collectors.toList());    ;    PAssert.that(unnested).containsInAnyOrder(expected);    pipeline.run();}
 void beam_f20632_0(GenerationStatus status)
{    String[] current = status.valueOf(BRANCH_KEY).orElse(new String[0]);    String[] next = Arrays.copyOf(current, current.length - 1);    status.setValue(BRANCH_KEY, next);}
 String beam_f20633_0(GenerationStatus status)
{    return Joiner.on("_").join(status.valueOf(BRANCH_KEY).orElse(new String[0]));}
public void beam_f20642_0()
{    Field beamField = Field.nullable("arrayField", FieldType.array(FieldType.INT32));    org.apache.avro.Schema.Field expectedAvroField = new org.apache.avro.Schema.Field("arrayField", ReflectData.makeNullable(org.apache.avro.Schema.createArray((org.apache.avro.Schema.create(Type.INT)))), "", null);    org.apache.avro.Schema.Field avroField = AvroUtils.toAvroField(beamField, "ignored");    assertEquals(expectedAvroField, avroField);}
private static List<org.apache.avro.Schema.Field> beam_f20643_0()
{    List<org.apache.avro.Schema.Field> fields = Lists.newArrayList();    fields.add(new org.apache.avro.Schema.Field("bool", org.apache.avro.Schema.create(Type.BOOLEAN), "", null));    fields.add(new org.apache.avro.Schema.Field("int", org.apache.avro.Schema.create(Type.INT), "", null));    return fields;}
public void beam_f20652_0()
{    Schema beamSchema = getBeamSchema();    org.apache.avro.Schema avroSchema = AvroUtils.toAvroSchema(beamSchema);    assertEquals(getAvroSchema(), avroSchema);}
public void beam_f20653_0()
{    org.apache.avro.Schema convertedSchema = AvroUtils.toAvroSchema(getBeamSchema());    org.apache.avro.Schema validatedSchema = new org.apache.avro.Schema.Parser().parse(convertedSchema.toString());    assertEquals(convertedSchema, validatedSchema);}
public boolean beam_f20662_0(final Object item0)
{    if (!(item0 instanceof org.apache.avro.Schema)) {        return false;    }    org.apache.avro.Schema item = (org.apache.avro.Schema) item0;    if (predicate.apply(item)) {        return true;    }    switch(item.getType()) {        case RECORD:            return item.getFields().stream().anyMatch(x -> matches(x.schema()));        case UNION:            return item.getTypes().stream().anyMatch(this::matches);        case ARRAY:            return matches(item.getElementType());        case MAP:            return matches(item.getValueType());        default:            return false;    }}
public void beam_f20664_0()
{    Schema schema = JavaBeanUtils.schemaFromJavaBeanClass(NullableBean.class, GetterTypeSupplier.INSTANCE);    assertTrue(schema.getField("str").getType().getNullable());    assertFalse(schema.getField("anInt").getType().getNullable());}
public void beam_f20673_0()
{    SimpleBean simpleBean = new SimpleBean();    List<FieldValueSetter> setters = JavaBeanUtils.getSetters(SimpleBean.class, SIMPLE_BEAN_SCHEMA, new SetterTypeSupplier());    assertEquals(12, setters.size());    setters.get(0).set(simpleBean, "field1");    setters.get(1).set(simpleBean, (byte) 41);    setters.get(2).set(simpleBean, (short) 42);    setters.get(3).set(simpleBean, (int) 43);    setters.get(4).set(simpleBean, (long) 44);    setters.get(5).set(simpleBean, true);    setters.get(6).set(simpleBean, DateTime.parse("1979-03-14").toInstant());    setters.get(7).set(simpleBean, DateTime.parse("1979-03-15").toInstant());    setters.get(8).set(simpleBean, "bytes1".getBytes(Charset.defaultCharset()));    setters.get(9).set(simpleBean, "bytes2".getBytes(Charset.defaultCharset()));    setters.get(10).set(simpleBean, new BigDecimal(42));    setters.get(11).set(simpleBean, "stringBuilder");    assertEquals("field1", simpleBean.getStr());    assertEquals((byte) 41, simpleBean.getaByte());    assertEquals((short) 42, simpleBean.getaShort());    assertEquals((int) 43, simpleBean.getAnInt());    assertEquals((long) 44, simpleBean.getaLong());    assertTrue(simpleBean.isaBoolean());    assertEquals(DateTime.parse("1979-03-14"), simpleBean.getDateTime());    assertEquals(DateTime.parse("1979-03-15").toInstant(), simpleBean.getInstant());    assertArrayEquals("Unexpected bytes", "bytes1".getBytes(Charset.defaultCharset()), simpleBean.getBytes());    assertEquals(ByteBuffer.wrap("bytes2".getBytes(Charset.defaultCharset())), simpleBean.getByteBuffer());    assertEquals(new BigDecimal(42), simpleBean.getBigDecimal());    assertEquals("stringBuilder", simpleBean.getStringBuilder().toString());}
public void beam_f20674_0()
{    BeanWithBoxedFields bean = new BeanWithBoxedFields();    bean.setaByte((byte) 41);    bean.setaShort((short) 42);    bean.setAnInt(43);    bean.setaLong(44L);    bean.setaBoolean(true);    List<FieldValueGetter> getters = JavaBeanUtils.getGetters(BeanWithBoxedFields.class, BEAN_WITH_BOXED_FIELDS_SCHEMA, new JavaBeanSchema.GetterTypeSupplier());    assertEquals((byte) 41, getters.get(0).get(bean));    assertEquals((short) 42, getters.get(1).get(bean));    assertEquals((int) 43, getters.get(2).get(bean));    assertEquals((long) 44, getters.get(3).get(bean));    assertTrue((Boolean) getters.get(4).get(bean));}
public void beam_f20683_0()
{    Schema schema = POJOUtils.schemaFromPojoClass(PrimitiveMapPOJO.class, JavaFieldTypeSupplier.INSTANCE);    SchemaTestUtils.assertSchemaEquivalent(PRIMITIVE_MAP_POJO_SCHEMA, schema);}
public void beam_f20684_0()
{    Schema schema = POJOUtils.schemaFromPojoClass(NestedMapPOJO.class, JavaFieldTypeSupplier.INSTANCE);    SchemaTestUtils.assertSchemaEquivalent(NESTED_MAP_POJO_SCHEMA, schema);}
public void beam_f20693_0()
{    assertEquals(4, new CountMissingFields().apply(LEFT, RIGHT).intValue());}
public void beam_f20694_0()
{    assertThat(new ListCommonFields().apply(LEFT, RIGHT), containsInAnyOrder("f0", "f1", "f2", "f3", "f3.f0", "f3.f1"));}
public Integer beam_f20703_0(Context context, Optional<Schema.Field> left, Optional<Schema.Field> right)
{    if (!left.isPresent() || !right.isPresent()) {        return 1;    } else {        return 0;    }}
public List<String> beam_f20704_0(List<String> left, List<String> right)
{    return Stream.concat(left.stream(), right.stream()).collect(Collectors.toList());}
public void beam_f20713_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("nested.*").resolve(NESTED_SCHEMA);    Schema outputSchema = SelectHelpers.getOutputSchema(NESTED_SCHEMA, fieldAccessDescriptor);    assertEquals(FLAT_SCHEMA, outputSchema);    Row row = SelectHelpers.selectRow(NESTED_ROW, fieldAccessDescriptor, NESTED_SCHEMA, outputSchema);    assertEquals(FLAT_ROW, row);}
public void beam_f20714_0()
{    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("nested2.nested.field1").resolve(DOUBLE_NESTED_SCHEMA);    Schema outputSchema = SelectHelpers.getOutputSchema(DOUBLE_NESTED_SCHEMA, fieldAccessDescriptor);    Schema expectedSchema = Schema.builder().addStringField("field1").build();    assertEquals(expectedSchema, outputSchema);    Row row = SelectHelpers.selectRow(DOUBLE_NESTED_ROW, fieldAccessDescriptor, DOUBLE_NESTED_SCHEMA, outputSchema);    Row expectedRow = Row.withSchema(expectedSchema).addValue("first").build();    assertEquals(expectedRow, row);}
public void beam_f20723_0()
{    Schema f1 = Schema.builder().addInt64Field("f0").build();    Schema f2 = Schema.builder().addRowField("f1", f1).build();    Schema f3 = Schema.builder().addRowField("f2", f2).build();        Row r1 = Row.withSchema(f1).addValue(42L).build();        Row r2 = Row.withSchema(f2).addValue(r1).build();        Row r3 = Row.withSchema(f3).addValue(r2).build();    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("f2.f1").resolve(f3);    Schema outputSchema = SelectHelpers.getOutputSchema(f3, fieldAccessDescriptor);    Row out = SelectHelpers.selectRow(r3, fieldAccessDescriptor, r3.getSchema(), outputSchema);    assertEquals(f2, outputSchema);    assertEquals(r2, out);}
public void beam_f20724_0()
{    Schema f1 = Schema.builder().addInt64Field("f0").build();    Schema f2 = Schema.builder().addRowField("f1", f1).build();    Schema f3 = Schema.builder().addRowField("f2", f2).build();    Schema f4 = Schema.builder().addRowField("f3", f3).build();        Row r1 = Row.withSchema(f1).addValue(42L).build();        Row r2 = Row.withSchema(f2).addValue(r1).build();        Row r3 = Row.withSchema(f3).addValue(r2).build();        Row r4 = Row.withSchema(f4).addValue(r3).build();    FieldAccessDescriptor fieldAccessDescriptor = FieldAccessDescriptor.withFieldNames("f3.f2").resolve(f4);    Schema outputSchema = SelectHelpers.getOutputSchema(f4, fieldAccessDescriptor);    Row out = SelectHelpers.selectRow(r4, fieldAccessDescriptor, r4.getSchema(), outputSchema);    assertEquals(f3, outputSchema);    assertEquals(r3, out);}
public void beam_f20733_0(String str)
{    this.str = str;}
public boolean beam_f20734_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    MismatchingNullableBean that = (MismatchingNullableBean) o;    return Objects.equals(str, that.str);}
public void beam_f20743_0(int anInt)
{    this.anInt = anInt;}
public long beam_f20744_0()
{    return aLong;}
public void beam_f20753_0(ByteBuffer byteBuffer)
{    this.byteBuffer = byteBuffer;}
public Instant beam_f20754_0()
{    return instant;}
public String beam_f20763_0()
{    return str;}
public byte beam_f20764_0()
{    return aByte;}
public BigDecimal beam_f20773_0()
{    return bigDecimal;}
public StringBuilder beam_f20774_0()
{    return stringBuilder;}
public int[] beam_f20783_0()
{    return integers;}
public void beam_f20784_0(int[] integers)
{    this.integers = integers;}
public List<List<String>> beam_f20793_0()
{    return lists;}
public void beam_f20794_0(List<List<String>> lists)
{    this.lists = lists;}
public boolean beam_f20803_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    PrimitiveMapBean that = (PrimitiveMapBean) o;    return Objects.equals(map, that.map);}
public int beam_f20804_0()
{    return Objects.hash(map);}
public Integer beam_f20813_0()
{    return anInt;}
public void beam_f20814_0(Integer anInt)
{    this.anInt = anInt;}
public ByteBuffer beam_f20823_0()
{    return bytes2;}
public void beam_f20824_0(ByteBuffer bytes2)
{    this.bytes2 = bytes2;}
public int beam_f20833_0()
{    int result = Objects.hash(str, aByte, aShort, anInt, aLong, aBoolean, dateTime, instant, byteBuffer, bigDecimal, stringBuilder);    result = 31 * result + Arrays.hashCode(bytes);    return result;}
public boolean beam_f20834_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    AnnotatedSimplePojo that = (AnnotatedSimplePojo) o;    return theByte == that.theByte && theShort == that.theShort && anInt == that.anInt && aLong == that.aLong && aBoolean == that.aBoolean && Objects.equals(str, that.str) && Objects.equals(dateTime, that.dateTime) && Objects.equals(instant, that.instant) && Arrays.equals(bytes, that.bytes) && Objects.equals(byteBuffer, that.byteBuffer) && Objects.equals(bigDecimal, that.bigDecimal) && Objects.equals(stringBuilder.toString(), that.stringBuilder.toString());}
public boolean beam_f20843_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    NestedArrayPOJO that = (NestedArrayPOJO) o;    return Arrays.equals(pojos, that.pojos);}
public int beam_f20844_0()
{    return Arrays.hashCode(pojos);}
public boolean beam_f20853_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    POJOWithBoxedFields that = (POJOWithBoxedFields) o;    return Objects.equals(aByte, that.aByte) && Objects.equals(aShort, that.aShort) && Objects.equals(anInt, that.anInt) && Objects.equals(aLong, that.aLong) && Objects.equals(aBoolean, that.aBoolean);}
public int beam_f20854_0()
{    return Objects.hash(aByte, aShort, anInt, aLong, aBoolean);}
public void beam_f20863_0()
{    BoundedWindow window = new IntervalWindow(new Instant(-137), Duration.millis(21L));    StateContext<BoundedWindow> context = StateContexts.windowOnlyContext(window);    thrown.expect(IllegalArgumentException.class);    context.getPipelineOptions();}
public void beam_f20864_0()
{    BoundedWindow window = new IntervalWindow(new Instant(-137), Duration.millis(21L));    StateContext<BoundedWindow> context = StateContexts.windowOnlyContext(window);    thrown.expect(IllegalArgumentException.class);    context.sideInput(view);}
public String beam_f20873_0(InputStream inStream) throws CoderException, IOException
{    return StringUtf8Coder.of().decode(inStream);}
public void beam_f20875_0() throws Exception
{    AssertionError error = null;    try {        CoderProperties.coderDeterministic(new BadDeterminsticCoder(), "TestData", "TestData");    } catch (AssertionError e) {        error = e;    }    assertNotNull("Expected AssertionError", error);    assertThat(error.getMessage(), CoreMatchers.containsString("<84>, <101>, <115>, <116>, <68>"));}
public boolean beam_f20884_0(Object other)
{    return (other instanceof ForgetfulSerializingCoder) && ((ForgetfulSerializingCoder) other).lostState == lostState;}
public int beam_f20885_0()
{    return lostState;}
public Integer beam_f20894_0()
{    return 0;}
public Integer beam_f20895_0(Integer accumulator, Integer input)
{    return accumulator + input;}
public Integer beam_f20904_0()
{    return 0;}
public Integer beam_f20905_0(Integer accumulator, Integer input)
{    return accumulator + input;}
public List<Integer> beam_f20914_0()
{    return new ArrayList<>();}
public List<Integer> beam_f20915_0(List<Integer> accumulator, Integer input)
{        if (!accumulator.isEmpty() && accumulator.get(accumulator.size() - 1) > input) {        sawOutOfOrder.set(true);    }    accumulator.add(input);    return accumulator;}
public static ExpectedLogs beam_f20925_0(Class<?> klass)
{    return ExpectedLogs.none(klass.getName());}
public void beam_f20926_0(String substring)
{    verify(Level.FINEST, substring);}
public void beam_f20935_0(String substring, Throwable t)
{    verify(Level.SEVERE, substring, t);}
public void beam_f20936_0(String substring)
{    verifyNotLogged(matcher(substring));}
protected boolean beam_f20945_0(LogRecord item)
{    return level.equals(item.getLevel()) && item.getMessage().contains(substring);}
private void beam_f20946_0(final Level level, final String substring, final Throwable throwable)
{    verifyLogged(matcher(level, substring, throwable));}
public void beam_f20955_0(LogRecord record)
{    logRecords.add(record);}
private Collection<LogRecord> beam_f20958_0()
{    return logRecords;}
public void beam_f20968_1() throws Throwable
{    String expected = generateRandomString();        expectedLogs.verifyNotLogged(expected);}
public void beam_f20969_1() throws Throwable
{    String expected = generateRandomString();        expectedLogs.verifyNotLogged(expected);}
public void beam_f20978_0() throws IOException
{    String tmpPath = tmpFolder.newFile().getPath();    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(containsString("Expected valid checksum, but received"));    new FileChecksumMatcher("", tmpPath);}
public void beam_f20979_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(containsString("Expected valid file path, but received"));    new FileChecksumMatcher("checksumString", "");}
public void beam_f20988_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.onlyPane(PAssert.PAssertionSite.capture(""));    Iterable<ValueInSingleWindow<Integer>> noFiring = ImmutableList.of(ValueInSingleWindow.of(9, BoundedWindow.TIMESTAMP_MIN_VALUE, GlobalWindow.INSTANCE, PaneInfo.NO_FIRING), ValueInSingleWindow.of(19, BoundedWindow.TIMESTAMP_MIN_VALUE, GlobalWindow.INSTANCE, PaneInfo.NO_FIRING));    assertThat(extractor.apply(noFiring), containsInAnyOrder(9, 19));}
public void beam_f20989_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.onlyPane(PAssert.PAssertionSite.capture(""));    Iterable<ValueInSingleWindow<Integer>> onlyFiring = ImmutableList.of(ValueInSingleWindow.of(2, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING), ValueInSingleWindow.of(1, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING));    assertThat(extractor.apply(onlyFiring), containsInAnyOrder(2, 1));}
public void beam_f20998_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.nonLatePanes();    Iterable<ValueInSingleWindow<Integer>> onlyOnTime = ImmutableList.of(ValueInSingleWindow.of(8, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.LATE, 0L, 0L)));    assertThat(extractor.apply(onlyOnTime), emptyIterable());}
public void beam_f20999_0()
{    SerializableFunction<Iterable<ValueInSingleWindow<Integer>>, Iterable<Integer>> extractor = PaneExtractors.nonLatePanes();    Iterable<ValueInSingleWindow<Integer>> onlyOnTime = ImmutableList.of(ValueInSingleWindow.of(8, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.LATE, 2L, 1L)), ValueInSingleWindow.of(7, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.NO_FIRING), ValueInSingleWindow.of(4, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(false, false, Timing.ON_TIME, 1L, 0L)), ValueInSingleWindow.of(1, new Instant(0L), GlobalWindow.INSTANCE, PaneInfo.createPane(true, false, Timing.EARLY)));    assertThat(extractor.apply(onlyOnTime), containsInAnyOrder(4, 1, 7));}
public void beam_f21009_0(NotSerializableObject value, ElementByteSizeObserver observer) throws Exception
{    observer.update(0L);}
private void beam_f21010_0()
{    throw new RuntimeException("Nested error");}
public void beam_f21019_0() throws Exception
{    thrown.expect(UnsupportedOperationException.class);    thrown.expectMessage(".hashCode() is not supported.");    PCollection<Integer> pcollection = pipeline.apply(Create.of(42));    PAssert.thatSingleton(pcollection).hashCode();}
public void beam_f21020_0() throws Exception
{    thrown.expect(UnsupportedOperationException.class);    thrown.expectMessage(".hashCode() is not supported.");    PCollection<Integer> pcollection = pipeline.apply(Create.of(42));    PAssert.that(pcollection).hashCode();}
public void beam_f21029_0() throws Exception
{    PCollection<Integer> pcollection = pipeline.apply(Create.timestamped(TimestampedValue.of(1, new Instant(100L)), TimestampedValue.of(2, new Instant(200L)), TimestampedValue.of(3, new Instant(300L)), TimestampedValue.of(4, new Instant(400L)))).apply(Window.into(SlidingWindows.of(Duration.millis(200L)).every(Duration.millis(100L)).withOffset(Duration.millis(50L))));    PAssert.that(pcollection).inWindow(new IntervalWindow(new Instant(-50L), new Instant(150L))).containsInAnyOrder(1);    PAssert.that(pcollection).inWindow(new IntervalWindow(new Instant(50L), new Instant(250L))).containsInAnyOrder(2, 1);    PAssert.that(pcollection).inWindow(new IntervalWindow(new Instant(150L), new Instant(350L))).containsInAnyOrder(2, 3);    PAssert.that(pcollection).inWindow(new IntervalWindow(new Instant(250L), new Instant(450L))).containsInAnyOrder(4, 3);    PAssert.that(pcollection).inWindow(new IntervalWindow(new Instant(350L), new Instant(550L))).containsInAnyOrder(4);    pipeline.run();}
public void beam_f21030_0()
{    PCollection<Long> vals = pipeline.apply(Create.empty(VarLongCoder.of()));    PAssert.that(vals).empty();    pipeline.run();}
private static Throwable beam_f21039_0(Pipeline pipeline)
{        try {        pipeline.run();    } catch (Throwable exc) {        return exc;    }    fail("assertion should have failed");    throw new RuntimeException("unreachable");}
public void beam_f21040_0()
{    PCollection<Integer> create = pipeline.apply("FirstCreate", Create.of(1, 2, 3));    PAssert.that(create).containsInAnyOrder(1, 2, 3);    PAssert.thatSingleton(create.apply(Sum.integersGlobally())).isEqualTo(6);    PAssert.thatMap(pipeline.apply("CreateMap", Create.of(KV.of(1, 2)))).isEqualTo(Collections.singletonMap(1, 2));    assertThat(PAssert.countAsserts(pipeline), equalTo(3));}
protected void beam_f21049_0()
{    try (ByteArrayInputStream bais = new ByteArrayInputStream(originalProperties)) {        System.getProperties().clear();        System.getProperties().load(bais);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void beam_f21050_0()
{    System.getProperties().put("TestA", "TestA");    assertNotNull(System.getProperty("TestA"));    assertNull(System.getProperty("TestB"));}
public void beam_f21059_0() throws Exception
{    AssertionError exc = assertionShouldFail(() -> assertThat(KV.of(1, 2), SerializableMatchers.kv(anything(), not(anything()))));    assertThat(exc.getMessage(), Matchers.containsString("value did not match"));}
public void beam_f21060_0() throws Exception
{    assertThat(KV.of("key", ImmutableList.of(1, 2, 3)), SerializableMatchers.<Object, Iterable<Integer>>kv(anything(), containsInAnyOrder(3, 2, 1)));}
public void beam_f21070_0()
{    WindowFn<Object, BoundedWindow> fn = StaticWindows.of(IntervalWindow.getCoder(), ImmutableList.of(first, second));    assertThat(fn.getDefaultWindowMappingFn().getSideInputWindow(first), Matchers.equalTo(first));    assertThat(fn.getDefaultWindowMappingFn().getSideInputWindow(second), Matchers.equalTo(second));}
public void beam_f21071_0()
{    WindowFn<Object, BoundedWindow> fn = StaticWindows.of(IntervalWindow.getCoder(), ImmutableList.of(second));    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("contains");    fn.getDefaultWindowMappingFn().getSideInputWindow(first);}
public void beam_f21080_0()
{    assertNotNull(pipeline);    assertNotNull(TestPipeline.create());}
public void beam_f21081_0()
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("@Rule");    TestPipeline.create().run();}
private static PCollection<String> beam_f21090_0(final PCollection<String> pCollection)
{    return pCollection.apply("Map2", MapElements.via(new SimpleFunction<String, String>() {        @Override        public String apply(final String input) {            return WHATEVER;        }    }));}
public String beam_f21091_0(final String input)
{    return WHATEVER;}
public void beam_f21100_0() throws Exception
{    final PCollection<String> pCollection = pCollection(pipeline);    PAssert.that(pCollection).containsInAnyOrder(WHATEVER);    pipeline.run().waitUntilFinish();    exception.expect(TestPipeline.AbandonedNodeException.class);    exception.expectMessage(P_ASSERT);        PAssert.that(pCollection).containsInAnyOrder(WHATEVER);}
public void beam_f21104_0() throws Exception
{    addTransform(pCollection(pipeline));}
public void beam_f21113_0()
{    Builder<Integer> stream = TestStream.create(VarIntCoder.of()).advanceWatermarkTo(new Instant(0L));    thrown.expect(IllegalArgumentException.class);    stream.advanceWatermarkTo(new Instant(-1L));}
public void beam_f21114_0()
{    Builder<Integer> stream = TestStream.create(VarIntCoder.of()).advanceWatermarkTo(BoundedWindow.TIMESTAMP_MAX_VALUE.minus(1L));    thrown.expect(IllegalArgumentException.class);    stream.advanceWatermarkTo(BoundedWindow.TIMESTAMP_MAX_VALUE);}
public void beam_f21123_0()
{    assertThat(WindowSupplier.of(IntervalWindow.getCoder(), ImmutableList.of(window, otherWindow)).get(), Matchers.containsInAnyOrder(otherWindow, window));}
public void beam_f21124_0()
{    WindowSupplier supplier = WindowSupplier.of(IntervalWindow.getCoder(), ImmutableList.of(window, otherWindow));    assertThat(SerializableUtils.clone(supplier).get(), Matchers.containsInAnyOrder(otherWindow, window));}
public void beam_f21133_0()
{    PCollection<Integer> input = intRangeCollection(p, 101);    PCollection<List<Integer>> quantiles = input.apply(ApproximateQuantiles.globally(5, new DescendingIntComparator()));    PAssert.that(quantiles).containsInAnyOrder(Arrays.asList(100, 75, 50, 25, 0));    p.run();}
public void beam_f21134_0()
{    PCollection<KV<String, Integer>> input = createInputTable(p);    PCollection<KV<String, List<Integer>>> quantiles = input.apply(ApproximateQuantiles.perKey(2));    PAssert.that(quantiles).containsInAnyOrder(KV.of("a", Arrays.asList(1, 3)), KV.of("b", Arrays.asList(1, 100)));    p.run();}
public void beam_f21143_0()
{    List<Integer> all = new ArrayList<>();    for (int i = 1; i < 1000; i++) {        all.add((int) Math.log(i));    }    testCombineFn(ApproximateQuantilesCombineFn.create(5), all, Arrays.asList(0, 5, 6, 6, 6));}
public void beam_f21144_0()
{    List<Integer> all = new ArrayList<>();    for (int i = 1; i < 1000; i++) {        all.add(1000 / i);    }    testCombineFn(ApproximateQuantilesCombineFn.create(5), all, Arrays.asList(1, 1, 2, 4, 1000));}
private List<Integer> beam_f21153_0(int size)
{    List<Integer> all = new ArrayList<>(size);    for (int i = 0; i < size; i++) {        all.add(i);    }    return all;}
public static Collection<Object[]> beam_f21154_0()
{    Collection<Object[]> testData = Lists.newArrayList();    for (int i = 0; i < epsilons.length; i++) {        for (int j = 0; j < maxElementExponents.length; j++) {            testData.add(new Object[] { epsilons[i], (long) Math.pow(10, maxElementExponents[j]), expectedNumBuffersValues[i][j], expectedBufferSizeValues[i][j] });        }    }    return testData;}
public void beam_f21163_0()
{    runApproximateUniqueWithDuplicates(elementCount, uniqueCount, sampleSize);}
public static Iterable<Object[]> beam_f21164_0() throws IOException
{    return ImmutableList.<Object[]>builder().add(new Object[] { 16 }, new Object[] { 64 }, new Object[] { 128 }, new Object[] { 256 }, new Object[] { 512 }, new Object[] { 1000 }, new Object[] { 2014 }, new Object[] { 15 }).build();}
public void beam_f21173_0()
{    final PCollection<Integer> input = p.apply(Create.of(Arrays.asList(1, 2, 3, 3)));    final PCollection<Long> estimate = input.apply(ApproximateUnique.globally(1000));    PAssert.thatSingleton(estimate).isEqualTo(3L);    p.run();}
public void beam_f21174_0()
{    runApproximateUniqueWithSkewedDistributions(10000, 2000, 1000);}
public void beam_f21183_0()
{    p.getCoderRegistry().registerCoderForClass(UserString.class, UserStringCoder.of());    PCollectionView<String> view = p.apply(Create.of("I")).apply(View.asSingleton());    PCollection<KV<String, KV<Integer, UserString>>> perKeyInput = p.apply(Create.timestamped(Arrays.asList(KV.of("a", KV.of(1, UserString.of("1"))), KV.of("a", KV.of(1, UserString.of("1"))), KV.of("a", KV.of(4, UserString.of("4"))), KV.of("b", KV.of(1, UserString.of("1"))), KV.of("b", KV.of(13, UserString.of("13")))), Arrays.asList(0L, 4L, 7L, 10L, 16L)).withCoder(KvCoder.of(StringUtf8Coder.of(), KvCoder.of(BigEndianIntegerCoder.of(), UserStringCoder.of()))));    TupleTag<Integer> maxIntTag = new TupleTag<>();    TupleTag<UserString> concatStringTag = new TupleTag<>();    PCollection<KV<String, KV<Integer, String>>> combineGlobally = perKeyInput.apply(Values.create()).apply(Combine.globally(CombineFns.compose().with(new GetIntegerFunction(), Max.ofIntegers(), maxIntTag).with(new GetUserStringFunction(), new ConcatStringWithContext(view), concatStringTag)).withoutDefaults().withSideInputs(ImmutableList.of(view))).apply(WithKeys.of("global")).apply("ExtractGloballyResult", ParDo.of(new ExtractResultDoFn(maxIntTag, concatStringTag)));    PCollection<KV<String, KV<Integer, String>>> combinePerKey = perKeyInput.apply(Combine.<String, KV<Integer, UserString>, CoCombineResult>perKey(CombineFns.compose().with(new GetIntegerFunction(), Max.ofIntegers(), maxIntTag).with(new GetUserStringFunction(), new ConcatStringWithContext(view), concatStringTag)).withSideInputs(ImmutableList.of(view))).apply("ExtractPerKeyResult", ParDo.of(new ExtractResultDoFn(maxIntTag, concatStringTag)));    PAssert.that(combineGlobally).containsInAnyOrder(KV.of("global", KV.of(13, "111134I")));    PAssert.that(combinePerKey).containsInAnyOrder(KV.of("a", KV.of(4, "114I")), KV.of("b", KV.of(13, "113I")));    p.run();}
public void beam_f21184_0()
{    p.getCoderRegistry().registerCoderForClass(UserString.class, NullableCoder.of(UserStringCoder.of()));    p.getCoderRegistry().registerCoderForClass(String.class, NullableCoder.of(StringUtf8Coder.of()));    PCollection<KV<String, KV<Integer, UserString>>> perKeyInput = p.apply(Create.timestamped(Arrays.asList(KV.of("a", KV.of(1, UserString.of("1"))), KV.of("a", KV.of(1, UserString.of("1"))), KV.of("a", KV.of(4, UserString.of("4"))), KV.of("b", KV.of(1, UserString.of("1"))), KV.of("b", KV.of(13, UserString.of("13")))), Arrays.asList(0L, 4L, 7L, 10L, 16L)).withCoder(KvCoder.of(NullableCoder.of(StringUtf8Coder.of()), KvCoder.of(BigEndianIntegerCoder.of(), NullableCoder.of(UserStringCoder.of())))));    TupleTag<Integer> maxIntTag = new TupleTag<>();    TupleTag<UserString> concatStringTag = new TupleTag<>();    PCollection<KV<String, KV<Integer, String>>> combinePerKey = perKeyInput.apply(Combine.perKey(CombineFns.compose().with(new GetIntegerFunction(), Max.ofIntegers(), maxIntTag).with(new GetUserStringFunction(), new OutputNullString(), concatStringTag))).apply("ExtractPerKeyResult", ParDo.of(new ExtractResultDoFn(maxIntTag, concatStringTag)));    PAssert.that(combinePerKey).containsInAnyOrder(KV.of("a", KV.of(4, (String) null)), KV.of("b", KV.of(13, (String) null)));    p.run();}
public boolean beam_f21193_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    UserString that = (UserString) o;    return Objects.equal(strValue, that.strValue);}
public int beam_f21194_0()
{    return Objects.hashCode(strValue);}
public UserString beam_f21204_0(UserString left, UserString right)
{    String retStr = left.strValue + right.strValue;    char[] chars = retStr.toCharArray();    Arrays.sort(chars);    return UserString.of(new String(chars));}
public UserString beam_f21205_0(UserString left, UserString right)
{    return null;}
protected void beam_f21214_0(List<KV<String, Integer>> table, Double globalMean, List<KV<String, Double>> perKeyMeans)
{    PCollection<KV<String, Integer>> input = createInput(pipeline, table);    PCollection<Double> mean = input.apply(Values.create()).apply(Combine.globally(new MeanInts()));    PCollection<KV<String, Double>> meanPerKey = input.apply(Combine.perKey(new MeanInts()));    PAssert.that(mean).containsInAnyOrder(globalMean);    PAssert.that(meanPerKey).containsInAnyOrder(perKeyMeans);    pipeline.run();}
public void beam_f21215_0(Integer element)
{    checkState(merges == 0);    checkState(outputs == 0);    inputs++;    sum += element;}
public void beam_f21224_0(Accumulator accumulator, OutputStream outStream) throws IOException
{    StringUtf8Coder.of().encode(accumulator.seed, outStream);    StringUtf8Coder.of().encode(accumulator.value, outStream);}
public Accumulator beam_f21225_0(InputStream inStream) throws IOException
{    String seed = StringUtf8Coder.of().decode(inStream);    String value = StringUtf8Coder.of().decode(inStream);    return new Accumulator(seed, value);}
public TestCombineFn.Accumulator beam_f21234_0(Iterable<TestCombineFn.Accumulator> accumulators, Context c)
{    String sideInputValue = c.sideInput(view).toString();    StringBuilder all = new StringBuilder();    for (TestCombineFn.Accumulator accumulator : accumulators) {        assertThat("Accumulators should all have the same Side Input Value", accumulator.seed, Matchers.equalTo(sideInputValue));        all.append(accumulator.value);        accumulator.value = "cleared in mergeAccumulators";    }    return new TestCombineFn.Accumulator(sideInputValue, all.toString());}
public String beam_f21235_0(TestCombineFn.Accumulator accumulator, Context c)
{    assertThat(accumulator.seed, Matchers.startsWith(c.sideInput(view).toString()));    char[] chars = accumulator.value.toCharArray();    Arrays.sort(chars);    return accumulator.seed + ":" + new String(chars);}
public Set<Integer> beam_f21244_0(Set<Integer> accumulator, Integer input)
{    accumulator.add(input);    return accumulator;}
public Set<Integer> beam_f21245_0(Iterable<Set<Integer>> accumulators)
{    Set<Integer> all = new HashSet<>();    for (Set<Integer> part : accumulators) {        all.addAll(part);    }    return all;}
public Coder<CountSum> beam_f21254_0(CoderRegistry registry, Coder<Integer> inputCoder)
{    return new CountSumCoder();}
public void beam_f21255_0(CountSum value, OutputStream outStream) throws IOException
{    LONG_CODER.encode(value.count, outStream);    DOUBLE_CODER.encode(value.sum, outStream);}
public void beam_f21265_0()
{    runTestSimpleCombine(Arrays.asList(KV.of("a", 1), KV.of("a", 1), KV.of("a", 4), KV.of("b", 1), KV.of("b", 13)), 20, Arrays.asList(KV.of("a", "114"), KV.of("b", "113")));}
public void beam_f21266_0()
{    runTestSimpleCombine(EMPTY_TABLE, 0, Collections.emptyList());}
public void beam_f21275_0()
{    UniqueInts combineFn = new UniqueInts() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("fnMetadata", "foobar"));        }    };    Combine.Globally<?, ?> combine = Combine.globally(combineFn).withFanout(1234);    DisplayData displayData = DisplayData.from(combine);    assertThat(displayData, hasDisplayItem("combineFn", combineFn.getClass()));    assertThat(displayData, hasDisplayItem("emitDefaultOnEmptyInput", true));    assertThat(displayData, hasDisplayItem("fanout", 1234));    assertThat(displayData, includesDisplayDataFor("combineFn", combineFn));}
public void beam_f21276_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("fnMetadata", "foobar"));}
public void beam_f21285_0()
{    SerializableFunction<Iterable<Object>, Object> combiner = xs -> Iterables.getFirst(xs, 0);    boolean lambdaClassSerializationThrows;    try {        SerializableUtils.clone(combiner.getClass());        lambdaClassSerializationThrows = false;    } catch (IllegalArgumentException e) {                lambdaClassSerializationThrows = true;    }    Assume.assumeTrue("Expected lambda class serialization to fail. " + "If it's fixed, we can remove special behavior in Combine.", lambdaClassSerializationThrows);    Combine.Globally<?, ?> combine = Combine.globally(combiner);        SerializableUtils.clone(combine);}
public void beam_f21286_0()
{    Combine.Globally<?, ?> combine = Combine.globally(xs -> Iterables.getFirst(xs, 0));    DisplayData displayData = DisplayData.from(combine);    MatcherAssert.assertThat(displayData.items(), not(empty()));}
public List<String> beam_f21295_0(List<String> accumulator, String input)
{    accumulator.add(input);    return accumulator;}
public List<String> beam_f21296_0(Iterable<List<String>> accumulators)
{                List<String> cur = createAccumulator();    for (List<String> accumulator : accumulators) {        accumulator.addAll(cur);        cur = accumulator;    }    return cur;}
public void beam_f21305_0()
{    FixedWindows windowFn = FixedWindows.of(Duration.standardMinutes(1));    final PCollectionView<Integer> view = pipeline.apply("CreateSideInput", Create.timestamped(TimestampedValue.of(1, new Instant(100)), TimestampedValue.of(3, new Instant(100)))).apply("WindowSideInput", Window.into(windowFn)).apply("CombineSideInput", Sum.integersGlobally().asSingletonView());    TimestampedValue<Void> nonEmptyElement = TimestampedValue.of(null, new Instant(100));    TimestampedValue<Void> emptyElement = TimestampedValue.atMinimumTimestamp(null);    PCollection<Integer> output = pipeline.apply("CreateMainInput", Create.timestamped(nonEmptyElement, emptyElement).withCoder(VoidCoder.of())).apply("WindowMainInput", Window.into(windowFn)).apply("OutputSideInput", ParDo.of(new DoFn<Void, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.sideInput(view));        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(4, 0);    PAssert.that(output).inWindow(windowFn.assignWindow(nonEmptyElement.getTimestamp())).containsInAnyOrder(4);    PAssert.that(output).inWindow(windowFn.assignWindow(emptyElement.getTimestamp())).containsInAnyOrder(0);    pipeline.run();}
public void beam_f21306_0(ProcessContext c)
{    c.output(c.sideInput(view));}
public void beam_f21315_0()
{    assertEquals("Count.PerElement", Count.perElement().getName());    assertEquals("Combine.globally(Count)", Count.globally().getName());}
public void beam_f21316_0()
{    PCollection<String> input = p.apply(Create.of(NO_LINES).withCoder(StringUtf8Coder.of()));    PCollection<String> windowed = input.apply(Window.into(FixedWindows.of(Duration.standardDays(1))));    String expected = Count.combineFn().getIncompatibleGlobalWindowErrorMessage();    exceptionRule.expect(IllegalStateException.class);    exceptionRule.expectMessage(expected);    windowed.apply(Count.globally());}
public int beam_f21326_0()
{    return myString.hashCode();}
public boolean beam_f21327_0(Object o)
{    return myString.equals(((UnserializableRecord) o).myString);}
public void beam_f21336_0() throws Exception
{    Coder<Record> coder = new RecordCoder();    Create.TimestampedValues<Record> values = Create.timestamped(TimestampedValue.of(new Record(), new Instant(0)), TimestampedValue.of(new Record2(), new Instant(0))).withCoder(coder);    assertThat(p.apply(values).getCoder(), equalTo(coder));}
public void beam_f21337_0() throws Exception
{    Coder<Record> coder = new RecordCoder();    p.getCoderRegistry().registerCoderForClass(Record.class, coder);    Create.TimestampedValues<Record> values = Create.timestamped(TimestampedValue.of(new Record(), new Instant(0)), TimestampedValue.of(new Record2(), new Instant(0))).withType(new TypeDescriptor<Record>() {    });    assertThat(p.apply(values).getCoder(), equalTo(coder));}
public void beam_f21346_0()
{    PCollection<String> out = p.apply(Create.of("a", "b", "c", "d").withSchema(STRING_SCHEMA, s -> Row.withSchema(STRING_SCHEMA).addValue(s).build(), r -> r.getString("field")));    assertThat(out.getCoder(), instanceOf(SchemaCoder.class));}
public void beam_f21347_0() throws Exception
{    List<UnserializableRecord> elements = ImmutableList.of(new UnserializableRecord("foo"), new UnserializableRecord("bar"), new UnserializableRecord("baz"));    CreateSource<UnserializableRecord> source = CreateSource.fromIterable(elements, new UnserializableRecord.UnserializableRecordCoder());    SerializableUtils.ensureSerializable(source);}
public Set<DisplayData> beam_f21356_0(final PTransform<? super PCollection<InputT>, ? extends POutput> root)
{    return displayDataForPrimitiveTransforms(root, null);}
public Set<DisplayData> beam_f21357_0(final PTransform<? super PCollection<InputT>, ? extends POutput> root, Coder<InputT> inputCoder)
{    Create.Values<InputT> input;    if (inputCoder != null) {        input = Create.empty(inputCoder);    } else {                input = (Create.Values<InputT>) Create.empty(VoidCoder.of());    }    Pipeline pipeline = Pipeline.create(options);    pipeline.apply("Input", input).apply("Transform", root);    return displayDataForPipeline(pipeline, root);}
public void beam_f21366_0(ProcessContext c) throws Exception
{    c.output(c.element());}
public void beam_f21367_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("primitiveKey", "primitiveValue"));}
public static Matcher<DisplayData> beam_f21377_0(String key, Boolean value)
{    return hasDisplayItem(key, DisplayData.Type.BOOLEAN, value);}
public static Matcher<DisplayData> beam_f21378_0(String key, Duration value)
{    return hasDisplayItem(key, DisplayData.Type.DURATION, value);}
public static Matcher<DisplayData> beam_f21387_0(final String path, final HasDisplayData subComponent)
{    return new CustomTypeSafeMatcher<DisplayData>("includes subcomponent") {        @Override        protected boolean matchesSafely(DisplayData displayData) {            DisplayData subComponentData = subComponentData(path);            if (subComponentData.items().isEmpty()) {                throw new UnsupportedOperationException("subComponent contains no display data; " + "cannot verify whether it is included");            }            DisplayDataComparison comparison = checkSubset(displayData, subComponentData, path);            return comparison.missingItems.isEmpty();        }        @Override        protected void describeMismatchSafely(DisplayData displayData, Description mismatchDescription) {            DisplayData subComponentDisplayData = subComponentData(path);            DisplayDataComparison comparison = checkSubset(displayData, subComponentDisplayData, path);            mismatchDescription.appendText("did not include:\n").appendValue(comparison.missingItems).appendText("\nNon-matching items:\n").appendValue(comparison.unmatchedItems);        }        private DisplayData subComponentData(final String path) {            return DisplayData.from(builder -> builder.include(path, subComponent));        }        private DisplayDataComparison checkSubset(DisplayData displayData, DisplayData included, String path) {            DisplayDataComparison comparison = new DisplayDataComparison(displayData.items());            for (Item item : included.items()) {                Item matchedItem = displayData.asMap().get(DisplayData.Identifier.of(DisplayData.Path.absolute(path), item.getNamespace(), item.getKey()));                if (matchedItem != null) {                    comparison.matched(matchedItem);                } else {                    comparison.missing(item);                }            }            return comparison;        }        class DisplayDataComparison {            Collection<Item> missingItems;            Collection<Item> unmatchedItems;            DisplayDataComparison(Collection<Item> superset) {                missingItems = Sets.newHashSet();                unmatchedItems = Sets.newHashSet(superset);            }            void matched(Item supersetItem) {                unmatchedItems.remove(supersetItem);            }            void missing(Item subsetItem) {                missingItems.add(subsetItem);            }        }    };}
protected boolean beam_f21388_0(DisplayData displayData)
{    DisplayData subComponentData = subComponentData(path);    if (subComponentData.items().isEmpty()) {        throw new UnsupportedOperationException("subComponent contains no display data; " + "cannot verify whether it is included");    }    DisplayDataComparison comparison = checkSubset(displayData, subComponentData, path);    return comparison.missingItems.isEmpty();}
public static Matcher<DisplayData.Item> beam_f21397_0(String... paths)
{    DisplayData.Path path = (paths.length == 0) ? DisplayData.Path.root() : DisplayData.Path.absolute(paths[0], Arrays.copyOfRange(paths, 1, paths.length));    return new FeatureMatcher<DisplayData.Item, DisplayData.Path>(is(path), " with namespace", "namespace") {        @Override        protected DisplayData.Path featureValueOf(DisplayData.Item actual) {            return actual.getPath();        }    };}
protected DisplayData.Path beam_f21398_0(DisplayData.Item actual)
{    return actual.getPath();}
protected T beam_f21407_0(DisplayData.Item actual)
{    @SuppressWarnings("unchecked")    T value = (T) actual.getValue();    return value;}
public static Matcher<DisplayData.Item> beam_f21408_0(String label)
{    return hasLabel(is(label));}
public void beam_f21417_0()
{    Matcher<DisplayData> matcher = hasDisplayItem(hasValue("bar"));    assertFalse(matcher.matches(createDisplayDataWithItem("foo", "baz")));    assertThat(createDisplayDataWithItem("foo", "bar"), matcher);}
public void beam_f21418_0()
{    Matcher<DisplayData> matcher = hasDisplayItem(hasPath("a", "b"));    final HasDisplayData subComponent = builder -> builder.include("b", (HasDisplayData) builder1 -> {        builder1.add(DisplayData.item("foo", "bar"));    });    assertFalse(matcher.matches(DisplayData.from(subComponent)));    assertThat(DisplayData.from(builder -> builder.include("a", subComponent)), matcher);}
public void beam_f21427_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("Location", "Seattle")).add(DisplayData.item("Forecast", "Rain"));}
public PCollection<String> beam_f21428_0(PCollection<String> begin)
{    throw new IllegalArgumentException("Should never be applied");}
public void beam_f21437_0()
{    DisplayData data = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", StaticValueProvider.of(1)));        }    });    assertThat(data.items(), hasSize(1));    assertThat(data, hasDisplayItem("foo", 1));}
public void beam_f21438_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", StaticValueProvider.of(1)));}
public void beam_f21447_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("now", value).withLabel("the current instant").withLinkUrl("http://time.gov").withNamespace(DisplayDataTest.class));}
public void beam_f21448_0()
{    DisplayData data = DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar"));        }    });    assertThat(data, hasDisplayItem(allOf(hasLabel(nullValue(String.class)), hasUrl(nullValue(String.class)))));}
public void beam_f21457_0(Builder builder)
{    builder.addIfNotNull(DisplayData.item("nullItem", (Class<?>) null).withLinkUrl("http://abc").withNamespace(DisplayDataTest.class).withLabel("Null item should be safe"));}
public void beam_f21458_0()
{    DisplayData.Path root = DisplayData.Path.root();    assertThat(root.getComponents(), empty());}
public void beam_f21467_0()
{    assertEquals("root string", "[]", DisplayData.Path.root().toString());    assertEquals("single component", "[a]", DisplayData.Path.absolute("a").toString());    assertEquals("hierarchy", "[a/b/c]", DisplayData.Path.absolute("a", "b", "c").toString());}
public void beam_f21468_0()
{    new EqualsTester().addEqualityGroup(DisplayData.Path.root(), DisplayData.Path.root()).addEqualityGroup(DisplayData.Path.root().extend("a"), DisplayData.Path.absolute("a")).addEqualityGroup(DisplayData.Path.root().extend("a").extend("b"), DisplayData.Path.absolute("a", "b")).testEquals();}
public void beam_f21477_0(Builder builder)
{    builder.include("p", new NoopDisplayData()).include("p", new NoopDisplayData());}
public void beam_f21478_0()
{    thrown.expectCause(isA(NullPointerException.class));    DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            builder.add(DisplayData.item("foo", "bar").withNamespace(null));        }    });}
public void beam_f21487_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("foo", "bar"));}
public void beam_f21488_0()
{    thrown.expectCause(isA(IllegalArgumentException.class));    DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "bar")).add(DisplayData.item("foo", "baz"));        }    });}
public void beam_f21497_0(Builder builder)
{    builder.include("p", componentA);}
public void beam_f21498_0()
{    HasDisplayData component = new DelegatingDisplayData(new DelegatingDisplayData(new NoopDisplayData()));    DisplayData data = DisplayData.from(component);    assertThat(data.items(), hasSize(2));}
public void beam_f21507_0(Builder builder)
{    builder.add(DisplayData.item("wrappedKey", "bar")).include("p", subcomponent);}
public void beam_f21508_0(Builder builder)
{    builder.delegate(wrapped);}
public void beam_f21517_0(Builder builder)
{    builder.add(DisplayData.item("integer", DisplayData.Type.INTEGER, "foobar"));}
public void beam_f21518_0()
{    assertEquals(DisplayData.Type.INTEGER, DisplayData.inferType(1234));    assertEquals(DisplayData.Type.INTEGER, DisplayData.inferType(1234L));    assertEquals(DisplayData.Type.FLOAT, DisplayData.inferType(12.3));    assertEquals(DisplayData.Type.FLOAT, DisplayData.inferType(12.3f));    assertEquals(DisplayData.Type.BOOLEAN, DisplayData.inferType(true));    assertEquals(DisplayData.Type.TIMESTAMP, DisplayData.inferType(Instant.now()));    assertEquals(DisplayData.Type.DURATION, DisplayData.inferType(Duration.millis(1234)));    assertEquals(DisplayData.Type.JAVA_CLASS, DisplayData.inferType(DisplayDataTest.class));    assertEquals(DisplayData.Type.STRING, DisplayData.inferType("hello world"));    assertEquals(null, DisplayData.inferType(null));    assertEquals(null, DisplayData.inferType(new Object() {    }));}
public void beam_f21527_0()
{    thrown.expectCause(isA(NullPointerException.class));    DisplayData.from(new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            builder.include(null, new NoopDisplayData());        }    });}
public void beam_f21528_0(Builder builder)
{    builder.include(null, new NoopDisplayData());}
public void beam_f21537_0() throws IOException
{    final String stringValue = "foobar";    final int intValue = 1234;    final double floatValue = 123.4;    final boolean boolValue = true;    final int durationMillis = 1234;    HasDisplayData component = new HasDisplayData() {        @Override        public void populateDisplayData(Builder builder) {            builder.add(DisplayData.item("string", stringValue)).add(DisplayData.item("long", intValue)).add(DisplayData.item("double", floatValue)).add(DisplayData.item("boolean", boolValue)).add(DisplayData.item("instant", new Instant(0))).add(DisplayData.item("duration", Duration.millis(durationMillis))).add(DisplayData.item("class", DisplayDataTest.class).withLinkUrl("http://abc").withLabel("baz"));        }    };    DisplayData data = DisplayData.from(component);    JsonNode json = MAPPER.readTree(MAPPER.writeValueAsBytes(data));    assertThat(json, hasExpectedJson(component, "STRING", "string", quoted(stringValue)));    assertThat(json, hasExpectedJson(component, "INTEGER", "long", intValue));    assertThat(json, hasExpectedJson(component, "FLOAT", "double", floatValue));    assertThat(json, hasExpectedJson(component, "BOOLEAN", "boolean", boolValue));    assertThat(json, hasExpectedJson(component, "DURATION", "duration", durationMillis));    assertThat(json, hasExpectedJson(component, "TIMESTAMP", "instant", quoted("1970-01-01T00:00:00.000Z")));    assertThat(json, hasExpectedJson(component, "JAVA_CLASS", "class", quoted(DisplayDataTest.class.getName()), quoted("DisplayDataTest"), "baz", "http://abc"));}
public void beam_f21538_0(Builder builder)
{    builder.add(DisplayData.item("string", stringValue)).add(DisplayData.item("long", intValue)).add(DisplayData.item("double", floatValue)).add(DisplayData.item("boolean", boolValue)).add(DisplayData.item("instant", new Instant(0))).add(DisplayData.item("duration", Duration.millis(durationMillis))).add(DisplayData.item("class", DisplayDataTest.class).withLinkUrl("http://abc").withLabel("baz"));}
public void beam_f21547_0(Builder builder)
{    throw new RuntimeException("oh noes!");}
public void beam_f21548_0(Builder builder)
{    builder.add(DisplayData.item("b", "b")).add(DisplayData.item("c", "c"));    try {        builder.include("p", failingComponent);        fail("Expected exception not thrown");    } catch (RuntimeException e) {        }    builder.include("p", safeComponent).add(DisplayData.item("d", "d"));}
private Matcher<Iterable<? super JsonNode>> beam_f21557_0(HasDisplayData component, String type, String key, Object value) throws IOException
{    return hasExpectedJson(component, type, key, value, null, null, null);}
private Matcher<Iterable<? super JsonNode>> beam_f21558_0(HasDisplayData component, String type, String key, Object value, Object shortValue, String label, String linkUrl) throws IOException
{    Class<?> nsClass = component.getClass();    StringBuilder builder = new StringBuilder();    builder.append("{");    builder.append(String.format("\"namespace\":\"%s\",", nsClass.getName()));    builder.append(String.format("\"type\":\"%s\",", type));    builder.append(String.format("\"key\":\"%s\",", key));    builder.append(String.format("\"value\":%s", value));    if (shortValue != null) {        builder.append(String.format(",\"shortValue\":%s", shortValue));    }    if (label != null) {        builder.append(String.format(",\"label\":\"%s\"", label));    }    if (linkUrl != null) {        builder.append(String.format(",\"linkUrl\":\"%s\"", linkUrl));    }    builder.append("}");    JsonNode jsonNode = MAPPER.readTree(builder.toString());    return hasItem(jsonNode);}
public void beam_f21568_0()
{    List<KV<String, String>> strings = Arrays.asList(KV.of("k1", "v1"), KV.of("k1", "v2"), KV.of("k2", "v1"));    PCollection<KV<String, String>> input = p.apply(Create.of(strings));    PCollection<KV<String, String>> output = input.apply(Distinct.withRepresentativeValueFn(new Keys<String>()).withRepresentativeType(TypeDescriptor.of(String.class)));    PAssert.that(output).satisfies(new Checker());    p.run();}
public void beam_f21569_0()
{    Instant base = new Instant(0);    TestStream<String> values = TestStream.create(StringUtf8Coder.of()).advanceWatermarkTo(base).addElements(TimestampedValue.of("k1", base), TimestampedValue.of("k2", base.plus(Duration.standardSeconds(10))), TimestampedValue.of("k3", base.plus(Duration.standardSeconds(20))), TimestampedValue.of("k1", base.plus(Duration.standardSeconds(30))), TimestampedValue.of("k2", base.plus(Duration.standardSeconds(40))), TimestampedValue.of("k3", base.plus(Duration.standardSeconds(50))), TimestampedValue.of("k4", base.plus(Duration.standardSeconds(60))), TimestampedValue.of("k5", base.plus(Duration.standardSeconds(70))), TimestampedValue.of("k6", base.plus(Duration.standardSeconds(80)))).advanceWatermarkToInfinity();    PCollection<String> distinctValues = windowedDistinctPipeline.apply(values).apply(Window.into(FixedWindows.of(Duration.standardSeconds(30)))).apply(Distinct.create());    PAssert.that(distinctValues).inWindow(new IntervalWindow(base, base.plus(Duration.standardSeconds(30)))).containsInAnyOrder("k1", "k2", "k3");    PAssert.that(distinctValues).inWindow(new IntervalWindow(base.plus(Duration.standardSeconds(30)), base.plus(Duration.standardSeconds(60)))).containsInAnyOrder("k1", "k2", "k3");    PAssert.that(distinctValues).inWindow(new IntervalWindow(base.plus(Duration.standardSeconds(60)), base.plus(Duration.standardSeconds(90)))).containsInAnyOrder("k4", "k5", "k6");    windowedDistinctPipeline.run();}
public void beam_f21578_0() throws Exception
{    for (DoFnTester.CloningBehavior cloning : DoFnTester.CloningBehavior.values()) {        try (DoFnTester<Long, String> tester = DoFnTester.of(new CounterDoFn())) {            tester.setCloningBehavior(cloning);                        assertThat(tester.processBundle(1L, 2L, 3L, 4L), hasItems("1", "2", "3", "4"));                        assertTrue(tester.peekOutputElements().isEmpty());        }    }}
public void beam_f21579_0() throws Exception
{    for (DoFnTester.CloningBehavior cloning : DoFnTester.CloningBehavior.values()) {        try (DoFnTester<Long, String> tester = DoFnTester.of(new CounterDoFn())) {            tester.setCloningBehavior(cloning);                        assertThat(tester.processBundle(1L, 2L, 3L, 4L), hasItems("1", "2", "3", "4"));            assertThat(tester.processBundle(5L, 6L, 7L), hasItems("5", "6", "7"));            assertThat(tester.processBundle(8L, 9L), hasItems("8", "9"));                        assertTrue(tester.peekOutputElements().isEmpty());        }    }}
public void beam_f21589_0() throws Exception
{    try (DoFnTester<Long, TimestampedValue<Long>> tester = DoFnTester.of(new ReifyTimestamps())) {        TimestampedValue<Long> input = TimestampedValue.of(1L, new Instant(100));        tester.processTimestampedElement(input);        assertThat(tester.takeOutputElements(), contains(input));    }}
public void beam_f21590_0(ProcessContext c)
{    c.output(TimestampedValue.of(c.element(), c.timestamp()));}
public void beam_f21599_0(ProcessContext c)
{    elements++;}
public void beam_f21600_0(FinishBundleContext c)
{    c.output(elements, Instant.now(), GlobalWindow.INSTANCE);}
public Boolean beam_f21609_0(Integer elem) throws Exception
{    return elem % 2 == 0;}
public void beam_f21610_0()
{    PCollection<Integer> output = p.apply(Create.of(591, 11789, 1257, 24578, 24799, 307)).apply(Filter.by(new TrivialFn(true)));    PAssert.that(output).containsInAnyOrder(591, 11789, 1257, 24578, 24799, 307);    p.run();}
public void beam_f21619_0()
{    assertThat(DisplayData.from(Filter.lessThan(123)), hasDisplayItem("predicate", "x < 123"));    assertThat(DisplayData.from(Filter.lessThanEq(234)), hasDisplayItem("predicate", "x ≤ 234"));    assertThat(DisplayData.from(Filter.greaterThan(345)), hasDisplayItem("predicate", "x > 345"));    assertThat(DisplayData.from(Filter.greaterThanEq(456)), hasDisplayItem("predicate", "x ≥ 456"));    assertThat(DisplayData.from(Filter.equal(567)), hasDisplayItem("predicate", "x == 567"));}
public void beam_f21620_0()
{    PCollection<Integer> output = p.apply(Create.of(591, 11789, 1257, 24578, 24799, 307)).apply(Filter.by(i -> true));    PAssert.that(output).containsInAnyOrder(591, 11789, 1257, 24578, 24799, 307);    p.run();}
public List<Integer> beam_f21629_0(Integer input) throws Exception
{    return ImmutableList.of(-input, input);}
public void beam_f21630_0() throws Exception
{    final PCollectionView<Integer> view = pipeline.apply("Create base", Create.of(40)).apply(View.asSingleton());    PCollection<Integer> output = pipeline.apply(Create.of(0, 1, 2)).apply(FlatMapElements.into(integers()).via(fn((input, c) -> ImmutableList.of(c.sideInput(view) - input, c.sideInput(view) + input), requiresSideInputs(view))));    PAssert.that(output).containsInAnyOrder(38, 39, 40, 40, 41, 42);    pipeline.run();}
public List<Integer> beam_f21639_0(Integer input)
{    return Collections.emptyList();}
public void beam_f21640_0()
{    SimpleFunction<Integer, List<Integer>> simpleFn = new SimpleFunction<Integer, List<Integer>>() {        @Override        public List<Integer> apply(Integer input) {            return Collections.emptyList();        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "baz"));        }    };    FlatMapElements<?, ?> simpleFlatMap = FlatMapElements.via(simpleFn);    assertThat(DisplayData.from(simpleFlatMap), hasDisplayItem("class", simpleFn.getClass()));    assertThat(DisplayData.from(simpleFlatMap), hasDisplayItem("foo", "baz"));}
public void beam_f21649_0() throws Exception
{    PCollection<Integer> output = pipeline.apply(Create.of(1, 2, 3)).apply(FlatMapElements.into(TypeDescriptors.integers()).via((Integer i) -> ImmutableList.of(i, -i)));    PAssert.that(output).containsInAnyOrder(1, 3, -1, -3, 2, -2);    pipeline.run();}
public void beam_f21650_0() throws Exception
{    PCollection<Integer> output = pipeline.apply(Create.of(1, 2, 3)).apply(FlatMapElements.into(TypeDescriptors.integers()).via(new Negater()::numAndNegation));    PAssert.that(output).containsInAnyOrder(1, 3, -1, -3, 2, -2);    pipeline.run();}
public String beam_f21659_0(ExceptionElement<Integer> input) throws Exception
{    return "";}
public void beam_f21660_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("bar", "buz"));}
public void beam_f21669_0(ProcessContext c)
{    for (String side : c.sideInput(view)) {        c.output(side);    }}
public void beam_f21670_0()
{    PCollection<String> output = PCollectionList.<String>empty(p).apply(Flatten.pCollections()).setCoder(StringUtf8Coder.of()).apply(ParDo.of(new IdentityFn<>()));    PAssert.that(output).empty();    p.run();}
public void beam_f21679_0()
{    PCollection<String> input1 = p.apply("CreateInput1", Create.of("Input1")).apply("Window1", Window.into(FixedWindows.of(Duration.standardMinutes(1))));    PCollection<String> input2 = p.apply("CreateInput2", Create.of("Input2")).apply("Window2", Window.into(FixedWindows.of(Duration.standardMinutes(1))));    PCollection<String> output = PCollectionList.of(input1).and(input2).apply(Flatten.pCollections());    p.run();    Assert.assertTrue(output.getWindowingStrategy().getWindowFn().isCompatible(FixedWindows.of(Duration.standardMinutes(1))));}
public void beam_f21680_0()
{    PCollection<String> input1 = p.apply("CreateInput1", Create.of("Input1")).apply("Window1", Window.into(Sessions.withGapDuration(Duration.standardMinutes(1))));    PCollection<String> input2 = p.apply("CreateInput2", Create.of("Input2")).apply("Window2", Window.into(Sessions.withGapDuration(Duration.standardMinutes(2))));    PCollection<String> output = PCollectionList.of(input1).and(input2).apply(Flatten.pCollections());    p.run();    Assert.assertTrue(output.getWindowingStrategy().getWindowFn().isCompatible(Sessions.withGapDuration(Duration.standardMinutes(2))));}
public void beam_f21689_0() throws Exception
{    PCollection<Integer> triggeredSums = p.apply(TestStream.create(VarIntCoder.of()).advanceWatermarkTo(new Instant(0)).addElements(TimestampedValue.of(2, new Instant(2)), TimestampedValue.of(5, new Instant(5))).advanceWatermarkTo(new Instant(100)).advanceProcessingTime(Duration.millis(10)).advanceWatermarkToInfinity()).apply(Window.<Integer>into(FixedWindows.of(Duration.millis(100))).withTimestampCombiner(TimestampCombiner.EARLIEST).accumulatingFiredPanes().withAllowedLateness(Duration.ZERO).triggering(Repeatedly.forever(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.millis(10))))).apply(Sum.integersGlobally().withoutDefaults());    PAssert.that(triggeredSums).containsInAnyOrder(7);    p.run();}
public void beam_f21690_0() throws Exception
{    List<KV<Map<String, String>, Integer>> ungroupedPairs = Arrays.asList();    PCollection<KV<Map<String, String>, Integer>> input = p.apply(Create.of(ungroupedPairs).withCoder(KvCoder.of(MapCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()), BigEndianIntegerCoder.of())));    thrown.expect(IllegalStateException.class);    thrown.expectMessage("must be deterministic");    input.apply(GroupByKey.create());}
public void beam_f21699_0() throws Exception
{    runLargeKeysTest(p, 10 << 10);}
public void beam_f21700_0() throws Exception
{    runLargeKeysTest(p, 100 << 10);}
public void beam_f21709_0()
{    List<KV<String, Integer>> ungroupedPairs = Arrays.asList();    PCollection<KV<String, Integer>> input = p.apply(Create.of(ungroupedPairs).withCoder(KvCoder.of(StringUtf8Coder.of(), BigEndianIntegerCoder.of()))).apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1))));    thrown.expect(IllegalStateException.class);    thrown.expectMessage("GroupByKey must have a valid Window merge function");    input.apply("GroupByKey", GroupByKey.create()).apply("GroupByKeyAgain", GroupByKey.create());}
private static KV<String, Collection<Integer>> beam_f21710_0(String key, Integer... values)
{    return KV.of(key, ImmutableList.copyOf(values));}
public int beam_f21719_0()
{    return ThreadLocalRandom.current().nextInt();}
public static DeterministicKeyCoder beam_f21720_0()
{    return INSTANCE;}
public void beam_f21730_0()
{    PCollection<KV<String, Iterable<String>>> collection = pipeline.apply("Input data", Create.of(data)).apply(GroupIntoBatches.ofSize(BATCH_SIZE)).setCoder(KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(StringUtf8Coder.of())));    PAssert.that("Incorrect batch size in one or more elements", collection).satisfies(new SerializableFunction<Iterable<KV<String, Iterable<String>>>, Void>() {        private boolean checkBatchSizes(Iterable<KV<String, Iterable<String>>> listToCheck) {            for (KV<String, Iterable<String>> element : listToCheck) {                if (Iterables.size(element.getValue()) != BATCH_SIZE) {                    return false;                }            }            return true;        }        @Override        public Void apply(Iterable<KV<String, Iterable<String>>> input) {            assertTrue(checkBatchSizes(input));            return null;        }    });    PAssert.thatSingleton("Incorrect collection size", collection.apply("Count", Count.globally())).isEqualTo(NUM_ELEMENTS / BATCH_SIZE);    pipeline.run();}
private boolean beam_f21731_0(Iterable<KV<String, Iterable<String>>> listToCheck)
{    for (KV<String, Iterable<String>> element : listToCheck) {        if (Iterables.size(element.getValue()) != BATCH_SIZE) {            return false;        }    }    return true;}
public void beam_f21740_0(int cacheSize)
{    int valueLen = 7;    TestUnionValues values = new TestUnionValues(0, 1, 0, 3, 0, 3, 3);    CoGbkResult result = new CoGbkResult(createSchema(5), values, cacheSize);    assertThat(values.maxPos(), equalTo(Math.min(cacheSize, valueLen)));    assertThat(result.getAll(new TupleTag<>("tag0")), contains(0, 2, 4));    assertThat(values.maxPos(), equalTo(valueLen));    assertThat(result.getAll(new TupleTag<>("tag3")), contains(3, 5, 6));    assertThat(result.getAll(new TupleTag<Integer>("tag2")), emptyIterable());    assertThat(result.getOnly(new TupleTag<>("tag1")), equalTo(1));    assertThat(result.getAll(new TupleTag<>("tag0")), contains(0, 2, 4));}
private CoGbkResultSchema beam_f21741_0(int size)
{    List<TupleTag<?>> tags = new ArrayList<>();    for (int i = 0; i < size; i++) {        tags.add(new TupleTag<Integer>("tag" + i));    }    return new CoGbkResultSchema(TupleTagList.of(tags));}
private PCollection<KV<Integer, String>> beam_f21750_0(String name, Pipeline p, List<KV<Integer, String>> list, List<Long> timestamps)
{    PCollection<KV<Integer, String>> input;    if (timestamps.isEmpty()) {        input = p.apply("Create" + name, Create.of(list).withCoder(KvCoder.of(BigEndianIntegerCoder.of(), StringUtf8Coder.of())));    } else {        input = p.apply("Create" + name, Create.timestamped(list, timestamps).withCoder(KvCoder.of(BigEndianIntegerCoder.of(), StringUtf8Coder.of())));    }    return input.apply("Identity" + name, ParDo.of(new DoFn<KV<Integer, String>, KV<Integer, String>>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.element());        }    }));}
public void beam_f21751_0(ProcessContext c)
{    c.output(c.element());}
public void beam_f21760_0()
{    TupleTag<String> namesTag = new TupleTag<>();    TupleTag<String> addressesTag = new TupleTag<>();    TupleTag<String> purchasesTag = new TupleTag<>();    PCollection<KV<Integer, CoGbkResult>> coGbkResults = buildPurchasesCoGbk(p, purchasesTag, addressesTag, namesTag);            PCollection<KV<String, Integer>> purchaseCountByKnownAddressesWithoutKnownNames = coGbkResults.apply(ParDo.of(new CorrelatePurchaseCountForAddressesWithoutNamesFn(purchasesTag, addressesTag, namesTag)));    PAssert.that(purchaseCountByKnownAddressesWithoutKnownNames).containsInAnyOrder(KV.of("29 School Rd", 2), KV.of("383 Jackson Street", 1));    p.run();}
public void beam_f21761_0()
{    TupleTag<String> clicksTag = new TupleTag<>();    TupleTag<String> purchasesTag = new TupleTag<>();    PCollection<KV<Integer, CoGbkResult>> coGbkResults = buildPurchasesCoGbkWithWindowing(p, clicksTag, purchasesTag);    PCollection<KV<String, String>> clickOfPurchase = coGbkResults.apply(ParDo.of(new ClickOfPurchaseFn(clicksTag, purchasesTag)));    PAssert.that(clickOfPurchase).containsInAnyOrder(KV.of("Click t0:Boat t1", "0:3"), KV.of("Click t0:Shoesi t2", "0:3"), KV.of("Click t0:Pens t3", "0:3"), KV.of("Click t4:Car t6", "4:7"), KV.of("Click t4:Book t7", "4:7"), KV.of("Click t6:Car t6", "4:7"), KV.of("Click t6:Book t7", "4:7"), KV.of("Click t8:House t8", "8:11"), KV.of("Click t8:Shoes t9", "8:11"), KV.of("Click t8:House t10", "8:11"));    p.run();}
public void beam_f21770_0()
{    PCollection<KV<String, Integer>> input = p.apply(Create.of(Arrays.asList(EMPTY_TABLE)).withCoder(KvCoder.of(StringUtf8Coder.of(), BigEndianIntegerCoder.of())));    PCollection<KV<Integer, String>> output = input.apply(KvSwap.create());    PAssert.that(output).empty();    p.run();}
public void beam_f21771_0()
{    assertThat(fn.defaultValue(), nullValue());}
public void beam_f21780_0()
{    TimestampedValue<Long> input = TimestampedValue.of(null, INSTANT.plus(10));    assertEquals("Null values are allowed", input, fn.addInput(TV, input));}
public void beam_f21781_0()
{    Iterable<TimestampedValue<Long>> accums = Lists.newArrayList(TV, TV_PLUS_TEN, TV_MINUS_TEN);    assertEquals(TV_PLUS_TEN, fn.mergeAccumulators(accums));}
public void beam_f21790_0() throws CannotProvideCoderException
{    Latest.LatestFn<Long> fn = new Latest.LatestFn<>();    CoderRegistry registry = CoderRegistry.createDefault();    TimestampedValue.TimestampedValueCoder<Long> inputCoder = TimestampedValue.TimestampedValueCoder.of(VarLongCoder.of());    assertThat("Default output coder should handle null values", fn.getDefaultOutputCoder(registry, inputCoder), instanceOf(NullableCoder.class));    assertThat("Default accumulator coder should handle null values", fn.getAccumulatorCoder(registry, inputCoder), instanceOf(NullableCoder.class));}
public void beam_f21791_0()
{    PCollection<String> output = p.apply(Create.timestamped(TimestampedValue.of("foo", new Instant(100)), TimestampedValue.of("bar", new Instant(300)), TimestampedValue.of("baz", new Instant(200)))).apply(Latest.globally());    PAssert.that(output).containsInAnyOrder("bar");    p.run();}
public T beam_f21800_0(T input) throws Exception
{    return input;}
public KV<T, String> beam_f21801_0(T input) throws Exception
{    return KV.of(input, "hello");}
public Integer beam_f21810_0(Integer input) throws Exception
{    return input;}
public void beam_f21811_0() throws Exception
{    pipeline.enableAbandonedNodeEnforcement(false);    pipeline.apply(Create.of(1, 2, 3)).apply("Polymorphic Identity", MapElements.via(new NestedPolymorphicSimpleFunction<>())).apply("Test Consumer", MapElements.via(new SimpleFunction<KV<Integer, String>, Integer>() {        @Override        public Integer apply(KV<Integer, String> input) {            return 42;        }    }));}
public void beam_f21820_0() throws Exception
{    pipeline.apply(Create.of("hello")).apply(WithKeys.of("k")).apply(new VoidValues<String, String>() {    });        pipeline.run();}
public void beam_f21821_0()
{    SerializableFunction<Integer, Integer> serializableFn = input -> input;    MapElements<?, ?> serializableMap = MapElements.into(integers()).via(serializableFn);    assertThat(DisplayData.from(serializableMap), hasDisplayItem("class", serializableFn.getClass()));}
public void beam_f21830_0()
{    InferableFunction<Integer, ?> inferableFn = new InferableFunction<Integer, Integer>() {        @Override        public Integer apply(Integer input) {            return input;        }        @Override        public void populateDisplayData(DisplayData.Builder builder) {            builder.add(DisplayData.item("foo", "baz"));        }    };    MapElements<?, ?> inferableMap = MapElements.via(inferableFn);    assertThat(DisplayData.from(inferableMap), hasDisplayItem("class", inferableFn.getClass()));    assertThat(DisplayData.from(inferableMap), hasDisplayItem("foo", "baz"));}
public Integer beam_f21831_0(Integer input)
{    return input;}
public int beam_f21840_0(int val)
{    return val * 2;}
public void beam_f21841_0()
{    Result<PCollection<Integer>, KV<Integer, Map<String, String>>> result = pipeline.apply(Create.of(0, 1)).apply(MapElements.into(TypeDescriptors.integers()).via((Integer i) -> 1 / i).exceptionsVia(new WithFailures.ExceptionAsMapHandler<Integer>() {    }));    PAssert.that(result.output()).containsInAnyOrder(1);    PAssert.thatSingleton(result.failures()).satisfies(kv -> {        assertEquals(Integer.valueOf(0), kv.getKey());        assertThat(kv.getValue().entrySet(), hasSize(3));        assertThat(kv.getValue(), hasKey("stackTrace"));        assertEquals("java.lang.ArithmeticException", kv.getValue().get("className"));        assertEquals("/ by zero", kv.getValue().get("message"));        return null;    });    pipeline.run();}
public void beam_f21850_0()
{    assertEquals("Combine.globally(MaxInteger)", Max.integersGlobally().getName());    assertEquals("Combine.globally(MaxDouble)", Max.doublesGlobally().getName());    assertEquals("Combine.globally(MaxLong)", Max.longsGlobally().getName());    assertEquals("Combine.perKey(MaxInteger)", Max.integersPerKey().getName());    assertEquals("Combine.perKey(MaxDouble)", Max.doublesPerKey().getName());    assertEquals("Combine.perKey(MaxLong)", Max.longsPerKey().getName());}
public void beam_f21851_0()
{    testCombineFn(Max.ofIntegers(), Lists.newArrayList(1, 2, 3, 4), 4);}
public void beam_f21860_0()
{    testCombineFn(Min.ofIntegers(), Lists.newArrayList(1, 2, 3, 4), 1);}
public void beam_f21861_0()
{    testCombineFn(Min.ofLongs(), Lists.newArrayList(1L, 2L, 3L, 4L), 1L);}
public void beam_f21870_0()
{    assertThat("startBundle should have been called", startBundleCalls, greaterThan(0));    assertThat("there should be one bundle that has been started but not finished", startBundleCalls, equalTo(finishBundleCalls + 1));    assertThat("teardown should not have been called", teardownCalled, is(false));    finishBundleCalls++;}
public void beam_f21871_0()
{    assertThat(setupCalled, is(true));    assertThat(startBundleCalls, anyOf(equalTo(finishBundleCalls)));    assertThat(teardownCalled, is(false));    teardownCalled = true;}
public void beam_f21880_0()
{    ExceptionThrowingFn fn = new ExceptionThrowingStatefulFn(MethodForException.FINISH_BUNDLE);    p.apply(Create.of(KV.of("a", 1), KV.of("b", 2), KV.of("a", 3))).apply(ParDo.of(fn));    try {        p.run();        fail("Pipeline should have failed with an exception");    } catch (Exception e) {        validate();    }}
public void beam_f21881_0()
{    ExceptionThrowingFn.callStateMap = new ConcurrentHashMap<>();    ExceptionThrowingFn.exceptionWasThrown.set(false);}
private void beam_f21890_0(MethodForException method) throws Exception
{    if (toThrow == method && !thrown) {        thrown = true;        exceptionWasThrown.set(true);        throw new Exception("Hasn't yet thrown");    }}
public void beam_f21891_0()
{    if (noOfInstancesToTearDown.decrementAndGet() == 0 && !exceptionWasThrown.get()) {        fail("Expected to have a processing method throw an exception");    }    assertThat("some lifecycle method should have been called", callStateMap.get(id()), is(notNullValue()));    updateCallState(CallState.TEARDOWN);}
public void beam_f21900_0(@Element Row row, OutputReceiver<String> r)
{    r.output(row.getString(0) + ":" + row.getInt32(1));}
public void beam_f21901_0()
{    List<MyPojo> pojoList = Lists.newArrayList(new MyPojo("a", 1), new MyPojo("b", 2), new MyPojo("c", 3));    Schema schema1 = Schema.builder().addStringField("string_field").addInt32Field("integer_field").build();    Schema schema2 = Schema.builder().addStringField("string2_field").addInt32Field("integer2_field").build();    Schema schema3 = Schema.builder().addStringField("string3_field").addInt32Field("integer3_field").build();    TupleTag<MyPojo> firstOutput = new TupleTag<>("first");    TupleTag<MyPojo> secondOutput = new TupleTag<>("second");    PCollectionTuple tuple = pipeline.apply(Create.of(pojoList).withSchema(schema1, o -> Row.withSchema(schema1).addValues(o.stringField, o.integerField).build(), r -> new MyPojo(r.getString("string_field"), r.getInt32("integer_field")))).apply("first", ParDo.of(new DoFn<MyPojo, MyPojo>() {        @ProcessElement        public void process(@Element Row row, MultiOutputReceiver r) {            r.getRowReceiver(firstOutput).output(Row.withSchema(schema2).addValues(row.getString(0), row.getInt32(1)).build());            r.getRowReceiver(secondOutput).output(Row.withSchema(schema3).addValues(row.getString(0), row.getInt32(1)).build());        }    }).withOutputTags(firstOutput, TupleTagList.of(secondOutput)));    tuple.get(firstOutput).setSchema(schema2, o -> Row.withSchema(schema2).addValues(o.stringField, o.integerField).build(), r -> new MyPojo(r.getString("string2_field"), r.getInt32("integer2_field")));    tuple.get(secondOutput).setSchema(schema3, o -> Row.withSchema(schema3).addValues(o.stringField, o.integerField).build(), r -> new MyPojo(r.getString("string3_field"), r.getInt32("integer3_field")));    PCollection<String> output1 = tuple.get(firstOutput).apply("second", ParDo.of(new DoFn<MyPojo, String>() {        @ProcessElement        public void process(@Element Row row, OutputReceiver<String> r) {            r.output(row.getString("string2_field") + ":" + row.getInt32("integer2_field"));        }    }));    PCollection<String> output2 = tuple.get(secondOutput).apply("third", ParDo.of(new DoFn<MyPojo, String>() {        @ProcessElement        public void process(@Element Row row, OutputReceiver<String> r) {            r.output(row.getString("string3_field") + ":" + row.getInt32("integer3_field"));        }    }));    PAssert.that(output1).containsInAnyOrder("a:1", "b:2", "c:3");    PAssert.that(output2).containsInAnyOrder("a:1", "b:2", "c:3");    pipeline.run();}
public void beam_f21910_0()
{    thrown.expect(IllegalArgumentException.class);    pipeline.apply(Create.of("a", "b", "c")).apply(ParDo.of(new DoFn<String, Void>() {        @ProcessElement        public void process(@Element Row row) {        }    }));    pipeline.run();}
public void beam_f21912_0()
{    List<MyPojo> pojoList = Lists.newArrayList(new MyPojo("a", 1), new MyPojo("b", 2), new MyPojo("c", 3));    Schema schema = Schema.builder().addStringField("string_field").addInt32Field("integer_field").build();    thrown.expect(IllegalArgumentException.class);    pipeline.apply(Create.of(pojoList).withSchema(schema, o -> Row.withSchema(schema).addValues(o.stringField, o.integerField).build(), r -> new MyPojo(r.getString("string_field"), r.getInt32("integer_field")))).apply(ParDo.of(new DoFn<MyPojo, Void>() {        @FieldAccess("a")        FieldAccessDescriptor fieldAccess = FieldAccessDescriptor.withFieldNames("baad");        @ProcessElement        public void process(@FieldAccess("a") Row row) {        }    }));    pipeline.run();}
public void beam_f21922_0(@FieldAccess("stringField") String stringField, @FieldAccess("integerField") Integer integerField, @FieldAccess("ints") Integer[] intArray, @FieldAccess("ints") List<Integer> intList, OutputReceiver<String> r)
{    r.output(stringField + ":" + integerField + ":" + Arrays.toString(intArray) + ":" + intList.toString());}
public void beam_f21923_0()
{    List<ForExtraction> pojoList = Lists.newArrayList(new AutoValue_ParDoSchemaTest_ForExtraction(1, "a", Lists.newArrayList(1, 2)), new AutoValue_ParDoSchemaTest_ForExtraction(2, "b", Lists.newArrayList(2, 3)), new AutoValue_ParDoSchemaTest_ForExtraction(3, "c", Lists.newArrayList(3, 4)));    PCollection<String> output = pipeline.apply(Create.of(pojoList)).apply(ParDo.of(new DoFn<ForExtraction, String>() {        @FieldAccess("stringSelector")        final FieldAccessDescriptor stringSelector = FieldAccessDescriptor.withFieldNames("stringField");        @FieldAccess("intSelector")        final FieldAccessDescriptor intSelector = FieldAccessDescriptor.withFieldNames("integerField");        @FieldAccess("intsSelector")        final FieldAccessDescriptor intsSelector = FieldAccessDescriptor.withFieldNames("ints");        @ProcessElement        public void process(@FieldAccess("stringSelector") String stringField, @FieldAccess("intSelector") int integerField, @FieldAccess("intsSelector") int[] intArray, OutputReceiver<String> r) {            r.output(stringField + ":" + integerField + ":" + Arrays.toString(intArray));        }    }));    PAssert.that(output).containsInAnyOrder("a:1:[1, 2]", "b:2:[2, 3]", "c:3:[3, 4]");    pipeline.run();}
private void beam_f21933_0(ProcessContext c, String value)
{    if (!sideInputViews.isEmpty()) {        List<Integer> sideInputValues = new ArrayList<>();        for (PCollectionView<Integer> sideInputView : sideInputViews) {            sideInputValues.add(c.sideInput(sideInputView));        }        value += ": " + sideInputValues;    }    c.output(value);    for (TupleTag<String> additionalOutputTupleTag : additionalOutputTupleTags) {        c.output(additionalOutputTupleTag, additionalOutputTupleTag.getId() + ": " + value);    }}
public void beam_f21934_0()
{    throw new RuntimeException("test error in initialize");}
public void beam_f21946_0()
{    List<Integer> inputs = Arrays.asList(3, -42, 666);    PCollection<String> output = pipeline.apply(Create.of(inputs)).apply(ParDo.of(new TestDoFn()));    PAssert.that(output).satisfies(ParDoTest.HasExpectedOutput.forInput(inputs));    pipeline.run();}
public void beam_f21947_0()
{    List<Integer> inputs = Arrays.asList();    PCollection<String> output = pipeline.apply(Create.of(inputs).withCoder(VarIntCoder.of())).apply("TestDoFn", ParDo.of(new TestDoFn()));    PAssert.that(output).satisfies(ParDoTest.HasExpectedOutput.forInput(inputs));    pipeline.run();}
public void beam_f21957_0()
{    DoFn<String, String> fn = new DoFn<String, String>() {        @ProcessElement        public void proccessElement(ProcessContext c) {        }        @Override        public void populateDisplayData(Builder builder) {            builder.add(DisplayData.item("fnMetadata", "foobar"));        }    };    SingleOutput<String, String> parDo = ParDo.of(fn);    DisplayData displayData = DisplayData.from(parDo);    assertThat(displayData, includesDisplayDataFor("fn", fn));    assertThat(displayData, hasDisplayItem("fn", fn.getClass()));}
public void beam_f21959_0(Builder builder)
{    builder.add(DisplayData.item("fnMetadata", "foobar"));}
public void beam_f21971_0(@Element Integer element, MultiOutputReceiver r)
{    r.get(additionalOutputTag).output(element);}
public void beam_f21972_0()
{    List<Integer> inputs = Arrays.asList(3, -42, 666);    TupleTag<String> notOutputTag = new TupleTag<String>("additional") {    };    pipeline.apply(Create.of(inputs)).apply(ParDo.of(new TestDoFn(Arrays.asList(), Arrays.asList(notOutputTag))));    thrown.expectMessage("additional");    pipeline.run();}
public void beam_f21987_0(OutputReceiver<List<Integer>> r, @SideInput(sideInputTag1) List<Integer> tag1)
{    List<Integer> sideSorted = Lists.newArrayList(tag1);    Collections.sort(sideSorted);    r.output(sideSorted);}
public void beam_f21988_0()
{    final List<Integer> side1Data = ImmutableList.of(2, 0);    final PCollectionView<List<Integer>> sideInput1 = pipeline.apply("CreateSideInput1", Create.of(side1Data)).apply("ViewSideInput1", View.asList());    final Integer side2Data = 5;    final PCollectionView<Integer> sideInput2 = pipeline.apply("CreateSideInput2", Create.of(side2Data)).apply("ViewSideInput2", View.asSingleton());    final List<Integer> side3Data = ImmutableList.of(1, 3);    final PCollectionView<Iterable<Integer>> sideInput3 = pipeline.apply("CreateSideInput3", Create.of(side3Data)).apply("ViewSideInput3", View.asIterable());    final List<KV<Integer, Integer>> side4Data = ImmutableList.of(KV.of(1, 2), KV.of(2, 3), KV.of(3, 4));    final PCollectionView<Map<Integer, Integer>> sideInput4 = pipeline.apply("CreateSideInput4", Create.of(side4Data)).apply("ViewSideInput4", View.asMap());    final List<KV<Integer, Integer>> side5Data = ImmutableList.of(KV.of(1, 2), KV.of(1, 3), KV.of(3, 4));    final PCollectionView<Map<Integer, Iterable<Integer>>> sideInput5 = pipeline.apply("CreateSideInput5", Create.of(side5Data)).apply("ViewSideInput5", View.asMultimap());        final String sideInputTag1 = "tag1";    final String sideInputTag2 = "tag2";    final String sideInputTag3 = "tag3";    final String sideInputTag4 = "tag4";    final String sideInputTag5 = "tag5";    final TupleTag<Integer> outputTag1 = new TupleTag<>();    final TupleTag<Integer> outputTag2 = new TupleTag<>();    final TupleTag<Integer> outputTag3 = new TupleTag<>();    final TupleTag<KV<Integer, Integer>> outputTag4 = new TupleTag<>();    final TupleTag<KV<Integer, Integer>> outputTag5 = new TupleTag<>();    DoFn<Integer, Integer> fn = new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(MultiOutputReceiver r, @SideInput(sideInputTag1) List<Integer> side1, @SideInput(sideInputTag2) Integer side2, @SideInput(sideInputTag3) Iterable<Integer> side3, @SideInput(sideInputTag4) Map<Integer, Integer> side4, @SideInput(sideInputTag5) Map<Integer, Iterable<Integer>> side5) {            side1.forEach(i -> r.get(outputTag1).output(i));            r.get(outputTag2).output(side2);            side3.forEach(i -> r.get(outputTag3).output(i));            side4.forEach((k, v) -> r.get(outputTag4).output(KV.of(k, v)));            side5.forEach((k, v) -> v.forEach(v2 -> r.get(outputTag5).output(KV.of(k, v2))));        }    };    PCollectionTuple output = pipeline.apply("Create main input", Create.of(2)).apply(ParDo.of(fn).withSideInput(sideInputTag1, sideInput1).withSideInput(sideInputTag2, sideInput2).withSideInput(sideInputTag3, sideInput3).withSideInput(sideInputTag4, sideInput4).withSideInput(sideInputTag5, sideInput5).withOutputTags(outputTag1, TupleTagList.of(outputTag2).and(outputTag3).and(outputTag4).and(outputTag5)));    output.get(outputTag1).setCoder(VarIntCoder.of());    output.get(outputTag2).setCoder(VarIntCoder.of());    output.get(outputTag3).setCoder(VarIntCoder.of());    output.get(outputTag4).setCoder(KvCoder.of(VarIntCoder.of(), VarIntCoder.of()));    output.get(outputTag5).setCoder(KvCoder.of(VarIntCoder.of(), VarIntCoder.of()));    PAssert.that(output.get(outputTag1)).containsInAnyOrder(side1Data);    PAssert.that(output.get(outputTag2)).containsInAnyOrder(side2Data);    PAssert.that(output.get(outputTag3)).containsInAnyOrder(side3Data);    PAssert.that(output.get(outputTag4)).containsInAnyOrder(side4Data);    PAssert.that(output.get(outputTag5)).containsInAnyOrder(side5Data);    pipeline.run();}
public void beam_f21997_0()
{    pipeline.enableAbandonedNodeEnforcement(false);    PCollection<String> output = pipeline.apply(Create.of(1)).apply(ParDo.of(new StrangelyNamedDoer()));    assertThat(output.getName(), containsString("ParDo(StrangelyNamedDoer)"));}
public void beam_f21998_0()
{    assertThat(ParDo.of(new TaggedOutputDummyFn(null, null)).withOutputTags(null, null).getName(), containsString("ParMultiDo(TaggedOutputDummy)"));}
public void beam_f22007_0(ProcessContext context, @Element TestDummy element)
{    context.output(element);    context.output(additionalOutputTag, element);}
public void beam_f22008_0(ProcessContext context)
{    context.output(1);}
public void beam_f22018_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(Arrays.asList(3, 42, 6)));    PCollection<String> output = input.apply(ParDo.of(new TestOutputTimestampDoFn<>())).apply(ParDo.of(new TestShiftTimestampDoFn<>(Duration.ZERO, Duration.ZERO))).apply(ParDo.of(new TestFormatTimestampDoFn<>()));    PAssert.that(output).containsInAnyOrder("processing: 3, timestamp: 3", "processing: 42, timestamp: 42", "processing: 6, timestamp: 6");    pipeline.run();}
public void beam_f22019_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(Arrays.asList(3, 42, 6)));    final TupleTag<Integer> mainOutputTag = new TupleTag<Integer>("main") {    };    final TupleTag<Integer> additionalOutputTag = new TupleTag<Integer>("additional") {    };    PCollection<String> output = input.apply(ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(@Element Integer element, MultiOutputReceiver r) {            r.get(additionalOutputTag).outputWithTimestamp(element, new Instant(element.longValue()));        }    }).withOutputTags(mainOutputTag, TupleTagList.of(additionalOutputTag))).get(additionalOutputTag).apply(ParDo.of(new TestShiftTimestampDoFn<>(Duration.ZERO, Duration.ZERO))).apply(ParDo.of(new TestFormatTimestampDoFn<>()));    PAssert.that(output).containsInAnyOrder("processing: 3, timestamp: 3", "processing: 42, timestamp: 42", "processing: 6, timestamp: 6");    pipeline.run();}
public void beam_f22028_0(@StateId(stateId) ValueState<Integer> state, OutputReceiver<Integer> r)
{    Integer currentValue = MoreObjects.firstNonNull(state.read(), 0);    r.output(currentValue);    state.write(currentValue + 1);}
public void beam_f22029_0()
{    final String stateId = "foo";    DoFn<KV<Integer, Integer>, Integer> onePerKey = new DoFn<KV<Integer, Integer>, Integer>() {        @StateId(stateId)        private final StateSpec<ValueState<Integer>> seenSpec = StateSpecs.value(VarIntCoder.of());        @ProcessElement        public void processElement(@Element KV<Integer, Integer> element, @StateId(stateId) ValueState<Integer> seenState, OutputReceiver<Integer> r) {            Integer seen = MoreObjects.firstNonNull(seenState.read(), 0);            if (seen == 0) {                seenState.write(seen + 1);                r.output(element.getValue());            }        }    };    int numKeys = 50;        List<KV<Integer, Integer>> input = new ArrayList<>();        Set<Integer> expectedOutput = new HashSet<>();    for (int key = 0; key < numKeys; ++key) {        int output = 1000 + key;        expectedOutput.add(output);        for (int i = 0; i < 15; ++i) {            input.add(KV.of(key, output));        }    }    Collections.shuffle(input);    PCollection<Integer> output = pipeline.apply(Create.of(input)).apply(ParDo.of(onePerKey));    PAssert.that(output).containsInAnyOrder(expectedOutput);    pipeline.run();}
public void beam_f22040_0(@StateId(stateId) ValueState<Integer> state, OutputReceiver<KV<String, Integer>> r)
{    Integer currentValue = MoreObjects.firstNonNull(state.read(), 0);    r.output(KV.of("sizzle", currentValue));    state.write(currentValue + 1);}
public void beam_f22041_0(@StateId(stateId) ValueState<Integer> state, OutputReceiver<Integer> r)
{    Integer currentValue = MoreObjects.firstNonNull(state.read(), 13);    r.output(currentValue);    state.write(currentValue + 13);}
public void beam_f22050_0()
{    final String stateId = "foo";    DoFn<KV<String, Double>, String> fn = new DoFn<KV<String, Double>, String>() {        private static final double EPSILON = 0.0001;        @StateId(stateId)        private final StateSpec<CombiningState<Double, CountSum<Double>, Double>> combiningState = StateSpecs.combining(new Mean.CountSumCoder<Double>(), Mean.of());        @ProcessElement        public void processElement(ProcessContext c, @Element KV<String, Double> element, @StateId(stateId) CombiningState<Double, CountSum<Double>, Double> state, OutputReceiver<String> r) {            state.add(element.getValue());            Double currentValue = state.read();            if (Math.abs(currentValue - 0.5) < EPSILON) {                r.output("right on");            }        }    };    PCollection<String> output = pipeline.apply(Create.of(KV.of("hello", 0.3), KV.of("hello", 0.6), KV.of("hello", 0.6))).apply(ParDo.of(fn));        PAssert.that(output).containsInAnyOrder("right on");    pipeline.run();}
public void beam_f22051_0(ProcessContext c, @Element KV<String, Double> element, @StateId(stateId) CombiningState<Double, CountSum<Double>, Double> state, OutputReceiver<String> r)
{    state.add(element.getValue());    Double currentValue = state.read();    if (Math.abs(currentValue - 0.5) < EPSILON) {        r.output("right on");    }}
public void beam_f22060_0()
{    final String stateId = "foo";    final String countStateId = "count";    Coder<MyInteger> myIntegerCoder = MyIntegerCoder.of();    pipeline.getCoderRegistry().registerCoderForClass(MyInteger.class, myIntegerCoder);    DoFn<KV<String, Integer>, Set<MyInteger>> fn = new DoFn<KV<String, Integer>, Set<MyInteger>>() {        @StateId(stateId)        private final StateSpec<SetState<MyInteger>> setState = StateSpecs.set();        @StateId(countStateId)        private final StateSpec<CombiningState<Integer, int[], Integer>> countState = StateSpecs.combiningFromInputInternal(VarIntCoder.of(), Sum.ofIntegers());        @ProcessElement        public void processElement(@Element KV<String, Integer> element, @StateId(stateId) SetState<MyInteger> state, @StateId(countStateId) CombiningState<Integer, int[], Integer> count, OutputReceiver<Set<MyInteger>> r) {            state.add(new MyInteger(element.getValue()));            count.add(1);            if (count.read() >= 4) {                Set<MyInteger> set = Sets.newHashSet(state.read());                r.output(set);            }        }    };    PCollection<Set<MyInteger>> output = pipeline.apply(Create.of(KV.of("hello", 97), KV.of("hello", 42), KV.of("hello", 42), KV.of("hello", 12))).apply(ParDo.of(fn)).setCoder(SetCoder.of(myIntegerCoder));    PAssert.that(output).containsInAnyOrder(Sets.newHashSet(new MyInteger(97), new MyInteger(42), new MyInteger(12)));    pipeline.run();}
public void beam_f22061_0(@Element KV<String, Integer> element, @StateId(stateId) SetState<MyInteger> state, @StateId(countStateId) CombiningState<Integer, int[], Integer> count, OutputReceiver<Set<MyInteger>> r)
{    state.add(new MyInteger(element.getValue()));    count.add(1);    if (count.read() >= 4) {        Set<MyInteger> set = Sets.newHashSet(state.read());        r.output(set);    }}
public MyInteger beam_f22070_0(MyInteger accumulator, Integer input)
{    return new MyInteger(accumulator.getValue() + input);}
public MyInteger beam_f22071_0(Iterable<MyInteger> accumulators)
{    int newValue = 0;    for (MyInteger myInteger : accumulators) {        newValue += myInteger.getValue();    }    return new MyInteger(newValue);}
public void beam_f22080_0()
{    final String timerId = "foo";    DoFn<String, Integer> fn = new DoFn<String, Integer>() {        @TimerId(timerId)        private final TimerSpec timer = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void processElement(ProcessContext c, @TimerId(timerId) Timer timer) {        }        @OnTimer(timerId)        public void onTimer() {        }    };    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("timer");    thrown.expectMessage("KvCoder");    pipeline.apply(Create.of("hello", "goodbye", "hello again")).apply(ParDo.of(fn));}
public void beam_f22083_0()
{    final String timerId = "foo";        DoFn<KV<Double, String>, Integer> fn = new DoFn<KV<Double, String>, Integer>() {        @TimerId(timerId)        private final TimerSpec timer = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void processElement(ProcessContext c, @TimerId(timerId) Timer timer) {        }        @OnTimer(timerId)        public void onTimer() {        }    };    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("timer");    thrown.expectMessage("deterministic");    pipeline.apply(Create.of(KV.of(1.0, "hello"), KV.of(5.4, "goodbye"), KV.of(7.2, "hello again"))).apply(ParDo.of(fn));}
public void beam_f22094_0(@Timestamp Instant timestamp, OutputReceiver<KV<Integer, Instant>> r)
{    r.output(KV.of(42, timestamp));}
public void beam_f22095_0() throws Exception
{    final String timerId = "foo";    DoFn<KV<String, Integer>, BoundedWindow> fn = new DoFn<KV<String, Integer>, BoundedWindow>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void processElement(@TimerId(timerId) Timer timer) {            timer.offset(Duration.standardSeconds(1)).setRelative();        }        @OnTimer(timerId)        public void onTimer(BoundedWindow window, OutputReceiver<BoundedWindow> r) {            r.output(window);        }        @Override        public TypeDescriptor<BoundedWindow> getOutputTypeDescriptor() {            return (TypeDescriptor) TypeDescriptor.of(IntervalWindow.class);        }    };    SlidingWindows windowing = SlidingWindows.of(Duration.standardMinutes(3)).every(Duration.standardMinutes(1));    PCollection<BoundedWindow> output = pipeline.apply(Create.timestamped(TimestampedValue.of(KV.of("hello", 24), new Instant(0L)))).apply(Window.into(windowing)).apply(ParDo.of(fn));    PAssert.that(output).containsInAnyOrder(new IntervalWindow(new Instant(0), Duration.standardMinutes(3)), new IntervalWindow(new Instant(0).minus(Duration.standardMinutes(1)), Duration.standardMinutes(3)), new IntervalWindow(new Instant(0).minus(Duration.standardMinutes(2)), Duration.standardMinutes(3)));    pipeline.run();}
public void beam_f22104_0(@StateId(stateId) ValueState<Integer> countState, @TimerId(timerId) Timer loopTimer, OutputReceiver<Integer> r)
{    int count = MoreObjects.firstNonNull(countState.read(), 0);    if (count < loopCount) {        r.output(count);        countState.write(count + 1);        loopTimer.offset(Duration.millis(1)).setRelative();    }}
public void beam_f22105_0() throws Exception
{    final String timerId = "foo";    final String stateId = "sizzle";    final int offset = 5000;    final int timerOutput = 4093;    DoFn<KV<String, Integer>, KV<String, Integer>> fn = new DoFn<KV<String, Integer>, KV<String, Integer>>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @StateId(stateId)        private final StateSpec<ValueState<String>> stateSpec = StateSpecs.value(StringUtf8Coder.of());        @ProcessElement        public void processElement(ProcessContext context, @TimerId(timerId) Timer timer, @StateId(stateId) ValueState<String> state, BoundedWindow window) {            timer.set(window.maxTimestamp());            state.write(context.element().getKey());            context.output(KV.of(context.element().getKey(), context.element().getValue() + offset));        }        @OnTimer(timerId)        public void onTimer(@StateId(stateId) ValueState<String> state, OutputReceiver<KV<String, Integer>> r) {            r.output(KV.of(state.read(), timerOutput));        }    };        int numKeys = 50;    List<KV<String, Integer>> input = new ArrayList<>();    List<KV<String, Integer>> expectedOutput = new ArrayList<>();    for (Integer key = 0; key < numKeys; ++key) {                expectedOutput.add(KV.of(key.toString(), timerOutput));        for (int i = 0; i < 15; ++i) {                        input.add(KV.of(key.toString(), i));            expectedOutput.add(KV.of(key.toString(), i + offset));        }    }    Collections.shuffle(input);    PCollection<KV<String, Integer>> output = pipeline.apply(Create.of(input)).apply(ParDo.of(fn));    PAssert.that(output).containsInAnyOrder(expectedOutput);    pipeline.run();}
public void beam_f22116_0(TimeDomain timeDomain, OutputReceiver<Integer> r)
{    if (timeDomain.equals(TimeDomain.PROCESSING_TIME)) {        r.output(42);    }}
public void beam_f22117_0() throws Exception
{    final String timerId = "foo";    DoFn<KV<String, Integer>, Integer> fn = new DoFn<KV<String, Integer>, Integer>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void processElement(@TimerId(timerId) Timer timer, OutputReceiver<Integer> r) {            timer.offset(Duration.standardSeconds(1)).setRelative();            r.output(3);        }        @OnTimer(timerId)        public void onTimer(OutputReceiver<Integer> r) {            r.output(42);        }    };    TestStream<KV<String, Integer>> stream = TestStream.create(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of())).advanceWatermarkTo(new Instant(0)).addElements(KV.of("hello", 37)).advanceWatermarkTo(new Instant(0).plus(Duration.standardSeconds(1))).advanceWatermarkToInfinity();    PCollection<Integer> output = pipeline.apply(stream).apply(ParDo.of(fn));    PAssert.that(output).containsInAnyOrder(3, 42);    pipeline.run();}
public void beam_f22126_0() throws Exception
{    final String timerId = "foo";    DoFn<KV<String, String>, String> fn = new DoFn<KV<String, String>, String>() {        @TimerId(timerId)        private final TimerSpec spec = TimerSpecs.timer(TimeDomain.PROCESSING_TIME);        @ProcessElement        public void processElement(ProcessContext context, @TimerId(timerId) Timer timer) {            timer.offset(Duration.standardSeconds(1)).setRelative();            context.output(context.element().getValue());        }        @OnTimer(timerId)        public void onTimer(OutputReceiver<String> r) {            r.output("timer_output");        }    };    TestStream<KV<String, String>> stream = TestStream.create(KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of())).addElements(KV.of("key", "input1")).addElements(KV.of("key", "input2")).advanceProcessingTime(Duration.standardSeconds(2)).advanceWatermarkToInfinity();    PCollection<String> output = pipeline.apply(stream).apply(ParDo.of(fn));            PAssert.that(output).containsInAnyOrder("input1", "input2", "timer_output");    pipeline.run();}
public void beam_f22127_0(ProcessContext context, @TimerId(timerId) Timer timer)
{    timer.offset(Duration.standardSeconds(1)).setRelative();    context.output(context.element().getValue());}
public void beam_f22136_0()
{    final String stateId = "foo";    MyIntegerCoder myIntegerCoder = MyIntegerCoder.of();    pipeline.getCoderRegistry().registerCoderForClass(MyInteger.class, myIntegerCoder);    DoFn<KV<String, Integer>, MyInteger> fn = new DoFn<KV<String, Integer>, MyInteger>() {        @StateId(stateId)        private final StateSpec<ValueState<MyInteger>> intState = StateSpecs.value();        @ProcessElement        public void processElement(ProcessContext c, @StateId(stateId) ValueState<MyInteger> state, OutputReceiver<MyInteger> r) {            MyInteger currentValue = MoreObjects.firstNonNull(state.read(), new MyInteger(0));            r.output(currentValue);            state.write(new MyInteger(currentValue.getValue() + 1));        }    };    PCollection<MyInteger> output = pipeline.apply(Create.of(KV.of("hello", 42), KV.of("hello", 97), KV.of("hello", 84))).apply(ParDo.of(fn)).setCoder(myIntegerCoder);    PAssert.that(output).containsInAnyOrder(new MyInteger(0), new MyInteger(1), new MyInteger(2));    pipeline.run();}
public void beam_f22137_0(ProcessContext c, @StateId(stateId) ValueState<MyInteger> state, OutputReceiver<MyInteger> r)
{    MyInteger currentValue = MoreObjects.firstNonNull(state.read(), new MyInteger(0));    r.output(currentValue);    state.write(new MyInteger(currentValue.getValue() + 1));}
public void beam_f22147_0(TestDummy value, ElementByteSizeObserver observer) throws Exception
{    observer.update(0L);}
public void beam_f22149_0(MultiOutputReceiver r)
{    r.get(mainOutputTag).output(1);    r.get(dummyOutputTag).output(new TestDummy());}
public MyInteger beam_f22158_0(InputStream inStream) throws IOException
{    return new MyInteger(delegate.decode(inStream));}
public static HasExpectedOutput beam_f22159_0(List<Integer> inputs)
{    return new HasExpectedOutput(new ArrayList<>(inputs), new ArrayList<>(), "");}
public int beam_f22168_0(Integer elem, int numPartitions)
{    return elem;}
public void beam_f22169_0()
{    PCollectionList<Integer> outputs = pipeline.apply(Create.of(591, 11789, 1257, 24578, 24799, 307)).apply(Partition.of(2, new ModFn()));    assertTrue(outputs.size() == 2);    PAssert.that(outputs.get(0)).containsInAnyOrder(24578);    PAssert.that(outputs.get(1)).containsInAnyOrder(591, 11789, 1257, 24799, 307);    pipeline.run();}
public void beam_f22178_0()
{    PTransform<PCollection<String>, PCollection<String>> transform = new PTransform<PCollection<String>, PCollection<String>>() {        @Override        public PCollection<String> expand(PCollection<String> begin) {            throw new IllegalArgumentException("Should never be applied");        }    };    DisplayData displayData = DisplayData.from(transform);    assertThat(displayData.items(), empty());}
public PCollection<String> beam_f22179_0(PCollection<String> begin)
{    throw new IllegalArgumentException("Should never be applied");}
public void beam_f22191_0(DoFn<String, String>.ProcessContext c)
{    super.process(c);}
public void beam_f22192_0() throws Exception
{    IdentityChildWithoutOverride fn = mock(IdentityChildWithoutOverride.class);    assertEquals(stop(), invokeProcessElement(fn));    verify(fn).process(mockProcessContext);}
public ProcessContinuation beam_f22210_0(ProcessContext c, RestrictionTracker<SomeRestriction, Void> tracker) throws Exception
{    return null;}
public SomeRestriction beam_f22211_0(String element)
{    return null;}
public void beam_f22227_0() throws Exception
{    MockFn fn = mock(MockFn.class);    DoFnInvoker<String, String> invoker = DoFnInvokers.invokerFor(fn);    final SomeRestrictionTracker tracker = mock(SomeRestrictionTracker.class);    final SomeRestrictionCoder coder = mock(SomeRestrictionCoder.class);    SomeRestriction restriction = new SomeRestriction();    final SomeRestriction part1 = new SomeRestriction();    final SomeRestriction part2 = new SomeRestriction();    final SomeRestriction part3 = new SomeRestriction();    when(fn.getRestrictionCoder()).thenReturn(coder);    when(fn.getInitialRestriction("blah")).thenReturn(restriction);    doAnswer(AdditionalAnswers.delegatesTo(new MockFn() {        @DoFn.SplitRestriction        @Override        public void splitRestriction(String element, SomeRestriction restriction, DoFn.OutputReceiver<SomeRestriction> receiver) {            receiver.output(part1);            receiver.output(part2);            receiver.output(part3);        }    })).when(fn).splitRestriction(eq("blah"), same(restriction), Mockito.any());    when(fn.newTracker(restriction)).thenReturn(tracker);    when(fn.processElement(mockProcessContext, tracker)).thenReturn(resume());    assertEquals(coder, invoker.invokeGetRestrictionCoder(CoderRegistry.createDefault()));    assertEquals(restriction, invoker.invokeGetInitialRestriction("blah"));    final List<SomeRestriction> outputs = new ArrayList<>();    invoker.invokeSplitRestriction("blah", restriction, new OutputReceiver<SomeRestriction>() {        @Override        public void output(SomeRestriction output) {            outputs.add(output);        }        @Override        public void outputWithTimestamp(SomeRestriction output, Instant timestamp) {            outputs.add(output);        }    });    assertEquals(Arrays.asList(part1, part2, part3), outputs);    assertEquals(tracker, invoker.invokeNewTracker(restriction));    assertEquals(resume(), invoker.invokeProcessElement(new FakeArgumentProvider<String, String>() {        @Override        public DoFn<String, String>.ProcessContext processContext(DoFn<String, String> fn) {            return mockProcessContext;        }        @Override        public RestrictionTracker<?, ?> restrictionTracker() {            return tracker;        }    }));}
public void beam_f22228_0(String element, SomeRestriction restriction, DoFn.OutputReceiver<SomeRestriction> receiver)
{    receiver.output(part1);    receiver.output(part2);    receiver.output(part3);}
public static CoderForDefaultTracker beam_f22238_0()
{    return new CoderForDefaultTracker();}
public RestrictionWithDefaultTracker beam_f22240_0(InputStream inStream)
{    return null;}
public void beam_f22252_0() throws Exception
{    DoFn<String, String> fn = mock(new DoFnInvokersTestHelper().newInnerPrivateDoFnWithTimers().getClass());    invokeOnTimer(TIMER_ID, fn);    DoFnInvokersTestHelper.verifyInnerPrivateDoFnWithTimers(fn, mockWindow);}
public void beam_f22253_0() throws Exception
{    DoFn<String, String> fn = mock(new DoFnInvokersTestHelper().newInnerAnonymousDoFnWithTimers().getClass());    invokeOnTimer(TIMER_ID, fn);    DoFnInvokersTestHelper.verifyInnerAnonymousDoFnWithTimers(fn, mockWindow);}
public void beam_f22263_0() throws Exception
{    DoFnInvoker<Integer, Integer> invoker = DoFnInvokers.invokerFor(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(@SuppressWarnings("unused") ProcessContext c) {            throw new IllegalArgumentException("bogus");        }    });    thrown.expect(UserCodeException.class);    thrown.expectMessage("bogus");    invoker.invokeProcessElement(new FakeArgumentProvider<Integer, Integer>() {        @Override        public DoFn<Integer, Integer>.ProcessContext processContext(DoFn<Integer, Integer> fn) {            return null;        }    });}
public void beam_f22264_0(@SuppressWarnings("unused") ProcessContext c)
{    throw new IllegalArgumentException("bogus");}
public void beam_f22273_0(@SuppressWarnings("unused") StartBundleContext c)
{    throw new IllegalArgumentException("bogus");}
public void beam_f22275_0() throws Exception
{    DoFnInvoker<Integer, Integer> invoker = DoFnInvokers.invokerFor(new DoFn<Integer, Integer>() {        @FinishBundle        public void finishBundle(@SuppressWarnings("unused") FinishBundleContext c) {            throw new IllegalArgumentException("bogus");        }        @ProcessElement        public void processElement(@SuppressWarnings("unused") ProcessContext c) {        }    });    thrown.expect(UserCodeException.class);    thrown.expectMessage("bogus");    invoker.invokeFinishBundle(null);}
private int beam_f22289_0(DoFn<Integer, String>.ProcessContext context)
{    return 0;}
public void beam_f22290_0() throws Exception
{    analyzeProcessElementMethod(new AnonymousMethod() {        private void method(DoFn<Integer, String>.ProcessContext c) {        }    });}
public void beam_f22307_0() throws Exception
{    DoFnSignatures.getSignature(new IdentityFn<String>() {    }.getClass());}
public void beam_f22308_0() throws Exception
{    DoFnSignature.ProcessElementMethod signature = analyzeProcessElementMethod(new AnonymousMethod() {        private DoFn.ProcessContinuation method(DoFn<Integer, String>.ProcessContext context) {            return null;        }    });    assertTrue(signature.hasReturnValue());}
public void beam_f22321_0() throws Exception
{    assertEquals(PCollection.IsBounded.BOUNDED, DoFnSignatures.getSignature(BaseFnWithoutContinuation.class).isBoundedPerElement());    assertEquals(PCollection.IsBounded.UNBOUNDED, DoFnSignatures.getSignature(BaseFnWithContinuation.class).isBoundedPerElement());}
public void beam_f22322_0() throws Exception
{    @BoundedPerElement    class BoundedFnWithContinuation extends BaseFnWithContinuation {    }    assertEquals(PCollection.IsBounded.BOUNDED, DoFnSignatures.getSignature(BoundedFnWithContinuation.class).isBoundedPerElement());    @UnboundedPerElement    class UnboundedFnWithContinuation extends BaseFnWithContinuation {    }    assertEquals(PCollection.IsBounded.UNBOUNDED, DoFnSignatures.getSignature(UnboundedFnWithContinuation.class).isBoundedPerElement());}
public void beam_f22335_0() throws Exception
{    class GoodGenericSplittableDoFn<RestrictionT, TrackerT, CoderT> extends DoFn<Integer, String> {        @ProcessElement        public ProcessContinuation processElement(ProcessContext context, TrackerT tracker) {            return null;        }        @GetInitialRestriction        public RestrictionT getInitialRestriction(Integer element) {            return null;        }        @SplitRestriction        public void splitRestriction(Integer element, RestrictionT restriction, OutputReceiver<RestrictionT> receiver) {        }        @NewTracker        public TrackerT newTracker(RestrictionT restriction) {            return null;        }        @GetRestrictionCoder        public CoderT getRestrictionCoder() {            return null;        }    }    DoFnSignature signature = DoFnSignatures.getSignature(new GoodGenericSplittableDoFn<SomeRestriction, RestrictionTracker<SomeRestriction, ?>, SomeRestrictionCoder>() {    }.getClass());    assertEquals(RestrictionTracker.class, signature.processElement().trackerT().getRawType());    assertTrue(signature.processElement().isSplittable());    assertTrue(signature.processElement().hasReturnValue());    assertEquals(SomeRestriction.class, signature.getInitialRestriction().restrictionT().getRawType());    assertEquals(SomeRestriction.class, signature.splitRestriction().restrictionT().getRawType());    assertEquals(RestrictionTracker.class, signature.newTracker().trackerT().getRawType());    assertEquals(SomeRestriction.class, signature.newTracker().restrictionT().getRawType());    assertEquals(SomeRestrictionCoder.class, signature.getRestrictionCoder().coderT().getRawType());}
public ProcessContinuation beam_f22336_0(ProcessContext context, TrackerT tracker)
{    return null;}
public void beam_f22349_0() throws Exception
{    class BadFn extends DoFn<Integer, String> {        @ProcessElement        public void process(ProcessContext context, RestrictionTracker<SomeRestriction, Void> tracker) {        }        @NewTracker        public void newTracker(SomeRestriction restriction) {        }        @GetInitialRestriction        public SomeRestriction getInitialRestriction(Integer element) {            return null;        }    }    thrown.expectMessage("Returns void, but must return a subtype of RestrictionTracker<SomeRestriction, ?>");    DoFnSignatures.getSignature(BadFn.class);}
public SomeRestriction beam_f22352_0(Integer element)
{    return null;}
public void beam_f22364_0() throws Exception
{    class BadFn {        private List<SomeRestriction> splitRestriction(String element, SomeRestriction restriction) {            return null;        }    }    thrown.expectMessage("First argument must be the element type Integer");    DoFnSignatures.analyzeSplitRestrictionMethod(errors(), TypeDescriptor.of(FakeDoFn.class), new AnonymousMethod() {        void method(String element, SomeRestriction restriction, DoFn.OutputReceiver<SomeRestriction> receiver) {        }    }.getMethod(), TypeDescriptor.of(Integer.class));}
private List<SomeRestriction> beam_f22365_0(String element, SomeRestriction restriction)
{    return null;}
public void beam_f22380_0() throws Exception
{    thrown.expectMessage("Must have a single argument");    DoFnSignatures.analyzeNewTrackerMethod(errors(), TypeDescriptor.of(FakeDoFn.class), new AnonymousMethod() {        private SomeRestrictionTracker method(SomeRestriction restriction, Object extra) {            return null;        }    }.getMethod());}
private SomeRestrictionTracker beam_f22381_0(SomeRestriction restriction, Object extra)
{    return null;}
public void beam_f22396_0()
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<String, String>() {        @ProcessElement        public void process(@Element Row row) {        }    }.getClass());    assertFalse(sig.processElement().getSchemaElementParameters().isEmpty());}
public void beam_f22398_0()
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<String, String>() {        @ProcessElement        public void process(@Element Row row1, @Timestamp Instant ts, @Element Row row2, OutputReceiver<String> o, @Element Integer intParameter) {        }    }.getClass());    assertEquals(3, sig.processElement().getSchemaElementParameters().size());    assertEquals(0, sig.processElement().getSchemaElementParameters().get(0).index());    assertEquals(TypeDescriptors.rows(), sig.processElement().getSchemaElementParameters().get(0).elementT());    assertEquals(1, sig.processElement().getSchemaElementParameters().get(1).index());    assertEquals(TypeDescriptors.rows(), sig.processElement().getSchemaElementParameters().get(1).elementT());    assertEquals(2, sig.processElement().getSchemaElementParameters().get(2).index());    assertEquals(TypeDescriptors.integers(), sig.processElement().getSchemaElementParameters().get(2).elementT());}
public void beam_f22422_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("TimerId");    thrown.expectMessage("TimerSpec");    thrown.expectMessage("bizzle");    thrown.expectMessage(not(mentionsState()));    DoFnSignatures.getSignature(new DoFn<String, String>() {        @TimerId("foo")        private final String bizzle = "bazzle";        @ProcessElement        public void foo(ProcessContext context) {        }    }.getClass());}
public void beam_f22424_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("No callback registered");    thrown.expectMessage("my-id");    thrown.expectMessage(not(mentionsState()));    thrown.expectMessage(mentionsTimers());    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @TimerId("my-id")        private final TimerSpec myfield1 = TimerSpecs.timer(TimeDomain.EVENT_TIME);        @ProcessElement        public void foo(ProcessContext context) {        }    }.getClass());}
public void beam_f22447_0() throws Exception
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFnOverridingAbstractTimerUse().getClass());    assertThat(sig.timerDeclarations().size(), equalTo(1));    assertThat(sig.processElement().extraParameters().size(), equalTo(2));    DoFnSignature.TimerDeclaration decl = sig.timerDeclarations().get(DoFnOverridingAbstractTimerUse.TIMER_ID);    TimerParameter timerParam = (TimerParameter) sig.processElement().extraParameters().get(1);    assertThat(decl.field(), equalTo(DoFnDeclaringTimerAndAbstractUse.class.getDeclaredField("myTimerSpec")));            assertThat(timerParam.referent(), equalTo(decl));}
public void beam_f22448_0() throws Exception
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFnOverridingAbstractCallback().getClass());    assertThat(sig.timerDeclarations().size(), equalTo(1));    assertThat(sig.onTimerMethods().size(), equalTo(1));    DoFnSignature.TimerDeclaration decl = sig.timerDeclarations().get(DoFnDeclaringTimerAndAbstractCallback.TIMER_ID);    DoFnSignature.OnTimerMethod callback = sig.onTimerMethods().get(DoFnDeclaringTimerAndAbstractCallback.TIMER_ID);    assertThat(decl.field(), equalTo(DoFnDeclaringTimerAndAbstractCallback.class.getDeclaredField("myTimerSpec")));            assertThat(callback.targetMethod(), equalTo(DoFnDeclaringTimerAndAbstractCallback.class.getDeclaredMethod("onMyTimer")));}
public void beam_f22469_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("State declarations must be final");    thrown.expectMessage("Non-final field");    thrown.expectMessage("myfield");    thrown.expectMessage(not(mentionsTimers()));    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @StateId("my-id")        private StateSpec<ValueState<Integer>> myfield = StateSpecs.value(VarIntCoder.of());        @ProcessElement        public void foo(ProcessContext context) {        }    }.getClass());}
public void beam_f22471_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("missing StateId annotation");    thrown.expectMessage("myProcessElement");    thrown.expectMessage("index 1");    thrown.expectMessage(not(mentionsTimers()));    DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @ProcessElement        public void myProcessElement(ProcessContext context, ValueState<Integer> noAnnotation) {        }    }.getClass());}
public void beam_f22488_0() throws Exception
{    class DoFnOverridingAbstractStateUse extends DoFnDeclaringStateAndAbstractUse {        @Override        public void processWithState(ProcessContext c, ValueState<String> state) {        }    }    DoFnSignature sig = DoFnSignatures.getSignature(new DoFnOverridingAbstractStateUse().getClass());    assertThat(sig.stateDeclarations().size(), equalTo(1));    assertThat(sig.processElement().extraParameters().size(), equalTo(2));    DoFnSignature.StateDeclaration decl = sig.stateDeclarations().get(DoFnOverridingAbstractStateUse.STATE_ID);    StateParameter stateParam = (StateParameter) sig.processElement().extraParameters().get(1);    assertThat(decl.field(), equalTo(DoFnDeclaringStateAndAbstractUse.class.getDeclaredField("myStateSpec")));            assertThat(stateParam.referent(), equalTo(decl));}
public void beam_f22490_0() throws Exception
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<KV<String, Integer>, Long>() {        @StateId("foo")        private final StateSpec<ValueState<Integer>> bizzleDecl = StateSpecs.value(VarIntCoder.of());        @ProcessElement        public void foo(ProcessContext context, @StateId("foo") ValueState<Integer> bizzle) {        }    }.getClass());    assertThat(sig.processElement().extraParameters().size(), equalTo(2));    final DoFnSignature.StateDeclaration decl = sig.stateDeclarations().get("foo");    sig.processElement().extraParameters().get(1).match(new Parameter.Cases.WithDefault<Void>() {        @Override        protected Void dispatchDefault(Parameter p) {            fail(String.format("Expected a state parameter but got %s", p));            return null;        }        @Override        public Void dispatch(StateParameter stateParam) {            assertThat(stateParam.referent(), equalTo(decl));            return null;        }    });}
public void beam_f22512_0()
{    DoFnSignature sig = DoFnSignatures.getSignature(new DoFn<String, String>() {        @StateId("foo")        private final StateSpec<ValueState<Integer>> bizzle = StateSpecs.value(VarIntCoder.of());        @ProcessElement        public void process(ProcessContext c) {        }        @OnWindowExpiration        public void bar(BoundedWindow b, @StateId("foo") ValueState<Integer> s, PipelineOptions p, OutputReceiver<String> o, MultiOutputReceiver m) {        }    }.getClass());    List<Parameter> params = sig.onWindowExpiration().extraParameters();    assertThat(params.size(), equalTo(5));    assertThat(params.get(0), instanceOf(WindowParameter.class));    assertThat(params.get(1), instanceOf(StateParameter.class));    assertThat(params.get(2), instanceOf(PipelineOptionsParameter.class));    assertThat(params.get(3), instanceOf(OutputReceiverParameter.class));    assertThat(params.get(4), instanceOf(TaggedOutputReceiverParameter.class));}
private Matcher<String> beam_f22515_0()
{    return anyOf(containsString("timer"), containsString("Timer"));}
public void beam_f22535_0() throws Exception
{    WindowedTimerDoFn fn = new WindowedTimerDoFn();    invokeOnTimer(fn, WindowedTimerDoFn.TIMER_ID);    assertThat(fn.window, theInstance(mockWindow));}
public void beam_f22537_0(BoundedWindow window)
{    this.window = window;}
public static void beam_f22552_0(DoFn<String, String> fn, DoFn<String, String>.ProcessContext context)
{    verify((InnerPrivateDoFn) fn).process(context);}
public DoFn<String, String> beam_f22553_0()
{    return new DoFn<String, String>() {        @ProcessElement        public void process(ProcessContext c) {        }    };}
public static void beam_f22571_0(DoFn<String, String> fn, BoundedWindow window)
{    verify((InnerPackagePrivateDoFnWithTimers) fn).onTimer(window);}
public static DoFn<String, String> beam_f22572_0()
{    return new StaticPrivateDoFnWithTimers();}
public static void beam_f22584_0(DoFn<String, String> fn, BoundedWindow window) throws Exception
{    fn.getClass().getMethod("verify", BoundedWindow.class).invoke(fn, window);}
public void beam_f22585_0()
{    PCollection<String> output = p.apply(Create.of("aj", "xj", "yj", "zj")).apply(Regex.find("[xyz]"));    PAssert.that(output).containsInAnyOrder("x", "y", "z");    p.run();}
public void beam_f22594_0()
{    PCollection<KV<String, String>> output = p.apply(Create.of("x y z")).apply(Regex.findKV("a (?<keyname>b) (?<valuename>c)", "keyname", "valuename"));    PAssert.that(output).empty();    p.run();}
public void beam_f22595_0()
{    PCollection<String> output = p.apply(Create.of("a", "x", "y", "z")).apply(Regex.matches("[xyz]"));    PAssert.that(output).containsInAnyOrder("x", "y", "z");    p.run();}
public void beam_f22604_0()
{    PCollection<KV<String, String>> output = p.apply(Create.of("x y z")).apply(Regex.findKV("a (?<keyname>b) (?<valuename>c)", "keyname", "valuename"));    PAssert.that(output).empty();    p.run();}
public void beam_f22605_0()
{    PCollection<String> output = p.apply(Create.of("xj", "yj", "zj")).apply(Regex.replaceAll("[xyz]", "new"));    PAssert.that(output).containsInAnyOrder("newj", "newj", "newj");    p.run();}
public void beam_f22614_0()
{    PCollection<KV<String, TimestampedValue<Integer>>> preified = pipeline.apply(Create.timestamped(TimestampedValue.of(KV.of("foo", TimestampedValue.of(0, new Instant(0))), new Instant(100)), TimestampedValue.of(KV.of("foo", TimestampedValue.of(1, new Instant(1))), new Instant(101L)), TimestampedValue.of(KV.of("bar", TimestampedValue.of(2, new Instant(2))), new Instant(102L)), TimestampedValue.of(KV.of("baz", TimestampedValue.of(3, new Instant(3))), new Instant(103L))));    PCollection<KV<String, Integer>> timestamped = preified.apply(ReifyTimestamps.extractFromValues());    PAssert.that(timestamped).containsInAnyOrder(KV.of("foo", 0), KV.of("foo", 1), KV.of("bar", 2), KV.of("baz", 3));    timestamped.apply("AssertElementTimestamps", ParDo.of(new DoFn<KV<String, Integer>, Void>() {        @ProcessElement        public void verifyTimestampsEqualValue(ProcessContext context) {            assertThat(new Instant(context.element().getValue().longValue()), equalTo(context.timestamp()));        }    }));    pipeline.run();}
public void beam_f22615_0(ProcessContext context)
{    assertThat(new Instant(context.element().getValue().longValue()), equalTo(context.timestamp()));}
public void beam_f22624_0(ProcessContext context)
{    assertThat(new Instant(context.element().getValue().longValue()), equalTo(context.timestamp()));}
public Void beam_f22625_0(Iterable<KV<String, Iterable<Integer>>> actual)
{    assertThat(actual, containsInAnyOrder(isKv(is("k1"), containsInAnyOrder(3)), isKv(is("k2"), containsInAnyOrder(4))));    return null;}
public static Iterable<Object[]> beam_f22634_0() throws IOException
{    return ImmutableList.<Object[]>builder().add(new Object[] { TestUtils.NO_LINES, 0 }, new Object[] { TestUtils.NO_LINES, 1 }, new Object[] { TestUtils.LINES, 1 }, new Object[] { TestUtils.LINES, TestUtils.LINES.size() / 2 }, new Object[] { TestUtils.LINES, TestUtils.LINES.size() * 2 }, new Object[] { TestUtils.LINES, TestUtils.LINES.size() - 1 }, new Object[] { TestUtils.LINES, TestUtils.LINES.size() }, new Object[] { TestUtils.LINES, TestUtils.LINES.size() + 1 }).build();}
public Void beam_f22635_0(Iterable<String> actualIter)
{    final int expectedSize = Math.min(limit, lines.size());            List<String> actual = new ArrayList<>();    for (String s : actualIter) {        actual.add(s);    }    assertEquals(expectedSize, actual.size());    Set<String> actualAsSet = new TreeSet<>(actual);    Set<String> linesAsSet = new TreeSet<>(lines);    assertEquals(actual.size(), actualAsSet.size());    assertEquals(lines.size(), linesAsSet.size());    assertTrue(linesAsSet.containsAll(actualAsSet));    return null;}
public void beam_f22644_0()
{    PCollection<Integer> input = pipeline.apply(Create.empty(BigEndianIntegerCoder.of()));    PCollection<Integer> output = input.apply(Window.into(FixedWindows.of(Duration.standardSeconds(3)))).apply(Sample.any(10));    PAssert.that(output).inWindow(new IntervalWindow(new Instant(0), Duration.standardSeconds(3))).satisfies(new VerifyCorrectSample<>(0, EMPTY));    pipeline.run();}
public void beam_f22645_0()
{    pipeline.enableAbandonedNodeEnforcement(false);    pipeline.apply(Create.empty(BigEndianIntegerCoder.of())).apply(Sample.any(-10));}
public void beam_f22654_0()
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("must override");    thrown.expectMessage("apply");    new SimpleFunction<Integer, Integer>() {    };}
public void beam_f22655_0() throws Exception
{    SimpleFunction<Integer, String> fn = new SimpleFunction<Integer, String>(Object::toString) {    };    assertThat(fn.getInputTypeDescriptor(), equalTo(TypeDescriptors.integers()));    assertThat(fn.getOutputTypeDescriptor(), equalTo(TypeDescriptors.strings()));}
public void beam_f22664_0() throws Exception
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.ALL_KEYS);    ByteKeyRange checkpoint = tracker.checkpoint();            assertEquals(ByteKeyRange.ALL_KEYS, checkpoint);    assertEquals(ByteKeyRangeTracker.NO_KEYS, tracker.currentRestriction());}
public void beam_f22665_0() throws Exception
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)));    assertFalse(tracker.tryClaim(ByteKey.of(0xd0)));    ByteKeyRange checkpoint = tracker.checkpoint();    assertEquals(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)), tracker.currentRestriction());    assertEquals(ByteKeyRangeTracker.NO_KEYS, checkpoint);}
public void beam_f22674_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)));    assertTrue(tracker.tryClaim(ByteKey.of(0x50)));    assertTrue(tracker.tryClaim(ByteKey.of(0x90)));    assertFalse(tracker.tryClaim(ByteKey.of(0xd0)));    tracker.checkDone();}
public void beam_f22675_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)));    assertTrue(tracker.tryClaim(ByteKey.of(0x50)));    assertTrue(tracker.tryClaim(ByteKey.of(0x90)));    assertFalse(tracker.tryClaim(ByteKey.of(0xc0)));    tracker.checkDone();}
public void beam_f22684_0()
{    ByteKeyRangeTracker tracker = ByteKeyRangeTracker.of(ByteKeyRange.ALL_KEYS);    tracker.tryClaim(ByteKey.of(0xa0));    assertThat(tracker.getSize(), allOf(greaterThan(0.), lessThan(1.)));    tracker = ByteKeyRangeTracker.of(ByteKeyRange.of(ByteKey.of(0x10), ByteKey.of(0xc0)));    tracker.tryClaim(ByteKey.of(0xa0));    assertThat(tracker.getSize(), allOf(greaterThan(0.), lessThan(1.)));}
public void beam_f22685_0() throws Exception
{    OffsetRange range = new OffsetRange(100, 200);    OffsetRangeTracker tracker = new OffsetRangeTracker(range);    assertEquals(range, tracker.currentRestriction());    assertTrue(tracker.tryClaim(100L));    assertTrue(tracker.tryClaim(150L));    assertTrue(tracker.tryClaim(199L));    assertFalse(tracker.tryClaim(200L));}
public void beam_f22694_0()
{    OffsetRangeTracker tracker = new OffsetRangeTracker(new OffsetRange(100, 200));    assertTrue(tracker.tryClaim(150L));    assertTrue(tracker.tryClaim(175L));    assertFalse(tracker.tryClaim(220L));    tracker.checkDone();}
public void beam_f22695_0()
{    OffsetRangeTracker tracker = new OffsetRangeTracker(new OffsetRange(100, 200));    assertTrue(tracker.tryClaim(150L));    assertTrue(tracker.tryClaim(175L));    assertFalse(tracker.tryClaim(200L));    tracker.checkDone();}
private static PairStringWithIndexToLengthBase beam_f22704_0(IsBounded bounded)
{    return (bounded == IsBounded.BOUNDED) ? new PairStringWithIndexToLengthBounded() : new PairStringWithIndexToLengthUnbounded();}
public void beam_f22705_0()
{    testPairWithIndexBasic(IsBounded.BOUNDED);}
private static SDFWithMultipleOutputsPerBlockBase beam_f22714_0(IsBounded bounded, int numClaimsPerCall)
{    return (bounded == IsBounded.BOUNDED) ? new SDFWithMultipleOutputsPerBlockBounded(numClaimsPerCall) : new SDFWithMultipleOutputsPerBlockUnbounded(numClaimsPerCall);}
public void beam_f22715_0()
{    testOutputAfterCheckpoint(IsBounded.BOUNDED);}
public void beam_f22724_0()
{    testWindowedSideInput(IsBounded.BOUNDED);}
public void beam_f22725_0()
{    testWindowedSideInput(IsBounded.UNBOUNDED);}
public void beam_f22734_0(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker)
{    checkState(tracker.tryClaim(tracker.currentRestriction().getFrom()));    c.output("main:" + c.element());    c.output(additionalOutput, "additional:" + c.element());}
public OffsetRange beam_f22735_0(Integer value)
{    return new OffsetRange(0, 1);}
public void beam_f22744_0()
{    assertEquals(State.OUTSIDE_BUNDLE, state);    state = State.INSIDE_BUNDLE;}
public void beam_f22745_0(ProcessContext c, RestrictionTracker<OffsetRange, Long> tracker)
{    assertEquals(State.INSIDE_BUNDLE, state);    assertTrue(tracker.tryClaim(0L));    c.output(c.element());}
public ProcessContinuation beam_f22755_0(@Element String element, RestrictionTracker<OffsetRange, Long> tracker)
{    return stop();}
public OffsetRange beam_f22756_0(String element)
{    return new OffsetRange(0, 1);}
public void beam_f22765_0()
{    PCollection<String> input = p.apply(Create.of(Arrays.asList(EMPTY_COLLECTION)).withCoder(StringUtf8Coder.of()));    PCollection<List<String>> top1 = input.apply(Top.of(1, new OrderByLength()));    PCollection<List<String>> top2 = input.apply(Top.largest(2));    PCollection<List<String>> top3 = input.apply(Top.smallest(3));    PCollection<KV<String, Integer>> inputTable = createEmptyInputTable(p);    PCollection<KV<String, List<Integer>>> largestPerKey = inputTable.apply(Top.largestPerKey(2));    PCollection<KV<String, List<Integer>>> smallestPerKey = inputTable.apply(Top.smallestPerKey(2));    PAssert.thatSingletonIterable(top1).empty();    PAssert.thatSingletonIterable(top2).empty();    PAssert.thatSingletonIterable(top3).empty();    PAssert.that(largestPerKey).empty();    PAssert.that(smallestPerKey).empty();    p.run();}
public void beam_f22766_0()
{    p.enableAbandonedNodeEnforcement(false);    Window<String> windowingFn = Window.into(FixedWindows.of(Duration.standardDays(10L)));    PCollection<String> input = p.apply(Create.empty(StringUtf8Coder.of())).apply(windowingFn);    expectedEx.expect(IllegalStateException.class);    expectedEx.expectMessage("Top");    expectedEx.expectMessage("GlobalWindows");    expectedEx.expectMessage("withoutDefaults");    expectedEx.expectMessage("asSingletonView");    input.apply(Top.of(1, new OrderByLength()));}
public void beam_f22775_0()
{    Integer[] ints = { 1, 2, 3, 4, 5 };    String[] strings = { "1", "2", "3", "4", "5" };    PCollection<Integer> input = p.apply(Create.of(Arrays.asList(ints)));    PCollection<String> output = input.apply(ToString.elements());    PAssert.that(output).containsInAnyOrder(strings);    p.run();}
public void beam_f22776_0()
{    ArrayList<KV<String, Integer>> kvs = new ArrayList<>();    kvs.add(KV.of("one", 1));    kvs.add(KV.of("two", 2));    ArrayList<String> expected = new ArrayList<>();    expected.add("one,1");    expected.add("two,2");    PCollection<KV<String, Integer>> input = p.apply(Create.of(kvs));    PCollection<String> output = input.apply(ToString.kvs());    PAssert.that(output).containsInAnyOrder(expected);    p.run();}
public void beam_f22785_0()
{    final PCollectionView<Integer> view = pipeline.apply("Create47", Create.timestamped(TimestampedValue.of(47, new Instant(1)), TimestampedValue.of(48, new Instant(11)))).apply("SideWindowInto", Window.into(FixedWindows.of(Duration.millis(10)))).apply(View.asSingleton());    PCollection<Integer> output = pipeline.apply("Create123", Create.timestamped(TimestampedValue.of(1, new Instant(4)), TimestampedValue.of(2, new Instant(8)), TimestampedValue.of(3, new Instant(12)))).apply("MainWindowInto", Window.into(FixedWindows.of(Duration.millis(10)))).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.sideInput(view));        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(47, 47, 48);    pipeline.run();}
public void beam_f22786_0(ProcessContext c)
{    c.output(c.sideInput(view));}
public void beam_f22795_0() throws Exception
{    final PCollectionView<List<Integer>> view = pipeline.apply("CreateEmptyView", Create.empty(VarIntCoder.of())).apply(View.asList());    PCollection<Integer> results = pipeline.apply("Create1", Create.of(1)).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            assertTrue(c.sideInput(view).isEmpty());            assertFalse(c.sideInput(view).iterator().hasNext());            c.output(1);        }    }).withSideInputs(view));        PAssert.that(results).containsInAnyOrder(1);    pipeline.run();}
public void beam_f22796_0(ProcessContext c)
{    assertTrue(c.sideInput(view).isEmpty());    assertFalse(c.sideInput(view).iterator().hasNext());    c.output(1);}
public void beam_f22805_0()
{    final PCollectionView<Iterable<Integer>> view = pipeline.apply("CreateSideInput", Create.of(11)).apply(View.asIterable());    PCollection<Integer> output = pipeline.apply("CreateMainInput", Create.of(29)).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            Iterator<Integer> iterator = c.sideInput(view).iterator();            while (iterator.hasNext()) {                try {                    iterator.remove();                    fail("Expected UnsupportedOperationException on remove()");                } catch (UnsupportedOperationException expected) {                }                c.output(iterator.next());            }        }    }).withSideInputs(view));        PAssert.that(output).containsInAnyOrder(11);    pipeline.run();}
public void beam_f22806_0(ProcessContext c)
{    Iterator<Integer> iterator = c.sideInput(view).iterator();    while (iterator.hasNext()) {        try {            iterator.remove();            fail("Expected UnsupportedOperationException on remove()");        } catch (UnsupportedOperationException expected) {        }        c.output(iterator.next());    }}
public void beam_f22815_0() throws org.apache.beam.sdk.coders.Coder.NonDeterministicException
{    throw new NonDeterministicException(this, "Test coder is not deterministic on purpose.");}
public void beam_f22816_0()
{    final PCollectionView<Map<String, Iterable<Integer>>> view = pipeline.apply("CreateSideInput", Create.of(KV.of("a", 1), KV.of("a", 1), KV.of("a", 2), KV.of("b", 3)).withCoder(KvCoder.of(new NonDeterministicStringCoder(), VarIntCoder.of()))).apply(View.asMultimap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.of("apple", "banana", "blackberry")).apply("OutputSideInputs", ParDo.of(new DoFn<String, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            for (Integer v : c.sideInput(view).get(c.element().substring(0, 1))) {                c.output(KV.of(c.element(), v));            }        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(KV.of("apple", 1), KV.of("apple", 1), KV.of("apple", 2), KV.of("banana", 3), KV.of("blackberry", 3));    pipeline.run();}
public void beam_f22825_0(ProcessContext c)
{    assertTrue(c.sideInput(view).isEmpty());    assertTrue(c.sideInput(view).entrySet().isEmpty());    assertFalse(c.sideInput(view).entrySet().iterator().hasNext());    c.output(c.element());}
public void beam_f22826_0() throws Exception
{    final PCollectionView<Map<String, Iterable<Integer>>> view = pipeline.apply("CreateEmptyView", Create.empty(KvCoder.of(new NonDeterministicStringCoder(), VarIntCoder.of()))).apply(View.asMultimap());    PCollection<Integer> results = pipeline.apply("Create1", Create.of(1)).apply("OutputSideInputs", ParDo.of(new DoFn<Integer, Integer>() {        @ProcessElement        public void processElement(ProcessContext c) {            assertTrue(c.sideInput(view).isEmpty());            assertTrue(c.sideInput(view).entrySet().isEmpty());            assertFalse(c.sideInput(view).entrySet().iterator().hasNext());            c.output(c.element());        }    }).withSideInputs(view));        PAssert.that(results).containsInAnyOrder(1);    pipeline.run();}
public void beam_f22835_0(ProcessContext c)
{    c.output(KV.of(c.element(), c.sideInput(view).get(c.element().substring(0, 1))));}
public void beam_f22836_0()
{    final PCollectionView<Map<String, Integer>> view = pipeline.apply("CreateSideInput", Create.timestamped(TimestampedValue.of(KV.of("a", 1), new Instant(1)), TimestampedValue.of(KV.of("b", 2), new Instant(4)), TimestampedValue.of(KV.of("b", 3), new Instant(18)))).apply("SideWindowInto", Window.into(FixedWindows.of(Duration.millis(10)))).apply(View.asMap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.timestamped(TimestampedValue.of("apple", new Instant(5)), TimestampedValue.of("banana", new Instant(4)), TimestampedValue.of("blackberry", new Instant(16)))).apply("MainWindowInto", Window.into(FixedWindows.of(Duration.millis(10)))).apply("OutputSideInputs", ParDo.of(new DoFn<String, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(KV.of(c.element(), c.sideInput(view).get(c.element().substring(0, 1))));        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(KV.of("apple", 1), KV.of("banana", 2), KV.of("blackberry", 3));    pipeline.run();}
public void beam_f22845_0(ProcessContext c)
{    assertTrue(c.sideInput(view).isEmpty());    assertTrue(c.sideInput(view).entrySet().isEmpty());    assertFalse(c.sideInput(view).entrySet().iterator().hasNext());    c.output(c.element());}
public void beam_f22846_0()
{    final PCollectionView<Map<String, Integer>> view = pipeline.apply("CreateSideInput", Create.of(KV.of("a", (Integer) null), KV.of("a", (Integer) null)).withCoder(KvCoder.of(StringUtf8Coder.of(), NullableCoder.of(VarIntCoder.of())))).apply(View.asMap());    PCollection<KV<String, Integer>> output = pipeline.apply("CreateMainInput", Create.of("apple", "banana", "blackberry")).apply("OutputSideInputs", ParDo.of(new DoFn<String, KV<String, Integer>>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(KV.of(c.element(), c.sideInput(view).get(c.element().substring(0, 1))));        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder(KV.of("apple", 1), KV.of("banana", 3), KV.of("blackberry", 3));            thrown.expectCause(ThrowableMessageMatcher.hasMessage(Matchers.containsString("Duplicate values for a")));    pipeline.run();}
public void beam_f22855_0(ProcessContext c)
{    c.output(c.element() + c.sideInput(view));}
public void beam_f22856_0()
{    final PCollectionView<Integer> view = pipeline.apply("CreateSideInput", Create.timestamped(TimestampedValue.of(2, new Instant(11)), TimestampedValue.of(3, new Instant(13)))).apply("WindowSideInput", Window.into(FixedWindows.of(Duration.millis(10)))).apply(Sum.integersGlobally().asSingletonView());    PCollection<String> output = pipeline.apply("CreateMainInput", Create.timestamped(TimestampedValue.of("A", new Instant(4)), TimestampedValue.of("B", new Instant(15)), TimestampedValue.of("C", new Instant(7)))).apply("WindowMainInput", Window.into(FixedWindows.of(Duration.millis(10)))).apply("OutputMainAndSideInputs", ParDo.of(new DoFn<String, String>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(c.element() + c.sideInput(view));        }    }).withSideInputs(view));    PAssert.that(output).containsInAnyOrder("A0", "B5", "C0");    pipeline.run();}
private void beam_f22865_0(Pipeline pipeline, PTransform<PCollection<KV<String, Integer>>, ? extends PCollectionView<?>> view)
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("Unable to create a side-input view from input");    thrown.expectCause(ThrowableMessageMatcher.hasMessage(Matchers.containsString("non-bounded PCollection")));    pipeline.apply(new PTransform<PBegin, PCollection<KV<String, Integer>>>() {        @Override        public PCollection<KV<String, Integer>> expand(PBegin input) {            return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));        }    }).apply(view);}
public PCollection<KV<String, Integer>> beam_f22866_0(PBegin input)
{    return PCollection.createPrimitiveOutputInternal(input.getPipeline(), WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED, KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));}
public void beam_f22875_0()
{    testViewNonmerging(pipeline, View.asList());}
public void beam_f22876_0()
{    testViewNonmerging(pipeline, View.asMap());}
public void beam_f22885_0()
{    testWaitWithParameters(Duration.standardMinutes(1), /* duration */    Duration.ZERO, /* lateness */    20, /* numMainElements */    FixedWindows.of(Duration.standardSeconds(1)), 10, /* numSignalElements */    FixedWindows.of(Duration.standardSeconds(1)));}
private void beam_f22886_0(Duration duration, Duration lateness, int numMainElements, @Nullable WindowFn<? super Long, ?> mainWindowFn, int numSignalElements, @Nullable WindowFn<? super Long, ?> signalWindowFn)
{    TEST_WAIT_MAX_MAIN_TIMESTAMP.set(null);    Instant base = Instant.now();    PCollection<Long> input = generateStreamWithBoundedDisorder("main", base, duration, numMainElements, lateness);    if (mainWindowFn == null) {        input.setIsBoundedInternal(PCollection.IsBounded.BOUNDED);    } else {        input = input.apply("Window main", Window.<Long>into(mainWindowFn).discardingFiredPanes().triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))).withAllowedLateness(lateness));    }    input = input.apply("Fire main", new Fire<>());    PCollection<Long> signal = generateStreamWithBoundedDisorder("signal", base, duration, numSignalElements, lateness);    if (signalWindowFn == null) {        signal.setIsBoundedInternal(PCollection.IsBounded.BOUNDED);    } else {        signal = signal.apply("Window signal", Window.<Long>into(signalWindowFn).discardingFiredPanes().triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))).withAllowedLateness(lateness));    }    signal = signal.apply("Fire signal", new Fire<>()).apply("Check sequencing", ParDo.of(new DoFn<Long, Long>() {        @ProcessElement        public void process(ProcessContext c) {            Instant maxMainTimestamp = TEST_WAIT_MAX_MAIN_TIMESTAMP.get();            if (maxMainTimestamp != null) {                assertFalse("Signal at timestamp " + c.timestamp() + " generated after main timestamp progressed to " + maxMainTimestamp, c.timestamp().isBefore(maxMainTimestamp));            }            c.output(c.element());        }    }));    PCollection<Long> output = input.apply(Wait.on(signal));    output.apply("Update main timestamp", ParDo.of(new DoFn<Long, Long>() {        @ProcessElement        public void process(ProcessContext c, BoundedWindow w) {            while (true) {                Instant maxMainTimestamp = TEST_WAIT_MAX_MAIN_TIMESTAMP.get();                Instant newMaxTimestamp = (maxMainTimestamp == null || c.timestamp().isAfter(maxMainTimestamp)) ? c.timestamp() : maxMainTimestamp;                if (TEST_WAIT_MAX_MAIN_TIMESTAMP.compareAndSet(maxMainTimestamp, newMaxTimestamp)) {                    break;                }            }            c.output(c.element());        }    }));    List<Long> expectedOutput = Lists.newArrayList();    for (int i = 0; i < numMainElements; ++i) {        expectedOutput.add((long) i);    }    PAssert.that(output).containsInAnyOrder(expectedOutput);    p.run();}
public void beam_f22895_0()
{    testMultiplePolls(true);}
private void beam_f22896_0(boolean terminationConditionElapsesBeforeOutputIsFinal)
{    List<Integer> all = Arrays.asList(0, 1, 2, 3, 4, 5, 6, 7, 8, 9);    PCollection<Integer> res = p.apply(Create.of("a")).apply(Watch.growthOf(new TimedPollFn<String, Integer>(all, standardSeconds(1), /* timeToOutputEverything */    standardSeconds(3), /* timeToDeclareOutputFinal */    standardSeconds(30))).withTerminationPerInput(Growth.afterTotalOf(standardSeconds(    terminationConditionElapsesBeforeOutputIsFinal ? 2 : 100))).withPollInterval(Duration.millis(300)).withOutputCoder(VarIntCoder.of())).apply("Drop input", Values.create());    PAssert.that(res).containsInAnyOrder(all);    p.run();}
public PollResult<OutputT> beam_f22905_0(InputT element, Context c) throws Exception
{    Instant now = Instant.now();    Duration elapsed = new Duration(baseTime, Instant.now());    if (elapsed.isLongerThan(timeToFail)) {        fail(String.format("Poll called %s after base time, which is longer than the threshold of %s", elapsed, timeToFail));    }    double fractionElapsed = 1.0 * elapsed.getMillis() / timeToOutputEverything.getMillis();    int numToEmit = (int) Math.min(outputs.size(), fractionElapsed * outputs.size());    List<TimestampedValue<OutputT>> toEmit = Lists.newArrayList();    for (int i = 0; i < numToEmit; ++i) {        toEmit.add(TimestampedValue.of(outputs.get(i), now));    }    return elapsed.isLongerThan(timeToDeclareOutputFinal) ? PollResult.complete(toEmit) : PollResult.incomplete(toEmit).withWatermark(now);}
public void beam_f22906_0()
{    Watch.Growth.Never<Object> c = never();    Integer state = c.forNewInput(Instant.now(), null);    assertFalse(c.canStopPolling(Instant.now(), state));}
public void beam_f22915_0()
{    GrowthTracker<String, Integer> tracker = newPollingGrowthTracker();    PollingGrowthState<Integer> residual = (PollingGrowthState<Integer>) tracker.checkpoint();    GrowthState primary = tracker.currentRestriction();    tracker.checkDone();        assertEquals(GrowthTracker.EMPTY_STATE, primary);        assertNull(residual.getPollWatermark());    assertEquals(0, residual.getCompleted().size());    assertEquals(0, (int) residual.getTerminationState());}
public void beam_f22916_0()
{    Instant now = Instant.now();    GrowthTracker<String, Integer> tracker = newPollingGrowthTracker();    PollResult<String> claim = PollResult.incomplete(Arrays.asList(TimestampedValue.of("d", now.plus(standardSeconds(4))), TimestampedValue.of("c", now.plus(standardSeconds(3))), TimestampedValue.of("a", now.plus(standardSeconds(1))), TimestampedValue.of("b", now.plus(standardSeconds(2))))).withWatermark(now.plus(standardSeconds(7)));    assertTrue(tracker.tryClaim(KV.of(claim, 1)));    PollingGrowthState<Integer> residual = (PollingGrowthState<Integer>) tracker.checkpoint();    assertFalse(newTracker(residual).tryClaim(KV.of(claim, 2)));}
public void beam_f22925_0()
{    Trigger trigger = AfterEach.inOrder(StubTrigger.named("t1"), StubTrigger.named("t2"), StubTrigger.named("t3"));    assertEquals("AfterEach.inOrder(t1, t2, t3)", trigger.toString());}
public void beam_f22926_0() throws Exception
{    BoundedWindow window = new IntervalWindow(new Instant(0), new Instant(10));    assertEquals(new Instant(9), AfterFirst.of(AfterWatermark.pastEndOfWindow(), AfterPane.elementCountAtLeast(4)).getWatermarkThatGuaranteesFiring(window));    assertEquals(BoundedWindow.TIMESTAMP_MAX_VALUE, AfterFirst.of(AfterPane.elementCountAtLeast(2), AfterPane.elementCountAtLeast(1)).getWatermarkThatGuaranteesFiring(window));}
public void beam_f22935_0()
{    Trigger trigger = AfterProcessingTime.pastFirstElementInPane();    assertEquals("AfterProcessingTime.pastFirstElementInPane()", trigger.toString());}
public void beam_f22936_0()
{    Trigger trigger = AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardMinutes(5));    assertEquals("AfterProcessingTime.pastFirstElementInPane().plusDelayOf(5 minutes)", trigger.toString());}
private static Instant beam_f22945_0(int year, int month, int day, int hours, int minutes)
{    return new DateTime(year, month, day, hours, minutes, DateTimeZone.UTC).toInstant();}
public void beam_f22946_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    final List<Long> timestamps = Arrays.asList(makeTimestamp(2014, 1, 1, 0, 0).getMillis(), makeTimestamp(2014, 1, 1, 23, 59).getMillis(), makeTimestamp(2014, 1, 2, 0, 0).getMillis(), makeTimestamp(2014, 1, 2, 5, 5).getMillis(), makeTimestamp(2015, 1, 1, 0, 0).getMillis(), makeTimestamp(2015, 1, 1, 5, 5).getMillis());    expected.put(new IntervalWindow(makeTimestamp(2014, 1, 1, 0, 0), makeTimestamp(2014, 1, 2, 0, 0)), set(timestamps.get(0), timestamps.get(1)));    expected.put(new IntervalWindow(makeTimestamp(2014, 1, 2, 0, 0), makeTimestamp(2014, 1, 3, 0, 0)), set(timestamps.get(2), timestamps.get(3)));    expected.put(new IntervalWindow(makeTimestamp(2015, 1, 1, 0, 0), makeTimestamp(2015, 1, 2, 0, 0)), set(timestamps.get(4), timestamps.get(5)));    assertEquals(expected, runWindowFn(CalendarWindows.days(1), timestamps));}
public void beam_f22955_0()
{    MonthsWindows windowFn = CalendarWindows.months(2);    WindowMappingFn<?> mapping = windowFn.getDefaultWindowMappingFn();    assertThat(mapping.getSideInputWindow(new BoundedWindow() {        @Override        public Instant maxTimestamp() {            return new Instant(100L);        }    }), equalTo(windowFn.assignWindow(new Instant(100L))));    assertThat(mapping.maximumLookback(), equalTo(Duration.ZERO));}
public Instant beam_f22956_0()
{    return new Instant(100L);}
public Instant beam_f22965_0()
{    return new Instant(100L);}
public void beam_f22966_0()
{    Instant endOfGlobalWindow = GlobalWindow.INSTANCE.maxTimestamp();    FixedWindows windowFn = FixedWindows.of(Duration.standardHours(1));    IntervalWindow truncatedWindow = windowFn.assignWindow(endOfGlobalWindow.minus(1));    assertThat(truncatedWindow.maxTimestamp(), equalTo(endOfGlobalWindow));}
public void beam_f22975_0() throws Exception
{    CoderProperties.coderDecodeEncodeEqual(GlobalWindow.Coder.INSTANCE, GlobalWindow.INSTANCE);}
public void beam_f22976_0()
{    CoderProperties.coderSerializable(GlobalWindow.Coder.INSTANCE);}
public void beam_f22985_0() throws Exception
{    assertSame(PaneInfo.createPane(true, true, Timing.EARLY), PaneInfo.createPane(true, true, Timing.EARLY));}
public void beam_f22986_0() throws Exception
{    Coder<PaneInfo> coder = PaneInfo.PaneInfoCoder.INSTANCE;    for (Timing timing : Timing.values()) {        long onTimeIndex = timing == Timing.EARLY ? -1 : 37;        CoderProperties.coderDecodeEncodeEqual(coder, PaneInfo.createPane(false, false, timing, 389, onTimeIndex));        CoderProperties.coderDecodeEncodeEqual(coder, PaneInfo.createPane(false, true, timing, 5077, onTimeIndex));        CoderProperties.coderDecodeEncodeEqual(coder, PaneInfo.createPane(true, false, timing, 0, 0));        CoderProperties.coderDecodeEncodeEqual(coder, PaneInfo.createPane(true, true, timing, 0, 0));    }}
public void beam_f22995_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(0), new Instant(10)), set(0));    expected.put(new IntervalWindow(new Instant(10), new Instant(20)), set(10));    expected.put(new IntervalWindow(new Instant(101), new Instant(111)), set(101));    assertEquals(expected, runWindowFn(Sessions.withGapDuration(new Duration(10)), Arrays.asList(0L, 10L, 101L)));}
public void beam_f22996_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(1), new Instant(19)), set(1, 2, 5, 9));    expected.put(new IntervalWindow(new Instant(100), new Instant(111)), set(100, 101));    assertEquals(expected, runWindowFn(Sessions.withGapDuration(new Duration(10)), Arrays.asList(1L, 2L, 5L, 9L, 100L, 101L)));}
public void beam_f23005_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(-5), new Instant(5)), set(1, 2));    expected.put(new IntervalWindow(new Instant(0), new Instant(10)), set(1, 2, 5, 9));    expected.put(new IntervalWindow(new Instant(5), new Instant(15)), set(5, 9, 10, 11));    expected.put(new IntervalWindow(new Instant(10), new Instant(20)), set(10, 11));    SlidingWindows windowFn = SlidingWindows.of(new Duration(10)).every(new Duration(5));    assertEquals(expected, runWindowFn(windowFn, Arrays.asList(1L, 2L, 5L, 9L, 10L, 11L)));    assertThat(windowFn.assignsToOneWindow(), is(false));}
public void beam_f23006_0() throws Exception
{    Map<IntervalWindow, Set<String>> expected = new HashMap<>();    expected.put(new IntervalWindow(new Instant(-5), new Instant(2)), set(1));    expected.put(new IntervalWindow(new Instant(0), new Instant(7)), set(1, 2, 5));    expected.put(new IntervalWindow(new Instant(5), new Instant(12)), set(5, 9, 10, 11));    expected.put(new IntervalWindow(new Instant(10), new Instant(17)), set(10, 11));    SlidingWindows windowFn = SlidingWindows.of(new Duration(7)).every(new Duration(5));    assertEquals(expected, runWindowFn(windowFn, Arrays.asList(1L, 2L, 5L, 9L, 10L, 11L)));    assertThat(windowFn.assignsToOneWindow(), is(false));}
public void beam_f23015_0() throws Exception
{    for (long timestamp : Arrays.asList(200, 800, 499, 500, 501, 700, 1000)) {        WindowFnTestUtils.validateGetOutputTimestamp(SlidingWindows.of(new Duration(1000)).every(new Duration(500)), timestamp);    }}
public void beam_f23016_0() throws Exception
{    for (long timestamp : Arrays.asList(200, 800, 700)) {        WindowFnTestUtils.validateNonInterferingOutputTimes(SlidingWindows.of(new Duration(1000)).every(new Duration(500)), timestamp);    }}
public Instant beam_f23025_0(BoundedWindow window)
{    return null;}
protected Trigger beam_f23026_0(List<Trigger> continuationTriggers)
{    return null;}
public void beam_f23035_0()
{    PCollection<String> input = p.apply(Create.empty(StringUtf8Coder.of()));    PCollection<String> output = input.apply(new WindowedCount(FixedWindows.of(new Duration(10))));    PAssert.that(output).empty();    p.run();}
public void beam_f23036_0() throws Exception
{    File tmpFile = tmpFolder.newFile("file.txt");    String filename = tmpFile.getPath();    try (PrintStream writer = new PrintStream(new FileOutputStream(tmpFile))) {        writer.println("a 1");        writer.println("b 2");        writer.println("b 3");        writer.println("c 11");        writer.println("d 11");    }    PCollection<String> output = p.begin().apply("ReadLines", TextIO.read().from(filename)).apply(ParDo.of(new ExtractWordsWithTimestampsFn())).apply(new WindowedCount(FixedWindows.of(Duration.millis(10))));    PAssert.that(output).containsInAnyOrder(output("a", 1, 1, 0, 10), output("b", 2, 2, 0, 10), output("c", 1, 11, 10, 20), output("d", 1, 11, 10, 20));    p.run();}
public void beam_f23045_0(TransformHierarchy.Node node)
{    if (node.getTransform() instanceof Window.Assign) {        foundAssign.set(true);    }}
public void beam_f23046_0()
{    pipeline.apply(Create.of(1, 2, 3)).apply(Window.<Integer>configure().triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO).accumulatingFiredPanes());    pipeline.traverseTopologically(new PipelineVisitor.Defaults() {        @Override        public void visitPrimitiveTransform(TransformHierarchy.Node node) {            assertThat(node.getTransform(), not(instanceOf(Window.Assign.class)));        }    });}
public void beam_f23055_0(WindowFn<?, ?> other) throws IncompatibleWindowException
{    if (!this.isCompatible(other)) {        throw new IncompatibleWindowException(other, "WindowOddEvenBuckets is only compatible with WindowOddEvenBuckets.");    }}
public Coder<IntervalWindow> beam_f23056_0()
{    return new IntervalWindow.IntervalWindowCoder();}
public void beam_f23065_0()
{    FixedWindows windowFn = FixedWindows.of(Duration.standardHours(5));    AfterWatermark.FromEndOfWindow triggerBuilder = AfterWatermark.pastEndOfWindow();    Duration allowedLateness = Duration.standardMinutes(10);    Window.ClosingBehavior closingBehavior = Window.ClosingBehavior.FIRE_IF_NON_EMPTY;    TimestampCombiner timestampCombiner = TimestampCombiner.END_OF_WINDOW;    Window<?> window = Window.into(windowFn).triggering(triggerBuilder).accumulatingFiredPanes().withAllowedLateness(allowedLateness, closingBehavior).withTimestampCombiner(timestampCombiner);    DisplayData primitiveDisplayData = Iterables.getOnlyElement(DisplayDataEvaluator.create().displayDataForPrimitiveTransforms(window));    assertThat(primitiveDisplayData, hasDisplayItem("windowFn", windowFn.getClass()));    assertThat(primitiveDisplayData, includesDisplayDataFor("windowFn", windowFn));    assertThat(primitiveDisplayData, hasDisplayItem("trigger", triggerBuilder.toString()));    assertThat(primitiveDisplayData, hasDisplayItem("accumulationMode", AccumulationMode.ACCUMULATING_FIRED_PANES.toString()));    assertThat(primitiveDisplayData, hasDisplayItem("allowedLateness", allowedLateness));    assertThat(primitiveDisplayData, hasDisplayItem("closingBehavior", closingBehavior.toString()));    assertThat(primitiveDisplayData, hasDisplayItem("timestampCombiner", timestampCombiner.toString()));}
public void beam_f23066_0()
{    FixedWindows windowFn = FixedWindows.of(Duration.standardHours(5));    Window<Object> original = Window.into(windowFn);    WindowingStrategy<?, ?> updated = WindowingStrategy.globalDefault().withWindowFn(windowFn);    DisplayData displayData = DisplayData.from(new Window.Assign<>(original, updated));    assertThat(displayData, hasDisplayItem("windowFn", windowFn.getClass()));    assertThat(displayData, includesDisplayDataFor("windowFn", windowFn));    assertThat(displayData, not(hasDisplayItem("trigger")));    assertThat(displayData, not(hasDisplayItem("accumulationMode")));    assertThat(displayData, not(hasDisplayItem("allowedLateness")));    assertThat(displayData, not(hasDisplayItem("closingBehavior")));    assertThat(displayData, not(hasDisplayItem("timestampCombiner")));}
public CustomWindow beam_f23075_0(InputStream inStream) throws IOException
{    IntervalWindow superWindow = INTERVAL_WINDOW_CODER.decode(inStream);    boolean isBig = VAR_INT_CODER.decode(inStream) != 0;    return new CustomWindow(superWindow.start(), superWindow.end(), isBig);}
public void beam_f23076_0() throws NonDeterministicException
{    INTERVAL_WINDOW_CODER.verifyDeterministic();    VAR_INT_CODER.verifyDeterministic();}
public void beam_f23085_0()
{    PCollection<String> input = p.apply(Create.of(Arrays.asList(COLLECTION)).withCoder(StringUtf8Coder.of()));    PCollection<KV<Void, String>> output = input.apply(WithKeys.of((Void) null));    PAssert.that(output).containsInAnyOrder(WITH_CONST_NULL_KEYS);    p.run();}
public void beam_f23086_0()
{    assertEquals("WithKeys", WithKeys.<Integer, String>of(100).getName());}
public void beam_f23095_0(DoFn<String, KV<String, Instant>>.ProcessContext c) throws Exception
{    c.output(KV.of(c.element(), c.timestamp()));}
public void beam_f23096_0()
{    SerializableFunction<String, Instant> timestampFn = input -> null;    String yearTwoThousand = "946684800000";    p.apply(Create.of("1234", "0", Integer.toString(Integer.MAX_VALUE), yearTwoThousand)).apply(WithTimestamps.of(timestampFn));    thrown.expect(PipelineExecutionException.class);    thrown.expectCause(isA(NullPointerException.class));    thrown.expectMessage("WithTimestamps");    thrown.expectMessage("cannot be null");    p.run();}
public void beam_f23105_0() throws Exception
{    ApiSurface apiSurface = ApiSurface.ofClass(ExposedWildcardBound.class).includingClass(Object.class).includingClass(ApiSurface.class).pruningPattern(".*");    assertThat(apiSurface.getExposedClasses(), emptyIterable());}
public void beam_f23106_0() throws Exception
{    ApiSurface apiSurface = ApiSurface.ofClass(NotPruned.class).pruningClass(PrunedPattern.class);    assertThat(apiSurface.getExposedClasses(), containsInAnyOrder((Class) NotPruned.class));}
public void beam_f23115_0()
{    BucketingFunction f = newFunc();    assertFalse(f.isSignificant());    f.add(0, 0);    assertFalse(f.isSignificant());    f.add(BUCKET_WIDTH, 0);    assertTrue(f.isSignificant());}
public void beam_f23116_0()
{    BucketingFunction f = newFunc();    for (int i = 0; i < 100; i++) {        f.add(i, i);        assertEquals(((i + 1) * i) / 2, f.get());    }}
public void beam_f23125_0() throws Exception
{    ByteArrayOutputStream bos = new ByteArrayOutputStream();    BufferedElementCountingOutputStream os = new BufferedElementCountingOutputStream(bos);    os.markElementStart();    os.write(new byte[] { 1 });    os.flush();    os.write(new byte[] { 2 });    os.close();    assertArrayEquals(new byte[] { 1, 1, 2, 0 }, bos.toByteArray());}
public void beam_f23126_0() throws Exception
{    BufferedElementCountingOutputStream os = testValues(toBytes("a"));    os.flush();    os.flush();    os.flush();}
private List<byte[]> beam_f23135_0(String... values)
{    ImmutableList.Builder<byte[]> builder = ImmutableList.builder();    for (String value : values) {        builder.add(value.getBytes(Charsets.UTF_8));    }    return builder.build();}
private BufferedElementCountingOutputStream beam_f23136_0(List<byte[]> expectedValues) throws Exception
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    BufferedElementCountingOutputStream os = createAndWriteValues(expectedValues, baos);    os.finish();    verifyValues(expectedValues, new ByteArrayInputStream(baos.toByteArray()));    return os;}
public void beam_f23145_0() throws Exception
{    expectedException.expect(UnsupportedOperationException.class);    expectedException.expectMessage("Caller does not own the underlying");    CoderUtils.decodeFromByteArray(new ClosingCoder(), new byte[0]);}
public void beam_f23146_0() throws Exception
{    expectedException.expect(UnsupportedOperationException.class);    expectedException.expectMessage("Caller does not own the underlying");    CoderUtils.decodeFromByteArray(new ClosingCoder(), new byte[0], Context.NESTED);}
public void beam_f23155_0()
{    assertEquals(getClass().getSimpleName(), ReflectHelpers.CLASS_SIMPLE_NAME.apply(getClass()));}
public void beam_f23156_0() throws Exception
{    assertEquals("testMethodFormatter()", ReflectHelpers.METHOD_FORMATTER.apply(getClass().getMethod("testMethodFormatter")));    assertEquals("oneArg(int)", ReflectHelpers.METHOD_FORMATTER.apply(getClass().getDeclaredMethod("oneArg", int.class)));    assertEquals("twoArg(String, List)", ReflectHelpers.METHOD_FORMATTER.apply(getClass().getDeclaredMethod("twoArg", String.class, List.class)));}
public void beam_f23167_0() throws InterruptedException
{    final ClassLoader[] classLoader = new ClassLoader[1];    Thread thread = new Thread(() -> classLoader[0] = ReflectHelpers.findClassLoader());    ClassLoader cl = new ClassLoader() {    };    thread.setContextClassLoader(cl);    thread.start();    thread.join();    assertEquals(cl, classLoader[0]);}
public String beam_f23168_0()
{    return "Alpha";}
public void beam_f23177_0()
{    writeToBoth(new byte[0], 0, 0);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23178_0()
{    writeToBoth(32);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23187_0()
{    writeToBoth(TEST_DATA, 0, TEST_DATA.length - 1);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23188_0()
{    writeToBoth(TEST_DATA, 1, TEST_DATA.length - 1);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23197_0()
{    writeToBoth(32);    resetBoth();    writeToBoth(32);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23198_0()
{    resetBoth();    writeToBoth(32);    assertStreamContentsEquals(stream, exposedStream);}
public void beam_f23207_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(containsString("Expected valid file path, but received"));    new FilePatternMatchingShardedFile("");}
public void beam_f23208_0() throws Exception
{    String contents1 = "To be or not to be, ", contents2 = "it is not a question.", contents3 = "should not be included";    File tmpFile1 = tmpFolder.newFile("result-000-of-002");    File tmpFile2 = tmpFolder.newFile("result-001-of-002");    File tmpFile3 = tmpFolder.newFile("tmp");    Files.write(contents1, tmpFile1, StandardCharsets.UTF_8);    Files.write(contents2, tmpFile2, StandardCharsets.UTF_8);    Files.write(contents3, tmpFile3, StandardCharsets.UTF_8);    filePattern = LocalResources.fromFile(tmpFolder.getRoot(), true).resolve("result-*", StandardResolveOptions.RESOLVE_FILE).toString();    FilePatternMatchingShardedFile shardedFile = new FilePatternMatchingShardedFile(filePattern);    assertThat(shardedFile.readFilesWithRetries(), containsInAnyOrder(contents1, contents2));}
public void beam_f23217_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("maxCumulativeBackoff PT-0.002S must be at least 1 millisecond");    defaultBackoff.withMaxCumulativeBackoff(Duration.millis(-2));}
public void beam_f23218_0() throws Exception
{    BackOff backOff = FluentBackoff.DEFAULT.withInitialBackoff(Duration.millis(500)).withMaxBackoff(Duration.standardSeconds(1)).backoff();    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(249L), lessThan(751L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(374L), lessThan(1126L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(500L), lessThanOrEqualTo(1500L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(500L), lessThanOrEqualTo(1500L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(500L), lessThanOrEqualTo(1500L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(500L), lessThanOrEqualTo(1500L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(500L), lessThanOrEqualTo(1500L)));        backOff.reset();    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(249L), lessThan(751L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(374L), lessThan(1126L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(500L), lessThanOrEqualTo(1500L)));    assertThat(backOff.nextBackOffMillis(), allOf(greaterThanOrEqualTo(500L), lessThanOrEqualTo(1500L)));}
public Coder<BoundedWindow> beam_f23228_0()
{        return (Coder) GlobalWindow.Coder.INSTANCE;}
public WindowMappingFn<BoundedWindow> beam_f23229_0()
{    return new WindowMappingFn<BoundedWindow>() {        @Override        public BoundedWindow getSideInputWindow(BoundedWindow window) {            return window;        }    };}
public void beam_f23238_0() throws Exception
{    CompletionStage<Integer> future = MoreFutures.supplyAsync(() -> 42);    assertThat(MoreFutures.get(future), equalTo(42));}
public void beam_f23239_0() throws Exception
{    final String testMessage = "this is just a test";    CompletionStage<Long> future = MoreFutures.supplyAsync(() -> {        throw new IllegalStateException(testMessage);    });    thrown.expect(ExecutionException.class);    thrown.expectCause(isA(IllegalStateException.class));    thrown.expectMessage(testMessage);    MoreFutures.get(future);}
public void beam_f23248_0()
{    MovingFunction f = newFunc();    int lost = 0;    for (int i = 0; i < SAMPLE_PERIOD * 2; i++) {        f.add(i, 1);        if (i >= SAMPLE_PERIOD && i % SAMPLE_UPDATE == 0) {            lost += SAMPLE_UPDATE;        }        assertEquals(i + 1 - lost, f.get(i));    }}
public void beam_f23249_0()
{    MovingFunction f = newFunc();    f.add(0, 1);    f.add(SAMPLE_PERIOD - 1, 1);    assertEquals(2, f.get(SAMPLE_PERIOD - 1));    assertEquals(1, f.get(SAMPLE_PERIOD + 3 * SAMPLE_UPDATE));    assertEquals(0, f.get(SAMPLE_PERIOD * 2));}
public void beam_f23259_0() throws Exception
{    List<Integer> value = Arrays.asList(1, 2, 3, 4);    MutationDetector detector = MutationDetectors.forValueWithCoder(value, ListCoder.of(VarIntCoder.of()));    value.set(0, 37);    thrown.expect(IllegalMutationException.class);    detector.verifyUnmodified();}
public void beam_f23260_0() throws Exception
{    List<Integer> value = Lists.newLinkedList(Arrays.asList(1, 2, 3, 4));    MutationDetector detector = MutationDetectors.forValueWithCoder(value, ListCoder.of(VarIntCoder.of()));    detector.verifyUnmodified();}
public void beam_f23269_0()
{    assertEquals("Embedded", NameUtils.approximateSimpleName("SomeDoFn$EmbeddedDoFn", true));    assertEquals("Embedded", NameUtils.approximateSimpleName("SomeFn$EmbeddedFn", true));    assertEquals("Some.Embedded", NameUtils.approximateSimpleName("SomeDoFn$EmbeddedDoFn", false));    assertEquals("Some.Embedded", NameUtils.approximateSimpleName("SomeFn$EmbeddedFn", false));}
public void beam_f23270_0()
{    assertEquals("Bar", NameUtils.approximateSimpleName("Foo$1$Bar", true));    assertEquals("Foo$1", NameUtils.approximateSimpleName("Foo$1", true));    assertEquals("Foo$1$2", NameUtils.approximateSimpleName("Foo$1$2", true));}
public void beam_f23279_0()
{    EmbeddedPTransform transform = new EmbeddedPTransform();    assertEquals("NameUtilsTest.EmbeddedPTransform", NameUtils.approximatePTransformName(transform.getClass()));    assertEquals("NameUtilsTest.EmbeddedPTransform", NameUtils.approximatePTransformName(transform.getBound().getClass()));    assertEquals("NameUtilsTest.SomeTransform", NameUtils.approximatePTransformName(AutoValue_NameUtilsTest_SomeTransform.class));    assertEquals("TextIO.Write", NameUtils.approximatePTransformName(TextIO.Write.class));}
public PDone beam_f23280_0(PBegin input)
{    return null;}
public void beam_f23289_0() throws Exception
{    String contents1 = "To be or not to be, ", contents2 = "it is not a question.", contents3 = "should not be included";    File tmpFile1 = tmpFolder.newFile("result-000-of-002");    File tmpFile2 = tmpFolder.newFile("result-001-of-002");    File tmpFile3 = tmpFolder.newFile("tmp");    Files.write(contents1, tmpFile1, StandardCharsets.UTF_8);    Files.write(contents2, tmpFile2, StandardCharsets.UTF_8);    Files.write(contents3, tmpFile3, StandardCharsets.UTF_8);    filePattern = LocalResources.fromFile(tmpFolder.getRoot(), true).resolve("result-*", StandardResolveOptions.RESOLVE_FILE).toString();    NumberedShardedFile shardedFile = new NumberedShardedFile(filePattern);    assertThat(shardedFile.readFilesWithRetries(), containsInAnyOrder(contents1, contents2));}
public void beam_f23290_0() throws Exception
{    File emptyFile = tmpFolder.newFile("result-000-of-001");    Files.write("", emptyFile, StandardCharsets.UTF_8);    NumberedShardedFile shardedFile = new NumberedShardedFile(filePattern);    assertThat(shardedFile.readFilesWithRetries(), empty());}
public void beam_f23299_0() throws Exception
{    Schema schema = Schema.builder().addArrayField("f_arrayOfIntArrays", FieldType.array(FieldType.INT32)).build();    String rowString = "{\n" + "\"f_arrayOfIntArrays\" : [ [1, 2], [3, 4], [5]]\n" + "}";    RowJsonDeserializer deserializer = RowJsonDeserializer.forSchema(schema);    Row parsedRow = newObjectMapperWith(deserializer).readValue(rowString, Row.class);    Row expectedRow = Row.withSchema(schema).addArray(Arrays.asList(1, 2), Arrays.asList(3, 4), Arrays.asList(5)).build();    assertEquals(expectedRow, parsedRow);}
public void beam_f23300_0() throws Exception
{    Schema schema = Schema.builder().addArrayField("f_arrayOfIntArrays", FieldType.array(FieldType.INT32)).build();    String rowString = "{\n" +     "\"f_arrayOfIntArrays\" : { }\n" + "}";    RowJsonDeserializer deserializer = RowJsonDeserializer.forSchema(schema);    thrown.expect(UnsupportedRowJsonException.class);    thrown.expectMessage("Expected JSON array");    newObjectMapperWith(deserializer).readValue(rowString, Row.class);}
public void beam_f23309_0() throws Exception
{    testSupportedConversion(FieldType.BOOLEAN, BOOLEAN_TRUE_STRING, BOOLEAN_TRUE_VALUE);}
public void beam_f23310_0() throws Exception
{    testSupportedConversion(FieldType.STRING, quoted(FLOAT_STRING), FLOAT_STRING);}
public void beam_f23319_0() throws Exception
{    testUnsupportedConversion(FieldType.STRING, BOOLEAN_TRUE_STRING);    testUnsupportedConversion(FieldType.STRING, BYTE_STRING);    testUnsupportedConversion(FieldType.STRING, SHORT_STRING);    testUnsupportedConversion(FieldType.STRING, INT_STRING);    testUnsupportedConversion(FieldType.STRING, LONG_STRING);    testUnsupportedConversion(FieldType.STRING, FLOAT_STRING);    testUnsupportedConversion(FieldType.STRING, DOUBLE_STRING);}
public void beam_f23320_0() throws Exception
{    testUnsupportedConversion(FieldType.BYTE, BOOLEAN_TRUE_STRING);    testUnsupportedConversion(FieldType.BYTE, quoted(BYTE_STRING));    testUnsupportedConversion(FieldType.BYTE, SHORT_STRING);    testUnsupportedConversion(FieldType.BYTE, INT_STRING);    testUnsupportedConversion(FieldType.BYTE, LONG_STRING);    testUnsupportedConversion(FieldType.BYTE, FLOAT_STRING);    testUnsupportedConversion(FieldType.BYTE, DOUBLE_STRING);}
private String beam_f23329_0(String fieldName, String fieldValue)
{    return "{\n" + "\"" + fieldName + "\" : " + fieldValue + "\n" + "}";}
private Matcher<UnsupportedRowJsonException> beam_f23330_0(String... message)
{    return allOf(Matchers.isA(UnsupportedRowJsonException.class), hasProperty("message", stringContainsInOrder(Arrays.asList(message))));}
private void beam_f23341_0(final ClassLoader loader, final Serializable customLoaderSourceInstance)
{    final Serializable copy = SerializableUtils.ensureSerializable(customLoaderSourceInstance);    assertEquals(loader, copy.getClass().getClassLoader());    assertEquals(copy.getClass().getClassLoader(), customLoaderSourceInstance.getClass().getClassLoader());}
public void beam_f23342_0()
{    testData = new byte[60 * 1024];    Arrays.fill(testData, (byte) 32);}
public void beam_f23351_0() throws Exception
{    expectedException.expect(UnsupportedOperationException.class);    expectedException.expectMessage("Caller does not own the underlying");    expectedException.expectMessage("close()");    os.close();}
public void beam_f23352_0() throws Exception
{    assertFalse(os.markSupported());    expectedException.expect(UnsupportedOperationException.class);    expectedException.expectMessage("Caller does not own the underlying");    expectedException.expectMessage("mark()");    os.mark(1);}
public void beam_f23361_0()
{    IOException cause = new IOException();    RuntimeException wrapped = UserCodeException.wrapIf(true, cause);    assertThat(wrapped, is(instanceOf(UserCodeException.class)));}
public void beam_f23362_0()
{    IOException cause = new IOException();    RuntimeException wrapped = UserCodeException.wrapIf(false, cause);    assertThat(wrapped, is(not(instanceOf(UserCodeException.class))));    assertEquals(cause, wrapped.getCause());}
private static ThrowableBottomStackFrameMethodMatcher beam_f23371_0(Matcher<StackTraceElement> frameMatcher)
{    return new ThrowableBottomStackFrameMethodMatcher(frameMatcher);}
private static StackFrameMethodMatcher beam_f23372_0(String methodName)
{    return new StackFrameMethodMatcher(is(methodName));}
public void beam_f23381_0() throws IOException
{    assertEquals(LONG_VALUES.length, LONG_ENCODED.length);    for (int i = 0; i < LONG_VALUES.length; ++i) {        byte[] encoded = encodeLong(LONG_VALUES[i]);        assertThat(encoded, equalTo(LONG_ENCODED[i]));        assertEquals(LONG_ENCODED[i].length, VarInt.getLength(LONG_VALUES[i]));    }    assertEquals(INT_VALUES.length, INT_ENCODED.length);    for (int i = 0; i < INT_VALUES.length; ++i) {        byte[] encoded = encodeInt(INT_VALUES[i]);        assertThat(encoded, equalTo(INT_ENCODED[i]));        assertEquals(INT_ENCODED[i].length, VarInt.getLength(INT_VALUES[i]));    }}
public void beam_f23382_0() throws IOException
{    final byte[] tooLargeNumber = { (byte) 0xff, (byte) 0xff, (byte) 0xff, (byte) 0xff, (byte) 0xff, (byte) 0xff, (byte) 0xff, (byte) 0xff, (byte) 0xff, 0x02 };    thrown.expect(IOException.class);    decodeLong(tooLargeNumber);}
public void beam_f23391_0()
{    CoderProperties.coderSerializable(WindowedValue.getValueOnlyCoder(GlobalWindow.Coder.INSTANCE));}
public void beam_f23392_0()
{    thrown.expect(IllegalArgumentException.class);    WindowedValue.of("foo", Instant.now(), ImmutableList.of(), PaneInfo.NO_FIRING);}
public void beam_f23401_0() throws Exception
{    createFileWithContents(tmpDir, "myTextFile.txt", "Simple Text");    File[] sourceFiles = tmpDir.listFiles();    Arrays.sort(sourceFiles);    assertThat(sourceFiles, not(arrayWithSize(0)));    try (FileOutputStream outputStream = new FileOutputStream(zipFile)) {        ZipFiles.zipDirectory(tmpDir, outputStream);    }    File outputDir = Files.createTempDir();    ZipFiles.unzipFile(zipFile, outputDir);    File[] outputFiles = outputDir.listFiles();    Arrays.sort(outputFiles);    assertThat(outputFiles, arrayWithSize(sourceFiles.length));    for (int i = 0; i < sourceFiles.length; i++) {        compareFileContents(sourceFiles[i], outputFiles[i]);    }    removeRecursive(outputDir.toPath());    assertTrue(zipFile.delete());}
public void beam_f23402_0() throws Exception
{    File zipDir = new File(tmpDir, "zip");    File subDir1 = new File(zipDir, "subDir1");    File subDir2 = new File(subDir1, "subdir2");    assertTrue(subDir2.mkdirs());    createFileWithContents(subDir2, "myTextFile.txt", "Simple Text");    ZipFiles.zipDirectory(tmpDir, zipFile);    try (ZipFile zip = new ZipFile(zipFile)) {        Enumeration<? extends ZipEntry> entries = zip.entries();        for (ZipEntry entry : ZipFiles.entries(zip)) {            assertTrue(entries.hasMoreElements());                        assertEquals(entry.getName(), entries.nextElement().getName());        }        assertFalse(entries.hasMoreElements());    }}
private static void beam_f23411_0(Path path) throws IOException
{    Iterable<File> files = Files.fileTraverser().depthFirstPostOrder(path.toFile());    for (File f : files) {        java.nio.file.Files.delete(f.toPath());    }}
private void beam_f23412_0(File dir, String fileName, String fileContents) throws IOException
{    File txtFile = new File(dir, fileName);    Files.asCharSink(txtFile, StandardCharsets.UTF_8).write(fileContents);}
public void beam_f23421_0()
{    PCollection<Integer> pCollection = PCollection.createPrimitiveOutputInternal(pipeline, WindowingStrategy.globalDefault(), IsBounded.BOUNDED, VarIntCoder.of());    TupleTag<Integer> tag = new TupleTag<>();    assertTrue(PCollectionTuple.of(tag, pCollection).has(tag));}
public void beam_f23422_0()
{    TupleTag<Object> tag = new TupleTag<>();    assertFalse(PCollectionTuple.empty(pipeline).has(tag));}
public void beam_f23431_0() throws Exception
{    File tmpFile = tmpFolder.newFile("file.txt");    String filename = tmpFile.getPath();    p.begin().apply(new SimpleTransform(filename));    p.run();}
public void beam_f23432_0()
{    Schema type = Stream.of(Schema.Field.of("f_byte", FieldType.BYTE).withNullable(true), Schema.Field.of("f_int16", FieldType.INT16).withNullable(true), Schema.Field.of("f_int32", FieldType.INT32).withNullable(true), Schema.Field.of("f_int64", FieldType.INT64).withNullable(true), Schema.Field.of("f_decimal", FieldType.DECIMAL).withNullable(true), Schema.Field.of("f_float", FieldType.FLOAT).withNullable(true), Schema.Field.of("f_double", FieldType.DOUBLE).withNullable(true), Schema.Field.of("f_string", FieldType.STRING).withNullable(true), Schema.Field.of("f_datetime", FieldType.DATETIME).withNullable(true), Schema.Field.of("f_boolean", FieldType.BOOLEAN).withNullable(true), Schema.Field.of("f_array", FieldType.array(FieldType.DATETIME)).withNullable(true), Schema.Field.of("f_map", FieldType.map(FieldType.INT32, FieldType.DOUBLE)).withNullable(true)).collect(toSchema());    Row row = Row.nullRow(type);    assertNull(row.getByte("f_byte"));    assertNull(row.getByte(0));    assertNull(row.getInt16("f_int16"));    assertNull(row.getInt16(1));    assertNull(row.getInt32("f_int32"));    assertNull(row.getInt32(2));    assertNull(row.getInt64("f_int64"));    assertNull(row.getInt64(3));    assertNull(row.getDecimal("f_decimal"));    assertNull(row.getDecimal(4));    assertNull(row.getFloat("f_float"));    assertNull(row.getFloat(5));    assertNull(row.getDouble("f_double"));    assertNull(row.getDouble(6));    assertNull(row.getString("f_string"));    assertNull(row.getString(7));    assertNull(row.getDateTime("f_datetime"));    assertNull(row.getDateTime(8));    assertNull(row.getBoolean("f_boolean"));    assertNull(row.getBoolean(9));    assertNull(row.getBoolean("f_array"));    assertNull(row.getBoolean(10));    assertNull(row.getBoolean("f_map"));    assertNull(row.getBoolean(11));}
public void beam_f23441_0()
{    List<List<Integer>> data = Lists.<List<Integer>>newArrayList(Lists.newArrayList(1, null, 3, null), null);    Schema type = Stream.of(Schema.Field.of("array", FieldType.array(FieldType.array(FieldType.INT32, true), true))).collect(toSchema());    Row row = Row.withSchema(type).addArray(data).build();    assertEquals(data, row.getArray("array"));}
public void beam_f23442_0()
{    List<Map<Integer, String>> data = ImmutableList.<Map<Integer, String>>builder().add(ImmutableMap.of(1, "value1")).add(ImmutableMap.of(2, "value2")).build();    Schema type = Stream.of(Schema.Field.of("array", FieldType.array(FieldType.map(FieldType.INT32, FieldType.STRING)))).collect(toSchema());    Row row = Row.withSchema(type).addArray(data).build();    assertEquals(data, row.getArray("array"));}
public void beam_f23451_0()
{    Schema type = Stream.of(Schema.Field.of("f_int", FieldType.INT32), Schema.Field.of("f_str", FieldType.STRING), Schema.Field.of("f_double", FieldType.DOUBLE)).collect(toSchema());    thrown.expect(IllegalArgumentException.class);    Row.withSchema(type).addValues(1, "2").build();}
public void beam_f23452_0()
{    byte[] a0 = new byte[] { 1, 2, 3, 4 };    byte[] b0 = new byte[] { 1, 2, 3, 4 };    Schema schema = Schema.of(Schema.Field.of("bytes", Schema.FieldType.BYTES));    Row a = Row.withSchema(schema).addValue(a0).build();    Row b = Row.withSchema(schema).addValue(b0).build();    Assert.assertEquals(a, b);}
private static TupleTag<Object> beam_f23461_0()
{    return new TupleTag<>();}
private TupleTag<Object> beam_f23462_0()
{    return new TupleTag<>();}
private static Generic<ActualFooT, String> beam_f23471_0()
{    return new Generic<ActualFooT, String>() {    };}
private static TypeDescriptor<ActualFooT> beam_f23472_0(Generic<ActualFooT, ActualBarT> instance)
{    return TypeDescriptors.extractFromTypeParameters(instance, Generic.class, new TypeDescriptors.TypeVariableExtractor<Generic<ActualFooT, ActualBarT>, ActualFooT>() {    });}
public T beam_f23481_0(T thingie)
{    return thingie;}
public void beam_f23482_0() throws Exception
{    Method identity = Id.class.getDeclaredMethod("identity", Object.class);    TypeToken<Id<String>> token = new TypeToken<Id<String>>() {    };    TypeDescriptor<Id<String>> descriptor = new TypeDescriptor<Id<String>>() {    };    assertEquals(token.method(identity).getParameters().get(0).getType().getType(), descriptor.getArgumentTypes(identity).get(0).getType());    TypeToken<Id<List<String>>> genericToken = new TypeToken<Id<List<String>>>() {    };    TypeDescriptor<Id<List<String>>> genericDescriptor = new TypeDescriptor<Id<List<String>>>() {    };    assertEquals(genericToken.method(identity).getParameters().get(0).getType().getType(), genericDescriptor.getArgumentTypes(identity).get(0).getType());}
public void beam_f23491_0() throws Exception
{    useWhereMethodToDefineTypeParam(new TypeDescriptor<String>() {    });}
private void beam_f23492_0(TypeDescriptor<T> parameterType)
{    TypeDescriptor<Set<T>> typeDescriptor = new TypeDescriptor<Set<T>>() {    }.where(new TypeParameter<T>() {    }, parameterType);    assertEquals(new TypeToken<Set<String>>() {    }.getType(), typeDescriptor.getType());}
public void beam_f23501_0()
{    PCollection<EmptyClass> input = p.apply(Create.of(1, 2, 3)).apply(ParDo.of(new EmptyClassDoFn()));    thrown.expect(IllegalStateException.class);        thrown.expectMessage(not(containsString("erasure")));    thrown.expectMessage(not(containsString("see TupleTag Javadoc")));        thrown.expectMessage("Building a Coder using a registered CoderProvider failed");    input.getCoder();}
public void beam_f23502_0()
{    p.enableAbandonedNodeEnforcement(false);    PCollection<Integer> created = p.apply(Create.of(1, 2, 3));    ParDo.SingleOutput<Integer, EmptyClass> uninferrableParDo = ParDo.of(new EmptyClassDoFn());    PCollection<EmptyClass> unencodable = created.apply(uninferrableParDo);    thrown.expect(IllegalStateException.class);    thrown.expectMessage("Unable to return a default Coder");    thrown.expectMessage("Inferring a Coder from the CoderRegistry failed");    unencodable.finishSpecifying(created, uninferrableParDo);}
public Timer beam_f23511_0(String name)
{    return VoidTimer.INSTANCE;}
public static Factory beam_f23512_0()
{    return INSTANCE;}
public static OfBuilder beam_f23526_0(String name)
{    return new OfBuilder(name);}
public static UsingBuilder<InputT> beam_f23527_0(PCollection<InputT> input)
{    return new UsingBuilder<>(DEFAULT_NAME, input);}
public Builders.Output<InputT> beam_f23536_0(ExtractEventTime<InputT> eventTimeExtractor)
{    this.eventTimeExtractor = eventTimeExtractor;    return this;}
public Builders.Output<InputT> beam_f23537_0(ExtractEventTime<InputT> eventTimeExtractor, Duration allowedTimestampSkew)
{    this.allowedTimestampSkew = allowedTimestampSkew;    return using(eventTimeExtractor);}
public String beam_f23546_0()
{    return this.getClass().getSimpleName() + " operator{" + "name='" + name + '\'' + '}';}
 OutBuilderT beam_f23547_0(boolean cond, UnaryFunction<InBuilderT, OutBuilderT> applyIfTrue, UnaryFunction<InBuilderT, OutBuilderT> applyIfFalse)
{    if (cond) {        return applyIfTrue.apply((InBuilderT) this);    }    return applyIfFalse.apply((InBuilderT) this);}
public WindowByBuilder<T> beam_f23556_0(UnaryFunction<InputT, T> keyExtractor, @Nullable TypeDescriptor<T> keyType)
{    @SuppressWarnings("unchecked")    final Builder<InputT, T> cast = (Builder<InputT, T>) this;    cast.keyExtractor = requireNonNull(keyExtractor);    cast.keyType = keyType;    return cast;}
public TriggeredByBuilder<KeyT> beam_f23557_0(WindowFn<Object, W> windowFn)
{    windowBuilder.windowBy(windowFn);    return this;}
public static ProjectedBuilder<InputT, InputT> beam_f23566_0(PCollection<InputT> input)
{    return named(null).of(input);}
public static OfBuilder beam_f23567_0(@Nullable String name)
{    return new Builder(name);}
public WindowedOutputBuilder<InputT> beam_f23576_0(WindowingStrategy.AccumulationMode accumulationMode)
{    windowBuilder.accumulationMode(accumulationMode);    return this;}
public WindowedOutputBuilder<InputT> beam_f23577_0(Duration allowedLateness)
{    windowBuilder.withAllowedLateness(allowedLateness);    return this;}
private CombinableReduceFunction<KV<Long, InputT>> beam_f23586_0()
{    return policy == SelectionPolicy.NEWEST ? values -> nonEmpty(values.collect(Collectors.maxBy((a, b) -> Long.compare(a.getKey(), b.getKey())))) : values -> nonEmpty(values.collect(Collectors.minBy((a, b) -> Long.compare(a.getKey(), b.getKey()))));}
private static T beam_f23587_0(Optional<T> in)
{    return in.orElseThrow(() -> new IllegalStateException("Empty reduce values?"));}
public static OfBuilder beam_f23596_0(@Nullable String name)
{    return new Builder<>(name);}
 Builders.Output<OutputT> beam_f23597_0(ExtractEventTime<InputT> eventTimeFn)
{        return eventTimeBy(eventTimeFn, null);}
public static ByBuilder<LeftT, RightT> beam_f23606_0(PCollection<LeftT> left, PCollection<RightT> right)
{    return named("FullJoin").of(left, right);}
public static OfBuilder beam_f23607_0(String name)
{    return new Builder<>(name);}
 WindowByBuilder<KeyT, OutputT> beam_f23616_0(BinaryFunctor<LeftT, RightT, OutputT> joinFunc)
{    return using(joinFunc, null);}
 OutputBuilder<KeyT, OutputT> beam_f23617_0(boolean cond, UnaryFunction<WindowByBuilder<KeyT, OutputT>, OutputBuilder<KeyT, OutputT>> fn)
{    return cond ? requireNonNull(fn).apply(this) : this;}
public WindowedOutputBuilder<KeyT, OutputT> beam_f23626_0(TimestampCombiner timestampCombiner)
{    windowBuilder.withTimestampCombiner(timestampCombiner);    return this;}
public WindowedOutputBuilder<KeyT, OutputT> beam_f23627_0(Window.OnTimeBehavior behavior)
{    windowBuilder.withOnTimeBehavior(behavior);    return this;}
public static OfBuilder beam_f23636_0(String name)
{    return new Builder<>(name);}
 UsingBuilder<LeftT, RightT, KeyT> beam_f23637_0(UnaryFunction<LeftT, KeyT> leftKeyExtractor, UnaryFunction<RightT, KeyT> rightKeyExtractor)
{    return by(leftKeyExtractor, rightKeyExtractor, null);}
 Builders.Output<OutputT> beam_f23646_0(UnaryFunctionEnv<InputT, OutputT> mapper)
{    return using(mapper, null);}
public UsingBuilder<T> beam_f23647_0(PCollection<T> input)
{    @SuppressWarnings("unchecked")    final Builder<T, ?> cast = (Builder) this;    cast.input = input;    return cast;}
 ValueByReduceByBuilder<InputT, T, InputT> beam_f23656_0(UnaryFunction<InputT, T> keyExtractor)
{    return keyBy(keyExtractor, null);}
 WithSortedValuesBuilder<KeyT, ValueT, OutputT> beam_f23657_0(ReduceFunction<ValueT, OutputT> reducer)
{    return reduceBy((Stream<ValueT> in, Collector<OutputT> ctx) -> ctx.collect(reducer.apply(in)));}
 WindowByBuilder<KeyT, ValueT> beam_f23666_0(VoidFunction<AccT> accumulatorFactory, BinaryFunction<AccT, ValueT, AccT> accumulate, CombinableBinaryFunction<AccT> mergeAccumulators, UnaryFunction<AccT, ValueT> outputFn)
{    return combineBy(accumulatorFactory, accumulate, mergeAccumulators, outputFn, null, null);}
 ReduceByBuilder<KeyT, T> beam_f23667_0(UnaryFunction<InputT, T> valueExtractor)
{    return valueBy(valueExtractor, null);}
public TriggeredByBuilder<KeyT, OutputT> beam_f23676_0(WindowFn<Object, W> windowFn)
{    windowBuilder.windowBy(windowFn);    return this;}
public AccumulationModeBuilder<KeyT, OutputT> beam_f23677_0(Trigger trigger)
{    windowBuilder.triggeredBy(trigger);    return this;}
private UnaryFunction<InputT, ValueT> beam_f23686_0()
{    return (UnaryFunction) UnaryFunction.identity();}
public boolean beam_f23687_0()
{    return reducer == null;}
public Optional<BinaryFunction<ValueT, ValueT, Integer>> beam_f23696_0()
{    return Optional.ofNullable(valueComparator);}
public Optional<TypeDescriptor<ValueT>> beam_f23697_0()
{    return Optional.ofNullable(valueType);}
 WindowByBuilder<ValueT> beam_f23706_0(ReduceByKey.CombineFunctionWithIdentity<ValueT> reduce, TypeDescriptor<ValueT> ignored)
{    return combineBy(reduce.identity(), reduce, reduce.valueDesc());}
 WindowByBuilder<ValueT> beam_f23707_0(ValueT identity, CombinableBinaryFunction<ValueT> reducer)
{    return combineBy(identity, reducer, null);}
public WindowByBuilder<OutputT> beam_f23716_0(BinaryFunction<ValueT, ValueT, Integer> valueComparator)
{    this.valueComparator = requireNonNull(valueComparator);    return this;}
public TriggeredByBuilder<OutputT> beam_f23717_0(WindowFn<Object, T> windowFn)
{    windowBuilder.windowBy(windowFn);    return this;}
public boolean beam_f23726_0()
{    return reducer == null;}
public ReduceFunctor<ValueT, OutputT> beam_f23727_0()
{    return requireNonNull(reducer, "Don't call #getReducer is #isCombineFnStyle() == true");}
 Join.WindowByBuilder<KeyT, OutputT> beam_f23736_0(BinaryFunctor<Optional<LeftT>, RightT, OutputT> joinFunc)
{    return using(joinFunc, null);}
public ByBuilder<FirstT, SecondT> beam_f23737_0(PCollection<FirstT> left, PCollection<SecondT> right)
{    @SuppressWarnings("unchecked")    final Builder<FirstT, SecondT, ?> cast = (Builder) this;    cast.left = requireNonNull(left);    cast.right = requireNonNull(right);    return cast;}
public WindowByBuilder<KeyT> beam_f23746_0(UnaryFunction<InputT, Long> valueExtractor)
{    this.valueExtractor = requireNonNull(valueExtractor);    return this;}
public TriggeredByBuilder<KeyT> beam_f23747_0(WindowFn<Object, W> windowFn)
{    windowBuilder.windowBy(windowFn);    return this;}
public PCollection<KV<KeyT, Long>> beam_f23756_0(PCollectionList<InputT> inputs)
{    return ReduceByKey.named(getName().orElse(null)).of(PCollectionLists.getOnlyElement(inputs)).keyBy(getKeyExtractor()).valueBy(getValueExtractor(), TypeDescriptors.longs()).combineBy(Sums.ofLongs()).applyIf(getWindow().isPresent(), builder -> {        @SuppressWarnings("unchecked")        final ReduceByKey.WindowByInternalBuilder<InputT, KeyT, Long> cast = (ReduceByKey.WindowByInternalBuilder) builder;        return cast.windowBy(getWindow().orElseThrow(() -> new IllegalStateException("Unable to resolve windowing for SumByKey expansion.")));    }).output();}
public static KeyByBuilder<InputT> beam_f23757_0(PCollection<InputT> input)
{    return named(null).of(input);}
public WindowByBuilder<KeyT, ValueT, T> beam_f23766_0(UnaryFunction<InputT, T> scoreExtractor, @Nullable TypeDescriptor<T> scoreType)
{    @SuppressWarnings("unchecked")    final Builder<InputT, KeyT, ValueT, T> cast = (Builder) this;    cast.scoreExtractor = requireNonNull(scoreExtractor);    cast.scoreType = scoreType;    return cast;}
public TriggeredByBuilder<KeyT, ValueT, ScoreT> beam_f23767_0(WindowFn<Object, W> windowFn)
{    windowBuilder.windowBy(windowFn);    return this;}
public Optional<TypeDescriptor<ValueT>> beam_f23776_0()
{    return Optional.ofNullable(valueType);}
public UnaryFunction<InputT, ScoreT> beam_f23777_0()
{    return scoreExtractor;}
 Optional<Window<T>> beam_f23786_0()
{    return Optional.ofNullable(window);}
 void beam_f23787_0(Window<T> window)
{    checkState(this.window == null, "Window is already set.");    this.window = window;}
public static TypeDescriptor<KV<K, V>> beam_f23796_0(TypeDescriptor<K> key, TypeDescriptor<V> value)
{    if (Objects.isNull(key) || Objects.isNull(value)) {        return null;    }    return new TypeDescriptor<KV<K, V>>() {    }.where(new TypeParameter<K>() {    }, key).where(new TypeParameter<V>() {    }, value);}
public static TypeDescriptor<KV<K, V>> beam_f23797_0(Class<K> key, Class<V> value)
{    if (Objects.isNull(key) || Objects.isNull(value)) {        return null;    }    return keyValues(TypeDescriptor.of(key), TypeDescriptor.of(value));}
public T beam_f23806_0(T left, T right)
{    return reduce.apply(left, right);}
public static ReduceByKey.CombineFunctionWithIdentity<Long> beam_f23807_0()
{    return SUMS_OF_LONG;}
public int beam_f23816_0()
{    return Objects.hash(first, second, third);}
public String beam_f23817_0()
{    return "Triple{" + "first=" + first + ", second=" + second + ", third=" + third + '}';}
public static BeamAccumulatorProvider.Factory beam_f23826_0()
{    return INSTANCE;}
public AccumulatorProvider beam_f23827_1()
{    if (isLogged.compareAndSet(false, true)) {            }    return PROVIDER;}
public void beam_f23836_0(ProcessContext context)
{    final KV<K, RightT> element = context.element();    final K key = element.getKey();    final Map<K, Iterable<LeftT>> map = context.sideInput(smallSideCollection);    final Iterable<LeftT> leftValues = map.getOrDefault(key, Collections.singletonList(null));    outCollector.setProcessContext(context);    leftValues.forEach(leftValue -> joiner.apply(leftValue, element.getValue(), outCollector));}
public void beam_f23837_0(ProcessContext context)
{    final KV<K, LeftT> element = context.element();    final K key = element.getKey();    final Map<K, Iterable<RightT>> map = context.sideInput(smallSideCollection);    final Iterable<RightT> rightValues = map.getOrDefault(key, Collections.singletonList(null));    outCollector.setProcessContext(context);    rightValues.forEach(rightValue -> joiner.apply(element.getValue(), rightValue, outCollector));}
public Context beam_f23846_0()
{        throw new UnsupportedOperationException("Not supported.");}
public Counter beam_f23847_0(String name)
{    return accumulators.getCounter(requireNonNull(operatorName, UNSUPPORTED), name);}
public PCollection<OutputT> beam_f23856_0(FlatMap<InputT, OutputT> operator, PCollectionList<InputT> inputs)
{    final AccumulatorProvider accumulators = new LazyAccumulatorProvider(AccumulatorProvider.of(inputs.getPipeline()));    final Mapper<InputT, OutputT> mapper = new Mapper<>(operator.getName().orElse(null), operator.getFunctor(), accumulators, operator.getEventTimeExtractor().orElse(null), operator.getAllowedTimestampSkew());    return PCollectionLists.getOnlyElement(inputs).apply("mapper", ParDo.of(mapper)).setTypeDescriptor(TypeAwareness.orObjects(operator.getOutputType()));}
public void beam_f23857_0(ProcessContext ctx)
{    collector.setProcessContext(ctx);    mapper.apply(ctx.element(), collector);}
public String beam_f23866_0()
{    return "full-join";}
 void beam_f23867_0(Iterable<LeftT> left, Iterable<RightT> right)
{    for (LeftT leftValue : left) {        if (right.iterator().hasNext()) {            for (RightT rightValue : right) {                getJoiner().apply(leftValue, rightValue, getCollector());            }        } else {            getJoiner().apply(leftValue, null, getCollector());        }    }}
public Histogram beam_f23876_0(String namespace, String name)
{    return getAccumulatorProvider().getHistogram(namespace, name);}
public Timer beam_f23877_0(String name)
{    return getAccumulatorProvider().getTimer(name);}
public static GenericTranslatorProvider beam_f23886_0()
{    return GenericTranslatorProvider.newBuilder().register(FlatMap.class, new FlatMapTranslator<>()).register(Union.class, new UnionTranslator<>()).register(ReduceByKey.class, new ReduceByKeyTranslator<>()).register(Join.class, new JoinTranslator<>()).register(op -> op instanceof CompositeOperator, new CompositeOperatorTranslator<>()).build();}
public static Builder beam_f23887_0()
{    return new Builder();}
 Optional<OperatorTranslator<?, ?, OperatorT>> beam_f23896_0(OperatorT operator)
{    if (checkTranslatorSuitableFor(operator)) {        return Optional.of(translator);    } else {        return Optional.empty();    }}
public Optional<OperatorTranslator<InputT, OutputT, OperatorT>> beam_f23897_0(OperatorT operator)
{    for (TranslationDescriptor descriptor : possibleTranslators) {        @SuppressWarnings("unchecked")        Optional<OperatorTranslator<InputT, OutputT, OperatorT>> maybeTranslator = descriptor.getTranslatorWhenSuitable(operator);        if (maybeTranslator.isPresent()) {            return maybeTranslator;        }    }    return Optional.empty();}
private static SerializableFunction<Iterable<InputT>, InputT> beam_f23906_0(ReduceFunctor<InputT, OutputT> reducer, AccumulatorProvider accumulatorProvider, @Nullable String operatorName)
{    @SuppressWarnings("unchecked")    final ReduceFunctor<InputT, InputT> combiner = (ReduceFunctor<InputT, InputT>) reducer;    return (Iterable<InputT> input) -> {        SingleValueCollector<InputT> collector = new SingleValueCollector<>(accumulatorProvider, operatorName);        combiner.apply(StreamSupport.stream(input.spliterator(), false), collector);        return collector.get();    };}
public KV<KeyT, ValueT> beam_f23907_0(InputT in)
{    return KV.of(keyExtractor.apply(in), valueExtractor.apply(in));}
public static TimestampExtractTransform<InputT, OutputT> beam_f23916_0(PCollectionTransform<InputT, OutputT> transform)
{    return new TimestampExtractTransform<>(null, transform);}
public static TimestampExtractTransform<InputT, OutputT> beam_f23917_0(String name, PCollectionTransform<InputT, OutputT> transform)
{    return new TimestampExtractTransform<>(name, transform);}
public static Class<? extends T> beam_f23926_0(String className, Class<T> superType)
{    try {        Class<?> cls = Thread.currentThread().getContextClassLoader().loadClass(className);        if (superType.isAssignableFrom(cls)) {            return (Class<? extends T>) cls;        } else {            throw new IllegalStateException(className + " is not " + superType);        }    } catch (ClassNotFoundException e) {        throw new IllegalStateException(e);    }}
public static void beam_f23927_0(Iterable<T> iterable, IOConsumer<T> consumer) throws IOException
{    IOException firstException = null;    for (T element : iterable) {        try {            consumer.accept(element);        } catch (IOException e) {            if (firstException != null) {                firstException.addSuppressed(e);            } else {                firstException = e;            }        }    }    if (firstException != null) {        throw firstException;    }}
public void beam_f23936_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final TypeDescriptor<String> keyType = TypeDescriptors.strings();    final PCollection<KV<String, Long>> counted = CountByKey.named("CountByKey1").of(dataset).keyBy(s -> s, keyType).output();    final CountByKey count = (CountByKey) TestUtils.getProducer(counted);    assertTrue(count.getKeyType().isPresent());    assertEquals(count.getKeyType().get(), keyType);    assertTrue(count.getOutputType().isPresent());    assertEquals(TypeDescriptors.kvs(keyType, TypeDescriptors.longs()), count.getOutputType().get());}
public void beam_f23937_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final FixedWindows windowing = FixedWindows.of(org.joda.time.Duration.standardHours(1));    final DefaultTrigger trigger = DefaultTrigger.of();    final PCollection<String> uniq = Distinct.named("Distinct1").of(dataset).windowBy(windowing).triggeredBy(trigger).discardingFiredPanes().withAllowedLateness(Duration.millis(1000)).output();    final Distinct distinct = (Distinct) TestUtils.getProducer(uniq);    assertTrue(distinct.getName().isPresent());    assertEquals("Distinct1", distinct.getName().get());    assertTrue(distinct.getWindow().isPresent());    @SuppressWarnings("unchecked")    final WindowDesc<?> windowDesc = WindowDesc.of((Window) distinct.getWindow().get());    assertEquals(windowing, windowDesc.getWindowFn());    assertEquals(trigger, windowDesc.getTrigger());    assertEquals(AccumulationMode.DISCARDING_FIRED_PANES, windowDesc.getAccumulationMode());    assertEquals(Duration.millis(1000), windowDesc.getAllowedLateness());}
public void beam_f23946_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<String> mapped = FlatMap.of(dataset).using((String s, Collector<String> c) -> c.collect(s)).output();    final FlatMap map = (FlatMap) TestUtils.getProducer(mapped);    assertFalse(map.getName().isPresent());}
public void beam_f23947_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<String> mapped = FlatMap.of(dataset).using((String s, Collector<String> c) -> c.collect(s)).eventTimeBy(in -> System.currentTimeMillis(), Duration.millis(100)).output();    final FlatMap map = (FlatMap) TestUtils.getProducer(mapped);    assertEquals(100, map.getAllowedTimestampSkew().getMillis());}
public void beam_f23956_0()
{    final Pipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> left = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> right = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<KV<Integer, String>> joined = Join.named("Join1").of(left, right).by(String::length, String::length).using((String l, String r, Collector<String> c) -> c.collect(l + r)).windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.millis(1000)).output();    final Join join = (Join) TestUtils.getProducer(joined);    assertTrue(join.getWindow().isPresent());    @SuppressWarnings("unchecked")    final WindowDesc<?> windowDesc = WindowDesc.of((Window) join.getWindow().get());    assertEquals(FixedWindows.of(org.joda.time.Duration.standardHours(1)), windowDesc.getWindowFn());    assertEquals(AfterWatermark.pastEndOfWindow(), windowDesc.getTrigger());    assertEquals(AccumulationMode.DISCARDING_FIRED_PANES, windowDesc.getAccumulationMode());    assertEquals(Duration.millis(1000), windowDesc.getAllowedLateness());}
public void beam_f23957_0()
{    final Pipeline pipeline = TestUtils.createTestPipeline();    final PCollection<String> left = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<String> right = TestUtils.createMockDataset(pipeline, TypeDescriptors.strings());    final PCollection<KV<Integer, String>> joined = Join.named("Join1").of(left, right).by(String::length, String::length).using((String l, String r, Collector<String> c) -> c.collect(l + r)).applyIf(true, b -> b.windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(AfterWatermark.pastEndOfWindow()).accumulationMode(AccumulationMode.DISCARDING_FIRED_PANES)).output();    final Join join = (Join) TestUtils.getProducer(joined);    assertTrue(join.getWindow().isPresent());    final Window<?> window = (Window) join.getWindow().get();    assertEquals(FixedWindows.of(org.joda.time.Duration.standardHours(1)), window.getWindowFn());    assertEquals(AfterWatermark.pastEndOfWindow(), WindowDesc.of(window).getTrigger());    assertEquals(AccumulationMode.DISCARDING_FIRED_PANES, WindowDesc.of(window).getAccumulationMode());}
public void beam_f23966_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<Long> reduced = ReduceByKey.named("ReduceByKeyValues").of(dataset).keyBy(s -> s).valueBy(s -> 1L).combineBy(Sums.ofLongs()).outputValues();    final OutputValues outputValues = (OutputValues) TestUtils.getProducer(reduced);    assertTrue(outputValues.getName().isPresent());    assertEquals("ReduceByKeyValues", outputValues.getName().get());}
public void beam_f23967_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<KV<String, Long>> reduced = ReduceByKey.of(dataset).keyBy(s -> s).valueBy(s -> 1L).combineBy(Sums.ofLongs()).output();    final ReduceByKey reduce = (ReduceByKey) TestUtils.getProducer(reduced);    assertFalse(reduce.getName().isPresent());}
public void beam_f23976_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<KV<String, Long>> reduced = ReduceByKey.of(dataset).keyBy(s -> s).valueBy(s -> 1L).combineBy(Sums.ofLongs()).applyIf(false, b -> b.windowBy(FixedWindows.of(Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).accumulationMode(AccumulationMode.DISCARDING_FIRED_PANES)).output();    final ReduceByKey reduce = (ReduceByKey) TestUtils.getProducer(reduced);    assertFalse(reduce.getWindow().isPresent());}
public void beam_f23977_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final TypeDescriptor<String> keyType = TypeDescriptors.strings();    final TypeDescriptor<Long> valueType = TypeDescriptors.longs();    final TypeDescriptor<Long> outputType = TypeDescriptors.longs();    final PCollection<KV<String, Long>> reduced = ReduceByKey.of(dataset).keyBy(s -> s, keyType).valueBy(s -> 1L, valueType).combineBy(Sums.ofLongs()).output();    final ReduceByKey reduce = (ReduceByKey) TestUtils.getProducer(reduced);    TypePropagationAssert.assertOperatorTypeAwareness(reduce, keyType, valueType, outputType);}
public void beam_f23986_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<KV<String, Long>> counted = SumByKey.of(dataset).keyBy(s -> s).valueBy(s -> 1L).applyIf(true, b -> b.windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).accumulationMode(AccumulationMode.DISCARDING_FIRED_PANES)).output();    final SumByKey sum = (SumByKey) TestUtils.getProducer(counted);    assertTrue(sum.getWindow().isPresent());    final Window<?> window = (Window) sum.getWindow().get();    assertEquals(FixedWindows.of(org.joda.time.Duration.standardHours(1)), window.getWindowFn());    assertEquals(DefaultTrigger.of(), WindowDesc.of(window).getTrigger());    assertEquals(AccumulationMode.DISCARDING_FIRED_PANES, WindowDesc.of(window).getAccumulationMode());}
public Optional<OperatorTranslator<InputT, OutputT, OperatorT>> beam_f23987_0(OperatorT operator)
{    return Optional.of((op, inputs) -> PCollection.<OutputT>createPrimitiveOutputInternal(inputs.getPipeline(), inputs.get(0).getWindowingStrategy(), inputs.get(0).isBounded(), null).setTypeDescriptor(TypeAwareness.orObjects(operator.getOutputType())));}
public void beam_f24000_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<Triple<String, Long, Long>> result = TopPerKey.of(dataset).keyBy(s -> s).valueBy(s -> 1L).scoreBy(s -> 1L).windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).accumulationMode(AccumulationMode.DISCARDING_FIRED_PANES).output();    final TopPerKey tpk = (TopPerKey) TestUtils.getProducer(result);    assertTrue(tpk.getWindow().isPresent());    @SuppressWarnings("unchecked")    final WindowDesc<?> windowDesc = WindowDesc.of((Window) tpk.getWindow().get());    assertEquals(FixedWindows.of(org.joda.time.Duration.standardHours(1)), windowDesc.getWindowFn());    assertEquals(DefaultTrigger.of(), windowDesc.getTrigger());    assertEquals(AccumulationMode.DISCARDING_FIRED_PANES, windowDesc.getAccumulationMode());}
public void beam_f24001_0()
{    final PCollection<String> dataset = TestUtils.createMockDataset(TypeDescriptors.strings());    final PCollection<Triple<String, Long, Long>> result = TopPerKey.of(dataset).keyBy(s -> s).valueBy(s -> 1L).scoreBy(s -> 1L).applyIf(true, b -> b.windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).accumulatingFiredPanes()).output();    final TopPerKey tpk = (TopPerKey) TestUtils.getProducer(result);    assertTrue(tpk.getWindow().isPresent());    @SuppressWarnings("unchecked")    final WindowDesc<?> windowDesc = WindowDesc.of((Window) tpk.getWindow().get());    assertEquals(FixedWindows.of(org.joda.time.Duration.standardHours(1)), windowDesc.getWindowFn());    assertEquals(DefaultTrigger.of(), windowDesc.getTrigger());    assertEquals(AccumulationMode.ACCUMULATING_FIRED_PANES, windowDesc.getAccumulationMode());}
public void beam_f24010_0()
{    try {        IOUtils.forEach(Arrays.asList(1, 2, 3), i -> {            throw new IOException("Number: " + i);        });    } catch (Exception e) {                assertEquals(2, e.getSuppressed().length);        assertTrue(e instanceof IOException);        assertEquals("Number: 1", e.getMessage());    }}
public void beam_f24011_0() throws IOException
{    IOUtils.forEach(Stream.of(1, 2, 3), i -> {        if (i == 2) {            throw new IOException("Number: " + i);        }    });}
public void beam_f24020_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 4, 3));    PCollection<String> mappedElements = MapElements.named("Int2Str").of(input).using(String::valueOf).output();    PAssert.that(mappedElements).containsInAnyOrder("1", "2", "4", "3");    pipeline.run();}
public void beam_f24021_0()
{    final PipelineOptions options = PipelineOptionsFactory.create();    Pipeline pipeline = Pipeline.create(options);    PCollection<String> dataset = pipeline.apply(Create.of("a", "x"));    PCollection<String> flatMapped = FlatMap.named("FlatMap1").of(dataset).using((String value, Collector<String> context) -> {        context.getCounter("my-counter").increment();        context.collect(value);    }).output();    PCollection<String> mapped = MapElements.named("MapThem").of(dataset).using((value, context) -> {                context.getCounter("my-counter").increment();        return value.toLowerCase();    }).output();}
public void beam_f24030_0()
{    PCollection<Integer> input = pipeline.apply(Create.of(0, 1, 2, 3, 4, 5)).setTypeDescriptor(TypeDescriptors.integers());        PCollection<String> strings = MapElements.named("int2str").of(input).using(i -> "#" + i).output();        PAssert.that(strings).containsInAnyOrder("#0", "#1", "#2", "#3", "#4", "#5");    pipeline.run();}
public void beam_f24031_0()
{    PCollection<String> words = pipeline.apply(Create.of(asList("Brown", "fox", ".", "")));        PCollection<String> letters = FlatMap.named("str2char").of(words).using((String s, Collector<String> collector) -> {        for (int i = 0; i < s.length(); i++) {            char c = s.charAt(i);            collector.collect(String.valueOf(c));        }    }).output();        PAssert.that(letters).containsInAnyOrder("B", "r", "o", "w", "n", "f", "o", "x", ".");    pipeline.run();}
public void beam_f24040_0()
{    PCollection<String> cats = pipeline.apply("cats", Create.of(asList("cheetah", "cat", "lynx", "jaguar"))).setTypeDescriptor(TypeDescriptors.strings());    PCollection<String> rodents = pipeline.apply("rodents", Create.of("squirrel", "mouse", "rat", "lemming", "beaver")).setTypeDescriptor(TypeDescriptors.strings());            PCollection<String> animals = Union.named("to-animals").of(cats, rodents).output();            PAssert.that(animals).containsInAnyOrder("cheetah", "cat", "lynx", "jaguar", "squirrel", "mouse", "rat", "lemming", "beaver");    pipeline.run();}
public void beam_f24041_0()
{    PCollection<SomeEventObject> events = pipeline.apply(Create.of(asList(new SomeEventObject(0), new SomeEventObject(1), new SomeEventObject(2), new SomeEventObject(3), new SomeEventObject(4))));            PCollection<SomeEventObject> timeStampedEvents = AssignEventTime.named("extract-event-time").of(events).using(SomeEventObject::getEventTimeInMillis).output();        pipeline.run();}
 List<T> beam_f24050_0()
{    throw new UnsupportedOperationException("Override either `getUnorderedOutput()`, or `validate`");}
 void beam_f24051_0(PCollection<T> outputs) throws AssertionError
{    PAssert.that(outputs).containsInAnyOrder(getUnorderedOutput());}
public void beam_f24061_0(long duration, TimeUnit unit)
{    hist.add(unit.toNanos(duration), 1);}
public void beam_f24062_0(Duration duration)
{    add(duration.toNanos(), TimeUnit.NANOSECONDS);}
 Map<String, V> beam_f24071_0(Class<T> type)
{    HashMap<String, V> m = new HashMap<>();    accs.forEach((name, accumulator) -> {        if (type.isAssignableFrom(accumulator.getClass())) {            @SuppressWarnings("unchecked")            T acc = (T) accumulator;            m.put(name, acc.getSnapshot());        }    });    return m;}
public static Factory beam_f24072_0()
{    return INSTANCE;}
public void beam_f24081_0()
{    execute(new TestCase<Integer, Long, KV<Integer, String>>() {        @Override        protected PCollection<KV<Integer, String>> getOutput(PCollection<Integer> left, PCollection<Long> right) {            return LeftJoin.named("broadcast-leftJoin").of(left, MapElements.of(right).using(i -> i).output()).by(e -> e, e -> (int) (e % 10)).using((Integer l, Optional<Long> r, Collector<String> c) -> c.collect(l + "+" + r.orElse(null))).output();        }        @Override        protected List<Integer> getLeftInput() {            return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1);        }        @Override        protected TypeDescriptor<Integer> getLeftInputType() {            return TypeDescriptors.integers();        }        @Override        protected TypeDescriptor<Long> getRightInputType() {            return TypeDescriptors.longs();        }        @Override        protected List<Long> getRightInput() {            return Arrays.asList(11L, 12L, 13L, 14L, 15L, 11L);        }        @Override        public List<KV<Integer, String>> getUnorderedOutput() {            return Arrays.asList(KV.of(0, "0+null"), KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(1, "1+11"), KV.of(1, "1+11"), KV.of(3, "3+13"), KV.of(3, "3+13"), KV.of(1, "1+11"), KV.of(1, "1+11"));        }    });}
protected PCollection<KV<Integer, String>> beam_f24082_0(PCollection<Integer> left, PCollection<Long> right)
{    return LeftJoin.named("broadcast-leftJoin").of(left, MapElements.of(right).using(i -> i).output()).by(e -> e, e -> (int) (e % 10)).using((Integer l, Optional<Long> r, Collector<String> c) -> c.collect(l + "+" + r.orElse(null))).output();}
protected TypeDescriptor<Integer> beam_f24091_0()
{    return TypeDescriptors.integers();}
protected List<Long> beam_f24092_0()
{    return Arrays.asList(11L, 12L, 13L, 14L, 15L);}
public List<KV<String, String>> beam_f24101_0()
{    return Arrays.asList(KV.of(sameHashCodeKey1, "FB+1"), KV.of(sameHashCodeKey2, "Ea+2"), KV.of("keyWithoutRightSide", "keyWithoutRightSide+null"));}
public void beam_f24102_0()
{    execute(new AbstractTestCase<Integer, KV<Integer, Long>>() {        @Override        protected PCollection<KV<Integer, Long>> getOutput(PCollection<Integer> input) {                        input = AssignEventTime.of(input).using(e -> 0).output();            return CountByKey.of(input).keyBy(e -> e).windowBy(FixedWindows.of(Duration.standardSeconds(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();        }        @Override        protected List<Integer> getInput() {            return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 10, 9, 8, 7, 6, 5, 4);        }        @Override        protected TypeDescriptor<Integer> getInputType() {            return TypeDescriptors.integers();        }        @Override        public List<KV<Integer, Long>> getUnorderedOutput() {            return Arrays.asList(KV.of(2, 1L), KV.of(4, 2L), KV.of(6, 2L), KV.of(8, 1L), KV.of(10, 1L), KV.of(1, 1L), KV.of(3, 1L), KV.of(5, 2L), KV.of(7, 2L), KV.of(9, 1L));        }    });}
public List<KV<Integer, Long>> beam_f24111_0()
{    return Arrays.asList(KV.of(1, 2L), KV.of(2, 1L), KV.of(3, 2L), KV.of(4, 1L), KV.of(5, 3L), KV.of(5, 2L), KV.of(6, 1L), KV.of(7, 2L), KV.of(10, 1L), KV.of(10, 1L), KV.of(9, 2L), KV.of(9, 2L));}
public void beam_f24112_0()
{    execute(new AbstractTestCase<Integer, Integer>() {        @Override        public List<Integer> getUnorderedOutput() {            return Arrays.asList(1, 2, 3);        }        @Override        protected PCollection<Integer> getOutput(PCollection<Integer> input) {            return Distinct.of(input).output();        }        @Override        protected List<Integer> getInput() {            return Arrays.asList(1, 2, 3, 3, 2, 1);        }        @Override        protected TypeDescriptor<Integer> getInputType() {            return TypeDescriptors.integers();        }    });}
protected TypeDescriptor<KV<Integer, Long>> beam_f24121_0()
{    return TypeDescriptors.kvs(TypeDescriptors.integers(), TypeDescriptors.longs());}
public void beam_f24122_0()
{    execute(new AbstractTestCase<KV<Integer, Long>, Integer>() {        @Override        public List<Integer> getUnorderedOutput() {            return Arrays.asList(2, 1, 3);        }        @Override        protected PCollection<Integer> getOutput(PCollection<KV<Integer, Long>> input) {            input = AssignEventTime.of(input).using(KV::getValue).output();            PCollection<KV<Integer, Long>> distinct = Distinct.of(input).projected(KV::getKey).windowBy(FixedWindows.of(org.joda.time.Duration.standardSeconds(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();            return MapElements.of(distinct).using(KV::getKey).output();        }        @Override        protected List<KV<Integer, Long>> getInput() {            List<KV<Integer, Long>> first = asTimedList(100, 1, 2, 3, 3, 2, 1);            first.addAll(asTimedList(100, 1, 2, 3, 3, 2, 1));            return first;        }        @Override        protected TypeDescriptor<KV<Integer, Long>> getInputType() {            return TypeDescriptors.kvs(TypeDescriptors.integers(), TypeDescriptors.longs());        }    });}
protected TypeDescriptor<KV<String, Long>> beam_f24131_0()
{    return TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.longs());}
public void beam_f24132_0()
{    execute(new AbstractTestCase<KV<String, Long>, String>() {        @Override        public List<String> getUnorderedOutput() {            return Arrays.asList("2.", "1.", "3.");        }        @Override        protected PCollection<String> getOutput(PCollection<KV<String, Long>> input) {            input = AssignEventTime.of(input).using(KV::getValue).output();            PCollection<KV<String, Long>> distinct = Distinct.of(input).projected(in -> in.getKey().substring(0, 1), Distinct.SelectionPolicy.NEWEST, TypeDescriptors.strings()).windowBy(FixedWindows.of(org.joda.time.Duration.standardSeconds(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();            return MapElements.of(distinct).using(KV::getKey).output();        }        @Override        protected List<KV<String, Long>> getInput() {            return asTimedList(100, "1", "2", "3", "3.", "2.", "1.");        }        @Override        protected TypeDescriptor<KV<String, Long>> getInputType() {            return TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.longs());        }    });}
protected TypeDescriptor<Integer> beam_f24141_0()
{    return TypeDescriptors.integers();}
public List<Integer> beam_f24142_0()
{    return Arrays.asList(2, 4, 6, 8, 10, 12, 14);}
protected PCollection<Integer> beam_f24151_0(PCollection<Integer> input)
{    return FlatMap.named("test").of(input).using((UnaryFunctor<Integer, Integer>) (elem, collector) -> {        collector.getCounter("input").increment();        collector.getCounter("sum").increment(elem);        collector.collect(elem * elem);    }).output();}
public List<Integer> beam_f24152_0()
{    return Arrays.asList(1, 4, 9, 16, 25, 36, 0, 100, 400);}
public void beam_f24161_0()
{    execute(new JoinTestCase<Integer, String, KV<Integer, String>>() {        @Override        protected PCollection<KV<Integer, String>> getOutput(PCollection<Integer> left, PCollection<String> right) {            return FullJoin.of(left, right).by(le -> le, String::length).using((Optional<Integer> l, Optional<String> r, Collector<String> c) -> c.collect(l.orElse(null) + "+" + r.orElse(null))).output();        }        @Override        protected List<Integer> getLeftInput() {            return Arrays.asList(1, 2, 3, 0, 4, 3, 1);        }        @Override        protected TypeDescriptor<Integer> getLeftInputType() {            return TypeDescriptors.integers();        }        @Override        protected List<String> getRightInput() {            return Arrays.asList("mouse", "rat", "cat", "X", "duck");        }        @Override        protected TypeDescriptor<String> getRightInputType() {            return TypeDescriptors.strings();        }        @Override        public List<KV<Integer, String>> getUnorderedOutput() {            return Arrays.asList(KV.of(1, "1+X"), KV.of(2, "2+null"), KV.of(3, "3+cat"), KV.of(3, "3+rat"), KV.of(0, "0+null"), KV.of(4, "4+duck"), KV.of(3, "3+cat"), KV.of(3, "3+rat"), KV.of(1, "1+X"), KV.of(5, "null+mouse"));        }    });}
protected PCollection<KV<Integer, String>> beam_f24162_0(PCollection<Integer> left, PCollection<String> right)
{    return FullJoin.of(left, right).by(le -> le, String::length).using((Optional<Integer> l, Optional<String> r, Collector<String> c) -> c.collect(l.orElse(null) + "+" + r.orElse(null))).output();}
protected TypeDescriptor<Integer> beam_f24171_0()
{    return TypeDescriptors.integers();}
protected List<Long> beam_f24172_0()
{    return Arrays.asList(11L, 12L, 13L, 14L, 15L);}
public List<KV<Integer, String>> beam_f24181_0()
{    return Arrays.asList(KV.of(0, "0+null"), KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(1, "1+11"), KV.of(1, "1+11"), KV.of(3, "3+13"), KV.of(3, "3+13"));}
public void beam_f24182_0()
{    execute(new JoinTestCase<Integer, Long, KV<Integer, String>>() {        @Override        protected PCollection<KV<Integer, String>> getOutput(PCollection<Integer> left, PCollection<Long> right) {            return RightJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Optional<Integer> l, Long r, Collector<String> c) -> c.collect(l.orElse(null) + "+" + r)).output();        }        @Override        protected List<Integer> getLeftInput() {            return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1);        }        @Override        protected TypeDescriptor<Integer> getLeftInputType() {            return TypeDescriptors.integers();        }        @Override        protected List<Long> getRightInput() {            return Arrays.asList(11L, 12L, 13L, 14L, 15L);        }        @Override        protected TypeDescriptor<Long> getRightInputType() {            return TypeDescriptors.longs();        }        @Override        public List<KV<Integer, String>> getUnorderedOutput() {            return Arrays.asList(KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(1, "1+11"), KV.of(1, "1+11"), KV.of(3, "3+13"), KV.of(3, "3+13"), KV.of(5, "null+15"));        }    });}
protected List<Integer> beam_f24191_0()
{    return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1);}
protected TypeDescriptor<Integer> beam_f24192_0()
{    return TypeDescriptors.integers();}
protected TypeDescriptor<Long> beam_f24201_0()
{    return TypeDescriptors.longs();}
public List<KV<Integer, String>> beam_f24202_0()
{    return Arrays.asList(KV.of(0, "0+null"), KV.of(2, "2+12"), KV.of(2, "2+12"), KV.of(4, "4+14"), KV.of(6, "6+16"), KV.of(8, "null+18"), KV.of(1, "1+null"), KV.of(1, "1+null"), KV.of(1, "null+11"), KV.of(3, "3+null"), KV.of(3, "3+null"), KV.of(3, "null+13"), KV.of(5, "5+null"), KV.of(5, "null+15"), KV.of(7, "null+17"));}
protected PCollection<KV<Integer, String>> beam_f24211_0(PCollection<Integer> left, PCollection<Long> right)
{    @SuppressWarnings("unchecked")    final WindowFn<Object, BoundedWindow> evenOddWindowFn = (WindowFn) new EvenOddWindowFn();    return RightJoin.of(left, right).by(e -> e, e -> (int) (e % 10)).using((Optional<Integer> l, Long r, Collector<String> c) -> {        c.collect(l.orElse(null) + "+" + r);    }).windowBy(evenOddWindowFn).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();}
protected List<Integer> beam_f24212_0()
{    return Arrays.asList(1, 2, 3, 0, 4, 3, 2, 1, 5, 6);}
protected TypeDescriptor<KV<String, Long>> beam_f24221_0()
{    return TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.longs());}
protected PCollection<KV<String, String>> beam_f24222_0(PCollection<KV<String, Long>> left, PCollection<KV<String, Long>> right)
{    left = AssignEventTime.named("assign-event-time-left").of(left).using(KV::getValue).output();    right = AssignEventTime.named("assign-event-time-right").of(right).using(KV::getValue).output();    final PCollection<KV<String, KV<String, String>>> joined = Join.of(left, right).by(p -> "", p -> "").using((KV<String, Long> l, KV<String, Long> r, Collector<KV<String, String>> c) -> c.collect(KV.of(l.getKey(), r.getKey()))).windowBy(Sessions.withGapDuration(org.joda.time.Duration.millis(10))).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();    return MapElements.of(joined).using(KV::getValue).output();}
public PCollection<OutputT> beam_f24231_0(Pipeline pipeline)
{    final PCollection<LeftT> left = pipeline.apply("left-input", Create.of(getLeftInput())).setTypeDescriptor(getLeftInputType());    final PCollection<RightT> right = pipeline.apply("right-input", Create.of(getRightInput())).setTypeDescriptor(getRightInputType());    return getOutput(left, right);}
public Collection<BoundedWindow> beam_f24232_0(AssignContext c)
{    final KV<Integer, Number> element = c.element();    final Number value = element.getValue();    if (value == null) {        return Collections.singleton(EVEN_WIN);    }    final NamedGlobalWindow win;    if (value.longValue() % 2 == 0) {        win = EVEN_WIN;    } else {        win = new NamedGlobalWindow("win: " + value.longValue());    }    return Collections.singleton(win);}
protected PCollection<String> beam_f24242_0(PCollection<Integer> input)
{    return MapElements.of(input).using((UnaryFunction<Integer, String>) String::valueOf).output();}
protected List<Integer> beam_f24243_0()
{    return Arrays.asList(1, 2, 3, 4, 5, 6, 7);}
public void beam_f24252_0()
{    execute(new AbstractTestCase<Integer, KV<Integer, Set<Integer>>>() {        @Override        protected List<Integer> getInput() {            return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 9);        }        @Override        protected TypeDescriptor<Integer> getInputType() {            return TypeDescriptors.integers();        }        @Override        protected PCollection<KV<Integer, Set<Integer>>> getOutput(PCollection<Integer> input) {            return ReduceByKey.of(input).keyBy(e -> e % 2).valueBy(e -> e).reduceBy(s -> s.collect(Collectors.toSet())).windowBy(new GlobalWindows()).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().output();        }        @Override        public List<KV<Integer, Set<Integer>>> getUnorderedOutput() {            return Arrays.asList(KV.of(0, Sets.newHashSet(2, 4, 6)), KV.of(1, Sets.newHashSet(1, 3, 5, 7, 9)));        }    });}
protected List<Integer> beam_f24253_0()
{    return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 9);}
public void beam_f24262_0()
{    execute(new AbstractTestCase<KV<Integer, Long>, KV<Integer, Long>>() {        @Override        protected PCollection<KV<Integer, Long>> getOutput(PCollection<KV<Integer, Long>> input) {            input = AssignEventTime.of(input).using(KV::getValue).output();            return ReduceByKey.of(input).keyBy(KV::getKey).valueBy(e -> 1L).combineBy(Sums.ofLongs()).windowBy(FixedWindows.of(org.joda.time.Duration.standardSeconds(1))).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();        }        @Override        protected List<KV<Integer, Long>> getInput() {            return Arrays.asList(KV.of(1, 300L), KV.of(2, 600L), KV.of(3, 900L), KV.of(2, 1300L), KV.of(3, 1600L), KV.of(1, 1900L), KV.of(3, 2300L), KV.of(2, 2600L), KV.of(1, 2900L), KV.of(2, 3300L), KV.of(2, 300L), KV.of(4, 600L), KV.of(3, 900L), KV.of(4, 1300L), KV.of(2, 1600L), KV.of(3, 1900L), KV.of(4, 2300L), KV.of(1, 2600L), KV.of(3, 2900L), KV.of(4, 3300L), KV.of(3, 3600L));        }        @Override        protected TypeDescriptor<KV<Integer, Long>> getInputType() {            return TypeDescriptors.kvs(TypeDescriptors.integers(), TypeDescriptors.longs());        }        @Override        public List<KV<Integer, Long>> getUnorderedOutput() {            return Arrays.asList(KV.of(2, 2L),             KV.of(4, 1L), KV.of(2, 2L),             KV.of(4, 1L), KV.of(2, 1L),             KV.of(4, 1L), KV.of(2, 1L),             KV.of(4, 1L), KV.of(1, 1L),             KV.of(3, 2L), KV.of(1, 1L),             KV.of(3, 2L), KV.of(1, 2L),             KV.of(3, 2L),             KV.of(3, 1L));        }    });}
protected PCollection<KV<Integer, Long>> beam_f24263_0(PCollection<KV<Integer, Long>> input)
{    input = AssignEventTime.of(input).using(KV::getValue).output();    return ReduceByKey.of(input).keyBy(KV::getKey).valueBy(e -> 1L).combineBy(Sums.ofLongs()).windowBy(FixedWindows.of(org.joda.time.Duration.standardSeconds(1))).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();}
public void beam_f24272_0()
{    execute(new AbstractTestCase<String, KV<String, Long>>() {        @Override        protected List<String> getInput() {            String[] words = "one two three four one two three four one two three one two one".split(" ");            return Arrays.asList(words);        }        @Override        protected TypeDescriptor<String> getInputType() {            return TypeDescriptors.strings();        }        @Override        public List<KV<String, Long>> getUnorderedOutput() {            return Arrays.asList(KV.of("one", 5L), KV.of("two", 4L), KV.of("three", 3L), KV.of("four", 2L));        }        @Override        protected PCollection<KV<String, Long>> getOutput(PCollection<String> input) {            return ReduceByKey.of(input).keyBy(e -> e, TypeDescriptor.of(String.class)).valueBy(e -> 1L, TypeDescriptor.of(Long.class)).combineBy(Sums.ofLongs()).output();        }    });}
protected List<String> beam_f24273_0()
{    String[] words = "one two three four one two three four one two three one two one".split(" ");    return Arrays.asList(words);}
public void beam_f24282_0()
{    execute(new AbstractTestCase<KV<String, Long>, KV<String, Long>>() {        @Override        protected List<KV<String, Long>> getInput() {            return Arrays.asList(KV.of("a", 20L), KV.of("c", 3_000L), KV.of("b", 10L), KV.of("b", 100L), KV.of("a", 4_000L), KV.of("c", 300L), KV.of("b", 1_000L), KV.of("b", 50_000L), KV.of("a", 100_000L), KV.of("a", 800L), KV.of("a", 80L));        }        @Override        protected TypeDescriptor<KV<String, Long>> getInputType() {            return TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.longs());        }        @Override        protected PCollection<KV<String, Long>> getOutput(PCollection<KV<String, Long>> input) {            return ReduceByKey.of(input).keyBy(KV::getKey).valueBy(KV::getValue).combineBy(Sums.ofLongs()).windowBy(new MergingByBucketSizeWindowFn<>(3)).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();        }        @SuppressWarnings("unchecked")        @Override        public List<KV<String, Long>> getUnorderedOutput() {            return Arrays.asList(KV.of("a", 880L), KV.of("a", 104_020L), KV.of("b", 1_110L), KV.of("b", 50_000L), KV.of("c", 3_300L));        }    });}
protected List<KV<String, Long>> beam_f24283_0()
{    return Arrays.asList(KV.of("a", 20L), KV.of("c", 3_000L), KV.of("b", 10L), KV.of("b", 100L), KV.of("a", 4_000L), KV.of("c", 300L), KV.of("b", 1_000L), KV.of("b", 50_000L), KV.of("a", 100_000L), KV.of("a", 800L), KV.of("a", 80L));}
public void beam_f24292_0()
{    execute(new AbstractTestCase<Integer, KV<Integer, Integer>>() {        @Override        protected List<Integer> getInput() {            return Arrays.asList(1, 2, 3, 4, 5);        }        @Override        protected TypeDescriptor<Integer> getInputType() {            return TypeDescriptors.integers();        }        @Override        protected PCollection<KV<Integer, Integer>> getOutput(PCollection<Integer> input) {            return ReduceByKey.named("test").of(input).keyBy(e -> e % 2).valueBy(e -> e).reduceBy(Fold.of(0, (Integer a, Integer b, Collector<Integer> ctx) -> {                if (b % 2 == 0) {                    ctx.getCounter("evens").increment();                } else {                    ctx.getCounter("odds").increment();                }                ctx.collect(a + b);            })).windowBy(new GlobalWindows()).triggeredBy(AfterWatermark.pastEndOfWindow()).discardingFiredPanes().output();        }        @SuppressWarnings("unchecked")        @Override        public List<KV<Integer, Integer>> getUnorderedOutput() {            return Arrays.asList(KV.of(1, 9), KV.of(0, 6));        }        @Override        public void validateAccumulators(SnapshotProvider snapshots) {            Map<String, Long> counters = snapshots.getCounterSnapshots();            assertEquals(Long.valueOf(2), counters.get("evens"));            assertEquals(Long.valueOf(3), counters.get("odds"));        }    });}
protected List<Integer> beam_f24293_0()
{    return Arrays.asList(1, 2, 3, 4, 5);}
public List<KV<Integer, Integer>> beam_f24302_0()
{    return Arrays.asList(KV.of(0, 3), KV.of(1, 5));}
public Collection<CountWindow> beam_f24303_0(AssignContext c) throws Exception
{    Number element = c.element();    return Collections.singleton(new CountWindow(element.longValue() / 4));}
public int beam_f24313_0()
{    return Integer.hashCode(id);}
public boolean beam_f24314_0(Object obj)
{    return obj instanceof UniqueWindow && this.id == ((UniqueWindow) obj).id;}
public String beam_f24323_0()
{    return str;}
public void beam_f24324_0()
{    execute(new AbstractTestCase<Integer, Integer>() {        @Override        protected PCollection<Integer> getOutput(PCollection<Integer> input) {            PCollection<Integer> withEventTime = AssignEventTime.of(input).using(i -> 1000L * i).output();            return ReduceWindow.of(withEventTime).combineBy(Sums.ofInts()).windowBy(FixedWindows.of(org.joda.time.Duration.standardHours(1))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();        }        @Override        protected List<Integer> getInput() {            return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);        }        @Override        protected TypeDescriptor<Integer> getInputType() {            return TypeDescriptors.integers();        }        @Override        public List<Integer> getUnorderedOutput() {            return Collections.singletonList(55);        }    });}
public List<Integer> beam_f24333_0()
{    return Collections.singletonList(55);}
public void beam_f24334_0()
{    execute(new AbstractTestCase<Integer, Integer>() {        @Override        protected PCollection<Integer> getOutput(PCollection<Integer> input) {            PCollection<Integer> withEventTime = AssignEventTime.of(input).using(i -> 1000L * i).output();            PCollection<Integer> first = ReduceWindow.named("first-reduce").of(withEventTime).combineBy(Sums.ofInts()).windowBy(FixedWindows.of(org.joda.time.Duration.standardSeconds(5))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().output();            return ReduceWindow.named("second-reduce").of(first).combineBy(Sums.ofInts()).output();        }        @Override        protected List<Integer> getInput() {            return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100);        }        @Override        protected TypeDescriptor<Integer> getInputType() {            return TypeDescriptors.integers();        }        @Override        public List<Integer> getUnorderedOutput() {            return Arrays.asList(10, 35, 10, 100);        }    });}
public List<KV<Integer, Long>> beam_f24343_0()
{    return Arrays.asList(KV.of(0, 20L), KV.of(1, 25L));}
public void beam_f24344_0()
{    execute(new AbstractTestCase<Item, Triple<String, String, Integer>>() {        @Override        protected PCollection<Triple<String, String, Integer>> getOutput(PCollection<Item> input) {            final PCollection<Item> timestampedElements = AssignEventTime.of(input).using(Item::getTimestamp).output();            return TopPerKey.of(timestampedElements).keyBy(Item::getKey).valueBy(Item::getValue).scoreBy(Item::getScore).windowBy(FixedWindows.of(Duration.millis(10))).triggeredBy(DefaultTrigger.of()).discardingFiredPanes().withAllowedLateness(Duration.ZERO).output();        }        @Override        public List<Triple<String, String, Integer>> getUnorderedOutput() {            return asList(Triple.of("one", "one-999", 999), Triple.of("two", "two", 10), Triple.of("three", "3-three", 2));        }        @Override        protected List<Item> getInput() {            return asList(new Item("one", "one-ZZZ-1", 1, 0L), new Item("one", "one-ZZZ-2", 2, 1L), new Item("one", "one-3", 3, 2L), new Item("one", "one-999", 999, 3L), new Item("two", "two", 10, 4L), new Item("three", "1-three", 1, 5L), new Item("three", "2-three", 0, 6L), new Item("one", "one-XXX-100", 100, 7L), new Item("three", "3-three", 2, 8L));        }        @Override        protected TypeDescriptor<Item> getInputType() {            return new TypeDescriptor<Item>() {            };        }    });}
protected TypeDescriptor<Item> beam_f24353_0()
{    return new TypeDescriptor<Item>() {    };}
 String beam_f24354_0()
{    return key;}
public List<Integer> beam_f24363_0()
{    return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12);}
public void beam_f24364_0()
{    execute(new TestCase<Integer>() {        @Override        public PCollection<Integer> getOutput(Pipeline pipeline) {            final PCollection<Integer> first = createDataset(pipeline, 1, 2, 3, 4, 5, 6);            final PCollection<Integer> second = createDataset(pipeline, 7, 8, 9, 10, 11, 12);            final PCollection<Integer> third = createDataset(pipeline, 13, 14, 15, 16, 17, 18);            return Union.of(first, second, third).output();        }        @Override        public List<Integer> getUnorderedOutput() {            return Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18);        }    });}
private void beam_f24373_0(MetricQueryResults metrics, String counterName2, String stepName2)
{    MatcherAssert.assertThat(metrics.getCounters(), Matchers.hasItem(metricsResult(stepName2, counterName2, stepName2, 2L, true)));    MatcherAssert.assertThat(metrics.getDistributions(), Matchers.hasItem(metricsResult(stepName2, counterName2, stepName2, DistributionResult.create(6, 2, 2, 4), true)));}
private void beam_f24374_0(MetricQueryResults metrics, String counterName2, String stepName3)
{    MatcherAssert.assertThat(metrics.getCounters(), Matchers.hasItem(metricsResult(stepName3, counterName2, stepName3, 6L, true)));    MatcherAssert.assertThat(metrics.getDistributions(), Matchers.hasItem(metricsResult(stepName3, counterName2, stepName3, DistributionResult.create(12, 4, 2, 4), true)));}
 static TestProvider beam_f24383_0(String translatorId)
{    return new TestProvider(new TestOpTranslator(translatorId, false));}
public Optional<OperatorTranslator<InputT, OutputT, OperatorT>> beam_f24384_0(OperatorT operator)
{    if (operator.getClass().equals(TestOperator.class)) {        @SuppressWarnings("unchecked")        OperatorTranslator<InputT, OutputT, OperatorT> opTranslator = (OperatorTranslator<InputT, OutputT, OperatorT>) this.opTranslator;        return Optional.of(opTranslator);    }    return Optional.empty();}
public PCollection<Void> beam_f24393_0(TestOperator operator, PCollectionList<Void> inputs)
{    throw new IllegalStateException("Not meant to actually translate something.");}
public boolean beam_f24394_0(TestOperator operator)
{    return canTranslate;}
 static void beam_f24403_0(String translatorId, Optional<OperatorTranslator<Void, Void, TestOperator>> maybeTranslator, Class<? extends OperatorTranslator> translatorClass)
{    assertNotNull(maybeTranslator);    assertTrue(maybeTranslator.isPresent());    OperatorTranslator<Void, Void, TestOperator> translator = maybeTranslator.get();    assertEquals(translator.getClass(), translatorClass);    Named namedTranslator = (Named) translator;    assertEquals(translatorId, namedTranslator.getName());}
public void beam_f24404_0()
{    final AccumulatorProvider accumulators = accFactory.create();    Counter counter = accumulators.getCounter(TEST_COUNTER_NAME);    Assert.assertNotNull(counter);    counter.increment();    counter.increment(2);    Map<String, Long> counterSnapshots = accFactory.getCounterSnapshots();    long counterValue = counterSnapshots.get(TEST_COUNTER_NAME);    Assert.assertEquals(3L, counterValue);    Histogram histogram = accumulators.getHistogram(TEST_HISTOGRAM_NAME);    Assert.assertNotNull(histogram);    histogram.add(1);    histogram.add(2, 2);    Map<String, Map<Long, Long>> histogramSnapshots = accFactory.getHistogramSnapshots();    Map<Long, Long> histogramValue = histogramSnapshots.get(TEST_HISTOGRAM_NAME);    long numOfValuesOfOne = histogramValue.get(1L);    Assert.assertEquals(1L, numOfValuesOfOne);    long numOfValuesOfTwo = histogramValue.get(2L);    Assert.assertEquals(2L, numOfValuesOfTwo);}
public static GcpCredentialFactory beam_f24413_0(PipelineOptions options)
{    return INSTANCE;}
public Credentials beam_f24414_0()
{    try {        return GoogleCredentials.getApplicationDefault().createScoped(SCOPES);    } catch (IOException e) {                return null;    }}
public static void beam_f24424_0()
{    throw new RuntimeException(NULL_CREDENTIAL_REASON);}
public String beam_f24425_1(PipelineOptions options)
{    try {        File configFile;        if (getEnvironment().containsKey("CLOUDSDK_CONFIG")) {            configFile = new File(getEnvironment().get("CLOUDSDK_CONFIG"), "properties");        } else if (isWindows() && getEnvironment().containsKey("APPDATA")) {            configFile = new File(getEnvironment().get("APPDATA"), "gcloud/properties");        } else {                        configFile = new File(System.getProperty("user.home"), ".config/gcloud/configurations/config_default");            if (!configFile.exists()) {                                configFile = new File(System.getProperty("user.home"), ".config/gcloud/properties");            }        }        String section = null;        Pattern projectPattern = Pattern.compile("^project\\s*=\\s*(.*)$");        Pattern sectionPattern = Pattern.compile("^\\[(.*)\\]$");        for (String line : Files.readLines(configFile, StandardCharsets.UTF_8)) {            line = line.trim();            if (line.isEmpty() || line.startsWith(";")) {                continue;            }            Matcher matcher = sectionPattern.matcher(line);            if (matcher.matches()) {                section = matcher.group(1);            } else if (section == null || "core".equals(section)) {                matcher = projectPattern.matcher(line);                if (matcher.matches()) {                    String project = matcher.group(1).trim();                                        return project;                }            }        }    } catch (IOException expected) {            }        return null;}
 static String beam_f24434_0(String zone)
{    String[] zoneParts = zone.split("-", -1);    checkArgument(zoneParts.length >= 2, "Invalid zone provided: %s", zone);    return zoneParts[0] + "-" + zoneParts[1];}
 static CloudResourceManager.Builder beam_f24435_0(CloudResourceManagerOptions options)
{    Credentials credentials = options.getGcpCredential();    if (credentials == null) {        NullCredentialInitializer.throwNullCredentialException();    }    return new CloudResourceManager.Builder(Transport.getTransport(), Transport.getJsonFactory(), chainHttpRequestInitializer(credentials,     new RetryHttpRequestInitializer(ImmutableList.of(404)))).setApplicationName(options.getAppName()).setGoogleClientRequestInitializer(options.getGoogleApiTrace());}
protected List<MatchResult> beam_f24444_0(List<String> specs) throws IOException
{    List<GcsPath> gcsPaths = toGcsPaths(specs);    List<GcsPath> globs = Lists.newArrayList();    List<GcsPath> nonGlobs = Lists.newArrayList();    List<Boolean> isGlobBooleans = Lists.newArrayList();    for (GcsPath path : gcsPaths) {        if (GcsUtil.isWildcard(path)) {            globs.add(path);            isGlobBooleans.add(true);        } else {            nonGlobs.add(path);            isGlobBooleans.add(false);        }    }    Iterator<MatchResult> globsMatchResults = matchGlobs(globs).iterator();    Iterator<MatchResult> nonGlobsMatchResults = matchNonGlobs(nonGlobs).iterator();    ImmutableList.Builder<MatchResult> ret = ImmutableList.builder();    for (Boolean isGlob : isGlobBooleans) {        if (isGlob) {            checkState(globsMatchResults.hasNext(), "Expect globsMatchResults has next: %s", globs);            ret.add(globsMatchResults.next());        } else {            checkState(nonGlobsMatchResults.hasNext(), "Expect nonGlobsMatchResults has next: %s", nonGlobs);            ret.add(nonGlobsMatchResults.next());        }    }    checkState(!globsMatchResults.hasNext(), "Expect no more elements in globsMatchResults.");    checkState(!nonGlobsMatchResults.hasNext(), "Expect no more elements in nonGlobsMatchResults.");    return ret.build();}
protected WritableByteChannel beam_f24445_0(GcsResourceId resourceId, CreateOptions createOptions) throws IOException
{    if (createOptions instanceof GcsCreateOptions) {        return options.getGcsUtil().create(resourceId.getGcsPath(), createOptions.mimeType(), ((GcsCreateOptions) createOptions).gcsUploadBufferSizeBytes());    } else {        return options.getGcsUtil().create(resourceId.getGcsPath(), createOptions.mimeType());    }}
 List<MatchResult> beam_f24454_0(List<GcsPath> gcsPaths) throws IOException
{    List<StorageObjectOrIOException> results = options.getGcsUtil().getObjects(gcsPaths);    ImmutableList.Builder<MatchResult> ret = ImmutableList.builder();    for (StorageObjectOrIOException result : results) {        ret.add(toMatchResult(result));    }    return ret.build();}
private MatchResult beam_f24455_0(StorageObjectOrIOException objectOrException)
{    @Nullable    IOException exception = objectOrException.ioException();    if (exception instanceof FileNotFoundException) {        return MatchResult.create(Status.NOT_FOUND, exception);    } else if (exception != null) {        return MatchResult.create(Status.ERROR, exception);    } else {        StorageObject object = objectOrException.storageObject();                assert object != null;        return MatchResult.create(Status.OK, ImmutableList.of(toMetadata(object)));    }}
public String beam_f24464_0(String path)
{    GcsPath gcsPath = getGcsPath(path);    checkArgument(gcsPath.isAbsolute(), "Must provide absolute paths for Dataflow");    checkArgument(!gcsPath.getObject().isEmpty(), "Missing object or bucket in path: '%s', did you mean: 'gs://some-bucket/%s'?", gcsPath, gcsPath.getBucket());    checkArgument(!gcsPath.getObject().contains("//"), "Dataflow Service does not allow objects with consecutive slashes");    return gcsPath.toResourceName();}
private void beam_f24465_0(String path, String errorMessage)
{    GcsPath gcsPath = getGcsPath(path);    try {        checkArgument(gcpOptions.getGcsUtil().bucketAccessible(gcsPath), errorMessage, path);    } catch (IOException e) {        throw new RuntimeException(String.format("Unable to verify that GCS bucket gs://%s exists.", gcsPath.getBucket()), e);    }}
public String beam_f24474_0()
{    return gcsPath.toString();}
public boolean beam_f24475_0(Object obj)
{    if (!(obj instanceof GcsResourceId)) {        return false;    }    GcsResourceId other = (GcsResourceId) obj;    return this.gcsPath.equals(other.gcsPath);}
public void beam_f24487_0(int statusCode, String errorMessage)
{    HttpCallMatcher matcher = (req, resp) -> resp.getStatusCode() == statusCode;    this.matchersAndLogs.add(MatcherAndError.create(matcher, simpleErrorMessage(errorMessage)));}
public void beam_f24488_0(int statusCode, String urlContains, String errorMessage)
{    HttpCallMatcher matcher = (request, response) -> {        if (response.getStatusCode() == statusCode && request.getUrl().toString().contains(urlContains)) {            return true;        }        return false;    };    this.matchersAndLogs.add(MatcherAndError.create(matcher, simpleErrorMessage(errorMessage)));}
public String beam_f24497_0()
{    return object;}
public void beam_f24498_0(FileSystem fs)
{    this.fs = fs;}
public boolean beam_f24507_0(Path other)
{    if (other instanceof GcsPath) {        GcsPath gcsPath = (GcsPath) other;        return startsWith(gcsPath.bucketAndObject());    } else {        return startsWith(other.toString());    }}
public boolean beam_f24508_0(String prefix)
{    return bucketAndObject().startsWith(prefix);}
public GcsPath beam_f24517_0()
{    return this;}
public GcsPath beam_f24518_0(LinkOption... options) throws IOException
{    return this;}
public boolean beam_f24527_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    GcsPath paths = (GcsPath) o;    return bucket.equals(paths.bucket) && object.equals(paths.object);}
public int beam_f24528_0()
{    int result = bucket.hashCode();    result = 31 * result + object.hashCode();    return result;}
public static boolean beam_f24537_0(GcsPath spec)
{    return GLOB_PREFIX.matcher(spec.getObject()).matches();}
protected void beam_f24538_0(Storage storageClient)
{    this.storageClient = storageClient;}
 List<Long> beam_f24547_0(List<GcsPath> paths) throws IOException
{    List<StorageObjectOrIOException> results = getObjects(paths);    ImmutableList.Builder<Long> ret = ImmutableList.builder();    for (StorageObjectOrIOException result : results) {        ret.add(toFileSize(result));    }    return ret.build();}
private Long beam_f24548_0(StorageObjectOrIOException storageObjectOrIOException) throws IOException
{    if (storageObjectOrIOException.ioException() != null) {        throw storageObjectOrIOException.ioException();    } else {        return storageObjectOrIOException.storageObject().getSize().longValue();    }}
public boolean beam_f24557_0(IOException e)
{    if (errorExtractor.itemNotFound(e) || errorExtractor.accessDenied(e)) {        return false;    }    return RetryDeterminer.SOCKET_ERRORS.shouldRetry(e);}
 void beam_f24558_0(String projectId, Bucket bucket, BackOff backoff, Sleeper sleeper) throws IOException
{    Storage.Buckets.Insert insertBucket = storageClient.buckets().insert(projectId, bucket);    insertBucket.setPredefinedAcl("projectPrivate");    insertBucket.setPredefinedDefaultObjectAcl("projectPrivate");    try {        ResilientOperation.retry(ResilientOperation.getGoogleRequestCallable(insertBucket), backoff, new RetryDeterminer<IOException>() {            @Override            public boolean shouldRetry(IOException e) {                if (errorExtractor.itemAlreadyExists(e) || errorExtractor.accessDenied(e)) {                    return false;                }                return RetryDeterminer.SOCKET_ERRORS.shouldRetry(e);            }        }, IOException.class, sleeper);        return;    } catch (GoogleJsonResponseException e) {        if (errorExtractor.accessDenied(e)) {            throw new AccessDeniedException(bucket.getName(), null, e.getMessage());        }        if (errorExtractor.itemAlreadyExists(e)) {            throw new FileAlreadyExistsException(bucket.getName(), null, e.getMessage());        }        throw e;    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new IOException(String.format("Error while attempting to create bucket gs://%s for rproject %s", bucket.getName(), projectId), e);    }}
 LinkedList<RewriteOp> beam_f24567_0(Iterable<String> srcFilenames, Iterable<String> destFilenames) throws IOException
{    List<String> srcList = Lists.newArrayList(srcFilenames);    List<String> destList = Lists.newArrayList(destFilenames);    checkArgument(srcList.size() == destList.size(), "Number of source files %s must equal number of destination files %s", srcList.size(), destList.size());    LinkedList<RewriteOp> rewrites = Lists.newLinkedList();    for (int i = 0; i < srcList.size(); i++) {        final GcsPath sourcePath = GcsPath.fromUri(srcList.get(i));        final GcsPath destPath = GcsPath.fromUri(destList.get(i));        rewrites.addLast(new RewriteOp(sourcePath, destPath));    }    return rewrites;}
 List<BatchRequest> beam_f24568_0(LinkedList<RewriteOp> rewrites) throws IOException
{    List<BatchRequest> batches = new ArrayList<>();    BatchRequest batch = createBatchRequest();    Iterator<RewriteOp> it = rewrites.iterator();    while (it.hasNext()) {        RewriteOp rewrite = it.next();        if (!rewrite.getReadyToEnqueue()) {            it.remove();            continue;        }        rewrite.enqueue(batch);        if (batch.size() >= MAX_REQUESTS_PER_BATCH) {            batches.add(batch);            batch = createBatchRequest();        }    }    if (batch.size() > 0) {        batches.add(batch);    }    return batches;}
public void beam_f24577_1(Void obj, HttpHeaders responseHeaders)
{    }
public void beam_f24578_1(GoogleJsonError e, HttpHeaders responseHeaders) throws IOException
{    if (e.getCode() == 404) {            } else {        throw new IOException(String.format("Error trying to delete %s: %s", file, e));    }}
public void beam_f24587_0(HttpRequest request) throws IOException
{            request.setReadTimeout(HANGING_GET_TIMEOUT_SEC * 1000);    request.setWriteTimeout(this.writeTimeout);    LoggingHttpBackOffHandler loggingHttpBackOffHandler = new LoggingHttpBackOffHandler(sleeper,     new ExponentialBackOff.Builder().setNanoClock(nanoClock).setMultiplier(2).build(), new ExponentialBackOff.Builder().setNanoClock(nanoClock).setMultiplier(2).build(), ignoredResponseCodes, this.customHttpErrors);    request.setUnsuccessfulResponseHandler(loggingHttpBackOffHandler);    request.setIOExceptionHandler(loggingHttpBackOffHandler);        if (responseInterceptor != null) {        request.setResponseInterceptor(responseInterceptor);    }}
public void beam_f24588_0(CustomHttpErrors customErrors)
{    this.customHttpErrors = customErrors;}
public Map<String, List<String>> beam_f24597_0() throws IOException
{    return Collections.emptyMap();}
public Map<String, List<String>> beam_f24598_0(URI uri) throws IOException
{    return Collections.emptyMap();}
public void beam_f24608_0() throws Exception
{    System.setProperty("user.home", tmpFolder.getRoot().getAbsolutePath());    DefaultProjectFactory projectFactory = spy(new DefaultProjectFactory());    when(projectFactory.getEnvironment()).thenReturn(ImmutableMap.of());    assertNull(projectFactory.create(PipelineOptionsFactory.create()));}
public void beam_f24609_0() throws Exception
{    GcpOptions options = PipelineOptionsFactory.as(GcpOptions.class);    options.setGcpCredential(new TestCredential());    options.setProject("");    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("--project is a required option");    options.getGcpTempLocation();}
public void beam_f24618_0() throws Exception
{    doReturn(fakeProject).when(mockGet).execute();    doThrow(new IOException("badness")).when(mockGcsUtil).createBucket(any(String.class), any(Bucket.class));    thrown.expect(RuntimeException.class);    thrown.expectMessage("Unable create default bucket");    GcpTempLocationFactory.tryCreateDefaultBucket(options, mockCrmClient);}
public void beam_f24619_0() throws Exception
{    doReturn(fakeProject).when(mockGet).execute();    when(mockGcsUtil.bucketOwner(any(GcsPath.class))).thenThrow(new IOException("badness"));    thrown.expect(RuntimeException.class);    thrown.expectMessage("Unable to determine the owner");    GcpTempLocationFactory.tryCreateDefaultBucket(options, mockCrmClient);}
public void beam_f24628_0() throws Exception
{    GcsOptions options = PipelineOptionsFactory.as(GcsOptions.class);    options.setGcpCredential(new TestCredential());    options.setGoogleApiTrace(new GoogleApiTracer().addTraceFor(Transport.newStorageClient(options).build().objects().get("aProjectId", "aObjectId"), "TraceDestination"));    Storage.Objects.Get getRequest = Transport.newStorageClient(options).build().objects().get("testBucketId", "testObjectId");    assertEquals("TraceDestination", getRequest.get("$trace"));    Storage.Objects.List listRequest = Transport.newStorageClient(options).build().objects().list("testProjectId");    assertNull(listRequest.get("$trace"));}
public void beam_f24629_0() throws Exception
{    String serializedValue = "{\"Api\":\"Token\"}";    assertEquals(serializedValue, MAPPER.writeValueAsString(MAPPER.readValue(serializedValue, GoogleApiTracer.class)));}
public void beam_f24638_0() throws Exception
{    MockitoAnnotations.initMocks(this);    when(mockGcsUtil.bucketAccessible(any(GcsPath.class))).thenReturn(true);    GcsOptions options = PipelineOptionsFactory.as(GcsOptions.class);    options.setGcpCredential(new TestCredential());    options.setGcsUtil(mockGcsUtil);    validator = GcsPathValidator.fromOptions(options);}
public void beam_f24639_0()
{    validator.validateInputFilePatternSupported("gs://bucket/path");}
public void beam_f24648_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("The resolved file: [tmp/] should not end with '/'.");    toResourceIdentifier("gs://my_bucket/").resolve("tmp/", StandardResolveOptions.RESOLVE_FILE);}
public void beam_f24649_0()
{    ResourceId tmpDir = toResourceIdentifier("gs://my_bucket/").resolve("tmp dir", StandardResolveOptions.RESOLVE_FILE);    thrown.expect(IllegalStateException.class);    thrown.expectMessage("Expected the gcsPath is a directory, but had [gs://my_bucket/tmp dir].");    tmpDir.resolve("aa", StandardResolveOptions.RESOLVE_FILE);}
public void beam_f24658_0()
{    MockitoAnnotations.initMocks(this);}
private static MockLowLevelHttpResponse beam_f24659_0(int code, String body)
{    MockLowLevelHttpResponse response = new MockLowLevelHttpResponse();    response.addHeader("custom_header", "value");    response.setStatusCode(code);    response.setContentType(Json.MEDIA_TYPE);    response.setContent(body);    return response;}
public void beam_f24668_0(long millis) throws InterruptedException
{    fastNanoTime += millis * 1000000L;}
public void beam_f24669_0() throws Exception
{    long sleepTimeMs = TimeUnit.SECONDS.toMillis(30);    long sleepTimeNano = TimeUnit.MILLISECONDS.toNanos(sleepTimeMs);    long fakeTimeNano = fastNanoClockAndSleeper.nanoTime();    long startTimeNano = System.nanoTime();    fastNanoClockAndSleeper.sleep(sleepTimeMs);    long maxTimeNano = startTimeNano + TimeUnit.SECONDS.toNanos(1);        assertTrue(System.nanoTime() < maxTimeNano);        assertEquals(fakeTimeNano + sleepTimeNano, fastNanoClockAndSleeper.nanoTime());}
public void beam_f24678_0()
{    GcsPath.fromComponents("invalid/", "");}
public void beam_f24679_0()
{    GcsPath.fromComponents(null, "a\nb");}
public void beam_f24688_0()
{    GcsPath a = GcsPath.fromComponents("bucket", "a/b/c");    Iterator<Path> it = a.iterator();    assertTrue(it.hasNext());    assertEquals("gs://bucket/", it.next().toString());    assertTrue(it.hasNext());    assertEquals("a", it.next().toString());    assertTrue(it.hasNext());    assertEquals("b", it.next().toString());    assertTrue(it.hasNext());    assertEquals("c", it.next().toString());    assertFalse(it.hasNext());}
public void beam_f24689_0()
{    GcsPath a = GcsPath.fromComponents("bucket", "a/b/c/d");    assertThat(a.subpath(0, 1).toString(), Matchers.equalTo("gs://bucket/"));    assertThat(a.subpath(0, 2).toString(), Matchers.equalTo("gs://bucket/a"));    assertThat(a.subpath(0, 3).toString(), Matchers.equalTo("gs://bucket/a/b"));    assertThat(a.subpath(0, 4).toString(), Matchers.equalTo("gs://bucket/a/b/c"));    assertThat(a.subpath(1, 2).toString(), Matchers.equalTo("a"));    assertThat(a.subpath(2, 3).toString(), Matchers.equalTo("b"));    assertThat(a.subpath(2, 4).toString(), Matchers.equalTo("b/c"));    assertThat(a.subpath(2, 5).toString(), Matchers.equalTo("b/c/d"));}
public void beam_f24698_0()
{    GcsOptions pipelineOptions = gcsOptionsWithTestCredential();    pipelineOptions.setExecutorService(Executors.newCachedThreadPool());    assertSame(pipelineOptions.getExecutorService(), pipelineOptions.getGcsUtil().executorService);}
public void beam_f24699_0()
{    GcsOptions pipelineOptions = PipelineOptionsFactory.as(GcsOptions.class);    GcsUtil gcsUtil = Mockito.mock(GcsUtil.class);    pipelineOptions.setGcsUtil(gcsUtil);    assertSame(gcsUtil, pipelineOptions.getGcsUtil());}
public void beam_f24708_0() throws Exception
{    JsonFactory jsonFactory = new JacksonFactory();    String contentBoundary = "batch_foobarbaz";    String contentBoundaryLine = "--" + contentBoundary;    String endOfContentBoundaryLine = "--" + contentBoundary + "--";    GenericJson error = new GenericJson().set("error", new GenericJson().set("code", 404));    error.setFactory(jsonFactory);    String content = contentBoundaryLine + "\n" + "Content-Type: application/http\n" + "\n" + "HTTP/1.1 404 Not Found\n" + "Content-Length: -1\n" + "\n" + error.toString() + "\n" + "\n" + endOfContentBoundaryLine + "\n";    thrown.expect(FileNotFoundException.class);    MockLowLevelHttpResponse notFoundResponse = new MockLowLevelHttpResponse().setContentType("multipart/mixed; boundary=" + contentBoundary).setContent(content).setStatusCode(HttpStatusCodes.STATUS_CODE_OK);    MockHttpTransport mockTransport = new MockHttpTransport.Builder().setLowLevelHttpResponse(notFoundResponse).build();    GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil();    gcsUtil.setStorageClient(new Storage(mockTransport, Transport.getJsonFactory(), null));    gcsUtil.fileSizes(ImmutableList.of(GcsPath.fromComponents("testbucket", "testobject")));}
public void beam_f24709_0() throws Exception
{    JsonFactory jsonFactory = new JacksonFactory();    String contentBoundary = "batch_foobarbaz";    String contentBoundaryLine = "--" + contentBoundary;    String endOfContentBoundaryLine = "--" + contentBoundary + "--";    GenericJson error = new GenericJson().set("error", new GenericJson().set("code", 404));    error.setFactory(jsonFactory);    String content = contentBoundaryLine + "\n" + "Content-Type: application/http\n" + "\n" + "HTTP/1.1 404 Not Found\n" + "Content-Length: -1\n" + "\n" + error.toString() + "\n" + "\n" + endOfContentBoundaryLine + "\n";    thrown.expect(FileNotFoundException.class);    final LowLevelHttpResponse mockResponse = Mockito.mock(LowLevelHttpResponse.class);    when(mockResponse.getContentType()).thenReturn("multipart/mixed; boundary=" + contentBoundary);        when(mockResponse.getStatusCode()).thenReturn(429, 200);    when(mockResponse.getContent()).thenReturn(toStream("error"), toStream(content));        MockHttpTransport mockTransport = new MockHttpTransport.Builder().setLowLevelHttpRequest(new MockLowLevelHttpRequest() {        @Override        public LowLevelHttpResponse execute() throws IOException {            return mockResponse;        }    }).build();    GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil();    gcsUtil.setStorageClient(new Storage(mockTransport, Transport.getJsonFactory(), new RetryHttpRequestInitializer()));    gcsUtil.fileSizes(ImmutableList.of(GcsPath.fromComponents("testbucket", "testobject")));}
public void beam_f24718_0() throws IOException
{    GcsOptions pipelineOptions = gcsOptionsWithTestCredential();    GcsUtil gcsUtil = pipelineOptions.getGcsUtil();    Storage mockStorage = Mockito.mock(Storage.class);    gcsUtil.setStorageClient(mockStorage);    Storage.Buckets mockStorageObjects = Mockito.mock(Storage.Buckets.class);    Storage.Buckets.Get mockStorageGet = Mockito.mock(Storage.Buckets.Get.class);    BackOff mockBackOff = BackOffAdapter.toGcpBackOff(FluentBackoff.DEFAULT.backoff());    when(mockStorage.buckets()).thenReturn(mockStorageObjects);    when(mockStorageObjects.get("testbucket")).thenReturn(mockStorageGet);    when(mockStorageGet.execute()).thenThrow(new SocketTimeoutException("SocketException")).thenReturn(new Bucket());    assertNotNull(gcsUtil.getBucket(GcsPath.fromComponents("testbucket", "testobject"), mockBackOff, new FastNanoClockAndSleeper()));}
public void beam_f24719_0() throws IOException
{    GcsOptions pipelineOptions = gcsOptionsWithTestCredential();    GcsUtil gcsUtil = pipelineOptions.getGcsUtil();    Storage mockStorage = Mockito.mock(Storage.class);    gcsUtil.setStorageClient(mockStorage);    Storage.Buckets mockStorageObjects = Mockito.mock(Storage.Buckets.class);    Storage.Buckets.Get mockStorageGet = Mockito.mock(Storage.Buckets.Get.class);    BackOff mockBackOff = BackOffAdapter.toGcpBackOff(FluentBackoff.DEFAULT.backoff());    when(mockStorage.buckets()).thenReturn(mockStorageObjects);    when(mockStorageObjects.get("testbucket")).thenReturn(mockStorageGet);    when(mockStorageGet.execute()).thenThrow(googleJsonResponseException(HttpStatusCodes.STATUS_CODE_NOT_FOUND, "It don't exist", "Nothing here to see"));    thrown.expect(FileNotFoundException.class);    thrown.expectMessage("It don't exist");    gcsUtil.getBucket(GcsPath.fromComponents("testbucket", "testobject"), mockBackOff, new FastNanoClockAndSleeper());}
public void beam_f24728_0() throws IOException
{    GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil();        List<BatchRequest> batches = gcsUtil.makeCopyBatches(gcsUtil.makeRewriteOps(makeStrings("s", 3), makeStrings("d", 3)));    assertThat(batches.size(), equalTo(1));    assertThat(sumBatchSizes(batches), equalTo(3));        batches = gcsUtil.makeCopyBatches(gcsUtil.makeRewriteOps(makeStrings("s", 100), makeStrings("d", 100)));    assertThat(batches.size(), equalTo(1));    assertThat(sumBatchSizes(batches), equalTo(100));        batches = gcsUtil.makeCopyBatches(gcsUtil.makeRewriteOps(makeStrings("s", 501), makeStrings("d", 501)));    assertThat(batches.size(), equalTo(6));    assertThat(sumBatchSizes(batches), equalTo(501));}
public void beam_f24729_0() throws IOException
{    GcsUtil gcsUtil = gcsOptionsWithTestCredential().getGcsUtil();    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Number of source files 3");    gcsUtil.makeRewriteOps(makeStrings("s", 3), makeStrings("d", 1));}
public void beam_f24738_0() throws IOException
{    when(mockLowLevelRequest.execute()).thenReturn(mockLowLevelResponse);    when(mockLowLevelResponse.getStatusCode()).thenReturn(    403).thenReturn(    200);    try {        Storage.Buckets.Get result = storage.buckets().get("test");        HttpResponse response = result.executeUnparsed();        assertNotNull(response);    } catch (HttpResponseException e) {        assertThat(e.getMessage(), Matchers.containsString("403"));    }    verify(mockHttpResponseInterceptor).interceptResponse(any(HttpResponse.class));    verify(mockLowLevelRequest, atLeastOnce()).addHeader(anyString(), anyString());    verify(mockLowLevelRequest).setTimeout(anyInt(), anyInt());    verify(mockLowLevelRequest).setWriteTimeout(anyInt());    verify(mockLowLevelRequest).execute();    verify(mockLowLevelResponse).getStatusCode();    expectedLogs.verifyWarn("Request failed with code 403");}
public void beam_f24739_0() throws IOException
{    when(mockLowLevelRequest.execute()).thenReturn(mockLowLevelResponse).thenReturn(mockLowLevelResponse).thenReturn(mockLowLevelResponse);    when(mockLowLevelResponse.getStatusCode()).thenReturn(    503).thenReturn(    429).thenReturn(200);    Storage.Buckets.Get result = storage.buckets().get("test");    HttpResponse response = result.executeUnparsed();    assertNotNull(response);    verify(mockHttpResponseInterceptor).interceptResponse(any(HttpResponse.class));    verify(mockLowLevelRequest, atLeastOnce()).addHeader(anyString(), anyString());    verify(mockLowLevelRequest, times(3)).setTimeout(anyInt(), anyInt());    verify(mockLowLevelRequest, times(3)).setWriteTimeout(anyInt());    verify(mockLowLevelRequest, times(3)).execute();    verify(mockLowLevelResponse, times(3)).getStatusCode();    expectedLogs.verifyDebug("Request failed with code 503");}
public static AsJsons<InputT> beam_f24748_0(Class<? extends InputT> inputClass)
{    return new AsJsons<>(inputClass);}
public AsJsons<InputT> beam_f24749_0(ObjectMapper mapper)
{    AsJsons<InputT> newTransform = new AsJsons<>(inputClass);    newTransform.customMapper = mapper;    return newTransform;}
public KV<InputT, Map<String, String>> beam_f24758_0(WithFailures.ExceptionElement<InputT> f) throws RuntimeException
{    if (!(f.exception() instanceof JsonProcessingException)) {        throw new RuntimeException(f.exception());    }    return KV.of(f.element(), ImmutableMap.of("className", f.exception().getClass().getName(), "message", f.exception().getMessage(), "stackTrace", Arrays.toString(f.exception().getStackTrace())));}
public static ParseJsons<OutputT> beam_f24759_0(Class<? extends OutputT> outputClass)
{    return new ParseJsons<>(outputClass);}
public WithFailures.Result<PCollection<OutputT>, FailureT> beam_f24768_0(PCollection<String> input)
{    return input.apply(MapElements.into(new TypeDescriptor<OutputT>() {    }).via(Contextful.fn((Contextful.Fn<String, OutputT>) (input1, c) -> readValue(input1), Requirements.empty())).exceptionsInto(failureType).exceptionsVia(exceptionHandler));}
public KV<OutputT, Map<String, String>> beam_f24769_0(WithFailures.ExceptionElement<OutputT> f) throws RuntimeException
{    if (!(f.exception() instanceof IOException)) {        throw new RuntimeException(f.exception());    }    return KV.of(f.element(), ImmutableMap.of("className", f.exception().getClass().getName(), "message", f.exception().getMessage(), "stackTrace", Arrays.toString(f.exception().getStackTrace())));}
public void beam_f24778_0()
{    ObjectMapper customMapper = new ObjectMapper().configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);    PCollection<MyPojo> output = pipeline.apply(Create.of(EXTRA_PROPERTIES_JSONS)).apply(ParseJsons.of(MyPojo.class).withMapper(customMapper)).setCoder(SerializableCoder.of(MyPojo.class));    PAssert.that(output).containsInAnyOrder(POJOS);    pipeline.run();}
public void beam_f24779_0()
{    PCollection<String> output = pipeline.apply(Create.of(POJOS)).apply(AsJsons.of(MyPojo.class)).setCoder(StringUtf8Coder.of());    PAssert.that(output).containsInAnyOrder(VALID_JSONS);    pipeline.run();}
public void beam_f24788_0(String myString)
{    this.myString = myString;}
public int beam_f24789_0()
{    return myInt;}
private void beam_f24798_0(WithFailures.Result<PCollection<String>, KV<MyPojo, Map<String, String>>> result)
{    PAssert.that(result.failures()).satisfies(kv -> {        for (KV<MyPojo, Map<String, String>> entry : kv) {            assertThat(entry.getValue().entrySet(), hasSize(3));            assertThat(entry.getValue(), hasKey("stackTrace"));            assertThat(entry.getValue(), hasKey("message"));            assertEquals("com.fasterxml.jackson.databind.JsonMappingException", entry.getValue().get("className"));        }        return null;    });}
private void beam_f24799_0(WithFailures.Result<PCollection<String>, KV<MyPojo, String>> result)
{    PAssert.that(result.failures()).containsInAnyOrder(KV.of(INVALID_POJOS.get(0), "com.fasterxml.jackson.databind.JsonMappingException"), KV.of(INVALID_POJOS.get(1), "com.fasterxml.jackson.databind.JsonMappingException"));}
public void beam_f24808_0(ProcessContext c)
{    KV<K, CoGbkResult> e = c.element();    Iterable<V1> leftValuesIterable = e.getValue().getAll(v1Tuple);    Iterable<V2> rightValuesIterable = e.getValue().getAll(v2Tuple);    for (V2 rightValue : rightValuesIterable) {        if (leftValuesIterable.iterator().hasNext()) {            for (V1 leftValue : leftValuesIterable) {                c.output(KV.of(e.getKey(), KV.of(leftValue, rightValue)));            }        } else {            c.output(KV.of(e.getKey(), KV.of(nullValue, rightValue)));        }    }}
public static FullOuterJoin<K, V1, V2> beam_f24809_0(PCollection<KV<K, V2>> rightCollection, V1 leftNullValue, V2 rightNullValue)
{    return new FullOuterJoin<>(rightCollection, leftNullValue, rightNullValue);}
public static PCollection<KV<K, KV<V1, V2>>> beam_f24818_0(final String name, final PCollection<KV<K, V1>> leftCollection, final PCollection<KV<K, V2>> rightCollection, final V1 leftNullValue, final V2 rightNullValue)
{    return leftCollection.apply(name, FullOuterJoin.with(rightCollection, leftNullValue, rightNullValue));}
public static PCollection<KV<K, KV<V1, V2>>> beam_f24819_0(final PCollection<KV<K, V1>> leftCollection, final PCollection<KV<K, V2>> rightCollection, final V1 leftNullValue, final V2 rightNullValue)
{    return fullOuterJoin("FullOuterJoin", leftCollection, rightCollection, leftNullValue, rightNullValue);}
public void beam_f24828_0()
{    leftListOfKv = new ArrayList<>();    rightListOfKv = new ArrayList<>();    expectedResult = new ArrayList<>();}
public void beam_f24829_0()
{    leftListOfKv.add(KV.of("Key1", 5L));    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key1", "foo"));    rightListOfKv.add(KV.of("Key2", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.fullOuterJoin(leftCollection, rightCollection, -1L, "");    expectedResult.add(KV.of("Key1", KV.of(5L, "foo")));    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24838_0()
{    leftListOfKv = new ArrayList<>();    rightListOfKv = new ArrayList<>();    expectedResult = new ArrayList<>();}
public void beam_f24839_0()
{    leftListOfKv.add(KV.of("Key1", 5L));    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key1", "foo"));    rightListOfKv.add(KV.of("Key2", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.leftOuterJoin(leftCollection, rightCollection, "");    expectedResult.add(KV.of("Key1", KV.of(5L, "foo")));    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24848_0()
{    leftListOfKv.add(KV.of("Key1", 5L));    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key1", "foo"));    rightListOfKv.add(KV.of("Key2", "bar"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.rightOuterJoin(leftCollection, rightCollection, -1L);    expectedResult.add(KV.of("Key1", KV.of(5L, "foo")));    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public void beam_f24849_0()
{    leftListOfKv.add(KV.of("Key2", 4L));    PCollection<KV<String, Long>> leftCollection = p.apply("CreateLeft", Create.of(leftListOfKv));    rightListOfKv.add(KV.of("Key2", "bar"));    rightListOfKv.add(KV.of("Key2", "gazonk"));    PCollection<KV<String, String>> rightCollection = p.apply("CreateRight", Create.of(rightListOfKv));    PCollection<KV<String, KV<Long, String>>> output = Join.rightOuterJoin(leftCollection, rightCollection, -1L);    expectedResult.add(KV.of("Key2", KV.of(4L, "bar")));    expectedResult.add(KV.of("Key2", KV.of(4L, "gazonk")));    PAssert.that(output).containsInAnyOrder(expectedResult);    p.run();}
public static KryoCoder<T> beam_f24858_0(List<KryoRegistrar> registrars)
{    return of(PipelineOptionsFactory.create(), registrars);}
public static KryoCoder<T> beam_f24859_0(PipelineOptions pipelineOptions)
{    return of(pipelineOptions, Collections.emptyList());}
 String beam_f24869_0()
{    return instanceId;}
 SerializableOptions beam_f24870_0()
{    return options;}
public static KryoCoderProvider beam_f24879_0(PipelineOptions pipelineOptions, List<KryoRegistrar> registrars)
{    final KryoOptions kryoOptions = pipelineOptions.as(KryoOptions.class);    return new KryoCoderProvider(KryoCoder.of(kryoOptions, registrars));}
public Coder<T> beam_f24880_0(TypeDescriptor<T> typeDescriptor, List<? extends Coder<?>> componentCoders) throws CannotProvideCoderException
{    if (hasUserProvidedRegistration(typeDescriptor)) {        return (Coder) coder;    }    if (OBJECT_TYPE.equals(typeDescriptor)) {        return (Coder) coder;    }    throw new CannotProvideCoderException(String.format("Cannot provide [%s], given type descriptor's [%s] raw type is not registered in Kryo.", KryoCoder.class.getSimpleName(), typeDescriptor));}
 InputChunked beam_f24889_0()
{    return inputChunked;}
 OutputChunked beam_f24890_0()
{    return outputChunked;}
public void beam_f24899_0() throws IOException
{    OPTIONS.as(KryoOptions.class).setKryoRegistrationRequired(true);    final KryoCoder<ClassToBeEncoded> coder = KryoCoder.of(OPTIONS);    assertEncoding(coder);}
public void beam_f24900_0() throws IOException
{    final KryoRegistrar registrarCoding = k -> k.register(ClassToBeEncoded.class);    final KryoRegistrar registrarDecoding = k -> {        };    final KryoCoder<ClassToBeEncoded> coderToEncode = KryoCoder.of(OPTIONS, registrarCoding);    final KryoCoder<ClassToBeEncoded> coderToDecode = KryoCoder.of(OPTIONS, registrarDecoding);    assertEncoding(coderToEncode, coderToDecode);}
public int beam_f24909_0()
{    return Objects.hash(firstField, secondField, thirdField);}
public boolean beam_f24910_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    TestClass testClass = (TestClass) o;    return Objects.equals(param, testClass.param);}
public boolean beam_f24920_0()
{    return true;}
public boolean beam_f24921_0(ByteString value)
{    return true;}
public static ProtoCoder<T> beam_f24930_0(Class<T> protoMessageClass)
{    return new ProtoCoder<>(protoMessageClass, ImmutableSet.of());}
public static ProtoCoder<T> beam_f24931_0(TypeDescriptor<T> protoMessageType)
{    @SuppressWarnings("unchecked")    Class<T> protoMessageClass = (Class<T>) protoMessageType.getRawType();    return of(protoMessageClass);}
public void beam_f24940_0() throws NonDeterministicException
{    ProtobufUtil.verifyDeterministic(this);}
public Class<T> beam_f24941_0()
{    return protoMessageClass;}
public void beam_f24950_0() throws Throwable
{    TEST_CODER.verifyDeterministic();}
public void beam_f24951_0()
{    assertTrue(TEST_CODER.consistentWithEquals());}
public void beam_f24960_0()
{    assertThat(getRecursiveDescriptorFullNames(MessageWithMap.class), equalTo(WITH_MAP_ALL));}
public void beam_f24961_0()
{    assertThat(getRecursiveDescriptorFullNames(ReferencesMessageWithMap.class), equalTo(REFERS_MAP_ALL));}
public void beam_f24970_0() throws Exception
{    assertEquals(ProtoCoder.of(new TypeDescriptor<MessageA>() {    }), ProtoCoder.of(MessageA.class));    assertEquals(ProtoCoder.of(new TypeDescriptor<MessageA>() {    }), ProtoCoder.getCoderProvider().coderFor(new TypeDescriptor<MessageA>() {    }, Collections.emptyList()));}
public void beam_f24971_0() throws Exception
{    thrown.expect(CannotProvideCoderException.class);    thrown.expectMessage("java.lang.Integer is not a subclass of com.google.protobuf.Message");    ProtoCoder.getCoderProvider().coderFor(new TypeDescriptor<Integer>() {    }, Collections.emptyList());}
public void beam_f24980_0() throws CoderException
{    MessageWithMap.Builder msg1B = MessageWithMap.newBuilder();    MessageWithMap.Builder msg2B = MessageWithMap.newBuilder();        for (int i = 0; i < 10; ++i) {        msg1B.getMutableField1().put("key" + i, MessageA.getDefaultInstance());        msg2B.getMutableField1().put("key" + (9 - i), MessageA.getDefaultInstance());    }        MessageWithMap msg1 = msg1B.build();    MessageWithMap msg2 = msg2B.build();    assertEquals(msg2, msg1);        Coder<MessageWithMap> coder = ProtoCoder.of(MessageWithMap.class);    assertNotEquals(CoderUtils.encodeToBase64(coder, msg2), CoderUtils.encodeToBase64(coder, msg1));}
public static GloballyDistinct<InputT> beam_f24981_0()
{    return GloballyDistinct.<InputT>builder().build();}
public PCollection<KV<K, Long>> beam_f24990_0(PCollection<KV<K, V>> input)
{    KvCoder<K, V> inputCoder = (KvCoder<K, V>) input.getCoder();    return input.apply(Combine.perKey(ApproximateDistinctFn.create(inputCoder.getValueCoder()).withPrecision(this.precision()).withSparseRepresentation(this.sparsePrecision()))).apply("Retrieve Cardinality", ParDo.of(RetrieveCardinality.perKey()));}
public static ApproximateDistinctFn<InputT> beam_f24991_0(Coder<InputT> coder)
{    try {        coder.verifyDeterministic();    } catch (Coder.NonDeterministicException e) {        throw new IllegalArgumentException("Coder must be deterministic to perform this sketch." + e.getMessage(), e);    }    return new ApproximateDistinctFn<>(12, 0, coder);}
public void beam_f25000_0(HyperLogLogPlus value, OutputStream outStream) throws IOException
{    if (value == null) {        throw new CoderException("cannot encode a null HyperLogLogPlus sketch");    }    BYTE_ARRAY_CODER.encode(value.getBytes(), outStream);}
public HyperLogLogPlus beam_f25001_0(InputStream inStream) throws IOException
{    return HyperLogLogPlus.Builder.build(BYTE_ARRAY_CODER.decode(inStream));}
public static GlobalSketch<InputT> beam_f25010_0()
{    return GlobalSketch.<InputT>builder().build();}
public static PerKeySketch<K, V> beam_f25011_0()
{    return PerKeySketch.<K, V>builder().build();}
public static CountMinSketchFn<InputT> beam_f25020_0(Coder<InputT> coder)
{    try {        coder.verifyDeterministic();    } catch (Coder.NonDeterministicException e) {        throw new IllegalArgumentException("Coder must be deterministic to perform this sketch." + e.getMessage(), e);    }    return new CountMinSketchFn<>(coder, 0.01, 0.999);}
public CountMinSketchFn<InputT> beam_f25021_0(double epsilon, double confidence)
{    if (epsilon <= 0D) {        throw new IllegalArgumentException("The relative error must be positive");    }    if (confidence <= 0D || confidence >= 1D) {        throw new IllegalArgumentException("The confidence must be between 0 and 1");    }    return new CountMinSketchFn<>(inputCoder, epsilon, confidence);}
public void beam_f25030_0(T element, long count, Coder<T> coder)
{    sketch().add(hashElement(element, coder), count);}
public void beam_f25031_0(T element, Coder<T> coder)
{    add(element, 1L, coder);}
 static Builder beam_f25040_0()
{    return new AutoValue_TDigestQuantiles_GlobalDigest.Builder().setCompression(100);}
public GlobalDigest beam_f25041_0(double cf)
{    return toBuilder().setCompression(cf).build();}
public MergingDigest beam_f25050_0(Iterable<MergingDigest> accumulators)
{    Iterator<MergingDigest> it = accumulators.iterator();    MergingDigest merged = it.next();    while (it.hasNext()) {        merged.add(it.next());    }    return merged;}
public Coder<MergingDigest> beam_f25051_0(CoderRegistry registry, Coder inputCoder)
{    return new MergingDigestCoder();}
public void beam_f25060_0()
{    final int cardinality = 1000;    final int p = 15;    final double expectedErr = 1.04 / Math.sqrt(p);    List<Integer> stream = new ArrayList<>();    for (int i = 1; i <= cardinality; i++) {        stream.addAll(Collections.nCopies(2, i));    }    Collections.shuffle(stream);    PCollection<Long> results = tp.apply("per key stream", Create.of(stream)).apply("create keys", WithKeys.of(1)).apply("per key cardinality", ApproximateDistinct.<Integer, Integer>perKey().withPrecision(p)).apply("extract values", Values.create());    PAssert.that("Verify Accuracy for cardinality per key", results).satisfies(new VerifyAccuracy(cardinality, expectedErr));    tp.run();}
public void beam_f25061_0()
{    final int cardinality = 500;    final int p = 15;    final double expectedErr = 1.04 / Math.sqrt(p);    Schema schema = SchemaBuilder.record("User").fields().requiredString("Pseudo").requiredInt("Age").endRecord();    List<GenericRecord> users = new ArrayList<>();    for (int i = 1; i <= cardinality; i++) {        GenericData.Record newRecord = new GenericData.Record(schema);        newRecord.put("Pseudo", "User" + i);        newRecord.put("Age", i);        users.add(newRecord);    }    PCollection<Long> results = tp.apply("Create stream", Create.of(users).withCoder(AvroCoder.of(schema))).apply("Test custom object", ApproximateDistinct.<GenericRecord>globally().withPrecision(p));    PAssert.that("Verify Accuracy for custom object", results).satisfies(new VerifyAccuracy(cardinality, expectedErr));    tp.run();}
public void beam_f25070_0()
{    double eps = 0.01;    double conf = 0.8;    int width = (int) Math.ceil(2 / eps);    int depth = (int) Math.ceil(-Math.log(1 - conf) / Math.log(2));    final CountMinSketchFn<Integer> fn = CountMinSketchFn.create(VarIntCoder.of()).withAccuracy(eps, conf);    assertThat(DisplayData.from(fn), hasDisplayItem("width", width));    assertThat(DisplayData.from(fn), hasDisplayItem("depth", depth));    assertThat(DisplayData.from(fn), hasDisplayItem("eps", eps));    assertThat(DisplayData.from(fn), hasDisplayItem("conf", conf));}
public Void beam_f25071_0(Sketch<T> sketch)
{    for (int i = 0; i < elements.length; i++) {        assertEquals((long) expectedHits[i], sketch.estimateCount(elements[i], coder));    }    return null;}
public Void beam_f25080_0(Iterable<KV<Double, Double>> input)
{    for (KV<Double, Double> pair : input) {        double expectedValue = pair.getKey() * (size + 1);        boolean isAccurate = Math.abs(pair.getValue() - expectedValue) / size <= expectedError;        Assert.assertTrue("not accurate enough : \nQuantile " + pair.getKey() + " is " + pair.getValue() + " and not " + expectedValue, isAccurate);    }    return null;}
public static Options beam_f25081_0()
{    return new Options("/tmp", 100, SorterType.HADOOP);}
private void beam_f25090_0() throws IOException
{    for (KV<byte[], byte[]> record : inMemorySorter.sort()) {        externalSorter.add(record);    }        inMemorySorter = null;}
public Iterable<KV<byte[], byte[]>> beam_f25091_0() throws IOException
{    if (!inMemorySorterFull) {        return inMemorySorter.sort();    } else {        return externalSorter.sort();    }}
public void beam_f25100_0(KV<byte[], byte[]> record) throws IOException
{    checkState(!sortCalled, "Records can only be added before sort()");    initHadoopSorter();    BytesWritable key = new BytesWritable(record.getKey());    BytesWritable value = new BytesWritable(record.getValue());    writer.append(key, value);}
public Iterable<KV<byte[], byte[]>> beam_f25101_0() throws IOException
{    checkState(!sortCalled, "sort() can only be called once.");    sortCalled = true;    initHadoopSorter();    writer.close();    return new SortedRecordsIterable();}
public void beam_f25110_0(KV<byte[], byte[]> record)
{    checkState(addIfRoom(record), "No space remaining for in memory sorting");}
public boolean beam_f25111_0(KV<byte[], byte[]> record)
{    checkState(!sortCalled, "Records can only be added before sort()");    long recordBytes = estimateRecordBytes(record);    if (roomInBuffer(numBytes + recordBytes, records.size() + 1L)) {        records.add(record);        numBytes += recordBytes;        return true;    } else {        return false;    }}
public void beam_f25120_0(byte[] key, byte[] value) throws IOException
{    Preconditions.checkState(!sortCalled, "Records can only be added before sort()");    CODER.encode(key, dataStream);    CODER.encode(value, dataStream);}
public Iterable<KV<byte[], byte[]>> beam_f25121_0() throws IOException
{    Preconditions.checkState(!sortCalled, "sort() can only be called once.");    sortCalled = true;    dataStream.close();    return mergeSortedFiles(sortInBatch());}
private int beam_f25130_0(int numFiles)
{    final long memory = maxMemory > 0 ? maxMemory : estimateAvailableMemory();    return (int) (memory / numFiles / 2);}
private static long beam_f25131_0()
{    System.gc();        final Runtime r = Runtime.getRuntime();    final long allocatedMemory = r.totalMemory() - r.freeMemory();    return r.maxMemory() - allocatedMemory;}
public void beam_f25140_0(ProcessContext c)
{    Iterable<KV<SecondaryKeyT, ValueT>> records = c.element().getValue();    try {        Sorter sorter = BufferedExternalSorter.create(sorterOptions);        for (KV<SecondaryKeyT, ValueT> record : records) {            sorter.add(KV.of(CoderUtils.encodeToByteArray(keyCoder, record.getKey()), CoderUtils.encodeToByteArray(valueCoder, record.getValue())));        }        c.output(KV.of(c.element().getKey(), new DecodingIterable(sorter.sort())));    } catch (IOException e) {        throw new RuntimeException(e);    }}
public Iterator<KV<SecondaryKeyT, ValueT>> beam_f25141_0()
{    return new DecodingIterator(iterable.iterator());}
public void beam_f25150_0() throws Exception
{    ExternalSorter mockExternalSorter = mock(ExternalSorter.class);    InMemorySorter mockInMemorySorter = mock(InMemorySorter.class);    BufferedExternalSorter testSorter = new BufferedExternalSorter(mockExternalSorter, mockInMemorySorter);    @SuppressWarnings("unchecked")    KV<byte[], byte[]>[] kvs = new KV[] { KV.of(new byte[] { 0 }, new byte[] {}), KV.of(new byte[] { 0, 1 }, new byte[] {}), KV.of(new byte[] { 1 }, new byte[] {}) };    when(mockInMemorySorter.addIfRoom(kvs[0])).thenReturn(true);    when(mockInMemorySorter.addIfRoom(kvs[1])).thenReturn(true);    when(mockInMemorySorter.addIfRoom(kvs[2])).thenReturn(false);    when(mockInMemorySorter.sort()).thenReturn(Arrays.asList(kvs[0], kvs[1]));    when(mockExternalSorter.sort()).thenReturn(Arrays.asList(kvs[0], kvs[1], kvs[2]));    testSorter.add(kvs[0]);    testSorter.add(kvs[1]);    testSorter.add(kvs[2]);    assertEquals(Arrays.asList(kvs[0], kvs[1], kvs[2]), testSorter.sort());    verify(mockExternalSorter, times(1)).add(kvs[0]);    verify(mockExternalSorter, times(1)).add(kvs[1]);    verify(mockExternalSorter, times(1)).add(kvs[2]);}
public void beam_f25151_0() throws Exception
{    SorterTestUtils.testEmpty(BufferedExternalSorter.create(BufferedExternalSorter.options().withTempLocation(tmpLocation.toString())));}
public void beam_f25160_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("memoryMB must be greater than zero");    BufferedExternalSorter.Options options = BufferedExternalSorter.options();    options.withMemoryMB(0);}
public void beam_f25161_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("memoryMB must be less than 2048");    BufferedExternalSorter.Options options = BufferedExternalSorter.options();    options.withMemoryMB(2048);}
public void beam_f25170_0() throws Exception
{    SorterTestUtils.testSingleElement(ExternalSorter.create(new ExternalSorter.Options().setTempLocation(tmpLocation.toString()).setSorterType(sorterType)));}
public void beam_f25171_0() throws Exception
{    SorterTestUtils.testEmptyKeyValueElement(ExternalSorter.create(new ExternalSorter.Options().setTempLocation(tmpLocation.toString()).setSorterType(sorterType)));}
public void beam_f25180_0() throws Exception
{    SorterTestUtils.testSingleElement(InMemorySorter.create(new InMemorySorter.Options()));}
public void beam_f25181_0() throws Exception
{    SorterTestUtils.testEmptyKeyValueElement(InMemorySorter.create(new InMemorySorter.Options()));}
public void beam_f25190_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("memoryMB must be greater than zero");    InMemorySorter.Options options = new InMemorySorter.Options();    options.setMemoryMB(0);}
public static void beam_f25191_0(Sorter sorter) throws Exception
{    assertThat(sorter.sort(), is(emptyIterable()));}
 static KvMatcher<K, V> beam_f25200_0(Matcher<K> keyMatcher, Matcher<V> valueMatcher)
{    return new KvMatcher<>(keyMatcher, valueMatcher);}
public boolean beam_f25201_0(KV<? extends K, ? extends V> kv)
{    return keyMatcher.matches(kv.getKey()) && valueMatcher.matches(kv.getValue());}
private static Map<String, TableProvider> beam_f25210_0()
{    return Stream.of(new PubsubJsonTableProvider(), new BigQueryTableProvider(), new TextTableProvider()).collect(toMap(TableProvider::getTableType, p -> p));}
public String beam_f25211_0()
{    return "google.cloud.datacatalog";}
 static Table.Builder beam_f25220_0(Entry entry)
{    return Table.builder().location(getLocation(entry)).properties(new JSONObject()).type("pubsub").comment("");}
private static String beam_f25221_0(Entry entry)
{    URI entryName = URI.create(entry.getLinkedResource());    String psPath = entryName.getPath();    Matcher bqPathMatcher = PS_PATH_PATTERN.matcher(psPath);    if (!bqPathMatcher.matches()) {        throw new IllegalArgumentException("Unsupported format for Pubsub topic: '" + entry.getLinkedResource() + "'");    }    return psPath.substring(1);}
private static String beam_f25230_0(String s)
{    return SIMPLE_ID.matcher(s).matches() ? s : ("`" + s + "`");}
public void beam_f25231_0() throws Exception
{    createBQTableWith(new TableRow().set("id", 1).set("name", "name1"), new TableRow().set("id", 2).set("name", "name2"), new TableRow().set("id", 3).set("name", "name3"));    TableReference bqTable = bigQuery.tableReference();    String tableId = String.format("bigquery.`table`.`%s`.`%s`.`%s`", bqTable.getProjectId(), bqTable.getDatasetId(), bqTable.getTableId());    PCollection<Row> result = readPipeline.apply("query", SqlTransform.query("SELECT id, name FROM " + tableId).withDefaultTableProvider("datacatalog", DataCatalogTableProvider.create(readPipeline.getOptions().as(DataCatalogPipelineOptions.class))));    PAssert.that(result).containsInAnyOrder(row(1, "name1"), row(2, "name2"), row(3, "name3"));    readPipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
 String beam_f25240_0()
{    return db;}
public String beam_f25241_0()
{    return "hcatalog";}
public BeamTableStatistics beam_f25250_0(PipelineOptions options)
{    return BeamTableStatistics.BOUNDED_UNKNOWN;}
public Schema beam_f25251_0()
{    return schema();}
public Set<String> beam_f25260_0()
{    throw new UnsupportedOperationException("Listing DBs is not supported in metastore");}
public TableProvider beam_f25261_0(String name)
{    return metastoreSchema.hasDatabase(name) ? new DatabaseProvider(name, metastoreSchema, configuration) : null;}
public void beam_f25270_0() throws Exception
{    initializeHCatalog();    PCollection<Row> inputMain = pipeline.apply("mainInput", create(row(1, "pcollection_1"), row(2, "pcollection_2")));    PCollection<Row> inputExtra = pipeline.apply("extraInput", create(row(1, "_extra_table_1"), row(2, "_extra_table_2")));    PCollection<Row> result = inputMain.apply(SqlTransform.query("SELECT \n" + "   x_tbl.f_int as f_int, \n" + "   (p_tbl.f_string || x_tbl.f_string || ' ' || h_tbl.f_str) AS f_string \n" + "FROM \n" + "     `extraSchema`.`extraTable` AS x_tbl \n" + "  INNER JOIN \n" + "     `hive`.`mytable` AS h_tbl \n" + "        ON h_tbl.f_int = x_tbl.f_int \n" + "  INNER JOIN \n" + "     PCOLLECTION AS p_tbl \n" + "        ON p_tbl.f_int = x_tbl.f_int").withTableProvider("extraSchema", extraTableProvider("extraTable", inputExtra)).withTableProvider("hive", hiveTableProvider()));    PAssert.that(result).containsInAnyOrder(row(1, "pcollection_1_extra_table_1 record 1"), row(2, "pcollection_2_extra_table_2 record 2"));    pipeline.run();}
private void beam_f25271_0() throws Exception
{    service.executeQuery("drop table " + TEST_TABLE);    service.executeQuery("create table " + TEST_TABLE + "(f_str string, f_int int)");}
 static Status beam_f25280_0(String[] args, InputStream inputStream, @Nullable OutputStream outputStream, @Nullable OutputStream errorStream) throws IOException
{    String[] modifiedArgs = checkConnectionArgs(args);    SqlLine sqlLine = new SqlLine();    if (outputStream != null) {        sqlLine.setOutputStream(new PrintStream(outputStream));    }    if (errorStream != null) {        sqlLine.setErrorStream(new PrintStream(errorStream));    }    return sqlLine.begin(modifiedArgs, inputStream, true);}
public static void beam_f25281_0()
{    project = TestPipeline.testingPipelineOptions().as(GcpOptions.class).getProject();    setProject = String.format("SET project = '%s';", project);    createPubsubTableStatement = "CREATE EXTERNAL TABLE taxi_rides (\n" + "         event_timestamp TIMESTAMP,\n" + "         attributes MAP<VARCHAR, VARCHAR>,\n" + "         payload ROW<\n" + "           ride_id VARCHAR,\n" + "           point_idx INT,\n" + "           latitude DOUBLE,\n" + "           longitude DOUBLE,\n" + "           meter_reading DOUBLE,\n" + "           meter_increment DOUBLE,\n" + "           ride_status VARCHAR,\n" + "           passenger_count TINYINT>)\n" + "       TYPE pubsub \n" + "       LOCATION '%s'\n" + "       TBLPROPERTIES '{\"timestampAttributeKey\": \"ts\"}';";    dateFormat.setTimeZone(TimeZone.getTimeZone("UTC"));}
public void beam_f25290_0() throws Exception
{    BeamSqlLine.main(new String[] {});}
public void beam_f25291_0() throws Exception
{    BeamSqlLine.main(new String[] { "-e", "" });}
public void beam_f25300_0() throws Exception
{    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();    String[] args = buildArgs("CREATE EXTERNAL TABLE table_test (col_a VARCHAR, col_b TIMESTAMP) TYPE 'test';", "INSERT INTO table_test SELECT '3', TIMESTAMP '2018-07-01 21:26:06';", "INSERT INTO table_test SELECT '3', TIMESTAMP '2018-07-01 21:26:07';", "SELECT TUMBLE_START(col_b, INTERVAL '1' SECOND), count(*) FROM table_test " + "GROUP BY TUMBLE(col_b, INTERVAL '1' SECOND);");    BeamSqlLine.runSqlLine(args, null, byteArrayOutputStream, null);    List<List<String>> lines = toLines(byteArrayOutputStream);    assertThat(Arrays.asList(Arrays.asList("2018-07-01 21:26:06", "1"), Arrays.asList("2018-07-01 21:26:07", "1")), everyItem(IsIn.isOneOf(lines.toArray())));}
public void beam_f25301_0() throws Exception
{    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();    String[] args = buildArgs("CREATE EXTERNAL TABLE table_test (col_a VARCHAR, col_b TIMESTAMP) TYPE 'test';", "INSERT INTO table_test SELECT '3', TIMESTAMP '2018-07-01 21:26:06';", "INSERT INTO table_test SELECT '4', TIMESTAMP '2018-07-01 21:26:07';", "INSERT INTO table_test SELECT '6', TIMESTAMP '2018-07-01 21:26:08';", "INSERT INTO table_test SELECT '7', TIMESTAMP '2018-07-01 21:26:09';", "SELECT HOP_END(col_b, INTERVAL '1' SECOND, INTERVAL '2' SECOND), count(*) FROM " + "table_test GROUP BY HOP(col_b, INTERVAL '1' SECOND, INTERVAL '2' SECOND);");    BeamSqlLine.runSqlLine(args, null, byteArrayOutputStream, null);    List<List<String>> lines = toLines(byteArrayOutputStream);    assertThat(Arrays.asList(Arrays.asList("2018-07-01 21:26:07", "1"), Arrays.asList("2018-07-01 21:26:08", "2"), Arrays.asList("2018-07-01 21:26:09", "2"), Arrays.asList("2018-07-01 21:26:10", "2"), Arrays.asList("2018-07-01 21:26:11", "1")), everyItem(IsIn.isOneOf(lines.toArray())));}
public BeamSqlCli beam_f25310_0(MetaStore metaStore)
{    return metaStore(metaStore, false, PipelineOptionsFactory.create());}
public BeamSqlCli beam_f25311_0(MetaStore metaStore, boolean autoLoadUdfUdaf, PipelineOptions pipelineOptions)
{    this.metaStore = metaStore;    BeamSqlEnv.BeamSqlEnvBuilder builder = BeamSqlEnv.builder(metaStore);    if (autoLoadUdfUdaf) {        builder.autoLoadUserDefinedFunctions();    }    builder.setPipelineOptions(pipelineOptions);    this.env = builder.build();    return this;}
public Row beam_f25322_0(Row input)
{    System.out.println(input.getValues() + suffix);    return input;}
private static PCollection<Customer> beam_f25323_0(Pipeline pipeline)
{    return pipeline.apply(Create.of(new Customer(1, "Foo", "Wonderland"), new Customer(2, "Bar", "Super Kingdom"), new Customer(3, "Baz", "Wonderland"), new Customer(4, "Grault", "Wonderland"), new Customer(5, "Qux", "Super Kingdom")));}
public int beam_f25332_0()
{    return Objects.hash(name, id, countryOfResidence);}
public int beam_f25333_0()
{    return id;}
public void beam_f25342_0(String key)
{    Map<String, String> options = new HashMap<>(connection.getPipelineOptionsMap());    options.remove(key);    connection.setPipelineOptionsMap(options);}
public void beam_f25343_0()
{    connection.setPipelineOptionsMap(Collections.emptyMap());}
public Collection<Function> beam_f25352_0(String name)
{    return Collections.emptySet();}
public Set<String> beam_f25353_0()
{    return tableProvider.getSubProviders();}
public RelProtoDataType beam_f25362_0(String name)
{    return illegal();}
public Set<String> beam_f25363_0()
{    return illegal();}
public static BeamCalciteTable beam_f25372_0(BeamSqlTable table)
{    return new BeamCalciteTable(table, ImmutableMap.of(), null);}
public RelDataType beam_f25373_0(RelDataTypeFactory typeFactory)
{    return CalciteUtils.toCalciteRowType(this.beamTable.getSchema(), typeFactory);}
public static BeamSqlEnv beam_f25382_0(TableProvider tableProvider)
{    return builder(tableProvider).setPipelineOptions(PipelineOptionsFactory.create()).build();}
public static BeamSqlEnv beam_f25383_0(TableProvider... tableProviders)
{    InMemoryMetaStore inMemoryMetaStore = new InMemoryMetaStore();    for (TableProvider tableProvider : tableProviders) {        inMemoryMetaStore.registerProvider(tableProvider);    }    return withTableProvider(inMemoryMetaStore);}
public BeamSqlEnvBuilder beam_f25392_0(RuleSet[] ruleSets)
{    this.ruleSets = ruleSets;    return this;}
public BeamSqlEnvBuilder beam_f25393_0(String functionName, Class<?> clazz, String method)
{    functionSet.add(new SimpleEntry<>(functionName, UdfImpl.create(clazz, method)));    return this;}
private void beam_f25402_0(JdbcConnection jdbcConnection)
{                schemaMap.forEach(jdbcConnection::setSchema);    if (Strings.isNullOrEmpty(currentSchemaName)) {        return;    }    try {        jdbcConnection.setSchema(currentSchemaName);    } catch (SQLException e) {        throw new RuntimeException(e);    }}
private void beam_f25403_0()
{    if (!autoLoadBuiltinFunctions) {        return;    }    for (BeamBuiltinFunctionProvider provider : ServiceLoader.load(BeamBuiltinFunctionProvider.class)) {        loadBuiltinUdf(provider.getBuiltinMethods());    }}
public boolean beam_f25412_0()
{    return unknown;}
public Double beam_f25413_0()
{    return rowCount;}
public Statement beam_f25422_0() throws SQLException
{    return connection.createStatement();}
public PreparedStatement beam_f25423_0(String sql) throws SQLException
{    return connection.prepareStatement(sql);}
public DatabaseMetaData beam_f25432_0() throws SQLException
{    return connection.getMetaData();}
public void beam_f25433_0(boolean readOnly) throws SQLException
{    connection.setReadOnly(readOnly);}
public PreparedStatement beam_f25442_0(String sql, int resultSetType, int resultSetConcurrency) throws SQLException
{    return connection.prepareStatement(sql, resultSetType, resultSetConcurrency);}
public CallableStatement beam_f25443_0(String sql, int resultSetType, int resultSetConcurrency) throws SQLException
{    return connection.prepareCall(sql, resultSetType, resultSetConcurrency);}
public Statement beam_f25452_0(int resultSetType, int resultSetConcurrency, int resultSetHoldability) throws SQLException
{    return connection.createStatement(resultSetType, resultSetConcurrency, resultSetHoldability);}
public PreparedStatement beam_f25453_0(String sql, int resultSetType, int resultSetConcurrency, int resultSetHoldability) throws SQLException
{    return connection.prepareStatement(sql, resultSetType, resultSetConcurrency, resultSetHoldability);}
public boolean beam_f25462_0(int timeout) throws SQLException
{    return connection.isValid(timeout);}
public void beam_f25463_0(String name, String value) throws SQLClientInfoException
{    connection.setClientInfo(name, value);}
public void beam_f25472_0(Executor executor, int milliseconds) throws SQLException
{    connection.setNetworkTimeout(executor, milliseconds);}
public int beam_f25473_0() throws SQLException
{    return connection.getNetworkTimeout();}
public Enumerator<T> beam_f25482_0(Queryable<T> queryable)
{    return connection.executeQuery(queryable);}
public AvaticaConnection beam_f25483_0(UnregisteredDriver driver, AvaticaFactory avaticaFactory, String url, Properties info, CalciteSchema rootSchema, JavaTypeFactory typeFactory)
{    return this.factory.newConnection(driver, avaticaFactory, url, info, rootSchema, typeFactory);}
public MetadataDef<BuiltInMetadata.NonCumulativeCost> beam_f25492_0()
{    return BuiltInMetadata.NonCumulativeCost.DEF;}
public RelOptCost beam_f25493_0(RelNode rel, RelMetadataQuery mq)
{        if (!(rel instanceof BeamRelNode)) {        return rel.computeSelfCost(rel.getCluster().getPlanner(), mq);    }                List<List> costKeys = mq.map.entrySet().stream().filter(entry -> entry.getValue() instanceof BeamCostModel).filter(entry -> ((BeamCostModel) entry.getValue()).isInfinite()).map(Map.Entry::getKey).collect(Collectors.toList());    costKeys.forEach(mq.map::remove);    return ((BeamRelNode) rel).beamComputeSelfCost(rel.getCluster().getPlanner(), mq);}
 void beam_f25502_0(String name, TableProvider tableProvider)
{    BeamCalciteSchema beamCalciteSchema = new BeamCalciteSchema(this, tableProvider);    getRootSchema().add(name, beamCalciteSchema);}
protected AvaticaFactory beam_f25503_0()
{    return JdbcFactory.wrap((CalciteFactory) super.createFactory());}
public SqlOperator beam_f25512_0()
{    return OPERATOR;}
public List<SqlNode> beam_f25513_0()
{    return ImmutableNullableList.of(name, expression);}
private Table beam_f25522_0()
{    return Table.builder().type(SqlDdlNodes.getString(type)).name(SqlDdlNodes.name(name)).schema(columnList.stream().collect(toSchema())).comment(SqlDdlNodes.getString(comment)).location(SqlDdlNodes.getString(location)).properties((tblProperties == null) ? new JSONObject() : parseObject(SqlDdlNodes.getString(tblProperties))).build();}
public static SqlDropTable beam_f25523_0(SqlParserPos pos, boolean ifExists, SqlIdentifier name)
{    return new SqlDropTable(pos, ifExists, name);}
public String beam_f25532_0()
{    return "{inf}";}
public String beam_f25533_0()
{    return "{huge}";}
private static double beam_f25542_0(BeamCostModel cost)
{    return cost.cpu + cost.cpuRate * RATE_IMPORTANCE;}
public double beam_f25543_0()
{    return 0;}
public String beam_f25552_0()
{    return "{" + cpu + " cpu, " + cpuRate + " cpuRate " + "}";}
public static BeamCostModel beam_f25553_0(RelOptCost ic)
{    BeamCostModel inputCost;    if (ic instanceof BeamCostModel) {        inputCost = ((BeamCostModel) ic);    } else {        inputCost = BeamCostModel.FACTORY.makeCost(ic.getRows(), ic.getCpu(), ic.getIo());    }    return inputCost;}
public int beam_f25562_0()
{    return 38;}
public boolean beam_f25563_0()
{    return true;}
public NodeStats beam_f25572_0(RelNode rel, RelMetadataQuery mq)
{    if (rel instanceof BeamRelNode) {        return this.getBeamNodeStats((BeamRelNode) rel, mq);    }    return NodeStats.UNKNOWN;}
private NodeStats beam_f25573_0(BeamRelNode rel, RelMetadataQuery mq)
{                                            List<List> keys = mq.map.entrySet().stream().filter(entry -> entry.getValue() instanceof NodeStats).filter(entry -> ((NodeStats) entry.getValue()).isUnknown()).map(Map.Entry::getKey).collect(Collectors.toList());    for (List key : keys) {        mq.map.remove(key);    }    return rel.estimateNodeStats(mq);}
 static DoFn<KV<Row, Row>, Row> beam_f25582_0(Schema outputSchema, int windowStartFieldIndex)
{    return new DoFn<KV<Row, Row>, Row>() {        @ProcessElement        public void processElement(@Element KV<Row, Row> kvRow, BoundedWindow window, OutputReceiver<Row> o) {            List<Object> fieldValues = Lists.newArrayListWithCapacity(kvRow.getKey().getValues().size() + kvRow.getValue().getValues().size());            fieldValues.addAll(kvRow.getKey().getValues());            fieldValues.addAll(kvRow.getValue().getValues());            if (windowStartFieldIndex != -1) {                fieldValues.add(windowStartFieldIndex, ((IntervalWindow) window).start());            }            o.output(Row.withSchema(outputSchema).addValues(fieldValues).build());        }    };}
public void beam_f25583_0(@Element KV<Row, Row> kvRow, BoundedWindow window, OutputReceiver<Row> o)
{    List<Object> fieldValues = Lists.newArrayListWithCapacity(kvRow.getKey().getValues().size() + kvRow.getValue().getValues().size());    fieldValues.addAll(kvRow.getKey().getValues());    fieldValues.addAll(kvRow.getValue().getValues());    if (windowStartFieldIndex != -1) {        fieldValues.add(windowStartFieldIndex, ((IntervalWindow) window).start());    }    o.output(Row.withSchema(outputSchema).addValues(fieldValues).build());}
public boolean beam_f25592_0()
{    return (input instanceof BeamSortRel) && ((BeamSortRel) input).isLimitOnly();}
 ScriptEvaluator beam_f25593_0()
{    ScriptEvaluator se = new ScriptEvaluator();    se.setParameters(new String[] { outputSchemaParam.name, processContextParam.name, DataContext.ROOT.name }, new Class[] { (Class) outputSchemaParam.getType(), (Class) processContextParam.getType(), (Class) DataContext.ROOT.getType() });    try {        se.cook(processElementBlock);    } catch (CompileException e) {        throw new RuntimeException("Could not compile CalcFn: " + processElementBlock, e);    }    return se;}
private static Expression beam_f25602_0(Expression input, FieldType mapValueType)
{    ParameterExpression value = Expressions.parameter(Object.class);    BlockBuilder block = new BlockBuilder();    block.add(value(value, mapValueType));    return Expressions.new_(WrappedMap.class, ImmutableList.of(Types.castIfNecessary(Map.class, input)), ImmutableList.<MemberDeclaration>of(Expressions.methodDecl(Modifier.PUBLIC, Object.class, "value", ImmutableList.of(value), block.toBlock())));}
private static Expression beam_f25603_0(Expression input, Schema schema)
{    ParameterExpression row = Expressions.parameter(Row.class);    ParameterExpression index = Expressions.parameter(int.class);    BlockBuilder body = new BlockBuilder(/* optimizing= */    false);    for (int i = 0; i < schema.getFieldCount(); i++) {        BlockBuilder list = new BlockBuilder(/* optimizing= */        false, body);        Expression returnValue = value(list, i, /* storageType= */        null, row, schema);        list.append(returnValue);        body.append("if i=" + i, Expressions.block(Expressions.ifThen(Expressions.equal(index, Expressions.constant(i, int.class)), list.toBlock())));    }    body.add(Expressions.throw_(Expressions.new_(IndexOutOfBoundsException.class)));    return Expressions.new_(WrappedRow.class, ImmutableList.of(Types.castIfNecessary(Row.class, input)), ImmutableList.<MemberDeclaration>of(Expressions.methodDecl(Modifier.PUBLIC, Object.class, "field", ImmutableList.of(row, index), body.toBlock())));}
public V beam_f25612_0(Object key)
{    return value(map.get(key));}
public T beam_f25613_0(int index)
{    return value(values.get(index));}
public RelOptCost beam_f25622_0(RelOptPlanner planner, RelMetadataQuery mq)
{        return planner.getCostFactory().makeHugeCost();}
public Result beam_f25623_0(EnumerableRelImplementor implementor, Prefer prefer)
{    final BlockBuilder list = new BlockBuilder();    final RelDataType rowType = getRowType();    final PhysType physType = PhysTypeImpl.of(implementor.getTypeFactory(), rowType, prefer.preferArray());    final Expression node = implementor.stash((BeamRelNode) getInput(), BeamRelNode.class);    list.add(Expressions.call(BeamEnumerableConverter.class, "toEnumerable", node));    return implementor.result(physType, list.toBlock());}
private static Enumerable<Object> beam_f25632_0(PipelineOptions options, BeamRelNode node)
{    long id = options.getOptionsId();    ConcurrentLinkedQueue<Row> values = new ConcurrentLinkedQueue<>();    checkArgument(options.getRunner().getCanonicalName().equals("org.apache.beam.runners.direct.DirectRunner"), "SELECT without INSERT is only supported in DirectRunner in SQL Shell.");    int limitCount = getLimitCount(node);    Collector.globalValues.put(id, values);    limitRun(options, node, new Collector(), values, limitCount);    Collector.globalValues.remove(id);        while (values.size() > limitCount) {        values.remove();    }    return Linq4j.asEnumerable(rowToAvaticaAndUnboxValues(values));}
public void beam_f25633_0(StartBundleContext context)
{    long id = context.getPipelineOptions().getOptionsId();    values = globalValues.get(id);}
private static boolean beam_f25642_0(Pipeline p)
{    class BoundednessVisitor extends PipelineVisitor.Defaults {        IsBounded boundedness = IsBounded.BOUNDED;        @Override        public void visitValue(PValue value, Node producer) {            if (value instanceof PCollection) {                boundedness = boundedness.and(((PCollection) value).isBounded());            }        }    }    BoundednessVisitor visitor = new BoundednessVisitor();    p.traverseTopologically(visitor);    return visitor.boundedness == IsBounded.UNBOUNDED;}
public void beam_f25643_0(PValue value, Node producer)
{    if (value instanceof PCollection) {        boundedness = boundedness.and(((PCollection) value).isBounded());    }}
public void beam_f25652_0(RelOptPlanner planner)
{    planner.addRule(BeamIOSinkRule.INSTANCE);    super.register(planner);}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25653_0()
{    return new Transform();}
public BeamCostModel beam_f25662_0(RelOptPlanner planner, RelMetadataQuery mq)
{    NodeStats estimates = BeamSqlRelUtils.getNodeStats(this, mq);    return BeamCostModel.FACTORY.makeCost(estimates.getRowCount(), estimates.getRate());}
protected BeamSqlTable beam_f25663_0()
{    return beamTable;}
public static boolean beam_f25672_0(Join join)
{    try {        extractJoinRexNodes(join.getCondition());    } catch (UnsupportedOperationException e) {        return false;    }    return true;}
public PCollectionList<KV<Row, Row>> beam_f25673_0(PCollectionList<Row> pinput)
{    BeamRelNode leftRelNode = BeamSqlRelUtils.getBeamRelInput(left);    Schema leftSchema = CalciteUtils.toSchema(left.getRowType());    Schema rightSchema = CalciteUtils.toSchema(right.getRowType());    assert pinput.size() == 2;    PCollection<Row> leftRows = pinput.get(0);    PCollection<Row> rightRows = pinput.get(1);    int leftRowColumnCount = leftRelNode.getRowType().getFieldCount();        List<Pair<RexNode, RexNode>> pairs = extractJoinRexNodes(condition);            Schema extractKeySchemaLeft = pairs.stream().map(pair -> getFieldBasedOnRexNode(leftSchema, pair.getKey(), 0)).collect(toSchema());    Schema extractKeySchemaRight = pairs.stream().map(pair -> getFieldBasedOnRexNode(rightSchema, pair.getValue(), leftRowColumnCount)).collect(toSchema());    SchemaCoder<Row> extractKeyRowCoder = SchemaCoder.of(extractKeySchemaLeft);        PCollection<KV<Row, Row>> extractedLeftRows = leftRows.apply("left_TimestampCombiner", Window.<Row>configure().withTimestampCombiner(TimestampCombiner.EARLIEST)).apply("left_ExtractJoinFields", MapElements.via(new BeamJoinTransforms.ExtractJoinFields(true, pairs, extractKeySchemaLeft, 0))).setCoder(KvCoder.of(extractKeyRowCoder, leftRows.getCoder()));    PCollection<KV<Row, Row>> extractedRightRows = rightRows.apply("right_TimestampCombiner", Window.<Row>configure().withTimestampCombiner(TimestampCombiner.EARLIEST)).apply("right_ExtractJoinFields", MapElements.via(new BeamJoinTransforms.ExtractJoinFields(false, pairs, extractKeySchemaRight, leftRowColumnCount))).setCoder(KvCoder.of(extractKeyRowCoder, rightRows.getCoder()));    return PCollectionList.of(extractedLeftRows).and(extractedRightRows);}
public static PCollection.IsBounded beam_f25682_0(RelNode relNode)
{    if (relNode instanceof BeamRelNode) {        return (((BeamRelNode) relNode).isBounded());    }    List<PCollection.IsBounded> boundednessOfInputs = new ArrayList<>();    for (RelNode inputRel : relNode.getInputs()) {        if (inputRel instanceof RelSubset) {                                    RelNode rel = ((RelSubset) inputRel).getBest();            if (rel == null) {                rel = ((RelSubset) inputRel).getRelList().get(0);            }            boundednessOfInputs.add(getBoundednessOfRelNode(rel));        } else {            boundednessOfInputs.add(getBoundednessOfRelNode(inputRel));        }    }        return (boundednessOfInputs.contains(PCollection.IsBounded.UNBOUNDED) ? PCollection.IsBounded.UNBOUNDED : PCollection.IsBounded.BOUNDED);}
public static boolean beam_f25683_0(RelNode relNode)
{    for (RelNode relInput : relNode.getInputs()) {        if (relInput instanceof RelSubset) {            relInput = ((RelSubset) relInput).getBest();        }                if (relInput != null && relInput instanceof BeamRelNode && (BeamJoinRel.seekable((BeamRelNode) relInput))) {            return true;        }    }        return false;}
public SetOp beam_f25693_0(RelTraitSet traitSet, List<RelNode> inputs, boolean all)
{    return new BeamMinusRel(getCluster(), traitSet, inputs, all);}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25694_0()
{    return new BeamSetOperatorRelBase(this, BeamSetOperatorRelBase.OpType.MINUS, all);}
public PCollection<Row> beam_f25703_0(PCollection<KV<Row, Row>> extractedLeftRows, PCollection<KV<Row, Row>> extractedRightRows, Schema leftSchema, Schema rightSchema)
{            boolean swapped = (extractedLeftRows.isBounded() == PCollection.IsBounded.BOUNDED);    JoinRelType realJoinType = (swapped && joinType != JoinRelType.INNER) ? JoinRelType.LEFT : joinType;    PCollection<KV<Row, Row>> realLeftRows = swapped ? extractedRightRows : extractedLeftRows;    PCollection<KV<Row, Row>> realRightRows = swapped ? extractedLeftRows : extractedRightRows;    Row realRightNullRow;    if (swapped) {        Schema leftNullSchema = buildNullSchema(leftSchema);        realRightRows = BeamJoinRel.setValueCoder(realRightRows, SchemaCoder.of(leftNullSchema));        realRightNullRow = Row.nullRow(leftNullSchema);    } else {        Schema rightNullSchema = buildNullSchema(rightSchema);        realRightRows = BeamJoinRel.setValueCoder(realRightRows, SchemaCoder.of(rightNullSchema));        realRightNullRow = Row.nullRow(rightNullSchema);    }        return sideInputJoinHelper(realJoinType, realLeftRows, realRightRows, realRightNullRow, swapped);}
private PCollection<Row> beam_f25704_0(JoinRelType joinType, PCollection<KV<Row, Row>> leftRows, PCollection<KV<Row, Row>> rightRows, Row rightNullRow, boolean swapped)
{    final PCollectionView<Map<Row, Iterable<Row>>> rowsView = rightRows.apply(View.asMultimap());    Schema schema = CalciteUtils.toSchema(getRowType());    return leftRows.apply(ParDo.of(new BeamJoinTransforms.SideInputJoinDoFn(joinType, rightNullRow, rowsView, swapped, schema)).withSideInputs(rowsView)).setRowSchema(schema);}
public PCollection<Row> beam_f25713_0(PCollectionList<Row> pinput)
{    checkArgument(pinput.size() == 1, "Wrong number of inputs for %s: %s", BeamIOSinkRel.class.getSimpleName(), pinput);    PCollection<Row> upstream = pinput.get(0);        if (fieldIndices.isEmpty()) {        return upstream.apply(Window.into(new GlobalWindows())).apply(new LimitTransform<>(startIndex)).setRowSchema(CalciteUtils.toSchema(getRowType()));    } else {        WindowingStrategy<?, ?> windowingStrategy = upstream.getWindowingStrategy();        if (!(windowingStrategy.getWindowFn() instanceof GlobalWindows)) {            throw new UnsupportedOperationException(String.format("`ORDER BY` is only supported for %s, actual windowing strategy: %s", GlobalWindows.class.getSimpleName(), windowingStrategy));        }        ReversedBeamSqlRowComparator comparator = new ReversedBeamSqlRowComparator(fieldIndices, orientation, nullsFirst);                PCollection<List<Row>> rawStream = upstream.apply("extractTopOffsetAndFetch", Top.of(startIndex + count, comparator).withoutDefaults()).setCoder(ListCoder.of(upstream.getCoder()));                if (startIndex > 0) {            rawStream = rawStream.apply("stripLeadingOffset", ParDo.of(new SubListFn<>(startIndex, startIndex + count))).setCoder(ListCoder.of(upstream.getCoder()));        }        return rawStream.apply("flatten", Flatten.iterables()).setRowSchema(CalciteUtils.toSchema(getRowType()));    }}
public PCollection<T> beam_f25714_0(PCollection<T> input)
{    Coder<T> coder = input.getCoder();    PCollection<KV<String, T>> keyedRow = input.apply(WithKeys.of("DummyKey")).setCoder(KvCoder.of(StringUtf8Coder.of(), coder));    return keyedRow.apply(ParDo.of(new LimitFn<T>(getCount(), startIndex)));}
public static BeamRelNode beam_f25723_0(RelNode input)
{    if (input instanceof RelSubset) {                input = ((RelSubset) input).getBest();    }    return (BeamRelNode) input;}
public static RelNode beam_f25724_0(RelNode input)
{    RelNode result = input;    if (input instanceof RelSubset) {                result = ((RelSubset) input).getBest();        result = result == null ? ((RelSubset) input).getOriginal() : result;    }    return result;}
public PTransform<PCollectionList<Row>, PCollection<Row>> beam_f25733_0()
{    return new BeamSetOperatorRelBase(this, BeamSetOperatorRelBase.OpType.UNION, all);}
public NodeStats beam_f25734_0(RelMetadataQuery mq)
{        NodeStats summationOfEstimates = inputs.stream().map(input -> BeamSqlRelUtils.getNodeStats(input, mq)).reduce(NodeStats.create(0, 0, 0), NodeStats::plus);            summationOfEstimates = all ? summationOfEstimates : summationOfEstimates.multiply(0.5);    return summationOfEstimates;}
public void beam_f25743_0(@Element Row row, OutputReceiver<Row> out)
{    @Nullable    List<Object> rawValues = row.getArray(unnestIndex);    if (rawValues == null) {        return;    }    Schema.TypeName typeName = outputSchema.getField(unnestIndex).getType().getCollectionElementType().getTypeName();    for (Object uncollectedValue : rawValues) {        if (typeName.equals(Schema.TypeName.ROW)) {            Row nestedRow = (Row) uncollectedValue;            out.output(Row.withSchema(outputSchema).addValues(row.getValues()).addValues(nestedRow.getValues()).build());        } else {            out.output(Row.withSchema(outputSchema).addValues(row.getValues()).addValue(uncollectedValue).build());        }    }}
public Map<String, String> beam_f25744_0()
{    return ImmutableMap.of();}
private static Duration beam_f25753_0(List<RexNode> parameters, int parameterIndex)
{    return Duration.millis(longValue(parameters.get(parameterIndex)));}
private static long beam_f25754_0(RexNode operand)
{    if (operand instanceof RexLiteral) {                return ((Number) RexLiteral.value(operand)).longValue();    } else {        throw new IllegalArgumentException(String.format("[%s] is not valid.", operand));    }}
public void beam_f25763_0(final RelOptRuleCall call)
{    super.onMatch(new JoinRelOptRuleCall(call, rel -> {        Join topJoin = (Join) rel;        Join bottomJoin = (Join) ((Join) rel).getRight();        return BeamJoinRel.isJoinLegal(topJoin) && BeamJoinRel.isJoinLegal(bottomJoin);    }));}
public void beam_f25764_0(RelOptRuleCall call)
{    super.onMatch(new JoinRelOptRuleCall(call, rel -> {        Join topJoin = (Join) rel.getInput(0);        Join bottomJoin = (Join) ((Join) rel.getInput(0)).getLeft();        return BeamJoinRel.isJoinLegal(topJoin) && BeamJoinRel.isJoinLegal(bottomJoin);    }));}
public void beam_f25773_0(RelOptRuleCall call)
{    LogicalCorrelate correlate = call.rel(0);    RelNode outer = call.rel(1);    RelNode uncollect = call.rel(2);    if (correlate.getCorrelationId().getId() != 0) {                return;    }    if (correlate.getRequiredColumns().cardinality() != 1) {                return;    }    if (correlate.getJoinType() != JoinRelType.INNER) {        return;    }    if (!(uncollect instanceof Uncollect)) {                uncollect = ((SingleRel) uncollect).getInput();        if (uncollect instanceof RelSubset) {            uncollect = ((RelSubset) uncollect).getOriginal();        }        if (!(uncollect instanceof Uncollect)) {            return;        }    }    RelNode project = ((Uncollect) uncollect).getInput();    if (project instanceof RelSubset) {        project = ((RelSubset) project).getOriginal();    }    if (!(project instanceof LogicalProject)) {        return;    }    if (((LogicalProject) project).getProjects().size() != 1) {                return;    }    RexNode exp = ((LogicalProject) project).getProjects().get(0);    if (!(exp instanceof RexFieldAccess)) {        return;    }    int fieldIndex = ((RexFieldAccess) exp).getField().getIndex();    call.transformTo(new BeamUnnestRel(correlate.getCluster(), correlate.getTraitSet().replace(BeamLogicalConvention.INSTANCE), outer, call.rel(2).getRowType(), fieldIndex));}
public RelNode beam_f25774_0(RelNode rel)
{    Values values = (Values) rel;    return new BeamValuesRel(values.getCluster(), values.getRowType(), values.getTuples(), values.getTraitSet().replace(BeamLogicalConvention.INSTANCE));}
public List<RelNode> beam_f25783_0()
{    return originalCall.getParents();}
public RelBuilder beam_f25784_0()
{    return originalCall.builder();}
private static CallImplementor beam_f25793_0(Method method)
{    final NullPolicy nullPolicy = getNullPolicy(method);    return RexImpTable.createImplementor(new ScalarReflectiveCallNotNullImplementor(method), nullPolicy, false);}
private static NullPolicy beam_f25794_0(Method m)
{    if (m.getAnnotation(Strict.class) != null) {        return NullPolicy.STRICT;    } else if (m.getAnnotation(SemiStrict.class) != null) {        return NullPolicy.SEMI_STRICT;    } else if (m.getDeclaringClass().getAnnotation(Strict.class) != null) {        return NullPolicy.STRICT;    } else if (m.getDeclaringClass().getAnnotation(SemiStrict.class) != null) {        return NullPolicy.SEMI_STRICT;    } else {        return NullPolicy.NONE;    }}
public static Iterable<Row> beam_f25803_0(CSVFormat csvFormat, String line, Schema schema)
{        if (!line.endsWith(csvFormat.getRecordSeparator())) {        line += csvFormat.getRecordSeparator();    }    try (CSVParser parser = CSVParser.parse(line, csvFormat)) {        List<Row> rows = new ArrayList<>();        for (CSVRecord rawRecord : parser.getRecords()) {            if (rawRecord.size() != schema.getFieldCount()) {                throw new IllegalArgumentException(String.format("Expect %d fields, but actually %d", schema.getFieldCount(), rawRecord.size()));            }            rows.add(IntStream.range(0, schema.getFieldCount()).mapToObj(idx -> autoCastField(schema.getField(idx), rawRecord.get(idx))).collect(toRow(schema)));        }        return rows;    } catch (IOException e) {        throw new IllegalArgumentException(String.format("Could not parse CSV records from %s with format %s", line, csvFormat), e);    }}
public static String beam_f25804_0(Row row, CSVFormat csvFormat)
{    StringWriter writer = new StringWriter();    try (CSVPrinter printer = csvFormat.print(writer)) {        for (int i = 0; i < row.getFieldCount(); i++) {            printer.print(row.getValue(i).toString());        }        printer.println();    } catch (IOException e) {        throw new IllegalArgumentException("encodeRecord failed!", e);    }    return writer.toString();}
private static String beam_f25813_0(JdbcConnection connection)
{    try {        return connection.getSchema();    } catch (SQLException e) {        throw new IllegalStateException("Unable to get current schema name from JdbcConnection. " + "Assuming table names in the query are fully-qualified from the root.", e);    }}
private static void beam_f25814_0(JdbcConnection connection, List<TableName> tableNames, SchemaWithName defaultSchema)
{    Set<String> topLevelSchemas = connection.getRootSchema().getSubSchemaNames();    List<TableName> simpleIdentifiers = tableNames.stream().filter(TableName::isSimple).collect(toList());    List<TableName> withoutMatchingSchemas = tableNames.stream().filter(name -> name.isCompound() && !topLevelSchemas.contains(name.getPrefix())).collect(toList());    List<TableName> explicitlyInDefaulSchema = tableNames.stream().filter(name -> name.isCompound() && name.getPrefix().equals(defaultSchema.name)).map(TableName::removePrefix).collect(toList());    List<TableName> shouldGoIntoDefaultSchema = ImmutableList.<TableName>builder().addAll(simpleIdentifiers).addAll(withoutMatchingSchemas).addAll(explicitlyInDefaulSchema).build();    defaultSchema.getCustomTableResolver().registerKnownTableNames(shouldGoIntoDefaultSchema);}
 String beam_f25823_0()
{    return name;}
 CustomTableResolver beam_f25824_0()
{    checkState(supportsCustomResolution());    return (CustomTableResolver) getTableProvider();}
public Row beam_f25833_0(Row accumulator, Row input)
{    return EMPTY_ROW;}
public Row beam_f25834_0(Iterable<Row> accumulators)
{    return EMPTY_ROW;}
 static CovarianceAccumulator beam_f25843_0(BigDecimal inputElementX, BigDecimal inputElementY)
{    return newCovarianceAccumulator(BigDecimal.ZERO, BigDecimal.ONE, inputElementX, inputElementY);}
 CovarianceAccumulator beam_f25844_0(CovarianceAccumulator otherCovariance)
{    if (EMPTY.equals(this)) {        return otherCovariance;    }    if (EMPTY.equals(otherCovariance)) {        return this;    }    BigDecimal increment = calculateIncrement(this, otherCovariance);    BigDecimal combinedCovariance = this.covariance().add(otherCovariance.covariance()).add(increment);    return newCovarianceAccumulator(combinedCovariance, this.count().add(otherCovariance.count()), calculateXavg(this, otherCovariance), calculateYavg(this, otherCovariance));}
public CovarianceAccumulator beam_f25853_0(CovarianceAccumulator currentVariance, Row rawInput)
{    if (rawInput == null) {        return currentVariance;    }    checkArgument(rawInput.getFieldCount() > 1);    return currentVariance.combineWith(CovarianceAccumulator.ofSingleElement(SqlFunctions.toBigDecimal((Object) rawInput.getValue(0)), SqlFunctions.toBigDecimal((Object) rawInput.getValue(1))));}
public CovarianceAccumulator beam_f25854_0(Iterable<CovarianceAccumulator> covariances)
{    return StreamSupport.stream(covariances.spliterator(), false).reduce(CovarianceAccumulator.ofZeroElements(), CovarianceAccumulator::combineWith);}
public static VarianceFn beam_f25863_0(Schema.TypeName typeName)
{    return newPopulation(BigDecimalConverter.forSqlType(typeName));}
public static VarianceFn beam_f25864_0(SerializableFunction<BigDecimal, V> decimalConverter)
{    return new VarianceFn<>(POP, decimalConverter);}
public static CombineFn<?, ?, ?> beam_f25873_0(String functionName, Schema.FieldType fieldType)
{    Function<Schema.FieldType, CombineFn<?, ?, ?>> aggregatorFactory = BUILTIN_AGGREGATOR_FACTORIES.get(functionName);    if (aggregatorFactory != null) {        return aggregatorFactory.apply(fieldType);    }    throw new UnsupportedOperationException(String.format("Aggregator [%s] is not supported", functionName));}
 static CombineFn beam_f25874_0(FieldType fieldType)
{    if (CalciteUtils.isDateTimeType(fieldType)) {        return new CustMax<>();    }    switch(fieldType.getTypeName()) {        case BOOLEAN:        case INT16:        case BYTE:        case FLOAT:        case DATETIME:        case DECIMAL:        case STRING:            return new CustMax<>();        case INT32:            return Max.ofIntegers();        case INT64:            return Max.ofLongs();        case DOUBLE:            return Max.ofDoubles();        default:            throw new UnsupportedOperationException(String.format("[%s] is not support in MAX", fieldType));    }}
public BigDecimal beam_f25883_0(BigDecimal left, BigDecimal right)
{    return left.add(right);}
public KV<Integer, BigDecimal> beam_f25884_0()
{    return KV.of(0, BigDecimal.ZERO);}
public Short beam_f25893_0(KV<Integer, BigDecimal> accumulator)
{    return accumulator.getKey() == 0 ? null : prepareOutput(accumulator).shortValue();}
public BigDecimal beam_f25894_0(Short record)
{    return new BigDecimal(record);}
public KV<Row, Row> beam_f25903_0(Row input)
{    Row row = joinColumns.stream().map(v -> getValue(v, input, leftRowColumnCount)).collect(toRow(schema));    return KV.of(row, input);}
private Schema.Field beam_f25904_0(Schema schema, Integer fieldIndex)
{    Schema.Field original = schema.getField(fieldIndex);    return original.withName("c" + fieldIndex);}
public void beam_f25913_0(ProcessContext context)
{    Row factRow = context.element();    Row joinSubRow = extractJoinSubRow(factRow);    List<Row> lookupRows = seekableTable.seekRow(joinSubRow);    for (Row lr : lookupRows) {        context.output(combineTwoRowsIntoOne(factRow, lr, factColOffset != 0, outputSchema));    }}
public void beam_f25914_0()
{    seekableTable.tearDown();}
public RelDataType beam_f25923_0(RelDataTypeFactory typeFactory)
{    ParameterizedType parameterizedType = findCombineFnSuperClass();    return CalciteUtils.sqlTypeWithAutoCast(typeFactory, parameterizedType.getActualTypeArguments()[0]);}
private ParameterizedType beam_f25924_0()
{    Class clazz = combineFn.getClass();    while (!clazz.getSuperclass().equals(CombineFn.class)) {        clazz = clazz.getSuperclass();    }    if (!(clazz.getGenericSuperclass() instanceof ParameterizedType)) {        throw new IllegalStateException("Subclass of " + CombineFn.class + " must be parameterized to be used as a UDAF");    } else {        return (ParameterizedType) clazz.getGenericSuperclass();    }}
public Long beam_f25933_0(byte[] bytes)
{    return (long) bytes.length;}
public String beam_f25934_0(String str)
{    return new StringBuilder(str).reverse().toString();}
public String beam_f25943_0(String originalValue, Long returnLength, String pattern)
{    if (returnLength < -1 || pattern.isEmpty()) {        throw new IllegalArgumentException("returnLength cannot be 0 or pattern cannot be empty.");    }    if (originalValue.length() == returnLength) {        return originalValue;    } else if (originalValue.length() < returnLength) {                return StringUtils.rightPad(originalValue, Math.toIntExact(returnLength), pattern);    } else {                return originalValue.substring(0, Math.toIntExact(returnLength));    }}
public byte[] beam_f25944_0(byte[] originalValue, Long returnLength)
{    return lpad(originalValue, returnLength, " ".getBytes(UTF_8));}
public Boolean beam_f25953_0(Double value)
{    return Double.isNaN(value);}
public static Function beam_f25954_0(Class<?> clazz, String methodName)
{    final Method method = findMethod(clazz, methodName);    if (method == null) {        return null;    }    return create(method);}
public ParameterListBuilder beam_f25963_0(final Class<?> type, final String name, final boolean optional)
{    final int ordinal = builder.size();    builder.add(new FunctionParameter() {        @Override        public int getOrdinal() {            return ordinal;        }        @Override        public String getName() {            return name;        }        @Override        public RelDataType getType(RelDataTypeFactory typeFactory) {            return CalciteUtils.sqlTypeWithAutoCast(typeFactory, type);        }        @Override        public boolean isOptional() {            return optional;        }    });    return this;}
public int beam_f25964_0()
{    return ordinal;}
public static SqlTypeName beam_f25973_0(FieldType type)
{    switch(type.getTypeName()) {        case ROW:            return SqlTypeName.ROW;        case ARRAY:            return SqlTypeName.ARRAY;        case MAP:            return SqlTypeName.MAP;        default:            SqlTypeName typeName = BEAM_TO_CALCITE_TYPE_MAPPING.get(type.withNullable(false));            if (typeName != null) {                return typeName;            } else {                                return BEAM_TO_CALCITE_DEFAULT_MAPPING.get(type);            }    }}
public static FieldType beam_f25974_0(SqlTypeName sqlTypeName)
{    switch(sqlTypeName) {        case MAP:        case MULTISET:        case ARRAY:        case ROW:            throw new IllegalArgumentException(String.format("%s is a type constructor that takes parameters, not a type," + "so it cannot be converted to a %s", sqlTypeName, Schema.FieldType.class.getSimpleName()));        default:            return CALCITE_TO_BEAM_TYPE_MAPPING.get(sqlTypeName);    }}
public int beam_f25983_0()
{    return index;}
public static SerializableRexNode.Builder beam_f25984_0(RexNode rexNode)
{    return new SerializableRexNode.Builder(rexNode);}
public PCollection<GenericRecord> beam_f25993_0(PCollection<Row> input)
{    return input.apply("RowsToGenericRecord", ParDo.of(new DoFn<Row, GenericRecord>() {        @ProcessElement        public void processElement(ProcessContext c) {            GenericRecord genericRecord = AvroUtils.toGenericRecord(c.element(), AvroUtils.toAvroSchema(beamSchema()));            c.output(genericRecord);        }    })).setCoder(AvroCoder.of(GenericRecord.class, AvroUtils.toAvroSchema(beamSchema())));}
public void beam_f25994_0(ProcessContext c)
{    GenericRecord genericRecord = AvroUtils.toGenericRecord(c.element(), AvroUtils.toAvroSchema(beamSchema()));    c.output(genericRecord);}
public TableProvider beam_f26003_0(String name)
{                                                                                                List<TableName> tablesToLookFor = knownTables.stream().filter(TableName::isCompound).filter(tableName -> tableName.getPrefix().equals(name)).collect(toList());    return tablesToLookFor.size() > 0 ? new TableNameTrackingProvider(1, tablesToLookFor) : null;}
public TableProvider beam_f26004_0(String name)
{                                                    List<TableName> matchingTables = tableNames.stream().filter(TableName::isCompound).filter(tableName -> tableName.getPath().size() > schemaLevel).filter(tableName -> tableName.getPath().get(schemaLevel).equals(name)).collect(toList());    return matchingTables.size() > 0 ? new TableNameTrackingProvider(schemaLevel + 1, matchingTables) : null;}
public PCollection<KV<byte[], byte[]>> beam_f26015_0(PCollection<Row> input)
{    return input.apply("encodeRecord", ParDo.of(new DoFn<Row, KV<byte[], byte[]>>() {        @ProcessElement        public void processElement(ProcessContext c) {            Row in = c.element();            c.output(KV.of(new byte[] {}, beamRow2CsvLine(in, format).getBytes(UTF_8)));        }    }));}
public void beam_f26016_0(ProcessContext c)
{    Row in = c.element();    c.output(KV.of(new byte[] {}, beamRow2CsvLine(in, format).getBytes(UTF_8)));}
public BeamTableStatistics beam_f26025_1(PipelineOptions options)
{    if (rowCountStatistics == null) {        try {            rowCountStatistics = BeamTableStatistics.createUnboundedTableStatistics(this.computeRate(numberOfRecordsForRate));        } catch (Exception e) {                        rowCountStatistics = BeamTableStatistics.UNBOUNDED_UNKNOWN;        }    }    return rowCountStatistics;}
 double beam_f26026_0(int numberOfRecords) throws NoEstimationException
{    Properties props = new Properties();    props.put("bootstrap.servers", bootstrapServers);    props.put("session.timeout.ms", "30000");    props.put("key.deserializer", "org.apache.kafka.common.serialization.ByteArrayDeserializer");    props.put("value.deserializer", "org.apache.kafka.common.serialization.ByteArrayDeserializer");    KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);    return computeRate(consumer, numberOfRecords);}
public PCollection.IsBounded beam_f26035_0()
{    return PCollection.IsBounded.BOUNDED;}
public BeamTableStatistics beam_f26036_0(PipelineOptions options)
{    return BeamTableStatistics.BOUNDED_UNKNOWN;}
private PubsubIO.Write<PubsubMessage> beam_f26045_0()
{    PubsubIO.Write<PubsubMessage> write = PubsubIO.writeMessages().to(getDeadLetterQueue());    return (getTimestampAttribute() == null) ? write : write.withTimestampAttribute(getTimestampAttribute());}
public POutput beam_f26046_0(PCollection<Row> input)
{    throw new UnsupportedOperationException("Writing to a Pubsub topic is not supported");}
public void beam_f26055_0(ProcessContext context)
{    try {        List<Object> values = getFieldValues(context);        context.output(Row.withSchema(messageSchema()).addValues(values).build());    } catch (UnsupportedRowJsonException jsonException) {        if (useDlq()) {            context.output(DLQ_TAG, context.element());        } else {            throw new RuntimeException("Error parsing message", jsonException);        }    }}
private List<Object> beam_f26056_0(ProcessContext context)
{    return messageSchema().getFields().stream().map(field -> getValueForField(field, context.timestamp(), context.element())).collect(toList());}
public PCollection<Row> beam_f26065_0(PBegin begin)
{    return begin.apply(GenerateSequence.from(0).withRate(elementsPerSecond, Duration.standardSeconds(1))).apply(MapElements.into(TypeDescriptor.of(Row.class)).via(elm -> Row.withSchema(TABLE_SCHEMA).addValues(elm, Instant.now()).build())).setRowSchema(getSchema());}
public BeamTableStatistics beam_f26066_0(PipelineOptions options)
{    return BeamTableStatistics.createUnboundedTableStatistics((double) elementsPerSecond);}
public static TestBoundedTable beam_f26075_0(final Object... args)
{    return new TestBoundedTable(TestTableUtils.buildBeamSqlSchema(args));}
public static TestBoundedTable beam_f26076_0(final Schema type)
{    return new TestBoundedTable(type);}
public void beam_f26085_0(Table table)
{    tables().put(table.getName(), new TableWithRows(instanceId, table));}
public void beam_f26086_0(String tableName)
{    tables().remove(tableName);}
public PCollection<Row> beam_f26095_0(PBegin begin)
{    TableWithRows tableWithRows = GLOBAL_TABLES.get(this.tableWithRows.tableProviderInstanceId).get(this.tableWithRows.table.getName());    return begin.apply(Create.of(tableWithRows.rows).withCoder(rowCoder()));}
public POutput beam_f26096_0(PCollection<Row> input)
{    input.apply(ParDo.of(new CollectorFn(tableWithRows)));    return PDone.in(input.getPipeline());}
public TestUnboundedTable beam_f26105_0(BeamTableStatistics statistics)
{    this.statistics = statistics;    return this;}
public BeamTableStatistics beam_f26106_0(PipelineOptions options)
{    return this.statistics;}
public PCollection<Row> beam_f26115_0(PBegin begin)
{    return begin.apply("ReadTextFiles", TextIO.read().from(filePattern)).apply("StringToRow", readConverter);}
public PDone beam_f26116_0(PCollection<Row> input)
{    return input.apply("RowToString", writeConverter).apply("WriteTextFiles", TextIO.write().withDelimiter(new char[] {}).to(filePattern));}
 Map<String, Class<? extends BeamSqlUdf>> beam_f26125_0()
{    return Collections.emptyMap();}
 Map<String, SerializableFunction<?, ?>> beam_f26126_0()
{    return Collections.emptyMap();}
private void beam_f26135_0(TableProvider provider)
{    Map<String, Table> tables = provider.getTables();    for (String tableName : tables.keySet()) {        if (this.tables.containsKey(tableName)) {            throw new IllegalStateException("Duplicate table: " + tableName + " from provider: " + provider);        }    }    this.tables.putAll(tables);}
 Map<String, TableProvider> beam_f26136_0()
{    return providers;}
public SqlTransform beam_f26145_0(String functionName, Class<? extends BeamSqlUdf> clazz)
{    return registerUdf(functionName, clazz, BeamSqlUdf.UDF_METHOD);}
public SqlTransform beam_f26146_0(String functionName, SerializableFunction sfn)
{    return registerUdf(functionName, sfn.getClass(), "apply");}
private static List<TableName> beam_f26155_0(SqlJoin join)
{    return ImmutableList.<TableName>builder().addAll(extractTableNamesFromNode(join.getLeft())).addAll(extractTableNamesFromNode(join.getRight())).build();}
private static List<TableName> beam_f26156_0(SqlIdentifier identifier)
{    return ImmutableList.of(TableName.create(identifier.names));}
public static DateTime beam_f26165_0(String str)
{    return findDateTimePattern(str).withZone(DateTimeZone.getDefault()).parseDateTime(str);}
public static DateTime beam_f26166_0(String str)
{        if (str.indexOf('.') == -1) {        return DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ssZ").parseDateTime(str);    } else {        return DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss.SSSZ").parseDateTime(str);    }}
public static Value beam_f26175_0(String timestampString)
{    DateTime dateTime = parseTimestampWithTimeZone(timestampString);        return Value.createTimestampValueFromUnixMicros(LongMath.checkedMultiply(dateTime.getMillis(), MICROS_PER_MILLI));}
private static void beam_f26176_0(long micros)
{    long subMilliPrecision = micros % 1000L;    if (subMilliPrecision != 0) {        throw new IllegalArgumentException(String.format("%s has sub-millisecond precision, which Beam ZetaSQL does" + " not currently support.", micros));    }}
public String beam_f26185_0(ResolvedColumn resolvedColumn)
{    return this.outputColumnMap.getOrDefault(resolvedColumn, resolvedColumn.getName());}
 static Builder beam_f26186_0(Map<String, Value> params)
{    return new Builder().withQueryParams(ImmutableMap.copyOf(params));}
private SimpleCatalog beam_f26195_0(SimpleCatalog currentCatalog, String nextCatalogName)
{    SimpleCatalog nextCatalog = new SimpleCatalog(nextCatalogName);    currentCatalog.addSimpleCatalog(nextCatalog);    return nextCatalog;}
 Builder beam_f26196_0(Map<String, Value> params)
{    this.queryParams = ImmutableMap.copyOf(params);    return this;}
public RexNode beam_f26205_0(RexBuilder rexBuilder, List<RexNode> operands)
{    Preconditions.checkArgument(operands.size() == 2, "IFNULL should have two arguments in function call.");    SqlOperator op = SqlStdOperatorTable.CASE;    List<RexNode> newOperands = ImmutableList.of(rexBuilder.makeCall(SqlStdOperatorTable.IS_NULL, ImmutableList.of(operands.get(0))), operands.get(1), operands.get(0));    return rexBuilder.makeCall(op, newOperands);}
public RexNode beam_f26206_0(RexBuilder rexBuilder, List<RexNode> operands)
{    Preconditions.checkArgument(operands.size() == 2, "NULLIF should have two arguments in function call.");    SqlOperator op = SqlStdOperatorMappingTable.ZETASQL_FUNCTION_TO_CALCITE_SQL_OPERATOR.get("$case_no_value");    List<RexNode> newOperands = ImmutableList.of(rexBuilder.makeCall(SqlStdOperatorTable.EQUALS, ImmutableList.of(operands.get(0), operands.get(1))), rexBuilder.makeNullLiteral(operands.get(1).getType()), operands.get(0));    return rexBuilder.makeCall(op, newOperands);}
public static Boolean beam_f26215_0(String str1, String str2)
{    return str1.endsWith(str2);}
public static String beam_f26216_0(String arg)
{    return arg;}
public static String beam_f26225_0(String str)
{    return ltrim(str, " ");}
public static String beam_f26226_0(String str, String seek)
{    return SqlFunctions.trim(true, false, seek, str, false);}
 static TableResolver beam_f26235_0(TableResolutionContext tableResolutionContext, String schemaName)
{    if (tableResolutionContext == null || !tableResolutionContext.hasCustomResolutionFor(schemaName)) {        return TableResolver.DEFAULT_ASSUME_LEAF_IS_TABLE;    }    return tableResolutionContext.getTableResolver(schemaName);}
 static SimpleTableWithPath beam_f26236_0(String topLevelSchema, List<String> path)
{    SimpleTableWithPath tableWithPath = new SimpleTableWithPath();    tableWithPath.table = new SimpleTable(Iterables.getLast(path));    tableWithPath.path = path;    tableWithPath.topLevelSchema = topLevelSchema;    return tableWithPath;}
public T beam_f26245_0(Class<T> c)
{    return c.isAssignableFrom(TableResolutionContext.class) ? (T) this : null;}
 static Table beam_f26246_0(Schema schema, List<String> tablePath)
{    Schema subSchema = schema;        for (int i = 0; i < tablePath.size() - 1; i++) {        subSchema = subSchema.getSubSchema(tablePath.get(i));    }        return subSchema.getTable(Iterables.getLast(tablePath));}
private RelNode beam_f26255_0(ResolvedAggregateScan node, RelNode input)
{                        List<RexNode> projects = new ArrayList<>();    List<String> fieldNames = new ArrayList<>();        for (ResolvedComputedColumn computedColumn : node.getGroupByList()) {        projects.add(getExpressionConverter().convertRexNodeFromResolvedExpr(computedColumn.getExpr(), node.getInputScan().getColumnList(), input.getRowType().getFieldList()));        fieldNames.add(getTrait().resolveAlias(computedColumn.getColumn()));    }        for (ResolvedComputedColumn resolvedComputedColumn : node.getAggregateList()) {                                        ResolvedAggregateFunctionCall aggregateFunctionCall = ((ResolvedAggregateFunctionCall) resolvedComputedColumn.getExpr());        if (aggregateFunctionCall.getArgumentList() != null && aggregateFunctionCall.getArgumentList().size() == 1) {            ResolvedExpr resolvedExpr = aggregateFunctionCall.getArgumentList().get(0);                                    projects.add(getExpressionConverter().convertRexNodeFromResolvedExpr(resolvedExpr, node.getInputScan().getColumnList(), input.getRowType().getFieldList()));            fieldNames.add(getTrait().resolveAlias(resolvedComputedColumn.getColumn()));        } else if (aggregateFunctionCall.getArgumentList() != null && aggregateFunctionCall.getArgumentList().size() > 1) {            throw new RuntimeException(aggregateFunctionCall.getFunction().getName() + " has more than one argument.");        }    }    return LogicalProject.create(input, projects, fieldNames);}
private AggregateCall beam_f26256_0(ResolvedComputedColumn computedColumn, int columnRefOff)
{    ResolvedAggregateFunctionCall aggregateFunctionCall = (ResolvedAggregateFunctionCall) computedColumn.getExpr();        if (aggregateFunctionCall.getFunction().getName().equals("avg")) {        FunctionSignature signature = aggregateFunctionCall.getSignature();        if (signature.getFunctionArgumentList().get(0).getType().getKind().equals(TypeKind.TYPE_INT64)) {            throw new RuntimeException(AVG_ILLEGAL_LONG_INPUT_TYPE);        }    }        if (aggregateFunctionCall.getDistinct()) {        throw new RuntimeException("Does not support " + aggregateFunctionCall.getFunction().getSqlName() + " DISTINCT. 'SELECT DISTINCT' syntax could be used to deduplicate before" + " aggregation.");    }    SqlAggFunction sqlAggFunction = (SqlAggFunction) SqlStdOperatorMappingTable.ZETASQL_FUNCTION_TO_CALCITE_SQL_OPERATOR.get(aggregateFunctionCall.getFunction().getName());    if (sqlAggFunction == null) {        throw new RuntimeException("Does not support ZetaSQL aggregate function: " + aggregateFunctionCall.getFunction().getName());    }    List<Integer> argList = new ArrayList<>();    for (ResolvedExpr expr : ((ResolvedAggregateFunctionCall) computedColumn.getExpr()).getArgumentList()) {                if (expr.nodeKind() == RESOLVED_CAST || expr.nodeKind() == RESOLVED_COLUMN_REF || expr.nodeKind() == RESOLVED_GET_STRUCT_FIELD) {            argList.add(columnRefOff);        } else {            throw new RuntimeException("Aggregate function only accepts Column Reference or CAST(Column Reference) as its" + " input.");        }    }        RelDataType returnType;    if (sqlAggFunction.equals(SqlStdOperatorTable.ANY_VALUE)) {        returnType = toSimpleRelDataType(computedColumn.getColumn().getType().getKind(), getCluster().getRexBuilder(), true);    } else {        returnType = toSimpleRelDataType(computedColumn.getColumn().getType().getKind(), getCluster().getRexBuilder(), false);    }    String aggName = getTrait().resolveAlias(computedColumn.getColumn());    return AggregateCall.create(sqlAggFunction, false, false, argList, -1, returnType, aggName);}
 RelOptCluster beam_f26265_0()
{    return cluster;}
 QueryTrait beam_f26266_0()
{    return trait;}
private RexNode beam_f26275_0(ResolvedComputedColumn column, List<ResolvedColumn> columnList, List<RelDataTypeField> fieldList, int windowFieldIndex)
{    if (column.getExpr().nodeKind() != RESOLVED_FUNCTION_CALL) {        return convertRexNodeFromResolvedExpr(column.getExpr(), columnList, fieldList);    }    ResolvedFunctionCall functionCall = (ResolvedFunctionCall) column.getExpr();        if (functionCall.getFunction().getName().equals(FIXED_WINDOW) || functionCall.getFunction().getName().equals(SLIDING_WINDOW) || functionCall.getFunction().getName().equals(SESSION_WINDOW)) {        throw new RuntimeException(functionCall.getFunction().getName() + " shouldn't appear in SELECT exprlist.");    }    if (!functionCall.getFunction().getGroup().equals(PRE_DEFINED_WINDOW_FUNCTIONS)) {                return convertRexNodeFromResolvedExpr(column.getExpr(), columnList, fieldList);    }                List<RexNode> operands = new ArrayList<>();    switch(functionCall.getFunction().getName()) {        case FIXED_WINDOW_START:        case SLIDING_WINDOW_START:        case SESSION_WINDOW_START:                case SESSION_WINDOW_END:            return rexBuilder().makeInputRef(fieldList.get(windowFieldIndex).getType(), windowFieldIndex);        case FIXED_WINDOW_END:                        operands.add(rexBuilder().makeInputRef(fieldList.get(windowFieldIndex).getType(), windowFieldIndex));                        operands.add(convertIntervalToRexIntervalLiteral((ResolvedLiteral) functionCall.getArgumentList().get(0)));            return rexBuilder().makeCall(SqlStdOperatorTable.PLUS, operands);        case SLIDING_WINDOW_END:            operands.add(rexBuilder().makeInputRef(fieldList.get(windowFieldIndex).getType(), windowFieldIndex));            operands.add(convertIntervalToRexIntervalLiteral((ResolvedLiteral) functionCall.getArgumentList().get(1)));            return rexBuilder().makeCall(SqlStdOperatorTable.PLUS, operands);        default:            throw new RuntimeException("Does not support window start/end: " + functionCall.getFunction().getName());    }}
public RexNode beam_f26276_0(ResolvedLiteral resolvedLiteral)
{    TypeKind kind = resolvedLiteral.getType().getKind();    RexNode ret;    switch(kind) {        case TYPE_BOOL:        case TYPE_INT32:        case TYPE_INT64:        case TYPE_FLOAT:        case TYPE_DOUBLE:        case TYPE_STRING:        case TYPE_TIMESTAMP:        case TYPE_DATE:        case TYPE_TIME:                case TYPE_BYTES:        case TYPE_ARRAY:        case TYPE_STRUCT:        case TYPE_ENUM:            ret = convertValueToRexNode(resolvedLiteral.getType(), resolvedLiteral.getValue());            break;        default:            throw new RuntimeException(MessageFormat.format("Unsupported ResolvedLiteral type: {0}, kind: {1}, value: {2}, class: {3}", resolvedLiteral.getType().typeName(), kind, resolvedLiteral.getValue(), resolvedLiteral.getClass()));    }    return ret;}
private RexNode beam_f26285_0(ResolvedFunctionCall functionCall, List<ResolvedColumn> columnList, List<RelDataTypeField> fieldList)
{    RexNode ret;    SqlOperator op;    List<RexNode> operands = new ArrayList<>();    if (functionCall.getFunction().getGroup().equals(PRE_DEFINED_WINDOW_FUNCTIONS)) {        switch(functionCall.getFunction().getName()) {            case FIXED_WINDOW:            case SESSION_WINDOW:                op = SqlStdOperatorMappingTable.ZETASQL_FUNCTION_TO_CALCITE_SQL_OPERATOR.get(functionCall.getFunction().getName());                                                operands.add(convertRexNodeFromResolvedExpr(functionCall.getArgumentList().get(0), columnList, fieldList));                                operands.add(convertIntervalToRexIntervalLiteral((ResolvedLiteral) functionCall.getArgumentList().get(1)));                break;            case SLIDING_WINDOW:                op = SqlStdOperatorMappingTable.ZETASQL_FUNCTION_TO_CALCITE_SQL_OPERATOR.get(SLIDING_WINDOW);                                operands.add(convertRexNodeFromResolvedExpr(functionCall.getArgumentList().get(0), columnList, fieldList));                                operands.add(convertIntervalToRexIntervalLiteral((ResolvedLiteral) functionCall.getArgumentList().get(1)));                                operands.add(convertIntervalToRexIntervalLiteral((ResolvedLiteral) functionCall.getArgumentList().get(2)));                break;            default:                throw new RuntimeException("Only support TUMBLE, HOP AND SESSION functions right now.");        }    } else if (functionCall.getFunction().getGroup().equals("ZetaSQL")) {        op = SqlStdOperatorMappingTable.ZETASQL_FUNCTION_TO_CALCITE_SQL_OPERATOR.get(functionCall.getFunction().getName());        if (op == null) {            throw new RuntimeException("Does not support ZetaSQL function: " + functionCall.getFunction().getName());        }                if (FUNCTION_FAMILY_DATE_ADD.contains(functionCall.getFunction().getName())) {            return convertTimestampAddFunction(functionCall, columnList, fieldList);        } else {            for (ResolvedExpr expr : functionCall.getArgumentList()) {                operands.add(convertRexNodeFromResolvedExpr(expr, columnList, fieldList));            }        }    } else {        throw new RuntimeException("Does not support function group: " + functionCall.getFunction().getGroup());    }    SqlOperatorRewriter rewriter = SqlStdOperatorMappingTable.ZETASQL_FUNCTION_TO_CALCITE_SQL_OPERATOR_REWRITER.get(functionCall.getFunction().getName());    if (rewriter != null) {        ret = rewriter.apply(rexBuilder(), operands);    } else {        ret = rexBuilder().makeCall(op, operands);    }    return ret;}
private RexNode beam_f26286_0(ResolvedFunctionCall functionCall, List<ResolvedColumn> columnList, List<RelDataTypeField> fieldList)
{    TimeUnit unit = TIME_UNIT_CASTING_MAP.get(((ResolvedLiteral) functionCall.getArgumentList().get(2)).getValue().getEnumValue());    if ((unit == TimeUnit.MICROSECOND) || (unit == TimeUnit.NANOSECOND)) {        throw Status.UNIMPLEMENTED.withDescription("Micro and Nanoseconds are not supported by Beam ZetaSQL").asRuntimeException();    }    SqlIntervalQualifier qualifier = new SqlIntervalQualifier(unit, null, SqlParserPos.ZERO);    RexNode intervalArgumentNode = convertRexNodeFromResolvedExpr(functionCall.getArgumentList().get(1), columnList, fieldList);    RexNode validatedIntervalArgument = rexBuilder().makeCall(SqlOperators.VALIDATE_TIME_INTERVAL, intervalArgumentNode, rexBuilder().makeFlag(unit));    RexNode intervalNode = rexBuilder().makeCall(SqlStdOperatorTable.MULTIPLY, rexBuilder().makeIntervalLiteral(unit.multiplier, qualifier), validatedIntervalArgument);    RexNode timestampNode = convertRexNodeFromResolvedExpr(functionCall.getArgumentList().get(0), columnList, fieldList);    RexNode dateTimePlusResult = rexBuilder().makeCall(SqlStdOperatorTable.DATETIME_PLUS, timestampNode, intervalNode);    RexNode validatedTimestampResult = rexBuilder().makeCall(SqlOperators.VALIDATE_TIMESTAMP, dateTimePlusResult);    return validatedTimestampResult;}
private RexNode beam_f26295_0(ResolvedParameter parameter)
{    assert parameter.getType().equals(queryParams.get(parameter.getName()).getType());    return convertValueToRexNode(queryParams.get(parameter.getName()).getType(), queryParams.get(parameter.getName()));}
private RexNode beam_f26296_0(ResolvedGetStructField resolvedGetStructField)
{    return rexBuilder().makeFieldAccess(convertRexNodeFromResolvedExpr(resolvedGetStructField.getExpr()), (int) resolvedGetStructField.getFieldIdx());}
 static JoinRelType beam_f26305_0(JoinType joinType)
{    if (!JOIN_TYPES.containsKey(joinType)) {        throw new UnsupportedOperationException("JOIN type: " + joinType + " is unsupported.");    }    return JOIN_TYPES.get(joinType);}
public boolean beam_f26306_0(ResolvedJoinScan zetaNode)
{    return zetaNode.getLeftScan() instanceof ResolvedWithRefScan || zetaNode.getRightScan() instanceof ResolvedWithRefScan;}
public RelNode beam_f26315_0(ResolvedLimitOffsetScan zetaNode, List<RelNode> inputs)
{    ResolvedOrderByScan inputOrderByScan = (ResolvedOrderByScan) zetaNode.getInputScan();    RelNode input = convertOrderByScanToLogicalScan(inputOrderByScan, inputs.get(0));    RelCollation relCollation = getRelCollation(inputOrderByScan);    RexNode offset = zetaNode.getOffset() == null ? null : getExpressionConverter().convertRexNodeFromResolvedExpr(zetaNode.getOffset());    RexNode fetch = getExpressionConverter().convertRexNodeFromResolvedExpr(zetaNode.getLimit(), zetaNode.getColumnList(), input.getRowType().getFieldList());    return LogicalSort.create(input, relCollation, offset, fetch);}
private static RelCollation beam_f26316_0(ResolvedOrderByScan node)
{    List<RelFieldCollation> fieldCollations = node.getOrderByItemList().stream().map(LimitOffsetScanToOrderByLimitConverter::orderByItemToFieldCollation).collect(toList());    return RelCollationImpl.of(fieldCollations);}
private RelConverter beam_f26325_0(ResolvedNode zetaNode)
{    if (!rules.containsKey(zetaNode.nodeKind())) {        throw new UnsupportedOperationException(String.format("Conversion of %s is not supported", zetaNode.nodeKind()));    }    return rules.get(zetaNode.nodeKind()).stream().filter(relConverter -> relConverter.canConvert(zetaNode)).findFirst().orElseThrow(() -> new UnsupportedOperationException(String.format("Cannot find a conversion rule for: %s", zetaNode)));}
public boolean beam_f26326_0(T zetaNode)
{    return true;}
private static RelNode beam_f26335_0(BiFunction<List<RelNode>, Boolean, RelNode> factory, boolean all, List<RelNode> inputs)
{    return inputs.stream().skip(2).reduce(    invokeFactory(factory, inputs.get(0), inputs.get(1), all),     (setOpNode, nextInput) -> invokeFactory(factory, setOpNode, nextInput, all));}
private static RelNode beam_f26336_0(BiFunction<List<RelNode>, Boolean, RelNode> factory, RelNode input1, RelNode input2, boolean all)
{    return factory.apply(ImmutableList.of(input1, input2), all);}
public List<ResolvedNode> beam_f26345_0(ResolvedWithRefScan zetaNode)
{        return Collections.singletonList(getTrait().withEntries.get(zetaNode.getWithQueryName()).getWithSubquery());}
public RelNode beam_f26346_0(ResolvedWithRefScan zetaNode, List<RelNode> inputs)
{        return inputs.get(0);}
private static RelDataType beam_f26355_0(RexBuilder r, RelDataType relDataType, boolean isNullable)
{    return r.getTypeFactory().createTypeWithNullability(relDataType, isNullable);}
private static Function<RexBuilder, RelDataType> beam_f26356_0(SqlTypeName typeName)
{    return (RexBuilder r) -> r.getTypeFactory().createSqlType(typeName);}
public SqlNode beam_f26365_0(Reader reader) throws SqlParseException
{    return null;}
public SqlNode beam_f26366_0(SqlNode sqlNode) throws ValidationException
{    return null;}
public BeamRelNode beam_f26376_0(String sqlStatement) throws ParseException, SqlConversionException
{    try {        return parseQuery(sqlStatement);    } catch (RelConversionException e) {        throw new SqlConversionException(e.getCause());    }}
public SqlNode beam_f26377_0(String sqlStatement) throws ParseException
{    return null;}
public void beam_f26386_0()
{    BeamSqlEnv sqlEnv = BeamSqlEnv.inMemory(readOnlyTableProvider);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, sqlEnv.parseQuery("SELECT ARRAY[1, 2, 3] f_arr"));    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addArrayField("f_arr", FieldType.INT32).build()).addValue(Arrays.asList(1, 2, 3)).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26387_0()
{    BeamSqlEnv sqlEnv = BeamSqlEnv.inMemory(readOnlyTableProvider);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, sqlEnv.parseQuery("SELECT rowWithArrayTestTable.col.field3[2] FROM rowWithArrayTestTable"));    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addInt64Field("int64").build()).addValue(6L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public Schema.FieldType beam_f26396_0()
{    return Schema.FieldType.DATETIME;}
public Instant beam_f26397_0(Long input)
{    return new Instant((long) input);}
public void beam_f26406_0()
{    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(INPUT_ROW_SCHEMA).addValues(1).addValue("20181018").build()).withRowSchema(INPUT_ROW_SCHEMA));    Schema resultType = Schema.builder().addInt32Field("f_int").addNullableField("f_date", DATETIME).build();    PCollection<Row> result = input.apply(SqlTransform.query("SELECT f_int, \n" + "   CAST( \n" + "     SUBSTRING(TRIM(f_string) FROM 1 FOR 4) \n" + "      ||'-' \n" + "      ||SUBSTRING(TRIM(f_string) FROM 5 FOR 2) \n" + "      ||'-' \n" + "      ||SUBSTRING(TRIM(f_string) FROM 7 FOR 2) as DATE) \n" + "FROM PCOLLECTION"));    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultType).addValues(1, new DateTime(2018, 10, 18, 0, 0, UTC)).build());    pipeline.run();}
public void beam_f26407_0()
{    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(INPUT_ROW_SCHEMA).addValues(1).addValue("20181018").build()).withRowSchema(INPUT_ROW_SCHEMA));    Schema resultType = Schema.builder().addInt32Field("f_int").addDateTimeField("f_date").build();    PCollection<Row> result = input.apply("sqlQuery", SqlTransform.query("SELECT f_int, \n" + "CASE WHEN CHAR_LENGTH(TRIM(f_string)) = 8 \n" + "    THEN CAST (\n" + "       SUBSTRING(TRIM(f_string) FROM 1 FOR 4) \n" + "        ||'-' \n" + "        ||SUBSTRING(TRIM(f_string) FROM 5 FOR 2) \n" + "        ||'-' \n" + "        ||SUBSTRING(TRIM(f_string) FROM 7 FOR 2) AS DATE)\n" + "    ELSE DATE '2001-01-01'\n" + "END \n" + "FROM PCOLLECTION"));    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultType).addValues(1, new DateTime(2018, 10, 18, 0, 0, UTC)).build());    pipeline.run();}
public void beam_f26416_0()
{    Schema schema = Schema.builder().addDoubleField("f_double1").addDoubleField("f_double2").addDoubleField("f_double3").addInt32Field("f_int1").addInt32Field("f_int2").addInt32Field("f_int3").build();    List<Row> rowsInTableB = TestUtils.RowsBuilder.of(schema).addRows(3.0, 1.0, 1.0, 3, 1, 0, 4.0, 2.0, 2.0, 4, 2, 0, 5.0, 3.0, 1.0, 5, 3, 0, 6.0, 4.0, 2.0, 6, 4, 0, 8.0, 4.0, 1.0, 8, 4, 0).getRows();    boundedInput = pipeline.apply(Create.of(rowsInTableB).withRowSchema(schema));}
public void beam_f26417_0()
{    String sql = "SELECT COVAR_POP(f_double1, f_double2) FROM PCOLLECTION GROUP BY f_int3";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(1.84, PRECISION));    pipeline.run().waitUntilFinish();}
public void beam_f26426_0()
{    String sql = "SELECT SUM(f_int1) FROM PCOLLECTION GROUP BY f_int3";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(6));    pipeline.run();}
public void beam_f26427_0()
{    String sql = "SELECT AVG(f_int1) FROM PCOLLECTION GROUP BY f_int3";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(2));    pipeline.run();}
public void beam_f26436_0() throws Exception
{    runAggregationFunctions(boundedInput1);}
public void beam_f26437_0() throws Exception
{    runAggregationFunctions(unboundedInput1);}
public void beam_f26446_0() throws Exception
{    runTumbleWindowFor31Days(boundedInputMonthly);}
private void beam_f26447_0(PCollection<Row> input) throws Exception
{    String sql = "SELECT f_int2, COUNT(*) AS `getFieldCount`," + " TUMBLE_START(f_timestamp, INTERVAL '31' DAY) AS `window_start`, " + " TUMBLE_END(f_timestamp, INTERVAL '31' DAY) AS `window_end` " + " FROM TABLE_A" + " GROUP BY f_int2, TUMBLE(f_timestamp, INTERVAL '31' DAY)";    PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), input).apply("testTumbleWindow", SqlTransform.query(sql));    Schema resultType = Schema.builder().addInt32Field("f_int2").addInt64Field("size").addDateTimeField("window_start").addDateTimeField("window_end").build();    List<Row> expectedRows = TestUtils.RowsBuilder.of(resultType).addRows(0, 1L, parseTimestampWithUTCTimeZone("2016-12-08 00:00:00"), parseTimestampWithUTCTimeZone("2017-01-08 00:00:00"), 0, 1L, parseTimestampWithUTCTimeZone("2017-01-08 00:00:00"), parseTimestampWithUTCTimeZone("2017-02-08 00:00:00"), 0, 1L, parseTimestampWithUTCTimeZone("2017-02-08 00:00:00"), parseTimestampWithUTCTimeZone("2017-03-11 00:00:00")).getRows();    PAssert.that(result).containsInAnyOrder(expectedRows);    pipeline.run().waitUntilFinish();}
public void beam_f26456_0() throws Exception
{    exceptions.expect(ParseException.class);    exceptions.expectCause(hasMessage(containsString("Cannot apply 'TUMBLE' to arguments of type 'TUMBLE(<BIGINT>, <INTERVAL HOUR>)'")));    pipeline.enableAbandonedNodeEnforcement(false);    String sql = "SELECT f_int2, COUNT(*) AS `getFieldCount` FROM TABLE_A " + "GROUP BY f_int2, TUMBLE(f_long, INTERVAL '1' HOUR)";    PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), boundedInput1).apply("testWindowOnNonTimestampField", SqlTransform.query(sql));    pipeline.run().waitUntilFinish();}
public void beam_f26457_0() throws Exception
{    exceptions.expect(ParseException.class);    exceptions.expectCause(hasMessage(containsString("Encountered \"*\"")));    pipeline.enableAbandonedNodeEnforcement(false);    String sql = "SELECT f_int2, COUNT(DISTINCT *) AS `size` " + "FROM PCOLLECTION GROUP BY f_int2";    PCollection<Row> result = boundedInput1.apply("testUnsupportedDistinct", SqlTransform.query(sql));    pipeline.run().waitUntilFinish();}
public void beam_f26466_0()
{    Schema schema = Schema.builder().addInt32Field("f_int").addDoubleField("f_double").addInt32Field("f_int2").build();    List<Row> rowsInTableB = TestUtils.RowsBuilder.of(schema).addRows(1, 1.0, 0, 4, 4.0, 0, 7, 7.0, 0, 13, 13.0, 0, 5, 5.0, 0, 10, 10.0, 0, 17, 17.0, 0).getRows();    boundedInput = pipeline.apply(Create.of(rowsInTableB).withRowSchema(schema));}
public void beam_f26467_0()
{    String sql = "SELECT VAR_POP(f_double) FROM PCOLLECTION GROUP BY f_int2";    PAssert.that(boundedInput.apply(SqlTransform.query(sql))).satisfies(matchesScalar(26.40816326, PRECISION));    pipeline.run().waitUntilFinish();}
public void beam_f26476_0()
{    PCollection<Row> input = pipeline.apply("boundedInput1", Create.empty(TypeDescriptor.of(Row.class)).withRowSchema(INPUT_SCHEMA));        TupleTag<Row> mainTag = new TupleTag<Row>("main") {    };    PCollectionTuple inputTuple = PCollectionTuple.of(mainTag, input);    Schema resultType = Schema.builder().addStringField("f_string").build();    PCollection<Row> result = inputTuple.apply("sqlQuery", SqlTransform.query("SELECT * FROM UNNEST (ARRAY ['a', 'b', 'c'])"));    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultType).addValues("a").build(), Row.withSchema(resultType).addValues("b").build(), Row.withSchema(resultType).addValues("c").build());    pipeline.run();}
public void beam_f26477_0()
{    PCollection<Row> input = pipeline.apply("boundedInput1", Create.empty(TypeDescriptor.of(Row.class)).withRowSchema(INPUT_SCHEMA));        TupleTag<Row> mainTag = new TupleTag<Row>("main") {    };    PCollectionTuple inputTuple = PCollectionTuple.of(mainTag, input);    Schema resultType = Schema.builder().addStringField("f_string").build();    PCollection<Row> result = inputTuple.apply("sqlQuery", SqlTransform.query("SELECT * FROM UNNEST (ARRAY ['a', 'b', 'c']) AS t(f_string)"));    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultType).addValues("a").build(), Row.withSchema(resultType).addValues("b").build(), Row.withSchema(resultType).addValues("c").build());    pipeline.run();}
private PCollection<Row> beam_f26486_0()
{    TestStream.Builder<Row> values = TestStream.create(schemaInTableA);    Row row = rowsInTableA.get(0);    values = values.advanceWatermarkTo(new Instant(row.getDateTime("f_timestamp")));    values = values.addElements(row);    return PBegin.in(pipeline).apply("unboundedInput2", values.advanceWatermarkToInfinity()).apply("unboundedInput2.fixedWindow1year", Window.into(FixedWindows.of(Duration.standardDays(365))));}
public static void beam_f26487_0()
{    registerTable("CUSTOMER", TestBoundedTable.of(Schema.FieldType.INT32, "c_custkey", Schema.FieldType.DOUBLE, "c_acctbal", Schema.FieldType.STRING, "c_city").addRows(1, 1.0, "Seattle", 5, 1.0, "Mountain View", 4, 4.0, "Portland"));    registerTable("ORDERS", TestBoundedTable.of(Schema.FieldType.INT32, "o_orderkey", Schema.FieldType.INT32, "o_custkey", Schema.FieldType.DOUBLE, "o_totalprice").addRows(1, 1, 1.0, 2, 2, 2.0, 3, 3, 3.0));}
public void beam_f26496_0() throws Exception
{    runNoReturnFilter(boundedInput1);}
public void beam_f26497_0() throws Exception
{    runNoReturnFilter(unboundedInput1);}
public void beam_f26506_0() throws Exception
{    String sql = "SELECT *  " + "FROM ORDER_DETAILS1 o1" + " JOIN ORDER_DETAILS2 o2" + " on " + " o1.order_id>o2.site_id";    pipeline.enableAbandonedNodeEnforcement(false);    queryFromOrderTables(sql);    pipeline.run();}
public void beam_f26507_0() throws Exception
{    String sql = "SELECT *  " + "FROM ORDER_DETAILS1 o1, ORDER_DETAILS2 o2";    pipeline.enableAbandonedNodeEnforcement(false);    queryFromOrderTables(sql);    pipeline.run();}
public void beam_f26516_0()
{    Schema nestedSchema = Schema.builder().addInt32Field("f_nestedInt").addStringField("f_nestedString").addInt32Field("f_nestedIntPlusOne").build();    Schema resultSchema = Schema.builder().addInt32Field("f_int").addInt32Field("f_int2").addStringField("f_varchar").addInt32Field("f_int3").build();    Schema inputType = Schema.builder().addInt32Field("f_int").addRowField("f_row", nestedSchema).build();    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(inputType).addValues(1, Row.withSchema(nestedSchema).addValues(312, "CC", 313).build()).build()).withRowSchema(inputType));    PCollection<Row> result = input.apply(SqlTransform.query("SELECT 1 as `f_int`, (3, 'BB', f_int + 1) as `f_row1` FROM PCOLLECTION")).setRowSchema(resultSchema);    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultSchema).addValues(1, 3, "BB", 2).build());    pipeline.run();}
public void beam_f26517_0()
{    Schema nestedSchema = Schema.builder().addInt32Field("f_nestedInt").addStringField("f_nestedString").addInt32Field("f_nestedIntPlusOne").build();    Schema resultSchema = Schema.builder().addStringField("f_nestedString").build();    Schema inputType = Schema.builder().addInt32Field("f_int").addRowField("f_nestedRow", nestedSchema).build();    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(inputType).addValues(1, Row.withSchema(nestedSchema).addValues(312, "CC", 313).build()).build(), Row.withSchema(inputType).addValues(2, Row.withSchema(nestedSchema).addValues(412, "DD", 413).build()).build()).withRowSchema(inputType));    PCollection<Row> result = input.apply(SqlTransform.query("SELECT `PCOLLECTION`.`f_nestedRow`.`f_nestedString` FROM PCOLLECTION")).setRowSchema(resultSchema);    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultSchema).addValues("CC").build(), Row.withSchema(resultSchema).addValues("DD").build());    pipeline.run();}
public void beam_f26526_0() throws Exception
{    runPartialFieldsInMultipleRow(boundedInput1);}
public void beam_f26527_0() throws Exception
{    runPartialFieldsInMultipleRow(unboundedInput1);}
public void beam_f26536_0() throws Exception
{    exceptions.expect(ParseException.class);    exceptions.expectCause(hasMessage(containsString("Column 'f_int_na' not found in any table")));    pipeline.enableAbandonedNodeEnforcement(false);    String sql = "SELECT f_int_na FROM TABLE_A";    PCollection<Row> result = PCollectionTuple.of(new TupleTag<>("TABLE_A"), boundedInput1).apply("testProjectUnknownField", SqlTransform.query(sql));    pipeline.run();}
public void beam_f26537_0()
{    String sql = "SELECT c_int64 as abc FROM PCOLLECTION";    Schema inputSchema = Schema.of(Schema.Field.of("c_int64", Schema.FieldType.INT64));    Schema outputSchema = Schema.of(Schema.Field.of("abc", Schema.FieldType.INT64));    PCollection<Row> input = pipeline.apply(Create.of(Row.withSchema(inputSchema).addValue(42L).build()).withRowSchema(inputSchema));    PCollection<Row> result = input.apply(SqlTransform.query(sql));    Assert.assertEquals(outputSchema, result.getSchema());    PAssert.that(result).containsInAnyOrder(Row.withSchema(outputSchema).addValue(42L).build());    pipeline.run();}
public void beam_f26546_0()
{    Set<SqlOperatorId> untestedOperators = new HashSet<>();    untestedOperators.addAll(getDeclaredOperators());        untestedOperators.removeAll(NON_ROW_OPERATORS);    untestedOperators.removeAll(getTestedOperators());    if (!untestedOperators.isEmpty()) {                List<SqlOperatorId> untestedList = Lists.newArrayList(untestedOperators);        untestedList.sort(orderByNameThenKind);        fail(String.format("No tests declared for %s operators:\n\t%s", untestedList.size(), Joiner.on("\n\t").join(untestedList)));    }}
public void beam_f26547_0()
{    Set<SqlOperatorId> undeclaredOperators = new HashSet<>();    undeclaredOperators.addAll(getTestedOperators());    undeclaredOperators.removeAll(getDeclaredOperators());    if (!undeclaredOperators.isEmpty()) {                List<SqlOperatorId> undeclaredList = Lists.newArrayList(undeclaredOperators);        undeclaredList.sort(orderByNameThenKind);        fail("Tests declared for nonexistent operators:\n\t" + Joiner.on("\n\t").join(undeclaredList));    }}
public void beam_f26556_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("$SUM0(c_tinyint)", (byte) 6).addExpr("$SUM0(c_smallint)", (short) 6).addExpr("$SUM0(c_integer)", 6).addExpr("$SUM0(c_bigint)", 6L).addExpr("$SUM0(c_float)", 6.0f).addExpr("$SUM0(c_double)", 6.0).addExpr("$SUM0(c_decimal)", BigDecimal.valueOf(6.0));    checker.buildRunAndCheck(getAggregationTestPCollection());}
public void beam_f26557_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("COUNT(*)", 4L).addExpr("COUNT(1)", 4L);    checker.buildRunAndCheck(getAggregationTestPCollection());}
public void beam_f26566_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("ROUND(c_integer, 0)", SqlFunctions.sround(INTEGER_VALUE, 0)).addExpr("ROUND(c_bigint, 0)", SqlFunctions.sround(LONG_VALUE, 0)).addExpr("ROUND(c_smallint, 0)", (short) SqlFunctions.sround(SHORT_VALUE, 0)).addExpr("ROUND(c_tinyint, 0)", (byte) SqlFunctions.sround(BYTE_VALUE, 0)).addExpr("ROUND(c_double, 0)", SqlFunctions.sround(DOUBLE_VALUE, 0)).addExpr("ROUND(c_float, 0)", (float) SqlFunctions.sround(FLOAT_VALUE, 0)).addExpr("ROUND(c_decimal, 0)", new BigDecimal(SqlFunctions.sround(ONE.doubleValue(), 0)));    checker.buildRunAndCheck();}
public void beam_f26567_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("LOG10(c_integer)", Math.log10(INTEGER_VALUE)).addExpr("LOG10(c_bigint)", Math.log10(LONG_VALUE)).addExpr("LOG10(c_smallint)", Math.log10(SHORT_VALUE)).addExpr("LOG10(c_tinyint)", Math.log10(BYTE_VALUE)).addExpr("LOG10(c_double)", Math.log10(DOUBLE_VALUE)).addExpr("LOG10(c_float)", Math.log10(FLOAT_VALUE)).addExpr("LOG10(c_decimal)", Math.log10(ONE.doubleValue()));    checker.buildRunAndCheck();}
public void beam_f26576_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("SIN(c_integer)", Math.sin(INTEGER_VALUE)).addExpr("SIN(c_bigint)", Math.sin(LONG_VALUE)).addExpr("SIN(c_smallint)", Math.sin(SHORT_VALUE)).addExpr("SIN(c_tinyint)", Math.sin(BYTE_VALUE)).addExpr("SIN(c_double)", Math.sin(DOUBLE_VALUE)).addExpr("SIN(c_float)", Math.sin(FLOAT_VALUE)).addExpr("SIN(c_decimal)", Math.sin(ONE.doubleValue()));    checker.buildRunAndCheck();}
public void beam_f26577_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("TAN(c_integer)", Math.tan(INTEGER_VALUE)).addExpr("TAN(c_bigint)", Math.tan(LONG_VALUE)).addExpr("TAN(c_smallint)", Math.tan(SHORT_VALUE)).addExpr("TAN(c_tinyint)", Math.tan(BYTE_VALUE)).addExpr("TAN(c_double)", Math.tan(DOUBLE_VALUE)).addExpr("TAN(c_float)", Math.tan(FLOAT_VALUE)).addExpr("TAN(c_decimal)", Math.tan(ONE.doubleValue()));    checker.buildRunAndCheck();}
public void beam_f26586_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("EXTRACT(YEAR FROM ts)", 1986L).addExpr("YEAR(ts)", 1986L).addExpr("QUARTER(ts)", 1L).addExpr("MONTH(ts)", 2L).addExpr("WEEK(ts)", 7L).addExpr("DAYOFMONTH(ts)", 15L).addExpr("DAYOFYEAR(ts)", 46L).addExpr("DAYOFWEEK(ts)", 7L).addExpr("HOUR(ts)", 11L).addExpr("MINUTE(ts)", 35L).addExpr("SECOND(ts)", 26L);    checker.buildRunAndCheck();}
public void beam_f26587_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("FLOOR(ts TO SECOND)", parseTimestampWithUTCTimeZone("1986-02-15 11:35:26")).addExpr("FLOOR(ts TO MINUTE)", parseTimestampWithUTCTimeZone("1986-02-15 11:35:00")).addExpr("FLOOR(ts TO HOUR)", parseTimestampWithUTCTimeZone("1986-02-15 11:00:00")).addExpr("FLOOR(ts TO DAY)", parseTimestampWithUTCTimeZone("1986-02-15 00:00:00")).addExpr("FLOOR(ts TO MONTH)", parseTimestampWithUTCTimeZone("1986-02-01 00:00:00")).addExpr("FLOOR(ts TO YEAR)", parseTimestampWithUTCTimeZone("1986-01-01 00:00:00")).addExpr("FLOOR(c_double)", 1.0);    checker.buildRunAndCheck(getFloorCeilingTestPCollection());}
public void beam_f26596_0()
{    Schema resultType = Schema.builder().addInt32Field("f_int2").addInt32Field("squaresum").build();    Row row = Row.withSchema(resultType).addValues(0, 354).build();    String sql1 = "SELECT f_int2, double_square_sum(f_int) AS `squaresum`" + " FROM PCOLLECTION GROUP BY f_int2";    PCollection<Row> result1 = boundedInput1.apply("testUdaf", SqlTransform.query(sql1).registerUdaf("double_square_sum", new SquareSquareSum()));    PAssert.that(result1).containsInAnyOrder(row);    pipeline.run().waitUntilFinish();}
public void beam_f26597_0()
{    exceptions.expect(ParseException.class);    exceptions.expectCause(hasMessage(containsString("CombineFn must be parameterized")));    pipeline.enableAbandonedNodeEnforcement(false);    Schema resultType = Schema.builder().addInt32Field("f_int2").addInt32Field("squaresum").build();    Row row = Row.withSchema(resultType).addValues(0, 354).build();    String sql1 = "SELECT f_int2, squaresum(f_int) AS `squaresum`" + " FROM PCOLLECTION GROUP BY f_int2";    PCollection<Row> result1 = boundedInput1.apply("testUdaf", SqlTransform.query(sql1).registerUdaf("squaresum", new RawCombineFn()));}
public Integer beam_f26606_0(Integer accumulator)
{    return accumulator;}
public Instant beam_f26607_0()
{    return new Instant(0L);}
public static Integer beam_f26616_0(Integer input)
{    return input * input * input;}
public Integer beam_f26617_0(Integer input)
{    return input * input * input;}
public void beam_f26626_0()
{    PCollection<Row> input = pCollectionOf2Elements();    Schema resultType = Schema.builder().addNullableField("f_mapElem", Schema.FieldType.INT32).build();    PCollection<Row> result = input.apply("sqlQuery", SqlTransform.query("SELECT f_intStringMap['key11'] FROM PCOLLECTION"));    PAssert.that(result).containsInAnyOrder(Row.withSchema(resultType).addValues(11).build(), Row.withSchema(resultType).addValue(null).build());    pipeline.run();}
private PCollection<Row> beam_f26627_0()
{    return pipeline.apply("boundedInput1", Create.of(Row.withSchema(INPUT_ROW_TYPE).addValues(1).addValue(ImmutableMap.of("key11", 11, "key22", 22)).build(), Row.withSchema(INPUT_ROW_TYPE).addValues(2).addValue(ImmutableMap.of("key33", 33, "key44", 44, "key55", 55)).build()).withRowSchema(INPUT_ROW_TYPE));}
public void beam_f26636_0()
{    PCollection<Row> inputMain = pipeline.apply("mainInput", create(row(1, "pcollection_1"), row(2, "pcollection_2")));    PCollection<Row> inputExtra = pipeline.apply("extraInput", create(row(1, "_extra_table_1"), row(2, "_extra_table_2")));    TableProvider extraInputProvider = extraTableProvider("extraTable", inputExtra);    PCollection<Row> result = inputMain.apply(SqlTransform.query("SELECT extra.f_int, main.f_string || extra.f_string AS f_string \n" + "FROM extraSchema.extraTable AS extra \n" + "   INNER JOIN \n" + " PCOLLECTION AS main \n" + "   ON main.f_int = extra.f_int").withTableProvider("extraSchema", extraInputProvider));    PAssert.that(result).containsInAnyOrder(row(1, "pcollection_1_extra_table_1"), row(2, "pcollection_2_extra_table_2"));    pipeline.run();}
public void beam_f26637_0()
{    PCollection<Row> inputMain = pipeline.apply("mainInput", create(row(1, "pcollection_1"), row(2, "pcollection_2")));    PCollection<Row> inputExtra = pipeline.apply("extraInput", create(row(1, "_extra_table_1"), row(2, "_extra_table_2")));    TableProvider extraInputProvider = extraTableProvider("extraTable", inputExtra);    PCollection<Row> result = inputMain.apply(SqlTransform.query("SELECT extra.f_int, main.f_string || extra.f_string AS f_string \n" + "FROM extraSchema.extraTable AS extra \n" + "   INNER JOIN \n" + " beam.PCOLLECTION AS main \n" + "   ON main.f_int = extra.f_int").withTableProvider("extraSchema", extraInputProvider));    PAssert.that(result).containsInAnyOrder(row(1, "pcollection_1_extra_table_1"), row(2, "pcollection_2_extra_table_2"));    pipeline.run();}
public static Iterable<Object[]> beam_f26646_0()
{    return Arrays.asList(new Object[][] {     { "field_id", "field_id" }, { "`field_id`", "field_id" }, { "`field``id`", "field`id" }, { "`field id`", "field id" }, { "`field-id`", "field-id" }, { "`field=id`", "field=id" }, { "`field.id`", "field.id" }, { "`field{id}`", "field{id}" }, { "`field|id`", "field|id" }, { "`field\\id`", "field\\id" }, { "`field\\a_id`", "field\\a_id" }, { "`field\b_id`", "field\b_id" }, { "`field\\b_id`", "field\\b_id" }, { "`field\\f_id`", "field\\f_id" }, { "`field\\n_id`", "field\\n_id" }, { "`field\\r_id`", "field\\r_id" }, { "`field\tid`", "field\tid" }, { "`field\\t_id`", "field\\t_id" }, { "`field\\v_id`", "field\\v_id" }, { "`field\\\\_id`", "field\\\\_id" }, { "`field\\?_id`", "field\\?_id" } });}
public void beam_f26647_0()
{    assertThat(alias(input), parsedAs(expected));}
public void beam_f26656_0() throws Exception
{    Class.forName("org.apache.beam.sdk.extensions.sql.impl.JdbcDriver");}
public void beam_f26657_0() throws Exception
{    Driver driver = DriverManager.getDriver(JdbcDriver.CONNECT_STRING_PREFIX);    assertTrue(driver instanceof JdbcDriver);}
public void beam_f26666_0() throws Exception
{    TestTableProvider tableProvider = new TestTableProvider();    Connection connection = JdbcDriver.connect(tableProvider, PipelineOptionsFactory.create());        Schema schema = Schema.builder().addDateTimeField("ts").build();    connection.createStatement().executeUpdate("CREATE EXTERNAL TABLE test (ts TIMESTAMP) TYPE 'test'");    ReadableInstant july1 = ISODateTimeFormat.dateTimeParser().parseDateTime("2018-07-01T01:02:03Z");    tableProvider.addRows("test", Row.withSchema(schema).addValue(july1).build());    ResultSet selectResult = connection.createStatement().executeQuery(String.format("SELECT ts FROM test"));    selectResult.next();    Timestamp ts = selectResult.getTimestamp(1);    assertThat(String.format("Wrote %s to a table, but got back %s", ISODateTimeFormat.basicDateTime().print(july1), ISODateTimeFormat.basicDateTime().print(ts.getTime())), ts.getTime(), equalTo(july1.getMillis()));}
public void beam_f26667_0() throws Exception
{    Calendar cal = Calendar.getInstance(TimeZone.getTimeZone("Asia/Tokyo"), Locale.ROOT);    TestTableProvider tableProvider = new TestTableProvider();    Connection connection = JdbcDriver.connect(tableProvider, PipelineOptionsFactory.create());        Schema schema = Schema.builder().addDateTimeField("ts").build();    connection.createStatement().executeUpdate("CREATE EXTERNAL TABLE test (ts TIMESTAMP) TYPE 'test'");    ReadableInstant july1 = ISODateTimeFormat.dateTimeParser().parseDateTime("2018-07-01T01:02:03Z");    tableProvider.addRows("test", Row.withSchema(schema).addValue(july1).build());    ResultSet selectResult = connection.createStatement().executeQuery(String.format("SELECT ts FROM test"));    selectResult.next();    Timestamp ts = selectResult.getTimestamp(1, cal);    assertThat(String.format("Wrote %s to a table, but got back %s", ISODateTimeFormat.basicDateTime().print(july1), ISODateTimeFormat.basicDateTime().print(ts.getTime())), ts.getTime(), equalTo(july1.getMillis()));}
private Row beam_f26676_0(Schema schema, Object... values)
{    return Row.withSchema(schema).addValues(values).build();}
public void beam_f26677_0() throws Exception
{    CalciteConnection connection = JdbcDriver.connect(BOUNDED_TABLE, PipelineOptionsFactory.create());    Statement statement = connection.createStatement();    assertEquals(0, statement.executeUpdate("SET runner = direct"));    assertTrue(statement.execute("SELECT * FROM test"));}
private String beam_f26686_0(FieldType fieldType)
{    return CalciteUtils.toSqlTypeName(fieldType).getName();}
private String beam_f26687_0(FieldType fieldType)
{    return "ARRAY<" + unparse(fieldType.getCollectionElementType()) + ">";}
public void beam_f26696_0() throws Exception
{    TestTableProvider tableProvider = new TestTableProvider();    BeamSqlEnv env = BeamSqlEnv.withTableProvider(tableProvider);    env.executeDdl("CREATE EXTERNAL TABLE person (id INT) TYPE text");    assertEquals(Table.builder().name("person").type("text").schema(Stream.of(Schema.Field.of("id", CalciteUtils.INTEGER).withNullable(true)).collect(toSchema())).properties(new JSONObject()).build(), tableProvider.getTables().get("person"));}
public void beam_f26697_0() throws Exception
{    TestTableProvider rootProvider = new TestTableProvider();    TestTableProvider testProvider = new TestTableProvider();    BeamSqlEnv env = BeamSqlEnv.builder(rootProvider).addSchema("test", testProvider).setPipelineOptions(PipelineOptionsFactory.create()).build();    assertNull(testProvider.getTables().get("person"));    env.executeDdl("CREATE EXTERNAL TABLE test.person (id INT) TYPE text");    assertNotNull(testProvider.getTables().get("person"));}
public void beam_f26706_0()
{    BeamCostModel inf1 = BeamCostModel.FACTORY.makeCost(Double.POSITIVE_INFINITY, 0);    BeamCostModel inf2 = BeamCostModel.FACTORY.makeInfiniteCost();    Assert.assertTrue(inf1.isLe(inf2));    Assert.assertTrue(inf2.isLe(inf1));    Assert.assertFalse(inf1.isLt(inf2));    Assert.assertFalse(inf2.isLt(inf1));}
public void beam_f26707_0()
{    BeamCostModel inf = BeamCostModel.FACTORY.makeCost(Double.POSITIVE_INFINITY, 0);    BeamCostModel huge = BeamCostModel.FACTORY.makeHugeCost();    Assert.assertTrue(huge.isLe(inf));    Assert.assertTrue(huge.isLt(inf));    Assert.assertFalse(inf.isLt(huge));    Assert.assertFalse(inf.isLt(huge));}
public void beam_f26716_0()
{    String sql = " select * from ORDER_DETAILS1 ";    RelNode root = env.parseQuery(sql);    root = root.getCluster().getPlanner().getRoot();        Assert.assertTrue(root instanceof RelSubset);    NodeStats estimates = BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());    Assert.assertFalse(estimates.isUnknown());}
protected static PCollection<Row> beam_f26717_0(String sql, Pipeline pipeline)
{    return BeamSqlRelUtils.toPCollection(pipeline, env.parseQuery(sql));}
public static void beam_f26726_0()
{    registerTable("ORDER_DETAILS_BOUNDED", TestBoundedTable.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.DECIMAL, "price").addRows(1L, 1, new BigDecimal(1.0), 1L, 1, new BigDecimal(1.0), 2L, 2, new BigDecimal(2.0), 4L, 4, new BigDecimal(4.0), 4L, 4, new BigDecimal(4.0)));    registerTable("ORDER_DETAILS_UNBOUNDED", TestUnboundedTable.of(Schema.FieldType.INT32, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.INT32, "price", Schema.FieldType.DATETIME, "order_time").timestampColumnIndex(3).addRows(Duration.ZERO, 1, 1, 1, FIRST_DATE, 1, 2, 6, FIRST_DATE).addRows(WINDOW_SIZE.plus(Duration.standardMinutes(1)), 2, 2, 7, SECOND_DATE, 2, 3, 8, SECOND_DATE,     1, 3, 3, FIRST_DATE).addRows(    WINDOW_SIZE.plus(WINDOW_SIZE).plus(Duration.standardMinutes(1)), 2, 3, 3, SECOND_DATE).setStatistics(BeamTableStatistics.createUnboundedTableStatistics(2d)));}
public void beam_f26727_0()
{    String sql = "SELECT order_id FROM ORDER_DETAILS_BOUNDED";    RelNode root = env.parseQuery(sql);    Assert.assertTrue(root instanceof BeamCalcRel);    NodeStats estimate = BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());    Assert.assertEquals(5d, estimate.getRowCount(), 0.001);    Assert.assertEquals(5d, estimate.getWindow(), 0.001);    Assert.assertEquals(0., estimate.getRate(), 0.001);}
public void beam_f26736_0() throws Exception
{    String sql = "SELECT *  " + "FROM ORDER_DETAILS1 o1" + " LEFT OUTER JOIN (SELECT * FROM ORDER_DETAILS2 WHERE FALSE) o2" + " on " + " o1.order_id=o2.site_id AND o2.price=o1.site_id";    PCollection<Row> rows = compilePipeline(sql, pipeline);    pipeline.enableAbandonedNodeEnforcement(false);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addField("order_id", Schema.FieldType.INT32).addField("site_id", Schema.FieldType.INT32).addField("price", Schema.FieldType.INT32).addNullableField("order_id0", Schema.FieldType.INT32).addNullableField("site_id0", Schema.FieldType.INT32).addNullableField("price0", Schema.FieldType.INT32).build()).addRows(1, 2, 3, null, null, null, 2, 3, 3, null, null, null, 3, 4, 5, null, null, null).getRows());    pipeline.run();}
public void beam_f26737_0() throws Exception
{    String sql = "SELECT *  " + "FROM ORDER_DETAILS1 o1" + " INNER JOIN (SELECT * FROM ORDER_DETAILS2 WHERE FALSE) o2" + " on " + " o1.order_id=o2.site_id AND o2.price=o1.site_id";    PCollection<Row> rows = compilePipeline(sql, pipeline);    pipeline.enableAbandonedNodeEnforcement(false);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addField("order_id", Schema.FieldType.INT32).addField("site_id", Schema.FieldType.INT32).addField("price", Schema.FieldType.INT32).addNullableField("order_id0", Schema.FieldType.INT32).addNullableField("site_id0", Schema.FieldType.INT32).addNullableField("price0", Schema.FieldType.INT32).build()).getRows());    pipeline.run();}
public static void beam_f26746_0()
{    registerTable("ORDER_DETAILS", TestUnboundedTable.of(Schema.FieldType.INT32, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.INT32, "price", Schema.FieldType.DATETIME, "order_time").timestampColumnIndex(3).addRows(Duration.ZERO, 1, 1, 1, FIRST_DATE, 1, 2, 6, FIRST_DATE).addRows(WINDOW_SIZE.plus(Duration.standardMinutes(1)), 2, 2, 7, SECOND_DATE, 2, 3, 8, SECOND_DATE,     1, 3, 3, FIRST_DATE).addRows(    WINDOW_SIZE.plus(WINDOW_SIZE).plus(Duration.standardMinutes(1)), 2, 3, 3, SECOND_DATE).setStatistics(BeamTableStatistics.createUnboundedTableStatistics(3d)));}
public void beam_f26747_0() throws Exception
{    String sql = "SELECT * FROM " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " JOIN " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o2 " + " on " + " o1.order_id=o2.order_id";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addField("order_id1", Schema.FieldType.INT32).addField("sum_site_id", Schema.FieldType.INT32).addField("order_id", Schema.FieldType.INT32).addField("sum_site_id0", Schema.FieldType.INT32).build()).addRows(1, 3, 1, 3, 2, 5, 2, 5).getStringRows());    pipeline.run();}
public PCollection.IsBounded beam_f26756_0()
{    return null;}
public PCollection<Row> beam_f26757_0(PBegin begin)
{    return null;}
public void beam_f26767_0()
{    String sql = "SELECT order_id, site_id, price " + " FROM ORDER_DETAILS1 " + " INTERSECT " + " SELECT order_id, site_id, price " + " FROM ORDER_DETAILS2 ";    RelNode root = env.parseQuery(sql);    while (!(root instanceof BeamIntersectRel)) {        root = root.getInput(0);    }    NodeStats estimate = BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());    Assert.assertFalse(estimate.isUnknown());    Assert.assertEquals(0d, estimate.getRate(), 0.01);    Assert.assertEquals(3. / 2., estimate.getRowCount(), 0.01);    Assert.assertEquals(3. / 2., estimate.getWindow(), 0.01);}
public static void beam_f26768_0()
{    registerTable("ORDER_DETAILS_BOUNDED", TestBoundedTable.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.DECIMAL, "price").addRows(1L, 1, new BigDecimal(1.0), 1L, 1, new BigDecimal(1.0), 2L, 2, new BigDecimal(2.0), 4L, 4, new BigDecimal(4.0), 4L, 4, new BigDecimal(4.0)));    registerTable("ORDER_DETAILS_UNBOUNDED", TestUnboundedTable.of(Schema.FieldType.INT32, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.INT32, "price", Schema.FieldType.DATETIME, "order_time").timestampColumnIndex(3).addRows(Duration.ZERO, 1, 1, 1, FIRST_DATE, 1, 2, 6, FIRST_DATE).addRows(WINDOW_SIZE.plus(Duration.standardMinutes(1)), 2, 2, 7, SECOND_DATE, 2, 3, 8, SECOND_DATE,     1, 3, 3, FIRST_DATE).addRows(    WINDOW_SIZE.plus(WINDOW_SIZE).plus(Duration.standardMinutes(1)), 2, 3, 3, SECOND_DATE).setStatistics(BeamTableStatistics.createUnboundedTableStatistics(2d)));}
public void beam_f26777_0()
{    String sql = "SELECT order_id, site_id, price " + "FROM ORDER_DETAILS1 " + " EXCEPT ALL " + "SELECT order_id, site_id, price " + "FROM ORDER_DETAILS2 ";    RelNode root = env.parseQuery(sql);    while (!(root instanceof BeamMinusRel)) {        root = root.getInput(0);    }    NodeStats estimate = BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());    Assert.assertFalse(estimate.isUnknown());    Assert.assertEquals(0d, estimate.getRate(), 0.01);    Assert.assertEquals(5. - 3. / 2., estimate.getRowCount(), 0.01);    Assert.assertEquals(5. - 3. / 2., estimate.getWindow(), 0.01);}
public void beam_f26778_0()
{    String sql = "SELECT * " + "FROM " + "(select order_id FROM ORDER_DETAILS_UNBOUNDED " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " EXCEPT ALL " + " select order_id FROM ORDER_DETAILS_UNBOUNDED " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR) ";    RelNode root = env.parseQuery(sql);    while (!(root instanceof BeamMinusRel)) {        root = root.getInput(0);    }    NodeStats estimate = BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());        Assert.assertEquals(4d / 2 - 4d / 4, estimate.getRate(), 0.01);    Assert.assertEquals(0d, estimate.getRowCount(), 0.01);}
public void beam_f26787_0() throws Exception
{    String sql = "SELECT o1.order_id, o1.sum_site_id, o2.buyer FROM " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " LEFT OUTER JOIN " + " ORDER_DETAILS1 o2 " + " on " + " o1.order_id=o2.order_id";    PCollection<Row> rows = compilePipeline(sql, pipeline);    rows.apply(ParDo.of(new BeamSqlOutputToConsoleFn("helloworld")));    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.builder().addField("order_id", Schema.FieldType.INT32).addField("sum_site_id", Schema.FieldType.INT32).addNullableField("buyer", Schema.FieldType.STRING).build()).addRows(1, 3, "james", 2, 5, "bond", 3, 3, null).getStringRows());    pipeline.run();}
public void beam_f26788_0() throws Exception
{    String sql = "SELECT o1.order_id, o1.sum_site_id, o2.buyer FROM " + " ORDER_DETAILS1 o2 " + " LEFT OUTER JOIN " + "(select order_id, sum(site_id) as sum_site_id FROM ORDER_DETAILS " + "          GROUP BY order_id, TUMBLE(order_time, INTERVAL '1' HOUR)) o1 " + " on " + " o1.order_id=o2.order_id";    pipeline.enableAbandonedNodeEnforcement(false);    compilePipeline(sql, pipeline);    pipeline.run();}
public static void beam_f26797_0()
{    BeamSideInputJoinRelTest.registerUnboundedTable();    registerTable("ORDER_DETAILS1", ORDER_DETAILS1);    registerTable("SITE_LKP", new SiteLookupTable(TestTableUtils.buildBeamSqlNullableSchema(Schema.FieldType.INT32, "site_id", nullable, Schema.FieldType.STRING, "site_name", nullable)));}
public void beam_f26798_0() throws Exception
{    String sql = "SELECT o1.order_id, o2.site_name FROM " + " ORDER_DETAILS1 o1 " + " JOIN SITE_LKP o2 " + " on " + " o1.site_id=o2.site_id " + " WHERE o1.site_id=2 ";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows.apply(ParDo.of(new TestUtils.BeamSqlRow2StringDoFn()))).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT32, "order_id", Schema.FieldType.STRING, "site_name").addRows(1, "SITE1").getStringRows());    pipeline.run();}
public void beam_f26807_0()
{    registerTable("ORDER_DETAILS", TestBoundedTable.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.DOUBLE, "price", Schema.FieldType.DATETIME, "order_time").addRows(1L, 2, 1.0, new DateTime(0), 1L, 1, 2.0, new DateTime(1), 2L, 4, 3.0, new DateTime(2), 2L, 1, 4.0, new DateTime(3), 5L, 5, 5.0, new DateTime(4), 6L, 6, 6.0, new DateTime(5), 7L, 7, 7.0, new DateTime(6), 8L, 8888, 8.0, new DateTime(7), 8L, 999, 9.0, new DateTime(8), 10L, 100, 10.0, new DateTime(9)));    registerTable("SUB_ORDER_RAM", TestBoundedTable.of(Schema.builder().addField("order_id", Schema.FieldType.INT64).addField("site_id", Schema.FieldType.INT32).addNullableField("price", Schema.FieldType.DOUBLE).build()));    registerTable("COUNT_TABLE", TestBoundedTable.of(Schema.builder().addField("count_star", Schema.FieldType.INT64).build()));}
public void beam_f26808_0()
{    String sql = "INSERT INTO SUB_ORDER_RAM(order_id, site_id, price)  SELECT " + " order_id, site_id, price " + "FROM ORDER_DETAILS " + "ORDER BY order_id asc, site_id desc limit 4";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.INT64, "order_id", Schema.FieldType.INT32, "site_id", Schema.FieldType.DOUBLE, "price").addRows(1L, 2, 1.0, 1L, 1, 2.0, 2L, 4, 3.0, 2L, 1, 4.0).getRows());    pipeline.run().waitUntilFinish();}
private NodeStats beam_f26817_0(String sql)
{    RelNode root = env.parseQuery(sql);    while (!(root instanceof BeamUncollectRel)) {        root = root.getInput(0);    }    return BeamSqlRelUtils.getNodeStats(root, root.getCluster().getMetadataQuery());}
public void beam_f26818_0()
{    NodeStats estimate = getEstimateOf("SELECT * FROM UNNEST (SELECT * FROM (VALUES (ARRAY ['a', 'b', 'c']),(ARRAY ['a', 'b', 'c']))) t1");    Assert.assertEquals(4d, estimate.getRowCount(), 0.001);    Assert.assertEquals(4d, estimate.getWindow(), 0.001);    Assert.assertEquals(0., estimate.getRate(), 0.001);}
public void beam_f26827_0()
{    String sql = "SELECT user_id, p.intField, p.stringField FROM NESTED as t, unnest(t.nested) as p";    PCollection<Row> rows = compilePipeline(sql, pipeline);    PAssert.that(rows).containsInAnyOrder(TestUtils.RowsBuilder.of(Schema.FieldType.STRING, "user_id", Schema.FieldType.INT32, "intField", Schema.FieldType.STRING, "stringField").addRows("1", 1, "test1", "1", 2, "test2").getRows());    pipeline.run();}
public static void beam_f26828_0()
{    registerTable("string_table", TestBoundedTable.of(Schema.FieldType.STRING, "name", Schema.FieldType.STRING, "description"));    registerTable("int_table", TestBoundedTable.of(Schema.FieldType.INT32, "c0", Schema.FieldType.INT32, "c1"));}
public void beam_f26837_0() throws Exception
{    RuleSet prepareRules = RuleSets.ofList(SortProjectTransposeRule.INSTANCE, EnumerableRules.ENUMERABLE_JOIN_RULE, EnumerableRules.ENUMERABLE_PROJECT_RULE, EnumerableRules.ENUMERABLE_SORT_RULE, EnumerableRules.ENUMERABLE_TABLE_SCAN_RULE);    String sqlQuery = "select * from \"tt\".\"large_table\" as large_table " + " JOIN \"tt\".\"medium_table\" as medium_table on large_table.\"medium_key\" = medium_table.\"large_key\" " + " JOIN \"tt\".\"small_table\" as small_table on medium_table.\"small_key\" = small_table.\"medium_key\" ";    RelNode originalPlan = transform(sqlQuery, prepareRules);    RelNode optimizedPlan = transform(sqlQuery, RuleSets.ofList(ImmutableList.<RelOptRule>builder().addAll(prepareRules).add(BeamJoinPushThroughJoinRule.LEFT).build()));    assertTopTableInJoins(originalPlan, "small_table");    assertTopTableInJoins(optimizedPlan, "large_table");}
public void beam_f26838_0() throws Exception
{    RuleSet prepareRules = RuleSets.ofList(SortProjectTransposeRule.INSTANCE, EnumerableRules.ENUMERABLE_JOIN_RULE, EnumerableRules.ENUMERABLE_PROJECT_RULE, EnumerableRules.ENUMERABLE_SORT_RULE, EnumerableRules.ENUMERABLE_TABLE_SCAN_RULE);    String sqlQuery = "select * from \"tt\".\"medium_table\" as medium_table " + " JOIN \"tt\".\"large_table\" as large_table on large_table.\"medium_key\" = medium_table.\"large_key\" " + " JOIN \"tt\".\"small_table\" as small_table on medium_table.\"small_key\" = small_table.\"medium_key\" ";    RelNode originalPlan = transform(sqlQuery, prepareRules);    RelNode optimizedPlan = transform(sqlQuery, RuleSets.ofList(ImmutableList.<RelOptRule>builder().addAll(prepareRules).add(BeamJoinPushThroughJoinRule.RIGHT).build()));    assertTopTableInJoins(originalPlan, "small_table");    assertTopTableInJoins(optimizedPlan, "large_table");}
protected Map<String, org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.schema.Table> beam_f26847_0()
{    return tables;}
public Statistic beam_f26848_0()
{    List<RelFieldCollation> collationFields = new ArrayList<>();    for (Integer key : pkColumns) {        collationFields.add(new RelFieldCollation(key, RelFieldCollation.Direction.ASCENDING, RelFieldCollation.NullDirection.LAST));    }    return Statistics.of(data.size(), ImmutableList.of(pkColumns), ImmutableList.of(RelCollations.of(collationFields)));}
public void beam_f26857_0()
{    VarianceAccumulator result = VarianceAccumulator.EMPTY.combineWith(ofSingleElement(THREE));    assertEquals(newVarianceAccumulator(ZERO, ONE, THREE), result);}
public void beam_f26858_0()
{    VarianceAccumulator result = ofSingleElement(THREE).combineWith(VarianceAccumulator.EMPTY);    assertEquals(newVarianceAccumulator(ZERO, ONE, THREE), result);}
public void beam_f26867_0() throws Exception
{    Schema resultType = Schema.builder().addBooleanField("field_1").addBooleanField("field_2").addBooleanField("field_3").addBooleanField("field_4").build();    Row resultRow = Row.withSchema(resultType).addValues(false, false, true, true).build();    String sql = "SELECT IS_NAN(f_float_2), IS_NAN(f_double_2), IS_NAN(f_float_3), IS_NAN(f_double_3) FROM PCOLLECTION";    PCollection<Row> result = boundedInputFloatDouble.apply("testUdf", SqlTransform.query(sql));    PAssert.that(result).containsInAnyOrder(resultRow);    pipeline.run().waitUntilFinish();}
public void beam_f26868_0() throws Exception
{    Schema resultType = Schema.builder().addInt64Field("field").build();    Row resultRow = Row.withSchema(resultType).addValues(10L).build();    Row resultRow2 = Row.withSchema(resultType).addValues(0L).build();    Row resultRow3 = Row.withSchema(resultType).addValues(2L).build();    String sql = "SELECT LENGTH(f_bytes) FROM PCOLLECTION WHERE f_func = 'LENGTH'";    PCollection<Row> result = boundedInputBytes.apply("testUdf", SqlTransform.query(sql));    PAssert.that(result).containsInAnyOrder(resultRow, resultRow2, resultRow3);    pipeline.run().waitUntilFinish();}
public void beam_f26877_0() throws Exception
{    ExpressionChecker checker = new ExpressionChecker().addExpr("STARTS_WITH('string1', 'stri')", true).addExpr("STARTS_WITH('string2', 'str1')", false).addExpr("STARTS_WITH('', '')", true).addExpr("STARTS_WITH('中文', '文')", false).addExpr("STARTS_WITH('中文', '中')", true);    checker.buildRunAndCheck();}
public void beam_f26878_0() throws Exception
{    ExpressionChecker checker = new ExpressionChecker().addExpr("LENGTH('')", 0L).addExpr("LENGTH('abcde')", 5L).addExpr("LENGTH('中文')", 2L).addExpr("LENGTH('\0\0')", 2L).addExpr("LENGTH('абвгд')", 5L).addExprWithNullExpectedValue("LENGTH(CAST(NULL as VARCHAR(0)))", TypeName.INT64).addExprWithNullExpectedValue("LENGTH(CAST(NULL as VARBINARY(0)))", TypeName.INT64);    checker.buildRunAndCheck();}
 Map<String, RelDataType> beam_f26887_0(Schema schema)
{    final RelDataType dataType = CalciteUtils.toCalciteRowType(schema, dataTypeFactory);    return dataType.getFieldNames().stream().collect(Collectors.toMap(x -> x, x -> dataType.getField(x, /*caseSensitive=*/    true, /*elideRecord=*/    false).getType()));}
public void beam_f26888_0()
{    final Schema schema = Schema.builder().addField("f1", Schema.FieldType.BYTE).addField("f2", Schema.FieldType.INT16).addField("f3", Schema.FieldType.INT32).addField("f4", Schema.FieldType.INT64).addField("f5", Schema.FieldType.FLOAT).addField("f6", Schema.FieldType.DOUBLE).addField("f7", Schema.FieldType.DECIMAL).addField("f8", Schema.FieldType.BOOLEAN).addField("f9", Schema.FieldType.BYTES).addField("f10", Schema.FieldType.STRING).build();    final Map<String, RelDataType> fields = calciteRowTypeFields(schema);    assertEquals(10, fields.size());    fields.values().forEach(x -> assertFalse(x.isNullable()));    assertEquals(SqlTypeName.TINYINT, fields.get("f1").getSqlTypeName());    assertEquals(SqlTypeName.SMALLINT, fields.get("f2").getSqlTypeName());    assertEquals(SqlTypeName.INTEGER, fields.get("f3").getSqlTypeName());    assertEquals(SqlTypeName.BIGINT, fields.get("f4").getSqlTypeName());    assertEquals(SqlTypeName.FLOAT, fields.get("f5").getSqlTypeName());    assertEquals(SqlTypeName.DOUBLE, fields.get("f6").getSqlTypeName());    assertEquals(SqlTypeName.DECIMAL, fields.get("f7").getSqlTypeName());    assertEquals(SqlTypeName.BOOLEAN, fields.get("f8").getSqlTypeName());    assertEquals(SqlTypeName.VARBINARY, fields.get("f9").getSqlTypeName());    assertEquals(SqlTypeName.VARCHAR, fields.get("f10").getSqlTypeName());}
public int beam_f26897_0()
{    return Objects.hash(ageYears, name);}
public Integer beam_f26898_0()
{    return amount;}
public void beam_f26907_0()
{    PCollection<PersonBean> people = PBegin.in(pipeline).apply("people", Create.of(new PersonBean("Foo", 5), new PersonBean("Bar", 53)));    PCollection<OrderBean> orders = PBegin.in(pipeline).apply("orders", Create.of(new OrderBean("Foo", 15), new OrderBean("Foo", 10), new OrderBean("Foo", 5), new OrderBean("Bar", 53), new OrderBean("Bar", 54), new OrderBean("Bar", 55)));    String sql = "SELECT name, SUM(amount) as total " + "FROM buyers INNER JOIN orders " + "ON buyerName = name " + "GROUP BY name";    PCollection<Row> result = tuple("buyers", people, "orders", orders).apply("sql", SqlTransform.query(sql));    PAssert.that(result).containsInAnyOrder(TestUtils.rowsBuilderOf(Schema.builder().addStringField("name").addInt32Field("total").build()).addRows("Foo", 30, "Bar", 162).getRows());    pipeline.run();}
protected PCollection<Row> beam_f26908_0()
{    try {        return TestBoundedTable.of(ROW_TYPE).addRows(parseTimestampWithUTCTimeZone("1986-02-15 11:35:26"), (byte) 1, (short) 1, 1, 1L, 1.0f, 1.0, BigDecimal.ONE, (byte) 127, (short) 32767, 2147483647, 9223372036854775807L).buildIOReader(pipeline.begin()).setRowSchema(ROW_TYPE);    } catch (Exception e) {        throw new RuntimeException(e);    }}
public SqlExpressionChecker beam_f26917_0(String expr)
{    exprs.add(expr);    return this;}
public void beam_f26918_0(Pipeline pipeline) throws Exception
{    checkPTransform(pipeline);    checkJdbc(pipeline.getOptions());}
public void beam_f26927_0()
{    ExpressionChecker checker = new ExpressionChecker().addExpr("c_tinyint_2 <= c_tinyint_1", false).addExpr("c_tinyint_1 <= c_tinyint_1", true).addExpr("c_tinyint_1 <= c_tinyint_2", true).addExpr("c_smallint_2 <= c_smallint_1", false).addExpr("c_smallint_1 <= c_smallint_1", true).addExpr("c_smallint_1 <= c_smallint_2", true).addExpr("c_integer_2 <= c_integer_1", false).addExpr("c_integer_1 <= c_integer_1", true).addExpr("c_integer_1 <= c_integer_2", true).addExpr("c_bigint_2 <= c_bigint_1", false).addExpr("c_bigint_1 <= c_bigint_1", true).addExpr("c_bigint_1 <= c_bigint_2", true).addExpr("c_float_2 <= c_float_1", false).addExpr("c_float_1 <= c_float_1", true).addExpr("c_float_1 <= c_float_2", true).addExpr("c_double_2 <= c_double_1", false).addExpr("c_double_1 <= c_double_1", true).addExpr("c_double_1 <= c_double_2", true).addExpr("c_decimal_2 <= c_decimal_1", false).addExpr("c_decimal_1 <= c_decimal_1", true).addExpr("c_decimal_1 <= c_decimal_2", true).addExpr("c_varchar_2 <= c_varchar_1", false).addExpr("c_varchar_1 <= c_varchar_1", true).addExpr("c_varchar_1 <= c_varchar_2", true).addExpr("c_boolean_false <= c_boolean_true", true).addExpr("c_boolean_true <= c_boolean_false", false);    checker.buildRunAndCheck();}
public void beam_f26928_0() throws Exception
{    ExpressionChecker checker = new ExpressionChecker().addExpr("1 IS NOT NULL", true).addExpr("NULL IS NOT NULL", false).addExpr("1 IS NULL", false).addExpr("NULL IS NULL", true);    checker.buildRunAndCheck();}
public Table beam_f26937_0(String tableName)
{    return delegateTableProvider.getTable(tableName);}
public Table beam_f26938_0(TableName fullTableName)
{                String actualTableName = String.join("_", fullTableName.getPath()) + "_" + fullTableName.getTableName();    return delegateTableProvider.getTable(actualTableName);}
public void beam_f26947_0() throws Exception
{    CustomResolutionTestTableProvider tableProvider = new CustomResolutionTestTableProvider();    tableProvider.createTable(Table.builder().name("testtable").schema(BASIC_SCHEMA).type("test").build());    tableProvider.addRows("testtable", row(1, "one"), row(2, "two"));    CustomResolutionTestTableProvider tableProvider2 = new CustomResolutionTestTableProvider();    tableProvider2.createTable(Table.builder().name("testtable2").schema(BASIC_SCHEMA).type("test").build());    tableProvider2.addRows("testtable2", row(3, "three"), row(4, "four"));    PCollection<Row> result = pipeline.apply(SqlTransform.query("SELECT id, name FROM testprovider2.testtable2").withTableProvider("testprovider2", tableProvider2).withDefaultTableProvider("testprovider", tableProvider));    PAssert.that(result).containsInAnyOrder(row(3, "three"), row(4, "four"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26948_0() throws Exception
{    CustomResolutionTestTableProvider tableProvider = new CustomResolutionTestTableProvider();    tableProvider.createTable(Table.builder().name("testtable").schema(BASIC_SCHEMA).type("test").build());    tableProvider.addRows("testtable", row(1, "one"), row(2, "two"));    CustomResolutionTestTableProvider tableProvider2 = new CustomResolutionTestTableProvider();    tableProvider2.createTable(Table.builder().name("testtable2").schema(BASIC_SCHEMA).type("test").build());    tableProvider2.addRows("testtable2", row(3, "three"), row(4, "four"));    PCollection<Row> result = pipeline.apply(SqlTransform.query("SELECT id, name FROM testprovider2.testtable2").withTableProvider("testprovider2", tableProvider2).withDefaultTableProvider("testprovider", tableProvider));    PAssert.that(result).containsInAnyOrder(row(3, "three"), row(4, "four"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26957_0() throws Exception
{    CustomResolutionTestTableProvider tableProvider = new CustomResolutionTestTableProvider();    tableProvider.createTable(Table.builder().name("testtable_blah_foo_bar").schema(BASIC_SCHEMA).type("test").build());    tableProvider.addRows("testtable_blah_foo_bar", row(3, "customer"), row(2, "nobody"));    CustomResolutionTestTableProvider tableProvider2 = new CustomResolutionTestTableProvider();    tableProvider2.createTable(Table.builder().name("testtable_blah_foo_bar2").schema(BASIC_SCHEMA).type("test").build());    tableProvider2.addRows("testtable_blah_foo_bar2", row(4, "customer"), row(1, "nobody"));    PCollection<Row> result = pipeline.apply(SqlTransform.query("SELECT testprovider2.testtable.blah.foo.bar2.id, testtable.blah.foo.bar.name \n" + "FROM \n" + "  testprovider2.testtable.blah.foo.bar2 \n" + "JOIN \n" + "  testtable.blah.foo.bar \n" + "USING(name)").withTableProvider("testprovider2", tableProvider2).withDefaultTableProvider("testprovider", tableProvider));    PAssert.that(result).containsInAnyOrder(row(4, "customer"), row(1, "nobody"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
public void beam_f26958_0() throws Exception
{    CustomResolutionTestTableProvider tableProvider = new CustomResolutionTestTableProvider();    tableProvider.createTable(Table.builder().name("testtable_blah_foo_bar").schema(BASIC_SCHEMA).type("test").build());    tableProvider.addRows("testtable_blah_foo_bar", row(3, "customer"), row(2, "nobody"));    CustomResolutionTestTableProvider tableProvider2 = new CustomResolutionTestTableProvider();    tableProvider2.createTable(Table.builder().name("testtable_blah_foo_bar2").schema(BASIC_SCHEMA).type("test").build());    tableProvider2.addRows("testtable_blah_foo_bar2", row(4, "customer"), row(1, "nobody"));    PCollection<Row> result = pipeline.apply(SqlTransform.query("SELECT b.id, a.name \n" + "FROM \n" + "  testprovider2.testtable.blah.foo.bar2 AS b \n" + "JOIN \n" + "  testtable.blah.foo.bar a\n" + "USING(name)").withTableProvider("testprovider2", tableProvider2).withDefaultTableProvider("testprovider", tableProvider));    PAssert.that(result).containsInAnyOrder(row(4, "customer"), row(1, "nobody"));    pipeline.run().waitUntilFinish(Duration.standardMinutes(2));}
private Row beam_f26967_0(Schema schema, Object... values)
{    return Row.withSchema(schema).addValues(values).build();}
public void beam_f26968_0()
{    BigQueryTableProvider provider = new BigQueryTableProvider();    Table table = getTable("testTable", bigQuery.tableSpec());    BeamSqlTable sqlTable = provider.buildBeamSqlTable(table);    BeamTableStatistics size = sqlTable.getTableStatistics(TestPipeline.testingPipelineOptions());    assertNotNull(size);    assertEquals(0d, size.getRowCount(), 0.1);}
 String beam_f26977_0()
{    return this.jobName;}
 void beam_f26978_0(String name, Table table)
{    tableSpecMap.put(name, table);}
public void beam_f26987_0()
{    KafkaCSVTestTable table = getTable(3);    for (int i = 0; i < 100; i++) {        table.addRecord(KafkaTestRecord.create("key" + i, i + ",1,2", "topic1", 1000));    }    BeamTableStatistics stats = table.getTableStatistics(null);    Assert.assertTrue(stats.isUnknown());}
public void beam_f26988_0(ProcessContext c)
{    System.out.println("we are here");    System.out.println(c.element().getValues());}
public void beam_f26997_0() throws InterruptedException
{    KafkaOptions kafkaOptions = pipeline.getOptions().as(KafkaOptions.class);    pipeline.getOptions().as(DirectOptions.class).setBlockOnRun(false);    String createTableString = "CREATE EXTERNAL TABLE kafka_table(\n" + "order_id INTEGER, \n" + "member_id INTEGER, \n" + "item_name INTEGER \n" + ") \n" + "TYPE 'kafka' \n" + "LOCATION '" + "'\n" + "TBLPROPERTIES '" + getKafkaPropertiesString(kafkaOptions) + "'";    TableProvider tb = new KafkaTableProvider();    BeamSqlEnv env = BeamSqlEnv.inMemory(tb);    env.executeDdl(createTableString);    PCollection<Row> queryOutput = BeamSqlRelUtils.toPCollection(pipeline, env.parseQuery("SELECT * FROM kafka_table"));    queryOutput.apply(ParDo.of(new FakeKvPair())).apply("waitForSuccess", ParDo.of(new StreamAssertEqual(ImmutableSet.of(row(TEST_TABLE_SCHEMA, 0, 1, 0), row(TEST_TABLE_SCHEMA, 1, 2, 1), row(TEST_TABLE_SCHEMA, 2, 3, 2)))));    queryOutput.apply(logRecords(""));    pipeline.run();    TimeUnit.MILLISECONDS.sleep(3000);    produceSomeRecords(3);    for (int i = 0; i < 200; i++) {        if (FLAG.getOrDefault(pipeline.getOptions().getOptionsId(), false)) {            return;        }        TimeUnit.MILLISECONDS.sleep(60);    }    Assert.fail();}
private static MapElements<Row, Void> beam_f26998_0(String suffix)
{    return MapElements.via(new SimpleFunction<Row, Void>() {        @Override        @Nullable        public Void apply(Row input) {            System.out.println(input.getValues() + suffix);            return null;        }    });}
public void beam_f27007_0(KafkaTestRecord record)
{    records.add(record);}
 double beam_f27008_0(int numberOfRecords) throws NoEstimationException
{    return super.computeRate(mkMockConsumer(new HashMap<>()), numberOfRecords);}
public static KafkaTestRecord beam_f27017_0(String newKey, String newValue, String newTopic, long newTimeStamp)
{    return new AutoValue_KafkaTestRecord(newKey, newValue, newTopic, newTimeStamp);}
public void beam_f27018_0()
{    String schemaString = "{\"namespace\": \"example.avro\",\n" + " \"type\": \"record\",\n" + " \"name\": \"User\",\n" + " \"fields\": [\n" + "     {\"name\": \"name\", \"type\": \"string\"},\n" + "     {\"name\": \"favorite_number\", \"type\": \"int\"},\n" + "     {\"name\": \"favorite_color\", \"type\": \"string\"},\n" + "     {\"name\": \"price\", \"type\": \"double\"}\n" + " ]\n" + "}";    Schema schema = (new Schema.Parser()).parse(schemaString);    GenericRecord before = new GenericData.Record(schema);    before.put("name", "Bob");    before.put("favorite_number", 256);    before.put("favorite_color", "red");    before.put("price", 2.4);    AvroCoder<GenericRecord> coder = AvroCoder.of(schema);    PCollection<Row> rows = pipeline.apply("create PCollection<GenericRecord>", Create.of(before).withCoder(coder)).apply("convert", GenericRecordReadConverter.builder().beamSchema(payloadSchema).build());    PAssert.that(rows).containsInAnyOrder(Row.withSchema(payloadSchema).addValues("Bob", 256, "red", 2.4).build());    pipeline.run();}
private static boolean beam_f27027_0(PubsubMessage message1, PubsubMessage message2)
{    return message1.getAttributeMap().equals(message2.getAttributeMap()) && Arrays.equals(message1.getPayload(), message2.getPayload());}
private Row beam_f27028_0(Schema schema, Object... values)
{    return Row.withSchema(schema).addValues(values).build();}
public void beam_f27037_0()
{    PubsubJsonTableProvider provider = new PubsubJsonTableProvider();    Schema messageSchema = Schema.builder().addDateTimeField("event_timestamp").addRowField("payload", Schema.builder().build()).build();    Table tableDefinition = tableDefinition().schema(messageSchema).build();    thrown.expectMessage("Unsupported");    thrown.expectMessage("'attributes'");    provider.buildBeamSqlTable(tableDefinition);}
public void beam_f27038_0()
{    PubsubJsonTableProvider provider = new PubsubJsonTableProvider();    Schema messageSchema = Schema.builder().addDateTimeField("event_timestamp").addMapField("attributes", VARCHAR, VARCHAR).build();    Table tableDefinition = tableDefinition().schema(messageSchema).build();    thrown.expectMessage("Unsupported");    thrown.expectMessage("'payload'");    provider.buildBeamSqlTable(tableDefinition);}
private static Set<V> beam_f27047_0(Iterable<PubsubMessage> messages, Function<? super PubsubMessage, V> mapper)
{    return StreamSupport.stream(messages.spliterator(), false).map(mapper).collect(toSet());}
public void beam_f27049_0() throws Exception
{    Files.write(tempFolder.newFile("test.csv").toPath(), "hello,13\n\ngoodbye,42\n".getBytes(Charsets.UTF_8));    BeamSqlEnv env = BeamSqlEnv.inMemory(new TextTableProvider());    env.executeDdl(String.format("CREATE EXTERNAL TABLE test %s TYPE text LOCATION '%s/*'", SQL_CSV_SCHEMA, tempFolder.getRoot()));    PCollection<Row> rows = BeamSqlRelUtils.toPCollection(pipeline, env.parseQuery("SELECT * FROM test"));    PAssert.that(rows).containsInAnyOrder(Row.withSchema(CSV_SCHEMA).addValues("hello", 13).build(), Row.withSchema(CSV_SCHEMA).addValues("goodbye", 42).build());    pipeline.run();}
public void beam_f27058_0() throws Exception
{    Table table = mockTable("person", "invalid");    store.createTable(table);}
public void beam_f27059_0() throws Exception
{    Table table = mockTable("person");    store.createTable(table);    store.createTable(table);}
public Map<String, Table> beam_f27070_0()
{    Map<String, Table> ret = new HashMap(names.length);    for (String name : names) {        ret.put(name, mockTable(name, "mock"));    }    return ret;}
public BeamSqlTable beam_f27071_0(Table table)
{    return null;}
public static List<String> beam_f27080_0(List<Row> rows)
{    List<String> strs = new ArrayList<>();    for (Row row : rows) {        strs.add(row.toString());    }    return strs;}
public static RowsBuilder beam_f27081_0(Schema type)
{    return RowsBuilder.of(type);}
public static PCollectionBuilder beam_f27090_0()
{    return new PCollectionBuilder();}
public PCollectionBuilder beam_f27091_0(Schema type)
{    this.type = type;    return this;}
public static DateTime beam_f27100_0(String str)
{        return DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ssZ").parseDateTime(str);}
public static DateTime beam_f27101_0(String str)
{    return DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss").parseDateTime(str);}
public FieldType beam_f27110_0(SourceOfRandomness random, GenerationStatus status)
{    incrementNesting(status);    return generateFieldType(random, status);}
 int beam_f27111_0(GenerationStatus status)
{    return status.valueOf(NESTING_KEY).orElse(-1);}
public void beam_f27120_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT Key FROM weird.`\\n\\t\\r\\f`");    PAssert.that(result).containsInAnyOrder(singleValue(14L), singleValue(15L));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27121_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT RowKey FROM " + FULL_ON_ID);    PAssert.that(result).containsInAnyOrder(singleValue(16L), singleValue(15L));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27130_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT t1.Key FROM c.d.e AS t1 INNER JOIN c.g.e AS t2 ON t1.Key = t2.RowKey");    PAssert.that(result).containsInAnyOrder(singleValue(15L));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
public void beam_f27131_0() throws Exception
{    FrameworkConfig cfg = initializeCalcite();    PCollection<Row> result = applySqlTransform(pipeline, cfg, "SELECT b.Key FROM a.b INNER JOIN c.d.e ON b.Key = e.Key");    PAssert.that(result).containsInAnyOrder(singleValue(14L), singleValue(15L));    pipeline.run().waitUntilFinish(Duration.standardMinutes(TWO_MINUTES));}
private Schema beam_f27140_0()
{    return Schema.builder().addStringField("field1").build();}
public void beam_f27141_0()
{    initializeBeamTableProvider();    initializeCalciteEnvironment();}
public void beam_f27150_0()
{    String sql = "SELECT @p0 IS NOT NULL AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createNullValue(TypeFactory.createStructType(Arrays.asList(new StructField("a", TypeFactory.createSimpleType(TypeKind.TYPE_STRING))))));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.BOOLEAN).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(false).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27151_0()
{    String sql = "SELECT IF(@p0, @p1, @p2) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createBoolValue(true), "p1", Value.createInt64Value(1), "p2", Value.createInt64Value(2));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.INT64).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27160_0()
{    String sql = "SELECT NULLIF(@p0, @p1) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("foo"), "p1", Value.createStringValue("null"));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.STRING).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("foo").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27161_0()
{    String sql = "SELECT IFNULL(@p0, @p1) AS ColA";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("foo"), "p1", Value.createStringValue("default"));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.STRING).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("foo").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27170_0()
{    String sql = "SELECT MOD(4, 2)";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(0L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27171_0()
{    String sql = "SELECT CAST (1243 as INT64), " + "CAST ('2018-09-15 12:59:59.000000+00' as TIMESTAMP), " + "CAST ('string' as STRING) " + " UNION ALL " + " SELECT CAST (1243 as INT64), " + "CAST ('2018-09-15 12:59:59.000000+00' as TIMESTAMP), " + "CAST ('string' as STRING);";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addDateTimeField("field2").addStringField("field3").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1243L, new DateTime(2018, 9, 15, 12, 59, 59, ISOChronology.getInstanceUTC()), "string").build(), Row.withSchema(schema).addValues(1243L, new DateTime(2018, 9, 15, 12, 59, 59, ISOChronology.getInstanceUTC()), "string").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27180_0()
{    String sql = "SELECT * " + "FROM KeyValue AS t1" + " FULL JOIN BigTable AS t2" + " on " + " t1.Key + t2.RowKey = 30";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27181_0()
{    String sql = "SELECT t3.Value, t2.Value, t1.Value, t1.Key, t3.ColId FROM KeyValue as t1 " + "JOIN BigTable as t2 " + "ON (t1.Key = t2.RowKey) " + "JOIN Spanner as t3 " + "ON (t3.ColId = t1.Key)";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addStringField("t3.Value").addStringField("t2.Value").addStringField("t1.Value").addInt64Field("t1.Key").addInt64Field("t3.ColId").build()).addValues("Spanner235", "BigTable235", "KeyValue235", 15L, 15L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27190_0()
{    String sql = "SELECT table_with_struct.struct_col.struct_col_str FROM table_with_struct WHERE id = 1;";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue("row_one").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27191_0()
{    String sql = "SELECT table_with_struct.id FROM table_with_struct WHERE" + " table_with_struct.struct_col.struct_col_str = 'row_one';";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue(1L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27200_0()
{    String sql = "SELECT Key, Value FROM KeyValue WHERE Key = 14 AND Value = 'non-existing';";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    PAssert.that(stream).containsInAnyOrder();    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27201_0()
{    String sql = "SELECT Key, Value FROM KeyValue WHERE Key = 14 OR Key = 15;";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addStringField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(14L, "KeyValue234").build(), Row.withSchema(schema).addValues(15L, "KeyValue235").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27210_0()
{    String sql = "SELECT Key, COUNT(*) FROM aggregate_test_table GROUP BY 1";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addInt64Field("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L, 2L).build(), Row.withSchema(schema).addValues(2L, 3L).build(), Row.withSchema(schema).addValues(3L, 2L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27211_0()
{    String sql = "SELECT Key AS K, COUNT(*) FROM aggregate_test_table GROUP BY K";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addInt64Field("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L, 2L).build(), Row.withSchema(schema).addValues(2L, 3L).build(), Row.withSchema(schema).addValues(3L, 2L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27220_0()
{    String sql = "SELECT Key, COUNT(*) FROM aggregate_test_table GROUP BY Key HAVING COUNT(*) > 2";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addInt64Field("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(2L, 3L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27221_0()
{    String sql = "SELECT " + "COUNT(*) as field_count, " + "TUMBLE_START(\"INTERVAL 1 SECOND\") as window_start, " + "TUMBLE_END(\"INTERVAL 1 SECOND\") as window_end " + "FROM KeyValue " + "GROUP BY TUMBLE(ts, \"INTERVAL 1 SECOND\");";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("count_start").addDateTimeField("field1").addDateTimeField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(1L, new DateTime(2018, 7, 1, 21, 26, 7, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 8, ISOChronology.getInstanceUTC())).build(), Row.withSchema(schema).addValues(1L, new DateTime(2018, 7, 1, 21, 26, 6, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 7, ISOChronology.getInstanceUTC())).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27230_0()
{    String sql = "SELECT " + "TIMESTAMP_ADD(TIMESTAMP '2008-12-25 15:30:00 UTC', INTERVAL 10 MINUTE), " + "TIMESTAMP_ADD(TIMESTAMP '2008-12-25 15:30:00+07:30', INTERVAL 10 MINUTE)";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addDateTimeField("f_timestamp_plus").addDateTimeField("f_timestamp_with_time_zone_plus").build()).addValues(DateTimeUtils.parseTimestampWithUTCTimeZone("2008-12-25 15:40:00"), parseTimestampWithTimeZone("2008-12-25 15:40:00+0730")).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27231_0()
{    String sql = "SELECT TIMESTAMP '2018-12-10 10:38:59-10:00'";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addDateTimeField("f_timestamp_with_time_zone").build()).addValues(parseTimestampWithTimeZone("2018-12-10 10:38:59-1000")).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27240_0()
{    String sql = "WITH T1 AS (SELECT * FROM KeyValue) SELECT T1.Key, COUNT(*) FROM T1 GROUP BY T1.Key";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").addInt64Field("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(14L, 1L).build(), Row.withSchema(schema).addValues(15L, 1L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27241_0()
{    String sql = "WITH T1 AS (SELECT * FROM window_test_table_two) SELECT " + "COUNT(*) as field_count, " + "SESSION_START(\"INTERVAL 3 SECOND\") as window_start, " + "SESSION_END(\"INTERVAL 3 SECOND\") as window_end " + "FROM T1 " + "GROUP BY SESSION(ts, \"INTERVAL 3 SECOND\");";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("count_star").addDateTimeField("field1").addDateTimeField("field2").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(2L, new DateTime(2018, 7, 1, 21, 26, 12, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 12, ISOChronology.getInstanceUTC())).build(), Row.withSchema(schema).addValues(2L, new DateTime(2018, 7, 1, 21, 26, 6, ISOChronology.getInstanceUTC()), new DateTime(2018, 7, 1, 21, 26, 6, ISOChronology.getInstanceUTC())).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27250_0()
{    String sql = "SELECT CASE WHEN 'abc' = '123' THEN 'not possible' END";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addNullableField("str_field", FieldType.STRING).build()).addValue(null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27251_0()
{    String sql = "SELECT CASE 2 WHEN 1 THEN 'not possible' END";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addNullableField("str_field", FieldType.STRING).build()).addValue(null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27260_0()
{    String sql = "SELECT STARTS_WITH('string1', 'ng0')";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.BOOLEAN).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues(false).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27261_0()
{    String sql = "SELECT STARTS_WITH(@p0, @p1)";    ImmutableMap<String, Value> params = ImmutableMap.<String, Value>builder().put("p0", Value.createSimpleNullValue(TypeKind.TYPE_STRING)).put("p1", Value.createStringValue("")).build();    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addNullableField("field1", FieldType.BOOLEAN).build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues((Boolean) null).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27270_0()
{    String sql = "SELECT concat('abc', 'def', '  ', 'xyz')";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("abcdef  xyz").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27271_0()
{    String sql = "SELECT concat('abc', 'def', '  ', 'xyz', 'kkk')";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("abcdef  xyzkkk").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27280_0()
{    String sql = "SELECT trim(@p0)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("   a b c   "));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("a b c").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27281_0()
{    String sql = "SELECT trim(@p0, @p1)";    ImmutableMap<String, Value> params = ImmutableMap.of("p0", Value.createStringValue("abxyzab"), "p1", Value.createStringValue("ab"));    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql, params);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("xyz").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27290_0()
{    String sql = "SELECT CAST(b'b' AS STRING)";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("b").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27291_0()
{    String sql = "SELECT CAST(bytes_col AS STRING) FROM table_all_types";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addStringField("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValues("1").build(), Row.withSchema(schema).addValues("2").build(), Row.withSchema(schema).addValues("3").build(), Row.withSchema(schema).addValues("4").build(), Row.withSchema(schema).addValues("5").build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27300_0()
{    String sql = "SELECT row_field FROM table_with_map";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema rowSchema = Schema.builder().addInt64Field("row_id").addStringField("data").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(Schema.builder().addRowField("row_field", rowSchema).build()).addValues(Row.withSchema(rowSchema).addValues(1L, "data1").build()).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27301_0()
{    String sql = "select sum(Key) from KeyValue\n" + "group by (select Key)";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Does not support sub-queries");    zetaSQLQueryPlanner.convertToBeamRel(sql);}
public void beam_f27310_0()
{    String sql = "SELECT row_id FROM table_all_types UNION ALL SELECT row_id FROM table_all_types_2";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    final Schema schema = Schema.builder().addInt64Field("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(schema).addValue(1L).build(), Row.withSchema(schema).addValue(2L).build(), Row.withSchema(schema).addValue(3L).build(), Row.withSchema(schema).addValue(4L).build(), Row.withSchema(schema).addValue(5L).build(), Row.withSchema(schema).addValue(6L).build(), Row.withSchema(schema).addValue(7L).build(), Row.withSchema(schema).addValue(8L).build(), Row.withSchema(schema).addValue(9L).build(), Row.withSchema(schema).addValue(10L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
public void beam_f27311_0()
{    String sql = "SELECT AVG(f_int_1) FROM aggregate_test_table;";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    thrown.expect(RuntimeException.class);    thrown.expectMessage("AVG(LONG) is not supported. You might want to use AVG(CAST(expression AS DOUBLE).");    zetaSQLQueryPlanner.convertToBeamRel(sql);}
public void beam_f27320_0()
{    String sql = "SELECT Key FROM KeyValue";    ZetaSQLQueryPlanner zetaSQLQueryPlanner = new ZetaSQLQueryPlanner(config);    BeamRelNode beamRelNode = zetaSQLQueryPlanner.convertToBeamRel(sql);    PCollection<Row> stream = BeamSqlRelUtils.toPCollection(pipeline, beamRelNode);    Schema singleField = Schema.builder().addInt64Field("field1").build();    PAssert.that(stream).containsInAnyOrder(Row.withSchema(singleField).addValues(14L).build(), Row.withSchema(singleField).addValues(15L).build());    pipeline.run().waitUntilFinish(Duration.standardMinutes(PIPELINE_EXECUTION_WAITTIME_MINUTES));}
private void beam_f27321_0()
{    initializeCalciteEnvironmentWithContext();}
public Combine.PerKey<K, InputT, byte[]> beam_f27330_0()
{    return Combine.perKey(initFn);}
public static Combine.Globally<byte[], byte[]> beam_f27331_0()
{    return Combine.globally(HllCountMergePartialFn.create());}
 void beam_f27340_0(int precision)
{    checkArgument(precision >= HllCount.MINIMUM_PRECISION && precision <= HllCount.MAXIMUM_PRECISION, "Invalid precision: %s. Valid range is [%s, %s].", precision, HllCount.MINIMUM_PRECISION, HllCount.MAXIMUM_PRECISION);    this.precision = precision;}
public byte[] beam_f27341_0()
{    return new byte[0];}
public HyperLogLogPlusPlus<Integer> beam_f27350_0(HyperLogLogPlusPlus<Integer> accumulator, Integer input)
{    accumulator.add(input.intValue());    return accumulator;}
public HyperLogLogPlusPlus<Long> beam_f27351_0()
{    return new HyperLogLogPlusPlus.Builder().normalPrecision(getPrecision()).buildForLongs();}
public HyperLogLogPlusPlus<HllT> beam_f27360_1(@Nullable HyperLogLogPlusPlus<HllT> accumulator, byte[] input)
{    if (input == null) {                return accumulator;    } else if (input.length == 0) {        return accumulator;    } else if (accumulator == null) {        @SuppressWarnings("unchecked")        HyperLogLogPlusPlus<HllT> result = (HyperLogLogPlusPlus<HllT>) HyperLogLogPlusPlus.forProto(input);        return result;    } else {        accumulator.merge(input);        return accumulator;    }}
public HyperLogLogPlusPlus<HllT> beam_f27361_0(Iterable<HyperLogLogPlusPlus<HllT>> accumulators)
{    HyperLogLogPlusPlus<HllT> merged = createAccumulator();    for (HyperLogLogPlusPlus<HllT> accumulator : accumulators) {        if (accumulator == null) {            continue;        }        if (merged == null) {            @SuppressWarnings("unchecked")            HyperLogLogPlusPlus<HllT> clonedAccumulator = (HyperLogLogPlusPlus<HllT>) HyperLogLogPlusPlus.forProto(accumulator.serializeToProto());                        merged = clonedAccumulator;        } else {            merged.merge(accumulator);        }    }    return merged;}
public void beam_f27370_0()
{    String tableSpec = String.format("%s.%s", DATASET_ID, SKETCH_TABLE_ID);    String query = String.format("SELECT HLL_COUNT.EXTRACT(%s) FROM %s", SKETCH_FIELD_NAME, tableSpec);    TableSchema tableSchema = new TableSchema().setFields(Collections.singletonList(new TableFieldSchema().setName(SKETCH_FIELD_NAME).setType(SKETCH_FIELD_TYPE)));    TestPipelineOptions options = TestPipeline.testingPipelineOptions().as(TestPipelineOptions.class);            options.setOnSuccessMatcher(BigqueryMatcher.createUsingStandardSql(APP_NAME, PROJECT_ID, query, EXPECTED_CHECKSUM));    Pipeline p = Pipeline.create(options);    p.apply(Create.of(TEST_DATA)).apply(HllCount.Init.forStrings().globally()).apply(BigQueryIO.<byte[]>write().to(tableSpec).withSchema(tableSchema).withFormatFunction(sketch -> new TableRow().set(SKETCH_FIELD_NAME, sketch)).withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));    p.run().waitUntilFinish();}
public void beam_f27371_0()
{    PCollection<byte[]> result = p.apply(Create.of(INTS1)).apply(HllCount.Init.forIntegers().globally());    PAssert.thatSingleton(result).isEqualTo(INTS1_SKETCH);    p.run();}
public void beam_f27380_0()
{    List<KV<String, String>> input = new ArrayList<>();    STRINGS.forEach(s -> input.add(KV.of("k", s)));    PCollection<KV<String, byte[]>> result = p.apply(Create.of(input)).apply(HllCount.Init.forStrings().perKey());    PAssert.that(result).containsInAnyOrder(Collections.singletonList(KV.of("k", STRINGS_SKETCH)));    p.run();}
public void beam_f27381_0()
{    List<KV<String, String>> input = new ArrayList<>();    STRINGS.forEach(s -> input.add(KV.of("k", s)));    PCollection<KV<String, byte[]>> result = p.apply(Create.of(input)).apply(HllCount.Init.forStrings().withPrecision(TEST_PRECISION).perKey());    PAssert.that(result).containsInAnyOrder(Collections.singletonList(KV.of("k", STRINGS_SKETCH_TEST_PRECISION)));    p.run();}
public void beam_f27390_0()
{    PCollection<byte[]> result = p.apply(Create.of(LONGS_SKETCH, LONGS_SKETCH_OF_EMPTY_SET)).apply(HllCount.MergePartial.globally());    PAssert.thatSingleton(result).isEqualTo(LONGS_SKETCH);    p.run();}
public void beam_f27391_0()
{    PCollection<byte[]> result = p.apply(Create.of(EMPTY_SKETCH, LONGS_SKETCH_OF_EMPTY_SET)).apply(HllCount.MergePartial.globally());    PAssert.thatSingleton(result).isEqualTo(LONGS_SKETCH_OF_EMPTY_SET);    p.run();}
public void beam_f27400_0()
{    PCollection<KV<String, Long>> result = p.apply(Create.of(KV.of("k", INTS1_SKETCH), KV.of("k", INTS2_SKETCH))).apply(HllCount.Extract.perKey());    PAssert.that(result).containsInAnyOrder(Arrays.asList(KV.of("k", INTS1_ESTIMATE), KV.of("k", INTS2_ESTIMATE)));    p.run();}
public void beam_f27401_0()
{    PCollection<KV<String, Long>> result = p.apply(Create.of(KV.of("k", EMPTY_SKETCH))).apply(HllCount.Extract.perKey());    PAssert.thatSingleton(result).isEqualTo(KV.of("k", 0L));    p.run();}
protected ManagedChannelBuilder<?> beam_f27410_0(ApiServiceDescriptor descriptor)
{    return channelFactory.builderFor(descriptor);}
public ManagedChannelFactory beam_f27411_0(List<ClientInterceptor> interceptors)
{    return new InterceptedManagedChannelFactory(channelFactory, interceptors);}
public StreamObserver<BeamFnApi.Elements> beam_f27420_0()
{    return outboundObserver;}
private CompletableFuture<Consumer<Data>> beam_f27421_0(LogicalEndpoint endpoint)
{    return consumers.computeIfAbsent(endpoint, (LogicalEndpoint unused) -> new CompletableFuture<>());}
public void beam_f27430_0() throws Exception
{    readFuture.awaitCompletion();}
public boolean beam_f27431_0()
{    return readFuture.isDone();}
public void beam_f27440_0()
{    future.complete(COMPLETED);}
public void beam_f27441_0(Throwable t)
{            future.obtrudeException(t);}
public static IdGenerator beam_f27450_0()
{    AtomicLong longs = new AtomicLong();    return () -> Long.toString(longs.decrementAndGet());}
public static void beam_f27451_0()
{    for (JvmInitializer initializer : ReflectHelpers.loadServicesOrdered(JvmInitializer.class)) {        initializer.onStartup();    }}
private void beam_f27460_0()
{    try {        while (true) {            int currentPhase = phaser.getPhase();            while (outboundObserver.isReady()) {                T value = queue.take();                if (value != POISON_PILL) {                    outboundObserver.onNext(value);                } else {                    return;                }            }            phaser.awaitAdvance(currentPhase);        }    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new IllegalStateException(e);    }}
public void beam_f27461_0(T value)
{    try {                while (!queue.offer(value, 60, TimeUnit.SECONDS)) {            checkState(!queueDrainer.isDone(), "Stream observer has finished.");        }    } catch (InterruptedException e) {        Thread.currentThread().interrupt();        throw new RuntimeException(e);    }}
public void beam_f27470_0(int i) throws IOException
{    output.write(i);    if (maximumChunkSize == output.size()) {        internalFlush();    }}
public void beam_f27471_0(byte[] b, int offset, int length) throws IOException
{    int spaceRemaining = maximumChunkSize - output.size();        if (length > spaceRemaining) {        output.write(b, offset, spaceRemaining);        offset += spaceRemaining;        length -= spaceRemaining;        internalFlush();    }        while (length > maximumChunkSize) {        output.write(b, offset, maximumChunkSize);        offset += maximumChunkSize;        length -= maximumChunkSize;        internalFlush();    }        output.write(b, offset, length);}
public void beam_f27480_0() throws Exception
{    queue.put((T) POISION_PILL);}
public void beam_f27481_0(T t) throws Exception
{    queue.put(t);}
public void beam_f27490_0(Throwable t)
{    inboundObserver.onError(t);}
public void beam_f27491_0()
{    inboundObserver.onCompleted();}
public StreamObserver<RespT> beam_f27500_0(BasicFactory<ReqT, RespT> baseOutboundObserverFactory, StreamObserver<ReqT> inboundObserver)
{    AdvancingPhaser phaser = new AdvancingPhaser(1);    inboundObserver = ForwardingClientResponseObserver.create(inboundObserver, phaser::arrive);    CallStreamObserver<RespT> outboundObserver = (CallStreamObserver<RespT>) baseOutboundObserverFactory.outboundObserverFor(inboundObserver);    return new BufferingStreamObserver<>(phaser, outboundObserver, executorService, bufferSize);}
public StreamObserver<RespT> beam_f27501_0(BasicFactory<ReqT, RespT> baseOutboundObserverFactory, StreamObserver<ReqT> inboundObserver)
{    return baseOutboundObserverFactory.outboundObserverFor(inboundObserver);}
public Statement beam_f27510_0(final Statement statement, Description arg1)
{    return new Statement() {        @Override        public void evaluate() throws Throwable {            Throwable thrown = null;            delegate = executorServiceSupplier.get();            try {                statement.evaluate();            } catch (Throwable t) {                thrown = t;            }            shutdown();            if (!awaitTermination(5, TimeUnit.SECONDS)) {                shutdownNow();                IllegalStateException e = new IllegalStateException("Test executor failed to shutdown cleanly.");                if (thrown != null) {                    thrown.addSuppressed(e);                } else {                    thrown = e;                }            }            if (thrown != null) {                throw thrown;            }        }    };}
public void beam_f27511_0() throws Throwable
{    Throwable thrown = null;    delegate = executorServiceSupplier.get();    try {        statement.evaluate();    } catch (Throwable t) {        thrown = t;    }    shutdown();    if (!awaitTermination(5, TimeUnit.SECONDS)) {        shutdownNow();        IllegalStateException e = new IllegalStateException("Test executor failed to shutdown cleanly.");        if (thrown != null) {            thrown.addSuppressed(e);        } else {            thrown = e;        }    }    if (thrown != null) {        throw thrown;    }}
private static Runnable beam_f27521_0()
{    return () -> {    };}
private static Consumer<T> beam_f27523_0()
{    return item -> {    };}
public void beam_f27536_0(EncodedBoundedWindow value, OutputStream outStream) throws CoderException, IOException
{    VarInt.encode(value.getEncodedWindow().size(), outStream);    value.getEncodedWindow().writeTo(outStream);}
public EncodedBoundedWindow beam_f27537_0(InputStream inStream) throws CoderException, IOException
{    int size = VarInt.decodeInt(inStream);    ByteString encodedWindow = ByteString.readFrom(ByteStreams.limit(inStream, size));    return EncodedBoundedWindow.forEncoding(encodedWindow);}
public void beam_f27546_0() throws Exception
{    final Collection<BeamFnApi.Elements> values = new ArrayList<>();    final AtomicBoolean onCompletedWasCalled = new AtomicBoolean();    CloseableFnDataReceiver<WindowedValue<byte[]>> consumer = BeamFnDataBufferingOutboundObserver.forLocation(OUTPUT_LOCATION, CODER, TestStreams.withOnNext(addToValuesConsumer(values)).withOnCompleted(setBooleanToTrue(onCompletedWasCalled)).build());        consumer.accept(valueInGlobalWindow(new byte[BeamFnDataBufferingOutboundObserver.DEFAULT_BUFFER_LIMIT_BYTES - 50]));    assertThat(values, empty());        consumer.accept(valueInGlobalWindow(new byte[50]));    assertEquals(messageWithData(new byte[BeamFnDataBufferingOutboundObserver.DEFAULT_BUFFER_LIMIT_BYTES - 50], new byte[50]), Iterables.get(values, 0));        consumer.accept(valueInGlobalWindow(new byte[BeamFnDataBufferingOutboundObserver.DEFAULT_BUFFER_LIMIT_BYTES - 50]));    assertEquals(1, values.size());        consumer.accept(valueInGlobalWindow(new byte[50]));    assertEquals(messageWithData(new byte[BeamFnDataBufferingOutboundObserver.DEFAULT_BUFFER_LIMIT_BYTES - 50], new byte[50]), Iterables.get(values, 1));        consumer.close();    assertEquals(messageWithData(), Iterables.get(values, 2));        try {        consumer.accept(valueInGlobalWindow(new byte[BeamFnDataBufferingOutboundObserver.DEFAULT_BUFFER_LIMIT_BYTES - 50]));        fail("Writing after close should be prohibited.");    } catch (IllegalStateException exn) {        }        try {        consumer.close();        fail("Closing twice should be prohibited.");    } catch (IllegalStateException exn) {        }}
public void beam_f27547_0() throws Exception
{    Collection<BeamFnApi.Elements> values = new ArrayList<>();    AtomicBoolean onCompletedWasCalled = new AtomicBoolean();    CloseableFnDataReceiver<WindowedValue<byte[]>> consumer = BeamFnDataBufferingOutboundObserver.forLocationWithBufferLimit(100, OUTPUT_LOCATION, CODER, TestStreams.withOnNext(addToValuesConsumer(values)).withOnCompleted(setBooleanToTrue(onCompletedWasCalled)).build());        consumer.accept(valueInGlobalWindow(new byte[51]));    assertThat(values, empty());        consumer.accept(valueInGlobalWindow(new byte[49]));    assertEquals(messageWithData(new byte[51], new byte[49]), Iterables.get(values, 0));            consumer.accept(valueInGlobalWindow(new byte[1]));    consumer.close();    assertEquals(BeamFnApi.Elements.newBuilder(messageWithData(new byte[1])).addData(BeamFnApi.Elements.Data.newBuilder().setInstructionId(OUTPUT_LOCATION.getInstructionId()).setTransformId(OUTPUT_LOCATION.getTransformId())).build(), Iterables.get(values, 1));}
public void beam_f27556_0() throws Exception
{    InboundDataClient client = CompletableFutureInboundDataClient.create();    Future<Void> waitingFuture = Executors.newSingleThreadExecutor().submit(() -> {        client.awaitCompletion();        return null;    });    try {        waitingFuture.get(50, TimeUnit.MILLISECONDS);        fail();    } catch (TimeoutException expected) {        }    client.complete();        waitingFuture.get();}
public void beam_f27557_0() throws Exception
{    CompletableFuture<Object> future = new CompletableFuture<>();    InboundDataClient client = CompletableFutureInboundDataClient.forBackingFuture(future);    assertThat(future.isDone(), is(false));    assertThat(client.isDone(), is(false));    client.complete();    assertThat(future.isDone(), is(true));    assertThat(client.isDone(), is(true));        client.awaitCompletion();}
public void beam_f27566_0()
{    IdGenerator gen = IdGenerators.decrementingLongs();    assertThat(gen.getId(), equalTo("-1"));    assertThat(gen.getId(), equalTo("-2"));}
public void beam_f27567_0()
{    IdGenerator gen = IdGenerators.decrementingLongs();    IdGenerator otherGen = IdGenerators.decrementingLongs();    assertThat(gen.getId(), equalTo("-1"));    assertThat(gen.getId(), equalTo("-2"));    assertThat(otherGen.getId(), equalTo("-1"));}
public Object beam_f27576_0()
{    throw new UnsupportedOperationException();}
public void beam_f27577_0() throws IllegalStateException
{    throw new UnsupportedOperationException();}
public void beam_f27587_0() throws Exception
{    final List<String> onNextValues = new ArrayList<>();    AdvancingPhaser phaser = new AdvancingPhaser(1);    final AtomicBoolean isCriticalSectionShared = new AtomicBoolean();    final BufferingStreamObserver<String> streamObserver = new BufferingStreamObserver<>(phaser, TestStreams.withOnNext((String t) -> {                                assertFalse(isCriticalSectionShared.getAndSet(true));        Uninterruptibles.sleepUninterruptibly(1, TimeUnit.MILLISECONDS);        onNextValues.add(t);        assertTrue(isCriticalSectionShared.getAndSet(false));    }).build(), executor, 3);    List<String> prefixes = ImmutableList.of("0", "1", "2", "3", "4");    List<Callable<String>> tasks = new ArrayList<>();    for (final String prefix : prefixes) {        tasks.add(() -> {            for (int i = 0; i < 10; i++) {                streamObserver.onNext(prefix + i);            }            return prefix;        });    }    List<Future<String>> results = executor.invokeAll(tasks);    for (Future<String> result : results) {        result.get();    }    streamObserver.onCompleted();        int[] prefixesIndex = new int[prefixes.size()];    assertEquals(50, onNextValues.size());    for (String onNextValue : onNextValues) {        int prefix = Integer.parseInt(onNextValue.substring(0, 1));        int suffix = Integer.parseInt(onNextValue.substring(1, 2));        assertEquals(prefixesIndex[prefix], suffix);        prefixesIndex[prefix] += 1;    }}
public void beam_f27588_0() throws Exception
{    AdvancingPhaser phaser = new AdvancingPhaser(1);    final AtomicBoolean elementsAllowed = new AtomicBoolean();    final BufferingStreamObserver<String> streamObserver = new BufferingStreamObserver<>(phaser, TestStreams.withOnNext((String t) -> assertTrue(elementsAllowed.get())).withIsReady(elementsAllowed::get).build(), executor, 3);        List<Future<String>> results = new ArrayList<>();    for (final String prefix : ImmutableList.of("0", "1", "2", "3", "4")) {        results.add(executor.submit(() -> {            for (int i = 0; i < 10; i++) {                streamObserver.onNext(prefix + i);            }            return prefix;        }));    }        Uninterruptibles.sleepUninterruptibly(10, TimeUnit.MILLISECONDS);    elementsAllowed.set(true);    phaser.arrive();    for (Future<String> result : results) {        result.get();    }    streamObserver.onCompleted();}
private void beam_f27597_0(Coder<T> coder, T... expected) throws IOException
{    ByteArrayOutputStream baos = new ByteArrayOutputStream();    for (T value : expected) {        int size = baos.size();        coder.encode(value, baos);                if (baos.size() - size == 0) {            baos.write(0);        }    }    Iterator<T> decoder = new DataStreamDecoder<>(coder, new ByteArrayInputStream(baos.toByteArray()));    Object[] actual = Iterators.toArray(decoder, Object.class);    assertArrayEquals(expected, actual);    assertFalse(decoder.hasNext());    assertFalse(decoder.hasNext());    thrown.expect(NoSuchElementException.class);    decoder.next();}
public void beam_f27598_0() throws Exception
{    List<ByteString> output = new ArrayList<>();    ElementDelimitedOutputStream outputStream = new ElementDelimitedOutputStream(output::add, 3);    outputStream.close();    assertThat(output, hasSize(0));}
public void beam_f27607_0()
{    StreamObserver<String> observer = OutboundObserverFactory.clientBuffered(Executors.newSingleThreadExecutor()).outboundObserverFor(this.fakeFactory(), mockRequestObserver);    assertThat(observer, instanceOf(BufferingStreamObserver.class));}
public void beam_f27608_0()
{    StreamObserver<String> observer = OutboundObserverFactory.clientBuffered(Executors.newSingleThreadExecutor(), 1).outboundObserverFor(this.fakeFactory(), mockRequestObserver);    assertThat(observer, instanceOf(BufferingStreamObserver.class));    assertEquals(1, ((BufferingStreamObserver<String>) observer).getBufferSize());}
public void beam_f27617_0() throws Throwable
{    ExecutorService service = Executors.newSingleThreadExecutor();    final TestExecutorService testService = TestExecutors.from(service);    final AtomicBoolean taskStarted = new AtomicBoolean();    final AtomicBoolean taskWasInterrupted = new AtomicBoolean();    final RuntimeException exceptionToThrow = new RuntimeException();    try {        testService.apply(new Statement() {            @Override            public void evaluate() throws Throwable {                testService.submit(this::taskToRun);                throw exceptionToThrow;            }            private void taskToRun() {                taskStarted.set(true);                try {                    while (true) {                        Thread.sleep(10000);                    }                } catch (InterruptedException e) {                    taskWasInterrupted.set(true);                    return;                }            }        }, null).evaluate();        fail();    } catch (RuntimeException thrownException) {        assertSame(exceptionToThrow, thrownException);        assertEquals(1, exceptionToThrow.getSuppressed().length);        assertEquals(IllegalStateException.class, exceptionToThrow.getSuppressed()[0].getClass());        assertEquals("Test executor failed to shutdown cleanly.", exceptionToThrow.getSuppressed()[0].getMessage());    }    assertTrue(service.isShutdown());}
public void beam_f27618_0() throws Throwable
{    testService.submit(this::taskToRun);    throw exceptionToThrow;}
public ThrowingFunction<WindowedValue<T>, WindowedValue<T>> beam_f27627_0(String ptransformId, PTransform ptransform) throws IOException
{    checkArgument(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN.equals(ptransform.getSpec().getUrn()));    checkArgument(ptransform.getInputsCount() == 1, "Expected only one input");    checkArgument(ptransform.getOutputsCount() == 1, "Expected only one output");    WindowIntoPayload payload = WindowIntoPayload.parseFrom(ptransform.getSpec().getPayload());    WindowFn<T, ?> windowFn = (WindowFn<T, ?>) WindowingStrategyTranslation.windowFnFromProto(payload.getWindowFn());    return AssignWindowsRunner.create(windowFn)::assignWindows;}
 static AssignWindowsRunner<T, W> beam_f27628_0(WindowFn<? super T, W> windowFn)
{        WindowFn<T, W> typedWindowFn = (WindowFn<T, W>) windowFn;    return new AssignWindowsRunner<>(typedWindowFn);}
public void beam_f27639_0()
{    consumer = beamFnDataClientFactory.send(apiServiceDescriptor, LogicalEndpoint.of(processBundleInstructionIdSupplier.get(), pTransformId), coder);}
public void beam_f27640_0() throws Exception
{    consumer.close();}
 void beam_f27649_0(WindowedValue<KV<KeyT, InputT>> elem) throws Exception
{    groupingTable.put(elem, (Object outputElem) -> output.accept((WindowedValue<KV<KeyT, AccumT>>) outputElem));}
 void beam_f27650_0() throws Exception
{    groupingTable.flush((Object outputElem) -> output.accept((WindowedValue<KV<KeyT, AccumT>>) outputElem));}
private void beam_f27659_0()
{    while (true) {        try {            bufferedInstructions.putFirst(POISON_PILL);            return;        } catch (InterruptedException e) {                }    }}
public void beam_f27660_0(Executor executor) throws InterruptedException, ExecutionException
{    BeamFnApi.InstructionRequest request;    while (!Objects.equals((request = bufferedInstructions.take()), POISON_PILL)) {        BeamFnApi.InstructionRequest currentRequest = request;        executor.execute(() -> {            try {                BeamFnApi.InstructionResponse response = delegateOnInstructionRequestType(currentRequest);                sendInstructionResponse(response);            } catch (Error e) {                sendErrorResponse(e);                throw e;            }        });    }    onFinish.get();}
public void beam_f27670_0(Builder requestBuilder, CompletableFuture<StateResponse> response)
{    throw new IllegalStateException(String.format("State API calls are unsupported because the " + "ProcessBundleRequest %s does not support state.", request));}
public Object beam_f27671_1(PipelineOptions pipelineOptions, BeamFnDataClient beamFnDataClient, BeamFnStateClient beamFnStateClient, String pTransformId, PTransform pTransform, Supplier<String> processBundleInstructionId, Map<String, PCollection> pCollections, Map<String, Coder> coders, Map<String, WindowingStrategy> windowingStrategies, PCollectionConsumerRegistry pCollectionConsumerRegistry, PTransformFunctionRegistry startFunctionRegistry, PTransformFunctionRegistry finishFunctionRegistry, BundleSplitListener splitListener)
{    String message = String.format("No factory registered for %s, known factories %s", pTransform.getSpec().getUrn(), knownUrns);        throw new IllegalStateException(message);}
public static FnDataReceiver<T> beam_f27680_0(Collection<FnDataReceiver<T>> consumers)
{    if (consumers.size() == 1) {        return Iterables.getOnlyElement(consumers);    }    return new MultiplexingFnDataReceiver<>(consumers);}
public void beam_f27681_0(T input) throws Exception
{    for (FnDataReceiver<T> consumer : consumers) {        consumer.accept(input);    }}
public InboundDataClient beam_f27690_1(ApiServiceDescriptor apiServiceDescriptor, LogicalEndpoint inputLocation, Coder<T> coder, FnDataReceiver<T> consumer)
{        QueueingFnDataReceiver<T> queueingConsumer = new QueueingFnDataReceiver<T>(consumer);    InboundDataClient inboundDataClient = this.mainClient.receive(apiServiceDescriptor, inputLocation, coder, queueingConsumer);    queueingConsumer.inboundDataClient = inboundDataClient;    this.inboundDataClients.computeIfAbsent(inboundDataClient, (InboundDataClient idcToStore) -> idcToStore);    return inboundDataClient;}
private boolean beam_f27691_0()
{    for (InboundDataClient inboundDataClient : inboundDataClients.keySet()) {        if (!inboundDataClient.isDone()) {            return false;        }    }    if (!this.queue.isEmpty()) {        return false;    }    return true;}
public PipelineOptions beam_f27700_0()
{    return context.pipelineOptions;}
public PipelineOptions beam_f27701_0()
{    return context.pipelineOptions;}
public void beam_f27710_0()
{    Instant target;    if (period.equals(Duration.ZERO)) {        target = currentTimestamp.plus(offset);    } else {        long millisSinceStart = currentTimestamp.plus(offset).getMillis() % period.getMillis();        target = millisSinceStart == 0 ? currentTimestamp : currentTimestamp.plus(period).minus(millisSinceStart);    }    target = minTargetAndGcTime(target);    output(target);}
public org.apache.beam.sdk.state.Timer beam_f27711_0(Duration offset)
{    this.offset = offset;    return this;}
public InputT beam_f27720_0(DoFn<InputT, OutputT> doFn)
{    return element();}
public Object beam_f27721_0(String tagId)
{    return sideInput(sideInputMapping.get(tagId));}
public State beam_f27730_0(String stateId)
{    StateDeclaration stateDeclaration = context.doFnSignature.stateDeclarations().get(stateId);    checkNotNull(stateDeclaration, "No state declaration found for %s", stateId);    StateSpec<?> spec;    try {        spec = (StateSpec<?>) stateDeclaration.field().get(context.doFn);    } catch (IllegalAccessException e) {        throw new RuntimeException(e);    }    return spec.bind(stateId, stateAccessor);}
public org.apache.beam.sdk.state.Timer beam_f27731_0(String timerId)
{    checkState(currentElement.getValue() instanceof KV, "Accessing timer in unkeyed context. Current element is not a KV: %s.", currentElement.getValue());    return new FnApiTimer(timerId, (WindowedValue) currentElement);}
public Instant beam_f27740_0()
{    return currentElement.getTimestamp();}
public PaneInfo beam_f27741_0()
{    return currentElement.getPane();}
public Object beam_f27750_0(int index)
{    throw new UnsupportedOperationException("Element parameters are not supported.");}
public Instant beam_f27751_0(DoFn<InputT, OutputT> doFn)
{    return timestamp();}
public PipelineOptions beam_f27760_0()
{    return context.pipelineOptions;}
public PipelineOptions beam_f27761_0()
{    return context.pipelineOptions;}
public static void beam_f27770_0(Function<String, String> environmentVarGetter) throws Exception
{    JvmInitializers.runOnStartup();    System.out.format("SDK Fn Harness started%n");    System.out.format("Harness ID %s%n", environmentVarGetter.apply(HARNESS_ID));    System.out.format("Logging location %s%n", environmentVarGetter.apply(LOGGING_API_SERVICE_DESCRIPTOR));    System.out.format("Control location %s%n", environmentVarGetter.apply(CONTROL_API_SERVICE_DESCRIPTOR));    System.out.format("Pipeline options %s%n", environmentVarGetter.apply(PIPELINE_OPTIONS));    String id = environmentVarGetter.apply(HARNESS_ID);    PipelineOptions options = PipelineOptionsTranslation.fromJson(environmentVarGetter.apply(PIPELINE_OPTIONS));    Endpoints.ApiServiceDescriptor loggingApiServiceDescriptor = getApiServiceDescriptor(environmentVarGetter.apply(LOGGING_API_SERVICE_DESCRIPTOR));    Endpoints.ApiServiceDescriptor controlApiServiceDescriptor = getApiServiceDescriptor(environmentVarGetter.apply(CONTROL_API_SERVICE_DESCRIPTOR));    main(id, options, loggingApiServiceDescriptor, controlApiServiceDescriptor);}
public static void beam_f27771_0(String id, PipelineOptions options, Endpoints.ApiServiceDescriptor loggingApiServiceDescriptor, Endpoints.ApiServiceDescriptor controlApiServiceDescriptor) throws Exception
{    ManagedChannelFactory channelFactory;    List<String> experiments = options.as(ExperimentalOptions.class).getExperiments();    if (experiments != null && experiments.contains("beam_fn_api_epoll")) {        channelFactory = ManagedChannelFactory.createEpoll();    } else {        channelFactory = ManagedChannelFactory.createDefault();    }    OutboundObserverFactory outboundObserverFactory = HarnessStreamObserverFactories.fromOptions(options);    channelFactory = channelFactory.withInterceptors(ImmutableList.of(AddHarnessIdInterceptor.create(id)));    main(id, options, loggingApiServiceDescriptor, controlApiServiceDescriptor, channelFactory, outboundObserverFactory);}
public void beam_f27782_0()
{    inboundObserverCompletion.complete(COMPLETED);}
public static PTransformRunnerFactory<?> beam_f27783_0(ValueMapFnFactory<InputT, OutputT> fnFactory)
{    return new Factory<>(new CompressedValueOnlyMapperFactory<>(fnFactory));}
public long beam_f27792_0(T value) throws Exception
{        CoderSizeEstimator.Observer observer = new CoderSizeEstimator.Observer();    coder.registerByteSizeObserver(value, observer);    if (!observer.getIsLazy()) {        observer.advance();        return observer.observedSize;    } else {                        CountingOutputStream os = new CountingOutputStream(ByteStreams.nullOutputStream());        coder.encode(value, os);        return os.getCount();    }}
public static WindowedPairInfo beam_f27793_0()
{    return theInstance;}
private GroupingTableEntry<K, InputT, AccumT> beam_f27802_0(final K key) throws Exception
{    return new GroupingTableEntry<K, InputT, AccumT>() {        final long keySize = keySizer.estimateSize(key);        AccumT accumulator = combiner.createAccumulator(key);                long accumulatorSize = 0;        @Override        public K getKey() {            return key;        }        @Override        public AccumT getValue() {            return accumulator;        }        @Override        public long getSize() {            return keySize + accumulatorSize;        }        @Override        public void compact() throws Exception {            AccumT newAccumulator = combiner.compact(key, accumulator);            if (newAccumulator != accumulator) {                accumulator = newAccumulator;                accumulatorSize = accumulatorSizer.estimateSize(newAccumulator);            }        }        @Override        public void add(InputT value) throws Exception {            accumulator = combiner.add(key, accumulator, value);            accumulatorSize = accumulatorSizer.estimateSize(accumulator);        }    };}
public K beam_f27803_0()
{    return key;}
public void beam_f27812_0(long maxSize)
{    this.maxSize = maxSize;}
public long beam_f27813_0()
{    return size;}
public PipelineOptions beam_f27822_0()
{    return context.pipelineOptions;}
public PipelineOptions beam_f27823_0()
{    return context.pipelineOptions;}
public void beam_f27832_0()
{    doFnInvoker.invokeFinishBundle(finishBundleContext);}
private void beam_f27833_0(Collection<FnDataReceiver<WindowedValue<T>>> consumers, WindowedValue<T> output)
{    try {        for (FnDataReceiver<WindowedValue<T>> consumer : consumers) {            consumer.accept(output);        }    } catch (Throwable t) {        throw UserCodeException.wrap(t);    }}
public void beam_f27842_1(StateResponse value)
{        CompletableFuture<StateResponse> responseFuture = outstandingRequests.remove(value.getId());    if (responseFuture != null) {        if (value.getError().isEmpty()) {            responseFuture.complete(value);        } else {            responseFuture.completeExceptionally(new IllegalStateException(value.getError()));        }    }}
public void beam_f27843_0(Throwable t)
{    closeAndCleanUp(t instanceof RuntimeException ? (RuntimeException) t : new RuntimeException(t));}
public void beam_f27852_0()
{    impl.clear();}
public void beam_f27853_0(T input)
{    impl.clear();    impl.append(input);}
public Iterable<T> beam_f27862_0()
{    return impl.get();}
public BagState<T> beam_f27863_0()
{        return this;}
public CombiningState<ElementT, AccumT, ResultT> beam_f27872_0()
{    return this;}
public ResultT beam_f27873_0()
{    Iterator<AccumT> iterator = impl.get().iterator();    if (iterator.hasNext()) {        return combineFn.extractOutput(iterator.next());    }    return combineFn.defaultValue();}
private BagUserState<T> beam_f27882_0(String stateId, Coder<T> valueCoder)
{    BagUserState<T> rval = new BagUserState<>(beamFnStateClient, processBundleInstructionId.get(), ptransformId, stateId, encodedCurrentWindowSupplier.get(), encodedCurrentKeySupplier.get(), valueCoder);    stateFinalizers.add(rval::asyncClose);    return rval;}
private StateKey beam_f27883_0(String stateId)
{    StateKey.Builder builder = StateKey.newBuilder();    builder.getBagUserStateBuilder().setWindow(encodedCurrentWindowSupplier.get()).setKey(encodedCurrentKeySupplier.get()).setTransformId(ptransformId).setUserStateId(stateId);    return builder.build();}
public static SideInputSpec beam_f27892_0(Coder<?> coder, Coder<W> windowCoder, ViewFn<?, ?> viewFn, WindowMappingFn<W> windowMappingFn)
{    return new AutoValue_SideInputSpec<>(coder, windowCoder, viewFn, windowMappingFn);}
public static Iterator<ByteString> beam_f27893_0(BeamFnStateClient beamFnStateClient, StateRequest stateRequestForFirstChunk)
{    return new LazyBlockingStateFetchingIterator(beamFnStateClient, stateRequestForFirstChunk);}
 static WindowMergingFnRunner<T, W> beam_f27902_0(WindowFn<?, W> windowFn)
{    if (windowFn.isNonMerging()) {        return new NonMergingWindowFnRunner();    } else {        return new MergingViaWindowFnRunner(windowFn);    }}
 KV<T, KV<Iterable<W>, Iterable<KV<W, Iterable<W>>>>> beam_f27903_0(KV<T, Iterable<W>> windowsToMerge)
{    return KV.of(windowsToMerge.getKey(), KV.of(windowsToMerge.getValue(), Collections.emptyList()));}
public WindowMappingFn<BoundedWindow> beam_f27912_0()
{    throw new UnsupportedOperationException();}
public boolean beam_f27913_0(WindowFn<?, ?> other)
{    throw new UnsupportedOperationException();}
public void beam_f27922_0() throws Exception
{    SdkComponents components = SdkComponents.create();    components.registerEnvironment(Environments.createDockerEnvironment("java"));    PTransform windowPTransform = PTransform.newBuilder().putInputs("in", "input").putOutputs("out", "output").setSpec(FunctionSpec.newBuilder().setUrn(PTransformTranslation.ASSIGN_WINDOWS_TRANSFORM_URN).setPayload(WindowIntoPayload.newBuilder().setWindowFn(WindowingStrategyTranslation.toProto(Sessions.withGapDuration(Duration.standardMinutes(12L)), components)).build().toByteString()).build()).build();    ThrowingFunction<WindowedValue<?>, WindowedValue<?>> fn = (ThrowingFunction) factory.forPTransform("transform", windowPTransform);    WindowedValue<?> output = fn.apply(WindowedValue.of(22L, new Instant(5), new IntervalWindow(new Instant(0L), new Instant(20027L)), PaneInfo.ON_TIME_AND_ONLY_FIRING));    assertThat(output, equalTo(WindowedValue.of(22L, new Instant(5), new IntervalWindow(new Instant(5L), Duration.standardMinutes(12L)), PaneInfo.ON_TIME_AND_ONLY_FIRING)));}
public IntervalWindow beam_f27923_0(Instant timestamp)
{    return new IntervalWindow(BoundedWindow.TIMESTAMP_MIN_VALUE, GlobalWindow.INSTANCE.maxTimestamp());}
public void beam_f27932_0() throws Exception
{    wasCloseCalled.set(true);}
public void beam_f27933_0(WindowedValue<String> t) throws Exception
{    outputValues.add(t);}
public void beam_f27942_0() throws Exception
{    List<WindowedValue<Long>> outValues = new ArrayList<>();    Collection<FnDataReceiver<WindowedValue<Long>>> consumers = ImmutableList.of(outValues::add);    ByteString encodedSource = ByteString.copyFrom(SerializableUtils.serializeToByteArray(CountingSource.upTo(3)));    BoundedSourceRunner<BoundedSource<Long>, Long> runner = new BoundedSourceRunner<>(PipelineOptionsFactory.create(), RunnerApi.FunctionSpec.newBuilder().setUrn(ProcessBundleHandler.JAVA_SOURCE_URN).setPayload(encodedSource).build(), consumers);    runner.start();    assertThat(outValues, contains(valueInGlobalWindow(0L), valueInGlobalWindow(1L), valueInGlobalWindow(2L)));}
public void beam_f27943_0() throws Exception
{    List<WindowedValue<String>> outputValues = new ArrayList<>();    MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    consumers.register("outputPC", "pTransformId", (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) outputValues::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "finish");    RunnerApi.FunctionSpec functionSpec = RunnerApi.FunctionSpec.newBuilder().setUrn("beam:source:java:0.1").setPayload(ByteString.copyFrom(SerializableUtils.serializeToByteArray(CountingSource.upTo(3)))).build();    RunnerApi.PTransform pTransform = RunnerApi.PTransform.newBuilder().setSpec(functionSpec).putInputs("input", "inputPC").putOutputs("output", "outputPC").build();    new BoundedSourceRunner.Factory<>().createRunnerForPTransform(PipelineOptionsFactory.create(), null, /* beamFnDataClient */    null, /* beamFnStateClient */    "pTransformId", pTransform, Suppliers.ofInstance("57L")::get, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);            Iterables.getOnlyElement(startFunctionRegistry.getFunctions()).run();    assertThat(outputValues, contains(valueInGlobalWindow(0L), valueInGlobalWindow(1L), valueInGlobalWindow(2L)));    outputValues.clear();        assertThat(consumers.keySet(), containsInAnyOrder("inputPC", "outputPC"));    consumers.getMultiplexingConsumer("inputPC").accept(valueInGlobalWindow(CountingSource.upTo(2)));    assertThat(outputValues, contains(valueInGlobalWindow(0L), valueInGlobalWindow(1L)));    assertThat(finishFunctionRegistry.getFunctions(), Matchers.empty());}
public void beam_f27952_0() throws Exception
{        MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    Deque<WindowedValue<KV<String, Integer>>> mainOutputValues = new ArrayDeque<>();    consumers.register(Iterables.getOnlyElement(pTransform.getOutputsMap().values()), TEST_COMBINE_ID, (FnDataReceiver) (FnDataReceiver<WindowedValue<KV<String, Integer>>>) mainOutputValues::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "finish");        MapFnRunners.forValueMapFnFactory(CombineRunners::createExtractOutputsMapFunction).createRunnerForPTransform(PipelineOptionsFactory.create(), null, null, TEST_COMBINE_ID, pTransform, null, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);    assertThat(startFunctionRegistry.getFunctions(), empty());    assertThat(finishFunctionRegistry.getFunctions(), empty());        mainOutputValues.clear();    assertThat(consumers.keySet(), containsInAnyOrder(inputPCollectionId, outputPCollectionId));    FnDataReceiver<WindowedValue<?>> input = consumers.getMultiplexingConsumer(inputPCollectionId);    input.accept(valueInGlobalWindow(KV.of("A", 9)));    input.accept(valueInGlobalWindow(KV.of("B", 5)));    input.accept(valueInGlobalWindow(KV.of("C", 7)));    assertThat(mainOutputValues, contains(valueInGlobalWindow(KV.of("A", -9)), valueInGlobalWindow(KV.of("B", -5)), valueInGlobalWindow(KV.of("C", -7))));}
public void beam_f27953_0() throws Exception
{        MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    Deque<WindowedValue<KV<String, Integer>>> mainOutputValues = new ArrayDeque<>();    consumers.register(Iterables.getOnlyElement(pTransform.getOutputsMap().values()), TEST_COMBINE_ID, (FnDataReceiver) (FnDataReceiver<WindowedValue<KV<String, Integer>>>) mainOutputValues::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "finish");        MapFnRunners.forValueMapFnFactory(CombineRunners::createCombineGroupedValuesMapFunction).createRunnerForPTransform(PipelineOptionsFactory.create(), null, null, TEST_COMBINE_ID, pTransform, null, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);    assertThat(startFunctionRegistry.getFunctions(), empty());    assertThat(finishFunctionRegistry.getFunctions(), empty());        mainOutputValues.clear();    assertThat(consumers.keySet(), containsInAnyOrder(inputPCollectionId, outputPCollectionId));    FnDataReceiver<WindowedValue<?>> input = consumers.getMultiplexingConsumer(inputPCollectionId);    input.accept(valueInGlobalWindow(KV.of("A", Arrays.asList("1", "2", "6"))));    input.accept(valueInGlobalWindow(KV.of("B", Arrays.asList("2", "3"))));    input.accept(valueInGlobalWindow(KV.of("C", Arrays.asList("5", "2"))));    assertThat(mainOutputValues, contains(valueInGlobalWindow(KV.of("A", -9)), valueInGlobalWindow(KV.of("B", -5)), valueInGlobalWindow(KV.of("C", -7))));}
public void beam_f27962_0() throws Exception
{    BeamFnApi.ProcessBundleDescriptor processBundleDescriptor = BeamFnApi.ProcessBundleDescriptor.newBuilder().putTransforms("2L", RunnerApi.PTransform.newBuilder().setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn(DATA_INPUT_URN).build()).build()).build();    Map<String, Message> fnApiRegistry = ImmutableMap.of("1L", processBundleDescriptor);    ProcessBundleHandler handler = new ProcessBundleHandler(PipelineOptionsFactory.create(), fnApiRegistry::get, beamFnDataClient, null, /* beamFnStateGrpcClientCache */    ImmutableMap.of(DATA_INPUT_URN, (PTransformRunnerFactory<Object>) (pipelineOptions, beamFnDataClient, beamFnStateClient, pTransformId, pTransform, processBundleInstructionId, pCollections, coders, windowingStrategies, pCollectionConsumerRegistry, startFunctionRegistry, finishFunctionRegistry, splitListener) -> {        thrown.expect(IllegalStateException.class);        thrown.expectMessage("TestException");        finishFunctionRegistry.register(pTransformId, ProcessBundleHandlerTest::throwException);        return null;    }));    handler.processBundle(BeamFnApi.InstructionRequest.newBuilder().setProcessBundle(BeamFnApi.ProcessBundleRequest.newBuilder().setProcessBundleDescriptorId("1L")).build());}
public void beam_f27963_0() throws Exception
{    BeamFnApi.ProcessBundleDescriptor processBundleDescriptor = BeamFnApi.ProcessBundleDescriptor.newBuilder().putTransforms("2L", RunnerApi.PTransform.newBuilder().setSpec(RunnerApi.FunctionSpec.newBuilder().setUrn(DATA_INPUT_URN).build()).build()).setStateApiServiceDescriptor(ApiServiceDescriptor.getDefaultInstance()).build();    Map<String, Message> fnApiRegistry = ImmutableMap.of("1L", processBundleDescriptor);    CompletableFuture<StateResponse> successfulResponse = new CompletableFuture<>();    CompletableFuture<StateResponse> unsuccessfulResponse = new CompletableFuture<>();    BeamFnStateGrpcClientCache mockBeamFnStateGrpcClient = Mockito.mock(BeamFnStateGrpcClientCache.class);    BeamFnStateClient mockBeamFnStateClient = Mockito.mock(BeamFnStateClient.class);    when(mockBeamFnStateGrpcClient.forApiServiceDescriptor(any())).thenReturn(mockBeamFnStateClient);    doAnswer(invocation -> {        StateRequest.Builder stateRequestBuilder = (StateRequest.Builder) invocation.getArguments()[0];        CompletableFuture<StateResponse> completableFuture = (CompletableFuture<StateResponse>) invocation.getArguments()[1];        new Thread(() -> {                                    Uninterruptibles.sleepUninterruptibly(500, TimeUnit.MILLISECONDS);            switch(stateRequestBuilder.getInstructionId()) {                case "SUCCESS":                    completableFuture.complete(StateResponse.getDefaultInstance());                    break;                case "FAIL":                    completableFuture.completeExceptionally(new RuntimeException("TEST ERROR"));            }        }).start();        return null;    }).when(mockBeamFnStateClient).handle(any(), any());    ProcessBundleHandler handler = new ProcessBundleHandler(PipelineOptionsFactory.create(), fnApiRegistry::get, beamFnDataClient, mockBeamFnStateGrpcClient, ImmutableMap.of(DATA_INPUT_URN, new PTransformRunnerFactory<Object>() {        @Override        public Object createRunnerForPTransform(PipelineOptions pipelineOptions, BeamFnDataClient beamFnDataClient, BeamFnStateClient beamFnStateClient, String pTransformId, PTransform pTransform, Supplier<String> processBundleInstructionId, Map<String, PCollection> pCollections, Map<String, Coder> coders, Map<String, WindowingStrategy> windowingStrategies, PCollectionConsumerRegistry pCollectionConsumerRegistry, PTransformFunctionRegistry startFunctionRegistry, PTransformFunctionRegistry finishFunctionRegistry, BundleSplitListener splitListener) throws IOException {            startFunctionRegistry.register(pTransformId, () -> doStateCalls(beamFnStateClient));            return null;        }        private void doStateCalls(BeamFnStateClient beamFnStateClient) {            beamFnStateClient.handle(StateRequest.newBuilder().setInstructionId("SUCCESS"), successfulResponse);            beamFnStateClient.handle(StateRequest.newBuilder().setInstructionId("FAIL"), unsuccessfulResponse);        }    }));    handler.processBundle(BeamFnApi.InstructionRequest.newBuilder().setProcessBundle(BeamFnApi.ProcessBundleRequest.newBuilder().setProcessBundleDescriptorId("1L")).build());    assertTrue(successfulResponse.isDone());    assertTrue(unsuccessfulResponse.isDone());}
public StreamObserver<BeamFnApi.Elements> beam_f27972_0(StreamObserver<BeamFnApi.Elements> outboundObserver)
{    outboundServerObserver.set(outboundObserver);    waitForClientToConnect.countDown();    return inboundServerObserver;}
public void beam_f27973_0() throws Exception
{    CountDownLatch waitForClientToConnect = new CountDownLatch(1);    AtomicInteger consumerInvoked = new AtomicInteger();    Collection<BeamFnApi.Elements> inboundServerValues = new ConcurrentLinkedQueue<>();    AtomicReference<StreamObserver<BeamFnApi.Elements>> outboundServerObserver = new AtomicReference<>();    CallStreamObserver<BeamFnApi.Elements> inboundServerObserver = TestStreams.withOnNext(inboundServerValues::add).build();    Endpoints.ApiServiceDescriptor apiServiceDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(this.getClass().getName() + "-" + UUID.randomUUID().toString()).build();    Server server = InProcessServerBuilder.forName(apiServiceDescriptor.getUrl()).addService(new BeamFnDataGrpc.BeamFnDataImplBase() {        @Override        public StreamObserver<BeamFnApi.Elements> data(StreamObserver<BeamFnApi.Elements> outboundObserver) {            outboundServerObserver.set(outboundObserver);            waitForClientToConnect.countDown();            return inboundServerObserver;        }    }).build();    server.start();    RuntimeException exceptionToThrow = new RuntimeException("TestFailure");    try {        ManagedChannel channel = InProcessChannelBuilder.forName(apiServiceDescriptor.getUrl()).build();        BeamFnDataGrpcClient clientFactory = new BeamFnDataGrpcClient(PipelineOptionsFactory.create(), (Endpoints.ApiServiceDescriptor descriptor) -> channel, OutboundObserverFactory.trivial());        InboundDataClient readFuture = clientFactory.receive(apiServiceDescriptor, ENDPOINT_A, CODER, t -> {            consumerInvoked.incrementAndGet();            throw exceptionToThrow;        });        waitForClientToConnect.await();                outboundServerObserver.get().onNext(ELEMENTS_A_1);        outboundServerObserver.get().onNext(ELEMENTS_A_2);        try {            readFuture.awaitCompletion();            fail("Expected channel to fail");        } catch (ExecutionException e) {            assertEquals(exceptionToThrow, e.getCause());        }                assertThat(inboundServerValues, empty());                assertEquals(1, consumerInvoked.get());    } finally {        server.shutdownNow();    }}
public void beam_f27982_0() throws Exception
{    mockStatic(MetricsEnvironment.class, withSettings().verboseLogging());    final String pCollectionA = "pCollectionA";    MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    FnDataReceiver<WindowedValue<String>> consumer = mock(FnDataReceiver.class, withSettings().verboseLogging());    ElementCountFnDataReceiver<String> wrapperConsumer = new ElementCountFnDataReceiver(consumer, pCollectionA, metricsContainerRegistry);    WindowedValue<String> element = WindowedValue.valueInGlobalWindow("elem");    wrapperConsumer.accept(element);    verify(consumer, times(1)).accept(element);        PowerMockito.verifyStatic(MetricsEnvironment.class, times(1));    MetricsEnvironment.scopedMetricsContainer(metricsContainerRegistry.getUnboundContainer());}
public void beam_f27983_0() throws Exception
{    List<String> consumer = new ArrayList<>();    FnDataReceiver<String> multiplexer = MultiplexingFnDataReceiver.forConsumers(ImmutableList.<FnDataReceiver<String>>of(consumer::add));    multiplexer.accept("foo");    multiplexer.accept("bar");    assertThat(consumer, contains("foo", "bar"));}
public void beam_f27992_1() throws Exception
{    CountDownLatch waitForClientToConnect = new CountDownLatch(1);    CountDownLatch receiveAllValuesA = new CountDownLatch(3);    CountDownLatch receiveAllValuesB = new CountDownLatch(2);    Collection<WindowedValue<String>> inboundValuesA = new ConcurrentLinkedQueue<>();    Collection<WindowedValue<String>> inboundValuesB = new ConcurrentLinkedQueue<>();    Collection<BeamFnApi.Elements> inboundServerValues = new ConcurrentLinkedQueue<>();    AtomicReference<StreamObserver<BeamFnApi.Elements>> outboundServerObserver = new AtomicReference<>();    CallStreamObserver<BeamFnApi.Elements> inboundServerObserver = TestStreams.withOnNext(inboundServerValues::add).build();    Endpoints.ApiServiceDescriptor apiServiceDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl(this.getClass().getName() + "-" + UUID.randomUUID().toString()).build();    Server server = InProcessServerBuilder.forName(apiServiceDescriptor.getUrl()).addService(new BeamFnDataGrpc.BeamFnDataImplBase() {        @Override        public StreamObserver<BeamFnApi.Elements> data(StreamObserver<BeamFnApi.Elements> outboundObserver) {            outboundServerObserver.set(outboundObserver);            waitForClientToConnect.countDown();            return inboundServerObserver;        }    }).build();    server.start();    try {        ManagedChannel channel = InProcessChannelBuilder.forName(apiServiceDescriptor.getUrl()).build();        BeamFnDataGrpcClient clientFactory = new BeamFnDataGrpcClient(PipelineOptionsFactory.create(), (Endpoints.ApiServiceDescriptor descriptor) -> channel, OutboundObserverFactory.trivial());        QueueingBeamFnDataClient queueingClient = new QueueingBeamFnDataClient(clientFactory);        InboundDataClient readFutureA = queueingClient.receive(apiServiceDescriptor, ENDPOINT_A, CODER, (WindowedValue<String> wv) -> {            inboundValuesA.add(wv);            receiveAllValuesA.countDown();        });        waitForClientToConnect.await();        Future<?> sendElementsFuture = executor.submit(() -> {            outboundServerObserver.get().onNext(ELEMENTS_A_1);                                    outboundServerObserver.get().onNext(ELEMENTS_B_1);        });                InboundDataClient readFutureB = queueingClient.receive(apiServiceDescriptor, ENDPOINT_B, CODER, (WindowedValue<String> wv) -> {            inboundValuesB.add(wv);            receiveAllValuesB.countDown();        });        Future<?> drainElementsFuture = executor.submit(() -> {            try {                queueingClient.drainAndBlock();            } catch (Exception e) {                                fail();            }        });        receiveAllValuesB.await();        assertThat(inboundValuesB, contains(valueInGlobalWindow("JKL"), valueInGlobalWindow("MNO")));        outboundServerObserver.get().onNext(ELEMENTS_A_2);                receiveAllValuesA.await();        assertThat(inboundValuesA, contains(valueInGlobalWindow("ABC"), valueInGlobalWindow("DEF"), valueInGlobalWindow("GHI")));                sendElementsFuture.get();        drainElementsFuture.get();    } finally {        server.shutdownNow();    }}
public StreamObserver<BeamFnApi.Elements> beam_f27993_0(StreamObserver<BeamFnApi.Elements> outboundObserver)
{    outboundServerObserver.set(outboundObserver);    waitForClientToConnect.countDown();    return inboundServerObserver;}
public void beam_f28002_0(ProcessContext context, @StateId("value") ValueState<String> valueState, @StateId("bag") BagState<String> bagState, @StateId("combine") CombiningState<String, String, String> combiningState)
{    context.output("value:" + valueState.read());    valueState.write(context.element().getValue());    context.output("bag:" + Iterables.toString(bagState.read()));    bagState.add(context.element().getValue());    context.output("combine:" + combiningState.read());    combiningState.add(context.element().getValue());}
public void beam_f28003_0() throws Exception
{    Pipeline p = Pipeline.create();    PCollection<KV<String, String>> valuePCollection = p.apply(Create.of(KV.of("unused", "unused")));    PCollection<String> outputPCollection = valuePCollection.apply(TEST_TRANSFORM_ID, ParDo.of(new TestStatefulDoFn()));    SdkComponents sdkComponents = SdkComponents.create(p.getOptions());    RunnerApi.Pipeline pProto = PipelineTranslation.toProto(p, sdkComponents);    String inputPCollectionId = sdkComponents.registerPCollection(valuePCollection);    String outputPCollectionId = sdkComponents.registerPCollection(outputPCollection);    RunnerApi.PTransform pTransform = pProto.getComponents().getTransformsOrThrow(pProto.getComponents().getTransformsOrThrow(TEST_TRANSFORM_ID).getSubtransforms(0));    FakeBeamFnStateClient fakeClient = new FakeBeamFnStateClient(ImmutableMap.of(bagUserStateKey("value", "X"), encode("X0"), bagUserStateKey("bag", "X"), encode("X0"), bagUserStateKey("combine", "X"), encode("X0")));    List<WindowedValue<String>> mainOutputValues = new ArrayList<>();    MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    consumers.register(outputPCollectionId, TEST_TRANSFORM_ID, (FnDataReceiver) (FnDataReceiver<WindowedValue<String>>) mainOutputValues::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(mock(MetricsContainerStepMap.class), mock(ExecutionStateTracker.class), "finish");    new FnApiDoFnRunner.Factory<>().createRunnerForPTransform(PipelineOptionsFactory.create(), null, /* beamFnDataClient */    fakeClient, TEST_TRANSFORM_ID, pTransform, Suppliers.ofInstance("57L")::get, pProto.getComponents().getPcollectionsMap(), pProto.getComponents().getCodersMap(), pProto.getComponents().getWindowingStrategiesMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);    Iterables.getOnlyElement(startFunctionRegistry.getFunctions()).run();    mainOutputValues.clear();    assertThat(consumers.keySet(), containsInAnyOrder(inputPCollectionId, outputPCollectionId));            FnDataReceiver<WindowedValue<?>> mainInput = consumers.getMultiplexingConsumer(inputPCollectionId);    mainInput.accept(valueInGlobalWindow(KV.of("X", "X1")));    mainInput.accept(valueInGlobalWindow(KV.of("Y", "Y1")));    mainInput.accept(valueInGlobalWindow(KV.of("X", "X2")));    mainInput.accept(valueInGlobalWindow(KV.of("Y", "Y2")));    assertThat(mainOutputValues, contains(valueInGlobalWindow("value:X0"), valueInGlobalWindow("bag:[X0]"), valueInGlobalWindow("combine:X0"), valueInGlobalWindow("value:null"), valueInGlobalWindow("bag:[]"), valueInGlobalWindow("combine:"), valueInGlobalWindow("value:X1"), valueInGlobalWindow("bag:[X0, X1]"), valueInGlobalWindow("combine:X0X1"), valueInGlobalWindow("value:Y1"), valueInGlobalWindow("bag:[Y1]"), valueInGlobalWindow("combine:Y1")));    mainOutputValues.clear();    Iterables.getOnlyElement(finishFunctionRegistry.getFunctions()).run();    assertThat(mainOutputValues, empty());    assertEquals(ImmutableMap.<StateKey, ByteString>builder().put(bagUserStateKey("value", "X"), encode("X2")).put(bagUserStateKey("bag", "X"), encode("X0", "X1", "X2")).put(bagUserStateKey("combine", "X"), encode("X0X1X2")).put(bagUserStateKey("value", "Y"), encode("Y2")).put(bagUserStateKey("bag", "Y"), encode("Y1", "Y2")).put(bagUserStateKey("combine", "Y"), encode("Y1Y2")).build(), fakeClient.getData());    mainOutputValues.clear();}
public void beam_f28012_0(OnTimerContext context, @StateId("bag") BagState<String> bagState, @TimerId("event") Timer eventTimeTimer, @TimerId("processing") Timer processingTimeTimer)
{    context.output("event" + Iterables.toString(bagState.read()));    bagState.add("event");    eventTimeTimer.set(context.timestamp().plus(11L));    processingTimeTimer.offset(Duration.millis(12L));    processingTimeTimer.setRelative();}
public void beam_f28013_0(OnTimerContext context, @StateId("bag") BagState<String> bagState, @TimerId("event") Timer eventTimeTimer, @TimerId("processing") Timer processingTimeTimer)
{    context.output("processing" + Iterables.toString(bagState.read()));    bagState.add("processing");    eventTimeTimer.set(context.timestamp().plus(21L));    processingTimeTimer.offset(Duration.millis(22L));    processingTimeTimer.setRelative();}
public void beam_f28022_0(PipelineOptions options)
{    beforeProcessingMock.accept(options);}
public void beam_f28023_0() throws Exception
{    Function<String, String> environmentVariableMock = mock(Function.class);    PipelineOptions options = PipelineOptionsFactory.create();    when(environmentVariableMock.apply("HARNESS_ID")).thenReturn("id");    when(environmentVariableMock.apply("PIPELINE_OPTIONS")).thenReturn(PipelineOptionsTranslation.toJson(options));    List<BeamFnApi.LogEntry> logEntries = new ArrayList<>();    List<BeamFnApi.InstructionResponse> instructionResponses = mock(List.class);    BeamFnLoggingGrpc.BeamFnLoggingImplBase loggingService = new BeamFnLoggingGrpc.BeamFnLoggingImplBase() {        @Override        public StreamObserver<BeamFnApi.LogEntry.List> logging(StreamObserver<LogControl> responseObserver) {            return TestStreams.withOnNext((BeamFnApi.LogEntry.List entries) -> logEntries.addAll(entries.getLogEntriesList())).withOnCompleted(responseObserver::onCompleted).build();        }    };    BeamFnControlGrpc.BeamFnControlImplBase controlService = new BeamFnControlGrpc.BeamFnControlImplBase() {        @Override        public StreamObserver<InstructionResponse> control(StreamObserver<InstructionRequest> responseObserver) {            CountDownLatch waitForResponses = new CountDownLatch(1);            options.as(GcsOptions.class).getExecutorService().submit(() -> {                responseObserver.onNext(INSTRUCTION_REQUEST);                Uninterruptibles.awaitUninterruptibly(waitForResponses);                responseObserver.onCompleted();            });            return TestStreams.withOnNext((InstructionResponse t) -> {                instructionResponses.add(t);                waitForResponses.countDown();            }).withOnCompleted(waitForResponses::countDown).build();        }    };    Server loggingServer = ServerBuilder.forPort(0).addService(loggingService).build();    loggingServer.start();    try {        Server controlServer = ServerBuilder.forPort(0).addService(controlService).build();        controlServer.start();        try {            Endpoints.ApiServiceDescriptor loggingDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl("localhost:" + loggingServer.getPort()).build();            Endpoints.ApiServiceDescriptor controlDescriptor = Endpoints.ApiServiceDescriptor.newBuilder().setUrl("localhost:" + controlServer.getPort()).build();            when(environmentVariableMock.apply("LOGGING_API_SERVICE_DESCRIPTOR")).thenReturn(TextFormat.printToString(loggingDescriptor));            when(environmentVariableMock.apply("CONTROL_API_SERVICE_DESCRIPTOR")).thenReturn(TextFormat.printToString(controlDescriptor));            FnHarness.main(environmentVariableMock);        } finally {            controlServer.shutdownNow();        }    } finally {        loggingServer.shutdownNow();    }            InOrder inOrder = inOrder(onStartupMock, beforeProcessingMock, environmentVariableMock, instructionResponses);    inOrder.verify(onStartupMock).run();    inOrder.verify(environmentVariableMock, atLeastOnce()).apply(any());    inOrder.verify(beforeProcessingMock).accept(any());    inOrder.verify(instructionResponses).add(INSTRUCTION_RESPONSE);}
public void beam_f28032_0() throws Exception
{    List<WindowedValue<?>> outputConsumer = new ArrayList<>();    MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    consumers.register("outputPC", EXPECTED_ID, outputConsumer::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class), "finish");    ValueMapFnFactory<String, String> factory = (ptId, pt) -> String::toUpperCase;    MapFnRunners.forValueMapFnFactory(factory).createRunnerForPTransform(PipelineOptionsFactory.create(), null, /* beamFnDataClient */    null, /* beamFnStateClient */    EXPECTED_ID, EXPECTED_PTRANSFORM, Suppliers.ofInstance("57L")::get, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);    assertThat(startFunctionRegistry.getFunctions(), empty());    assertThat(finishFunctionRegistry.getFunctions(), empty());    assertThat(consumers.keySet(), containsInAnyOrder("inputPC", "outputPC"));    consumers.getMultiplexingConsumer("inputPC").accept(valueInGlobalWindow("abc"));    assertThat(outputConsumer, contains(valueInGlobalWindow("ABC")));}
public void beam_f28033_0() throws Exception
{    List<WindowedValue<?>> outputConsumer = new ArrayList<>();    MetricsContainerStepMap metricsContainerRegistry = new MetricsContainerStepMap();    PCollectionConsumerRegistry consumers = new PCollectionConsumerRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class));    consumers.register("outputPC", EXPECTED_ID, outputConsumer::add);    PTransformFunctionRegistry startFunctionRegistry = new PTransformFunctionRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class), "start");    PTransformFunctionRegistry finishFunctionRegistry = new PTransformFunctionRegistry(metricsContainerRegistry, mock(ExecutionStateTracker.class), "finish");    MapFnRunners.forWindowedValueMapFnFactory(this::createMapFunctionForPTransform).createRunnerForPTransform(PipelineOptionsFactory.create(), null, /* beamFnDataClient */    null, /* beamFnStateClient */    EXPECTED_ID, EXPECTED_PTRANSFORM, Suppliers.ofInstance("57L")::get, Collections.emptyMap(), Collections.emptyMap(), Collections.emptyMap(), consumers, startFunctionRegistry, finishFunctionRegistry, null);    assertThat(startFunctionRegistry.getFunctions(), empty());    assertThat(finishFunctionRegistry.getFunctions(), empty());    assertThat(consumers.keySet(), containsInAnyOrder("inputPC", "outputPC"));    consumers.getMultiplexingConsumer("inputPC").accept(valueInGlobalWindow("abc"));    assertThat(outputConsumer, contains(valueInGlobalWindow("ABC")));}
public Long beam_f28042_0(Object key, Long accumulator)
{    return accumulator;}
public void beam_f28043_0() throws Exception
{    IdentitySizeEstimator underlying = new IdentitySizeEstimator();    SizeEstimator<Long> estimator = new SamplingSizeEstimator<>(underlying, 0.05, 1.0, 10, new Random(1));        for (int k = 0; k < 10; k++) {        assertEquals(100, estimator.estimateSize(100L));        assertEquals(k + 1, underlying.calls);    }        for (int k = 10; k < 20; k++) {        assertEquals(100, estimator.estimateSize(100L));    }    assertThat(underlying.calls, between(11, 19));    int initialCalls = underlying.calls;        for (int k = 20; k < 1020; k++) {        assertEquals(100, estimator.estimateSize(100L));    }    assertThat(underlying.calls - initialCalls, between(40, 60));}
public long beam_f28052_0(String element)
{    return (long) Math.pow(10, element.length());}
public Object beam_f28053_0(Object pair)
{    return ((KV<Object, ?>) pair).getKey();}
public StreamObserver<StateRequest> beam_f28062_0(StreamObserver<StateResponse> outboundObserver)
{    Uninterruptibles.putUninterruptibly(outboundServerObservers, outboundObserver);    return inboundServerObserver;}
public void beam_f28063_0() throws Exception
{    testServer.shutdownNow();    testChannel.shutdownNow();}
public void beam_f28072_0()
{    Iterable<Object> iterable = new LazyCachingIteratorToIterable<>(Iterators.forArray());    assertArrayEquals(new Object[0], Iterables.toArray(iterable, Object.class));        assertArrayEquals(new Object[0], Iterables.toArray(iterable, Object.class));    thrown.expect(NoSuchElementException.class);    iterable.iterator().next();}
public void beam_f28073_0()
{    Iterable<String> iterable = new LazyCachingIteratorToIterable<>(Iterators.forArray("A", "B", "C"));    Iterator<String> iterator1 = iterable.iterator();    assertTrue(iterator1.hasNext());    assertEquals("A", iterator1.next());    Iterator<String> iterator2 = iterable.iterator();    assertTrue(iterator2.hasNext());    assertEquals("A", iterator2.next());    assertTrue(iterator2.hasNext());    assertEquals("B", iterator2.next());    assertTrue(iterator1.hasNext());    assertEquals("B", iterator1.next());    assertTrue(iterator1.hasNext());    assertEquals("C", iterator1.next());    assertFalse(iterator1.hasNext());    assertTrue(iterator2.hasNext());    assertEquals("C", iterator2.next());    assertFalse(iterator2.hasNext());    thrown.expect(NoSuchElementException.class);    iterator1.next();}
public void beam_f28082_0() throws Exception
{    testFetch(ByteString.EMPTY, ByteString.copyFromUtf8("BC"), ByteString.EMPTY, ByteString.EMPTY, ByteString.copyFromUtf8("DEF"), ByteString.EMPTY);}
private void beam_f28083_0(ByteString... expected)
{    BeamFnStateClient fakeStateClient = (requestBuilder, response) -> {        ByteString continuationToken = requestBuilder.getGet().getContinuationToken();                int requestedPosition = 0;        if (!ByteString.EMPTY.equals(continuationToken)) {            requestedPosition = Integer.parseInt(continuationToken.toStringUtf8());        }                ByteString newContinuationToken = ByteString.EMPTY;        if (requestedPosition != expected.length - 1) {            newContinuationToken = ByteString.copyFromUtf8(Integer.toString(requestedPosition + 1));        }        response.complete(StateResponse.newBuilder().setId(requestBuilder.getId()).setGet(StateGetResponse.newBuilder().setData(expected[requestedPosition]).setContinuationToken(newContinuationToken)).build());    };    Iterator<ByteString> byteStrings = new LazyBlockingStateFetchingIterator(fakeStateClient, StateRequest.getDefaultInstance());    assertArrayEquals(expected, Iterators.toArray(byteStrings, Object.class));}
private static RunnerApi.PTransform beam_f28092_0(WindowFn<?, W> windowFn) throws Exception
{    SdkComponents components = SdkComponents.create();    components.registerEnvironment(Environments.createDockerEnvironment("test"));    RunnerApi.FunctionSpec functionSpec = RunnerApi.FunctionSpec.newBuilder().setUrn(WindowMergingFnRunner.URN).setPayload(WindowingStrategyTranslation.toProto(windowFn, components).toByteString()).build();    return RunnerApi.PTransform.newBuilder().setSpec(functionSpec).build();}
public static AttributeValueCoder beam_f28093_0()
{    return INSTANCE;}
public AmazonDynamoDB beam_f28102_0()
{    return AmazonDynamoDBClientBuilder.standard().withCredentials(getCredentialsProvider()).withRegion(region).build();}
public static Read<T> beam_f28103_0()
{    return new AutoValue_DynamoDBIO_Read.Builder().build();}
public Read<T> beam_f28112_0(Coder<T> coder)
{    checkArgument(coder != null, "coder can not be null");    return toBuilder().setCoder(coder).build();}
public PCollection<T> beam_f28113_0(PBegin input)
{    checkArgument((getScanRequestFn() != null), "withScanRequestFn() is required");    checkArgument((getAwsClientsProvider() != null), "withAwsClientsProvider() is required");    ScanRequest scanRequest = getScanRequestFn().apply(null);    checkArgument((scanRequest.getTotalSegments() != null && scanRequest.getTotalSegments() > 0), "TotalSegments is required with withScanRequestFn() and greater zero");    PCollection<Read<T>> splits = (PCollection<Read<T>>) input.apply("Create", Create.of(this)).apply("Split", ParDo.of(new SplitFn()));    splits.setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {    }));    PCollection<T> output = (PCollection<T>) splits.apply("Reshuffle", Reshuffle.viaRandomKey()).apply("Read", ParDo.of(new ReadFn()));    output.setCoder(getCoder());    return output;}
public Write<T> beam_f28122_0(RetryConfiguration retryConfiguration)
{    checkArgument(retryConfiguration != null, "retryConfiguration is required");    return builder().setRetryConfiguration(retryConfiguration).build();}
public Write<T> beam_f28123_0(SerializableFunction<T, KV<String, WriteRequest>> writeItemMapperFn)
{    return builder().setWriteItemMapperFn(writeItemMapperFn).build();}
public AWSCredentialsProvider beam_f28132_0(JsonParser jsonParser, DeserializationContext context, TypeDeserializer typeDeserializer) throws IOException
{    Map<String, String> asMap = jsonParser.readValueAs(new TypeReference<Map<String, String>>() {    });    String typeNameKey = typeDeserializer.getPropertyName();    String typeName = asMap.get(typeNameKey);    if (typeName == null) {        throw new IOException(String.format("AWS credentials provider type name key '%s' not found", typeNameKey));    }    if (typeName.equals(AWSStaticCredentialsProvider.class.getSimpleName())) {        return new AWSStaticCredentialsProvider(new BasicAWSCredentials(asMap.get(AWS_ACCESS_KEY_ID), asMap.get(AWS_SECRET_KEY)));    } else if (typeName.equals(PropertiesFileCredentialsProvider.class.getSimpleName())) {        return new PropertiesFileCredentialsProvider(asMap.get(CREDENTIALS_FILE_PATH));    } else if (typeName.equals(ClasspathPropertiesFileCredentialsProvider.class.getSimpleName())) {        return new ClasspathPropertiesFileCredentialsProvider(asMap.get(CREDENTIALS_FILE_PATH));    } else if (typeName.equals(DefaultAWSCredentialsProviderChain.class.getSimpleName())) {        return new DefaultAWSCredentialsProviderChain();    } else if (typeName.equals(EnvironmentVariableCredentialsProvider.class.getSimpleName())) {        return new EnvironmentVariableCredentialsProvider();    } else if (typeName.equals(SystemPropertiesCredentialsProvider.class.getSimpleName())) {        return new SystemPropertiesCredentialsProvider();    } else if (typeName.equals(ProfileCredentialsProvider.class.getSimpleName())) {        return new ProfileCredentialsProvider();    } else if (typeName.equals(EC2ContainerCredentialsProviderWrapper.class.getSimpleName())) {        return new EC2ContainerCredentialsProviderWrapper();    } else {        throw new IOException(String.format("AWS credential provider type '%s' is not supported", typeName));    }}
public void beam_f28133_0(AWSCredentialsProvider credentialsProvider, JsonGenerator jsonGenerator, SerializerProvider serializers) throws IOException
{    serializers.defaultSerializeValue(credentialsProvider, jsonGenerator);}
protected String beam_f28142_0()
{    return S3ResourceId.SCHEME;}
 void beam_f28143_0(AmazonS3 amazonS3)
{    this.amazonS3 = Suppliers.ofInstance(amazonS3);}
private PathWithEncoding beam_f28152_0(S3ResourceId path)
{    ObjectMetadata s3Metadata;    try {        s3Metadata = getObjectMetadata(path);    } catch (AmazonClientException e) {        if (e instanceof AmazonS3Exception && ((AmazonS3Exception) e).getStatusCode() == 404) {            return PathWithEncoding.create(path, new FileNotFoundException());        }        return PathWithEncoding.create(path, new IOException(e));    }    return PathWithEncoding.create(path, Strings.nullToEmpty(s3Metadata.getContentEncoding()));}
private List<MatchResult> beam_f28153_0(Collection<S3ResourceId> paths) throws IOException
{    List<Callable<MatchResult>> tasks = new ArrayList<>(paths.size());    for (final S3ResourceId path : paths) {        tasks.add(() -> matchNonGlobPath(path));    }    return callTasks(tasks);}
 void beam_f28162_0(S3ResourceId sourcePath, S3ResourceId destinationPath) throws IOException
{    try {        ObjectMetadata sourceObjectMetadata = getObjectMetadata(sourcePath);        if (sourceObjectMetadata.getContentLength() < MAX_COPY_OBJECT_SIZE_BYTES) {            atomicCopy(sourcePath, destinationPath, sourceObjectMetadata);        } else {            multipartCopy(sourcePath, destinationPath, sourceObjectMetadata);        }    } catch (AmazonClientException e) {        throw new IOException(e);    }}
 CopyObjectResult beam_f28163_0(S3ResourceId sourcePath, S3ResourceId destinationPath, ObjectMetadata sourceObjectMetadata) throws AmazonClientException
{    CopyObjectRequest copyObjectRequest = new CopyObjectRequest(sourcePath.getBucket(), sourcePath.getKey(), destinationPath.getBucket(), destinationPath.getKey());    copyObjectRequest.setNewObjectMetadata(sourceObjectMetadata);    copyObjectRequest.setStorageClass(options.getS3StorageClass());    copyObjectRequest.setSourceSSECustomerKey(options.getSSECustomerKey());    copyObjectRequest.setDestinationSSECustomerKey(options.getSSECustomerKey());    return amazonS3.get().copyObject(copyObjectRequest);}
public long beam_f28172_0() throws ClosedChannelException
{    if (!isOpen()) {        throw new ClosedChannelException();    }    return position;}
public SeekableByteChannel beam_f28173_0(long newPosition) throws IOException
{    if (!isOpen()) {        throw new ClosedChannelException();    }    checkArgument(newPosition >= 0, "newPosition too low");    checkArgument(newPosition < contentLength, "new position too high");    if (newPosition == position) {        return this;    }        if (s3Object != null) {        s3Object.close();        s3Object = null;    }    position = newPosition;    return this;}
 String beam_f28182_0()
{        return key.substring(1);}
 Optional<Long> beam_f28183_0()
{    return Optional.fromNullable(size);}
 boolean beam_f28192_0()
{    return GLOB_PREFIX.matcher(getKey()).matches();}
 String beam_f28193_0()
{    Matcher m = GLOB_PREFIX.matcher(getKey());    checkArgument(m.matches(), String.format("Glob expression: [%s] is not expandable.", getKey()));    return m.group("PREFIX");}
 static boolean beam_f28202_0(boolean... values)
{    boolean one = false;    for (boolean value : values) {        if (!one && value) {            one = true;        } else if (value) {            return false;        }    }    return true;}
private AWSCredentialsProvider beam_f28203_0()
{    return new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey));}
public static Write beam_f28212_0()
{    return new AutoValue_SnsIO_Write.Builder().build();}
public static RetryConfiguration beam_f28213_0(int maxAttempts, Duration maxDuration)
{    checkArgument(maxAttempts > 0, "maxAttempts should be greater than 0");    checkArgument(maxDuration != null && maxDuration.isLongerThan(Duration.ZERO), "maxDuration should be greater than 0");    return new AutoValue_SnsIO_RetryConfiguration.Builder().setMaxAttempts(maxAttempts).setMaxDuration(maxDuration).setRetryPredicate(DEFAULT_RETRY_PREDICATE).build();}
public void beam_f28222_0() throws Exception
{        producer = spec.getAWSClientsProvider().createSnsPublisher();    checkArgument(topicExists(producer, spec.getTopicName()), "Topic %s does not exist", spec.getTopicName());    retryBackoff = FluentBackoff.DEFAULT.withMaxRetries(    0).withInitialBackoff(RETRY_INITIAL_BACKOFF);    if (spec.getRetryConfiguration() != null) {        retryBackoff = retryBackoff.withMaxRetries(spec.getRetryConfiguration().getMaxAttempts() - 1).withMaxCumulativeBackoff(spec.getRetryConfiguration().getMaxDuration());    }}
public void beam_f28223_1(ProcessContext context) throws Exception
{    PublishRequest request = context.element();    Sleeper sleeper = Sleeper.DEFAULT;    BackOff backoff = retryBackoff.backoff();    int attempt = 0;    while (true) {        attempt++;        try {            PublishResult pr = producer.publish(request);            context.output(pr);            break;        } catch (Exception ex) {                        if (spec.getRetryConfiguration() == null || !spec.getRetryConfiguration().getRetryPredicate().test(ex)) {                SNS_WRITE_FAILURES.inc();                                throw new IOException("Error writing to SNS (no attempt made to retry)", ex);            }            if (!BackOffUtils.next(sleeper, backoff)) {                throw new IOException(String.format("Error writing to SNS after %d attempt(s). No more attempts allowed", attempt), ex);            } else {                                            }        }    }}
public String beam_f28232_0()
{    return awsRegion;}
public static Read beam_f28233_0()
{    return new AutoValue_SqsIO_Read.Builder().setMaxNumRecords(Long.MAX_VALUE).build();}
public void beam_f28242_0() throws Exception
{    if (sqs != null) {        sqs.shutdown();    }}
public Instant beam_f28243_0()
{    return oldestPendingTimestamp;}
private void beam_f28253_0()
{    final ReceiveMessageRequest receiveMessageRequest = new ReceiveMessageRequest(source.getRead().queueUrl());    receiveMessageRequest.setMaxNumberOfMessages(MAX_NUMBER_OF_MESSAGES);    receiveMessageRequest.setAttributeNames(Arrays.asList(MessageSystemAttributeName.SentTimestamp.toString()));    final ReceiveMessageResult receiveMessageResult = source.getSqs().receiveMessage(receiveMessageRequest);    final List<Message> messages = receiveMessageResult.getMessages();    if (messages == null || messages.isEmpty()) {        return;    }    for (Message message : messages) {        messagesNotYetRead.add(message);    }}
private Instant beam_f28254_0(final Message message)
{    return new Instant(Long.parseLong(message.getAttributes().get(MessageSystemAttributeName.SentTimestamp.toString())));}
public void beam_f28263_0() throws IOException
{    AttributeValue expected = new AttributeValue();    expected.setN("123");    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public void beam_f28264_0() throws IOException
{    AttributeValue expected = new AttributeValue();    expected.setBOOL(false);    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public AmazonCloudWatch beam_f28273_0()
{    return Mockito.mock(AmazonCloudWatch.class);}
public AmazonDynamoDB beam_f28274_0()
{    return db;}
public void beam_f28283_0() throws Throwable
{    thrown.expectMessage("Error writing to DynamoDB");    final List<WriteRequest> writeRequests = DynamoDBIOTestHelper.generateWriteRequests(numOfItems);    AmazonDynamoDB amazonDynamoDBMock = Mockito.mock(AmazonDynamoDB.class);    Mockito.when(amazonDynamoDBMock.batchWriteItem(Mockito.any(BatchWriteItemRequest.class))).thenThrow(new AmazonDynamoDBException("Service unavailable"));    pipeline.apply(Create.of(writeRequests)).apply(DynamoDBIO.<WriteRequest>write().withWriteRequestMapperFn((SerializableFunction<WriteRequest, KV<String, WriteRequest>>) writeRequest -> KV.of(tableName, writeRequest)).withRetryConfiguration(DynamoDBIO.RetryConfiguration.create(4, Duration.standardSeconds(10))).withAwsClientsProvider(AwsClientsProviderMock.of(amazonDynamoDBMock)));    try {        pipeline.run().waitUntilFinish();    } catch (final Pipeline.PipelineExecutionException e) {                expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 1));        expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 2));        expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 3));        throw e.getCause();    }    fail("Pipeline is expected to fail because we were unable to write to DynamoDB.");}
 static void beam_f28284_0()
{    localStackContainer.start();    if (dynamoDBClient == null) {        dynamoDBClient = AmazonDynamoDBClientBuilder.standard().withEndpointConfiguration(localStackContainer.getEndpointConfiguration(LocalStackContainer.Service.DYNAMODB)).withCredentials(localStackContainer.getDefaultCredentialsProvider()).build();    }}
public void beam_f28293_0()
{    List<Module> modules = ObjectMapper.findModules(ReflectHelpers.findClassLoader());    assertThat(modules, hasItem(Matchers.instanceOf(AwsModule.class)));}
public void beam_f28294_0() throws Exception
{    String awsKeyId = "key-id";    String awsSecretKey = "secret-key";    AWSStaticCredentialsProvider credentialsProvider = new AWSStaticCredentialsProvider(new BasicAWSCredentials(awsKeyId, awsSecretKey));    String serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);    AWSCredentialsProvider deserializedCredentialsProvider = objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());    assertEquals(credentialsProvider.getCredentials().getAWSAccessKeyId(), deserializedCredentialsProvider.getCredentials().getAWSAccessKeyId());    assertEquals(credentialsProvider.getCredentials().getAWSSecretKey(), deserializedCredentialsProvider.getCredentials().getAWSSecretKey());}
 static MatchResultMatcher beam_f28303_0(long sizeBytes, long lastModifiedMillis, ResourceId resourceId, boolean isReadSeekEfficient)
{    return create(MatchResult.Metadata.builder().setSizeBytes(sizeBytes).setLastModifiedMillis(lastModifiedMillis).setResourceId(resourceId).setIsReadSeekEfficient(isReadSeekEfficient).build());}
 static MatchResultMatcher beam_f28304_0(MatchResult.Status expectedStatus, IOException expectedException)
{    return new MatchResultMatcher(expectedStatus, null, expectedException);}
public void beam_f28313_0() throws IOException
{    testCopy(s3Options());    testCopy(s3OptionsWithSSECustomerKey());}
private GetObjectMetadataRequest beam_f28314_0(S3ResourceId path, S3Options options)
{    GetObjectMetadataRequest getObjectMetadataRequest = new GetObjectMetadataRequest(path.getBucket(), path.getKey());    getObjectMetadataRequest.setSSECustomerKey(options.getSSECustomerKey());    return getObjectMetadataRequest;}
public void beam_f28323_0()
{    S3FileSystem s3FileSystem = buildMockedS3FileSystem(s3Options());    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/filethatexists");    long lastModifiedMillis = 1540000000000L;    ObjectMetadata s3ObjectMetadata = new ObjectMetadata();    s3ObjectMetadata.setContentLength(100);    s3ObjectMetadata.setLastModified(new Date(lastModifiedMillis));    s3ObjectMetadata.setContentEncoding("gzip");    when(s3FileSystem.getAmazonS3Client().getObjectMetadata(argThat(new GetObjectMetadataRequestMatcher(new GetObjectMetadataRequest(path.getBucket(), path.getKey()))))).thenReturn(s3ObjectMetadata);    MatchResult result = s3FileSystem.matchNonGlobPath(path);    assertThat(result, MatchResultMatcher.create(ImmutableList.of(MatchResult.Metadata.builder().setSizeBytes(100).setLastModifiedMillis(lastModifiedMillis).setResourceId(path).setIsReadSeekEfficient(false).build())));}
public void beam_f28324_0()
{    S3FileSystem s3FileSystem = buildMockedS3FileSystem(s3Options());    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/filethatexists");    long lastModifiedMillis = 1540000000000L;    ObjectMetadata s3ObjectMetadata = new ObjectMetadata();    s3ObjectMetadata.setContentLength(100);    s3ObjectMetadata.setLastModified(new Date(lastModifiedMillis));    s3ObjectMetadata.setContentEncoding(null);    when(s3FileSystem.getAmazonS3Client().getObjectMetadata(argThat(new GetObjectMetadataRequestMatcher(new GetObjectMetadataRequest(path.getBucket(), path.getKey()))))).thenReturn(s3ObjectMetadata);    MatchResult result = s3FileSystem.matchNonGlobPath(path);    assertThat(result, MatchResultMatcher.create(ImmutableList.of(MatchResult.Metadata.builder().setSizeBytes(100).setLastModifiedMillis(lastModifiedMillis).setResourceId(path).setIsReadSeekEfficient(true).build())));}
public void beam_f28333_0()
{    for (TestCase testCase : PATH_TEST_CASES) {        ResourceId resourceId = S3ResourceId.fromUri(testCase.baseUri);        ResourceId resolved = resourceId.resolve(testCase.relativePath, testCase.resolveOptions);        assertEquals(testCase.expectedResult, resolved.toString());    }        assertEquals(S3ResourceId.fromUri("s3://bucket/tmp/aa"), S3ResourceId.fromUri("s3://bucket/tmp/").resolve("aa", RESOLVE_FILE));    assertEquals(S3ResourceId.fromUri("s3://bucket/tmp/aa/bb/cc/"), S3ResourceId.fromUri("s3://bucket/tmp/").resolve("aa", RESOLVE_DIRECTORY).resolve("bb", RESOLVE_DIRECTORY).resolve("cc", RESOLVE_DIRECTORY));        assertEquals(S3ResourceId.fromUri("s3://bucket/tmp/aa"), S3ResourceId.fromUri("s3://bucket/tmp/bb/").resolve("s3://bucket/tmp/aa", RESOLVE_FILE));        assertEquals(S3ResourceId.fromUri("s3://my-bucket/tmp"), S3ResourceId.fromUri("s3://my-bucket").resolve("tmp", RESOLVE_FILE));        assertEquals(S3ResourceId.fromUri("s3://bucket/输出 目录/输出 文件01.txt"), S3ResourceId.fromUri("s3://bucket/输出 目录/").resolve("输出 文件01.txt", RESOLVE_FILE));}
public void beam_f28334_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Cannot resolve a file with a directory path: [tmp/]");    S3ResourceId.fromUri("s3://my_bucket/").resolve("tmp/", RESOLVE_FILE);}
public void beam_f28343_0()
{    assertNull(S3ResourceId.fromUri("s3://my_bucket/").getFilename());    assertEquals("abc", S3ResourceId.fromUri("s3://my_bucket/abc").getFilename());    assertEquals("abc", S3ResourceId.fromUri("s3://my_bucket/abc/").getFilename());    assertEquals("def", S3ResourceId.fromUri("s3://my_bucket/abc/def").getFilename());    assertEquals("def", S3ResourceId.fromUri("s3://my_bucket/abc/def/").getFilename());    assertEquals("xyz.txt", S3ResourceId.fromUri("s3://my_bucket/abc/xyz.txt").getFilename());}
public void beam_f28344_0()
{    S3ResourceId path = S3ResourceId.fromUri("s3://bucket/dir/subdir/object");    assertEquals("bucket", path.getBucket());    assertEquals("dir/subdir/object", path.getKey());        path = S3ResourceId.fromUri("s3://bucket/dir/subdir/");    S3ResourceId parent = (S3ResourceId) path.resolve("..", RESOLVE_DIRECTORY);    assertEquals("bucket", parent.getBucket());    assertEquals("dir/", parent.getKey());    assertNotEquals(path, parent);    assertTrue(path.getKey().startsWith(parent.getKey()));    assertFalse(parent.getKey().startsWith(path.getKey()));        S3ResourceId grandParent = (S3ResourceId) parent.resolve("..", RESOLVE_DIRECTORY);    assertEquals("bucket", grandParent.getBucket());    assertEquals("", grandParent.getKey());}
 static S3Options beam_f28353_0()
{    S3Options options = s3Options();    options.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);    return options;}
 static S3Options beam_f28354_0()
{    S3Options options = s3Options();    options.setSSECustomerKey(new SSECustomerKey("86glyTlCNZgccSxW8JxMa6ZdjdK3N141glAysPUZ3AA="));    return options;}
public void beam_f28363_0()
{    assertTrue(atMostOne(true));    assertTrue(atMostOne(false));    assertFalse(atMostOne(true, true));    assertTrue(atMostOne(true, false));    assertTrue(atMostOne(false, true));    assertTrue(atMostOne(false, false));    assertFalse(atMostOne(true, true, true));    assertFalse(atMostOne(true, true, false));    assertFalse(atMostOne(true, false, true));    assertTrue(atMostOne(true, false, false));    assertFalse(atMostOne(false, true, true));    assertTrue(atMostOne(false, true, false));    assertTrue(atMostOne(false, false, true));    assertTrue(atMostOne(false, false, false));}
public AddPermissionResult beam_f28366_0(AddPermissionRequest addPermissionRequest)
{    throw new RuntimeException("Not implemented");}
public CreateTopicResult beam_f28375_0(String name)
{    return new CreateTopicResult().withTopicArn(name);}
public DeleteEndpointResult beam_f28376_0(DeleteEndpointRequest deleteEndpointRequest)
{    throw new RuntimeException("Not implemented");}
public GetTopicAttributesResult beam_f28385_0(GetTopicAttributesRequest getTopicAttributesRequest)
{    throw new RuntimeException("Not implemented");}
public GetTopicAttributesResult beam_f28386_0(String topicArn)
{    GetTopicAttributesResult result = Mockito.mock(GetTopicAttributesResult.class);    SdkHttpMetadata metadata = Mockito.mock(SdkHttpMetadata.class);    Mockito.when(metadata.getHttpHeaders()).thenReturn(new HashMap<>());    Mockito.when(metadata.getHttpStatusCode()).thenReturn(200);    Mockito.when(result.getSdkHttpMetadata()).thenReturn(metadata);    return result;}
public ListSubscriptionsByTopicResult beam_f28395_0(String topicArn)
{    throw new RuntimeException("Not implemented");}
public ListSubscriptionsByTopicResult beam_f28396_0(String topicArn, String nextToken)
{    throw new RuntimeException("Not implemented");}
public SetEndpointAttributesResult beam_f28405_0(SetEndpointAttributesRequest setEndpointAttributesRequest)
{    throw new RuntimeException("Not implemented");}
public SetPlatformApplicationAttributesResult beam_f28406_0(SetPlatformApplicationAttributesRequest setPlatformApplicationAttributesRequest)
{    throw new RuntimeException("Not implemented");}
public UnsubscribeResult beam_f28415_0(String subscriptionArn)
{    throw new RuntimeException("Not implemented");}
public ResponseMetadata beam_f28417_0(AmazonWebServiceRequest request)
{    throw new RuntimeException("Not implemented");}
public void beam_f28426_0()
{    final AmazonSQS client = embeddedSqsRestServer.getClient();    final String queueUrl = embeddedSqsRestServer.getQueueUrl();    List<SendMessageRequest> messages = new ArrayList<>();    for (int i = 0; i < 100; i++) {        final SendMessageRequest request = new SendMessageRequest(queueUrl, "This is a test " + i);        messages.add(request);    }    pipeline.apply(Create.of(messages)).apply(SqsIO.write());    pipeline.run().waitUntilFinish();    List<String> received = new ArrayList<>();    while (received.size() < 100) {        final ReceiveMessageResult receiveMessageResult = client.receiveMessage(queueUrl);        if (receiveMessageResult.getMessages() != null) {            for (Message message : receiveMessageResult.getMessages()) {                received.add(message.getBody());            }        }    }    assertEquals(100, received.size());    for (int i = 0; i < 100; i++) {        received.contains("This is a test " + i);    }}
protected void beam_f28427_0()
{    sqsRestServer = SQSRestServerBuilder.start();    String endpoint = "http://localhost:9324";    String region = "elasticmq";    String accessKey = "x";    String secretKey = "x";    client = AmazonSQSClientBuilder.standard().withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(accessKey, secretKey))).withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endpoint, region)).build();    final CreateQueueResult queue = client.createQueue("test");    queueUrl = queue.getQueueUrl();}
public DynamoDbClient beam_f28436_0()
{    DynamoDbClientBuilder builder = DynamoDbClient.builder().credentialsProvider(awsCredentialsProvider).region(Region.of(region));    if (serviceEndpoint != null) {        builder.endpointOverride(serviceEndpoint);    }    return builder.build();}
public static Read<T> beam_f28437_0()
{    return new AutoValue_DynamoDBIO_Read.Builder().build();}
public Read<T> beam_f28446_0(Coder<T> coder)
{    checkArgument(coder != null, "coder can not be null");    return toBuilder().setCoder(coder).build();}
public PCollection<T> beam_f28447_0(PBegin input)
{    checkArgument((getScanRequestFn() != null), "withScanRequestFn() is required");    checkArgument((getDynamoDbClientProvider() != null), "withDynamoDbClientProvider() is required");    ScanRequest scanRequest = getScanRequestFn().apply(null);    checkArgument((scanRequest.totalSegments() != null && scanRequest.totalSegments() > 0), "TotalSegments is required with withScanRequestFn() and greater zero");    PCollection<Read<T>> splits = (PCollection<Read<T>>) input.apply("Create", Create.of(this)).apply("Split", ParDo.of(new SplitFn()));    splits.setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {    }));    PCollection<T> output = (PCollection<T>) splits.apply("Reshuffle", Reshuffle.viaRandomKey()).apply("Read", ParDo.of(new ReadFn()));    output.setCoder(getCoder());    return output;}
public Write<T> beam_f28456_0(RetryConfiguration retryConfiguration)
{    checkArgument(retryConfiguration != null, "retryConfiguration is required");    return toBuilder().setRetryConfiguration(retryConfiguration).build();}
public Write<T> beam_f28457_0(SerializableFunction<T, KV<String, WriteRequest>> writeItemMapperFn)
{    return toBuilder().setWriteItemMapperFn(writeItemMapperFn).build();}
public AwsCredentialsProvider beam_f28466_0(JsonParser jsonParser, DeserializationContext context, TypeDeserializer typeDeserializer) throws IOException
{    Map<String, String> asMap = jsonParser.readValueAs(new TypeReference<Map<String, String>>() {    });    String typeNameKey = typeDeserializer.getPropertyName();    String typeName = asMap.get(typeNameKey);    if (typeName == null) {        throw new IOException(String.format("AWS credentials provider type name key '%s' not found", typeNameKey));    }    if (typeName.equals(StaticCredentialsProvider.class.getSimpleName())) {        return StaticCredentialsProvider.create(AwsBasicCredentials.create(asMap.get(ACCESS_KEY_ID), asMap.get(SECRET_ACCESS_KEY)));    } else if (typeName.equals(DefaultCredentialsProvider.class.getSimpleName())) {        return DefaultCredentialsProvider.create();    } else if (typeName.equals(EnvironmentVariableCredentialsProvider.class.getSimpleName())) {        return EnvironmentVariableCredentialsProvider.create();    } else if (typeName.equals(SystemPropertyCredentialsProvider.class.getSimpleName())) {        return SystemPropertyCredentialsProvider.create();    } else if (typeName.equals(ProfileCredentialsProvider.class.getSimpleName())) {        return ProfileCredentialsProvider.create();    } else if (typeName.equals(ContainerCredentialsProvider.class.getSimpleName())) {        return ContainerCredentialsProvider.builder().build();    } else {        throw new IOException(String.format("AWS credential provider type '%s' is not supported", typeName));    }}
public void beam_f28467_0(AwsCredentialsProvider credentialsProvider, JsonGenerator jsonGenerator, SerializerProvider serializer) throws IOException
{    serializer.defaultSerializeValue(credentialsProvider, jsonGenerator);}
public void beam_f28476_0() throws IOException
{    AttributeValue expected = AttributeValue.builder().b(SdkBytes.fromByteArray("hello".getBytes(StandardCharsets.UTF_8))).build();    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public void beam_f28477_0() throws IOException
{    AttributeValue expected = AttributeValue.builder().ss(ImmutableList.of("foo", "bar")).build();    AttributeValueCoder coder = AttributeValueCoder.of();    ByteArrayOutputStream output = new ByteArrayOutputStream();    coder.encode(expected, output);    ByteArrayInputStream in = new ByteArrayInputStream(output.toByteArray());    AttributeValue actual = coder.decode(in);    Assert.assertEquals(expected, actual);}
public static void beam_f28486_0()
{    DynamoDBIOTestHelper.stopServerClient(tableName);}
public void beam_f28487_0()
{    DynamoDBIOTestHelper.createTestTable(tableName);}
public void beam_f28496_0()
{    List<KV<String, Integer>> items = ImmutableList.of(KV.of("test1", 111), KV.of("test2", 222), KV.of("test3", 333));    final PCollection<Void> output = pipeline.apply(Create.of(items)).apply(DynamoDBIO.<KV<String, Integer>>write().withWriteRequestMapperFn((SerializableFunction<KV<String, Integer>, KV<String, WriteRequest>>) entry -> {        Map<String, AttributeValue> putRequest = ImmutableMap.of("hashKey1", AttributeValue.builder().s(entry.getKey()).build(), "rangeKey2", AttributeValue.builder().n(entry.getValue().toString()).build());        WriteRequest writeRequest = WriteRequest.builder().putRequest(PutRequest.builder().item(putRequest).build()).build();        return KV.of(tableName, writeRequest);    }).withRetryConfiguration(DynamoDBIO.RetryConfiguration.builder().setMaxAttempts(5).setMaxDuration(Duration.standardMinutes(1)).setRetryPredicate(DEFAULT_RETRY_PREDICATE).build()).withDynamoDbClientProvider(DynamoDbClientProviderMock.of(DynamoDBIOTestHelper.getDynamoDBClient())));    final PCollection<Long> publishedResultsSize = output.apply(Count.globally());    PAssert.that(publishedResultsSize).containsInAnyOrder(0L);    pipeline.run().waitUntilFinish();        int actualItemCount = DynamoDBIOTestHelper.readDataFromTable(tableName).size();    assertEquals(3, actualItemCount);}
public void beam_f28497_0() throws Throwable
{    thrown.expectMessage("Error writing to DynamoDB");    List<KV<String, Integer>> items = ImmutableList.of(KV.of("test1", 111), KV.of("test2", 222), KV.of("test3", 333));    DynamoDbClient amazonDynamoDBMock = Mockito.mock(DynamoDbClient.class);    Mockito.when(amazonDynamoDBMock.batchWriteItem(Mockito.any(BatchWriteItemRequest.class))).thenThrow(DynamoDbException.builder().message("Service unavailable").build());    pipeline.apply(Create.of(items)).apply(DynamoDBIO.<KV<String, Integer>>write().withWriteRequestMapperFn((SerializableFunction<KV<String, Integer>, KV<String, WriteRequest>>) entry -> {        Map<String, AttributeValue> putRequest = ImmutableMap.of("hashKey1", AttributeValue.builder().s(entry.getKey()).build(), "rangeKey2", AttributeValue.builder().n(entry.getValue().toString()).build());        WriteRequest writeRequest = WriteRequest.builder().putRequest(PutRequest.builder().item(putRequest).build()).build();        return KV.of(tableName, writeRequest);    }).withRetryConfiguration(DynamoDBIO.RetryConfiguration.builder().setMaxAttempts(4).setMaxDuration(Duration.standardSeconds(10)).setRetryPredicate(DEFAULT_RETRY_PREDICATE).build()).withDynamoDbClientProvider(DynamoDbClientProviderMock.of(amazonDynamoDBMock)));    try {        pipeline.run().waitUntilFinish();    } catch (final Pipeline.PipelineExecutionException e) {                expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 1));        expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 2));        expectedLogs.verifyWarn(String.format(DynamoDBIO.Write.WriteFn.RETRY_ATTEMPT_LOG, 3));        throw e.getCause();    }    fail("Pipeline is expected to fail because we were unable to write to DynamoDb.");}
 static void beam_f28506_0(String tableName)
{    DeleteTableRequest request = DeleteTableRequest.builder().tableName(tableName).build();    dynamoDBClient.deleteTable(request);}
 static void beam_f28507_0(String tableName)
{    CreateTableResponse res = createDynamoTable(tableName);    TableDescription tableDesc = res.tableDescription();    Assert.assertEquals(tableName, tableDesc.tableName());    Assert.assertTrue(tableDesc.keySchema().toString().contains(ATTR_NAME_1));    Assert.assertTrue(tableDesc.keySchema().toString().contains(ATTR_NAME_2));    Assert.assertEquals(tableDesc.provisionedThroughput().readCapacityUnits(), Long.valueOf(1000));    Assert.assertEquals(tableDesc.provisionedThroughput().writeCapacityUnits(), Long.valueOf(1000));    Assert.assertEquals(TableStatus.ACTIVE, tableDesc.tableStatus());    Assert.assertEquals("arn:aws:dynamodb:ddblocal:000000000000:table/" + tableName, tableDesc.tableArn());    ListTablesResponse tables = dynamoDBClient.listTables();    Assert.assertEquals(1, tables.tableNames().size());}
public Read beam_f28516_0(List<String> addresses)
{    checkArgument(addresses != null, "addresses can not be null");    checkArgument(!addresses.isEmpty(), "addresses can not be empty");    return builder().setAddresses(addresses).build();}
public Read beam_f28517_0(long maxNumRecords)
{    return builder().setMaxNumRecords(maxNumRecords).build();}
public Coder<AmqpCheckpointMark> beam_f28526_0()
{    return SerializableCoder.of(AmqpCheckpointMark.class);}
public Instant beam_f28527_0()
{    return watermark;}
public void beam_f28536_0() throws Exception
{    messenger = Messenger.Factory.create();    messenger.start();}
public void beam_f28537_0(ProcessContext processContext) throws Exception
{    Message message = processContext.element();    messenger.put(message);    messenger.send();}
protected void beam_f28546_0()
{    try {        getBrokerService().addConnector("amqp://localhost:0");    } catch (Exception e) {        throw new RuntimeException(e);    }}
public String beam_f28547_0(String queueName)
{    return getBrokerService().getDefaultSocketURIString() + "/" + queueName;}
private void beam_f28556_0(PipelineResult pipelineResult, String writeTimeMetricName)
{    NamedTestResult metricResult = getMetricSupplier(writeTimeMetricName).apply(new MetricsReader(pipelineResult, NAMESPACE));    IOITMetrics.publish(TEST_ID, TEST_TIMESTAMP, metricsBigQueryDataset, metricsBigQueryTable, Collections.singletonList(metricResult));}
private static Function<MetricsReader, NamedTestResult> beam_f28557_0(String metricName)
{    return reader -> {        long startTime = reader.getStartTimeMetric(metricName);        long endTime = reader.getEndTimeMetric(metricName);        return NamedTestResult.create(TEST_ID, TEST_TIMESTAMP, metricName, (endTime - startTime) / 1e3);    };}
public Read<T> beam_f28566_0(String keyspace)
{    checkArgument(keyspace != null, "keyspace can not be null");    return withKeyspace(ValueProvider.StaticValueProvider.of(keyspace));}
public Read<T> beam_f28567_0(ValueProvider<String> keyspace)
{    return builder().setKeyspace(keyspace).build();}
public Read<T> beam_f28576_0(String password)
{    checkArgument(password != null, "password can not be null");    return withPassword(ValueProvider.StaticValueProvider.of(password));}
public Read<T> beam_f28577_0(ValueProvider<String> password)
{    return builder().setPassword(password).build();}
public Read<T> beam_f28586_0()
{    if (!mapperFactoryFn().isPresent() && entity().isPresent()) {        setMapperFactoryFn(new DefaultObjectMapperFactory(entity().get()));    }    return autoBuild();}
public Coder<T> beam_f28587_0()
{    return spec.coder();}
public void beam_f28596_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    if (spec.hosts() != null) {        builder.add(DisplayData.item("hosts", spec.hosts().toString()));    }    if (spec.port() != null) {        builder.add(DisplayData.item("port", spec.port()));    }    builder.addIfNotNull(DisplayData.item("keyspace", spec.keyspace()));    builder.addIfNotNull(DisplayData.item("table", spec.table()));    builder.addIfNotNull(DisplayData.item("username", spec.username()));    builder.addIfNotNull(DisplayData.item("localDc", spec.localDc()));    builder.addIfNotNull(DisplayData.item("consistencyLevel", spec.consistencyLevel()));}
private static List<TokenRange> beam_f28597_0(Cluster cluster, String keyspace, String table)
{    try (Session session = cluster.newSession()) {        ResultSet resultSet = session.execute("SELECT range_start, range_end, partitions_count, mean_partition_size FROM " + "system.size_estimates WHERE keyspace_name = ? AND table_name = ?", keyspace, table);        ArrayList<TokenRange> tokenRanges = new ArrayList<>();        for (Row row : resultSet) {            TokenRange tokenRange = new TokenRange(row.getLong("partitions_count"), row.getLong("mean_partition_size"), new BigInteger(row.getString("range_start")), new BigInteger(row.getString("range_end")));            tokenRanges.add(tokenRange);        }                return tokenRanges;    }}
private Mapper<T> beam_f28606_0(Session session, Class<T> enitity)
{    return source.spec.mapperFactoryFn().apply(session);}
 static Builder<T> beam_f28607_0(MutationType mutationType)
{    return new AutoValue_CassandraIO_Write.Builder<T>().setMutationType(mutationType);}
public Write<T> beam_f28616_0(ValueProvider<String> username)
{    return builder().setUsername(username).build();}
public Write<T> beam_f28617_0(String password)
{    checkArgument(password != null, "CassandraIO." + getMutationTypeName() + "().withPassword(password) called with " + "null password");    return withPassword(ValueProvider.StaticValueProvider.of(password));}
private String beam_f28626_0()
{    return mutationType() == null ? MutationType.WRITE.name().toLowerCase() : mutationType().name().toLowerCase();}
public Write<T> beam_f28627_0()
{    if (!mapperFactoryFn().isPresent() && entity().isPresent()) {        setMapperFactoryFn(new DefaultObjectMapperFactory(entity().get()));    }    return autoBuild();}
 void beam_f28636_0() throws ExecutionException, InterruptedException
{    if (this.mutateFutures.size() > 0) {                waitForFuturesToFinish();    }    if (session != null) {        session.close();    }    if (cluster != null) {        cluster.close();    }}
private void beam_f28637_0() throws ExecutionException, InterruptedException
{    for (Future<Void> future : mutateFutures) {        future.get();    }}
public String beam_f28646_0()
{    return String.format("(%s,%s]", start.toString(), end.toString());}
private static BigInteger beam_f28647_0(String partitioner)
{    if (partitioner.endsWith("RandomPartitioner")) {        return BigInteger.ZERO;    } else if (partitioner.endsWith("Murmur3Partitioner")) {        return new BigInteger("2").pow(63).negate();    } else {        throw new UnsupportedOperationException("Unsupported partitioner. " + "Only Random and Murmur3 are supported");    }}
public void beam_f28656_0()
{    runWrite();    runRead();}
private void beam_f28657_0()
{    pipelineWrite.apply("GenSequence", GenerateSequence.from(0).to((long) options.getNumberOfRecords())).apply("PrepareTestRows", ParDo.of(new TestRow.DeterministicallyConstructTestRowFn())).apply("MapToEntity", ParDo.of(new CreateScientistFn())).apply("WriteToCassandra", CassandraIO.<Scientist>write().withHosts(options.getCassandraHost()).withPort(options.getCassandraPort()).withKeyspace(KEYSPACE).withEntity(Scientist.class));    pipelineWrite.run().waitUntilFinish();}
public static void beam_f28666_0() throws InterruptedException
{    shutdownHook.shutDownNow();}
private static void beam_f28667_1() throws Exception
{        session.execute(String.format("CREATE TABLE IF NOT EXISTS %s.%s(person_id int, person_name text, PRIMARY KEY" + "(person_id));", CASSANDRA_KEYSPACE, CASSANDRA_TABLE));    session.execute(String.format("CREATE TABLE IF NOT EXISTS %s.%s(person_id int, person_name text, PRIMARY KEY" + "(person_id));", CASSANDRA_KEYSPACE, CASSANDRA_TABLE_WRITE));        String[] scientists = { "Einstein", "Darwin", "Copernicus", "Pasteur", "Curie", "Faraday", "Newton", "Bohr", "Galilei", "Maxwell" };    for (int i = 0; i < NUM_ROWS; i++) {        int index = i % scientists.length;        session.execute(String.format("INSERT INTO %s.%s(person_id, person_name) values(" + i + ", '" + scientists[index] + "');", CASSANDRA_KEYSPACE, CASSANDRA_TABLE));    }    flushMemTables();}
public Future<Void> beam_f28676_0(String entity)
{    counter.incrementAndGet();    return executor.submit(asyncTask);}
public Future<Void> beam_f28677_0(String entity)
{    counter.incrementAndGet();    return executor.submit(asyncTask);}
public void beam_f28686_0()
{        List<TokenRange> tokenRanges = new ArrayList<>();    tokenRanges.add(new TokenRange(1, 1, BigInteger.valueOf(Long.MIN_VALUE), new BigInteger("0")));    assertEquals(0.5, getRingFraction(tokenRanges), 0);        tokenRanges.add(new TokenRange(1, 1, new BigInteger("0"), BigInteger.valueOf(Long.MAX_VALUE)));    assertEquals(1.0, getRingFraction(tokenRanges), 0);}
public void beam_f28687_0()
{    List<TokenRange> tokenRanges = new ArrayList<>();        tokenRanges.add(new TokenRange(1, 1000, BigInteger.valueOf(Long.MIN_VALUE), BigInteger.valueOf(Long.MAX_VALUE)));    assertEquals(1000, getEstimatedSizeBytesFromTokenRanges(tokenRanges));        tokenRanges = new ArrayList<>();    tokenRanges.add(new TokenRange(1, 1000, BigInteger.valueOf(Long.MIN_VALUE), new BigInteger("0")));    assertEquals(2000, getEstimatedSizeBytesFromTokenRanges(tokenRanges));            tokenRanges = new ArrayList<>();    tokenRanges.add(new TokenRange(1, 1000, BigInteger.valueOf(Long.MIN_VALUE), new BigInteger("-3")));    tokenRanges.add(new TokenRange(1, 1000, new BigInteger("-2"), new BigInteger("10000")));    tokenRanges.add(new TokenRange(2, 3000, new BigInteger("10001"), BigInteger.valueOf(Long.MAX_VALUE)));    assertEquals(8000, getEstimatedSizeBytesFromTokenRanges(tokenRanges));}
public PDone beam_f28696_0(PCollection<T> input)
{    TableSchema tableSchema = getTableSchema(jdbcUrl(), table());    Properties properties = properties();    set(properties, ClickHouseQueryParam.MAX_INSERT_BLOCK_SIZE, maxInsertBlockSize());    set(properties, ClickHouseQueryParam.INSERT_QUORUM, insertQuorum());    set(properties, "insert_distributed_sync", insertDistributedSync());    set(properties, "insert_deduplication", insertDeduplicate());    WriteFn<T> fn = new AutoValue_ClickHouseIO_WriteFn.Builder<T>().jdbcUrl(jdbcUrl()).table(table()).maxInsertBlockSize(maxInsertBlockSize()).schema(tableSchema).properties(properties).initialBackoff(initialBackoff()).maxCumulativeBackoff(maxCumulativeBackoff()).maxRetries(maxRetries()).build();    input.apply(ParDo.of(fn));    return PDone.in(input.getPipeline());}
public Write<T> beam_f28697_0(long value)
{    return toBuilder().maxInsertBlockSize(value).build();}
 static String beam_f28706_0(TableSchema schema, String table)
{    String columnsStr = schema.columns().stream().filter(x -> !x.materializedOrAlias()).map(x -> quoteIdentifier(x.name())).collect(Collectors.joining(", "));    return "INSERT INTO " + quoteIdentifier(table) + " (" + columnsStr + ")";}
public void beam_f28707_0() throws SQLException
{    connection = new ClickHouseDataSource(jdbcUrl(), properties()).getConnection();    retryBackoff = FluentBackoff.DEFAULT.withMaxRetries(maxRetries()).withMaxCumulativeBackoff(maxCumulativeBackoff()).withInitialBackoff(initialBackoff());}
 static void beam_f28716_0(ClickHouseRowBinaryStream stream, ColumnType columnType, Object value) throws IOException
{    switch(columnType.typeName()) {        case FIXEDSTRING:            byte[] bytes;            if (value instanceof String) {                bytes = ((String) value).getBytes(Charsets.UTF_8);            } else {                bytes = ((byte[]) value);            }            stream.writeBytes(bytes);            break;        case FLOAT32:            stream.writeFloat32((Float) value);            break;        case FLOAT64:            stream.writeFloat64((Double) value);            break;        case INT8:            stream.writeInt8((Byte) value);            break;        case INT16:            stream.writeInt16((Short) value);            break;        case INT32:            stream.writeInt32((Integer) value);            break;        case INT64:            stream.writeInt64((Long) value);            break;        case STRING:            stream.writeString((String) value);            break;        case UINT8:            stream.writeUInt8((Short) value);            break;        case UINT16:            stream.writeUInt16((Integer) value);            break;        case UINT32:            stream.writeUInt32((Long) value);            break;        case UINT64:            stream.writeUInt64((Long) value);            break;        case ENUM8:            Integer enum8 = columnType.enumValues().get((String) value);            Preconditions.checkNotNull(enum8, "unknown enum value '" + value + "', possible values: " + columnType.enumValues());            stream.writeInt8(enum8);            break;        case ENUM16:            Integer enum16 = columnType.enumValues().get((String) value);            Preconditions.checkNotNull(enum16, "unknown enum value '" + value + "', possible values: " + columnType.enumValues());            stream.writeInt16(enum16);            break;        case DATE:            Days epochDays = Days.daysBetween(EPOCH_INSTANT, (ReadableInstant) value);            stream.writeUInt16(epochDays.getDays());            break;        case DATETIME:            long epochSeconds = ((ReadableInstant) value).getMillis() / 1000L;            stream.writeUInt32(epochSeconds);            break;        case ARRAY:            List<Object> values = (List<Object>) value;            stream.writeUnsignedLeb128(values.size());            for (Object arrayValue : values) {                writeValue(stream, columnType.arrayElementType(), arrayValue);            }            break;    }}
 static void beam_f28717_0(ClickHouseRowBinaryStream stream, TableSchema schema, Row row) throws IOException
{    for (TableSchema.Column column : schema.columns()) {        if (!column.materializedOrAlias()) {            Object value = row.getValue(column.name());            if (column.columnType().nullable()) {                writeNullableValue(stream, column.columnType(), value);            } else {                if (value == null) {                    value = column.defaultValue();                }                writeValue(stream, column.columnType(), value);            }        }    }}
public static ColumnType beam_f28726_0(TypeName typeName)
{    return ColumnType.builder().typeName(typeName).nullable(false).build();}
public static ColumnType beam_f28727_0(TypeName typeName)
{    return ColumnType.builder().typeName(typeName).nullable(true).build();}
public void beam_f28736_0() throws SQLException
{    int size = 1000000;    int done = 0;        executeSql("CREATE TABLE test_atomic_insert (" + "  f0 Int64, " + "  f1 Int64 MATERIALIZED CAST(if((rand() % " + size + ") = 0, '', '1') AS Int64)" + ") ENGINE=MergeTree ORDER BY (f0)");    pipeline.apply(RangeBundle.of(size)).apply(ClickHouseIO.<Row>write(clickHouse.getJdbcUrl(), "test_atomic_insert").withMaxInsertBlockSize(size).withInitialBackoff(Duration.millis(1)).withMaxRetries(2));    long count = 0L;    for (int i = 0; shouldAttempt(i, count); i++) {        done += safeRun() ? 1 : 0;        count = executeQueryAsLong("SELECT COUNT(*) FROM test_atomic_insert");    }        assertEquals(((long) done) * size, count);    assertTrue("insert didn't succeed after " + MAX_ATTEMPTS + " attempts", count > 0L);}
public void beam_f28737_0() throws SQLException
{    int size = 1000000;        executeSql("CREATE TABLE test_idempotent_insert (" + "  f0 Int64, " + "  f1 Int64 MATERIALIZED CAST(if((rand() % " + size + ") = 0, '', '1') AS Int64)" + ") ENGINE=ReplicatedMergeTree('/clickHouse/tables/0/test_idempotent_insert', 'replica_0') " + "ORDER BY (f0)");    pipeline.apply(RangeBundle.of(size)).apply(ClickHouseIO.<Row>write(clickHouse.getJdbcUrl(), "test_idempotent_insert").withMaxInsertBlockSize(size).withInitialBackoff(Duration.millis(1)).withMaxRetries(2));    long count = 0L;    for (int i = 0; shouldAttempt(i, count); i++) {        safeRun();        count = executeQueryAsLong("SELECT COUNT(*) FROM test_idempotent_insert");    }        assertEquals(size, count);    assertTrue("insert didn't succeed after " + MAX_ATTEMPTS + " attempts", count > 0L);}
public void beam_f28746_0() throws Exception
{    Schema schema = Schema.of(Schema.Field.of("f0", FieldType.INT64), Schema.Field.of("f1", FieldType.INT64));    Row row1 = Row.withSchema(schema).addValue(1L).addValue(2L).build();    Row row2 = Row.withSchema(schema).addValue(2L).addValue(4L).build();    Row row3 = Row.withSchema(schema).addValue(3L).addValue(6L).build();    executeSql("CREATE TABLE test_int64 (f0 Int64, f1 Int64) ENGINE=Log");    pipeline.apply(Create.of(row1, row2, row3).withRowSchema(schema)).apply(write("test_int64"));    pipeline.run().waitUntilFinish();    long sum0 = executeQueryAsLong("SELECT SUM(f0) FROM test_int64");    long sum1 = executeQueryAsLong("SELECT SUM(f1) FROM test_int64");    assertEquals(6L, sum0);    assertEquals(12L, sum1);}
public void beam_f28747_0() throws Exception
{    Schema schema = Schema.of(Schema.Field.nullable("f0", FieldType.INT64));    Row row1 = Row.withSchema(schema).addValue(1L).build();    Row row2 = Row.withSchema(schema).addValue(null).build();    Row row3 = Row.withSchema(schema).addValue(3L).build();    executeSql("CREATE TABLE test_nullable_int64 (f0 Nullable(Int64)) ENGINE=Log");    pipeline.apply(Create.of(row1, row2, row3).withRowSchema(schema)).apply(write("test_nullable_int64"));    pipeline.run().waitUntilFinish();    long sum = executeQueryAsLong("SELECT SUM(f0) FROM test_nullable_int64");    long count0 = executeQueryAsLong("SELECT COUNT(*) FROM test_nullable_int64");    long count1 = executeQueryAsLong("SELECT COUNT(f0) FROM test_nullable_int64");    assertEquals(4L, sum);    assertEquals(3L, count0);    assertEquals(2L, count1);}
private ClickHouseIO.Write<T> beam_f28756_0(String table)
{    return ClickHouseIO.<T>write(clickHouse.getJdbcUrl(), table).withMaxRetries(0);}
public void beam_f28757_0()
{    assertEquals(ColumnType.DATE, ColumnType.parse("Date"));}
public void beam_f28766_0()
{    assertEquals(ColumnType.UINT16, ColumnType.parse("UInt16"));}
public void beam_f28767_0()
{    assertEquals(ColumnType.UINT32, ColumnType.parse("UInt32"));}
public void beam_f28776_0()
{    assertEquals(ColumnType.nullable(TableSchema.TypeName.INT32), ColumnType.parse("Nullable(Int32)"));}
public void beam_f28777_0()
{    assertEquals(ColumnType.array(ColumnType.nullable(TableSchema.TypeName.INT32)), ColumnType.parse("Array(Nullable(Int32))"));}
public static String beam_f28786_0(String testIdentifier)
{    SimpleDateFormat formatter = new SimpleDateFormat();    formatter.applyPattern("yyyy_MM_dd_HH_mm_ss_S");    return String.format("BEAMTEST_%s_%s", testIdentifier, formatter.format(new Date()));}
public static String beam_f28787_0(PostgresIOTestPipelineOptions options)
{    return String.format("jdbc:postgresql://%s:%s/%s", options.getPostgresServerName(), options.getPostgresPort(), options.getPostgresDatabaseName());}
public Coder<Accum> beam_f28796_0(CoderRegistry registry, Coder<String> inputCoder) throws CannotProvideCoderException
{    return SerializableCoder.of(Accum.class);}
public Coder<String> beam_f28797_0(CoderRegistry registry, Coder<String> inputCoder)
{    return inputCoder;}
public void beam_f28806_0() throws Exception
{    exceptionRule.expect(SQLException.class);    exceptionRule.expectMessage("Problem with connection");    executeWithRetry(4, 1_000, IOITHelperTest::failingFunction);    assertEquals(4, listOfExceptionsThrown.size());}
public void beam_f28807_0() throws Exception
{    startTimeMeasure = System.currentTimeMillis();    executeWithRetry(IOITHelperTest::recoveringFunction);    assertEquals(1, listOfExceptionsThrown.size());}
public static TestRow beam_f28816_0(Integer seed)
{    return create(seed, getNameForSeed(seed));}
public static String beam_f28817_0(Integer seed)
{    return "Testval" + seed;}
public static ConnectionConfiguration beam_f28826_0(String[] addresses, String index, String type)
{    checkArgument(addresses != null, "addresses can not be null");    checkArgument(addresses.length > 0, "addresses can not be empty");    checkArgument(index != null, "index can not be null");    checkArgument(type != null, "type can not be null");    return new AutoValue_ElasticsearchIO_ConnectionConfiguration.Builder().setAddresses(Arrays.asList(addresses)).setIndex(index).setType(type).build();}
public ConnectionConfiguration beam_f28827_0(String username)
{    checkArgument(username != null, "username can not be null");    checkArgument(!username.isEmpty(), "username can not be empty");    return builder().setUsername(username).build();}
public Read beam_f28836_0(ConnectionConfiguration connectionConfiguration)
{    checkArgument(connectionConfiguration != null, "connectionConfiguration can not be null");    return builder().setConnectionConfiguration(connectionConfiguration).build();}
public Read beam_f28837_0(String query)
{    checkArgument(query != null, "query can not be null");    checkArgument(!query.isEmpty(), "query can not be empty");    return withQuery(ValueProvider.StaticValueProvider.of(query));}
 static long beam_f28846_0(ConnectionConfiguration connectionConfiguration) throws IOException
{                                    JsonNode statsJson = getStats(connectionConfiguration, false);    JsonNode indexStats = statsJson.path("indices").path(connectionConfiguration.getIndex()).path("primaries");    JsonNode store = indexStats.path("store");    return store.path("size_in_bytes").asLong();}
public void beam_f28847_0(DisplayData.Builder builder)
{    spec.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("shard", shardPreference));    builder.addIfNotNull(DisplayData.item("numSlices", numSlices));    builder.addIfNotNull(DisplayData.item("sliceId", sliceId));}
public String beam_f28856_0() throws NoSuchElementException
{    if (current == null) {        throw new NoSuchElementException();    }    return current;}
public void beam_f28857_0() throws IOException
{        String requestBody = String.format("{\"scroll_id\" : [\"%s\"]}", scrollId);    HttpEntity entity = new NStringEntity(requestBody, ContentType.APPLICATION_JSON);    try {        restClient.performRequest("DELETE", "/_search/scroll", Collections.emptyMap(), entity);    } finally {        if (restClient != null) {            restClient.close();        }    }}
public Write beam_f28866_0(FieldValueExtractFn idFn)
{    checkArgument(idFn != null, "idFn must not be null");    return builder().setIdFn(idFn).build();}
public Write beam_f28867_0(FieldValueExtractFn indexFn)
{    checkArgument(indexFn != null, "indexFn must not be null");    return builder().setIndexFn(indexFn).build();}
public void beam_f28876_0(ProcessContext context) throws Exception
{    String document = context.element();    String documentMetadata = getDocumentMetadata(document);        if (spec.getUsePartialUpdate()) {        batch.add(String.format("{ \"update\" : %s }%n{ \"doc\" : %s, \"doc_as_upsert\" : true }%n", documentMetadata, document));    } else {        batch.add(String.format("{ \"index\" : %s }%n%s%n", documentMetadata, document));    }    currentBatchSizeBytes += document.getBytes(StandardCharsets.UTF_8).length;    if (batch.size() >= spec.getMaxBatchSize() || currentBatchSizeBytes >= spec.getMaxBatchSizeBytes()) {        flushBatch();    }}
public void beam_f28877_0(FinishBundleContext context) throws IOException, InterruptedException
{    flushBatch();}
public void beam_f28886_0() throws Exception
{    ElasticsearchIOTestCommon elasticsearchIOTestCommonWrite = new ElasticsearchIOTestCommon(writeConnectionConfiguration, restClient, true);    elasticsearchIOTestCommonWrite.setPipeline(pipeline);    elasticsearchIOTestCommonWrite.testWrite();}
public void beam_f28887_0() throws Exception
{    elasticsearchIOTestCommon.testSizes();}
public void beam_f28896_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testReadWithQueryValueProvider();}
public void beam_f28897_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWrite();}
public void beam_f28906_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWritePartialUpdate();}
public void beam_f28907_0() throws Exception
{        ConnectionConfiguration connectionConfiguration = ConnectionConfiguration.create(new String[] { "http://" + ES_IP + ":" + esHttpPort }, UPDATE_INDEX, UPDATE_TYPE);    ElasticsearchIOTestCommon elasticsearchIOTestCommonWithErrors = new ElasticsearchIOTestCommon(connectionConfiguration, restClient, false);    elasticsearchIOTestCommonWithErrors.setPipeline(pipeline);    elasticsearchIOTestCommonWithErrors.testWritePartialUpdateWithErrors();}
public void beam_f28916_0() throws Exception
{        ElasticsearchIOTestCommon elasticsearchIOTestCommonWrite = new ElasticsearchIOTestCommon(writeConnectionConfiguration, restClient, true);    elasticsearchIOTestCommonWrite.setPipeline(pipeline);    elasticsearchIOTestCommonWrite.testWrite();}
public void beam_f28917_0() throws Exception
{    elasticsearchIOTestCommon.testSizes();}
public void beam_f28926_0() throws Exception
{            createIndex(getEsIndex());    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testRead();}
public void beam_f28927_0() throws Exception
{            createIndex(getEsIndex());    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testReadWithQueryString();}
public void beam_f28936_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWriteWithTypeFn2x5x();}
public void beam_f28937_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWriteWithFullAddressing();}
public void beam_f28947_0() throws Exception
{    elasticsearchIOTestCommon.testSplit(10_000);}
public void beam_f28948_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testRead();}
public void beam_f28957_0() throws IOException
{    if (connectionConfiguration == null) {        connectionConfiguration = ConnectionConfiguration.create(fillAddresses(), getEsIndex(), ES_TYPE).withSocketAndRetryTimeout(120000).withConnectTimeout(5000);        elasticsearchIOTestCommon = new ElasticsearchIOTestCommon(connectionConfiguration, getRestClient(), false);    }}
public void beam_f28958_0() throws Exception
{            createIndex(getEsIndex());    elasticsearchIOTestCommon.testSizes();}
public void beam_f28967_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWriteWithIdFn();}
public void beam_f28968_0() throws Exception
{    elasticsearchIOTestCommon.setPipeline(pipeline);    elasticsearchIOTestCommon.testWriteWithIndexFn();}
public static void beam_f28978_0(String[] args) throws Exception
{    PipelineOptionsFactory.register(ElasticsearchPipelineOptions.class);    ElasticsearchPipelineOptions options = PipelineOptionsFactory.fromArgs(args).as(ElasticsearchPipelineOptions.class);    createAndPopulateReadIndex(options);}
private static void beam_f28979_0(ElasticsearchPipelineOptions options) throws Exception
{        ConnectionConfiguration connectionConfiguration = getConnectionConfiguration(options, IndexMode.READ);    try (RestClient restClient = connectionConfiguration.createClient()) {        ElasticsearchIOTestUtils.insertTestDocuments(connectionConfiguration, NUM_DOCS_ITESTS, restClient);    }}
 void beam_f28988_0() throws Exception
{    testReadWithQueryInternal((read, query) -> read.withQuery(ValueProvider.StaticValueProvider.of(query)));}
private void beam_f28989_0(BiFunction<Read, String, Read> queryConfigurer) throws IOException
{    if (!useAsITests) {        ElasticsearchIOTestUtils.insertTestDocuments(connectionConfiguration, numDocs, restClient);    }    String query = "{\n" + "  \"query\": {\n" + "  \"match\" : {\n" + "    \"scientist\" : {\n" + "      \"query\" : \"Einstein\"\n" + "    }\n" + "  }\n" + "  }\n" + "}";    Read read = ElasticsearchIO.read().withConnectionConfiguration(connectionConfiguration);    read = queryConfigurer.apply(read, query);    PCollection<String> output = pipeline.apply(read);    PAssert.thatSingleton(output.apply("Count", Count.globally())).isEqualTo(numDocs / NUM_SCIENTISTS);    pipeline.run();}
 void beam_f28998_0() throws Exception
{        long docsPerScientist = 10;    long adjustedNumDocs = docsPerScientist * FAMOUS_SCIENTISTS.length;    List<String> data = ElasticsearchIOTestUtils.createDocuments(adjustedNumDocs, ElasticsearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);    pipeline.apply(Create.of(data)).apply(ElasticsearchIO.write().withConnectionConfiguration(connectionConfiguration).withIndexFn(new ExtractValueFn("scientist")));    pipeline.run();        for (String scientist : FAMOUS_SCIENTISTS) {        String index = scientist.toLowerCase();        long count = refreshIndexAndGetCurrentNumDocs(restClient, index, connectionConfiguration.getType());        assertEquals(scientist + " index holds incorrect count", docsPerScientist, count);    }}
public String beam_f28999_0(JsonNode input)
{    return "TYPE_" + input.path(fieldName).asText().hashCode() % 2;}
private void beam_f29008_0(ElasticsearchIO.Write write) throws Exception
{    List<String> data = ElasticsearchIOTestUtils.createDocuments(numDocs, ElasticsearchIOTestUtils.InjectionMode.DO_NOT_INJECT_INVALID_DOCS);    pipeline.apply(Create.of(data)).apply(write);    pipeline.run();    long currentNumDocs = refreshIndexAndGetCurrentNumDocs(connectionConfiguration, restClient);    assertEquals(numDocs, currentNumDocs);    int count = countByScientistName(connectionConfiguration, restClient, "Einstein");    assertEquals(numDocs / NUM_SCIENTISTS, count);}
 static void beam_f29009_0(ConnectionConfiguration connectionConfiguration, RestClient restClient) throws IOException
{    deleteIndex(restClient, connectionConfiguration.getIndex());}
 static int beam_f29018_0(ConnectionConfiguration connectionConfiguration, RestClient restClient, String field, String value) throws IOException
{    String requestBody = "{\n" + "  \"query\" : {\"match\": {\n" + "    \"" + field + "\": \"" + value + "\"\n" + "  }}\n" + "}\n";    String endPoint = String.format("/%s/%s/_search", connectionConfiguration.getIndex(), connectionConfiguration.getType());    HttpEntity httpEntity = new NStringEntity(requestBody, ContentType.APPLICATION_JSON);    Response response = restClient.performRequest("GET", endPoint, Collections.emptyMap(), httpEntity);    JsonNode searchResult = parseResponse(response.getEntity());    return searchResult.path("hits").path("total").asInt();}
public static void beam_f29019_0(ConnectionConfiguration connectionConfiguration, RestClient restClient) throws IOException
{    String endpoint = String.format("/%s", connectionConfiguration.getIndex());    String requestString = String.format("{\"mappings\":{\"%s\":{\"properties\":{\"age\":{\"type\":\"long\"}," + " \"scientist\":{\"type\":\"%s\"}, \"id\":{\"type\":\"long\"}}}}}", connectionConfiguration.getType(), getBackendVersion(connectionConfiguration) == 2 ? "string" : "text");    HttpEntity requestBody = new NStringEntity(requestString, ContentType.APPLICATION_JSON);    Request request = new Request("PUT", endpoint);    request.setEntity(requestBody);    restClient.performRequest(request);}
public static String beam_f29028_0(int lineCount)
{    Map<Integer, String> expectedHashes = ImmutableMap.of(1000, "8604c70b43405ef9803cb49b77235ea2", 100_000, "4c8bb3b99dcc59459b20fefba400d446", 1_000_000, "9796db06e7a7960f974d5a91164afff1", 100_000_000, "6ce05f456e2fdc846ded2abd0ec1de95");    return getHashForRecordCount(lineCount, expectedHashes);}
public void beam_f29029_0(ProcessContext c)
{    c.output(String.format("IO IT Test line of text. Line seed: %s", c.element()));}
private void beam_f29038_0(PipelineResult result)
{    String uuid = UUID.randomUUID().toString();    Timestamp timestamp = Timestamp.now();    Set<Function<MetricsReader, NamedTestResult>> metricSuppliers = fillMetricSuppliers(uuid, timestamp.toString());    new IOITMetrics(metricSuppliers, result, FILEIOIT_NAMESPACE, uuid, timestamp.toString()).publish(bigQueryDataset, bigQueryTable);}
private Set<Function<MetricsReader, NamedTestResult>> beam_f29039_0(String uuid, String timestamp)
{    Set<Function<MetricsReader, NamedTestResult>> metricSuppliers = new HashSet<>();    metricSuppliers.add((reader) -> {        long writeStartTime = reader.getStartTimeMetric("startTime");        long writeEndTime = reader.getEndTimeMetric("middleTime");        double writeTime = (writeEndTime - writeStartTime) / 1e3;        return NamedTestResult.create(uuid, timestamp, "write_time", writeTime);    });    metricSuppliers.add((reader) -> {        long readStartTime = reader.getStartTimeMetric("middleTime");        long readEndTime = reader.getEndTimeMetric("endTime");        double readTime = (readEndTime - readStartTime) / 1e3;        return NamedTestResult.create(uuid, timestamp, "read_time", readTime);    });    metricSuppliers.add((reader) -> {        long writeStartTime = reader.getStartTimeMetric("startTime");        long readEndTime = reader.getEndTimeMetric("endTime");        double runTime = (readEndTime - writeStartTime) / 1e3;        return NamedTestResult.create(uuid, timestamp, "run_time", runTime);    });    if (gatherGcsPerformanceMetrics) {        metricSuppliers.add(reader -> {            MetricsReader actualReader = reader.withNamespace("org.apache.beam.sdk.extensions.gcp.storage.GcsFileSystem");            long numCopies = actualReader.getCounterMetric("num_copies");            long copyTimeMsec = actualReader.getCounterMetric("copy_time_msec");            double copiesPerSec = (numCopies < 0 || copyTimeMsec < 0) ? -1 : numCopies / (copyTimeMsec / 1e3);            return NamedTestResult.create(uuid, timestamp, "copies_per_sec", copiesPerSec);        });    }    return metricSuppliers;}
public void beam_f29048_0()
{    PCollection<String> testFileNames = pipeline.apply("Generate sequence", GenerateSequence.from(0).to(numberOfRecords)).apply("Create xml records", MapElements.via(new LongToBird())).apply("Gather write start time", ParDo.of(new TimeMonitor<>(XMLIOIT_NAMESPACE, "writeStart"))).apply("Write xml files", FileIO.<Bird>write().via(XmlIO.sink(Bird.class).withRootElement("birds").withCharset(charset)).to(filenamePrefix).withPrefix("birds").withSuffix(".xml")).getPerDestinationOutputFilenames().apply("Gather write end time", ParDo.of(new TimeMonitor<>(XMLIOIT_NAMESPACE, "writeEnd"))).apply("Get file names", Values.create());    PCollection<Bird> birds = testFileNames.apply("Find files", FileIO.matchAll()).apply("Read matched files", FileIO.readMatches()).apply("Gather read start time", ParDo.of(new TimeMonitor<>(XMLIOIT_NAMESPACE, "readStart"))).apply("Read xml files", XmlIO.<Bird>readFiles().withRecordClass(Bird.class).withRootElement("birds").withRecordElement("bird").withCharset(charset)).apply("Gather read end time", ParDo.of(new TimeMonitor<>(XMLIOIT_NAMESPACE, "readEnd")));    PCollection<String> consolidatedHashcode = birds.apply("Map xml records to strings", MapElements.via(new BirdToString())).apply("Calculate hashcode", Combine.globally(new HashingFn()));    String expectedHash = getHashForRecordCount(numberOfRecords, EXPECTED_HASHES);    PAssert.thatSingleton(consolidatedHashcode).isEqualTo(expectedHash);    testFileNames.apply("Delete test files", ParDo.of(new FileBasedIOITHelper.DeleteFileFn()).withSideInputs(consolidatedHashcode.apply(View.asSingleton())));    PipelineResult result = pipeline.run();    result.waitUntilFinish();    collectAndPublishResults(result);}
private void beam_f29049_0(PipelineResult result)
{    String uuid = UUID.randomUUID().toString();    String timestamp = Timestamp.now().toString();    Set<Function<MetricsReader, NamedTestResult>> metricSuppliers = fillMetricSuppliers(uuid, timestamp);    new IOITMetrics(metricSuppliers, result, XMLIOIT_NAMESPACE, uuid, timestamp).publish(bigQueryDataset, bigQueryTable);}
public int beam_f29058_0()
{    int result = name.hashCode();    result = 31 * result + adjective.hashCode();    return result;}
public String beam_f29059_0()
{    return String.format("Bird: %s, %s", name, adjective);}
 void beam_f29068_0(int maxFilesPerPartition)
{    this.maxFilesPerPartition = maxFilesPerPartition;}
 void beam_f29069_0(long maxBytesPerPartition)
{    this.maxBytesPerPartition = maxBytesPerPartition;}
 PCollection<WriteBundlesToFiles.Result<DestinationT>> beam_f29078_0(PCollection<KV<DestinationT, ElementT>> input, PCollectionView<String> tempFilePrefix)
{    checkState(numFileShards > 0);    PCollection<KV<ShardedKey<DestinationT>, ElementT>> shardedRecords = input.apply("AddShard", ParDo.of(new DoFn<KV<DestinationT, ElementT>, KV<ShardedKey<DestinationT>, ElementT>>() {        int shardNumber;        @Setup        public void setup() {            shardNumber = ThreadLocalRandom.current().nextInt(numFileShards);        }        @ProcessElement        public void processElement(@Element KV<DestinationT, ElementT> element, OutputReceiver<KV<ShardedKey<DestinationT>, ElementT>> o) {            DestinationT destination = element.getKey();            o.output(KV.of(ShardedKey.of(destination, ++shardNumber % numFileShards), element.getValue()));        }    })).setCoder(KvCoder.of(ShardedKeyCoder.of(destinationCoder), elementCoder));    return writeShardedRecords(shardedRecords, tempFilePrefix);}
public void beam_f29079_0()
{    shardNumber = ThreadLocalRandom.current().nextInt(numFileShards);}
private static String beam_f29088_0(long timeMicros)
{    java.time.format.DateTimeFormatter formatter;    if (timeMicros % 1000000 == 0) {        formatter = ISO_LOCAL_TIME_FORMATTER_SECONDS;    } else if (timeMicros % 1000 == 0) {        formatter = ISO_LOCAL_TIME_FORMATTER_MILLIS;    } else {        formatter = ISO_LOCAL_TIME_FORMATTER_MICROS;    }    return LocalTime.ofNanoOfDay(timeMicros * 1000).format(formatter);}
 static TableRow beam_f29089_0(GenericRecord record, TableSchema schema)
{    return convertGenericRecordToTableRow(record, schema.getFields());}
 PendingJobManager beam_f29098_0(PendingJob pendingJob, @Nullable SerializableFunction<PendingJob, Exception> onSuccess)
{    this.pendingJobs.add(new JobInfo(pendingJob, onSuccess));    return this;}
 void beam_f29099_1() throws Exception
{        Sleeper sleeper = Sleeper.DEFAULT;    while (!pendingJobs.isEmpty()) {        List<JobInfo> retryJobs = Lists.newArrayList();        for (JobInfo jobInfo : pendingJobs) {            if (jobInfo.pendingJob.pollJob()) {                                                Exception e = jobInfo.onSuccess.apply(jobInfo.pendingJob);                if (e != null) {                    throw e;                }            } else {                                                retryJobs.add(jobInfo);            }        }        pendingJobs = retryJobs;        if (!pendingJobs.isEmpty()) {                        nextBackOff(sleeper, backOff);                        for (JobInfo job : pendingJobs) {                job.pendingJob.runJob();            }        }    }}
 static RetryJobIdResult beam_f29108_1(RetryJobId currentJobId, SerializableFunction<RetryJobId, Job> lookupJob)
{    for (int retryIndex = currentJobId.getRetryIndex(); ; retryIndex++) {        RetryJobId jobId = new RetryJobId(currentJobId.getJobIdPrefix(), retryIndex);        try {            Job loadJob = lookupJob.apply(jobId);            if (loadJob == null) {                                                return new RetryJobIdResult(jobId, true);            }            JobStatus jobStatus = loadJob.getStatus();            if (jobStatus == null) {                                return new RetryJobIdResult(jobId, true);            }            if ("PENDING".equals(jobStatus.getState()) || "RUNNING".equals(jobStatus.getState())) {                                                                                                return new RetryJobIdResult(jobId, false);            }            if (jobStatus.getErrorResult() == null && (jobStatus.getErrors() == null || jobStatus.getErrors().isEmpty())) {                                                return new RetryJobIdResult(jobId, false);            }                                                        } catch (RuntimeException e) {                        return new RetryJobIdResult(jobId, true);        }    }}
 static TableReferenceProto.TableReference beam_f29109_0(TableReference ref)
{    TableReferenceProto.TableReference.Builder builder = TableReferenceProto.TableReference.newBuilder();    if (ref.getProjectId() != null) {        builder.setProjectId(ref.getProjectId());    }    return builder.setDatasetId(ref.getDatasetId()).setTableId(ref.getTableId()).build();}
 static String beam_f29118_0(@Nullable Job job) throws IOException
{    if (job != null && job.getConfiguration().getLoad() != null) {                        job = job.clone();        job.getConfiguration().getLoad().setSchema(null);        job.getConfiguration().getLoad().setSourceUris(null);    }    return job == null ? "null" : job.toPrettyString();}
 static String beam_f29119_0(@Nullable JobStatus status) throws IOException
{    return status == null ? "Unknown status: null." : status.toPrettyString();}
 static void beam_f29128_0(DatasetService datasetService, TableReference table)
{    try {        datasetService.getTable(table);    } catch (Exception e) {        ApiErrorExtractor errorExtractor = new ApiErrorExtractor();        if ((e instanceof IOException) && errorExtractor.itemNotFound((IOException) e)) {            throw new IllegalArgumentException(String.format(RESOURCE_NOT_FOUND_ERROR, "table", toTableSpec(table)), e);        } else if (e instanceof RuntimeException) {            throw (RuntimeException) e;        } else {            throw new RuntimeException(String.format(UNABLE_TO_CONFIRM_PRESENCE_OF_RESOURCE_ERROR, "table", toTableSpec(table)), e);        }    }}
 static String beam_f29129_0(String prefix, TableDestination tableDestination, int partition, long index)
{        String destinationHash = Hashing.murmur3_128().hashUnencodedChars(tableDestination.toString()).toString();    String jobId = String.format("%s_%s", prefix, destinationHash);    if (partition >= 0) {        jobId += String.format("_%05d", partition);    }    if (index >= 0) {        jobId += String.format("_%05d", index);    }    return jobId;}
public TableReference beam_f29138_0(String from)
{    return parseTableSpec(from);}
public String beam_f29139_0(TimePartitioning partitioning)
{    return toJsonString(partitioning);}
public int beam_f29148_0()
{    return Objects.hash(row, error, table);}
public static BigQueryInsertErrorCoder beam_f29149_0()
{    return INSTANCE;}
public static TypedRead<T> beam_f29158_0(SerializableFunction<SchemaAndRecord, T> parseFn)
{    return new AutoValue_BigQueryIO_TypedRead.Builder<T>().setValidate(true).setWithTemplateCompatibility(false).setBigQueryServices(new BigQueryServicesImpl()).setParseFn(parseFn).setMethod(Method.DEFAULT).build();}
public TableRow beam_f29159_0(SchemaAndRecord schemaAndRecord)
{    return BigQueryAvroUtils.convertGenericRecordToTableRow(schemaAndRecord.getRecord(), schemaAndRecord.getTableSchema());}
public Read beam_f29168_0(ValueProvider<String> tableSpec)
{    return new Read(this.inner.from(tableSpec));}
public Read beam_f29169_0(TableReference table)
{    return new Read(this.inner.from(table));}
private BigQueryStorageQuerySource<T> beam_f29178_0(String stepUuid, Coder<T> outputCoder)
{    return BigQueryStorageQuerySource.create(stepUuid, getQuery(), getFlattenResults(), getUseLegacySql(), MoreObjects.firstNonNull(getQueryPriority(), QueryPriority.BATCH), getQueryLocation(), getKmsKey(), getParseFn(), outputCoder, getBigQueryServices());}
public void beam_f29179_0(PipelineOptions options)
{            BigQueryOptions bqOptions = options.as(BigQueryOptions.class);    if (getMethod() != Method.DIRECT_READ) {        String tempLocation = bqOptions.getTempLocation();        checkArgument(!Strings.isNullOrEmpty(tempLocation), "BigQueryIO.Read needs a GCS temp location to store temp files.");        if (getBigQueryServices() == null) {            try {                GcsPath.fromUri(tempLocation);            } catch (IllegalArgumentException e) {                throw new IllegalArgumentException(String.format("BigQuery temp location expected a valid 'gs://' path, but was given '%s'", tempLocation), e);            }        }    }    ValueProvider<TableReference> table = getTableProvider();        if (getValidate()) {        if (table != null) {            checkArgument(table.isAccessible(), "Cannot call validate if table is dynamically set.");        }        if (table != null && table.get().getProjectId() != null) {                        DatasetService datasetService = getBigQueryServices().getDatasetService(bqOptions);            BigQueryHelpers.verifyDatasetPresence(datasetService, table.get());            BigQueryHelpers.verifyTablePresence(datasetService, table.get());        } else if (getQuery() != null) {            checkArgument(getQuery().isAccessible(), "Cannot call validate if query is dynamically set.");            JobService jobService = getBigQueryServices().getJobService(bqOptions);            try {                jobService.dryRunQuery(bqOptions.getProject(), new JobConfigurationQuery().setQuery(getQuery().get()).setFlattenResults(getFlattenResults()).setUseLegacySql(getUseLegacySql()), getQueryLocation());            } catch (Exception e) {                throw new IllegalArgumentException(String.format(QUERY_VALIDATION_FAILURE_ERROR, getQuery().get()), e);            }        }    }}
public void beam_f29188_0(ProcessContext c) throws Exception
{    ReadSession readSession = c.sideInput(readSessionView);    TableSchema tableSchema = BigQueryHelpers.fromJsonString(c.sideInput(tableSchemaView), TableSchema.class);    Stream stream = c.element();    BigQueryStorageStreamSource<T> streamSource = BigQueryStorageStreamSource.create(readSession, stream, tableSchema, getParseFn(), outputCoder, getBigQueryServices());                BoundedSource.BoundedReader<T> reader = streamSource.createReader(c.getPipelineOptions());    for (boolean more = reader.start(); more; more = reader.advance()) {        c.output(reader.getCurrent());    }}
 void beam_f29189_1(ContextContainer c) throws Exception
{    BigQueryOptions options = c.getPipelineOptions().as(BigQueryOptions.class);    String jobUuid = c.getJobId();    TableReference tempTable = createTempTableReference(options.getProject(), createJobIdToken(options.getJobName(), jobUuid));    DatasetService datasetService = getBigQueryServices().getDatasetService(options);        datasetService.deleteTable(tempTable);        datasetService.deleteDataset(tempTable.getProjectId(), tempTable.getDatasetId());}
public TypedRead<T> beam_f29198_0(ToBeamRowFunction<T> toRowFn, FromBeamRowFunction<T> fromRowFn)
{    return toBuilder().setToBeamRowFn(toRowFn).setFromBeamRowFn(fromRowFn).build();}
public TypedRead<T> beam_f29199_0(String tableSpec)
{    return from(StaticValueProvider.of(tableSpec));}
public TypedRead<T> beam_f29208_0(String location)
{    return toBuilder().setQueryLocation(location).build();}
public TypedRead<T> beam_f29209_0(Method method)
{    return toBuilder().setMethod(method).build();}
 static List<ResourceId> beam_f29218_0(String extractDestinationDir, Job extractJob) throws IOException
{    JobStatistics jobStats = extractJob.getStatistics();    List<Long> counts = jobStats.getExtract().getDestinationUriFileCounts();    if (counts.size() != 1) {        String errorMessage = counts.isEmpty() ? "No destination uri file count received." : String.format("More than one destination uri file count received. First two are %s, %s", counts.get(0), counts.get(1));        throw new RuntimeException(errorMessage);    }    long filesCount = counts.get(0);    ImmutableList.Builder<ResourceId> paths = ImmutableList.builder();    ResourceId extractDestinationDirResourceId = FileSystems.matchNewResource(extractDestinationDir, true);    for (long i = 0; i < filesCount; ++i) {        ResourceId filePath = extractDestinationDirResourceId.resolve(String.format("%012d%s", i, ".avro"), ResolveOptions.StandardResolveOptions.RESOLVE_FILE);        paths.add(filePath);    }    return paths.build();}
public static Write<T> beam_f29219_0()
{    return new AutoValue_BigQueryIO_Write.Builder<T>().setValidate(true).setBigQueryServices(new BigQueryServicesImpl()).setCreateDisposition(Write.CreateDisposition.CREATE_IF_NEEDED).setWriteDisposition(Write.WriteDisposition.WRITE_EMPTY).setNumFileShards(0).setMethod(Write.Method.DEFAULT).setExtendedErrorInfo(false).setSkipInvalidRows(false).setIgnoreUnknownValues(false).setMaxFilesPerPartition(BatchLoads.DEFAULT_MAX_FILES_PER_PARTITION).setMaxBytesPerPartition(BatchLoads.DEFAULT_MAX_BYTES_PER_PARTITION).setOptimizeWrites(false).setUseBeamSchema(false).build();}
public Write<T> beam_f29228_0(ValueProvider<TableSchema> schema)
{    checkArgument(schema != null, "schema can not be null");    return withJsonSchema(NestedValueProvider.of(schema, new TableSchemaToJsonSchema()));}
public Write<T> beam_f29229_0(String jsonSchema)
{    checkArgument(jsonSchema != null, "jsonSchema can not be null");    return withJsonSchema(StaticValueProvider.of(jsonSchema));}
public Write<T> beam_f29238_0(WriteDisposition writeDisposition)
{    checkArgument(writeDisposition != null, "writeDisposition can not be null");    return toBuilder().setWriteDisposition(writeDisposition).build();}
public Write<T> beam_f29239_0(String tableDescription)
{    checkArgument(tableDescription != null, "tableDescription can not be null");    return toBuilder().setTableDescription(tableDescription).build();}
public Write<T> beam_f29248_0()
{    return toBuilder().setExtendedErrorInfo(true).build();}
public Write<T> beam_f29249_0()
{    return toBuilder().setSkipInvalidRows(true).build();}
 Write<T> beam_f29258_0(long maxBytesPerPartition)
{    checkArgument(maxBytesPerPartition > 0, "maxFilesPerPartition must be > 0, but was: %s", maxBytesPerPartition);    return toBuilder().setMaxBytesPerPartition(maxBytesPerPartition).build();}
public void beam_f29259_0(PipelineOptions pipelineOptions)
{    BigQueryOptions options = pipelineOptions.as(BigQueryOptions.class);        if (getJsonTableRef() != null && getJsonTableRef().isAccessible() && getValidate()) {        TableReference table = getTableWithDefaultProject(options).get();        DatasetService datasetService = getBigQueryServices().getDatasetService(options);                                        BigQueryHelpers.verifyDatasetPresence(datasetService, table);        if (getCreateDisposition() == BigQueryIO.Write.CreateDisposition.CREATE_NEVER) {            BigQueryHelpers.verifyTablePresence(datasetService, table);        }        if (getWriteDisposition() == BigQueryIO.Write.WriteDisposition.WRITE_EMPTY) {            BigQueryHelpers.verifyTableNotExistOrEmpty(datasetService, table);        }    }}
public static JobStatistics beam_f29268_0(BigQueryServices bqServices, BigQueryOptions options, AtomicReference<JobStatistics> dryRunJobStats, String query, Boolean flattenResults, Boolean useLegacySql, @Nullable String location) throws InterruptedException, IOException
{    if (dryRunJobStats.get() == null) {        JobStatistics jobStatistics = bqServices.getJobService(options).dryRunQuery(options.getProject(), createBasicQueryConfig(query, flattenResults, useLegacySql), location);        dryRunJobStats.compareAndSet(null, jobStatistics);    }    return dryRunJobStats.get();}
public static TableReference beam_f29269_1(BigQueryServices bqServices, BigQueryOptions options, AtomicReference<JobStatistics> dryRunJobStats, String stepUuid, String query, Boolean flattenResults, Boolean useLegacySql, QueryPriority priority, @Nullable String location, @Nullable String kmsKey) throws InterruptedException, IOException
{        String effectiveLocation = location;    DatasetService tableService = bqServices.getDatasetService(options);    if (effectiveLocation == null) {        List<TableReference> referencedTables = dryRunQueryIfNeeded(bqServices, options, dryRunJobStats, query, flattenResults, useLegacySql, location).getQuery().getReferencedTables();        if (referencedTables != null && !referencedTables.isEmpty()) {            TableReference referencedTable = referencedTables.get(0);            effectiveLocation = tableService.getTable(referencedTable).getLocation();        }    }        String jobIdToken = createJobIdToken(options.getJobName(), stepUuid);    TableReference queryResultTable = createTempTableReference(options.getProject(), jobIdToken);        tableService.createDataset(queryResultTable.getProjectId(), queryResultTable.getDatasetId(), effectiveLocation, "Temporary tables for query results of job " + options.getJobName(), TimeUnit.DAYS.toMillis(1));                    String queryJobId = jobIdToken + "-query-" + BigQueryHelpers.randomUUIDString();        JobReference jobReference = new JobReference().setProjectId(options.getProject()).setLocation(effectiveLocation).setJobId(queryJobId);    JobConfigurationQuery queryConfiguration = createBasicQueryConfig(query, flattenResults, useLegacySql).setAllowLargeResults(true).setDestinationTable(queryResultTable).setCreateDisposition("CREATE_IF_NEEDED").setWriteDisposition("WRITE_TRUNCATE").setPriority(priority.name());    if (kmsKey != null) {        queryConfiguration.setDestinationEncryptionConfiguration(new EncryptionConfiguration().setKmsKeyName(kmsKey));    }    JobService jobService = bqServices.getJobService(options);    jobService.startQueryJob(jobReference, queryConfiguration);    Job job = jobService.pollJob(jobReference, JOB_POLL_MAX_RETRIES);    if (BigQueryHelpers.parseStatus(job) != Status.SUCCEEDED) {        throw new IOException(String.format("Query job %s failed, status: %s", queryJobId, BigQueryHelpers.statusToPrettyString(job.getStatus())));    }        return queryResultTable;}
 long beam_f29278_0(BigQueryOptions bqOptions) throws Exception
{    return BigQueryQueryHelper.dryRunQueryIfNeeded(bqServices, bqOptions, dryRunJobStats, query.get(), flattenResults, useLegacySql, location).getQuery().getTotalBytesProcessed();}
 TableReference beam_f29279_0(BigQueryOptions bqOptions, String stepUuid) throws IOException, InterruptedException
{    return BigQueryQueryHelper.executeQuery(bqServices, bqOptions, dryRunJobStats, stepUuid, query.get(), flattenResults, useLegacySql, priority, location, kmsKey);}
public void beam_f29288_0(JobReference jobRef, JobConfigurationLoad loadConfig) throws InterruptedException, IOException
{    Job job = new Job().setJobReference(jobRef).setConfiguration(new JobConfiguration().setLoad(loadConfig));    startJob(job, errorExtractor, client);}
public void beam_f29289_0(JobReference jobRef, JobConfigurationExtract extractConfig) throws InterruptedException, IOException
{    Job job = new Job().setJobReference(jobRef).setConfiguration(new JobConfiguration().setExtract(extractConfig));    startJob(job, errorExtractor, client);}
public Job beam_f29298_0(JobReference jobRef) throws IOException, InterruptedException
{    return getJob(jobRef, Sleeper.DEFAULT, createDefaultBackoff());}
public Job beam_f29299_1(JobReference jobRef, Sleeper sleeper, BackOff backoff) throws IOException, InterruptedException
{    String jobId = jobRef.getJobId();    Exception lastException;    do {        try {            return client.jobs().get(jobRef.getProjectId(), jobId).execute();        } catch (GoogleJsonResponseException e) {            if (errorExtractor.itemNotFound(e)) {                                return null;            }                        lastException = e;        } catch (IOException e) {                        lastException = e;        }    } while (nextBackOff(sleeper, backoff));    throw new IOException(String.format("Unable to find BigQuery job: %s, aborting after %d retries.", jobRef, MAX_RPC_RETRIES), lastException);}
public Dataset beam_f29308_0(String projectId, String datasetId) throws IOException, InterruptedException
{    return executeWithRetries(client.datasets().get(projectId, datasetId), String.format("Unable to get dataset: %s, aborting after %d retries.", datasetId, MAX_RPC_RETRIES), Sleeper.DEFAULT, createDefaultBackoff(), DONT_RETRY_NOT_FOUND);}
public void beam_f29309_0(String projectId, String datasetId, @Nullable String location, @Nullable String description, @Nullable Long defaultTableExpirationMs) throws IOException, InterruptedException
{    createDataset(projectId, datasetId, location, description, defaultTableExpirationMs, Sleeper.DEFAULT, createDefaultBackoff());}
private static HttpRequestInitializer beam_f29318_0(Credentials credential, HttpRequestInitializer httpRequestInitializer)
{    if (credential == null) {        return new ChainingHttpRequestInitializer(new NullCredentialInitializer(), httpRequestInitializer);    } else {        return new ChainingHttpRequestInitializer(new HttpCredentialsAdapter(credential), httpRequestInitializer);    }}
public static CustomHttpErrors beam_f29319_0()
{    CustomHttpErrors.Builder builder = new CustomHttpErrors.Builder();                builder.addErrorForCodeAndUrlContains(403, "/tables?", "The GCP project is most likely exceeding the rate limit on " + "bigquery.tables.list, please find the instructions to increase this limit at: " + "https://cloud.google.com/service-infrastructure/docs/rate-limiting#configure");    return builder.build();}
public boolean beam_f29328_0()
{    return executor.isShutdown();}
public boolean beam_f29329_0()
{    return executor.isTerminated();}
public void beam_f29338_0(Runnable runnable)
{    executor.execute(new SemaphoreRunnable(runnable));}
public void beam_f29339_0()
{    try {        semaphore.acquire();    } catch (InterruptedException e) {        throw new RuntimeException("semaphore acquisition interrupted. task canceled.");    }    try {        runnable.run();    } finally {        semaphore.release();    }}
public T beam_f29349_0(GenericRecord input)
{    return parseFn.apply(new SchemaAndRecord(input, schema.get()));}
public static BigQueryStorageQuerySource<T> beam_f29350_0(String stepUuid, ValueProvider<String> queryProvider, Boolean flattenResults, Boolean useLegacySql, QueryPriority priority, @Nullable String location, @Nullable String kmsKey, SerializableFunction<SchemaAndRecord, T> parseFn, Coder<T> outputCoder, BigQueryServices bqServices)
{    return new BigQueryStorageQuerySource<>(stepUuid, queryProvider, flattenResults, useLegacySql, priority, location, kmsKey, parseFn, outputCoder, bqServices);}
public Coder<T> beam_f29360_0()
{    return outputCoder;}
public void beam_f29361_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("table", BigQueryHelpers.toTableSpec(readSession.getTableReference())).withLabel("Table")).add(DisplayData.item("readSession", readSession.getName()).withLabel("Read session")).add(DisplayData.item("stream", stream.getName()).withLabel("Stream"));}
public synchronized void beam_f29370_0()
{    storageClient.close();}
public synchronized BigQueryStorageStreamSource<T> beam_f29371_0()
{    return source;}
private List<String> beam_f29380_0()
{    if (selectedFieldsProvider != null) {        return selectedFieldsProvider.get();    } else if (tableReadOptions != null && !tableReadOptions.getSelectedFieldsList().isEmpty()) {        return tableReadOptions.getSelectedFieldsList();    }    return null;}
 static BigQueryTableSource<T> beam_f29381_0(String stepUuid, BigQueryTableSourceDef tableDef, BigQueryServices bqServices, Coder<T> coder, SerializableFunction<SchemaAndRecord, T> parseFn)
{    return new BigQueryTableSource<>(stepUuid, tableDef, bqServices, coder, parseFn);}
public Schema beam_f29391_0(BigQueryOptions bqOptions)
{    try {        TableReference tableRef = getTableReference(bqOptions);        TableSchema tableSchema = bqServices.getDatasetService(bqOptions).getTable(tableRef).getSchema();        return BigQueryUtils.fromTableSchema(tableSchema);    } catch (IOException | InterruptedException | NullPointerException e) {        throw new BigQuerySchemaRetrievalException("Exception while trying to retrieve schema", e);    }}
public static Builder beam_f29392_0()
{    return new AutoValue_BigQueryUtils_ConversionOptions.Builder().setTruncateTimestamps(TruncateTimestamps.REJECT);}
public static final BigQueryIO.TypedRead.FromBeamRowFunction<TableRow> beam_f29401_0()
{    return TABLE_ROW_FROM_BEAM_ROW_FUNCTION;}
public static SerializableFunction<Row, TableRow> beam_f29402_0()
{    return ROW_TO_TABLE_ROW;}
private static Object beam_f29411_0(FieldType fieldType, Object jsonBQValue)
{    if (jsonBQValue instanceof String && JSON_VALUE_PARSERS.containsKey(fieldType.getTypeName())) {        return JSON_VALUE_PARSERS.get(fieldType.getTypeName()).apply((String) jsonBQValue);    }    if (jsonBQValue instanceof List) {        return ((List<Object>) jsonBQValue).stream().map(v -> ((Map<String, Object>) v).get("v")).map(v -> toBeamValue(fieldType.getCollectionElementType(), v)).collect(toList());    }    if (jsonBQValue instanceof Map) {        TableRow tr = new TableRow();        tr.putAll((Map<String, Object>) jsonBQValue);        return toBeamRow(fieldType.getRowSchema(), tr);    }    throw new UnsupportedOperationException("Converting BigQuery type '" + jsonBQValue.getClass() + "' to '" + fieldType + "' is not supported");}
public static Object beam_f29412_0(FieldType beamFieldType, Object avroValue, BigQueryUtils.ConversionOptions options)
{    TypeName beamFieldTypeName = beamFieldType.getTypeName();    if (avroValue == null) {        if (beamFieldType.getNullable()) {            return null;        } else {            throw new IllegalArgumentException(String.format("Field %s not nullable", beamFieldType));        }    }    switch(beamFieldTypeName) {        case INT16:        case INT32:        case INT64:        case FLOAT:        case DOUBLE:        case BYTE:        case BOOLEAN:            return convertAvroPrimitiveTypes(beamFieldTypeName, avroValue);        case DATETIME:                        switch(options.getTruncateTimestamps()) {                case TRUNCATE:                    return truncateToMillis(avroValue);                case REJECT:                    return safeToMillis(avroValue);                default:                    throw new IllegalArgumentException(String.format("Unknown timestamp truncation option: %s", options.getTruncateTimestamps()));            }        case STRING:            return convertAvroPrimitiveTypes(beamFieldTypeName, avroValue);        case ARRAY:            return convertAvroArray(beamFieldType, avroValue, options);        case LOGICAL_TYPE:            String identifier = beamFieldType.getLogicalType().getIdentifier();            if (SQL_DATE_TIME_TYPES.contains(identifier)) {                switch(options.getTruncateTimestamps()) {                    case TRUNCATE:                        return truncateToMillis(avroValue);                    case REJECT:                        return safeToMillis(avroValue);                    default:                        throw new IllegalArgumentException(String.format("Unknown timestamp truncation option: %s", options.getTruncateTimestamps()));                }            } else if (SQL_STRING_TYPES.contains(identifier)) {                return convertAvroPrimitiveTypes(TypeName.STRING, avroValue);            } else {                throw new RuntimeException("Unknown logical type " + identifier);            }        case ROW:            Schema rowSchema = beamFieldType.getRowSchema();            if (rowSchema == null) {                throw new IllegalArgumentException("Nested ROW missing row schema");            }            GenericData.Record record = (GenericData.Record) avroValue;            return toBeamRow(record, rowSchema, options);        case DECIMAL:            throw new RuntimeException("Does not support converting DECIMAL type value");        case MAP:            throw new RuntimeException("Does not support converting MAP type value");        default:            throw new RuntimeException("Does not support converting unknown type value: " + beamFieldTypeName);    }}
public void beam_f29421_0()
{    destinations = Maps.newHashMap();}
public void beam_f29422_0(ProcessContext context)
{    dynamicDestinations.setSideInputAccessorFromProcessContext(context);    context.output(KV.of(destinations.computeIfAbsent(context.element().getKey(), dest -> getTableDestination(context, dest)), context.element().getValue()));}
public Coder<DestinationT> beam_f29431_0()
{    return null;}
 Coder<DestinationT> beam_f29432_0(CoderRegistry registry) throws CannotProvideCoderException
{    Coder<DestinationT> destinationCoder = getDestinationCoder();    if (destinationCoder != null) {        return destinationCoder;    }        TypeDescriptor<DestinationT> descriptor = extractFromTypeParameters(this, DynamicDestinations.class, new TypeDescriptors.TypeVariableExtractor<DynamicDestinations<T, DestinationT>, DestinationT>() {    });    try {        return registry.getCoder(descriptor);    } catch (CannotProvideCoderException e) {        throw new CannotProvideCoderException("Failed to infer coder for DestinationT from type " + descriptor + ", please provide it explicitly by overriding getDestinationCoder()", e);    }}
public TableDestination beam_f29441_0(TableDestination destination)
{    return destination;}
public Coder<TableDestination> beam_f29442_0()
{    if (clusteringEnabled) {        return TableDestinationCoderV3.of();    } else {        return TableDestinationCoderV2.of();    }}
public TableSchema beam_f29451_0(DestinationT destination)
{    String jsonSchema = this.jsonSchema.get();    checkArgument(jsonSchema != null, "jsonSchema can not be null");    return BigQueryHelpers.fromJsonString(jsonSchema, TableSchema.class);}
public String beam_f29452_0()
{    return MoreObjects.toStringHelper(this).add("inner", inner).add("jsonSchema", jsonSchema).toString();}
private static boolean beam_f29461_0(Sleeper sleeper, BackOff backoff) throws InterruptedException
{    try {        return BackOffUtils.next(sleeper, backoff);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public TableDestination beam_f29462_0(DestinationT destination)
{    TableDestination wrappedDestination = super.getTable(destination);    Table existingTable = getBigQueryTable(wrappedDestination.getTableReference());    if (existingTable == null) {        return wrappedDestination;    } else {        return new TableDestination(wrappedDestination.getTableSpec(), existingTable.getDescription(), existingTable.getTimePartitioning());    }}
public PCollection<T> beam_f29471_0(PCollection<T> input)
{    TupleTag<T> mainOutput = new TupleTag<>();    TupleTag<Void> cleanupSignal = new TupleTag<>();    PCollectionTuple outputs = input.apply(ParDo.of(new IdentityFn<T>()).withOutputTags(mainOutput, TupleTagList.of(cleanupSignal)));    PCollectionView<Iterable<Void>> cleanupSignalView = outputs.get(cleanupSignal).setCoder(VoidCoder.of()).apply(View.asIterable());    input.getPipeline().apply("Create(CleanupOperation)", Create.of(cleanupOperation)).apply("Cleanup", ParDo.of(new DoFn<CleanupOperation, Void>() {        @ProcessElement        public void processElement(ProcessContext c) throws Exception {            c.element().cleanup(new ContextContainer(c, jobIdSideInput));        }    }).withSideInputs(jobIdSideInput, cleanupSignalView));    return outputs.get(mainOutput).setCoder(input.getCoder());}
public void beam_f29472_0(ProcessContext c) throws Exception
{    c.element().cleanup(new ContextContainer(c, jobIdSideInput));}
public void beam_f29481_0(ProcessContext c)
{    c.output(c.sideInput(view));}
public GenericRecord beam_f29482_0()
{    return record;}
public void beam_f29491_0()
{    tableRows = new HashMap<>();    uniqueIdsForTableRows = new HashMap<>();}
public void beam_f29492_0(@Element KV<ShardedKey<String>, TableRowInfo<ElementT>> element, @Timestamp Instant timestamp, BoundedWindow window, PaneInfo pane)
{    String tableSpec = element.getKey().getKey();    List<ValueInSingleWindow<TableRow>> rows = BigQueryHelpers.getOrCreateMapListValue(tableRows, tableSpec);    List<String> uniqueIds = BigQueryHelpers.getOrCreateMapListValue(uniqueIdsForTableRows, tableSpec);    TableRow tableRow = toTableRow.apply(element.getValue().tableRow);    rows.add(ValueInSingleWindow.of(tableRow, timestamp, window, pane));    uniqueIds.add(element.getValue().uniqueId);}
 StreamingWriteTables<ElementT> beam_f29501_0(SerializableFunction<ElementT, TableRow> toTableRow)
{    return new StreamingWriteTables<>(bigQueryServices, retryPolicy, extendedErrorInfo, skipInvalidRows, ignoreUnknownValues, elementCoder, toTableRow);}
public WriteResult beam_f29502_0(PCollection<KV<TableDestination, ElementT>> input)
{    if (extendedErrorInfo) {        TupleTag<BigQueryInsertError> failedInsertsTag = new TupleTag<>(FAILED_INSERTS_TAG_ID);        PCollection<BigQueryInsertError> failedInserts = writeAndGetErrors(input, failedInsertsTag, BigQueryInsertErrorCoder.of(), ErrorContainer.BIG_QUERY_INSERT_ERROR_ERROR_CONTAINER);        return WriteResult.withExtendedErrors(input.getPipeline(), failedInsertsTag, failedInserts);    } else {        TupleTag<TableRow> failedInsertsTag = new TupleTag<>(FAILED_INSERTS_TAG_ID);        PCollection<TableRow> failedInserts = writeAndGetErrors(input, failedInsertsTag, TableRowJsonCoder.of(), ErrorContainer.TABLE_ROW_ERROR_CONTAINER);        return WriteResult.in(input.getPipeline(), failedInsertsTag, failedInserts);    }}
public String beam_f29511_0()
{    return tableDescription;}
public String beam_f29512_0()
{    String toString = "tableSpec: " + tableSpec;    if (tableDescription != null) {        toString += " tableDescription: " + tableDescription;    }    return toString;}
public static TableDestinationCoderV3 beam_f29523_0()
{    return INSTANCE;}
public void beam_f29524_0(TableDestination value, OutputStream outStream) throws IOException
{    TableDestinationCoder.of().encode(value, outStream);    timePartitioningCoder.encode(value.getJsonTimePartitioning(), outStream);    clusteringCoder.encode(value.getJsonClustering(), outStream);}
public void beam_f29534_0(TableRow value, OutputStream outStream) throws IOException
{    encode(value, outStream, Context.NESTED);}
public void beam_f29535_0(TableRow value, OutputStream outStream, Context context) throws IOException
{    String strValue = MAPPER.writeValueAsString(value);    StringUtf8Coder.of().encode(strValue, outStream, context);}
 Result beam_f29544_0()
{    checkState(isClosed, "Not yet closed");    return new Result(resourceId, out.getCount());}
public void beam_f29545_0()
{    randomUUID = UUID.randomUUID().toString();}
public String beam_f29554_0()
{    return String.format("%s:%s.%s", table.getTableReference().getProjectId(), table.getTableReference().getDatasetId(), table.getTableReference().getTableId());}
public TableReference beam_f29555_0()
{    return table.getTableReference();}
private void beam_f29564_0(long l)
{    try {        Thread.sleep(l);    } catch (InterruptedException e) {        throw new RuntimeException(e);    }}
public boolean beam_f29565_0(Object other)
{    if (other instanceof Result) {        Result<DestinationT> o = (Result<DestinationT>) other;        return Objects.equals(this.filename, o.filename) && Objects.equals(this.fileByteSize, o.fileByteSize) && Objects.equals(this.destination, o.destination);    }    return false;}
public void beam_f29575_0(ProcessContext c, @Element KV<DestinationT, ElementT> element, BoundedWindow window) throws Exception
{    String tempFilePrefix = c.sideInput(tempFilePrefixView);    DestinationT destination = c.element().getKey();    TableRowWriter writer;    if (writers.containsKey(destination)) {        writer = writers.get(destination);    } else {                if (writers.size() <= maxNumWritersPerBundle) {            writer = createAndInsertWriter(destination, tempFilePrefix, window);        } else {                                    c.output(unwrittenRecordsTag, KV.of(ShardedKey.of(destination, (++spilledShardNumber) % SPILLED_RECORD_SHARDING_FACTOR), element.getValue()));            return;        }    }    if (writer.getByteSize() > maxFileSize) {                writer.close();        TableRowWriter.Result result = writer.getResult();        c.output(new Result<>(result.resourceId.toString(), result.byteSize, destination));        writer = createAndInsertWriter(destination, tempFilePrefix, window);    }    try {        writer.write(toRowFunction.apply(element.getValue()));    } catch (Exception e) {                try {            writer.close();                } catch (Exception closeException) {                        e.addSuppressed(closeException);        }        throw e;    }}
public void beam_f29576_0(FinishBundleContext c) throws Exception
{    List<Exception> exceptionList = Lists.newArrayList();    for (TableRowWriter writer : writers.values()) {        try {            writer.close();        } catch (Exception e) {            exceptionList.add(e);        }    }    if (!exceptionList.isEmpty()) {        Exception e = new IOException("Failed to close some writers");        for (Exception thrown : exceptionList) {            e.addSuppressed(thrown);        }        throw e;    }    for (Map.Entry<DestinationT, TableRowWriter> entry : writers.entrySet()) {        try {            DestinationT destination = entry.getKey();            TableRowWriter writer = entry.getValue();            TableRowWriter.Result result = writer.getResult();            c.output(new Result<>(result.resourceId.toString(), result.byteSize, destination), writerWindows.get(destination).maxTimestamp(), writerWindows.get(destination));        } catch (Exception e) {            exceptionList.add(e);        }    }    writers.clear();}
 boolean beam_f29585_0(int numFiles, long numBytes)
{    if (filenames.isEmpty()) {        return true;    }    return this.numFiles + numFiles <= maxNumFiles && this.byteSize + numBytes <= maxSizeBytes;}
private static DestinationData beam_f29586_0(int maxNumFiles, long maxSizeBytes)
{    DestinationData destinationData = new DestinationData();        destinationData.partitions.add(new PartitionData(maxNumFiles, maxSizeBytes));    return destinationData;}
private BigQueryHelpers.PendingJob beam_f29595_1(JobService jobService, DatasetService datasetService, String jobIdPrefix, TableReference ref, List<TableReference> tempTables, WriteDisposition writeDisposition, CreateDisposition createDisposition, String kmsKey)
{    JobConfigurationTableCopy copyConfig = new JobConfigurationTableCopy().setSourceTables(tempTables).setDestinationTable(ref).setWriteDisposition(writeDisposition.name()).setCreateDisposition(createDisposition.name());    if (kmsKey != null) {        copyConfig.setDestinationEncryptionConfiguration(new EncryptionConfiguration().setKmsKeyName(kmsKey));    }    String bqLocation = BigQueryHelpers.getDatasetLocation(datasetService, ref.getProjectId(), ref.getDatasetId());    String projectId = ref.getProjectId();    BigQueryHelpers.PendingJob retryJob = new BigQueryHelpers.PendingJob(jobId -> {        JobReference jobRef = new JobReference().setProjectId(projectId).setJobId(jobId.getJobId()).setLocation(bqLocation);                try {            jobService.startCopyJob(jobRef, copyConfig);        } catch (IOException | InterruptedException e) {                        throw new RuntimeException(e);        }        return null;    },     jobId -> {        JobReference jobRef = new JobReference().setProjectId(projectId).setJobId(jobId.getJobId()).setLocation(bqLocation);        try {            return jobService.pollJob(jobRef, BatchLoads.LOAD_JOB_POLL_MAX_RETRIES);        } catch (InterruptedException e) {            throw new RuntimeException(e);        }    },     jobId -> {        JobReference jobRef = new JobReference().setProjectId(projectId).setJobId(jobId.getJobId()).setLocation(bqLocation);        try {            return jobService.getJob(jobRef);        } catch (InterruptedException | IOException e) {            throw new RuntimeException(e);        }    }, maxRetryJobs, jobIdPrefix);    return retryJob;}
 static void beam_f29596_1(DatasetService tableService, List<TableReference> tempTables)
{    for (TableReference tableRef : tempTables) {        try {                        tableService.deleteTable(tableRef);        } catch (Exception e) {                    }    }}
public void beam_f29606_0(ProcessContext c, BoundedWindow window) throws Exception
{    dynamicDestinations.setSideInputAccessorFromProcessContext(c);    DestinationT destination = c.element().getKey().getKey();    TableSchema tableSchema;    if (firstPaneCreateDisposition == CreateDisposition.CREATE_NEVER) {        tableSchema = null;    } else if (jsonSchemas.containsKey(destination)) {        tableSchema = BigQueryHelpers.fromJsonString(jsonSchemas.get(destination), TableSchema.class);    } else {        tableSchema = dynamicDestinations.getSchema(destination);        checkArgument(tableSchema != null, "Unless create disposition is %s, a schema must be specified, i.e. " + "DynamicDestinations.getSchema() may not return null. " + "However, create disposition is %s, and %s returned null for destination %s", CreateDisposition.CREATE_NEVER, firstPaneCreateDisposition, dynamicDestinations, destination);        jsonSchemas.put(destination, BigQueryHelpers.toJsonString(tableSchema));    }    TableDestination tableDestination = dynamicDestinations.getTable(destination);    checkArgument(tableDestination != null, "DynamicDestinations.getTable() may not return null, " + "but %s returned null for destination %s", dynamicDestinations, destination);    boolean destinationCoderSupportsClustering = !(dynamicDestinations.getDestinationCoder() instanceof TableDestinationCoderV2);    checkArgument(tableDestination.getClustering() == null || destinationCoderSupportsClustering, "DynamicDestinations.getTable() may only return destinations with clustering configured" + " if a destination coder is supplied that supports clustering, but %s is configured" + " to use TableDestinationCoderV2. Set withClustering() on BigQueryIO.write() and, " + " if you provided a custom DynamicDestinations instance, override" + " getDestinationCoder() to return TableDestinationCoderV3.", dynamicDestinations);    TableReference tableReference = tableDestination.getTableReference();    if (Strings.isNullOrEmpty(tableReference.getProjectId())) {        tableReference.setProjectId(c.getPipelineOptions().as(BigQueryOptions.class).getProject());        tableDestination = tableDestination.withTableReference(tableReference);    }    Integer partition = c.element().getKey().getShardNumber();    List<String> partitionFiles = Lists.newArrayList(c.element().getValue());    String jobIdPrefix = BigQueryHelpers.createJobId(c.sideInput(loadJobIdPrefixView), tableDestination, partition, c.pane().getIndex());    if (tempTable) {                tableReference.setTableId(jobIdPrefix);    }    WriteDisposition writeDisposition = firstPaneWriteDisposition;    CreateDisposition createDisposition = firstPaneCreateDisposition;    if (c.pane().getIndex() > 0 && !tempTable) {                        writeDisposition = WriteDisposition.WRITE_APPEND;        createDisposition = CreateDisposition.CREATE_NEVER;    } else if (tempTable) {                        writeDisposition = WriteDisposition.WRITE_TRUNCATE;        createDisposition = CreateDisposition.CREATE_IF_NEEDED;    }    BigQueryHelpers.PendingJob retryJob = startLoad(bqServices.getJobService(c.getPipelineOptions().as(BigQueryOptions.class)), bqServices.getDatasetService(c.getPipelineOptions().as(BigQueryOptions.class)), jobIdPrefix, tableReference, tableDestination.getTimePartitioning(), tableDestination.getClustering(), tableSchema, partitionFiles, writeDisposition, createDisposition);    pendingJobs.add(new PendingJobData(window, retryJob, partitionFiles, tableDestination, tableReference));}
public void beam_f29607_0(FinishBundleContext c) throws Exception
{    DatasetService datasetService = bqServices.getDatasetService(c.getPipelineOptions().as(BigQueryOptions.class));    PendingJobManager jobManager = new PendingJobManager();    for (PendingJobData pendingJob : pendingJobs) {        jobManager = jobManager.addPendingJob(pendingJob.retryJob,         j -> {            try {                if (pendingJob.tableDestination.getTableDescription() != null) {                    TableReference ref = pendingJob.tableReference;                    datasetService.patchTableDescription(ref.clone().setTableId(BigQueryHelpers.stripPartitionDecorator(ref.getTableId())), pendingJob.tableDestination.getTableDescription());                }                c.output(mainOutputTag, KV.of(pendingJob.tableDestination, BigQueryHelpers.toJsonString(pendingJob.tableReference)), pendingJob.window.maxTimestamp(), pendingJob.window);                for (String file : pendingJob.partitionFiles) {                    c.output(temporaryFilesTag, file, pendingJob.window.maxTimestamp(), pendingJob.window);                }                return null;            } catch (IOException | InterruptedException e) {                return e;            }        });    }    jobManager.waitForDone();}
 BigtableConfig beam_f29616_0(BigtableOptions options)
{    checkArgument(options != null, "Bigtable options can not be null");    return toBuilder().setBigtableOptions(options).build();}
 BigtableConfig beam_f29617_0(SerializableFunction<BigtableOptions.Builder, BigtableOptions.Builder> configurator)
{    checkArgument(configurator != null, "configurator can not be null");    return toBuilder().setBigtableOptionsConfigurator(configurator).build();}
public static Read beam_f29626_0()
{    return Read.create();}
public static Write beam_f29627_0()
{    return Write.create();}
public Read beam_f29636_0(String tableId)
{    return withTableId(ValueProvider.StaticValueProvider.of(tableId));}
public Read beam_f29637_0(BigtableOptions options)
{    checkArgument(options != null, "options can not be null");    return withBigtableOptions(options.toBuilder());}
public void beam_f29646_0(PipelineOptions options)
{    validateTableExists(getBigtableConfig(), options);}
public void beam_f29647_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    getBigtableConfig().populateDisplayData(builder);    List<ByteKeyRange> keyRanges = getKeyRanges();    for (int i = 0; i < keyRanges.size() && i < 5; i++) {        builder.addIfNotDefault(DisplayData.item("keyRange " + i, keyRanges.get(i).toString()), ByteKeyRange.ALL_KEYS.toString());    }    if (getRowFilter() != null) {        builder.add(DisplayData.item("rowFilter", getRowFilter().toString()).withLabel("Table Row Filter"));    }}
public Write beam_f29656_0(ValueProvider<String> tableId)
{    BigtableConfig config = getBigtableConfig();    return toBuilder().setBigtableConfig(config.withTableId(tableId)).build();}
public Write beam_f29657_0(String tableId)
{    return withTableId(ValueProvider.StaticValueProvider.of(tableId));}
public void beam_f29666_0(DisplayData.Builder builder)
{    withWriteResults().populateDisplayData(builder);}
public String beam_f29667_0()
{    return MoreObjects.toStringHelper(Write.class).add("config", getBigtableConfig()).toString();}
public void beam_f29676_0(DisplayData.Builder builder)
{    config.populateDisplayData(builder);}
private void beam_f29677_1() throws IOException
{        if (failures.isEmpty()) {        return;    }    StringBuilder logEntry = new StringBuilder();    int i = 0;    List<BigtableWriteException> suppressed = Lists.newArrayList();    for (; i < 10 && !failures.isEmpty(); ++i) {        BigtableWriteException exc = failures.remove();        logEntry.append("\n").append(exc.getMessage());        if (exc.getCause() != null) {            logEntry.append(": ").append(exc.getCause().getMessage());        }        suppressed.add(exc);    }    String message = String.format("At least %d errors occurred writing to Bigtable. First %d errors: %s", i + failures.size(), i, logEntry.toString());        IOException exception = new IOException(message);    for (BigtableWriteException e : suppressed) {        exception.addSuppressed(e);    }    throw exception;}
private static boolean beam_f29686_0(List<ByteKeyRange> ranges)
{    int index = 0;    if (ranges.size() < 2) {        return true;    }    ByteKey lastEndKey = ranges.get(index++).getEndKey();    while (index < ranges.size()) {        ByteKeyRange currentKeyRange = ranges.get(index++);        if (!lastEndKey.equals(currentKeyRange.getStartKey())) {            return false;        }        lastEndKey = currentKeyRange.getEndKey();    }    return true;}
private static List<ByteKeyRange> beam_f29687_0(List<ByteKeyRange> ranges)
{    List<ByteKeyRange> response = new ArrayList<>();    if (ranges.size() < 2) {        response.add(ranges.get(0));    } else {        response.add(ByteKeyRange.of(ranges.get(0).getStartKey(), ranges.get(ranges.size() - 1).getEndKey()));    }    return response;}
private List<BigtableSource> beam_f29696_1(long sampleSizeBytes, long desiredBundleSizeBytes, ByteKeyRange range)
{            if (sampleSizeBytes <= desiredBundleSizeBytes) {        return Collections.singletonList(this.withSingleRange(ByteKeyRange.of(range.getStartKey(), range.getEndKey())));    }    checkArgument(sampleSizeBytes > 0, "Sample size %s bytes must be greater than 0.", sampleSizeBytes);    checkArgument(desiredBundleSizeBytes > 0, "Desired bundle size %s bytes must be greater than 0.", desiredBundleSizeBytes);    int splitCount = (int) Math.ceil(((double) sampleSizeBytes) / desiredBundleSizeBytes);    List<ByteKey> splitKeys = range.split(splitCount);    ImmutableList.Builder<BigtableSource> splits = ImmutableList.builder();    Iterator<ByteKey> keys = splitKeys.iterator();    ByteKey prev = keys.next();    while (keys.hasNext()) {        ByteKey next = keys.next();        splits.add(this.withSingleRange(ByteKeyRange.of(prev, next)).withEstimatedSizeBytes(sampleSizeBytes / splitCount));        prev = next;    }    return splits.build();}
public List<ByteKeyRange> beam_f29697_0()
{    return ranges;}
public final long beam_f29706_0()
{    return rangeTracker.getSplitPointsConsumed();}
public final synchronized BigtableSource beam_f29707_1(double fraction)
{    ByteKey splitKey;    ByteKeyRange range = rangeTracker.getRange();    try {        splitKey = range.interpolateKey(fraction);    } catch (RuntimeException e) {                return null;    }        BigtableSource primary;    BigtableSource residual;    try {        primary = source.withSingleRange(ByteKeyRange.of(range.getStartKey(), splitKey));        residual = source.withSingleRange(ByteKeyRange.of(splitKey, range.getEndKey()));    } catch (RuntimeException e) {                return null;    }    if (!rangeTracker.trySplitAtPosition(splitKey)) {        return null;    }    this.source = primary;    return residual;}
public void beam_f29717_0() throws IOException
{    try {        if (bulkMutation != null) {            try {                bulkMutation.flush();            } catch (InterruptedException e) {                Thread.currentThread().interrupt();                                throw new IOException(e);            }            bulkMutation = null;        }    } finally {        if (session != null) {            session.close();            session = null;        }    }}
public CompletionStage<MutateRowResponse> beam_f29718_0(KV<ByteString, Iterable<Mutation>> record) throws IOException
{    MutateRowsRequest.Entry request = MutateRowsRequest.Entry.newBuilder().setRowKey(record.getKey()).addAllMutations(record.getValue()).build();    CompletableFuture<MutateRowResponse> result = new CompletableFuture<>();    Futures.addCallback(new VendoredListenableFutureAdapter<>(bulkMutation.add(request)), new FutureCallback<MutateRowResponse>() {        @Override        public void onSuccess(MutateRowResponse mutateRowResponse) {            result.complete(mutateRowResponse);        }        @Override        public void onFailure(Throwable throwable) {            result.completeExceptionally(throwable);        }    }, directExecutor());    return result;}
public void beam_f29727_0(BigtableWriteResult value, OutputStream outStream) throws CoderException, IOException
{    LONG_CODER.encode(value.getRowsWritten(), outStream);}
public BigtableWriteResult beam_f29728_0(InputStream inStream) throws CoderException, IOException
{    return BigtableWriteResult.create(LONG_CODER.decode(inStream));}
public boolean beam_f29737_0(long nowMsSinceEpoch)
{    double delayProbability = throttlingProbability(nowMsSinceEpoch);            allRequests.add(nowMsSinceEpoch, 1);    return (random.nextDouble() < delayProbability);}
public void beam_f29738_0(long nowMsSinceEpoch)
{    successfulRequests.add(nowMsSinceEpoch, 1);}
private static List<Query> beam_f29747_0(Query query, @Nullable String namespace, Datastore datastore, QuerySplitter querySplitter, int numSplits) throws DatastoreException
{        return querySplitter.getSplits(query, forNamespace(namespace).build(), numSplits, datastore);}
 static Query beam_f29748_1(String gql, Datastore datastore, String namespace) throws DatastoreException
{    String gqlQueryWithZeroLimit = gql + " LIMIT 0";    try {        Query translatedQuery = translateGqlQuery(gqlQueryWithZeroLimit, datastore, namespace);                return translatedQuery.toBuilder().clearLimit().build();    } catch (DatastoreException e) {                if (e.getCode() == Code.INVALID_ARGUMENT) {                                                return translateGqlQuery(gql, datastore, namespace);        } else {            throw e;        }    }}
public DatastoreV1.Read beam_f29757_0(int numQuerySplits)
{    return toBuilder().setNumQuerySplits(Math.min(Math.max(numQuerySplits, 0), NUM_QUERY_SPLITS_MAX)).build();}
public DatastoreV1.Read beam_f29758_0(String localhost)
{    return toBuilder().setLocalhost(localhost).build();}
public String beam_f29767_0()
{    return localhost;}
public void beam_f29768_0(DisplayData.Builder builder)
{    builder.addIfNotNull(DisplayData.item("projectId", getProjectValueProvider()).withLabel("ProjectId")).addIfNotNull(DisplayData.item("namespace", getNamespaceValueProvider()).withLabel("Namespace"));}
public void beam_f29777_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.include("options", options);}
public Write beam_f29778_0()
{    return new Write(null, null);}
public DeleteKey beam_f29787_0(String projectId)
{    checkArgument(projectId != null, "projectId can not be null");    return withProjectId(StaticValueProvider.of(projectId));}
public DeleteKey beam_f29788_0(String localhost)
{    checkArgument(localhost != null, "localhost can not be null");    return new DeleteKey(projectId, localhost);}
public void beam_f29797_0(StartBundleContext c)
{    datastore = datastoreFactory.getDatastore(c.getPipelineOptions(), projectId.get(), localhost);    writeBatcher.start();    if (throttler == null) {                throttler = new AdaptiveThrottler(120000, 10000, 1.25);    }}
public void beam_f29798_0(ProcessContext c) throws Exception
{    Mutation write = c.element();    int size = write.getSerializedSize();    if (mutations.size() > 0 && mutationsSize + size >= DatastoreV1.DATASTORE_BATCH_UPDATE_BYTES_LIMIT) {        flushBatch();    }    mutations.add(c.element());    mutationsSize += size;    if (mutations.size() >= writeBatcher.nextBatchSize(System.currentTimeMillis())) {        flushBatch();    }}
public Mutation beam_f29807_0(Key key)
{        checkArgument(isValidKey(key), "Keys to be deleted from the Cloud Datastore must be complete:\n%s", key);    return makeDelete(key).build();}
public void beam_f29808_0(Builder builder)
{    builder.add(DisplayData.item("deleteKeyFn", this.getClass()).withLabel("Create Delete Mutation"));}
public String beam_f29817_0()
{    return String.format("projects/%s", projectId);}
public String beam_f29818_0()
{    return projectId;}
public boolean beam_f29827_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    SubscriptionPath that = (SubscriptionPath) o;    return this.subscriptionName.equals(that.subscriptionName) && this.projectId.equals(that.projectId);}
public int beam_f29828_0()
{    return Objects.hashCode(projectId, subscriptionName);}
public String beam_f29837_0()
{    return path;}
public static TopicPath beam_f29838_0(String path)
{    return new TopicPath(path);}
public SubscriptionPath beam_f29847_0(ProjectPath project, TopicPath topic, int ackDeadlineSeconds) throws IOException
{        String subscriptionName = topic.getName() + "_beam_" + ThreadLocalRandom.current().nextLong();    SubscriptionPath subscription = PubsubClient.subscriptionPathFromName(project.getId(), subscriptionName);    createSubscription(topic, subscription, ackDeadlineSeconds);    return subscription;}
public List<CoderProvider> beam_f29848_0()
{    return ImmutableList.of(CoderProviders.forCoder(TypeDescriptor.of(PubsubMessage.class), PubsubMessageWithAttributesCoder.of()), CoderProviders.forCoder(TypeDescriptor.of(PubsubMessage.class), PubsubMessageWithMessageIdCoder.of()), CoderProviders.forCoder(TypeDescriptor.of(PubsubMessage.class), PubsubMessageWithAttributesAndMessageIdCoder.of()));}
public void beam_f29857_0(SubscriptionPath subscription, List<String> ackIds) throws IOException
{    AcknowledgeRequest request = AcknowledgeRequest.newBuilder().setSubscription(subscription.getPath()).addAllAckIds(ackIds).build();        subscriberStub().acknowledge(request);}
public void beam_f29858_0(SubscriptionPath subscription, List<String> ackIds, int deadlineSeconds) throws IOException
{    ModifyAckDeadlineRequest request = ModifyAckDeadlineRequest.newBuilder().setSubscription(subscription.getPath()).addAllAckIds(ackIds).setAckDeadlineSeconds(deadlineSeconds).build();        subscriberStub().modifyAckDeadline(request);}
private static void beam_f29867_0(String project)
{    Matcher match = PROJECT_ID_REGEXP.matcher(project);    if (!match.matches()) {        throw new IllegalArgumentException("Illegal project name specified in Pubsub subscription: " + project);    }}
private static void beam_f29868_0(String name)
{    if (name.length() < PUBSUB_NAME_MIN_LENGTH) {        throw new IllegalArgumentException("Pubsub object name is shorter than 3 characters: " + name);    }    if (name.length() > PUBSUB_NAME_MAX_LENGTH) {        throw new IllegalArgumentException("Pubsub object name is longer than 255 characters: " + name);    }    if (name.startsWith("goog")) {        throw new IllegalArgumentException("Pubsub object name cannot start with goog: " + name);    }    Matcher match = PUBSUB_NAME_REGEXP.matcher(name);    if (!match.matches()) {        throw new IllegalArgumentException("Illegal Pubsub object name specified: " + name + " Please see Javadoc for naming rules.");    }}
public PubsubTopic beam_f29877_0(String from)
{    return PubsubTopic.fromPath(from);}
public TopicPath beam_f29878_0(PubsubTopic from)
{    return PubsubClient.topicPathFromName(from.project, from.topic);}
public static Read<PubsubMessage> beam_f29887_0()
{    return new AutoValue_PubsubIO_Read.Builder<PubsubMessage>().setPubsubClientFactory(FACTORY).setCoder(PubsubMessageWithMessageIdCoder.of()).setParseFn(new IdentityMessageFn()).setNeedsAttributes(false).setNeedsMessageId(true).build();}
public static Read<PubsubMessage> beam_f29888_0()
{    return new AutoValue_PubsubIO_Read.Builder<PubsubMessage>().setPubsubClientFactory(FACTORY).setCoder(PubsubMessageWithAttributesCoder.of()).setParseFn(new IdentityMessageFn()).setNeedsAttributes(true).setNeedsMessageId(false).build();}
public static Write<PubsubMessage> beam_f29897_0()
{    return PubsubIO.<PubsubMessage>write().withFormatFn(new IdentityMessageFn());}
public static Write<String> beam_f29898_0()
{    return PubsubIO.<String>write().withFormatFn(new FormatPayloadAsUtf8());}
public Read<T> beam_f29907_0(String idAttribute)
{    return toBuilder().setIdAttribute(idAttribute).build();}
public Read<T> beam_f29908_0(Coder<T> coder, SimpleFunction<PubsubMessage, T> parseFn)
{    return toBuilder().setCoder(coder).setParseFn(parseFn).build();}
public Write<T> beam_f29917_0(String timestampAttribute)
{    return toBuilder().setTimestampAttribute(timestampAttribute).build();}
public Write<T> beam_f29918_0(String idAttribute)
{    return toBuilder().setIdAttribute(idAttribute).build();}
public String beam_f29927_0(PubsubMessage input)
{    return new String(input.getPayload(), StandardCharsets.UTF_8);}
public T beam_f29928_0(PubsubMessage input)
{    try {        return CoderUtils.decodeFromByteArray(coder, input.getPayload());    } catch (CoderException e) {        throw new RuntimeException("Could not decode Pubsub message", e);    }}
public List<IncomingMessage> beam_f29938_0(long requestTimeMsSinceEpoch, SubscriptionPath subscription, int batchSize, boolean returnImmediately) throws IOException
{    PullRequest request = new PullRequest().setReturnImmediately(returnImmediately).setMaxMessages(batchSize);    PullResponse response = pubsub.projects().subscriptions().pull(subscription.getPath(), request).execute();    if (response.getReceivedMessages() == null || response.getReceivedMessages().isEmpty()) {        return ImmutableList.of();    }    List<IncomingMessage> incomingMessages = new ArrayList<>(response.getReceivedMessages().size());    for (ReceivedMessage message : response.getReceivedMessages()) {        PubsubMessage pubsubMessage = message.getMessage();        @Nullable        Map<String, String> attributes = pubsubMessage.getAttributes();                byte[] elementBytes = pubsubMessage.decodeData();        if (elementBytes == null) {            elementBytes = new byte[0];        }                long timestampMsSinceEpoch = extractTimestamp(timestampAttribute, message.getMessage().getPublishTime(), attributes);                String ackId = message.getAckId();        checkState(!Strings.isNullOrEmpty(ackId));                @Nullable        String recordId = null;        if (idAttribute != null && attributes != null) {            recordId = attributes.get(idAttribute);        }        if (Strings.isNullOrEmpty(recordId)) {                        recordId = pubsubMessage.getMessageId();        }        incomingMessages.add(new IncomingMessage(elementBytes, attributes, timestampMsSinceEpoch, requestTimeMsSinceEpoch, ackId, recordId));    }    return incomingMessages;}
public void beam_f29939_0(SubscriptionPath subscription, List<String> ackIds) throws IOException
{    AcknowledgeRequest request = new AcknowledgeRequest().setAckIds(ackIds);    pubsub.projects().subscriptions().acknowledge(subscription.getPath(), request).execute();}
public boolean beam_f29948_0()
{    return false;}
public byte[] beam_f29949_0()
{    return message;}
public static Coder<PubsubMessage> beam_f29958_0(TypeDescriptor<PubsubMessage> ignored)
{    return of();}
public static PubsubMessageWithAttributesAndMessageIdCoder beam_f29959_0()
{    return new PubsubMessageWithAttributesAndMessageIdCoder();}
public static PubsubMessageWithMessageIdCoder beam_f29968_0()
{    return new PubsubMessageWithMessageIdCoder();}
public void beam_f29969_0(PubsubMessage value, OutputStream outStream) throws IOException
{    PAYLOAD_CODER.encode(value.getPayload(), outStream);    MESSAGE_ID_CODER.encode(value.getMessageId(), outStream);}
public void beam_f29978_0()
{    synchronized (STATE) {        checkState(STATE.isActive, "No test still in flight");        checkState(STATE.remainingPendingIncomingMessages.isEmpty(), "Still waiting for %s messages to be pulled", STATE.remainingPendingIncomingMessages.size());        checkState(STATE.pendingAckIncomingMessages.isEmpty(), "Still waiting for %s messages to be ACKed", STATE.pendingAckIncomingMessages.size());        checkState(STATE.ackDeadline.isEmpty(), "Still waiting for %s messages to be ACKed", STATE.ackDeadline.size());        STATE.isActive = false;        STATE.remainingPendingIncomingMessages = null;        STATE.pendingAckIncomingMessages = null;        STATE.ackDeadline = null;    }}
public static PubsubTestClientFactory beam_f29979_0()
{    return new PubsubTestClientFactory() {        int numCalls = 0;        @Override        public void close() throws IOException {            checkState(numCalls == 1, "Expected exactly one subscription to be created, got %s", numCalls);        }        @Override        public PubsubClient newClient(@Nullable String timestampAttribute, @Nullable String idAttribute, PubsubOptions options) throws IOException {            return new PubsubTestClient() {                @Override                public void createSubscription(TopicPath topic, SubscriptionPath subscription, int ackDeadlineSeconds) throws IOException {                    checkState(numCalls == 0, "Expected at most one subscription to be created");                    numCalls++;                }            };        }        @Override        public String getKind() {            return "CreateSubscriptionTest";        }    };}
public List<IncomingMessage> beam_f29989_0(long requestTimeMsSinceEpoch, SubscriptionPath subscription, int batchSize, boolean returnImmediately) throws IOException
{    synchronized (STATE) {        checkState(inPullMode(), "Can only pull in pull mode");        long now = STATE.clock.currentTimeMillis();        checkState(requestTimeMsSinceEpoch == now, "Simulated time %s does not match request time %s", now, requestTimeMsSinceEpoch);        checkState(subscription.equals(STATE.expectedSubscription), "Subscription %s does not match expected %s", subscription, STATE.expectedSubscription);        checkState(returnImmediately, "Pull only supported if returning immediately");        List<IncomingMessage> incomingMessages = new ArrayList<>();        Iterator<IncomingMessage> pendItr = STATE.remainingPendingIncomingMessages.iterator();        while (pendItr.hasNext()) {            IncomingMessage incomingMessage = pendItr.next();            pendItr.remove();            IncomingMessage incomingMessageWithRequestTime = incomingMessage.withRequestTime(requestTimeMsSinceEpoch);            incomingMessages.add(incomingMessageWithRequestTime);            STATE.pendingAckIncomingMessages.put(incomingMessageWithRequestTime.ackId, incomingMessageWithRequestTime);            STATE.ackDeadline.put(incomingMessageWithRequestTime.ackId, requestTimeMsSinceEpoch + STATE.ackTimeoutSec * 1000);            if (incomingMessages.size() >= batchSize) {                break;            }        }        return incomingMessages;    }}
public void beam_f29990_0(SubscriptionPath subscription, List<String> ackIds) throws IOException
{    synchronized (STATE) {        checkState(inPullMode(), "Can only acknowledge in pull mode");        checkState(subscription.equals(STATE.expectedSubscription), "Subscription %s does not match expected %s", subscription, STATE.expectedSubscription);        for (String ackId : ackIds) {            checkState(STATE.ackDeadline.remove(ackId) != null, "No message with ACK id %s is waiting for an ACK", ackId);            checkState(STATE.pendingAckIncomingMessages.remove(ackId) != null, "No message with ACK id %s is waiting for an ACK", ackId);        }    }}
public boolean beam_f29999_0()
{    synchronized (STATE) {        checkState(inPullMode(), "Can only check EOF in pull mode");        return STATE.remainingPendingIncomingMessages.isEmpty();    }}
public void beam_f30000_0(OutgoingMessage value, OutputStream outStream) throws CoderException, IOException
{    ByteArrayCoder.of().encode(value.elementBytes, outStream);    ATTRIBUTES_CODER.encode(value.attributes, outStream);    BigEndianLongCoder.of().encode(value.timestampMsSinceEpoch, outStream);    RECORD_ID_CODER.encode(value.recordId, outStream);}
public TopicPath beam_f30009_0()
{    return topic.get();}
public ValueProvider<TopicPath> beam_f30010_0()
{    return topic;}
public void beam_f30019_0() throws IOException
{    checkState(reader != null && safeToAckIds != null, "Cannot finalize a restored checkpoint");        try {        int n = safeToAckIds.size();        List<String> batchSafeToAckIds = new ArrayList<>(Math.min(n, ACK_BATCH_SIZE));        for (String ackId : safeToAckIds) {            batchSafeToAckIds.add(ackId);            if (batchSafeToAckIds.size() >= ACK_BATCH_SIZE) {                reader.ackBatch(batchSafeToAckIds);                n -= batchSafeToAckIds.size();                                batchSafeToAckIds = new ArrayList<>(Math.min(n, ACK_BATCH_SIZE));            }        }        if (!batchSafeToAckIds.isEmpty()) {            reader.ackBatch(batchSafeToAckIds);        }    } finally {        int remainingInFlight = reader.numInFlightCheckpoints.decrementAndGet();        checkState(remainingInFlight >= 0, "Miscounted in-flight checkpoints");        reader.maybeCloseClient();        reader = null;        safeToAckIds = null;    }}
private static long beam_f30020_0(PubsubReader reader)
{    if (reader.outer.outer.clock == null) {        return System.currentTimeMillis();    } else {        return reader.outer.outer.clock.currentTimeMillis();    }}
private void beam_f30029_0(long nowMsSinceEpoch, List<String> ackIds) throws IOException
{    int extensionSec = (ackTimeoutMs * ACK_EXTENSION_PCT) / (100 * 1000);    pubsubClient.get().modifyAckDeadline(subscription, ackIds, extensionSec);    numExtendedDeadlines.add(nowMsSinceEpoch, ackIds.size());}
private long beam_f30030_0()
{    if (outer.outer.clock == null) {        return System.currentTimeMillis();    } else {        return outer.outer.clock.currentTimeMillis();    }}
public byte[] beam_f30039_0() throws NoSuchElementException
{    if (current == null) {        throw new NoSuchElementException();    }    return current.recordId.getBytes(StandardCharsets.UTF_8);}
public void beam_f30040_0() throws IOException
{    active.set(false);    maybeCloseClient();}
public Coder<PubsubMessage> beam_f30049_0()
{    if (outer.getNeedsMessageId()) {        return outer.getNeedsAttributes() ? PubsubMessageWithAttributesAndMessageIdCoder.of() : PubsubMessageWithMessageIdCoder.of();    } else {        return outer.getNeedsAttributes() ? PubsubMessageWithAttributesCoder.of() : PubsubMessagePayloadOnlyCoder.of();    }}
public boolean beam_f30051_0()
{        return true;}
public String beam_f30060_0()
{    return idAttribute;}
public boolean beam_f30061_0()
{    return needsAttributes;}
 static String beam_f30070_0(Description description, String name) throws IOException
{    StringBuilder topicName = new StringBuilder(TOPIC_PREFIX);    if (description.getClassName() != null) {        try {            topicName.append(Class.forName(description.getClassName()).getSimpleName()).append("-");        } catch (ClassNotFoundException e) {            throw new RuntimeException(e);        }    }    if (description.getMethodName() != null) {        topicName.append(description.getMethodName()).append("-");    }    DATETIME_FORMAT.printTo(topicName, Instant.now());    return topicName.toString() + "-" + name + "-" + String.valueOf(ThreadLocalRandom.current().nextLong());}
public TopicPath beam_f30071_0()
{    return eventsTopicPath;}
private void beam_f30080_0() throws IOException
{    if (pubsub == null) {        return;    }    try {        if (resultTopicPath != null) {            pubsub.deleteTopic(resultTopicPath);        }    } finally {        pubsub.close();        pubsub = null;        resultTopicPath = null;    }}
public PTransform<PBegin, PDone> beam_f30081_0()
{    return new PublishStart(startTopicPath);}
public void beam_f30090_0(ProcessContext context, @StateId(SEEN_EVENTS) BagState<T> seenEvents)
{    seenEvents.add(context.element().getValue());    ImmutableSet<T> eventsSoFar = ImmutableSet.copyOf(seenEvents.read());        try {        if (successPredicate.apply(eventsSoFar)) {            context.output("SUCCESS");        }    } catch (Throwable e) {        context.output("FAILURE: " + e.getMessage());    }}
public static BatchSpannerRead beam_f30091_0(SpannerConfig spannerConfig, PCollectionView<Transaction> txView, TimestampBound timestampBound)
{    return new AutoValue_BatchSpannerRead(spannerConfig, txView, timestampBound);}
public void beam_f30100_0() throws Exception
{    spannerAccessor = config.getSpannerConfig().connectToSpanner();}
public void beam_f30101_0() throws Exception
{    spannerAccessor.close();}
public int beam_f30110_0()
{    return Objects.hashCode(mutations);}
public String beam_f30111_0()
{    StringBuilder sb = new StringBuilder("MutationGroup(");    sb.append(mutations);    sb.append(")");    return sb.toString();}
private static int beam_f30120_0(Date date)
{    MutableDateTime jodaDate = new MutableDateTime();    jodaDate.setDate(date.getYear(), date.getMonth(), date.getDayOfMonth());    return Days.daysBetween(MIN_DATE, jodaDate).getDays();}
 static long beam_f30121_0(Mutation m)
{    if (m.getOperation() == Mutation.Op.DELETE) {        return sizeOf(m.getKeySet());    }    long result = 0;    for (Value v : m.getValues()) {        switch(v.getType().getCode()) {            case ARRAY:                result += estimateArrayValue(v);                break;            case STRUCT:                throw new IllegalArgumentException("Structs are not supported in mutation.");            default:                result += estimatePrimitiveValue(v);        }    }    return result;}
public PCollection<Struct> beam_f30130_0(PCollection<ReadOperation> input)
{    PCollectionView<Transaction> txView = getTxView();    if (txView == null) {        Pipeline begin = input.getPipeline();        SpannerIO.CreateTransaction createTx = SpannerIO.createTransaction().withSpannerConfig(getSpannerConfig()).withTimestampBound(getTimestampBound());        txView = begin.apply(createTx);    }    return input.apply("Naive read from Cloud Spanner", ParDo.of(new NaiveSpannerReadFn(getSpannerConfig(), txView)).withSideInputs(txView));}
public void beam_f30131_0() throws Exception
{    spannerAccessor = config.connectToSpanner();}
private void beam_f30140_0(boolean invert, byte[] src, int srcPos, byte[] dest, int destPos, int length)
{    System.arraycopy(src, srcPos, dest, destPos, length);    if (invert) {        for (int i = destPos; i < destPos + length; i++) {            dest[i] = (byte) ~dest[i];        }    }}
public void beam_f30141_0(long value)
{                    byte[] bufer = new byte[9];    int len = 0;    while (value != 0) {        len++;        bufer[9 - len] = (byte) (value & 0xff);        value >>>= 8;    }    bufer[9 - len - 1] = (byte) len;    len++;    byte[] encodedArray = new byte[len];    System.arraycopy(bufer, 9 - len, encodedArray, 0, len);    encodedArrays.add(encodedArray);}
public void beam_f30150_0()
{    writeTrailingBytes(INFINITY_ENCODED_DECREASING);}
public void beam_f30151_0(byte[] value)
{    if ((value == null) || (value.length == 0)) {        throw new IllegalArgumentException("Value cannot be null or have 0 elements");    }    encodedArrays.add(value);}
public boolean beam_f30160_0()
{    return readInfinityInternal(INFINITY_ENCODED_DECREASING);}
private boolean beam_f30161_0(byte[] codes)
{    if (encodedArrays.isEmpty() || ((encodedArrays.get(0)).length - firstArrayPosition < 1)) {        throw new IllegalArgumentException("Invalid encoded byte array");    }    byte[] store = encodedArrays.get(0);    if (store.length - firstArrayPosition < 2) {        return false;    }    if ((store[firstArrayPosition] == codes[0]) && (store[firstArrayPosition + 1] == codes[1])) {        if ((store.length - firstArrayPosition - 2) == 0) {                        encodedArrays.remove(0);            firstArrayPosition = 0;        } else {            firstArrayPosition = firstArrayPosition + 2;        }        return true;    } else {        return false;    }}
public ReadOperation beam_f30170_0(String sql)
{    return withQuery(Statement.of(sql));}
public ReadOperation beam_f30171_0(KeySet keySet)
{    return toBuilder().setKeySet(keySet).build();}
public BatchClient beam_f30180_0()
{    return batchClient;}
public DatabaseAdminClient beam_f30181_0()
{    return databaseAdminClient;}
public SpannerConfig beam_f30190_0(String instanceId)
{    return withInstanceId(ValueProvider.StaticValueProvider.of(instanceId));}
public SpannerConfig beam_f30191_0(ValueProvider<String> databaseId)
{    return toBuilder().setDatabaseId(databaseId).build();}
public ReadAll beam_f30200_0(SpannerConfig spannerConfig)
{    return toBuilder().setSpannerConfig(spannerConfig).build();}
public ReadAll beam_f30201_0(String projectId)
{    return withProjectId(ValueProvider.StaticValueProvider.of(projectId));}
public ReadAll beam_f30210_0(PCollectionView<Transaction> transaction)
{    return toBuilder().setTransaction(transaction).build();}
public ReadAll beam_f30211_0(Timestamp timestamp)
{    return withTimestampBound(TimestampBound.ofReadTimestamp(timestamp));}
public Read beam_f30220_0(String databaseId)
{    return withDatabaseId(ValueProvider.StaticValueProvider.of(databaseId));}
public Read beam_f30221_0(ValueProvider<String> databaseId)
{    SpannerConfig config = getSpannerConfig();    return withSpannerConfig(config.withDatabaseId(databaseId));}
public Read beam_f30230_0(ReadOperation operation)
{    return toBuilder().setReadOperation(operation).build();}
public Read beam_f30231_0(String... columns)
{    return withColumns(Arrays.asList(columns));}
public CreateTransaction beam_f30240_0(SpannerConfig spannerConfig)
{    return toBuilder().setSpannerConfig(spannerConfig).build();}
public CreateTransaction beam_f30241_0(String projectId)
{    return withProjectId(ValueProvider.StaticValueProvider.of(projectId));}
public CreateTransaction beam_f30250_0(TimestampBound timestampBound)
{    return toBuilder().setTimestampBound(timestampBound).build();}
public Write beam_f30251_0(SpannerConfig spannerConfig)
{    return toBuilder().setSpannerConfig(spannerConfig).build();}
 Write beam_f30260_0(ServiceFactory<Spanner, SpannerOptions> serviceFactory)
{    SpannerConfig config = getSpannerConfig();    return withSpannerConfig(config.withServiceFactory(serviceFactory));}
public WriteGrouped beam_f30261_0()
{    return new WriteGrouped(this);}
public SpannerWriteResult beam_f30270_0(PCollection<MutationGroup> input)
{        PCollection<Void> schemaSeed = input.getPipeline().apply("Create Seed", Create.of((Void) null));    if (spec.getSchemaReadySignal() != null) {                schemaSeed = schemaSeed.apply("Wait for schema", Wait.on(spec.getSchemaReadySignal()));    }    final PCollectionView<SpannerSchema> schemaView = schemaSeed.apply("Read information schema", ParDo.of(new ReadSpannerSchema(spec.getSpannerConfig()))).apply("Schema View", View.asSingleton());            PCollectionTuple filteredMutations = input.apply("To Global Window", Window.into(new GlobalWindows())).apply("Filter Unbatchable Mutations", ParDo.of(new BatchableMutationFilterFn(schemaView, UNBATCHABLE_MUTATIONS_TAG, spec.getBatchSizeBytes(), spec.getMaxNumMutations())).withSideInputs(schemaView).withOutputTags(BATCHABLE_MUTATIONS_TAG, TupleTagList.of(UNBATCHABLE_MUTATIONS_TAG)));            PCollection<Iterable<MutationGroup>> batchedMutations = filteredMutations.get(BATCHABLE_MUTATIONS_TAG).apply("Gather And Sort", ParDo.of(new GatherBundleAndSortFn(spec.getBatchSizeBytes(), spec.getMaxNumMutations(), spec.getGroupingFactor(), schemaView)).withSideInputs(schemaView)).apply("Create Batches", ParDo.of(new BatchFn(spec.getBatchSizeBytes(), spec.getMaxNumMutations(), schemaView)).withSideInputs(schemaView));        PCollectionTuple result = PCollectionList.of(filteredMutations.get(UNBATCHABLE_MUTATIONS_TAG)).and(batchedMutations).apply("Merge", Flatten.pCollections()).apply("Write mutations to Spanner", ParDo.of(new WriteToSpannerFn(spec.getSpannerConfig(), spec.getFailureMode(), FAILED_MUTATIONS_TAG)).withOutputTags(MAIN_OUT_TAG, TupleTagList.of(FAILED_MUTATIONS_TAG)));    return new SpannerWriteResult(input.getPipeline(), result.get(MAIN_OUT_TAG), result.get(FAILED_MUTATIONS_TAG), FAILED_MUTATIONS_TAG);}
 static MutationGroup beam_f30271_0(byte[] bytes)
{    ByteArrayInputStream bis = new ByteArrayInputStream(bytes);    try {        return CODER.decode(bis);    } catch (IOException e) {        throw new RuntimeException(e);    }}
public void beam_f30280_0(ProcessContext c)
{    MutationGroup mg = c.element();    if (mg.primary().getOperation() == Op.DELETE && !isPointDelete(mg.primary())) {                c.output(unbatchableMutationsTag, Arrays.asList(mg));        unBatchableMutationGroupsCounter.inc();        return;    }    SpannerSchema spannerSchema = c.sideInput(schemaView);    long groupSize = MutationSizeEstimator.sizeOf(mg);    long groupCells = MutationCellCounter.countOf(spannerSchema, mg);    if (groupSize >= batchSizeBytes || groupCells >= maxNumMutations) {        c.output(unbatchableMutationsTag, Arrays.asList(mg));        unBatchableMutationGroupsCounter.inc();    } else {        c.output(mg);        batchableMutationGroupsCounter.inc();    }}
public void beam_f30281_0() throws Exception
{    spannerAccessor = spannerConfig.connectToSpanner();}
public List<Column> beam_f30290_0(String table)
{    return columns().get(table.toLowerCase());}
public List<KeyPart> beam_f30291_0(String table)
{    return keyParts().get(table.toLowerCase());}
public PCollection<MutationGroup> beam_f30300_0()
{    return failedMutations;}
public PCollection<Void> beam_f30301_0()
{    return output;}
private QueryResponse beam_f30311_0(QueryResponse response)
{    List<TableRow> rows = response.getRows();    TableSchema schema = response.getSchema();    response.setRows(rows.stream().map(r -> getTypedTableRow(schema.getFields(), r)).collect(Collectors.toList()));    return response;}
public List<TableRow> beam_f30312_0(String query, String projectId, boolean typed) throws IOException, InterruptedException
{    Random rnd = new Random(System.currentTimeMillis());    String temporaryDatasetId = "_dataflow_temporary_dataset_" + rnd.nextInt(1000000);    String temporaryTableId = "dataflow_temporary_table_" + rnd.nextInt(1000000);    TableReference tempTableReference = new TableReference().setProjectId(projectId).setDatasetId(temporaryDatasetId).setTableId(temporaryTableId);    createNewDataset(projectId, temporaryDatasetId);    createNewTable(projectId, temporaryDatasetId, new Table().setTableReference(tempTableReference));    JobConfigurationQuery jcQuery = new JobConfigurationQuery().setFlattenResults(false).setAllowLargeResults(true).setDestinationTable(tempTableReference).setQuery(query);    JobConfiguration jc = new JobConfiguration().setQuery(jcQuery);    Job job = new Job().setConfiguration(jc);    Job insertedJob = bqClient.jobs().insert(projectId, job).execute();    GetQueryResultsResponse qResponse;    do {        qResponse = bqClient.jobs().getQueryResults(projectId, insertedJob.getJobReference().getJobId()).execute();    } while (!qResponse.getJobComplete());    final TableSchema schema = qResponse.getSchema();    final List<TableRow> rows = qResponse.getRows();    deleteDataset(projectId, temporaryDatasetId);    return !typed ? rows : rows.stream().map(r -> getTypedTableRow(schema.getFields(), r)).collect(Collectors.toList());}
public static BigqueryMatcher beam_f30321_0(String applicationName, String projectId, String query, String expectedChecksum)
{    return new BigqueryMatcher(applicationName, projectId, query, true, expectedChecksum);}
protected boolean beam_f30322_1(PipelineResult pipelineResult)
{                try {        if (usingStandardSql) {            response = bigqueryClient.queryWithRetriesUsingStandardSql(query, this.projectId);        } else {            response = bigqueryClient.queryWithRetries(query, this.projectId);        }    } catch (IOException | InterruptedException e) {        if (e instanceof InterruptedIOException) {            Thread.currentThread().interrupt();        }        throw new RuntimeException("Failed to fetch BigQuery data.", e);    }    if (!response.getJobComplete()) {                return false;    } else {                actualChecksum = generateHash(response.getRows());                return expectedChecksum.equals(actualChecksum);    }}
public JobService beam_f30331_0(BigQueryOptions bqOptions)
{    return jobService;}
public DatasetService beam_f30332_0(BigQueryOptions bqOptions)
{    return datasetService;}
public Table beam_f30342_0(TableReference tableRef, @Nullable List<String> selectedFields) throws InterruptedException, IOException
{    synchronized (tables) {        Map<String, TableContainer> dataset = tables.get(tableRef.getProjectId(), tableRef.getDatasetId());        if (dataset == null) {            throwNotFound("Tried to get a dataset %s:%s, but no such dataset was set", tableRef.getProjectId(), tableRef.getDatasetId());        }        TableContainer tableContainer = dataset.get(tableRef.getTableId());        return tableContainer == null ? null : tableContainer.getTable();    }}
public List<TableRow> beam_f30343_0(String projectId, String datasetId, String tableId) throws InterruptedException, IOException
{    synchronized (tables) {        return getTableContainer(projectId, datasetId, tableId).getRows();    }}
public long beam_f30352_0(TableReference ref, List<TableRow> rowList, @Nullable List<String> insertIdList) throws IOException, InterruptedException
{    List<ValueInSingleWindow<TableRow>> windowedRows = Lists.newArrayList();    for (TableRow row : rowList) {        windowedRows.add(ValueInSingleWindow.of(row, GlobalWindow.TIMESTAMP_MAX_VALUE, GlobalWindow.INSTANCE, PaneInfo.ON_TIME_AND_ONLY_FIRING));    }    return insertAll(ref, windowedRows, insertIdList, InsertRetryPolicy.alwaysRetry(), null, null, false, false);}
public long beam_f30353_0(TableReference ref, List<ValueInSingleWindow<TableRow>> rowList, @Nullable List<String> insertIdList, InsertRetryPolicy retryPolicy, List<ValueInSingleWindow<T>> failedInserts, ErrorContainer<T> errorContainer, boolean skipInvalidRows, boolean ignoreUnknownValues) throws IOException, InterruptedException
{    Map<TableRow, List<TableDataInsertAllResponse.InsertErrors>> insertErrors = getInsertErrors();    synchronized (tables) {        if (insertIdList != null) {            assertEquals(rowList.size(), insertIdList.size());        } else {            insertIdList = Lists.newArrayListWithExpectedSize(rowList.size());            for (int i = 0; i < rowList.size(); ++i) {                insertIdList.add(Integer.toString(ThreadLocalRandom.current().nextInt()));            }        }        long dataSize = 0;        TableContainer tableContainer = getTableContainer(ref.getProjectId(), ref.getDatasetId(), BigQueryHelpers.stripPartitionDecorator(ref.getTableId()));        for (int i = 0; i < rowList.size(); ++i) {            TableRow row = rowList.get(i).getValue();            List<TableDataInsertAllResponse.InsertErrors> allErrors = insertErrors.get(row);            boolean shouldInsert = true;            if (allErrors != null) {                for (TableDataInsertAllResponse.InsertErrors errors : allErrors) {                    if (!retryPolicy.shouldRetry(new Context(errors))) {                        shouldInsert = false;                    }                }            }            if (shouldInsert) {                dataSize += tableContainer.addRow(row, insertIdList.get(i));            } else {                errorContainer.add(failedInserts, allErrors.get(allErrors.size() - 1), ref, rowList.get(i));            }        }        return dataSize;    }}
public int beam_f30362_0()
{    synchronized (allJobs) {        return numExtractJobCalls;    }}
public void beam_f30363_0(JobReference jobRef, JobConfigurationQuery query)
{    synchronized (allJobs) {        Job job = new Job();        job.setJobReference(jobRef);        job.setConfiguration(new JobConfiguration().setQuery(query));        job.setKind(" bigquery#job");        job.setStatus(new JobStatus().setState("PENDING"));        allJobs.put(jobRef.getProjectId(), jobRef.getJobId(), new JobInfo(job));    }}
private JobStatus beam_f30372_0(JobReference jobRef, JobConfigurationLoad load) throws InterruptedException, IOException
{    TableReference destination = load.getDestinationTable();    TableSchema schema = load.getSchema();    checkArgument(schema != null, "No schema specified");    List<ResourceId> sourceFiles = filesForLoadJobs.get(jobRef.getProjectId(), jobRef.getJobId());    WriteDisposition writeDisposition = WriteDisposition.valueOf(load.getWriteDisposition());    CreateDisposition createDisposition = CreateDisposition.valueOf(load.getCreateDisposition());    checkArgument("NEWLINE_DELIMITED_JSON".equals(load.getSourceFormat()));    Table existingTable = datasetService.getTable(destination);    if (!validateDispositions(existingTable, createDisposition, writeDisposition)) {        return new JobStatus().setState("FAILED").setErrorResult(new ErrorProto());    }    if (existingTable == null) {        TableReference strippedDestination = destination.clone().setTableId(BigQueryHelpers.stripPartitionDecorator(destination.getTableId()));        existingTable = new Table().setTableReference(strippedDestination).setSchema(schema);        if (load.getTimePartitioning() != null) {            existingTable = existingTable.setTimePartitioning(load.getTimePartitioning());        }        if (load.getClustering() != null) {            existingTable = existingTable.setClustering(load.getClustering());        }        datasetService.createTable(existingTable);    }    List<TableRow> rows = Lists.newArrayList();    for (ResourceId filename : sourceFiles) {        rows.addAll(readRows(filename.toString()));    }    datasetService.insertAll(destination, rows, null);    FileSystems.delete(sourceFiles);    return new JobStatus().setState("DONE");}
private JobStatus beam_f30373_0(JobConfigurationTableCopy copy) throws InterruptedException, IOException
{    List<TableReference> sources = copy.getSourceTables();    TableReference destination = copy.getDestinationTable();    WriteDisposition writeDisposition = WriteDisposition.valueOf(copy.getWriteDisposition());    CreateDisposition createDisposition = CreateDisposition.valueOf(copy.getCreateDisposition());    Table existingTable = datasetService.getTable(destination);    if (!validateDispositions(existingTable, createDisposition, writeDisposition)) {        return new JobStatus().setState("FAILED").setErrorResult(new ErrorProto());    }    TimePartitioning partitioning = null;    Clustering clustering = null;    TableSchema schema = null;    boolean first = true;    List<TableRow> allRows = Lists.newArrayList();    for (TableReference source : sources) {        Table table = checkNotNull(datasetService.getTable(source));        if (!first) {            if (!Objects.equals(partitioning, table.getTimePartitioning())) {                return new JobStatus().setState("FAILED").setErrorResult(new ErrorProto());            }            if (!Objects.equals(clustering, table.getClustering())) {                return new JobStatus().setState("FAILED").setErrorResult(new ErrorProto());            }            if (!Objects.equals(schema, table.getSchema())) {                return new JobStatus().setState("FAILED").setErrorResult(new ErrorProto());            }        }        partitioning = table.getTimePartitioning();        clustering = table.getClustering();        schema = table.getSchema();        first = false;        allRows.addAll(datasetService.getAllRows(source.getProjectId(), source.getDatasetId(), source.getTableId()));    }    datasetService.createTable(new Table().setTableReference(destination).setSchema(schema).setTimePartitioning(partitioning).setClustering(clustering).setEncryptionConfiguration(copy.getDestinationEncryptionConfiguration()));    datasetService.insertAll(destination, allRows, null);    return new JobStatus().setState("DONE");}
public boolean beam_f30382_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    FakeBatchTransactionId that = (FakeBatchTransactionId) o;    return Objects.equal(id, that.id);}
public int beam_f30383_0()
{    return Objects.hashCode(id);}
public void beam_f30392_0() throws Exception
{    CoderRegistry.createDefault().getCoder(TableRow.class);}
public void beam_f30393_0()
{    TableReference ref = BigQueryHelpers.parseTableSpec("my-project:data_set.table_name");    assertEquals("my-project", ref.getProjectId());    assertEquals("data_set", ref.getDatasetId());    assertEquals("table_name", ref.getTableId());}
public void beam_f30402_0()
{    CoderProperties.coderSerializable(ShardedKeyCoder.of(GlobalWindow.Coder.INSTANCE));}
public void beam_f30403_0()
{    CoderProperties.coderSerializable(TableRowInfoCoder.of(TableRowJsonCoder.of()));}
public void beam_f30412_0() throws Exception
{    setupTestEnvironment("1T", false);    runBigQueryIOReadPipeline();}
public void beam_f30413_0() throws Exception
{    setupTestEnvironment("empty", true);    runBigQueryIOReadPipeline();}
private void beam_f30422_0(BigQueryIO.TypedRead read, String query, String kmsKey, boolean validate)
{    assertNull(read.getTable());    assertEquals(query, read.getQuery().get());    assertEquals(kmsKey, read.getKmsKey());    assertEquals(validate, read.getValidate());}
public void beam_f30423_0() throws IOException, InterruptedException
{    FakeDatasetService.setUp();    BigQueryIO.clearCreatedTables();    fakeDatasetService.createDataset("project-id", "dataset-id", "", "", null);}
public void beam_f30432_0(ProcessContext c) throws Exception
{    c.output(KV.of((String) c.element().get("name"), Long.valueOf((String) c.element().get("number"))));}
public void beam_f30433_0()
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("Invalid BigQueryIO.Read: Specifies a table with a result flattening preference," + " which only applies to queries");    p.apply("ReadMyTable", BigQueryIO.read().from("foo.com:project:somedataset.sometable").withoutResultFlattening());    p.run();}
public void beam_f30442_0() throws IOException, InterruptedException
{        Table someTable = new Table();    someTable.setSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER"))));    someTable.setTableReference(new TableReference().setProjectId("non-executing-project").setDatasetId("schema_dataset").setTableId("schema_table"));    someTable.setNumBytes(1024L * 1024L);    FakeDatasetService fakeDatasetService = new FakeDatasetService();    fakeDatasetService.createDataset("non-executing-project", "schema_dataset", "", "", null);    fakeDatasetService.createTable(someTable);    List<TableRow> records = Lists.newArrayList(new TableRow().set("name", "a").set("number", 1L), new TableRow().set("name", "b").set("number", 2L), new TableRow().set("name", "c").set("number", 3L));    fakeDatasetService.insertAll(someTable.getTableReference(), records, null);    FakeBigQueryServices fakeBqServices = new FakeBigQueryServices().withJobService(new FakeJobService()).withDatasetService(fakeDatasetService);        BigQueryIO.TypedRead<TableRow> read = BigQueryIO.readTableRowsWithSchema().from("non-executing-project:schema_dataset.schema_table").withTestServices(fakeBqServices).withoutValidation();    PCollection<TableRow> bqRows = p.apply(read);    Schema expectedSchema = Schema.of(Schema.Field.of("name", Schema.FieldType.STRING).withNullable(true), Schema.Field.of("number", Schema.FieldType.INT64).withNullable(true));    assertEquals(expectedSchema, bqRows.getSchema());    PCollection<Row> output = bqRows.apply(Select.fieldNames("name", "number"));    PAssert.that(output).containsInAnyOrder(ImmutableList.of(Row.withSchema(expectedSchema).addValues("a", 1L).build(), Row.withSchema(expectedSchema).addValues("b", 2L).build(), Row.withSchema(expectedSchema).addValues("c", 3L).build()));    p.run();}
public void beam_f30443_0()
{    String tableSpec = "project:dataset.tableid";    BigQueryIO.Read read = BigQueryIO.read().from(tableSpec).withoutResultFlattening().usingStandardSql().withoutValidation();    DisplayData displayData = DisplayData.from(read);    assertThat(displayData, hasDisplayItem("table", tableSpec));    assertThat(displayData, hasDisplayItem("flattenResults", false));    assertThat(displayData, hasDisplayItem("useLegacySql", false));    assertThat(displayData, hasDisplayItem("validation", false));}
public void beam_f30452_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    BigQueryOptions bqOptions = options.as(BigQueryOptions.class);    bqOptions.setProject("project");    TableReference sourceTableRef = BigQueryHelpers.parseTableSpec("project:dataset.table");    fakeDatasetService.createDataset(sourceTableRef.getProjectId(), sourceTableRef.getDatasetId(), "asia-northeast1", "Fake plastic tree^H^H^H^Htables", null);    fakeDatasetService.createTable(new Table().setTableReference(sourceTableRef).setLocation("asia-northeast1"));    Table queryResultTable = new Table().setSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER"))));    List<TableRow> expected = ImmutableList.of(new TableRow().set("name", "a").set("number", 1L), new TableRow().set("name", "b").set("number", 2L), new TableRow().set("name", "c").set("number", 3L), new TableRow().set("name", "d").set("number", 4L), new TableRow().set("name", "e").set("number", 5L), new TableRow().set("name", "f").set("number", 6L));    String encodedQuery = FakeBigQueryServices.encodeQueryResult(queryResultTable, expected);    String stepUuid = "testStepUuid";    TableReference tempTableReference = createTempTableReference(bqOptions.getProject(), createJobIdToken(options.getJobName(), stepUuid));    fakeJobService.expectDryRunQuery(bqOptions.getProject(), encodedQuery, new JobStatistics().setQuery(new JobStatistics2().setTotalBytesProcessed(100L).setReferencedTables(ImmutableList.of(sourceTableRef, tempTableReference))));    BoundedSource<TableRow> bqSource = BigQueryQuerySourceDef.create(fakeBqServices, ValueProvider.StaticValueProvider.of(encodedQuery), true, /* flattenResults */    true, /* useLegacySql */    QueryPriority.BATCH, null, null).toSource(stepUuid, TableRowJsonCoder.of(), BigQueryIO.TableRowParser.INSTANCE);    options.setTempLocation(testFolder.getRoot().getAbsolutePath());    List<TableRow> read = convertStringsToLong(SourceTestUtils.readFromSplitsOfSource(bqSource, 0L, /* ignored */    options));    assertThat(read, containsInAnyOrder(Iterables.toArray(expected, TableRow.class)));    List<? extends BoundedSource<TableRow>> sources = bqSource.split(100, options);    assertEquals(2, sources.size());}
public void beam_f30453_0() throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    BigQueryOptions bqOptions = options.as(BigQueryOptions.class);    bqOptions.setProject("project");    Table queryResultTable = new Table().setSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER"))));    List<TableRow> expected = ImmutableList.of(new TableRow().set("name", "a").set("number", 1L), new TableRow().set("name", "b").set("number", 2L), new TableRow().set("name", "c").set("number", 3L), new TableRow().set("name", "d").set("number", 4L), new TableRow().set("name", "e").set("number", 5L), new TableRow().set("name", "f").set("number", 6L));    String encodedQuery = FakeBigQueryServices.encodeQueryResult(queryResultTable, expected);    String stepUuid = "testStepUuid";    fakeJobService.expectDryRunQuery(bqOptions.getProject(), encodedQuery, new JobStatistics().setQuery(new JobStatistics2().setTotalBytesProcessed(100L).setReferencedTables(ImmutableList.of())));    BoundedSource<TableRow> bqSource = BigQueryQuerySourceDef.create(fakeBqServices, ValueProvider.StaticValueProvider.of(encodedQuery), true, /* flattenResults */    true, /* useLegacySql */    QueryPriority.BATCH, null, null).toSource(stepUuid, TableRowJsonCoder.of(), BigQueryIO.TableRowParser.INSTANCE);    options.setTempLocation(testFolder.getRoot().getAbsolutePath());    List<TableRow> read = convertStringsToLong(SourceTestUtils.readFromSplitsOfSource(bqSource, 0L, /* ignored */    options));    assertThat(read, containsInAnyOrder(Iterables.toArray(expected, TableRow.class)));    List<? extends BoundedSource<TableRow>> sources = bqSource.split(100, options);    assertEquals(2, sources.size());}
private void beam_f30463_0(String tableSize)
{    PipelineOptionsFactory.register(BigQueryIOStorageQueryOptions.class);    options = TestPipeline.testingPipelineOptions().as(BigQueryIOStorageQueryOptions.class);    options.setNumRecords(EXPECTED_NUM_RECORDS.get(tableSize));    String project = TestPipeline.testingPipelineOptions().as(GcpOptions.class).getProject();    options.setInputTable(project + '.' + DATASET_ID + '.' + TABLE_PREFIX + tableSize);}
private void beam_f30464_0()
{    Pipeline p = Pipeline.create(options);    PCollection<Long> count = p.apply("Query", BigQueryIO.read(TableRowParser.INSTANCE).fromQuery("SELECT * FROM `" + options.getInputTable() + "`").usingStandardSql().withMethod(Method.DIRECT_READ)).apply("Count", Count.globally());    PAssert.thatSingleton(count).isEqualTo(options.getNumRecords());    p.run().waitUntilFinish();}
public void beam_f30473_0() throws Exception
{    TypedRead<TableRow> typedRead = getDefaultTypedRead().usingStandardSql();    checkTypedReadQueryObject(typedRead, DEFAULT_QUERY);    assertFalse(typedRead.getUseLegacySql());}
public void beam_f30474_0() throws Exception
{    TypedRead<TableRow> typedRead = getDefaultTypedRead().withQueryPriority(QueryPriority.INTERACTIVE);    checkTypedReadQueryObject(typedRead, DEFAULT_QUERY);    assertEquals(QueryPriority.INTERACTIVE, typedRead.getQueryPriority());}
public void beam_f30483_0() throws Exception
{    TypedRead<TableRow> typedRead = getDefaultTypedRead();    DisplayData displayData = DisplayData.from(typedRead);    assertThat(displayData, hasDisplayItem("query", DEFAULT_QUERY));}
public void beam_f30484_0() throws Exception
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    TypedRead<TableRow> typedRead = getDefaultTypedRead();    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveSourceTransforms(typedRead);    assertThat(displayData, hasItem(hasDisplayItem("query")));}
public void beam_f30493_0() throws Exception
{    Table queryResultTable = new Table().setSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER")))).setNumBytes(1024L * 1024L);    String encodedQuery = FakeBigQueryServices.encodeQueryResult(queryResultTable);    fakeJobService.expectDryRunQuery(options.getProject(), encodedQuery, new JobStatistics().setQuery(new JobStatistics2().setTotalBytesProcessed(1024L * 1024L).setReferencedTables(ImmutableList.of())));    String stepUuid = "testStepUuid";    TableReference tempTableReference = createTempTableReference(options.getProject(), createJobIdToken(options.getJobName(), stepUuid));    CreateReadSessionRequest expectedRequest = CreateReadSessionRequest.newBuilder().setParent("projects/" + options.getProject()).setTableReference(BigQueryHelpers.toTableRefProto(tempTableReference)).setRequestedStreams(1024).setUnknownFields(UnknownFieldSet.newBuilder().addField(7, UnknownFieldSet.Field.newBuilder().addVarint(2).build()).build()).build();    ReadSession.Builder builder = ReadSession.newBuilder();    for (int i = 0; i < 1024; i++) {        builder.addStreams(Stream.newBuilder().setName("stream-" + i));    }    StorageClient fakeStorageClient = mock(StorageClient.class);    when(fakeStorageClient.createReadSession(expectedRequest)).thenReturn(builder.build());    BigQueryStorageQuerySource<TableRow> querySource = BigQueryStorageQuerySource.create(stepUuid, ValueProvider.StaticValueProvider.of(encodedQuery), /* flattenResults = */    true, /* useLegacySql = */    true, /* priority = */    QueryPriority.BATCH, /* location = */    null, /* kmsKey = */    null, new TableRowParser(), TableRowJsonCoder.of(), new FakeBigQueryServices().withDatasetService(fakeDatasetService).withJobService(fakeJobService).withStorageClient(fakeStorageClient));    List<? extends BoundedSource<TableRow>> sources = querySource.split(1024, options);    assertEquals(1024, sources.size());}
private static GenericRecord beam_f30494_0(String name, long number, Schema schema)
{    GenericRecord genericRecord = new Record(schema);    genericRecord.put("name", name);    genericRecord.put("number", number);    return genericRecord;}
private void beam_f30503_0()
{    Pipeline p = Pipeline.create(options);    PCollection<Long> count = p.apply("Read", BigQueryIO.read(TableRowParser.INSTANCE).from(options.getInputTable()).withMethod(Method.DIRECT_READ)).apply("Count", Count.globally());    PAssert.thatSingleton(count).isEqualTo(options.getNumRecords());    p.run().waitUntilFinish();}
public void beam_f30504_0() throws Exception
{    setUpTestEnvironment("1G");    runBigQueryIOStorageReadPipeline();}
public void beam_f30513_0() throws Throwable
{    options = TestPipeline.testingPipelineOptions();    options.as(BigQueryOptions.class).setProject("project-id");    options.as(BigQueryOptions.class).setTempLocation(testFolder.getRoot().getAbsolutePath());    p = TestPipeline.fromOptions(options);    p.apply(base, description).evaluate();}
public void beam_f30514_0() throws Exception
{    FakeDatasetService.setUp();}
public void beam_f30523_0()
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("withReadOptions() already called");    p.apply("ReadMyTable", BigQueryIO.read(new TableRowParser()).withCoder(TableRowJsonCoder.of()).withMethod(Method.DIRECT_READ).from("foo.com:project:dataset.table").withReadOptions(TableReadOptions.newBuilder().build()).withSelectedFields(Lists.newArrayList("field1")));}
public void beam_f30524_0()
{    thrown.expect(IllegalStateException.class);    thrown.expectMessage("withReadOptions() already called");    p.apply("ReadMyTable", BigQueryIO.read(new TableRowParser()).withCoder(TableRowJsonCoder.of()).withMethod(Method.DIRECT_READ).from("foo.com:project:dataset.table").withReadOptions(TableReadOptions.newBuilder().build()).withRowRestriction("field > 1"));}
public void beam_f30533_0() throws Exception
{    fakeDatasetService.createDataset("project-id", "dataset", "", "", null);    TableReference tableRef = BigQueryHelpers.parseTableSpec("project-id:dataset.table");    Table table = new Table().setTableReference(tableRef).setNumBytes(100L);    fakeDatasetService.createTable(table);    BigQueryStorageTableSource<TableRow> tableSource = BigQueryStorageTableSource.create(ValueProvider.StaticValueProvider.of(BigQueryHelpers.parseTableSpec("dataset.table")), null, null, null, new TableRowParser(), TableRowJsonCoder.of(), new FakeBigQueryServices().withDatasetService(fakeDatasetService));    assertEquals(100, tableSource.getEstimatedSizeBytes(options));}
public void beam_f30534_0() throws Exception
{    doTableSourceInitialSplitTest(1024L, 1024);}
private static GenericRecord beam_f30543_0(String name, long number, Schema schema)
{    GenericRecord genericRecord = new Record(schema);    genericRecord.put("name", name);    genericRecord.put("number", number);    return genericRecord;}
private static ReadRowsResponse beam_f30544_0(Schema schema, Collection<GenericRecord> genericRecords, double fractionConsumed) throws Exception
{    GenericDatumWriter<GenericRecord> writer = new GenericDatumWriter<>(schema);    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();    Encoder binaryEncoder = ENCODER_FACTORY.binaryEncoder(outputStream, null);    for (GenericRecord genericRecord : genericRecords) {        writer.write(genericRecord, binaryEncoder);    }    binaryEncoder.flush();    return ReadRowsResponse.newBuilder().setAvroRows(AvroRows.newBuilder().setSerializedBinaryRows(ByteString.copyFrom(outputStream.toByteArray())).setRowCount(genericRecords.size())).setStatus(StreamStatus.newBuilder().setUnknownFields(UnknownFieldSet.newBuilder().addField(2, UnknownFieldSet.Field.newBuilder().addFixed32(java.lang.Float.floatToIntBits((float) fractionConsumed)).build()).build())).build();}
public void beam_f30553_0() throws Exception
{    Stream parentStream = Stream.newBuilder().setName("parent").build();    List<ReadRowsResponse> parentResponses = Lists.newArrayList(createResponse(AVRO_SCHEMA, Lists.newArrayList(createRecord("A", 1, AVRO_SCHEMA), createRecord("B", 2, AVRO_SCHEMA)), 0.25), createResponse(AVRO_SCHEMA, Lists.newArrayList(createRecord("C", 3, AVRO_SCHEMA)), 0.50), createResponse(AVRO_SCHEMA, Lists.newArrayList(createRecord("D", 4, AVRO_SCHEMA), createRecord("E", 5, AVRO_SCHEMA)), 0.75));    StorageClient fakeStorageClient = mock(StorageClient.class);    when(fakeStorageClient.readRows(ReadRowsRequest.newBuilder().setReadPosition(StreamPosition.newBuilder().setStream(parentStream)).build())).thenReturn(new FakeBigQueryServerStream<>(parentResponses));                when(fakeStorageClient.splitReadStream(SplitReadStreamRequest.newBuilder().setOriginalStream(parentStream).setUnknownFields(UnknownFieldSet.newBuilder().addField(2, UnknownFieldSet.Field.newBuilder().addFixed32(java.lang.Float.floatToIntBits(0.5f)).build()).build()).build())).thenReturn(SplitReadStreamResponse.newBuilder().setPrimaryStream(Stream.newBuilder().setName("primary")).setRemainderStream(Stream.newBuilder().setName("residual")).build());        when(fakeStorageClient.readRows(ReadRowsRequest.newBuilder().setReadPosition(StreamPosition.newBuilder().setStream(Stream.newBuilder().setName("primary")).setOffset(2)).build())).thenThrow(new FailedPreconditionException("Given row offset is invalid for stream.", new StatusRuntimeException(Status.FAILED_PRECONDITION), GrpcStatusCode.of(Code.FAILED_PRECONDITION), /* retryable = */    false));    BigQueryStorageStreamSource<TableRow> streamSource = BigQueryStorageStreamSource.create(ReadSession.newBuilder().setName("readSession").setAvroSchema(AvroSchema.newBuilder().setSchema(AVRO_SCHEMA_STRING)).build(), parentStream, TABLE_SCHEMA, new TableRowParser(), TableRowJsonCoder.of(), new FakeBigQueryServices().withStorageClient(fakeStorageClient));            BoundedReader<TableRow> parent = streamSource.createReader(options);    assertTrue(parent.start());    assertEquals("A", parent.getCurrent().get("name"));    assertTrue(parent.advance());    assertEquals("B", parent.getCurrent().get("name"));    assertNull(parent.splitAtFraction(0.5));        assertTrue(parent.advance());    assertEquals("C", parent.getCurrent().get("name"));    assertTrue(parent.advance());    assertEquals("D", parent.getCurrent().get("name"));    assertTrue(parent.advance());    assertEquals("E", parent.getCurrent().get("name"));    assertFalse(parent.advance());}
public KV<String, Long> beam_f30554_0(SchemaAndRecord input)
{    return KV.of(input.getRecord().get("name").toString(), (Long) input.getRecord().get("number"));}
public void beam_f30563_0() throws Exception
{    writeDynamicDestinations(true, false);}
public void beam_f30564_0() throws Exception
{    writeDynamicDestinations(true, true);}
 void beam_f30573_0(BigQueryIO.Write.Method insertMethod) throws Exception
{    testTimePartitioningClustering(insertMethod, true, true);}
public void beam_f30574_0() throws Exception
{    testTimePartitioning(BigQueryIO.Write.Method.STREAMING_INSERTS);}
public void beam_f30583_0() throws Exception
{    TableRow row1 = new TableRow().set("name", "a").set("number", "1");    TableRow row2 = new TableRow().set("name", "b").set("number", "2");    TableRow row3 = new TableRow().set("name", "c").set("number", "3");    TableDataInsertAllResponse.InsertErrors ephemeralError = new TableDataInsertAllResponse.InsertErrors().setErrors(ImmutableList.of(new ErrorProto().setReason("timeout")));    TableDataInsertAllResponse.InsertErrors persistentError = new TableDataInsertAllResponse.InsertErrors().setErrors(ImmutableList.of(new ErrorProto().setReason("invalidQuery")));    fakeDatasetService.failOnInsert(ImmutableMap.of(row1, ImmutableList.of(ephemeralError, ephemeralError), row2, ImmutableList.of(ephemeralError, ephemeralError, persistentError)));    PCollection<TableRow> failedRows = p.apply(Create.of(row1, row2, row3)).apply(BigQueryIO.writeTableRows().to("project-id:dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withMethod(BigQueryIO.Write.Method.STREAMING_INSERTS).withSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER")))).withFailedInsertRetryPolicy(InsertRetryPolicy.retryTransientErrors()).withTestServices(fakeBqServices).withoutValidation()).getFailedInserts();            PAssert.that(failedRows).containsInAnyOrder(row2);    p.run();        assertThat(fakeDatasetService.getAllRows("project-id", "dataset-id", "table-id"), containsInAnyOrder(row1, row3));}
public void beam_f30584_0() throws Exception
{    p.apply(Create.of(new TableRow().set("name", "a").set("number", 1), new TableRow().set("name", "b").set("number", 2), new TableRow().set("name", "c").set("number", 3)).withCoder(TableRowJsonCoder.of())).apply(BigQueryIO.writeTableRows().to("dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withSchema(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER")))).withTestServices(fakeBqServices).withoutValidation());    p.run();}
public Instant beam_f30593_0(Instant inputTimestamp, PartitionedGlobalWindow window)
{    return inputTimestamp;}
public Instant beam_f30594_0()
{    return GlobalWindow.INSTANCE.maxTimestamp();}
public void beam_f30604_0(boolean streaming) throws Exception
{    List<Integer> inserts = new ArrayList<>();    for (int i = 0; i < 10; i++) {        inserts.add(i);    }            WindowFn<Integer, PartitionedGlobalWindow> windowFn = new PartitionedGlobalWindows<>(i -> Integer.toString(i % 5));    final Map<Integer, TableDestination> targetTables = Maps.newHashMap();    Map<String, String> schemas = Maps.newHashMap();    for (int i = 0; i < 5; i++) {        TableDestination destination = new TableDestination("project-id:dataset-id" + ".table-id-" + i, "");        targetTables.put(i, destination);                schemas.put(destination.getTableSpec(), toJsonString(new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("number").setType("INTEGER"), new TableFieldSchema().setName("custom_" + i).setType("STRING")))));    }    SerializableFunction<ValueInSingleWindow<Integer>, TableDestination> tableFunction = input -> {        PartitionedGlobalWindow window = (PartitionedGlobalWindow) input.getWindow();                checkArgument(window.value.equals(Integer.toString(input.getValue() % 5)), "Incorrect element");        return targetTables.get(input.getValue() % 5);    };    PCollection<Integer> input = p.apply("CreateSource", Create.of(inserts));    if (streaming) {        input = input.setIsBoundedInternal(PCollection.IsBounded.UNBOUNDED);    }    PCollectionView<Map<String, String>> schemasView = p.apply("CreateSchemaMap", Create.of(schemas)).apply("ViewSchemaAsMap", View.asMap());    input.apply(Window.into(windowFn)).apply(BigQueryIO.<Integer>write().to(tableFunction).withFormatFunction(i -> new TableRow().set("name", "number" + i).set("number", i)).withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED).withSchemaFromView(schemasView).withTestServices(fakeBqServices).withoutValidation());    p.run();    for (int i = 0; i < 5; ++i) {        String tableId = String.format("table-id-%d", i);        String tableSpec = String.format("project-id:dataset-id.%s", tableId);                assertThat(toJsonString(fakeDatasetService.getTable(new TableReference().setProjectId("project-id").setDatasetId("dataset-id").setTableId(tableId)).getSchema()), equalTo(schemas.get(tableSpec)));                assertThat(fakeDatasetService.getAllRows("project-id", "dataset-id", tableId), containsInAnyOrder(new TableRow().set("name", String.format("number%d", i)).set("number", i), new TableRow().set("name", String.format("number%d", i + 5)).set("number", i + 5)));    }}
public void beam_f30605_0() throws Exception
{    p.apply(Create.of(new TableRow().set("name", "a").set("number", 1), new TableRow().set("name", "b").set("number", 2), new TableRow().set("name", "c").set("number", 3)).withCoder(TableRowJsonCoder.of())).apply(BigQueryIO.writeTableRows().to("project-id:dataset-id.table-id").withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_NEVER).withTestServices(fakeBqServices).withoutValidation());    thrown.expect(RuntimeException.class);    thrown.expectMessage("Failed to create job");    p.run();}
public TableRow beam_f30614_0(Long input)
{    return null;}
public void beam_f30615_0() throws Exception
{    testWriteValidatesDataset(false);}
public void beam_f30624_0() throws Exception
{    long numFiles = 10;    long fileSize = BatchLoads.DEFAULT_MAX_BYTES_PER_PARTITION / 3;        long expectedNumPartitions = 4;    testWritePartition(2, numFiles, fileSize, expectedNumPartitions);}
private void beam_f30625_0(long numTables, long numFilesPerTable, long fileSize, long expectedNumPartitionsPerTable) throws Exception
{    p.enableAbandonedNodeEnforcement(false);                boolean isSingleton = numTables == 1 && numFilesPerTable == 0;    DynamicDestinations<String, TableDestination> dynamicDestinations = new DynamicDestinationsHelpers.ConstantTableDestinations<>(ValueProvider.StaticValueProvider.of("SINGLETON"), "");    List<ShardedKey<TableDestination>> expectedPartitions = Lists.newArrayList();    if (isSingleton) {        expectedPartitions.add(ShardedKey.of(new TableDestination("SINGLETON", ""), 1));    } else {        for (int i = 0; i < numTables; ++i) {            for (int j = 1; j <= expectedNumPartitionsPerTable; ++j) {                String tableName = String.format("project-id:dataset-id.tables%05d", i);                expectedPartitions.add(ShardedKey.of(new TableDestination(tableName, ""), j));            }        }    }    List<WriteBundlesToFiles.Result<TableDestination>> files = Lists.newArrayList();    Map<String, List<String>> filenamesPerTable = Maps.newHashMap();    for (int i = 0; i < numTables; ++i) {        String tableName = String.format("project-id:dataset-id.tables%05d", i);        List<String> filenames = filenamesPerTable.computeIfAbsent(tableName, k -> Lists.newArrayList());        for (int j = 0; j < numFilesPerTable; ++j) {            String fileName = String.format("%s_files%05d", tableName, j);            filenames.add(fileName);            files.add(new WriteBundlesToFiles.Result<>(fileName, fileSize, new TableDestination(tableName, "")));        }    }    TupleTag<KV<ShardedKey<TableDestination>, List<String>>> multiPartitionsTag = new TupleTag<KV<ShardedKey<TableDestination>, List<String>>>("multiPartitionsTag") {    };    TupleTag<KV<ShardedKey<TableDestination>, List<String>>> singlePartitionTag = new TupleTag<KV<ShardedKey<TableDestination>, List<String>>>("singlePartitionTag") {    };    String tempFilePrefix = testFolder.newFolder("BigQueryIOTest").getAbsolutePath();    PCollectionView<String> tempFilePrefixView = p.apply(Create.of(tempFilePrefix)).apply(View.asSingleton());    WritePartition<TableDestination> writePartition = new WritePartition<>(isSingleton, dynamicDestinations, tempFilePrefixView, BatchLoads.DEFAULT_MAX_FILES_PER_PARTITION, BatchLoads.DEFAULT_MAX_BYTES_PER_PARTITION, multiPartitionsTag, singlePartitionTag);    DoFnTester<Iterable<WriteBundlesToFiles.Result<TableDestination>>, KV<ShardedKey<TableDestination>, List<String>>> tester = DoFnTester.of(writePartition);    tester.setSideInput(tempFilePrefixView, GlobalWindow.INSTANCE, tempFilePrefix);    tester.processElement(files);    List<KV<ShardedKey<TableDestination>, List<String>>> partitions;    if (expectedNumPartitionsPerTable > 1) {        partitions = tester.takeOutputElements(multiPartitionsTag);    } else {        partitions = tester.takeOutputElements(singlePartitionTag);    }    List<ShardedKey<TableDestination>> partitionsResult = Lists.newArrayList();    Map<String, List<String>> filesPerTableResult = Maps.newHashMap();    for (KV<ShardedKey<TableDestination>, List<String>> partition : partitions) {        String table = partition.getKey().getKey().getTableSpec();        partitionsResult.add(partition.getKey());        List<String> tableFilesResult = filesPerTableResult.computeIfAbsent(table, k -> Lists.newArrayList());        tableFilesResult.addAll(partition.getValue());    }    assertThat(partitionsResult, containsInAnyOrder(Iterables.toArray(expectedPartitions, ShardedKey.class)));    if (isSingleton) {        assertEquals(1, filesPerTableResult.size());        List<String> singletonFiles = filesPerTableResult.values().iterator().next();        assertTrue(Files.exists(Paths.get(singletonFiles.get(0))));        assertThat(Files.readAllBytes(Paths.get(singletonFiles.get(0))).length, equalTo(0));    } else {        assertEquals(filenamesPerTable, filesPerTableResult);    }    for (List<String> filenames : filesPerTableResult.values()) {        for (String filename : filenames) {            Files.deleteIfExists(Paths.get(filename));        }    }}
private static void beam_f30634_0(File tempDir, int expectedNumFiles)
{    assertEquals(expectedNumFiles, tempDir.listFiles(File::isFile).length);}
public void beam_f30635_0() throws Exception
{    TableRow row1 = new TableRow().set("name", "a").set("number", "1");    TableRow row2 = new TableRow().set("name", "b").set("number", "2");    TableSchema schema = new TableSchema().setFields(ImmutableList.of(new TableFieldSchema().setName("number").setType("INTEGER")));    p.apply(Create.of(row1, row2)).apply(BigQueryIO.writeTableRows().to("project-id:dataset-id.table-id$20171127").withTestServices(fakeBqServices).withMethod(BigQueryIO.Write.Method.STREAMING_INSERTS).withSchema(schema).withoutValidation());    p.run();}
private static void beam_f30644_0(Options options) throws Exception
{            Pipeline p = Pipeline.create(options);    BigQueryOptions bigQueryOptions = options.as(BigQueryOptions.class);    PCollection<TableRow> flattenedCollection = p.apply("ReadFlattened", BigQueryIO.readTableRows().fromQuery(options.getInput()));    PCollection<TableRow> nonFlattenedCollection = p.apply("ReadNonFlattened", BigQueryIO.readTableRows().fromQuery(options.getInput()).withoutResultFlattening());    PCollection<TableRow> unflattenableCollection = p.apply("ReadUnflattenable", BigQueryIO.readTableRows().fromQuery(options.getUnflattenableInput()).withoutResultFlattening());        BigqueryClient bigQueryClient = new BigqueryClient(bigQueryOptions.getAppName());    TableRow queryFlattenedTyped = bigQueryClient.queryWithRetries(options.getInput(), bigQueryOptions.getProject(), true).getRows().get(0);    TableRow queryUnflattened = bigQueryClient.queryUnflattened(options.getInput(), bigQueryOptions.getProject(), true).get(0);    TableRow queryUnflattenable = bigQueryClient.queryUnflattened(options.getUnflattenableInput(), bigQueryOptions.getProject(), true).get(0);        PAssert.thatSingleton(flattenedCollection).isEqualTo(queryFlattenedTyped);    PAssert.thatSingleton(nonFlattenedCollection).isEqualTo(queryUnflattened);    PAssert.thatSingleton(unflattenableCollection).isEqualTo(queryUnflattenable);    PAssert.thatSingleton(flattenedCollection).notEqualTo(queryUnflattened);    p.run().waitUntilFinish();}
public void beam_f30645_0()
{    MockitoAnnotations.initMocks(this);        request = new MockLowLevelHttpRequest() {        @Override        public LowLevelHttpResponse execute() throws IOException {            return response;        }    };        MockHttpTransport transport = new MockHttpTransport.Builder().setLowLevelHttpRequest(request).build();        bigquery = new Bigquery.Builder(transport, Transport.getJsonFactory(), new RetryHttpRequestInitializer()).build();}
public void beam_f30654_0() throws Exception
{    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(404);    BigQueryServicesImpl.JobServiceImpl jobService = new BigQueryServicesImpl.JobServiceImpl(bigquery);    JobReference jobRef = new JobReference().setProjectId("projectId").setJobId("jobId");    Job job = jobService.getJob(jobRef, Sleeper.DEFAULT, BackOff.ZERO_BACKOFF);    assertEquals(null, job);    verify(response, times(1)).getStatusCode();    verify(response, times(1)).getContent();    verify(response, times(1)).getContentType();}
public void beam_f30655_0() throws Exception
{    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(401);    BigQueryServicesImpl.JobServiceImpl jobService = new BigQueryServicesImpl.JobServiceImpl(bigquery);    JobReference jobRef = new JobReference().setProjectId("projectId").setJobId("jobId");    thrown.expect(IOException.class);    thrown.expectMessage(String.format("Unable to find BigQuery job: %s", jobRef));    jobService.getJob(jobRef, Sleeper.DEFAULT, BackOff.STOP_BACKOFF);}
public void beam_f30664_0() throws Exception
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    List<ValueInSingleWindow<TableRow>> rows = new ArrayList<>();    rows.add(wrapValue(new TableRow()));        when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(403).thenReturn(200);    when(response.getContent()).thenReturn(toStream(errorWithReasonAndStatus("rateLimitExceeded", 403))).thenReturn(toStream(new TableDataInsertAllResponse()));    DatasetServiceImpl dataService = new DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    dataService.insertAll(ref, rows, null, BackOffAdapter.toGcpBackOff(TEST_BACKOFF.backoff()), new MockSleeper(), InsertRetryPolicy.alwaysRetry(), null, null, false, false);    verify(response, times(2)).getStatusCode();    verify(response, times(2)).getContent();    verify(response, times(2)).getContentType();    expectedLogs.verifyInfo("BigQuery insertAll error, retrying:");}
public void beam_f30665_0() throws Exception
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    List<ValueInSingleWindow<TableRow>> rows = new ArrayList<>();    rows.add(wrapValue(new TableRow()));        when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(403).thenReturn(200);    when(response.getContent()).thenReturn(toStream(errorWithReasonAndStatus("quotaExceeded", 403))).thenReturn(toStream(new TableDataInsertAllResponse()));    DatasetServiceImpl dataService = new DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    dataService.insertAll(ref, rows, null, BackOffAdapter.toGcpBackOff(TEST_BACKOFF.backoff()), new MockSleeper(), InsertRetryPolicy.alwaysRetry(), null, null, false, false);    verify(response, times(2)).getStatusCode();    verify(response, times(2)).getContent();    verify(response, times(2)).getContentType();    expectedLogs.verifyInfo("BigQuery insertAll error, retrying:");}
public void beam_f30674_0() throws IOException
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    Table testTable = new Table().setTableReference(ref);    when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(200);    when(response.getContent()).thenReturn(toStream(testTable));    BigQueryServicesImpl.DatasetServiceImpl services = new BigQueryServicesImpl.DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    Table ret = services.tryCreateTable(testTable, new RetryBoundedBackOff(0, BackOff.ZERO_BACKOFF), Sleeper.DEFAULT);    assertEquals(testTable, ret);    verify(response, times(1)).getStatusCode();    verify(response, times(1)).getContent();    verify(response, times(1)).getContentType();}
public void beam_f30675_0() throws IOException
{    TableReference ref = new TableReference().setProjectId("project").setDatasetId("dataset").setTableId("table");    Table testTable = new Table().setTableReference(ref);            when(response.getContentType()).thenReturn(Json.MEDIA_TYPE);    when(response.getStatusCode()).thenReturn(403).thenReturn(200);    when(response.getContent()).thenReturn(toStream(errorWithReasonAndStatus("actually forbidden", 403))).thenReturn(toStream(testTable));    thrown.expect(GoogleJsonResponseException.class);    thrown.expectMessage("actually forbidden");    BigQueryServicesImpl.DatasetServiceImpl services = new BigQueryServicesImpl.DatasetServiceImpl(bigquery, PipelineOptionsFactory.create());    try {        services.tryCreateTable(testTable, new RetryBoundedBackOff(3, BackOff.ZERO_BACKOFF), Sleeper.DEFAULT);        fail();    } catch (IOException e) {        verify(response, times(1)).getStatusCode();        verify(response, times(1)).getContent();        verify(response, times(1)).getContentType();        throw e;    }}
public TableDestination beam_f30684_0(TableDestination destination)
{    return destination;}
public TableSchema beam_f30685_0(TableDestination destination)
{    return SCHEMA;}
private List<TableRow> beam_f30694_1(String query, int maxRetry) throws Exception
{    FluentBackoff backoffFactory = FluentBackoff.DEFAULT.withMaxRetries(maxRetry).withInitialBackoff(Duration.standardSeconds(1L));    Sleeper sleeper = Sleeper.DEFAULT;    BackOff backoff = BackOffAdapter.toGcpBackOff(backoffFactory.backoff());    do {                QueryResponse response = BQ_CLIENT.queryWithRetries(query, project);        if (response.getRows() != null) {                        return response.getRows();        }    } while (BackOffUtils.next(sleeper, backoff));        return Collections.emptyList();}
private void beam_f30695_0(String outputTable) throws Exception
{    List<String> legacyQueryExpectedRes = ImmutableList.of("apple", "orange");    List<TableRow> tableRows = getTableRowsFromQuery(String.format("SELECT fruit from [%s];", outputTable), MAX_RETRY);    List<String> tableResult = tableRows.stream().flatMap(row -> row.getF().stream().map(cell -> cell.getV().toString())).sorted().collect(Collectors.toList());    assertEquals(legacyQueryExpectedRes, tableResult);}
public void beam_f30704_0() throws Exception
{    final String outputTable = project + ":" + BIG_QUERY_DATASET_ID + "." + "testNewTypesQueryWithoutReshuffleWithCustom";    BigQueryToTableOptions options = this.setupNewTypesQueryTest(outputTable);    options.setExperiments(ImmutableList.of("enable_custom_bigquery_sink", "enable_custom_bigquery_source"));    this.runBigQueryToTablePipeline(options);    this.verifyNewTypesQueryRes(outputTable);}
public void beam_f30705_0() throws Exception
{    final String outputTable = project + ":" + BIG_QUERY_DATASET_ID + "." + "testLegacyQueryWithoutReshuffleWithCustom";    BigQueryToTableOptions options = this.setupLegacyQueryTest(outputTable);    options.setExperiments(ImmutableList.of("enable_custom_bigquery_sink", "enable_custom_bigquery_source"));    this.runBigQueryToTablePipeline(options);    this.verifyLegacyQueryRes(outputTable);}
public void beam_f30714_0()
{    TableRow row = toTableRow().apply(ARRAY_ROW_ROW);    assertThat(row.size(), equalTo(1));    row = ((List<TableRow>) row.get("rows")).get(0);    assertThat(row.size(), equalTo(9));    assertThat(row, hasEntry("id", "123"));    assertThat(row, hasEntry("value", "123.456"));    assertThat(row, hasEntry("name", "test"));    assertThat(row, hasEntry("valid", "false"));    assertThat(row, hasEntry("binary", "ABCD1234"));}
public void beam_f30715_0()
{    TableRow row = toTableRow().apply(NULL_FLAT_ROW);    assertThat(row.size(), equalTo(9));    assertThat(row, hasEntry("id", null));    assertThat(row, hasEntry("value", null));    assertThat(row, hasEntry("name", null));    assertThat(row, hasEntry("timestamp_variant1", null));    assertThat(row, hasEntry("timestamp_variant2", null));    assertThat(row, hasEntry("timestamp_variant3", null));    assertThat(row, hasEntry("timestamp_variant4", null));    assertThat(row, hasEntry("valid", null));    assertThat(row, hasEntry("binary", null));}
public Instant beam_f30724_0(Long input)
{        return new Instant((long) input);}
public Long beam_f30725_0(Instant base)
{    return base.getMillis();}
public void beam_f30734_0()
{    Row beamRow = BigQueryUtils.toBeamRow(ARRAY_ROW_TYPE, BQ_ARRAY_ROW_ROW);    assertEquals(ARRAY_ROW_ROW, beamRow);}
public void beam_f30735_0()
{    Row flatRowExpected = Row.withSchema(AVRO_FLAT_TYPE).addValues(123L, 123.456, "test", false).build();    Row expected = Row.withSchema(AVRO_ARRAY_TYPE).addValues((Object) Arrays.asList(flatRowExpected)).build();    GenericData.Record record = new GenericData.Record(AvroUtils.toAvroSchema(AVRO_ARRAY_TYPE));    GenericData.Record flat = new GenericData.Record(AvroUtils.toAvroSchema(AVRO_FLAT_TYPE));    flat.put("id", 123L);    flat.put("value", 123.456);    flat.put("name", "test");    flat.put("valid", false);    record.put("rows", Arrays.asList(flat));    Row beamRow = BigQueryUtils.toBeamRow(record, AVRO_ARRAY_TYPE, BigQueryUtils.ConversionOptions.builder().build());    assertEquals(expected, beamRow);}
private Table beam_f30744_0()
{    return new Table().setSchema(new TableSchema().setFields(Arrays.asList(new TableFieldSchema().setName("name").setType("STRING"), new TableFieldSchema().setName("answer").setType("INTEGER"))));}
private TableRow beam_f30745_0(Object... args)
{    List<TableCell> cells = new ArrayList<>();    for (Object a : args) {        cells.add(new TableCell().setV(a));    }    return new TableRow().setF(cells);}
public void beam_f30754_0() throws Exception
{    for (TableRow value : TEST_VALUES) {        CoderProperties.coderDecodeEncodeEqual(TEST_CODER, value);    }}
public void beam_f30755_0() throws Exception
{    CoderProperties.coderEncodesBase64(TEST_CODER, TEST_VALUES, TEST_ENCODINGS);}
public void beam_f30764_0()
{    assertTrue(config.withValidate(true).getValidate());}
public void beam_f30765_0()
{    assertEquals(SERVICE, config.withBigtableService(SERVICE).getBigtableService());    thrown.expect(IllegalArgumentException.class);    config.withBigtableService(null);}
public String beam_f30774_0()
{    throw new IllegalStateException("Value is not accessible");}
public boolean beam_f30775_0()
{    return false;}
public void beam_f30784_0()
{    ReadOptions options = PipelineOptionsFactory.fromArgs().withValidation().as(ReadOptions.class);    BigtableIO.Read read = BigtableIO.read().withoutValidation().withProjectId(options.getBigtableProject()).withInstanceId(options.getBigtableInstanceId()).withTableId(options.getBigtableTableId());        thrown.expect(PipelineRunMissingException.class);    p.apply(read);}
public void beam_f30785_0()
{    BigtableIO.Write write = BigtableIO.write().withBigtableOptions(BIGTABLE_OPTIONS).withTableId("table").withInstanceId("instance").withProjectId("project");    assertEquals("table", write.getBigtableConfig().getTableId().get());    assertEquals("options_project", write.getBigtableOptions().getProjectId());    assertEquals("options_instance", write.getBigtableOptions().getInstanceId());    assertEquals("instance", write.getBigtableConfig().getInstanceId().get());    assertEquals("project", write.getBigtableConfig().getProjectId().get());}
public void beam_f30794_0() throws Exception
{    final String table = "TEST-TABLE";    BigtableIO.Read read = BigtableIO.read().withBigtableOptions(BIGTABLE_OPTIONS).withTableId(table).withBigtableService(service);        thrown.expect(IllegalArgumentException.class);    thrown.expectMessage(String.format("Table %s does not exist", table));    p.apply(read);    p.run();}
public void beam_f30795_0() throws Exception
{    final String table = "TEST-EMPTY-TABLE";    service.createTable(table);    service.setupSampleRowKeys(table, 1, 1L);    runReadTest(defaultRead.withTableId(table), new ArrayList<>());    logged.verifyInfo("Closing reader after reading 0 records.");}
public void beam_f30804_0() throws Exception
{    final String table = "TEST-FEW-ROWS-SPLIT-EXHAUSTIVE-TABLE";    final int numRows = 10;    final int numSamples = 1;    final long bytesPerRow = 1L;    makeTableData(table, numRows);    service.setupSampleRowKeys(table, numSamples, bytesPerRow);    BigtableSource source = new BigtableSource(config.withTableId(ValueProvider.StaticValueProvider.of(table)), null, Arrays.asList(service.getTableRange(table)), null);    assertSplitAtFractionExhaustive(source, null);}
public void beam_f30805_0() throws Exception
{    final String table = "TEST-SPLIT-AT-FRACTION";    final int numRows = 10;    final int numSamples = 1;    final long bytesPerRow = 1L;    makeTableData(table, numRows);    service.setupSampleRowKeys(table, numSamples, bytesPerRow);    BigtableSource source = new BigtableSource(config.withTableId(ValueProvider.StaticValueProvider.of(table)), null, Arrays.asList(service.getTableRange(table)), null);        assertSplitAtFractionFails(source, 0, 0.1, null);    assertSplitAtFractionFails(source, 0, 1.0, null);        assertSplitAtFractionSucceedsAndConsistent(source, 1, 0.333, null);    assertSplitAtFractionSucceedsAndConsistent(source, 1, 0.666, null);        assertSplitAtFractionFails(source, 3, 0.2, null);    assertSplitAtFractionSucceedsAndConsistent(source, 3, 0.571, null);    assertSplitAtFractionSucceedsAndConsistent(source, 3, 0.9, null);        assertSplitAtFractionFails(source, 6, 0.5, null);    assertSplitAtFractionSucceedsAndConsistent(source, 6, 0.7, null);}
public void beam_f30814_0() throws Exception
{    final String table = "TEST-MANY-ROWS-SPLITS-TABLE";    final int numRows = 1000;    final int numSamples = 10;    final int numSplits = 20;    final long bytesPerRow = 100L;        makeTableData(table, numRows);    service.setupSampleRowKeys(table, numSamples, bytesPerRow);        BigtableSource source = new BigtableSource(config.withTableId(ValueProvider.StaticValueProvider.of(table)), null, /*filter*/    Arrays.asList(ByteKeyRange.ALL_KEYS), null);    List<BigtableSource> splits = source.split(numRows * bytesPerRow / numSplits, null);        assertThat(splits, hasSize(numSplits));    assertSourcesEqualReferenceSource(source, splits, null);}
public void beam_f30815_0() throws Exception
{    final String table = "TEST-MANY-ROWS-SPLITS-TABLE-MULTIPLE-RANGES";    final int numRows = 1000;    final int numSamples = 10;    final int numSplits = 20;                            final int expectedNumSplits = 24;    final long bytesPerRow = 100L;        makeTableData(table, numRows);    service.setupSampleRowKeys(table, numSamples, bytesPerRow);    ByteKey splitKey1 = ByteKey.copyFrom("key000000330".getBytes(StandardCharsets.UTF_8));    ByteKey splitKey2 = ByteKey.copyFrom("key000000730".getBytes(StandardCharsets.UTF_8));    ByteKeyRange tableRange = service.getTableRange(table);    List<ByteKeyRange> keyRanges = Arrays.asList(tableRange.withEndKey(splitKey1), tableRange.withStartKey(splitKey1).withEndKey(splitKey2), tableRange.withStartKey(splitKey2));        BigtableSource source = new BigtableSource(config.withTableId(ValueProvider.StaticValueProvider.of(table)), null, /*filter*/    keyRanges, null);    BigtableSource referenceSource = new BigtableSource(config.withTableId(ValueProvider.StaticValueProvider.of(table)), null, /*filter*/    ImmutableList.of(service.getTableRange(table)), null);    List<BigtableSource> splits = source.split(numRows * bytesPerRow / numSplits, null);        assertThat(splits, hasSize(expectedNumSplits));    assertSourcesEqualReferenceSource(referenceSource, splits, null);}
public void beam_f30824_0() throws Exception
{    final String table = "table";    final String key = "key";    final String value = "value";    service.createTable(table);    Instant elementTimestamp = Instant.parse("2019-06-10T00:00:00");    Duration windowDuration = Duration.standardMinutes(1);    TestStream<KV<ByteString, Iterable<Mutation>>> writeInputs = TestStream.create(bigtableCoder).advanceWatermarkTo(elementTimestamp).addElements(makeWrite(key, value)).advanceWatermarkToInfinity();    TestStream<String> testInputs = TestStream.create(StringUtf8Coder.of()).advanceWatermarkTo(elementTimestamp).addElements("done").advanceWatermarkToInfinity();    PCollection<BigtableWriteResult> writes = p.apply("rows", writeInputs).apply("window rows", Window.<KV<ByteString, Iterable<Mutation>>>into(FixedWindows.of(windowDuration)).withAllowedLateness(Duration.ZERO)).apply("write", defaultWrite.withTableId(table).withWriteResults());    PCollection<String> inputs = p.apply("inputs", testInputs).apply("window inputs", Window.into(FixedWindows.of(windowDuration))).apply("wait", Wait.on(writes));    BoundedWindow expectedWindow = new IntervalWindow(elementTimestamp, windowDuration);    PAssert.that(inputs).inWindow(expectedWindow).containsInAnyOrder("done");    p.run();}
public void beam_f30825_0(ProcessContext ctx)
{    for (int i = 0; i < ctx.element(); i++) {        ctx.output(makeWrite("key", "value"));    }}
private static Row beam_f30834_0(ByteString key, ByteString value)
{        Column.Builder newColumn = TEST_COLUMN.toBuilder().addCells(Cell.newBuilder().setValue(value));    return Row.newBuilder().setKey(key).addFamilies(TEST_FAMILY.toBuilder().addColumns(newColumn)).build();}
private static List<Row> beam_f30835_0(String tableId, int numRows)
{    service.createTable(tableId);    Map<ByteString, ByteString> testData = service.getTable(tableId);    List<Row> testRows = new ArrayList<>(numRows);    for (int i = 0; i < numRows; ++i) {        ByteString key = ByteString.copyFromUtf8(String.format("key%09d", i));        ByteString value = ByteString.copyFromUtf8(String.format("value%09d", i));        testData.put(key, value);        testRows.add(makeRow(key, value));    }    return testRows;}
public List<SampleRowKeysResponse> beam_f30844_0(BigtableSource source)
{    List<SampleRowKeysResponse> samples = sampleRowKeys.get(source.getTableId().get());    checkNotNull(samples, "No samples found for table %s", source.getTableId().get());    return samples;}
 void beam_f30845_0(String tableId, int numSamples, long bytesPerRow)
{    verifyTableExists(tableId);    checkArgument(numSamples > 0, "Number of samples must be positive: %s", numSamples);    checkArgument(bytesPerRow > 0, "Bytes/Row must be positive: %s", bytesPerRow);    ImmutableList.Builder<SampleRowKeysResponse> ret = ImmutableList.builder();    SortedMap<ByteString, ByteString> rows = getTable(tableId);    int currentSample = 1;    int rowsSoFar = 0;    for (Map.Entry<ByteString, ByteString> entry : rows.entrySet()) {        if (((double) rowsSoFar) / rows.size() >= ((double) currentSample) / numSamples) {                        ret.add(SampleRowKeysResponse.newBuilder().setRowKey(entry.getKey()).setOffsetBytes(rowsSoFar * bytesPerRow).build());                        currentSample++;        }        ++rowsSoFar;    }        ret.add(SampleRowKeysResponse.newBuilder().setOffsetBytes(rows.size() * bytesPerRow).build());    sampleRowKeys.put(tableId, ret.build());}
public void beam_f30856_0()
{    MockitoAnnotations.initMocks(this);    BigtableOptions options = new BigtableOptions.Builder().setProjectId("project").setInstanceId("instance").build();    when(mockSession.getOptions()).thenReturn(options);    when(mockSession.createBulkMutation(eq(TABLE_NAME))).thenReturn(mockBulkMutation);    when(mockSession.getDataClient()).thenReturn(mockBigtableDataClient);}
public void beam_f30857_0() throws IOException
{    ByteKey start = ByteKey.copyFrom("a".getBytes(StandardCharsets.UTF_8));    ByteKey end = ByteKey.copyFrom("b".getBytes(StandardCharsets.UTF_8));    when(mockBigtableSource.getRanges()).thenReturn(Arrays.asList(ByteKeyRange.of(start, end)));    when(mockBigtableSource.getTableId()).thenReturn(StaticValueProvider.of("table_name"));    @SuppressWarnings("unchecked")    ResultScanner<Row> mockResultScanner = Mockito.mock(ResultScanner.class);    Row expectedRow = Row.newBuilder().setKey(ByteString.copyFromUtf8("a")).build();    when(mockResultScanner.next()).thenReturn(expectedRow).thenReturn(null);    when(mockBigtableDataClient.readRows(any(ReadRowsRequest.class))).thenReturn(mockResultScanner);    BigtableService.Reader underTest = new BigtableServiceImpl.BigtableReaderImpl(mockSession, mockBigtableSource);    underTest.start();    Assert.assertEquals(expectedRow, underTest.getCurrentRow());    Assert.assertFalse(underTest.advance());    underTest.close();    verify(mockResultScanner, times(1)).close();}
private List<KV<ByteString, ByteString>> beam_f30866_0(String tableName) throws IOException
{        RowRange range = RowRange.newBuilder().setStartKeyClosed(ByteString.EMPTY).setEndKeyOpen(ByteString.EMPTY).build();    RowSet rowSet = RowSet.newBuilder().addRowRanges(range).build();    ReadRowsRequest.Builder readRowsRequestBuilder = ReadRowsRequest.newBuilder().setTableName(tableName).setRows(rowSet);    ResultScanner<Row> scanner = session.getDataClient().readRows(readRowsRequestBuilder.build());    Row currentRow;    List<KV<ByteString, ByteString>> tableData = new ArrayList<>();    while ((currentRow = scanner.next()) != null) {        ByteString key = currentRow.getKey();        ByteString value = currentRow.getFamilies(0).getColumns(0).getCells(0).getValue();        tableData.add(KV.of(key, value));    }    scanner.close();    return tableData;}
private void beam_f30867_0(String tableName)
{    DeleteTableRequest.Builder deleteTableRequestBuilder = DeleteTableRequest.newBuilder().setName(tableName);    tableAdminClient.deleteTable(deleteTableRequestBuilder.build());}
public void beam_f30876_0() throws Exception
{    DatastoreV1.Read read = DatastoreIO.v1().read().withQuery(QUERY).withNamespace(NAMESPACE).withProjectId(PROJECT_ID).withLocalhost(LOCALHOST);    assertEquals(QUERY, read.getQuery());    assertEquals(PROJECT_ID, read.getProjectId().get());    assertEquals(NAMESPACE, read.getNamespace().get());    assertEquals(LOCALHOST, read.getLocalhost());}
public void beam_f30877_0() throws Exception
{    DatastoreV1.Read read = DatastoreIO.v1().read().withProjectId(PROJECT_ID).withLiteralGqlQuery(GQL_QUERY).withQuery(QUERY);    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("withQuery() and withLiteralGqlQuery() are exclusive");    read.expand(null);}
public void beam_f30886_0()
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    PTransform<PCollection<Entity>, ?> write = DatastoreIO.v1().write().withProjectId("myProject");    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveTransforms(write);    assertThat("DatastoreIO write should include the project in its primitive display data", displayData, hasItem(hasDisplayItem("projectId")));    assertThat("DatastoreIO write should include the upsertFn in its primitive display data", displayData, hasItem(hasDisplayItem("upsertFn")));}
public void beam_f30887_0()
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    PTransform<PCollection<Entity>, ?> write = DatastoreIO.v1().deleteEntity().withProjectId("myProject");    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveTransforms(write);    assertThat("DatastoreIO write should include the project in its primitive display data", displayData, hasItem(hasDisplayItem("projectId")));    assertThat("DatastoreIO write should include the deleteEntityFn in its primitive display data", displayData, hasItem(hasDisplayItem("deleteEntityFn")));}
public void beam_f30896_0()
{    Key key = makeKey("bird", "finch").build();    DeleteKeyFn deleteKeyFn = new DeleteKeyFn();    Mutation expectedMutation = makeDelete(key).build();    assertEquals(expectedMutation, deleteKeyFn.apply(key));}
public void beam_f30897_0()
{    DatastoreWriterFn datastoreWriter = new DatastoreWriterFn(PROJECT_ID, null);    DisplayData displayData = DisplayData.from(datastoreWriter);    assertThat(displayData, hasDisplayItem("projectId", PROJECT_ID));}
public void beam_f30906_0() throws Exception
{        int numSplits = 0;    int expectedNumSplits = 20;    long entityBytes = expectedNumSplits * DEFAULT_BUNDLE_SIZE_BYTES;        long timestamp = 1234L;    RunQueryRequest latestTimestampRequest = makeRequest(makeLatestTimestampQuery(NAMESPACE), NAMESPACE);    RunQueryResponse latestTimestampResponse = makeLatestTimestampResponse(timestamp);        RunQueryRequest statRequest = makeRequest(makeStatKindQuery(NAMESPACE, timestamp), NAMESPACE);    RunQueryResponse statResponse = makeStatKindResponse(entityBytes);    when(mockDatastore.runQuery(latestTimestampRequest)).thenReturn(latestTimestampResponse);    when(mockDatastore.runQuery(statRequest)).thenReturn(statResponse);    when(mockQuerySplitter.getSplits(eq(QUERY), any(PartitionId.class), eq(expectedNumSplits), any(Datastore.class))).thenReturn(splitQuery(QUERY, expectedNumSplits));    SplitQueryFn splitQueryFn = new SplitQueryFn(V_1_OPTIONS, numSplits, mockDatastoreFactory);    DoFnTester<Query, Query> doFnTester = DoFnTester.of(splitQueryFn);    doFnTester.setCloningBehavior(CloningBehavior.DO_NOT_CLONE);    List<Query> queries = doFnTester.processBundle(QUERY);    assertEquals(expectedNumSplits, queries.size());    verify(mockQuerySplitter, times(1)).getSplits(eq(QUERY), any(PartitionId.class), eq(expectedNumSplits), any(Datastore.class));    verify(mockDatastore, times(1)).runQuery(latestTimestampRequest);    verify(mockDatastore, times(1)).runQuery(statRequest);}
public void beam_f30907_0() throws Exception
{    Query queryWithLimit = QUERY.toBuilder().setLimit(Int32Value.newBuilder().setValue(1)).build();    SplitQueryFn splitQueryFn = new SplitQueryFn(V_1_OPTIONS, 10, mockDatastoreFactory);    DoFnTester<Query, Query> doFnTester = DoFnTester.of(splitQueryFn);    doFnTester.setCloningBehavior(CloningBehavior.DO_NOT_CLONE);    List<Query> queries = doFnTester.processBundle(queryWithLimit);    assertEquals(1, queries.size());    verifyNoMoreInteractions(mockDatastore);    verifyNoMoreInteractions(mockQuerySplitter);}
public void beam_f30916_0()
{    DatastoreV1.WriteBatcher writeBatcher = new DatastoreV1.WriteBatcherImpl();    writeBatcher.start();    assertEquals(DatastoreV1.DATASTORE_BATCH_UPDATE_ENTITIES_START, writeBatcher.nextBatchSize(0));}
public void beam_f30917_0()
{    DatastoreV1.WriteBatcher writeBatcher = new DatastoreV1.WriteBatcherImpl();    writeBatcher.start();    writeBatcher.addRequestLatency(0, 1000, 200);    writeBatcher.addRequestLatency(0, 1000, 200);    assertEquals(DatastoreV1.DATASTORE_BATCH_UPDATE_ENTITIES_LIMIT, writeBatcher.nextBatchSize(0));}
private static Query beam_f30926_0(String namespace, long timestamp)
{    Query.Builder statQuery = Query.newBuilder();    if (namespace == null) {        statQuery.addKindBuilder().setName("__Stat_Kind__");    } else {        statQuery.addKindBuilder().setName("__Stat_Ns_Kind__");    }    statQuery.setFilter(makeAndFilter(makeFilter("kind_name", EQUAL, makeValue(KIND).build()).build(), makeFilter("timestamp", EQUAL, makeValue(timestamp * 1000000L).build()).build()));    return statQuery.build();}
private static Query beam_f30927_0(String namespace)
{    Query.Builder timestampQuery = Query.newBuilder();    if (namespace == null) {        timestampQuery.addKindBuilder().setName("__Stat_Total__");    } else {        timestampQuery.addKindBuilder().setName("__Stat_Ns_Total__");    }    timestampQuery.addOrder(makeOrder("timestamp", DESCENDING));    timestampQuery.setLimit(Int32Value.newBuilder().setValue(1));    return timestampQuery.build();}
public void beam_f30938_0() throws Exception
{    testE2EV1ReadWithGQLQuery(0);}
public void beam_f30939_0() throws Exception
{    testE2EV1ReadWithGQLQuery(99);}
 static void beam_f30948_1(V1TestOptions options, String project, String ancestor) throws Exception
{    Datastore datastore = getDatastore(options, project);    Query query = V1TestUtil.makeAncestorKindQuery(options.getKind(), options.getNamespace(), ancestor);    V1TestReader reader = new V1TestReader(datastore, query, options.getNamespace());    V1TestWriter writer = new V1TestWriter(datastore, new DeleteMutationBuilder());    long numEntities = 0;    while (reader.advance()) {        Entity entity = reader.getCurrent();        numEntities++;        writer.write(entity);    }    writer.close();    }
 static long beam_f30949_0(V1TestOptions options, String project, String ancestor) throws Exception
{        Datastore datastore = V1TestUtil.getDatastore(options, project);    Query query = V1TestUtil.makeAncestorKindQuery(options.getKind(), options.getNamespace(), ancestor);    V1TestReader reader = new V1TestReader(datastore, query, options.getNamespace());    long numEntitiesRead = 0;    while (reader.advance()) {        reader.getCurrent();        numEntitiesRead++;    }    return numEntitiesRead;}
private Iterator<EntityResult> beam_f30958_0() throws DatastoreException
{    Query.Builder query = this.query.toBuilder();    query.setLimit(Int32Value.newBuilder().setValue(QUERY_BATCH_LIMIT));    if (currentBatch != null && !currentBatch.getEndCursor().isEmpty()) {        query.setStartCursor(currentBatch.getEndCursor());    }    RunQueryRequest request = makeRequest(query.build(), namespace);    RunQueryResponse response = datastore.runQuery(request);    currentBatch = response.getBatch();    int numFetch = currentBatch.getEntityResultsCount();        moreResults = (numFetch == QUERY_BATCH_LIMIT) || (currentBatch.getMoreResults() == NOT_FINISHED);        if (numFetch == 0) {        return null;    }    return currentBatch.getEntityResultsList().iterator();}
public void beam_f30959_0()
{    PipelineOptionsFactory.register(V1TestOptions.class);    options = TestPipeline.testingPipelineOptions().as(V1TestOptions.class);    project = TestPipeline.testingPipelineOptions().as(GcpOptions.class).getProject();    ancestor = UUID.randomUUID().toString();}
public void beam_f30968_0()
{    thrown.expect(NumberFormatException.class);    PubsubClient.extractTimestamp(null, "not-a-date", null);}
public void beam_f30969_0()
{    thrown.expect(RuntimeException.class);    thrown.expectMessage("PubSub message is missing a value for timestamp attribute myAttribute");    PubsubClient.extractTimestamp("myAttribute", null, null);}
public void beam_f30978_0()
{    thrown.expect(NumberFormatException.class);    parse("not-a-timestamp");}
public void beam_f30979_0()
{    thrown.expect(NumberFormatException.class);    parse("null");}
public void beam_f30988_0() throws IOException
{    client.close();    inProcessChannel.shutdownNow();}
public void beam_f30989_0() throws IOException
{    String expectedSubscription = SUBSCRIPTION.getPath();    final PullRequest expectedRequest = PullRequest.newBuilder().setSubscription(expectedSubscription).setReturnImmediately(true).setMaxMessages(10).build();    Timestamp timestamp = Timestamp.newBuilder().setSeconds(PUB_TIME / 1000).setNanos((int) (PUB_TIME % 1000) * 1000).build();    PubsubMessage expectedPubsubMessage = PubsubMessage.newBuilder().setMessageId(MESSAGE_ID).setData(ByteString.copyFrom(DATA.getBytes(StandardCharsets.UTF_8))).setPublishTime(timestamp).putAllAttributes(ATTRIBUTES).putAllAttributes(ImmutableMap.of(TIMESTAMP_ATTRIBUTE, String.valueOf(MESSAGE_TIME), ID_ATTRIBUTE, RECORD_ID)).build();    ReceivedMessage expectedReceivedMessage = ReceivedMessage.newBuilder().setMessage(expectedPubsubMessage).setAckId(ACK_ID).build();    final PullResponse response = PullResponse.newBuilder().addAllReceivedMessages(ImmutableList.of(expectedReceivedMessage)).build();    final List<PullRequest> requestsReceived = new ArrayList<>();    SubscriberImplBase subscriberImplBase = new SubscriberImplBase() {        @Override        public void pull(PullRequest request, StreamObserver<PullResponse> responseObserver) {            requestsReceived.add(request);            responseObserver.onNext(response);            responseObserver.onCompleted();        }    };    Server server = InProcessServerBuilder.forName(channelName).addService(subscriberImplBase).build().start();    try {        List<IncomingMessage> acutalMessages = client.pull(REQ_TIME, SUBSCRIPTION, 10, true);        assertEquals(1, acutalMessages.size());        IncomingMessage actualMessage = acutalMessages.get(0);        assertEquals(ACK_ID, actualMessage.ackId);        assertEquals(DATA, new String(actualMessage.elementBytes, StandardCharsets.UTF_8));        assertEquals(RECORD_ID, actualMessage.recordId);        assertEquals(REQ_TIME, actualMessage.requestTimeMsSinceEpoch);        assertEquals(MESSAGE_TIME, actualMessage.timestampMsSinceEpoch);        assertEquals(expectedRequest, Iterables.getOnlyElement(requestsReceived));    } finally {        server.shutdownNow();    }}
public void beam_f30998_0()
{    String subscription = "projects/project/subscriptions/subscription";    PubsubIO.Read<String> read = PubsubIO.readStrings().fromSubscription(StaticValueProvider.of(subscription)).withTimestampAttribute("myTimestamp").withIdAttribute("myId");    DisplayData displayData = DisplayData.from(read);    assertThat(displayData, hasDisplayItem("subscription", subscription));    assertThat(displayData, hasDisplayItem("timestampAttribute", "myTimestamp"));    assertThat(displayData, hasDisplayItem("idAttribute", "myId"));}
public void beam_f30999_0()
{    String subscription = "projects/project/subscriptions/subscription";    PubsubIO.Read<String> read = PubsubIO.readStrings().fromSubscription(StaticValueProvider.of(subscription));    assertNull(read.getTopicProvider());    assertNotNull(read.getSubscriptionProvider());    assertNotNull(DisplayData.from(read));}
public void beam_f31008_0()
{    DisplayDataEvaluator evaluator = DisplayDataEvaluator.create();    PubsubIO.Write<?> write = PubsubIO.writeStrings().to("projects/project/topics/topic");    Set<DisplayData> displayData = evaluator.displayDataForPrimitiveTransforms(write);    assertThat("PubsubIO.Write should include the topic in its primitive display data", displayData, hasItem(hasDisplayItem("topic")));}
public String beam_f31009_0()
{    return MoreObjects.toStringHelper(getClass()).add("intField", intField).add("stringField", stringField).toString();}
public void beam_f31018_0()
{    AvroCoder<AvroGeneratedUser> coder = AvroCoder.of(AvroGeneratedUser.class);    List<AvroGeneratedUser> inputs = ImmutableList.of(new AvroGeneratedUser("Bob", 256, null), new AvroGeneratedUser("Alice", 128, null), new AvroGeneratedUser("Ted", null, "white"));    setupTestClient(inputs, coder);    PCollection<AvroGeneratedUser> read = readPipeline.apply(PubsubIO.readAvrosWithBeamSchema(AvroGeneratedUser.class).fromSubscription(SUBSCRIPTION.getPath()).withClock(CLOCK).withClientFactory(clientFactory));    PAssert.that(read).containsInAnyOrder(inputs);    readPipeline.run();}
public void beam_f31019_0()
{    String topic = "projects/project/topics/topic";    PubsubIO.Write<?> write = PubsubIO.writeStrings().to(topic).withClientFactory(PubsubGrpcClient.FACTORY).withTimestampAttribute("myTimestamp").withIdAttribute("myId");    DisplayData displayData = DisplayData.from(write);    assertThat(displayData, hasDisplayItem("topic", topic));    assertThat(displayData, hasDisplayItem("timestampAttribute", "myTimestamp"));    assertThat(displayData, hasDisplayItem("idAttribute", "myId"));}
public void beam_f31028_0() throws IOException
{            client = new PubsubJsonClient(null, null, mockPubsub);    String expectedTopic = TOPIC.getPath();    PubsubMessage expectedPubsubMessage = new PubsubMessage().encodeData(DATA.getBytes(StandardCharsets.UTF_8)).setAttributes(ImmutableMap.<String, String>builder().put("k", "v").build());    PublishRequest expectedRequest = new PublishRequest().setMessages(ImmutableList.of(expectedPubsubMessage));    PublishResponse expectedResponse = new PublishResponse().setMessageIds(ImmutableList.of(MESSAGE_ID));    when((Object) (mockPubsub.projects().topics().publish(expectedTopic, expectedRequest).execute())).thenReturn(expectedResponse);    Map<String, String> attrs = new HashMap<>();    attrs.put("k", "v");    OutgoingMessage actualMessage = new OutgoingMessage(DATA.getBytes(StandardCharsets.UTF_8), attrs, MESSAGE_TIME, RECORD_ID);    int n = client.publish(TOPIC, ImmutableList.of(actualMessage));    assertEquals(1, n);}
public void beam_f31029_0() throws Exception
{    ListTopicsResponse expectedResponse1 = new ListTopicsResponse();    expectedResponse1.setTopics(Collections.singletonList(buildTopic(1)));    expectedResponse1.setNextPageToken("AVgJH3Z7aHxiDBs");    ListTopicsResponse expectedResponse2 = new ListTopicsResponse();    expectedResponse2.setTopics(Collections.singletonList(buildTopic(2)));    Topics.List request = mockPubsub.projects().topics().list(PROJECT.getPath());    when((Object) (request.execute())).thenReturn(expectedResponse1, expectedResponse2);    List<TopicPath> topicPaths = client.listTopics(PROJECT);    assertEquals(2, topicPaths.size());}
public void beam_f31038_0() throws Exception
{    TypeDescriptor<PubsubMessage> typeDescriptor = new TypeDescriptor<PubsubMessage>() {    };    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(typeDescriptor));}
public void beam_f31039_0() throws Exception
{    SerializableUtils.ensureSerializableByCoder(TEST_CODER, TEST_VALUE, "error");}
public void beam_f31048_0() throws IOException
{    final AtomicLong now = new AtomicLong();    Clock clock = now::get;    IncomingMessage expectedIncomingMessage = new IncomingMessage(DATA.getBytes(StandardCharsets.UTF_8), null, MESSAGE_TIME, REQ_TIME, ACK_ID, MESSAGE_ID);    try (PubsubTestClientFactory factory = PubsubTestClient.createFactoryForPull(clock, SUBSCRIPTION, ACK_TIMEOUT_S, Lists.newArrayList(expectedIncomingMessage))) {        try (PubsubTestClient client = (PubsubTestClient) factory.newClient(null, null, null)) {            now.set(REQ_TIME);            client.advance();            List<IncomingMessage> incomingMessages = client.pull(now.get(), SUBSCRIPTION, 1, true);            assertEquals(1, incomingMessages.size());            assertEquals(expectedIncomingMessage, incomingMessages.get(0));                        now.addAndGet((ACK_TIMEOUT_S + 10) * 1000);            client.advance();            incomingMessages = client.pull(now.get(), SUBSCRIPTION, 1, true);            assertEquals(1, incomingMessages.size());            assertEquals(expectedIncomingMessage.withRequestTime(now.get()), incomingMessages.get(0));            now.addAndGet(10 * 1000);            client.advance();                        client.modifyAckDeadline(SUBSCRIPTION, ImmutableList.of(ACK_ID), 20);                        now.addAndGet(30 * 1000);            client.advance();            incomingMessages = client.pull(now.get(), SUBSCRIPTION, 1, true);            assertEquals(1, incomingMessages.size());            assertEquals(expectedIncomingMessage.withRequestTime(now.get()), incomingMessages.get(0));                        client.modifyAckDeadline(SUBSCRIPTION, ImmutableList.of(ACK_ID), 20);                        now.addAndGet(15 * 1000);            client.advance();            client.acknowledge(SUBSCRIPTION, ImmutableList.of(ACK_ID));        }    }}
public void beam_f31049_0() throws IOException
{    OutgoingMessage expectedOutgoingMessage = new OutgoingMessage(DATA.getBytes(StandardCharsets.UTF_8), null, MESSAGE_TIME, MESSAGE_ID);    try (PubsubTestClientFactory factory = PubsubTestClient.createFactoryForPublish(TOPIC, Sets.newHashSet(expectedOutgoingMessage), ImmutableList.of())) {        try (PubsubTestClient client = (PubsubTestClient) factory.newClient(null, null, null)) {            client.publish(TOPIC, ImmutableList.of(expectedOutgoingMessage));        }    }}
private void beam_f31058_0()
{    setupOneMessage(ImmutableList.of(new IncomingMessage(DATA.getBytes(StandardCharsets.UTF_8), null, TIMESTAMP, 0, ACK_ID, RECORD_ID)));}
public void beam_f31059_0() throws IOException
{    factory.close();    now = null;    clock = null;    primSource = null;    factory = null;}
public void beam_f31068_0() throws IOException
{    Map<String, Integer> dataToMessageNum = new HashMap<>();    final int m = 97;    final int n = 10000;    List<IncomingMessage> incoming = new ArrayList<>();    for (int i = 0; i < n; i++) {                int messageNum = ((i / m) * m) + (m - 1) - (i % m);        String data = String.format("data_%d", messageNum);        dataToMessageNum.put(data, messageNum);        String recid = String.format("recordid_%d", messageNum);        String ackId = String.format("ackid_%d", messageNum);        incoming.add(new IncomingMessage(data.getBytes(StandardCharsets.UTF_8), null, messageNumToTimestamp(messageNum), 0, ackId, recid));    }    setupOneMessage(incoming);    PubsubReader reader = primSource.createReader(p.getOptions(), null);    PubsubTestClient pubsubClient = (PubsubTestClient) reader.getPubsubClient();    for (int i = 0; i < n; i++) {        if (i == 0) {            assertTrue(reader.start());        } else {            assertTrue(reader.advance());        }                now.addAndGet(30);        pubsubClient.advance();        String data = data(reader.getCurrent());        Integer messageNum = dataToMessageNum.remove(data);                assertNotNull(messageNum);                assertEquals(new Instant(messageNumToTimestamp(messageNum)), reader.getCurrentTimestamp());                String recid = String.format("recordid_%d", messageNum);        assertArrayEquals(recid.getBytes(StandardCharsets.UTF_8), reader.getCurrentRecordId());        if (i % 1000 == 999) {                        long watermark = reader.getWatermark().getMillis();            long minOutstandingTimestamp = Long.MAX_VALUE;            for (Integer outstandingMessageNum : dataToMessageNum.values()) {                minOutstandingTimestamp = Math.min(minOutstandingTimestamp, messageNumToTimestamp(outstandingMessageNum));            }            assertThat(watermark, lessThanOrEqualTo(minOutstandingTimestamp));                        PubsubCheckpoint checkpoint = reader.getCheckpointMark();            if (i % 2000 == 1999) {                checkpoint.finalizeCheckpoint();            }        }    }        assertFalse(reader.advance());        assertTrue(dataToMessageNum.isEmpty());    reader.close();}
public void beam_f31069_0() throws Exception
{    TopicPath topicPath = PubsubClient.topicPathFromName("my_project", "my_topic");    factory = PubsubTestClient.createFactoryForCreateSubscription();    PubsubUnboundedSource source = new PubsubUnboundedSource(factory, StaticValueProvider.of(PubsubClient.projectPathFromId("my_project")), StaticValueProvider.of(topicPath), null, /* subscription */    null, /* timestampLabel */    null, /* idLabel */    false);    assertThat(source.getSubscription(), nullValue());    assertThat(source.getSubscription(), nullValue());    PipelineOptions options = PipelineOptionsFactory.create();    List<PubsubSource> splits = (new PubsubSource(source)).split(3, options);        assertThat(splits, hasSize(greaterThan(0)));    for (PubsubSource split : splits) {                assertThat(split, equalTo(splits.get(0)));    }    assertThat(splits.get(0).subscriptionPath, not(nullValue()));}
public void beam_f31078_0() throws Exception
{    SpannerSchema.Builder builder = SpannerSchema.builder();    builder.addColumn("test", "key", "FLOAT64");    builder.addKeyPart("test", "key", false);    builder.addColumn("test", "keydesc", "FLOAT64");    builder.addKeyPart("test", "keydesc", true);    SpannerSchema schema = builder.build();    List<Mutation> sortedMutations = Arrays.asList(Mutation.newInsertOrUpdateBuilder("test").set("key").to(1.0).set("keydesc").to(0.).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(2.).set("keydesc").to((Long) null).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(2.).set("keydesc").to(10.).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(2.).set("keydesc").to(9.).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to(2.).set("keydesc").to(0.).build());    verifyEncodedOrdering(schema, sortedMutations);}
public void beam_f31079_0() throws Exception
{    SpannerSchema.Builder builder = SpannerSchema.builder();    builder.addColumn("test", "key", "STRING");    builder.addKeyPart("test", "key", false);    builder.addColumn("test", "keydesc", "STRING");    builder.addKeyPart("test", "keydesc", true);    SpannerSchema schema = builder.build();    List<Mutation> sortedMutations = Arrays.asList(Mutation.newInsertOrUpdateBuilder("test").set("key").to("a").set("keydesc").to("bc").build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to("b").set("keydesc").to((String) null).build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to("b").set("keydesc").to("z").build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to("b").set("keydesc").to("y").build(), Mutation.newInsertOrUpdateBuilder("test").set("key").to("b").set("keydesc").to("a").build());    verifyEncodedOrdering(schema, sortedMutations);}
public void beam_f31088_0() throws Exception
{    Mutation int64 = Mutation.newInsertOrUpdateBuilder("test").set("one").toInt64Array(new long[] { 1L, 2L, 3L }).build();    Mutation float64 = Mutation.newInsertOrUpdateBuilder("test").set("one").toFloat64Array(new double[] { 1., 2. }).build();    Mutation bool = Mutation.newInsertOrUpdateBuilder("test").set("one").toBoolArray(new boolean[] { true, true, false, true }).build();    assertThat(MutationSizeEstimator.sizeOf(int64), is(24L));    assertThat(MutationSizeEstimator.sizeOf(float64), is(16L));    assertThat(MutationSizeEstimator.sizeOf(bool), is(4L));}
public void beam_f31089_0() throws Exception
{    Mutation int64 = Mutation.newInsertOrUpdateBuilder("test").set("one").toInt64Array((long[]) null).build();    Mutation float64 = Mutation.newInsertOrUpdateBuilder("test").set("one").toFloat64Array((double[]) null).build();    Mutation bool = Mutation.newInsertOrUpdateBuilder("test").set("one").toBoolArray((boolean[]) null).build();    assertThat(MutationSizeEstimator.sizeOf(int64), is(0L));    assertThat(MutationSizeEstimator.sizeOf(float64), is(0L));    assertThat(MutationSizeEstimator.sizeOf(bool), is(0L));}
 Long beam_f31098_0(OrderedCode orderedCode)
{    return orderedCode.readNumIncreasing();}
 Long beam_f31099_0(OrderedCode orderedCode)
{    return orderedCode.readNumDecreasing();}
public void beam_f31108_0()
{    testOrdering(UnsignedNumber.TEST_CASES);}
public void beam_f31109_0()
{    testEncoding(BytesTest.TEST_CASES);}
public void beam_f31118_0()
{    byte[] first = { 'a', 'b', 'c' };    byte[] second = { 'd', 'e', 'f' };    byte[] last = { 'x', 'y', 'z' };    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeBytesDecreasing(first);    byte[] firstEncoded = orderedCode.getEncodedBytes();    assertArrayEquals(orderedCode.readBytesDecreasing(), first);    orderedCode.writeBytesDecreasing(first);    orderedCode.writeBytesDecreasing(second);    orderedCode.writeBytesDecreasing(last);    byte[] allEncoded = orderedCode.getEncodedBytes();    assertArrayEquals(orderedCode.readBytesDecreasing(), first);    assertArrayEquals(orderedCode.readBytesDecreasing(), second);    assertArrayEquals(orderedCode.readBytesDecreasing(), last);    orderedCode = new OrderedCode(firstEncoded);    orderedCode.writeBytesDecreasing(second);    orderedCode.writeBytesDecreasing(last);    assertArrayEquals(orderedCode.getEncodedBytes(), allEncoded);    assertArrayEquals(orderedCode.readBytesDecreasing(), first);    assertArrayEquals(orderedCode.readBytesDecreasing(), second);    assertArrayEquals(orderedCode.readBytesDecreasing(), last);    orderedCode = new OrderedCode(allEncoded);    assertArrayEquals(orderedCode.readBytesDecreasing(), first);    assertArrayEquals(orderedCode.readBytesDecreasing(), second);    assertArrayEquals(orderedCode.readBytesDecreasing(), last);}
public void beam_f31119_0()
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeNumIncreasing(0);    orderedCode.writeNumIncreasing(1);    orderedCode.writeNumIncreasing(Long.MIN_VALUE);    orderedCode.writeNumIncreasing(Long.MAX_VALUE);    assertEquals(0, orderedCode.readNumIncreasing());    assertEquals(1, orderedCode.readNumIncreasing());    assertEquals(Long.MIN_VALUE, orderedCode.readNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readNumIncreasing());}
public void beam_f31128_0()
{    assertDecodedSignedNumIncreasingEquals(Long.MIN_VALUE, "003f8000000000000000");    assertDecodedSignedNumIncreasingEquals(Long.MIN_VALUE + 1, "003f8000000000000001");    assertDecodedSignedNumIncreasingEquals(Integer.MIN_VALUE - 1L, "077fffffff");    assertDecodedSignedNumIncreasingEquals(Integer.MIN_VALUE, "0780000000");    assertDecodedSignedNumIncreasingEquals(Integer.MIN_VALUE + 1, "0780000001");    assertDecodedSignedNumIncreasingEquals(-65, "3fbf");    assertDecodedSignedNumIncreasingEquals(-64, "40");    assertDecodedSignedNumIncreasingEquals(-63, "41");    assertDecodedSignedNumIncreasingEquals(-3, "7d");    assertDecodedSignedNumIncreasingEquals(-2, "7e");    assertDecodedSignedNumIncreasingEquals(-1, "7f");    assertDecodedSignedNumIncreasingEquals(0, "80");    assertDecodedSignedNumIncreasingEquals(1, "81");    assertDecodedSignedNumIncreasingEquals(2, "82");    assertDecodedSignedNumIncreasingEquals(3, "83");    assertDecodedSignedNumIncreasingEquals(63, "bf");    assertDecodedSignedNumIncreasingEquals(64, "c040");    assertDecodedSignedNumIncreasingEquals(65, "c041");    assertDecodedSignedNumIncreasingEquals(Integer.MAX_VALUE - 1, "f87ffffffe");    assertDecodedSignedNumIncreasingEquals(Integer.MAX_VALUE, "f87fffffff");    assertDecodedSignedNumIncreasingEquals(Integer.MAX_VALUE + 1L, "f880000000");    assertDecodedSignedNumIncreasingEquals(Long.MAX_VALUE - 1, "ffc07ffffffffffffffe");    assertDecodedSignedNumIncreasingEquals(Long.MAX_VALUE, "ffc07fffffffffffffff");}
private static void beam_f31129_0(long num)
{    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeSignedNumIncreasing(num);    assertEquals("Unexpected result when decoding writeSignedNumIncreasing(" + num + ")", num, orderedCode.readSignedNumIncreasing());    assertFalse("Unexpected remaining encoded bytes after decoding " + num, orderedCode.hasRemainingEncodedBytes());}
public void beam_f31138_0()
{    byte[] escapeChars = new byte[] { OrderedCode.ESCAPE1, OrderedCode.NULL_CHARACTER, OrderedCode.SEPARATOR, OrderedCode.ESCAPE2, OrderedCode.INFINITY, OrderedCode.FF_CHARACTER };    byte[] anotherArray = new byte[] { 'a', 'b', 'c', 'd', 'e' };    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeTrailingBytes(escapeChars);    assertArrayEquals(orderedCode.getEncodedBytes(), escapeChars);    assertArrayEquals(orderedCode.readTrailingBytes(), escapeChars);    try {        orderedCode.readInfinity();        fail("Expected IllegalArgumentException.");    } catch (IllegalArgumentException e) {        }    orderedCode = new OrderedCode();    orderedCode.writeTrailingBytes(anotherArray);    assertArrayEquals(orderedCode.getEncodedBytes(), anotherArray);    assertArrayEquals(orderedCode.readTrailingBytes(), anotherArray);}
public void beam_f31139_0()
{    byte[] first = { 'a', 'b', 'c' };    byte[] second = { 'd', 'e', 'f' };    byte[] last = { 'x', 'y', 'z' };    byte[] escapeChars = new byte[] { OrderedCode.ESCAPE1, OrderedCode.NULL_CHARACTER, OrderedCode.SEPARATOR, OrderedCode.ESCAPE2, OrderedCode.INFINITY, OrderedCode.FF_CHARACTER };    OrderedCode orderedCode = new OrderedCode();    orderedCode.writeBytes(first);    orderedCode.writeBytes(second);    orderedCode.writeBytes(last);    orderedCode.writeInfinity();    orderedCode.writeNumIncreasing(0);    orderedCode.writeNumIncreasing(1);    orderedCode.writeNumIncreasing(Long.MIN_VALUE);    orderedCode.writeNumIncreasing(Long.MAX_VALUE);    orderedCode.writeSignedNumIncreasing(0);    orderedCode.writeSignedNumIncreasing(1);    orderedCode.writeSignedNumIncreasing(Long.MIN_VALUE);    orderedCode.writeSignedNumIncreasing(Long.MAX_VALUE);    orderedCode.writeTrailingBytes(escapeChars);    byte[] allEncoded = orderedCode.getEncodedBytes();    assertArrayEquals(orderedCode.readBytes(), first);    assertArrayEquals(orderedCode.readBytes(), second);    assertFalse(orderedCode.readInfinity());    assertArrayEquals(orderedCode.readBytes(), last);    assertTrue(orderedCode.readInfinity());    assertEquals(0, orderedCode.readNumIncreasing());    assertEquals(1, orderedCode.readNumIncreasing());    assertFalse(orderedCode.readInfinity());    assertEquals(Long.MIN_VALUE, orderedCode.readNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readNumIncreasing());    assertEquals(0, orderedCode.readSignedNumIncreasing());    assertEquals(1, orderedCode.readSignedNumIncreasing());    assertFalse(orderedCode.readInfinity());    assertEquals(Long.MIN_VALUE, orderedCode.readSignedNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readSignedNumIncreasing());    assertArrayEquals(orderedCode.getEncodedBytes(), escapeChars);    assertArrayEquals(orderedCode.readTrailingBytes(), escapeChars);    orderedCode = new OrderedCode(allEncoded);    assertArrayEquals(orderedCode.readBytes(), first);    assertArrayEquals(orderedCode.readBytes(), second);    assertFalse(orderedCode.readInfinity());    assertArrayEquals(orderedCode.readBytes(), last);    assertTrue(orderedCode.readInfinity());    assertEquals(0, orderedCode.readNumIncreasing());    assertEquals(1, orderedCode.readNumIncreasing());    assertFalse(orderedCode.readInfinity());    assertEquals(Long.MIN_VALUE, orderedCode.readNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readNumIncreasing());    assertEquals(0, orderedCode.readSignedNumIncreasing());    assertEquals(1, orderedCode.readSignedNumIncreasing());    assertFalse(orderedCode.readInfinity());    assertEquals(Long.MIN_VALUE, orderedCode.readSignedNumIncreasing());    assertEquals(Long.MAX_VALUE, orderedCode.readSignedNumIncreasing());    assertArrayEquals(orderedCode.getEncodedBytes(), escapeChars);    assertArrayEquals(orderedCode.readTrailingBytes(), escapeChars);}
public boolean beam_f31148_0(Statement argument)
{    if (!(argument instanceof Statement)) {        return false;    }    Statement st = (Statement) argument;    return st.getSql().contains("information_schema.columns");}
private void beam_f31149_0(ReadOnlyTransaction tx, List<Struct> rows)
{    Type type = Type.struct(Type.StructField.of("table_name", Type.string()), Type.StructField.of("column_name", Type.string()), Type.StructField.of("column_ordering", Type.string()));    when(tx.executeQuery(argThat(new ArgumentMatcher<Statement>() {        @Override        public boolean matches(Statement argument) {            if (!(argument instanceof Statement)) {                return false;            }            Statement st = (Statement) argument;            return st.getSql().contains("information_schema.index_columns");        }    }))).thenReturn(ResultSets.forRows(type, rows));}
public void beam_f31158_0() throws Exception
{    Timestamp timestamp = Timestamp.ofTimeMicroseconds(12345);    TimestampBound timestampBound = TimestampBound.ofReadTimestamp(timestamp);    SpannerConfig spannerConfig = SpannerConfig.create().withProjectId("test").withInstanceId("123").withDatabaseId("aaa").withServiceFactory(serviceFactory);    PCollectionView<Transaction> tx = pipeline.apply("tx", SpannerIO.createTransaction().withSpannerConfig(spannerConfig).withTimestampBound(timestampBound));    PCollection<ReadOperation> reads = pipeline.apply(Create.of(ReadOperation.create().withQuery("SELECT * FROM users"), ReadOperation.create().withTable("users").withColumns("id", "name")));    PCollection<Struct> one = reads.apply("read all", SpannerIO.readAll().withSpannerConfig(spannerConfig).withTransaction(tx));    BatchTransactionId txId = new FakeBatchTransactionId("tx");    when(mockBatchTx.getBatchTransactionId()).thenReturn(txId);    when(serviceFactory.mockBatchClient().batchReadOnlyTransaction(timestampBound)).thenReturn(mockBatchTx);    when(serviceFactory.mockBatchClient().batchReadOnlyTransaction(any(BatchTransactionId.class))).thenReturn(mockBatchTx);    Partition fakePartition = FakePartitionFactory.createFakeReadPartition(ByteString.copyFromUtf8("partition"));    when(mockBatchTx.partitionQuery(any(PartitionOptions.class), eq(Statement.of("SELECT * FROM users")))).thenReturn(Arrays.asList(fakePartition, fakePartition));    when(mockBatchTx.partitionRead(any(PartitionOptions.class), eq("users"), eq(KeySet.all()), eq(Arrays.asList("id", "name")))).thenReturn(Arrays.asList(fakePartition));    when(mockBatchTx.execute(any(Partition.class))).thenReturn(ResultSets.forRows(FAKE_TYPE, FAKE_ROWS.subList(0, 2)), ResultSets.forRows(FAKE_TYPE, FAKE_ROWS.subList(2, 4)), ResultSets.forRows(FAKE_TYPE, FAKE_ROWS.subList(4, 6)));    PAssert.that(one).containsInAnyOrder(FAKE_ROWS);    pipeline.run();}
public void beam_f31159_0() throws Exception
{    MockitoAnnotations.initMocks(this);    serviceFactory = new FakeServiceFactory();    ReadOnlyTransaction tx = mock(ReadOnlyTransaction.class);    when(serviceFactory.mockDatabaseClient().readOnlyTransaction()).thenReturn(tx);        when(serviceFactory.mockDatabaseClient().writeAtLeastOnce(mutationBatchesCaptor.capture())).thenReturn(null);        preparePkMetadata(tx, Arrays.asList(pkMetadata("tEsT", "key", "ASC")));    prepareColumnMetadata(tx, Arrays.asList(columnMetadata("tEsT", "key", "INT64", CELLS_PER_KEY)));}
public void beam_f31168_0() throws Exception
{    SpannerIO.Write write = SpannerIO.write().withDatabaseId("123");    thrown.expect(NullPointerException.class);    thrown.expectMessage("requires instance id to be set with");    write.expand(null);}
public void beam_f31169_0() throws Exception
{    SpannerIO.Write write = SpannerIO.write().withInstanceId("123");    thrown.expect(NullPointerException.class);    thrown.expectMessage("requires database id to be set with");    write.expand(null);}
public void beam_f31178_0()
{    Mutation all = Mutation.delete("test", KeySet.all());    Mutation prefix = Mutation.delete("test", KeySet.prefixRange(Key.of(1L)));    Mutation range = Mutation.delete("test", KeySet.range(KeyRange.openOpen(Key.of(1L), Key.newBuilder().build())));    MutationGroup[] mutationGroups = new MutationGroup[] { g(m(1L)), g(m(2L), m(3L)),     g(m(1L), m(3L), m(4L), m(5L)), g(del(1L)),     g(del(5L, 6L)), g(all), g(prefix), g(range) };    long mutationSize = MutationSizeEstimator.sizeOf(m(1L));    BatchableMutationFilterFn testFn = new BatchableMutationFilterFn(null, null, mutationSize * 3, 1000);    ProcessContext mockProcessContext = Mockito.mock(ProcessContext.class);    when(mockProcessContext.sideInput(any())).thenReturn(getSchema());        doNothing().when(mockProcessContext).output(mutationGroupCaptor.capture());    doNothing().when(mockProcessContext).output(any(), mutationGroupListCaptor.capture());        for (MutationGroup m : mutationGroups) {        when(mockProcessContext.element()).thenReturn(m);        testFn.processElement(mockProcessContext);    }        assertThat(mutationGroupCaptor.getAllValues(), containsInAnyOrder(g(m(1L)), g(m(2L), m(3L)), g(del(1L))));        Iterable<MutationGroup> unbatchableMutations = Iterables.concat(mutationGroupListCaptor.getAllValues());    assertThat(unbatchableMutations, containsInAnyOrder(    g(m(1L), m(3L), m(4L), m(5L)),     g(del(5L, 6L)), g(all), g(prefix), g(range)));}
public void beam_f31179_0()
{    MutationGroup[] mutationGroups = new MutationGroup[] { g(m(1L)), g(m(2L)), g(del(1L)), g(del(5L, 6L)) };    BatchableMutationFilterFn testFn = new BatchableMutationFilterFn(null, null, 0, 0);    ProcessContext mockProcessContext = Mockito.mock(ProcessContext.class);    when(mockProcessContext.sideInput(any())).thenReturn(getSchema());        doNothing().when(mockProcessContext).output(mutationGroupCaptor.capture());    doNothing().when(mockProcessContext).output(any(), mutationGroupListCaptor.capture());        for (MutationGroup m : mutationGroups) {        when(mockProcessContext.element()).thenReturn(m);        testFn.processElement(mockProcessContext);    }        assertTrue(mutationGroupCaptor.getAllValues().isEmpty());        Iterable<MutationGroup> unbatchableMutations = Iterables.concat(mutationGroupListCaptor.getAllValues());    assertThat(unbatchableMutations, containsInAnyOrder(mutationGroups));}
private static Mutation beam_f31188_0(Long start, Long end)
{    return Mutation.delete("test", KeySet.range(KeyRange.closedClosed(Key.of(start), Key.of(end))));}
private static Iterable<Mutation> beam_f31189_0(Iterable<Mutation> expected)
{    final ImmutableSet<Mutation> mutations = ImmutableSet.copyOf(expected);    return argThat(new ArgumentMatcher<Iterable<Mutation>>() {        @Override        public boolean matches(Iterable<Mutation> argument) {            if (!(argument instanceof Iterable)) {                return false;            }            ImmutableSet<Mutation> actual = ImmutableSet.copyOf((Iterable) argument);            return actual.equals(mutations);        }        @Override        public String toString() {            return "Iterable must match " + mutations;        }    });}
public void beam_f31198_0() throws Exception
{    SpannerConfig spannerConfig = createSpannerConfig();    PCollectionView<Transaction> tx = p.apply(SpannerIO.createTransaction().withSpannerConfig(spannerConfig).withTimestampBound(TimestampBound.strong()));    PCollection<Struct> allRecords = p.apply(SpannerIO.read().withSpannerConfig(spannerConfig).withBatching(false).withQuery("SELECT t.table_name FROM information_schema.tables AS t WHERE t" + ".table_catalog = '' AND t.table_schema = ''")).apply(MapElements.into(TypeDescriptor.of(ReadOperation.class)).via((SerializableFunction<Struct, ReadOperation>) input -> {        String tableName = input.getString(0);        return ReadOperation.create().withQuery("SELECT * FROM " + tableName);    })).apply(SpannerIO.readAll().withTransaction(tx).withSpannerConfig(spannerConfig));    PAssert.thatSingleton(allRecords.apply("Count rows", Count.globally())).isEqualTo(5L);    p.run();}
private void beam_f31199_0()
{    DatabaseClient databaseClient = getDatabaseClient();    List<Mutation> mutations = new ArrayList<>();    for (int i = 0; i < 5L; i++) {        mutations.add(Mutation.newInsertOrUpdateBuilder(options.getTable()).set("key").to((long) i).set("value").to(RandomUtils.randomAlphaNumeric(100)).build());    }    databaseClient.writeAtLeastOnce(mutations);}
public void beam_f31208_0() throws Exception
{    int numRecords = 100;    p.apply(GenerateSequence.from(0).to(numRecords)).apply(ParDo.of(new GenerateMutations(options.getTable()))).apply(SpannerIO.write().withProjectId(project).withInstanceId(options.getInstanceId()).withDatabaseId(databaseName));    PipelineResult result = p.run();    result.waitUntilFinish();    assertThat(result.getState(), is(PipelineResult.State.DONE));    assertThat(countNumberOfRecords(), equalTo((long) numRecords));}
public void beam_f31209_0() throws Exception
{    int numRecords = 100;    SpannerWriteResult stepOne = p.apply("first step", GenerateSequence.from(0).to(numRecords)).apply(ParDo.of(new GenerateMutations(options.getTable()))).apply(SpannerIO.write().withProjectId(project).withInstanceId(options.getInstanceId()).withDatabaseId(databaseName));    p.apply("second step", GenerateSequence.from(numRecords).to(2 * numRecords)).apply("Gen mutations", ParDo.of(new GenerateMutations(options.getTable()))).apply(Wait.on(stepOne.getOutput())).apply("write to table2", SpannerIO.write().withProjectId(project).withInstanceId(options.getInstanceId()).withDatabaseId(databaseName));    PipelineResult result = p.run();    result.waitUntilFinish();    assertThat(result.getState(), is(PipelineResult.State.DONE));    assertThat(countNumberOfRecords(), equalTo(2L * numRecords));}
public void beam_f31218_0()
{    TestPipelineOptions options = TestPipeline.testingPipelineOptions().as(TestPipelineOptions.class);    assertNotNull(options.getTempRoot());    options.setTempLocation(options.getTempRoot() + "/testGcsWriteWithKmsKey");    GcsOptions gcsOptions = options.as(GcsOptions.class);    ResourceId filenamePrefix = FileSystems.matchNewResource(gcsOptions.getGcpTempLocation(), true).resolve(String.format("GcsKmsKeyIT-%tF-%<tH-%<tM-%<tS-%<tL.output", new Date()), StandardResolveOptions.RESOLVE_FILE);    Pipeline p = Pipeline.create(options);    p.apply("ReadLines", TextIO.read().from(INPUT_FILE)).apply("WriteLines", TextIO.write().to(filenamePrefix));    PipelineResult result = p.run();    State state = result.waitUntilFinish();    assertThat(state, equalTo(State.DONE));    String filePattern = filenamePrefix + "*-of-*";    assertThat(result, new FileChecksumMatcher(EXPECTED_CHECKSUM, filePattern));        try {        MatchResult matchResult = Iterables.getOnlyElement(FileSystems.match(Collections.singletonList(filePattern)));        GcsUtil gcsUtil = gcsOptions.getGcsUtil();        for (Metadata metadata : matchResult.metadata()) {            String kmsKey = gcsUtil.getObject(GcsPath.fromUri(metadata.resourceId().toString())).getKmsKeyName();            assertNotNull(kmsKey);        }    } catch (IOException e) {        throw new AssertionError(e);    }}
public void beam_f31219_0() throws IOException
{    MockitoAnnotations.initMocks(this);    when(mockBigqueryClient.jobs()).thenReturn(mockJobs);    when(mockJobs.query(anyString(), any(QueryRequest.class))).thenReturn(mockQuery);    PowerMockito.mockStatic(BigqueryClient.class);    when(BigqueryClient.getNewBigquerryClient(anyString())).thenReturn(mockBigqueryClient);    bqClient = spy(new BigqueryClient("test-app"));}
public void beam_f31228_0(ObjectOutput out) throws IOException
{    out.writeUTF(conf.getClass().getCanonicalName());    conf.write(out);}
public void beam_f31229_0(ObjectInput in) throws IOException, ClassNotFoundException
{    String className = in.readUTF();    try {        conf = (Configuration) Class.forName(className).getDeclaredConstructor().newInstance();        conf.readFields(in);    } catch (InstantiationException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {        throw new IOException("Unable to create configuration: " + e);    }}
public int beam_f31238_0()
{    return type.hashCode();}
public static CoderProvider beam_f31239_0()
{    return new WritableCoderProvider();}
public void beam_f31248_0() throws Exception
{    assertThat(CoderRegistry.createDefault().getCoder(NullWritable.class), instanceOf(WritableCoder.class));}
protected List<MatchResult> beam_f31249_0(List<String> specs)
{    ImmutableList.Builder<MatchResult> resultsBuilder = ImmutableList.builder();    for (String spec : specs) {        try {            final Set<Metadata> metadata = new HashSet<>();            if (spec.contains("**")) {                                int index = spec.indexOf("**");                metadata.addAll(matchRecursiveGlob(spec.substring(0, index + 1), spec.substring(index + 1)));            } else {                                final Path path = new Path(spec);                final FileStatus[] fileStatuses = path.getFileSystem(configuration).globStatus(path);                if (fileStatuses != null) {                    for (FileStatus fileStatus : fileStatuses) {                        metadata.add(toMetadata(fileStatus));                    }                }            }            if (metadata.isEmpty()) {                resultsBuilder.add(MatchResult.create(Status.NOT_FOUND, Collections.emptyList()));            } else {                resultsBuilder.add(MatchResult.create(Status.OK, new ArrayList<>(metadata)));            }        } catch (IOException e) {            resultsBuilder.add(MatchResult.create(Status.ERROR, e));        }    }    return resultsBuilder.build();}
protected HadoopResourceId beam_f31258_0(String singleResourceSpec, boolean isDirectory)
{    if (singleResourceSpec.endsWith("/") && !isDirectory) {        throw new IllegalArgumentException(String.format("Expected file path but received directory path %s", singleResourceSpec));    }    return !singleResourceSpec.endsWith("/") && isDirectory ? new HadoopResourceId(dropEmptyAuthority(singleResourceSpec + "/")) : new HadoopResourceId(dropEmptyAuthority(singleResourceSpec));}
protected String beam_f31259_0()
{    return scheme;}
private static URI beam_f31268_0(String uriStr)
{    URI uri = URI.create(uriStr);    String prefix = uri.getScheme() + ":///";    if (uriStr.startsWith(prefix)) {        return URI.create(uri.getScheme() + ":/" + uriStr.substring(prefix.length()));    } else {        return uri;    }}
public Configuration beam_f31269_0(JsonParser jsonParser, DeserializationContext deserializationContext) throws IOException
{    Map<String, String> rawConfiguration = jsonParser.readValueAs(new TypeReference<Map<String, String>>() {    });    Configuration configuration = new Configuration(false);    for (Map.Entry<String, String> entry : rawConfiguration.entrySet()) {        configuration.set(entry.getKey(), entry.getValue());    }    return configuration;}
public boolean beam_f31278_0()
{    return uri.getPath().endsWith("/");}
public String beam_f31279_0()
{    return new Path(uri).getName();}
public void beam_f31288_0()
{    HadoopFileSystemOptions options = PipelineOptionsFactory.fromArgs("--hdfsConfiguration=[" + "{\"propertyA\": \"A\"}," + "{\"propertyB\": \"B\"}]").as(HadoopFileSystemOptions.class);    assertEquals(2, options.getHdfsConfiguration().size());    assertThat(options.getHdfsConfiguration().get(0), Matchers.<Map.Entry<String, String>>contains(new AbstractMap.SimpleEntry("propertyA", "A")));    assertThat(options.getHdfsConfiguration().get(1), Matchers.<Map.Entry<String, String>>contains(new AbstractMap.SimpleEntry("propertyB", "B")));}
public void beam_f31289_0()
{    HadoopFileSystemOptions.ConfigurationLocator projectFactory = spy(new HadoopFileSystemOptions.ConfigurationLocator());    when(projectFactory.getEnvironment()).thenReturn(ImmutableMap.of());    assertNull(projectFactory.create(PipelineOptionsFactory.create()));}
public void beam_f31298_0() throws Exception
{    Configuration configuration = new Configuration();    configuration.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, tmpFolder.getRoot().getAbsolutePath());    MiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(configuration);    hdfsCluster = builder.build();    hdfsClusterBaseUri = new URI(configuration.get("fs.defaultFS") + "/");    fileSystem = new HadoopFileSystem(Objects.requireNonNull(hdfsClusterBaseUri).getScheme(), configuration);}
public void beam_f31299_0()
{    hdfsCluster.shutdown();}
public void beam_f31308_0() throws Exception
{    create("dir/file", "data".getBytes(StandardCharsets.UTF_8));    final MatchResult matchResult = Iterables.getOnlyElement(fileSystem.match(Collections.singletonList(testPath("dir").toString())));    assertThat(matchResult, equalTo(MatchResult.create(Status.OK, ImmutableList.of(Metadata.builder().setResourceId(testPath("dir")).setIsReadSeekEfficient(true).setSizeBytes(0L).setLastModifiedMillis(lastModified("dir")).build()))));}
public void beam_f31309_0() throws Exception
{    create("testFileAA", "testDataAA".getBytes(StandardCharsets.UTF_8));    create("testFileBB", "testDataBB".getBytes(StandardCharsets.UTF_8));        assertArrayEquals("testDataAA".getBytes(StandardCharsets.UTF_8), read("testFileAA", 0));    assertArrayEquals("testDataBB".getBytes(StandardCharsets.UTF_8), read("testFileBB", 0));    List<MatchResult> matchResults = fileSystem.match(ImmutableList.of(testPath("testFileAA").toString(), testPath("testFileA").toString(), testPath("testFileBB").toString()));    assertThat(matchResults, hasSize(3));    final List<MatchResult> expected = ImmutableList.of(MatchResult.create(Status.OK, ImmutableList.of(Metadata.builder().setResourceId(testPath("testFileAA")).setIsReadSeekEfficient(true).setSizeBytes("testDataAA".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("testFileAA")).build())), MatchResult.create(Status.NOT_FOUND, ImmutableList.of()), MatchResult.create(Status.OK, ImmutableList.of(Metadata.builder().setResourceId(testPath("testFileBB")).setIsReadSeekEfficient(true).setSizeBytes("testDataBB".getBytes(StandardCharsets.UTF_8).length).setLastModifiedMillis(lastModified("testFileBB")).build())));    assertThat(matchResults, equalTo(expected));}
private void beam_f31318_0(String relativePath, byte[] contents) throws Exception
{    try (WritableByteChannel channel = fileSystem.create(testPath(relativePath), StandardCreateOptions.builder().setMimeType(MimeTypes.BINARY).build())) {        channel.write(ByteBuffer.wrap(contents));    }}
private byte[] beam_f31319_0(String relativePath, long bytesToSkip) throws Exception
{    try (ReadableByteChannel channel = fileSystem.open(testPath(relativePath))) {        InputStream inputStream = Channels.newInputStream(channel);        if (bytesToSkip > 0) {            ByteStreams.skipFully(inputStream, bytesToSkip);        }        return ByteStreams.toByteArray(inputStream);    }}
public Read<K, V> beam_f31328_0(SimpleFunction<?, K> function)
{    checkArgument(function != null, "function can not be null");        return toBuilder().setKeyTranslationFunction(function).setKeyTypeDescriptor(function.getOutputTypeDescriptor()).build();}
public Read<K, V> beam_f31329_0(SimpleFunction<?, V> function)
{    checkArgument(function != null, "function can not be null");        return toBuilder().setValueTranslationFunction(function).setValueTypeDescriptor(function.getOutputTypeDescriptor()).build();}
public List<BoundedSource<KV<K, V>>> beam_f31338_1(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{        if (inputSplit != null) {                return ImmutableList.of(this);    }    computeSplitsIfNecessary();        return inputSplits.stream().map(serializableInputSplit -> new HadoopInputFormatBoundedSource<>(conf, keyCoder, valueCoder, keyTranslationFunction, valueTranslationFunction, serializableInputSplit)).collect(Collectors.toList());}
public long beam_f31339_0(PipelineOptions po) throws Exception
{    if (inputSplit == null) {                computeSplitsIfNecessary();        return boundedSourceEstimatedSize;    }    return inputSplit.getSplit().getLength();}
public boolean beam_f31348_0() throws IOException
{    try {        progressValue.set(getProgress());        if (recordReader.nextKeyValue()) {            recordsReturned.incrementAndGet();            return true;        }        doneReading = true;    } catch (InterruptedException e) {        throw new IOException("Unable to read data: ", e);    }    return false;}
public KV<K, V> beam_f31349_1()
{    K key;    V value;    try {                key = transformKeyOrValue(recordReader.getCurrentKey(), keyTranslationFunction, keyCoder);                value = transformKeyOrValue(recordReader.getCurrentValue(), valueTranslationFunction, valueCoder);    } catch (IOException | InterruptedException e) {                throw new IllegalStateException("Unable to read data: " + "{}", e);    }    return KV.of(key, value);}
private void beam_f31358_0(ObjectInputStream in) throws IOException
{    ObjectWritable ow = new ObjectWritable();    ow.setConf(new Configuration(false));    ow.readFields(in);    this.inputSplit = (InputSplit) ow.get();}
private void beam_f31359_0(ObjectOutputStream out) throws IOException
{    new ObjectWritable(inputSplit).write(out);}
public PDone beam_f31369_0(PCollection<KV<KeyT, ValueT>> input)
{        if (input.isBounded().equals(PCollection.IsBounded.UNBOUNDED) || !input.getWindowingStrategy().equals(WindowingStrategy.globalDefault())) {        checkArgument(configTransform != null, "Writing of unbounded data can be processed only with configuration transformation provider. See %s.withConfigurationTransform()", Write.class);    }    verifyInputWindowing(input);    TypeDescriptor<Configuration> configType = new TypeDescriptor<Configuration>() {    };    input.getPipeline().getCoderRegistry().registerCoderForType(configType, new ConfigurationCoder());    PCollectionView<Configuration> configView = createConfigurationView(input);    return processJob(input, configView);}
private PDone beam_f31370_0(PCollection<KV<KeyT, ValueT>> input, PCollectionView<Configuration> configView)
{    TypeDescriptor<Iterable<Integer>> iterableIntType = TypeDescriptors.iterables(TypeDescriptors.integers());    PCollection<KV<KeyT, ValueT>> validatedInput = input.apply(ParDo.of(new SetupJobFn<>(externalSynchronization, configView, input.getTypeDescriptor())).withSideInputs(configView));    PCollection<KV<Integer, KV<KeyT, ValueT>>> writeData = withPartitioning ? validatedInput.apply("GroupDataByPartition", new GroupDataByPartition<>(configView)) : validatedInput.apply("PrepareNonPartitionedTasks", ParDo.of(new PrepareNonPartitionedTasksFn<KeyT, ValueT>(configView, externalSynchronization)).withSideInputs(configView));    PCollection<Iterable<Integer>> collectedFinishedWrites = writeData.apply("Write", ParDo.of(new WriteFn<KeyT, ValueT>(configView, externalSynchronization)).withSideInputs(configView)).setTypeDescriptor(TypeDescriptors.integers()).apply("CollectWriteTasks", Combine.globally(new IterableCombinerFn<>(TypeDescriptors.integers())).withoutDefaults()).setTypeDescriptor(iterableIntType);    return PDone.in(collectedFinishedWrites.apply("CommitWriteJob", ParDo.of(new CommitJobFn<Integer>(configView, externalSynchronization)).withSideInputs(configView)).getPipeline());}
private RecordWriter<KeyT, ValueT> beam_f31379_1(OutputFormat<KeyT, ValueT> outputFormatObj, TaskAttemptContext taskAttemptContext) throws IllegalStateException
{    try {                return outputFormatObj.getRecordWriter(taskAttemptContext);    } catch (InterruptedException | IOException e) {        throw new IllegalStateException("Unable to create RecordWriter object: ", e);    }}
private static OutputCommitter beam_f31380_0(OutputFormat<?, ?> outputFormatObj, Configuration conf, TaskAttemptContext taskAttemptContext) throws IllegalStateException
{    OutputCommitter outputCommitter;    try {        outputCommitter = outputFormatObj.getOutputCommitter(taskAttemptContext);        if (outputCommitter != null) {            outputCommitter.setupJob(new JobContextImpl(conf, taskAttemptContext.getJobID()));        }    } catch (Exception e) {        throw new IllegalStateException("Unable to create OutputCommitter object: ", e);    }    return outputCommitter;}
public void beam_f31389_0(ProcessContext c)
{    Configuration config = c.sideInput(configView);    cleanupJob(config);}
private void beam_f31390_0(Configuration config)
{    externalSynchronization.releaseJobIdLock(config);    JobID jobID = HadoopFormats.getJobId(config);    TaskAttemptContext cleanupTaskContext = HadoopFormats.createCleanupTaskContext(config, jobID);    OutputFormat<?, ?> outputFormat = HadoopFormats.createOutputFormatFromConfig(config);    try {        OutputCommitter outputCommitter = outputFormat.getOutputCommitter(cleanupTaskContext);        outputCommitter.commitJob(cleanupTaskContext);    } catch (Exception e) {        throw new RuntimeException("Unable to commit job.", e);    }}
public void beam_f31399_1(FinishBundleContext c)
{    if (bundleTaskContextMap == null) {        return;    }    for (Map.Entry<KV<BoundedWindow, Integer>, TaskContext<KeyT, ValueT>> entry : bundleTaskContextMap.entrySet()) {        TaskContext<KeyT, ValueT> taskContext = entry.getValue();        try {            taskContext.getRecordWriter().close(taskContext.getTaskAttemptContext());            taskContext.getOutputCommitter().commitTask(taskContext.getTaskAttemptContext());                    } catch (Exception e) {            processTaskException(taskContext, e);        }        BoundedWindow window = entry.getKey().getKey();        c.output(taskContext.getTaskId(), Objects.requireNonNull(window).maxTimestamp(), window);    }}
private void beam_f31400_1(TaskContext<KeyT, ValueT> taskContext, Exception e)
{        taskContext.abortTask();    throw new IllegalArgumentException(e);}
 static TaskAttemptContext beam_f31409_0(Configuration conf, TaskAttemptID taskAttemptID)
{    return new TaskAttemptContextImpl(conf, taskAttemptID);}
 static TaskAttemptID beam_f31410_0(JobID jobID, int taskId, int attemptId)
{    final TaskID tId = createTaskID(jobID, taskId);    return new TaskAttemptID(tId, attemptId);}
public void beam_f31419_1(Configuration conf)
{    Path path = new Path(locksDir, String.format(LOCKS_DIR_PATTERN, getJobJtIdentifier(conf)));    try (FileSystem fileSystem = fileSystemFactory.apply(conf)) {        if (fileSystem.delete(path, true)) {                    } else {                    }    } catch (IOException e) {        String formattedExceptionMessage = String.format("Delete of lock directory %s was unsuccessful", path);                throw new IllegalStateException(formattedExceptionMessage, e);    }}
public TaskID beam_f31420_0(Configuration conf)
{    JobID jobId = HadoopFormats.getJobId(conf);    boolean lockAcquired = false;    int taskIdCandidate = 0;    while (!lockAcquired) {        taskIdCandidate = RANDOM_GEN.nextInt(Integer.MAX_VALUE);        Path path = new Path(locksDir, String.format(LOCKS_DIR_TASK_PATTERN, getJobJtIdentifier(conf), taskIdCandidate));        lockAcquired = tryCreateFile(conf, path);    }    return HadoopFormats.createTaskID(jobId, taskIdCandidate);}
public Coder<Iterable<T>> beam_f31429_0(CoderRegistry registry, Coder<T> inputCoder)
{    return IterableCoder.of(inputCoder);}
public Coder<CollectionAccumulator<T>> beam_f31430_0(CoderRegistry registry, Coder<T> inputCoder)
{    return new CollectionAccumulatorCoder<>(inputCoder);}
public boolean beam_f31442_0()
{    return false;}
public Text beam_f31443_0()
{    return null;}
public String beam_f31453_0()
{    return "Employee{" + "Name='" + empName + '\'' + ", Address=" + empAddress + '}';}
public RecordReader<Text, Employee> beam_f31454_0(InputSplit split, TaskAttemptContext context)
{    return new EmployeeRecordReader();}
public Employee beam_f31464_0()
{    return currentValue;}
public float beam_f31465_0()
{    return (float) recordsRead / split.getLength();}
public static void beam_f31476_0()
{    PipelineOptionsFactory.register(HadoopFormatIOTestOptions.class);    options = TestPipeline.testingPipelineOptions().as(HadoopFormatIOTestOptions.class);}
public void beam_f31477_0()
{        String expectedHashCode = "1a30ad400afe4ebf5fde75f5d2d95408";    Long expectedRecordsCount = 1000L;    Configuration conf = getConfiguration(options);    PCollection<KV<Long, String>> cassandraData = pipeline.apply(HadoopFormatIO.<Long, String>read().withConfiguration(conf).withValueTranslation(myValueTranslate));    PAssert.thatSingleton(cassandraData.apply("Count", Count.globally())).isEqualTo(expectedRecordsCount);    PCollection<String> textValues = cassandraData.apply(Values.create());        PCollection<String> consolidatedHashcode = textValues.apply(Combine.globally(new HashingFn()).withoutDefaults());    PAssert.that(consolidatedHashcode).containsInAnyOrder(expectedHashCode);    pipeline.run().waitUntilFinish();}
public static void beam_f31486_0() throws Exception
{    cassandraPort = NetworkTestHelper.getAvailableLocalPort();    cassandraNativePort = NetworkTestHelper.getAvailableLocalPort();    replacePortsInConfFile();        cassandra.start();    final SocketOptions socketOptions = new SocketOptions();        socketOptions.setReadTimeoutMillis(0);        socketOptions.setConnectTimeoutMillis(60 * 1000);    cluster = Cluster.builder().addContactPoint(CASSANDRA_HOST).withClusterName("beam").withSocketOptions(socketOptions).withPort(cassandraNativePort).build();    session = cluster.connect();    createCassandraData();}
private static void beam_f31487_0() throws Exception
{    URI uri = HadoopFormatIOCassandraTest.class.getResource("/cassandra.yaml").toURI();    Path cassandraYamlPath = new File(uri).toPath();    String content = new String(Files.readAllBytes(cassandraYamlPath), Charset.defaultCharset());    content = content.replaceAll("9042", String.valueOf(cassandraNativePort));    content = content.replaceAll("9061", String.valueOf(cassandraPort));    Files.write(cassandraYamlPath, content.getBytes(Charset.defaultCharset()));}
public String beam_f31496_0(LinkedMapWritable mapw)
{    String rowValue = "";    rowValue = convertMapWRowToString(mapw);    return rowValue;}
private String beam_f31497_0(LinkedMapWritable mapw)
{    String rowValue = "";    rowValue = addFieldValuesToRow(rowValue, mapw, "User_Name");    rowValue = addFieldValuesToRow(rowValue, mapw, "Item_Code");    rowValue = addFieldValuesToRow(rowValue, mapw, "Txn_ID");    rowValue = addFieldValuesToRow(rowValue, mapw, "Item_ID");    rowValue = addFieldValuesToRow(rowValue, mapw, "last_updated");    rowValue = addFieldValuesToRow(rowValue, mapw, "Price");    rowValue = addFieldValuesToRow(rowValue, mapw, "Title");    rowValue = addFieldValuesToRow(rowValue, mapw, "Description");    rowValue = addFieldValuesToRow(rowValue, mapw, "Age");    rowValue = addFieldValuesToRow(rowValue, mapw, "Item_Name");    rowValue = addFieldValuesToRow(rowValue, mapw, "Item_Price");    rowValue = addFieldValuesToRow(rowValue, mapw, "Availability");    rowValue = addFieldValuesToRow(rowValue, mapw, "Batch_Num");    rowValue = addFieldValuesToRow(rowValue, mapw, "Last_Ordered");    rowValue = addFieldValuesToRow(rowValue, mapw, "City");    return rowValue;}
private static Map<String, String> beam_f31506_0(String id, String name)
{    Map<String, String> data = new HashMap<>();    data.put("id", id);    data.put("scientist", name);    return data;}
public static void beam_f31507_0() throws IOException
{    ElasticEmbeddedServer.shutdown();}
private static void beam_f31516_0() throws SQLException
{    DatabaseTestHelper.deleteTable(dataSource, tableName);}
public void beam_f31517_0()
{    writePipeline.apply("Generate sequence", GenerateSequence.from(0).to(numberOfRows)).apply("Produce db rows", ParDo.of(new TestRow.DeterministicallyConstructTestRowFn())).apply("Prevent fusion before writing", Reshuffle.viaRandomKey()).apply("Collect write time", ParDo.of(new TimeMonitor<>(NAMESPACE, "write_time"))).apply("Construct rows for DBOutputFormat", ParDo.of(new ConstructDBOutputFormatRowFn())).apply("Write using Hadoop OutputFormat", HadoopFormatIO.<TestRowDBWritable, NullWritable>write().withConfiguration(hadoopConfiguration.get()).withPartitioning().withExternalSynchronization(new HDFSSynchronization(tmpFolder.getRoot().getAbsolutePath())));    PipelineResult writeResult = writePipeline.run();    writeResult.waitUntilFinish();    PCollection<String> consolidatedHashcode = readPipeline.apply("Read using Hadoop InputFormat", HadoopFormatIO.<LongWritable, TestRowDBWritable>read().withConfiguration(hadoopConfiguration.get())).apply("Collect read time", ParDo.of(new TimeMonitor<>(NAMESPACE, "read_time"))).apply("Get values only", Values.create()).apply("Values as string", ParDo.of(new TestRow.SelectNameFn())).apply("Calculate hashcode", Combine.globally(new HashingFn()));    PAssert.thatSingleton(consolidatedHashcode).isEqualTo(getExpectedHashForRowCount(numberOfRows));    PipelineResult readResult = readPipeline.run();    readResult.waitUntilFinish();    collectAndPublishMetrics(writeResult, readResult);}
public void beam_f31526_0()
{    HadoopFormatIO.Read<String, String> read = HadoopFormatIO.<String, String>read().withValueTranslation(myValueTranslate).withConfiguration(serConf.get()).withKeyTranslation(myKeyTranslate);    assertEquals(serConf.get(), read.getConfiguration().get());    assertEquals(myKeyTranslate, read.getKeyTranslationFunction());    assertEquals(myValueTranslate, read.getValueTranslationFunction());    assertEquals(myKeyTranslate.getOutputTypeDescriptor(), read.getKeyTypeDescriptor());    assertEquals(myValueTranslate.getOutputTypeDescriptor(), read.getValueTypeDescriptor());}
public void beam_f31527_0()
{    SerializableConfiguration diffConf = loadTestConfiguration(EmployeeInputFormat.class, Employee.class, Text.class);    HadoopFormatIO.Read<String, String> read = HadoopFormatIO.<String, String>read().withConfiguration(serConf.get()).withKeyTranslation(myKeyTranslate).withConfiguration(diffConf.get());    assertEquals(diffConf.get(), read.getConfiguration().get());    assertEquals(myKeyTranslate, read.getKeyTranslationFunction());    assertEquals(null, read.getValueTranslationFunction());    assertEquals(myKeyTranslate.getOutputTypeDescriptor(), read.getKeyTypeDescriptor());    assertEquals(diffConf.get().getClass("value.class", Object.class), read.getValueTypeDescriptor().getRawType());}
public void beam_f31536_0()
{    Configuration configuration = new Configuration();    configuration.setClass("key.class", Text.class, Object.class);    configuration.setClass("value.class", Employee.class, Object.class);    thrown.expect(IllegalArgumentException.class);    HadoopFormatIO.<Text, Employee>read().withConfiguration(configuration);}
public void beam_f31537_0()
{    Configuration configuration = new Configuration();    configuration.setClass("mapreduce.job.inputformat.class", EmployeeInputFormat.class, InputFormat.class);    configuration.setClass("value.class", Employee.class, Object.class);    thrown.expect(IllegalArgumentException.class);    HadoopFormatIO.<Text, Employee>read().withConfiguration(configuration);}
public void beam_f31546_0() throws Exception
{    InputFormat<Text, Employee> mockInputFormat = Mockito.mock(EmployeeInputFormat.class);    thrown.expect(IOException.class);    thrown.expectMessage(String.format("Null RecordReader object returned by %s", mockInputFormat.getClass()));    Mockito.when(mockInputFormat.createRecordReader(Mockito.any(InputSplit.class), Mockito.any(TaskAttemptContext.class))).thenReturn(null);    HadoopInputFormatBoundedSource<Text, Employee> boundedSource = new HadoopInputFormatBoundedSource<>(serConf, WritableCoder.of(Text.class), AvroCoder.of(Employee.class),     null,     null, new SerializableSplit());    boundedSource.setInputFormatObj(mockInputFormat);    SourceTestUtils.readFromSource(boundedSource, p.getOptions());}
public void beam_f31547_0() throws Exception
{    InputFormat mockInputFormat = Mockito.mock(EmployeeInputFormat.class);    EmployeeRecordReader mockReader = Mockito.mock(EmployeeRecordReader.class);    Mockito.when(mockInputFormat.createRecordReader(Mockito.any(), Mockito.any())).thenReturn(mockReader);    Mockito.when(mockReader.nextKeyValue()).thenReturn(false);    InputSplit mockInputSplit = Mockito.mock(NewObjectsEmployeeInputSplit.class);    HadoopInputFormatBoundedSource<Text, Employee> boundedSource = new HadoopInputFormatBoundedSource<>(serConf, WritableCoder.of(Text.class), AvroCoder.of(Employee.class),     null,     null, new SerializableSplit(mockInputSplit));    boundedSource.setInputFormatObj(mockInputFormat);    BoundedReader<KV<Text, Employee>> reader = boundedSource.createReader(p.getOptions());    assertFalse(reader.start());    assertEquals(Double.valueOf(1), reader.getFractionConsumed());    reader.close();}
public void beam_f31556_0() throws Exception
{    List<BoundedSource<KV<Text, Employee>>> boundedSourceList = getBoundedSourceList(ReuseObjectsEmployeeInputFormat.class, Text.class, Employee.class, WritableCoder.of(Text.class), AvroCoder.of(Employee.class));    List<KV<Text, Employee>> bundleRecords = new ArrayList<>();    for (BoundedSource<KV<Text, Employee>> source : boundedSourceList) {        List<KV<Text, Employee>> elems = SourceTestUtils.readFromSource(source, p.getOptions());        bundleRecords.addAll(elems);    }    List<KV<Text, Employee>> referenceRecords = TestEmployeeDataSet.getEmployeeData();    assertThat(bundleRecords, containsInAnyOrder(referenceRecords.toArray()));}
public void beam_f31557_0() throws Exception
{    List<BoundedSource<KV<Text, Employee>>> boundedSourceList = getBoundedSourceList(ConfigurableEmployeeInputFormat.class, Text.class, Employee.class, WritableCoder.of(Text.class), AvroCoder.of(Employee.class));    for (BoundedSource<KV<Text, Employee>> source : boundedSourceList) {                HadoopInputFormatBoundedSource<Text, Employee> hifSource = (HadoopInputFormatBoundedSource<Text, Employee>) source;        hifSource.createInputFormatInstance();        ConfigurableEmployeeInputFormat inputFormatObj = (ConfigurableEmployeeInputFormat) hifSource.getInputFormat();        assertTrue(inputFormatObj.isConfSet);    }}
private void beam_f31566_0(HadoopFormatIO.Write<Text, LongWritable> write, String outputDir)
{    pipeline.apply(Create.of(SENTENCES)).apply(ParDo.of(new ConvertToLowerCaseFn())).apply(new WordCount.CountWords()).apply("ConvertToHadoopFormat", ParDo.of(new ConvertToHadoopFormatFn<>(KV_STR_INT_2_TXT_LONGWRITABLE))).setTypeDescriptor(TypeDescriptors.kvs(new TypeDescriptor<Text>() {    }, new TypeDescriptor<LongWritable>() {    })).apply(write);    pipeline.run();    Map<String, Long> results = loadWrittenDataAsMap(outputDir);    MatcherAssert.assertThat(results.entrySet(), equalTo(computeWordCounts(SENTENCES).entrySet()));}
private List<KV<Text, LongWritable>> beam_f31567_0(String outputDir)
{    return Arrays.stream(Objects.requireNonNull(new File(outputDir).list())).filter(fileName -> fileName.startsWith("part-r")).map(fileName -> outputDir + File.separator + fileName).flatMap(this::extractResultsFromFile).collect(Collectors.toList());}
public void beam_f31576_0(@DoFn.Element InputT element, OutputReceiver<OutputT> outReceiver)
{    outReceiver.output(transformFn.apply(element));}
public void beam_f31577_0(@DoFn.Element String element, OutputReceiver<String> receiver)
{    receiver.output(element.toLowerCase());}
public void beam_f31586_0()
{    Configuration configuration = new Configuration();    configuration.setClass(HadoopFormatIO.OUTPUT_FORMAT_CLASS_ATTR, TextOutputFormat.class, OutputFormat.class);    configuration.setClass(HadoopFormatIO.OUTPUT_KEY_CLASS, Text.class, Object.class);    configuration.setClass(HadoopFormatIO.OUTPUT_VALUE_CLASS, Employee.class, Object.class);    configuration.set(HadoopFormatIO.OUTPUT_DIR, tmpFolder.getRoot().getAbsolutePath());    runValidationPipeline(configuration);    thrown.expect(Pipeline.PipelineExecutionException.class);    thrown.expectMessage("Configuration must contain \"mapreduce.job.id\"");    p.run().waitUntilFinish();}
public void beam_f31587_0() throws IOException
{    conf.set(HadoopFormatIO.OUTPUT_DIR, tmpFolder.getRoot().getAbsolutePath());    List<KV<Text, Employee>> data = TestEmployeeDataSet.getEmployeeData();    PCollection<KV<Text, Employee>> input = p.apply(Create.of(data)).setTypeDescriptor(TypeDescriptors.kvs(new TypeDescriptor<Text>() {    }, new TypeDescriptor<Employee>() {    }));    input.apply("Write", HadoopFormatIO.<Text, Employee>write().withConfiguration(conf).withPartitioning().withExternalSynchronization(new HDFSSynchronization(getLocksDirPath())));    p.run();    List<KV<Text, Employee>> writtenOutput = EmployeeOutputFormat.getWrittenOutput();    assertEquals(data.size(), writtenOutput.size());    assertTrue(data.containsAll(writtenOutput));    assertTrue(writtenOutput.containsAll(data));    Mockito.verify(EmployeeOutputFormat.getOutputCommitter()).commitJob(Mockito.any());    Mockito.verify(EmployeeOutputFormat.getOutputCommitter(), Mockito.times(REDUCERS_COUNT)).commitTask(Mockito.any());}
public void beam_f31596_0()
{    int tasksCount = 100;    for (int i = 0; i < tasksCount; i++) {        TaskID taskID = tested.acquireTaskIdLock(configuration);        assertTrue(isFileExists(getTaskIdPath(taskID)));    }    String jobFolderName = getFileInJobFolder("");    File jobFolder = new File(jobFolderName);    assertTrue(jobFolder.isDirectory());        assertEquals(tasksCount * 2, jobFolder.list().length);}
public void beam_f31597_0()
{    int tasksCount = 100;    int taskId = 25;    for (int i = 0; i < tasksCount; i++) {        TaskAttemptID taskAttemptID = tested.acquireTaskAttemptIdLock(configuration, taskId);        assertTrue(isFileExists(getTaskAttemptIdPath(taskId, taskAttemptID.getId())));    }}
public void beam_f31606_0() throws IOException
{    IterableCombinerFn<String> tested = new IterableCombinerFn<>(STRING_TYPE_DESCRIPTOR);    IterableCombinerFn.CollectionAccumulator<String> originalAccumulator = tested.createAccumulator();    FIRST_ITEMS.forEach(originalAccumulator::addInput);    Coder<IterableCombinerFn.CollectionAccumulator<String>> accumulatorCoder = tested.getAccumulatorCoder(null, StringUtf8Coder.of());    byte[] bytes;    try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream()) {        accumulatorCoder.encode(originalAccumulator, byteArrayOutputStream);        byteArrayOutputStream.flush();        bytes = byteArrayOutputStream.toByteArray();    }    IterableCombinerFn.CollectionAccumulator<String> decodedAccumulator;    try (ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes)) {        decodedAccumulator = accumulatorCoder.decode(byteArrayInputStream);    }    String[] originalItems = FIRST_ITEMS.toArray(new String[0]);    MatcherAssert.assertThat(originalAccumulator.extractOutput(), Matchers.containsInAnyOrder(originalItems));    MatcherAssert.assertThat(decodedAccumulator.extractOutput(), Matchers.containsInAnyOrder(originalItems));}
public RecordReader<Text, Employee> beam_f31607_0(InputSplit split, TaskAttemptContext context)
{    return new ReuseObjectsEmployeeRecordReader();}
public Employee beam_f31617_0()
{    return currentValue;}
public float beam_f31618_0()
{    return (float) recordsRead / split.getLength();}
public void beam_f31627_0(DataOutput out) throws IOException
{    out.writeInt(id);    out.writeChars(name);}
public void beam_f31628_0(DataInput in) throws IOException
{    id = in.readInt();    name = in.readUTF();}
public Read beam_f31637_0(byte[] startRow, byte[] stopRow)
{    checkArgument(startRow != null, "startRowcan not be null");    checkArgument(stopRow != null, "stopRowcan not be null");    ByteKeyRange keyRange = ByteKeyRange.of(ByteKey.copyFrom(startRow), ByteKey.copyFrom(stopRow));    return withKeyRange(keyRange);}
public void beam_f31639_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("configuration", serializableConfiguration.get().toString()));    builder.add(DisplayData.item("tableId", tableId));    builder.addIfNotNull(DisplayData.item("scan", serializableScan.get().toString()));}
 HBaseSource beam_f31648_0(ByteKey endKey) throws IOException
{    checkNotNull(endKey, "endKey");    Read newRead = new Read(read.serializableConfiguration, read.tableId, new SerializableScan(new Scan(read.serializableScan.get()).setStopRow(endKey.getBytes())));    return new HBaseSource(newRead, estimatedSizeBytes);}
public long beam_f31649_1(PipelineOptions pipelineOptions) throws Exception
{    if (estimatedSizeBytes == null) {        try (Connection connection = ConnectionFactory.createConnection(read.serializableConfiguration.get())) {            estimatedSizeBytes = HBaseUtils.estimateSizeBytes(connection, read.tableId, read.serializableScan.get());        }            }    return estimatedSizeBytes;}
public void beam_f31658_1() throws IOException
{        if (scanner != null) {        scanner.close();        scanner = null;    }    if (connection != null) {        connection.close();        connection = null;    }}
public synchronized HBaseSource beam_f31659_0()
{    return source;}
public Configuration beam_f31669_0()
{    return serializableConfiguration.get();}
public void beam_f31670_0() throws Exception
{    connection = ConnectionFactory.createConnection(serializableConfiguration.get());}
private static MutationType beam_f31679_0(Mutation mutation)
{    if (mutation instanceof Put) {        return MutationType.PUT;    } else if (mutation instanceof Delete) {        return MutationType.DELETE;    } else {                throw new IllegalArgumentException("Only Put and Delete are supported");    }}
 static CoderProvider beam_f31680_0()
{    return HBASE_MUTATION_CODER_PROVIDER;}
public void beam_f31689_0() throws Exception
{    connection = ConnectionFactory.createConnection(serializableConfiguration.get());}
private static Scan beam_f31690_0(Scan scan, ByteKeyRange range) throws IOException
{    return new Scan(scan).setStartRow(range.getStartKey().getBytes()).setStopRow(range.getEndKey().getBytes());}
 static long beam_f31699_0(Connection connection, String tableId, Scan scan) throws Exception
{        long estimatedSizeBytes = 0L;    List<HRegionLocation> regionLocations = getRegionLocations(connection, tableId, scan);        Set<byte[]> tableRegions = new TreeSet<>(Bytes.BYTES_COMPARATOR);    for (HRegionLocation regionLocation : regionLocations) {        tableRegions.add(regionLocation.getRegionInfo().getRegionName());    }        Admin admin = connection.getAdmin();    ClusterStatus clusterStatus = admin.getClusterStatus();    Collection<ServerName> servers = clusterStatus.getServers();    for (ServerName serverName : servers) {        ServerLoad serverLoad = clusterStatus.getLoad(serverName);        for (RegionLoad regionLoad : serverLoad.getRegionsLoad().values()) {            byte[] regionId = regionLoad.getName();            if (tableRegions.contains(regionId)) {                long regionSizeBytes = regionLoad.getStorefileSizeMB() * 1_048_576L;                estimatedSizeBytes += regionSizeBytes;            }        }    }    return estimatedSizeBytes;}
 static List<HRegionLocation> beam_f31700_0(Connection connection, String tableId, Scan scan) throws Exception
{    byte[] startRow = scan.getStartRow();    byte[] stopRow = scan.getStopRow();    final List<HRegionLocation> regionLocations = new ArrayList<>();    final boolean scanWithNoLowerBound = startRow.length == 0;    final boolean scanWithNoUpperBound = stopRow.length == 0;    TableName tableName = TableName.valueOf(tableId);    RegionLocator regionLocator = connection.getRegionLocator(tableName);    List<HRegionLocation> tableRegionInfos = regionLocator.getAllRegionLocations();    for (HRegionLocation regionLocation : tableRegionInfos) {        final byte[] startKey = regionLocation.getRegionInfo().getStartKey();        final byte[] endKey = regionLocation.getRegionInfo().getEndKey();        boolean isLastRegion = endKey.length == 0;                if ((scanWithNoLowerBound || isLastRegion || Bytes.compareTo(startRow, endKey) < 0) && (scanWithNoUpperBound || Bytes.compareTo(stopRow, startKey) > 0)) {            regionLocations.add(regionLocation);        }    }    return regionLocations;}
public void beam_f31709_0()
{    runWrite();    runRead();}
private void beam_f31710_0()
{    pipelineWrite.apply("Generate Sequence", GenerateSequence.from(0).to((long) numberOfRows)).apply("Prepare TestRows", ParDo.of(new TestRow.DeterministicallyConstructTestRowFn())).apply("Prepare mutations", ParDo.of(new ConstructMutations())).apply("Write to HBase", HBaseIO.write().withConfiguration(conf).withTableId(TABLE_NAME));    pipelineWrite.run().waitUntilFinish();}
public void beam_f31719_0()
{    HBaseIO.Write write = HBaseIO.write().withTableId("table").withConfiguration(conf);    assertEquals("table", write.getTableId());    assertNotNull("configuration", write.getConfiguration());}
public void beam_f31720_0()
{    HBaseIO.Write write = HBaseIO.write().withConfiguration(conf);    thrown.expect(IllegalArgumentException.class);    write.expand(null);}
public void beam_f31729_0() throws Exception
{    final String table = tmpTable.getName();    final int numRows = 1001;    createAndWriteData(table, numRows);    String regex = ".*17.*";    Filter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator(regex));    runReadTestLength(HBaseIO.read().withConfiguration(conf).withTableId(table).withFilter(filter), true, 20);}
public void beam_f31730_0() throws Exception
{    final String table = tmpTable.getName();    final int numRows = 1001;    final ByteKey startKey = ByteKey.copyFrom("2".getBytes(StandardCharsets.UTF_8));    createAndWriteData(table, numRows);        final ByteKeyRange prefixRange = ByteKeyRange.ALL_KEYS.withEndKey(startKey);    runReadTestLength(HBaseIO.read().withConfiguration(conf).withTableId(table).withKeyRange(prefixRange), false, 126);}
public void beam_f31739_0() throws Exception
{    final String table = tmpTable.getName();    final String key = "KEY";    createTable(table);    p.apply(Create.of(makeBadMutation(key))).apply(HBaseIO.write().withConfiguration(conf).withTableId(table));    thrown.expect(Pipeline.PipelineExecutionException.class);    thrown.expectCause(Matchers.instanceOf(IllegalArgumentException.class));    thrown.expectMessage("No columns to insert");    p.run().waitUntilFinish();}
public void beam_f31740_0()
{    final String table = tmpTable.getName();    HBaseIO.Write write = HBaseIO.write().withTableId(table).withConfiguration(conf);    DisplayData displayData = DisplayData.from(write);    assertThat(displayData, hasDisplayItem("tableId", table));}
private static Mutation beam_f31749_0(String key, String value)
{    return new Put(key.getBytes(StandardCharsets.UTF_8)).addColumn(COLUMN_FAMILY, COLUMN_NAME, Bytes.toBytes(value)).addColumn(COLUMN_FAMILY, COLUMN_EMAIL, Bytes.toBytes(value + "@email.com"));}
private static Mutation beam_f31750_0(String key)
{    return new Put(key.getBytes(StandardCharsets.UTF_8));}
public void beam_f31759_0()
{    assertNotNull(DEFAULT_SERIALIZABLE_SCAN.get());    thrown.expect(NullPointerException.class);    new SerializableScan(null);}
public static HCatalogBeamSchema beam_f31760_0(Map<String, String> config)
{    try {        HiveConf hiveConf = new HiveConf();        config.forEach(hiveConf::set);        return new HCatalogBeamSchema(new HiveMetaStoreClient(hiveConf));    } catch (Exception e) {        throw new RuntimeException(e);    }}
public Read beam_f31769_0(Duration pollingInterval)
{    return toBuilder().setPollingInterval(pollingInterval).build();}
public Read beam_f31770_0(List<String> partitionCols)
{    return toBuilder().setPartitionCols(partitionCols).build();}
public long beam_f31779_0(PipelineOptions pipelineOptions) throws Exception
{    IMetaStoreClient client = null;    try {        HiveConf hiveConf = HCatalogUtils.createHiveConf(spec);        client = HCatalogUtils.createMetaStoreClient(hiveConf);        Table table = HCatUtil.getTable(client, spec.getDatabase(), spec.getTable());        return StatsUtils.getFileSizeForTable(hiveConf, table);    } finally {                if (client != null) {            client.close();        }    }}
public List<BoundedSource<HCatRecord>> beam_f31780_1(long desiredBundleSizeBytes, PipelineOptions options) throws Exception
{    int desiredSplitCount = 1;    long estimatedSizeBytes = getEstimatedSizeBytes(options);    if (desiredBundleSizeBytes > 0 && estimatedSizeBytes > 0) {        desiredSplitCount = (int) Math.ceil((double) estimatedSizeBytes / desiredBundleSizeBytes);    }    ReaderContext readerContext = getReaderContext(desiredSplitCount);                List<BoundedSource<HCatRecord>> res = new ArrayList<>();    for (int split = 0; split < readerContext.numSplits(); split++) {        res.add(new BoundedHCatalogSource(spec.withContext(readerContext).withSplitId(split)));    }    return res;}
public Write beam_f31790_0(Map<String, String> partition)
{    return toBuilder().setPartition(partition).build();}
public Write beam_f31791_0(long batchSize)
{    return toBuilder().setBatchSize(batchSize).build();}
 static IMetaStoreClient beam_f31800_0(Configuration conf) throws IOException, MetaException
{    final HiveConf hiveConf = HCatUtil.getHiveConf(conf);    return HCatUtil.getHiveMetastoreClient(hiveConf);}
 static HiveConf beam_f31801_0(Read readRequest) throws IOException
{    Configuration conf = createConfiguration(readRequest.getConfigProperties());    return HCatUtil.getHiveConf(conf);}
private List<Integer> beam_f31810_0(Read read) throws Exception
{    return IntStream.range(0, metaStoreClient.listPartitions(read.getDatabase(), read.getTable(), Short.MAX_VALUE).size()).boxed().collect(Collectors.toList());}
private ReaderContext beam_f31811_0(Read readRequest, Integer partitionIndexToRead) throws Exception
{    final List<Partition> partitions = metaStoreClient.listPartitions(readRequest.getDatabase(), readRequest.getTable(), Short.MAX_VALUE);    final Partition partition = partitions.get(partitionIndexToRead);    checkArgument(partition != null, "Unable to find a partition to read at index " + partitionIndexToRead);    final int desiredSplitCount = HCatalogUtils.getSplitCount(readRequest, partition);    final List<String> values = partition.getValues();    final List<String> partitionCols = readRequest.getPartitionCols();    checkArgument(values.size() == partitionCols.size(), "Number of input partitions should be equal to the values of list partition values.");    List<String> filter = new ArrayList<>();    for (int i = 0; i < partitionCols.size(); i++) {        filter.add(partitionCols.get(i) + "=" + "'" + values.get(i) + "'");    }    final String filterString = String.join(" and ", filter);    ReadEntity entity = new ReadEntity.Builder().withDatabase(readRequest.getDatabase()).withFilter(filterString).withTable(readRequest.getTable()).build();        Map<String, String> configProps = new HashMap<>(readRequest.getConfigProperties());    configProps.put(HCatConstants.HCAT_DESIRED_PARTITION_NUM_SPLITS, String.valueOf(desiredSplitCount));    return DataTransferFactory.getHCatReader(entity, configProps).prepareRead();}
public void beam_f31820_0() throws Exception
{    driver.close();    sessionState.close();}
public static ReaderContext beam_f31821_0(Map<String, String> config) throws HCatException
{    return DataTransferFactory.getHCatReader(READ_ENTITY, config).prepareRead();}
private static DefaultHCatRecord beam_f31830_0(int value)
{    return new DefaultHCatRecord(Arrays.asList("record " + value, value));}
public static void beam_f31831_0() throws IOException
{    service = new EmbeddedMetastoreService(TMP_FOLDER.getRoot().getAbsolutePath());}
private void beam_f31840_0()
{    service.executeQuery("drop table " + TEST_TABLE);    service.executeQuery("drop table " + TEST_TABLE_PARTITIONED);    service.executeQuery("create table " + TEST_TABLE + "(mycol1 string, mycol2 int)");    service.executeQuery("create table " + TEST_TABLE_PARTITIONED + "(mycol1 string, mycol2 int) " + "partitioned by (part1 string, part2 int)");}
public static void beam_f31841_0() throws Exception
{    PipelineOptionsFactory.register(HCatalogPipelineOptions.class);    options = TestPipeline.testingPipelineOptions().as(HCatalogPipelineOptions.class);    final String metastoreUri = String.format("thrift://%s:%s", options.getHCatalogMetastoreHostName(), options.getHCatalogMetastorePort());    configProperties = ImmutableMap.of("hive.metastore.uris", metastoreUri);    helper = new HiveDatabaseTestHelper(options.getHCatalogMetastoreHostName(), options.getHCatalogHivePort(), options.getHCatalogHiveDatabaseName(), options.getHCatalogHiveUsername(), options.getHCatalogHivePassword());    try {        tableName = helper.createHiveTable(testIdentifier);    } catch (Exception e) {        helper.closeConnection();        throw new Exception("Problem with creating table for " + testIdentifier + ": " + e, e);    }}
public void beam_f31850_0(ProcessContext c)
{    c.output(c.element().get(0).toString());}
private Map<String, String> beam_f31851_0()
{    Map<String, String> partitions = new HashMap<>();    partitions.put("load_date", "2019-05-14T23:28:04.425Z");    partitions.put("product_type", "1");    return partitions;}
public void beam_f31860_0() throws Exception
{    ReaderContext context = getReaderContext(getConfigPropertiesAsMap(service.getHiveConf()));    HCatalogIO.Read spec = HCatalogIO.read().withConfigProperties(getConfigPropertiesAsMap(service.getHiveConf())).withContext(context).withTable(TEST_TABLE);    List<String> records = new ArrayList<>();    for (int i = 0; i < context.numSplits(); i++) {        BoundedHCatalogSource source = new BoundedHCatalogSource(spec.withSplitId(i));        for (HCatRecord record : SourceTestUtils.readFromSource(source, OPTIONS)) {            records.add(record.get(0).toString());        }    }    assertThat(records, containsInAnyOrder(getExpectedRecords(TEST_RECORDS_COUNT).toArray()));}
public void beam_f31861_0() throws Exception
{    final int numRows = 1500;    final int numSamples = 10;    final long bytesPerRow = 15;    ReaderContext context = getReaderContext(getConfigPropertiesAsMap(service.getHiveConf()));    HCatalogIO.Read spec = HCatalogIO.read().withConfigProperties(getConfigPropertiesAsMap(service.getHiveConf())).withContext(context).withTable(TEST_TABLE);    BoundedHCatalogSource source = new BoundedHCatalogSource(spec);    List<BoundedSource<HCatRecord>> unSplitSource = source.split(-1, OPTIONS);    assertEquals(1, unSplitSource.size());    List<BoundedSource<HCatRecord>> splits = source.split(numRows * bytesPerRow / numSamples, OPTIONS);    assertTrue(splits.size() >= 1);    SourceTestUtils.assertSourcesEqualReferenceSource(unSplitSource.get(0), splits, OPTIONS);}
public static ReadAll<ParameterT, OutputT> beam_f31870_0()
{    return new AutoValue_JdbcIO_ReadAll.Builder<ParameterT, OutputT>().setFetchSize(DEFAULT_FETCH_SIZE).setOutputParallelization(true).build();}
public static Write<T> beam_f31871_0()
{    return new Write();}
public DataSourceConfiguration beam_f31880_0(ValueProvider<String> password)
{    return builder().setPassword(password).build();}
public DataSourceConfiguration beam_f31881_0(String connectionProperties)
{    checkArgument(connectionProperties != null, "connectionProperties can not be null");    return withConnectionProperties(ValueProvider.StaticValueProvider.of(connectionProperties));}
public ReadRows beam_f31890_0(boolean outputParallelization)
{    return toBuilder().setOutputParallelization(outputParallelization).build();}
public PCollection<Row> beam_f31891_0(PBegin input)
{    checkArgument(getQuery() != null, "withQuery() is required");    checkArgument((getDataSourceProviderFn() != null), "withDataSourceConfiguration() or withDataSourceProviderFn() is required");    Schema schema = inferBeamSchema();    PCollection<Row> rows = input.apply(JdbcIO.<Row>read().withDataSourceProviderFn(getDataSourceProviderFn()).withQuery(getQuery()).withCoder(RowCoder.of(schema)).withRowMapper(SchemaUtil.BeamRowMapper.of(schema)).withFetchSize(getFetchSize()).withOutputParallelization(getOutputParallelization()).withStatementPreparator(getStatementPreparator()));    rows.setRowSchema(schema);    return rows;}
public Read<T> beam_f31900_0(Coder<T> coder)
{    checkArgument(coder != null, "coder can not be null");    return toBuilder().setCoder(coder).build();}
public Read<T> beam_f31901_0(int fetchSize)
{    checkArgument(fetchSize > 0, "fetch size must be > 0");    return toBuilder().setFetchSize(fetchSize).build();}
public ReadAll<ParameterT, OutputT> beam_f31910_0(RowMapper<OutputT> rowMapper)
{    checkArgument(rowMapper != null, "JdbcIO.readAll().withRowMapper(rowMapper) called with null rowMapper");    return toBuilder().setRowMapper(rowMapper).build();}
public ReadAll<ParameterT, OutputT> beam_f31911_0(Coder<OutputT> coder)
{    checkArgument(coder != null, "JdbcIO.readAll().withCoder(coder) called with null coder");    return toBuilder().setCoder(coder).build();}
public Write<T> beam_f31920_0(SerializableFunction<Void, DataSource> dataSourceProviderFn)
{    return new Write(inner.withDataSourceProviderFn(dataSourceProviderFn));}
public Write<T> beam_f31921_0(String statement)
{    return new Write(inner.withStatement(statement));}
private List<SchemaUtil.FieldWithIndex> beam_f31930_0(Schema schema)
{    Schema tableSchema;    try (Connection connection = inner.getDataSourceProviderFn().apply(null).getConnection();        PreparedStatement statement = connection.prepareStatement((String.format("SELECT * FROM %s", inner.getTable())))) {        tableSchema = SchemaUtil.toBeamSchema(statement.getMetaData());        statement.close();    } catch (SQLException e) {        throw new RuntimeException("Error while determining columns from table: " + inner.getTable(), e);    }    if (tableSchema.getFieldCount() < schema.getFieldCount()) {        throw new RuntimeException("Input schema has more fields than actual table.");    }        List<Schema.Field> missingFields = tableSchema.getFields().stream().filter(line -> schema.getFields().stream().noneMatch(s -> s.getName().equalsIgnoreCase(line.getName()))).collect(Collectors.toList());        if (checkNullabilityForFields(missingFields)) {        throw new RuntimeException("Non nullable fields are not allowed without schema.");    }    List<SchemaUtil.FieldWithIndex> tableFilteredFields = tableSchema.getFields().stream().map((tableField) -> {        Optional<Schema.Field> optionalSchemaField = schema.getFields().stream().filter((f) -> SchemaUtil.compareSchemaField(tableField, f)).findFirst();        return (optionalSchemaField.isPresent()) ? SchemaUtil.FieldWithIndex.of(tableField, schema.getFields().indexOf(optionalSchemaField.get())) : null;    }).filter(Objects::nonNull).collect(Collectors.toList());    if (tableFilteredFields.size() != schema.getFieldCount()) {        throw new RuntimeException("Provided schema doesn't match with database schema.");    }    return tableFilteredFields;}
private void beam_f31931_0()
{    IntStream.range(0, fields.size()).forEach((index) -> {        Schema.FieldType fieldType = fields.get(index).getField().getType();        preparedStatementFieldSetterList.add(JdbcUtil.getPreparedStatementSetCaller(fieldType));    });}
public WriteVoid<T> beam_f31940_0(String table)
{    checkArgument(table != null, "table name can not be null");    return toBuilder().setTable(table).build();}
public PCollection<Void> beam_f31941_0(PCollection<T> input)
{    checkArgument(getStatement() != null, "withStatement() is required");    checkArgument(getPreparedStatementSetter() != null, "withPreparedStatementSetter() is required");    checkArgument((getDataSourceProviderFn() != null), "withDataSourceConfiguration() or withDataSourceProviderFn() is required");    return input.apply(ParDo.of(new WriteFn<>(this)));}
public static synchronized SerializableFunction<Void, DataSource> beam_f31950_0(DataSourceConfiguration config)
{    if (instance == null) {        instance = new PoolableDataSourceProvider(config);    }    return instance;}
public DataSource beam_f31951_0(Void input)
{    return buildDataSource(input);}
private static JdbcIO.PreparedStatementSetCaller beam_f31960_0()
{    return (element, ps, i, fieldWithIndex) -> {        validateLogicalTypeLength(fieldWithIndex.getField(), element.getString(fieldWithIndex.getIndex()).length());        ps.setString(i + 1, element.getString(fieldWithIndex.getIndex()));    };}
private static void beam_f31961_0(Schema.Field field, Integer length)
{    try {        if (field.getType().getTypeName().isLogicalType() && !field.getType().getLogicalType().getArgument().isEmpty()) {            int maxLimit = Integer.parseInt(field.getType().getLogicalType().getArgument());            if (field.getType().getTypeName().isLogicalType() && length >= maxLimit) {                throw new RuntimeException(String.format("Length of Schema.Field[%s] data exceeds database column capacity", field.getName()));            }        }    } catch (NumberFormatException e) {        }}
public String beam_f31970_0()
{    return argument;}
public Schema.FieldType beam_f31971_0()
{    return baseType;}
public String beam_f31980_0(String base)
{    checkArgument(base == null || base.length() <= maxLength);    return base;}
public static VariableLengthBytes beam_f31981_0(String identifier, int maxLength)
{    return new VariableLengthBytes(identifier, maxLength);}
private static BeamFieldConverter beam_f31990_0()
{    return (index, md) -> {        JDBCType elementJdbcType = valueOf(md.getColumnTypeName(index));        BeamFieldConverter elementFieldConverter = jdbcTypeToBeamFieldConverter(elementJdbcType);        String label = md.getColumnLabel(index);        Schema.FieldType elementBeamType = elementFieldConverter.create(index, md).getType();        return Schema.Field.of(label, Schema.FieldType.array(elementBeamType)).withNullable(md.isNullable(index) == ResultSetMetaData.columnNullable);    };}
private static ResultSetFieldExtractor beam_f31991_0(Schema.FieldType fieldType)
{    Schema.TypeName typeName = fieldType.getTypeName();    switch(typeName) {        case ARRAY:            Schema.FieldType elementType = fieldType.getCollectionElementType();            ResultSetFieldExtractor elementExtractor = createFieldExtractor(elementType);            return createArrayExtractor(elementExtractor);        case DATETIME:            return TIMESTAMP_EXTRACTOR;        case LOGICAL_TYPE:            return createLogicalTypeExtractor(fieldType.getLogicalType());        default:            if (!RESULTSET_FIELD_EXTRACTORS.containsKey(typeName)) {                throw new UnsupportedOperationException("BeamRowMapper does not have support for fields of type " + fieldType.toString());            }            return RESULTSET_FIELD_EXTRACTORS.get(typeName);    }}
public static boolean beam_f32000_0(List<Schema.Field> fields)
{    return fields.stream().anyMatch(field -> !field.getType().getNullable());}
public static boolean beam_f32001_0(Schema.FieldType a, Schema.FieldType b)
{    if (a.getTypeName().equals(b.getTypeName())) {        return !a.getTypeName().equals(Schema.TypeName.LOGICAL_TYPE) || compareSchemaFieldType(a.getLogicalType().getBaseType(), b.getLogicalType().getBaseType());    } else if (a.getTypeName().isLogicalType()) {        return a.getLogicalType().getBaseType().getTypeName().equals(b.getTypeName());    } else if (b.getTypeName().isLogicalType()) {        return b.getLogicalType().getBaseType().getTypeName().equals(a.getTypeName());    }    return false;}
private void beam_f32010_0(PipelineResult writeResult, PipelineResult readResult)
{    String uuid = UUID.randomUUID().toString();    String timestamp = Timestamp.now().toString();    Set<Function<MetricsReader, NamedTestResult>> metricSuppliers = getWriteMetricSuppliers(uuid, timestamp);    IOITMetrics writeMetrics = new IOITMetrics(metricSuppliers, writeResult, NAMESPACE, uuid, timestamp);    writeMetrics.publish(bigQueryDataset, bigQueryTable);    IOITMetrics readMetrics = new IOITMetrics(getReadMetricSuppliers(uuid, timestamp), readResult, NAMESPACE, uuid, timestamp);    readMetrics.publish(bigQueryDataset, bigQueryTable);}
private Set<Function<MetricsReader, NamedTestResult>> beam_f32011_0(String uuid, String timestamp)
{    Set<Function<MetricsReader, NamedTestResult>> suppliers = new HashSet<>();    suppliers.add(reader -> {        long writeStart = reader.getStartTimeMetric("write_time");        long writeEnd = reader.getEndTimeMetric("write_time");        return NamedTestResult.create(uuid, timestamp, "write_time", (writeEnd - writeStart) / 1e3);    });    return suppliers;}
public void beam_f32020_0() throws Exception
{    JdbcIO.DataSourceConfiguration config = JdbcIO.DataSourceConfiguration.create("org.apache.derby.jdbc.ClientDriver", "jdbc:derby://localhost:" + port + "/target/beam");    try (Connection conn = config.buildDatasource().getConnection()) {        assertTrue(conn.isValid(0));    }}
public void beam_f32021_0() throws Exception
{    String username = "sa";    String password = "sa";    JdbcIO.DataSourceConfiguration config = JdbcIO.DataSourceConfiguration.create("org.apache.derby.jdbc.ClientDriver", "jdbc:derby://localhost:" + port + "/target/beam").withUsername(username).withPassword(password);    try (Connection conn = config.buildDatasource().getConnection()) {        assertTrue(conn.isValid(0));    }}
public void beam_f32030_0() throws Exception
{    final long rowsToAdd = 1000L;    String tableName = DatabaseTestHelper.getTestTableName("UT_WRITE");    DatabaseTestHelper.createTable(dataSource, tableName);    try {        ArrayList<KV<Integer, String>> data = getDataToWrite(rowsToAdd);        pipeline.apply(Create.of(data)).apply(getJdbcWrite(tableName));        pipeline.run();        assertRowCount(tableName, EXPECTED_ROW_COUNT);    } finally {        DatabaseTestHelper.deleteTable(dataSource, tableName);    }}
public void beam_f32031_0() throws Exception
{    final long rowsToAdd = 1000L;    String firstTableName = DatabaseTestHelper.getTestTableName("UT_WRITE");    String secondTableName = DatabaseTestHelper.getTestTableName("UT_WRITE_AFTER_WAIT");    DatabaseTestHelper.createTable(dataSource, firstTableName);    DatabaseTestHelper.createTable(dataSource, secondTableName);    try {        ArrayList<KV<Integer, String>> data = getDataToWrite(rowsToAdd);        PCollection<KV<Integer, String>> dataCollection = pipeline.apply(Create.of(data));        PCollection<Void> rowsWritten = dataCollection.apply(getJdbcWrite(firstTableName).withResults());        dataCollection.apply(Wait.on(rowsWritten)).apply(getJdbcWrite(secondTableName));        pipeline.run();        assertRowCount(firstTableName, EXPECTED_ROW_COUNT);        assertRowCount(secondTableName, EXPECTED_ROW_COUNT);    } finally {        DatabaseTestHelper.deleteTable(dataSource, firstTableName);    }}
public void beam_f32040_0() throws Exception
{    final int rowsToAdd = 10;    String tableName = DatabaseTestHelper.getTestTableName("UT_WRITE_PS_NON_ROW");    DatabaseTestHelper.createTableForRowWithSchema(dataSource, tableName);    try {        List<RowWithSchema> data = getRowsWithSchemaToWrite(rowsToAdd);        pipeline.apply(Create.of(data)).apply(JdbcIO.<RowWithSchema>write().withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create("org.apache.derby.jdbc.ClientDriver", "jdbc:derby://localhost:" + port + "/target/beam")).withBatchSize(10L).withTable(tableName));        pipeline.run();        assertRowCount(tableName, rowsToAdd);    } finally {        DatabaseTestHelper.deleteTable(dataSource, tableName);    }}
public void beam_f32041_0() throws Exception
{    Schema schema = Schema.builder().addField("bigint_col", Schema.FieldType.INT64).addField("binary_col", Schema.FieldType.BYTES).addField("bit_col", Schema.FieldType.BOOLEAN).addField("char_col", Schema.FieldType.STRING).addField("decimal_col", Schema.FieldType.DECIMAL).addField("double_col", Schema.FieldType.DOUBLE).addField("float_col", Schema.FieldType.FLOAT).addField("integer_col", Schema.FieldType.INT32).addField("datetime_col", Schema.FieldType.DATETIME).addField("int16_col", Schema.FieldType.INT16).addField("byte_col", Schema.FieldType.BYTE).build();    Row row = Row.withSchema(schema).addValues(42L, "binary".getBytes(Charset.forName("UTF-8")), true, "char", BigDecimal.valueOf(25L), 20.5D, 15.5F, 10, new DateTime(), (short) 5, Byte.parseByte("1", 2)).build();    PreparedStatement psMocked = mock(PreparedStatement.class);    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.INT64).set(row, psMocked, 0, SchemaUtil.FieldWithIndex.of(schema.getField(0), 0));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.BYTES).set(row, psMocked, 1, SchemaUtil.FieldWithIndex.of(schema.getField(1), 1));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.BOOLEAN).set(row, psMocked, 2, SchemaUtil.FieldWithIndex.of(schema.getField(2), 2));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.STRING).set(row, psMocked, 3, SchemaUtil.FieldWithIndex.of(schema.getField(3), 3));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.DECIMAL).set(row, psMocked, 4, SchemaUtil.FieldWithIndex.of(schema.getField(4), 4));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.DOUBLE).set(row, psMocked, 5, SchemaUtil.FieldWithIndex.of(schema.getField(5), 5));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.FLOAT).set(row, psMocked, 6, SchemaUtil.FieldWithIndex.of(schema.getField(6), 6));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.INT32).set(row, psMocked, 7, SchemaUtil.FieldWithIndex.of(schema.getField(7), 7));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.DATETIME).set(row, psMocked, 8, SchemaUtil.FieldWithIndex.of(schema.getField(8), 8));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.INT16).set(row, psMocked, 9, SchemaUtil.FieldWithIndex.of(schema.getField(9), 9));    JdbcUtil.getPreparedStatementSetCaller(Schema.FieldType.BYTE).set(row, psMocked, 10, SchemaUtil.FieldWithIndex.of(schema.getField(10), 10));    verify(psMocked, times(1)).setLong(1, 42L);    verify(psMocked, times(1)).setBytes(2, "binary".getBytes(Charset.forName("UTF-8")));    verify(psMocked, times(1)).setBoolean(3, true);    verify(psMocked, times(1)).setString(4, "char");    verify(psMocked, times(1)).setBigDecimal(5, BigDecimal.valueOf(25L));    verify(psMocked, times(1)).setDouble(6, 20.5D);    verify(psMocked, times(1)).setFloat(7, 15.5F);    verify(psMocked, times(1)).setInt(8, 10);    verify(psMocked, times(1)).setTimestamp(9, new Timestamp(row.getDateTime("datetime_col").getMillis()));    verify(psMocked, times(1)).setInt(10, (short) 5);    verify(psMocked, times(1)).setByte(11, Byte.parseByte("1", 2));}
public void beam_f32050_0() throws Exception
{    Schema wantSchema = Schema.builder().addField("col1", Schema.FieldType.INT64).addField("col2", Schema.FieldType.INT64).addField("col3", Schema.FieldType.INT64).build();    String generatedStmt = JdbcUtil.generateStatement("test_table", wantSchema.getFields());    String expectedStmt = "INSERT INTO test_table(col1, col2, col3) VALUES(?, ?, ?)";    assertEquals(expectedStmt, generatedStmt);}
public String beam_f32051_0()
{    return name;}
private static JdbcFieldInfo beam_f32060_0(String columnLabel, int columnType, int precision)
{    return new JdbcFieldInfo(columnLabel, columnType, null, false, precision, 0);}
private static JdbcFieldInfo beam_f32061_0(String columnLabel, int columnType, int precision, int scale)
{    return new JdbcFieldInfo(columnLabel, columnType, null, false, precision, scale);}
public List<Message> beam_f32070_0()
{    return atomicRead(() -> messages);}
public void beam_f32071_0(Message message)
{    atomicWrite(() -> messages.add(message));}
public Read<T> beam_f32080_0(ConnectionFactory connectionFactory)
{    checkArgument(connectionFactory != null, "connectionFactory can not be null");    return builder().setConnectionFactory(connectionFactory).build();}
public Read<T> beam_f32081_0(String queue)
{    checkArgument(queue != null, "queue can not be null");    return builder().setQueue(queue).build();}
public void beam_f32090_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.addIfNotNull(DisplayData.item("queue", getQueue()));    builder.addIfNotNull(DisplayData.item("topic", getTopic()));}
 UnboundedSource<T, JmsCheckpointMark> beam_f32091_0()
{    return new UnboundedJmsSource<T>(this);}
public Instant beam_f32100_0()
{    if (currentMessage == null) {        throw new NoSuchElementException();    }    return currentTimestamp;}
public CheckpointMark beam_f32101_0()
{    return checkpointMark;}
public void beam_f32110_0() throws Exception
{    if (producer == null) {        if (spec.getUsername() != null) {            this.connection = spec.getConnectionFactory().createConnection(spec.getUsername(), spec.getPassword());        } else {            this.connection = spec.getConnectionFactory().createConnection();        }        this.connection.start();                this.session = this.connection.createSession(false, Session.AUTO_ACKNOWLEDGE);        Destination destination;        if (spec.getQueue() != null) {            destination = session.createQueue(spec.getQueue());        } else {            destination = session.createTopic(spec.getTopic());        }        this.producer = this.session.createProducer(destination);    }}
public void beam_f32111_0(ProcessContext ctx) throws Exception
{    String value = ctx.element();    TextMessage message = session.createTextMessage(value);    producer.send(message);}
public String beam_f32120_0()
{    return jmsType;}
public long beam_f32121_0()
{    return jmsExpiration;}
public void beam_f32130_0()
{    pipeline.apply(JmsIO.read().withConnectionFactory(connectionFactory).withQueue(QUEUE));    runPipelineExpectingJmsConnectException("User name [null] or password is invalid.");}
public void beam_f32131_0()
{    pipeline.apply(JmsIO.read().withConnectionFactory(connectionFactory).withQueue(QUEUE).withUsername(USERNAME).withPassword("BAD"));    runPipelineExpectingJmsConnectException("User name [" + USERNAME + "] or password is invalid.");}
public String beam_f32140_0(Message message) throws Exception
{    BytesMessage bytesMessage = (BytesMessage) message;    byte[] bytes = new byte[(int) bytesMessage.getBodyLength()];    return new String(bytes, StandardCharsets.UTF_8);}
private ConnectionFactory beam_f32141_0(ConnectionFactory factory, long delay)
{    return proxyMethod(factory, ConnectionFactory.class, "createConnection", (Connection connection) -> proxyMethod(connection, Connection.class, "createSession", (Session session) -> proxyMethod(session, Session.class, "createConsumer", (MessageConsumer consumer) -> proxyMethod(consumer, MessageConsumer.class, "receiveNoWait", (ActiveMQMessage message) -> {        final Callback originalCallback = message.getAcknowledgeCallback();        message.setAcknowledgeCallback(() -> {            Thread.sleep(delay);            originalCallback.execute();        });        return message;    }))));}
public Instant beam_f32150_0(PartitionContext ctx, KafkaRecord<K, V> record)
{    Instant ts = timestampFunction.apply(record);    if (ts.isAfter(maxEventTimestamp)) {        maxEventTimestamp = ts;    }    return ts;}
public Instant beam_f32151_0(PartitionContext ctx)
{                                Instant now = Instant.now();    return getWatermark(ctx, now);}
public String beam_f32160_0()
{    return "PartitionMark{" + "topic='" + topic + '\'' + ", partition=" + partition + ", nextOffset=" + nextOffset + ", watermarkMillis=" + watermarkMillis + '}';}
 static void beam_f32161_0()
{    checkArgument(ProducerSpEL.supportsTransactions(), "%s %s", "This version of Kafka client does not support transactions required to support", "exactly-once semantics. Please use Kafka client version 0.11 or newer.");}
 void beam_f32170_1(long lastRecordId, Counter numTransactions) throws IOException
{    try {                                        ProducerSpEL.sendOffsetsToTransaction(producer, ImmutableMap.of(new TopicPartition(spec.getTopic(), shard), new OffsetAndMetadata(0L, JSON_MAPPER.writeValueAsString(new ShardMetadata(lastRecordId, writerId)))), spec.getSinkGroupId());        ProducerSpEL.commitTransaction(producer);        numTransactions.inc();                committedId = lastRecordId;    } catch (KafkaException e) {        ProducerSpEL.abortTransaction(producer);        throw e;    }}
private ShardWriter<K, V> beam_f32171_1(int shard, ValueState<String> writerIdState, long nextId) throws IOException
{    String producerName = String.format("producer_%d_for_%s", shard, spec.getSinkGroupId());    Producer<K, V> producer = initializeExactlyOnceProducer(spec, producerName);        try {        String writerId = writerIdState.read();        OffsetAndMetadata committed;        try (Consumer<?, ?> consumer = openConsumer(spec)) {            committed = consumer.committed(new TopicPartition(spec.getTopic(), shard));        }        long committedSeqId = -1;        if (committed == null || committed.metadata() == null || committed.metadata().isEmpty()) {            checkState(nextId == 0 && writerId == null, "State exists for shard %s (nextId %s, writerId '%s'), but there is no state " + "stored with Kafka topic '%s' group id '%s'", shard, nextId, writerId, spec.getTopic(), spec.getSinkGroupId());            writerId = String.format("%X - %s", new Random().nextInt(Integer.MAX_VALUE), DateTimeFormat.forPattern("yyyy-MM-dd HH:mm:ss").withZone(DateTimeZone.UTC).print(DateTimeUtils.currentTimeMillis()));            writerIdState.write(writerId);                    } else {            ShardMetadata metadata = JSON_MAPPER.readValue(committed.metadata(), ShardMetadata.class);            checkNotNull(metadata.writerId);            if (writerId == null) {                                throw new IllegalStateException(String.format("Kafka metadata exists for shard %s, but there is no stored state for it. " + "This mostly indicates groupId '%s' is used else where or in earlier runs. " + "Try another group id. Metadata for this shard on Kafka : '%s'", shard, spec.getSinkGroupId(), committed.metadata()));            }            checkState(writerId.equals(metadata.writerId), "Writer ids don't match. This is mostly a unintended misuse of groupId('%s')." + "Beam '%s', Kafka '%s'", spec.getSinkGroupId(), writerId, metadata.writerId);            committedSeqId = metadata.sequenceId;            checkState(committedSeqId >= (nextId - 1), "Committed sequence id can not be lower than %s, partition metadata : %s", nextId - 1, committed.metadata());        }                return new ShardWriter<>(shard, writerId, producer, producerName, spec, committedSeqId);    } catch (IOException e) {        producer.close();        throw e;    }}
public static WriteRecords<K, V> beam_f32180_0()
{    return new AutoValue_KafkaIO_WriteRecords.Builder<K, V>().setProducerConfig(WriteRecords.DEFAULT_PRODUCER_PROPERTIES).setEOS(false).setNumShards(0).setConsumerFactoryFn(Read.KAFKA_CONSUMER_FACTORY_FN).build();}
public PTransform<PBegin, PCollection<KV<K, V>>> beam_f32181_0(External.Configuration config)
{    ImmutableList.Builder<String> listBuilder = ImmutableList.builder();    for (String topic : config.topics) {        listBuilder.add(topic);    }    setTopics(listBuilder.build());    Class keyDeserializer = resolveClass(config.keyDeserializer);    setKeyDeserializer(keyDeserializer);    setKeyCoder(resolveCoder(keyDeserializer));    Class valueDeserializer = resolveClass(config.valueDeserializer);    setValueDeserializer(valueDeserializer);    setValueCoder(resolveCoder(valueDeserializer));    Map<String, Object> consumerConfig = new HashMap<>();    for (KV<String, String> kv : config.consumerConfig) {        consumerConfig.put(kv.getKey(), kv.getValue());    }        consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializer.getName());    consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializer.getName());    setConsumerConfig(consumerConfig);        setTopicPartitions(Collections.emptyList());    setConsumerFactoryFn(Read.KAFKA_CONSUMER_FACTORY_FN);    setMaxNumRecords(Long.MAX_VALUE);    setCommitOffsetsInFinalizeEnabled(false);    setTimestampPolicyFactory(TimestampPolicyFactory.withProcessingTime());        return build().withoutMetadata();}
public Read<K, V> beam_f32190_0(List<String> topics)
{    checkState(getTopicPartitions().isEmpty(), "Only topics or topicPartitions can be set, not both");    return toBuilder().setTopics(ImmutableList.copyOf(topics)).build();}
public Read<K, V> beam_f32191_0(List<TopicPartition> topicPartitions)
{    checkState(getTopics().isEmpty(), "Only topics or topicPartitions can be set, not both");    return toBuilder().setTopicPartitions(ImmutableList.copyOf(topicPartitions)).build();}
public Read<K, V> beam_f32200_0(Duration maxReadTime)
{    return toBuilder().setMaxReadTime(maxReadTime).build();}
public Read<K, V> beam_f32201_0()
{    return withTimestampPolicyFactory(TimestampPolicyFactory.withLogAppendTime());}
public Read<K, V> beam_f32210_0()
{    return toBuilder().setCommitOffsetsInFinalizeEnabled(true).build();}
public Read<K, V> beam_f32211_0(Map<String, Object> offsetConsumerConfig)
{    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();}
public void beam_f32220_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    read.populateDisplayData(builder);}
private static Map<String, Object> beam_f32221_0(Map<String, Object> currentConfig, Map<String, String> ignoredProperties, Map<String, Object> updates)
{    for (String key : updates.keySet()) {        checkArgument(!ignoredProperties.containsKey(key), "No need to configure '%s'. %s", key, ignoredProperties.get(key));    }    Map<String, Object> config = new HashMap<>(currentConfig);    config.putAll(updates);    return config;}
public WriteRecords<K, V> beam_f32230_0(KafkaPublishTimestampFunction<ProducerRecord<K, V>> timestampFunction)
{    return toBuilder().setPublishTimestampFunction(timestampFunction).build();}
public WriteRecords<K, V> beam_f32231_0(int numShards, String sinkGroupId)
{    KafkaExactlyOnceSink.ensureEOSSupport();    checkArgument(numShards >= 1, "numShards should be >= 1");    checkArgument(sinkGroupId != null, "sinkGroupId is required for exactly-once sink");    return toBuilder().setEOS(true).setNumShards(numShards).setSinkGroupId(sinkGroupId).build();}
public void beam_f32240_0(String keySerializer)
{    this.keySerializer = keySerializer;}
public void beam_f32241_0(String valueSerializer)
{    this.valueSerializer = valueSerializer;}
public Write<K, V> beam_f32250_0(int numShards, String sinkGroupId)
{    return withWriteRecordsTransform(getWriteRecordsTransform().withEOS(numShards, sinkGroupId));}
public Write<K, V> beam_f32251_0(SerializableFunction<Map<String, Object>, ? extends Consumer<?, ?>> consumerFactoryFn)
{    return withWriteRecordsTransform(getWriteRecordsTransform().withConsumerFactoryFn(consumerFactoryFn));}
public PDone beam_f32260_0(PCollection<V> input)
{    return input.apply("Kafka values with default key", MapElements.via(new SimpleFunction<V, KV<K, V>>() {        @Override        public KV<K, V> apply(V element) {            return KV.of(null, element);        }    })).setCoder(KvCoder.of(new NullOnlyCoder<>(), input.getCoder())).apply(kvWriteTransform);}
public KV<K, V> beam_f32261_0(V element)
{    return KV.of(null, element);}
public long beam_f32270_0()
{    return offset;}
public Headers beam_f32271_0()
{    if (!ConsumerSpEL.hasHeaders()) {        throw new RuntimeException("The version kafka-clients does not support record headers, " + "please use version 0.11.0.0 or newer");    }    return headers;}
private Object beam_f32280_0(Iterable<KV<String, byte[]>> records)
{    if (!ConsumerSpEL.hasHeaders()) {        return null;    }        ConsumerRecord<String, String> consumerRecord = new ConsumerRecord<>("", 0, 0L, "", "");    records.forEach(kv -> consumerRecord.headers().add(kv.getKey(), kv.getValue()));    return consumerRecord.headers();}
private Iterable<KV<String, byte[]>> beam_f32281_0(KafkaRecord record)
{    if (!ConsumerSpEL.hasHeaders()) {        return Collections.emptyList();    }    List<KV<String, byte[]>> vals = new ArrayList<>();    for (Header header : record.getHeaders()) {        vals.add(KV.of(header.key(), header.value()));    }    return vals;}
public boolean beam_f32290_1() throws IOException
{    /* Read first record (if any). we need to loop here because :     *  - (a) some records initially need to be skipped if they are before consumedOffset     *  - (b) if curBatch is empty, we want to fetch next batch and then advance.     *  - (c) curBatch is an iterator of iterators. we interleave the records from each.     *        curBatch.next() might return an empty iterator.     */    while (true) {        if (curBatch.hasNext()) {            PartitionState<K, V> pState = curBatch.next();            if (!pState.recordIter.hasNext()) {                                                pState.recordIter = Collections.emptyIterator();                curBatch.remove();                continue;            }            elementsRead.inc();            elementsReadBySplit.inc();            ConsumerRecord<byte[], byte[]> rawRecord = pState.recordIter.next();            long expected = pState.nextOffset;            long offset = rawRecord.offset();            if (offset < expected) {                                                                                continue;            }                        long offsetGap = offset - expected;            if (curRecord == null) {                                offsetGap = 0;            }                                                KafkaRecord<K, V> record = new KafkaRecord<>(rawRecord.topic(), rawRecord.partition(), rawRecord.offset(), consumerSpEL.getRecordTimestamp(rawRecord), consumerSpEL.getRecordTimestampType(rawRecord), ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null, keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()), valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));            curTimestamp = pState.timestampPolicy.getTimestampForRecord(pState.mkTimestampPolicyContext(), record);            curRecord = record;            int recordSize = (rawRecord.key() == null ? 0 : rawRecord.key().length) + (rawRecord.value() == null ? 0 : rawRecord.value().length);            pState.recordConsumed(offset, recordSize, offsetGap);            bytesRead.inc(recordSize);            bytesReadBySplit.inc(recordSize);            return true;        } else {                        nextBatch();            if (!curBatch.hasNext()) {                return false;            }        }    }}
public Instant beam_f32291_1()
{    if (source.getSpec().getWatermarkFn() != null) {                if (curRecord == null) {                        return initialWatermark;        }        return source.getSpec().getWatermarkFn().apply(curRecord);    }        return partitionStates.stream().map(PartitionState::updateAndGetWatermark).min(Comparator.naturalOrder()).get();}
public long beam_f32300_0()
{    return messageBacklog;}
public Instant beam_f32301_0()
{    return backlogCheckTime;}
 void beam_f32310_0(KafkaCheckpointMark checkpointMark)
{    if (finalizedCheckpointMark.getAndSet(checkpointMark) != null) {        checkpointMarkCommitsSkipped.inc();    }    checkpointMarkCommitsEnqueued.inc();}
private void beam_f32311_1() throws IOException
{    curBatch = Collections.emptyIterator();    ConsumerRecords<byte[], byte[]> records;    try {                records = availableRecordsQueue.poll(RECORDS_DEQUEUE_POLL_TIMEOUT.getMillis(), TimeUnit.MILLISECONDS);    } catch (InterruptedException e) {        Thread.currentThread().interrupt();                return;    }    if (records == null) {                if (consumerPollException.get() != null) {            throw new IOException("Exception while reading from Kafka", consumerPollException.get());        }        return;    }    partitionStates.forEach(p -> p.recordIter = records.records(p.topicPartition).iterator());        curBatch = Iterators.cycle(new ArrayList<>(partitionStates));}
public Coder<KafkaCheckpointMark> beam_f32320_0()
{    return AvroCoder.of(KafkaCheckpointMark.class);}
public boolean beam_f32321_0()
{        return false;}
public void beam_f32330_1(RecordMetadata metadata, Exception exception)
{    if (exception == null) {        return;    }    synchronized (KafkaWriter.this) {        if (sendException == null) {            sendException = exception;        }        numSendFailures++;    }        }
public static ProducerRecordCoder<K, V> beam_f32331_0(Coder<K> keyCoder, Coder<V> valueCoder)
{    return new ProducerRecordCoder<>(keyCoder, valueCoder);}
public boolean beam_f32340_0()
{    return kvCoder.consistentWithEquals();}
 static boolean beam_f32341_0()
{    return supportsTransactions;}
public byte[] beam_f32353_0(String topic, Instant instant)
{    return LONG_SERIALIZER.serialize(topic, instant.getMillis());}
 static TimestampPolicyFactory<K, V> beam_f32355_0()
{    return (tp, prev) -> new ProcessingTimePolicy<>();}
public Instant beam_f32364_0(PartitionContext context)
{    return lastRecordTimestamp;}
private static List<Long> beam_f32365_0(TimestampPolicy<String, String> policy, Instant now, List<Long> timestampOffsets)
{    return timestampOffsets.stream().map(ts -> {        Instant result = policy.getTimestampForRecord(null, new KafkaRecord<>("topic", 0, 0, now.getMillis() + ts, KafkaTimestampType.CREATE_TIME, new RecordHeaders(), "key", "value"));        return result.getMillis() - now.getMillis();    }).collect(Collectors.toList());}
public static void beam_f32375_0() throws IOException
{    options = IOITHelper.readIOTestPipelineOptions(Options.class);    sourceOptions = fromJsonString(options.getSourceOptions(), SyntheticSourceOptions.class);}
public void beam_f32376_0() throws IOException
{    writePipeline.apply("Generate records", Read.from(new SyntheticBoundedSource(sourceOptions))).apply("Measure write time", ParDo.of(new TimeMonitor<>(NAMESPACE, WRITE_TIME_METRIC_NAME))).apply("Write to Kafka", writeToKafka());    PCollection<String> hashcode = readPipeline.apply("Read from Kafka", readFromKafka()).apply("Measure read time", ParDo.of(new TimeMonitor<>(NAMESPACE, READ_TIME_METRIC_NAME))).apply("Map records to strings", MapElements.via(new MapKafkaRecordsToStrings())).apply("Calculate hashcode", Combine.globally(new HashingFn()).withoutDefaults());    PAssert.thatSingleton(hashcode).isEqualTo(EXPECTED_HASHCODE);    PipelineResult writeResult = writePipeline.run();    writeResult.waitUntilFinish();    PipelineResult readResult = readPipeline.run();    PipelineResult.State readState = readResult.waitUntilFinish(Duration.standardSeconds(options.getReadTimeout()));    cancelIfNotTerminal(readResult, readState);    Set<NamedTestResult> metrics = readMetrics(writeResult, readResult);    IOITMetrics.publish(TEST_ID, TIMESTAMP, options.getBigQueryDataset(), options.getBigQueryTable(), metrics);}
public void beam_f32385_0()
{        int recordsAdded = 0;    for (TopicPartition tp : assignedPartitions.get()) {        long curPos = consumer.position(tp);        for (ConsumerRecord<byte[], byte[]> r : records.get(tp)) {            if (r.offset() >= curPos) {                consumer.addRecord(r);                recordsAdded++;            }        }    }    if (recordsAdded == 0) {        if (config.get("inject.error.at.eof") != null) {            consumer.setException(new KafkaException("Injected error in consumer.poll()"));        }                                Uninterruptibles.sleepUninterruptibly(10, TimeUnit.MILLISECONDS);            }    consumer.schedulePollTask(this);}
public Consumer<byte[], byte[]> beam_f32386_0(Map<String, Object> config)
{    return mkMockConsumer(topics, partitionsPerTopic, numElements, offsetResetStrategy, config);}
public void beam_f32395_0()
{    int numElements = 1000;    List<String> topics = ImmutableList.of("test");    KafkaIO.Read<byte[], Long> reader = KafkaIO.<byte[], Long>read().withBootstrapServers("none").withTopicPartitions(ImmutableList.of(new TopicPartition("test", 5))).withConsumerFactoryFn(new ConsumerFactoryFn(topics, 10, numElements,     OffsetResetStrategy.EARLIEST)).withKeyDeserializer(ByteArrayDeserializer.class).withValueDeserializer(LongDeserializer.class).withMaxNumRecords(numElements / 10);    PCollection<Long> input = p.apply(reader.withoutMetadata()).apply(Values.create());        PAssert.that(input).satisfies(new AssertMultipleOf(5));    PAssert.thatSingleton(input.apply(Count.globally())).isEqualTo(numElements / 10L);    p.run();}
public void beam_f32396_0(ProcessContext c) throws Exception
{    c.output(c.element() - c.timestamp().getMillis());}
public void beam_f32405_0()
{                            final int numElements = 1000;    final int numPartitions = 10;    String topic = "testUnboundedSourceWithoutBoundedWrapper";    KafkaIO.Read<byte[], Long> reader = KafkaIO.<byte[], Long>read().withBootstrapServers(topic).withTopic(topic).withConsumerFactoryFn(new ConsumerFactoryFn(ImmutableList.of(topic), numPartitions, numElements, OffsetResetStrategy.EARLIEST)).withKeyDeserializer(ByteArrayDeserializer.class).withValueDeserializer(LongDeserializer.class).withTimestampPolicyFactory(new TimestampPolicyWithEndOfSource<>(numElements / numPartitions - 1));    p.apply("readFromKafka", reader.withoutMetadata()).apply(Values.create()).apply(Window.into(FixedWindows.of(Duration.standardDays(100))));    PipelineResult result = p.run();    MetricName elementsRead = SourceMetrics.elementsRead().getName();    MetricQueryResults metrics = result.metrics().queryMetrics(MetricsFilter.builder().addNameFilter(MetricNameFilter.inNamespace(elementsRead.getNamespace())).build());    assertThat(metrics.getCounters(), hasItem(attemptedMetricsResult(elementsRead.getNamespace(), elementsRead.getName(), "readFromKafka", (long) numElements)));}
public void beam_f32406_0(ProcessContext ctx)
{    ctx.output(ctx.element().getKV());}
public void beam_f32415_0() throws Exception
{            int numElements = 1000;    try (MockProducerWrapper producerWrapper = new MockProducerWrapper()) {        ProducerSendCompletionThread completionThread = new ProducerSendCompletionThread(producerWrapper.mockProducer).start();        String topic = "test";        p.apply(mkKafkaReadTransform(numElements, new ValueAsTimestampFn()).withoutMetadata()).apply(ParDo.of(new KV2ProducerRecord(topic))).setCoder(ProducerRecordCoder.of(VarIntCoder.of(), VarLongCoder.of())).apply(KafkaIO.<Integer, Long>writeRecords().withBootstrapServers("none").withTopic(topic).withKeySerializer(IntegerSerializer.class).withValueSerializer(LongSerializer.class).withInputTimestamp().withProducerFactoryFn(new ProducerFactoryFn(producerWrapper.producerKey)));        p.run();        completionThread.shutdown();        verifyProducerRecords(producerWrapper.mockProducer, topic, numElements, false, true);    }}
public void beam_f32416_0() throws Exception
{        int numElements = 1000;    try (MockProducerWrapper producerWrapper = new MockProducerWrapper()) {        ProducerSendCompletionThread completionThread = new ProducerSendCompletionThread(producerWrapper.mockProducer).start();        String defaultTopic = "test";        p.apply(mkKafkaReadTransform(numElements, new ValueAsTimestampFn()).withoutMetadata()).apply(ParDo.of(new KV2ProducerRecord(defaultTopic, false))).setCoder(ProducerRecordCoder.of(VarIntCoder.of(), VarLongCoder.of())).apply(KafkaIO.<Integer, Long>writeRecords().withBootstrapServers("none").withKeySerializer(IntegerSerializer.class).withValueSerializer(LongSerializer.class).withInputTimestamp().withProducerFactoryFn(new ProducerFactoryFn(producerWrapper.producerKey)));        p.run();        completionThread.shutdown();                List<ProducerRecord<Integer, Long>> sent = producerWrapper.mockProducer.history();        for (int i = 0; i < numElements; i++) {            ProducerRecord<Integer, Long> record = sent.get(i);            if (i % 2 == 0) {                assertEquals("test_2", record.topic());            } else {                assertEquals("test_1", record.topic());            }            assertEquals(i, record.key().intValue());            assertEquals(i, record.value().longValue());            assertEquals(i, record.timestamp().intValue());        }    }}
public void beam_f32425_0()
{    try (MockProducerWrapper producerWrapper = new MockProducerWrapper()) {        KafkaIO.Write<Integer, Long> write = KafkaIO.<Integer, Long>write().withBootstrapServers("myServerA:9092,myServerB:9092").withTopic("myTopic").withValueSerializer(LongSerializer.class).withProducerFactoryFn(new ProducerFactoryFn(producerWrapper.producerKey)).withProducerConfigUpdates(ImmutableMap.of("retry.backoff.ms", 100));        DisplayData displayData = DisplayData.from(write);        assertThat(displayData, hasDisplayItem("topic", "myTopic"));        assertThat(displayData, hasDisplayItem("bootstrap.servers", "myServerA:9092,myServerB:9092"));        assertThat(displayData, hasDisplayItem("retries", 3));        assertThat(displayData, hasDisplayItem("retry.backoff.ms", 100));    }}
public Long beam_f32427_0(String topic, byte[] bytes)
{    return 0L;}
public Producer<Integer, Long> beam_f32439_0(Map<String, Object> config)
{        Utils.newInstance((Class<? extends Serializer<?>>) ((Class<?>) config.get(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG)).asSubclass(Serializer.class)).configure(config, true);    Utils.newInstance((Class<? extends Serializer<?>>) ((Class<?>) config.get(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG)).asSubclass(Serializer.class)).configure(config, false);    return MOCK_PRODUCER_MAP.get(producerKey);}
 ProducerSendCompletionThread beam_f32440_0()
{    injectorThread.submit(() -> {        int errorsInjected = 0;        while (!done.get()) {            boolean successful;            if (errorsInjected < maxErrors && ((numCompletions + 1) % errorFrequency) == 0) {                successful = mockProducer.errorNext(new InjectedErrorException("Injected Error #" + (errorsInjected + 1)));                if (successful) {                    errorsInjected++;                }            } else {                successful = mockProducer.completeNext();            }            if (successful) {                numCompletions++;            } else {                                try {                    Thread.sleep(1);                } catch (InterruptedException e) {                                }            }        }    });    return this;}
public void beam_f32449_0() throws IOException
{    ProducerRecord<String, String> decodedRecord = verifySerialization(1, System.currentTimeMillis());    assertEquals(1, decodedRecord.partition().intValue());}
public void beam_f32450_0() throws IOException
{    ProducerRecord<String, String> decodedRecord = verifySerialization(null, System.currentTimeMillis());    assertNull(decodedRecord.partition());}
public AmazonCloudWatch beam_f32459_0()
{    AmazonCloudWatchClientBuilder clientBuilder = AmazonCloudWatchClientBuilder.standard().withCredentials(getCredentialsProvider());    if (serviceEndpoint == null) {        clientBuilder.withRegion(region);    } else {        clientBuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(serviceEndpoint, region.getName()));    }    return clientBuilder.build();}
public IKinesisProducer beam_f32460_0(KinesisProducerConfiguration config)
{    config.setRegion(region.getName());    config.setCredentialsProvider(getCredentialsProvider());    return new KinesisProducer(config);}
public boolean beam_f32469_0(Object o)
{    return o instanceof Absent;}
public int beam_f32470_0()
{    return 0;}
public Read beam_f32479_0(InitialPositionInStream initialPosition)
{    return toBuilder().setInitialPosition(new StartingPoint(initialPosition)).build();}
public Read beam_f32480_0(Instant initialTimestamp)
{    return toBuilder().setInitialPosition(new StartingPoint(initialTimestamp)).build();}
public Read beam_f32489_0(Duration watermarkIdleDurationThreshold)
{    return toBuilder().setWatermarkPolicyFactory(WatermarkPolicyFactory.withArrivalTimePolicy(watermarkIdleDurationThreshold)).build();}
public Read beam_f32490_0()
{    return toBuilder().setWatermarkPolicyFactory(WatermarkPolicyFactory.withProcessingTimePolicy()).build();}
public Write beam_f32499_0(String awsAccessKey, String awsSecretKey, Regions region, String serviceEndpoint)
{    return withAWSClientsProvider(new BasicKinesisProvider(awsAccessKey, awsSecretKey, region, serviceEndpoint));}
 Write beam_f32500_0(int retries)
{    return builder().setRetries(retries).build();}
public boolean beam_f32509_1() throws IOException
{        try {        shardReadersPool = createShardReadersPool();        shardReadersPool.start();    } catch (TransientKinesisException e) {        throw new IOException(e);    }    return advance();}
public boolean beam_f32510_0() throws IOException
{    currentRecord = shardReadersPool.nextRecord();    return currentRecord.isPresent();}
 ShardReadersPool beam_f32519_0() throws TransientKinesisException
{    return new ShardReadersPool(kinesis, initialCheckpointGenerator.generate(kinesis), watermarkPolicyFactory);}
public List<KinesisReaderCheckpoint> beam_f32520_0(int desiredNumSplits)
{    int partitionSize = divideAndRoundUp(shardCheckpoints.size(), desiredNumSplits);    List<KinesisReaderCheckpoint> checkpoints = newArrayList();    for (List<ShardCheckpoint> shardPartition : partition(shardCheckpoints, partitionSize)) {        checkpoints.add(new KinesisReaderCheckpoint(shardPartition));    }    return checkpoints;}
public byte[] beam_f32530_0()
{    return getData().array();}
public boolean beam_f32531_0(Object obj)
{    return EqualsBuilder.reflectionEquals(this, obj);}
public KinesisRecord beam_f32540_0(InputStream inStream) throws IOException
{    ByteBuffer data = ByteBuffer.wrap(BYTE_ARRAY_CODER.decode(inStream));    String sequenceNumber = STRING_CODER.decode(inStream);    String partitionKey = STRING_CODER.decode(inStream);    Instant approximateArrivalTimestamp = INSTANT_CODER.decode(inStream);    long subSequenceNumber = VAR_LONG_CODER.decode(inStream);    Instant readTimestamp = INSTANT_CODER.decode(inStream);    String streamName = STRING_CODER.decode(inStream);    String shardId = STRING_CODER.decode(inStream);    return new KinesisRecord(data, sequenceNumber, subSequenceNumber, partitionKey, approximateArrivalTimestamp, readTimestamp, streamName, shardId);}
public List<KinesisSource> beam_f32541_0(int desiredNumSplits, PipelineOptions options) throws Exception
{    KinesisReaderCheckpoint checkpoint = initialCheckpointGenerator.generate(SimplifiedKinesisClient.from(awsClientsProvider, limit));    List<KinesisSource> sources = newArrayList();    for (KinesisReaderCheckpoint partition : checkpoint.splitInto(desiredNumSplits)) {        sources.add(new KinesisSource(awsClientsProvider, new StaticCheckpointGenerator(partition), streamName, upToDateThreshold, watermarkPolicyFactory, limit));    }    return sources;}
public String beam_f32550_0()
{    return String.format("Checkpoint %s for stream %s, shard %s: %s", shardIteratorType, streamName, shardId, sequenceNumber);}
public String beam_f32551_0(SimplifiedKinesisClient kinesisClient) throws TransientKinesisException
{    if (checkpointIsInTheMiddleOfAUserRecord()) {        return kinesisClient.getShardIterator(streamName, shardId, AT_SEQUENCE_NUMBER, sequenceNumber, null);    }    return kinesisClient.getShardIterator(streamName, shardId, shardIteratorType, sequenceNumber, timestamp);}
 void beam_f32560_1()
{        poolOpened.set(false);    executorService.shutdown();    awaitTermination();    if (!executorService.isTerminated()) {                executorService.shutdownNow();        awaitTermination();    }}
private void beam_f32561_1()
{    int attemptsLeft = ATTEMPTS_TO_SHUTDOWN;    boolean isTerminated = executorService.isTerminated();    while (!isTerminated && attemptsLeft-- > 0) {        try {            isTerminated = executorService.awaitTermination(10, TimeUnit.SECONDS);        } catch (InterruptedException e) {                        throw new RuntimeException(e);        }        if (!isTerminated && attemptsLeft > 0) {                    }    }}
 ShardCheckpoint beam_f32570_0()
{    return checkpoint.get();}
 void beam_f32571_0(KinesisRecord record)
{    checkpoint.set(checkpoint.get().moveAfter(record));    watermarkPolicy.get().update(record);}
public long beam_f32580_0(String streamName, Instant countSince) throws TransientKinesisException
{    return getBacklogBytes(streamName, countSince, new Instant());}
public long beam_f32581_0(final String streamName, final Instant countSince, final Instant countTo) throws TransientKinesisException
{    return wrapExceptions(() -> {        Minutes period = Minutes.minutesBetween(countSince, countTo);        if (period.isLessThan(Minutes.ONE)) {            return 0L;        }        GetMetricStatisticsRequest request = createMetricStatisticsRequest(streamName, countSince, countTo, period);        long totalSizeInBytes = 0;        GetMetricStatisticsResult result = cloudWatch.getMetricStatistics(request);        for (Datapoint point : result.getDatapoints()) {            totalSizeInBytes += point.getSum().longValue();        }        return totalSizeInBytes;    });}
 Set<Shard> beam_f32590_1(SimplifiedKinesisClient kinesis, String streamName, StartingPoint startingPoint) throws TransientKinesisException
{    List<Shard> allShards = kinesis.listShards(streamName);    Set<Shard> initialShards = findInitialShardsWithoutParents(streamName, allShards);    Set<Shard> startingPointShards = new HashSet<>();    Set<Shard> expiredShards;    do {        Set<Shard> validShards = validateShards(kinesis, initialShards, streamName, startingPoint);        startingPointShards.addAll(validShards);        expiredShards = Sets.difference(initialShards, validShards);        if (!expiredShards.isEmpty()) {                    }        initialShards = findNextShards(allShards, expiredShards);    } while (!expiredShards.isEmpty());    return startingPointShards;}
private Set<Shard> beam_f32591_0(List<Shard> allShards, Set<Shard> expiredShards)
{    Set<Shard> nextShards = new HashSet<>();    for (Shard expiredShard : expiredShards) {        boolean successorFound = false;        for (Shard shard : allShards) {            if (Objects.equals(expiredShard.getShardId(), shard.getParentShardId())) {                nextShards.add(shard);                successorFound = true;            } else if (Objects.equals(expiredShard.getShardId(), shard.getAdjacentParentShardId())) {                successorFound = true;            }        }        if (!successorFound) {                        throw new IllegalStateException("No successors were found for shard: " + expiredShard);        }    }    return nextShards;}
 static WatermarkPolicyFactory beam_f32600_0()
{    return ArrivalTimeWatermarkPolicy::new;}
 static WatermarkPolicyFactory beam_f32601_0(Duration watermarkIdleDurationThreshold)
{    return () -> new ArrivalTimeWatermarkPolicy(watermarkIdleDurationThreshold);}
public boolean beam_f32611_0(Object obj)
{    return EqualsBuilder.reflectionEquals(this, obj);}
public int beam_f32612_0()
{    return reflectionHashCode(this);}
public CreateStreamResult beam_f32623_0(CreateStreamRequest createStreamRequest)
{    throw new RuntimeException("Not implemented");}
public CreateStreamResult beam_f32624_0(String streamName, Integer shardCount)
{    throw new RuntimeException("Not implemented");}
public DescribeStreamConsumerResult beam_f32633_0(DescribeStreamConsumerRequest describeStreamConsumerRequest)
{    throw new RuntimeException("Not implemented");}
public DescribeStreamSummaryResult beam_f32634_0(DescribeStreamSummaryRequest describeStreamSummaryRequest)
{    throw new RuntimeException("Not implemented");}
public ListStreamsResult beam_f32643_0()
{    throw new RuntimeException("Not implemented");}
public ListStreamsResult beam_f32644_0(String exclusiveStartStreamName)
{    throw new RuntimeException("Not implemented");}
public RegisterStreamConsumerResult beam_f32653_0(RegisterStreamConsumerRequest registerStreamConsumerRequest)
{    throw new RuntimeException("Not implemented");}
public RemoveTagsFromStreamResult beam_f32654_0(RemoveTagsFromStreamRequest removeTagsFromStreamRequest)
{    throw new RuntimeException("Not implemented");}
public void beam_f32664_0()
{    new EqualsTester().addEqualityGroup(CustomOptional.absent(), CustomOptional.absent()).addEqualityGroup(CustomOptional.of(3), CustomOptional.of(3)).addEqualityGroup(CustomOptional.of(11)).addEqualityGroup(CustomOptional.of("3")).testEquals();}
public void beam_f32665_0() throws Exception
{    when(shard1.getShardId()).thenReturn("shard-01");    when(shard2.getShardId()).thenReturn("shard-02");    when(shard3.getShardId()).thenReturn("shard-03");    Set<Shard> shards = Sets.newHashSet(shard1, shard2, shard3);    StartingPoint startingPoint = new StartingPoint(InitialPositionInStream.LATEST);    when(startingPointShardsFinder.findShardsAtStartingPoint(kinesisClient, "stream", startingPoint)).thenReturn(shards);    DynamicCheckpointGenerator underTest = new DynamicCheckpointGenerator("stream", startingPoint, startingPointShardsFinder);    KinesisReaderCheckpoint checkpoint = underTest.generate(kinesisClient);    assertThat(checkpoint).hasSize(3);}
public String beam_f32674_0(byte[] value)
{    return null;}
public void beam_f32675_0()
{    int noOfShards = 3;    int noOfEventsPerShard = 100;    List<List<AmazonKinesisMock.TestData>> testData = provideTestData(noOfShards, noOfEventsPerShard);    PCollection<AmazonKinesisMock.TestData> result = p.apply(KinesisIO.read().withStreamName("stream").withInitialPositionInStream(InitialPositionInStream.TRIM_HORIZON).withAWSClientsProvider(new AmazonKinesisMock.Provider(testData, 10)).withArrivalTimeWatermarkPolicy().withMaxNumRecords(noOfShards * noOfEventsPerShard)).apply(ParDo.of(new KinesisRecordToTestData()));    PAssert.that(result).containsInAnyOrder(Iterables.concat(testData));    p.run();}
public void beam_f32684_0()
{    Properties properties = new Properties();    properties.setProperty("KinesisPort", "qwe");    KinesisIO.Write write = KinesisIO.write().withStreamName(STREAM).withPartitionKey(PARTITION_KEY).withAWSClientsProvider(new FakeKinesisProvider()).withProducerProperties(properties);    thrown.expect(IllegalArgumentException.class);    write.expand(null);}
public void beam_f32685_0()
{    KinesisServiceMock kinesisService = KinesisServiceMock.getInstance();    Properties properties = new Properties();    properties.setProperty("KinesisEndpoint", "localhost");    properties.setProperty("KinesisPort", "4567");    properties.setProperty("VerifyCertificate", "false");    Iterable<byte[]> data = ImmutableList.of("1".getBytes(StandardCharsets.UTF_8), "2".getBytes(StandardCharsets.UTF_8), "3".getBytes(StandardCharsets.UTF_8));    p.apply(Create.of(data)).apply(KinesisIO.write().withStreamName(STREAM).withPartitionKey(PARTITION_KEY).withAWSClientsProvider(new FakeKinesisProvider()).withProducerProperties(properties));    p.run().waitUntilFinish();    assertEquals(3, kinesisService.getAddedRecords().get());}
private AmazonKinesis beam_f32694_0()
{    int statusCode = isExistingStream ? 200 : 404;    SdkHttpMetadata httpMetadata = mock(SdkHttpMetadata.class);    when(httpMetadata.getHttpStatusCode()).thenReturn(statusCode);    DescribeStreamResult streamResult = mock(DescribeStreamResult.class);    when(streamResult.getSdkHttpMetadata()).thenReturn(httpMetadata);    AmazonKinesis client = mock(AmazonKinesis.class);    when(client.describeStream(any(String.class))).thenReturn(streamResult);    return client;}
public ListenableFuture<UserRecordResult> beam_f32695_0(String stream, String partitionKey, ByteBuffer data)
{    throw new UnsupportedOperationException("Not implemented");}
public synchronized void beam_f32705_0()
{    DateTime arrival = DateTime.now();    for (int i = 0; i < addedRecords.size(); i++) {        UserRecord record = addedRecords.get(i);        arrival = arrival.plusSeconds(1);        kinesisService.addShardedData(record.getData(), arrival);        addedRecords.remove(i);    }}
public synchronized void beam_f32706_0()
{    throw new UnsupportedOperationException("Not implemented");}
public void beam_f32715_0() throws IOException
{    reader.start();    reader.getCurrent();}
public void beam_f32716_0() throws IOException
{    when(shardReadersPool.nextRecord()).thenReturn(CustomOptional.of(a)).thenReturn(CustomOptional.absent());    assertThat(reader.start()).isTrue();}
public AtomicInteger beam_f32725_0()
{    return addedRecords;}
public String beam_f32726_0()
{    return existedStream;}
private KinesisRecord beam_f32735_0(ExtendedSequenceNumber extendedSequenceNumber)
{    KinesisRecord record = mock(KinesisRecord.class);    when(record.getExtendedSequenceNumber()).thenReturn(extendedSequenceNumber);    return record;}
private ShardCheckpoint beam_f32736_0(ShardIteratorType iteratorType, String sequenceNumber, Long subSequenceNumber)
{    return new ShardCheckpoint(STREAM_NAME, SHARD_ID, iteratorType, sequenceNumber, subSequenceNumber);}
public void beam_f32745_0() throws TransientKinesisException, KinesisShardClosedException
{    when(firstIterator.readNextBatch()).thenReturn(ImmutableList.of(a, b, c));    KinesisReaderCheckpoint checkpoint = new KinesisReaderCheckpoint(ImmutableList.of(firstCheckpoint, secondCheckpoint));    WatermarkPolicyFactory watermarkPolicyFactory = WatermarkPolicyFactory.withArrivalTimePolicy();    ShardReadersPool shardReadersPool = new ShardReadersPool(kinesis, checkpoint, watermarkPolicyFactory, 2);    shardReadersPool.start();    Stopwatch stopwatch = Stopwatch.createStarted();    shardReadersPool.stop();    assertThat(stopwatch.elapsed(TimeUnit.MILLISECONDS)).isLessThan(TIMEOUT_IN_MILLIS);}
public void beam_f32746_0() throws Exception
{    when(firstIterator.readNextBatch()).thenThrow(KinesisShardClosedException.class);    when(firstIterator.findSuccessiveShardRecordIterators()).thenReturn(Collections.emptyList());    shardReadersPool.start();    verify(firstIterator, timeout(TIMEOUT_IN_MILLIS).times(1)).readNextBatch();    verify(secondIterator, timeout(TIMEOUT_IN_MILLIS).atLeast(2)).readNextBatch();}
public void beam_f32755_0() throws IOException, TransientKinesisException
{    when(firstResult.getRecords()).thenReturn(asList(a, b, c));    when(secondResult.getRecords()).thenReturn(singletonList(d));    when(thirdResult.getRecords()).thenReturn(Collections.emptyList());    when(a.getApproximateArrivalTimestamp()).thenReturn(NOW);    when(b.getApproximateArrivalTimestamp()).thenReturn(NOW.plus(Duration.standardSeconds(1)));    when(c.getApproximateArrivalTimestamp()).thenReturn(NOW.plus(Duration.standardSeconds(2)));    when(d.getApproximateArrivalTimestamp()).thenReturn(NOW.plus(Duration.standardSeconds(3)));    iterator.ackRecord(a);    assertThat(iterator.getCheckpoint()).isEqualTo(aCheckpoint);    iterator.ackRecord(b);    assertThat(iterator.getCheckpoint()).isEqualTo(bCheckpoint);    iterator.ackRecord(c);    assertThat(iterator.getCheckpoint()).isEqualTo(cCheckpoint);    iterator.ackRecord(d);    assertThat(iterator.getCheckpoint()).isEqualTo(dCheckpoint);}
public void beam_f32756_0() throws IOException, TransientKinesisException, KinesisShardClosedException
{    when(firstResult.getRecords()).thenReturn(singletonList(a));    when(secondResult.getRecords()).thenReturn(singletonList(b));    when(a.getApproximateArrivalTimestamp()).thenReturn(NOW);    when(b.getApproximateArrivalTimestamp()).thenReturn(NOW.plus(Duration.standardSeconds(1)));    when(kinesisClient.getRecords(SECOND_ITERATOR, STREAM_NAME, SHARD_ID)).thenThrow(ExpiredIteratorException.class);    when(aCheckpoint.getShardIterator(kinesisClient)).thenReturn(SECOND_REFRESHED_ITERATOR);    when(kinesisClient.getRecords(SECOND_REFRESHED_ITERATOR, STREAM_NAME, SHARD_ID)).thenReturn(secondResult);    assertThat(iterator.readNextBatch()).isEqualTo(singletonList(a));    iterator.ackRecord(a);    assertThat(iterator.readNextBatch()).isEqualTo(singletonList(b));    assertThat(iterator.readNextBatch()).isEqualTo(Collections.emptyList());}
public void beam_f32765_0()
{    shouldHandleGetShardIteratorError(new NullPointerException(), RuntimeException.class);}
private void beam_f32766_0(Exception thrownException, Class<? extends Exception> expectedExceptionClass)
{    GetShardIteratorRequest request = new GetShardIteratorRequest().withStreamName(STREAM).withShardId(SHARD_1).withShardIteratorType(ShardIteratorType.LATEST);    when(kinesis.getShardIterator(request)).thenThrow(thrownException);    try {        underTest.getShardIterator(STREAM, SHARD_1, ShardIteratorType.LATEST, null, null);        failBecauseExceptionWasNotThrown(expectedExceptionClass);    } catch (Exception e) {        assertThat(e).isExactlyInstanceOf(expectedExceptionClass);    } finally {        reset(kinesis);    }}
public void beam_f32775_0() throws Exception
{    Instant countSince = new Instant("2017-04-06T10:00:00.000Z");    Instant countTo = new Instant("2017-04-06T11:00:00.000Z");    Minutes periodTime = Minutes.minutesBetween(countSince, countTo);    GetMetricStatisticsRequest metricStatisticsRequest = underTest.createMetricStatisticsRequest(STREAM, countSince, countTo, periodTime);    GetMetricStatisticsResult result = new GetMetricStatisticsResult().withDatapoints(new Datapoint().withSum(1.0));    when(cloudWatch.getMetricStatistics(metricStatisticsRequest)).thenReturn(result);    long backlogBytes = underTest.getBacklogBytes(STREAM, countSince, countTo);    assertThat(backlogBytes).isEqualTo(1L);}
public void beam_f32776_0() throws Exception
{    Instant countSince = new Instant("2017-04-06T10:00:00.000Z");    Instant countTo = new Instant("2017-04-06T11:00:00.000Z");    Minutes periodTime = Minutes.minutesBetween(countSince, countTo);    GetMetricStatisticsRequest metricStatisticsRequest = underTest.createMetricStatisticsRequest(STREAM, countSince, countTo, periodTime);    GetMetricStatisticsResult result = new GetMetricStatisticsResult().withDatapoints(new Datapoint().withSum(1.0), new Datapoint().withSum(3.0), new Datapoint().withSum(2.0));    when(cloudWatch.getMetricStatistics(metricStatisticsRequest)).thenReturn(result);    long backlogBytes = underTest.getBacklogBytes(STREAM, countSince, countTo);    assertThat(backlogBytes).isEqualTo(6L);}
public void beam_f32785_0() throws Exception
{    final Integer limit = 100;    doAnswer((Answer<GetRecordsResult>) invocation -> {        GetRecordsRequest request = (GetRecordsRequest) invocation.getArguments()[0];        List<Record> records = generateRecords(request.getLimit());        return new GetRecordsResult().withRecords(records).withMillisBehindLatest(1000L);    }).when(kinesis).getRecords(any(GetRecordsRequest.class));    GetKinesisRecordsResult result = underTest.getRecords(SHARD_ITERATOR, STREAM, SHARD_1, limit);    assertThat(result.getRecords().size()).isEqualTo(limit);}
private List<Record> beam_f32786_0(int num)
{    List<Record> records = new ArrayList<>();    for (int i = 0; i < num; i++) {        byte[] value = new byte[1024];        Arrays.fill(value, (byte) i);        records.add(new Record().withSequenceNumber(String.valueOf(i)).withPartitionKey("key").withData(ByteBuffer.wrap(value)));    }    return records;}
private void beam_f32795_0(Shard shard, Instant startTimestamp)
{    prepareShard(shard, null, ShardIteratorType.AT_TIMESTAMP, startTimestamp);}
private void beam_f32796_0(Shard shard, ShardIteratorType shardIteratorType)
{    prepareShard(shard, null, shardIteratorType, null);}
public static Read<T> beam_f32805_0()
{    return new AutoValue_KuduIO_Read.Builder<T>().setKuduService(new KuduServiceImpl<>()).build();}
public static Write<T> beam_f32806_0()
{    return new AutoValue_KuduIO_Write.Builder<T>().setKuduService(new KuduServiceImpl<>()).build();}
public Read<T> beam_f32815_0(Coder<T> coder)
{    checkArgument(coder != null, "coder cannot be null");    return builder().setCoder(coder).build();}
 Read<T> beam_f32816_0(KuduService<T> kuduService)
{    checkArgument(kuduService != null, "kuduService cannot be null");    return builder().setKuduService(kuduService).build();}
public Write beam_f32825_0(String table)
{    checkArgument(table != null, "table cannot be null");    return builder().setTable(table).build();}
public Write beam_f32826_0(FormatFunction<T> formatFn)
{    checkArgument(formatFn != null, "formatFn cannot be null");    return builder().setFormatFn(formatFn).build();}
public void beam_f32835_0() throws Exception
{    writer.close();    writer = null;}
public void beam_f32836_0(DisplayData.Builder builder)
{    super.populateDisplayData(builder);    builder.add(DisplayData.item("masterAddresses", spec.masterAddresses().toString()));    builder.add(DisplayData.item("table", spec.table()));}
public T beam_f32845_0() throws NoSuchElementException
{    if (current != null) {        return source.spec.getParseFn().apply(current);    } else {        throw new NoSuchElementException("No current record (Indicates misuse. Perhaps advance() was not called?)");    }}
public boolean beam_f32846_0() throws KuduException
{        if (iter == null || (!iter.hasNext() && scanner.hasMoreRows())) {        iter = scanner.nextRows();    }    if (iter != null && iter.hasNext()) {        current = iter.next();        ++recordsReturned;        return true;    }    return false;}
public static void beam_f32855_0() throws Exception
{    try {        if (client.tableExists(options.getKuduTable())) {            client.deleteTable(options.getKuduTable());        }    } finally {        client.close();    }}
public void beam_f32856_0() throws Exception
{    runWrite();    runReadAll();    readPipeline = TestPipeline.create();    runReadProjectedColumns();    readPipeline = TestPipeline.create();    runReadWithPredicates();}
public void beam_f32865_1(Long entity)
{        }
public void beam_f32866_1()
{    }
public Operation beam_f32876_0(TableAndRecord<Long> input)
{    Upsert upsert = input.getTable().newUpsert();    PartialRow row = upsert.getRow();    row.addLong(COL_ID, input.getRecord());    row.addString(COL_NAME, input.getRecord() + ": name");    return upsert;}
 static int beam_f32877_0(KuduTable table) throws KuduException
{    KuduScanner scanner = table.getAsyncClient().syncClient().newScannerBuilder(table).build();    try {        int rowCount = 0;        while (scanner.hasMoreRows()) {            rowCount += scanner.nextRows().getNumRows();        }        return rowCount;    } finally {        scanner.close();    }}
public FindQuery beam_f32886_0(Bson filters)
{    return withFilters(bson2BsonDocument(filters));}
public FindQuery beam_f32887_0(int limit)
{    return toBuilder().setLimit(limit).build();}
 GridFS beam_f32896_0(Mongo mongo)
{    DB db = database() == null ? mongo.getDB("gridfs") : mongo.getDB(database());    return bucket() == null ? new GridFS(db) : new GridFS(db, bucket());}
public Read<T> beam_f32897_0(String uri)
{    checkNotNull(uri);    ConnectionConfiguration config = ConnectionConfiguration.create(uri, connectionConfiguration().database(), connectionConfiguration().bucket());    return toBuilder().setConnectionConfiguration(config).build();}
public void beam_f32906_0()
{    mongo = source.spec.connectionConfiguration().setupMongo();    gridfs = source.spec.connectionConfiguration().setupGridFS(mongo);}
public void beam_f32907_0()
{    mongo.close();}
public void beam_f32916_0(DisplayData.Builder builder)
{    spec.populateDisplayData(builder);}
public Coder<ObjectId> beam_f32917_0()
{    return SerializableCoder.of(ObjectId.class);}
public Write<T> beam_f32926_0(String bucket)
{    checkNotNull(bucket);    ConnectionConfiguration config = ConnectionConfiguration.create(connectionConfiguration().uri(), connectionConfiguration().database(), bucket);    return toBuilder().setConnectionConfiguration(config).build();}
public Write<T> beam_f32927_0(String filename)
{    checkNotNull(filename);    return toBuilder().setFilename(filename).build();}
public void beam_f32936_0() throws Exception
{    try {        if (gridFsFile != null) {            outputStream.flush();            outputStream.close();            outputStream = null;            gridFsFile = null;        }    } finally {        if (mongo != null) {            mongo.close();            mongo = null;            gridfs = null;        }    }}
public static Read beam_f32937_0()
{    return new AutoValue_MongoDbIO_Read.Builder().setMaxConnectionIdleTime(60000).setNumSplits(0).setBucketAuto(false).setSslEnabled(false).setIgnoreSSLCertificate(false).setSslInvalidHostNameAllowed(false).setQueryFn(FindQuery.create()).build();}
public Read beam_f32946_0(String filter)
{    checkArgument(filter != null, "filter can not be null");    checkArgument(this.queryFn().getClass() != FindQuery.class, "withFilter is only supported for FindQuery API");    FindQuery findQuery = (FindQuery) queryFn();    FindQuery queryWithFilter = findQuery.toBuilder().setFilters(bson2BsonDocument(Document.parse(filter))).build();    return builder().setQueryFn(queryWithFilter).build();}
public Read beam_f32947_0(final String... fieldNames)
{    checkArgument(fieldNames.length > 0, "projection can not be null");    checkArgument(this.queryFn().getClass() != FindQuery.class, "withFilter is only supported for FindQuery API");    FindQuery findQuery = (FindQuery) queryFn();    FindQuery queryWithProjection = findQuery.toBuilder().setProjection(Arrays.asList(fieldNames)).build();    return builder().setQueryFn(queryWithProjection).build();}
public BoundedReader<Document> beam_f32956_0(PipelineOptions options)
{    return new BoundedMongoDbReader(this);}
public long beam_f32957_0(PipelineOptions pipelineOptions)
{    try (MongoClient mongoClient = new MongoClient(new MongoClientURI(spec.uri(), getOptions(spec.maxConnectionIdleTime(), spec.sslEnabled(), spec.sslInvalidHostNameAllowed())))) {        return getEstimatedSizeBytes(mongoClient, spec.database(), spec.collection());    }}
public Document beam_f32966_0()
{    return current;}
public void beam_f32967_1()
{    try {        if (cursor != null) {            cursor.close();        }    } catch (Exception e) {            }    try {        client.close();    } catch (Exception e) {            }}
public Write beam_f32976_0(String collection)
{    checkArgument(collection != null, "collection can not be null");    return builder().setCollection(collection).build();}
public Write beam_f32977_0(long batchSize)
{    checkArgument(batchSize >= 0, "Batch size must be >= 0, but was %s", batchSize);    return builder().setBatchSize(batchSize).build();}
public java.security.cert.X509Certificate[] beam_f32986_0()
{    return null;}
 static SSLContext beam_f32989_0()
{    try {                SSLContext sc = SSLContext.getInstance("TLS");        sc.init(null, trustAllCerts, new java.security.SecureRandom());        HttpsURLConnection.setDefaultSSLSocketFactory(sc.getSocketFactory());        KeyStore ks = KeyStore.getInstance("JKS");        ks.load(SSLUtils.class.getClassLoader().getResourceAsStream("resources/.keystore"), "changeit".toCharArray());        KeyManagerFactory kmf = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());        kmf.init(ks, "changeit".toCharArray());        SSLContext ctx = SSLContext.getInstance("TLS");        ctx.init(kmf.getKeyManagers(), trustAllCerts, null);        SSLContext.setDefault(ctx);        return ctx;    } catch (Exception e) {        throw new RuntimeException(e);    }}
public static void beam_f32998_0() throws Exception
{    new MongoClient(options.getMongoDBHostName()).getDatabase(options.getMongoDBDatabaseName()).drop();}
public void beam_f32999_0()
{    final String mongoUrl = String.format("mongodb://%s:%s", options.getMongoDBHostName(), options.getMongoDBPort());    writePipeline.apply("Generate sequence", GenerateSequence.from(0).to(options.getNumberOfRecords())).apply("Produce documents", MapElements.via(new LongToDocumentFn())).apply("Collect write time metric", ParDo.of(new TimeMonitor<>(NAMESPACE, "write_time"))).apply("Write documents to MongoDB", MongoDbIO.write().withUri(mongoUrl).withDatabase(options.getMongoDBDatabaseName()).withCollection(collection));    PipelineResult writeResult = writePipeline.run();    writeResult.waitUntilFinish();    PCollection<String> consolidatedHashcode = readPipeline.apply("Read all documents", MongoDbIO.read().withUri(mongoUrl).withDatabase(options.getMongoDBDatabaseName()).withCollection(collection)).apply("Collect read time metrics", ParDo.of(new TimeMonitor<>(NAMESPACE, "read_time"))).apply("Map documents to Strings", MapElements.via(new DocumentToStringFn())).apply("Calculate hashcode", Combine.globally(new HashingFn()));    String expectedHash = getHashForRecordCount(options.getNumberOfRecords(), EXPECTED_HASHES);    PAssert.thatSingleton(consolidatedHashcode).isEqualTo(expectedHash);    PipelineResult readResult = readPipeline.run();    readResult.waitUntilFinish();    collectAndPublishMetrics(writeResult, readResult);}
public void beam_f33008_0()
{        ArrayList<Document> documents = new ArrayList<>();    documents.add(new Document("_id", new ObjectId("52cc8f6254c5317943000005")));    List<BsonDocument> buckets = MongoDbIO.BoundedMongoDbSource.splitKeysToMatch(documents);    assertEquals(2, buckets.size());    assertEquals("{ \"$match\" : { \"_id\" : { \"$lte\" : { \"$oid\" : \"52cc8f6254c5317943000005\" } } } }", buckets.get(0).toString());    assertEquals("{ \"$match\" : { \"_id\" : { \"$gt\" : { \"$oid\" : \"52cc8f6254c5317943000005\" } } } }", buckets.get(1).toString());        documents.add(new Document("_id", new ObjectId("52cc8f6254c5317943000007")));    documents.add(new Document("_id", new ObjectId("54242e9e54c531ef8800001f")));    buckets = MongoDbIO.BoundedMongoDbSource.splitKeysToMatch(documents);    assertEquals(4, buckets.size());    assertEquals("{ \"$match\" : { \"_id\" : { \"$lte\" : { \"$oid\" : \"52cc8f6254c5317943000005\" } } } }", buckets.get(0).toString());    assertEquals("{ \"$match\" : { \"_id\" : { \"$gt\" : { \"$oid\" : \"52cc8f6254c5317943000005\" }, \"$lte\" : { \"$oid\" : \"52cc8f6254c5317943000007\" } } } }", buckets.get(1).toString());    assertEquals("{ \"$match\" : { \"_id\" : { \"$gt\" : { \"$oid\" : \"52cc8f6254c5317943000007\" }, \"$lte\" : { \"$oid\" : \"54242e9e54c531ef8800001f\" } } } }", buckets.get(2).toString());    assertEquals("{ \"$match\" : { \"_id\" : { \"$gt\" : { \"$oid\" : \"54242e9e54c531ef8800001f\" } } } }", buckets.get(3).toString());}
public void beam_f33009_0()
{    List<BsonDocument> aggregates = new ArrayList<BsonDocument>();    aggregates.add(new BsonDocument("$match", new BsonDocument("country", new BsonDocument("$eq", new BsonString("England")))));    MongoDbIO.Read spec = MongoDbIO.read().withUri("mongodb://localhost:" + port).withDatabase(DATABASE).withCollection(COLLECTION).withQueryFn(AggregationQuery.create().withMongoDbPipeline(aggregates));    MongoDatabase database = client.getDatabase(DATABASE);    List<Document> buckets = MongoDbIO.BoundedMongoDbSource.buildAutoBuckets(database, spec);    assertEquals(10, buckets.size());}
private static List<Document> beam_f33018_0(final int n)
{    final String[] scientists = new String[] { "Einstein", "Darwin", "Copernicus", "Pasteur", "Curie", "Faraday", "Newton", "Bohr", "Galilei", "Maxwell" };    final String[] country = new String[] { "Germany", "England", "Poland", "France", "France", "England", "England", "Denmark", "Florence", "Scotland" };    List<Document> documents = new ArrayList<>();    for (int i = 1; i <= n; i++) {        int index = i % scientists.length;        Document document = new Document();        document.append("scientist", scientists[index]);        document.append("country", country[index]);        documents.add(document);    }    return documents;}
private static int beam_f33019_0(final String collectionName)
{    return Iterators.size(getCollection(collectionName).find().iterator());}
public ConnectionConfiguration beam_f33028_0(String clientId)
{    checkArgument(clientId != null, "clientId can not be null");    return builder().setClientId(clientId).build();}
public ConnectionConfiguration beam_f33029_0(String username)
{    checkArgument(username != null, "username can not be null");    return builder().setUsername(username).build();}
public void beam_f33038_0(Message message, Instant timestamp)
{    if (timestamp.isBefore(oldestMessageTimestamp)) {        oldestMessageTimestamp = timestamp;    }    messages.add(message);}
public void beam_f33039_1()
{        for (Message message : messages) {        try {            message.ack();        } catch (Exception e) {                    }    }    oldestMessageTimestamp = Instant.now();    messages.clear();}
public boolean beam_f33048_1() throws IOException
{        Read spec = source.spec;    try {        client = spec.connectionConfiguration().createClient();                checkpointMark.clientId = client.getClientId().toString();        connection = client.blockingConnection();        connection.connect();        connection.subscribe(new Topic[] { new Topic(spec.connectionConfiguration().getTopic(), QoS.AT_LEAST_ONCE) });        return advance();    } catch (Exception e) {        throw new IOException(e);    }}
public boolean beam_f33049_0() throws IOException
{    try {        LOG.trace("MQTT reader (client ID {}) waiting message ...", client.getClientId());        Message message = connection.receive(1, TimeUnit.SECONDS);        if (message == null) {            return false;        }        current = message.getPayload();        currentTimestamp = Instant.now();        checkpointMark.add(message, currentTimestamp);    } catch (Exception e) {        throw new IOException(e);    }    return true;}
public PDone beam_f33058_0(PCollection<byte[]> input)
{    input.apply(ParDo.of(new WriteFn(this)));    return PDone.in(input.getPipeline());}
public void beam_f33059_0(DisplayData.Builder builder)
{    connectionConfiguration().populateDisplayData(builder);    builder.add(DisplayData.item("retained", retained()));}
public void beam_f33068_0() throws Exception
{    ByteArrayOutputStream bos = new ByteArrayOutputStream();    ObjectOutputStream out = new ObjectOutputStream(bos);    MqttIO.MqttCheckpointMark cp1 = new MqttIO.MqttCheckpointMark(UUID.randomUUID().toString());    out.writeObject(cp1);    ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray());    ObjectInputStream in = new ObjectInputStream(bis);    MqttIO.MqttCheckpointMark cp2 = (MqttIO.MqttCheckpointMark) in.readObject();        assertEquals(0, in.available());        assertEquals(0, cp2.messages.size());    assertEquals(cp1.clientId, cp2.clientId);    assertEquals(cp1.oldestMessageTimestamp, cp2.oldestMessageTimestamp);}
public void beam_f33069_0() throws Exception
{    if (brokerService != null) {        brokerService.stop();        brokerService.waitUntilStopped();        brokerService = null;    }}
public long beam_f33078_0() throws IOException
{    return seekableByteChannel.size();}
public SeekableInputStream beam_f33079_0()
{    return new DelegatingSeekableInputStream(Channels.newInputStream(seekableByteChannel)) {        @Override        public long getPos() throws IOException {            return seekableByteChannel.position();        }        @Override        public void seek(long newPos) throws IOException {            seekableByteChannel.position(newPos);        }    };}
public PositionOutputStream beam_f33088_0(long blockSizeHint)
{    return new BeamOutputStream(outputStream);}
public boolean beam_f33089_0()
{    return false;}
public void beam_f33098_0()
{    List<GenericRecord> records = generateGenericRecords(1000);    PCollection<GenericRecord> writeThenRead = mainPipeline.apply(Create.of(records).withCoder(AvroCoder.of(SCHEMA))).apply(FileIO.<GenericRecord>write().via(ParquetIO.sink(SCHEMA)).to(temporaryFolder.getRoot().getAbsolutePath())).getPerDestinationOutputFilenames().apply(Values.create()).apply(FileIO.matchAll()).apply(FileIO.readMatches()).apply(ParquetIO.readFiles(SCHEMA));    PAssert.that(writeThenRead).containsInAnyOrder(records);    mainPipeline.run().waitUntilFinish();}
private List<GenericRecord> beam_f33099_0(long count)
{    ArrayList<GenericRecord> data = new ArrayList<>();    GenericRecordBuilder builder = new GenericRecordBuilder(SCHEMA);    for (int i = 0; i < count; i++) {        int index = i % SCIENTISTS.length;        GenericRecord record = builder.set("name", SCIENTISTS[index]).build();        data.add(record);    }    return data;}
public Read beam_f33108_0(boolean queueDeclare)
{    return builder().setQueueDeclare(queueDeclare).build();}
public Read beam_f33109_0(String name, String type, String routingKey)
{    checkArgument(name != null, "name can not be null");    checkArgument(type != null, "type can not be null");    return builder().setExchange(name).setExchangeType(type).setRoutingKey(routingKey).build();}
public void beam_f33118_0() throws IOException
{    for (Long sessionId : sessionIds) {        channel.basicAck(sessionId, false);    }    channel.txCommit();    oldestTimestamp = Instant.now();    sessionIds.clear();}
public Instant beam_f33119_0()
{    return checkpointMark.oldestTimestamp;}
public Write beam_f33128_0(String uri)
{    checkArgument(uri != null, "uri can not be null");    return builder().setUri(uri).build();}
public Write beam_f33129_0(String exchange, String exchangeType)
{    checkArgument(exchange != null, "exchange can not be null");    checkArgument(exchangeType != null, "exchangeType can not be null");    return builder().setExchange(exchange).setExchangeType(exchangeType).build();}
public byte[] beam_f33138_0()
{    return body;}
public String beam_f33139_0()
{    return contentType;}
public Date beam_f33148_0()
{    return timestamp;}
public String beam_f33149_0()
{    return type;}
public void beam_f33158_0() throws Exception
{    final int maxNumRecords = 10;    PCollection<RabbitMqMessage> raw = p.apply(RabbitMqIO.read().withUri("amqp://guest:guest@localhost:" + port).withQueue("READ").withMaxNumRecords(maxNumRecords));    PCollection<String> output = raw.apply(MapElements.into(TypeDescriptors.strings()).via((RabbitMqMessage message) -> new String(message.getBody(), StandardCharsets.UTF_8)));    List<String> records = generateRecords(maxNumRecords).stream().map(record -> new String(record, StandardCharsets.UTF_8)).collect(Collectors.toList());    PAssert.that(output).containsInAnyOrder(records);    ConnectionFactory connectionFactory = new ConnectionFactory();    connectionFactory.setUri("amqp://guest:guest@localhost:" + port);    Connection connection = null;    Channel channel = null;    try {        connection = connectionFactory.newConnection();        channel = connection.createChannel();        channel.queueDeclare("READ", false, false, false, null);        for (String record : records) {            channel.basicPublish("", "READ", null, record.getBytes(StandardCharsets.UTF_8));        }        p.run();    } finally {        if (channel != null) {            channel.close();        }        if (connection != null) {            connection.close();        }    }}
public void beam_f33159_1() throws Exception
{    final int maxNumRecords = 10;    PCollection<RabbitMqMessage> raw = p.apply(RabbitMqIO.read().withUri("amqp://guest:guest@localhost:" + port).withExchange("READEXCHANGE", "fanout", "test").withMaxNumRecords(maxNumRecords));    PCollection<String> output = raw.apply(MapElements.into(TypeDescriptors.strings()).via((RabbitMqMessage message) -> new String(message.getBody(), StandardCharsets.UTF_8)));    List<String> records = generateRecords(maxNumRecords).stream().map(record -> new String(record, StandardCharsets.UTF_8)).collect(Collectors.toList());    PAssert.that(output).containsInAnyOrder(records);    ConnectionFactory connectionFactory = new ConnectionFactory();    connectionFactory.setUri("amqp://guest:guest@localhost:" + port);    Connection connection = null;    Channel channel = null;    try {        connection = connectionFactory.newConnection();        channel = connection.createChannel();        channel.exchangeDeclare("READEXCHANGE", "fanout");        Channel finalChannel = channel;        Thread publisher = new Thread(() -> {            try {                Thread.sleep(5000);            } catch (Exception e) {                            }            for (int i = 0; i < maxNumRecords; i++) {                try {                    finalChannel.basicPublish("READEXCHANGE", "test", null, ("Test " + i).getBytes(StandardCharsets.UTF_8));                } catch (Exception e) {                                    }            }        });        publisher.start();        p.run();        publisher.join();    } finally {        if (channel != null) {            channel.close();        }        if (connection != null) {            connection.close();        }    }}
public RedisConnectionConfiguration beam_f33168_0(String auth)
{    checkArgument(auth != null, "auth can not be null");    return builder().setAuth(auth).build();}
public RedisConnectionConfiguration beam_f33169_0(int timeout)
{    checkArgument(timeout >= 0, "timeout can not be negative");    return builder().setTimeout(timeout).build();}
public Read beam_f33178_0(int timeout)
{    checkArgument(timeout >= 0, "timeout can not be negative");    return builder().setConnectionConfiguration(connectionConfiguration().withTimeout(timeout)).build();}
public Read beam_f33179_0(String keyPattern)
{    checkArgument(keyPattern != null, "keyPattern can not be null");    return builder().setKeyPattern(keyPattern).build();}
public ReadAll beam_f33188_0(int batchSize)
{    return builder().setBatchSize(batchSize).build();}
public PCollection<KV<String, String>> beam_f33189_0(PCollection<String> input)
{    checkArgument(connectionConfiguration() != null, "withConnectionConfiguration() is required");    return input.apply(ParDo.of(new ReadFn(connectionConfiguration(), batchSize()))).setCoder(KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of())).apply(new Reparallelize());}
public PCollection<KV<String, String>> beam_f33198_0(PCollection<KV<String, String>> input)
{            PCollectionView<Iterable<KV<String, String>>> empty = input.apply("Consume", Filter.by(SerializableFunctions.constant(false))).apply(View.asIterable());    PCollection<KV<String, String>> materialized = input.apply("Identity", ParDo.of(new DoFn<KV<String, String>, KV<String, String>>() {        @ProcessElement        public void processElement(ProcessContext context) {            context.output(context.element());        }    }).withSideInputs(empty));    return materialized.apply(Reshuffle.viaRandomKey());}
public void beam_f33199_0(ProcessContext context)
{    context.output(context.element());}
public void beam_f33208_0()
{    pipeline = jedis.pipelined();    pipeline.multi();    batchCount = 0;}
public void beam_f33209_0(ProcessContext processContext)
{    KV<String, String> record = processContext.element();    writeRecord(record);    batchCount++;    if (batchCount >= DEFAULT_BATCH_SIZE) {        pipeline.exec();        pipeline.sync();        pipeline.multi();        batchCount = 0;    }}
private void beam_f33218_0(String key, Long expireTime)
{    if (expireTime != null) {        pipeline.pexpire(key, expireTime);    }}
public void beam_f33219_0()
{    if (pipeline.isInMulti()) {        pipeline.exec();        pipeline.sync();    }    batchCount = 0;}
public void beam_f33228_0()
{    String key = "testWriteWithMethodSAdd";    List<String> values = Arrays.asList("0", "1", "2", "3", "2", "4", "0", "5");    List<KV<String, String>> data = buildConstantKeyList(key, values);    PCollection<KV<String, String>> write = p.apply(Create.of(data));    write.apply(RedisIO.write().withEndpoint(REDIS_HOST, port).withMethod(Method.SADD));    p.run();    Set<String> expected = new HashSet<>(values);    Set<String> members = client.smembers(key);    assertEquals(expected, members);}
public void beam_f33229_0()
{    String key = "testWriteWithMethodPFAdd";    List<String> values = Arrays.asList("0", "1", "2", "3", "2", "4", "0", "5");    List<KV<String, String>> data = buildConstantKeyList(key, values);    PCollection<KV<String, String>> write = p.apply(Create.of(data));    write.apply(RedisIO.write().withEndpoint(REDIS_HOST, port).withMethod(Method.PFADD));    p.run();    long count = client.pfcount(key);    assertEquals(6, count);}
 static ClusterState beam_f33238_0(AuthorizedSolrClient<CloudSolrClient> authorizedSolrClient)
{    authorizedSolrClient.solrClient.connect();    return authorizedSolrClient.solrClient.getZkStateReader().getClusterState();}
public void beam_f33239_0() throws IOException
{    solrClient.close();}
public ConnectionConfiguration beam_f33248_0(String username, String password)
{    checkArgument(username != null, "username can not be null");    checkArgument(!username.isEmpty(), "username can not be empty");    checkArgument(password != null, "password can not be null");    checkArgument(!password.isEmpty(), "password can not be empty");    return builder().setUsername(username).setPassword(password).build();}
private void beam_f33249_0(DisplayData.Builder builder)
{    builder.add(DisplayData.item("zkHost", getZkHost()));    builder.addIfNotNull(DisplayData.item("username", getUsername()));}
public Read beam_f33258_0(String query)
{    checkArgument(query != null, "query can not be null");    checkArgument(!query.isEmpty(), "query can not be empty");    return builder().setQuery(query).build();}
 Read beam_f33259_0(int batchSize)
{            checkArgument(batchSize > 0 && batchSize < MAX_BATCH_SIZE, "Valid values for batchSize are 1 (inclusize) to %s (exclusive), but was: %s ", MAX_BATCH_SIZE, batchSize);    return builder().setBatchSize(batchSize).build();}
public Write beam_f33268_0(RetryConfiguration retryConfiguration)
{    checkArgument(retryConfiguration != null, "retryConfiguration is required");    return builder().setRetryConfiguration(retryConfiguration).build();}
public PDone beam_f33269_0(PCollection<SolrInputDocument> input)
{    checkState(getConnectionConfiguration() != null, "withConnectionConfiguration() is required");    checkState(getCollection() != null, "to() is required");    input.apply(ParDo.of(new WriteFn(this)));    return PDone.in(input.getPipeline());}
public void beam_f33278_0()
{    assertThat(TEST_CODER.getEncodedTypeDescriptor(), equalTo(TypeDescriptor.of(SolrDocument.class)));}
public static void beam_f33279_0() throws Exception
{            String password = "SolrRocks";        byte[] salt = new byte[random().nextInt(30) + 1];    random().nextBytes(salt);    String base64Salt = BaseEncoding.base64().encode(salt);    String sha56 = Sha256AuthenticationProvider.sha256(password, base64Salt);    String credential = sha56 + " " + base64Salt;    String securityJson = "{" + "'authentication':{" + "  'blockUnknown': true," + "  'class':'solr.BasicAuthPlugin'," + "  'credentials':{'solr':'" + credential + "'}}" + "}";    configureCluster(3).addConfig("conf", getFile("cloud-minimal/conf").toPath()).configure();    ZkStateReader zkStateReader = cluster.getSolrClient().getZkStateReader();    zkStateReader.getZkClient().setData("/security.json", securityJson.getBytes(Charset.defaultCharset()), true);    String zkAddress = cluster.getZkServer().getZkAddress();    connectionConfiguration = SolrIO.ConnectionConfiguration.create(zkAddress).withBasicCredentials("solr", password);    solrClient = connectionConfiguration.createClient();    SolrIOTestUtils.createCollection(SOLR_COLLECTION, NUM_SHARDS, 1, solrClient);}
public void beam_f33288_0()
{    assertTrue(DEFAULT_RETRY_PREDICATE.test(new IOException("test")));    assertTrue(DEFAULT_RETRY_PREDICATE.test(new SolrServerException("test")));    assertTrue(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.CONFLICT, "test")));    assertTrue(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.SERVER_ERROR, "test")));    assertTrue(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.SERVICE_UNAVAILABLE, "test")));    assertTrue(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.INVALID_STATE, "test")));    assertTrue(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.UNKNOWN, "test")));    assertTrue(DEFAULT_RETRY_PREDICATE.test(new HttpSolrClient.RemoteSolrException("localhost", SolrException.ErrorCode.SERVICE_UNAVAILABLE.code, "test", new Exception())));    assertFalse(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.BAD_REQUEST, "test")));    assertFalse(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.FORBIDDEN, "test")));    assertFalse(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.NOT_FOUND, "test")));    assertFalse(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.UNAUTHORIZED, "test")));    assertFalse(DEFAULT_RETRY_PREDICATE.test(new SolrException(SolrException.ErrorCode.UNSUPPORTED_MEDIA_TYPE, "test")));}
public void beam_f33289_0()
{    SolrIO.Write write1 = SolrIO.write().withConnectionConfiguration(connectionConfiguration).withMaxBatchSize(BATCH_SIZE);    assertTrue(write1.getMaxBatchSize() == BATCH_SIZE);    SolrIO.Write write2 = SolrIO.write().withConnectionConfiguration(connectionConfiguration);    assertTrue(write2.getMaxBatchSize() == DEFAULT_BATCH_SIZE);}
private double[] beam_f33298_0(int desiredNumBundles)
{    double[] relativeSizes = new double[desiredNumBundles];    for (int i = 0; i < relativeSizes.length; ++i) {        relativeSizes[i] = options.bundleSizeDistribution.sample(options.hashFunction().hashInt(i).asLong());    }    return relativeSizes;}
public void beam_f33299_0(Record record)
{            long hashCodeOfVal = options.hashFunction().hashBytes(record.kv.getValue()).asLong();    Random random = new Random(hashCodeOfVal);    SyntheticDelay.delay(record.sleepMsec, options.cpuUtilizationInMixedDelay, options.delayType, random);}
public long beam_f33308_0(PipelineOptions options)
{    return getEndOffset();}
public SyntheticSourceReader beam_f33309_0(PipelineOptions pipelineOptions)
{    return new SyntheticSourceReader(this);}
public Double beam_f33318_0()
{    double realFractionConsumed = super.getFractionConsumed();    SyntheticSourceOptions.ProgressShape shape = getCurrentSource().sourceOptions.progressShape;    switch(shape) {        case LINEAR:            return realFractionConsumed;        case LINEAR_REGRESSING:            return 0.9 - 0.8 * realFractionConsumed;        default:            throw new AssertionError("Unexpected progress shape: " + shape);    }}
protected boolean beam_f33319_0() throws NoSuchElementException
{    return isAtSplitPoint;}
public Object beam_f33329_0()
{    return sampler.getDistribution();}
public void beam_f33330_0(int seed)
{    this.seed = seed;}
public SyntheticRecordsCheckpoint beam_f33339_0(InputStream inStream) throws IOException
{    long startPosition = LONG_CODER.decode(inStream);    long endPosition = LONG_CODER.decode(inStream);    return new SyntheticRecordsCheckpoint(startPosition, endPosition);}
public long beam_f33341_0()
{    return startPosition;}
private void beam_f33350_0(long milliseconds)
{    if (options.reportThrottlingMicros && milliseconds > 0) {        throttlingCounter.inc(TimeUnit.MILLISECONDS.toMicros(milliseconds));    }}
private boolean beam_f33351_0()
{    return options.maxWorkerThroughput < 0 || rateLimiterCache.getUnchecked(idAndThroughput).tryAcquire();}
public KV<byte[], byte[]> beam_f33360_0() throws NoSuchElementException
{    if (currentKVPair == null) {        throw new NoSuchElementException("Current record is unavailable because either the reader is " + "at the beginning of the input and start() or advance() wasn't called, " + "or the last start() or advance() returned false.");    }    return currentKVPair;}
public Instant beam_f33361_0() throws NoSuchElementException
{    if (eventTime == null) {        throw new NoSuchElementException("Current timestamp is unavailable because either the reader is " + "at the beginning of the input and start() or advance() wasn't called, " + "or the last start() or advance() returned false.");    }    return eventTime;}
public void beam_f33371_0()
{    splitter = new BundleSplitter(options);    int expectedBundleCount = 4;    List bundleSizes = splitter.getBundleSizes(expectedBundleCount, 0, options.numRecords);    assertEquals(expectedBundleCount, bundleSizes.size());}
public void beam_f33372_0()
{    long expectedBundleSize = 2;    options.bundleSizeDistribution = fromRealDistribution(new ConstantRealDistribution(2));    splitter = new BundleSplitter(options);    List<OffsetRange> bundleSizes = splitter.getBundleSizes(4, 0, options.numRecords);    bundleSizes.stream().map(range -> range.getTo() - range.getFrom()).forEach(size -> assertEquals(expectedBundleSize, size.intValue()));}
private void beam_f33381_0(long splitPointFrequency) throws Exception
{    PipelineOptions options = PipelineOptionsFactory.create();    testSourceOptions.splitPointFrequencyRecords = splitPointFrequency;    SyntheticBoundedSource source = new SyntheticBoundedSource(testSourceOptions);    assertEquals(10 * (10 + 20), source.getEstimatedSizeBytes(options));    SourceTestUtils.assertUnstartedReaderReadsSameAsItsSource(source.createReader(options), options);}
public void beam_f33382_0() throws Exception
{    testSplitAtFractionP(1);    testSplitAtFractionP(3);}
public void beam_f33391_0() throws Exception
{    thrown.expect(IllegalArgumentException.class);    thrown.expectMessage("hotKeyFraction should be a non-negative number, but found -0.25");    String options = "{\"hotKeyFraction\":-0.25}";    optionsFromString(options, SyntheticOptions.class);}
public void beam_f33392_0() throws Exception
{    thrown.expect(JsonMappingException.class);    thrown.expectMessage("lower bound of uniform distribution should be a non-negative number, but found -100.0");    String syntheticOptions = "{\"delayDistribution\":{\"type\":\"uniform\",\"lower\":-100,\"upper\":200}}";    optionsFromString(syntheticOptions, SyntheticOptions.class);}
public void beam_f33401_0() throws Exception
{    SyntheticStep.Options options = SyntheticTestUtils.optionsFromString("{\"outputRecordsPerInputRecord\": 2," + " \"preservesInputKeyDistribution\": false," + "\"keySizeBytes\": 10," + "\"valueSizeBytes\": 20," + "\"numHotKeys\": 3," + "\"hotKeyFraction\": 0.3," + "\"seed\": 123456}", SyntheticStep.Options.class);    options.delayDistribution = SyntheticOptions.fromRealDistribution(new ConstantRealDistribution(10));    PCollection<KV<byte[], byte[]>> result = p.apply(Create.of(ImmutableList.of(KV.of(intToByteArray(1), intToByteArray(11))))).apply(ParDo.of(new SyntheticStep(options)));    PAssert.that(result).satisfies((Iterable<KV<byte[], byte[]>> input) -> {        int count = 0;        for (KV<byte[], byte[]> elm : input) {            count += 1;            assertEquals(10, elm.getKey().length);            assertEquals(20, elm.getValue().length);        }        assertEquals(2, count);        return null;    });    p.run().waitUntilFinish();}
 static T beam_f33402_0(String json, Class<T> type) throws IOException
{    ObjectMapper mapper = new ObjectMapper();    T result = mapper.readValue(json, type);    result.validate();    return result;}
public static ParseResult beam_f33411_0(String fileLocation, String content)
{    return new ParseResult(fileLocation, content, new Metadata(), null);}
public static ParseResult beam_f33412_0(String fileLocation, String partialContent, Metadata partialMetadata, Throwable error)
{    return new ParseResult(fileLocation, partialContent, partialMetadata, error);}
private int beam_f33421_0()
{    int hashCode = 0;    for (String name : metadataNames) {        hashCode += name.hashCode() ^ Arrays.hashCode(metadata.getValues(name));    }    return hashCode;}
public String beam_f33422_0()
{    return MoreObjects.toStringHelper(this).add("fileLocation", fileLocation).add("content", "<" + content.length() + " chars>").add("metadata", metadata).add("error", getError() == null ? null : Throwables.getStackTraceAsString(getError())).toString();}
public ParseFiles beam_f33431_0(String contentTypeHint)
{    checkNotNull(contentTypeHint, "contentTypeHint can not be null.");    return toBuilder().setContentTypeHint(contentTypeHint).build();}
public ParseFiles beam_f33432_0(Metadata metadata)
{    Metadata inputMetadata = this.getInputMetadata();    if (inputMetadata != null) {        for (String name : metadata.names()) {            inputMetadata.set(name, metadata.get(name));        }    } else {        inputMetadata = metadata;    }    return toBuilder().setInputMetadata(inputMetadata).build();}
public void beam_f33441_0(ProcessContext c)
{    ParseResult result = c.element();    Metadata m = new Metadata();        if (result.getFileLocation().endsWith("valid/apache-beam-tika.odt")) {        m.set("Author", result.getMetadata().get("Author"));    }    ParseResult newResult = ParseResult.success(result.getFileLocation(), result.getContent(), m);    c.output(newResult);}
public void beam_f33442_0() throws IOException
{    String path = getClass().getResource("/damaged.pdf").getPath();    PCollection<ParseResult> res = p.apply("ParseInvalidPdfFile", TikaIO.parse().filepattern(path));    PAssert.thatSingleton(res).satisfies(input -> {        assertEquals(path, input.getFileLocation());        assertFalse(input.isSuccess());        assertTrue(input.getError() instanceof TikaException);        return null;    });    p.run();}
public T beam_f33451_0(InputStream inStream) throws IOException
{    return decode(inStream, Context.NESTED);}
public T beam_f33452_0(InputStream inStream, Context context) throws IOException
{    try {        if (!context.isWholeStream) {            long limit = VarInt.decodeLong(inStream);            inStream = ByteStreams.limit(inStream, limit);        }        @SuppressWarnings("unchecked")        T obj = (T) jaxbUnmarshaller.get().unmarshal(new CloseIgnoringInputStream(inStream));        return obj;    } catch (JAXBException e) {        throw new CoderException(e);    }}
private MappingConfiguration<T> beam_f33463_0(String recordElement)
{    return toBuilder().setRecordElement(recordElement).build();}
private MappingConfiguration<T> beam_f33464_0(Class<T> recordClass)
{    return toBuilder().setRecordClass(recordClass).build();}
public Read<T> beam_f33473_0(String rootElement)
{    return withConfiguration(getConfiguration().withRootElement(rootElement));}
public Read<T> beam_f33474_0(String recordElement)
{    return withConfiguration(getConfiguration().withRecordElement(recordElement));}
public PCollection<T> beam_f33483_0(PBegin input)
{    getConfiguration().validate();    return input.apply(org.apache.beam.sdk.io.Read.from(createSource()));}
private ReadFiles<T> beam_f33484_0(MappingConfiguration<T> configuration)
{    return toBuilder().setConfiguration(configuration).build();}
public Write<T> beam_f33493_0(Class<T> recordClass)
{    return toBuilder().setRecordClass(recordClass).build();}
public Write<T> beam_f33494_0(String rootElement)
{    return toBuilder().setRootElement(rootElement).build();}
public void beam_f33503_0() throws IOException
{    outputStream.write(("\n</" + getRootElement() + ">").getBytes(Charset.forName(getCharset())));    outputStream.flush();}
protected FileBasedSource<T> beam_f33504_0(Metadata metadata, long start, long end)
{    return new XmlSource<>(configuration, getMinBundleSize(), metadata, start, end);}
protected boolean beam_f33513_0()
{        return true;}
protected long beam_f33514_0()
{    return currentByteOffset;}
public void beam_f33523_0() throws Exception
{    JAXBCoder<TestType> jaxbCoder = JAXBCoder.of(TestType.class);    TestCoder nesting = new TestCoder(jaxbCoder);    byte[] encoded = CoderUtils.encodeToByteArray(nesting, new TestType("abc", 9999));    assertEquals(new TestType("abc", 9999), CoderUtils.decodeFromByteArray(nesting, encoded));}
public void beam_f33524_0() throws Throwable
{    final JAXBCoder<TestType> coder = JAXBCoder.of(TestType.class);    int numThreads = 100;    final CountDownLatch ready = new CountDownLatch(numThreads);    final CountDownLatch start = new CountDownLatch(1);    final CountDownLatch done = new CountDownLatch(numThreads);    final AtomicReference<Throwable> thrown = new AtomicReference<>();    Executor executor = Executors.newCachedThreadPool();    for (int i = 0; i < numThreads; i++) {        final TestType elem = new TestType("abc", i);        final int index = i;        executor.execute(() -> {            ready.countDown();            try {                start.await();            } catch (InterruptedException ignored) {            }            try {                byte[] encoded = CoderUtils.encodeToByteArray(coder, elem);                assertEquals(new TestType("abc", index), CoderUtils.decodeFromByteArray(coder, encoded));            } catch (Throwable e) {                thrown.compareAndSet(null, e);            }            done.countDown();        });    }    ready.await();    start.countDown();    done.await();    Throwable actuallyThrown = thrown.get();    if (actuallyThrown != null) {        throw actuallyThrown;    }}
public void beam_f33533_0()
{    testWriteThenRead(Method.SINK_AND_READ_FILES, BIRDS, StandardCharsets.UTF_8);}
public void beam_f33534_0()
{    testWriteThenRead(Method.SINK_AND_READ_FILES, BIRDS, StandardCharsets.ISO_8859_1);}
public String beam_f33543_0()
{    return adjective;}
public void beam_f33544_0(String adjective)
{    this.adjective = adjective;}
private String beam_f33553_0(Train train)
{    return "<train size=\"" + train.size + "\"><name>" + train.name + "</name><number>" + train.number + "</number><color>" + train.color + "</color></train>";}
private File beam_f33554_0(String fileName, List<Train> trains) throws IOException
{    File file = tempFolder.newFile(fileName);    try (BufferedWriter writer = Files.newBufferedWriter(file.toPath(), StandardCharsets.UTF_8)) {        writer.write("<trains>");        writer.newLine();        for (Train train : trains) {            String str = trainToXMLElement(train);            writer.write(str);            writer.newLine();        }        writer.write("</trains>");        writer.newLine();    }    return file;}
public void beam_f33563_0() throws IOException
{    File file = tempFolder.newFile("trainXMLSmall");    Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8));    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("something").withRecordElement("train").withRecordClass(Train.class).createSource();    exception.expectMessage("Unexpected close tag </trains>; expected </something>.");    readEverythingFromReader(source.createReader(null));}
public void beam_f33564_0() throws IOException
{    File file = tempFolder.newFile("trainXMLSmall");    Files.write(file.toPath(), trainXML.getBytes(StandardCharsets.UTF_8));    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("something").withRecordClass(Train.class).createSource();    assertEquals(readEverythingFromReader(source.createReader(null)), new ArrayList<Train>());}
public void beam_f33573_0() throws IOException
{    String fileName = "temp.xml";    List<Train> trains = generateRandomTrainList(100);    File file = createRandomTrainXML(fileName, trains);    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(Train.class).withMinBundleSize(1024).createSource();    assertThat(trainsToStrings(trains), containsInAnyOrder(trainsToStrings(readEverythingFromReader(source.createReader(null))).toArray()));}
public void beam_f33574_0() throws Exception
{    String fileName = "temp.xml";    List<Train> trains = generateRandomTrainList(10);    File file = createRandomTrainXML(fileName, trains);    BoundedSource<Train> source = XmlIO.<Train>read().from(file.toPath().toString()).withRootElement("trains").withRecordElement("train").withRecordClass(Train.class).withMinBundleSize(10).createSource();    List<? extends BoundedSource<Train>> splits = source.split(100, null);    assertTrue(splits.size() > 2);    List<Train> results = new ArrayList<>();    for (BoundedSource<Train> split : splits) {        results.addAll(readEverythingFromReader(split.createReader(null)));    }    assertThat(trainsToStrings(trains), containsInAnyOrder(trainsToStrings(results).toArray()));}
public PCollection<GenericRecord> beam_f33583_0(PBegin input)
{    return input.apply(FileIO.match().filepattern(spec.getPayload().toStringUtf8())).apply(FileIO.readMatches()).apply(ParquetIO.readFiles(schema)).setCoder(AvroCoder.of(schema));}
public PCollection<String> beam_f33584_0(PCollection<GenericRecord> input)
{    return input.apply(FileIO.<GenericRecord>write().via(ParquetIO.sink(schema)).to(spec.getPayload().toStringUtf8())).getPerDestinationOutputFilenames().apply(Values.create());}
 void beam_f33593_0() throws IOException
{    Optional<SyntheticStep> syntheticStep = createStep(options.getStepOptions());    PCollection<KV<byte[], byte[]>> input = pipeline.apply("Read input", readFromSource(sourceOptions)).apply("Collect start time metrics", ParDo.of(runtimeMonitor)).apply("Total bytes monitor", ParDo.of(new ByteMonitor(METRICS_NAMESPACE, "totalBytes.count")));    input = applyWindowing(input);    for (int branch = 0; branch < options.getFanout(); branch++) {        applyStepIfPresent(input, format("Synthetic step (%s)", branch), syntheticStep).apply(format("Group by key (%s)", branch), GroupByKey.create()).apply(format("Ungroup and reiterate (%s)", branch), ParDo.of(new UngroupAndReiterate(options.getIterations()))).apply(format("Collect end time metrics (%s)", branch), ParDo.of(runtimeMonitor));    }}
public void beam_f33594_0(ProcessContext c)
{    byte[] key = c.element().getKey();        for (int i = 0; i < iterations; i++) {        for (byte[] value : c.element().getValue()) {            if (i == iterations - 1) {                c.output(KV.of(key, value));            }        }    }}
private void beam_f33603_0(List<NamedTestResult> testResults)
{    String dataset = options.getBigQueryDataset();    String table = options.getBigQueryTable();    checkBigQueryOptions(dataset, table);    BigQueryResultsPublisher.create(dataset, NamedTestResult.getSchema()).publish(testResults, table);}
private static void beam_f33604_0(String dataset, String table)
{    Preconditions.checkArgument(dataset != null, "Please specify --bigQueryDataset option if you want to publish to BigQuery");    Preconditions.checkArgument(table != null, "Please specify --bigQueryTable option if you want to publish to BigQuery");}
public Map<String, Object> beam_f33613_0()
{    return ImmutableMap.<String, Object>builder().put("timestamp", timestamp).put("runtime", runtime).put("total_bytes_count", totalBytesCount).build();}
protected void beam_f33614_0()
{    PCollection<KV<byte[], byte[]>> input = pipeline.apply("Read input", readFromSource(sourceOptions)).apply(ParDo.of(runtimeMonitor)).apply(ParDo.of(new ByteMonitor(METRICS_NAMESPACE, "totalBytes.count")));    for (int i = 0; i < options.getIterations(); i++) {        input = input.apply(String.format("Step: %d", i), ParDo.of(new CounterOperation<>(options.getNumberOfCounters(), options.getNumberOfCounterOperations())));    }    input.apply(ParDo.of(runtimeMonitor));}
public byte[] beam_f33623_0(KV<byte[], byte[]> input)
{    return encodeInputElement(input);}
public PubsubMessage beam_f33624_0(KV<byte[], byte[]> input)
{    return new PubsubMessage(encodeInputElement(input), encodeInputElementToMapOfStrings(input));}
public static void beam_f33633_0(String[] args) throws IOException
{    new Main().runAll(args);}
public void beam_f33634_0(Auction value, OutputStream outStream) throws CoderException, IOException
{    LONG_CODER.encode(value.id, outStream);    STRING_CODER.encode(value.itemName, outStream);    STRING_CODER.encode(value.description, outStream);    LONG_CODER.encode(value.initialBid, outStream);    LONG_CODER.encode(value.reserve, outStream);    INSTANT_CODER.encode(value.dateTime, outStream);    INSTANT_CODER.encode(value.expires, outStream);    LONG_CODER.encode(value.seller, outStream);    LONG_CODER.encode(value.category, outStream);    STRING_CODER.encode(value.extra, outStream);}
public int beam_f33643_0()
{    return Objects.hashCode(id, itemName, description, initialBid, reserve, dateTime, expires, seller, category, extra);}
public void beam_f33644_0(AuctionBid value, OutputStream outStream) throws CoderException, IOException
{    Auction.CODER.encode(value.auction, outStream);    Bid.CODER.encode(value.bid, outStream);}
public Object beam_f33653_0(AuctionCount v)
{    return v;}
public boolean beam_f33654_0(Object otherObject)
{    if (this == otherObject) {        return true;    }    if (otherObject == null || getClass() != otherObject.getClass()) {        return false;    }    AuctionCount other = (AuctionCount) otherObject;    return Objects.equals(auction, other.auction) && Objects.equals(num, other.num);}
public long beam_f33663_0()
{    return 8L + 8L;}
public String beam_f33664_0()
{    try {        return NexmarkUtils.MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
public long beam_f33674_0()
{    return 8L + 8L + 8L + 8L + extra.length() + 1L;}
public String beam_f33675_0()
{    try {        return NexmarkUtils.MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
public CategoryPrice beam_f33685_0(InputStream inStream) throws CoderException, IOException
{    long category = LONG_CODER.decode(inStream);    long price = LONG_CODER.decode(inStream);    boolean isLast = INT_CODER.decode(inStream) != 0;    return new CategoryPrice(category, price, isLast);}
public Object beam_f33687_0(CategoryPrice v)
{    return v;}
public String beam_f33697_0()
{    try {        return NexmarkUtils.MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
public boolean beam_f33698_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    Done done = (Done) o;    return Objects.equal(message, done.message);}
public long beam_f33708_0()
{    if (newPerson != null) {        return 1 + newPerson.sizeInBytes();    } else if (newAuction != null) {        return 1 + newAuction.sizeInBytes();    } else if (bid != null) {        return 1 + bid.sizeInBytes();    } else {        throw new RuntimeException("invalid event");    }}
public String beam_f33709_0()
{    if (newPerson != null) {        return newPerson.toString();    } else if (newAuction != null) {        return newAuction.toString();    } else if (bid != null) {        return bid.toString();    } else {        throw new RuntimeException("invalid event");    }}
public NameCityStateId beam_f33719_0(InputStream inStream) throws CoderException, IOException
{    String name = STRING_CODER.decode(inStream);    String city = STRING_CODER.decode(inStream);    String state = STRING_CODER.decode(inStream);    long id = LONG_CODER.decode(inStream);    return new NameCityStateId(name, city, state, id);}
public Object beam_f33721_0(NameCityStateId v)
{    return v;}
public boolean beam_f33731_0(String annotation)
{    return extra.startsWith(annotation + ": ");}
public Person beam_f33732_0(String annotation)
{    if (hasAnnotation(annotation)) {        return new Person(id, name, emailAddress, creditCard, city, state, dateTime, extra.substring(annotation.length() + 2));    } else {        return this;    }}
public String beam_f33742_0()
{    try {        return NexmarkUtils.MAPPER.writeValueAsString(this);    } catch (JsonProcessingException e) {        throw new RuntimeException(e);    }}
public boolean beam_f33743_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    SellerPrice that = (SellerPrice) o;    return seller == that.seller && price == that.price;}
 int beam_f33752_0(Schema schema)
{    switch(eventType) {        case PERSON:            return schema.indexOf("newPerson");        case AUCTION:            return schema.indexOf("newAuction");        case BID:            return schema.indexOf("bid");        default:            throw new RuntimeException("Unexpected event type.");    }}
public PCollection<Row> beam_f33753_0(PCollection<Event> input)
{    if (!input.hasSchema()) {        throw new RuntimeException("Input PCollection must have a schema!");    }    int index = getNestedIndex(input.getSchema());    return input.apply(ParDo.of(new DoFn<Event, Row>() {        @ProcessElement        public void processElement(@Element Row row, OutputReceiver<Row> o) {            o.output(row.getRow(index));        }    })).setRowSchema(input.getSchema().getField(index).getType().getRowSchema());}
public int beam_f33762_0()
{    return Objects.hash(debug, query, sourceType, sinkType, exportSummaryToBigQuery, pubSubMode, numEvents, numEventGenerators, rateShape, firstEventRate, nextEventRate, rateUnit, ratePeriodSec, preloadSeconds, streamTimeout, isRateLimited, useWallclockEventTime, avgPersonByteSize, avgAuctionByteSize, avgBidByteSize, hotAuctionRatio, hotSellersRatio, hotBiddersRatio, windowSizeSec, windowPeriodSec, watermarkHoldbackSec, numInFlightAuctions, numActivePeople, coderStrategy, cpuDelayMs, diskBusyBytes, auctionSkip, fanout, maxAuctionsWaitingTime, occasionalDelaySec, probDelayedEvent, maxLogEvents, usePubsubPublishTime, outOfOrderGroupSize);}
public boolean beam_f33763_0(Object obj)
{    if (this == obj) {        return true;    }    if (obj == null) {        return false;    }    if (getClass() != obj.getClass()) {        return false;    }    NexmarkConfiguration other = (NexmarkConfiguration) obj;    if (debug != other.debug) {        return false;    }    if (auctionSkip != other.auctionSkip) {        return false;    }    if (avgAuctionByteSize != other.avgAuctionByteSize) {        return false;    }    if (avgBidByteSize != other.avgBidByteSize) {        return false;    }    if (avgPersonByteSize != other.avgPersonByteSize) {        return false;    }    if (coderStrategy != other.coderStrategy) {        return false;    }    if (cpuDelayMs != other.cpuDelayMs) {        return false;    }    if (diskBusyBytes != other.diskBusyBytes) {        return false;    }    if (fanout != other.fanout) {        return false;    }    if (maxAuctionsWaitingTime != other.maxAuctionsWaitingTime) {        return false;    }    if (firstEventRate != other.firstEventRate) {        return false;    }    if (hotAuctionRatio != other.hotAuctionRatio) {        return false;    }    if (hotBiddersRatio != other.hotBiddersRatio) {        return false;    }    if (hotSellersRatio != other.hotSellersRatio) {        return false;    }    if (isRateLimited != other.isRateLimited) {        return false;    }    if (maxLogEvents != other.maxLogEvents) {        return false;    }    if (nextEventRate != other.nextEventRate) {        return false;    }    if (rateUnit != other.rateUnit) {        return false;    }    if (numEventGenerators != other.numEventGenerators) {        return false;    }    if (numEvents != other.numEvents) {        return false;    }    if (numInFlightAuctions != other.numInFlightAuctions) {        return false;    }    if (numActivePeople != other.numActivePeople) {        return false;    }    if (occasionalDelaySec != other.occasionalDelaySec) {        return false;    }    if (preloadSeconds != other.preloadSeconds) {        return false;    }    if (streamTimeout != other.streamTimeout) {        return false;    }    if (Double.doubleToLongBits(probDelayedEvent) != Double.doubleToLongBits(other.probDelayedEvent)) {        return false;    }    if (pubSubMode != other.pubSubMode) {        return false;    }    if (ratePeriodSec != other.ratePeriodSec) {        return false;    }    if (rateShape != other.rateShape) {        return false;    }    if (query != other.query) {        return false;    }    if (sinkType != other.sinkType) {        return false;    }    if (exportSummaryToBigQuery != other.exportSummaryToBigQuery) {        return false;    }    if (sourceType != other.sourceType) {        return false;    }    if (useWallclockEventTime != other.useWallclockEventTime) {        return false;    }    if (watermarkHoldbackSec != other.watermarkHoldbackSec) {        return false;    }    if (windowPeriodSec != other.windowPeriodSec) {        return false;    }    if (windowSizeSec != other.windowSizeSec) {        return false;    }    if (usePubsubPublishTime != other.usePubsubPublishTime) {        return false;    }    if (outOfOrderGroupSize != other.outOfOrderGroupSize) {        return false;    }    return true;}
private String beam_f33772_0(long now)
{    String baseFilename = options.getOutputPath();    if (Strings.isNullOrEmpty(baseFilename)) {        throw new RuntimeException("Missing --outputPath");    }    switch(options.getResourceNameMode()) {        case VERBATIM:            return baseFilename;        case QUERY:            return String.format("%s/nexmark_%s.txt", baseFilename, queryName);        case QUERY_AND_SALT:            return String.format("%s/nexmark_%s_%d.txt", baseFilename, queryName, now);        case QUERY_RUNNER_AND_MODE:            return String.format("%s/nexmark_%s_%s_%s", baseFilename, queryName, options.getRunner().getSimpleName(), options.isStreaming());    }    throw new RuntimeException("Unrecognized enum " + options.getResourceNameMode());}
private String beam_f33773_0(long now)
{    String baseFilename = options.getOutputPath();    if (Strings.isNullOrEmpty(baseFilename)) {        throw new RuntimeException("Missing --outputPath");    }    switch(options.getResourceNameMode()) {        case VERBATIM:            return baseFilename;        case QUERY:            return String.format("%s/logs_%s", baseFilename, queryName);        case QUERY_AND_SALT:            return String.format("%s/logs_%s_%d", baseFilename, queryName, now);        case QUERY_RUNNER_AND_MODE:            return String.format("%s/logs_%s_%s_%s", baseFilename, queryName, options.getRunner().getSimpleName(), options.isStreaming());    }    throw new RuntimeException("Unrecognized enum " + options.getResourceNameMode());}
private void beam_f33782_0(PCollection<String> formattedResults)
{    checkArgument((options.getBootstrapServers() != null), "Missing --bootstrapServers");    NexmarkUtils.console("Writing results to Kafka Topic %s", options.getKafkaResultsTopic());    formattedResults.apply(queryName + ".WriteKafkaResults", KafkaIO.<Void, String>write().withBootstrapServers(options.getBootstrapServers()).withTopic(options.getKafkaResultsTopic()).withValueSerializer(StringSerializer.class).values());}
private void beam_f33783_0(PCollection<String> formattedResults, long now)
{    String shortTopic = shortTopic(now);    NexmarkUtils.console("Writing results to Pubsub %s", shortTopic);    PubsubIO.Write<String> io = PubsubIO.writeStrings().to(shortTopic).withIdAttribute(NexmarkUtils.PUBSUB_ID);    if (!configuration.usePubsubPublishTime) {        io = io.withTimestampAttribute(NexmarkUtils.PUBSUB_TIMESTAMP);    }    formattedResults.apply(queryName + ".WritePubsubResults", io);}
private void beam_f33792_0(NexmarkQueryModel model)
{    List<Long> counts = Lists.newArrayList(model.simulator().resultsPerWindow());    Collections.sort(counts);    int n = counts.size();    if (n < 5) {        NexmarkUtils.console("Query%s: only %d samples", model.configuration.query, n);    } else {        NexmarkUtils.console("Query%d: N:%d; min:%d; 1st%%:%d; mean:%d; 3rd%%:%d; max:%d", model.configuration.query, n, counts.get(0), counts.get(n / 4), counts.get(n / 2), counts.get(n - 1 - n / 4), counts.get(n - 1));    }}
public NexmarkPerf beam_f33793_0() throws IOException
{    if (options.getManageResources() && !options.getMonitorJobs()) {        throw new RuntimeException("If using --manageResources then must also use --monitorJobs.");    }                checkState(queryName == null);    if (configuration.sourceType.equals(SourceType.PUBSUB)) {        pubsubHelper = PubsubHelper.create(options);    }    try {        NexmarkUtils.console("Running %s", configuration.toShortString());        if (configuration.numEvents < 0) {            NexmarkUtils.console("skipping since configuration is disabled");            return null;        }        NexmarkQuery<? extends KnownSize> query = getNexmarkQuery();        if (query == null) {            NexmarkUtils.console("skipping since configuration is not implemented");            return null;        }        queryName = query.getName();                if (!"".equals(options.getTempLocation())) {            options.setTempLocation(options.getTempLocation() + "/" + queryName);        }        NexmarkQueryModel model = getNexmarkQueryModel();        if (options.getJustModelResultRate()) {            if (model == null) {                throw new RuntimeException(String.format("No model for %s", queryName));            }            modelResultRates(model);            return null;        }        final Instant now = Instant.now();        Pipeline p = Pipeline.create(options);        NexmarkUtils.setupPipeline(configuration.coderStrategy, p);                PCollection<Event> source = createSource(p, now);        if (query.getTransform().needsSideInput()) {            query.getTransform().setSideInput(NexmarkUtils.prepareSideInput(p, configuration));        }        if (options.getLogEvents()) {            source = source.apply(queryName + ".Events.Log", NexmarkUtils.log(queryName + ".Events"));        }                if (source != null) {                        if (configuration.sinkType == NexmarkUtils.SinkType.AVRO) {                sinkEventsToAvro(source);            }                        if (configuration.query == NexmarkQueryName.LOG_TO_SHARDED_FILES) {                String path = null;                if (options.getOutputPath() != null && !options.getOutputPath().isEmpty()) {                    path = logsDir(now.getMillis());                }                ((Query10) query.getTransform()).setOutputPath(path);                ((Query10) query.getTransform()).setMaxNumWorkers(maxNumWorkers());            }                        PCollection<TimestampedValue<KnownSize>> results = (PCollection<TimestampedValue<KnownSize>>) source.apply(query);            if (options.getAssertCorrectness()) {                if (model == null) {                    throw new RuntimeException(String.format("No model for %s", queryName));                }                                results.setIsBoundedInternal(PCollection.IsBounded.BOUNDED);                                                PAssert.that(results).satisfies(model.assertionFor());            }                        sink(results, now.getMillis());        }        mainResult = p.run();        mainResult.waitUntilFinish(Duration.standardSeconds(configuration.streamTimeout));        return monitor(query);    } finally {        if (pubsubHelper != null) {            pubsubHelper.cleanup();            pubsubHelper = null;        }        configuration = null;        queryName = null;    }}
private Map<NexmarkQueryName, NexmarkQuery> beam_f33802_0()
{    return ImmutableMap.<NexmarkQueryName, NexmarkQuery>builder().put(NexmarkQueryName.PASSTHROUGH, new NexmarkQuery(configuration, new Query0())).put(NexmarkQueryName.CURRENCY_CONVERSION, new NexmarkQuery(configuration, new Query1(configuration))).put(NexmarkQueryName.SELECTION, new NexmarkQuery(configuration, new Query2(configuration))).put(NexmarkQueryName.LOCAL_ITEM_SUGGESTION, new NexmarkQuery(configuration, new Query3(configuration))).put(NexmarkQueryName.AVERAGE_PRICE_FOR_CATEGORY, new NexmarkQuery(configuration, new Query4(configuration))).put(NexmarkQueryName.HOT_ITEMS, new NexmarkQuery(configuration, new Query5(configuration))).put(NexmarkQueryName.AVERAGE_SELLING_PRICE_BY_SELLER, new NexmarkQuery(configuration, new Query6(configuration))).put(NexmarkQueryName.HIGHEST_BID, new NexmarkQuery(configuration, new Query7(configuration))).put(NexmarkQueryName.MONITOR_NEW_USERS, new NexmarkQuery(configuration, new Query8(configuration))).put(NexmarkQueryName.WINNING_BIDS, new NexmarkQuery(configuration, new Query9(configuration))).put(NexmarkQueryName.LOG_TO_SHARDED_FILES, new NexmarkQuery(configuration, new Query10(configuration))).put(NexmarkQueryName.USER_SESSIONS, new NexmarkQuery(configuration, new Query11(configuration))).put(NexmarkQueryName.PROCESSING_TIME_WINDOWS, new NexmarkQuery(configuration, new Query12(configuration))).put(NexmarkQueryName.BOUNDED_SIDE_INPUT_JOIN, new NexmarkQuery(configuration, new BoundedSideInputJoin(configuration))).put(NexmarkQueryName.SESSION_SIDE_INPUT_JOIN, new NexmarkQuery(configuration, new SessionSideInputJoin(configuration))).build();}
public void beam_f33803_0(ProcessContext c) throws IOException
{    byte[] payload = c.element().getPayload();    Event event = CoderUtils.decodeFromByteArray(Event.CODER, payload);    c.output(event);}
private static List<NexmarkConfiguration> beam_f33812_0()
{    List<NexmarkConfiguration> configurations = new ArrayList<>();    NexmarkConfiguration configuration = new NexmarkConfiguration();    configurations.add(configuration);    return configurations;}
private static List<NexmarkConfiguration> beam_f33813_0()
{    List<NexmarkConfiguration> configurations = new ArrayList<>();    for (NexmarkQueryName query : NexmarkQueryName.values()) {        NexmarkConfiguration configuration = NexmarkConfiguration.DEFAULT.copy();        configuration.query = query;        configuration.numEvents = 100_000;        if (query == NexmarkQueryName.AVERAGE_PRICE_FOR_CATEGORY || query == NexmarkQueryName.AVERAGE_SELLING_PRICE_BY_SELLER || query == NexmarkQueryName.WINNING_BIDS) {                        configuration.numEvents /= 10;        }        configurations.add(configuration);    }    return configurations;}
private static String beam_f33822_0(boolean isStreaming)
{    return isStreaming ? "streaming" : "batch";}
public long beam_f33823_0(long rate)
{    return (usPerUnit + rate / 2) / rate;}
public static PTransform<PBegin, PCollection<Event>> beam_f33832_0(NexmarkConfiguration configuration)
{    return Read.from(new BoundedEventSource(standardGeneratorConfig(configuration), configuration.numEventGenerators));}
public static PTransform<PBegin, PCollection<Event>> beam_f33833_0(NexmarkConfiguration configuration)
{    return Read.from(new UnboundedEventSource(NexmarkUtils.standardGeneratorConfig(configuration), configuration.numEventGenerators, configuration.watermarkHoldbackSec, configuration.isRateLimited));}
public static ParDo.SingleOutput<T, TimestampedValue<T>> beam_f33842_0(String name)
{    return ParDo.of(new DoFn<T, TimestampedValue<T>>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(TimestampedValue.of(c.element(), c.timestamp()));        }    });}
public void beam_f33843_0(ProcessContext c)
{    c.output(TimestampedValue.of(c.element(), c.timestamp()));}
public void beam_f33852_0(ProcessContext c, @StateId(DISK_BUSY) ValueState<byte[]> state)
{    long remain = bytes;    long now = System.currentTimeMillis();    while (remain > 0) {        long thisBytes = Math.min(remain, MAX_BUFFER_SIZE);        remain -= thisBytes;        byte[] arr = new byte[(int) thisBytes];        for (int i = 0; i < thisBytes; i++) {            arr[i] = (byte) now;        }        state.write(arr);        now = System.currentTimeMillis();    }    c.output(c.element().getValue());}
public static PTransform<PCollection<T>, PCollection<T>> beam_f33853_0(final long bytes)
{    return new DiskBusyTransform<>(bytes);}
private static Coder<KnownSize> beam_f33862_0(Coder<T> trueCoder)
{    return new CastingCoder<>(trueCoder);}
public static PCollection<KnownSize> beam_f33863_0(final String name, PCollection<T> elements)
{    return elements.apply(name + ".Forget", castToKnownSize()).setCoder(makeCastingCoder(elements.getCoder()));}
public void beam_f33872_0()
{    for (SubscriptionPath subscription : createdSubscriptions) {        try {            NexmarkUtils.console("delete subscription %s", subscription);            pubsubClient.deleteSubscription(subscription);        } catch (IOException ex) {            NexmarkUtils.console("could not delete subscription %s", subscription);        }    }    for (TopicPath topic : createdTopics) {        try {            NexmarkUtils.console("delete topic %s", topic);            pubsubClient.deleteTopic(topic);        } catch (IOException ex) {            NexmarkUtils.console("could not delete topic %s", topic);        }    }}
 TimestampedValue<InputT> beam_f33873_0()
{    if (!input.hasNext()) {        return null;    }    TimestampedValue<InputT> timestampedInput = input.next();    NexmarkUtils.info("input: %s", timestampedInput);    return timestampedInput;}
public Iterator<Long> beam_f33882_0()
{    return new Iterator<Long>() {        @Override        public boolean hasNext() {            while (true) {                if (!pendingCounts.isEmpty()) {                    return true;                }                if (isDone) {                    if (currentCount > 0) {                        pendingCounts.add(currentCount);                        currentCount = 0;                        currentWindow = BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis();                        return true;                    } else {                        return false;                    }                }                run();            }        }        @Override        public Long next() {            Long result = pendingCounts.get(0);            pendingCounts.remove(0);            return result;        }        @Override        public void remove() {            throw new UnsupportedOperationException();        }    };}
public boolean beam_f33883_0()
{    while (true) {        if (!pendingCounts.isEmpty()) {            return true;        }        if (isDone) {            if (currentCount > 0) {                pendingCounts.add(currentCount);                currentCount = 0;                currentWindow = BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis();                return true;            } else {                return false;            }        }        run();    }}
protected Collection<String> beam_f33892_0(Iterator<TimestampedValue<Bid>> itr)
{    return toValue(itr);}
public NexmarkQueryTransform<T> beam_f33893_0()
{    return transform;}
public void beam_f33902_0(PCollection<KV<Long, String>> sideInput)
{    this.sideInput = sideInput;}
public PCollection<KV<Long, String>> beam_f33903_0()
{    return sideInput;}
public void beam_f33912_0(ProcessContext c)
{    c.output(c.element().price);}
public void beam_f33913_0(ProcessContext c)
{    Event e = c.element();    if (e.bid != null) {        c.outputWithTimestamp(e, new Instant(e.bid.dateTime));    } else if (e.newPerson != null) {        c.outputWithTimestamp(e, new Instant(e.newPerson.dateTime));    } else if (e.newAuction != null) {        c.outputWithTimestamp(e, new Instant(e.newAuction.dateTime));    }}
public PCollection<Bid> beam_f33922_0(PCollection<Event> events)
{    return events.apply(NexmarkQueryUtil.JUST_BIDS).apply(name + ".ToEuros", ParDo.of(new DoFn<Bid, Bid>() {        @ProcessElement        public void processElement(ProcessContext c) {            Bid bid = c.element();            c.output(new Bid(bid.auction, bid.bidder, (bid.price * 89) / 100, bid.dateTime, bid.extra));        }    }));}
public void beam_f33923_0(ProcessContext c)
{    Bid bid = c.element();    c.output(new Bid(bid.auction, bid.bidder, (bid.price * 89) / 100, bid.dateTime, bid.extra));}
public void beam_f33932_1(ProcessContext c)
{    if (c.element().hasAnnotation("LATE")) {        lateCounter.inc();            } else {        onTimeCounter.inc();    }    int shardNum = (int) Math.abs((long) c.element().hashCode() % numLogShards);    String shard = String.format("shard-%05d-of-%05d", shardNum, numLogShards);    c.output(KV.of(shard, c.element()));}
public void beam_f33933_1(ProcessContext c, BoundedWindow window)
{    int numLate = 0;    int numOnTime = 0;    for (Event event : c.element().getValue()) {        if (event.hasAnnotation("LATE")) {            numLate++;        } else {            numOnTime++;        }    }    String shard = c.element().getKey();        if (c.pane().getTiming() == PaneInfo.Timing.LATE) {        if (numLate == 0) {                        unexpectedLatePaneCounter.inc();        }        if (numOnTime > 0) {                        unexpectedOnTimeElementCounter.inc();        }        lateCounter.inc();    } else if (c.pane().getTiming() == PaneInfo.Timing.EARLY) {        if (numOnTime + numLate < configuration.maxLogEvents) {                    }        earlyCounter.inc();    } else {        onTimeCounter.inc();    }    c.output(c.element());}
protected void beam_f33942_0()
{    TimestampedValue<Event> timestampedEvent = nextInput();    if (timestampedEvent == null) {        allDone();        return;    }    Event event = timestampedEvent.getValue();    if (event.bid == null) {                return;    }    Bid bid = event.bid;    Bid resultBid = new Bid(bid.auction, bid.bidder, bid.price * 89 / 100, bid.dateTime, bid.extra);    TimestampedValue<Bid> result = TimestampedValue.of(resultBid, timestampedEvent.getTimestamp());    addResult(result);}
public AbstractSimulator<?, Bid> beam_f33943_0()
{    return new Simulator(configuration);}
public void beam_f33952_1(ProcessContext c, @TimerId(PERSON_STATE_EXPIRING) Timer timer, @StateId(PERSON) ValueState<Person> personState, @StateId(AUCTIONS) ValueState<List<Auction>> auctionsState)
{                    Person existingPerson = personState.read();    if (existingPerson != null) {                for (Auction newAuction : c.element().getValue().getAll(NexmarkQueryUtil.AUCTION_TAG)) {            newAuctionCounter.inc();            newOldOutputCounter.inc();            c.output(KV.of(newAuction, existingPerson));        }        return;    }    Person theNewPerson = null;    for (Person newPerson : c.element().getValue().getAll(NexmarkQueryUtil.PERSON_TAG)) {        if (theNewPerson == null) {            theNewPerson = newPerson;        } else {            if (theNewPerson.equals(newPerson)) {                            } else {                            }            fatalCounter.inc();            continue;        }        newPersonCounter.inc();                        List<Auction> pendingAuctions = auctionsState.read();        if (pendingAuctions != null) {            for (Auction pendingAuction : pendingAuctions) {                oldNewOutputCounter.inc();                c.output(KV.of(pendingAuction, newPerson));            }            auctionsState.clear();        }                for (Auction newAuction : c.element().getValue().getAll(NexmarkQueryUtil.AUCTION_TAG)) {            newAuctionCounter.inc();            newNewOutputCounter.inc();            c.output(KV.of(newAuction, newPerson));        }                personState.write(newPerson);                Instant firingTime = new Instant(newPerson.dateTime).plus(Duration.standardSeconds(maxAuctionsWaitingTime));        timer.set(firingTime);    }    if (theNewPerson != null) {        return;    }            List<Auction> pendingAuctions = auctionsState.read();    if (pendingAuctions == null) {        pendingAuctions = new ArrayList<>();    }    for (Auction newAuction : c.element().getValue().getAll(NexmarkQueryUtil.AUCTION_TAG)) {        newAuctionCounter.inc();        pendingAuctions.add(newAuction);    }    auctionsState.write(pendingAuctions);}
public void beam_f33953_0(OnTimerContext context, @StateId(PERSON) ValueState<Person> personState)
{    personState.clear();}
private void beam_f33962_0(Instant newWindowStart)
{    while (!newWindowStart.equals(windowStart)) {        averages(windowStart.plus(Duration.standardSeconds(configuration.windowSizeSec)));        windowStart = windowStart.plus(Duration.standardSeconds(configuration.windowPeriodSec));        winningPricesByCategory.removeIf(categoryPriceTimestampedValue -> categoryPriceTimestampedValue.getTimestamp().isBefore(windowStart));        if (winningPricesByCategory.isEmpty()) {            windowStart = newWindowStart;        }    }}
private void beam_f33963_0(Auction auction, Bid bid, Instant timestamp)
{    winningPricesByCategory.add(TimestampedValue.of(new CategoryPrice(auction.category, bid.price, false), timestamp));}
private void beam_f33972_0(Instant end)
{    Map<Long, Long> counts = new TreeMap<>();    long maxCount = 0L;    for (Map.Entry<Long, List<Instant>> entry : bids.entrySet()) {        long count = 0L;        long auction = entry.getKey();        for (Instant bid : entry.getValue()) {            if (bid.isBefore(end)) {                count++;            }        }        if (count > 0) {            counts.put(auction, count);            maxCount = Math.max(maxCount, count);        }    }    for (Map.Entry<Long, Long> entry : counts.entrySet()) {        long auction = entry.getKey();        long count = entry.getValue();        if (count == maxCount) {            AuctionCount result = new AuctionCount(auction, count);            addResult(TimestampedValue.of(result, end));        }    }}
private boolean beam_f33973_0(Instant cutoff)
{    boolean anyRemain = false;    for (Map.Entry<Long, List<Instant>> entry : bids.entrySet()) {        long auction = entry.getKey();        Iterator<Instant> itr = entry.getValue().iterator();        while (itr.hasNext()) {            Instant bid = itr.next();            if (bid.isBefore(cutoff)) {                NexmarkUtils.info("retire: %s for %s", bid, auction);                itr.remove();            } else {                anyRemain = true;            }        }    }    return anyRemain;}
public Long beam_f33982_0(List<Bid> accumulator)
{    if (accumulator.isEmpty()) {        return 0L;    }    long sumOfPrice = 0;    for (Bid bid : accumulator) {        sumOfPrice += bid.price;    }    return Math.round((double) sumOfPrice / accumulator.size());}
public PCollection<SellerPrice> beam_f33983_0(PCollection<Event> events)
{    return events.apply(Filter.by(new AuctionOrBid())).apply(new WinningBids(name + ".WinningBids", configuration)).apply(name + ".Rekey", ParDo.of(new DoFn<AuctionBid, KV<Long, Bid>>() {        @ProcessElement        public void processElement(ProcessContext c) {            Auction auction = c.element().auction;            Bid bid = c.element().bid;            c.output(KV.of(auction.seller, bid));        }    })).apply(Window.<KV<Long, Bid>>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))).accumulatingFiredPanes().withAllowedLateness(Duration.ZERO)).apply(Combine.perKey(new MovingMeanSellingPrice(10))).apply(name + ".Select", ParDo.of(new DoFn<KV<Long, Long>, SellerPrice>() {        @ProcessElement        public void processElement(ProcessContext c) {            c.output(new SellerPrice(c.element().getKey(), c.element().getValue()));        }    }));}
public void beam_f33992_0(ProcessContext c)
{    long maxPrice = c.sideInput(maxPriceView);    Bid bid = c.element();    if (bid.price == maxPrice) {        c.output(bid);    }}
private void beam_f33993_0(Instant timestamp)
{    for (Bid bid : highestBids) {        addResult(TimestampedValue.of(bid, timestamp));    }    highestBids.clear();}
private void beam_f34002_0(Auction auction, Person person, Instant timestamp)
{    addResult(TimestampedValue.of(new IdNameReserve(person.id, person.name, auction.reserve), timestamp));}
public void beam_f34003_0()
{    TimestampedValue<Event> timestampedEvent = nextInput();    if (timestampedEvent == null) {        allDone();        return;    }    Event event = timestampedEvent.getValue();    if (event.bid != null) {                return;    }    Instant timestamp = timestampedEvent.getTimestamp();    Instant newWindowStart = windowStart(Duration.standardSeconds(configuration.windowSizeSec), Duration.standardSeconds(configuration.windowSizeSec), timestamp);    if (!newWindowStart.equals(windowStart)) {                retirePersons();        retireAuctions();        windowStart = newWindowStart;    }    if (event.newAuction != null) {                Person person = newPersons.get(event.newAuction.seller);        if (person != null) {            addResult(event.newAuction, person, timestamp);        } else {                        newAuctions.put(event.newAuction.seller, event.newAuction);        }    } else {                for (Auction auction : newAuctions.get(event.newPerson.id)) {            addResult(auction, event.newPerson, timestamp);        }                newAuctions.removeAll(event.newPerson.id);                newPersons.put(event.newPerson.id, event.newPerson);    }}
protected void beam_f34012_0()
{    TimestampedValue<Event> timestampedEvent = nextInput();    if (timestampedEvent == null) {        for (Long bidder : ImmutableSet.copyOf(activeSessions.keySet())) {            flushSession(bidder);        }        allDone();        return;    }    Event event = timestampedEvent.getValue();    if (event.bid == null) {                return;    }    List<TimestampedValue<Event>> activeSession = activeSessions.get(event.bid.bidder);    if (activeSession == null) {        beginSession(timestampedEvent);    } else if (timestampedEvent.getTimestamp().isAfter(activeSession.get(activeSession.size() - 1).getTimestamp().plus(configuration.sessionGap))) {        flushSession(event.bid.bidder);        beginSession(timestampedEvent);    } else {        activeSession.add(timestampedEvent);    }}
private void beam_f34013_0(TimestampedValue<Event> timestampedEvent)
{    checkState(activeSessions.get(timestampedEvent.getValue().bid.bidder) == null);    List<TimestampedValue<Event>> session = new ArrayList<>();    session.add(timestampedEvent);    activeSessions.put(timestampedEvent.getValue().bid.bidder, session);}
public Long beam_f34022_0(Long price)
{    return (price * 89) / 100;}
public PCollection<Bid> beam_f34023_0(PCollection<Event> allEvents)
{    return allEvents.apply(Filter.by(NexmarkQueryUtil.IS_BID)).apply(getName() + ".SelectEvent", new SelectEvent(Type.BID)).apply(QUERY).apply(Convert.fromRows(Bid.class));}
public static AuctionOrBidWindow beam_f34032_0(long expectedAuctionDurationMs, Instant timestamp, Bid bid)
{        return new AuctionOrBidWindow(timestamp, timestamp.plus(expectedAuctionDurationMs * 2), bid.auction, false);}
public boolean beam_f34033_0()
{    return isAuctionWindow;}
public void beam_f34043_0(MergeContext c) throws Exception
{        Map<Long, AuctionOrBidWindow> idToTrueAuctionWindow = new TreeMap<>();    Map<Long, List<AuctionOrBidWindow>> idToBidAuctionWindows = new TreeMap<>();    for (AuctionOrBidWindow window : c.windows()) {        if (window.isAuctionWindow()) {            idToTrueAuctionWindow.put(window.auction, window);        } else {            List<AuctionOrBidWindow> bidWindows = idToBidAuctionWindows.computeIfAbsent(window.auction, k -> new ArrayList<>());            bidWindows.add(window);        }    }        for (Map.Entry<Long, AuctionOrBidWindow> entry : idToTrueAuctionWindow.entrySet()) {        long auction = entry.getKey();        AuctionOrBidWindow auctionWindow = entry.getValue();        List<AuctionOrBidWindow> bidWindows = idToBidAuctionWindows.get(auction);        if (bidWindows != null) {            List<AuctionOrBidWindow> toBeMerged = new ArrayList<>();            for (AuctionOrBidWindow bidWindow : bidWindows) {                if (bidWindow.start().isBefore(auctionWindow.end())) {                    toBeMerged.add(bidWindow);                }                                    }            if (!toBeMerged.isEmpty()) {                toBeMerged.add(auctionWindow);                c.merge(toBeMerged, auctionWindow);            }        }    }}
public boolean beam_f34044_0(WindowFn<?, ?> other)
{    return other instanceof AuctionOrBidWindowFn;}
private void beam_f34053_0()
{    Iterator<Bid> itr = bidsWithoutAuctions.iterator();    while (itr.hasNext()) {        Bid bid = itr.next();        if (captureBestBid(bid, false)) {            NexmarkUtils.info("bid now accounted for: %s", bid);            itr.remove();        }    }}
private TimestampedValue<AuctionBid> beam_f34054_0(Instant timestamp)
{    Map<Instant, List<Long>> toBeRetired = new TreeMap<>();    for (Map.Entry<Long, Auction> entry : openAuctions.entrySet()) {        if (entry.getValue().expires.compareTo(timestamp) <= 0) {            List<Long> idsAtTime = toBeRetired.computeIfAbsent(entry.getValue().expires, k -> new ArrayList<>());            idsAtTime.add(entry.getKey());        }    }    for (Map.Entry<Instant, List<Long>> entry : toBeRetired.entrySet()) {        for (long id : entry.getValue()) {            Auction auction = openAuctions.get(id);            NexmarkUtils.info("retiring auction: %s", auction);            openAuctions.remove(id);            Bid bestBid = bestBids.get(id);            if (bestBid != null) {                TimestampedValue<AuctionBid> result = TimestampedValue.of(new AuctionBid(auction, bestBid), auction.expires);                NexmarkUtils.info("winning: %s", result);                return result;            }        }    }    return null;}
public List<BoundedEventSource> beam_f34064_0(long desiredBundleSizeBytes, PipelineOptions options)
{    NexmarkUtils.info("slitting bounded source %s into %d sub-sources", config, numEventGenerators);    List<BoundedEventSource> results = new ArrayList<>();        for (GeneratorConfig subConfig : config.split(numEventGenerators)) {        results.add(new BoundedEventSource(subConfig, 1));    }    return results;}
public long beam_f34065_0(PipelineOptions options)
{    return config.getEstimatedSizeBytes();}
public GeneratorConfig beam_f34075_0()
{    return config;}
public GeneratorConfig beam_f34076_0(long eventId)
{    long newMaxEvents = eventId - (config.firstEventId + config.firstEventNumber);    GeneratorConfig remainConfig = config.copyWith(config.firstEventId, config.maxEvents - newMaxEvents, config.firstEventNumber + newMaxEvents);    config = config.copyWith(config.firstEventId, newMaxEvents, config.firstEventNumber);    return remainConfig;}
public void beam_f34085_0(GeneratorCheckpoint value, OutputStream outStream) throws CoderException, IOException
{    LONG_CODER.encode(value.numEvents, outStream);    LONG_CODER.encode(value.wallclockBaseTime, outStream);}
public GeneratorCheckpoint beam_f34086_0(InputStream inStream) throws CoderException, IOException
{    long numEvents = LONG_CODER.decode(inStream);    long wallclockBaseTime = LONG_CODER.decode(inStream);    return new GeneratorCheckpoint(numEvents, wallclockBaseTime);}
public int beam_f34097_0()
{    return configuration.hotSellersRatio;}
public int beam_f34098_0()
{    return configuration.numInFlightAuctions;}
public long beam_f34107_0()
{    return firstEventId + firstEventNumber + maxEvents;}
public long beam_f34108_0(long numEvents)
{    return firstEventNumber + numEvents;}
public static Bid beam_f34117_0(long eventId, Random random, long timestamp, GeneratorConfig config)
{    long auction;        if (random.nextInt(config.getHotAuctionRatio()) > 0) {                auction = (lastBase0AuctionId(eventId) / HOT_AUCTION_RATIO) * HOT_AUCTION_RATIO;    } else {        auction = nextBase0AuctionId(eventId, random, config);    }    auction += GeneratorConfig.FIRST_AUCTION_ID;    long bidder;        if (random.nextInt(config.getHotBiddersRatio()) > 0) {                        bidder = (lastBase0PersonId(eventId) / HOT_BIDDER_RATIO) * HOT_BIDDER_RATIO + 1;    } else {        bidder = nextBase0PersonId(eventId, random, config);    }    bidder += GeneratorConfig.FIRST_PERSON_ID;    long price = PriceGenerator.nextPrice(random);    int currentSize = 8 + 8 + 8 + 8;    String extra = nextExtra(random, currentSize, config.getAvgBidByteSize());    return new Bid(auction, bidder, price, new Instant(timestamp), extra);}
public static long beam_f34118_0(Random random, long n)
{    if (n < Integer.MAX_VALUE) {        return random.nextInt((int) n);    } else {                return Math.abs(random.nextLong() % n);    }}
public static long beam_f34127_0(Random random)
{    return Math.round(Math.pow(10.0, random.nextDouble() * 6.0) * 100.0);}
public static String beam_f34128_0(Random random, int maxLength)
{    int len = MIN_STRING_LENGTH + random.nextInt(maxLength - MIN_STRING_LENGTH);    StringBuilder sb = new StringBuilder();    while (len-- > 0) {        if (random.nextInt(13) == 0) {            sb.append(' ');        } else {            sb.append((char) ('a' + random.nextInt(26)));        }    }    return sb.toString().trim();}
public Instant beam_f34138_0()
{    return new Instant(watermark);}
public GeneratorCheckpoint beam_f34139_0()
{    return generator.toCheckpoint();}
public void beam_f34149_0() throws Exception
{    assertEquals(ROW_SIZE, RowSize.of(ROW).sizeInBytes());}
public void beam_f34150_0() throws Exception
{    PCollection<Row> rows = testPipeline.apply(TestStream.create(SchemaCoder.of(ROW_TYPE)).addElements(ROW).advanceWatermarkToInfinity());    PAssert.that(rows).satisfies(new CorrectSize());    testPipeline.run();}
public PipelineResult beam_f34159_0(Pipeline pipeline)
{    return null;}
public void beam_f34160_0()
{    options = PipelineOptionsFactory.create().as(NexmarkOptions.class);    options.setBigQueryTable("nexmark");    options.setBigQueryDataset("nexmark");    options.setRunner(DirectRunner.class);    options.setStreaming(true);    options.setProject("nexmark-test");    options.setResourceNameMode(NexmarkUtils.ResourceNameMode.QUERY_RUNNER_AND_MODE);    publisher = new FakeBigQueryResultsPublisher();}
private void beam_f34169_0(String name, NexmarkQueryTransform<T> query, NexmarkQueryModel<T> model, boolean streamingMode)
{    NexmarkUtils.setupPipeline(NexmarkUtils.CoderStrategy.HAND, p);    PCollection<Event> events = p.apply(name + ".Read", streamingMode ? NexmarkUtils.streamEventsSource(CONFIG) : NexmarkUtils.batchEventsSource(CONFIG));    PCollection<TimestampedValue<T>> results = (PCollection<TimestampedValue<T>>) events.apply(new NexmarkQuery<>(CONFIG, query));    PAssert.that(results).satisfies(model.assertionFor());    PipelineResult result = p.run();    result.waitUntilFinish();}
public void beam_f34170_0()
{    queryMatchesModel("Query0TestBatch", new Query0(), new Query0Model(CONFIG), false);}
public void beam_f34179_0()
{    queryMatchesModel("SqlQuery2TestStreaming", new SqlQuery2(CONFIG.auctionSkip), new Query2Model(CONFIG), true);}
public void beam_f34180_0()
{    queryMatchesModel("Query3TestBatch", new Query3(CONFIG), new Query3Model(CONFIG), false);}
public void beam_f34189_0()
{    queryMatchesModel("SqlQuery5TestStreaming", new SqlQuery5(CONFIG), new Query5Model(CONFIG), true);}
public void beam_f34190_0()
{    queryMatchesModel("Query6TestBatch", new Query6(CONFIG), new Query6Model(CONFIG), false);}
public void beam_f34199_0()
{    queryMatchesModel("Query9TestStreaming", new Query9(CONFIG), new Query9Model(CONFIG), true);}
public void beam_f34200_0()
{    NexmarkUtils.setupPipeline(NexmarkUtils.CoderStrategy.HAND, p);}
public void beam_f34209_0() throws Exception
{    NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy();    config.sideInputType = NexmarkUtils.SideInputType.DIRECT;    config.numEventGenerators = 1;    config.numEvents = 5000;    config.sideInputRowCount = 10;    config.sideInputNumShards = 3;    PCollection<KV<Long, String>> sideInput = NexmarkUtils.prepareSideInput(p, config);    try {        PCollection<Event> input = p.apply(NexmarkUtils.batchEventsSource(config));        PCollection<Bid> justBids = input.apply(NexmarkQueryUtil.JUST_BIDS);        PCollection<Long> bidCount = justBids.apply("Count Bids", Count.globally());        NexmarkQueryTransform<Bid> query = new SqlBoundedSideInputJoin(config);        query.setSideInput(sideInput);        PCollection<TimestampedValue<Bid>> output = (PCollection<TimestampedValue<Bid>>) input.apply(new NexmarkQuery(config, query));        PCollection<Long> outputCount = output.apply("Count outputs", Count.globally());        PAssert.that(PCollectionList.of(bidCount).and(outputCount).apply(Flatten.pCollections())).satisfies(counts -> {            assertThat(Iterables.size(counts), equalTo(2));            assertThat(Iterables.get(counts, 0), greaterThan(0L));            assertThat(Iterables.get(counts, 0), equalTo(Iterables.get(counts, 1)));            return null;        });        p.run();    } finally {        NexmarkUtils.cleanUpSideInput(config);    }}
public void beam_f34210_0() throws Exception
{    NexmarkConfiguration config = NexmarkConfiguration.DEFAULT.copy();    config.sideInputType = NexmarkUtils.SideInputType.DIRECT;    config.numEventGenerators = 1;    config.numEvents = 5000;    config.sideInputRowCount = 10;    config.sideInputNumShards = 3;    queryMatchesModel("SqlBoundedSideInputJoinTestBatch", config, new SqlBoundedSideInputJoin(config), new BoundedSideInputJoinModel(config), false);}
private static Bid beam_f34219_0(long id)
{    return new Bid(id, 3L, 100L, new Instant(432342L + id), "extra_" + id);}
private static AuctionPrice beam_f34220_0(Bid bid)
{    return new AuctionPrice(bid.auction, bid.price);}
public void beam_f34229_0() throws Exception
{    NexmarkOptions options = PipelineOptionsFactory.as(NexmarkOptions.class);    long n = 200L;    BoundedEventSource source = new BoundedEventSource(makeConfig(n), 1);    SourceTestUtils.assertUnstartedReaderReadsSameAsItsSource(source.createReader(options), options);}
public void beam_f34230_0() throws Exception
{    NexmarkOptions options = PipelineOptionsFactory.as(NexmarkOptions.class);    long n = 20L;    BoundedEventSource source = new BoundedEventSource(makeConfig(n), 1);        SourceTestUtils.assertSplitAtFractionFails(source, 10, 0.3, options);    SourceTestUtils.assertSplitAtFractionSucceedsAndConsistent(source, 5, 0.3, options);    SourceTestUtils.assertSplitAtFractionExhaustive(source, options);}
public void beam_f34239_0(int n, UnboundedReader<Event> reader, Generator modelGenerator) throws IOException
{    for (int i = 0; i < n; i++) {        assertTrue(modelGenerator.hasNext());        Event modelEvent = modelGenerator.next().getValue();        assertTrue(reader.advance());        Event actualEvent = reader.getCurrent();        assertEquals(modelEvent.toString(), actualEvent.toString());        add(actualEvent);    }}
public void beam_f34240_0() throws IOException
{    Random random = new Random(297);    int n = 47293;    GeneratorConfig config = makeConfig(n);    Generator modelGenerator = new Generator(config);    EventIdChecker checker = new EventIdChecker();    PipelineOptions options = TestPipeline.testingPipelineOptions();    UnboundedEventSource source = new UnboundedEventSource(config, 1, 0, false);    UnboundedReader<Event> reader = source.createReader(options, null);    while (n > 0) {        int m = Math.min(459 + random.nextInt(455), n);        System.out.printf("reading %d...%n", m);        checker.add(m, reader, modelGenerator);        n -= m;        System.out.printf("splitting with %d remaining...%n", n);        CheckpointMark checkpointMark = reader.getCheckpointMark();        reader = source.createReader(options, (GeneratorCheckpoint) checkpointMark);    }    assertFalse(reader.advance());}
public long beam_f34249_0(String name)
{    Iterable<MetricResult<DistributionResult>> timeDistributions = getDistributions(name);    return getGreatestMax(timeDistributions);}
private Long beam_f34250_0(Iterable<MetricResult<DistributionResult>> distributions)
{    Optional<Long> greatestMax = StreamSupport.stream(distributions.spliterator(), true).map(element -> element.getAttempted().getMax()).filter(this::isCredible).max(Long::compareTo);    return greatestMax.orElse(ERRONEOUS_METRIC_VALUE);}
public double beam_f34259_0()
{    return value;}
public static BigQueryClient beam_f34260_0(String dataset)
{    BigQueryOptions options = BigQueryOptions.newBuilder().build();    return new BigQueryClient(options.getService(), options.getProjectId(), dataset);}
public void beam_f34269_0(TestResult result, String tableName, long nowInMillis)
{    Map<String, Object> row = getRowOfSchema(result);        row.put("timestamp", nowInMillis / 1000);    client.insertRow(row, schema, tableName);}
public void beam_f34270_0(TestResult result, String tableName)
{    client.insertRow(getRowOfSchema(result), schema, tableName);}
public void beam_f34280_0()
{    List<Integer> sampleInputData = Arrays.asList(1, 1, 1, 1, 1);    createTestPipeline(sampleInputData, new MonitorWithCounter());    PipelineResult result = testPipeline.run();    MetricsReader reader = new MetricsReader(result, NAMESPACE);    assertEquals(5, reader.getCounterMetric("counter"));}
public void beam_f34281_0()
{    List<Integer> sampleInputData = Arrays.asList(1, 2, 3, 4, 5);    createTestPipelineWithBranches(sampleInputData);    PipelineResult result = testPipeline.run();    MetricsReader reader = new MetricsReader(result, NAMESPACE, 0);    assertEquals(1, reader.getStartTimeMetric("timeDist"));}
public void beam_f34290_0()
{    ImmutableMap<String, String> schema = ImmutableMap.<String, String>builder().put("timestamp", "timestamp").put("field1", "string").build();    this.bigQueryClient = new FakeBigQueryClient();    this.publisher = new BigQueryResultsPublisher(bigQueryClient, schema);}
public void beam_f34291_0()
{    long now = 1000L;    publisher.publish(new SampleTestResult("a", "b"), TABLE_NAME, now);    Map<String, ?> rowInTable = bigQueryClient.getRows(TABLE_NAME).get(0);    assertEquals(2, rowInTable.entrySet().size());    assertEquals(1L, rowInTable.get("timestamp"));    assertEquals("a", rowInTable.get("field1"));}
