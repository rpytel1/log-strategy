close
 void kafkatest_f5_0(long duration, TimeUnit unit)
{    close(Duration.ofMillis(unit.toMillis(duration)));}
f5
0
createTopics
 CreateTopicsResult kafkatest_f6_0(Collection<NewTopic> newTopics)
{    return createTopics(newTopics, new CreateTopicsOptions());}
f6
0
alterConfigs
 AlterConfigsResult kafkatest_f15_0(Map<ConfigResource, Config> configs)
{    return alterConfigs(configs, new AlterConfigsOptions());}
f15
0
incrementalAlterConfigs
 AlterConfigsResult kafkatest_f16_0(Map<ConfigResource, Collection<AlterConfigOp>> configs)
{    return incrementalAlterConfigs(configs, new AlterConfigsOptions());}
f16
0
describeDelegationToken
 DescribeDelegationTokenResult kafkatest_f25_0()
{    return describeDelegationToken(new DescribeDelegationTokenOptions());}
f25
0
describeConsumerGroups
 DescribeConsumerGroupsResult kafkatest_f26_0(Collection<String> groupIds)
{    return describeConsumerGroups(groupIds, new DescribeConsumerGroupsOptions());}
f26
0
listPartitionReassignments
 ListPartitionReassignmentsResult kafkatest_f35_0()
{    return listPartitionReassignments(new ListPartitionReassignmentsOptions());}
f35
0
listPartitionReassignments
 ListPartitionReassignmentsResult kafkatest_f36_0(Set<TopicPartition> partitions)
{    return listPartitionReassignments(partitions, new ListPartitionReassignmentsOptions());}
f36
0
id
public byte kafkatest_f45_0()
{    return id;}
f45
0
forId
public static OpType kafkatest_f46_0(final byte id)
{    return OP_TYPES.get(id);}
f46
0
values
public Map<ConfigResource, KafkaFuture<Void>> kafkatest_f55_0()
{    return futures;}
f55
0
all
public KafkaFuture<Void> kafkatest_f56_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0]));}
f56
0
toString
public String kafkatest_f65_0()
{    return "Config(entries=" + entries.values() + ")";}
f65
0
name
public String kafkatest_f66_0()
{    return name;}
f66
0
toString
public String kafkatest_f75_0()
{    return "ConfigEntry(" + "name=" + name + ", value=" + value + ", source=" + source + ", isSensitive=" + isSensitive + ", isReadOnly=" + isReadOnly + ", synonyms=" + synonyms + ")";}
f75
0
name
public String kafkatest_f76_0()
{    return name;}
f76
0
isSimpleConsumerGroup
public boolean kafkatest_f85_0()
{    return isSimpleConsumerGroup;}
f85
0
members
public Collection<MemberDescription> kafkatest_f86_0()
{    return members;}
f86
0
timeoutMs
public CreateAclsOptions kafkatest_f95_0(Integer timeoutMs)
{    this.timeoutMs = timeoutMs;    return this;}
f95
0
values
public Map<AclBinding, KafkaFuture<Void>> kafkatest_f96_0()
{    return futures;}
f96
0
values
public Map<String, KafkaFuture<Void>> kafkatest_f105_0()
{    return values;}
f105
0
all
public KafkaFuture<Void> kafkatest_f106_0()
{    return KafkaFuture.allOf(values.values().toArray(new KafkaFuture[0]));}
f106
0
values
public List<FilterResult> kafkatest_f115_0()
{    return values;}
f115
0
values
public Map<AclBindingFilter, KafkaFuture<FilterResults>> kafkatest_f116_0()
{    return futures;}
f116
0
lowWatermark
public long kafkatest_f125_0()
{    return lowWatermark;}
f125
0
lowWatermarks
public Map<TopicPartition, KafkaFuture<DeletedRecords>> kafkatest_f126_0()
{    return futures;}
f126
0
includeAuthorizedOperations
public boolean kafkatest_f135_0()
{    return includeAuthorizedOperations;}
f135
0
nodes
public KafkaFuture<Collection<Node>> kafkatest_f136_0()
{    return nodes;}
f136
0
apply
public Map<ConfigResource, Config> kafkatest_f145_0(Void v)
{    Map<ConfigResource, Config> configs = new HashMap<>(futures.size());    for (Map.Entry<ConfigResource, KafkaFuture<Config>> entry : futures.entrySet()) {        try {            configs.put(entry.getKey(), entry.getValue().get());        } catch (InterruptedException | ExecutionException e) {            // completed successfully.            throw new RuntimeException(e);        }    }    return configs;}
f145
0
includeAuthorizedOperations
public DescribeConsumerGroupsOptions kafkatest_f146_0(boolean includeAuthorizedOperations)
{    this.includeAuthorizedOperations = includeAuthorizedOperations;    return this;}
f146
0
all
public KafkaFuture<Map<Integer, Map<String, LogDirInfo>>> kafkatest_f155_0()
{    return KafkaFuture.allOf(futures.values().toArray(new KafkaFuture[0])).thenApply(new KafkaFuture.BaseFunction<Void, Map<Integer, Map<String, LogDirInfo>>>() {        @Override        public Map<Integer, Map<String, LogDirInfo>> apply(Void v) {            Map<Integer, Map<String, LogDirInfo>> descriptions = new HashMap<>(futures.size());            for (Map.Entry<Integer, KafkaFuture<Map<String, LogDirInfo>>> entry : futures.entrySet()) {                try {                    descriptions.put(entry.getKey(), entry.getValue().get());                } catch (InterruptedException | ExecutionException e) {                    // This should be unreachable, because allOf ensured that all the futures completed successfully.                    throw new RuntimeException(e);                }            }            return descriptions;        }    });}
f155
0
apply
public Map<Integer, Map<String, LogDirInfo>> kafkatest_f156_0(Void v)
{    Map<Integer, Map<String, LogDirInfo>> descriptions = new HashMap<>(futures.size());    for (Map.Entry<Integer, KafkaFuture<Map<String, LogDirInfo>>> entry : futures.entrySet()) {        try {            descriptions.put(entry.getKey(), entry.getValue().get());        } catch (InterruptedException | ExecutionException e) {            // This should be unreachable, because allOf ensured that all the futures completed successfully.            throw new RuntimeException(e);        }    }    return descriptions;}
f156
0
timeoutMs
public DescribeTopicsOptions kafkatest_f165_0(Integer timeoutMs)
{    this.timeoutMs = timeoutMs;    return this;}
f165
0
includeAuthorizedOperations
public DescribeTopicsOptions kafkatest_f166_0(boolean includeAuthorizedOperations)
{    this.includeAuthorizedOperations = includeAuthorizedOperations;    return this;}
f166
0
accept
public void kafkatest_f175_0(Map<TopicPartition, Optional<Throwable>> topicPartitions, Throwable throwable)
{    if (throwable != null) {        result.completeExceptionally(throwable);    } else if (!topicPartitions.containsKey(partition)) {        result.completeExceptionally(new UnknownTopicOrPartitionException("Preferred leader election for partition \"" + partition + "\" was not attempted"));    } else {        Optional<Throwable> exception = topicPartitions.get(partition);        if (exception.isPresent()) {            result.completeExceptionally(exception.get());        } else {            result.complete(null);        }    }}
f175
0
partitions
public KafkaFuture<Set<TopicPartition>> kafkatest_f176_0()
{    final KafkaFutureImpl<Set<TopicPartition>> result = new KafkaFutureImpl<>();    electionResult.partitions().whenComplete(new KafkaFuture.BiConsumer<Map<TopicPartition, Optional<Throwable>>, Throwable>() {        @Override        public void accept(Map<TopicPartition, Optional<Throwable>> topicPartitions, Throwable throwable) {            if (throwable != null) {                result.completeExceptionally(throwable);            } else {                result.complete(topicPartitions.keySet());            }        }    });    return result;}
f176
0
handleDisconnection
public void kafkatest_f185_0(String destination)
{// Do nothing}
f185
0
handleFatalException
public void kafkatest_f186_0(KafkaException e)
{    updateFailed(e);}
f186
0
metadataFetchDelayMs
public long kafkatest_f196_0(long now)
{    switch(state) {        case QUIESCENT:            // so there is a metadata refresh backoff period.            return Math.max(delayBeforeNextAttemptMs(now), delayBeforeNextExpireMs(now));        case UPDATE_REQUESTED:            // Respect the backoff, even if an update has been requested            return delayBeforeNextAttemptMs(now);        default:            // An update is already pending, so we don't need to initiate another one.            return Long.MAX_VALUE;    }}
f196
0
delayBeforeNextExpireMs
private long kafkatest_f197_0(long now)
{    long timeSinceUpdate = now - lastMetadataUpdateMs;    return Math.max(0, metadataExpireMs - timeSinceUpdate);}
f197
0
calcDeadlineMs
private long kafkatest_f206_0(long now, Integer optionTimeoutMs)
{    if (optionTimeoutMs != null)        return now + Math.max(0, optionTimeoutMs);    return now + defaultTimeoutMs;}
f206
0
prettyPrintException
 static String kafkatest_f207_0(Throwable throwable)
{    if (throwable == null)        return "Null exception.";    if (throwable.getMessage() != null) {        return throwable.getClass().getSimpleName() + ": " + throwable.getMessage();    }    return throwable.getClass().getSimpleName();}
f207
0
provide
public Node kafkatest_f216_0()
{    if (metadataManager.isReady()) {        // In that case, we will postpone node assignment.        return client.leastLoadedNode(time.milliseconds());    }    metadataManager.requestUpdate();    return null;}
f216
0
curNode
protected Node kafkatest_f217_0()
{    return curNode;}
f217
0
timeoutPendingCalls
private voidf226_1TimeoutProcessor processor)
{    int numTimedOut = processor.handleTimeouts(pendingCalls, "Timed out waiting for a node assignment.");    if (numTimedOut > 0)        }
private voidf226
1
timeoutCallsToSend
private intf227_1TimeoutProcessor processor)
{    int numTimedOut = 0;    for (List<Call> callList : callsToSend.values()) {        numTimedOut += processor.handleTimeouts(callList, "Timed out waiting to send the call.");    }    if (numTimedOut > 0)            return numTimedOut;}
private intf227
1
hasActiveExternalCalls
private boolean kafkatest_f236_0()
{    if (hasActiveExternalCalls(pendingCalls)) {        return true;    }    for (List<Call> callList : callsToSend.values()) {        if (hasActiveExternalCalls(callList)) {            return true;        }    }    return hasActiveExternalCalls(correlationIdToCalls.values());}
f236
0
threadShouldExit
private booleanf237_1long now, long curHardShutdownTimeMs)
{    if (!hasActiveExternalCalls()) {        log.trace("All work has been completed, and the I/O thread is now exiting.");        return true;    }    if (now >= curHardShutdownTimeMs) {                return true;    }        return false;}
private booleanf237
1
groupIdIsUnrepresentable
private static boolean kafkatest_f246_0(String groupId)
{    return groupId == null;}
f246
0
numPendingCalls
 int kafkatest_f247_0()
{    return runnable.pendingCalls.size();}
f247
0
listTopics
public ListTopicsResult kafkatest_f256_0(final ListTopicsOptions options)
{    final KafkaFutureImpl<Map<String, TopicListing>> topicListingFuture = new KafkaFutureImpl<>();    final long now = time.milliseconds();    runnable.call(new Call("listTopics", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return MetadataRequest.Builder.allTopics();        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            MetadataResponse response = (MetadataResponse) abstractResponse;            Map<String, TopicListing> topicListing = new HashMap<>();            for (MetadataResponse.TopicMetadata topicMetadata : response.topicMetadata()) {                String topicName = topicMetadata.topic();                boolean isInternal = topicMetadata.isInternal();                if (!topicMetadata.isInternal() || options.shouldListInternal())                    topicListing.put(topicName, new TopicListing(topicName, isInternal));            }            topicListingFuture.complete(topicListing);        }        @Override        void handleFailure(Throwable throwable) {            topicListingFuture.completeExceptionally(throwable);        }    }, now);    return new ListTopicsResult(topicListingFuture);}
f256
0
createRequest
 AbstractRequest.Builder kafkatest_f257_0(int timeoutMs)
{    return MetadataRequest.Builder.allTopics();}
f257
0
describeCluster
public DescribeClusterResult kafkatest_f266_0(DescribeClusterOptions options)
{    final KafkaFutureImpl<Collection<Node>> describeClusterFuture = new KafkaFutureImpl<>();    final KafkaFutureImpl<Node> controllerFuture = new KafkaFutureImpl<>();    final KafkaFutureImpl<String> clusterIdFuture = new KafkaFutureImpl<>();    final KafkaFutureImpl<Set<AclOperation>> authorizedOperationsFuture = new KafkaFutureImpl<>();    final long now = time.milliseconds();    runnable.call(new Call("listNodes", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            // simplifies communication with older brokers)            return new MetadataRequest.Builder(new MetadataRequestData().setTopics(Collections.emptyList()).setAllowAutoTopicCreation(true).setIncludeClusterAuthorizedOperations(options.includeAuthorizedOperations()));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            MetadataResponse response = (MetadataResponse) abstractResponse;            describeClusterFuture.complete(response.brokers());            controllerFuture.complete(controller(response));            clusterIdFuture.complete(response.clusterId());            authorizedOperationsFuture.complete(validAclOperations(response.clusterAuthorizedOperations()));        }        private Node controller(MetadataResponse response) {            if (response.controller() == null || response.controller().id() == MetadataResponse.NO_CONTROLLER_ID)                return null;            return response.controller();        }        @Override        void handleFailure(Throwable throwable) {            describeClusterFuture.completeExceptionally(throwable);            controllerFuture.completeExceptionally(throwable);            clusterIdFuture.completeExceptionally(throwable);            authorizedOperationsFuture.completeExceptionally(throwable);        }    }, now);    return new DescribeClusterResult(describeClusterFuture, controllerFuture, clusterIdFuture, authorizedOperationsFuture);}
f266
0
createRequest
 AbstractRequest.Builder kafkatest_f267_0(int timeoutMs)
{    // simplifies communication with older brokers)    return new MetadataRequest.Builder(new MetadataRequestData().setTopics(Collections.emptyList()).setAllowAutoTopicCreation(true).setIncludeClusterAuthorizedOperations(options.includeAuthorizedOperations()));}
f267
0
createRequest
 AbstractRequest.Builder kafkatest_f276_0(int timeoutMs)
{    return new CreateAclsRequest.Builder(aclCreations);}
f276
0
handleResponse
 void kafkatest_f277_0(AbstractResponse abstractResponse)
{    CreateAclsResponse response = (CreateAclsResponse) abstractResponse;    List<AclCreationResponse> responses = response.aclCreationResponses();    Iterator<AclCreationResponse> iter = responses.iterator();    for (AclCreation aclCreation : aclCreations) {        KafkaFutureImpl<Void> future = futures.get(aclCreation.acl());        if (!iter.hasNext()) {            future.completeExceptionally(new UnknownServerException("The broker reported no creation result for the given ACL."));        } else {            AclCreationResponse creation = iter.next();            if (creation.error().isFailure()) {                future.completeExceptionally(creation.error().exception());            } else {                future.complete(null);            }        }    }}
f277
0
handleFailure
 void kafkatest_f286_0(Throwable throwable)
{    completeAllExceptionally(unifiedRequestFutures.values(), throwable);}
f286
0
createRequest
 AbstractRequest.Builder kafkatest_f287_0(int timeoutMs)
{    return new DescribeConfigsRequest.Builder(Collections.singleton(resource)).includeSynonyms(options.includeSynonyms());}
f287
0
handleFailure
 void kafkatest_f296_0(Throwable throwable)
{    completeAllExceptionally(futures.values(), throwable);}
f296
0
incrementalAlterConfigs
public AlterConfigsResult kafkatest_f297_0(Map<ConfigResource, Collection<AlterConfigOp>> configs, final AlterConfigsOptions options)
{    final Map<ConfigResource, KafkaFutureImpl<Void>> allFutures = new HashMap<>();    // We must make a separate AlterConfigs request for every BROKER resource we want to alter    // and send the request to that specific broker. Other resources are grouped together into    // a single request that may be sent to any broker.    final Collection<ConfigResource> unifiedRequestResources = new ArrayList<>();    for (ConfigResource resource : configs.keySet()) {        if (dependsOnSpecificNode(resource)) {            NodeProvider nodeProvider = new ConstantNodeIdProvider(Integer.parseInt(resource.name()));            allFutures.putAll(incrementalAlterConfigs(configs, options, Collections.singleton(resource), nodeProvider));        } else            unifiedRequestResources.add(resource);    }    if (!unifiedRequestResources.isEmpty())        allFutures.putAll(incrementalAlterConfigs(configs, options, unifiedRequestResources, new LeastLoadedNodeProvider()));    return new AlterConfigsResult(new HashMap<>(allFutures));}
f297
0
handleFailure
 void kafkatest_f306_0(Throwable throwable)
{    completeAllExceptionally(futures.values(), throwable);}
f306
0
describeLogDirs
public DescribeLogDirsResult kafkatest_f307_0(Collection<Integer> brokers, DescribeLogDirsOptions options)
{    final Map<Integer, KafkaFutureImpl<Map<String, DescribeLogDirsResponse.LogDirInfo>>> futures = new HashMap<>(brokers.size());    for (Integer brokerId : brokers) {        futures.put(brokerId, new KafkaFutureImpl<>());    }    final long now = time.milliseconds();    for (final Integer brokerId : brokers) {        runnable.call(new Call("describeLogDirs", calcDeadlineMs(now, options.timeoutMs()), new ConstantNodeIdProvider(brokerId)) {            @Override            public AbstractRequest.Builder createRequest(int timeoutMs) {                // Query selected partitions in all log directories                return new DescribeLogDirsRequest.Builder(null);            }            @Override            public void handleResponse(AbstractResponse abstractResponse) {                DescribeLogDirsResponse response = (DescribeLogDirsResponse) abstractResponse;                KafkaFutureImpl<Map<String, DescribeLogDirsResponse.LogDirInfo>> future = futures.get(brokerId);                if (response.logDirInfos().size() > 0) {                    future.complete(response.logDirInfos());                } else {                    // response.logDirInfos() will be empty if and only if the user is not authorized to describe clsuter resource.                    future.completeExceptionally(Errors.CLUSTER_AUTHORIZATION_FAILED.exception());                }            }            @Override            void handleFailure(Throwable throwable) {                completeAllExceptionally(futures.values(), throwable);            }        }, now);    }    return new DescribeLogDirsResult(new HashMap<>(futures));}
f307
0
createRequest
public AbstractRequest.Builder kafkatest_f316_0(int timeoutMs)
{    return new CreatePartitionsRequest.Builder(requestMap, timeoutMs, options.validateOnly());}
f316
0
handleResponse
public void kafkatest_f317_0(AbstractResponse abstractResponse)
{    CreatePartitionsResponse response = (CreatePartitionsResponse) abstractResponse;    // Check for controller change    for (ApiError error : response.errors().values()) {        if (error.error() == Errors.NOT_CONTROLLER) {            metadataManager.clearController();            metadataManager.requestUpdate();            throw error.exception();        }    }    for (Map.Entry<String, ApiError> result : response.errors().entrySet()) {        KafkaFutureImpl<Void> future = futures.get(result.getKey());        if (result.getValue().isSuccess()) {            future.complete(null);        } else {            future.completeExceptionally(result.getValue().exception());        }    }}
f317
0
createDelegationToken
public CreateDelegationTokenResult kafkatest_f326_0(final CreateDelegationTokenOptions options)
{    final KafkaFutureImpl<DelegationToken> delegationTokenFuture = new KafkaFutureImpl<>();    final long now = time.milliseconds();    List<CreatableRenewers> renewers = new ArrayList<>();    for (KafkaPrincipal principal : options.renewers()) {        renewers.add(new CreatableRenewers().setPrincipalName(principal.getName()).setPrincipalType(principal.getPrincipalType()));    }    runnable.call(new Call("createDelegationToken", calcDeadlineMs(now, options.timeoutMs()), new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder<CreateDelegationTokenRequest> createRequest(int timeoutMs) {            return new CreateDelegationTokenRequest.Builder(new CreateDelegationTokenRequestData().setRenewers(renewers).setMaxLifetimeMs(options.maxlifeTimeMs()));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            CreateDelegationTokenResponse response = (CreateDelegationTokenResponse) abstractResponse;            if (response.hasError()) {                delegationTokenFuture.completeExceptionally(response.error().exception());            } else {                CreateDelegationTokenResponseData data = response.data();                TokenInformation tokenInfo = new TokenInformation(data.tokenId(), new KafkaPrincipal(data.principalType(), data.principalName()), options.renewers(), data.issueTimestampMs(), data.maxTimestampMs(), data.expiryTimestampMs());                DelegationToken token = new DelegationToken(tokenInfo, data.hmac());                delegationTokenFuture.complete(token);            }        }        @Override        void handleFailure(Throwable throwable) {            delegationTokenFuture.completeExceptionally(throwable);        }    }, now);    return new CreateDelegationTokenResult(delegationTokenFuture);}
f326
0
createRequest
 AbstractRequest.Builder<CreateDelegationTokenRequest> kafkatest_f327_0(int timeoutMs)
{    return new CreateDelegationTokenRequest.Builder(new CreateDelegationTokenRequestData().setRenewers(renewers).setMaxLifetimeMs(options.maxlifeTimeMs()));}
f327
0
handleResponse
 void kafkatest_f336_0(AbstractResponse abstractResponse)
{    ExpireDelegationTokenResponse response = (ExpireDelegationTokenResponse) abstractResponse;    if (response.hasError()) {        expiryTimeFuture.completeExceptionally(response.error().exception());    } else {        expiryTimeFuture.complete(response.expiryTimestamp());    }}
f336
0
handleFailure
 void kafkatest_f337_0(Throwable throwable)
{    expiryTimeFuture.completeExceptionally(throwable);}
f337
0
getNode
public Optional<Node> kafkatest_f346_0()
{    return node;}
f346
0
setNode
public void kafkatest_f347_0(Node node)
{    this.node = Optional.ofNullable(node);}
f347
0
getDescribeConsumerGroupsCall
private Call kafkatest_f356_0(ConsumerGroupOperationContext<ConsumerGroupDescription, DescribeConsumerGroupsOptions> context)
{    return new Call("describeConsumerGroups", context.getDeadline(), new ConstantNodeIdProvider(context.getNode().get().id())) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new DescribeGroupsRequest.Builder(new DescribeGroupsRequestData().setGroups(Collections.singletonList(context.getGroupId())).setIncludeAuthorizedOperations(context.getOptions().includeAuthorizedOperations()));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            final DescribeGroupsResponse response = (DescribeGroupsResponse) abstractResponse;            List<DescribedGroup> describedGroups = response.data().groups();            if (describedGroups.isEmpty()) {                context.getFuture().completeExceptionally(new InvalidGroupIdException("No consumer group found for GroupId: " + context.getGroupId()));                return;            }            if (describedGroups.size() > 1 || !describedGroups.get(0).groupId().equals(context.getGroupId())) {                String ids = Arrays.toString(describedGroups.stream().map(DescribedGroup::groupId).toArray());                context.getFuture().completeExceptionally(new InvalidGroupIdException("DescribeConsumerGroup request for GroupId: " + context.getGroupId() + " returned " + ids));                return;            }            final DescribedGroup describedGroup = describedGroups.get(0);            // If coordinator changed since we fetched it, retry            if (context.hasCoordinatorMoved(response)) {                rescheduleTask(context, () -> getDescribeConsumerGroupsCall(context));                return;            }            final Errors groupError = Errors.forCode(describedGroup.errorCode());            if (handleGroupRequestError(groupError, context.getFuture()))                return;            final String protocolType = describedGroup.protocolType();            if (protocolType.equals(ConsumerProtocol.PROTOCOL_TYPE) || protocolType.isEmpty()) {                final List<DescribedGroupMember> members = describedGroup.members();                final List<MemberDescription> memberDescriptions = new ArrayList<>(members.size());                final Set<AclOperation> authorizedOperations = validAclOperations(describedGroup.authorizedOperations());                for (DescribedGroupMember groupMember : members) {                    Set<TopicPartition> partitions = Collections.emptySet();                    if (groupMember.memberAssignment().length > 0) {                        final Assignment assignment = ConsumerProtocol.deserializeAssignment(ByteBuffer.wrap(groupMember.memberAssignment()));                        partitions = new HashSet<>(assignment.partitions());                    }                    final MemberDescription memberDescription = new MemberDescription(groupMember.memberId(), Optional.ofNullable(groupMember.groupInstanceId()), groupMember.clientId(), groupMember.clientHost(), new MemberAssignment(partitions));                    memberDescriptions.add(memberDescription);                }                final ConsumerGroupDescription consumerGroupDescription = new ConsumerGroupDescription(context.getGroupId(), protocolType.isEmpty(), memberDescriptions, describedGroup.protocolData(), ConsumerGroupState.parse(describedGroup.groupState()), context.getNode().get(), authorizedOperations);                context.getFuture().complete(consumerGroupDescription);            }        }        @Override        void handleFailure(Throwable throwable) {            context.getFuture().completeExceptionally(throwable);        }    };}
f356
0
createRequest
 AbstractRequest.Builder kafkatest_f357_0(int timeoutMs)
{    return new DescribeGroupsRequest.Builder(new DescribeGroupsRequestData().setGroups(Collections.singletonList(context.getGroupId())).setIncludeAuthorizedOperations(context.getOptions().includeAuthorizedOperations()));}
f357
0
tryComplete
private synchronized void kafkatest_f366_0()
{    if (remaining.isEmpty()) {        ArrayList<Object> results = new ArrayList<>(listings.values());        results.addAll(errors);        future.complete(results);    }}
f366
0
listConsumerGroups
public ListConsumerGroupsResult kafkatest_f367_0(ListConsumerGroupsOptions options)
{    final KafkaFutureImpl<Collection<Object>> all = new KafkaFutureImpl<>();    final long nowMetadata = time.milliseconds();    final long deadline = calcDeadlineMs(nowMetadata, options.timeoutMs());    runnable.call(new Call("findAllBrokers", deadline, new LeastLoadedNodeProvider()) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new MetadataRequest.Builder(new MetadataRequestData().setTopics(Collections.emptyList()).setAllowAutoTopicCreation(true));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            MetadataResponse metadataResponse = (MetadataResponse) abstractResponse;            Collection<Node> nodes = metadataResponse.brokers();            if (nodes.isEmpty())                throw new StaleMetadataException("Metadata fetch failed due to missing broker list");            HashSet<Node> allNodes = new HashSet<>(nodes);            final ListConsumerGroupsResults results = new ListConsumerGroupsResults(allNodes, all);            for (final Node node : allNodes) {                final long nowList = time.milliseconds();                runnable.call(new Call("listConsumerGroups", deadline, new ConstantNodeIdProvider(node.id())) {                    @Override                    AbstractRequest.Builder createRequest(int timeoutMs) {                        return new ListGroupsRequest.Builder(new ListGroupsRequestData());                    }                    private void maybeAddConsumerGroup(ListGroupsResponseData.ListedGroup group) {                        String protocolType = group.protocolType();                        if (protocolType.equals(ConsumerProtocol.PROTOCOL_TYPE) || protocolType.isEmpty()) {                            final String groupId = group.groupId();                            final ConsumerGroupListing groupListing = new ConsumerGroupListing(groupId, protocolType.isEmpty());                            results.addListing(groupListing);                        }                    }                    @Override                    void handleResponse(AbstractResponse abstractResponse) {                        final ListGroupsResponse response = (ListGroupsResponse) abstractResponse;                        synchronized (results) {                            Errors error = Errors.forCode(response.data().errorCode());                            if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.COORDINATOR_NOT_AVAILABLE) {                                throw error.exception();                            } else if (error != Errors.NONE) {                                results.addError(error.exception(), node);                            } else {                                for (ListGroupsResponseData.ListedGroup group : response.data().groups()) {                                    maybeAddConsumerGroup(group);                                }                            }                            results.tryComplete(node);                        }                    }                    @Override                    void handleFailure(Throwable throwable) {                        synchronized (results) {                            results.addError(throwable, node);                            results.tryComplete(node);                        }                    }                }, nowList);            }        }        @Override        void handleFailure(Throwable throwable) {            KafkaException exception = new KafkaException("Failed to find brokers to send ListGroups", throwable);            all.complete(Collections.singletonList(exception));        }    }, nowMetadata);    return new ListConsumerGroupsResult(all);}
f367
0
getListConsumerGroupOffsetsCall
private Callf376_1ConsumerGroupOperationContext<Map<TopicPartition, OffsetAndMetadata>, ListConsumerGroupOffsetsOptions> context)
{    return new Call("listConsumerGroupOffsets", context.getDeadline(), new ConstantNodeIdProvider(context.getNode().get().id())) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            return new OffsetFetchRequest.Builder(context.getGroupId(), context.getOptions().topicPartitions());        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            final OffsetFetchResponse response = (OffsetFetchResponse) abstractResponse;            final Map<TopicPartition, OffsetAndMetadata> groupOffsetsListing = new HashMap<>();            // If coordinator changed since we fetched it, retry            if (context.hasCoordinatorMoved(response)) {                rescheduleTask(context, () -> getListConsumerGroupOffsetsCall(context));                return;            }            if (handleGroupRequestError(response.error(), context.getFuture()))                return;            for (Map.Entry<TopicPartition, OffsetFetchResponse.PartitionData> entry : response.responseData().entrySet()) {                final TopicPartition topicPartition = entry.getKey();                OffsetFetchResponse.PartitionData partitionData = entry.getValue();                final Errors error = partitionData.error;                if (error == Errors.NONE) {                    final Long offset = partitionData.offset;                    final String metadata = partitionData.metadata;                    final Optional<Integer> leaderEpoch = partitionData.leaderEpoch;                    groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, leaderEpoch, metadata));                } else {                                    }            }            context.getFuture().complete(groupOffsetsListing);        }        @Override        void handleFailure(Throwable throwable) {            context.getFuture().completeExceptionally(throwable);        }    };}
private Callf376
1
createRequest
 AbstractRequest.Builder kafkatest_f377_0(int timeoutMs)
{    return new OffsetFetchRequest.Builder(context.getGroupId(), context.getOptions().topicPartitions());}
f377
0
getDeleteConsumerGroupOffsetsCall
private Call kafkatest_f386_0(ConsumerGroupOperationContext<Map<TopicPartition, Errors>, DeleteConsumerGroupOffsetsOptions> context, Set<TopicPartition> partitions)
{    return new Call("deleteConsumerGroupOffsets", context.getDeadline(), new ConstantNodeIdProvider(context.getNode().get().id())) {        @Override        AbstractRequest.Builder createRequest(int timeoutMs) {            final OffsetDeleteRequestTopicCollection topics = new OffsetDeleteRequestTopicCollection();            partitions.stream().collect(Collectors.groupingBy(TopicPartition::topic)).forEach((topic, topicPartitions) -> {                topics.add(new OffsetDeleteRequestTopic().setName(topic).setPartitions(topicPartitions.stream().map(tp -> new OffsetDeleteRequestPartition().setPartitionIndex(tp.partition())).collect(Collectors.toList())));            });            return new OffsetDeleteRequest.Builder(new OffsetDeleteRequestData().setGroupId(context.groupId).setTopics(topics));        }        @Override        void handleResponse(AbstractResponse abstractResponse) {            final OffsetDeleteResponse response = (OffsetDeleteResponse) abstractResponse;            // If coordinator changed since we fetched it, retry            if (context.hasCoordinatorMoved(response)) {                rescheduleTask(context, () -> getDeleteConsumerGroupOffsetsCall(context, partitions));                return;            }            // If the error is an error at the group level, the future is failed with it            final Errors groupError = Errors.forCode(response.data.errorCode());            if (handleGroupRequestError(groupError, context.getFuture()))                return;            final Map<TopicPartition, Errors> partitions = new HashMap<>();            response.data.topics().forEach(topic -> {                topic.partitions().forEach(partition -> {                    partitions.put(new TopicPartition(topic.name(), partition.partitionIndex()), Errors.forCode(partition.errorCode()));                });            });            context.getFuture().complete(partitions);        }        @Override        void handleFailure(Throwable throwable) {            context.getFuture().completeExceptionally(throwable);        }    };}
f386
0
createRequest
 AbstractRequest.Builder kafkatest_f387_0(int timeoutMs)
{    final OffsetDeleteRequestTopicCollection topics = new OffsetDeleteRequestTopicCollection();    partitions.stream().collect(Collectors.groupingBy(TopicPartition::topic)).forEach((topic, topicPartitions) -> {        topics.add(new OffsetDeleteRequestTopic().setName(topic).setPartitions(topicPartitions.stream().map(tp -> new OffsetDeleteRequestPartition().setPartitionIndex(tp.partition())).collect(Collectors.toList())));    });    return new OffsetDeleteRequest.Builder(new OffsetDeleteRequestData().setGroupId(context.groupId).setTopics(topics));}
f387
0
createRequest
public AbstractRequest.Builder kafkatest_f396_0(int timeoutMs)
{    AlterPartitionReassignmentsRequestData data = new AlterPartitionReassignmentsRequestData();    for (Map.Entry<String, Map<Integer, Optional<NewPartitionReassignment>>> entry : topicsToReassignments.entrySet()) {        String topicName = entry.getKey();        Map<Integer, Optional<NewPartitionReassignment>> partitionsToReassignments = entry.getValue();        List<ReassignablePartition> reassignablePartitions = new ArrayList<>();        for (Map.Entry<Integer, Optional<NewPartitionReassignment>> partitionEntry : partitionsToReassignments.entrySet()) {            int partitionIndex = partitionEntry.getKey();            Optional<NewPartitionReassignment> reassignment = partitionEntry.getValue();            ReassignablePartition reassignablePartition = new ReassignablePartition().setPartitionIndex(partitionIndex).setReplicas(reassignment.map(NewPartitionReassignment::targetBrokers).orElse(null));            reassignablePartitions.add(reassignablePartition);        }        ReassignableTopic reassignableTopic = new ReassignableTopic().setName(topicName).setPartitions(reassignablePartitions);        data.topics().add(reassignableTopic);    }    data.setTimeoutMs(timeoutMs);    return new AlterPartitionReassignmentsRequest.Builder(data);}
f396
0
handleResponse
public void kafkatest_f397_0(AbstractResponse abstractResponse)
{    AlterPartitionReassignmentsResponse response = (AlterPartitionReassignmentsResponse) abstractResponse;    Map<TopicPartition, ApiException> errors = new HashMap<>();    int receivedResponsesCount = 0;    Errors topLevelError = Errors.forCode(response.data().errorCode());    switch(topLevelError) {        case NONE:            receivedResponsesCount += validateTopicResponses(response.data().responses(), errors);            break;        case NOT_CONTROLLER:            handleNotControllerError(topLevelError);            break;        default:            for (ReassignableTopicResponse topicResponse : response.data().responses()) {                String topicName = topicResponse.name();                for (ReassignablePartitionResponse partition : topicResponse.partitions()) {                    errors.put(new TopicPartition(topicName, partition.partitionIndex()), new ApiError(topLevelError, topLevelError.message()).exception());                    receivedResponsesCount += 1;                }            }            break;    }    assertResponseCountMatch(errors, receivedResponsesCount);    for (Map.Entry<TopicPartition, ApiException> entry : errors.entrySet()) {        ApiException exception = entry.getValue();        if (exception == null)            futures.get(entry.getKey()).complete(null);        else            futures.get(entry.getKey()).completeExceptionally(exception);    }}
f397
0
dependsOnSpecificNode
private boolean kafkatest_f406_0(ConfigResource resource)
{    return (resource.type() == ConfigResource.Type.BROKER && !resource.isDefault()) || resource.type() == ConfigResource.Type.BROKER_LOGGER;}
f406
0
removeMemberFromConsumerGroup
public MembershipChangeResult kafkatest_f407_0(String groupId, RemoveMemberFromConsumerGroupOptions options)
{    final long startFindCoordinatorMs = time.milliseconds();    final long deadline = calcDeadlineMs(startFindCoordinatorMs, options.timeoutMs());    KafkaFutureImpl<RemoveMemberFromGroupResult> future = new KafkaFutureImpl<>();    ConsumerGroupOperationContext<RemoveMemberFromGroupResult, RemoveMemberFromConsumerGroupOptions> context = new ConsumerGroupOperationContext<>(groupId, options, deadline, future);    Call findCoordinatorCall = getFindCoordinatorCall(context, () -> KafkaAdminClient.this.getRemoveMembersFromGroupCall(context));    runnable.call(findCoordinatorCall, startFindCoordinatorMs);    return new MembershipChangeResult(future);}
f407
0
all
public KafkaFuture<Collection<ConsumerGroupListing>> kafkatest_f416_0()
{    return all;}
f416
0
valid
public KafkaFuture<Collection<ConsumerGroupListing>> kafkatest_f417_0()
{    return valid;}
f417
0
names
public KafkaFuture<Set<String>> kafkatest_f426_0()
{    return future.thenApply(new KafkaFuture.BaseFunction<Map<String, TopicListing>, Set<String>>() {        @Override        public Set<String> apply(Map<String, TopicListing> namesToListings) {            return namesToListings.keySet();        }    });}
f426
0
apply
public Set<String> kafkatest_f427_0(Map<String, TopicListing> namesToListings)
{    return namesToListings.keySet();}
f427
0
clientId
public String kafkatest_f436_0()
{    return clientId;}
f436
0
host
public String kafkatest_f437_0()
{    return host;}
f437
0
totalCount
public int kafkatest_f446_0()
{    return totalCount;}
f446
0
assignments
public List<List<Integer>> kafkatest_f447_0()
{    return newAssignments;}
f447
0
toString
public String kafkatest_f456_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(name=").append(name).append(", numPartitions=").append(numPartitions.map(String::valueOf).orElse("default")).append(", replicationFactor=").append(replicationFactor.map(String::valueOf).orElse("default")).append(", replicasAssignments=").append(replicasAssignments).append(", configs=").append(configs).append(")");    return bld.toString();}
f456
0
replicas
public List<Integer> kafkatest_f457_0()
{    return replicas;}
f457
0
topLevelError
public Errors kafkatest_f466_0()
{    return topLevelError;}
f466
0
hasError
public boolean kafkatest_f467_0()
{    return hasError;}
f467
0
partitions
public List<TopicPartitionInfo> kafkatest_f476_0()
{    return partitions;}
f476
0
authorizedOperations
public Set<AclOperation> kafkatest_f477_0()
{    return authorizedOperations;}
f477
0
maxUsableProduceMagic
public synchronized byte kafkatest_f486_0()
{    return maxUsableProduceMagic;}
f486
0
toString
public String kafkatest_f487_0()
{    return clientDnsLookup;}
f487
0
createdTimeMs
public long kafkatest_f496_0()
{    return createdTimeMs;}
f496
0
correlationId
public int kafkatest_f497_0()
{    return correlationId;}
f497
0
hasResponse
public boolean kafkatest_f506_0()
{    return responseBody != null;}
f506
0
requestLatencyMs
public long kafkatest_f507_0()
{    return latencyMs;}
f507
0
isBlackedOut
public boolean kafkatest_f516_0(String id, long now)
{    NodeConnectionState state = nodeState.get(id);    return state != null && state.state.isDisconnected() && now - state.lastConnectAttemptMs < state.reconnectBackoffMs;}
f516
0
connectionDelay
public long kafkatest_f517_0(String id, long now)
{    NodeConnectionState state = nodeState.get(id);    if (state == null)        return 0;    if (state.state.isDisconnected()) {        long timeWaited = now - state.lastConnectAttemptMs;        return Math.max(state.reconnectBackoffMs - timeWaited, 0);    } else {        // data acked) will cause a wakeup once data can be sent.        return Long.MAX_VALUE;    }}
f517
0
checkingApiVersions
public void kafkatest_f526_0(String id)
{    NodeConnectionState nodeState = nodeState(id);    nodeState.state = ConnectionState.CHECKING_API_VERSIONS;}
f526
0
ready
public void kafkatest_f527_0(String id)
{    NodeConnectionState nodeState = nodeState(id);    nodeState.state = ConnectionState.READY;    nodeState.authenticationException = null;    resetReconnectBackoff(nodeState);}
f527
0
updateReconnectBackoff
private void kafkatest_f536_0(NodeConnectionState nodeState)
{    if (this.reconnectBackoffMaxMs > this.reconnectBackoffInitMs) {        nodeState.failedAttempts += 1;        double backoffExp = Math.min(nodeState.failedAttempts - 1, this.reconnectBackoffMaxExp);        double backoffFactor = Math.pow(RECONNECT_BACKOFF_EXP_BASE, backoffExp);        long reconnectBackoffMs = (long) (this.reconnectBackoffInitMs * backoffFactor);        // Actual backoff is randomized to avoid connection storms.        double randomFactor = ThreadLocalRandom.current().nextDouble(0.8, 1.2);        nodeState.reconnectBackoffMs = (long) (randomFactor * reconnectBackoffMs);    }}
f536
0
remove
public void kafkatest_f537_0(String id)
{    nodeState.remove(id);}
f537
0
isConnected
public boolean kafkatest_f546_0()
{    return this == CHECKING_API_VERSIONS || this == READY;}
f546
0
postProcessParsedConfig
protected Map<String, Object> kafkatest_f547_0(final Map<String, Object> parsedValues)
{    return CommonClientConfigs.postProcessReconnectBackoffConfigs(this, parsedValues);}
f547
0
groupInstanceId
public Optional<String> kafkatest_f556_0()
{    return groupInstanceId;}
f556
0
subscriptionUserData
 ByteBuffer kafkatest_f557_0(Set<String> topics)
{    return null;}
f557
0
userData
public ByteBuffer kafkatest_f567_0()
{    return userData;}
f567
0
groupSubscription
public Map<String, Subscription> kafkatest_f568_0()
{    return subscriptions;}
f568
0
value
public V kafkatest_f577_0()
{    return value;}
f577
0
offset
public long kafkatest_f578_0()
{    return offset;}
f578
0
records
public Iterable<ConsumerRecord<K, V>> kafkatest_f587_0(String topic)
{    if (topic == null)        throw new IllegalArgumentException("Topic must be non-null.");    List<List<ConsumerRecord<K, V>>> recs = new ArrayList<>();    for (Map.Entry<TopicPartition, List<ConsumerRecord<K, V>>> entry : records.entrySet()) {        if (entry.getKey().topic().equals(topic))            recs.add(entry.getValue());    }    return new ConcatenatedIterable<>(recs);}
f587
0
partitions
public Set<TopicPartition> kafkatest_f588_0()
{    return Collections.unmodifiableSet(records.keySet());}
f588
0
memberData
protected MemberData kafkatest_f597_0(Subscription subscription)
{    return new MemberData(subscription.ownedPartitions(), Optional.empty());}
f597
0
assign
public Map<String, List<TopicPartition>> kafkatest_f598_0(Map<String, Integer> partitionsPerTopic, Map<String, Subscription> subscriptions)
{    final Map<String, List<TopicPartition>> assignments = super.assign(partitionsPerTopic, subscriptions);    adjustAssignment(subscriptions, assignments);    return assignments;}
f598
0
ensureActiveGroup
 boolean kafkatest_f608_0(final Timer timer)
{    // when sending heartbeats and does not necessarily require us to rejoin the group.    if (!ensureCoordinatorReady(timer)) {        return false;    }    startHeartbeatThreadIfNeeded();    return joinGroupIfNeeded(timer);}
f608
0
startHeartbeatThreadIfNeeded
private synchronized void kafkatest_f609_0()
{    if (heartbeatThread == null) {        heartbeatThread = new HeartbeatThread();        heartbeatThread.start();    }}
f609
0
handle
public voidf618_1JoinGroupResponse joinResponse, RequestFuture<ByteBuffer> future)
{    Errors error = joinResponse.error();    if (error == Errors.NONE) {                sensors.joinLatency.record(response.requestLatencyMs());        synchronized (AbstractCoordinator.this) {            if (state != MemberState.REBALANCING) {                // if the consumer was woken up before a rebalance completes, we may have already left                // the group. In this case, we do not want to continue with the sync group.                future.raise(new UnjoinedGroupException());            } else {                AbstractCoordinator.this.generation = new Generation(joinResponse.data().generationId(), joinResponse.data().memberId(), joinResponse.data().protocolName());                if (joinResponse.isLeader()) {                    onJoinLeader(joinResponse).chain(future);                } else {                    onJoinFollower().chain(future);                }            }        }    } else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS) {                // backoff and retry        future.raise(error);    } else if (error == Errors.UNKNOWN_MEMBER_ID) {        // reset the member id and retry immediately        resetGenerationOnResponseError(ApiKeys.JOIN_GROUP, error);                future.raise(error);    } else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) {        // re-discover the coordinator and retry with backoff        markCoordinatorUnknown();                future.raise(error);    } else if (error == Errors.FENCED_INSTANCE_ID) {                future.raise(error);    } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL || error == Errors.INVALID_SESSION_TIMEOUT || error == Errors.INVALID_GROUP_ID || error == Errors.GROUP_AUTHORIZATION_FAILED || error == Errors.GROUP_MAX_SIZE_REACHED) {        // log the error and re-throw the exception                if (error == Errors.GROUP_MAX_SIZE_REACHED) {            future.raise(new GroupMaxSizeReachedException("Consumer group " + rebalanceConfig.groupId + " already has the configured maximum number of members."));        } else if (error == Errors.GROUP_AUTHORIZATION_FAILED) {            future.raise(GroupAuthorizationException.forGroupId(rebalanceConfig.groupId));        } else {            future.raise(error);        }    } else if (error == Errors.UNSUPPORTED_VERSION) {                future.raise(error);    } else if (error == Errors.MEMBER_ID_REQUIRED) {        // and send another join group request in next cycle.        synchronized (AbstractCoordinator.this) {            AbstractCoordinator.this.generation = new Generation(OffsetCommitRequest.DEFAULT_GENERATION_ID, joinResponse.data().memberId(), null);            AbstractCoordinator.this.rejoinNeeded = true;            AbstractCoordinator.this.state = MemberState.UNJOINED;        }        future.raise(error);    } else {        // unexpected error, throw the exception                future.raise(new KafkaException("Unexpected error in join group response: " + error.message()));    }}
public voidf618
1
onJoinFollower
private RequestFuture<ByteBuffer>f619_1)
{    // send follower's sync group with an empty assignment    SyncGroupRequest.Builder requestBuilder = new SyncGroupRequest.Builder(new SyncGroupRequestData().setGroupId(rebalanceConfig.groupId).setMemberId(generation.memberId).setGroupInstanceId(this.rebalanceConfig.groupInstanceId.orElse(null)).setGenerationId(generation.generationId).setAssignments(Collections.emptyList()));        return sendSyncGroupRequest(requestBuilder);}
private RequestFuture<ByteBuffer>f619
1
coordinator
private synchronized Node kafkatest_f628_0()
{    return this.coordinator;}
f628
0
markCoordinatorUnknown
protected synchronized void kafkatest_f629_0()
{    markCoordinatorUnknown(false);}
f629
0
requestRejoin
protected synchronized void kafkatest_f638_0()
{    this.rejoinNeeded = true;}
f638
0
close
public final void kafkatest_f639_0()
{    close(time.timer(0));}
f639
0
createMeter
protected Meter kafkatest_f648_0(Metrics metrics, String groupName, String baseName, String descriptiveName)
{    return new Meter(new WindowedCount(), metrics.metricName(baseName + "-rate", groupName, String.format("The number of %s per second", descriptiveName)), metrics.metricName(baseName + "-total", groupName, String.format("The total number of %s", descriptiveName)));}
f648
0
measure
public double kafkatest_f649_0(MetricConfig config, long now)
{    return TimeUnit.SECONDS.convert(now - heartbeat.lastHeartbeatSend(), TimeUnit.MILLISECONDS);}
f649
0
hasMemberId
public boolean kafkatest_f658_0()
{    return !memberId.isEmpty();}
f658
0
equals
public boolean kafkatest_f659_0(final Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    final Generation that = (Generation) o;    return generationId == that.generationId && Objects.equals(memberId, that.memberId) && Objects.equals(protocol, that.protocol);}
f659
0
hashCode
public int kafkatest_f668_0()
{    return memberId.hashCode();}
f668
0
toString
public String kafkatest_f669_0()
{    return "MemberInfo [member.id: " + memberId + ", group.instance.id: " + groupInstanceId.orElse("{}") + "]";}
f669
0
canParticipateInReassignment
private booleanf678_1String consumer, Map<String, List<TopicPartition>> currentAssignment, Map<String, List<TopicPartition>> consumer2AllPotentialPartitions, Map<TopicPartition, List<String>> partition2AllPotentialConsumers)
{    List<TopicPartition> currentPartitions = currentAssignment.get(consumer);    int currentAssignmentSize = currentPartitions.size();    int maxAssignmentSize = consumer2AllPotentialPartitions.get(consumer).size();    if (currentAssignmentSize > maxAssignmentSize)            if (currentAssignmentSize < maxAssignmentSize)        // if a consumer is not assigned all its potential partitions it is subject to reassignment        return true;    for (TopicPartition partition : currentPartitions) // is subject to reassignment    if (canParticipateInReassignment(partition, partition2AllPotentialConsumers))        return true;    return false;}
private booleanf678
1
balance
private void kafkatest_f679_0(Map<String, List<TopicPartition>> currentAssignment, Map<TopicPartition, ConsumerGenerationPair> prevAssignment, List<TopicPartition> sortedPartitions, List<TopicPartition> unassignedPartitions, TreeSet<String> sortedCurrentSubscriptions, Map<String, List<TopicPartition>> consumer2AllPotentialPartitions, Map<TopicPartition, List<String>> partition2AllPotentialConsumers, Map<TopicPartition, String> currentPartitionConsumer, boolean revocationRequired)
{    boolean initializing = currentAssignment.get(sortedCurrentSubscriptions.last()).isEmpty();    boolean reassignmentPerformed = false;    // assign all unassigned partitions    for (TopicPartition partition : unassignedPartitions) {        // skip if there is no potential consumer for the partition        if (partition2AllPotentialConsumers.get(partition).isEmpty())            continue;        assignPartition(partition, sortedCurrentSubscriptions, currentAssignment, consumer2AllPotentialPartitions, currentPartitionConsumer);    }    // narrow down the reassignment scope to only those partitions that can actually be reassigned    Set<TopicPartition> fixedPartitions = new HashSet<>();    for (TopicPartition partition : partition2AllPotentialConsumers.keySet()) if (!canParticipateInReassignment(partition, partition2AllPotentialConsumers))        fixedPartitions.add(partition);    sortedPartitions.removeAll(fixedPartitions);    unassignedPartitions.removeAll(fixedPartitions);    // narrow down the reassignment scope to only those consumers that are subject to reassignment    Map<String, List<TopicPartition>> fixedAssignments = new HashMap<>();    for (String consumer : consumer2AllPotentialPartitions.keySet()) if (!canParticipateInReassignment(consumer, currentAssignment, consumer2AllPotentialPartitions, partition2AllPotentialConsumers)) {        sortedCurrentSubscriptions.remove(consumer);        fixedAssignments.put(consumer, currentAssignment.remove(consumer));    }    // create a deep copy of the current assignment so we can revert to it if we do not get a more balanced assignment later    Map<String, List<TopicPartition>> preBalanceAssignment = deepCopy(currentAssignment);    Map<TopicPartition, String> preBalancePartitionConsumers = new HashMap<>(currentPartitionConsumer);    // if we don't already need to revoke something due to subscription changes, first try to balance by only moving newly added partitions    if (!revocationRequired) {        performReassignments(unassignedPartitions, currentAssignment, prevAssignment, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer);    }    reassignmentPerformed = performReassignments(sortedPartitions, currentAssignment, prevAssignment, sortedCurrentSubscriptions, consumer2AllPotentialPartitions, partition2AllPotentialConsumers, currentPartitionConsumer);    // make sure we are getting a more balanced assignment; otherwise, revert to previous assignment    if (!initializing && reassignmentPerformed && getBalanceScore(currentAssignment) >= getBalanceScore(preBalanceAssignment)) {        deepCopy(preBalanceAssignment, currentAssignment);        currentPartitionConsumer.clear();        currentPartitionConsumer.putAll(preBalancePartitionConsumers);    }    // add the fixed assignments (those that could not change) back    for (Entry<String, List<TopicPartition>> entry : fixedAssignments.entrySet()) {        String consumer = entry.getKey();        currentAssignment.put(consumer, entry.getValue());        sortedCurrentSubscriptions.add(consumer);    }    fixedAssignments.clear();}
f679
0
compare
public int kafkatest_f688_0(TopicPartition o1, TopicPartition o2)
{    int ret = map.get(o1).size() - map.get(o2).size();    if (ret == 0) {        ret = o1.topic().compareTo(o2.topic());        if (ret == 0)            ret = o1.partition() - o2.partition();    }    return ret;}
f688
0
compare
public int kafkatest_f689_0(String o1, String o2)
{    int ret = map.get(o1).size() - map.get(o2).size();    if (ret == 0)        ret = o1.compareTo(o2);    return ret;}
f689
0
toString
public String kafkatest_f698_0()
{    return this.srcMemberId + "->" + this.dstMemberId;}
f698
0
hashCode
public int kafkatest_f699_0()
{    final int prime = 31;    int result = 1;    result = prime * result + ((this.srcMemberId == null) ? 0 : this.srcMemberId.hashCode());    result = prime * result + ((this.dstMemberId == null) ? 0 : this.dstMemberId.hashCode());    return result;}
f699
0
metadata
protected JoinGroupRequestData.JoinGroupRequestProtocolCollectionf708_1)
{        this.joinedSubscription = subscriptions.subscription();    JoinGroupRequestData.JoinGroupRequestProtocolCollection protocolSet = new JoinGroupRequestData.JoinGroupRequestProtocolCollection();    List<String> topics = new ArrayList<>(joinedSubscription);    for (ConsumerPartitionAssignor assignor : assignors) {        Subscription subscription = new Subscription(topics, assignor.subscriptionUserData(joinedSubscription), subscriptions.assignedPartitionsList());        ByteBuffer metadata = ConsumerProtocol.serializeSubscription(subscription);        protocolSet.add(new JoinGroupRequestData.JoinGroupRequestProtocol().setName(assignor.name()).setMetadata(Utils.toArray(metadata)));    }    return protocolSet;}
protected JoinGroupRequestData.JoinGroupRequestProtocolCollectionf708
1
updatePatternSubscription
public void kafkatest_f709_0(Cluster cluster)
{    final Set<String> topicsToSubscribe = cluster.topics().stream().filter(subscriptions::matchesSubscribedPattern).collect(Collectors.toSet());    if (subscriptions.subscribeFromPattern(topicsToSubscribe))        metadata.requestUpdateForNewTopics();}
f709
0
timeToNextPoll
public long kafkatest_f718_0(long now)
{    if (!autoCommitEnabled)        return timeToNextHeartbeat(now);    return Math.min(nextAutoCommitTimer.remainingMs(), timeToNextHeartbeat(now));}
f718
0
updateGroupSubscription
private void kafkatest_f719_0(Set<String> topics)
{    // which ensures that all metadata changes will eventually be seen    if (this.subscriptions.groupSubscribe(topics))        metadata.requestUpdateForNewTopics();    // we can check after rebalance completion whether anything has changed    if (!client.ensureFreshMetadata(time.timer(Long.MAX_VALUE)))        throw new TimeoutException();    maybeUpdateSubscriptionMetadata();}
f719
0
invokeCompletedOffsetCommitCallbacks
 void kafkatest_f728_0()
{    if (asyncCommitFenced.get()) {        throw new FencedInstanceIdException("Get fenced exception for group.instance.id " + rebalanceConfig.groupInstanceId.orElse("unset_instance_id") + ", current member.id is " + memberId());    }    while (true) {        OffsetCommitCompletion completion = completedOffsetCommits.poll();        if (completion == null) {            break;        }        completion.invoke();    }}
f728
0
commitOffsetsAsync
public void kafkatest_f729_0(final Map<TopicPartition, OffsetAndMetadata> offsets, final OffsetCommitCallback callback)
{    invokeCompletedOffsetCommitCallbacks();    if (!coordinatorUnknown()) {        doCommitOffsetsAsync(offsets, callback);    } else {        // we don't know the current coordinator, so try to find it and then send the commit        // or fail (we don't want recursive retries which can cause offset commits to arrive        // out of order). Note that there may be multiple offset commits chained to the same        // coordinator lookup request. This is fine because the listeners will be invoked in        // the same order that they were added. Note also that AbstractCoordinator prevents        // multiple concurrent coordinator lookup requests.        pendingAsyncCommits.incrementAndGet();        lookupCoordinator().addListener(new RequestFutureListener<Void>() {            @Override            public void onSuccess(Void value) {                pendingAsyncCommits.decrementAndGet();                doCommitOffsetsAsync(offsets, callback);                client.pollNoWakeup();            }            @Override            public void onFailure(RuntimeException e) {                pendingAsyncCommits.decrementAndGet();                completedOffsetCommits.add(new OffsetCommitCompletion(callback, offsets, new RetriableCommitFailedException(e)));            }        });    }    // ensure the commit has a chance to be transmitted (without blocking on its completion).    // Note that commits are treated as heartbeats by the coordinator, so there is no need to    // explicitly allow heartbeats through delayed task execution.    client.pollNoWakeup();}
f729
0
maybeAutoCommitOffsetsSync
private voidf738_1Timer timer)
{    if (autoCommitEnabled) {        Map<TopicPartition, OffsetAndMetadata> allConsumedOffsets = subscriptions.allConsumed();        try {                        if (!commitOffsetsSync(allConsumedOffsets, timer))                        } catch (WakeupException | InterruptException e) {                        // rethrow wakeups since they are triggered by the user            throw e;        } catch (Exception e) {            // consistent with async auto-commit failures, we do not propagate the exception                    }    }}
private voidf738
1
onComplete
public voidf739_1Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception)
{    if (exception != null)        }
public voidf739
1
onConsume
public ConsumerRecords<K, V>f748_1ConsumerRecords<K, V> records)
{    ConsumerRecords<K, V> interceptRecords = records;    for (ConsumerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptRecords = interceptor.onConsume(interceptRecords);        } catch (Exception e) {            // do not propagate interceptor exception, log and continue calling other interceptors                    }    }    return interceptRecords;}
public ConsumerRecords<K, V>f748
1
onCommit
public voidf749_1Map<TopicPartition, OffsetAndMetadata> offsets)
{    for (ConsumerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptor.onCommit(offsets);        } catch (Exception e) {            // do not propagate interceptor exception, just log                    }    }}
public voidf749
1
send
public RequestFuture<ClientResponse> kafkatest_f758_0(Node node, AbstractRequest.Builder<?> requestBuilder)
{    return send(node, requestBuilder, requestTimeoutMs);}
f758
0
send
public RequestFuture<ClientResponse> kafkatest_f759_0(Node node, AbstractRequest.Builder<?> requestBuilder, int requestTimeoutMs)
{    long now = time.milliseconds();    RequestFutureCompletionHandler completionHandler = new RequestFutureCompletionHandler();    ClientRequest clientRequest = client.newClientRequest(node.idString(), requestBuilder, now, true, requestTimeoutMs, completionHandler);    unsent.put(node, clientRequest);    // wakeup the client in case it is blocking in poll so that we can send the queued request    client.wakeup();    return completionHandler.future;}
f759
0
poll
public void kafkatest_f768_0(Timer timer, PollCondition pollCondition)
{    poll(timer, pollCondition, false);}
f768
0
poll
public void kafkatest_f769_0(Timer timer, PollCondition pollCondition, boolean disableWakeup)
{    // there may be handlers which need to be invoked if we woke up the previous call to poll    firePendingCompletedRequests();    lock.lock();    try {        // Handle async disconnects prior to attempting any sends        handlePendingDisconnects();        // send all the requests we can send now        long pollDelayMs = trySend(timer.currentTimeMs());        // handler), the client will be woken up.        if (pendingCompletion.isEmpty() && (pollCondition == null || pollCondition.shouldBlock())) {            // if there are no requests in flight, do not block longer than the retry backoff            long pollTimeout = Math.min(timer.remainingMs(), pollDelayMs);            if (client.inFlightRequestCount() == 0)                pollTimeout = Math.min(pollTimeout, retryBackoffMs);            client.poll(pollTimeout, timer.currentTimeMs());        } else {            client.poll(0, timer.currentTimeMs());        }        timer.update();        // handle any disconnects by failing the active requests. note that disconnects must        // be checked immediately following poll since any subsequent call to client.ready()        // will reset the disconnect status        checkDisconnects(timer.currentTimeMs());        if (!disableWakeup) {            // trigger wakeups after checking for disconnects so that the callbacks will be ready            // to be fired on the next call to poll()            maybeTriggerWakeup();        }        // throw InterruptException if this thread is interrupted        maybeThrowInterruptException();        // try again to send requests since buffer space may have been        // cleared or a connect finished in the poll        trySend(timer.currentTimeMs());        // fail requests that couldn't be sent if they have expired        failExpiredRequests(timer.currentTimeMs());        // clean unsent requests collection to keep the map from growing indefinitely        unsent.clean();    } finally {        lock.unlock();    }    // called without the lock to avoid deadlock potential if handlers need to acquire locks    firePendingCompletedRequests();    metadata.maybeThrowAnyException();}
f769
0
handlePendingDisconnects
private void kafkatest_f778_0()
{    lock.lock();    try {        while (true) {            Node node = pendingDisconnects.poll();            if (node == null)                break;            failUnsentRequests(node, DisconnectException.INSTANCE);            client.disconnect(node.idString());        }    } finally {        lock.unlock();    }}
f778
0
disconnectAsync
public void kafkatest_f779_0(Node node)
{    pendingDisconnects.offer(node);    client.wakeup();}
f779
0
maybeThrowAuthFailure
public void kafkatest_f788_0(Node node)
{    lock.lock();    try {        AuthenticationException exception = client.authenticationException(node);        if (exception != null)            throw exception;    } finally {        lock.unlock();    }}
f788
0
tryConnect
public void kafkatest_f789_0(Node node)
{    lock.lock();    try {        client.ready(node, time.milliseconds());    } finally {        lock.unlock();    }}
f789
0
removeExpiredRequests
private Collection<ClientRequest> kafkatest_f798_0(long now)
{    List<ClientRequest> expiredRequests = new ArrayList<>();    for (ConcurrentLinkedQueue<ClientRequest> requests : unsent.values()) {        Iterator<ClientRequest> requestIterator = requests.iterator();        while (requestIterator.hasNext()) {            ClientRequest request = requestIterator.next();            long elapsedMs = Math.max(0, now - request.createdTimeMs());            if (elapsedMs > request.requestTimeoutMs()) {                expiredRequests.add(request);                requestIterator.remove();            } else                break;        }    }    return expiredRequests;}
f798
0
clean
public void kafkatest_f799_0()
{    // queue after it has been removed from the map    synchronized (unsent) {        Iterator<ConcurrentLinkedQueue<ClientRequest>> iterator = unsent.values().iterator();        while (iterator.hasNext()) {            ConcurrentLinkedQueue<ClientRequest> requests = iterator.next();            if (requests.isEmpty())                iterator.remove();        }    }}
f799
0
deserializeSubscriptionV0
public static Subscription kafkatest_f808_0(ByteBuffer buffer)
{    Struct struct = SUBSCRIPTION_V0.read(buffer);    ByteBuffer userData = struct.getBytes(USER_DATA_KEY_NAME);    List<String> topics = new ArrayList<>();    for (Object topicObj : struct.getArray(TOPICS_KEY_NAME)) topics.add((String) topicObj);    return new Subscription(topics, userData, Collections.emptyList());}
f808
0
deserializeSubscriptionV1
public static Subscription kafkatest_f809_0(ByteBuffer buffer)
{    Struct struct = SUBSCRIPTION_V1.read(buffer);    ByteBuffer userData = struct.getBytes(USER_DATA_KEY_NAME);    List<String> topics = new ArrayList<>();    for (Object topicObj : struct.getArray(TOPICS_KEY_NAME)) topics.add((String) topicObj);    List<TopicPartition> ownedPartitions = new ArrayList<>();    for (Object structObj : struct.getArray(OWNED_PARTITIONS_KEY_NAME)) {        Struct assignment = (Struct) structObj;        String topic = assignment.getString(TOPIC_KEY_NAME);        for (Object partitionObj : assignment.getArray(PARTITIONS_KEY_NAME)) {            ownedPartitions.add(new TopicPartition(topic, (Integer) partitionObj));        }    }    return new Subscription(topics, userData, ownedPartitions);}
f809
0
hasCompletedFetches
protected boolean kafkatest_f818_0()
{    return !completedFetches.isEmpty();}
f818
0
hasAvailableFetches
public boolean kafkatest_f819_0()
{    return completedFetches.stream().anyMatch(fetch -> subscriptions.isFetchable(fetch.partition));}
f819
0
resetOffsetsIfNeeded
public void kafkatest_f828_0()
{    // Raise exception from previous offset fetch if there is one    RuntimeException exception = cachedListOffsetsException.getAndSet(null);    if (exception != null)        throw exception;    Set<TopicPartition> partitions = subscriptions.partitionsNeedingReset(time.milliseconds());    if (partitions.isEmpty())        return;    final Map<TopicPartition, Long> offsetResetTimestamps = new HashMap<>();    for (final TopicPartition partition : partitions) {        Long timestamp = offsetResetStrategyTimestamp(partition);        if (timestamp != null)            offsetResetTimestamps.put(partition, timestamp);    }    resetOffsetsAsync(offsetResetTimestamps);}
f828
0
validateOffsetsIfNeeded
public void kafkatest_f829_0()
{    RuntimeException exception = cachedOffsetForLeaderException.getAndSet(null);    if (exception != null)        throw exception;    // Validate each partition against the current leader and epoch    subscriptions.assignedPartitions().forEach(topicPartition -> {        ConsumerMetadata.LeaderAndEpoch leaderAndEpoch = metadata.leaderAndEpoch(topicPartition);        subscriptions.maybeValidatePositionForCurrentLeader(topicPartition, leaderAndEpoch);    });    // Collect positions needing validation, with backoff    Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate = subscriptions.partitionsNeedingValidation(time.milliseconds()).stream().collect(Collectors.toMap(Function.identity(), subscriptions::position));    validateOffsetsAsync(partitionsToValidate);}
f829
0
resetOffsetsAsync
private voidf838_1Map<TopicPartition, Long> partitionResetTimestamps)
{    Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> timestampsToSearchByNode = groupListOffsetRequests(partitionResetTimestamps, new HashSet<>());    for (Map.Entry<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> entry : timestampsToSearchByNode.entrySet()) {        Node node = entry.getKey();        final Map<TopicPartition, ListOffsetRequest.PartitionData> resetTimestamps = entry.getValue();        subscriptions.setNextAllowedRetry(resetTimestamps.keySet(), time.milliseconds() + requestTimeoutMs);        RequestFuture<ListOffsetResult> future = sendListOffsetRequest(node, resetTimestamps, false);        future.addListener(new RequestFutureListener<ListOffsetResult>() {            @Override            public void onSuccess(ListOffsetResult result) {                if (!result.partitionsToRetry.isEmpty()) {                    subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs);                    metadata.requestUpdate();                }                for (Map.Entry<TopicPartition, ListOffsetData> fetchedOffset : result.fetchedOffsets.entrySet()) {                    TopicPartition partition = fetchedOffset.getKey();                    ListOffsetData offsetData = fetchedOffset.getValue();                    ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition);                    resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData);                }            }            @Override            public void onFailure(RuntimeException e) {                subscriptions.requestFailed(resetTimestamps.keySet(), time.milliseconds() + retryBackoffMs);                metadata.requestUpdate();                if (!(e instanceof RetriableException) && !cachedListOffsetsException.compareAndSet(null, e))                                }        });    }}
private voidf838
1
onSuccess
public void kafkatest_f839_0(ListOffsetResult result)
{    if (!result.partitionsToRetry.isEmpty()) {        subscriptions.requestFailed(result.partitionsToRetry, time.milliseconds() + retryBackoffMs);        metadata.requestUpdate();    }    for (Map.Entry<TopicPartition, ListOffsetData> fetchedOffset : result.fetchedOffsets.entrySet()) {        TopicPartition partition = fetchedOffset.getKey();        ListOffsetData offsetData = fetchedOffset.getValue();        ListOffsetRequest.PartitionData requestedReset = resetTimestamps.get(partition);        resetOffsetIfNeeded(partition, timestampToOffsetResetStrategy(requestedReset.timestamp), offsetData);    }}
f839
0
groupListOffsetRequests
private Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>>f848_1Map<TopicPartition, Long> timestampsToSearch, Set<TopicPartition> partitionsToRetry)
{    final Map<TopicPartition, ListOffsetRequest.PartitionData> partitionDataMap = new HashMap<>();    for (Map.Entry<TopicPartition, Long> entry : timestampsToSearch.entrySet()) {        TopicPartition tp = entry.getKey();        Long offset = entry.getValue();        Optional<MetadataCache.PartitionInfoAndEpoch> currentInfo = metadata.partitionInfoIfCurrent(tp);        if (!currentInfo.isPresent()) {                        metadata.requestUpdate();            partitionsToRetry.add(tp);        } else if (currentInfo.get().partitionInfo().leader() == null) {                        metadata.requestUpdate();            partitionsToRetry.add(tp);        } else if (client.isUnavailable(currentInfo.get().partitionInfo().leader())) {            client.maybeThrowAuthFailure(currentInfo.get().partitionInfo().leader());            // The connection has failed and we need to await the blackout period before we can            // try again. No need to request a metadata update since the disconnect will have            // done so already.                        partitionsToRetry.add(tp);        } else {            partitionDataMap.put(tp, new ListOffsetRequest.PartitionData(offset, Optional.of(currentInfo.get().epoch())));        }    }    return regroupPartitionMapByNode(partitionDataMap);}
private Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>>f848
1
sendListOffsetRequest
private RequestFuture<ListOffsetResult>f849_1final Node node, final Map<TopicPartition, ListOffsetRequest.PartitionData> timestampsToSearch, boolean requireTimestamp)
{    ListOffsetRequest.Builder builder = ListOffsetRequest.Builder.forConsumer(requireTimestamp, isolationLevel).setTargetTimes(timestampsToSearch);        return client.send(node, builder).compose(new RequestFutureAdapter<ClientResponse, ListOffsetResult>() {        @Override        public void onSuccess(ClientResponse response, RequestFuture<ListOffsetResult> future) {            ListOffsetResponse lor = (ListOffsetResponse) response.responseBody();            log.trace("Received ListOffsetResponse {} from broker {}", lor, node);            handleListOffsetResponse(timestampsToSearch, lor, future);        }    });}
private RequestFuture<ListOffsetResult>f849
1
parseRecord
private ConsumerRecord<K, V> kafkatest_f858_0(TopicPartition partition, RecordBatch batch, Record record)
{    try {        long offset = record.offset();        long timestamp = record.timestamp();        Optional<Integer> leaderEpoch = maybeLeaderEpoch(batch.partitionLeaderEpoch());        TimestampType timestampType = batch.timestampType();        Headers headers = new RecordHeaders(record.headers());        ByteBuffer keyBytes = record.key();        byte[] keyByteArray = keyBytes == null ? null : Utils.toArray(keyBytes);        K key = keyBytes == null ? null : this.keyDeserializer.deserialize(partition.topic(), headers, keyByteArray);        ByteBuffer valueBytes = record.value();        byte[] valueByteArray = valueBytes == null ? null : Utils.toArray(valueBytes);        V value = valueBytes == null ? null : this.valueDeserializer.deserialize(partition.topic(), headers, valueByteArray);        return new ConsumerRecord<>(partition.topic(), partition.partition(), offset, timestamp, timestampType, record.checksumOrNull(), keyByteArray == null ? ConsumerRecord.NULL_SIZE : keyByteArray.length, valueByteArray == null ? ConsumerRecord.NULL_SIZE : valueByteArray.length, key, value, headers, leaderEpoch);    } catch (RuntimeException e) {        throw new SerializationException("Error deserializing key/value for partition " + partition + " at offset " + record.offset() + ". If needed, please seek past the record to continue consumption.", e);    }}
f858
0
maybeLeaderEpoch
private Optional<Integer> kafkatest_f859_0(int leaderEpoch)
{    return leaderEpoch == RecordBatch.NO_PARTITION_LEADER_EPOCH ? Optional.empty() : Optional.of(leaderEpoch);}
f859
0
nextFetchedRecord
private Recordf868_1)
{    while (true) {        if (records == null || !records.hasNext()) {            maybeCloseRecordStream();            if (!batches.hasNext()) {                // fetching the same batch repeatedly).                if (currentBatch != null)                    nextFetchOffset = currentBatch.nextOffset();                drain();                return null;            }            currentBatch = batches.next();            lastEpoch = currentBatch.partitionLeaderEpoch() == RecordBatch.NO_PARTITION_LEADER_EPOCH ? Optional.empty() : Optional.of(currentBatch.partitionLeaderEpoch());            maybeEnsureValid(currentBatch);            if (isolationLevel == IsolationLevel.READ_COMMITTED && currentBatch.hasProducerId()) {                // remove from the aborted transaction queue all aborted transactions which have begun                // before the current batch's last offset and add the associated producerIds to the                // aborted producer set                consumeAbortedTransactionsUpTo(currentBatch.lastOffset());                long producerId = currentBatch.producerId();                if (containsAbortMarker(currentBatch)) {                    abortedProducerIds.remove(producerId);                } else if (isBatchAborted(currentBatch)) {                                        nextFetchOffset = currentBatch.nextOffset();                    continue;                }            }            records = currentBatch.streamingIterator(decompressionBufferSupplier);        } else {            Record record = records.next();            // skip any records out of range            if (record.offset() >= nextFetchOffset) {                // we only do validation when the message should not be skipped.                maybeEnsureValid(record);                // control records are not returned to the user                if (!currentBatch.isControlBatch()) {                    return record;                } else {                    // Increment the next fetch offset when we skip a control batch.                    nextFetchOffset = record.offset() + 1;                }            }        }    }}
private Recordf868
1
fetchRecords
private List<ConsumerRecord<K, V>> kafkatest_f869_0(int maxRecords)
{    // Error when fetching the next record before deserialization.    if (corruptLastRecord)        throw new KafkaException("Received exception when fetching the next record from " + partition + ". If needed, please seek past the record to " + "continue consumption.", cachedRecordException);    if (isConsumed)        return Collections.emptyList();    List<ConsumerRecord<K, V>> records = new ArrayList<>();    try {        for (int i = 0; i < maxRecords; i++) {            // use the last record to do deserialization again.            if (cachedRecordException == null) {                corruptLastRecord = true;                lastRecord = nextFetchedRecord();                corruptLastRecord = false;            }            if (lastRecord == null)                break;            records.add(parseRecord(partition, currentBatch, lastRecord));            recordsRead++;            bytesRead += lastRecord.sizeInBytes();            nextFetchOffset = lastRecord.offset() + 1;            // In some cases, the deserialization may have thrown an exception and the retry may succeed,            // we allow user to move forward in this case.            cachedRecordException = null;        }    } catch (SerializationException se) {        cachedRecordException = se;        if (records.isEmpty())            throw se;    } catch (KafkaException e) {        cachedRecordException = e;        if (records.isEmpty())            throw new KafkaException("Received exception when fetching the next record from " + partition + ". If needed, please seek past the record to " + "continue consumption.", e);    }    return records;}
f869
0
maybeUpdateAssignment
private void kafkatest_f878_0(SubscriptionState subscription)
{    int newAssignmentId = subscription.assignmentId();    if (this.assignmentId != newAssignmentId) {        Set<TopicPartition> newAssignedPartitions = subscription.assignedPartitions();        for (TopicPartition tp : this.assignedPartitions) {            if (!newAssignedPartitions.contains(tp)) {                metrics.removeSensor(partitionLagMetricName(tp));                metrics.removeSensor(partitionLeadMetricName(tp));                metrics.removeMetric(partitionPreferredReadReplicaMetricName(tp));            }        }        for (TopicPartition tp : newAssignedPartitions) {            if (!this.assignedPartitions.contains(tp)) {                MetricName metricName = partitionPreferredReadReplicaMetricName(tp);                if (metrics.metric(metricName) == null) {                    metrics.addMetric(metricName, (Gauge<Integer>) (config, now) -> subscription.preferredReadReplica(tp, 0L).orElse(-1));                }            }        }        this.assignedPartitions = newAssignedPartitions;        this.assignmentId = newAssignmentId;    }}
f878
0
recordPartitionLead
private void kafkatest_f879_0(TopicPartition tp, long lead)
{    this.recordsFetchLead.record(lead);    String name = partitionLeadMetricName(tp);    Sensor recordsLead = this.metrics.getSensor(name);    if (recordsLead == null) {        Map<String, String> metricTags = topicPartitionTags(tp);        recordsLead = this.metrics.sensor(name);        recordsLead.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLead, metricTags), new Value());        recordsLead.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLeadMin, metricTags), new Min());        recordsLead.add(this.metrics.metricInstance(metricsRegistry.partitionRecordsLeadAvg, metricTags), new Avg());    }    recordsLead.record(lead);}
f879
0
update
private void kafkatest_f888_0(long now)
{    heartbeatTimer.update(now);    sessionTimer.update(now);    pollTimer.update(now);}
f888
0
poll
public void kafkatest_f889_0(long now)
{    update(now);    pollTimer.reset(maxPollIntervalMs);}
f889
0
resetSessionTimeout
public void kafkatest_f898_0()
{    update(time.milliseconds());    sessionTimer.reset(rebalanceConfig.sessionTimeoutMs);}
f898
0
pollTimeoutExpired
public boolean kafkatest_f899_0(long now)
{    update(now);    return pollTimer.isExpired();}
f899
0
toString
public String kafkatest_f910_0()
{    return "Subscription(" + "topics=" + topics + ')';}
f910
0
partitions
public List<TopicPartition> kafkatest_f911_0()
{    return partitions;}
f911
0
toNewGroupAssignment
private static GroupAssignment kafkatest_f920_0(Map<String, PartitionAssignor.Assignment> oldAssignments)
{    Map<String, Assignment> newAssignments = new HashMap<>();    for (Map.Entry<String, PartitionAssignor.Assignment> entry : oldAssignments.entrySet()) {        String member = entry.getKey();        PartitionAssignor.Assignment oldAssignment = entry.getValue();        newAssignments.put(member, new Assignment(oldAssignment.partitions(), oldAssignment.userData()));    }    return new GroupAssignment(newAssignments);}
f920
0
getAssignorInstances
public static List<ConsumerPartitionAssignor>f921_1List<String> assignorClasses, Map<String, Object> configs)
{    List<ConsumerPartitionAssignor> assignors = new ArrayList<>();    if (assignorClasses == null)        return assignors;    for (Object klass : assignorClasses) {        // first try to get the class if passed in as a string        if (klass instanceof String) {            try {                klass = Class.forName((String) klass, true, Utils.getContextOrKafkaClassLoader());            } catch (ClassNotFoundException classNotFound) {                throw new KafkaException(klass + " ClassNotFoundException exception occurred", classNotFound);            }        }        if (klass instanceof Class<?>) {            Object assignor = Utils.newInstance((Class<?>) klass);            if (assignor instanceof Configurable)                ((Configurable) assignor).configure(configs);            if (assignor instanceof ConsumerPartitionAssignor) {                assignors.add((ConsumerPartitionAssignor) assignor);            } else if (assignor instanceof PartitionAssignor) {                assignors.add(new PartitionAssignorAdapter((PartitionAssignor) assignor));                            } else {                throw new KafkaException(klass + " is not an instance of " + PartitionAssignor.class.getName() + " or an instance of " + ConsumerPartitionAssignor.class.getName());            }        } else {            throw new KafkaException("List contains element of type " + klass.getClass().getName() + ", expected String or Class");        }    }    return assignors;}
public static List<ConsumerPartitionAssignor>f921
1
raise
public void kafkatest_f930_0(RuntimeException e)
{    try {        if (e == null)            throw new IllegalArgumentException("The exception passed to raise must not be null");        if (!result.compareAndSet(INCOMPLETE_SENTINEL, e))            throw new IllegalStateException("Invalid attempt to complete a request future which is already complete");        fireFailure();    } finally {        completedLatch.countDown();    }}
f930
0
raise
public void kafkatest_f931_0(Errors error)
{    raise(error.exception());}
f931
0
onFailure
public void kafkatest_f940_0(RuntimeException e)
{    future.raise(e);}
f940
0
failure
public static RequestFuture<T> kafkatest_f941_0(RuntimeException e)
{    RequestFuture<T> future = new RequestFuture<>();    future.raise(e);    return future;}
f941
0
setSubscriptionType
private void kafkatest_f950_0(SubscriptionType type)
{    if (this.subscriptionType == SubscriptionType.NONE)        this.subscriptionType = type;    else if (this.subscriptionType != type)        throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);}
f950
0
subscribe
public synchronized boolean kafkatest_f951_0(Set<String> topics, ConsumerRebalanceListener listener)
{    registerRebalanceListener(listener);    setSubscriptionType(SubscriptionType.AUTO_TOPICS);    return changeSubscription(topics);}
f951
0
registerRebalanceListener
private void kafkatest_f960_0(ConsumerRebalanceListener listener)
{    if (listener == null)        throw new IllegalArgumentException("RebalanceListener cannot be null");    this.rebalanceListener = listener;}
f960
0
hasPatternSubscription
 synchronized boolean kafkatest_f961_0()
{    return this.subscriptionType == SubscriptionType.AUTO_PATTERN;}
f961
0
assignedStateOrNull
private TopicPartitionState kafkatest_f970_0(TopicPartition tp)
{    return this.assignment.stateValue(tp);}
f970
0
seekValidated
public synchronized void kafkatest_f971_0(TopicPartition tp, FetchPosition position)
{    assignedState(tp).seekValidated(position);}
f971
0
position
public synchronized void kafkatest_f980_0(TopicPartition tp, FetchPosition position)
{    assignedState(tp).position(position);}
f980
0
maybeValidatePositionForCurrentLeader
public synchronized boolean kafkatest_f981_0(TopicPartition tp, Metadata.LeaderAndEpoch leaderAndEpoch)
{    return assignedState(tp).maybeValidatePosition(leaderAndEpoch);}
f981
0
updateLogStartOffset
 synchronized void kafkatest_f990_0(TopicPartition tp, long logStartOffset)
{    assignedState(tp).logStartOffset(logStartOffset);}
f990
0
updateLastStableOffset
 synchronized void kafkatest_f991_0(TopicPartition tp, long lastStableOffset)
{    assignedState(tp).lastStableOffset(lastStableOffset);}
f991
0
hasDefaultOffsetResetPolicy
 boolean kafkatest_f1000_0()
{    return defaultResetStrategy != OffsetResetStrategy.NONE;}
f1000
0
isOffsetResetNeeded
public synchronized boolean kafkatest_f1001_0(TopicPartition partition)
{    return assignedState(partition).awaitingReset();}
f1001
0
isPaused
public synchronized boolean kafkatest_f1010_0(TopicPartition tp)
{    TopicPartitionState assignedOrNull = assignedStateOrNull(tp);    return assignedOrNull != null && assignedOrNull.isPaused();}
f1010
0
isFetchable
 synchronized boolean kafkatest_f1011_0(TopicPartition tp)
{    TopicPartitionState assignedOrNull = assignedStateOrNull(tp);    return assignedOrNull != null && assignedOrNull.isFetchable();}
f1011
0
preferredReadReplica
private Optional<Integer> kafkatest_f1020_0(long timeMs)
{    if (preferredReadReplicaExpireTimeMs != null && timeMs > preferredReadReplicaExpireTimeMs) {        preferredReadReplica = null;        return Optional.empty();    } else {        return Optional.ofNullable(preferredReadReplica);    }}
f1020
0
updatePreferredReadReplica
private void kafkatest_f1021_0(int preferredReadReplica, Supplier<Long> timeMs)
{    if (this.preferredReadReplica == null || preferredReadReplica != this.preferredReadReplica) {        this.preferredReadReplica = preferredReadReplica;        this.preferredReadReplicaExpireTimeMs = timeMs.get();    }}
f1021
0
setNextAllowedRetry
private void kafkatest_f1030_0(long nextAllowedRetryTimeMs)
{    this.nextRetryTimeMs = nextAllowedRetryTimeMs;}
f1030
0
requestFailed
private void kafkatest_f1031_0(long nextAllowedRetryTimeMs)
{    this.nextRetryTimeMs = nextAllowedRetryTimeMs;}
f1031
0
resume
private void kafkatest_f1040_0()
{    this.paused = false;}
f1040
0
isFetchable
private boolean kafkatest_f1041_0()
{    return !paused && hasValidPosition();}
f1041
0
validTransitions
public Collection<FetchState> kafkatest_f1050_0()
{    return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);}
f1050
0
hasPosition
public boolean kafkatest_f1051_0()
{    return true;}
f1051
0
hashCode
public int kafkatest_f1060_0()
{    return Objects.hash(offset, offsetEpoch, currentLeader);}
f1060
0
toString
public String kafkatest_f1061_0()
{    return "FetchPosition{" + "offset=" + offset + ", offsetEpoch=" + offsetEpoch + ", currentLeader=" + currentLeader + '}';}
f1061
0
assign
public voidf1070_1Collection<TopicPartition> partitions)
{    acquireAndEnsureOpen();    try {        if (partitions == null) {            throw new IllegalArgumentException("Topic partition collection to assign to cannot be null");        } else if (partitions.isEmpty()) {            this.unsubscribe();        } else {            for (TopicPartition tp : partitions) {                String topic = (tp != null) ? tp.topic() : null;                if (topic == null || topic.trim().isEmpty())                    throw new IllegalArgumentException("Topic partitions to assign to cannot have null or empty topic");            }            fetcher.clearBufferedDataForUnassignedPartitions(partitions);            // are committed since there will be no following rebalance            if (coordinator != null)                this.coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds());                        if (this.subscriptions.assignFromUser(new HashSet<>(partitions)))                metadata.requestUpdateForNewTopics();        }    } finally {        release();    }}
public voidf1070
1
poll
public ConsumerRecords<K, V> kafkatest_f1071_0(final long timeoutMs)
{    return poll(time.timer(timeoutMs), false);}
f1071
0
commitAsync
public void kafkatest_f1080_0()
{    commitAsync(null);}
f1080
0
commitAsync
public void kafkatest_f1081_0(OffsetCommitCallback callback)
{    commitAsync(subscriptions.allConsumed(), callback);}
f1081
0
committed
public OffsetAndMetadata kafkatest_f1090_0(TopicPartition partition, final Duration timeout)
{    acquireAndEnsureOpen();    try {        maybeThrowInvalidGroupIdException();        Map<TopicPartition, OffsetAndMetadata> offsets = coordinator.fetchCommittedOffsets(Collections.singleton(partition), time.timer(timeout));        if (offsets == null) {            throw new TimeoutException("Timeout of " + timeout.toMillis() + "ms expired before the last " + "committed offset for partition " + partition + " could be determined. Try tuning default.api.timeout.ms " + "larger to relax the threshold.");        } else {            offsets.forEach(this::updateLastSeenEpochIfNewer);            return offsets.get(partition);        }    } finally {        release();    }}
f1090
0
metrics
public Map<MetricName, ? extends Metric> kafkatest_f1091_0()
{    return Collections.unmodifiableMap(this.metrics.metrics());}
f1091
0
offsetsForTimes
public Map<TopicPartition, OffsetAndTimestamp> kafkatest_f1100_0(Map<TopicPartition, Long> timestampsToSearch, Duration timeout)
{    acquireAndEnsureOpen();    try {        for (Map.Entry<TopicPartition, Long> entry : timestampsToSearch.entrySet()) {            // OffsetAndTimestamp is always positive.            if (entry.getValue() < 0)                throw new IllegalArgumentException("The target time for partition " + entry.getKey() + " is " + entry.getValue() + ". The target time cannot be negative.");        }        return fetcher.offsetsForTimes(timestampsToSearch, time.timer(timeout));    } finally {        release();    }}
f1100
0
beginningOffsets
public Map<TopicPartition, Long> kafkatest_f1101_0(Collection<TopicPartition> partitions)
{    return beginningOffsets(partitions, Duration.ofMillis(defaultApiTimeoutMs));}
f1101
0
close
private voidf1110_1long timeoutMs, boolean swallowException)
{    log.trace("Closing the Kafka consumer");    AtomicReference<Throwable> firstException = new AtomicReference<>();    try {        if (coordinator != null)            coordinator.close(time.timer(Math.min(timeoutMs, requestTimeoutMs)));    } catch (Throwable t) {        firstException.compareAndSet(null, t);            }    Utils.closeQuietly(fetcher, "fetcher", firstException);    Utils.closeQuietly(interceptors, "consumer interceptors", firstException);    Utils.closeQuietly(metrics, "consumer metrics", firstException);    Utils.closeQuietly(client, "consumer network client", firstException);    Utils.closeQuietly(keyDeserializer, "consumer key deserializer", firstException);    Utils.closeQuietly(valueDeserializer, "consumer value deserializer", firstException);    AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics);        Throwable exception = firstException.get();    if (exception != null && !swallowException) {        if (exception instanceof InterruptException) {            throw (InterruptException) exception;        }        throw new KafkaException("Failed to close kafka consumer", exception);    }}
private voidf1110
1
updateFetchPositions
private boolean kafkatest_f1111_0(final Timer timer)
{    // If any partitions have been truncated due to a leader change, we need to validate the offsets    fetcher.validateOffsetsIfNeeded();    cachedSubscriptionHashAllFetchPositions = subscriptions.hasAllFetchPositions();    if (cachedSubscriptionHashAllFetchPositions)        return true;    // by always ensuring that assigned partitions have an initial position.    if (coordinator != null && !coordinator.refreshCommittedOffsetsIfNeeded(timer))        return false;    // If there are partitions still needing a position and a reset policy is defined,    // request reset using the default policy. If no reset strategy is defined and there    // are partitions with a missing position, then we will raise an exception.    subscriptions.resetMissingPositions();    // Finally send an asynchronous request to lookup and update the positions of any    // partitions which are awaiting reset.    fetcher.resetOffsetsIfNeeded();    return true;}
f1111
0
assignment
public synchronized Set<TopicPartition> kafkatest_f1120_0()
{    return this.subscriptions.assignedPartitions();}
f1120
0
rebalance
public synchronized void kafkatest_f1121_0(Collection<TopicPartition> newAssignment)
{    // TODO: Rebalance callbacks    this.records.clear();    this.subscriptions.assignFromSubscribed(newAssignment);}
f1121
0
poll
public synchronized ConsumerRecords<K, V> kafkatest_f1130_0(final Duration timeout)
{    ensureNotClosed();    // the callback    synchronized (pollTasks) {        Runnable task = pollTasks.poll();        if (task != null)            task.run();    }    if (wakeup.get()) {        wakeup.set(false);        throw new WakeupException();    }    if (pollException != null) {        RuntimeException exception = this.pollException;        this.pollException = null;        throw exception;    }    // Handle seeks that need to wait for a poll() call to be processed    for (TopicPartition tp : subscriptions.assignedPartitions()) if (!subscriptions.hasValidPosition(tp))        updateFetchPosition(tp);    // update the consumed offset    final Map<TopicPartition, List<ConsumerRecord<K, V>>> results = new HashMap<>();    for (Map.Entry<TopicPartition, List<ConsumerRecord<K, V>>> entry : this.records.entrySet()) {        if (!subscriptions.isPaused(entry.getKey())) {            final List<ConsumerRecord<K, V>> recs = entry.getValue();            for (final ConsumerRecord<K, V> rec : recs) {                long position = subscriptions.position(entry.getKey()).offset;                if (beginningOffsets.get(entry.getKey()) != null && beginningOffsets.get(entry.getKey()) > position) {                    throw new OffsetOutOfRangeException(Collections.singletonMap(entry.getKey(), position));                }                if (assignment().contains(entry.getKey()) && rec.offset() >= position) {                    results.computeIfAbsent(entry.getKey(), partition -> new ArrayList<>()).add(rec);                    SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition(rec.offset() + 1, rec.leaderEpoch(), new Metadata.LeaderAndEpoch(Node.noNode(), rec.leaderEpoch()));                    subscriptions.position(entry.getKey(), newPosition);                }            }        }    }    this.records.clear();    return new ConsumerRecords<>(results);}
f1130
0
addRecord
public synchronized void kafkatest_f1131_0(ConsumerRecord<K, V> record)
{    ensureNotClosed();    TopicPartition tp = new TopicPartition(record.topic(), record.partition());    Set<TopicPartition> currentAssigned = this.subscriptions.assignedPartitions();    if (!currentAssigned.contains(tp))        throw new IllegalStateException("Cannot add records for a partition that is not assigned to the consumer");    List<ConsumerRecord<K, V>> recs = this.records.computeIfAbsent(tp, k -> new ArrayList<>());    recs.add(record);}
f1131
0
commitSync
public synchronized void kafkatest_f1140_0(Duration timeout)
{    commitSync(this.subscriptions.allConsumed());}
f1140
0
commitSync
public void kafkatest_f1141_0(Map<TopicPartition, OffsetAndMetadata> offsets, final Duration timeout)
{    commitSync(offsets);}
f1141
0
seekToEnd
public synchronized void kafkatest_f1150_0(Collection<TopicPartition> partitions)
{    ensureNotClosed();    subscriptions.requestOffsetReset(partitions, OffsetResetStrategy.LATEST);}
f1150
0
addEndOffsets
public synchronized void kafkatest_f1151_0(final Map<TopicPartition, Long> newOffsets)
{    innerUpdateEndOffsets(newOffsets, false);}
f1151
0
offsetsForTimes
public synchronized Map<TopicPartition, OffsetAndTimestamp> kafkatest_f1160_0(Map<TopicPartition, Long> timestampsToSearch)
{    throw new UnsupportedOperationException("Not implemented yet.");}
f1160
0
beginningOffsets
public synchronized Map<TopicPartition, Long> kafkatest_f1161_0(Collection<TopicPartition> partitions)
{    if (offsetsException != null) {        RuntimeException exception = this.offsetsException;        this.offsetsException = null;        throw exception;    }    Map<TopicPartition, Long> result = new HashMap<>();    for (TopicPartition tp : partitions) {        Long beginningOffset = beginningOffsets.get(tp);        if (beginningOffset == null)            throw new IllegalStateException("The partition " + tp + " does not have a beginning offset.");        result.put(tp, beginningOffset);    }    return result;}
f1161
0
ensureNotClosed
private void kafkatest_f1170_0()
{    if (this.closed)        throw new IllegalStateException("This consumer has already been closed.");}
f1170
0
updateFetchPosition
private void kafkatest_f1171_0(TopicPartition tp)
{    if (subscriptions.isOffsetResetNeeded(tp)) {        resetOffsetPosition(tp);    } else if (!committed.containsKey(tp)) {        subscriptions.requestOffsetReset(tp);        resetOffsetPosition(tp);    } else {        subscriptions.seek(tp, committed.get(tp).offset());    }}
f1171
0
partition
public TopicPartition kafkatest_f1180_0()
{    return partitions.isEmpty() ? null : partitions.iterator().next();}
f1180
0
partitions
public Set<TopicPartition> kafkatest_f1181_0()
{    return partitions;}
f1181
0
leaderEpoch
public Optional<Integer> kafkatest_f1190_0()
{    return leaderEpoch;}
f1190
0
toString
public String kafkatest_f1191_0()
{    return "(timestamp=" + timestamp + ", leaderEpoch=" + leaderEpoch.orElse(null) + ", offset=" + offset + ")";}
f1191
0
assign
public Map<String, List<TopicPartition>> kafkatest_f1200_0(Map<String, Integer> partitionsPerTopic, Map<String, Subscription> subscriptions)
{    Map<String, List<TopicPartition>> assignment = new HashMap<>();    List<MemberInfo> memberInfoList = new ArrayList<>();    for (Map.Entry<String, Subscription> memberSubscription : subscriptions.entrySet()) {        assignment.put(memberSubscription.getKey(), new ArrayList<>());        memberInfoList.add(new MemberInfo(memberSubscription.getKey(), memberSubscription.getValue().groupInstanceId()));    }    CircularIterator<MemberInfo> assigner = new CircularIterator<>(Utils.sorted(memberInfoList));    for (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) {        final String topic = partition.topic();        while (!subscriptions.get(assigner.peek().memberId).topics().contains(topic)) assigner.next();        assignment.get(assigner.next().memberId).add(partition);    }    return assignment;}
f1200
0
allPartitionsSorted
private List<TopicPartition> kafkatest_f1201_0(Map<String, Integer> partitionsPerTopic, Map<String, Subscription> subscriptions)
{    SortedSet<String> topics = new TreeSet<>();    for (Subscription subscription : subscriptions.values()) topics.addAll(subscription.topics());    List<TopicPartition> allPartitions = new ArrayList<>();    for (String topic : topics) {        Integer numPartitionsForTopic = partitionsPerTopic.get(topic);        if (numPartitionsForTopic != null)            allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));    }    return allPartitions;}
f1201
0
toForget
public List<TopicPartition> kafkatest_f1210_0()
{    return toForget;}
f1210
0
sessionPartitions
public Map<TopicPartition, PartitionData> kafkatest_f1211_0()
{    return sessionPartitions;}
f1211
0
verifyIncrementalFetchResponsePartitions
private String kafkatest_f1220_0(FetchResponse<?> response)
{    Set<TopicPartition> extra = findMissing(response.responseData().keySet(), sessionPartitions.keySet());    if (!extra.isEmpty()) {        StringBuilder bld = new StringBuilder();        bld.append("extra=(").append(Utils.join(extra, ", ")).append("), ");        bld.append("response=(").append(Utils.join(response.responseData().keySet(), ", ")).append("), ");        return bld.toString();    }    return null;}
f1220
0
responseDataToLogString
private String kafkatest_f1221_0(FetchResponse<?> response)
{    if (!log.isTraceEnabled()) {        int implied = sessionPartitions.size() - response.responseData().size();        if (implied > 0) {            return String.format(" with %d response partition(s), %d implied partition(s)", response.responseData().size(), implied);        } else {            return String.format(" with %d response partition(s)", response.responseData().size());        }    }    StringBuilder bld = new StringBuilder();    bld.append(" with response=(").append(Utils.join(response.responseData().keySet(), ", ")).append(")");    String prefix = ", implied=(";    String suffix = "";    for (TopicPartition partition : sessionPartitions.keySet()) {        if (!response.responseData().containsKey(partition)) {            bld.append(prefix);            bld.append(partition);            prefix = ", ";            suffix = ")";        }    }    bld.append(suffix);    return bld.toString();}
f1221
0
canSendMore
public boolean kafkatest_f1230_0(String node)
{    Deque<NetworkClient.InFlightRequest> queue = requests.get(node);    return queue == null || queue.isEmpty() || (queue.peekFirst().send.completed() && queue.size() < this.maxInFlightRequestsPerConnection);}
f1230
0
count
public int kafkatest_f1231_0(String node)
{    Deque<NetworkClient.InFlightRequest> queue = requests.get(node);    return queue == null ? 0 : queue.size();}
f1231
0
isUpdateDue
public boolean kafkatest_f1240_0(long now)
{    return false;}
f1240
0
maybeUpdate
public long kafkatest_f1241_0(long now)
{    return Long.MAX_VALUE;}
f1241
0
requestUpdate
public synchronized int kafkatest_f1251_0()
{    this.needUpdate = true;    return this.updateVersion;}
f1251
0
updateLastSeenEpochIfNewer
public synchronized boolean kafkatest_f1252_0(TopicPartition topicPartition, int leaderEpoch)
{    Objects.requireNonNull(topicPartition, "TopicPartition cannot be null");    return updateLastSeenEpoch(topicPartition, leaderEpoch, oldEpoch -> leaderEpoch > oldEpoch, true);}
f1252
0
checkInvalidTopics
private voidf1261_1Cluster cluster)
{    if (!cluster.invalidTopics().isEmpty()) {                invalidTopics = new HashSet<>(cluster.invalidTopics());    }}
private voidf1261
1
checkUnauthorizedTopics
private voidf1262_1Cluster cluster)
{    if (!cluster.unauthorizedTopics().isEmpty()) {                unauthorizedTopics = new HashSet<>(cluster.unauthorizedTopics());    }}
private voidf1262
1
clearRecoverableErrors
private void kafkatest_f1271_0()
{    invalidTopics = Collections.emptySet();    unauthorizedTopics = Collections.emptySet();}
f1271
0
failedUpdate
public synchronized void kafkatest_f1272_0(long now, KafkaException fatalException)
{    this.lastRefreshMs = now;    this.fatalException = fatalException;}
f1272
0
leaderAndEpoch
public synchronized LeaderAndEpoch kafkatest_f1281_0(TopicPartition tp)
{    return partitionInfoIfCurrent(tp).map(infoAndEpoch -> {        Node leader = infoAndEpoch.partitionInfo().leader();        return new LeaderAndEpoch(leader == null ? Node.noNode() : leader, Optional.of(infoAndEpoch.epoch()));    }).orElse(new LeaderAndEpoch(Node.noNode(), lastSeenLeaderEpoch(tp)));}
f1281
0
noLeaderOrEpoch
public static LeaderAndEpoch kafkatest_f1282_0()
{    return NO_LEADER_OR_EPOCH;}
f1282
0
empty
 static MetadataCache kafkatest_f1291_0()
{    return new MetadataCache(null, Collections.emptyList(), Collections.emptyList(), Collections.emptySet(), Collections.emptySet(), Collections.emptySet(), null, Cluster.empty());}
f1291
0
toString
public String kafkatest_f1292_0()
{    return "MetadataCache{" + "clusterId='" + clusterId + '\'' + ", nodes=" + nodes + ", partitions=" + metadataByPartition.values() + ", controller=" + controller + '}';}
f1292
0
close
public void kafkatest_f1301_0(String nodeId)
{    selector.close(nodeId);    for (InFlightRequest request : inFlightRequests.clearAll(nodeId)) if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA)        metadataUpdater.handleDisconnection(request.destination);    connectionStates.remove(nodeId);}
f1301
0
connectionDelay
public long kafkatest_f1302_0(Node node, long now)
{    return connectionStates.connectionDelay(node.idString(), now);}
f1302
0
doSend
private voidf1311_1ClientRequest clientRequest, boolean isInternalRequest, long now)
{    ensureActive();    String nodeId = clientRequest.destination();    if (!isInternalRequest) {        // READY state.)        if (!canSendRequest(nodeId, now))            throw new IllegalStateException("Attempt to send a request to node " + nodeId + " which is not ready.");    }    AbstractRequest.Builder<?> builder = clientRequest.requestBuilder();    try {        NodeApiVersions versionInfo = apiVersions.get(nodeId);        short version;        // information itself.  It is also the case when discoverBrokerVersions is set to false.        if (versionInfo == null) {            version = builder.latestAllowedVersion();            if (discoverBrokerVersions && log.isTraceEnabled())                log.trace("No version information found when sending {} with correlation id {} to node {}. " + "Assuming version {}.", clientRequest.apiKey(), clientRequest.correlationId(), nodeId, version);        } else {            version = versionInfo.latestUsableVersion(clientRequest.apiKey(), builder.oldestAllowedVersion(), builder.latestAllowedVersion());        }        // The call to build may also throw UnsupportedVersionException, if there are essential        // fields that cannot be represented in the chosen version.        doSend(clientRequest, isInternalRequest, now, builder.build(version));    } catch (UnsupportedVersionException unsupportedVersionException) {        // If the version is not supported, skip sending the request over the wire.        // Instead, simply add it to the local queue of aborted requests.                ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(builder.latestAllowedVersion()), clientRequest.callback(), clientRequest.destination(), now, now, false, unsupportedVersionException, null, null);        abortedSends.add(clientResponse);        if (isInternalRequest && clientRequest.apiKey() == ApiKeys.METADATA)            metadataUpdater.handleFatalException(unsupportedVersionException);    }}
private voidf1311
1
doSend
private voidf1312_1ClientRequest clientRequest, boolean isInternalRequest, long now, AbstractRequest request)
{    String destination = clientRequest.destination();    RequestHeader header = clientRequest.makeHeader(request.version());    if (log.isDebugEnabled()) {        int latestClientVersion = clientRequest.apiKey().latestVersion();        if (header.apiVersion() == latestClientVersion) {            log.trace("Sending {} {} with correlation id {} to node {}", clientRequest.apiKey(), request, clientRequest.correlationId(), destination);        } else {                    }    }    Send send = request.toSend(destination, header);    InFlightRequest inFlightRequest = new InFlightRequest(clientRequest, header, isInternalRequest, request, send, now);    this.inFlightRequests.add(inFlightRequest);    selector.send(send);}
private voidf1312
1
initiateClose
public void kafkatest_f1321_0()
{    if (state.compareAndSet(State.ACTIVE, State.CLOSING)) {        wakeup();    }}
f1321
0
active
public boolean kafkatest_f1322_0()
{    return state.get() == State.ACTIVE;}
f1322
0
handleCompletedSends
private void kafkatest_f1331_0(List<ClientResponse> responses, long now)
{    // if no response is expected then when the send is completed, return it    for (Send send : this.selector.completedSends()) {        InFlightRequest request = this.inFlightRequests.lastSent(send.destination());        if (!request.expectResponse) {            this.inFlightRequests.completeLastSent(send.destination());            responses.add(request.completed(null, now));        }    }}
f1331
0
maybeThrottle
private void kafkatest_f1332_0(AbstractResponse response, short apiVersion, String nodeId, long now)
{    int throttleTimeMs = response.throttleTimeMs();    if (throttleTimeMs > 0 && response.shouldClientThrottle(apiVersion)) {        connectionStates.throttle(nodeId, now + throttleTimeMs);        log.trace("Connection to node {} is throttled for {} ms until timestamp {}", nodeId, throttleTimeMs, now + throttleTimeMs);    }}
f1332
0
isUpdateDue
public boolean kafkatest_f1341_0(long now)
{    return !hasFetchInProgress() && this.metadata.timeToNextUpdate(now) == 0;}
f1341
0
hasFetchInProgress
private boolean kafkatest_f1342_0()
{    return inProgressRequestVersion != null;}
f1342
0
newClientRequest
public ClientRequest kafkatest_f1351_0(String nodeId, AbstractRequest.Builder<?> requestBuilder, long createdTimeMs, boolean expectResponse)
{    return newClientRequest(nodeId, requestBuilder, createdTimeMs, expectResponse, defaultRequestTimeoutMs, null);}
f1351
0
newClientRequest
public ClientRequest kafkatest_f1352_0(String nodeId, AbstractRequest.Builder<?> requestBuilder, long createdTimeMs, boolean expectResponse, int requestTimeoutMs, RequestCompletionHandler callback)
{    return new ClientRequest(nodeId, requestBuilder, correlation++, clientId, createdTimeMs, expectResponse, requestTimeoutMs, callback);}
f1352
0
create
public static NodeApiVersions kafkatest_f1361_0(Collection<ApiVersion> overrides)
{    List<ApiVersion> apiVersions = new LinkedList<>(overrides);    for (ApiKeys apiKey : ApiKeys.values()) {        boolean exists = false;        for (ApiVersion apiVersion : apiVersions) {            if (apiVersion.apiKey == apiKey.id) {                exists = true;                break;            }        }        if (!exists) {            apiVersions.add(new ApiVersion(apiKey));        }    }    return new NodeApiVersions(apiVersions);}
f1361
0
latestUsableVersion
public short kafkatest_f1362_0(ApiKeys apiKey)
{    return latestUsableVersion(apiKey, apiKey.oldestVersion(), apiKey.latestVersion());}
f1362
0
safeAllocateByteBuffer
private ByteBuffer kafkatest_f1371_0(int size)
{    boolean error = true;    try {        ByteBuffer buffer = allocateByteBuffer(size);        error = false;        return buffer;    } finally {        if (error) {            this.lock.lock();            try {                this.nonPooledAvailableMemory += size;                if (!this.waiters.isEmpty())                    this.waiters.peekFirst().signal();            } finally {                this.lock.unlock();            }        }    }}
f1371
0
allocateByteBuffer
protected ByteBuffer kafkatest_f1372_0(int size)
{    return ByteBuffer.allocate(size);}
f1372
0
totalMemory
public long kafkatest_f1381_0()
{    return this.totalMemory;}
f1381
0
waiters
 Deque<Condition> kafkatest_f1382_0()
{    return this.waiters;}
f1382
0
valueOrError
 RecordMetadata kafkatest_f1393_0() throws ExecutionException
{    if (this.result.error() != null)        throw new ExecutionException(this.result.error());    else        return value();}
f1393
0
checksumOrNull
 Long kafkatest_f1394_0()
{    return this.checksum;}
f1394
0
tryAppendForSplit
private boolean kafkatest_f1403_0(long timestamp, ByteBuffer key, ByteBuffer value, Header[] headers, Thunk thunk)
{    if (!recordsBuilder.hasRoomFor(timestamp, key, value, headers)) {        return false;    } else {        // No need to get the CRC.        this.recordsBuilder.append(timestamp, key, value, headers);        this.maxRecordSize = Math.max(this.maxRecordSize, AbstractRecords.estimateSizeInBytesUpperBound(magic(), recordsBuilder.compressionType(), key, value, headers));        FutureRecordMetadata future = new FutureRecordMetadata(this.produceFuture, this.recordCount, timestamp, thunk.future.checksumOrNull(), key == null ? -1 : key.remaining(), value == null ? -1 : value.remaining(), Time.SYSTEM);        // Chain the future to the original thunk.        thunk.future.chain(future);        this.thunks.add(thunk);        this.recordCount++;        return true;    }}
f1403
0
abort
public void kafkatest_f1404_0(RuntimeException exception)
{    if (!finalState.compareAndSet(null, FinalState.ABORTED))        throw new IllegalStateException("Batch has already been completed in final state " + finalState.get());    log.trace("Aborting batch for partition {}", topicPartition, exception);    completeFutureAndFireCallbacks(ProduceResponse.INVALID_OFFSET, RecordBatch.NO_TIMESTAMP, exception);}
f1404
0
finalState
public FinalState kafkatest_f1413_0()
{    return this.finalState.get();}
f1413
0
attempts
 int kafkatest_f1414_0()
{    return attempts.get();}
f1414
0
compressionRatio
public double kafkatest_f1423_0()
{    return recordsBuilder.compressionRatio();}
f1423
0
isFull
public boolean kafkatest_f1424_0()
{    return recordsBuilder.isFull();}
f1424
0
isWritable
public boolean kafkatest_f1433_0()
{    return !recordsBuilder.isClosed();}
f1433
0
magic
public byte kafkatest_f1434_0()
{    return recordsBuilder.magic();}
f1434
0
await
public void kafkatest_f1443_0() throws InterruptedException
{    latch.await();}
f1443
0
await
public boolean kafkatest_f1444_0(long timeout, TimeUnit unit) throws InterruptedException
{    return latch.await(timeout, unit);}
f1444
0
onSend
public ProducerRecord<K, V>f1453_1ProducerRecord<K, V> record)
{    ProducerRecord<K, V> interceptRecord = record;    for (ProducerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptRecord = interceptor.onSend(interceptRecord);        } catch (Exception e) {            // be careful not to throw exception from here            if (record != null)                            else                        }    }    return interceptRecord;}
public ProducerRecord<K, V>f1453
1
onAcknowledgement
public voidf1454_1RecordMetadata metadata, Exception exception)
{    for (ProducerInterceptor<K, V> interceptor : this.interceptors) {        try {            interceptor.onAcknowledgement(metadata, exception);        } catch (Exception e) {            // do not propagate interceptor exceptions, just log                    }    }}
public voidf1454
1
update
public synchronized void kafkatest_f1463_0(int requestVersion, MetadataResponse response, long now)
{    super.update(requestVersion, response, now);    notifyAll();}
f1463
0
failedUpdate
public synchronized void kafkatest_f1464_0(long now, KafkaException fatalException)
{    super.failedUpdate(now, fatalException);    if (fatalException != null)        notifyAll();}
f1464
0
recordsBuilder
private MemoryRecordsBuilder kafkatest_f1473_0(ByteBuffer buffer, byte maxUsableMagic)
{    if (transactionManager != null && maxUsableMagic < RecordBatch.MAGIC_VALUE_V2) {        throw new UnsupportedVersionException("Attempting to use idempotence with a broker which does not " + "support the required message format (v2). The broker must be version 0.11 or later.");    }    return MemoryRecords.builder(buffer, maxUsableMagic, compression, TimestampType.CREATE_TIME, 0L);}
f1473
0
tryAppend
private RecordAppendResult kafkatest_f1474_0(long timestamp, byte[] key, byte[] value, Header[] headers, Callback callback, Deque<ProducerBatch> deque)
{    ProducerBatch last = deque.peekLast();    if (last != null) {        FutureRecordMetadata future = last.tryAppend(timestamp, key, value, headers, callback, time.milliseconds());        if (future == null)            last.closeForRecordAppends();        else            return new RecordAppendResult(future, deque.size() > 1 || last.isFull(), false, false);    }    return null;}
f1474
0
ready
public ReadyCheckResult kafkatest_f1483_0(Cluster cluster, long nowMs)
{    Set<Node> readyNodes = new HashSet<>();    long nextReadyCheckDelayMs = Long.MAX_VALUE;    Set<String> unknownLeaderTopics = new HashSet<>();    boolean exhausted = this.free.queued() > 0;    for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {        Deque<ProducerBatch> deque = entry.getValue();        synchronized (deque) {            // When producing to a large number of partitions, this path is hot and deques are often empty.            // We check whether a batch exists first to avoid the more expensive checks whenever possible.            ProducerBatch batch = deque.peekFirst();            if (batch != null) {                TopicPartition part = entry.getKey();                Node leader = cluster.leaderFor(part);                if (leader == null) {                    // This is a partition for which leader is not known, but messages are available to send.                    // Note that entries are currently not removed from batches when deque is empty.                    unknownLeaderTopics.add(part.topic());                } else if (!readyNodes.contains(leader) && !isMuted(part, nowMs)) {                    long waitedTimeMs = batch.waitedTimeMs(nowMs);                    boolean backingOff = batch.attempts() > 0 && waitedTimeMs < retryBackoffMs;                    long timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;                    boolean full = deque.size() > 1 || batch.isFull();                    boolean expired = waitedTimeMs >= timeToWaitMs;                    boolean sendable = full || expired || exhausted || closed || flushInProgress();                    if (sendable && !backingOff) {                        readyNodes.add(leader);                    } else {                        long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);                        // Note that this results in a conservative estimate since an un-sendable partition may have                        // a leader that will later be found to have sendable data. However, this is good enough                        // since we'll just wake up and then sleep again for the remaining time.                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);                    }                }            }        }    }    return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);}
f1483
0
hasUndrained
public boolean kafkatest_f1484_0()
{    for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {        Deque<ProducerBatch> deque = entry.getValue();        synchronized (deque) {            if (!deque.isEmpty())                return true;        }    }    return false;}
f1484
0
flushInProgress
 boolean kafkatest_f1493_0()
{    return flushesInProgress.get() > 0;}
f1493
0
batches
 Map<TopicPartition, Deque<ProducerBatch>> kafkatest_f1494_0()
{    return Collections.unmodifiableMap(batches);}
f1494
0
mutePartition
public void kafkatest_f1503_0(TopicPartition tp)
{    muted.put(tp, Long.MAX_VALUE);}
f1503
0
unmutePartition
public void kafkatest_f1504_0(TopicPartition tp, long throttleUntilTimeMs)
{    muted.put(tp, throttleUntilTimeMs);}
f1504
0
run
public voidf1513_1)
{        // main loop, runs until close is called    while (running) {        try {            runOnce();        } catch (Exception e) {                    }    }        // wait until these are completed.    while (!forceClose && ((this.accumulator.hasUndrained() || this.client.inFlightRequestCount() > 0) || hasPendingTransactionalRequests())) {        try {            runOnce();        } catch (Exception e) {                    }    }    // Abort the transaction if any commit or abort didn't go through the transaction manager's queue    while (!forceClose && transactionManager != null && transactionManager.hasOngoingTransaction()) {        if (!transactionManager.isCompleting()) {                        transactionManager.beginAbort();        }        try {            runOnce();        } catch (Exception e) {                    }    }    if (forceClose) {        // the futures.        if (transactionManager != null) {                        transactionManager.close();        }                this.accumulator.abortIncompleteBatches();    }    try {        this.client.close();    } catch (Exception e) {            }    }
public voidf1513
1
runOnce
 void kafkatest_f1514_0()
{    if (transactionManager != null) {        try {            transactionManager.resetProducerIdIfNeeded();            if (!transactionManager.isTransactional()) {                // this is an idempotent producer, so make sure we have a producer id                maybeWaitForProducerId();            } else if (transactionManager.hasUnresolvedSequences() && !transactionManager.hasFatalError()) {                transactionManager.transitionToFatalError(new KafkaException("The client hasn't received acknowledgment for " + "some previously sent messages and can no longer retry them. It isn't safe to continue."));            } else if (maybeSendAndPollTransactionalRequest()) {                return;            }            // is no producer id (for the idempotent case).            if (transactionManager.hasFatalError() || !transactionManager.hasProducerId()) {                RuntimeException lastError = transactionManager.lastError();                if (lastError != null)                    maybeAbortBatches(lastError);                client.poll(retryBackoffMs, time.milliseconds());                return;            } else if (transactionManager.hasAbortableError()) {                accumulator.abortUndrainedBatches(transactionManager.lastError());            }        } catch (AuthenticationException e) {            // This is already logged as error, but propagated here to perform any clean ups.            log.trace("Authentication exception while processing transactional request: {}", e);            transactionManager.authenticationFailed(e);        }    }    long currentTimeMs = time.milliseconds();    long pollTimeout = sendProducerData(currentTimeMs);    client.poll(pollTimeout, currentTimeMs);}
f1514
0
awaitNodeReady
private Node kafkatest_f1523_0(FindCoordinatorRequest.CoordinatorType coordinatorType) throws IOException
{    Node node = coordinatorType != null ? transactionManager.coordinator(coordinatorType) : client.leastLoadedNode(time.milliseconds());    if (node != null && NetworkClientUtils.awaitReady(client, node, time, requestTimeoutMs)) {        return node;    }    return null;}
f1523
0
maybeWaitForProducerId
private voidf1524_1)
{    while (!forceClose && !transactionManager.hasProducerId() && !transactionManager.hasError()) {        Node node = null;        try {            node = awaitNodeReady(null);            if (node != null) {                ClientResponse response = sendAndAwaitInitProducerIdRequest(node);                InitProducerIdResponse initProducerIdResponse = (InitProducerIdResponse) response.responseBody();                Errors error = initProducerIdResponse.error();                if (error == Errors.NONE) {                    ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(initProducerIdResponse.data.producerId(), initProducerIdResponse.data.producerEpoch());                    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);                    return;                } else if (error.exception() instanceof RetriableException) {                                    } else {                    transactionManager.transitionToFatalError(error.exception());                    break;                }            } else {                            }        } catch (UnsupportedVersionException e) {            transactionManager.transitionToFatalError(e);            break;        } catch (IOException e) {                    }        log.trace("Retry InitProducerIdRequest in {}ms.", retryBackoffMs);        time.sleep(retryBackoffMs);        metadata.requestUpdate();    }}
private voidf1524
1
sendProduceRequest
private void kafkatest_f1533_0(long now, int destination, short acks, int timeout, List<ProducerBatch> batches)
{    if (batches.isEmpty())        return;    Map<TopicPartition, MemoryRecords> produceRecordsByPartition = new HashMap<>(batches.size());    final Map<TopicPartition, ProducerBatch> recordsByPartition = new HashMap<>(batches.size());    // find the minimum magic version used when creating the record sets    byte minUsedMagic = apiVersions.maxUsableProduceMagic();    for (ProducerBatch batch : batches) {        if (batch.magic() < minUsedMagic)            minUsedMagic = batch.magic();    }    for (ProducerBatch batch : batches) {        TopicPartition tp = batch.topicPartition;        MemoryRecords records = batch.records();        // which is supporting the new magic version to one which doesn't, then we will need to convert.        if (!records.hasMatchingMagic(minUsedMagic))            records = batch.records().downConvert(minUsedMagic, 0, time).records();        produceRecordsByPartition.put(tp, records);        recordsByPartition.put(tp, batch);    }    String transactionalId = null;    if (transactionManager != null && transactionManager.isTransactional()) {        transactionalId = transactionManager.transactionalId();    }    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout, produceRecordsByPartition, transactionalId);    RequestCompletionHandler callback = new RequestCompletionHandler() {        public void onComplete(ClientResponse response) {            handleProduceResponse(response, recordsByPartition, time.milliseconds());        }    };    String nodeId = Integer.toString(destination);    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, requestTimeoutMs, callback);    client.send(clientRequest, now);    log.trace("Sent produce request to {}: {}", nodeId, requestBuilder);}
f1533
0
onComplete
public void kafkatest_f1534_0(ClientResponse response)
{    handleProduceResponse(response, recordsByPartition, time.milliseconds());}
f1534
0
createMetricName
private MetricName kafkatest_f1543_0(String name, String description)
{    return this.metrics.metricInstance(createTemplate(name, METRIC_GROUP_NAME, description, this.tags));}
f1543
0
createTopicTemplate
private MetricNameTemplate kafkatest_f1544_0(String name, String description)
{    return createTemplate(name, TOPIC_METRIC_GROUP_NAME, description, this.topicTags);}
f1544
0
topicRecordErrorTotal
public MetricName kafkatest_f1553_0(Map<String, String> tags)
{    return this.metrics.metricInstance(this.topicRecordErrorTotal, tags);}
f1553
0
allTemplates
public List<MetricNameTemplate> kafkatest_f1554_0()
{    return allTemplates;}
f1554
0
await
public void kafkatest_f1563_0()
{    boolean completed = false;    while (!completed) {        try {            latch.await();            completed = true;        } catch (InterruptedException e) {        // Keep waiting until done, we have no other option for these transactional requests.        }    }    if (!isSuccessful())        throw error();}
f1563
0
await
public void kafkatest_f1564_0(long timeout, TimeUnit unit)
{    try {        boolean success = latch.await(timeout, unit);        if (!isSuccessful()) {            throw error();        }        if (!success) {            throw new TimeoutException("Timeout expired after " + timeout + unit.name().toLowerCase(Locale.ROOT) + " while awaiting " + operation);        }    } catch (InterruptedException e) {        throw new InterruptException("Received interrupt while awaiting " + operation, e);    }}
f1564
0
lastAckedSequence
 OptionalInt kafkatest_f1573_0(TopicPartition partition)
{    TopicPartitionEntry entry = topicPartitionBookkeeping.get(partition);    if (entry != null && entry.lastAckedSequence != NO_LAST_ACKED_SEQUENCE_NUMBER)        return OptionalInt.of(entry.lastAckedSequence);    else        return OptionalInt.empty();}
f1573
0
resetSequenceNumbers
public void kafkatest_f1574_0(Consumer<ProducerBatch> resetSequence)
{    TreeSet<ProducerBatch> newInflights = new TreeSet<>(Comparator.comparingInt(ProducerBatch::baseSequence));    for (ProducerBatch inflightBatch : inflightBatchesBySequence) {        resetSequence.accept(inflightBatch);        newInflights.add(inflightBatch);    }    inflightBatchesBySequence = newInflights;}
f1574
0
lastError
 RuntimeException kafkatest_f1583_0()
{    return lastError;}
f1583
0
failIfNotReadyForSend
public synchronized void kafkatest_f1584_0()
{    if (hasError())        throw new KafkaException("Cannot perform send because at least one previous transactional or " + "idempotent request has failed with errors.", lastError);    if (isTransactional()) {        if (!hasProducerId())            throw new IllegalStateException("Cannot perform a 'send' before completing a call to initTransactions " + "when transactions are enabled.");        if (currentState != State.IN_TRANSACTION)            throw new IllegalStateException("Cannot call send in state " + currentState);    }}
f1584
0
transitionToAbortableError
 synchronized voidf1593_1RuntimeException exception)
{    if (currentState == State.ABORTING_TRANSACTION) {                return;    }    transitionTo(State.ABORTABLE_ERROR, exception);}
 synchronized voidf1593
1
transitionToFatalError
 synchronized void kafkatest_f1594_0(RuntimeException exception)
{    transitionTo(State.FATAL_ERROR, exception);}
f1594
0
sequenceNumber
 synchronized Integer kafkatest_f1603_0(TopicPartition topicPartition)
{    if (!isTransactional())        topicPartitionBookkeeper.addPartition(topicPartition);    return topicPartitionBookkeeper.getPartition(topicPartition).nextSequence;}
f1603
0
incrementSequenceNumber
 synchronized void kafkatest_f1604_0(TopicPartition topicPartition, int increment)
{    Integer currentSequence = sequenceNumber(topicPartition);    currentSequence = DefaultRecordBatch.incrementSequence(currentSequence, increment);    topicPartitionBookkeeper.getPartition(topicPartition).nextSequence = currentSequence;}
f1604
0
handleCompletedBatch
public synchronized voidf1613_1ProducerBatch batch, ProduceResponse.PartitionResponse response)
{    if (!hasProducerIdAndEpoch(batch.producerId(), batch.producerEpoch())) {                return;    }    maybeUpdateLastAckedSequence(batch.topicPartition, batch.baseSequence() + batch.recordCount - 1);     Set last ack'd sequence number for topic-partition {} to {}", batch.producerId(), batch.topicPartition, lastAckedSequence(batch.topicPartition).orElse(-1));    updateLastAckedOffset(response, batch);    removeInFlightBatch(batch);}
public synchronized voidf1613
1
maybeTransitionToErrorState
private void kafkatest_f1614_0(RuntimeException exception)
{    if (exception instanceof ClusterAuthorizationException || exception instanceof TransactionalIdAuthorizationException || exception instanceof ProducerFencedException || exception instanceof UnsupportedVersionException) {        transitionToFatalError(exception);    } else if (isTransactional()) {        transitionToAbortableError(exception);    }}
f1614
0
isNextSequence
private boolean kafkatest_f1623_0(TopicPartition topicPartition, int sequence)
{    return sequence - lastAckedSequence(topicPartition).orElse(NO_LAST_ACKED_SEQUENCE_NUMBER) == 1;}
f1623
0
setNextSequence
private void kafkatest_f1624_0(TopicPartition topicPartition, int sequence)
{    topicPartitionBookkeeper.getPartition(topicPartition).nextSequence = sequence;}
f1624
0
hasInFlightTransactionalRequest
 boolean kafkatest_f1633_0()
{    return inFlightRequestCorrelationId != NO_INFLIGHT_REQUEST_CORRELATION_ID;}
f1633
0
hasFatalError
 boolean kafkatest_f1634_0()
{    return currentState == State.FATAL_ERROR;}
f1634
0
transitionTo
private voidf1643_1State target, RuntimeException error)
{    if (!currentState.isTransitionValid(currentState, target)) {        String idString = transactionalId == null ? "" : "TransactionalId " + transactionalId + ": ";        throw new KafkaException(idString + "Invalid transition attempted from state " + currentState.name() + " to state " + target.name());    }    if (target == State.FATAL_ERROR || target == State.ABORTABLE_ERROR) {        if (error == null)            throw new IllegalArgumentException("Cannot transition to " + target + " with a null exception");        lastError = error;    } else {        lastError = null;    }    if (lastError != null)            else            currentState = target;}
private voidf1643
1
ensureTransactional
private void kafkatest_f1644_0()
{    if (!isTransactional())        throw new IllegalStateException("Transactional method invoked on a non-transactional producer.");}
f1644
0
fatalError
 void kafkatest_f1653_0(RuntimeException e)
{    result.setError(e);    transitionToFatalError(e);    result.done();}
f1653
0
abortableError
 void kafkatest_f1654_0(RuntimeException e)
{    result.setError(e);    transitionToAbortableError(e);    result.done();}
f1654
0
isRetry
 boolean kafkatest_f1663_0()
{    return isRetry;}
f1663
0
isEndTxn
 boolean kafkatest_f1664_0()
{    return false;}
f1664
0
requestBuilder
 FindCoordinatorRequest.Builder kafkatest_f1673_0()
{    return builder;}
f1673
0
priority
 Priority kafkatest_f1674_0()
{    return Priority.FIND_COORDINATOR;}
f1674
0
priority
 Priority kafkatest_f1683_0()
{    return Priority.ADD_PARTITIONS_OR_OFFSETS;}
f1683
0
handleResponse
public voidf1684_1AbstractResponse response)
{    AddOffsetsToTxnResponse addOffsetsToTxnResponse = (AddOffsetsToTxnResponse) response;    Errors error = addOffsetsToTxnResponse.error();    if (error == Errors.NONE) {                // note the result is not completed until the TxnOffsetCommit returns        pendingRequests.add(txnOffsetCommitHandler(result, offsets, builder.consumerGroupId()));        transactionStarted = true;    } else if (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) {        lookupCoordinator(FindCoordinatorRequest.CoordinatorType.TRANSACTION, transactionalId);        reenqueue();    } else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.CONCURRENT_TRANSACTIONS) {        reenqueue();    } else if (error == Errors.INVALID_PRODUCER_EPOCH) {        fatalError(error.exception());    } else if (error == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED) {        fatalError(error.exception());    } else if (error == Errors.GROUP_AUTHORIZATION_FAILED) {        abortableError(GroupAuthorizationException.forGroupId(builder.consumerGroupId()));    } else {        fatalError(new KafkaException("Unexpected error in AddOffsetsToTxnResponse: " + error.message()));    }}
public voidf1684
1
configureTransactionState
private static TransactionManagerf1693_1ProducerConfig config, LogContext logContext, Logger log)
{    TransactionManager transactionManager = null;    boolean userConfiguredIdempotence = false;    if (config.originals().containsKey(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG))        userConfiguredIdempotence = true;    boolean userConfiguredTransactions = false;    if (config.originals().containsKey(ProducerConfig.TRANSACTIONAL_ID_CONFIG))        userConfiguredTransactions = true;    boolean idempotenceEnabled = config.getBoolean(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG);    if (!idempotenceEnabled && userConfiguredIdempotence && userConfiguredTransactions)        throw new ConfigException("Cannot set a " + ProducerConfig.TRANSACTIONAL_ID_CONFIG + " without also enabling idempotence.");    if (userConfiguredTransactions)        idempotenceEnabled = true;    if (idempotenceEnabled) {        String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG);        int transactionTimeoutMs = config.getInt(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG);        long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);        transactionManager = new TransactionManager(logContext, transactionalId, transactionTimeoutMs, retryBackoffMs);        if (transactionManager.isTransactional())                    else                }    return transactionManager;}
private static TransactionManagerf1693
1
configureRetries
private static intf1694_1ProducerConfig config, boolean idempotenceEnabled, Logger log)
{    boolean userConfiguredRetries = false;    if (config.originals().containsKey(ProducerConfig.RETRIES_CONFIG)) {        userConfiguredRetries = true;    }    if (idempotenceEnabled && !userConfiguredRetries) {        // We recommend setting infinite retries when the idempotent producer is enabled, so it makes sense to make        // this the default.                return Integer.MAX_VALUE;    }    if (idempotenceEnabled && config.getInt(ProducerConfig.RETRIES_CONFIG) == 0) {        throw new ConfigException("Must set " + ProducerConfig.RETRIES_CONFIG + " to non-zero when using the idempotent producer.");    }    return config.getInt(ProducerConfig.RETRIES_CONFIG);}
private static intf1694
1
send
public Future<RecordMetadata> kafkatest_f1703_0(ProducerRecord<K, V> record)
{    return send(record, null);}
f1703
0
send
public Future<RecordMetadata> kafkatest_f1704_0(ProducerRecord<K, V> record, Callback callback)
{    // intercept the record, which can be potentially modified; this method does not throw exceptions    ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);    return doSend(interceptedRecord, callback);}
f1704
0
close
public void kafkatest_f1713_0()
{    close(Duration.ofMillis(Long.MAX_VALUE));}
f1713
0
close
public void kafkatest_f1714_0(Duration timeout)
{    close(timeout, false);}
f1714
0
get
public RecordMetadata kafkatest_f1723_0(long timeout, TimeUnit unit) throws ExecutionException
{    throw this.exception;}
f1723
0
isCancelled
public boolean kafkatest_f1724_0()
{    return false;}
f1724
0
verifyTransactionsInitialized
private void kafkatest_f1733_0()
{    if (!this.transactionInitialized) {        throw new IllegalStateException("MockProducer hasn't been initialized for transactions.");    }}
f1733
0
verifyNoTransactionInFlight
private void kafkatest_f1734_0()
{    if (!this.transactionInFlight) {        throw new IllegalStateException("There is no open transaction.");    }}
f1734
0
close
public void kafkatest_f1743_0(Duration timeout)
{    if (producerFencedOnClose) {        throw new ProducerFencedException("MockProducer is fenced.");    }    this.closed = true;}
f1743
0
closed
public boolean kafkatest_f1744_0()
{    return this.closed;}
f1744
0
commitCount
public long kafkatest_f1753_0()
{    return this.commitCount;}
f1753
0
history
public synchronized List<ProducerRecord<K, V>> kafkatest_f1754_0()
{    return new ArrayList<>(this.sent);}
f1754
0
addSerializerToConfig
public static Map<String, Object> kafkatest_f1764_0(Map<String, Object> configs, Serializer<?> keySerializer, Serializer<?> valueSerializer)
{    Map<String, Object> newConfigs = new HashMap<>(configs);    if (keySerializer != null)        newConfigs.put(KEY_SERIALIZER_CLASS_CONFIG, keySerializer.getClass());    if (valueSerializer != null)        newConfigs.put(VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer.getClass());    return newConfigs;}
f1764
0
addSerializerToConfig
public static Properties kafkatest_f1765_0(Properties properties, Serializer<?> keySerializer, Serializer<?> valueSerializer)
{    Properties newProperties = new Properties();    newProperties.putAll(properties);    if (keySerializer != null)        newProperties.put(KEY_SERIALIZER_CLASS_CONFIG, keySerializer.getClass().getName());    if (valueSerializer != null)        newProperties.put(VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer.getClass().getName());    return newProperties;}
f1765
0
partition
public Integer kafkatest_f1774_0()
{    return partition;}
f1774
0
toString
public String kafkatest_f1775_0()
{    String headers = this.headers == null ? "null" : this.headers.toString();    String key = this.key == null ? "null" : this.key.toString();    String value = this.value == null ? "null" : this.value.toString();    String timestamp = this.timestamp == null ? "null" : this.timestamp.toString();    return "ProducerRecord(topic=" + topic + ", partition=" + partition + ", headers=" + headers + ", key=" + key + ", value=" + value + ", timestamp=" + timestamp + ")";}
f1775
0
serializedValueSize
public int kafkatest_f1784_0()
{    return this.serializedValueSize;}
f1784
0
topic
public String kafkatest_f1785_0()
{    return this.topicPartition.topic();}
f1785
0
operation
public AclOperation kafkatest_f1798_0()
{    return data.operation();}
f1798
0
permissionType
public AclPermissionType kafkatest_f1799_0()
{    return data.permissionType();}
f1799
0
permissionType
 AclPermissionType kafkatest_f1808_0()
{    return permissionType;}
f1808
0
findIndefiniteField
public String kafkatest_f1809_0()
{    if (principal() == null)        return "Principal is NULL";    if (host() == null)        return "Host is NULL";    if (operation() == AclOperation.ANY)        return "Operation is ANY";    if (operation() == AclOperation.UNKNOWN)        return "Operation is UNKNOWN";    if (permissionType() == AclPermissionType.ANY)        return "Permission type is ANY";    if (permissionType() == AclPermissionType.UNKNOWN)        return "Permission type is UNKNOWN";    return null;}
f1809
0
toString
public String kafkatest_f1818_0()
{    return data.toString();}
f1818
0
isUnknown
public boolean kafkatest_f1819_0()
{    return data.isUnknown();}
f1819
0
toFilter
public AclBindingFilter kafkatest_f1828_0()
{    return new AclBindingFilter(pattern.toFilter(), entry.toFilter());}
f1828
0
toString
public String kafkatest_f1829_0()
{    return "(pattern=" + pattern + ", entry=" + entry + ")";}
f1829
0
findIndefiniteField
public String kafkatest_f1838_0()
{    String indefinite = patternFilter.findIndefiniteField();    if (indefinite != null)        return indefinite;    return entryFilter.findIndefiniteField();}
f1838
0
matches
public boolean kafkatest_f1839_0(AclBinding binding)
{    return patternFilter.matches(binding.pattern()) && entryFilter.matches(binding.entry());}
f1839
0
isUnknown
public boolean kafkatest_f1848_0()
{    return this == UNKNOWN;}
f1848
0
removeEldestEntry
protected boolean kafkatest_f1849_0(Map.Entry<K, V> eldest)
{    // require this. prefix to make lgtm.com happy    return this.size() > maxSize;}
f1849
0
empty
public static Cluster kafkatest_f1858_0()
{    return new Cluster(null, new ArrayList<>(0), new ArrayList<>(0), Collections.emptySet(), Collections.emptySet(), null);}
f1858
0
bootstrap
public static Cluster kafkatest_f1859_0(List<InetSocketAddress> addresses)
{    List<Node> nodes = new ArrayList<>();    int nodeId = -1;    for (InetSocketAddress address : addresses) nodes.add(new Node(nodeId--, address.getHostString(), address.getPort()));    return new Cluster(null, true, nodes, new ArrayList<>(0), Collections.emptySet(), Collections.emptySet(), Collections.emptySet(), null);}
f1859
0
availablePartitionsForTopic
public List<PartitionInfo> kafkatest_f1868_0(String topic)
{    return availablePartitionsByTopic.getOrDefault(topic, Collections.emptyList());}
f1868
0
partitionsForNode
public List<PartitionInfo> kafkatest_f1869_0(int nodeId)
{    return partitionsByNode.getOrDefault(nodeId, Collections.emptyList());}
f1869
0
equals
public boolean kafkatest_f1878_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Cluster cluster = (Cluster) o;    return isBootstrapConfigured == cluster.isBootstrapConfigured && Objects.equals(nodes, cluster.nodes) && Objects.equals(unauthorizedTopics, cluster.unauthorizedTopics) && Objects.equals(invalidTopics, cluster.invalidTopics) && Objects.equals(internalTopics, cluster.internalTopics) && Objects.equals(controller, cluster.controller) && Objects.equals(partitionsByTopicPartition, cluster.partitionsByTopicPartition) && Objects.equals(clusterResource, cluster.clusterResource);}
f1878
0
hashCode
public int kafkatest_f1879_0()
{    return Objects.hash(isBootstrapConfigured, nodes, unauthorizedTopics, invalidTopics, internalTopics, controller, partitionsByTopicPartition, clusterResource);}
f1879
0
getInt
public Integer kafkatest_f1888_0(String key)
{    return (Integer) get(key);}
f1888
0
getLong
public Long kafkatest_f1889_0(String key)
{    return (Long) get(key);}
f1889
0
originals
public Map<String, Object> kafkatest_f1898_0()
{    Map<String, Object> copy = new RecordingMap<>();    copy.putAll(originals);    return copy;}
f1898
0
originalsStrings
public Map<String, String> kafkatest_f1899_0()
{    Map<String, String> copy = new RecordingMap<>();    for (Map.Entry<String, ?> entry : originals.entrySet()) {        if (!(entry.getValue() instanceof String))            throw new ClassCastException("Non-string value found in original settings for key " + entry.getKey() + ": " + (entry.getValue() == null ? null : entry.getValue().getClass().getName()));        copy.put(entry.getKey(), (String) entry.getValue());    }    return copy;}
f1899
0
getConfiguredInstance
public T kafkatest_f1908_0(String key, Class<T> t)
{    Class<?> c = getClass(key);    return getConfiguredInstance(c, t, originals());}
f1908
0
getConfiguredInstances
public List<T> kafkatest_f1909_0(String key, Class<T> t)
{    return getConfiguredInstances(key, t, Collections.emptyMap());}
f1909
0
get
public V kafkatest_f1918_0(Object key)
{    if (key instanceof String) {        String stringKey = (String) key;        String keyWithPrefix;        if (prefix.isEmpty()) {            keyWithPrefix = stringKey;        } else {            keyWithPrefix = prefix + stringKey;        }        ignore(keyWithPrefix);        if (withIgnoreFallback)            ignore(stringKey);    }    return super.get(key);}
f1918
0
get
public V kafkatest_f1919_0(Object key)
{    if (key instanceof String && originals.containsKey(key)) {        // Intentionally ignore the result; call just to mark the original entry as used        originals.get(key);    }    // But always use the resolved entry    return super.get(key);}
f1919
0
define
public ConfigDef kafkatest_f1928_0(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation, String group, int orderInGroup, Width width, String displayName, Recommender recommender)
{    return define(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, Collections.emptyList(), recommender);}
f1928
0
define
public ConfigDef kafkatest_f1929_0(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation, String group, int orderInGroup, Width width, String displayName)
{    return define(name, type, defaultValue, validator, importance, documentation, group, orderInGroup, width, displayName, Collections.<String>emptyList());}
f1929
0
define
public ConfigDef kafkatest_f1938_0(String name, Type type, Object defaultValue, Validator validator, Importance importance, String documentation)
{    return define(name, type, defaultValue, validator, importance, documentation, null, -1, Width.NONE, name);}
f1938
0
define
public ConfigDef kafkatest_f1939_0(String name, Type type, Object defaultValue, Importance importance, String documentation)
{    return define(name, type, defaultValue, null, importance, documentation);}
f1939
0
validate
public List<ConfigValue> kafkatest_f1948_0(Map<String, String> props)
{    return new ArrayList<>(validateAll(props).values());}
f1948
0
validateAll
public Map<String, ConfigValue> kafkatest_f1949_0(Map<String, String> props)
{    Map<String, ConfigValue> configValues = new HashMap<>();    for (String name : configKeys.keySet()) {        configValues.put(name, new ConfigValue(name));    }    List<String> undefinedConfigKeys = undefinedDependentConfigs();    for (String undefinedConfigKey : undefinedConfigKeys) {        ConfigValue undefinedConfigValue = new ConfigValue(undefinedConfigKey);        undefinedConfigValue.addErrorMessage(undefinedConfigKey + " is referred in the dependents, but not defined.");        undefinedConfigValue.visible(false);        configValues.put(undefinedConfigKey, undefinedConfigValue);    }    Map<String, Object> parsed = parseForValidate(props, configValues);    return validate(parsed, configValues);}
f1949
0
convertToStringMapWithPasswordValues
public static Map<String, String> kafkatest_f1958_0(Map<String, ?> configs)
{    Map<String, String> result = new HashMap<>();    for (Map.Entry<String, ?> entry : configs.entrySet()) {        Object value = entry.getValue();        String strValue;        if (value instanceof Password)            strValue = ((Password) value).value();        else if (value instanceof List)            strValue = convertToString(value, Type.LIST);        else if (value instanceof Class)            strValue = convertToString(value, Type.CLASS);        else            strValue = convertToString(value, null);        if (strValue != null)            result.put(entry.getKey(), strValue);    }    return result;}
f1958
0
atLeast
public static Range kafkatest_f1959_0(Number min)
{    return new Range(min, null);}
f1959
0
toString
public String kafkatest_f1968_0()
{    return "[" + Utils.join(validStrings, ", ") + "]";}
f1968
0
ensureValid
public void kafkatest_f1969_0(String name, Object value)
{    if (value == null) {        // Pass in the string null to avoid the spotbugs warning        throw new ConfigException(name, "null", "entry must be non null");    }}
f1969
0
toString
public String kafkatest_f1978_0()
{    return "non-empty string";}
f1978
0
nonEmptyStringWithoutControlChars
public static NonEmptyStringWithoutControlChars kafkatest_f1979_0()
{    return new NonEmptyStringWithoutControlChars();}
f1979
0
addColumnValue
private void kafkatest_f1988_0(StringBuilder builder, String value)
{    builder.append("<td>");    builder.append(value);    builder.append("</td>");}
f1988
0
toHtmlTable
public String kafkatest_f1989_0(Map<String, String> dynamicUpdateModes)
{    boolean hasUpdateModes = !dynamicUpdateModes.isEmpty();    List<ConfigKey> configs = sortedConfigs();    StringBuilder b = new StringBuilder();    b.append("<table class=\"data-table\"><tbody>\n");    b.append("<tr>\n");    // print column headers    for (String headerName : headers()) {        addHeader(b, headerName);    }    if (hasUpdateModes)        addHeader(b, "Dynamic Update Mode");    b.append("</tr>\n");    for (ConfigKey key : configs) {        if (key.internalConfig) {            continue;        }        b.append("<tr>\n");        // print column values        for (String headerName : headers()) {            addColumnValue(b, getConfigValue(key, headerName));            b.append("</td>");        }        if (hasUpdateModes) {            String updateMode = dynamicUpdateModes.get(key.name);            if (updateMode == null)                updateMode = "read-only";            addColumnValue(b, updateMode);        }        b.append("</tr>\n");    }    b.append("</tbody></table>");    return b.toString();}
f1989
0
toString
public String kafkatest_f1998_0()
{    return base.toString();}
f1998
0
embeddedDependents
private static List<String> kafkatest_f1999_0(final String keyPrefix, final List<String> dependents)
{    if (dependents == null)        return null;    final List<String> updatedDependents = new ArrayList<>(dependents.size());    for (String dependent : dependents) {        updatedDependents.add(keyPrefix + dependent);    }    return updatedDependents;}
f1999
0
id
public byte kafkatest_f2008_0()
{    return id;}
f2008
0
forId
public static Type kafkatest_f2009_0(final byte id)
{    return TYPES.getOrDefault(id, UNKNOWN);}
f2009
0
replace
private static String kafkatest_f2018_0(Map<String, Map<String, Map<String, String>>> lookupsByProvider, String value, Pattern pattern)
{    if (value == null) {        return null;    }    Matcher matcher = pattern.matcher(value);    StringBuilder builder = new StringBuilder();    int i = 0;    while (matcher.find()) {        ConfigVariable configVar = new ConfigVariable(matcher);        Map<String, Map<String, String>> lookupsByPath = lookupsByProvider.get(configVar.providerName);        if (lookupsByPath != null) {            Map<String, String> keyValues = lookupsByPath.get(configVar.path);            String replacement = keyValues.get(configVar.variable);            builder.append(value, i, matcher.start());            if (replacement == null) {                // No replacements will be performed; just return the original value                builder.append(matcher.group(0));            } else {                builder.append(replacement);            }            i = matcher.end();        }    }    builder.append(value, i, value.length());    return builder.toString();}
f2018
0
toString
public String kafkatest_f2019_0()
{    return "(" + providerName + ":" + (path != null ? path + ":" : "") + variable + ")";}
f2019
0
recommendedValues
public void kafkatest_f2028_0(List<Object> recommendedValues)
{    this.recommendedValues = recommendedValues;}
f2028
0
addErrorMessage
public void kafkatest_f2029_0(String errorMessage)
{    this.errorMessages.add(errorMessage);}
f2029
0
get
public ConfigData kafkatest_f2039_0(String path, Set<String> keys)
{    Map<String, String> data = new HashMap<>();    if (path == null || path.isEmpty()) {        return new ConfigData(data);    }    try (Reader reader = reader(path)) {        Properties properties = new Properties();        properties.load(reader);        for (String key : keys) {            String value = properties.getProperty(key);            if (value != null) {                data.put(key, value);            }        }        return new ConfigData(data);    } catch (IOException e) {        throw new ConfigException("Could not read properties from file " + path);    }}
f2039
0
reader
protected Reader kafkatest_f2040_0(String path) throws IOException
{    return Files.newBufferedReader(Paths.get(path));}
f2040
0
toString
public String kafkatest_f2050_0()
{    return name;}
f2050
0
valueOf
public static ElectionType kafkatest_f2051_0(byte value)
{    if (value == PREFERRED.value) {        return PREFERRED;    } else if (value == UNCLEAN.value) {        return UNCLEAN;    } else {        throw new IllegalArgumentException(String.format("Value %s must be one of %s", value, Arrays.asList(ElectionType.values())));    }}
f2051
0
groupId
public String kafkatest_f2060_0()
{    return groupId;}
f2060
0
forGroupId
public static GroupAuthorizationException kafkatest_f2061_0(String groupId)
{    return new GroupAuthorizationException("Not authorized to access group: " + groupId, groupId);}
f2061
0
toString
public String kafkatest_f2070_0()
{    return "RecordHeader(key = " + key + ", value = " + Arrays.toString(value()) + ")";}
f2070
0
add
public Headers kafkatest_f2071_0(Header header) throws IllegalStateException
{    Objects.requireNonNull(header, "Header cannot be null.");    canWrite();    headers.add(header);    return this;}
f2071
0
canWrite
private void kafkatest_f2080_0()
{    if (isReadOnly)        throw new IllegalStateException("RecordHeaders has been closed.");}
f2080
0
closeAware
private Iterator<Header> kafkatest_f2081_0(final Iterator<Header> original)
{    return new Iterator<Header>() {        @Override        public boolean hasNext() {            return original.hasNext();        }        public Header next() {            return original.next();        }        @Override        public void remove() {            canWrite();            original.remove();        }    };}
f2081
0
maybeAddAll
public void kafkatest_f2090_0(List<?> candidateList)
{    for (Object candidate : candidateList) {        this.maybeAdd(candidate);    }}
f2090
0
onUpdate
public void kafkatest_f2091_0(ClusterResource cluster)
{    for (ClusterResourceListener clusterResourceListener : clusterResourceListeners) {        clusterResourceListener.onUpdate(cluster);    }}
f2091
0
thenApply
public KafkaFuture<R> kafkatest_f2100_0(Function<T, R> function)
{    return thenApply((BaseFunction<T, R>) function);}
f2100
0
accept
public void kafkatest_f2101_0(T val, Throwable exception)
{    try {        if (exception != null) {            biConsumer.accept(null, exception);        } else {            biConsumer.accept(val, null);        }    } catch (Throwable e) {        if (exception == null) {            exception = e;        }    }    if (exception != null) {        future.completeExceptionally(exception);    } else {        future.complete(val);    }}
f2101
0
isCancelled
public synchronized boolean kafkatest_f2110_0()
{    return exception instanceof CancellationException;}
f2110
0
isCompletedExceptionally
public synchronized boolean kafkatest_f2111_0()
{    return exception != null;}
f2111
0
partitionStates
public List<PartitionState<S>> kafkatest_f2120_0()
{    List<PartitionState<S>> result = new ArrayList<>(map.size());    for (Map.Entry<TopicPartition, S> entry : map.entrySet()) {        result.add(new PartitionState<>(entry.getKey(), entry.getValue()));    }    return result;}
f2120
0
stream
public Stream<PartitionState<S>> kafkatest_f2121_0()
{    return map.entrySet().stream().map(entry -> new PartitionState<>(entry.getKey(), entry.getValue()));}
f2121
0
equals
public boolean kafkatest_f2130_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    PartitionState<?> that = (PartitionState<?>) o;    return topicPartition.equals(that.topicPartition) && value.equals(that.value);}
f2130
0
hashCode
public int kafkatest_f2131_0()
{    int result = topicPartition.hashCode();    result = 31 * result + value.hashCode();    return result;}
f2131
0
maybeComplete
private void kafkatest_f2140_0()
{    if (remainingResponses <= 0)        future.complete(null);}
f2140
0
completedFuture
public static KafkaFuture<U> kafkatest_f2141_0(U value)
{    KafkaFuture<U> future = new KafkaFutureImpl<U>();    future.complete(value);    return future;}
f2141
0
tryAllocate
public ByteBuffer kafkatest_f2150_0(int sizeBytes)
{    return ByteBuffer.allocate(sizeBytes);}
f2150
0
release
public void kafkatest_f2151_0(ByteBuffer previouslyAllocated)
{// nop}
f2151
0
isOutOfMemory
public boolean kafkatest_f2160_0()
{    return availableMemory.get() <= 0;}
f2160
0
bufferToBeReturned
protected void kafkatest_f2161_0(ByteBuffer justAllocated)
{    log.trace("allocated buffer of size {} ", justAllocated.capacity());}
f2161
0
equals
public boolean kafkatest_f2170_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    MetricName other = (MetricName) obj;    return group.equals(other.group) && name.equals(other.name) && tags.equals(other.tags);}
f2170
0
toString
public String kafkatest_f2171_0()
{    return "MetricName [name=" + name + ", group=" + group + ", description=" + description + ", tags=" + tags + "]";}
f2171
0
name
public MetricName kafkatest_f2180_0()
{    return name;}
f2180
0
stat
public Measurable kafkatest_f2181_0()
{    return stat;}
f2181
0
unregister
private void kafkatest_f2191_0(KafkaMbean mbean)
{    MBeanServer server = ManagementFactory.getPlatformMBeanServer();    try {        if (server.isRegistered(mbean.name()))            server.unregisterMBean(mbean.name());    } catch (JMException e) {        throw new KafkaException("Error unregistering mbean", e);    }}
f2191
0
reregister
private void kafkatest_f2192_0(KafkaMbean mbean)
{    unregister(mbean);    try {        ManagementFactory.getPlatformMBeanServer().registerMBean(mbean, mbean.name());    } catch (JMException e) {        throw new KafkaException("Error registering mbean " + mbean.name(), e);    }}
f2192
0
setAttributes
public AttributeList kafkatest_f2201_0(AttributeList list)
{    throw new UnsupportedOperationException("Set not allowed.");}
f2201
0
config
public MetricConfig kafkatest_f2202_0()
{    return this.config;}
f2202
0
eventWindow
public long kafkatest_f2211_0()
{    return eventWindow;}
f2211
0
eventWindow
public MetricConfig kafkatest_f2212_0(long window)
{    this.eventWindow = window;    return this;}
f2212
0
newThread
public Thread kafkatest_f2221_0(Runnable runnable)
{    return KafkaThread.daemon("SensorExpiryThread", runnable);}
f2221
0
measure
public double kafkatest_f2222_0(MetricConfig config, long now)
{    return metrics.size();}
f2222
0
getSensor
public Sensor kafkatest_f2231_0(String name)
{    return this.sensors.get(Objects.requireNonNull(name));}
f2231
0
sensor
public Sensor kafkatest_f2232_0(String name)
{    return this.sensor(name, Sensor.RecordingLevel.INFO);}
f2232
0
addMetric
public void kafkatest_f2241_0(MetricName metricName, Measurable measurable)
{    addMetric(metricName, null, measurable);}
f2241
0
addMetric
public void kafkatest_f2242_0(MetricName metricName, MetricConfig config, Measurable measurable)
{    addMetric(metricName, config, (MetricValueProvider<?>) measurable);}
f2242
0
metric
public KafkaMetric kafkatest_f2251_0(MetricName metricName)
{    return this.metrics.get(metricName);}
f2251
0
run
public voidf2252_1)
{    for (Map.Entry<String, Sensor> sensorEntry : sensors.entrySet()) {        // and thus not necessary to optimize        synchronized (sensorEntry.getValue()) {            if (sensorEntry.getValue().hasExpired()) {                                removeSensor(sensorEntry.getKey());            }        }    }}
public voidf2252
1
acceptable
public boolean kafkatest_f2261_0(double value)
{    return (upper && value <= bound) || (!upper && value >= bound);}
f2261
0
hashCode
public int kafkatest_f2262_0()
{    final int prime = 31;    int result = 1;    result = prime * result + (int) this.bound;    result = prime * result + (this.upper ? 1 : 0);    return result;}
f2262
0
checkForest
private void kafkatest_f2271_0(Set<Sensor> sensors)
{    if (!sensors.add(this))        throw new IllegalArgumentException("Circular dependency in sensors: " + name() + " is its own parent.");    for (Sensor parent : parents) parent.checkForest(sensors);}
f2271
0
name
public String kafkatest_f2272_0()
{    return this.name;}
f2272
0
add
public boolean kafkatest_f2281_0(CompoundStat stat)
{    return add(stat, null);}
f2281
0
add
public synchronized boolean kafkatest_f2282_0(CompoundStat stat, MetricConfig config)
{    if (hasExpired())        return false;    this.stats.add(Objects.requireNonNull(stat));    Object lock = metricLock();    for (NamedMeasurable m : stat.stats()) {        final KafkaMetric metric = new KafkaMetric(lock, m.name(), m.stat(), config == null ? this.config : config, time);        if (!metrics.containsKey(metric.metricName())) {            registry.registerMetric(metric);            metrics.put(metric.metricName(), metric);        }    }    return true;}
f2282
0
record
public void kafkatest_f2291_0(MetricConfig config, double value, long now)
{    total += value;}
f2291
0
measure
public double kafkatest_f2292_0(MetricConfig config, long now)
{    return total;}
f2292
0
reset
public void kafkatest_f2301_0(long now)
{    super.reset(now);    histogram.clear();}
f2301
0
name
public MetricName kafkatest_f2302_0()
{    return this.name;}
f2302
0
toBin
public int kafkatest_f2311_0(double x)
{    int binNumber = (int) ((x - min) / bucketWidth);    if (binNumber < MIN_BIN_NUMBER) {        return MIN_BIN_NUMBER;    }    if (binNumber > maxBinNumber) {        return maxBinNumber;    }    return binNumber;}
f2311
0
bins
public int kafkatest_f2312_0()
{    return this.bins;}
f2312
0
name
public MetricName kafkatest_f2321_0()
{    return this.name;}
f2321
0
percentile
public double kafkatest_f2322_0()
{    return this.percentile;}
f2322
0
record
public void kafkatest_f2331_0(MetricConfig config, double value, long timeMs)
{    this.stat.record(config, value, timeMs);}
f2331
0
measure
public double kafkatest_f2332_0(MetricConfig config, long now)
{    double value = stat.measure(config, now);    return value / convert(windowSize(config, now));}
f2332
0
purgeObsoleteSamples
protected void kafkatest_f2341_0(MetricConfig config, long now)
{    long expireAge = config.samples() * config.timeWindowMs();    for (Sample sample : samples) {        if (now - sample.lastWindowMs >= expireAge)            sample.reset(now);    }}
f2341
0
reset
public void kafkatest_f2342_0(long now)
{    this.eventCount = 0;    this.lastWindowMs = now;    this.value = initialValue;}
f2342
0
serverSessionExpirationTimeNanos
 Long kafkatest_f2352_0()
{    return null;}
f2352
0
clientSessionReauthenticationTimeNanos
 Long kafkatest_f2353_0()
{    return null;}
f2353
0
serverChannelBuilder
public static ChannelBuilder kafkatest_f2362_0(ListenerName listenerName, boolean isInterBrokerListener, SecurityProtocol securityProtocol, AbstractConfig config, CredentialCache credentialCache, DelegationTokenCache tokenCache, Time time)
{    return create(securityProtocol, Mode.SERVER, JaasContext.Type.SERVER, config, listenerName, isInterBrokerListener, null, true, credentialCache, tokenCache, time);}
f2362
0
create
private static ChannelBuilder kafkatest_f2363_0(SecurityProtocol securityProtocol, Mode mode, JaasContext.Type contextType, AbstractConfig config, ListenerName listenerName, boolean isInterBrokerListener, String clientSaslMechanism, boolean saslHandshakeRequestEnable, CredentialCache credentialCache, DelegationTokenCache tokenCache, Time time)
{    Map<String, ?> configs;    if (listenerName == null)        configs = config.values();    else        configs = config.valuesWithPrefixOverride(listenerName.configPrefix());    ChannelBuilder channelBuilder;    switch(securityProtocol) {        case SSL:            requireNonNullMode(mode, securityProtocol);            channelBuilder = new SslChannelBuilder(mode, listenerName, isInterBrokerListener);            break;        case SASL_SSL:        case SASL_PLAINTEXT:            requireNonNullMode(mode, securityProtocol);            Map<String, JaasContext> jaasContexts;            if (mode == Mode.SERVER) {                @SuppressWarnings("unchecked")                List<String> enabledMechanisms = (List<String>) configs.get(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG);                jaasContexts = new HashMap<>(enabledMechanisms.size());                for (String mechanism : enabledMechanisms) jaasContexts.put(mechanism, JaasContext.loadServerContext(listenerName, mechanism, configs));            } else {                // Use server context for inter-broker client connections and client context for other clients                JaasContext jaasContext = contextType == JaasContext.Type.CLIENT ? JaasContext.loadClientContext(configs) : JaasContext.loadServerContext(listenerName, clientSaslMechanism, configs);                jaasContexts = Collections.singletonMap(clientSaslMechanism, jaasContext);            }            channelBuilder = new SaslChannelBuilder(mode, jaasContexts, securityProtocol, listenerName, isInterBrokerListener, clientSaslMechanism, saslHandshakeRequestEnable, credentialCache, tokenCache, time);            break;        case PLAINTEXT:            channelBuilder = new PlaintextChannelBuilder(listenerName);            break;        default:            throw new IllegalArgumentException("Unexpected securityProtocol " + securityProtocol);    }    channelBuilder.configure(configs);    return channelBuilder;}
f2363
0
prepare
public void kafkatest_f2372_0() throws AuthenticationException, IOException
{    boolean authenticating = false;    try {        if (!transportLayer.ready())            transportLayer.handshake();        if (transportLayer.ready() && !authenticator.complete()) {            authenticating = true;            authenticator.authenticate();        }    } catch (AuthenticationException e) {        // Clients are notified of authentication exceptions to enable operations to be terminated        // without retries. Other errors are handled as network exceptions in Selector.        String remoteDesc = remoteAddress != null ? remoteAddress.toString() : null;        state = new ChannelState(ChannelState.State.AUTHENTICATION_FAILED, e, remoteDesc);        if (authenticating) {            delayCloseOnAuthenticationFailure();            throw new DelayedResponseAuthenticationException(e);        }        throw e;    }    if (ready()) {        ++successfulAuthentications;        state = ChannelState.READY;    }}
f2372
0
disconnect
public void kafkatest_f2373_0()
{    disconnected = true;    if (state == ChannelState.NOT_CONNECTED && remoteAddress != null) {        // if we captured the remote address we can provide more information        state = new ChannelState(ChannelState.State.NOT_CONNECTED, remoteAddress.toString());    }    transportLayer.disconnect();}
f2373
0
handleChannelMuteEvent
public void kafkatest_f2382_0(ChannelMuteEvent event)
{    boolean stateChanged = false;    switch(event) {        case REQUEST_RECEIVED:            if (muteState == ChannelMuteState.MUTED) {                muteState = ChannelMuteState.MUTED_AND_RESPONSE_PENDING;                stateChanged = true;            }            break;        case RESPONSE_SENT:            if (muteState == ChannelMuteState.MUTED_AND_RESPONSE_PENDING) {                muteState = ChannelMuteState.MUTED;                stateChanged = true;            }            if (muteState == ChannelMuteState.MUTED_AND_THROTTLED_AND_RESPONSE_PENDING) {                muteState = ChannelMuteState.MUTED_AND_THROTTLED;                stateChanged = true;            }            break;        case THROTTLE_STARTED:            if (muteState == ChannelMuteState.MUTED_AND_RESPONSE_PENDING) {                muteState = ChannelMuteState.MUTED_AND_THROTTLED_AND_RESPONSE_PENDING;                stateChanged = true;            }            break;        case THROTTLE_ENDED:            if (muteState == ChannelMuteState.MUTED_AND_THROTTLED) {                muteState = ChannelMuteState.MUTED;                stateChanged = true;            }            if (muteState == ChannelMuteState.MUTED_AND_THROTTLED_AND_RESPONSE_PENDING) {                muteState = ChannelMuteState.MUTED_AND_RESPONSE_PENDING;                stateChanged = true;            }    }    if (!stateChanged) {        throw new IllegalStateException("Cannot transition from " + muteState.name() + " for " + event.name());    }}
f2382
0
muteState
public ChannelMuteState kafkatest_f2383_0()
{    return muteState;}
f2383
0
setSend
public void kafkatest_f2392_0(Send send)
{    if (this.send != null)        throw new IllegalStateException("Attempt to begin a send operation with prior send operation still in progress, connection id is " + id);    this.send = send;    this.transportLayer.addInterestOps(SelectionKey.OP_WRITE);}
f2392
0
read
public NetworkReceive kafkatest_f2393_0() throws IOException
{    NetworkReceive result = null;    if (receive == null) {        receive = new NetworkReceive(maxReceiveSize, id, memoryPool);    }    receive(receive);    if (receive.complete()) {        receive.payload().rewind();        result = receive;        receive = null;    } else if (receive.requiredMemoryAmountKnown() && !receive.memoryAllocated() && isInMutableState()) {        // pool must be out of memory, mute ourselves.        mute();    }    return result;}
f2393
0
toString
public String kafkatest_f2402_0()
{    return super.toString() + " id=" + id;}
f2402
0
successfulAuthentications
public int kafkatest_f2403_0()
{    return successfulAuthentications;}
f2403
0
normalised
public static ListenerName kafkatest_f2412_0(String value)
{    return new ListenerName(value.toUpperCase(Locale.ROOT));}
f2412
0
value
public String kafkatest_f2413_0()
{    return value;}
f2413
0
readFrom
public long kafkatest_f2422_0(ScatteringByteChannel channel) throws IOException
{    int read = 0;    if (size.hasRemaining()) {        int bytesRead = channel.read(size);        if (bytesRead < 0)            throw new EOFException();        read += bytesRead;        if (!size.hasRemaining()) {            size.rewind();            int receiveSize = size.getInt();            if (receiveSize < 0)                throw new InvalidReceiveException("Invalid receive (size = " + receiveSize + ")");            if (maxSize != UNLIMITED && receiveSize > maxSize)                throw new InvalidReceiveException("Invalid receive (size = " + receiveSize + " larger than " + maxSize + ")");            // may be 0 for some payloads (SASL)            requestedBufferSize = receiveSize;            if (receiveSize == 0) {                buffer = EMPTY_BUFFER;            }        }    }    if (buffer == null && requestedBufferSize != -1) {        // we know the size we want but havent been able to allocate it yet        buffer = memoryPool.tryAllocate(requestedBufferSize);        if (buffer == null)            log.trace("Broker low on memory - could not allocate buffer of size {} for source {}", requestedBufferSize, source);    }    if (buffer != null) {        int bytesRead = channel.read(buffer);        if (bytesRead < 0)            throw new EOFException();        read += bytesRead;    }    return read;}
f2422
0
requiredMemoryAmountKnown
public boolean kafkatest_f2423_0()
{    return requestedBufferSize != -1;}
f2423
0
complete
public boolean kafkatest_f2434_0()
{    return true;}
f2434
0
close
public void kafkatest_f2435_0()
{    if (principalBuilder instanceof Closeable)        Utils.closeQuietly((Closeable) principalBuilder, "principal builder");}
f2435
0
read
public int kafkatest_f2445_0(ByteBuffer dst) throws IOException
{    return socketChannel.read(dst);}
f2445
0
read
public long kafkatest_f2446_0(ByteBuffer[] dsts) throws IOException
{    return socketChannel.read(dsts);}
f2446
0
isMute
public boolean kafkatest_f2455_0()
{    return key.isValid() && (key.interestOps() & SelectionKey.OP_READ) == 0;}
f2455
0
hasBytesBuffered
public boolean kafkatest_f2456_0()
{    return false;}
f2456
0
listenerName
public ListenerName kafkatest_f2465_0()
{    return listenerName;}
f2465
0
buildChannel
public KafkaChannelf2466_1String id, SelectionKey key, int maxReceiveSize, MemoryPool memoryPool) throws KafkaException
{    try {        SocketChannel socketChannel = (SocketChannel) key.channel();        Socket socket = socketChannel.socket();        TransportLayer transportLayer = buildTransportLayer(id, key, socketChannel);        Supplier<Authenticator> authenticatorCreator;        if (mode == Mode.SERVER) {            authenticatorCreator = () -> buildServerAuthenticator(configs, Collections.unmodifiableMap(saslCallbackHandlers), id, transportLayer, Collections.unmodifiableMap(subjects), Collections.unmodifiableMap(connectionsMaxReauthMsByMechanism));        } else {            LoginManager loginManager = loginManagers.get(clientSaslMechanism);            authenticatorCreator = () -> buildClientAuthenticator(configs, saslCallbackHandlers.get(clientSaslMechanism), id, socket.getInetAddress().getHostName(), loginManager.serviceName(), transportLayer, subjects.get(clientSaslMechanism));        }        return new KafkaChannel(id, transportLayer, authenticatorCreator, maxReceiveSize, memoryPool != null ? memoryPool : MemoryPool.NONE);    } catch (Exception e) {                throw new KafkaException(e);    }}
public KafkaChannelf2466
1
createConnectionsMaxReauthMsMap
private void kafkatest_f2475_0(Map<String, ?> configs)
{    for (String mechanism : jaasContexts.keySet()) {        String prefix = ListenerName.saslMechanismPrefix(mechanism);        Long connectionsMaxReauthMs = (Long) configs.get(prefix + BrokerSecurityConfigs.CONNECTIONS_MAX_REAUTH_MS);        if (connectionsMaxReauthMs == null)            connectionsMaxReauthMs = (Long) configs.get(BrokerSecurityConfigs.CONNECTIONS_MAX_REAUTH_MS);        if (connectionsMaxReauthMs != null)            connectionsMaxReauthMsByMechanism.put(mechanism, connectionsMaxReauthMs);    }}
f2475
0
defaultLoginClass
private Class<? extends Login> kafkatest_f2476_0(Map<String, ?> configs)
{    if (jaasContexts.containsKey(SaslConfigs.GSSAPI_MECHANISM))        return KerberosLogin.class;    if (OAuthBearerLoginModule.OAUTHBEARER_MECHANISM.equals(clientSaslMechanism))        return OAuthBearerRefreshingLogin.class;    return DefaultLogin.class;}
f2476
0
wakeup
public void kafkatest_f2485_0()
{    this.nioSelector.wakeup();}
f2485
0
close
public void kafkatest_f2486_0()
{    List<String> connections = new ArrayList<>(channels.keySet());    try {        for (String id : connections) close(id);    } finally {        // If there is any exception thrown in close(id), we should still be able        // to close the remaining objects, especially the sensors because keeping        // the sensors may lead to failure to start up the ReplicaFetcherThread if        // the old sensors with the same names has not yet been cleaned up.        AtomicReference<Throwable> firstException = new AtomicReference<>();        Utils.closeQuietly(nioSelector, "nioSelector", firstException);        Utils.closeQuietly(sensors, "sensors", firstException);        Utils.closeQuietly(channelBuilder, "channelBuilder", firstException);        Throwable exception = firstException.get();        if (exception instanceof RuntimeException && !(exception instanceof SecurityException)) {            throw (RuntimeException) exception;        }    }}
f2486
0
disconnected
public Map<String, ChannelState> kafkatest_f2495_0()
{    return this.disconnected;}
f2495
0
connected
public List<String> kafkatest_f2496_0()
{    return this.connected;}
f2496
0
clear
private void kafkatest_f2505_0()
{    this.completedSends.clear();    this.completedReceives.clear();    this.connected.clear();    this.disconnected.clear();    // Remove closed channels after all their staged receives have been processed or if a send was requested    for (Iterator<Map.Entry<String, KafkaChannel>> it = closingChannels.entrySet().iterator(); it.hasNext(); ) {        KafkaChannel channel = it.next().getValue();        Deque<NetworkReceive> deque = this.stagedReceives.get(channel);        boolean sendFailed = failedSends.remove(channel.id());        if (deque == null || deque.isEmpty() || sendFailed) {            doClose(channel, true);            it.remove();        }    }    for (String channel : this.failedSends) this.disconnected.put(channel, ChannelState.FAILED_SEND);    this.failedSends.clear();    this.madeReadProgressLastPoll = false;}
f2505
0
select
private int kafkatest_f2506_0(long timeoutMs) throws IOException
{    if (timeoutMs < 0L)        throw new IllegalArgumentException("timeout should be >= 0");    if (timeoutMs == 0L)        return this.nioSelector.selectNow();    else        return this.nioSelector.select(timeoutMs);}
f2506
0
channel
public KafkaChannel kafkatest_f2515_0(String id)
{    return this.channels.get(id);}
f2515
0
closingChannel
public KafkaChannel kafkatest_f2516_0(String id)
{    return closingChannels.get(id);}
f2516
0
numStagedReceives
public int kafkatest_f2525_0(KafkaChannel channel)
{    Deque<NetworkReceive> deque = stagedReceives.get(channel);    return deque == null ? 0 : deque.size();}
f2525
0
createMeter
private Meter kafkatest_f2526_0(Metrics metrics, String groupName, Map<String, String> metricTags, SampledStat stat, String baseName, String descriptiveName)
{    MetricName rateMetricName = metrics.metricName(baseName + "-rate", groupName, String.format("The number of %s per second", descriptiveName), metricTags);    MetricName totalMetricName = metrics.metricName(baseName + "-total", groupName, String.format("The total number of %s", descriptiveName), metricTags);    if (stat == null)        return new Meter(rateMetricName, totalMetricName);    else        return new Meter(stat, rateMetricName, totalMetricName);}
f2526
0
closeNow
public final void kafkatest_f2535_0()
{    if (closed)        throw new IllegalStateException("Attempt to close a channel that has already been closed");    handleCloseOnAuthenticationFailure(channel);    closed = true;}
f2535
0
update
public void kafkatest_f2536_0(String connectionId, long currentTimeNanos)
{    lruConnections.put(connectionId, currentTimeNanos);}
f2536
0
reconfigure
public void kafkatest_f2545_0(Map<String, ?> configs)
{    sslFactory.reconfigure(configs);}
f2545
0
listenerName
public ListenerName kafkatest_f2546_0()
{    return listenerName;}
f2546
0
ready
public boolean kafkatest_f2557_0()
{    return state == State.READY;}
f2557
0
finishConnect
public boolean kafkatest_f2558_0() throws IOException
{    boolean connected = socketChannel.finishConnect();    if (connected)        key.interestOps(key.interestOps() & ~SelectionKey.OP_CONNECT | SelectionKey.OP_READ);    return connected;}
f2558
0
flush
protected boolean kafkatest_f2567_0(ByteBuffer buf) throws IOException
{    int remaining = buf.remaining();    if (remaining > 0) {        int written = socketChannel.write(buf);        return written >= remaining;    }    return true;}
f2567
0
handshake
public void kafkatest_f2568_0() throws IOException
{    if (state == State.NOT_INITALIZED)        startHandshake();    if (state == State.READY)        throw renegotiationException();    if (state == State.CLOSING)        throw closingException();    int read = 0;    boolean readable = key.isReadable();    try {        // if handshake fails)        if (readable)            read = readFromSocketChannel();        doHandshake();    } catch (SSLException e) {        maybeProcessHandshakeFailure(e, true, null);    } catch (IOException e) {        maybeThrowSslAuthenticationException();        // in the socket channel to read and unwrap, process the data so that any SSL handshake exceptions are reported.        try {            do {                handshakeUnwrap(false, true);            } while (readable && readFromSocketChannel() > 0);        } catch (SSLException e1) {            maybeProcessHandshakeFailure(e1, false, e);        }        // If we get here, this is not a handshake failure, throw the original IOException        throw e;    }    // Read from socket failed, so throw any pending handshake exception or EOF exception.    if (read == -1) {        maybeThrowSslAuthenticationException();        throw new EOFException("EOF during handshake, handshake status is " + handshakeStatus);    }}
f2568
0
read
public long kafkatest_f2577_0(ByteBuffer[] dsts) throws IOException
{    return read(dsts, 0, dsts.length);}
f2577
0
read
public long kafkatest_f2578_0(ByteBuffer[] dsts, int offset, int length) throws IOException
{    if ((offset < 0) || (length < 0) || (offset > dsts.length - length))        throw new IndexOutOfBoundsException();    int totalRead = 0;    int i = offset;    while (i < length) {        if (dsts[i].hasRemaining()) {            int read = read(dsts[i]);            if (read > 0)                totalRead += read;            else                break;        }        if (!dsts[i].hasRemaining()) {            i++;        }    }    return totalRead;}
f2578
0
readFromAppBuffer
private int kafkatest_f2587_0(ByteBuffer dst)
{    appReadBuffer.flip();    int remaining = Math.min(appReadBuffer.remaining(), dst.remaining());    if (remaining > 0) {        int limit = appReadBuffer.limit();        appReadBuffer.limit(appReadBuffer.position() + remaining);        dst.put(appReadBuffer);        appReadBuffer.limit(limit);    }    appReadBuffer.compact();    return remaining;}
f2587
0
netReadBufferSize
protected int kafkatest_f2588_0()
{    return sslEngine.getSession().getPacketBufferSize();}
f2588
0
hasBytesBuffered
public boolean kafkatest_f2597_0()
{    return hasBytesBuffered;}
f2597
0
updateBytesBuffered
private void kafkatest_f2598_0(boolean madeProgress)
{    if (madeProgress)        hasBytesBuffered = netReadBuffer.position() != 0 || appReadBuffer.position() != 0;    else        hasBytesBuffered = false;}
f2598
0
hasRack
public boolean kafkatest_f2607_0()
{    return rack != null;}
f2607
0
rack
public String kafkatest_f2608_0()
{    return rack;}
f2608
0
offlineReplicas
public Node[] kafkatest_f2617_0()
{    return offlineReplicas;}
f2617
0
toString
public String kafkatest_f2618_0()
{    return String.format("Partition(topic = %s, partition = %d, leader = %s, replicas = %s, isr = %s, offlineReplicas = %s)", topic, partition, leader == null ? "none" : leader.idString(), formatNodeIds(replicas), formatNodeIds(inSyncReplicas), formatNodeIds(offlineReplicas));}
f2618
0
parseResponse
public Struct kafkatest_f2627_0(short version, ByteBuffer buffer)
{    return responseSchema(version).read(buffer);}
f2627
0
parseResponse
protected Struct kafkatest_f2628_0(short version, ByteBuffer buffer, short fallbackVersion)
{    int bufferPosition = buffer.position();    try {        return responseSchema(version).read(buffer);    } catch (SchemaException e) {        if (version != fallbackVersion) {            buffer.position(bufferPosition);            return responseSchema(fallbackVersion).read(buffer);        } else            throw e;    }}
f2628
0
readShort
public short kafkatest_f2637_0()
{    return buf.getShort();}
f2637
0
readInt
public int kafkatest_f2638_0()
{    return buf.getInt();}
f2638
0
exception
public ApiException kafkatest_f2647_0(String message)
{    if (message == null) {        // If no error message was specified, return an exception with the default error message.        return exception;    }    // Return an exception with the given error message.    return builder.apply(message);}
f2647
0
exceptionName
public String kafkatest_f2648_0()
{    return exception == null ? null : exception.getClass().getName();}
f2648
0
deepToString
public static String kafkatest_f2657_0(Iterator<?> iter)
{    StringBuilder bld = new StringBuilder("[");    String prefix = "";    while (iter.hasNext()) {        Object object = iter.next();        bld.append(prefix);        bld.append(object.toString());        prefix = ", ";    }    bld.append("]");    return bld.toString();}
f2657
0
indentString
private static String kafkatest_f2658_0(int size)
{    StringBuilder b = new StringBuilder(size);    for (int i = 0; i < size; i++) b.append(" ");    return b.toString();}
f2658
0
nullable
public static ArrayOf kafkatest_f2667_0(Type type)
{    return new ArrayOf(type, true);}
f2667
0
isNullable
public boolean kafkatest_f2668_0()
{    return nullable;}
f2668
0
toString
public String kafkatest_f2677_0()
{    return def.name + ":" + def.type;}
f2677
0
withFields
public Field kafkatest_f2678_0(Field... fields)
{    Schema elementType = new Schema(fields);    return new Field(name, new ArrayOf(elementType), docString, false, null);}
f2678
0
fields
public BoundField[] kafkatest_f2687_0()
{    return this.fields;}
f2687
0
toString
public String kafkatest_f2688_0()
{    StringBuilder b = new StringBuilder();    b.append('{');    for (int i = 0; i < this.fields.length; i++) {        b.append(this.fields[i].toString());        if (i < this.fields.length - 1)            b.append(',');    }    b.append("}");    return b.toString();}
f2688
0
get
public Long kafkatest_f2700_0(Field.Int64 field)
{    return getLong(field.name);}
f2700
0
get
public UUID kafkatest_f2701_0(Field.UUID field)
{    return getUUID(field.name);}
f2701
0
getOrElse
public Short kafkatest_f2710_0(Field.Int16 field, short alternative)
{    if (hasField(field.name))        return getShort(field.name);    return alternative;}
f2710
0
getOrElse
public Byte kafkatest_f2711_0(Field.Int8 field, byte alternative)
{    if (hasField(field.name))        return getByte(field.name);    return alternative;}
f2711
0
hasField
public boolean kafkatest_f2720_0(Field def)
{    return schema.get(def.name) != null;}
f2720
0
hasField
public boolean kafkatest_f2721_0(Field.ComplexArray def)
{    return schema.get(def.name) != null;}
f2721
0
getInt
public Integer kafkatest_f2730_0(String name)
{    return (Integer) get(name);}
f2730
0
getUnsignedInt
public Long kafkatest_f2731_0(String name)
{    return (Long) get(name);}
f2731
0
getBoolean
public Boolean kafkatest_f2740_0(BoundField field)
{    return (Boolean) get(field);}
f2740
0
getBoolean
public Boolean kafkatest_f2741_0(String name)
{    return (Boolean) get(name);}
f2741
0
set
public Struct kafkatest_f2750_0(Field.Int32 def, int value)
{    return set(def.name, value);}
f2750
0
set
public Struct kafkatest_f2751_0(Field.Int64 def, long value)
{    return set(def.name, value);}
f2751
0
setIfExists
public Struct kafkatest_f2760_0(Field def, Object value)
{    return setIfExists(def.name, value);}
f2760
0
setIfExists
public Struct kafkatest_f2761_0(String fieldName, Object value)
{    BoundField field = this.schema.get(fieldName);    if (field != null)        this.values[field.index] = value;    return this;}
f2761
0
validate
public void kafkatest_f2770_0()
{    this.schema.validate(this);}
f2770
0
toString
public String kafkatest_f2771_0()
{    StringBuilder b = new StringBuilder();    b.append('{');    for (int i = 0; i < this.values.length; i++) {        BoundField f = this.schema.get(i);        b.append(f.def.name);        b.append('=');        if (f.def.type instanceof ArrayOf && this.values[i] != null) {            Object[] arrayValue = (Object[]) this.values[i];            b.append('[');            for (int j = 0; j < arrayValue.length; j++) {                b.append(arrayValue[j]);                if (j < arrayValue.length - 1)                    b.append(',');            }            b.append(']');        } else            b.append(this.values[i]);        if (i < this.values.length - 1)            b.append(',');    }    b.append('}');    return b.toString();}
f2771
0
validate
public Boolean kafkatest_f2780_0(Object item)
{    if (item instanceof Boolean)        return (Boolean) item;    else        throw new SchemaException(item + " is not a Boolean.");}
f2780
0
documentation
public String kafkatest_f2781_0()
{    return "Represents a boolean value in a byte. " + "Values 0 and 1 are used to represent false and true respectively. " + "When reading a boolean value, any non-zero value is considered true.";}
f2781
0
sizeOf
public int kafkatest_f2790_0(Object o)
{    return 2;}
f2790
0
typeName
public String kafkatest_f2791_0()
{    return "INT16";}
f2791
0
write
public void kafkatest_f2800_0(ByteBuffer buffer, Object o)
{    ByteUtils.writeUnsignedInt(buffer, (long) o);}
f2800
0
read
public Object kafkatest_f2801_0(ByteBuffer buffer)
{    return ByteUtils.readUnsignedInt(buffer);}
f2801
0
validate
public Long kafkatest_f2810_0(Object item)
{    if (item instanceof Long)        return (Long) item;    else        throw new SchemaException(item + " is not a Long.");}
f2810
0
documentation
public String kafkatest_f2811_0()
{    return "Represents an integer between -2<sup>63</sup> and 2<sup>63</sup>-1 inclusive. " + "The values are encoded using eight bytes in network byte order (big-endian).";}
f2811
0
sizeOf
public int kafkatest_f2820_0(Object o)
{    return 2 + Utils.utf8Length((String) o);}
f2820
0
typeName
public String kafkatest_f2821_0()
{    return "STRING";}
f2821
0
documentation
public String kafkatest_f2830_0()
{    return "Represents a sequence of characters or null. For non-null strings, first the length N is given as an " + INT16 + ". Then N bytes follow which are the UTF-8 encoding of the character sequence. " + "A null value is encoded with length of -1 and there are no following bytes.";}
f2830
0
write
public void kafkatest_f2831_0(ByteBuffer buffer, Object o)
{    ByteBuffer arg = (ByteBuffer) o;    int pos = arg.position();    buffer.putInt(arg.remaining());    buffer.put(arg);    arg.position(pos);}
f2831
0
sizeOf
public int kafkatest_f2840_0(Object o)
{    if (o == null)        return 4;    ByteBuffer buffer = (ByteBuffer) o;    return 4 + buffer.remaining();}
f2840
0
typeName
public String kafkatest_f2841_0()
{    return "NULLABLE_BYTES";}
f2841
0
documentation
public String kafkatest_f2850_0()
{    return "Represents a sequence of Kafka records as " + NULLABLE_BYTES + ". " + "For a detailed description of records see " + "<a href=\"/documentation/#messageformat\">Message Sets</a>.";}
f2850
0
write
public void kafkatest_f2851_0(ByteBuffer buffer, Object o)
{    ByteUtils.writeVarint((Integer) o, buffer);}
f2851
0
typeName
public String kafkatest_f2860_0()
{    return "VARLONG";}
f2860
0
sizeOf
public int kafkatest_f2861_0(Object o)
{    return ByteUtils.sizeOfVarlong((Long) o);}
f2861
0
lastOffset
public long kafkatest_f2870_0()
{    return offset();}
f2870
0
isValid
public boolean kafkatest_f2871_0()
{    return outerRecord().isValid();}
f2871
0
hasMagic
public boolean kafkatest_f2880_0(byte magic)
{    return magic == outerRecord().magic();}
f2880
0
hasTimestampType
public boolean kafkatest_f2881_0(TimestampType timestampType)
{    return outerRecord().timestampType() == timestampType;}
f2881
0
sizeInBytes
public int kafkatest_f2890_0()
{    return outerRecord().sizeInBytes() + LOG_OVERHEAD;}
f2890
0
countOrNull
public Integer kafkatest_f2891_0()
{    return null;}
f2891
0
isTransactional
public boolean kafkatest_f2900_0()
{    return false;}
f2900
0
partitionLeaderEpoch
public int kafkatest_f2901_0()
{    return RecordBatch.NO_PARTITION_LEADER_EPOCH;}
f2901
0
writeHeader
 static void kafkatest_f2911_0(DataOutputStream out, long offset, int size) throws IOException
{    out.writeLong(offset);    out.writeInt(size);}
f2911
0
nextBatch
public AbstractLegacyRecordBatch kafkatest_f2912_0() throws IOException
{    offsetAndSizeBuffer.clear();    Utils.readFully(stream, offsetAndSizeBuffer);    if (offsetAndSizeBuffer.hasRemaining())        return null;    long offset = offsetAndSizeBuffer.getLong(Records.OFFSET_OFFSET);    int size = offsetAndSizeBuffer.getInt(Records.SIZE_OFFSET);    if (size < LegacyRecord.RECORD_OVERHEAD_V0)        throw new CorruptRecordException(String.format("Record size is less than the minimum record overhead (%d)", LegacyRecord.RECORD_OVERHEAD_V0));    if (size > maxMessageSize)        throw new CorruptRecordException(String.format("Record size exceeds the largest allowable message size (%d).", maxMessageSize));    ByteBuffer batchBuffer = ByteBuffer.allocate(size);    Utils.readFully(stream, batchBuffer);    if (batchBuffer.hasRemaining())        return null;    batchBuffer.flip();    return new BasicLegacyRecordBatch(offset, new LegacyRecord(batchBuffer));}
f2912
0
setMaxTimestamp
public void kafkatest_f2922_0(TimestampType timestampType, long timestamp)
{    if (record.magic() == RecordBatch.MAGIC_VALUE_V0)        throw new UnsupportedOperationException("Cannot set timestamp for a record with magic = 0");    long currentTimestamp = record.timestamp();    // We don't need to recompute crc if the timestamp is not updated.    if (record.timestampType() == timestampType && currentTimestamp == timestamp)        return;    setTimestampAndUpdateCrc(timestampType, timestamp);}
f2922
0
setPartitionLeaderEpoch
public void kafkatest_f2923_0(int epoch)
{    throw new UnsupportedOperationException("Magic versions prior to 2 do not support partition leader epoch");}
f2923
0
producerId
public long kafkatest_f2932_0()
{    return RecordBatch.NO_PRODUCER_ID;}
f2932
0
producerEpoch
public short kafkatest_f2933_0()
{    return RecordBatch.NO_PRODUCER_EPOCH;}
f2933
0
nextOffset
public long kafkatest_f2942_0()
{    return lastOffset() + 1;}
f2942
0
isCompressed
public boolean kafkatest_f2943_0()
{    return compressionType() != CompressionType.NONE;}
f2943
0
estimateSizeInBytes
public static int kafkatest_f2952_0(byte magic, CompressionType compressionType, Iterable<SimpleRecord> records)
{    int size = 0;    if (magic <= RecordBatch.MAGIC_VALUE_V1) {        for (SimpleRecord record : records) size += Records.LOG_OVERHEAD + LegacyRecord.recordSize(magic, record.key(), record.value());    } else {        size = DefaultRecordBatch.sizeInBytes(records);    }    return estimateCompressedSizeInBytes(size, compressionType);}
f2952
0
estimateCompressedSizeInBytes
private static int kafkatest_f2953_0(int size, CompressionType compressionType)
{    return compressionType == CompressionType.NONE ? size : Math.min(Math.max(size / 2, 1024), 1 << 16);}
f2953
0
get
public ByteBuffer kafkatest_f2964_0(int minCapacity)
{    if (cachedBuffer != null && cachedBuffer.capacity() >= minCapacity) {        ByteBuffer res = cachedBuffer;        cachedBuffer = null;        return res;    } else {        cachedBuffer = null;        return ByteBuffer.allocate(minCapacity);    }}
f2964
0
release
public void kafkatest_f2965_0(ByteBuffer buffer)
{    buffer.clear();    cachedBuffer = buffer;}
f2965
0
getAndCreateEstimationIfAbsent
private static float[] kafkatest_f2974_0(String topic)
{    float[] compressionRatioForTopic = COMPRESSION_RATIO.get(topic);    if (compressionRatioForTopic == null) {        compressionRatioForTopic = initialCompressionRatio();        float[] existingCompressionRatio = COMPRESSION_RATIO.putIfAbsent(topic, compressionRatioForTopic);        // Someone created the compression ratio array before us, use it.        if (existingCompressionRatio != null)            return existingCompressionRatio;    }    return compressionRatioForTopic;}
f2974
0
initialCompressionRatio
private static float[] kafkatest_f2975_0()
{    float[] compressionRatio = new float[CompressionType.values().length];    for (CompressionType type : CompressionType.values()) {        compressionRatio[type.id] = type.rate;    }    return compressionRatio;}
f2975
0
wrapForInput
public InputStream kafkatest_f2984_0(ByteBuffer buffer, byte messageVersion, BufferSupplier decompressionBufferSupplier)
{    try {        return (InputStream) SnappyConstructors.INPUT.invoke(new ByteBufferInputStream(buffer));    } catch (Throwable e) {        throw new KafkaException(e);    }}
f2984
0
wrapForOutput
public OutputStream kafkatest_f2985_0(ByteBufferOutputStream buffer, byte messageVersion)
{    try {        return new KafkaLZ4BlockOutputStream(buffer, messageVersion == RecordBatch.MAGIC_VALUE_V0);    } catch (Throwable e) {        throw new KafkaException(e);    }}
f2985
0
recordConversionStats
public RecordConversionStats kafkatest_f2994_0()
{    return recordConversionStats;}
f2994
0
offset
public long kafkatest_f2995_0()
{    return offset;}
f2995
0
hasKey
public boolean kafkatest_f3005_0()
{    return key != null;}
f3005
0
key
public ByteBuffer kafkatest_f3006_0()
{    return key == null ? null : key.duplicate();}
f3006
0
equals
public boolean kafkatest_f3015_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    DefaultRecord that = (DefaultRecord) o;    return sizeInBytes == that.sizeInBytes && attributes == that.attributes && offset == that.offset && timestamp == that.timestamp && sequence == that.sequence && Objects.equals(key, that.key) && Objects.equals(value, that.value) && Arrays.equals(headers, that.headers);}
f3015
0
hashCode
public int kafkatest_f3016_0()
{    int result = sizeInBytes;    result = 31 * result + (int) attributes;    result = 31 * result + Long.hashCode(offset);    result = 31 * result + Long.hashCode(timestamp);    result = 31 * result + sequence;    result = 31 * result + (key != null ? key.hashCode() : 0);    result = 31 * result + (value != null ? value.hashCode() : 0);    result = 31 * result + Arrays.hashCode(headers);    return result;}
f3016
0
skipLengthDelimitedField
private static int kafkatest_f3025_0(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException
{    boolean needMore = false;    int sizeInBytes = -1;    int bytesToSkip = -1;    while (true) {        if (needMore) {            readMore(buffer, input, bytesRemaining);            needMore = false;        }        if (bytesToSkip < 0) {            if (buffer.remaining() < 5 && bytesRemaining.value > 0) {                needMore = true;            } else {                sizeInBytes = ByteUtils.readVarint(buffer);                if (sizeInBytes <= 0)                    return sizeInBytes;                else                    bytesToSkip = sizeInBytes;            }        } else {            if (bytesToSkip > buffer.remaining()) {                bytesToSkip -= buffer.remaining();                buffer.position(buffer.limit());                needMore = true;            } else {                buffer.position(buffer.position() + bytesToSkip);                return sizeInBytes;            }        }    }}
f3025
0
readMore
private static void kafkatest_f3026_0(ByteBuffer buffer, DataInput input, IntRef bytesRemaining) throws IOException
{    if (bytesRemaining.value > 0) {        byte[] array = buffer.array();        // first copy the remaining bytes to the beginning of the array;        // at most 4 bytes would be shifted here        int stepsToLeftShift = buffer.position();        int bytesToLeftShift = buffer.remaining();        for (int i = 0; i < bytesToLeftShift; i++) {            array[i] = array[i + stepsToLeftShift];        }        // then try to read more bytes to the remaining of the array        int bytesRead = Math.min(bytesRemaining.value, array.length - bytesToLeftShift);        input.readFully(array, bytesToLeftShift, bytesRead);        buffer.rewind();        // only those many bytes are readable        buffer.limit(bytesToLeftShift + bytesRead);        bytesRemaining.value -= bytesRead;    } else {        throw new InvalidRecordException("Invalid record size: expected to read more bytes in record payload");    }}
f3026
0
magic
public byte kafkatest_f3035_0()
{    return buffer.get(MAGIC_OFFSET);}
f3035
0
ensureValid
public void kafkatest_f3036_0()
{    if (sizeInBytes() < RECORD_BATCH_OVERHEAD)        throw new InvalidRecordException("Record batch is corrupt (the size " + sizeInBytes() + " is smaller than the minimum allowed overhead " + RECORD_BATCH_OVERHEAD + ")");    if (!isValid())        throw new InvalidRecordException("Record is corrupt (stored crc = " + checksum() + ", computed crc = " + computeChecksum() + ")");}
f3036
0
lastOffsetDelta
private int kafkatest_f3045_0()
{    return buffer.getInt(LAST_OFFSET_DELTA_OFFSET);}
f3045
0
lastSequence
public int kafkatest_f3046_0()
{    int baseSequence = baseSequence();    if (baseSequence == RecordBatch.NO_SEQUENCE)        return RecordBatch.NO_SEQUENCE;    return incrementSequence(baseSequence, lastOffsetDelta());}
f3046
0
partitionLeaderEpoch
public int kafkatest_f3055_0()
{    return buffer.getInt(PARTITION_LEADER_EPOCH_OFFSET);}
f3055
0
compressedIterator
private CloseableIterator<Record> kafkatest_f3056_0(BufferSupplier bufferSupplier, boolean skipKeyValue)
{    final ByteBuffer buffer = this.buffer.duplicate();    buffer.position(RECORDS_OFFSET);    final DataInputStream inputStream = new DataInputStream(compressionType().wrapForInput(buffer, magic(), bufferSupplier));    if (skipKeyValue) {        // this buffer is used to skip length delimited fields like key, value, headers        byte[] skipArray = new byte[MAX_SKIP_BUFFER_SIZE];        return new StreamRecordIterator(inputStream) {            @Override            protected Record doReadRecord(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime) throws IOException {                return DefaultRecord.readPartiallyFrom(inputStream, skipArray, baseOffset, firstTimestamp, baseSequence, logAppendTime);            }        };    } else {        return new StreamRecordIterator(inputStream) {            @Override            protected Record doReadRecord(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime) throws IOException {                return DefaultRecord.readFrom(inputStream, baseOffset, firstTimestamp, baseSequence, logAppendTime);            }        };    }}
f3056
0
setLastOffset
public void kafkatest_f3066_0(long offset)
{    buffer.putLong(BASE_OFFSET_OFFSET, offset - lastOffsetDelta());}
f3066
0
setMaxTimestamp
public void kafkatest_f3067_0(TimestampType timestampType, long maxTimestamp)
{    long currentMaxTimestamp = maxTimestamp();    // We don't need to recompute crc if the timestamp is not updated.    if (timestampType() == timestampType && currentMaxTimestamp == maxTimestamp)        return;    byte attributes = computeAttributes(compressionType(), timestampType, isTransactional(), isControlBatch());    buffer.putShort(ATTRIBUTES_OFFSET, attributes);    buffer.putLong(MAX_TIMESTAMP_OFFSET, maxTimestamp);    long crc = computeChecksum();    ByteUtils.writeUnsignedInt(buffer, CRC_OFFSET, crc);}
f3067
0
writeEmptyHeader
public static void kafkatest_f3076_0(ByteBuffer buffer, byte magic, long producerId, short producerEpoch, int baseSequence, long baseOffset, long lastOffset, int partitionLeaderEpoch, TimestampType timestampType, long timestamp, boolean isTransactional, boolean isControlRecord)
{    int offsetDelta = (int) (lastOffset - baseOffset);    writeHeader(buffer, baseOffset, offsetDelta, DefaultRecordBatch.RECORD_BATCH_OVERHEAD, magic, CompressionType.NONE, timestampType, RecordBatch.NO_TIMESTAMP, timestamp, producerId, producerEpoch, baseSequence, isTransactional, isControlRecord, partitionLeaderEpoch, 0);}
f3076
0
writeHeader
 static void kafkatest_f3077_0(ByteBuffer buffer, long baseOffset, int lastOffsetDelta, int sizeInBytes, byte magic, CompressionType compressionType, TimestampType timestampType, long firstTimestamp, long maxTimestamp, long producerId, short epoch, int sequence, boolean isTransactional, boolean isControlBatch, int partitionLeaderEpoch, int numRecords)
{    if (magic < RecordBatch.CURRENT_MAGIC_VALUE)        throw new IllegalArgumentException("Invalid magic value " + magic);    if (firstTimestamp < 0 && firstTimestamp != NO_TIMESTAMP)        throw new IllegalArgumentException("Invalid message timestamp " + firstTimestamp);    short attributes = computeAttributes(compressionType, timestampType, isTransactional, isControlBatch);    int position = buffer.position();    buffer.putLong(position + BASE_OFFSET_OFFSET, baseOffset);    buffer.putInt(position + LENGTH_OFFSET, sizeInBytes - LOG_OVERHEAD);    buffer.putInt(position + PARTITION_LEADER_EPOCH_OFFSET, partitionLeaderEpoch);    buffer.put(position + MAGIC_OFFSET, magic);    buffer.putShort(position + ATTRIBUTES_OFFSET, attributes);    buffer.putLong(position + FIRST_TIMESTAMP_OFFSET, firstTimestamp);    buffer.putLong(position + MAX_TIMESTAMP_OFFSET, maxTimestamp);    buffer.putInt(position + LAST_OFFSET_DELTA_OFFSET, lastOffsetDelta);    buffer.putLong(position + PRODUCER_ID_OFFSET, producerId);    buffer.putShort(position + PRODUCER_EPOCH_OFFSET, epoch);    buffer.putInt(position + BASE_SEQUENCE_OFFSET, sequence);    buffer.putInt(position + RECORDS_COUNT_OFFSET, numRecords);    long crc = Crc32C.compute(buffer, ATTRIBUTES_OFFSET, sizeInBytes - ATTRIBUTES_OFFSET);    buffer.putInt(position + CRC_OFFSET, (int) crc);    buffer.position(position + RECORD_BATCH_OVERHEAD);}
f3077
0
remove
public void kafkatest_f3086_0()
{    throw new UnsupportedOperationException();}
f3086
0
readNext
protected Record kafkatest_f3087_0(long baseOffset, long firstTimestamp, int baseSequence, Long logAppendTime)
{    try {        return doReadRecord(baseOffset, firstTimestamp, baseSequence, logAppendTime);    } catch (EOFException e) {        throw new InvalidRecordException("Incorrect declared batch size, premature EOF reached");    } catch (IOException e) {        throw new KafkaException("Failed to decompress record stream", e);    }}
f3087
0
lastSequence
public int kafkatest_f3096_0()
{    return loadBatchHeader().lastSequence();}
f3096
0
checksum
public long kafkatest_f3097_0()
{    return loadBatchHeader().checksum();}
f3097
0
buildRecordValue
private Struct kafkatest_f3106_0()
{    Struct struct = new Struct(END_TXN_MARKER_SCHEMA_VERSION_V0);    struct.set("version", CURRENT_END_TXN_MARKER_VERSION);    struct.set("coordinator_epoch", coordinatorEpoch);    return struct;}
f3106
0
serializeValue
public ByteBuffer kafkatest_f3107_0()
{    Struct valueStruct = buildRecordValue();    ByteBuffer value = ByteBuffer.allocate(valueStruct.sizeOf());    valueStruct.writeTo(value);    value.flip();    return value;}
f3107
0
checksum
public long kafkatest_f3116_0()
{    return loadBatchHeader().checksum();}
f3116
0
maxTimestamp
public long kafkatest_f3117_0()
{    return loadBatchHeader().maxTimestamp();}
f3117
0
loadFullBatch
protected RecordBatch kafkatest_f3126_0()
{    if (fullBatch == null) {        batchHeader = null;        fullBatch = loadBatchWithSize(sizeInBytes(), "full record batch");    }    return fullBatch;}
f3126
0
loadBatchHeader
protected RecordBatch kafkatest_f3127_0()
{    if (fullBatch != null)        return fullBatch;    if (batchHeader == null)        batchHeader = loadBatchWithSize(headerSize(), "record batch header");    return batchHeader;}
f3127
0
slice
public FileRecords kafkatest_f3136_0(int position, int size) throws IOException
{    if (position < 0)        throw new IllegalArgumentException("Invalid position: " + position + " in read from " + this);    if (position > sizeInBytes() - start)        throw new IllegalArgumentException("Slice from position " + position + " exceeds end position of " + this);    if (size < 0)        throw new IllegalArgumentException("Invalid size: " + size + " in read from " + this);    int end = this.start + position + size;    // handle integer overflow or if end is beyond the end of the file    if (end < 0 || end >= start + sizeInBytes())        end = start + sizeInBytes();    return new FileRecords(file, channel, this.start + position, end, true);}
f3136
0
append
public int kafkatest_f3137_0(MemoryRecords records) throws IOException
{    if (records.sizeInBytes() > Integer.MAX_VALUE - size.get())        throw new IllegalArgumentException("Append of size " + records.sizeInBytes() + " bytes is too large for segment with current file position at " + size.get());    int written = records.writeFullyTo(channel);    size.getAndAdd(written);    return written;}
f3137
0
downConvert
public ConvertedRecords<? extends Records> kafkatest_f3146_0(byte toMagic, long firstOffset, Time time)
{    ConvertedRecords<MemoryRecords> convertedRecords = RecordsUtil.downConvert(batches, toMagic, firstOffset, time);    if (convertedRecords.recordConversionStats().numRecordsConverted() == 0) {        // one full record batch, even if it requires exceeding the max fetch size requested by the client.        return new ConvertedRecords<>(this, RecordConversionStats.EMPTY);    } else {        return convertedRecords;    }}
f3146
0
writeTo
public long kafkatest_f3147_0(GatheringByteChannel destChannel, long offset, int length) throws IOException
{    long newSize = Math.min(channel.size(), end) - start;    int oldSize = sizeInBytes();    if (newSize < oldSize)        throw new KafkaException(String.format("Size of FileRecords %s has been truncated during write: old size %d, new size %d", file.getAbsolutePath(), oldSize, newSize));    long position = start + offset;    int count = Math.min(length, oldSize);    final long bytesTransferred;    if (destChannel instanceof TransportLayer) {        TransportLayer tl = (TransportLayer) destChannel;        bytesTransferred = tl.transferFrom(channel, position, count);    } else {        bytesTransferred = channel.transferTo(position, count, destChannel);    }    return bytesTransferred;}
f3147
0
batchIterator
private AbstractIterator<FileChannelRecordBatch> kafkatest_f3156_0(int start)
{    final int end;    if (isSlice)        end = this.end;    else        end = this.sizeInBytes();    FileLogInputStream inputStream = new FileLogInputStream(this, start, end);    return new RecordBatchIterator<>(inputStream);}
f3156
0
open
public static FileRecords kafkatest_f3157_0(File file, boolean mutable, boolean fileAlreadyExists, int initFileSize, boolean preallocate) throws IOException
{    FileChannel channel = openChannel(file, mutable, fileAlreadyExists, initFileSize, preallocate);    int end = (!fileAlreadyExists && preallocate) ? 0 : Integer.MAX_VALUE;    return new FileRecords(file, channel, 0, end, false);}
f3157
0
hashCode
public int kafkatest_f3166_0()
{    return Objects.hash(timestamp, offset, leaderEpoch);}
f3166
0
toString
public String kafkatest_f3167_0()
{    return "TimestampAndOffset(" + "timestamp=" + timestamp + ", offset=" + offset + ", leaderEpoch=" + leaderEpoch + ')';}
f3167
0
mark
public void kafkatest_f3176_0(int readlimit)
{    throw new RuntimeException("mark not supported");}
f3176
0
reset
public void kafkatest_f3177_0()
{    throw new RuntimeException("reset not supported");}
f3177
0
ensureNotFinished
private void kafkatest_f3186_0()
{    if (finished) {        throw new IllegalStateException(CLOSED_STREAM);    }}
f3186
0
close
public void kafkatest_f3187_0() throws IOException
{    try {        if (!finished) {            // basically flush the buffer writing the last block            writeBlock();            // write the end block            writeEndMark();        }    } finally {        try {            if (out != null) {                try (OutputStream outStream = out) {                    outStream.flush();                }            }        } finally {            out = null;            buffer = null;            compressedBuffer = null;            finished = true;        }    }}
f3187
0
fromByte
public static BD kafkatest_f3196_0(byte bd)
{    int reserved2 = (bd >>> 0) & 15;    int blockMaximumSize = (bd >>> 4) & 7;    int reserved3 = (bd >>> 7) & 1;    return new BD(reserved2, blockMaximumSize, reserved3);}
f3196
0
validate
private void kafkatest_f3197_0()
{    if (reserved2 != 0) {        throw new RuntimeException("Reserved2 field must be 0");    }    if (blockSizeValue < 4 || blockSizeValue > 7) {        throw new RuntimeException("Block size value must be between 4 and 7");    }    if (reserved3 != 0) {        throw new RuntimeException("Reserved3 field must be 0");    }}
f3197
0
iterator
public java.util.Iterator<ConvertedRecords<?>> kafkatest_f3206_0(long maximumReadSize)
{    // We typically expect only one iterator instance to be created, so null out the first converted batch after    // first use to make it available for GC.    ConvertedRecords firstBatch = firstConvertedBatch;    firstConvertedBatch = null;    return new Iterator(records, maximumReadSize, firstBatch);}
f3206
0
makeNext
protected ConvertedRecords kafkatest_f3207_0()
{    // If we have cached the first down-converted batch, return that now    if (firstConvertedBatch != null) {        ConvertedRecords convertedBatch = firstConvertedBatch;        firstConvertedBatch = null;        return convertedBatch;    }    while (batchIterator.hasNext()) {        final List<RecordBatch> batches = new ArrayList<>();        boolean isFirstBatch = true;        long sizeSoFar = 0;        // Figure out batches we should down-convert based on the size constraints        while (batchIterator.hasNext() && (isFirstBatch || (batchIterator.peek().sizeInBytes() + sizeSoFar) <= maximumReadSize)) {            RecordBatch currentBatch = batchIterator.next();            batches.add(currentBatch);            sizeSoFar += currentBatch.sizeInBytes();            isFirstBatch = false;        }        ConvertedRecords convertedRecords = RecordsUtil.downConvert(batches, toMagic, firstOffset, time);        // We return converted records only when we have at least one valid batch of messages after conversion.        if (convertedRecords.records().sizeInBytes() > 0)            return convertedRecords;    }    return allDone();}
f3207
0
wrapperRecordTimestampType
public TimestampType kafkatest_f3216_0()
{    return wrapperRecordTimestampType;}
f3216
0
ensureValid
public void kafkatest_f3217_0()
{    if (sizeInBytes() < RECORD_OVERHEAD_V0)        throw new InvalidRecordException("Record is corrupt (crc could not be retrieved as the record is too " + "small, size = " + sizeInBytes() + ")");    if (!isValid())        throw new InvalidRecordException("Record is corrupt (stored crc = " + checksum() + ", computed crc = " + computeChecksum() + ")");}
f3217
0
timestamp
public long kafkatest_f3226_0()
{    if (magic() == RecordBatch.MAGIC_VALUE_V0)        return RecordBatch.NO_TIMESTAMP;    else {        // case 2        if (wrapperRecordTimestampType == TimestampType.LOG_APPEND_TIME && wrapperRecordTimestamp != null)            return wrapperRecordTimestamp;        else            return buffer.getLong(TIMESTAMP_OFFSET);    }}
f3226
0
timestampType
public TimestampType kafkatest_f3227_0()
{    return timestampType(magic(), wrapperRecordTimestampType, attributes());}
f3227
0
create
public static LegacyRecord kafkatest_f3236_0(byte magic, long timestamp, byte[] key, byte[] value)
{    return create(magic, timestamp, key, value, CompressionType.NONE, TimestampType.CREATE_TIME);}
f3236
0
writeCompressedRecordHeader
public static void kafkatest_f3237_0(ByteBuffer buffer, byte magic, int recordSize, long timestamp, CompressionType compressionType, TimestampType timestampType)
{    int recordPosition = buffer.position();    int valueSize = recordSize - recordOverhead(magic);    // write the record header with a null value (the key is always null for the wrapper)    write(buffer, magic, timestamp, null, null, compressionType, timestampType);    buffer.position(recordPosition);    // now fill in the value size    buffer.putInt(recordPosition + keyOffset(magic), valueSize);    // compute and fill the crc from the beginning of the message    long crc = Crc32.crc32(buffer, MAGIC_OFFSET, recordSize - MAGIC_OFFSET);    ByteUtils.writeUnsignedInt(buffer, recordPosition + CRC_OFFSET, crc);}
f3237
0
computeChecksum
public static long kafkatest_f3246_0(byte magic, byte attributes, long timestamp, byte[] key, byte[] value)
{    return computeChecksum(magic, attributes, timestamp, wrapNullable(key), wrapNullable(value));}
f3246
0
computeChecksum
private static long kafkatest_f3247_0(byte magic, byte attributes, long timestamp, ByteBuffer key, ByteBuffer value)
{    Crc32 crc = new Crc32();    crc.update(magic);    crc.update(attributes);    if (magic > RecordBatch.MAGIC_VALUE_V0)        Checksums.updateLong(crc, timestamp);    // update for the key    if (key == null) {        Checksums.updateInt(crc, -1);    } else {        int size = key.remaining();        Checksums.updateInt(crc, size);        Checksums.update(crc, key, size);    }    // update for the value    if (value == null) {        Checksums.updateInt(crc, -1);    } else {        int size = value.remaining();        Checksums.updateInt(crc, size);        Checksums.update(crc, value, size);    }    return crc.getValue();}
f3247
0
downConvert
public ConvertedRecords<MemoryRecords> kafkatest_f3256_0(byte toMagic, long firstOffset, Time time)
{    return RecordsUtil.downConvert(batches(), toMagic, firstOffset, time);}
f3256
0
batchIterator
public AbstractIterator<MutableRecordBatch> kafkatest_f3257_0()
{    return new RecordBatchIterator<>(new ByteBufferLogInputStream(buffer.duplicate(), Integer.MAX_VALUE));}
f3257
0
hashCode
public int kafkatest_f3266_0()
{    return buffer.hashCode();}
f3266
0
updateRetainedBatchMetadata
private void kafkatest_f3267_0(MutableRecordBatch retainedBatch, int numMessagesInBatch, boolean headerOnly)
{    int bytesRetained = headerOnly ? DefaultRecordBatch.RECORD_BATCH_OVERHEAD : retainedBatch.sizeInBytes();    updateRetainedBatchMetadata(retainedBatch.maxTimestamp(), retainedBatch.lastOffset(), retainedBatch.lastOffset(), numMessagesInBatch, bytesRetained);}
f3267
0
maxTimestamp
public long kafkatest_f3276_0()
{    return maxTimestamp;}
f3276
0
shallowOffsetOfMaxTimestamp
public long kafkatest_f3277_0()
{    return shallowOffsetOfMaxTimestamp;}
f3277
0
builder
public static MemoryRecordsBuilder kafkatest_f3286_0(ByteBuffer buffer, byte magic, CompressionType compressionType, TimestampType timestampType, long baseOffset, long logAppendTime, long producerId, short producerEpoch, int baseSequence, boolean isTransactional, int partitionLeaderEpoch)
{    return builder(buffer, magic, compressionType, timestampType, baseOffset, logAppendTime, producerId, producerEpoch, baseSequence, isTransactional, false, partitionLeaderEpoch);}
f3286
0
builder
public static MemoryRecordsBuilder kafkatest_f3287_0(ByteBuffer buffer, byte magic, CompressionType compressionType, TimestampType timestampType, long baseOffset, long logAppendTime, long producerId, short producerEpoch, int baseSequence, boolean isTransactional, boolean isControlBatch, int partitionLeaderEpoch)
{    return new MemoryRecordsBuilder(buffer, magic, compressionType, timestampType, baseOffset, logAppendTime, producerId, producerEpoch, baseSequence, isTransactional, isControlBatch, partitionLeaderEpoch, buffer.remaining());}
f3287
0
withIdempotentRecords
public static MemoryRecords kafkatest_f3296_0(long initialOffset, CompressionType compressionType, long producerId, short producerEpoch, int baseSequence, int partitionLeaderEpoch, SimpleRecord... records)
{    return withRecords(RecordBatch.CURRENT_MAGIC_VALUE, initialOffset, compressionType, TimestampType.CREATE_TIME, producerId, producerEpoch, baseSequence, partitionLeaderEpoch, false, records);}
f3296
0
withTransactionalRecords
public static MemoryRecords kafkatest_f3297_0(CompressionType compressionType, long producerId, short producerEpoch, int baseSequence, SimpleRecord... records)
{    return withRecords(RecordBatch.CURRENT_MAGIC_VALUE, 0L, compressionType, TimestampType.CREATE_TIME, producerId, producerEpoch, baseSequence, RecordBatch.NO_PARTITION_LEADER_EPOCH, true, records);}
f3297
0
write
public void kafkatest_f3306_0(int b)
{    throw new IllegalStateException("MemoryRecordsBuilder is closed for record appends");}
f3306
0
buffer
public ByteBuffer kafkatest_f3307_0()
{    return bufferStream.buffer();}
f3307
0
uncompressedBytesWritten
public int kafkatest_f3316_0()
{    return uncompressedRecordsSizeInBytes + batchHeaderSizeInBytes;}
f3316
0
setProducerState
public void kafkatest_f3317_0(long producerId, short producerEpoch, int baseSequence, boolean isTransactional)
{    if (isClosed()) {        // once a batch has been sent to the broker risks introducing duplicates.        throw new IllegalStateException("Trying to set producer state of an already closed batch. This indicates a bug on the client.");    }    this.producerId = producerId;    this.producerEpoch = producerEpoch;    this.baseSequence = baseSequence;    this.isTransactional = isTransactional;}
f3317
0
appendWithOffset
private Long kafkatest_f3326_0(long offset, boolean isControlRecord, long timestamp, ByteBuffer key, ByteBuffer value, Header[] headers)
{    try {        if (isControlRecord != isControlBatch)            throw new IllegalArgumentException("Control records can only be appended to control batches");        if (lastOffset != null && offset <= lastOffset)            throw new IllegalArgumentException(String.format("Illegal offset %s following previous offset %s " + "(Offsets must increase monotonically).", offset, lastOffset));        if (timestamp < 0 && timestamp != RecordBatch.NO_TIMESTAMP)            throw new IllegalArgumentException("Invalid negative timestamp " + timestamp);        if (magic < RecordBatch.MAGIC_VALUE_V2 && headers != null && headers.length > 0)            throw new IllegalArgumentException("Magic v" + magic + " does not support record headers");        if (firstTimestamp == null)            firstTimestamp = timestamp;        if (magic > RecordBatch.MAGIC_VALUE_V1) {            appendDefaultRecord(offset, timestamp, key, value, headers);            return null;        } else {            return appendLegacyRecord(offset, timestamp, key, value, magic);        }    } catch (IOException e) {        throw new KafkaException("I/O exception when writing to the append stream, closing", e);    }}
f3326
0
appendWithOffset
public Long kafkatest_f3327_0(long offset, long timestamp, byte[] key, byte[] value, Header[] headers)
{    return appendWithOffset(offset, false, timestamp, wrapNullable(key), wrapNullable(value), headers);}
f3327
0
append
public Long kafkatest_f3336_0(SimpleRecord record)
{    return appendWithOffset(nextSequentialOffset(), record);}
f3336
0
appendControlRecord
private Long kafkatest_f3337_0(long timestamp, ControlRecordType type, ByteBuffer value)
{    Struct keyStruct = type.recordKey();    ByteBuffer key = ByteBuffer.allocate(keyStruct.sizeOf());    keyStruct.writeTo(key);    key.flip();    return appendWithOffset(nextSequentialOffset(), true, timestamp, key, value, Record.EMPTY_HEADERS);}
f3337
0
toInnerOffset
private long kafkatest_f3346_0(long offset)
{    // use relative offsets for compressed messages with magic v1    if (magic > 0 && compressionType != CompressionType.NONE)        return offset - baseOffset;    return offset;}
f3346
0
recordWritten
private void kafkatest_f3347_0(long offset, long timestamp, int size)
{    if (numRecords == Integer.MAX_VALUE)        throw new IllegalArgumentException("Maximum number of records per batch exceeded, max records: " + Integer.MAX_VALUE);    if (offset - baseOffset > Integer.MAX_VALUE)        throw new IllegalArgumentException("Maximum offset delta exceeded, base offset: " + baseOffset + ", last offset: " + offset);    numRecords += 1;    uncompressedRecordsSizeInBytes += size;    lastOffset = offset;    if (magic > RecordBatch.MAGIC_VALUE_V0 && timestamp > maxTimestamp) {        maxTimestamp = timestamp;        offsetOfMaxTimestamp = offset;    }}
f3347
0
estimatedSizeInBytes
public int kafkatest_f3356_0()
{    return builtRecords != null ? builtRecords.sizeInBytes() : estimatedBytesWritten();}
f3356
0
magic
public byte kafkatest_f3357_0()
{    return magic;}
f3357
0
writeTo
public longf3366_1GatheringByteChannel channel) throws IOException
{    if (completed())        throw new KafkaException("This operation cannot be invoked on a complete request.");    int totalWrittenPerCall = 0;    boolean sendComplete;    do {        long written = current.writeTo(channel);        totalWrittenPerCall += written;        sendComplete = current.completed();        if (sendComplete) {            updateRecordConversionStats(current);            current = sendQueue.poll();        }    } while (!completed() && sendComplete);    totalWritten += totalWrittenPerCall;    if (completed() && totalWritten != size)         expected: {} actual: {}", size, totalWritten);    log.trace("Bytes written as part of multi-send call: {}, total bytes written so far: {}, expected bytes to write: {}", totalWrittenPerCall, totalWritten, size);    return totalWrittenPerCall;}
public longf3366
1
recordConversionStats
public Map<TopicPartition, RecordConversionStats> kafkatest_f3367_0()
{    return recordConversionStats;}
f3367
0
hasValue
public boolean kafkatest_f3376_0()
{    return valueSize >= 0;}
f3376
0
value
public ByteBuffer kafkatest_f3377_0()
{    throw new UnsupportedOperationException("value is skipped in PartialDefaultRecord");}
f3377
0
completed
public boolean kafkatest_f3386_0()
{    return remaining <= 0 && !pending;}
f3386
0
writeTo
public final long kafkatest_f3387_0(GatheringByteChannel channel) throws IOException
{    long written = 0;    if (remaining > 0) {        written = writeTo(channel, size() - remaining, remaining);        if (written < 0)            throw new EOFException("Wrote negative bytes to channel. This shouldn't happen.");        remaining -= written;    }    pending = TransportLayers.hasPendingWrites(channel);    if (remaining <= 0 && pending)        channel.write(EMPTY_BYTE_BUFFER);    return written;}
f3387
0
value
public ByteBuffer kafkatest_f3396_0()
{    return value;}
f3396
0
timestamp
public long kafkatest_f3397_0()
{    return timestamp;}
f3397
0
clientAddress
public InetAddress kafkatest_f3406_0()
{    return clientAddress;}
f3406
0
principal
public KafkaPrincipal kafkatest_f3407_0()
{    return principal;}
f3407
0
toString
public String kafkatest_f3416_0()
{    return "DefaultPartitionView{" + "replicas=" + replicas + ", leader=" + leader + '}';}
f3416
0
select
public Optional<ReplicaView> kafkatest_f3417_0(TopicPartition topicPartition, ClientMetadata clientMetadata, PartitionView partitionView)
{    if (clientMetadata.rackId() != null && !clientMetadata.rackId().isEmpty()) {        Set<ReplicaView> sameRackReplicas = partitionView.replicas().stream().filter(replicaInfo -> clientMetadata.rackId().equals(replicaInfo.endpoint().rack())).collect(Collectors.toSet());        if (sameRackReplicas.isEmpty()) {            return Optional.of(partitionView.leader());        } else {            if (sameRackReplicas.contains(partitionView.leader())) {                // Use the leader if it's in this rack                return Optional.of(partitionView.leader());            } else {                // Otherwise, get the most caught-up replica                return sameRackReplicas.stream().max(ReplicaView.comparator());            }        }    } else {        return Optional.of(partitionView.leader());    }}
f3417
0
toString
public String kafkatest_f3426_0()
{    return "DefaultReplicaView{" + "endpoint=" + endpoint + ", logEndOffset=" + logEndOffset + ", timeSinceLastCaughtUpMs=" + timeSinceLastCaughtUpMs + '}';}
f3426
0
controllerId
public int kafkatest_f3427_0()
{    return controllerId;}
f3427
0
toSend
public Send kafkatest_f3436_0(String destination, RequestHeader header)
{    return new NetworkSend(destination, serialize(header));}
f3436
0
serialize
public ByteBuffer kafkatest_f3437_0(RequestHeader header)
{    return serialize(header.toStruct(), toStruct());}
f3437
0
errorCounts
protected Map<Errors, Integer> kafkatest_f3446_0(Errors error)
{    return Collections.singletonMap(error, 1);}
f3446
0
errorCounts
protected Map<Errors, Integer> kafkatest_f3447_0(Map<?, Errors> errors)
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (Errors error : errors.values()) updateErrorCounts(errorCounts, error);    return errorCounts;}
f3447
0
build
public AddOffsetsToTxnRequest kafkatest_f3456_0(short version)
{    return new AddOffsetsToTxnRequest(version, transactionalId, producerId, producerEpoch, consumerGroupId);}
f3456
0
toString
public String kafkatest_f3457_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=AddOffsetsToTxnRequest").append(", transactionalId=").append(transactionalId).append(", producerId=").append(producerId).append(", producerEpoch=").append(producerEpoch).append(", consumerGroupId=").append(consumerGroupId).append(")");    return bld.toString();}
f3457
0
throttleTimeMs
public int kafkatest_f3466_0()
{    return throttleTimeMs;}
f3466
0
error
public Errors kafkatest_f3467_0()
{    return error;}
f3467
0
toString
public String kafkatest_f3476_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=AddPartitionsToTxnRequest").append(", transactionalId=").append(transactionalId).append(", producerId=").append(producerId).append(", producerEpoch=").append(producerEpoch).append(", partitions=").append(partitions).append(")");    return bld.toString();}
f3476
0
transactionalId
public String kafkatest_f3477_0()
{    return transactionalId;}
f3477
0
errors
public Map<TopicPartition, Errors> kafkatest_f3486_0()
{    return errors;}
f3486
0
errorCounts
public Map<Errors, Integer> kafkatest_f3487_0()
{    return errorCounts(errors);}
f3487
0
build
public AlterConfigsRequest kafkatest_f3496_0(short version)
{    return new AlterConfigsRequest(version, configs, validateOnly);}
f3496
0
configs
public Map<ConfigResource, Config> kafkatest_f3497_0()
{    return configs;}
f3497
0
toStruct
protected Struct kafkatest_f3506_0(short version)
{    Struct struct = new Struct(ApiKeys.ALTER_CONFIGS.responseSchema(version));    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    List<Struct> resourceStructs = new ArrayList<>(errors.size());    for (Map.Entry<ConfigResource, ApiError> entry : errors.entrySet()) {        Struct resourceStruct = struct.instance(RESOURCES_KEY_NAME);        ConfigResource resource = entry.getKey();        entry.getValue().write(resourceStruct);        resourceStruct.set(RESOURCE_TYPE_KEY_NAME, resource.type().id());        resourceStruct.set(RESOURCE_NAME_KEY_NAME, resource.name());        resourceStructs.add(resourceStruct);    }    struct.set(RESOURCES_KEY_NAME, resourceStructs.toArray(new Struct[0]));    return struct;}
f3506
0
parse
public static AlterConfigsResponse kafkatest_f3507_0(ByteBuffer buffer, short version)
{    return new AlterConfigsResponse(ApiKeys.ALTER_CONFIGS.parseResponse(version, buffer));}
f3507
0
data
public AlterPartitionReassignmentsResponseData kafkatest_f3516_0()
{    return data;}
f3516
0
shouldClientThrottle
public boolean kafkatest_f3517_0(short version)
{    return true;}
f3517
0
partitionDirs
public Map<TopicPartition, String> kafkatest_f3526_0()
{    return partitionDirs;}
f3526
0
parse
public static AlterReplicaLogDirsRequest kafkatest_f3527_0(ByteBuffer buffer, short version)
{    return new AlterReplicaLogDirsRequest(ApiKeys.ALTER_REPLICA_LOG_DIRS.parseRequest(version, buffer), version);}
f3527
0
write
public void kafkatest_f3536_0(Struct struct)
{    struct.set(ERROR_CODE, error.code());    if (error != Errors.NONE)        struct.setIfExists(ERROR_MESSAGE, message);}
f3536
0
is
public boolean kafkatest_f3537_0(Errors error)
{    return this.error == error;}
f3537
0
build
public ApiVersionsRequest kafkatest_f3546_0(short version)
{    return new ApiVersionsRequest(version);}
f3546
0
toString
public String kafkatest_f3547_0()
{    return "(type=ApiVersionsRequest)";}
f3547
0
throttleTimeMs
public int kafkatest_f3556_0()
{    return throttleTimeMs;}
f3556
0
apiVersions
public Collection<ApiVersion> kafkatest_f3557_0()
{    return apiKeyToApiVersion.values();}
f3557
0
build
public ControlledShutdownRequest kafkatest_f3566_0(short version)
{    return new ControlledShutdownRequest(data, version);}
f3566
0
toString
public String kafkatest_f3567_0()
{    return data.toString();}
f3567
0
data
public ControlledShutdownResponseData kafkatest_f3576_0()
{    return data;}
f3576
0
prepareResponse
public static ControlledShutdownResponse kafkatest_f3577_0(Errors error, Set<TopicPartition> tps)
{    ControlledShutdownResponseData data = new ControlledShutdownResponseData();    data.setErrorCode(error.code());    ControlledShutdownResponseData.RemainingPartitionCollection pSet = new ControlledShutdownResponseData.RemainingPartitionCollection();    tps.forEach(tp -> {        pSet.add(new RemainingPartition().setTopicName(tp.topic()).setPartitionIndex(tp.partition()));    });    data.setRemainingPartitions(pSet);    return new ControlledShutdownResponse(data);}
f3577
0
aclCreations
public List<AclCreation> kafkatest_f3586_0()
{    return aclCreations;}
f3586
0
getErrorResponse
public AbstractResponse kafkatest_f3587_0(int throttleTimeMs, Throwable throwable)
{    short versionId = version();    switch(versionId) {        case 0:        case 1:            List<CreateAclsResponse.AclCreationResponse> responses = new ArrayList<>();            for (int i = 0; i < aclCreations.size(); i++) responses.add(new CreateAclsResponse.AclCreationResponse(ApiError.fromThrowable(throwable)));            return new CreateAclsResponse(throttleTimeMs, responses);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.CREATE_ACLS.latestVersion()));    }}
f3587
0
errorCounts
public Map<Errors, Integer> kafkatest_f3596_0()
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (AclCreationResponse response : aclCreationResponses) updateErrorCounts(errorCounts, response.error.error());    return errorCounts;}
f3596
0
parse
public static CreateAclsResponse kafkatest_f3597_0(ByteBuffer buffer, short version)
{    return new CreateAclsResponse(ApiKeys.CREATE_ACLS.responseSchema(version).read(buffer));}
f3597
0
prepareResponse
public static CreateDelegationTokenResponse kafkatest_f3606_0(int throttleTimeMs, Errors error, KafkaPrincipal owner, long issueTimestamp, long expiryTimestamp, long maxTimestamp, String tokenId, ByteBuffer hmac)
{    CreateDelegationTokenResponseData data = new CreateDelegationTokenResponseData().setThrottleTimeMs(throttleTimeMs).setErrorCode(error.code()).setPrincipalType(owner.getPrincipalType()).setPrincipalName(owner.getName()).setIssueTimestampMs(issueTimestamp).setExpiryTimestampMs(expiryTimestamp).setMaxTimestampMs(maxTimestamp).setTokenId(tokenId).setHmac(hmac.array());    return new CreateDelegationTokenResponse(data);}
f3606
0
prepareResponse
public static CreateDelegationTokenResponse kafkatest_f3607_0(int throttleTimeMs, Errors error, KafkaPrincipal owner)
{    return prepareResponse(throttleTimeMs, error, owner, -1, -1, -1, "", ByteBuffer.wrap(new byte[] {}));}
f3607
0
totalCount
public int kafkatest_f3616_0()
{    return totalCount;}
f3616
0
newAssignments
public List<List<Integer>> kafkatest_f3617_0()
{    return newAssignments;}
f3617
0
getErrorResponse
public AbstractResponse kafkatest_f3626_0(int throttleTimeMs, Throwable e)
{    Map<String, ApiError> topicErrors = new HashMap<>();    for (String topic : newPartitions.keySet()) {        topicErrors.put(topic, ApiError.fromThrowable(e));    }    short versionId = version();    switch(versionId) {        case 0:        case 1:            return new CreatePartitionsResponse(throttleTimeMs, topicErrors);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.CREATE_PARTITIONS.latestVersion()));    }}
f3626
0
parse
public static CreatePartitionsRequest kafkatest_f3627_0(ByteBuffer buffer, short version)
{    return new CreatePartitionsRequest(ApiKeys.CREATE_PARTITIONS.parseRequest(version, buffer), version);}
f3627
0
toString
public String kafkatest_f3636_0()
{    return data.toString();}
f3636
0
data
public CreateTopicsRequestData kafkatest_f3637_0()
{    return data;}
f3637
0
shouldClientThrottle
public boolean kafkatest_f3646_0(short version)
{    return version >= 3;}
f3646
0
schemaVersions
public static Schema[] kafkatest_f3647_0()
{    return new Schema[] { DELETE_ACLS_REQUEST_V0, DELETE_ACLS_REQUEST_V1 };}
f3647
0
error
public ApiError kafkatest_f3656_0()
{    return error;}
f3656
0
acl
public AclBinding kafkatest_f3657_0()
{    return acl;}
f3657
0
parse
public static DeleteAclsResponse kafkatest_f3666_0(ByteBuffer buffer, short version)
{    return new DeleteAclsResponse(ApiKeys.DELETE_ACLS.responseSchema(version).read(buffer));}
f3666
0
toString
public String kafkatest_f3667_0()
{    return "(responses=" + Utils.join(responses, ",") + ")";}
f3667
0
errors
public Map<String, Errors> kafkatest_f3676_0()
{    Map<String, Errors> errorMap = new HashMap<>();    for (DeletableGroupResult result : data.results()) {        errorMap.put(result.groupId(), Errors.forCode(result.errorCode()));    }    return errorMap;}
f3676
0
get
public Errors kafkatest_f3677_0(String group) throws IllegalArgumentException
{    DeletableGroupResult result = data.results().find(group);    if (result == null) {        throw new IllegalArgumentException("could not find group " + group + " in the delete group response");    }    return Errors.forCode(result.errorCode());}
f3677
0
getErrorResponse
public AbstractResponse kafkatest_f3686_0(int throttleTimeMs, Throwable e)
{    Map<TopicPartition, DeleteRecordsResponse.PartitionResponse> responseMap = new HashMap<>();    for (Map.Entry<TopicPartition, Long> entry : partitionOffsets.entrySet()) {        responseMap.put(entry.getKey(), new DeleteRecordsResponse.PartitionResponse(DeleteRecordsResponse.INVALID_LOW_WATERMARK, Errors.forException(e)));    }    short versionId = version();    switch(versionId) {        case 0:        case 1:            return new DeleteRecordsResponse(throttleTimeMs, responseMap);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.DELETE_RECORDS.latestVersion()));    }}
f3686
0
timeout
public int kafkatest_f3687_0()
{    return timeout;}
f3687
0
parse
public static DeleteRecordsResponse kafkatest_f3696_0(ByteBuffer buffer, short version)
{    return new DeleteRecordsResponse(ApiKeys.DELETE_RECORDS.responseSchema(version).read(buffer));}
f3696
0
shouldClientThrottle
public boolean kafkatest_f3697_0(short version)
{    return version >= 1;}
f3697
0
data
public DeleteTopicsResponseData kafkatest_f3706_0()
{    return data;}
f3706
0
errorCounts
public Map<Errors, Integer> kafkatest_f3707_0()
{    HashMap<Errors, Integer> counts = new HashMap<>();    for (DeletableTopicResult result : data.responses()) {        Errors error = Errors.forCode(result.errorCode());        counts.put(error, counts.getOrDefault(error, 0) + 1);    }    return counts;}
f3707
0
filter
public AclBindingFilter kafkatest_f3716_0()
{    return filter;}
f3716
0
validate
private void kafkatest_f3717_0(AclBindingFilter filter, short version)
{    if (version == 0 && filter.patternFilter().patternType() != PatternType.LITERAL && filter.patternFilter().patternType() != PatternType.ANY) {        throw new UnsupportedVersionException("Version 0 only supports literal resource pattern types");    }    if (filter.isUnknown()) {        throw new IllegalArgumentException("Filter contain UNKNOWN elements");    }}
f3717
0
validate
private void kafkatest_f3726_0(short version)
{    if (version == 0) {        final boolean unsupported = acls.stream().map(AclBinding::pattern).map(ResourcePattern::patternType).anyMatch(patternType -> patternType != PatternType.LITERAL);        if (unsupported) {            throw new UnsupportedVersionException("Version 0 only supports literal resource pattern types");        }    }    final boolean unknown = acls.stream().anyMatch(AclBinding::isUnknown);    if (unknown) {        throw new IllegalArgumentException("Contain UNKNOWN elements");    }}
f3726
0
schemaVersions
public static Schema[] kafkatest_f3727_0()
{    return new Schema[] { DESCRIBE_CONFIGS_REQUEST_V0, DESCRIBE_CONFIGS_REQUEST_V1, DESCRIBE_CONFIGS_REQUEST_V2 };}
f3727
0
parse
public static DescribeConfigsRequest kafkatest_f3736_0(ByteBuffer buffer, short version)
{    return new DescribeConfigsRequest(ApiKeys.DESCRIBE_CONFIGS.parseRequest(version, buffer), version);}
f3736
0
schemaVersions
public static Schema[] kafkatest_f3737_0()
{    return new Schema[] { DESCRIBE_CONFIGS_RESPONSE_V0, DESCRIBE_CONFIGS_RESPONSE_V1, DESCRIBE_CONFIGS_RESPONSE_V2 };}
f3737
0
forId
public static ConfigSource kafkatest_f3746_0(byte id)
{    if (id < 0)        throw new IllegalArgumentException("id should be positive, id: " + id);    if (id >= VALUES.length)        return UNKNOWN_CONFIG;    return VALUES[id];}
f3746
0
name
public String kafkatest_f3747_0()
{    return name;}
f3747
0
shouldClientThrottle
public boolean kafkatest_f3756_0(short version)
{    return version >= 2;}
f3756
0
build
public DescribeDelegationTokenRequest kafkatest_f3757_0(short version)
{    return new DescribeDelegationTokenRequest(data, version);}
f3757
0
throttleTimeMs
public int kafkatest_f3766_0()
{    return data.throttleTimeMs();}
f3766
0
error
public Errors kafkatest_f3767_0()
{    return Errors.forCode(data.errorCode());}
f3767
0
parse
public static DescribeGroupsRequest kafkatest_f3776_0(ByteBuffer buffer, short version)
{    return new DescribeGroupsRequest(ApiKeys.DESCRIBE_GROUPS.parseRequest(version, buffer), version);}
f3776
0
groupMember
public static DescribedGroupMember kafkatest_f3777_0(final String memberId, final String groupInstanceId, final String clientId, final String clientHost, final byte[] assignment, final byte[] metadata)
{    return new DescribedGroupMember().setMemberId(memberId).setGroupInstanceId(groupInstanceId).setClientId(clientId).setClientHost(clientHost).setMemberAssignment(assignment).setMemberMetadata(metadata);}
f3777
0
parse
public static DescribeGroupsResponse kafkatest_f3786_0(ByteBuffer buffer, short version)
{    return new DescribeGroupsResponse(ApiKeys.DESCRIBE_GROUPS.responseSchema(version).read(buffer), version);}
f3786
0
shouldClientThrottle
public boolean kafkatest_f3787_0(short version)
{    return version >= 2;}
f3787
0
schemaVersions
public static Schema[] kafkatest_f3796_0()
{    return new Schema[] { DESCRIBE_LOG_DIRS_RESPONSE_V0, DESCRIBE_LOG_DIRS_RESPONSE_V1 };}
f3796
0
toStruct
protected Struct kafkatest_f3797_0(short version)
{    Struct struct = new Struct(ApiKeys.DESCRIBE_LOG_DIRS.responseSchema(version));    struct.set(THROTTLE_TIME_MS, throttleTimeMs);    List<Struct> logDirStructArray = new ArrayList<>();    for (Map.Entry<String, LogDirInfo> logDirInfosEntry : logDirInfos.entrySet()) {        LogDirInfo logDirInfo = logDirInfosEntry.getValue();        Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);        logDirStruct.set(ERROR_CODE, logDirInfo.error.code());        logDirStruct.set(LOG_DIR_KEY_NAME, logDirInfosEntry.getKey());        Map<String, Map<Integer, ReplicaInfo>> replicaInfosByTopic = CollectionUtils.groupPartitionDataByTopic(logDirInfo.replicaInfos);        List<Struct> topicStructArray = new ArrayList<>();        for (Map.Entry<String, Map<Integer, ReplicaInfo>> replicaInfosByTopicEntry : replicaInfosByTopic.entrySet()) {            Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);            topicStruct.set(TOPIC_NAME, replicaInfosByTopicEntry.getKey());            List<Struct> partitionStructArray = new ArrayList<>();            for (Map.Entry<Integer, ReplicaInfo> replicaInfosByPartitionEntry : replicaInfosByTopicEntry.getValue().entrySet()) {                Struct partitionStruct = topicStruct.instance(PARTITIONS_KEY_NAME);                ReplicaInfo replicaInfo = replicaInfosByPartitionEntry.getValue();                partitionStruct.set(PARTITION_ID, replicaInfosByPartitionEntry.getKey());                partitionStruct.set(SIZE_KEY_NAME, replicaInfo.size);                partitionStruct.set(OFFSET_LAG_KEY_NAME, replicaInfo.offsetLag);                partitionStruct.set(IS_FUTURE_KEY_NAME, replicaInfo.isFuture);                partitionStructArray.add(partitionStruct);            }            topicStruct.set(PARTITIONS_KEY_NAME, partitionStructArray.toArray());            topicStructArray.add(topicStruct);        }        logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());        logDirStructArray.add(logDirStruct);    }    struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());    return struct;}
f3797
0
toString
public String kafkatest_f3806_0()
{    return "ElectLeadersRequest(" + "electionType=" + electionType + ", topicPartitions=" + ((topicPartitions == null) ? "null" : MessageUtil.deepToString(topicPartitions.iterator())) + ", timeoutMs=" + timeoutMs + ")";}
f3806
0
toRequestData
private ElectLeadersRequestData kafkatest_f3807_0(short version)
{    if (electionType != ElectionType.PREFERRED && version == 0) {        throw new UnsupportedVersionException("API Version 0 only supports PREFERRED election type");    }    ElectLeadersRequestData data = new ElectLeadersRequestData().setTimeoutMs(timeoutMs);    if (topicPartitions != null) {        for (Map.Entry<String, List<Integer>> tp : CollectionUtils.groupPartitionsByTopic(topicPartitions).entrySet()) {            data.topicPartitions().add(new ElectLeadersRequestData.TopicPartitions().setTopic(tp.getKey()).setPartitionId(tp.getValue()));        }    } else {        data.setTopicPartitions(null);    }    data.setElectionType(electionType.value);    return data;}
f3807
0
errorCounts
public Map<Errors, Integer> kafkatest_f3816_0()
{    HashMap<Errors, Integer> counts = new HashMap<>();    for (ReplicaElectionResult result : data.replicaElectionResults()) {        for (PartitionResult partitionResult : result.partitionResult()) {            Errors error = Errors.forCode(partitionResult.errorCode());            counts.put(error, counts.getOrDefault(error, 0) + 1);        }    }    return counts;}
f3816
0
parse
public static ElectLeadersResponse kafkatest_f3817_0(ByteBuffer buffer, short version)
{    return new ElectLeadersResponse(ApiKeys.ELECT_LEADERS.responseSchema(version).read(buffer), version);}
f3817
0
producerEpoch
public short kafkatest_f3826_0()
{    return producerEpoch;}
f3826
0
command
public TransactionResult kafkatest_f3827_0()
{    return result;}
f3827
0
parse
public static EndTxnResponse kafkatest_f3836_0(ByteBuffer buffer, short version)
{    return new EndTxnResponse(ApiKeys.END_TXN.parseResponse(version, buffer));}
f3836
0
toString
public String kafkatest_f3837_0()
{    return "EndTxnResponse(" + "error=" + error + ", throttleTimeMs=" + throttleTimeMs + ')';}
f3837
0
parse
public static ExpireDelegationTokenRequest kafkatest_f3846_0(ByteBuffer buffer, short version)
{    return new ExpireDelegationTokenRequest(ApiKeys.EXPIRE_DELEGATION_TOKEN.parseRequest(version, buffer), version);}
f3846
0
toStruct
protected Struct kafkatest_f3847_0()
{    return data.toStruct(version());}
f3847
0
errorCounts
public Map<Errors, Integer> kafkatest_f3856_0()
{    return Collections.singletonMap(error(), 1);}
f3856
0
toStruct
protected Struct kafkatest_f3857_0(short version)
{    return data.toStruct(version);}
f3857
0
equals
public boolean kafkatest_f3866_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    FetchMetadata that = (FetchMetadata) o;    return sessionId == that.sessionId && epoch == that.epoch;}
f3866
0
nextCloseExisting
public FetchMetadata kafkatest_f3867_0()
{    return new FetchMetadata(sessionId, INITIAL_EPOCH);}
f3867
0
forConsumer
public static Builder kafkatest_f3876_0(int maxWait, int minBytes, Map<TopicPartition, PartitionData> fetchData)
{    return new Builder(ApiKeys.FETCH.oldestVersion(), ApiKeys.FETCH.latestVersion(), CONSUMER_REPLICA_ID, maxWait, minBytes, fetchData);}
f3876
0
forReplica
public static Builder kafkatest_f3877_0(short allowedVersion, int replicaId, int maxWait, int minBytes, Map<TopicPartition, PartitionData> fetchData)
{    return new Builder(allowedVersion, allowedVersion, replicaId, maxWait, minBytes, fetchData);}
f3877
0
toString
public String kafkatest_f3886_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=FetchRequest").append(", replicaId=").append(replicaId).append(", maxWait=").append(maxWait).append(", minBytes=").append(minBytes).append(", maxBytes=").append(maxBytes).append(", fetchData=").append(fetchData).append(", isolationLevel=").append(isolationLevel).append(", toForget=").append(Utils.join(toForget, ", ")).append(", metadata=").append(metadata).append(", rackId=").append(rackId).append(")");    return bld.toString();}
f3886
0
getErrorResponse
public AbstractResponse kafkatest_f3887_0(int throttleTimeMs, Throwable e)
{    // The error is indicated in two ways: by setting the same error code in all partitions, and by    // setting the top-level error code.  The form where we set the same error code in all partitions    // is needed in order to maintain backwards compatibility with older versions of the protocol    // in which there was no top-level error code. Note that for incremental fetch responses, there    // may not be any partitions at all in the response.  For this reason, the top-level error code    // is essential for them.    Errors error = Errors.forException(e);    LinkedHashMap<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();    for (Map.Entry<TopicPartition, PartitionData> entry : fetchData.entrySet()) {        FetchResponse.PartitionData<MemoryRecords> partitionResponse = new FetchResponse.PartitionData<>(error, FetchResponse.INVALID_HIGHWATERMARK, FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), null, MemoryRecords.EMPTY);        responseData.put(entry.getKey(), partitionResponse);    }    return new FetchResponse<>(error, responseData, throttleTimeMs, metadata.sessionId());}
f3887
0
metadata
public FetchMetadata kafkatest_f3896_0()
{    return metadata;}
f3896
0
rackId
public String kafkatest_f3897_0()
{    return rackId;}
f3897
0
toString
public String kafkatest_f3906_0()
{    return "(error=" + error + ", highWaterMark=" + highWatermark + ", lastStableOffset = " + lastStableOffset + ", logStartOffset = " + logStartOffset + ", preferredReadReplica = " + preferredReadReplica.map(Object::toString).orElse("absent") + ", abortedTransactions = " + abortedTransactions + ", recordsSizeInBytes=" + records.sizeInBytes() + ")";}
f3906
0
parse
public static FetchResponse<MemoryRecords> kafkatest_f3907_0(Struct struct)
{    LinkedHashMap<TopicPartition, PartitionData<MemoryRecords>> responseData = new LinkedHashMap<>();    for (Object topicResponseObj : struct.getArray(RESPONSES_KEY_NAME)) {        Struct topicResponse = (Struct) topicResponseObj;        String topic = topicResponse.get(TOPIC_NAME);        for (Object partitionResponseObj : topicResponse.getArray(PARTITIONS_KEY_NAME)) {            Struct partitionResponse = (Struct) partitionResponseObj;            Struct partitionResponseHeader = partitionResponse.getStruct(PARTITION_HEADER_KEY_NAME);            int partition = partitionResponseHeader.get(PARTITION_ID);            Errors error = Errors.forCode(partitionResponseHeader.get(ERROR_CODE));            long highWatermark = partitionResponseHeader.get(HIGH_WATERMARK);            long lastStableOffset = partitionResponseHeader.getOrElse(LAST_STABLE_OFFSET, INVALID_LAST_STABLE_OFFSET);            long logStartOffset = partitionResponseHeader.getOrElse(LOG_START_OFFSET, INVALID_LOG_START_OFFSET);            Optional<Integer> preferredReadReplica = Optional.of(partitionResponseHeader.getOrElse(PREFERRED_READ_REPLICA, INVALID_PREFERRED_REPLICA_ID)).filter(Predicate.isEqual(INVALID_PREFERRED_REPLICA_ID).negate());            BaseRecords baseRecords = partitionResponse.getRecords(RECORD_SET_KEY_NAME);            if (!(baseRecords instanceof MemoryRecords))                throw new IllegalStateException("Unknown records type found: " + baseRecords.getClass());            MemoryRecords records = (MemoryRecords) baseRecords;            List<AbortedTransaction> abortedTransactions = null;            if (partitionResponseHeader.hasField(ABORTED_TRANSACTIONS_KEY_NAME)) {                Object[] abortedTransactionsArray = partitionResponseHeader.getArray(ABORTED_TRANSACTIONS_KEY_NAME);                if (abortedTransactionsArray != null) {                    abortedTransactions = new ArrayList<>(abortedTransactionsArray.length);                    for (Object abortedTransactionObj : abortedTransactionsArray) {                        Struct abortedTransactionStruct = (Struct) abortedTransactionObj;                        long producerId = abortedTransactionStruct.get(PRODUCER_ID);                        long firstOffset = abortedTransactionStruct.get(FIRST_OFFSET);                        abortedTransactions.add(new AbortedTransaction(producerId, firstOffset));                    }                }            }            PartitionData<MemoryRecords> partitionData = new PartitionData<>(error, highWatermark, lastStableOffset, logStartOffset, preferredReadReplica, abortedTransactions, records);            responseData.put(new TopicPartition(topic, partition), partitionData);        }    }    return new FetchResponse<>(Errors.forCode(struct.getOrElse(ERROR_CODE, (short) 0)), responseData, struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME), struct.getOrElse(SESSION_ID, INVALID_SESSION_ID));}
f3907
0
addResponseData
private static void kafkatest_f3916_0(Struct struct, int throttleTimeMs, String dest, Queue<Send> sends)
{    Object[] allTopicData = struct.getArray(RESPONSES_KEY_NAME);    if (struct.hasField(ERROR_CODE)) {        ByteBuffer buffer = ByteBuffer.allocate(14);        buffer.putInt(throttleTimeMs);        buffer.putShort(struct.get(ERROR_CODE));        buffer.putInt(struct.get(SESSION_ID));        buffer.putInt(allTopicData.length);        buffer.rewind();        sends.add(new ByteBufferSend(dest, buffer));    } else if (struct.hasField(THROTTLE_TIME_MS)) {        ByteBuffer buffer = ByteBuffer.allocate(8);        buffer.putInt(throttleTimeMs);        buffer.putInt(allTopicData.length);        buffer.rewind();        sends.add(new ByteBufferSend(dest, buffer));    } else {        ByteBuffer buffer = ByteBuffer.allocate(4);        buffer.putInt(allTopicData.length);        buffer.rewind();        sends.add(new ByteBufferSend(dest, buffer));    }    for (Object topicData : allTopicData) addTopicData(dest, sends, (Struct) topicData);}
f3916
0
addTopicData
private static void kafkatest_f3917_0(String dest, Queue<Send> sends, Struct topicData)
{    String topic = topicData.get(TOPIC_NAME);    Object[] allPartitionData = topicData.getArray(PARTITIONS_KEY_NAME);    // include the topic header and the count for the number of partitions    ByteBuffer buffer = ByteBuffer.allocate(STRING.sizeOf(topic) + 4);    STRING.write(buffer, topic);    buffer.putInt(allPartitionData.length);    buffer.rewind();    sends.add(new ByteBufferSend(dest, buffer));    for (Object partitionData : allPartitionData) addPartitionData(dest, sends, (Struct) partitionData);}
f3917
0
parse
public static FindCoordinatorRequest kafkatest_f3926_0(ByteBuffer buffer, short version)
{    return new FindCoordinatorRequest(ApiKeys.FIND_COORDINATOR.parseRequest(version, buffer), version);}
f3926
0
toStruct
protected Struct kafkatest_f3927_0()
{    return data.toStruct(version());}
f3927
0
errorCounts
public Map<Errors, Integer> kafkatest_f3936_0()
{    return Collections.singletonMap(error(), 1);}
f3936
0
toStruct
protected Struct kafkatest_f3937_0(short version)
{    return data.toStruct(version);}
f3937
0
toStruct
protected Struct kafkatest_f3946_0()
{    return data.toStruct(version());}
f3946
0
throttleTimeMs
public int kafkatest_f3947_0()
{    return data.throttleTimeMs();}
f3947
0
data
public IncrementalAlterConfigsRequestData kafkatest_f3956_0()
{    return data;}
f3956
0
toStruct
protected Struct kafkatest_f3957_0()
{    return data.toStruct(version);}
f3957
0
parse
public static IncrementalAlterConfigsResponse kafkatest_f3966_0(ByteBuffer buffer, short version)
{    return new IncrementalAlterConfigsResponse(ApiKeys.INCREMENTAL_ALTER_CONFIGS.responseSchema(version).read(buffer), version);}
f3966
0
build
public InitProducerIdRequest kafkatest_f3967_0(short version)
{    if (data.transactionTimeoutMs() <= 0)        throw new IllegalArgumentException("transaction timeout value is not positive: " + data.transactionTimeoutMs());    if (data.transactionalId() != null && data.transactionalId().isEmpty())        throw new IllegalArgumentException("Must set either a null or a non-empty transactional id.");    return new InitProducerIdRequest(data, version);}
f3967
0
toString
public String kafkatest_f3976_0()
{    return data.toString();}
f3976
0
error
public Errors kafkatest_f3977_0()
{    return Errors.forCode(data.errorCode());}
f3977
0
data
public JoinGroupRequestData kafkatest_f3986_0()
{    return data;}
f3986
0
getErrorResponse
public AbstractResponse kafkatest_f3987_0(int throttleTimeMs, Throwable e)
{    short versionId = version();    switch(versionId) {        case 0:        case 1:            return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(Errors.forException(e).code()).setGenerationId(JoinGroupResponse.UNKNOWN_GENERATION_ID).setProtocolName(JoinGroupResponse.UNKNOWN_PROTOCOL).setLeader(JoinGroupResponse.UNKNOWN_MEMBER_ID).setMemberId(JoinGroupResponse.UNKNOWN_MEMBER_ID).setMembers(Collections.emptyList()));        case 2:        case 3:        case 4:        case 5:            return new JoinGroupResponse(new JoinGroupResponseData().setThrottleTimeMs(throttleTimeMs).setErrorCode(Errors.forException(e).code()).setGenerationId(JoinGroupResponse.UNKNOWN_GENERATION_ID).setProtocolName(JoinGroupResponse.UNKNOWN_PROTOCOL).setLeader(JoinGroupResponse.UNKNOWN_MEMBER_ID).setMemberId(JoinGroupResponse.UNKNOWN_MEMBER_ID).setMembers(Collections.emptyList()));        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.JOIN_GROUP.latestVersion()));    }}
f3987
0
toStruct
protected Struct kafkatest_f3996_0(short version)
{    return data.toStruct(version);}
f3996
0
toString
public String kafkatest_f3997_0()
{    return data.toString();}
f3997
0
partitionStates
public Map<TopicPartition, PartitionState> kafkatest_f4006_0()
{    return partitionStates;}
f4006
0
liveLeaders
public Set<Node> kafkatest_f4007_0()
{    return liveLeaders;}
f4007
0
toStruct
protected Struct kafkatest_f4016_0(short version)
{    Struct struct = new Struct(ApiKeys.LEADER_AND_ISR.responseSchema(version));    List<Struct> responseDatas = new ArrayList<>(responses.size());    for (Map.Entry<TopicPartition, Errors> response : responses.entrySet()) {        Struct partitionData = struct.instance(PARTITIONS);        TopicPartition partition = response.getKey();        partitionData.set(TOPIC_NAME, partition.topic());        partitionData.set(PARTITION_ID, partition.partition());        partitionData.set(ERROR_CODE, response.getValue().code());        responseDatas.add(partitionData);    }    struct.set(PARTITIONS, responseDatas.toArray());    struct.set(ERROR_CODE, error.code());    return struct;}
f4016
0
toString
public String kafkatest_f4017_0()
{    return "LeaderAndIsrResponse(" + "responses=" + responses + ", error=" + error + ")";}
f4017
0
memberResponses
public List<MemberResponse> kafkatest_f4026_0()
{    return data.members();}
f4026
0
error
public Errors kafkatest_f4027_0()
{    return getError(Errors.forCode(data.errorCode()), data.members());}
f4027
0
build
public ListGroupsRequest kafkatest_f4036_0(short version)
{    return new ListGroupsRequest(data, version);}
f4036
0
toString
public String kafkatest_f4037_0()
{    return data.toString();}
f4037
0
shouldClientThrottle
public boolean kafkatest_f4046_0(short version)
{    return version >= 2;}
f4046
0
schemaVersions
public static Schema[] kafkatest_f4047_0()
{    return new Schema[] { LIST_OFFSET_REQUEST_V0, LIST_OFFSET_REQUEST_V1, LIST_OFFSET_REQUEST_V2, LIST_OFFSET_REQUEST_V3, LIST_OFFSET_REQUEST_V4, LIST_OFFSET_REQUEST_V5 };}
f4047
0
isolationLevel
public IsolationLevel kafkatest_f4056_0()
{    return isolationLevel;}
f4056
0
partitionTimestamps
public Map<TopicPartition, PartitionData> kafkatest_f4057_0()
{    return partitionTimestamps;}
f4057
0
parse
public static ListOffsetResponse kafkatest_f4066_0(ByteBuffer buffer, short version)
{    return new ListOffsetResponse(ApiKeys.LIST_OFFSETS.parseResponse(version, buffer));}
f4066
0
toStruct
protected Struct kafkatest_f4067_0(short version)
{    Struct struct = new Struct(ApiKeys.LIST_OFFSETS.responseSchema(version));    struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);    Map<String, Map<Integer, PartitionData>> topicsData = CollectionUtils.groupPartitionDataByTopic(responseData);    List<Struct> topicArray = new ArrayList<>();    for (Map.Entry<String, Map<Integer, PartitionData>> topicEntry : topicsData.entrySet()) {        Struct topicData = struct.instance(TOPICS);        topicData.set(TOPIC_NAME, topicEntry.getKey());        List<Struct> partitionArray = new ArrayList<>();        for (Map.Entry<Integer, PartitionData> partitionEntry : topicEntry.getValue().entrySet()) {            PartitionData offsetPartitionData = partitionEntry.getValue();            Struct partitionData = topicData.instance(PARTITIONS);            partitionData.set(PARTITION_ID, partitionEntry.getKey());            partitionData.set(ERROR_CODE, offsetPartitionData.error.code());            if (version == 0) {                partitionData.set(OFFSETS, offsetPartitionData.offsets.toArray());            } else {                partitionData.set(TIMESTAMP, offsetPartitionData.timestamp);                partitionData.set(OFFSET, offsetPartitionData.offset);                RequestUtils.setLeaderEpochIfExists(partitionData, LEADER_EPOCH, offsetPartitionData.leaderEpoch);            }            partitionArray.add(partitionData);        }        topicData.set(PARTITIONS, partitionArray.toArray());        topicArray.add(topicData);    }    struct.set(TOPICS, topicArray.toArray());    return struct;}
f4067
0
parse
public static ListPartitionReassignmentsResponse kafkatest_f4076_0(ByteBuffer buffer, short version)
{    return new ListPartitionReassignmentsResponse(ApiKeys.LIST_PARTITION_REASSIGNMENTS.responseSchema(version).read(buffer), version);}
f4076
0
data
public ListPartitionReassignmentsResponseData kafkatest_f4077_0()
{    return data;}
f4077
0
build
public MetadataRequest kafkatest_f4086_0(short version)
{    if (version < 1)        throw new UnsupportedVersionException("MetadataRequest versions older than 1 are not supported.");    if (!data.allowAutoTopicCreation() && version < 4)        throw new UnsupportedVersionException("MetadataRequest versions older than 4 don't support the " + "allowAutoTopicCreation field");    return new MetadataRequest(data, version);}
f4086
0
toString
public String kafkatest_f4087_0()
{    return data.toString();}
f4087
0
toStruct
protected Struct kafkatest_f4096_0(short version)
{    return data.toStruct(version);}
f4096
0
throttleTimeMs
public int kafkatest_f4097_0()
{    return data.throttleTimeMs();}
f4097
0
brokers
public Collection<Node> kafkatest_f4106_0()
{    return holder().brokers;}
f4106
0
topicMetadata
public Collection<TopicMetadata> kafkatest_f4107_0()
{    return holder().topicMetadata;}
f4107
0
authorizedOperations
public int kafkatest_f4116_0()
{    return authorizedOperations;}
f4116
0
equals
public boolean kafkatest_f4117_0(final Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    final TopicMetadata that = (TopicMetadata) o;    return isInternal == that.isInternal && error == that.error && Objects.equals(topic, that.topic) && Objects.equals(partitionMetadata, that.partitionMetadata) && Objects.equals(authorizedOperations, that.authorizedOperations);}
f4117
0
isr
public List<Node> kafkatest_f4126_0()
{    return isr;}
f4126
0
offlineReplicas
public List<Node> kafkatest_f4127_0()
{    return offlineReplicas;}
f4127
0
build
public OffsetCommitRequest kafkatest_f4136_0(short version)
{    if (data.groupInstanceId() != null && version < 7) {        throw new UnsupportedVersionException("The broker offset commit protocol version " + version + " does not support usage of config group.instance.id.");    }    return new OffsetCommitRequest(data, version);}
f4136
0
toString
public String kafkatest_f4137_0()
{    return data.toString();}
f4137
0
parse
public static OffsetCommitResponse kafkatest_f4146_0(ByteBuffer buffer, short version)
{    return new OffsetCommitResponse(ApiKeys.OFFSET_COMMIT.parseResponse(version, buffer), version);}
f4146
0
toStruct
public Struct kafkatest_f4147_0(short version)
{    return data.toStruct(version);}
f4147
0
toStruct
protected Struct kafkatest_f4156_0()
{    return data.toStruct(version());}
f4156
0
toStruct
protected Struct kafkatest_f4157_0(short version)
{    return data.toStruct(version);}
f4157
0
partitions
public List<TopicPartition> kafkatest_f4166_0()
{    if (isAllPartitions()) {        return null;    }    List<TopicPartition> partitions = new ArrayList<>();    for (OffsetFetchRequestTopic topic : data.topics()) {        for (Integer partitionIndex : topic.partitionIndexes()) {            partitions.add(new TopicPartition(topic.name(), partitionIndex));        }    }    return partitions;}
f4166
0
groupId
public String kafkatest_f4167_0()
{    return data.groupId();}
f4167
0
toString
public String kafkatest_f4176_0()
{    return "PartitionData(" + "offset=" + offset + ", leaderEpoch=" + leaderEpoch.orElse(NO_PARTITION_LEADER_EPOCH) + ", metadata=" + metadata + ", error='" + error.toString() + ")";}
f4176
0
hashCode
public int kafkatest_f4177_0()
{    return Objects.hash(offset, leaderEpoch, metadata, error);}
f4177
0
schemaVersions
public static Schema[] kafkatest_f4186_0()
{    return new Schema[] { OFFSET_FOR_LEADER_EPOCH_REQUEST_V0, OFFSET_FOR_LEADER_EPOCH_REQUEST_V1, OFFSET_FOR_LEADER_EPOCH_REQUEST_V2, OFFSET_FOR_LEADER_EPOCH_REQUEST_V3 };}
f4186
0
epochsByTopicPartition
public Map<TopicPartition, PartitionData> kafkatest_f4187_0()
{    return epochsByPartition;}
f4187
0
getErrorResponse
public AbstractResponse kafkatest_f4196_0(int throttleTimeMs, Throwable e)
{    Errors error = Errors.forException(e);    Map<TopicPartition, EpochEndOffset> errorResponse = new HashMap<>();    for (TopicPartition tp : epochsByPartition.keySet()) {        errorResponse.put(tp, new EpochEndOffset(error, EpochEndOffset.UNDEFINED_EPOCH, EpochEndOffset.UNDEFINED_EPOCH_OFFSET));    }    return new OffsetsForLeaderEpochResponse(throttleTimeMs, errorResponse);}
f4196
0
toString
public String kafkatest_f4197_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(currentLeaderEpoch=").append(currentLeaderEpoch).append(", leaderEpoch=").append(leaderEpoch).append(")");    return bld.toString();}
f4197
0
schemaVersions
public static Schema[] kafkatest_f4206_0()
{    return new Schema[] { PRODUCE_REQUEST_V0, PRODUCE_REQUEST_V1, PRODUCE_REQUEST_V2, PRODUCE_REQUEST_V3, PRODUCE_REQUEST_V4, PRODUCE_REQUEST_V5, PRODUCE_REQUEST_V6, PRODUCE_REQUEST_V7 };}
f4206
0
forCurrentMagic
public static Builder kafkatest_f4207_0(short acks, int timeout, Map<TopicPartition, MemoryRecords> partitionRecords)
{    return forMagic(RecordBatch.CURRENT_MAGIC_VALUE, acks, timeout, partitionRecords, null);}
f4207
0
toString
public String kafkatest_f4216_0(boolean verbose)
{    // Use the same format as `Struct.toString()`    StringBuilder bld = new StringBuilder();    bld.append("{acks=").append(acks).append(",timeout=").append(timeout);    if (verbose)        bld.append(",partitionSizes=").append(Utils.mkString(partitionSizes, "[", "]", "=", ","));    else        bld.append(",numPartitions=").append(partitionSizes.size());    bld.append("}");    return bld.toString();}
f4216
0
getErrorResponse
public ProduceResponse kafkatest_f4217_0(int throttleTimeMs, Throwable e)
{    /* In case the producer doesn't actually want any response */    if (acks == 0)        return null;    Errors error = Errors.forException(e);    Map<TopicPartition, ProduceResponse.PartitionResponse> responseMap = new HashMap<>();    ProduceResponse.PartitionResponse partitionResponse = new ProduceResponse.PartitionResponse(error);    for (TopicPartition tp : partitions()) responseMap.put(tp, partitionResponse);    short versionId = version();    switch(versionId) {        case 0:        case 1:        case 2:        case 3:        case 4:        case 5:        case 6:        case 7:            return new ProduceResponse(responseMap, throttleTimeMs);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.PRODUCE.latestVersion()));    }}
f4217
0
clearPartitionRecords
public void kafkatest_f4226_0()
{    partitionRecords = null;}
f4226
0
validateRecords
public static void kafkatest_f4227_0(short version, MemoryRecords records)
{    if (version >= 3) {        Iterator<MutableRecordBatch> iterator = records.batches().iterator();        if (!iterator.hasNext())            throw new InvalidRecordException("Produce requests with version " + version + " must have at least " + "one record batch");        MutableRecordBatch entry = iterator.next();        if (entry.magic() != RecordBatch.MAGIC_VALUE_V2)            throw new InvalidRecordException("Produce requests with version " + version + " are only allowed to " + "contain record batches with magic version 2");        if (version < 7 && entry.compressionType() == CompressionType.ZSTD) {            throw new UnsupportedCompressionTypeException("Produce requests with version " + version + " are not allowed to " + "use ZStandard compression");        }        if (iterator.hasNext())            throw new InvalidRecordException("Produce requests with version " + version + " are only allowed to " + "contain exactly one record batch");    }// Note that we do not do similar validation for older versions to ensure compatibility with// clients which send the wrong magic version in the wrong version of the produce request. The broker// did not do this validation before, so we maintain that behavior here.}
f4227
0
parse
public static ProduceResponse kafkatest_f4236_0(ByteBuffer buffer, short version)
{    return new ProduceResponse(ApiKeys.PRODUCE.responseSchema(version).read(buffer));}
f4236
0
shouldClientThrottle
public boolean kafkatest_f4237_0(short version)
{    return version >= 6;}
f4237
0
toStruct
protected Struct kafkatest_f4246_0(short version)
{    return data.toStruct(version);}
f4246
0
throttleTimeMs
public int kafkatest_f4247_0()
{    return data.throttleTimeMs();}
f4247
0
listener
public String kafkatest_f4256_0()
{    return listenerName.value();}
f4256
0
securityProtocol
public SecurityProtocol kafkatest_f4257_0()
{    return securityProtocol;}
f4257
0
apiVersion
public short kafkatest_f4266_0()
{    return apiVersion;}
f4266
0
clientId
public String kafkatest_f4267_0()
{    return clientId;}
f4267
0
resourcePatternSetStructFields
 static void kafkatest_f4276_0(ResourcePattern pattern, Struct struct)
{    struct.set(RESOURCE_TYPE, pattern.resourceType().code());    struct.set(RESOURCE_NAME, pattern.name());    struct.setIfExists(RESOURCE_PATTERN_TYPE, pattern.patternType().code());}
f4276
0
resourcePatternFilterFromStructFields
 static ResourcePatternFilter kafkatest_f4277_0(Struct struct)
{    byte resourceType = struct.get(RESOURCE_TYPE);    String name = struct.get(RESOURCE_NAME_FILTER);    PatternType patternType = PatternType.fromCode(struct.getOrElse(RESOURCE_PATTERN_TYPE_FILTER, PatternType.LITERAL.code()));    return new ResourcePatternFilter(ResourceType.fromCode(resourceType), name, patternType);}
f4277
0
sizeOf
public int kafkatest_f4286_0()
{    return toStruct().sizeOf();}
f4286
0
toStruct
public Struct kafkatest_f4287_0()
{    Struct struct = new Struct(SCHEMA);    struct.set(CORRELATION_KEY_FIELD, correlationId);    return struct;}
f4287
0
error
public Errors kafkatest_f4296_0()
{    return Errors.forCode(data.errorCode());}
f4296
0
errorCounts
public Map<Errors, Integer> kafkatest_f4297_0()
{    return Collections.singletonMap(Errors.forCode(data.errorCode()), 1);}
f4297
0
getErrorResponse
public AbstractResponse kafkatest_f4306_0(int throttleTimeMs, Throwable e)
{    SaslHandshakeResponseData response = new SaslHandshakeResponseData();    response.setErrorCode(ApiError.fromThrowable(e).error().code());    return new SaslHandshakeResponse(response);}
f4306
0
parse
public static SaslHandshakeRequest kafkatest_f4307_0(ByteBuffer buffer, short version)
{    return new SaslHandshakeRequest(ApiKeys.SASL_HANDSHAKE.parseRequest(version, buffer), version);}
f4307
0
toString
public String kafkatest_f4316_0()
{    StringBuilder bld = new StringBuilder();    bld.append("(type=StopReplicaRequest").append(", controllerId=").append(controllerId).append(", controllerEpoch=").append(controllerEpoch).append(", deletePartitions=").append(deletePartitions).append(", brokerEpoch=").append(brokerEpoch).append(", partitions=").append(Utils.join(partitions, ",")).append(")");    return bld.toString();}
f4316
0
getErrorResponse
public StopReplicaResponse kafkatest_f4317_0(int throttleTimeMs, Throwable e)
{    Errors error = Errors.forException(e);    Map<TopicPartition, Errors> responses = new HashMap<>(partitions.size());    for (TopicPartition partition : partitions) {        responses.put(partition, error);    }    short versionId = version();    switch(versionId) {        case 0:        case 1:            return new StopReplicaResponse(error, responses);        default:            throw new IllegalArgumentException(String.format("Version %d is not valid. Valid versions for %s are 0 to %d", versionId, this.getClass().getSimpleName(), ApiKeys.STOP_REPLICA.latestVersion()));    }}
f4317
0
parse
public static StopReplicaResponse kafkatest_f4326_0(ByteBuffer buffer, short version)
{    return new StopReplicaResponse(ApiKeys.STOP_REPLICA.parseResponse(version, buffer));}
f4326
0
toStruct
protected Struct kafkatest_f4327_0(short version)
{    Struct struct = new Struct(ApiKeys.STOP_REPLICA.responseSchema(version));    List<Struct> responseDatas = new ArrayList<>(responses.size());    for (Map.Entry<TopicPartition, Errors> response : responses.entrySet()) {        Struct partitionData = struct.instance(PARTITIONS);        TopicPartition partition = response.getKey();        partitionData.set(TOPIC_NAME, partition.topic());        partitionData.set(PARTITION_ID, partition.partition());        partitionData.set(ERROR_CODE, response.getValue().code());        responseDatas.add(partitionData);    }    struct.set(PARTITIONS, responseDatas.toArray());    struct.set(ERROR_CODE, error.code());    return struct;}
f4327
0
error
public Errors kafkatest_f4336_0()
{    return Errors.forCode(data.errorCode());}
f4336
0
errorCounts
public Map<Errors, Integer> kafkatest_f4337_0()
{    return Collections.singletonMap(Errors.forCode(data.errorCode()), 1);}
f4337
0
toStruct
protected Struct kafkatest_f4346_0()
{    return data.toStruct(version());}
f4346
0
getErrorResponseTopics
 static List<TxnOffsetCommitResponseTopic> kafkatest_f4347_0(List<TxnOffsetCommitRequestTopic> requestTopics, Errors e)
{    List<TxnOffsetCommitResponseTopic> responseTopicData = new ArrayList<>();    for (TxnOffsetCommitRequestTopic entry : requestTopics) {        List<TxnOffsetCommitResponsePartition> responsePartitions = new ArrayList<>();        for (TxnOffsetCommitRequestPartition requestPartition : entry.partitions()) {            responsePartitions.add(new TxnOffsetCommitResponsePartition().setPartitionIndex(requestPartition.partitionIndex()).setErrorCode(e.code()));        }        responseTopicData.add(new TxnOffsetCommitResponseTopic().setName(entry.name()).setPartitions(responsePartitions));    }    return responseTopicData;}
f4347
0
errors
public Map<TopicPartition, Errors> kafkatest_f4356_0()
{    Map<TopicPartition, Errors> errorMap = new HashMap<>();    for (TxnOffsetCommitResponseTopic topic : data.topics()) {        for (TxnOffsetCommitResponsePartition partition : topic.partitions()) {            errorMap.put(new TopicPartition(topic.name(), partition.partitionIndex()), Errors.forCode(partition.errorCode()));        }    }    return errorMap;}
f4356
0
parse
public static TxnOffsetCommitResponse kafkatest_f4357_0(ByteBuffer buffer, short version)
{    return new TxnOffsetCommitResponse(ApiKeys.TXN_OFFSET_COMMIT.parseResponse(version, buffer), version);}
f4357
0
toString
public String kafkatest_f4366_0()
{    return "(host=" + host + ", port=" + port + ", listenerName=" + listenerName + ", securityProtocol=" + securityProtocol + ")";}
f4366
0
toStruct
protected Struct kafkatest_f4367_0()
{    short version = version();    Struct struct = new Struct(ApiKeys.UPDATE_METADATA.requestSchema(version));    struct.set(CONTROLLER_ID, controllerId);    struct.set(CONTROLLER_EPOCH, controllerEpoch);    struct.setIfExists(BROKER_EPOCH, brokerEpoch);    if (struct.hasField(TOPIC_STATES)) {        Map<String, Map<Integer, PartitionState>> topicStates = CollectionUtils.groupPartitionDataByTopic(partitionStates);        List<Struct> topicStatesData = new ArrayList<>(topicStates.size());        for (Map.Entry<String, Map<Integer, PartitionState>> entry : topicStates.entrySet()) {            Struct topicStateData = struct.instance(TOPIC_STATES);            topicStateData.set(TOPIC_NAME, entry.getKey());            Map<Integer, PartitionState> partitionMap = entry.getValue();            List<Struct> partitionStatesData = new ArrayList<>(partitionMap.size());            for (Map.Entry<Integer, PartitionState> partitionEntry : partitionMap.entrySet()) {                Struct partitionStateData = topicStateData.instance(PARTITION_STATES);                partitionStateData.set(PARTITION_ID, partitionEntry.getKey());                partitionEntry.getValue().setStruct(partitionStateData);                partitionStatesData.add(partitionStateData);            }            topicStateData.set(PARTITION_STATES, partitionStatesData.toArray());            topicStatesData.add(topicStateData);        }        struct.set(TOPIC_STATES, topicStatesData.toArray());    } else {        List<Struct> partitionStatesData = new ArrayList<>(partitionStates.size());        for (Map.Entry<TopicPartition, PartitionState> entry : partitionStates.entrySet()) {            Struct partitionStateData = struct.instance(PARTITION_STATES);            TopicPartition topicPartition = entry.getKey();            partitionStateData.set(TOPIC_NAME, topicPartition.topic());            partitionStateData.set(PARTITION_ID, topicPartition.partition());            entry.getValue().setStruct(partitionStateData);            partitionStatesData.add(partitionStateData);        }        struct.set(PARTITION_STATES, partitionStatesData.toArray());    }    List<Struct> brokersData = new ArrayList<>(liveBrokers.size());    for (Broker broker : liveBrokers) {        Struct brokerData = struct.instance(LIVE_BROKERS);        brokerData.set(BROKER_ID, broker.id);        if (version == 0) {            EndPoint endPoint = broker.endPoints.get(0);            brokerData.set(HOST, endPoint.host);            brokerData.set(PORT, endPoint.port);        } else {            List<Struct> endPointsData = new ArrayList<>(broker.endPoints.size());            for (EndPoint endPoint : broker.endPoints) {                Struct endPointData = brokerData.instance(ENDPOINTS);                endPointData.set(PORT, endPoint.port);                endPointData.set(HOST, endPoint.host);                endPointData.set(SECURITY_PROTOCOL_TYPE, endPoint.securityProtocol.id);                if (version >= 3)                    endPointData.set(LISTENER_NAME, endPoint.listenerName.value());                endPointsData.add(endPointData);            }            brokerData.set(ENDPOINTS, endPointsData.toArray());            if (version >= 2) {                brokerData.set(RACK, broker.rack);            }        }        brokersData.add(brokerData);    }    struct.set(LIVE_BROKERS, brokersData.toArray());    return struct;}
f4367
0
toStruct
protected Struct kafkatest_f4376_0(short version)
{    Struct struct = new Struct(ApiKeys.UPDATE_METADATA.responseSchema(version));    struct.set(ERROR_CODE, error.code());    return struct;}
f4376
0
schemaVersions
public static Schema[] kafkatest_f4377_0()
{    return new Schema[] { WRITE_TXN_MARKERS_REQUEST_V0 };}
f4377
0
build
public WriteTxnMarkersRequest kafkatest_f4386_0(short version)
{    return new WriteTxnMarkersRequest(version, markers);}
f4386
0
markers
public List<TxnMarkerEntry> kafkatest_f4387_0()
{    return markers;}
f4387
0
errorCounts
public Map<Errors, Integer> kafkatest_f4396_0()
{    Map<Errors, Integer> errorCounts = new HashMap<>();    for (Map<TopicPartition, Errors> allErrors : errors.values()) {        for (Errors error : allErrors.values()) updateErrorCounts(errorCounts, error);    }    return errorCounts;}
f4396
0
parse
public static WriteTxnMarkersResponse kafkatest_f4397_0(ByteBuffer buffer, short version)
{    return new WriteTxnMarkersResponse(ApiKeys.WRITE_TXN_MARKERS.parseResponse(version, buffer));}
f4397
0
toString
public String kafkatest_f4406_0()
{    return "(resourceType=" + resourceType + ", name=" + ((name == null) ? "<any>" : name) + ")";}
f4406
0
isUnknown
public boolean kafkatest_f4407_0()
{    return resourceType.isUnknown();}
f4407
0
matches
public boolean kafkatest_f4416_0(Resource other)
{    if ((name != null) && (!name.equals(other.name())))        return false;    return (resourceType == ResourceType.ANY) || (resourceType.equals(other.resourceType()));}
f4416
0
matchesAtMostOne
public boolean kafkatest_f4417_0()
{    return findIndefiniteField() == null;}
f4417
0
hashCode
public int kafkatest_f4426_0()
{    return Objects.hash(resourceType, name, patternType);}
f4426
0
isUnknown
public boolean kafkatest_f4427_0()
{    return resourceType.isUnknown() || patternType.isUnknown();}
f4427
0
hashCode
public int kafkatest_f4436_0()
{    return Objects.hash(resourceType, name, patternType);}
f4436
0
fromString
public static ResourceType kafkatest_f4437_0(String str) throws IllegalArgumentException
{    try {        return ResourceType.valueOf(str.toUpperCase(Locale.ROOT));    } catch (IllegalArgumentException e) {        return UNKNOWN;    }}
f4437
0
getName
public String kafkatest_f4448_0()
{    return name;}
f4448
0
getPrincipalType
public String kafkatest_f4449_0()
{    return principalType;}
f4449
0
listenerName
public String kafkatest_f4458_0()
{    return listenerName;}
f4458
0
map
public Map<String, String> kafkatest_f4459_0()
{    return extensionsMap;}
f4459
0
session
public SSLSession kafkatest_f4468_0()
{    return session;}
f4468
0
securityProtocol
public SecurityProtocol kafkatest_f4469_0()
{    return SecurityProtocol.SSL;}
f4469
0
createCache
public Cache<C> kafkatest_f4480_0(String mechanism, Class<C> credentialClass)
{    Cache<C> cache = new Cache<>(credentialClass);    @SuppressWarnings("unchecked")    Cache<C> oldCache = (Cache<C>) cacheMap.putIfAbsent(mechanism, cache);    return oldCache == null ? cache : oldCache;}
f4480
0
cache
public Cache<C> kafkatest_f4481_0(String mechanism, Class<C> credentialClass)
{    Cache<?> cache = cacheMap.get(mechanism);    if (cache != null) {        if (cache.credentialClass() != credentialClass)            throw new IllegalArgumentException("Invalid credential class " + credentialClass + ", expected " + cache.credentialClass());        return (Cache<C>) cache;    } else        return null;}
f4481
0
convertToKafkaPrincipal
private KafkaPrincipal kafkatest_f4490_0(Principal principal)
{    return new KafkaPrincipal(KafkaPrincipal.USER_TYPE, principal.getName());}
f4490
0
close
public void kafkatest_f4491_0()
{    if (oldPrincipalBuilder != null)        oldPrincipalBuilder.close();}
f4491
0
closeAll
public static void kafkatest_f4501_0()
{    synchronized (LoginManager.class) {        for (LoginMetadata<String> key : new ArrayList<>(STATIC_INSTANCES.keySet())) STATIC_INSTANCES.remove(key).login.close();        for (LoginMetadata<Password> key : new ArrayList<>(DYNAMIC_INSTANCES.keySet())) DYNAMIC_INSTANCES.remove(key).login.close();    }}
f4501
0
configuredClassOrDefault
private static Class<? extends T> kafkatest_f4502_0(Map<String, ?> configs, JaasContext jaasContext, String saslMechanism, String configName, Class<? extends T> defaultClass)
{    String prefix = jaasContext.type() == JaasContext.Type.SERVER ? ListenerName.saslMechanismPrefix(saslMechanism) : "";    @SuppressWarnings("unchecked")    Class<? extends T> clazz = (Class<? extends T>) configs.get(prefix + configName);    if (clazz != null && jaasContext.configurationEntries().size() != 1) {        String errorMessage = configName + " cannot be specified with multiple login modules in the JAAS context. " + SaslConfigs.SASL_JAAS_CONFIG + " must be configured to override mechanism-specific configs.";        throw new ConfigException(errorMessage);    }    if (clazz == null)        clazz = defaultClass;    return clazz;}
f4502
0
clientSessionReauthenticationTimeNanos
public Long kafkatest_f4511_0()
{    return reauthInfo.clientSessionReauthenticationTimeNanos;}
f4511
0
reauthenticationLatencyMs
public Long kafkatest_f4512_0()
{    return reauthInfo.reauthenticationLatencyMs();}
f4512
0
principal
public KafkaPrincipal kafkatest_f4521_0()
{    return new KafkaPrincipal(KafkaPrincipal.USER_TYPE, clientPrincipalName);}
f4521
0
complete
public boolean kafkatest_f4522_0()
{    return saslState == SaslState.COMPLETE;}
f4522
0
reauthenticating
public boolean kafkatest_f4531_0()
{    return apiVersionsResponseFromOriginalAuthentication != null;}
f4531
0
apiVersionsResponse
public ApiVersionsResponse kafkatest_f4532_0()
{    return reauthenticating() ? apiVersionsResponseFromOriginalAuthentication : apiVersionsResponseReceivedFromBroker;}
f4532
0
authenticate
public voidf4542_1) throws IOException
{    if (saslState != SaslState.REAUTH_PROCESS_HANDSHAKE) {        if (netOutBuffer != null && !flushNetOutBufferAndUpdateInterestOps())            return;        if (saslServer != null && saslServer.isComplete()) {            setSaslState(SaslState.COMPLETE);            return;        }        // allocate on heap (as opposed to any socket server memory pool)        if (netInBuffer == null)            netInBuffer = new NetworkReceive(MAX_RECEIVE_SIZE, connectionId);        netInBuffer.readFrom(transportLayer);        if (!netInBuffer.complete())            return;        netInBuffer.payload().rewind();    }    byte[] clientToken = new byte[netInBuffer.payload().remaining()];    netInBuffer.payload().get(clientToken, 0, clientToken.length);    // reset the networkReceive as we read all the data.    netInBuffer = null;    try {        switch(saslState) {            case REAUTH_PROCESS_HANDSHAKE:            case HANDSHAKE_OR_VERSIONS_REQUEST:            case HANDSHAKE_REQUEST:                handleKafkaRequest(clientToken);                break;            case REAUTH_BAD_MECHANISM:                throw new SaslAuthenticationException(reauthInfo.badMechanismErrorMessage);            case INITIAL_REQUEST:                if (handleKafkaRequest(clientToken))                    break;            // This is required for interoperability with 0.9.0.x clients which do not send handshake request            case AUTHENTICATE:                handleSaslToken(clientToken);                // update SASL state. Current SASL state will be updated when outgoing writes to the client complete.                if (saslServer.isComplete())                    setSaslState(SaslState.COMPLETE);                break;            default:                break;        }    } catch (AuthenticationException e) {        // Exception will be propagated after response is sent to client        setSaslState(SaslState.FAILED, e);    } catch (Exception e) {        // In the case of IOExceptions and other unexpected exceptions, fail immediately        saslState = SaslState.FAILED;                throw e;    }}
public voidf4542
1
principal
public KafkaPrincipal kafkatest_f4543_0()
{    SaslAuthenticationContext context = new SaslAuthenticationContext(saslServer, securityProtocol, clientAddress(), listenerName.value());    KafkaPrincipal principal = principalBuilder.build(context);    if (ScramMechanism.isScram(saslMechanism) && Boolean.parseBoolean((String) saslServer.getNegotiatedProperty(ScramLoginModule.TOKEN_AUTH_CONFIG))) {        principal.tokenAuthenticated(true);    }    return principal;}
f4543
0
setSaslState
private voidf4552_1SaslState saslState, AuthenticationException exception)
{    if (netOutBuffer != null && !netOutBuffer.completed()) {        pendingSaslState = saslState;        pendingException = exception;    } else {        this.saslState = saslState;                this.pendingSaslState = null;        this.pendingException = null;        if (exception != null)            throw exception;    }}
private voidf4552
1
flushNetOutBufferAndUpdateInterestOps
private boolean kafkatest_f4553_0() throws IOException
{    boolean flushedCompletely = flushNetOutBuffer();    if (flushedCompletely) {        transportLayer.removeInterestOps(SelectionKey.OP_WRITE);        if (pendingSaslState != null)            setSaslState(pendingSaslState, pendingException);    } else        transportLayer.addInterestOps(SelectionKey.OP_WRITE);    return flushedCompletely;}
f4553
0
handleApiVersionsRequest
private void kafkatest_f4562_0(RequestContext context, ApiVersionsRequest apiVersionsRequest) throws IOException
{    if (saslState != SaslState.HANDSHAKE_OR_VERSIONS_REQUEST)        throw new IllegalStateException("Unexpected ApiVersions request received during SASL authentication state " + saslState);    if (apiVersionsRequest.hasUnsupportedRequestVersion())        sendKafkaResponse(context, apiVersionsRequest.getErrorResponse(0, Errors.UNSUPPORTED_VERSION.exception()));    else {        sendKafkaResponse(context, apiVersionsResponse());        setSaslState(SaslState.HANDSHAKE_REQUEST);    }}
f4562
0
buildResponseOnAuthenticateFailure
private void kafkatest_f4563_0(RequestContext context, AbstractResponse response)
{    authenticationFailureSend = context.buildResponse(response);}
f4563
0
calcCompletionTimesAndReturnSessionLifetimeMs
private longf4572_1)
{    long retvalSessionLifetimeMs = 0L;    long authenticationEndMs = time.milliseconds();    authenticationEndNanos = time.nanoseconds();    Long credentialExpirationMs = (Long) saslServer.getNegotiatedProperty(SaslInternalConfigs.CREDENTIAL_LIFETIME_MS_SASL_NEGOTIATED_PROPERTY_KEY);    Long connectionsMaxReauthMs = connectionsMaxReauthMsByMechanism.get(saslMechanism);    if (credentialExpirationMs != null || connectionsMaxReauthMs != null) {        if (credentialExpirationMs == null)            retvalSessionLifetimeMs = zeroIfNegative(connectionsMaxReauthMs.longValue());        else if (connectionsMaxReauthMs == null)            retvalSessionLifetimeMs = zeroIfNegative(credentialExpirationMs.longValue() - authenticationEndMs);        else            retvalSessionLifetimeMs = zeroIfNegative(Math.min(credentialExpirationMs.longValue() - authenticationEndMs, connectionsMaxReauthMs.longValue()));        if (retvalSessionLifetimeMs > 0L)            sessionExpirationTimeNanos = Long.valueOf(authenticationEndNanos + 1000 * 1000 * retvalSessionLifetimeMs);    }    if (credentialExpirationMs != null) {        if (sessionExpirationTimeNanos != null)             session max lifetime from broker config={} ms, credential expiration={} ({} ms); session expiration = {} ({} ms), sending {} ms to client", connectionsMaxReauthMs, new Date(credentialExpirationMs), Long.valueOf(credentialExpirationMs.longValue() - authenticationEndMs), new Date(authenticationEndMs + retvalSessionLifetimeMs), retvalSessionLifetimeMs, retvalSessionLifetimeMs);        else             session max lifetime from broker config={} ms, credential expiration={} ({} ms); no session expiration, sending 0 ms to client", connectionsMaxReauthMs, new Date(credentialExpirationMs), Long.valueOf(credentialExpirationMs.longValue() - authenticationEndMs));    } else {        if (sessionExpirationTimeNanos != null)             session max lifetime from broker config={} ms, no credential expiration; session expiration = {} ({} ms), sending {} ms to client", connectionsMaxReauthMs, new Date(authenticationEndMs + retvalSessionLifetimeMs), retvalSessionLifetimeMs, retvalSessionLifetimeMs);        else             session max lifetime from broker config={} ms, no credential expiration; no session expiration, sending 0 ms to client", connectionsMaxReauthMs);    }    return retvalSessionLifetimeMs;}
private longf4572
1
reauthenticationLatencyMs
public Long kafkatest_f4573_0()
{    if (!reauthenticating())        return null;    // record at least 1 ms if there is some latency    long latencyNanos = authenticationEndNanos - reauthenticationBeginNanos;    return latencyNanos == 0L ? 0L : Math.max(1L, Long.valueOf(Math.round(latencyNanos / 1000.0 / 1000.0)));}
f4573
0
loadServerContext
public static JaasContextf4583_1ListenerName listenerName, String mechanism, Map<String, ?> configs)
{    if (listenerName == null)        throw new IllegalArgumentException("listenerName should not be null for SERVER");    if (mechanism == null)        throw new IllegalArgumentException("mechanism should not be null for SERVER");    String globalContextName = GLOBAL_CONTEXT_NAME_SERVER;    String listenerContextName = listenerName.value().toLowerCase(Locale.ROOT) + "." + GLOBAL_CONTEXT_NAME_SERVER;    Password dynamicJaasConfig = (Password) configs.get(mechanism.toLowerCase(Locale.ROOT) + "." + SaslConfigs.SASL_JAAS_CONFIG);    if (dynamicJaasConfig == null && configs.get(SaslConfigs.SASL_JAAS_CONFIG) != null)            return load(Type.SERVER, listenerContextName, globalContextName, dynamicJaasConfig);}
public static JaasContextf4583
1
loadClientContext
public static JaasContext kafkatest_f4584_0(Map<String, ?> configs)
{    String globalContextName = GLOBAL_CONTEXT_NAME_CLIENT;    Password dynamicJaasConfig = (Password) configs.get(SaslConfigs.SASL_JAAS_CONFIG);    return load(JaasContext.Type.CLIENT, null, globalContextName, dynamicJaasConfig);}
f4584
0
zkSecuritySysConfigString
public static String kafkatest_f4593_0()
{    String loginConfig = System.getProperty(JAVA_LOGIN_CONFIG_PARAM);    String clientEnabled = System.getProperty(ZK_SASL_CLIENT, "default:" + DEFAULT_ZK_SASL_CLIENT);    String contextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, "default:" + DEFAULT_ZK_LOGIN_CONTEXT_NAME);    return "[" + JAVA_LOGIN_CONFIG_PARAM + "=" + loginConfig + ", " + ZK_SASL_CLIENT + "=" + clientEnabled + ", " + ZK_LOGIN_CONTEXT_NAME_KEY + "=" + contextName + "]";}
f4593
0
isZkSecurityEnabled
public static booleanf4594_1)
{    boolean zkSaslEnabled = Boolean.parseBoolean(System.getProperty(ZK_SASL_CLIENT, DEFAULT_ZK_SASL_CLIENT));    String zkLoginContextName = System.getProperty(ZK_LOGIN_CONTEXT_NAME_KEY, DEFAULT_ZK_LOGIN_CONTEXT_NAME);        boolean foundLoginConfigEntry;    try {        Configuration loginConf = Configuration.getConfiguration();        foundLoginConfigEntry = loginConf.getAppConfigurationEntry(zkLoginContextName) != null;    } catch (Exception e) {        throw new KafkaException("Exception while loading Zookeeper JAAS login context " + zkSecuritySysConfigString(), e);    }    if (foundLoginConfigEntry && !zkSaslEnabled) {                throw new KafkaException("Exception while determining if ZooKeeper is secure " + zkSecuritySysConfigString());    }    return foundLoginConfigEntry;}
public static booleanf4594
1
subject
public Subject kafkatest_f4604_0()
{    return subject;}
f4604
0
serviceName
public String kafkatest_f4605_0()
{    return serviceName;}
f4605
0
toString
public String kafkatest_f4614_0()
{    StringBuilder result = new StringBuilder();    result.append(serviceName);    if (hostName != null) {        result.append('/');        result.append(hostName);    }    if (realm != null) {        result.append('@');        result.append(realm);    }    return result.toString();}
f4614
0
serviceName
public String kafkatest_f4615_0()
{    return serviceName;}
f4615
0
shortName
public String kafkatest_f4624_0(KerberosName kerberosName) throws IOException
{    String[] params;    if (kerberosName.hostName() == null) {        // if it is already simple, just return it        if (kerberosName.realm() == null)            return kerberosName.serviceName();        params = new String[] { kerberosName.realm(), kerberosName.serviceName() };    } else {        params = new String[] { kerberosName.realm(), kerberosName.serviceName(), kerberosName.hostName() };    }    for (KerberosRule r : principalToLocalRules) {        String result = r.apply(params);        if (result != null)            return result;    }    throw new NoMatchingRule("No rules apply to " + kerberosName + ", rules " + principalToLocalRules);}
f4624
0
toString
public String kafkatest_f4625_0()
{    return "KerberosShortNamer(principalToLocalRules = " + principalToLocalRules + ")";}
f4625
0
run
public voidf4634_1)
{        while (true) {        /*                 * Refresh thread's main loop. Each expiring credential lives for one iteration                 * of the loop. Thread will exit if the loop exits from here.                 */        long nowMs = currentMs();        Long nextRefreshMs = refreshMs(nowMs);        if (nextRefreshMs == null) {            loginContextFactory.refresherThreadDone();            return;        }        // should generally never happen except due to a bug        if (nextRefreshMs.longValue() < nowMs) {                        // refresh in 10 seconds            nextRefreshMs = Long.valueOf(nowMs + 10 * 1000);        }                time.sleep(nextRefreshMs - nowMs);        if (Thread.currentThread().isInterrupted()) {                        loginContextFactory.refresherThreadDone();            return;        }        while (true) {            /*                     * Perform a re-login over and over again with some intervening delay                     * unless/until either the refresh succeeds or we are interrupted.                     */            try {                reLogin();                // success                break;            } catch (ExitRefresherThreadDueToIllegalStateException e) {                                loginContextFactory.refresherThreadDone();                return;            } catch (LoginException loginException) {                 will sleep %d seconds before trying again.", principalLogText(), DELAY_SECONDS_BEFORE_NEXT_RETRY_WHEN_RELOGIN_FAILS), loginException);                // Sleep and allow loop to run/try again unless interrupted                time.sleep(DELAY_SECONDS_BEFORE_NEXT_RETRY_WHEN_RELOGIN_FAILS * 1000);                if (Thread.currentThread().isInterrupted()) {                                        loginContextFactory.refresherThreadDone();                    return;                }            }        }    }}
public voidf4634
1
subject
public Subject kafkatest_f4635_0()
{    // field requires volatile keyword    return subject;}
f4635
0
principalLogText
private String kafkatest_f4644_0()
{    return expiringCredential == null ? principalName : expiringCredential.getClass().getSimpleName() + ":" + principalName;}
f4644
0
currentMs
private long kafkatest_f4645_0()
{    return time.milliseconds();}
f4645
0
expiringCredential
public ExpiringCredentialf4654_1)
{    Set<OAuthBearerToken> privateCredentialTokens = expiringCredentialRefreshingLogin.subject().getPrivateCredentials(OAuthBearerToken.class);    if (privateCredentialTokens.isEmpty())        return null;    final OAuthBearerToken token = privateCredentialTokens.iterator().next();    if (log.isDebugEnabled())            return new ExpiringCredential() {        @Override        public String principalName() {            return token.principalName();        }        @Override        public Long startTimeMs() {            return token.startTimeMs();        }        @Override        public long expireTimeMs() {            return token.lifetimeMs();        }        @Override        public Long absoluteLastRefreshTimeMs() {            return null;        }    };}
public ExpiringCredentialf4654
1
principalName
public String kafkatest_f4655_0()
{    return token.principalName();}
f4655
0
getMechanismName
public String kafkatest_f4664_0()
{    return OAuthBearerLoginModule.OAUTHBEARER_MECHANISM;}
f4664
0
hasInitialResponse
public boolean kafkatest_f4665_0()
{    return true;}
f4665
0
getMechanismNames
public String[] kafkatest_f4675_0(Map<String, ?> props)
{    return OAuthBearerSaslServer.mechanismNamesCompatibleWithPolicy(props);}
f4675
0
configured
public boolean kafkatest_f4676_0()
{    return configured;}
f4676
0
getAuthorizationID
public String kafkatest_f4685_0()
{    if (!complete)        throw new IllegalStateException("Authentication exchange has not completed");    return tokenForNegotiatedProperty.principalName();}
f4685
0
getMechanismName
public String kafkatest_f4686_0()
{    return OAuthBearerLoginModule.OAUTHBEARER_MECHANISM;}
f4686
0
handleCallbackError
private voidf4695_1Exception e) throws SaslException
{    String msg = String.format("%s: %s", INTERNAL_ERROR_ON_SERVER, e.getMessage());        throw new SaslException(msg);}
private voidf4695
1
mechanismNamesCompatibleWithPolicy
public static String[] kafkatest_f4696_0(Map<String, ?> props)
{    return props != null && "true".equals(String.valueOf(props.get(Sasl.POLICY_NOPLAINTEXT))) ? new String[] {} : new String[] { OAuthBearerLoginModule.OAUTHBEARER_MECHANISM };}
f4696
0
header
public Map<String, Object> kafkatest_f4705_0()
{    return header;}
f4705
0
principalName
public String kafkatest_f4706_0()
{    return principalName;}
f4706
0
rawClaim
public Object kafkatest_f4715_0(String claimName)
{    return claims().get(Objects.requireNonNull(claimName));}
f4715
0
expirationTime
public Number kafkatest_f4716_0() throws OAuthBearerIllegalTokenException
{    return claim("exp", Number.class);}
f4716
0
time
 void kafkatest_f4725_0(Time time)
{    this.time = Objects.requireNonNull(time);}
f4725
0
configured
public boolean kafkatest_f4726_0()
{    return configured;}
f4726
0
optionValue
private String kafkatest_f4735_0(String key)
{    return optionValue(key, null);}
f4735
0
optionValue
private String kafkatest_f4736_0(String key, String defaultValue)
{    String explicitValue = option(key);    return explicitValue != null ? explicitValue : defaultValue;}
f4736
0
configure
public void kafkatest_f4745_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    if (!OAuthBearerLoginModule.OAUTHBEARER_MECHANISM.equals(saslMechanism))        throw new IllegalArgumentException(String.format("Unexpected SASL mechanism: %s", saslMechanism));    if (Objects.requireNonNull(jaasConfigEntries).size() != 1 || jaasConfigEntries.get(0) == null)        throw new IllegalArgumentException(String.format("Must supply exactly 1 non-null JAAS mechanism configuration (size was %d)", jaasConfigEntries.size()));    final Map<String, String> unmodifiableModuleOptions = Collections.unmodifiableMap((Map<String, String>) jaasConfigEntries.get(0).getOptions());    this.moduleOptions = unmodifiableModuleOptions;    configured = true;}
f4745
0
handle
public void kafkatest_f4746_0(Callback[] callbacks) throws UnsupportedCallbackException
{    if (!configured())        throw new IllegalStateException("Callback handler not configured");    for (Callback callback : callbacks) {        if (callback instanceof OAuthBearerValidatorCallback) {            OAuthBearerValidatorCallback validationCallback = (OAuthBearerValidatorCallback) callback;            try {                handleCallback(validationCallback);            } catch (OAuthBearerIllegalTokenException e) {                OAuthBearerValidationResult failureReason = e.reason();                String failureScope = failureReason.failureScope();                validationCallback.error(failureScope != null ? "insufficient_scope" : "invalid_token", failureScope, failureReason.failureOpenIdConfig());            }        } else if (callback instanceof OAuthBearerExtensionsValidatorCallback) {            OAuthBearerExtensionsValidatorCallback extensionsCallback = (OAuthBearerExtensionsValidatorCallback) callback;            extensionsCallback.inputExtensions().map().forEach((extensionName, v) -> extensionsCallback.valid(extensionName));        } else            throw new UnsupportedCallbackException(callback);    }}
f4746
0
newFailure
public static OAuthBearerValidationResult kafkatest_f4755_0(String failureDescription)
{    return newFailure(failureDescription, null, null);}
f4755
0
newFailure
public static OAuthBearerValidationResult kafkatest_f4756_0(String failureDescription, String failureScope, String failureOpenIdConfig)
{    return new OAuthBearerValidationResult(false, failureDescription, failureScope, failureOpenIdConfig);}
f4756
0
validateTimeConsistency
public static OAuthBearerValidationResult kafkatest_f4765_0(OAuthBearerUnsecuredJws jwt)
{    Number issuedAt;    Number expirationTime;    try {        issuedAt = Objects.requireNonNull(jwt).issuedAt();        expirationTime = jwt.expirationTime();    } catch (OAuthBearerIllegalTokenException e) {        return e.reason();    }    if (expirationTime != null && issuedAt != null && expirationTime.doubleValue() <= issuedAt.doubleValue())        return OAuthBearerValidationResult.newFailure(String.format("The Expiration Time time (%f seconds) was not after the Issued At time (%f seconds)", expirationTime.doubleValue(), issuedAt.doubleValue()));    return OAuthBearerValidationResult.newSuccess();}
f4765
0
validateScope
public static OAuthBearerValidationResult kafkatest_f4766_0(OAuthBearerToken token, List<String> requiredScope)
{    final Set<String> tokenScope = token.scope();    if (requiredScope == null || requiredScope.isEmpty())        return OAuthBearerValidationResult.newSuccess();    for (String requiredScopeElement : requiredScope) {        if (!tokenScope.contains(requiredScopeElement))            return OAuthBearerValidationResult.newFailure(String.format("The provided scope (%s) was mising a required scope (%s).  All required scope elements: %s", String.valueOf(tokenScope), requiredScopeElement, requiredScope.toString()), requiredScope.toString(), null);    }    return OAuthBearerValidationResult.newSuccess();}
f4766
0
error
public void kafkatest_f4775_0(String invalidExtensionName, String errorMessage)
{    if (Objects.requireNonNull(invalidExtensionName).isEmpty())        throw new IllegalArgumentException("extension name must not be empty");    this.invalidExtensions.put(invalidExtensionName, errorMessage);}
f4775
0
initialize
public void kafkatest_f4776_0(Subject subject, CallbackHandler callbackHandler, Map<String, ?> sharedState, Map<String, ?> options)
{    this.subject = Objects.requireNonNull(subject);    if (!(Objects.requireNonNull(callbackHandler) instanceof AuthenticateCallbackHandler))        throw new IllegalArgumentException(String.format("Callback handler must be castable to %s: %s", AuthenticateCallbackHandler.class.getName(), callbackHandler.getClass().getName()));    this.callbackHandler = (AuthenticateCallbackHandler) callbackHandler;}
f4776
0
errorCode
public String kafkatest_f4785_0()
{    return errorCode;}
f4785
0
errorDescription
public String kafkatest_f4786_0()
{    return errorDescription;}
f4786
0
token
public void kafkatest_f4795_0(OAuthBearerToken token)
{    this.token = Objects.requireNonNull(token);    this.errorStatus = null;    this.errorScope = null;    this.errorOpenIDConfiguration = null;}
f4795
0
error
public void kafkatest_f4796_0(String errorStatus, String errorScope, String errorOpenIDConfiguration)
{    if (Objects.requireNonNull(errorStatus).isEmpty())        throw new IllegalArgumentException("error status must not be empty");    this.errorStatus = errorStatus;    this.errorScope = errorScope;    this.errorOpenIDConfiguration = errorOpenIDConfiguration;    this.token = null;}
f4796
0
createSaslServer
public SaslServer kafkatest_f4806_0(String mechanism, String protocol, String serverName, Map<String, ?> props, CallbackHandler cbh) throws SaslException
{    if (!PLAIN_MECHANISM.equals(mechanism))        throw new SaslException(String.format("Mechanism \'%s\' is not supported. Only PLAIN is supported.", mechanism));    return new PlainSaslServer(cbh);}
f4806
0
getMechanismNames
public String[] kafkatest_f4807_0(Map<String, ?> props)
{    if (props == null)        return new String[] { PLAIN_MECHANISM };    String noPlainText = (String) props.get(Sasl.POLICY_NOPLAINTEXT);    if ("true".equals(noPlainText))        return new String[] {};    else        return new String[] { PLAIN_MECHANISM };}
f4807
0
login
public boolean kafkatest_f4817_0()
{    return true;}
f4817
0
logout
public boolean kafkatest_f4818_0()
{    return true;}
f4818
0
hash
public byte[] kafkatest_f4827_0(byte[] str)
{    return messageDigest.digest(str);}
f4827
0
xor
public byte[] kafkatest_f4828_0(byte[] first, byte[] second)
{    if (first.length != second.length)        throw new IllegalArgumentException("Argument arrays must be of the same length");    byte[] result = new byte[first.length];    for (int i = 0; i < result.length; i++) result[i] = (byte) (first[i] ^ second[i]);    return result;}
f4828
0
clientSignature
public byte[] kafkatest_f4837_0(byte[] storedKey, ClientFirstMessage clientFirstMessage, ServerFirstMessage serverFirstMessage, ClientFinalMessage clientFinalMessage) throws InvalidKeyException
{    byte[] authMessage = authMessage(clientFirstMessage, serverFirstMessage, clientFinalMessage);    return hmac(storedKey, authMessage);}
f4837
0
clientProof
public byte[] kafkatest_f4838_0(byte[] saltedPassword, ClientFirstMessage clientFirstMessage, ServerFirstMessage serverFirstMessage, ClientFinalMessage clientFinalMessage) throws InvalidKeyException
{    byte[] clientKey = clientKey(saltedPassword);    byte[] storedKey = hash(clientKey);    byte[] clientSignature = hmac(storedKey, authMessage(clientFirstMessage, serverFirstMessage, clientFinalMessage));    return xor(clientKey, clientSignature);}
f4838
0
mechanismName
public final String kafkatest_f4847_0()
{    return mechanismName;}
f4847
0
hashAlgorithm
public String kafkatest_f4848_0()
{    return hashAlgorithm;}
f4848
0
nonce
public String kafkatest_f4857_0()
{    return nonce;}
f4857
0
authorizationId
public String kafkatest_f4858_0()
{    return authorizationId;}
f4858
0
channelBinding
public byte[] kafkatest_f4867_0()
{    return channelBinding;}
f4867
0
nonce
public String kafkatest_f4868_0()
{    return nonce;}
f4868
0
hasInitialResponse
public boolean kafkatest_f4877_0()
{    return true;}
f4877
0
evaluateChallenge
public byte[]f4878_1byte[] challenge) throws SaslException
{    try {        switch(state) {            case SEND_CLIENT_FIRST_MESSAGE:                if (challenge != null && challenge.length != 0)                    throw new SaslException("Expected empty challenge");                clientNonce = formatter.secureRandomString();                NameCallback nameCallback = new NameCallback("Name:");                ScramExtensionsCallback extensionsCallback = new ScramExtensionsCallback();                try {                    callbackHandler.handle(new Callback[] { nameCallback });                    try {                        callbackHandler.handle(new Callback[] { extensionsCallback });                    } catch (UnsupportedCallbackException e) {                                            }                } catch (Throwable e) {                    throw new SaslException("User name or extensions could not be obtained", e);                }                String username = nameCallback.getName();                String saslName = formatter.saslName(username);                Map<String, String> extensions = extensionsCallback.extensions();                this.clientFirstMessage = new ScramMessages.ClientFirstMessage(saslName, clientNonce, extensions);                setState(State.RECEIVE_SERVER_FIRST_MESSAGE);                return clientFirstMessage.toBytes();            case RECEIVE_SERVER_FIRST_MESSAGE:                this.serverFirstMessage = new ServerFirstMessage(challenge);                if (!serverFirstMessage.nonce().startsWith(clientNonce))                    throw new SaslException("Invalid server nonce: does not start with client nonce");                if (serverFirstMessage.iterations() < mechanism.minIterations())                    throw new SaslException("Requested iterations " + serverFirstMessage.iterations() + " is less than the minimum " + mechanism.minIterations() + " for " + mechanism);                PasswordCallback passwordCallback = new PasswordCallback("Password:", false);                try {                    callbackHandler.handle(new Callback[] { passwordCallback });                } catch (Throwable e) {                    throw new SaslException("User name could not be obtained", e);                }                this.clientFinalMessage = handleServerFirstMessage(passwordCallback.getPassword());                setState(State.RECEIVE_SERVER_FINAL_MESSAGE);                return clientFinalMessage.toBytes();            case RECEIVE_SERVER_FINAL_MESSAGE:                ServerFinalMessage serverFinalMessage = new ServerFinalMessage(challenge);                if (serverFinalMessage.error() != null)                    throw new SaslException("Sasl authentication using " + mechanism + " failed with error: " + serverFinalMessage.error());                handleServerFinalMessage(serverFinalMessage.serverSignature());                setState(State.COMPLETE);                return null;            default:                throw new IllegalSaslStateException("Unexpected challenge in Sasl client state " + state);        }    } catch (SaslException e) {        setState(State.FAILED);        throw e;    }}
public byte[]f4878
1
getMechanismNames
public String[] kafkatest_f4888_0(Map<String, ?> props)
{    Collection<String> mechanisms = ScramMechanism.mechanismNames();    return mechanisms.toArray(new String[mechanisms.size()]);}
f4888
0
initialize
public static void kafkatest_f4889_0()
{    Security.addProvider(new ScramSaslClientProvider());}
f4889
0
verifyClientProof
private void kafkatest_f4899_0(ClientFinalMessage clientFinalMessage) throws SaslException
{    try {        byte[] expectedStoredKey = scramCredential.storedKey();        byte[] clientSignature = formatter.clientSignature(expectedStoredKey, clientFirstMessage, serverFirstMessage, clientFinalMessage);        byte[] computedStoredKey = formatter.storedKey(clientSignature, clientFinalMessage.proof());        if (!Arrays.equals(computedStoredKey, expectedStoredKey))            throw new SaslException("Invalid client credentials");    } catch (InvalidKeyException e) {        throw new SaslException("Sasl client verification failed", e);    }}
f4899
0
clearCredentials
private void kafkatest_f4900_0()
{    scramCredential = null;    clientFirstMessage = null;    serverFirstMessage = null;}
f4900
0
iterations
public int kafkatest_f4910_0()
{    return iterations;}
f4910
0
scramCredential
public void kafkatest_f4911_0(ScramCredential scramCredential)
{    this.scramCredential = scramCredential;}
f4911
0
createSslClientAuth
private static SslClientAuthf4921_1String key)
{    SslClientAuth auth = SslClientAuth.forConfig(key);    if (auth != null) {        return auth;    }        return SslClientAuth.NONE;}
private static SslClientAuthf4921
1
createSecureRandom
private static SecureRandom kafkatest_f4922_0(String key)
{    if (key == null) {        return null;    }    try {        return SecureRandom.getInstance(key);    } catch (GeneralSecurityException e) {        throw new KafkaException(e);    }}
f4922
0
shouldBeRebuilt
public boolean kafkatest_f4931_0(Map<String, Object> nextConfigs)
{    if (!nextConfigs.equals(configs)) {        return true;    }    if (truststore != null && truststore.modified()) {        return true;    }    if (keystore != null && keystore.modified()) {        return true;    }    return false;}
f4931
0
load
 KeyStore kafkatest_f4932_0()
{    try (InputStream in = Files.newInputStream(Paths.get(path))) {        KeyStore ks = KeyStore.getInstance(type);        // If a password is not set access to the truststore is still available, but integrity checking is disabled.        char[] passwordChars = password != null ? password.value().toCharArray() : null;        ks.load(in, passwordChars);        return ks;    } catch (GeneralSecurityException | IOException e) {        throw new KafkaException("Failed to load SSL keystore " + path + " of type " + type, e);    }}
f4932
0
createSslEngine
public SSLEngine kafkatest_f4941_0(String peerHost, int peerPort)
{    if (sslEngineBuilder == null) {        throw new IllegalStateException("SslFactory has not been configured.");    }    return sslEngineBuilder.createSslEngine(mode, peerHost, peerPort, endpointIdentification);}
f4941
0
sslContext
public SSLContext kafkatest_f4942_0()
{    return sslEngineBuilder.sslContext();}
f4942
0
createSslEngineForValidation
private static SSLEngine kafkatest_f4951_0(SslEngineBuilder sslEngineBuilder, Mode mode)
{    // Use empty hostname, disable hostname verification    return sslEngineBuilder.createSslEngine(mode, "", 0, "");}
f4951
0
validate
 static void kafkatest_f4952_0(SSLEngine clientEngine, SSLEngine serverEngine) throws SSLException
{    SslEngineValidator clientValidator = new SslEngineValidator(clientEngine);    SslEngineValidator serverValidator = new SslEngineValidator(serverEngine);    try {        clientValidator.beginHandshake();        serverValidator.beginHandshake();        while (!serverValidator.complete() || !clientValidator.complete()) {            clientValidator.handshake(serverValidator);            serverValidator.handshake(clientValidator);        }    } finally {        clientValidator.close();        serverValidator.close();    }}
f4952
0
toString
public String kafkatest_f4961_0()
{    return "SslPrincipalMapper(rules = " + rules + ")";}
f4961
0
apply
 String kafkatest_f4962_0(String distinguishedName)
{    if (isDefault) {        return distinguishedName;    }    String result = null;    final Matcher m = pattern.matcher(distinguishedName);    if (m.matches()) {        result = distinguishedName.replaceAll(pattern.pattern(), escapeLiteralBackReferences(replacement, m.groupCount()));    }    if (toLowerCase && result != null) {        result = result.toLowerCase(Locale.ENGLISH);    } else if (toUpperCase & result != null) {        result = result.toUpperCase(Locale.ENGLISH);    }    return result;}
f4962
0
credential
public ScramCredential kafkatest_f4971_0(String mechanism, String tokenId)
{    CredentialCache.Cache<ScramCredential> cache = credentialCache.cache(mechanism, ScramCredential.class);    return cache == null ? null : cache.get(tokenId);}
f4971
0
owner
public String kafkatest_f4972_0(String tokenId)
{    TokenInformation tokenInfo = tokenCache.get(tokenId);    return tokenInfo == null ? null : tokenInfo.owner().getName();}
f4972
0
credentialCache
public CredentialCache.Cache<ScramCredential> kafkatest_f4981_0(String mechanism)
{    return credentialCache.cache(mechanism, ScramCredential.class);}
f4981
0
updateCredentials
private void kafkatest_f4982_0(String tokenId, Map<String, ScramCredential> scramCredentialMap)
{    for (String mechanism : ScramMechanism.mechanismNames()) {        CredentialCache.Cache<ScramCredential> cache = credentialCache.cache(mechanism, ScramCredential.class);        if (cache != null) {            ScramCredential credential = scramCredentialMap.get(mechanism);            if (credential == null) {                cache.remove(tokenId);            } else {                cache.put(tokenId, credential);            }        }    }}
f4982
0
issueTimestamp
public long kafkatest_f4991_0()
{    return issueTimestamp;}
f4991
0
expiryTimestamp
public long kafkatest_f4992_0()
{    return expiryTimestamp;}
f4992
0
serialize
public byte[] kafkatest_f5001_0(String topic, byte[] data)
{    return data;}
f5001
0
deserialize
public ByteBuffer kafkatest_f5002_0(String topic, byte[] data)
{    if (data == null)        return null;    return ByteBuffer.wrap(data);}
f5002
0
deserialize
public T kafkatest_f5011_0(String topic, Headers headers, byte[] data)
{    return deserialize(topic, data);}
f5011
0
configure
public void kafkatest_f5012_0(Map<String, ?> configs, boolean isKey)
{    deserializer.configure(configs, isKey);}
f5012
0
deserialize
public Float kafkatest_f5021_0(final String topic, final byte[] data)
{    if (data == null)        return null;    if (data.length != 4) {        throw new SerializationException("Size of data received by Deserializer is not 4");    }    int value = 0;    for (byte b : data) {        value <<= 8;        value |= b & 0xFF;    }    return Float.intBitsToFloat(value);}
f5021
0
serialize
public byte[] kafkatest_f5022_0(final String topic, final Float data)
{    if (data == null)        return null;    long bits = Float.floatToRawIntBits(data);    return new byte[] { (byte) (bits >>> 24), (byte) (bits >>> 16), (byte) (bits >>> 8), (byte) bits };}
f5022
0
serializer
public Serializer<T> kafkatest_f5031_0()
{    return serializer;}
f5031
0
deserializer
public Deserializer<T> kafkatest_f5032_0()
{    return deserializer;}
f5032
0
ByteBuffer
public static Serde<ByteBuffer> kafkatest_f5041_0()
{    return new ByteBufferSerde();}
f5041
0
Bytes
public static Serde<Bytes> kafkatest_f5042_0()
{    return new BytesSerde();}
f5042
0
deserialize
public String kafkatest_f5051_0(String topic, byte[] data)
{    try {        if (data == null)            return null;        else            return new String(data, encoding);    } catch (UnsupportedEncodingException e) {        throw new SerializationException("Error when deserializing byte[] to string due to unsupported encoding " + encoding);    }}
f5051
0
configure
public void kafkatest_f5052_0(Map<String, ?> configs, boolean isKey)
{    String propertyName = isKey ? "key.serializer.encoding" : "value.serializer.encoding";    Object encodingValue = configs.get(propertyName);    if (encodingValue == null)        encodingValue = configs.get("serializer.encoding");    if (encodingValue instanceof String)        encoding = (String) encodingValue;}
f5052
0
equals
public boolean kafkatest_f5061_0(Object obj)
{    if (this == obj)        return true;    if (obj == null)        return false;    if (getClass() != obj.getClass())        return false;    TopicPartition other = (TopicPartition) obj;    return partition == other.partition && Objects.equals(topic, other.topic);}
f5061
0
toString
public String kafkatest_f5062_0()
{    return topic + "-" + partition;}
f5062
0
partition
public int kafkatest_f5071_0()
{    return partition;}
f5071
0
brokerId
public int kafkatest_f5072_0()
{    return brokerId;}
f5072
0
maybeComputeNext
private Boolean kafkatest_f5081_0()
{    state = State.FAILED;    next = makeNext();    if (state == State.DONE) {        return false;    } else {        state = State.READY;        return true;    }}
f5081
0
getVersion
public static String kafkatest_f5082_0()
{    return VERSION;}
f5082
0
getStartTimeMs
public Long kafkatest_f5091_0()
{    return startTimeMs;}
f5091
0
value
public T kafkatest_f5092_0(MetricConfig config, long now)
{    return value;}
f5092
0
limit
public int kafkatest_f5101_0()
{    return buffer.limit();}
f5101
0
position
public void kafkatest_f5102_0(int position)
{    ensureRemaining(position - buffer.position());    buffer.position(position);}
f5102
0
toString
public String kafkatest_f5111_0()
{    return Bytes.toString(bytes, 0, bytes.length);}
f5111
0
toString
private static String kafkatest_f5112_0(final byte[] b, int off, int len)
{    StringBuilder result = new StringBuilder();    if (b == null)        return result.toString();    // just in case we are passed a 'len' that is > buffer length...    if (off >= b.length)        return result.toString();    if (off + len > b.length)        len = b.length - off;    for (int i = off; i < off + len; ++i) {        int ch = b[i] & 0xFF;        if (ch >= ' ' && ch <= '~' && ch != '\\') {            result.append((char) ch);        } else {            result.append("\\x");            result.append(HEX_CHARS_UPPER[ch / 0x10]);            result.append(HEX_CHARS_UPPER[ch % 0x10]);        }    }    return result.toString();}
f5112
0
writeUnsignedIntLE
public static void kafkatest_f5121_0(OutputStream out, int value) throws IOException
{    out.write(value);    out.write(value >>> 8);    out.write(value >>> 16);    out.write(value >>> 24);}
f5121
0
writeUnsignedIntLE
public static void kafkatest_f5122_0(byte[] buffer, int offset, int value)
{    buffer[offset] = (byte) value;    buffer[offset + 1] = (byte) (value >>> 8);    buffer[offset + 2] = (byte) (value >>> 16);    buffer[offset + 3] = (byte) (value >>> 24);}
f5122
0
writeVarint
public static void kafkatest_f5131_0(int value, DataOutput out) throws IOException
{    writeUnsignedVarint((value << 1) ^ (value >> 31), out);}
f5131
0
writeVarint
public static void kafkatest_f5132_0(int value, ByteBuffer buffer)
{    writeUnsignedVarint((value << 1) ^ (value >> 31), buffer);}
f5132
0
update
public static void kafkatest_f5141_0(Checksum checksum, ByteBuffer buffer, int offset, int length)
{    if (buffer.hasArray()) {        checksum.update(buffer.array(), buffer.position() + buffer.arrayOffset() + offset, length);    } else {        int start = buffer.position() + offset;        for (int i = start; i < start + length; i++) checksum.update(buffer.get(i));    }}
f5141
0
updateInt
public static void kafkatest_f5142_0(Checksum checksum, int input)
{    checksum.update((byte) (input >> 24));    checksum.update((byte) (input >> 16));    checksum.update((byte) (input >> 8));    checksum.update((byte) input);}
f5142
0
remove
public void kafkatest_f5152_0()
{    inner.remove();}
f5152
0
subtractMap
public static Map<K, V> kafkatest_f5153_0(Map<? extends K, ? extends V> minuend, Map<? extends K, ? extends V> subtrahend)
{    return minuend.entrySet().stream().filter(entry -> !subtrahend.containsKey(entry.getKey())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));}
f5153
0
size
public int kafkatest_f5162_0()
{    return map.size();}
f5162
0
values
public Collection<V> kafkatest_f5163_0()
{    return map.values();}
f5163
0
crc32
public static long kafkatest_f5172_0(byte[] bytes)
{    return crc32(bytes, 0, bytes.length);}
f5172
0
crc32
public static long kafkatest_f5173_0(byte[] bytes, int offset, int size)
{    Crc32 crc = new Crc32();    crc.update(bytes, offset, size);    return crc.getValue();}
f5173
0
create
public Checksum kafkatest_f5182_0()
{    try {        return (Checksum) CONSTRUCTOR.invoke();    } catch (Throwable throwable) {        // Should never happen        throw new RuntimeException(throwable);    }}
f5182
0
create
public Checksum kafkatest_f5183_0()
{    return new PureJavaCrc32C();}
f5183
0
resetExitProcedure
public static void kafkatest_f5192_0()
{    exitProcedure = DEFAULT_EXIT_PROCEDURE;}
f5192
0
resetHaltProcedure
public static void kafkatest_f5193_0()
{    haltProcedure = DEFAULT_HALT_PROCEDURE;}
f5193
0
setNext
public void kafkatest_f5202_0(int next)
{    this.next = next;}
f5202
0
indexToElement
private static Element kafkatest_f5203_0(Element head, Element[] elements, int index)
{    if (index == HEAD_INDEX) {        return head;    }    return elements[index];}
f5203
0
remove
public void kafkatest_f5212_0()
{    if (lastReturnedSlot == INVALID_INDEX) {        throw new IllegalStateException();    }    if (cur == indexToElement(head, elements, lastReturnedSlot)) {        cursor--;        cur = indexToElement(head, elements, cur.prev());    }    ImplicitLinkedHashCollection.this.removeElementAtSlot(lastReturnedSlot);    lastReturnedSlot = INVALID_INDEX;}
f5212
0
set
public void kafkatest_f5213_0(E e)
{    throw new UnsupportedOperationException();}
f5213
0
clear
public void kafkatest_f5222_0()
{    ImplicitLinkedHashCollection.this.clear();}
f5222
0
iterator
public final Iterator<E> kafkatest_f5223_0()
{    return listIterator(0);}
f5223
0
mustAdd
public final void kafkatest_f5232_0(E newElement)
{    if (!add(newElement)) {        throw new RuntimeException("Unable to add " + newElement);    }}
f5232
0
addInternal
 int kafkatest_f5233_0(Element newElement, Element[] addElements)
{    int slot = slot(addElements, newElement);    for (int seen = 0; seen < addElements.length; seen++) {        Element element = addElements[slot];        if (element == null) {            addElements[slot] = newElement;            return slot;        }        if (element.equals(newElement)) {            return INVALID_INDEX;        }        slot = (slot + 1) % addElements.length;    }    throw new RuntimeException("Not enough hash table slots to add a new element.");}
f5233
0
hashCode
public int kafkatest_f5242_0()
{    return this.valuesList().hashCode();}
f5242
0
numSlots
 final int kafkatest_f5243_0()
{    return elements.length;}
f5243
0
isJava9Compatible
 boolean kafkatest_f5252_0()
{    return majorVersion >= 9;}
f5252
0
daemon
public static KafkaThread kafkatest_f5253_0(final String name, Runnable runnable)
{    return new KafkaThread(name, runnable, true);}
f5253
0
isTraceEnabled
public boolean kafkatest_f5262_0(Marker marker)
{    return logger.isTraceEnabled(marker);}
f5262
0
isDebugEnabled
public boolean kafkatest_f5263_0()
{    return logger.isDebugEnabled();}
f5263
0
trace
public void kafkatest_f5272_0(String format, Object arg)
{    if (logger.isTraceEnabled()) {        writeLog(null, LocationAwareLogger.TRACE_INT, format, new Object[] { arg }, null);    }}
f5272
0
trace
public void kafkatest_f5273_0(String format, Object arg1, Object arg2)
{    if (logger.isTraceEnabled()) {        writeLog(null, LocationAwareLogger.TRACE_INT, format, new Object[] { arg1, arg2 }, null);    }}
f5273
0
debug
public voidf5282_1String format, Object arg)
{    if (logger.isDebugEnabled()) {        writeLog(null, LocationAware    }}
public voidf5282
1
debug
public voidf5283_1String format, Object arg1, Object arg2)
{    if (logger.isDebugEnabled()) {        writeLog(null, LocationAware    }}
public voidf5283
1
warn
public voidf5292_1String format, Object arg)
{    writeLog(null, LocationAware}
public voidf5292
1
warn
public voidf5293_1String message, Object arg1, Object arg2)
{    writeLog(null, LocationAware}
public voidf5293
1
error
public voidf5302_1String format, Object arg)
{    writeLog(null, LocationAware}
public voidf5302
1
error
public voidf5303_1String format, Object arg1, Object arg2)
{    writeLog(null, LocationAware}
public voidf5303
1
info
public voidf5312_1String format, Object arg)
{    writeLog(null, LocationAware}
public voidf5312
1
info
public voidf5313_1String format, Object arg1, Object arg2)
{    writeLog(null, LocationAware}
public voidf5313
1
getName
public String kafkatest_f5322_0()
{    return logger.getName();}
f5322
0
isTraceEnabled
public boolean kafkatest_f5323_0()
{    return logger.isTraceEnabled();}
f5323
0
isErrorEnabled
public boolean kafkatest_f5332_0(Marker marker)
{    return logger.isErrorEnabled(marker);}
f5332
0
trace
public void kafkatest_f5333_0(String message)
{    if (logger.isTraceEnabled()) {        logger.trace(addPrefix(message));    }}
f5333
0
trace
public void kafkatest_f5342_0(Marker marker, String msg, Throwable t)
{    if (logger.isTraceEnabled()) {        logger.trace(marker, addPrefix(msg), t);    }}
f5342
0
debug
public voidf5343_1String message)
{    if (logger.isDebugEnabled()) {            }}
public voidf5343
1
debug
public voidf5352_1Marker marker, String msg, Throwable t)
{    if (logger.isDebugEnabled()) {            }}
public voidf5352
1
warn
public voidf5353_1String message)
{    }
public voidf5353
1
warn
public voidf5362_1Marker marker, String msg, Throwable t)
{    }
public voidf5362
1
error
public voidf5363_1String message)
{    }
public voidf5363
1
error
public voidf5372_1Marker marker, String msg, Throwable t)
{    }
public voidf5372
1
info
public voidf5373_1String message)
{    }
public voidf5373
1
info
public voidf5382_1Marker marker, String msg, Throwable t)
{    }
public voidf5382
1
register
public voidf5383_1) throws ReflectiveOperationException
{    Map<String, Object> jvmSignalHandlers = new ConcurrentHashMap<>();    for (String signal : SIGNALS) {        register(signal, jvmSignalHandlers);    }    }
public voidf5383
1
unmapJava9
private static MethodHandle kafkatest_f5392_0(MethodHandles.Lookup lookup) throws ReflectiveOperationException
{    Class<?> unsafeClass = Class.forName("sun.misc.Unsafe");    MethodHandle unmapper = lookup.findVirtual(unsafeClass, "invokeCleaner", methodType(void.class, ByteBuffer.class));    Field f = unsafeClass.getDeclaredField("theUnsafe");    f.setAccessible(true);    Object theUnsafe = f.get(null);    return unmapper.bindTo(theUnsafe);}
f5392
0
nonNull
private static boolean kafkatest_f5393_0(Object o)
{    return o != null;}
f5393
0
parseKafkaPrincipal
public static KafkaPrincipal kafkatest_f5402_0(String str)
{    if (str == null || str.isEmpty()) {        throw new IllegalArgumentException("expected a string in format principalType:principalName but got " + str);    }    String[] split = str.split(":", 2);    if (split.length != 2) {        throw new IllegalArgumentException("expected a string in format principalType:principalName but got " + str);    }    return new KafkaPrincipal(split[0], split[1]);}
f5402
0
addConfiguredSecurityProviders
public static voidf5403_1Map<String, ?> configs)
{    String securityProviderClassesStr = (String) configs.get(SecurityConfig.SECURITY_PROVIDERS_CONFIG);    if (securityProviderClassesStr == null || securityProviderClassesStr.equals("")) {        return;    }    try {        String[] securityProviderClasses = securityProviderClassesStr.replaceAll("\\s+", "").split(",");        for (int index = 0; index < securityProviderClasses.length; index++) {            SecurityProviderCreator securityProviderCreator = (SecurityProviderCreator) Class.forName(securityProviderClasses[index]).newInstance();            securityProviderCreator.configure(configs);            Security.insertProviderAt(securityProviderCreator.getProvider(), index + 1);        }    } catch (ClassCastException e) {            } catch (ClassNotFoundException cnfe) {            } catch (IllegalAccessException | InstantiationException e) {            }}
public static voidf5403
1
getExitCode
public int kafkatest_f5412_0()
{    return exitCode;}
f5412
0
execute
public void kafkatest_f5413_0() throws IOException
{    this.run();}
f5413
0
schedule
public Future<T> kafkatest_f5422_0(final ScheduledExecutorService executor, final Callable<T> callable, long delayMs)
{    return executor.schedule(callable, delayMs, TimeUnit.MILLISECONDS);}
f5422
0
milliseconds
public long kafkatest_f5423_0()
{    return System.currentTimeMillis();}
f5423
0
updateAndReset
public void kafkatest_f5432_0(long timeoutMs)
{    update();    reset(timeoutMs);}
f5432
0
reset
public void kafkatest_f5433_0(long timeoutMs)
{    if (timeoutMs < 0)        throw new IllegalArgumentException("Invalid negative timeout " + timeoutMs);    this.startMs = this.currentTimeMs;    if (currentTimeMs > Long.MAX_VALUE - timeoutMs)        this.deadlineMs = Long.MAX_VALUE;    else        this.deadlineMs = currentTimeMs + timeoutMs;}
f5433
0
utf8
public static String kafkatest_f5442_0(ByteBuffer buffer, int length)
{    return utf8(buffer, 0, length);}
f5442
0
utf8
public static String kafkatest_f5443_0(ByteBuffer buffer)
{    return utf8(buffer, buffer.remaining());}
f5443
0
toArray
public static byte[] kafkatest_f5452_0(ByteBuffer buffer, int size)
{    return toArray(buffer, 0, size);}
f5452
0
toNullableArray
public static byte[] kafkatest_f5453_0(ByteBuffer buffer)
{    return buffer == null ? null : toArray(buffer);}
f5453
0
murmur2
public static int kafkatest_f5462_0(final byte[] data)
{    int length = data.length;    int seed = 0x9747b28c;    // 'm' and 'r' are mixing constants generated offline.    // They're not really 'magic', they just happen to work well.    final int m = 0x5bd1e995;    final int r = 24;    // Initialize the hash to a random value    int h = seed ^ length;    int length4 = length / 4;    for (int i = 0; i < length4; i++) {        final int i4 = i * 4;        int k = (data[i4 + 0] & 0xff) + ((data[i4 + 1] & 0xff) << 8) + ((data[i4 + 2] & 0xff) << 16) + ((data[i4 + 3] & 0xff) << 24);        k *= m;        k ^= k >>> r;        k *= m;        h *= m;        h ^= k;    }    // Handle the last few bytes of the input array    switch(length % 4) {        case 3:            h ^= (data[(length & ~3) + 2] & 0xff) << 16;        case 2:            h ^= (data[(length & ~3) + 1] & 0xff) << 8;        case 1:            h ^= data[length & ~3] & 0xff;            h *= m;    }    h ^= h >>> 13;    h *= m;    h ^= h >>> 15;    return h;}
f5462
0
getHost
public static String kafkatest_f5463_0(String address)
{    Matcher matcher = HOST_PORT_PATTERN.matcher(address);    return matcher.matches() ? matcher.group(1) : null;}
f5463
0
loadProps
public static Properties kafkatest_f5472_0(String filename) throws IOException
{    Properties props = new Properties();    if (filename != null) {        try (InputStream propStream = Files.newInputStream(Paths.get(filename))) {            props.load(propStream);        }    } else {        System.out.println("Did not load any properties since the property file is not specified");    }    return props;}
f5472
0
propsToStringMap
public static Map<String, String> kafkatest_f5473_0(Properties props)
{    Map<String, String> result = new HashMap<>();    for (Map.Entry<Object, Object> entry : props.entrySet()) result.put(entry.getKey().toString(), entry.getValue().toString());    return result;}
f5473
0
getKey
public K kafkatest_f5482_0()
{    return k;}
f5482
0
getValue
public V kafkatest_f5483_0()
{    return v;}
f5483
0
getKafkaClassLoader
public static ClassLoader kafkatest_f5492_0()
{    return Utils.class.getClassLoader();}
f5492
0
getContextOrKafkaClassLoader
public static ClassLoader kafkatest_f5493_0()
{    ClassLoader cl = Thread.currentThread().getContextClassLoader();    if (cl == null)        return getKafkaClassLoader();    else        return cl;}
f5493
0
readFully
public static final void kafkatest_f5502_0(InputStream inputStream, ByteBuffer destinationBuffer) throws IOException
{    if (!destinationBuffer.hasArray())        throw new IllegalArgumentException("destinationBuffer must be backed by an array");    int initialOffset = destinationBuffer.arrayOffset() + destinationBuffer.position();    byte[] array = destinationBuffer.array();    int length = destinationBuffer.remaining();    int totalBytesRead = 0;    do {        int bytesRead = inputStream.read(array, initialOffset + totalBytesRead, length - totalBytesRead);        if (bytesRead == -1)            break;        totalBytesRead += bytesRead;    } while (length > totalBytesRead);    destinationBuffer.position(destinationBuffer.position() + totalBytesRead);}
f5502
0
writeFully
public static void kafkatest_f5503_0(FileChannel channel, ByteBuffer sourceBuffer) throws IOException
{    while (sourceBuffer.hasRemaining()) channel.write(sourceBuffer);}
f5503
0
exception
public Optional<ApiException> kafkatest_f5512_0()
{    return exception == null ? Optional.empty() : Optional.of(exception);}
f5512
0
exception
public Optional<ApiException> kafkatest_f5513_0()
{    return exception == null ? Optional.empty() : Optional.of(exception);}
f5513
0
equals
public boolean kafkatest_f5522_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof Action)) {        return false;    }    Action that = (Action) o;    return Objects.equals(this.resourcePattern, that.resourcePattern) && Objects.equals(this.operation, that.operation) && this.resourceReferenceCount == that.resourceReferenceCount && this.logIfAllowed == that.logIfAllowed && this.logIfDenied == that.logIfDenied;}
f5522
0
hashCode
public int kafkatest_f5523_0()
{    return Objects.hash(resourcePattern, operation, resourceReferenceCount, logIfAllowed, logIfDenied);}
f5523
0
configs
public Map<String, String> kafkatest_f5532_0()
{    return configs;}
f5532
0
toString
public String kafkatest_f5533_0()
{    return "CreateTopicPolicy.RequestMetadata(topic=" + topic + ", numPartitions=" + numPartitions + ", replicationFactor=" + replicationFactor + ", replicasAssignments=" + replicasAssignments + ", configs=" + configs + ")";}
f5533
0
clientConfigs
 static Map<String, Object> kafkatest_f5542_0(String... overrides)
{    Map<String, Object> map = new HashMap<>();    map.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:8121");    map.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, "1000");    if (overrides.length % 2 != 0) {        throw new IllegalStateException();    }    for (int i = 0; i < overrides.length; i += 2) {        map.put(overrides[i], overrides[i + 1]);    }    return map;}
f5542
0
kafkaAdminClientNetworkThreadPrefix
public static String kafkatest_f5543_0()
{    return KafkaAdminClient.NETWORK_THREAD_PREFIX;}
f5543
0
testMetadataRefreshBackoff
public void kafkatest_f5552_0()
{    mgr.transitionToUpdatePending(time.milliseconds());    assertEquals(Long.MAX_VALUE, mgr.metadataFetchDelayMs(time.milliseconds()));    mgr.updateFailed(new RuntimeException());    assertEquals(refreshBackoffMs, mgr.metadataFetchDelayMs(time.milliseconds()));    // Even if we explicitly request an update, the backoff should be respected    mgr.requestUpdate();    assertEquals(refreshBackoffMs, mgr.metadataFetchDelayMs(time.milliseconds()));    time.sleep(refreshBackoffMs);    assertEquals(0, mgr.metadataFetchDelayMs(time.milliseconds()));}
f5552
0
testAuthenticationFailure
public void kafkatest_f5553_0()
{    mgr.transitionToUpdatePending(time.milliseconds());    mgr.updateFailed(new AuthenticationException("Authentication failed"));    assertEquals(refreshBackoffMs, mgr.metadataFetchDelayMs(time.milliseconds()));    try {        mgr.isReady();        fail("Expected AuthenticationException to be thrown");    } catch (AuthenticationException e) {    // Expected    }    mgr.update(mockCluster(), time.milliseconds());    assertTrue(mgr.isReady());}
f5553
0
mockBootstrapCluster
private static Cluster kafkatest_f5562_0()
{    return Cluster.bootstrap(ClientUtils.parseAndValidateAddresses(singletonList("localhost:8121"), ClientDnsLookup.DEFAULT));}
f5562
0
mockClientEnv
private static AdminClientUnitTestEnv kafkatest_f5563_0(String... configVals)
{    return new AdminClientUnitTestEnv(mockCluster(0), configVals);}
f5563
0
testUnreachableBootstrapServer
public void kafkatest_f5572_0() throws Exception
{    // This tests the scenario in which the bootstrap server is unreachable for a short while,    // which prevents AdminClient from being able to send the initial metadata request    Cluster cluster = Cluster.bootstrap(singletonList(new InetSocketAddress("localhost", 8121)));    Map<Node, Long> unreachableNodes = Collections.singletonMap(cluster.nodes().get(0), 200L);    try (final AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(Time.SYSTEM, cluster, AdminClientUnitTestEnv.clientConfigs(), unreachableNodes)) {        Cluster discoveredCluster = mockCluster(0);        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(body -> body instanceof MetadataRequest, MetadataResponse.prepareResponse(discoveredCluster.nodes(), discoveredCluster.clusterResource().clusterId(), 1, Collections.emptyList()));        env.kafkaClient().prepareResponse(body -> body instanceof CreateTopicsRequest, prepareCreateTopicsResponse("myTopic", Errors.NONE));        KafkaFuture<Void> future = env.adminClient().createTopics(Collections.singleton(new NewTopic("myTopic", Collections.singletonMap(0, asList(0, 1, 2)))), new CreateTopicsOptions().timeoutMs(10000)).all();        future.get();    }}
f5572
0
testPropagatedMetadataFetchException
public void kafkatest_f5573_0() throws Exception
{    Cluster cluster = mockCluster(0);    try (final AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(Time.SYSTEM, cluster, newStrMap(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:8121", AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, "10"))) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().createPendingAuthenticationError(cluster.nodeById(0), TimeUnit.DAYS.toMillis(1));        env.kafkaClient().prepareResponse(prepareCreateTopicsResponse("myTopic", Errors.NONE));        KafkaFuture<Void> future = env.adminClient().createTopics(Collections.singleton(new NewTopic("myTopic", Collections.singletonMap(0, asList(0, 1, 2)))), new CreateTopicsOptions().timeoutMs(1000)).all();        TestUtils.assertFutureError(future, SaslAuthenticationException.class);    }}
f5573
0
testDescribeAcls
public void kafkatest_f5582_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Test a call where we get back ACL1 and ACL2.        env.kafkaClient().prepareResponse(new DescribeAclsResponse(0, ApiError.NONE, asList(ACL1, ACL2)));        assertCollectionIs(env.adminClient().describeAcls(FILTER1).values().get(), ACL1, ACL2);        // Test a call where we get back no results.        env.kafkaClient().prepareResponse(new DescribeAclsResponse(0, ApiError.NONE, Collections.<AclBinding>emptySet()));        assertTrue(env.adminClient().describeAcls(FILTER2).values().get().isEmpty());        // Test a call where we get back an error.        env.kafkaClient().prepareResponse(new DescribeAclsResponse(0, new ApiError(Errors.SECURITY_DISABLED, "Security is disabled"), Collections.<AclBinding>emptySet()));        TestUtils.assertFutureError(env.adminClient().describeAcls(FILTER2).values(), SecurityDisabledException.class);        // Test a call where we supply an invalid filter.        TestUtils.assertFutureError(env.adminClient().describeAcls(UNKNOWN_FILTER).values(), InvalidRequestException.class);    }}
f5582
0
testCreateAcls
public void kafkatest_f5583_0() throws Exception
{    try (AdminClientUnitTestEnv env = mockClientEnv()) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Test a call where we successfully create two ACLs.        env.kafkaClient().prepareResponse(new CreateAclsResponse(0, asList(new AclCreationResponse(ApiError.NONE), new AclCreationResponse(ApiError.NONE))));        CreateAclsResult results = env.adminClient().createAcls(asList(ACL1, ACL2));        assertCollectionIs(results.values().keySet(), ACL1, ACL2);        for (KafkaFuture<Void> future : results.values().values()) future.get();        results.all().get();        // Test a call where we fail to create one ACL.        env.kafkaClient().prepareResponse(new CreateAclsResponse(0, asList(new AclCreationResponse(new ApiError(Errors.SECURITY_DISABLED, "Security is disabled")), new AclCreationResponse(ApiError.NONE))));        results = env.adminClient().createAcls(asList(ACL1, ACL2));        assertCollectionIs(results.values().keySet(), ACL1, ACL2);        TestUtils.assertFutureError(results.values().get(ACL1), SecurityDisabledException.class);        results.values().get(ACL2).get();        TestUtils.assertFutureError(results.all(), SecurityDisabledException.class);    }}
f5583
0
testDescribeCluster
public void kafkatest_f5592_0() throws Exception
{    final HashMap<Integer, Node> nodes = new HashMap<>();    Node node0 = new Node(0, "localhost", 8121);    Node node1 = new Node(1, "localhost", 8122);    Node node2 = new Node(2, "localhost", 8123);    Node node3 = new Node(3, "localhost", 8124);    nodes.put(0, node0);    nodes.put(1, node1);    nodes.put(2, node2);    nodes.put(3, node3);    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.emptyList(), Collections.emptySet(), Collections.emptySet(), nodes.get(0));    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster, AdminClientConfig.RETRIES_CONFIG, "2")) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Prepare the metadata response used for the first describe cluster        MetadataResponse response = MetadataResponse.prepareResponse(0, new ArrayList<>(nodes.values()), env.cluster().clusterResource().clusterId(), 2, Collections.emptyList(), MetadataResponse.AUTHORIZED_OPERATIONS_OMITTED);        env.kafkaClient().prepareResponse(response);        // Prepare the metadata response used for the second describe cluster        MetadataResponse response2 = MetadataResponse.prepareResponse(0, new ArrayList<>(nodes.values()), env.cluster().clusterResource().clusterId(), 3, Collections.emptyList(), 1 << AclOperation.DESCRIBE.code() | 1 << AclOperation.ALTER.code());        env.kafkaClient().prepareResponse(response2);        // Test DescribeCluster with the authorized operations omitted.        final DescribeClusterResult result = env.adminClient().describeCluster();        assertEquals(env.cluster().clusterResource().clusterId(), result.clusterId().get());        assertEquals(2, result.controller().get().id());        assertEquals(null, result.authorizedOperations().get());        // Test DescribeCluster with the authorized operations included.        final DescribeClusterResult result2 = env.adminClient().describeCluster();        assertEquals(env.cluster().clusterResource().clusterId(), result2.clusterId().get());        assertEquals(3, result2.controller().get().id());        assertEquals(new HashSet<>(Arrays.asList(AclOperation.DESCRIBE, AclOperation.ALTER)), result2.authorizedOperations().get());    }}
f5592
0
testListConsumerGroups
public void kafkatest_f5593_0() throws Exception
{    final HashMap<Integer, Node> nodes = new HashMap<>();    Node node0 = new Node(0, "localhost", 8121);    Node node1 = new Node(1, "localhost", 8122);    Node node2 = new Node(2, "localhost", 8123);    Node node3 = new Node(3, "localhost", 8124);    nodes.put(0, node0);    nodes.put(1, node1);    nodes.put(2, node2);    nodes.put(3, node3);    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.emptyList(), Collections.emptySet(), Collections.emptySet(), nodes.get(0));    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster, AdminClientConfig.RETRIES_CONFIG, "2")) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        // Empty metadata response should be retried        env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(Collections.emptyList(), env.cluster().clusterResource().clusterId(), -1, Collections.emptyList()));        env.kafkaClient().prepareResponse(MetadataResponse.prepareResponse(env.cluster().nodes(), env.cluster().clusterResource().clusterId(), env.cluster().controller().id(), Collections.emptyList()));        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.NONE.code()).setGroups(Arrays.asList(new ListGroupsResponseData.ListedGroup().setGroupId("group-1").setProtocolType(ConsumerProtocol.PROTOCOL_TYPE), new ListGroupsResponseData.ListedGroup().setGroupId("group-connect-1").setProtocolType("connector")))), node0);        // handle retriable errors        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.COORDINATOR_NOT_AVAILABLE.code()).setGroups(Collections.emptyList())), node1);        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.COORDINATOR_LOAD_IN_PROGRESS.code()).setGroups(Collections.emptyList())), node1);        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.NONE.code()).setGroups(Arrays.asList(new ListGroupsResponseData.ListedGroup().setGroupId("group-2").setProtocolType(ConsumerProtocol.PROTOCOL_TYPE), new ListGroupsResponseData.ListedGroup().setGroupId("group-connect-2").setProtocolType("connector")))), node1);        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.NONE.code()).setGroups(Arrays.asList(new ListGroupsResponseData.ListedGroup().setGroupId("group-3").setProtocolType(ConsumerProtocol.PROTOCOL_TYPE), new ListGroupsResponseData.ListedGroup().setGroupId("group-connect-3").setProtocolType("connector")))), node2);        // fatal error        env.kafkaClient().prepareResponseFrom(new ListGroupsResponse(new ListGroupsResponseData().setErrorCode(Errors.UNKNOWN_SERVER_ERROR.code()).setGroups(Collections.emptyList())), node3);        final ListConsumerGroupsResult result = env.adminClient().listConsumerGroups();        TestUtils.assertFutureError(result.all(), UnknownServerException.class);        Collection<ConsumerGroupListing> listings = result.valid().get();        assertEquals(3, listings.size());        Set<String> groupIds = new HashSet<>();        for (ConsumerGroupListing listing : listings) {            groupIds.add(listing.groupId());        }        assertEquals(Utils.mkSet("group-1", "group-2", "group-3"), groupIds);        assertEquals(1, result.errors().get().size());    }}
f5593
0
testDeleteConsumerGroupOffsetsNonRetriableErrors
public void kafkatest_f5602_0() throws Exception
{    // Non-retriable errors throw an exception    final Map<Integer, Node> nodes = new HashMap<>();    nodes.put(0, new Node(0, "localhost", 8121));    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptyList(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));    final String groupId = "group-0";    final TopicPartition tp1 = new TopicPartition("foo", 0);    final List<Errors> retriableErrors = Arrays.asList(Errors.GROUP_AUTHORIZATION_FAILED, Errors.INVALID_GROUP_ID, Errors.GROUP_ID_NOT_FOUND);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        for (Errors error : retriableErrors) {            env.kafkaClient().prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, env.cluster().controller()));            env.kafkaClient().prepareResponse(prepareOffsetDeleteResponse(error));            DeleteConsumerGroupOffsetsResult errorResult = env.adminClient().deleteConsumerGroupOffsets(groupId, Stream.of(tp1).collect(Collectors.toSet()));            TestUtils.assertFutureError(errorResult.all(), error.exception().getClass());            TestUtils.assertFutureError(errorResult.partitionResult(tp1), error.exception().getClass());        }    }}
f5602
0
testDeleteConsumerGroupOffsetsFindCoordinatorRetriableErrors
public void kafkatest_f5603_0() throws Exception
{    // Retriable FindCoordinatorResponse errors should be retried    final Map<Integer, Node> nodes = new HashMap<>();    nodes.put(0, new Node(0, "localhost", 8121));    final Cluster cluster = new Cluster("mockClusterId", nodes.values(), Collections.<PartitionInfo>emptyList(), Collections.<String>emptySet(), Collections.<String>emptySet(), nodes.get(0));    final String groupId = "group-0";    final TopicPartition tp1 = new TopicPartition("foo", 0);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.COORDINATOR_NOT_AVAILABLE, Node.noNode()));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.COORDINATOR_LOAD_IN_PROGRESS, Node.noNode()));        env.kafkaClient().prepareResponse(prepareFindCoordinatorResponse(Errors.NONE, env.cluster().controller()));        env.kafkaClient().prepareResponse(prepareOffsetDeleteResponse("foo", 0, Errors.NONE));        final DeleteConsumerGroupOffsetsResult result = env.adminClient().deleteConsumerGroupOffsets(groupId, Stream.of(tp1).collect(Collectors.toSet()));        assertNull(result.all().get());        assertNull(result.partitionResult(tp1).get());    }}
f5603
0
create
public KafkaAdminClient.TimeoutProcessor kafkatest_f5612_0(long now)
{    return new FailureInjectingTimeoutProcessor(now);}
f5612
0
shouldInjectFailure
 synchronized boolean kafkatest_f5613_0()
{    numTries++;    if (numTries == 1) {        failuresInjected++;        return true;    }    return false;}
f5613
0
markTopicForDeletion
public void kafkatest_f5622_0(final String name)
{    if (!allTopics.containsKey(name)) {        throw new IllegalArgumentException(String.format("Topic %s did not exist.", name));    }    allTopics.get(name).markedForDeletion = true;}
f5622
0
timeoutNextRequest
public void kafkatest_f5623_0(int numberOfRequest)
{    timeoutNextRequests = numberOfRequest;}
f5623
0
renewDelegationToken
public RenewDelegationTokenResult kafkatest_f5632_0(byte[] hmac, RenewDelegationTokenOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5632
0
expireDelegationToken
public ExpireDelegationTokenResult kafkatest_f5633_0(byte[] hmac, ExpireDelegationTokenOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5633
0
removeMemberFromConsumerGroup
public MembershipChangeResult kafkatest_f5642_0(String groupId, RemoveMemberFromConsumerGroupOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5642
0
createAcls
public CreateAclsResult kafkatest_f5643_0(Collection<AclBinding> acls, CreateAclsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5643
0
alterPartitionReassignments
public AlterPartitionReassignmentsResult kafkatest_f5652_0(Map<TopicPartition, Optional<NewPartitionReassignment>> reassignments, AlterPartitionReassignmentsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5652
0
listPartitionReassignments
public ListPartitionReassignmentsResult kafkatest_f5653_0(Optional<Set<TopicPartition>> partitions, ListPartitionReassignmentsOptions options)
{    throw new UnsupportedOperationException("Not implemented yet");}
f5653
0
testMaxUsableProduceMagic
public void kafkatest_f5663_0()
{    ApiVersions apiVersions = new ApiVersions();    assertEquals(RecordBatch.CURRENT_MAGIC_VALUE, apiVersions.maxUsableProduceMagic());    apiVersions.update("0", NodeApiVersions.create());    assertEquals(RecordBatch.CURRENT_MAGIC_VALUE, apiVersions.maxUsableProduceMagic());    apiVersions.update("1", NodeApiVersions.create(Collections.singleton(new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE, (short) 0, (short) 2))));    assertEquals(RecordBatch.MAGIC_VALUE_V1, apiVersions.maxUsableProduceMagic());    apiVersions.remove("1");    assertEquals(RecordBatch.CURRENT_MAGIC_VALUE, apiVersions.maxUsableProduceMagic());}
f5663
0
testParseAndValidateAddresses
public void kafkatest_f5664_0() throws UnknownHostException
{    checkWithoutLookup("127.0.0.1:8000");    checkWithoutLookup("localhost:8080");    checkWithoutLookup("[::1]:8000");    checkWithoutLookup("[2001:db8:85a3:8d3:1319:8a2e:370:7348]:1234", "localhost:10000");    List<InetSocketAddress> validatedAddresses = checkWithoutLookup("localhost:10000");    assertEquals(1, validatedAddresses.size());    InetSocketAddress onlyAddress = validatedAddresses.get(0);    assertEquals("localhost", onlyAddress.getHostName());    assertEquals(10000, onlyAddress.getPort());}
f5664
0
checkWithoutLookup
private List<InetSocketAddress> kafkatest_f5673_0(String... url)
{    return ClientUtils.parseAndValidateAddresses(Arrays.asList(url), ClientDnsLookup.DEFAULT);}
f5673
0
checkWithLookup
private List<InetSocketAddress> kafkatest_f5674_0(List<String> url)
{    return ClientUtils.parseAndValidateAddresses(url, ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY);}
f5674
0
testSingleIPWithDefault
public void kafkatest_f5683_0() throws UnknownHostException
{    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.DEFAULT);    InetAddress currAddress = connectionStates.currentAddress(nodeId1);    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.DEFAULT);    assertSame(currAddress, connectionStates.currentAddress(nodeId1));}
f5683
0
testSingleIPWithUseAll
public void kafkatest_f5684_0() throws UnknownHostException
{    assertEquals(1, ClientUtils.resolve("localhost", ClientDnsLookup.USE_ALL_DNS_IPS).size());    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.USE_ALL_DNS_IPS);    InetAddress currAddress = connectionStates.currentAddress(nodeId1);    connectionStates.connecting(nodeId1, time.milliseconds(), "localhost", ClientDnsLookup.USE_ALL_DNS_IPS);    assertSame(currAddress, connectionStates.currentAddress(nodeId1));}
f5684
0
testDeserializerToMapConfig
public void kafkatest_f5693_0()
{    Map<String, Object> configs = new HashMap<>();    configs.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializerClass);    configs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializerClass);    Map<String, Object> newConfigs = ConsumerConfig.addDeserializerToConfig(configs, null, null);    assertEquals(newConfigs.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG), keyDeserializerClass);    assertEquals(newConfigs.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG), valueDeserializerClass);    configs.clear();    configs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeserializerClass);    newConfigs = ConsumerConfig.addDeserializerToConfig(configs, keyDeserializer, null);    assertEquals(newConfigs.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG), keyDeserializerClass);    assertEquals(newConfigs.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG), valueDeserializerClass);    configs.clear();    configs.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeserializerClass);    newConfigs = ConsumerConfig.addDeserializerToConfig(configs, null, valueDeserializer);    assertEquals(newConfigs.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG), keyDeserializerClass);    assertEquals(newConfigs.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG), valueDeserializerClass);    configs.clear();    newConfigs = ConsumerConfig.addDeserializerToConfig(configs, keyDeserializer, valueDeserializer);    assertEquals(newConfigs.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG), keyDeserializerClass);    assertEquals(newConfigs.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG), valueDeserializerClass);}
f5693
0
iterator
public void kafkatest_f5694_0() throws Exception
{    Map<TopicPartition, List<ConsumerRecord<Integer, String>>> records = new LinkedHashMap<>();    String topic = "topic";    records.put(new TopicPartition(topic, 0), new ArrayList<ConsumerRecord<Integer, String>>());    ConsumerRecord<Integer, String> record1 = new ConsumerRecord<>(topic, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, 1, "value1");    ConsumerRecord<Integer, String> record2 = new ConsumerRecord<>(topic, 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, 2, "value2");    records.put(new TopicPartition(topic, 1), Arrays.asList(record1, record2));    records.put(new TopicPartition(topic, 2), new ArrayList<ConsumerRecord<Integer, String>>());    ConsumerRecords<Integer, String> consumerRecords = new ConsumerRecords<>(records);    Iterator<ConsumerRecord<Integer, String>> iter = consumerRecords.iterator();    int c = 0;    for (; iter.hasNext(); c++) {        ConsumerRecord<Integer, String> record = iter.next();        assertEquals(1, record.partition());        assertEquals(topic, record.topic());        assertEquals(c, record.offset());    }    assertEquals(2, c);}
f5694
0
setupCoordinator
private void kafkatest_f5703_0(int retryBackoffMs, int rebalanceTimeoutMs, Optional<String> groupInstanceId)
{    LogContext logContext = new LogContext();    this.mockTime = new MockTime();    ConsumerMetadata metadata = new ConsumerMetadata(retryBackoffMs, 60 * 60 * 1000L, false, false, new SubscriptionState(logContext, OffsetResetStrategy.EARLIEST), logContext, new ClusterResourceListeners());    this.mockClient = new MockClient(mockTime, metadata);    this.consumerClient = new ConsumerNetworkClient(logContext, mockClient, metadata, mockTime, retryBackoffMs, REQUEST_TIMEOUT_MS, HEARTBEAT_INTERVAL_MS);    Metrics metrics = new Metrics();    mockClient.updateMetadata(TestUtils.metadataUpdateWith(1, emptyMap()));    this.node = metadata.fetch().nodes().get(0);    this.coordinatorNode = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    GroupRebalanceConfig rebalanceConfig = new GroupRebalanceConfig(SESSION_TIMEOUT_MS, rebalanceTimeoutMs, HEARTBEAT_INTERVAL_MS, GROUP_ID, groupInstanceId, retryBackoffMs, !groupInstanceId.isPresent());    this.coordinator = new DummyCoordinator(rebalanceConfig, consumerClient, metrics, mockTime);}
f5703
0
testCoordinatorDiscoveryBackoff
public void kafkatest_f5704_0()
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    // blackout the coordinator for 10 milliseconds to simulate a disconnect.    // after backing off, we should be able to connect.    mockClient.blackout(coordinatorNode, 10L);    long initialTime = mockTime.milliseconds();    coordinator.ensureCoordinatorReady(mockTime.timer(Long.MAX_VALUE));    long endTime = mockTime.milliseconds();    assertTrue(endTime - initialTime >= RETRY_BACKOFF_MS);}
f5704
0
testJoinGroupRequestWithGroupInstanceIdNotFound
public void kafkatest_f5713_0()
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(mockTime.timer(0));    mockClient.prepareResponse(joinGroupFollowerResponse(defaultGeneration, memberId, JoinGroupResponse.UNKNOWN_MEMBER_ID, Errors.UNKNOWN_MEMBER_ID));    RequestFuture<ByteBuffer> future = coordinator.sendJoinGroupRequest();    assertTrue(consumerClient.poll(future, mockTime.timer(REQUEST_TIMEOUT_MS)));    assertEquals(Errors.UNKNOWN_MEMBER_ID.message(), future.exception().getMessage());    assertTrue(coordinator.rejoinNeededOrPending());    assertTrue(coordinator.hasMatchingGenerationId(defaultGeneration));}
f5713
0
testLeaveGroupSentWithGroupInstanceIdUnSet
public void kafkatest_f5714_0()
{    checkLeaveGroupRequestSent(Optional.empty());    checkLeaveGroupRequestSent(Optional.of("groupInstanceId"));}
f5714
0
testPollHeartbeatAwakesHeartbeatThread
public void kafkatest_f5723_0() throws Exception
{    final int longRetryBackoffMs = 10000;    setupCoordinator(longRetryBackoffMs);    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(syncGroupResponse(Errors.NONE));    coordinator.ensureActiveGroup();    final CountDownLatch heartbeatDone = new CountDownLatch(1);    mockClient.prepareResponse(body -> {        heartbeatDone.countDown();        return body instanceof HeartbeatRequest;    }, heartbeatResponse(Errors.NONE));    mockTime.sleep(HEARTBEAT_INTERVAL_MS);    coordinator.pollHeartbeat(mockTime.milliseconds());    if (!heartbeatDone.await(1, TimeUnit.SECONDS)) {        fail("Should have received a heartbeat request after calling pollHeartbeat");    }}
f5723
0
testLookupCoordinator
public void kafkatest_f5724_0()
{    setupCoordinator();    mockClient.blackout(node, 50);    RequestFuture<Void> noBrokersAvailableFuture = coordinator.lookupCoordinator();    assertTrue("Failed future expected", noBrokersAvailableFuture.failed());    mockTime.sleep(50);    RequestFuture<Void> future = coordinator.lookupCoordinator();    assertFalse("Request not sent", future.isDone());    assertSame("New request sent while one is in progress", future, coordinator.lookupCoordinator());    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(mockTime.timer(Long.MAX_VALUE));    assertNotSame("New request not sent after previous completed", future, coordinator.lookupCoordinator());}
f5724
0
testWakeupAfterSyncGroupSentExternalCompletion
public void kafkatest_f5733_0() throws Exception
{    setupCoordinator();    mockClient.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    mockClient.prepareResponse(joinGroupFollowerResponse(1, memberId, leaderId, Errors.NONE));    mockClient.prepareResponse(new MockClient.RequestMatcher() {        private int invocations = 0;        @Override        public boolean matches(AbstractRequest body) {            invocations++;            boolean isSyncGroupRequest = body instanceof SyncGroupRequest;            if (isSyncGroupRequest && invocations == 1)                // simulate wakeup after the request sent                throw new WakeupException();            return isSyncGroupRequest;        }    }, syncGroupResponse(Errors.NONE));    AtomicBoolean heartbeatReceived = prepareFirstHeartbeat();    try {        coordinator.ensureActiveGroup();        fail("Should have woken up from ensureActiveGroup()");    } catch (WakeupException e) {    }    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(0, coordinator.onJoinCompleteInvokes);    assertFalse(heartbeatReceived.get());    // the join group completes in this poll()    consumerClient.poll(mockTime.timer(0));    coordinator.ensureActiveGroup();    assertEquals(1, coordinator.onJoinPrepareInvokes);    assertEquals(1, coordinator.onJoinCompleteInvokes);    awaitFirstHeartbeat(heartbeatReceived);}
f5733
0
matches
public boolean kafkatest_f5734_0(AbstractRequest body)
{    invocations++;    boolean isSyncGroupRequest = body instanceof SyncGroupRequest;    if (isSyncGroupRequest && invocations == 1)        // simulate wakeup after the request sent        throw new WakeupException();    return isSyncGroupRequest;}
f5734
0
heartbeatResponse
private HeartbeatResponse kafkatest_f5743_0(Errors error)
{    return new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(error.code()));}
f5743
0
joinGroupFollowerResponse
private JoinGroupResponse kafkatest_f5744_0(int generationId, String memberId, String leaderId, Errors error)
{    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(error.code()).setGenerationId(generationId).setProtocolName("dummy-subprotocol").setMemberId(memberId).setLeader(leaderId).setMembers(Collections.emptyList()));}
f5744
0
testMemberInfoSortingWithoutGroupInstanceId
public void kafkatest_f5753_0()
{    MemberInfo m1 = new MemberInfo("a", Optional.empty());    MemberInfo m2 = new MemberInfo("b", Optional.empty());    MemberInfo m3 = new MemberInfo("c", Optional.empty());    List<MemberInfo> memberInfoList = Arrays.asList(m1, m2, m3);    assertEquals(memberInfoList, Utils.sorted(memberInfoList));}
f5753
0
testMemberInfoSortingWithAllGroupInstanceId
public void kafkatest_f5754_0()
{    MemberInfo m1 = new MemberInfo("a", Optional.of("y"));    MemberInfo m2 = new MemberInfo("b", Optional.of("z"));    MemberInfo m3 = new MemberInfo("c", Optional.of("x"));    List<MemberInfo> memberInfoList = Arrays.asList(m1, m2, m3);    assertEquals(Arrays.asList(m3, m1, m2), Utils.sorted(memberInfoList));}
f5754
0
testTwoConsumersOneTopicOnePartition
public void kafkatest_f5763_0()
{    String consumer1 = "consumer1";    String consumer2 = "consumer2";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 1);    subscriptions.put(consumer1, new Subscription(topics(topic)));    subscriptions.put(consumer2, new Subscription(topics(topic)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(partitions(tp(topic, 0)), assignment.get(consumer1));    assertEquals(Collections.<TopicPartition>emptyList(), assignment.get(consumer2));    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(isFullyBalanced(assignment));}
f5763
0
testTwoConsumersOneTopicTwoPartitions
public void kafkatest_f5764_0()
{    String consumer1 = "consumer1";    String consumer2 = "consumer2";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 2);    subscriptions.put(consumer1, new Subscription(topics(topic)));    subscriptions.put(consumer2, new Subscription(topics(topic)));    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    assertEquals(partitions(tp(topic, 0)), assignment.get(consumer1));    assertEquals(partitions(tp(topic, 1)), assignment.get(consumer2));    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(isFullyBalanced(assignment));}
f5764
0
testLargeAssignmentWithMultipleConsumersLeaving
public void kafkatest_f5773_0()
{    Random rand = new Random();    int topicCount = 40;    int consumerCount = 200;    Map<String, Integer> partitionsPerTopic = new HashMap<>();    for (int i = 0; i < topicCount; i++) partitionsPerTopic.put(getTopicName(i, topicCount), rand.nextInt(10) + 1);    for (int i = 0; i < consumerCount; i++) {        List<String> topics = new ArrayList<>();        for (int j = 0; j < rand.nextInt(20); j++) topics.add(getTopicName(rand.nextInt(topicCount), topicCount));        subscriptions.put(getConsumerName(i, consumerCount), new Subscription(topics));    }    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    for (int i = 1; i < consumerCount; i++) {        String consumer = getConsumerName(i, consumerCount);        subscriptions.put(consumer, buildSubscription(subscriptions.get(consumer).topics(), assignment.get(consumer)));    }    for (int i = 0; i < 50; ++i) {        String c = getConsumerName(rand.nextInt(consumerCount), consumerCount);        subscriptions.remove(c);    }    assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(assignor.isSticky());}
f5773
0
testNewSubscription
public void kafkatest_f5774_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    for (int i = 1; i < 5; i++) partitionsPerTopic.put(getTopicName(i, 5), 1);    for (int i = 0; i < 3; i++) {        List<String> topics = new ArrayList<>();        for (int j = i; j <= 3 * i - 2; j++) topics.add(getTopicName(j, 5));        subscriptions.put(getConsumerName(i, 3), new Subscription(topics));    }    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    subscriptions.get(getConsumerName(0, 3)).topics().add(getTopicName(1, 5));    assignment = assignor.assign(partitionsPerTopic, subscriptions);    verifyValidityAndBalance(subscriptions, assignment, partitionsPerTopic);    assertTrue(assignor.isSticky());}
f5774
0
getCanonicalName
private String kafkatest_f5783_0(String str, int i, int maxNum)
{    return str + pad(i, Integer.toString(maxNum).length());}
f5783
0
pad
private String kafkatest_f5784_0(int num, int digits)
{    StringBuilder sb = new StringBuilder();    int iDigits = Integer.toString(num).length();    for (int i = 1; i <= digits - iDigits; ++i) sb.append("0");    sb.append(num);    return sb.toString();}
f5784
0
buildRebalanceConfig
private GroupRebalanceConfig kafkatest_f5793_0(Optional<String> groupInstanceId)
{    return new GroupRebalanceConfig(sessionTimeoutMs, rebalanceTimeoutMs, heartbeatIntervalMs, groupId, groupInstanceId, retryBackoffMs, !groupInstanceId.isPresent());}
f5793
0
teardown
public void kafkatest_f5794_0()
{    this.metrics.close();    this.coordinator.close(time.timer(0));}
f5794
0
onFailure
public void kafkatest_f5804_0(RuntimeException e, RequestFuture<Object> future)
{    assertTrue("Unexpected exception type: " + e.getClass(), e instanceof DisconnectException);    assertTrue(coordinator.coordinatorUnknown());    asyncCallbackInvoked.set(true);}
f5804
0
testNotCoordinator
public void kafkatest_f5805_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // not_coordinator will mark coordinator as unknown    time.sleep(sessionTimeoutMs);    // should send out the heartbeat    RequestFuture<Void> future = coordinator.sendHeartbeatRequest();    assertEquals(1, consumerClient.pendingRequestCount());    assertFalse(future.isDone());    client.prepareResponse(heartbeatResponse(Errors.NOT_COORDINATOR));    time.sleep(sessionTimeoutMs);    consumerClient.poll(time.timer(0));    assertTrue(future.isDone());    assertTrue(future.failed());    assertEquals(Errors.NOT_COORDINATOR.exception(), future.exception());    assertTrue(coordinator.coordinatorUnknown());}
f5805
0
matches
public boolean kafkatest_f5814_0(AbstractRequest body)
{    client.updateMetadata(TestUtils.metadataUpdateWith(1, singletonMap(topic1, 1)));    return true;}
f5814
0
matches
public boolean kafkatest_f5815_0(AbstractRequest body)
{    JoinGroupRequest join = (JoinGroupRequest) body;    Iterator<JoinGroupRequestData.JoinGroupRequestProtocol> protocolIterator = join.data().protocols().iterator();    assertTrue(protocolIterator.hasNext());    JoinGroupRequestData.JoinGroupRequestProtocol protocolMetadata = protocolIterator.next();    ByteBuffer metadata = ByteBuffer.wrap(protocolMetadata.metadata());    ConsumerPartitionAssignor.Subscription subscription = ConsumerProtocol.deserializeSubscription(metadata);    metadata.rewind();    return subscription.topics().contains(topic1);}
f5815
0
testPendingMemberShouldLeaveGroup
public void kafkatest_f5824_0()
{    final String consumerId = "consumer-id";    subscriptions.subscribe(singleton(topic1), rebalanceListener);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // here we return a DEFAULT_GENERATION_ID, but valid member id and leader id.    client.prepareResponse(joinGroupFollowerResponse(-1, consumerId, "leader-id", Errors.MEMBER_ID_REQUIRED));    // execute join group    coordinator.joinGroupIfNeeded(time.timer(0));    final AtomicBoolean received = new AtomicBoolean(false);    client.prepareResponse(body -> {        received.set(true);        LeaveGroupRequest leaveRequest = (LeaveGroupRequest) body;        return validateLeaveGroup(groupId, consumerId, leaveRequest);    }, new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.NONE.code())));    coordinator.maybeLeaveGroup("pending member leaves");    assertTrue(received.get());}
f5824
0
testUnexpectedErrorOnSyncGroup
public void kafkatest_f5825_0()
{    final String consumerId = "consumer";    subscriptions.subscribe(singleton(topic1), rebalanceListener);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // join initially, but let coordinator rebalance on sync    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(Collections.emptyList(), Errors.UNKNOWN_SERVER_ERROR));    coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));}
f5825
0
testRebalanceAfterTopicUnavailableWithPatternSubscribe
public void kafkatest_f5834_0()
{    unavailableTopicTest(true, Collections.emptySet());}
f5834
0
testRebalanceAfterNotMatchingTopicUnavailableWithPatternSubscribe
public void kafkatest_f5835_0()
{    unavailableTopicTest(true, Collections.singleton("notmatching"));}
f5835
0
testCoordinatorDisconnectAfterNotCoordinatorError
public void kafkatest_f5844_0()
{    testInFlightRequestsFailedAfterCoordinatorMarkedDead(Errors.NOT_COORDINATOR);}
f5844
0
testCoordinatorDisconnectAfterCoordinatorNotAvailableError
public void kafkatest_f5845_0()
{    testInFlightRequestsFailedAfterCoordinatorMarkedDead(Errors.COORDINATOR_NOT_AVAILABLE);}
f5845
0
testCommitOffsetAsyncWithDefaultCallback
public void kafkatest_f5854_0()
{    int invokedBeforeTest = mockOffsetCommitCallback.invoked;    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.NONE);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), mockOffsetCommitCallback);    coordinator.invokeCompletedOffsetCommitCallbacks();    assertEquals(invokedBeforeTest + 1, mockOffsetCommitCallback.invoked);    assertNull(mockOffsetCommitCallback.exception);}
f5854
0
testCommitAfterLeaveGroup
public void kafkatest_f5855_0()
{    // enable auto-assignment    subscriptions.subscribe(singleton(topic1), rebalanceListener);    joinAsFollowerAndReceiveAssignment("consumer", coordinator, singletonList(t1p));    // now switch to manual assignment    client.prepareResponse(new LeaveGroupResponse(new LeaveGroupResponseData().setErrorCode(Errors.NONE.code())));    subscriptions.unsubscribe();    coordinator.maybeLeaveGroup("test commit after leave");    subscriptions.assignFromUser(singleton(t1p));    // the client should not reuse generation/memberId from auto-subscribed generation    client.prepareResponse(body -> {        OffsetCommitRequest commitRequest = (OffsetCommitRequest) body;        return commitRequest.data().memberId().equals(OffsetCommitRequest.DEFAULT_MEMBER_ID) && commitRequest.data().generationId() == OffsetCommitRequest.DEFAULT_GENERATION_ID;    }, offsetCommitResponse(singletonMap(t1p, Errors.NONE)));    AtomicBoolean success = new AtomicBoolean(false);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), callback(success));    coordinator.invokeCompletedOffsetCommitCallbacks();    assertTrue(success.get());}
f5855
0
onComplete
public void kafkatest_f5864_0(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception)
{    committedOffsets.add(firstOffset);}
f5864
0
run
public void kafkatest_f5865_0()
{    coordinator.commitOffsetsSync(singletonMap(t1p, secondOffset), time.timer(10000));    committedOffsets.add(secondOffset);}
f5865
0
testRefreshOffsetWithValidation
public void kafkatest_f5874_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    subscriptions.assignFromUser(singleton(t1p));    // Initial leader epoch of 4    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("kafka-cluster", 1, Collections.emptyMap(), singletonMap(topic1, 1), tp -> 4);    client.updateMetadata(metadataResponse);    // Load offsets from previous epoch    client.prepareResponse(offsetFetchResponse(t1p, Errors.NONE, "", 100L, 3));    coordinator.refreshCommittedOffsetsIfNeeded(time.timer(Long.MAX_VALUE));    // Offset gets loaded, but requires validation    assertEquals(Collections.emptySet(), subscriptions.missingFetchPositions());    assertFalse(subscriptions.hasAllFetchPositions());    assertTrue(subscriptions.awaitingValidation(t1p));    assertEquals(subscriptions.position(t1p).offset, 100L);    assertNull(subscriptions.validPosition(t1p));}
f5874
0
testFetchCommittedOffsets
public void kafkatest_f5875_0()
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    long offset = 500L;    String metadata = "blahblah";    Optional<Integer> leaderEpoch = Optional.of(15);    OffsetFetchResponse.PartitionData data = new OffsetFetchResponse.PartitionData(offset, leaderEpoch, metadata, Errors.NONE);    client.prepareResponse(new OffsetFetchResponse(Errors.NONE, singletonMap(t1p, data)));    Map<TopicPartition, OffsetAndMetadata> fetchedOffsets = coordinator.fetchCommittedOffsets(singleton(t1p), time.timer(Long.MAX_VALUE));    assertNotNull(fetchedOffsets);    assertEquals(new OffsetAndMetadata(offset, leaderEpoch, metadata), fetchedOffsets.get(t1p));}
f5875
0
testAuthenticationFailureInEnsureActiveGroup
public void kafkatest_f5884_0()
{    client.createPendingAuthenticationError(node, 300);    try {        coordinator.ensureActiveGroup();        fail("Expected an authentication error.");    } catch (AuthenticationException e) {    // OK    }}
f5884
0
testThreadSafeAssignedPartitionsMetric
public void kafkatest_f5885_0() throws Exception
{    // Get the assigned-partitions metric    final Metric metric = metrics.metric(new MetricName("assigned-partitions", "consumer" + groupId + "-coordinator-metrics", "", Collections.emptyMap()));    // Start polling the metric in the background    final AtomicBoolean doStop = new AtomicBoolean();    final AtomicReference<Exception> exceptionHolder = new AtomicReference<>();    final AtomicInteger observedSize = new AtomicInteger();    Thread poller = new Thread() {        @Override        public void run() {            // Poll as fast as possible to reproduce ConcurrentModificationException            while (!doStop.get()) {                try {                    int size = ((Double) metric.metricValue()).intValue();                    observedSize.set(size);                } catch (Exception e) {                    exceptionHolder.set(e);                    return;                }            }        }    };    poller.start();    // Assign two partitions to trigger a metric change that can lead to ConcurrentModificationException    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // Change the assignment several times to increase likelihood of concurrent updates    Set<TopicPartition> partitions = new HashSet<>();    int totalPartitions = 10;    for (int partition = 0; partition < totalPartitions; partition++) {        partitions.add(new TopicPartition(topic1, partition));        subscriptions.assignFromUser(partitions);    }    // Wait for the metric poller to observe the final assignment change or raise an error    TestUtils.waitForCondition(new TestCondition() {        @Override        public boolean conditionMet() {            return observedSize.get() == totalPartitions || exceptionHolder.get() != null;        }    }, "Failed to observe expected assignment change");    doStop.set(true);    poller.join();    assertNull("Failed fetching the metric at least once", exceptionHolder.get());}
f5885
0
testCloseTimeoutCoordinatorUnavailableForCommit
public void kafkatest_f5894_0() throws Exception
{    try (ConsumerCoordinator coordinator = prepareCoordinatorForCloseTest(true, true, groupInstanceId)) {        makeCoordinatorUnknown(coordinator, Errors.COORDINATOR_NOT_AVAILABLE);        time.sleep(autoCommitIntervalMs);        closeVerifyTimeout(coordinator, 1000, 1000, 1000);    }}
f5894
0
testCloseMaxWaitCoordinatorUnavailableForCommit
public void kafkatest_f5895_0() throws Exception
{    try (ConsumerCoordinator coordinator = prepareCoordinatorForCloseTest(true, true, groupInstanceId)) {        makeCoordinatorUnknown(coordinator, Errors.COORDINATOR_NOT_AVAILABLE);        time.sleep(autoCommitIntervalMs);        closeVerifyTimeout(coordinator, Long.MAX_VALUE, requestTimeoutMs, requestTimeoutMs);    }}
f5895
0
receiveFencedInstanceIdException
private void kafkatest_f5904_0()
{    subscriptions.assignFromUser(singleton(t1p));    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    prepareOffsetCommitRequest(singletonMap(t1p, 100L), Errors.FENCED_INSTANCE_ID);    coordinator.commitOffsetsAsync(singletonMap(t1p, new OffsetAndMetadata(100L)), new MockCommitCallback());    coordinator.invokeCompletedOffsetCommitCallbacks();}
f5904
0
prepareCoordinatorForCloseTest
private ConsumerCoordinator kafkatest_f5905_0(final boolean useGroupManagement, final boolean autoCommit, final Optional<String> groupInstanceId)
{    final String consumerId = "consumer";    rebalanceConfig = buildRebalanceConfig(groupInstanceId);    ConsumerCoordinator coordinator = buildCoordinator(rebalanceConfig, new Metrics(), assignors, autoCommit);    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    if (useGroupManagement) {        subscriptions.subscribe(singleton(topic1), rebalanceListener);        client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));        client.prepareResponse(syncGroupResponse(singletonList(t1p), Errors.NONE));        coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));    } else {        subscriptions.assignFromUser(singleton(t1p));    }    subscriptions.seek(t1p, 100);    coordinator.poll(time.timer(Long.MAX_VALUE));    return coordinator;}
f5905
0
heartbeatResponse
private HeartbeatResponse kafkatest_f5914_0(Errors error)
{    return new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(error.code()));}
f5914
0
joinGroupLeaderResponse
private JoinGroupResponse kafkatest_f5915_0(int generationId, String memberId, Map<String, List<String>> subscriptions, Errors error)
{    List<JoinGroupResponseData.JoinGroupResponseMember> metadata = new ArrayList<>();    for (Map.Entry<String, List<String>> subscriptionEntry : subscriptions.entrySet()) {        ConsumerPartitionAssignor.Subscription subscription = new ConsumerPartitionAssignor.Subscription(subscriptionEntry.getValue());        ByteBuffer buf = ConsumerProtocol.serializeSubscription(subscription);        metadata.add(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId(subscriptionEntry.getKey()).setMetadata(buf.array()));    }    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(error.code()).setGenerationId(generationId).setProtocolName(partitionAssignor.name()).setLeader(memberId).setMemberId(memberId).setMembers(metadata));}
f5915
0
joinAsFollowerAndReceiveAssignment
private void kafkatest_f5924_0(String consumerId, ConsumerCoordinator coordinator, List<TopicPartition> assignment)
{    client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    client.prepareResponse(joinGroupFollowerResponse(1, consumerId, "leader", Errors.NONE));    client.prepareResponse(syncGroupResponse(assignment, Errors.NONE));    coordinator.joinGroupIfNeeded(time.timer(Long.MAX_VALUE));}
f5924
0
prepareOffsetCommitRequest
private void kafkatest_f5925_0(Map<TopicPartition, Long> expectedOffsets, Errors error)
{    prepareOffsetCommitRequest(expectedOffsets, error, false);}
f5925
0
onPartitionsRevoked
public void kafkatest_f5934_0(Collection<TopicPartition> partitions)
{    this.revoked = partitions;    revokedCount++;}
f5934
0
onPartitionsLost
public void kafkatest_f5935_0(Collection<TopicPartition> partitions)
{    this.lost = partitions;    lostCount++;}
f5935
0
testPatternSubscription
private void kafkatest_f5946_0(boolean includeInternalTopics)
{    subscription.subscribe(Pattern.compile("__.*"), new NoOpConsumerRebalanceListener());    ConsumerMetadata metadata = newConsumerMetadata(includeInternalTopics);    MetadataRequest.Builder builder = metadata.newMetadataRequestBuilder();    assertTrue(builder.isAllTopics());    List<MetadataResponse.TopicMetadata> topics = new ArrayList<>();    topics.add(topicMetadata("__consumer_offsets", true));    topics.add(topicMetadata("__matching_topic", false));    topics.add(topicMetadata("non_matching_topic", false));    MetadataResponse response = MetadataResponse.prepareResponse(singletonList(node), "clusterId", node.id(), topics);    metadata.update(response, time.milliseconds());    if (includeInternalTopics)        assertEquals(Utils.mkSet("__matching_topic", "__consumer_offsets"), metadata.fetch().topics());    else        assertEquals(Collections.singleton("__matching_topic"), metadata.fetch().topics());}
f5946
0
testUserAssignment
public void kafkatest_f5947_0()
{    subscription.assignFromUser(Utils.mkSet(new TopicPartition("foo", 0), new TopicPartition("bar", 0), new TopicPartition("__consumer_offsets", 0)));    testBasicSubscription(Utils.mkSet("foo", "bar"), Utils.mkSet("__consumer_offsets"));    subscription.assignFromUser(Utils.mkSet(new TopicPartition("baz", 0), new TopicPartition("__consumer_offsets", 0)));    testBasicSubscription(Utils.mkSet("baz"), Utils.mkSet("__consumer_offsets"));}
f5947
0
testDisconnectWithUnsentRequests
public void kafkatest_f5956_0()
{    RequestFuture<ClientResponse> future = consumerClient.send(node, heartbeat());    assertTrue(consumerClient.hasPendingRequests(node));    assertFalse(client.hasInFlightRequests(node.idString()));    consumerClient.disconnectAsync(node);    consumerClient.pollNoWakeup();    assertTrue(future.failed());    assertTrue(future.exception() instanceof DisconnectException);}
f5956
0
testDisconnectWithInFlightRequests
public void kafkatest_f5957_0()
{    RequestFuture<ClientResponse> future = consumerClient.send(node, heartbeat());    consumerClient.pollNoWakeup();    assertTrue(consumerClient.hasPendingRequests(node));    assertTrue(client.hasInFlightRequests(node.idString()));    consumerClient.disconnectAsync(node);    consumerClient.pollNoWakeup();    assertTrue(future.failed());    assertTrue(future.exception() instanceof DisconnectException);}
f5957
0
testInvalidTopicExceptionPropagatedFromMetadata
public void kafkatest_f5966_0()
{    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("clusterId", 1, Collections.singletonMap("topic", Errors.INVALID_TOPIC_EXCEPTION), Collections.emptyMap());    metadata.update(metadataResponse, time.milliseconds());    consumerClient.poll(time.timer(Duration.ZERO));}
f5966
0
testTopicAuthorizationExceptionPropagatedFromMetadata
public void kafkatest_f5967_0()
{    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("clusterId", 1, Collections.singletonMap("topic", Errors.TOPIC_AUTHORIZATION_FAILED), Collections.emptyMap());    metadata.update(metadataResponse, time.milliseconds());    consumerClient.poll(time.timer(Duration.ZERO));}
f5967
0
testTrySend
public void kafkatest_f5976_0()
{    final AtomicBoolean isReady = new AtomicBoolean();    final AtomicInteger checkCount = new AtomicInteger();    client = new MockClient(time, metadata) {        @Override        public boolean ready(Node node, long now) {            checkCount.incrementAndGet();            if (isReady.get())                return super.ready(node, now);            else                return false;        }    };    consumerClient = new ConsumerNetworkClient(new LogContext(), client, metadata, time, 100, 10, Integer.MAX_VALUE);    consumerClient.send(node, heartbeat());    consumerClient.send(node, heartbeat());    assertEquals(2, consumerClient.pendingRequestCount(node));    assertEquals(0, client.inFlightRequestCount(node.idString()));    consumerClient.trySend(time.milliseconds());    // only check one time when the node doesn't ready    assertEquals(1, checkCount.getAndSet(0));    assertEquals(2, consumerClient.pendingRequestCount(node));    assertEquals(0, client.inFlightRequestCount(node.idString()));    isReady.set(true);    consumerClient.trySend(time.milliseconds());    // check node ready or not for every request    assertEquals(2, checkCount.getAndSet(0));    assertEquals(2, consumerClient.pendingRequestCount(node));    assertEquals(2, client.inFlightRequestCount(node.idString()));}
f5976
0
ready
public boolean kafkatest_f5977_0(Node node, long now)
{    checkCount.incrementAndGet();    if (isReady.get())        return super.ready(node, now);    else        return false;}
f5977
0
serializeDeserializeAssignment
public void kafkatest_f5986_0()
{    List<TopicPartition> partitions = Arrays.asList(tp1, tp2);    ByteBuffer buffer = ConsumerProtocol.serializeAssignment(new Assignment(partitions, ByteBuffer.wrap(new byte[0])));    Assignment parsedAssignment = ConsumerProtocol.deserializeAssignment(buffer);    assertEquals(toSet(partitions), toSet(parsedAssignment.partitions()));    assertEquals(0, parsedAssignment.userData().limit());}
f5986
0
deserializeNullAssignmentUserData
public void kafkatest_f5987_0()
{    List<TopicPartition> partitions = Arrays.asList(tp1, tp2);    ByteBuffer buffer = ConsumerProtocol.serializeAssignment(new Assignment(partitions, null));    Assignment parsedAssignment = ConsumerProtocol.deserializeAssignment(buffer);    assertEquals(toSet(partitions), toSet(parsedAssignment.partitions()));    assertNull(parsedAssignment.userData());}
f5987
0
testFetchSkipsBlackedOutNodes
public void kafkatest_f5996_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    client.updateMetadata(initialUpdateResponse);    Node node = initialUpdateResponse.brokers().iterator().next();    client.blackout(node, 500);    assertEquals(0, fetcher.sendFetches());    time.sleep(500);    assertEquals(1, fetcher.sendFetches());}
f5996
0
testFetcherIgnoresControlRecords
public void kafkatest_f5997_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    long producerId = 1;    short producerEpoch = 0;    int baseSequence = 0;    int partitionLeaderEpoch = 0;    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.idempotentBuilder(buffer, CompressionType.NONE, 0L, producerId, producerEpoch, baseSequence);    builder.append(0L, "key".getBytes(), null);    builder.close();    MemoryRecords.writeEndTransactionalMarker(buffer, 1L, time.milliseconds(), partitionLeaderEpoch, producerId, producerEpoch, new EndTransactionMarker(ControlRecordType.ABORT, 0));    buffer.flip();    client.prepareResponse(fullFetchResponse(tp0, MemoryRecords.readableRecords(buffer), Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> partitionRecords = fetchedRecords();    assertTrue(partitionRecords.containsKey(tp0));    List<ConsumerRecord<byte[], byte[]>> records = partitionRecords.get(tp0);    assertEquals(1, records.size());    assertEquals(2L, subscriptions.position(tp0).offset);    ConsumerRecord<byte[], byte[]> record = records.get(0);    assertArrayEquals("key".getBytes(), record.key());}
f5997
0
testInvalidDefaultRecordBatch
public void kafkatest_f6006_0()
{    buildFetcher();    ByteBuffer buffer = ByteBuffer.allocate(1024);    ByteBufferOutputStream out = new ByteBufferOutputStream(buffer);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(out, DefaultRecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.CREATE_TIME, 0L, 10L, 0L, (short) 0, 0, false, false, 0, 1024);    builder.append(10L, "key".getBytes(), "value".getBytes());    builder.close();    buffer.flip();    // Garble the CRC    buffer.position(17);    buffer.put("beef".getBytes());    buffer.position(0);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, MemoryRecords.readableRecords(buffer), Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    // the fetchedRecords() should always throw exception due to the bad batch.    for (int i = 0; i < 2; i++) {        try {            fetcher.fetchedRecords();            fail("fetchedRecords should have raised KafkaException");        } catch (KafkaException e) {            assertEquals(0, subscriptions.position(tp0).offset);        }    }}
f6006
0
testParseInvalidRecordBatch
public void kafkatest_f6007_0()
{    buildFetcher();    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V2, 0L, CompressionType.NONE, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    ByteBuffer buffer = records.buffer();    // flip some bits to fail the crc    buffer.putInt(32, buffer.get(32) ^ 87238423);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, MemoryRecords.readableRecords(buffer), Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    try {        fetcher.fetchedRecords();        fail("fetchedRecords should have raised");    } catch (KafkaException e) {        // the position should not advance since no data has been returned        assertEquals(0, subscriptions.position(tp0).offset);    }}
f6007
0
testFetchDuringRebalance
public void kafkatest_f6016_0()
{    buildFetcher();    subscriptions.subscribe(singleton(topicName), listener);    subscriptions.assignFromSubscribed(singleton(tp0));    subscriptions.seek(tp0, 0);    client.updateMetadata(initialUpdateResponse);    assertEquals(1, fetcher.sendFetches());    // Now the rebalance happens and fetch positions are cleared    subscriptions.assignFromSubscribed(singleton(tp0));    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    // The active fetch should be ignored since its position is no longer valid    assertTrue(fetcher.fetchedRecords().isEmpty());}
f6016
0
testInFlightFetchOnPausedPartition
public void kafkatest_f6017_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    subscriptions.pause(tp0);    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    assertNull(fetcher.fetchedRecords().get(tp0));}
f6017
0
testFetchFencedLeaderEpoch
public void kafkatest_f6026_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.FENCED_LEADER_EPOCH, 100L, 0));    consumerClient.poll(time.timer(0));    assertEquals("Should not return any records", 0, fetcher.fetchedRecords().size());    assertEquals("Should have requested metadata update", 0L, metadata.timeToNextUpdate(time.milliseconds()));}
f6026
0
testFetchUnknownLeaderEpoch
public void kafkatest_f6027_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.UNKNOWN_LEADER_EPOCH, 100L, 0));    consumerClient.poll(time.timer(0));    assertEquals("Should not return any records", 0, fetcher.fetchedRecords().size());    assertNotEquals("Should not have requested metadata update", 0L, metadata.timeToNextUpdate(time.milliseconds()));}
f6027
0
testSeekBeforeException
public void kafkatest_f6036_0()
{    buildFetcher(OffsetResetStrategy.NONE, new ByteArrayDeserializer(), new ByteArrayDeserializer(), 2, IsolationLevel.READ_UNCOMMITTED);    assignFromUser(Utils.mkSet(tp0));    subscriptions.seek(tp0, 1);    assertEquals(1, fetcher.sendFetches());    Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new HashMap<>();    partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100, FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), null, records));    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0));    consumerClient.poll(time.timer(0));    assertEquals(2, fetcher.fetchedRecords().get(tp0).size());    subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));    subscriptions.seek(tp1, 1);    assertEquals(1, fetcher.sendFetches());    partitions = new HashMap<>();    partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.OFFSET_OUT_OF_RANGE, 100, FetchResponse.INVALID_LAST_STABLE_OFFSET, FetchResponse.INVALID_LOG_START_OFFSET, Optional.empty(), null, MemoryRecords.EMPTY));    client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions), 0, INVALID_SESSION_ID));    consumerClient.poll(time.timer(0));    assertEquals(1, fetcher.fetchedRecords().get(tp0).size());    subscriptions.seek(tp1, 10);    // Should not throw OffsetOutOfRangeException after the seek    assertEquals(0, fetcher.fetchedRecords().size());}
f6036
0
testFetchDisconnected
public void kafkatest_f6037_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp0, this.records, Errors.NONE, 100L, 0), true);    consumerClient.poll(time.timer(0));    assertEquals(0, fetcher.fetchedRecords().size());    // disconnects should have no affect on subscription state    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(0, subscriptions.position(tp0).offset);}
f6037
0
testUpdateFetchPositionResetToEarliestOffset
public void kafkatest_f6046_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.EARLIEST);    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.EARLIEST_TIMESTAMP), listOffsetResponse(Errors.NONE, 1L, 5L));    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(5, subscriptions.position(tp0).offset);}
f6046
0
testResetOffsetsMetadataRefresh
public void kafkatest_f6047_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0, OffsetResetStrategy.LATEST);    // First fetch fails with stale metadata    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP), listOffsetResponse(Errors.NOT_LEADER_FOR_PARTITION, 1L, 5L), false);    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.hasValidPosition(tp0));    // Expect a metadata refresh    client.prepareMetadataUpdate(initialUpdateResponse);    consumerClient.pollNoWakeup();    assertFalse(client.hasPendingMetadataUpdates());    // Next fetch succeeds    time.sleep(retryBackoffMs);    client.prepareResponse(listOffsetRequestMatcher(ListOffsetRequest.LATEST_TIMESTAMP), listOffsetResponse(Errors.NONE, 1L, 5L));    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    assertTrue(subscriptions.isFetchable(tp0));    assertEquals(5, subscriptions.position(tp0).offset);}
f6047
0
testUpdateFetchPositionOfPausedPartitionsWithoutAValidPosition
public void kafkatest_f6056_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.requestOffsetReset(tp0);    // paused partition does not have a valid position    subscriptions.pause(tp0);    fetcher.resetOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertTrue(subscriptions.isOffsetResetNeeded(tp0));    // because tp is paused    assertFalse(subscriptions.isFetchable(tp0));    assertFalse(subscriptions.hasValidPosition(tp0));}
f6056
0
testUpdateFetchPositionOfPausedPartitionsWithAValidPosition
public void kafkatest_f6057_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 10);    // paused partition already has a valid position    subscriptions.pause(tp0);    fetcher.resetOffsetsIfNeeded();    assertFalse(subscriptions.isOffsetResetNeeded(tp0));    // because tp is paused    assertFalse(subscriptions.isFetchable(tp0));    assertTrue(subscriptions.hasValidPosition(tp0));    assertEquals(10, subscriptions.position(tp0).offset);}
f6057
0
testQuotaMetrics
public void kafkatest_f6066_0()
{    buildFetcher();    MockSelector selector = new MockSelector(time);    Sensor throttleTimeSensor = Fetcher.throttleTimeSensor(metrics, metricsRegistry);    Cluster cluster = TestUtils.singletonCluster("test", 1);    Node node = cluster.nodes().get(0);    NetworkClient client = new NetworkClient(selector, metadata, "mock", Integer.MAX_VALUE, 1000, 1000, 64 * 1024, 64 * 1024, 1000, ClientDnsLookup.DEFAULT, time, true, new ApiVersions(), throttleTimeSensor, new LogContext());    short apiVersionsResponseVersion = ApiKeys.API_VERSIONS.latestVersion();    ByteBuffer buffer = ApiVersionsResponse.createApiVersionsResponse(400, RecordBatch.CURRENT_MAGIC_VALUE).serialize(apiVersionsResponseVersion, new ResponseHeader(0));    selector.delayedReceive(new DelayedReceive(node.idString(), new NetworkReceive(node.idString(), buffer)));    while (!client.ready(node, time.milliseconds())) {        client.poll(1, time.milliseconds());        // If a throttled response is received, advance the time to ensure progress.        time.sleep(client.throttleDelayMs(node, time.milliseconds()));    }    selector.clear();    for (int i = 1; i <= 3; i++) {        int throttleTimeMs = 100 * i;        FetchRequest.Builder builder = FetchRequest.Builder.forConsumer(100, 100, new LinkedHashMap<>());        builder.rackId("");        ClientRequest request = client.newClientRequest(node.idString(), builder, time.milliseconds(), true);        client.send(request, time.milliseconds());        client.poll(1, time.milliseconds());        FetchResponse response = fullFetchResponse(tp0, nextRecords, Errors.NONE, i, throttleTimeMs);        buffer = response.serialize(ApiKeys.FETCH.latestVersion(), new ResponseHeader(request.correlationId()));        selector.completeReceive(new NetworkReceive(node.idString(), buffer));        client.poll(1, time.milliseconds());        // If a throttled response is received, advance the time to ensure progress.        time.sleep(client.throttleDelayMs(node, time.milliseconds()));        selector.clear();    }    Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();    KafkaMetric avgMetric = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchThrottleTimeAvg));    KafkaMetric maxMetric = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchThrottleTimeMax));    // Throttle times are ApiVersions=400, Fetch=(100, 200, 300)    assertEquals(250, (Double) avgMetric.metricValue(), EPSILON);    assertEquals(400, (Double) maxMetric.metricValue(), EPSILON);    client.close();}
f6066
0
testFetcherMetrics
public void kafkatest_f6067_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    MetricName maxLagMetric = metrics.metricInstance(metricsRegistry.recordsLagMax);    Map<String, String> tags = new HashMap<>();    tags.put("topic", tp0.topic());    tags.put("partition", String.valueOf(tp0.partition()));    MetricName partitionLagMetric = metrics.metricName("records-lag", metricGroup, tags);    Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();    KafkaMetric recordsFetchLagMax = allMetrics.get(maxLagMetric);    // recordsFetchLagMax should be initialized to NaN    assertEquals(Double.NaN, (Double) recordsFetchLagMax.metricValue(), EPSILON);    // recordsFetchLagMax should be hw - fetchOffset after receiving an empty FetchResponse    fetchRecords(tp0, MemoryRecords.EMPTY, Errors.NONE, 100L, 0);    assertEquals(100, (Double) recordsFetchLagMax.metricValue(), EPSILON);    KafkaMetric partitionLag = allMetrics.get(partitionLagMetric);    assertEquals(100, (Double) partitionLag.metricValue(), EPSILON);    // recordsFetchLagMax should be hw - offset of the last message after receiving a non-empty FetchResponse    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    for (int v = 0; v < 3; v++) builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, "key".getBytes(), ("value-" + v).getBytes());    fetchRecords(tp0, builder.build(), Errors.NONE, 200L, 0);    assertEquals(197, (Double) recordsFetchLagMax.metricValue(), EPSILON);    assertEquals(197, (Double) partitionLag.metricValue(), EPSILON);    // verify de-registration of partition lag    subscriptions.unsubscribe();    fetcher.sendFetches();    assertFalse(allMetrics.containsKey(partitionLagMetric));}
f6067
0
fetchRecords
private Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> kafkatest_f6076_0(TopicPartition tp, MemoryRecords records, Errors error, long hw, long lastStableOffset, int throttleTime)
{    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fullFetchResponse(tp, records, error, hw, lastStableOffset, throttleTime));    consumerClient.poll(time.timer(0));    return fetchedRecords();}
f6076
0
fetchRecords
private Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> kafkatest_f6077_0(TopicPartition tp, MemoryRecords records, Errors error, long hw, long lastStableOffset, long logStartOffset, int throttleTime)
{    assertEquals(1, fetcher.sendFetches());    client.prepareResponse(fetchResponse(tp, records, error, hw, lastStableOffset, logStartOffset, throttleTime));    consumerClient.poll(time.timer(0));    return fetchedRecords();}
f6077
0
testReturnCommittedTransactions
public void kafkatest_f6086_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    int currentOffset = 0;    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()));    currentOffset += commitTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            FetchRequest request = (FetchRequest) body;            assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());            return true;        }    }, fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.containsKey(tp0));    assertEquals(fetchedRecords.get(tp0).size(), 2);}
f6086
0
matches
public boolean kafkatest_f6087_0(AbstractRequest body)
{    FetchRequest request = (FetchRequest) body;    assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());    return true;}
f6087
0
testReturnAbortedTransactionsinUncommittedMode
public void kafkatest_f6096_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_UNCOMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    int currentOffset = 0;    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()));    abortTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    abortedTransactions.add(new FetchResponse.AbortedTransaction(1, 0));    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.containsKey(tp0));}
f6096
0
testConsumerPositionUpdatedWhenSkippingAbortedTransactions
public void kafkatest_f6097_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    long currentOffset = 0;    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "abort1-1".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "abort1-2".getBytes(), "value".getBytes()));    currentOffset += abortTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    abortedTransactions.add(new FetchResponse.AbortedTransaction(1, 0));    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    // Ensure that we don't return any of the aborted records, but yet advance the consumer position.    assertFalse(fetchedRecords.containsKey(tp0));    assertEquals(currentOffset, subscriptions.position(tp0).offset);}
f6097
0
testEmptyControlBatch
public void kafkatest_f6106_0()
{    buildFetcher(OffsetResetStrategy.EARLIEST, new ByteArrayDeserializer(), new ByteArrayDeserializer(), Integer.MAX_VALUE, IsolationLevel.READ_COMMITTED);    ByteBuffer buffer = ByteBuffer.allocate(1024);    int currentOffset = 1;    // Empty control batch should not cause an exception    DefaultRecordBatch.writeEmptyHeader(buffer, RecordBatch.MAGIC_VALUE_V2, 1L, (short) 0, -1, 0, 0, RecordBatch.NO_PARTITION_LEADER_EPOCH, TimestampType.CREATE_TIME, time.milliseconds(), true, true);    currentOffset += appendTransactionalRecords(buffer, 1L, currentOffset, new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()), new SimpleRecord(time.milliseconds(), "key".getBytes(), "value".getBytes()));    commitTransaction(buffer, 1L, currentOffset);    buffer.flip();    List<FetchResponse.AbortedTransaction> abortedTransactions = new ArrayList<>();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    assignFromUser(singleton(tp0));    subscriptions.seek(tp0, 0);    // normal fetch    assertEquals(1, fetcher.sendFetches());    assertFalse(fetcher.hasCompletedFetches());    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            FetchRequest request = (FetchRequest) body;            assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());            return true;        }    }, fullFetchResponseWithAbortedTransactions(records, abortedTransactions, Errors.NONE, 100L, 100L, 0));    consumerClient.poll(time.timer(0));    assertTrue(fetcher.hasCompletedFetches());    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();    assertTrue(fetchedRecords.containsKey(tp0));    assertEquals(fetchedRecords.get(tp0).size(), 2);}
f6106
0
matches
public boolean kafkatest_f6107_0(AbstractRequest body)
{    FetchRequest request = (FetchRequest) body;    assertEquals(IsolationLevel.READ_COMMITTED, request.isolationLevel());    return true;}
f6107
0
testOffsetValidationAwaitsNodeApiVersion
public void kafkatest_f6116_0()
{    buildFetcher();    assignFromUser(singleton(tp0));    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(tp0.topic(), 4);    final int epochOne = 1;    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochOne), 0L);    Node node = metadata.fetch().nodes().get(0);    assertFalse(client.isConnected(node.idString()));    // Seek with a position and leader+epoch    Metadata.LeaderAndEpoch leaderAndEpoch = new Metadata.LeaderAndEpoch(metadata.leaderAndEpoch(tp0).leader, Optional.of(epochOne));    subscriptions.seekUnvalidated(tp0, new SubscriptionState.FetchPosition(20L, Optional.of(epochOne), leaderAndEpoch));    assertFalse(client.isConnected(node.idString()));    assertTrue(subscriptions.awaitingValidation(tp0));    // No version information is initially available, but the node is now connected    fetcher.validateOffsetsIfNeeded();    assertTrue(subscriptions.awaitingValidation(tp0));    assertTrue(client.isConnected(node.idString()));    apiVersions.update(node.idString(), NodeApiVersions.create());    // On the next call, the OffsetForLeaderEpoch request is sent and validation completes    Map<TopicPartition, EpochEndOffset> endOffsetMap = new HashMap<>();    endOffsetMap.put(tp0, new EpochEndOffset(Errors.NONE, epochOne, 30L));    OffsetsForLeaderEpochResponse resp = new OffsetsForLeaderEpochResponse(endOffsetMap);    client.prepareResponseFrom(resp, node);    fetcher.validateOffsetsIfNeeded();    consumerClient.pollNoWakeup();    assertFalse(subscriptions.awaitingValidation(tp0));    assertEquals(20L, subscriptions.position(tp0).offset);}
f6116
0
testOffsetValidationSkippedForOldBroker
public void kafkatest_f6117_0()
{    // Old brokers may require CLUSTER permission to use the OffsetForLeaderEpoch API,    // so we should skip offset validation and not send the request.    buildFetcher();    assignFromUser(singleton(tp0));    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put(tp0.topic(), 4);    final int epochOne = 1;    final int epochTwo = 2;    // Start with metadata, epoch=1    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochOne), 0L);    // Offset validation requires OffsetForLeaderEpoch request v3 or higher    Node node = metadata.fetch().nodes().get(0);    apiVersions.update(node.idString(), NodeApiVersions.create(singleton(new ApiVersionsResponse.ApiVersion(ApiKeys.OFFSET_FOR_LEADER_EPOCH, (short) 0, (short) 2))));    // Seek with a position and leader+epoch    Metadata.LeaderAndEpoch leaderAndEpoch = new Metadata.LeaderAndEpoch(metadata.leaderAndEpoch(tp0).leader, Optional.of(epochOne));    subscriptions.seekUnvalidated(tp0, new SubscriptionState.FetchPosition(0, Optional.of(epochOne), leaderAndEpoch));    // Update metadata to epoch=2, enter validation    metadata.update(TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, tp -> epochTwo), 0L);    fetcher.validateOffsetsIfNeeded();    // Offset validation is skipped    assertFalse(subscriptions.awaitingValidation(tp0));}
f6117
0
listOffsetResponse
private ListOffsetResponse kafkatest_f6126_0(TopicPartition tp, Errors error, long timestamp, long offset)
{    ListOffsetResponse.PartitionData partitionData = new ListOffsetResponse.PartitionData(error, timestamp, offset, Optional.empty());    Map<TopicPartition, ListOffsetResponse.PartitionData> allPartitionData = new HashMap<>();    allPartitionData.put(tp, partitionData);    return new ListOffsetResponse(allPartitionData);}
f6126
0
fullFetchResponseWithAbortedTransactions
private FetchResponse<MemoryRecords> kafkatest_f6127_0(MemoryRecords records, List<FetchResponse.AbortedTransaction> abortedTransactions, Errors error, long lastStableOffset, long hw, int throttleTime)
{    Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = Collections.singletonMap(tp0, new FetchResponse.PartitionData<>(error, hw, lastStableOffset, 0L, abortedTransactions, records));    return new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions), throttleTime, INVALID_SESSION_ID);}
f6127
0
buildFetcher
private void kafkatest_f6136_0(Deserializer<?> keyDeserializer, Deserializer<?> valueDeserializer)
{    buildFetcher(OffsetResetStrategy.EARLIEST, keyDeserializer, valueDeserializer, Integer.MAX_VALUE, IsolationLevel.READ_UNCOMMITTED);}
f6136
0
buildFetcher
private void kafkatest_f6137_0(OffsetResetStrategy offsetResetStrategy, Deserializer<K> keyDeserializer, Deserializer<V> valueDeserializer, int maxPollRecords, IsolationLevel isolationLevel)
{    buildFetcher(new MetricConfig(), offsetResetStrategy, keyDeserializer, valueDeserializer, maxPollRecords, isolationLevel);}
f6137
0
testShouldNotHeartbeat
public void kafkatest_f6146_0()
{    heartbeat.sentHeartbeat(time.milliseconds());    time.sleep(heartbeatIntervalMs / 2);    assertFalse(heartbeat.shouldHeartbeat(time.milliseconds()));}
f6146
0
testTimeToNextHeartbeat
public void kafkatest_f6147_0()
{    heartbeat.sentHeartbeat(time.milliseconds());    assertEquals(heartbeatIntervalMs, heartbeat.timeToNextHeartbeat(time.milliseconds()));    time.sleep(heartbeatIntervalMs);    assertEquals(0, heartbeat.timeToNextHeartbeat(time.milliseconds()));    time.sleep(heartbeatIntervalMs);    assertEquals(0, heartbeat.timeToNextHeartbeat(time.milliseconds()));}
f6147
0
prepare
public void kafkatest_f6156_0(Map<String, List<TopicPartition>> result)
{    this.result = result;}
f6156
0
testEmptyResponse
public void kafkatest_f6157_0()
{    OffsetsForLeaderEpochClient offsetClient = newOffsetClient();    RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future = offsetClient.sendAsyncRequest(Node.noNode(), Collections.emptyMap());    OffsetsForLeaderEpochResponse resp = new OffsetsForLeaderEpochResponse(Collections.emptyMap());    client.prepareResponse(resp);    consumerClient.pollNoWakeup();    OffsetsForLeaderEpochClient.OffsetForEpochResult result = future.value();    assertTrue(result.partitionsToRetry().isEmpty());    assertTrue(result.endOffsets().isEmpty());}
f6157
0
shouldThrowKafkaExceptionOnNonAssignor
public void kafkatest_f6166_0()
{    classNames = Arrays.asList(String.class.getName());    assertThrows(KafkaException.class, () -> getAssignorInstances(classNames, Collections.emptyMap()));}
f6166
0
shouldThrowKafkaExceptionOnAssignorNotFound
public void kafkatest_f6167_0()
{    classNames = Arrays.asList("Non-existent assignor");    assertThrows(KafkaException.class, () -> getAssignorInstances(classNames, Collections.emptyMap()));}
f6167
0
testBasicCompletion
public void kafkatest_f6176_0()
{    RequestFuture<String> future = new RequestFuture<>();    String value = "foo";    future.complete(value);    assertTrue(future.isDone());    assertEquals(value, future.value());}
f6176
0
testBasicFailure
public void kafkatest_f6177_0()
{    RequestFuture<String> future = new RequestFuture<>();    RuntimeException exception = new RuntimeException();    future.raise(exception);    assertTrue(future.isDone());    assertEquals(exception, future.exception());}
f6177
0
listenerInvokedIfAddedBeforeFutureCompletion
public void kafkatest_f6186_0()
{    RequestFuture<Void> future = new RequestFuture<>();    MockRequestFutureListener<Void> listener = new MockRequestFutureListener<>();    future.addListener(listener);    future.complete(null);    assertOnSuccessInvoked(listener);}
f6186
0
listenerInvokedIfAddedBeforeFutureFailure
public void kafkatest_f6187_0()
{    RequestFuture<Void> future = new RequestFuture<>();    MockRequestFutureListener<Void> listener = new MockRequestFutureListener<>();    future.addListener(listener);    future.raise(new RuntimeException());    assertOnFailureInvoked(listener);}
f6187
0
assertOnSuccessInvoked
private static void kafkatest_f6196_0(MockRequestFutureListener<T> listener)
{    assertEquals(1, listener.numOnSuccessCalls.get());    assertEquals(0, listener.numOnFailureCalls.get());}
f6196
0
assertOnFailureInvoked
private static void kafkatest_f6197_0(MockRequestFutureListener<T> listener)
{    assertEquals(0, listener.numOnSuccessCalls.get());    assertEquals(1, listener.numOnFailureCalls.get());}
f6197
0
partitionPause
public void kafkatest_f6206_0()
{    state.assignFromUser(singleton(tp0));    state.seek(tp0, 100);    assertTrue(state.isFetchable(tp0));    state.pause(tp0);    assertFalse(state.isFetchable(tp0));    state.resume(tp0);    assertTrue(state.isFetchable(tp0));}
f6206
0
invalidPositionUpdate
public void kafkatest_f6207_0()
{    state.subscribe(singleton(topic), rebalanceListener);    assertTrue(state.checkAssignmentMatchedSubscription(singleton(tp0)));    state.assignFromSubscribed(singleton(tp0));    state.position(tp0, new SubscriptionState.FetchPosition(0, Optional.empty(), leaderAndEpoch));}
f6207
0
unsubscribeUserAssignment
public void kafkatest_f6216_0()
{    state.assignFromUser(new HashSet<>(Arrays.asList(tp0, tp1)));    state.unsubscribe();    state.subscribe(singleton(topic), rebalanceListener);    assertEquals(singleton(topic), state.subscription());}
f6216
0
unsubscribeUserSubscribe
public void kafkatest_f6217_0()
{    state.subscribe(singleton(topic), rebalanceListener);    state.unsubscribe();    state.assignFromUser(singleton(tp0));    assertEquals(singleton(tp0), state.assignedPartitions());    assertEquals(1, state.numAssignedPartitions());}
f6217
0
testMaybeCompleteValidation
public void kafkatest_f6226_0()
{    Node broker1 = new Node(1, "localhost", 9092);    state.assignFromUser(Collections.singleton(tp0));    int currentEpoch = 10;    long initialOffset = 10L;    int initialOffsetEpoch = 5;    SubscriptionState.FetchPosition initialPosition = new SubscriptionState.FetchPosition(initialOffset, Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, initialPosition);    assertTrue(state.awaitingValidation(tp0));    Optional<OffsetAndMetadata> divergentOffsetMetadataOpt = state.maybeCompleteValidation(tp0, initialPosition, new EpochEndOffset(initialOffsetEpoch, initialOffset + 5));    assertEquals(Optional.empty(), divergentOffsetMetadataOpt);    assertFalse(state.awaitingValidation(tp0));    assertEquals(initialPosition, state.position(tp0));}
f6226
0
testMaybeCompleteValidationAfterPositionChange
public void kafkatest_f6227_0()
{    Node broker1 = new Node(1, "localhost", 9092);    state.assignFromUser(Collections.singleton(tp0));    int currentEpoch = 10;    long initialOffset = 10L;    int initialOffsetEpoch = 5;    long updateOffset = 20L;    int updateOffsetEpoch = 8;    SubscriptionState.FetchPosition initialPosition = new SubscriptionState.FetchPosition(initialOffset, Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, initialPosition);    assertTrue(state.awaitingValidation(tp0));    SubscriptionState.FetchPosition updatePosition = new SubscriptionState.FetchPosition(updateOffset, Optional.of(updateOffsetEpoch), new Metadata.LeaderAndEpoch(broker1, Optional.of(currentEpoch)));    state.seekUnvalidated(tp0, updatePosition);    Optional<OffsetAndMetadata> divergentOffsetMetadataOpt = state.maybeCompleteValidation(tp0, initialPosition, new EpochEndOffset(initialOffsetEpoch, initialOffset + 5));    assertEquals(Optional.empty(), divergentOffsetMetadataOpt);    assertTrue(state.awaitingValidation(tp0));    assertEquals(updatePosition, state.position(tp0));}
f6227
0
testInvalidSocketSendBufferSize
public void kafkatest_f6236_0()
{    Map<String, Object> config = new HashMap<>();    config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    config.put(ConsumerConfig.SEND_BUFFER_CONFIG, -2);    new KafkaConsumer<>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());}
f6236
0
testInvalidSocketReceiveBufferSize
public void kafkatest_f6237_0()
{    Map<String, Object> config = new HashMap<>();    config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    config.put(ConsumerConfig.RECEIVE_BUFFER_CONFIG, -2);    new KafkaConsumer<>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());}
f6237
0
testAssignOnNullTopicPartition
public void kafkatest_f6246_0()
{    try (KafkaConsumer<byte[], byte[]> consumer = newConsumer((String) null)) {        consumer.assign(null);    }}
f6246
0
testAssignOnEmptyTopicPartition
public void kafkatest_f6247_0()
{    try (KafkaConsumer<byte[], byte[]> consumer = newConsumer(groupId)) {        consumer.assign(Collections.<TopicPartition>emptyList());        assertTrue(consumer.subscription().isEmpty());        assertTrue(consumer.assignment().isEmpty());    }}
f6247
0
verifyHeartbeatSentWhenFetchedDataReady
public void kafkatest_f6256_0() throws Exception
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    Node coordinator = prepareRebalance(client, node, assignor, singletonList(tp0), null);    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    consumer.poll(Duration.ZERO);    // respond to the outstanding fetch so that we have data available on the next poll    client.respondFrom(fetchResponse(tp0, 0, 5), node);    client.poll(0, time.milliseconds());    client.prepareResponseFrom(fetchResponse(tp0, 5, 0), node);    AtomicBoolean heartbeatReceived = prepareHeartbeatResponse(client, coordinator);    time.sleep(heartbeatIntervalMs);    Thread.sleep(heartbeatIntervalMs);    consumer.poll(Duration.ZERO);    assertTrue(heartbeatReceived.get());    consumer.close(Duration.ofMillis(0));}
f6256
0
verifyPollTimesOutDuringMetadataUpdate
public void kafkatest_f6257_0()
{    final Time time = new MockTime();    final SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    final ConsumerMetadata metadata = createMetadata(subscription);    final MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    final ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    final KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    prepareRebalance(client, node, assignor, singletonList(tp0), null);    consumer.poll(Duration.ZERO);    // The underlying client should NOT get a fetch request    final Queue<ClientRequest> requests = client.requests();    Assert.assertEquals(0, requests.size());}
f6257
0
testResetUsingAutoResetPolicy
public void kafkatest_f6266_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.LATEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupId, groupInstanceId);    consumer.assign(singletonList(tp0));    client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);    Node coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    client.prepareResponseFrom(offsetResponse(Collections.singletonMap(tp0, -1L), Errors.NONE), coordinator);    client.prepareResponse(listOffsetsResponse(Collections.singletonMap(tp0, 50L)));    consumer.poll(Duration.ZERO);    assertEquals(50L, consumer.position(tp0));}
f6266
0
testOffsetIsValidAfterSeek
public void kafkatest_f6267_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.LATEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupId, Optional.empty());    consumer.assign(singletonList(tp0));    consumer.seek(tp0, 20L);    consumer.poll(Duration.ZERO);    assertEquals(subscription.validPosition(tp0).offset, 20L);}
f6267
0
testSubscriptionChangesWithAutoCommitEnabled
public void kafkatest_f6276_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    Map<String, Integer> tpCounts = new HashMap<>();    tpCounts.put(topic, 1);    tpCounts.put(topic2, 1);    tpCounts.put(topic3, 1);    initMetadata(client, tpCounts);    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    // initial subscription    consumer.subscribe(Arrays.asList(topic, topic2), getConsumerRebalanceListener(consumer));    // verify that subscription has changed but assignment is still unchanged    assertTrue(consumer.subscription().size() == 2);    assertTrue(consumer.subscription().contains(topic) && consumer.subscription().contains(topic2));    assertTrue(consumer.assignment().isEmpty());    // mock rebalance responses    Node coordinator = prepareRebalance(client, node, assignor, Arrays.asList(tp0, t2p0), null);    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    consumer.poll(Duration.ZERO);    // verify that subscription is still the same, and now assignment has caught up    assertEquals(2, consumer.subscription().size());    assertTrue(consumer.subscription().contains(topic) && consumer.subscription().contains(topic2));    assertEquals(2, consumer.assignment().size());    assertTrue(consumer.assignment().contains(tp0) && consumer.assignment().contains(t2p0));    // mock a response to the outstanding fetch so that we have data available on the next poll    Map<TopicPartition, FetchInfo> fetches1 = new HashMap<>();    fetches1.put(tp0, new FetchInfo(0, 1));    fetches1.put(t2p0, new FetchInfo(0, 10));    client.respondFrom(fetchResponse(fetches1), node);    client.poll(0, time.milliseconds());    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));    // clear out the prefetch so it doesn't interfere with the rest of the test    fetches1.put(tp0, new FetchInfo(1, 0));    fetches1.put(t2p0, new FetchInfo(10, 0));    client.respondFrom(fetchResponse(fetches1), node);    client.poll(0, time.milliseconds());    // verify that the fetch occurred as expected    assertEquals(11, records.count());    assertEquals(1L, consumer.position(tp0));    assertEquals(10L, consumer.position(t2p0));    // subscription change    consumer.subscribe(Arrays.asList(topic, topic3), getConsumerRebalanceListener(consumer));    // verify that subscription has changed but assignment is still unchanged    assertTrue(consumer.subscription().size() == 2);    assertTrue(consumer.subscription().contains(topic) && consumer.subscription().contains(topic3));    assertTrue(consumer.assignment().size() == 2);    assertTrue(consumer.assignment().contains(tp0) && consumer.assignment().contains(t2p0));    // mock the offset commit response for to be revoked partitions    Map<TopicPartition, Long> partitionOffsets1 = new HashMap<>();    partitionOffsets1.put(tp0, 1L);    partitionOffsets1.put(t2p0, 10L);    AtomicBoolean commitReceived = prepareOffsetCommitResponse(client, coordinator, partitionOffsets1);    // mock rebalance responses    prepareRebalance(client, node, assignor, Arrays.asList(tp0, t3p0), coordinator);    // mock a response to the next fetch from the new assignment    Map<TopicPartition, FetchInfo> fetches2 = new HashMap<>();    fetches2.put(tp0, new FetchInfo(1, 1));    fetches2.put(t3p0, new FetchInfo(0, 100));    client.prepareResponse(fetchResponse(fetches2));    records = consumer.poll(Duration.ofMillis(1));    // verify that the fetch occurred as expected    assertEquals(101, records.count());    assertEquals(2L, consumer.position(tp0));    assertEquals(100L, consumer.position(t3p0));    // verify that the offset commits occurred as expected    assertTrue(commitReceived.get());    // verify that subscription is still the same, and now assignment has caught up    assertTrue(consumer.subscription().size() == 2);    assertTrue(consumer.subscription().contains(topic) && consumer.subscription().contains(topic3));    assertTrue(consumer.assignment().size() == 2);    assertTrue(consumer.assignment().contains(tp0) && consumer.assignment().contains(t3p0));    consumer.unsubscribe();    // verify that subscription and assignment are both cleared    assertTrue(consumer.subscription().isEmpty());    assertTrue(consumer.assignment().isEmpty());    client.requests().clear();    consumer.close();}
f6276
0
testSubscriptionChangesWithAutoCommitDisabled
public void kafkatest_f6277_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    Map<String, Integer> tpCounts = new HashMap<>();    tpCounts.put(topic, 1);    tpCounts.put(topic2, 1);    initMetadata(client, tpCounts);    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, false, groupInstanceId);    // initial subscription    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    // verify that subscription has changed but assignment is still unchanged    assertEquals(singleton(topic), consumer.subscription());    assertEquals(Collections.emptySet(), consumer.assignment());    // mock rebalance responses    prepareRebalance(client, node, assignor, singletonList(tp0), null);    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    consumer.poll(Duration.ZERO);    // verify that subscription is still the same, and now assignment has caught up    assertEquals(singleton(topic), consumer.subscription());    assertEquals(singleton(tp0), consumer.assignment());    consumer.poll(Duration.ZERO);    // subscription change    consumer.subscribe(singleton(topic2), getConsumerRebalanceListener(consumer));    // verify that subscription has changed but assignment is still unchanged    assertEquals(singleton(topic2), consumer.subscription());    assertEquals(singleton(tp0), consumer.assignment());    // the auto commit is disabled, so no offset commit request should be sent    for (ClientRequest req : client.requests()) assertNotSame(ApiKeys.OFFSET_COMMIT, req.requestBuilder().apiKey());    // subscription change    consumer.unsubscribe();    // verify that subscription and assignment are both updated    assertEquals(Collections.emptySet(), consumer.subscription());    assertEquals(Collections.emptySet(), consumer.assignment());    // the auto commit is disabled, so no offset commit request should be sent    for (ClientRequest req : client.requests()) assertNotSame(ApiKeys.OFFSET_COMMIT, req.requestBuilder().apiKey());    client.requests().clear();    consumer.close();}
f6277
0
testLeaveGroupTimeout
public void kafkatest_f6286_0() throws Exception
{    Map<TopicPartition, Errors> response = new HashMap<>();    response.put(tp0, Errors.NONE);    OffsetCommitResponse commitResponse = offsetCommitResponse(response);    consumerCloseTest(5000, singletonList(commitResponse), 5000, false);}
f6286
0
testCloseNoWait
public void kafkatest_f6287_0() throws Exception
{    consumerCloseTest(0, Collections.<AbstractResponse>emptyList(), 0, false);}
f6287
0
consumerCloseTest
private void kafkatest_f6296_0(final long closeTimeoutMs, List<? extends AbstractResponse> responses, long waitMs, boolean interrupt) throws Exception
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    final KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, false, Optional.empty());    consumer.subscribe(singleton(topic), getConsumerRebalanceListener(consumer));    Node coordinator = prepareRebalance(client, node, assignor, singletonList(tp0), null);    client.prepareMetadataUpdate(TestUtils.metadataUpdateWith(1, Collections.singletonMap(topic, 1)));    consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));    // Poll with responses    client.prepareResponseFrom(fetchResponse(tp0, 0, 1), node);    client.prepareResponseFrom(fetchResponse(tp0, 1, 0), node);    consumer.poll(Duration.ZERO);    // Initiate close() after a commit request on another thread.    // Kafka consumer is single-threaded, but the implementation allows calls on a    // different thread as long as the calls are not executed concurrently. So this is safe.    ExecutorService executor = Executors.newSingleThreadExecutor();    final AtomicReference<Exception> closeException = new AtomicReference<>();    try {        Future<?> future = executor.submit(new Runnable() {            @Override            public void run() {                consumer.commitAsync();                try {                    consumer.close(Duration.ofMillis(closeTimeoutMs));                } catch (Exception e) {                    closeException.set(e);                }            }        });        // if close timeout is not zero.        try {            future.get(100, TimeUnit.MILLISECONDS);            if (closeTimeoutMs != 0)                fail("Close completed without waiting for commit or leave response");        } catch (TimeoutException e) {        // Expected exception        }        // Ensure close has started and queued at least one more request after commitAsync        client.waitForRequests(2, 1000);        // In non-graceful mode, close() times out without an exception even though commit response is pending        for (int i = 0; i < responses.size(); i++) {            client.waitForRequests(1, 1000);            client.respondFrom(responses.get(i), coordinator);            if (i != responses.size() - 1) {                try {                    future.get(100, TimeUnit.MILLISECONDS);                    fail("Close completed without waiting for response");                } catch (TimeoutException e) {                // Expected exception                }            }        }        if (waitMs > 0)            time.sleep(waitMs);        if (interrupt) {            assertTrue("Close terminated prematurely", future.cancel(true));            TestUtils.waitForCondition(new TestCondition() {                @Override                public boolean conditionMet() {                    return closeException.get() != null;                }            }, "InterruptException did not occur within timeout.");            assertTrue("Expected exception not thrown " + closeException, closeException.get() instanceof InterruptException);        } else {            // Should succeed without TimeoutException or ExecutionException            future.get(500, TimeUnit.MILLISECONDS);            assertNull("Unexpected exception during close", closeException.get());        }    } finally {        executor.shutdownNow();    }}
f6296
0
run
public void kafkatest_f6297_0()
{    consumer.commitAsync();    try {        consumer.close(Duration.ofMillis(closeTimeoutMs));    } catch (Exception e) {        closeException.set(e);    }}
f6297
0
testRebalanceException
public void kafkatest_f6306_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, Collections.singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RoundRobinAssignor();    KafkaConsumer<String, String> consumer = newConsumer(time, client, subscription, metadata, assignor, true, groupInstanceId);    consumer.subscribe(singleton(topic), getExceptionConsumerRebalanceListener());    Node coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);    client.prepareResponseFrom(joinGroupFollowerResponse(assignor, 1, "memberId", "leaderId", Errors.NONE), coordinator);    client.prepareResponseFrom(syncGroupResponse(singletonList(tp0), Errors.NONE), coordinator);    // assign throws    try {        consumer.updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE));        fail("Should throw exception");    } catch (Throwable e) {        assertEquals("boom!", e.getCause().getMessage());    }    // the assignment is still updated regardless of the exception    assertEquals(singleton(tp0), subscription.assignedPartitions());    // close's revoke throws    try {        consumer.close(Duration.ofMillis(0));        fail("Should throw exception");    } catch (Throwable e) {        assertEquals("boom!", e.getCause().getCause().getMessage());    }    consumer.close(Duration.ofMillis(0));    // the assignment is still updated regardless of the exception    assertTrue(subscription.assignedPartitions().isEmpty());}
f6306
0
consumerWithPendingAuthenticationError
private KafkaConsumer<String, String> kafkatest_f6307_0()
{    Time time = new MockTime();    SubscriptionState subscription = new SubscriptionState(new LogContext(), OffsetResetStrategy.EARLIEST);    ConsumerMetadata metadata = createMetadata(subscription);    MockClient client = new MockClient(time, metadata);    initMetadata(client, singletonMap(topic, 1));    Node node = metadata.fetch().nodes().get(0);    ConsumerPartitionAssignor assignor = new RangeAssignor();    client.createPendingAuthenticationError(node, 0);    return newConsumer(time, client, subscription, metadata, assignor, false, groupInstanceId);}
f6307
0
prepareRebalance
private Node kafkatest_f6317_0(MockClient client, Node node, ConsumerPartitionAssignor assignor, List<TopicPartition> partitions, Node coordinator)
{    if (coordinator == null) {        // lookup coordinator        client.prepareResponseFrom(FindCoordinatorResponse.prepareResponse(Errors.NONE, node), node);        coordinator = new Node(Integer.MAX_VALUE - node.id(), node.host(), node.port());    }    // join group    client.prepareResponseFrom(joinGroupFollowerResponse(assignor, 1, "memberId", "leaderId", Errors.NONE), coordinator);    // sync group    client.prepareResponseFrom(syncGroupResponse(partitions, Errors.NONE), coordinator);    return coordinator;}
f6317
0
prepareHeartbeatResponse
private AtomicBoolean kafkatest_f6318_0(MockClient client, Node coordinator)
{    final AtomicBoolean heartbeatReceived = new AtomicBoolean(false);    client.prepareResponseFrom(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            heartbeatReceived.set(true);            return true;        }    }, new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(Errors.NONE.code())), coordinator);    return heartbeatReceived;}
f6318
0
listOffsetsResponse
private ListOffsetResponse kafkatest_f6327_0(Map<TopicPartition, Long> offsets)
{    return listOffsetsResponse(offsets, Collections.emptyMap());}
f6327
0
listOffsetsResponse
private ListOffsetResponse kafkatest_f6328_0(Map<TopicPartition, Long> partitionOffsets, Map<TopicPartition, Errors> partitionErrors)
{    Map<TopicPartition, ListOffsetResponse.PartitionData> partitionData = new HashMap<>();    for (Map.Entry<TopicPartition, Long> partitionOffset : partitionOffsets.entrySet()) {        partitionData.put(partitionOffset.getKey(), new ListOffsetResponse.PartitionData(Errors.NONE, ListOffsetResponse.UNKNOWN_TIMESTAMP, partitionOffset.getValue(), Optional.empty()));    }    for (Map.Entry<TopicPartition, Errors> partitionError : partitionErrors.entrySet()) {        partitionData.put(partitionError.getKey(), new ListOffsetResponse.PartitionData(partitionError.getValue(), ListOffsetResponse.UNKNOWN_TIMESTAMP, ListOffsetResponse.UNKNOWN_OFFSET, Optional.empty()));    }    return new ListOffsetResponse(partitionData);}
f6328
0
testSimpleMockDeprecated
public void kafkatest_f6337_0()
{    consumer.subscribe(Collections.singleton("test"));    assertEquals(0, consumer.poll(1000).count());    consumer.rebalance(Arrays.asList(new TopicPartition("test", 0), new TopicPartition("test", 1)));    // Mock consumers need to seek manually since they cannot automatically reset offsets    HashMap<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(new TopicPartition("test", 0), 0L);    beginningOffsets.put(new TopicPartition("test", 1), 0L);    consumer.updateBeginningOffsets(beginningOffsets);    consumer.seek(new TopicPartition("test", 0), 0);    ConsumerRecord<String, String> rec1 = new ConsumerRecord<>("test", 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, "key1", "value1");    ConsumerRecord<String, String> rec2 = new ConsumerRecord<>("test", 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, "key2", "value2");    consumer.addRecord(rec1);    consumer.addRecord(rec2);    ConsumerRecords<String, String> recs = consumer.poll(1);    Iterator<ConsumerRecord<String, String>> iter = recs.iterator();    assertEquals(rec1, iter.next());    assertEquals(rec2, iter.next());    assertFalse(iter.hasNext());    assertEquals(2L, consumer.position(new TopicPartition("test", 0)));    consumer.commitSync();    assertEquals(2L, consumer.committed(new TopicPartition("test", 0)).offset());}
f6337
0
testConsumerRecordsIsEmptyWhenReturningNoRecords
public void kafkatest_f6338_0()
{    TopicPartition partition = new TopicPartition("test", 0);    consumer.assign(Collections.singleton(partition));    consumer.addRecord(new ConsumerRecord<String, String>("test", 0, 0, null, null));    consumer.updateEndOffsets(Collections.singletonMap(partition, 1L));    consumer.seekToEnd(Collections.singleton(partition));    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));    assertThat(records.count(), is(0));    assertThat(records.isEmpty(), is(true));}
f6338
0
testOneConsumerOneTopic
public void kafkatest_f6347_0()
{    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, 3);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumer1, new Subscription(topics(topic1))));    assertEquals(Collections.singleton(consumer1), assignment.keySet());    assertAssignment(partitions(tp(topic1, 0), tp(topic1, 1), tp(topic1, 2)), assignment.get(consumer1));}
f6347
0
testOnlyAssignsPartitionsFromSubscribedTopics
public void kafkatest_f6348_0()
{    String otherTopic = "other";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic1, 3);    partitionsPerTopic.put(otherTopic, 3);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumer1, new Subscription(topics(topic1))));    assertEquals(Collections.singleton(consumer1), assignment.keySet());    assertAssignment(partitions(tp(topic1, 0), tp(topic1, 1), tp(topic1, 2)), assignment.get(consumer1));}
f6348
0
testStaticMemberRangeAssignmentPersistentAfterMemberIdChanges
public void kafkatest_f6357_0()
{    Map<String, Integer> partitionsPerTopic = setupPartitionsPerTopicWithTwoTopics(5, 5);    Map<String, Subscription> consumers = new HashMap<>();    for (MemberInfo m : staticMemberInfos) {        Subscription subscription = new Subscription(topics(topic1, topic2), null, Collections.emptyList());        subscription.setGroupInstanceId(m.groupInstanceId);        consumers.put(m.memberId, subscription);    }    Map<String, List<TopicPartition>> expectedInstanceAssignment = new HashMap<>();    expectedInstanceAssignment.put(instance1, partitions(tp(topic1, 0), tp(topic1, 1), tp(topic2, 0), tp(topic2, 1)));    expectedInstanceAssignment.put(instance2, partitions(tp(topic1, 2), tp(topic1, 3), tp(topic2, 2), tp(topic2, 3)));    expectedInstanceAssignment.put(instance3, partitions(tp(topic1, 4), tp(topic2, 4)));    Map<String, List<TopicPartition>> staticAssignment = checkStaticAssignment(assignor, partitionsPerTopic, consumers);    assertEquals(expectedInstanceAssignment, staticAssignment);    // Now switch the member.id fields for each member info, the assignment should    // stay the same as last time.    String consumer4 = "consumer4";    String consumer5 = "consumer5";    consumers.put(consumer4, consumers.get(consumer3));    consumers.remove(consumer3);    consumers.put(consumer5, consumers.get(consumer2));    consumers.remove(consumer2);    Map<String, List<TopicPartition>> newStaticAssignment = checkStaticAssignment(assignor, partitionsPerTopic, consumers);    assertEquals(staticAssignment, newStaticAssignment);}
f6357
0
checkStaticAssignment
 static Map<String, List<TopicPartition>> kafkatest_f6358_0(AbstractPartitionAssignor assignor, Map<String, Integer> partitionsPerTopic, Map<String, Subscription> consumers)
{    Map<String, List<TopicPartition>> assignmentByMemberId = assignor.assign(partitionsPerTopic, consumers);    Map<String, List<TopicPartition>> assignmentByInstanceId = new HashMap<>();    for (Map.Entry<String, Subscription> entry : consumers.entrySet()) {        String memberId = entry.getKey();        Optional<String> instanceId = entry.getValue().groupInstanceId();        instanceId.ifPresent(id -> assignmentByInstanceId.put(id, assignmentByMemberId.get(memberId)));    }    return assignmentByInstanceId;}
f6358
0
testOnlyAssignsPartitionsFromSubscribedTopics
public void kafkatest_f6367_0()
{    String otherTopic = "other";    Map<String, Integer> partitionsPerTopic = new HashMap<>();    partitionsPerTopic.put(topic, 3);    partitionsPerTopic.put(otherTopic, 3);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumerId, new Subscription(topics(topic))));    assertEquals(partitions(tp(topic, 0), tp(topic, 1), tp(topic, 2)), assignment.get(consumerId));}
f6367
0
testOneConsumerMultipleTopics
public void kafkatest_f6368_0()
{    Map<String, Integer> partitionsPerTopic = setupPartitionsPerTopicWithTwoTopics(1, 2);    Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, Collections.singletonMap(consumerId, new Subscription(topics(topic1, topic2))));    assertEquals(partitions(tp(topic1, 0), tp(topic2, 0), tp(topic2, 1)), assignment.get(consumerId));}
f6368
0
topics
private static List<String> kafkatest_f6377_0(String... topics)
{    return Arrays.asList(topics);}
f6377
0
partitions
private static List<TopicPartition> kafkatest_f6378_0(TopicPartition... partitions)
{    return Arrays.asList(partitions);}
f6378
0
buildSubscriptionWithGeneration
private Subscription kafkatest_f6387_0(List<String> topics, List<TopicPartition> partitions, int generation)
{    return new Subscription(topics, serializeTopicPartitionAssignment(new MemberData(partitions, Optional.of(generation))));}
f6387
0
buildSubscriptionWithOldSchema
private static Subscription kafkatest_f6388_0(List<String> topics, List<TopicPartition> partitions)
{    Struct struct = new Struct(StickyAssignor.STICKY_ASSIGNOR_USER_DATA_V0);    List<Struct> topicAssignments = new ArrayList<>();    for (Map.Entry<String, List<Integer>> topicEntry : CollectionUtils.groupPartitionsByTopic(partitions).entrySet()) {        Struct topicAssignment = new Struct(StickyAssignor.TOPIC_ASSIGNMENT);        topicAssignment.set(StickyAssignor.TOPIC_KEY_NAME, topicEntry.getKey());        topicAssignment.set(StickyAssignor.PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());        topicAssignments.add(topicAssignment);    }    struct.set(StickyAssignor.TOPIC_PARTITIONS_KEY_NAME, topicAssignments.toArray());    ByteBuffer buffer = ByteBuffer.allocate(StickyAssignor.STICKY_ASSIGNOR_USER_DATA_V0.sizeOf(struct));    StickyAssignor.STICKY_ASSIGNOR_USER_DATA_V0.write(buffer, struct);    buffer.flip();    return new Subscription(topics, buffer);}
f6388
0
testSessionless
public void kafkatest_f6397_0()
{    FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);    FetchSessionHandler.Builder builder = handler.newBuilder();    builder.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));    FetchSessionHandler.FetchRequestData data = builder.build();    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 110, 210)), data.toSend(), data.sessionPartitions());    assertEquals(INVALID_SESSION_ID, data.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data.metadata().epoch());    FetchResponse<MemoryRecords> resp = new FetchResponse<>(Errors.NONE, respMap(new RespEntry("foo", 0, 0, 0), new RespEntry("foo", 1, 0, 0)), 0, INVALID_SESSION_ID);    handler.handleResponse(resp);    FetchSessionHandler.Builder builder2 = handler.newBuilder();    builder2.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    FetchSessionHandler.FetchRequestData data2 = builder2.build();    assertEquals(INVALID_SESSION_ID, data2.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data2.metadata().epoch());    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200)), data.toSend(), data.sessionPartitions());}
f6397
0
testIncrementals
public void kafkatest_f6398_0()
{    FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);    FetchSessionHandler.Builder builder = handler.newBuilder();    builder.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 110, 210, Optional.empty()));    FetchSessionHandler.FetchRequestData data = builder.build();    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 110, 210)), data.toSend(), data.sessionPartitions());    assertEquals(INVALID_SESSION_ID, data.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data.metadata().epoch());    FetchResponse<MemoryRecords> resp = new FetchResponse<>(Errors.NONE, respMap(new RespEntry("foo", 0, 10, 20), new RespEntry("foo", 1, 10, 20)), 0, 123);    handler.handleResponse(resp);    // Test an incremental fetch request which adds one partition and modifies another.    FetchSessionHandler.Builder builder2 = handler.newBuilder();    builder2.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder2.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 120, 210, Optional.empty()));    builder2.add(new TopicPartition("bar", 0), new FetchRequest.PartitionData(20, 200, 200, Optional.empty()));    FetchSessionHandler.FetchRequestData data2 = builder2.build();    assertFalse(data2.metadata().isFull());    assertMapEquals(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 120, 210), new ReqEntry("bar", 0, 20, 200, 200)), data2.sessionPartitions());    assertMapEquals(reqMap(new ReqEntry("bar", 0, 20, 200, 200), new ReqEntry("foo", 1, 10, 120, 210)), data2.toSend());    FetchResponse<MemoryRecords> resp2 = new FetchResponse<>(Errors.NONE, respMap(new RespEntry("foo", 1, 20, 20)), 0, 123);    handler.handleResponse(resp2);    // Skip building a new request.  Test that handling an invalid fetch session epoch response results    // in a request which closes the session.    FetchResponse<MemoryRecords> resp3 = new FetchResponse<>(Errors.INVALID_FETCH_SESSION_EPOCH, respMap(), 0, INVALID_SESSION_ID);    handler.handleResponse(resp3);    FetchSessionHandler.Builder builder4 = handler.newBuilder();    builder4.add(new TopicPartition("foo", 0), new FetchRequest.PartitionData(0, 100, 200, Optional.empty()));    builder4.add(new TopicPartition("foo", 1), new FetchRequest.PartitionData(10, 120, 210, Optional.empty()));    builder4.add(new TopicPartition("bar", 0), new FetchRequest.PartitionData(20, 200, 200, Optional.empty()));    FetchSessionHandler.FetchRequestData data4 = builder4.build();    assertTrue(data4.metadata().isFull());    assertEquals(data2.metadata().sessionId(), data4.metadata().sessionId());    assertEquals(INITIAL_EPOCH, data4.metadata().epoch());    assertMapsEqual(reqMap(new ReqEntry("foo", 0, 0, 100, 200), new ReqEntry("foo", 1, 10, 120, 210), new ReqEntry("bar", 0, 20, 200, 200)), data4.sessionPartitions(), data4.toSend());}
f6398
0
testCompleteLastSentThrowsIfNoInFlights
public void kafkatest_f6407_0()
{    inFlightRequests.completeLastSent(dest);}
f6407
0
addRequest
private int kafkatest_f6408_0(String destination)
{    return addRequest(destination, 0, 10000);}
f6408
0
testRequestUpdate
public void kafkatest_f6417_0()
{    assertFalse(metadata.updateRequested());    int[] epochs = { 42, 42, 41, 41, 42, 43, 43, 42, 41, 44 };    boolean[] updateResult = { true, false, false, false, false, true, false, false, false, true };    TopicPartition tp = new TopicPartition("topic", 0);    for (int i = 0; i < epochs.length; i++) {        metadata.updateLastSeenEpochIfNewer(tp, epochs[i]);        if (updateResult[i]) {            assertTrue("Expected metadata update to be requested [" + i + "]", metadata.updateRequested());        } else {            assertFalse("Did not expect metadata update to be requested [" + i + "]", metadata.updateRequested());        }        metadata.update(emptyMetadataResponse(), 0L);        assertFalse(metadata.updateRequested());    }}
f6417
0
testRejectOldMetadata
public void kafkatest_f6418_0()
{    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put("topic-1", 1);    TopicPartition tp = new TopicPartition("topic-1", 0);    metadata.update(emptyMetadataResponse(), 0L);    // First epoch seen, accept it    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 100);        metadata.update(metadataResponse, 10L);        assertNotNull(metadata.fetch().partition(tp));        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }    // Fake an empty ISR, but with an older epoch, should reject it    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 99, (error, partition, leader, leaderEpoch, replicas, isr, offlineReplicas) -> new MetadataResponse.PartitionMetadata(error, partition, leader, leaderEpoch, replicas, Collections.emptyList(), offlineReplicas));        metadata.update(metadataResponse, 20L);        assertEquals(metadata.fetch().partition(tp).inSyncReplicas().length, 1);        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }    // Fake an empty ISR, with same epoch, accept it    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 100, (error, partition, leader, leaderEpoch, replicas, isr, offlineReplicas) -> new MetadataResponse.PartitionMetadata(error, partition, leader, leaderEpoch, replicas, Collections.emptyList(), offlineReplicas));        metadata.update(metadataResponse, 20L);        assertEquals(metadata.fetch().partition(tp).inSyncReplicas().length, 0);        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }    // Empty metadata response, should not keep old partition but should keep the last-seen epoch    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), Collections.emptyMap());        metadata.update(metadataResponse, 20L);        assertNull(metadata.fetch().partition(tp));        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }    // Back in the metadata, with old epoch, should not get added    {        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 1, Collections.emptyMap(), partitionCounts, _tp -> 99);        metadata.update(metadataResponse, 10L);        assertNull(metadata.fetch().partition(tp));        assertEquals(metadata.lastSeenLeaderEpoch(tp).get().longValue(), 100);    }}
f6418
0
testNodeIfOffline
public void kafkatest_f6427_0()
{    Map<String, Integer> partitionCounts = new HashMap<>();    partitionCounts.put("topic-1", 1);    Node node0 = new Node(0, "localhost", 9092);    Node node1 = new Node(1, "localhost", 9093);    MetadataResponse metadataResponse = TestUtils.metadataUpdateWith("dummy", 2, Collections.emptyMap(), partitionCounts, _tp -> 99, (error, partition, leader, leaderEpoch, replicas, isr, offlineReplicas) -> new MetadataResponse.PartitionMetadata(error, partition, node0, leaderEpoch, Collections.singletonList(node0), Collections.emptyList(), Collections.singletonList(node1)));    metadata.update(emptyMetadataResponse(), 0L);    metadata.update(metadataResponse, 10L);    TopicPartition tp = new TopicPartition("topic-1", 0);    assertOptional(metadata.fetch().nodeIfOnline(tp, 0), node -> assertEquals(node.id(), 0));    assertFalse(metadata.fetch().nodeIfOnline(tp, 1).isPresent());    assertEquals(metadata.fetch().nodeById(0).id(), 0);    assertEquals(metadata.fetch().nodeById(1).id(), 1);}
f6427
0
matches
public boolean kafkatest_f6428_0(AbstractRequest body)
{    return true;}
f6428
0
throttle
public void kafkatest_f6437_0(Node node, long durationMs)
{    connectionState(node.idString()).throttle(time.milliseconds() + durationMs);}
f6437
0
delayReady
public void kafkatest_f6438_0(Node node, long durationMs)
{    connectionState(node.idString()).setReadyDelayed(time.milliseconds() + durationMs);}
f6438
0
maybeAwaitWakeup
private synchronized void kafkatest_f6447_0()
{    try {        int remainingBlockingWakeups = numBlockingWakeups;        if (remainingBlockingWakeups <= 0)            return;        while (numBlockingWakeups == remainingBlockingWakeups) wait();    } catch (InterruptedException e) {        throw new InterruptException(e);    }}
f6447
0
poll
public List<ClientResponse> kafkatest_f6448_0(long timeoutMs, long now)
{    maybeAwaitWakeup();    checkTimeoutOfPendingRequests(now);    // We skip metadata updates if all nodes are currently blacked out    if (metadataUpdater.isUpdateNeeded() && leastLoadedNode(now) != null) {        MetadataUpdate metadataUpdate = metadataUpdates.poll();        if (metadataUpdate != null) {            metadataUpdater.update(time, metadataUpdate);        } else {            metadataUpdater.updateWithCurrentMetadata(time);        }    }    List<ClientResponse> copy = new ArrayList<>();    ClientResponse response;    while ((response = this.responses.poll()) != null) {        response.onComplete();        copy.add(response);    }    return copy;}
f6448
0
respondFrom
public void kafkatest_f6457_0(AbstractResponse response, Node node, boolean disconnected)
{    Iterator<ClientRequest> iterator = requests.iterator();    while (iterator.hasNext()) {        ClientRequest request = iterator.next();        if (request.destination().equals(node.idString())) {            iterator.remove();            short version = request.requestBuilder().latestAllowedVersion();            responses.add(new ClientResponse(request.makeHeader(version), request.callback(), request.destination(), request.createdTimeMs(), time.milliseconds(), disconnected, null, null, response));            return;        }    }    throw new IllegalArgumentException("No requests available to node " + node);}
f6457
0
prepareResponse
public void kafkatest_f6458_0(AbstractResponse response)
{    prepareResponse(ALWAYS_TRUE, response, false);}
f6458
0
waitForRequests
public void kafkatest_f6467_0(final int minRequests, long maxWaitMs) throws InterruptedException
{    TestUtils.waitForCondition(new TestCondition() {        @Override        public boolean conditionMet() {            return requests.size() >= minRequests;        }    }, maxWaitMs, "Expected requests have not been sent");}
f6467
0
conditionMet
public boolean kafkatest_f6468_0()
{    return requests.size() >= minRequests;}
f6468
0
hasPendingResponses
public boolean kafkatest_f6477_0()
{    return !responses.isEmpty() || !futureResponses.isEmpty();}
f6477
0
inFlightRequestCount
public int kafkatest_f6478_0(String node)
{    int result = 0;    for (ClientRequest req : requests) {        if (req.destination().equals(node))            ++result;    }    return result;}
f6478
0
leastLoadedNode
public Node kafkatest_f6487_0(long now)
{    // Consistent with NetworkClient, we do not return nodes awaiting reconnect backoff    for (Node node : metadataUpdater.fetchNodes()) {        if (!connectionState(node.idString()).isBackingOff(now))            return node;    }    return null;}
f6487
0
setNodeApiVersions
public void kafkatest_f6488_0(NodeApiVersions nodeApiVersions)
{    this.nodeApiVersions = nodeApiVersions;}
f6488
0
throttle
 void kafkatest_f6499_0(long untilMs)
{    throttledUntilMs = untilMs;}
f6499
0
setUnreachable
 void kafkatest_f6500_0(long untilMs)
{    unreachableUntilMs = untilMs;}
f6500
0
ready
 boolean kafkatest_f6509_0(long now)
{    switch(state) {        case CONNECTED:            return notThrottled(now);        case CONNECTING:            if (isReadyDelayed(now))                return false;            state = State.CONNECTED;            return ready(now);        case DISCONNECTED:            if (isBackingOff(now)) {                return false;            } else if (isUnreachable(now)) {                backingOffUntilMs = now + 100;                return false;            }            state = State.CONNECTING;            return ready(now);        default:            throw new IllegalArgumentException("Invalid state: " + state);    }}
f6509
0
createNetworkClient
private NetworkClient kafkatest_f6510_0(long reconnectBackoffMaxMs)
{    return new NetworkClient(selector, metadataUpdater, "mock", Integer.MAX_VALUE, reconnectBackoffMsTest, reconnectBackoffMaxMs, 64 * 1024, 64 * 1024, defaultRequestTimeoutMs, ClientDnsLookup.DEFAULT, time, true, new ApiVersions(), new LogContext());}
f6510
0
testClose
public void kafkatest_f6519_0()
{    client.ready(node, time.milliseconds());    awaitReady(client, node);    client.poll(1, time.milliseconds());    assertTrue("The client should be ready", client.isReady(node, time.milliseconds()));    ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000, Collections.<TopicPartition, MemoryRecords>emptyMap());    ClientRequest request = client.newClientRequest(node.idString(), builder, time.milliseconds(), true);    client.send(request, time.milliseconds());    assertEquals("There should be 1 in-flight request after send", 1, client.inFlightRequestCount(node.idString()));    assertTrue(client.hasInFlightRequests(node.idString()));    assertTrue(client.hasInFlightRequests());    client.close(node.idString());    assertEquals("There should be no in-flight request after close", 0, client.inFlightRequestCount(node.idString()));    assertFalse(client.hasInFlightRequests(node.idString()));    assertFalse(client.hasInFlightRequests());    assertFalse("Connection should not be ready after close", client.isReady(node, 0));}
f6519
0
testUnsupportedVersionDuringInternalMetadataRequest
public void kafkatest_f6520_0()
{    List<String> topics = Arrays.asList("topic_1");    // disabling auto topic creation for versions less than 4 is not supported    MetadataRequest.Builder builder = new MetadataRequest.Builder(topics, false, (short) 3);    client.sendInternalMetadataRequest(builder, node.idString(), time.milliseconds());    assertEquals(UnsupportedVersionException.class, metadataUpdater.getAndClearFailure().getClass());}
f6520
0
testThrottlingNotEnabledForConnectionToOlderBroker
public void kafkatest_f6529_0()
{    // Instrument the test so that the max protocol version for PRODUCE returned from the node is 5 and thus    // client-side throttling is not enabled. Also, return a response with a 100ms throttle delay.    setExpectedApiVersionsResponse(createExpectedApiVersionsResponse(ApiKeys.PRODUCE, (short) 5));    while (!client.ready(node, time.milliseconds())) client.poll(1, time.milliseconds());    selector.clear();    int correlationId = sendEmptyProduceRequest();    client.poll(1, time.milliseconds());    sendThrottledProduceResponse(correlationId, 100);    client.poll(1, time.milliseconds());    // Since client-side throttling is disabled, the connection is ready even though the response indicated a    // throttle delay.    assertTrue(client.ready(node, time.milliseconds()));    assertEquals(0, client.throttleDelayMs(node, time.milliseconds()));}
f6529
0
sendEmptyProduceRequest
private int kafkatest_f6530_0()
{    ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000, Collections.emptyMap());    TestCallbackHandler handler = new TestCallbackHandler();    ClientRequest request = client.newClientRequest(node.idString(), builder, time.milliseconds(), true, defaultRequestTimeoutMs, handler);    client.send(request, time.milliseconds());    return request.correlationId();}
f6530
0
testConnectionDelayConnected
public void kafkatest_f6539_0()
{    awaitReady(client, node);    long now = time.milliseconds();    long delay = client.connectionDelay(node, now);    assertEquals(Long.MAX_VALUE, delay);}
f6539
0
testConnectionDelayDisconnected
public void kafkatest_f6540_0()
{    awaitReady(client, node);    // First disconnection    selector.serverDisconnect(node.idString());    client.poll(defaultRequestTimeoutMs, time.milliseconds());    long delay = client.connectionDelay(node, time.milliseconds());    long expectedDelay = reconnectBackoffMsTest;    double jitter = 0.3;    assertEquals(expectedDelay, delay, expectedDelay * jitter);    // Sleep until there is no connection delay    time.sleep(delay);    assertEquals(0, client.connectionDelay(node, time.milliseconds()));    // Start connecting and disconnect before the connection is established    client.ready(node, time.milliseconds());    selector.serverDisconnect(node.idString());    client.poll(defaultRequestTimeoutMs, time.milliseconds());    // Second attempt should take twice as long with twice the jitter    expectedDelay = Math.round(delay * 2);    delay = client.connectionDelay(node, time.milliseconds());    jitter = 0.6;    assertEquals(expectedDelay, delay, expectedDelay * jitter);}
f6540
0
onComplete
public void kafkatest_f6549_0(ClientResponse response)
{    this.executed = true;    this.response = response;}
f6549
0
handleFatalException
public void kafkatest_f6550_0(KafkaException exception)
{    failure = exception;    super.handleFatalException(exception);}
f6550
0
testLatestUsableVersionOutOfRange
public void kafkatest_f6559_0()
{    NodeApiVersions apiVersions = NodeApiVersions.create(Collections.singleton(new ApiVersion(ApiKeys.PRODUCE, (short) 300, (short) 300)));    apiVersions.latestUsableVersion(ApiKeys.PRODUCE);}
f6559
0
testUsableVersionLatestVersions
public void kafkatest_f6560_0()
{    List<ApiVersion> versionList = new LinkedList<>();    for (ApiVersion apiVersion : ApiVersionsResponse.defaultApiVersionsResponse().apiVersions()) {        versionList.add(apiVersion);    }    // Add an API key that we don't know about.    versionList.add(new ApiVersion((short) 100, (short) 0, (short) 1));    NodeApiVersions versions = new NodeApiVersions(versionList);    for (ApiKeys apiKey : ApiKeys.values()) {        assertEquals(apiKey.latestVersion(), versions.latestUsableVersion(apiKey));    }}
f6560
0
asyncAllocate
private CountDownLatch kafkatest_f6569_0(final BufferPool pool, final int size)
{    final CountDownLatch completed = new CountDownLatch(1);    Thread thread = new Thread() {        public void run() {            try {                pool.allocate(size, maxBlockTimeMs);            } catch (InterruptedException e) {                e.printStackTrace();            } finally {                completed.countDown();            }        }    };    thread.start();    return completed;}
f6569
0
run
public void kafkatest_f6570_0()
{    try {        pool.allocate(size, maxBlockTimeMs);    } catch (InterruptedException e) {        e.printStackTrace();    } finally {        completed.countDown();    }}
f6570
0
freeSize
protected int kafkatest_f6579_0()
{    return freeSize.get();}
f6579
0
outOfMemoryOnAllocation
public void kafkatest_f6580_0()
{    BufferPool bufferPool = new BufferPool(1024, 1024, metrics, time, metricGroup) {        @Override        protected ByteBuffer allocateByteBuffer(int size) {            throw new OutOfMemoryError();        }    };    try {        bufferPool.allocateByteBuffer(1024);        // should not reach here        fail("Should have thrown OutOfMemoryError");    } catch (OutOfMemoryError ignored) {    }    assertEquals(bufferPool.availableMemory(), 1024);}
f6580
0
testBatchAbort
public void kafkatest_f6589_0() throws Exception
{    ProducerBatch batch = new ProducerBatch(new TopicPartition("topic", 1), memoryRecordsBuilder, now);    MockCallback callback = new MockCallback();    FutureRecordMetadata future = batch.tryAppend(now, null, new byte[10], Record.EMPTY_HEADERS, callback, now);    KafkaException exception = new KafkaException();    batch.abort(exception);    assertTrue(future.isDone());    assertEquals(1, callback.invocations);    assertEquals(exception, callback.exception);    assertNull(callback.metadata);    // subsequent completion should be ignored    assertFalse(batch.done(500L, 2342342341L, null));    assertFalse(batch.done(-1, -1, new KafkaException()));    assertEquals(1, callback.invocations);    assertTrue(future.isDone());    try {        future.get();        fail("Future should have thrown");    } catch (ExecutionException e) {        assertEquals(exception, e.getCause());    }}
f6589
0
testBatchCannotAbortTwice
public void kafkatest_f6590_0() throws Exception
{    ProducerBatch batch = new ProducerBatch(new TopicPartition("topic", 1), memoryRecordsBuilder, now);    MockCallback callback = new MockCallback();    FutureRecordMetadata future = batch.tryAppend(now, null, new byte[10], Record.EMPTY_HEADERS, callback, now);    KafkaException exception = new KafkaException();    batch.abort(exception);    assertEquals(1, callback.invocations);    assertEquals(exception, callback.exception);    assertNull(callback.metadata);    try {        batch.abort(new KafkaException());        fail("Expected exception from abort");    } catch (IllegalStateException e) {    // expected    }    assertEquals(1, callback.invocations);    assertTrue(future.isDone());    try {        future.get();        fail("Future should have thrown");    } catch (ExecutionException e) {        assertEquals(exception, e.getCause());    }}
f6590
0
onSend
public ProducerRecord<Integer, String> kafkatest_f6600_0(ProducerRecord<Integer, String> record)
{    onSendCount++;    if (throwExceptionOnSend)        throw new KafkaException("Injected exception in AppendProducerInterceptor.onSend");    return new ProducerRecord<>(record.topic(), record.partition(), record.key(), record.value().concat(appendStr));}
f6600
0
onAcknowledgement
public void kafkatest_f6601_0(RecordMetadata metadata, Exception exception)
{    onAckCount++;    if (exception != null) {        onErrorAckCount++;        // if RecordMetadata.TopicPartition is null        if (metadata != null && metadata.topic().length() >= 0) {            onErrorAckWithTopicSetCount++;            if (metadata.partition() >= 0)                onErrorAckWithTopicPartitionSetCount++;        }    }    if (throwExceptionOnAck)        throw new KafkaException("Injected exception in AppendProducerInterceptor.onAcknowledgement");}
f6601
0
testMetadataUpdateWaitTime
public void kafkatest_f6611_0() throws Exception
{    long time = 0;    metadata.update(responseWithCurrentTopics(), time);    assertTrue("No update needed.", metadata.timeToNextUpdate(time) > 0);    // first try with a max wait time of 0 and ensure that this returns back without waiting forever    try {        metadata.awaitUpdate(metadata.requestUpdate(), 0);        fail("Wait on metadata update was expected to timeout, but it didn't");    } catch (TimeoutException te) {    // expected    }    // now try with a higher timeout value once    final long twoSecondWait = 2000;    try {        metadata.awaitUpdate(metadata.requestUpdate(), twoSecondWait);        fail("Wait on metadata update was expected to timeout, but it didn't");    } catch (TimeoutException te) {    // expected    }}
f6611
0
testTimeToNextUpdateOverwriteBackoff
public void kafkatest_f6612_0()
{    long now = 10000;    // New topic added to fetch set and update requested. It should allow immediate update.    metadata.update(responseWithCurrentTopics(), now);    metadata.add("new-topic");    assertEquals(0, metadata.timeToNextUpdate(now));    // Even though add is called, immediate update isn't necessary if the new topic set isn't    // containing a new topic,    metadata.update(responseWithCurrentTopics(), now);    metadata.add("new-topic");    assertEquals(metadataExpireMs, metadata.timeToNextUpdate(now));    // If the new set of topics containing a new topic then it should allow immediate update.    metadata.add("another-new-topic");    assertEquals(0, metadata.timeToNextUpdate(now));}
f6612
0
testAppendLargeCompressed
public void kafkatest_f6621_0() throws Exception
{    testAppendLarge(CompressionType.GZIP);}
f6621
0
testAppendLargeNonCompressed
public void kafkatest_f6622_0() throws Exception
{    testAppendLarge(CompressionType.NONE);}
f6622
0
testNextReadyCheckDelay
public void kafkatest_f6631_0() throws Exception
{    // Next check time will use lingerMs since this test won't trigger any retries/backoff    int lingerMs = 10;    // test case assumes that the records do not fill the batch completely    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    // Just short of going over the limit so we trigger linger time    int appends = expectedNumAppends(batchSize);    // Partition on node1 only    for (int i = 0; i < appends; i++) accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    RecordAccumulator.ReadyCheckResult result = accum.ready(cluster, time.milliseconds());    assertEquals("No nodes should be ready.", 0, result.readyNodes.size());    assertEquals("Next check time should be the linger time", lingerMs, result.nextReadyCheckDelayMs);    time.sleep(lingerMs / 2);    // Add partition on node2 only    for (int i = 0; i < appends; i++) accum.append(tp3, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    result = accum.ready(cluster, time.milliseconds());    assertEquals("No nodes should be ready.", 0, result.readyNodes.size());    assertEquals("Next check time should be defined by node1, half remaining linger time", lingerMs / 2, result.nextReadyCheckDelayMs);    // Add data for another partition on node1, enough to make data sendable immediately    for (int i = 0; i < appends + 1; i++) accum.append(tp2, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    result = accum.ready(cluster, time.milliseconds());    assertEquals("Node1 should be ready", Collections.singleton(node1), result.readyNodes);    // Note this can actually be < linger time because it may use delays from partitions that aren't sendable    // but have leaders with other sendable data.    assertTrue("Next check time should be defined by node2, at most linger time", result.nextReadyCheckDelayMs <= lingerMs);}
f6631
0
testRetryBackoff
public void kafkatest_f6632_0() throws Exception
{    int lingerMs = Integer.MAX_VALUE / 16;    long retryBackoffMs = Integer.MAX_VALUE / 8;    int deliveryTimeoutMs = Integer.MAX_VALUE;    long totalSize = 10 * 1024;    int batchSize = 1024 + DefaultRecordBatch.RECORD_BATCH_OVERHEAD;    String metricGrpName = "producer-metrics";    final RecordAccumulator accum = new RecordAccumulator(logContext, batchSize, CompressionType.NONE, lingerMs, retryBackoffMs, deliveryTimeoutMs, metrics, metricGrpName, time, new ApiVersions(), null, new BufferPool(totalSize, batchSize, metrics, time, metricGrpName));    long now = time.milliseconds();    accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    RecordAccumulator.ReadyCheckResult result = accum.ready(cluster, now + lingerMs + 1);    assertEquals("Node1 should be ready", Collections.singleton(node1), result.readyNodes);    Map<Integer, List<ProducerBatch>> batches = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, now + lingerMs + 1);    assertEquals("Node1 should be the only ready node.", 1, batches.size());    assertEquals("Partition 0 should only have one batch drained.", 1, batches.get(0).size());    // Reenqueue the batch    now = time.milliseconds();    accum.reenqueue(batches.get(0).get(0), now);    // Put message for partition 1 into accumulator    accum.append(tp2, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    result = accum.ready(cluster, now + lingerMs + 1);    assertEquals("Node1 should be ready", Collections.singleton(node1), result.readyNodes);    // tp1 should backoff while tp2 should not    batches = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, now + lingerMs + 1);    assertEquals("Node1 should be the only ready node.", 1, batches.size());    assertEquals("Node1 should only have one batch drained.", 1, batches.get(0).size());    assertEquals("Node1 should only have one batch for partition 1.", tp2, batches.get(0).get(0).topicPartition);    // Partition 0 can be drained after retry backoff    result = accum.ready(cluster, now + retryBackoffMs + 1);    assertEquals("Node1 should be ready", Collections.singleton(node1), result.readyNodes);    batches = accum.drain(cluster, result.readyNodes, Integer.MAX_VALUE, now + retryBackoffMs + 1);    assertEquals("Node1 should be the only ready node.", 1, batches.size());    assertEquals("Node1 should only have one batch drained.", 1, batches.get(0).size());    assertEquals("Node1 should only have one batch for partition 0.", tp1, batches.get(0).get(0).topicPartition);}
f6632
0
doExpireBatchSingle
private void kafkatest_f6641_0(int deliveryTimeoutMs) throws InterruptedException
{    int lingerMs = 300;    List<Boolean> muteStates = Arrays.asList(false, true);    Set<Node> readyNodes = null;    List<ProducerBatch> expiredBatches = new ArrayList<>();    // test case assumes that the records do not fill the batch completely    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(deliveryTimeoutMs, batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    // Make the batches ready due to linger. These batches are not in retry    for (Boolean mute : muteStates) {        if (time.milliseconds() < System.currentTimeMillis())            time.setCurrentTimeMs(System.currentTimeMillis());        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);        assertEquals("No partition should be ready.", 0, accum.ready(cluster, time.milliseconds()).readyNodes.size());        time.sleep(lingerMs);        readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;        assertEquals("Our partition's leader should be ready", Collections.singleton(node1), readyNodes);        expiredBatches = accum.expiredBatches(time.milliseconds());        assertEquals("The batch should not expire when just linger has passed", 0, expiredBatches.size());        if (mute)            accum.mutePartition(tp1);        else            accum.unmutePartition(tp1, 0L);        // Advance the clock to expire the batch.        time.sleep(deliveryTimeoutMs - lingerMs);        expiredBatches = accum.expiredBatches(time.milliseconds());        assertEquals("The batch may expire when the partition is muted", 1, expiredBatches.size());        assertEquals("No partitions should be ready.", 0, accum.ready(cluster, time.milliseconds()).readyNodes.size());    }}
f6641
0
testExpiredBatchSingle
public void kafkatest_f6642_0() throws InterruptedException
{    doExpireBatchSingle(3200);}
f6642
0
testSoonToExpireBatchesArePickedUpForExpiry
public void kafkatest_f6651_0() throws InterruptedException
{    int lingerMs = 500;    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    Set<Node> readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    Map<Integer, List<ProducerBatch>> drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertTrue(drained.isEmpty());    // assertTrue(accum.soonToExpireInFlightBatches().isEmpty());    // advanced clock and send one batch out but it should not be included in soon to expire inflight    // batches because batch's expiry is quite far.    time.sleep(lingerMs + 1);    readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertEquals("A batch did not drain after linger", 1, drained.size());    // assertTrue(accum.soonToExpireInFlightBatches().isEmpty());    // Queue another batch and advance clock such that batch expiry time is earlier than request timeout.    accum.append(tp2, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);    time.sleep(lingerMs * 4);    // Now drain and check that accumulator picked up the drained batch because its expiry is soon.    readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;    drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());    assertEquals("A batch did not drain after linger", 1, drained.size());}
f6651
0
testExpiredBatchesRetry
public void kafkatest_f6652_0() throws InterruptedException
{    int lingerMs = 3000;    int rtt = 1000;    int deliveryTimeoutMs = 3200;    Set<Node> readyNodes;    List<ProducerBatch> expiredBatches;    List<Boolean> muteStates = Arrays.asList(false, true);    // test case assumes that the records do not fill the batch completely    int batchSize = 1025;    RecordAccumulator accum = createTestRecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * batchSize, CompressionType.NONE, lingerMs);    // Test batches in retry.    for (Boolean mute : muteStates) {        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, 0, false);        time.sleep(lingerMs);        readyNodes = accum.ready(cluster, time.milliseconds()).readyNodes;        assertEquals("Our partition's leader should be ready", Collections.singleton(node1), readyNodes);        Map<Integer, List<ProducerBatch>> drained = accum.drain(cluster, readyNodes, Integer.MAX_VALUE, time.milliseconds());        assertEquals("There should be only one batch.", 1, drained.get(node1.id()).size());        time.sleep(rtt);        accum.reenqueue(drained.get(node1.id()).get(0), time.milliseconds());        if (mute)            accum.mutePartition(tp1);        else            accum.unmutePartition(tp1, 0L);        // test expiration        time.sleep(deliveryTimeoutMs - rtt);        accum.drain(cluster, Collections.singleton(node1), Integer.MAX_VALUE, time.milliseconds());        expiredBatches = accum.expiredBatches(time.milliseconds());        assertEquals("RecordAccumulator has expired batches if the partition is not muted", mute ? 1 : 0, expiredBatches.size());    }}
f6652
0
createTestRecordAccumulator
private RecordAccumulator kafkatest_f6661_0(int deliveryTimeoutMs, int batchSize, long totalSize, CompressionType type, int lingerMs)
{    long retryBackoffMs = 100L;    String metricGrpName = "producer-metrics";    return new RecordAccumulator(logContext, batchSize, type, lingerMs, retryBackoffMs, deliveryTimeoutMs, metrics, metricGrpName, time, new ApiVersions(), null, new BufferPool(totalSize, batchSize, metrics, time, metricGrpName));}
f6661
0
setup
public void kafkatest_f6662_0()
{    setupWithTransactionState(null);}
f6662
0
testRetries
public void kafkatest_f6671_0() throws Exception
{    // create a sender with retries = 1    int maxRetries = 1;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    try {        Sender sender = new Sender(logContext, client, metadata, this.accumulator, false, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, null, apiVersions);        // do a successful retry        Future<RecordMetadata> future = accumulator.append(tp0, 0L, "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;        // connect        sender.runOnce();        // send produce request        sender.runOnce();        String id = client.requests().peek().destination();        Node node = new Node(Integer.parseInt(id), "localhost", 0);        assertEquals(1, client.inFlightRequestCount());        assertTrue(client.hasInFlightRequests());        assertEquals(1, sender.inFlightBatches(tp0).size());        assertTrue("Client ready status should be true", client.isReady(node, time.milliseconds()));        client.disconnect(id);        assertEquals(0, client.inFlightRequestCount());        assertFalse(client.hasInFlightRequests());        assertFalse("Client ready status should be false", client.isReady(node, time.milliseconds()));        // the batch is in accumulator.inFlightBatches until it expires        assertEquals(1, sender.inFlightBatches(tp0).size());        // receive error        sender.runOnce();        // reconnect        sender.runOnce();        // resend        sender.runOnce();        assertEquals(1, client.inFlightRequestCount());        assertTrue(client.hasInFlightRequests());        assertEquals(1, sender.inFlightBatches(tp0).size());        long offset = 0;        client.respond(produceResponse(tp0, offset, Errors.NONE, 0));        sender.runOnce();        assertTrue("Request should have retried and completed", future.isDone());        assertEquals(offset, future.get().offset());        assertEquals(0, sender.inFlightBatches(tp0).size());        // do an unsuccessful retry        future = accumulator.append(tp0, 0L, "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;        // send produce request        sender.runOnce();        assertEquals(1, sender.inFlightBatches(tp0).size());        for (int i = 0; i < maxRetries + 1; i++) {            client.disconnect(client.requests().peek().destination());            // receive error            sender.runOnce();            assertEquals(0, sender.inFlightBatches(tp0).size());            // reconnect            sender.runOnce();            // resend            sender.runOnce();            assertEquals(i > 0 ? 0 : 1, sender.inFlightBatches(tp0).size());        }        sender.runOnce();        assertFutureFailure(future, NetworkException.class);        assertEquals(0, sender.inFlightBatches(tp0).size());    } finally {        m.close();    }}
f6671
0
testSendInOrder
public void kafkatest_f6672_0() throws Exception
{    int maxRetries = 1;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    try {        Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, null, apiVersions);        // Create a two broker cluster, with partition 0 on broker 0 and partition 1 on broker 1        MetadataResponse metadataUpdate1 = TestUtils.metadataUpdateWith(2, Collections.singletonMap("test", 2));        client.prepareMetadataUpdate(metadataUpdate1);        // Send the first message.        TopicPartition tp2 = new TopicPartition("test", 1);        accumulator.append(tp2, 0L, "key1".getBytes(), "value1".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);        // connect        sender.runOnce();        // send produce request        sender.runOnce();        String id = client.requests().peek().destination();        assertEquals(ApiKeys.PRODUCE, client.requests().peek().requestBuilder().apiKey());        Node node = new Node(Integer.parseInt(id), "localhost", 0);        assertEquals(1, client.inFlightRequestCount());        assertTrue(client.hasInFlightRequests());        assertTrue("Client ready status should be true", client.isReady(node, time.milliseconds()));        assertEquals(1, sender.inFlightBatches(tp2).size());        time.sleep(900);        // Now send another message to tp2        accumulator.append(tp2, 0L, "key2".getBytes(), "value2".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);        // Update metadata before sender receives response from broker 0. Now partition 2 moves to broker 0        MetadataResponse metadataUpdate2 = TestUtils.metadataUpdateWith(1, Collections.singletonMap("test", 2));        client.prepareMetadataUpdate(metadataUpdate2);        // Sender should not send the second message to node 0.        assertEquals(1, sender.inFlightBatches(tp2).size());        // receive the response for the previous send, and send the new batch        sender.runOnce();        assertEquals(1, client.inFlightRequestCount());        assertTrue(client.hasInFlightRequests());        assertEquals(1, sender.inFlightBatches(tp2).size());    } finally {        m.close();    }}
f6672
0
testIdempotenceWithMultipleInflightsRetriedInOrder
public void kafkatest_f6681_0() throws Exception
{    // Send multiple in flight requests, retry them all one at a time, in the correct order.    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    assertEquals(0, transactionManager.sequenceNumber(tp0).longValue());    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    String nodeId = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(nodeId), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    // Send second ProduceRequest    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    // Send third ProduceRequest    Future<RecordMetadata> request3 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertEquals(3, client.inFlightRequestCount());    assertEquals(3, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertFalse(request1.isDone());    assertFalse(request2.isDone());    assertFalse(request3.isDone());    assertTrue(client.isReady(node, time.milliseconds()));    sendIdempotentProducerResponse(0, tp0, Errors.LEADER_NOT_AVAILABLE, -1L);    // receive response 0    sender.runOnce();    // Queue the fourth request, it shouldn't be sent until the first 3 complete.    Future<RecordMetadata> request4 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    assertEquals(2, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(1, tp0, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1L);    // re send request 1, receive response 2    sender.runOnce();    sendIdempotentProducerResponse(2, tp0, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1L);    // receive response 3    sender.runOnce();    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertEquals(1, client.inFlightRequestCount());    // Do nothing, we are reduced to one in flight request during retries.    sender.runOnce();    // the batch for request 4 shouldn't have been drained, and hence the sequence should not have been incremented.    assertEquals(3, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(0, tp0, Errors.NONE, 0L);    // receive response 1    sender.runOnce();    assertEquals(OptionalInt.of(0), transactionManager.lastAckedSequence(tp0));    assertTrue(request1.isDone());    assertEquals(0, request1.get().offset());    assertFalse(client.hasInFlightRequests());    assertEquals(0, sender.inFlightBatches(tp0).size());    // send request 2;    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    sendIdempotentProducerResponse(1, tp0, Errors.NONE, 1L);    // receive response 2    sender.runOnce();    assertEquals(OptionalInt.of(1), transactionManager.lastAckedSequence(tp0));    assertTrue(request2.isDone());    assertEquals(1, request2.get().offset());    assertFalse(client.hasInFlightRequests());    assertEquals(0, sender.inFlightBatches(tp0).size());    // send request 3    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    sendIdempotentProducerResponse(2, tp0, Errors.NONE, 2L);    // receive response 3, send request 4 since we are out of 'retry' mode.    sender.runOnce();    assertEquals(OptionalInt.of(2), transactionManager.lastAckedSequence(tp0));    assertTrue(request3.isDone());    assertEquals(2, request3.get().offset());    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    sendIdempotentProducerResponse(3, tp0, Errors.NONE, 3L);    // receive response 4    sender.runOnce();    assertEquals(OptionalInt.of(3), transactionManager.lastAckedSequence(tp0));    assertTrue(request4.isDone());    assertEquals(3, request4.get().offset());}
f6681
0
testIdempotenceWithMultipleInflightsWhereFirstFailsFatallyAndSequenceOfFutureBatchesIsAdjusted
public void kafkatest_f6682_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    setupWithTransactionState(transactionManager);    prepareAndReceiveInitProducerId(producerId, Errors.NONE);    assertTrue(transactionManager.hasProducerId());    assertEquals(0, transactionManager.sequenceNumber(tp0).longValue());    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    String nodeId = client.requests().peek().destination();    Node node = new Node(Integer.valueOf(nodeId), "localhost", 0);    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    // Send second ProduceRequest    Future<RecordMetadata> request2 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertEquals(2, client.inFlightRequestCount());    assertEquals(2, transactionManager.sequenceNumber(tp0).longValue());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertFalse(request1.isDone());    assertFalse(request2.isDone());    assertTrue(client.isReady(node, time.milliseconds()));    sendIdempotentProducerResponse(0, tp0, Errors.MESSAGE_TOO_LARGE, -1L);    // receive response 0, should adjust sequences of future batches.    sender.runOnce();    assertFutureFailure(request1, RecordTooLargeException.class);    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(1, tp0, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1L);    // receive response 1    sender.runOnce();    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    assertEquals(0, client.inFlightRequestCount());    // resend request 1    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(OptionalInt.empty(), transactionManager.lastAckedSequence(tp0));    sendIdempotentProducerResponse(0, tp0, Errors.NONE, 0L);    // receive response 1    sender.runOnce();    assertEquals(OptionalInt.of(0), transactionManager.lastAckedSequence(tp0));    assertEquals(0, client.inFlightRequestCount());    assertTrue(request1.isDone());    assertEquals(0, request2.get().offset());}
f6682
0
testCloseWithProducerIdReset
public void kafkatest_f6691_0() throws Exception
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));    setupWithTransactionState(transactionManager);    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, 10, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // connect and send.    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    Map<TopicPartition, OffsetAndError> responses = new LinkedHashMap<>();    responses.put(tp1, new OffsetAndError(-1, Errors.NOT_LEADER_FOR_PARTITION));    responses.put(tp0, new OffsetAndError(-1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER));    client.respond(produceResponse(responses));    // initiate close    sender.initiateClose();    sender.runOnce();    assertTrue(failedResponse.isDone());    assertFalse("Expected transaction state to be reset upon receiving an OutOfOrderSequenceException", transactionManager.hasProducerId());    TestUtils.waitForCondition(new TestCondition() {        @Override        public boolean conditionMet() {            prepareInitProducerResponse(Errors.NONE, producerId + 1, (short) 1);            sender.runOnce();            return !accumulator.hasUndrained();        }    }, 5000, "Failed to drain batches");}
f6691
0
conditionMet
public boolean kafkatest_f6692_0()
{    prepareInitProducerResponse(Errors.NONE, producerId + 1, (short) 1);    sender.runOnce();    return !accumulator.hasUndrained();}
f6692
0
sendIdempotentProducerResponse
 void kafkatest_f6701_0(final int expectedSequence, TopicPartition tp, Errors responseError, long responseOffset, long logStartOffset)
{    client.respond(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            ProduceRequest produceRequest = (ProduceRequest) body;            assertTrue(produceRequest.hasIdempotentRecords());            MemoryRecords records = produceRequest.partitionRecordsOrFail().get(tp0);            Iterator<MutableRecordBatch> batchIterator = records.batches().iterator();            RecordBatch firstBatch = batchIterator.next();            assertFalse(batchIterator.hasNext());            assertEquals(expectedSequence, firstBatch.baseSequence());            return true;        }    }, produceResponse(tp, responseOffset, responseError, 0, logStartOffset));}
f6701
0
matches
public boolean kafkatest_f6702_0(AbstractRequest body)
{    ProduceRequest produceRequest = (ProduceRequest) body;    assertTrue(produceRequest.hasIdempotentRecords());    MemoryRecords records = produceRequest.partitionRecordsOrFail().get(tp0);    Iterator<MutableRecordBatch> batchIterator = records.batches().iterator();    RecordBatch firstBatch = batchIterator.next();    assertFalse(batchIterator.hasNext());    assertEquals(expectedSequence, firstBatch.baseSequence());    return true;}
f6702
0
matches
public boolean kafkatest_f6711_0(AbstractRequest body)
{    return body instanceof ProduceRequest && ((ProduceRequest) body).hasIdempotentRecords();}
f6711
0
testSequenceNumberIncrement
public void kafkatest_f6712_0() throws InterruptedException
{    final long producerId = 343434L;    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));    setupWithTransactionState(transactionManager);    int maxRetries = 10;    Metrics m = new Metrics();    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);    Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries, senderMetrics, time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            if (body instanceof ProduceRequest) {                ProduceRequest request = (ProduceRequest) body;                MemoryRecords records = request.partitionRecordsOrFail().get(tp0);                Iterator<MutableRecordBatch> batchIterator = records.batches().iterator();                assertTrue(batchIterator.hasNext());                RecordBatch batch = batchIterator.next();                assertFalse(batchIterator.hasNext());                assertEquals(0, batch.baseSequence());                assertEquals(producerId, batch.producerId());                assertEquals(0, batch.producerEpoch());                return true;            }            return false;        }    }, produceResponse(tp0, 0, Errors.NONE, 0));    // connect.    sender.runOnce();    // send.    sender.runOnce();    // receive response    sender.runOnce();    assertTrue(responseFuture.isDone());    assertEquals(OptionalInt.of(0), transactionManager.lastAckedSequence(tp0));    assertEquals(1L, (long) transactionManager.sequenceNumber(tp0));}
f6712
0
testWhenFirstBatchExpireNoSendSecondBatchIfGuaranteeOrder
public void kafkatest_f6721_0() throws InterruptedException
{    long deliveryTimeoutMs = 1500L;    setupWithTransactionState(null, true, null);    // Send first ProduceRequest    accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);    // send request    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    time.sleep(deliveryTimeoutMs / 2);    // Send second ProduceRequest    accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false);    // must not send request because the partition is muted    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());    // expire the first batch only    time.sleep(deliveryTimeoutMs / 2);    client.respond(produceResponse(tp0, 0L, Errors.NONE, 0, 0L));    // receive response (offset=0)    sender.runOnce();    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());    // Drain the second request only this time    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    assertEquals(1, sender.inFlightBatches(tp0).size());}
f6721
0
testExpiredBatchDoesNotRetry
public void kafkatest_f6722_0() throws Exception
{    long deliverTimeoutMs = 1500L;    setupWithTransactionState(null, false, null);    // Send first ProduceRequest    Future<RecordMetadata> request1 = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    // send request    sender.runOnce();    assertEquals(1, client.inFlightRequestCount());    time.sleep(deliverTimeoutMs);    Map<TopicPartition, ProduceResponse.PartitionResponse> responseMap = new HashMap<>();    responseMap.put(tp0, new ProduceResponse.PartitionResponse(Errors.NONE, 0L, 0L, 0L));    // return a retriable error    client.respond(produceResponse(tp0, -1, Errors.NOT_LEADER_FOR_PARTITION, -1));    // expire the batch    sender.runOnce();    assertTrue(request1.isDone());    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());    // receive first response and do not reenqueue.    sender.runOnce();    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());    // run again and must not send anything.    sender.runOnce();    assertEquals(0, client.inFlightRequestCount());    assertEquals(0, sender.inFlightBatches(tp0).size());}
f6722
0
matches
public boolean kafkatest_f6731_0(AbstractRequest body)
{    if (body instanceof EndTxnRequest) {        assertSame(requiredResult, ((EndTxnRequest) body).command());        matched = true;        return true;    } else {        return false;    }}
f6731
0
allocate
public ByteBuffer kafkatest_f6732_0(int size, long maxTimeToBlockMs) throws InterruptedException
{    ByteBuffer buffer = super.allocate(size, maxTimeToBlockMs);    allocatedBuffers.put(buffer, Boolean.TRUE);    return buffer;}
f6732
0
setupWithTransactionState
private void kafkatest_f6741_0(TransactionManager transactionManager, boolean guaranteeOrder, BufferPool customPool)
{    int deliveryTimeoutMs = 1500;    long totalSize = 1024 * 1024;    String metricGrpName = "producer-metrics";    MetricConfig metricConfig = new MetricConfig().tags(Collections.singletonMap("client-id", CLIENT_ID));    this.metrics = new Metrics(metricConfig, time);    BufferPool pool = (customPool == null) ? new BufferPool(totalSize, batchSize, metrics, time, metricGrpName) : customPool;    this.accumulator = new RecordAccumulator(logContext, batchSize, CompressionType.NONE, 0, 0L, deliveryTimeoutMs, metrics, metricGrpName, time, apiVersions, transactionManager, pool);    this.senderMetricsRegistry = new SenderMetricsRegistry(this.metrics);    this.sender = new Sender(logContext, this.client, this.metadata, this.accumulator, guaranteeOrder, MAX_REQUEST_SIZE, ACKS_ALL, Integer.MAX_VALUE, this.senderMetricsRegistry, this.time, REQUEST_TIMEOUT, RETRY_BACKOFF_MS, transactionManager, apiVersions);    metadata.add("test");    this.client.updateMetadata(TestUtils.metadataUpdateWith(1, Collections.singletonMap("test", 2)));}
f6741
0
assertSendFailure
private void kafkatest_f6742_0(Class<? extends RuntimeException> expectedError) throws Exception
{    Future<RecordMetadata> future = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), null, null, MAX_BLOCK_TIMEOUT, false).future;    sender.runOnce();    assertTrue(future.isDone());    try {        future.get();        fail("Future should have raised " + expectedError.getSimpleName());    } catch (ExecutionException e) {        assertTrue(expectedError.isAssignableFrom(e.getCause().getClass()));    }}
f6742
0
setup
public void kafkatest_f6751_0()
{    Map<String, String> metricTags = new LinkedHashMap<>();    metricTags.put("client-id", CLIENT_ID);    int batchSize = 16 * 1024;    int deliveryTimeoutMs = 3000;    long totalSize = 1024 * 1024;    String metricGrpName = "producer-metrics";    MetricConfig metricConfig = new MetricConfig().tags(metricTags);    this.brokerNode = new Node(0, "localhost", 2211);    this.transactionManager = new TransactionManager(logContext, transactionalId, transactionTimeoutMs, DEFAULT_RETRY_BACKOFF_MS);    Metrics metrics = new Metrics(metricConfig, time);    SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(metrics);    this.accumulator = new RecordAccumulator(logContext, batchSize, CompressionType.NONE, 0, 0L, deliveryTimeoutMs, metrics, metricGrpName, time, apiVersions, transactionManager, new BufferPool(totalSize, batchSize, metrics, time, metricGrpName));    this.sender = new Sender(logContext, this.client, this.metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, MAX_RETRIES, senderMetrics, this.time, REQUEST_TIMEOUT, 50, transactionManager, apiVersions);    this.metadata.add("test");    this.client.updateMetadata(TestUtils.metadataUpdateWith(1, singletonMap("test", 2)));}
f6751
0
testSenderShutdownWithPendingTransactions
public void kafkatest_f6752_0() throws Exception
{    long pid = 13131L;    short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    FutureRecordMetadata sendFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    prepareAddPartitionsToTxn(tp0, Errors.NONE);    prepareProduceResponse(Errors.NONE, pid, epoch);    sender.initiateClose();    sender.runOnce();    TransactionalRequestResult result = transactionManager.beginCommit();    sender.runOnce();    prepareEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertTrue(result.isCompleted());    sender.run();    assertTrue(sendFuture.isDone());}
f6752
0
testHasOngoingTransactionSuccessfulCommit
public void kafkatest_f6761_0()
{    long pid = 13131L;    short epoch = 1;    TopicPartition partition = new TopicPartition("foo", 0);    assertFalse(transactionManager.hasOngoingTransaction());    doInitTransactions(pid, epoch);    assertFalse(transactionManager.hasOngoingTransaction());    transactionManager.beginTransaction();    assertTrue(transactionManager.hasOngoingTransaction());    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(partition);    assertTrue(transactionManager.hasOngoingTransaction());    prepareAddPartitionsToTxn(partition, Errors.NONE);    sender.runOnce();    transactionManager.beginCommit();    assertTrue(transactionManager.hasOngoingTransaction());    prepareEndTxnResponse(Errors.NONE, TransactionResult.COMMIT, pid, epoch);    sender.runOnce();    assertFalse(transactionManager.hasOngoingTransaction());}
f6761
0
testHasOngoingTransactionAbortableError
public void kafkatest_f6762_0()
{    long pid = 13131L;    short epoch = 1;    TopicPartition partition = new TopicPartition("foo", 0);    assertFalse(transactionManager.hasOngoingTransaction());    doInitTransactions(pid, epoch);    assertFalse(transactionManager.hasOngoingTransaction());    transactionManager.beginTransaction();    assertTrue(transactionManager.hasOngoingTransaction());    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(partition);    assertTrue(transactionManager.hasOngoingTransaction());    prepareAddPartitionsToTxn(partition, Errors.NONE);    sender.runOnce();    transactionManager.transitionToAbortableError(new KafkaException());    assertTrue(transactionManager.hasOngoingTransaction());    transactionManager.beginAbort();    assertTrue(transactionManager.hasOngoingTransaction());    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    sender.runOnce();    assertFalse(transactionManager.hasOngoingTransaction());}
f6762
0
testMaybeAddPartitionToTransactionAfterFatalError
public void kafkatest_f6771_0()
{    long pid = 13131L;    short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.transitionToFatalError(new KafkaException());    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(new TopicPartition("foo", 0));}
f6771
0
testIsSendToPartitionAllowedWithPendingPartitionAfterAbortableError
public void kafkatest_f6772_0()
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    transactionManager.transitionToAbortableError(new KafkaException());    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    assertTrue(transactionManager.hasAbortableError());}
f6772
0
testAdjustSequenceNumbersAfterFatalError
public void kafkatest_f6781_0()
{    final long producerId = 13131L;    final short epoch = 1;    ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(producerId, epoch);    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);    ProducerBatch b1 = writeIdempotentBatchWithValue(transactionManager, tp0, "1");    ProducerBatch b2 = writeIdempotentBatchWithValue(transactionManager, tp0, "2");    ProducerBatch b3 = writeIdempotentBatchWithValue(transactionManager, tp0, "3");    ProducerBatch b4 = writeIdempotentBatchWithValue(transactionManager, tp0, "4");    ProducerBatch b5 = writeIdempotentBatchWithValue(transactionManager, tp0, "5");    assertEquals(5, transactionManager.sequenceNumber(tp0).intValue());    // First batch succeeds    long b1AppendTime = time.milliseconds();    ProduceResponse.PartitionResponse b1Response = new ProduceResponse.PartitionResponse(Errors.NONE, 500L, b1AppendTime, 0L);    b1.done(500L, b1AppendTime, null);    transactionManager.handleCompletedBatch(b1, b1Response);    // Second batch fails with a fatal error. Sequence numbers are adjusted by one for remaining    // inflight batches.    ProduceResponse.PartitionResponse b2Response = new ProduceResponse.PartitionResponse(Errors.MESSAGE_TOO_LARGE, -1, -1, 0L);    assertFalse(transactionManager.canRetry(b2Response, b2));    b2.done(-1L, -1L, Errors.MESSAGE_TOO_LARGE.exception());    transactionManager.handleFailedBatch(b2, Errors.MESSAGE_TOO_LARGE.exception(), true);    assertEquals(4, transactionManager.sequenceNumber(tp0).intValue());    assertEquals(1, b3.baseSequence());    assertEquals(2, b4.baseSequence());    assertEquals(3, b5.baseSequence());    // The remaining batches are doomed to fail, but they can be retried. Expected    // sequence numbers should remain the same.    ProduceResponse.PartitionResponse b3Response = new ProduceResponse.PartitionResponse(Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, -1, -1, 0L);    assertTrue(transactionManager.canRetry(b3Response, b3));    assertEquals(4, transactionManager.sequenceNumber(tp0).intValue());    assertEquals(1, b3.baseSequence());    assertEquals(2, b4.baseSequence());    assertEquals(3, b5.baseSequence());}
f6781
0
testBatchFailureAfterProducerReset
public void kafkatest_f6782_0()
{    // This tests a scenario where the producerId is reset while pending requests are still inflight.    // The returned responses should not update internal state.    final long producerId = 13131L;    final short epoch = 1;    ProducerIdAndEpoch producerIdAndEpoch = new ProducerIdAndEpoch(producerId, epoch);    TransactionManager transactionManager = new TransactionManager();    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);    ProducerBatch b1 = writeIdempotentBatchWithValue(transactionManager, tp0, "1");    ProducerIdAndEpoch updatedProducerIdAndEpoch = new ProducerIdAndEpoch(producerId + 1, epoch);    transactionManager.resetProducerId();    transactionManager.setProducerIdAndEpoch(updatedProducerIdAndEpoch);    ProducerBatch b2 = writeIdempotentBatchWithValue(transactionManager, tp0, "2");    assertEquals(1, transactionManager.sequenceNumber(tp0).intValue());    ProduceResponse.PartitionResponse b1Response = new ProduceResponse.PartitionResponse(Errors.UNKNOWN_PRODUCER_ID, -1, -1, 400L);    assertFalse(transactionManager.canRetry(b1Response, b1));    transactionManager.handleFailedBatch(b1, Errors.UNKNOWN_PRODUCER_ID.exception(), true);    assertEquals(1, transactionManager.sequenceNumber(tp0).intValue());    assertEquals(b2, transactionManager.nextBatchBySequence(tp0));}
f6782
0
testUnsupportedInitTransactions
public void kafkatest_f6791_0()
{    transactionManager.initializeTransactions();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    // InitProducerRequest is queued    sender.runOnce();    // FindCoordinator is queued after peeking InitProducerRequest    sender.runOnce();    assertFalse(transactionManager.hasError());    assertNotNull(transactionManager.coordinator(CoordinatorType.TRANSACTION));    client.prepareUnsupportedVersionResponse(body -> {        InitProducerIdRequest initProducerIdRequest = (InitProducerIdRequest) body;        assertEquals(initProducerIdRequest.data.transactionalId(), transactionalId);        assertEquals(initProducerIdRequest.data.transactionTimeoutMs(), transactionTimeoutMs);        return true;    });    // InitProducerRequest is dequeued    sender.runOnce();    assertTrue(transactionManager.hasFatalError());    assertTrue(transactionManager.lastError() instanceof UnsupportedVersionException);}
f6791
0
testUnsupportedForMessageFormatInTxnOffsetCommit
public void kafkatest_f6792_0()
{    final String consumerGroupId = "consumer";    final long pid = 13131L;    final short epoch = 1;    final TopicPartition tp = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    TransactionalRequestResult sendOffsetsResult = transactionManager.sendOffsetsToTransaction(singletonMap(tp, new OffsetAndMetadata(39L)), consumerGroupId);    prepareAddOffsetsToTxnResponse(Errors.NONE, consumerGroupId, pid, epoch);    // AddOffsetsToTxn Handled, TxnOffsetCommit Enqueued    sender.runOnce();    // FindCoordinator Enqueued    sender.runOnce();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.GROUP, consumerGroupId);    // FindCoordinator Returned    sender.runOnce();    prepareTxnOffsetCommitResponse(consumerGroupId, pid, epoch, singletonMap(tp, Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT));    // TxnOffsetCommit Handled    sender.runOnce();    assertTrue(transactionManager.hasError());    assertTrue(transactionManager.lastError() instanceof UnsupportedForMessageFormatException);    assertTrue(sendOffsetsResult.isCompleted());    assertFalse(sendOffsetsResult.isSuccessful());    assertTrue(sendOffsetsResult.error() instanceof UnsupportedForMessageFormatException);    assertFatalError(UnsupportedForMessageFormatException.class);}
f6792
0
testTransactionalIdAuthorizationFailureInTxnOffsetCommit
public void kafkatest_f6801_0()
{    final String consumerGroupId = "consumer";    final long pid = 13131L;    final short epoch = 1;    final TopicPartition tp = new TopicPartition("foo", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    TransactionalRequestResult sendOffsetsResult = transactionManager.sendOffsetsToTransaction(singletonMap(tp, new OffsetAndMetadata(39L)), consumerGroupId);    prepareAddOffsetsToTxnResponse(Errors.NONE, consumerGroupId, pid, epoch);    // AddOffsetsToTxn Handled, TxnOffsetCommit Enqueued    sender.runOnce();    // FindCoordinator Enqueued    sender.runOnce();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.GROUP, consumerGroupId);    // FindCoordinator Returned    sender.runOnce();    prepareTxnOffsetCommitResponse(consumerGroupId, pid, epoch, singletonMap(tp, Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED));    // TxnOffsetCommit Handled    sender.runOnce();    assertTrue(transactionManager.hasError());    assertTrue(transactionManager.lastError() instanceof TransactionalIdAuthorizationException);    assertTrue(sendOffsetsResult.isCompleted());    assertFalse(sendOffsetsResult.isSuccessful());    assertTrue(sendOffsetsResult.error() instanceof TransactionalIdAuthorizationException);    assertFatalError(TransactionalIdAuthorizationException.class);}
f6801
0
testTopicAuthorizationFailureInAddPartitions
public void kafkatest_f6802_0()
{    final long pid = 13131L;    final short epoch = 1;    final TopicPartition tp0 = new TopicPartition("foo", 0);    final TopicPartition tp1 = new TopicPartition("bar", 0);    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp1);    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(tp0, Errors.TOPIC_AUTHORIZATION_FAILED);    errors.put(tp1, Errors.OPERATION_NOT_ATTEMPTED);    prepareAddPartitionsToTxn(errors);    sender.runOnce();    assertTrue(transactionManager.hasError());    assertTrue(transactionManager.lastError() instanceof TopicAuthorizationException);    assertFalse(transactionManager.isPartitionPendingAdd(tp0));    assertFalse(transactionManager.isPartitionPendingAdd(tp1));    assertFalse(transactionManager.isPartitionAdded(tp0));    assertFalse(transactionManager.isPartitionAdded(tp1));    assertFalse(transactionManager.hasPartitionsToAdd());    TopicAuthorizationException exception = (TopicAuthorizationException) transactionManager.lastError();    assertEquals(singleton(tp0.topic()), exception.unauthorizedTopics());    assertAbortableError(TopicAuthorizationException.class);}
f6802
0
testAllowAbortOnProduceFailure
public void kafkatest_f6811_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    prepareProduceResponse(Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, pid, epoch);    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    // Send AddPartitionsRequest    sender.runOnce();    // Send Produce Request, returns OutOfOrderSequenceException.    sender.runOnce();    TransactionalRequestResult abortResult = transactionManager.beginAbort();    // try to abort    sender.runOnce();    assertTrue(abortResult.isCompleted());    assertTrue(abortResult.isSuccessful());    // make sure we are ready for a transaction now.    assertTrue(transactionManager.isReady());}
f6811
0
testAbortableErrorWhileAbortInProgress
public void kafkatest_f6812_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    // Send AddPartitionsRequest    sender.runOnce();    // Send Produce Request    sender.runOnce();    TransactionalRequestResult abortResult = transactionManager.beginAbort();    assertTrue(transactionManager.isAborting());    assertFalse(transactionManager.hasError());    sendProduceResponse(Errors.OUT_OF_ORDER_SEQUENCE_NUMBER, pid, epoch);    prepareEndTxnResponse(Errors.NONE, TransactionResult.ABORT, pid, epoch);    // receive the produce response    sender.runOnce();    // we do not transition to ABORTABLE_ERROR since we were already aborting    assertTrue(transactionManager.isAborting());    assertFalse(transactionManager.hasError());    // handle the abort    sender.runOnce();    assertTrue(abortResult.isCompleted());    assertTrue(abortResult.isSuccessful());    // make sure we are ready for a transaction now.    assertTrue(transactionManager.isReady());}
f6812
0
testHandlingOfCoordinatorLoadingErrorOnTxnOffsetCommit
public void kafkatest_f6821_0()
{    testRetriableErrorInTxnOffsetCommit(Errors.COORDINATOR_LOAD_IN_PROGRESS);}
f6821
0
testRetriableErrorInTxnOffsetCommit
private void kafkatest_f6822_0(Errors error)
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    offsets.put(tp0, new OffsetAndMetadata(1));    offsets.put(tp1, new OffsetAndMetadata(1));    final String consumerGroupId = "myconsumergroup";    TransactionalRequestResult addOffsetsResult = transactionManager.sendOffsetsToTransaction(offsets, consumerGroupId);    prepareAddOffsetsToTxnResponse(Errors.NONE, consumerGroupId, pid, epoch);    // send AddOffsetsToTxnResult    sender.runOnce();    // The request should complete only after the TxnOffsetCommit completes.    assertFalse(addOffsetsResult.isCompleted());    Map<TopicPartition, Errors> txnOffsetCommitResponse = new HashMap<>();    txnOffsetCommitResponse.put(tp0, Errors.NONE);    txnOffsetCommitResponse.put(tp1, error);    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.GROUP, consumerGroupId);    prepareTxnOffsetCommitResponse(consumerGroupId, pid, epoch, txnOffsetCommitResponse);    assertNull(transactionManager.coordinator(CoordinatorType.GROUP));    // try to send TxnOffsetCommitRequest, but find we don't have a group coordinator.    sender.runOnce();    // send find coordinator for group request    sender.runOnce();    assertNotNull(transactionManager.coordinator(CoordinatorType.GROUP));    assertTrue(transactionManager.hasPendingOffsetCommits());    // send TxnOffsetCommitRequest request.    sender.runOnce();    // The TxnOffsetCommit failed.    assertTrue(transactionManager.hasPendingOffsetCommits());    // We should only be done after both RPCs complete successfully.    assertFalse(addOffsetsResult.isCompleted());    txnOffsetCommitResponse.put(tp1, Errors.NONE);    prepareTxnOffsetCommitResponse(consumerGroupId, pid, epoch, txnOffsetCommitResponse);    // Send TxnOffsetCommitRequest again.    sender.runOnce();    assertTrue(addOffsetsResult.isCompleted());    assertTrue(addOffsetsResult.isSuccessful());}
f6822
0
testTransitionToAbortableErrorOnBatchExpiry
public void kafkatest_f6831_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    Future<RecordMetadata> responseFuture = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(responseFuture.isDone());    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    assertFalse(transactionManager.transactionContainsPartition(tp0));    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    // send addPartitions.    sender.runOnce();    // Check that only addPartitions was sent.    assertTrue(transactionManager.transactionContainsPartition(tp0));    assertTrue(transactionManager.isSendToPartitionAllowed(tp0));    assertFalse(responseFuture.isDone());    // Sleep 10 seconds to make sure that the batches in the queue would be expired if they can't be drained.    time.sleep(10000);    // Disconnect the target node for the pending produce request. This will ensure that sender will try to    // expire the batch.    Node clusterNode = metadata.fetch().nodes().get(0);    client.disconnect(clusterNode.idString());    client.blackout(clusterNode, 100);    // We should try to flush the produce, but expire it instead without sending anything.    sender.runOnce();    assertTrue(responseFuture.isDone());    try {        // make sure the produce was expired.        responseFuture.get();        fail("Expected to get a TimeoutException since the queued ProducerBatch should have been expired");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    assertTrue(transactionManager.hasAbortableError());}
f6831
0
testTransitionToAbortableErrorOnMultipleBatchExpiry
public void kafkatest_f6832_0() throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp1);    Future<RecordMetadata> firstBatchResponse = accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    Future<RecordMetadata> secondBatchResponse = accumulator.append(tp1, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false).future;    assertFalse(firstBatchResponse.isDone());    assertFalse(secondBatchResponse.isDone());    Map<TopicPartition, Errors> partitionErrors = new HashMap<>();    partitionErrors.put(tp0, Errors.NONE);    partitionErrors.put(tp1, Errors.NONE);    prepareAddPartitionsToTxn(partitionErrors);    assertFalse(transactionManager.transactionContainsPartition(tp0));    assertFalse(transactionManager.isSendToPartitionAllowed(tp0));    // send addPartitions.    sender.runOnce();    // Check that only addPartitions was sent.    assertTrue(transactionManager.transactionContainsPartition(tp0));    assertTrue(transactionManager.transactionContainsPartition(tp1));    assertTrue(transactionManager.isSendToPartitionAllowed(tp1));    assertTrue(transactionManager.isSendToPartitionAllowed(tp1));    assertFalse(firstBatchResponse.isDone());    assertFalse(secondBatchResponse.isDone());    // Sleep 10 seconds to make sure that the batches in the queue would be expired if they can't be drained.    time.sleep(10000);    // Disconnect the target node for the pending produce request. This will ensure that sender will try to    // expire the batch.    Node clusterNode = metadata.fetch().nodes().get(0);    client.disconnect(clusterNode.idString());    client.blackout(clusterNode, 100);    // We should try to flush the produce, but expire it instead without sending anything.    sender.runOnce();    assertTrue(firstBatchResponse.isDone());    assertTrue(secondBatchResponse.isDone());    try {        // make sure the produce was expired.        firstBatchResponse.get();        fail("Expected to get a TimeoutException since the queued ProducerBatch should have been expired");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    try {        // make sure the produce was expired.        secondBatchResponse.get();        fail("Expected to get a TimeoutException since the queued ProducerBatch should have been expired");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof TimeoutException);    }    assertTrue(transactionManager.hasAbortableError());}
f6832
0
testRetryCommitTransactionAfterAbortTimeout
public void kafkatest_f6841_0() throws InterruptedException
{    verifyCommitOrAbortTranscationRetriable(TransactionResult.ABORT, TransactionResult.COMMIT);}
f6841
0
verifyCommitOrAbortTranscationRetriable
private void kafkatest_f6842_0(TransactionResult firstTransactionResult, TransactionResult retryTransactionResult) throws InterruptedException
{    final long pid = 13131L;    final short epoch = 1;    doInitTransactions(pid, epoch);    transactionManager.beginTransaction();    transactionManager.failIfNotReadyForSend();    transactionManager.maybeAddPartitionToTransaction(tp0);    accumulator.append(tp0, time.milliseconds(), "key".getBytes(), "value".getBytes(), Record.EMPTY_HEADERS, null, MAX_BLOCK_TIMEOUT, false);    prepareAddPartitionsToTxnResponse(Errors.NONE, tp0, epoch, pid);    prepareProduceResponse(Errors.NONE, pid, epoch);    // send addPartitions.    sender.runOnce();    // send produce request.    sender.runOnce();    TransactionalRequestResult result = firstTransactionResult == TransactionResult.COMMIT ? transactionManager.beginCommit() : transactionManager.beginAbort();    prepareEndTxnResponse(Errors.NONE, firstTransactionResult, pid, epoch, true);    sender.runOnce();    assertFalse(result.isCompleted());    try {        result.await(MAX_BLOCK_TIMEOUT, TimeUnit.MILLISECONDS);        fail("Should have raised TimeoutException");    } catch (TimeoutException e) {    }    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    sender.runOnce();    TransactionalRequestResult retryResult = retryTransactionResult == TransactionResult.COMMIT ? transactionManager.beginCommit() : transactionManager.beginAbort();    // check if cached result is reused.    assertEquals(retryResult, result);    prepareEndTxnResponse(Errors.NONE, retryTransactionResult, pid, epoch, false);    sender.runOnce();    assertTrue(retryResult.isCompleted());    assertFalse(transactionManager.hasOngoingTransaction());}
f6842
0
prepareAddPartitionsToTxnResponse
private void kafkatest_f6851_0(Errors error, final TopicPartition topicPartition, final short epoch, final long pid)
{    client.prepareResponse(addPartitionsRequestMatcher(topicPartition, epoch, pid), new AddPartitionsToTxnResponse(0, singletonMap(topicPartition, error)));}
f6851
0
sendAddPartitionsToTxnResponse
private void kafkatest_f6852_0(Errors error, final TopicPartition topicPartition, final short epoch, final long pid)
{    client.respond(addPartitionsRequestMatcher(topicPartition, epoch, pid), new AddPartitionsToTxnResponse(0, singletonMap(topicPartition, error)));}
f6852
0
doInitTransactions
private void kafkatest_f6861_0(long pid, short epoch)
{    transactionManager.initializeTransactions();    prepareFindCoordinatorResponse(Errors.NONE, false, CoordinatorType.TRANSACTION, transactionalId);    // find coordinator    sender.runOnce();    sender.runOnce();    assertEquals(brokerNode, transactionManager.coordinator(CoordinatorType.TRANSACTION));    prepareInitPidResponse(Errors.NONE, false, pid, epoch);    // get pid.    sender.runOnce();    assertTrue(transactionManager.hasProducerId());}
f6861
0
assertAbortableError
private void kafkatest_f6862_0(Class<? extends RuntimeException> cause)
{    try {        transactionManager.beginCommit();        fail("Should have raised " + cause.getSimpleName());    } catch (KafkaException e) {        assertTrue(cause.isAssignableFrom(e.getCause().getClass()));        assertTrue(transactionManager.hasError());    }    assertTrue(transactionManager.hasError());    transactionManager.beginAbort();    assertFalse(transactionManager.hasError());}
f6862
0
testInterceptorConstructClose
public void kafkatest_f6871_0()
{    try {        Properties props = new Properties();        // test with client ID assigned by KafkaProducer        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");        props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, MockProducerInterceptor.class.getName());        props.setProperty(MockProducerInterceptor.APPEND_STRING_PROP, "something");        KafkaProducer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());        assertEquals(1, MockProducerInterceptor.INIT_COUNT.get());        assertEquals(0, MockProducerInterceptor.CLOSE_COUNT.get());        // Cluster metadata will only be updated on calling onSend.        Assert.assertNull(MockProducerInterceptor.CLUSTER_META.get());        producer.close();        assertEquals(1, MockProducerInterceptor.INIT_COUNT.get());        assertEquals(1, MockProducerInterceptor.CLOSE_COUNT.get());    } finally {        // cleanup since we are using mutable static variables in MockProducerInterceptor        MockProducerInterceptor.resetCounters();    }}
f6871
0
testPartitionerClose
public void kafkatest_f6872_0()
{    try {        Properties props = new Properties();        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");        MockPartitioner.resetCounters();        props.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG, MockPartitioner.class.getName());        KafkaProducer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());        assertEquals(1, MockPartitioner.INIT_COUNT.get());        assertEquals(0, MockPartitioner.CLOSE_COUNT.get());        producer.close();        assertEquals(1, MockPartitioner.INIT_COUNT.get());        assertEquals(1, MockPartitioner.CLOSE_COUNT.get());    } finally {        // cleanup since we are using mutable static variables in MockPartitioner        MockPartitioner.resetCounters();    }}
f6872
0
testMetadataWithPartitionOutOfRange
public void kafkatest_f6881_0() throws Exception
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    configs.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, 60000);    // Create a record with a partition higher than the initial (outdated) partition range    ProducerRecord<String, String> record = new ProducerRecord<>(topic, 2, null, "value");    ProducerMetadata metadata = mock(ProducerMetadata.class);    MockTime mockTime = new MockTime();    when(metadata.fetch()).thenReturn(onePartitionCluster, onePartitionCluster, threePartitionCluster);    KafkaProducer<String, String> producer = new KafkaProducer<String, String>(configs, new StringSerializer(), new StringSerializer(), metadata, new MockClient(Time.SYSTEM, metadata), null, mockTime) {        @Override        Sender newSender(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata) {            // give Sender its own Metadata instance so that we can isolate Metadata calls from KafkaProducer            return super.newSender(logContext, kafkaClient, newMetadata(0, 100_000));        }    };    // One request update if metadata is available but outdated for the given record    producer.send(record);    verify(metadata, times(2)).requestUpdate();    verify(metadata, times(2)).awaitUpdate(anyInt(), anyLong());    verify(metadata, times(3)).fetch();    producer.close(Duration.ofMillis(0));}
f6881
0
newSender
 Sender kafkatest_f6882_0(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata)
{    // give Sender its own Metadata instance so that we can isolate Metadata calls from KafkaProducer    return super.newSender(logContext, kafkaClient, newMetadata(0, 100_000));}
f6882
0
testInterceptorPartitionSetOnTooLargeRecord
public void kafkatest_f6891_0()
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9999");    configs.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, "1");    String topic = "topic";    ProducerRecord<String, String> record = new ProducerRecord<>(topic, "value");    ProducerMetadata metadata = newMetadata(0, 90000);    metadata.add(topic);    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap(topic, 1));    metadata.update(initialUpdateResponse, Time.SYSTEM.milliseconds());    // it is safe to suppress, since this is a mock class    @SuppressWarnings("unchecked")    ProducerInterceptors<String, String> interceptors = mock(ProducerInterceptors.class);    KafkaProducer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, null, interceptors, Time.SYSTEM);    when(interceptors.onSend(any())).then(invocation -> invocation.getArgument(0));    producer.send(record);    verify(interceptors).onSend(record);    verify(interceptors).onSendError(eq(record), notNull(), notNull());    producer.close(Duration.ofMillis(0));}
f6891
0
testPartitionsForWithNullTopic
public void kafkatest_f6892_0()
{    Properties props = new Properties();    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");    try (KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(props, new ByteArraySerializer(), new ByteArraySerializer())) {        assertThrows(NullPointerException.class, () -> producer.partitionsFor(null));    }}
f6892
0
testCloseIsForcedOnPendingInitProducerId
public void kafkatest_f6901_0() throws InterruptedException
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");    configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "this-is-a-transactional-id");    Time time = new MockTime();    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap("testTopic", 1));    ProducerMetadata metadata = newMetadata(0, Long.MAX_VALUE);    metadata.update(initialUpdateResponse, time.milliseconds());    MockClient client = new MockClient(time, metadata);    Producer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, client, null, time);    ExecutorService executorService = Executors.newSingleThreadExecutor();    CountDownLatch assertionDoneLatch = new CountDownLatch(1);    client.prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, host1));    executorService.submit(() -> {        assertThrows(KafkaException.class, producer::initTransactions);        assertionDoneLatch.countDown();    });    client.waitForRequests(1, 2000);    producer.close(Duration.ofMillis(1000));    assertionDoneLatch.await(5000, TimeUnit.MILLISECONDS);}
f6901
0
testCloseIsForcedOnPendingAddOffsetRequest
public void kafkatest_f6902_0() throws InterruptedException
{    Map<String, Object> configs = new HashMap<>();    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9000");    configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "this-is-a-transactional-id");    Time time = new MockTime();    MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap("testTopic", 1));    ProducerMetadata metadata = newMetadata(0, Long.MAX_VALUE);    metadata.update(initialUpdateResponse, time.milliseconds());    MockClient client = new MockClient(time, metadata);    Producer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer(), metadata, client, null, time);    ExecutorService executorService = Executors.newSingleThreadExecutor();    CountDownLatch assertionDoneLatch = new CountDownLatch(1);    client.prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, host1));    executorService.submit(() -> {        assertThrows(KafkaException.class, producer::initTransactions);        assertionDoneLatch.countDown();    });    client.waitForRequests(1, 2000);    producer.close(Duration.ofMillis(1000));    assertionDoneLatch.await(5000, TimeUnit.MILLISECONDS);}
f6902
0
shouldThrowOnBeginTransactionIfTransactionsNotInitialized
public void kafkatest_f6911_0()
{    buildMockProducer(true);    producer.beginTransaction();}
f6911
0
shouldBeginTransactions
public void kafkatest_f6912_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    assertTrue(producer.transactionInFlight());}
f6912
0
shouldThrowOnAbortTransactionIfNoTransactionGotStarted
public void kafkatest_f6921_0()
{    buildMockProducer(true);    producer.initTransactions();    try {        producer.abortTransaction();        fail("Should have thrown as producer has no open transaction");    } catch (IllegalStateException e) {    }}
f6921
0
shouldAbortEmptyTransaction
public void kafkatest_f6922_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    producer.abortTransaction();    assertFalse(producer.transactionInFlight());    assertTrue(producer.transactionAborted());    assertFalse(producer.transactionCommitted());}
f6922
0
shouldDropMessagesOnAbortIfTransactionsAreEnabled
public void kafkatest_f6931_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    producer.send(record1);    producer.send(record2);    producer.abortTransaction();    assertTrue(producer.history().isEmpty());    producer.beginTransaction();    producer.commitTransaction();    assertTrue(producer.history().isEmpty());}
f6931
0
shouldThrowOnAbortForNonAutoCompleteIfTransactionsAreEnabled
public void kafkatest_f6932_0() throws Exception
{    buildMockProducer(false);    producer.initTransactions();    producer.beginTransaction();    Future<RecordMetadata> md1 = producer.send(record1);    assertFalse(md1.isDone());    producer.abortTransaction();    assertTrue(md1.isDone());}
f6932
0
shouldPreserveCommittedConsumerGroupsOffsetsOnAbortIfTransactionsAreEnabled
public void kafkatest_f6941_0()
{    buildMockProducer(true);    producer.initTransactions();    producer.beginTransaction();    String group = "g";    Map<TopicPartition, OffsetAndMetadata> groupCommit = new HashMap<TopicPartition, OffsetAndMetadata>() {        {            put(new TopicPartition(topic, 0), new OffsetAndMetadata(42L, null));            put(new TopicPartition(topic, 1), new OffsetAndMetadata(73L, null));        }    };    producer.sendOffsetsToTransaction(groupCommit, group);    producer.commitTransaction();    producer.beginTransaction();    producer.abortTransaction();    Map<String, Map<TopicPartition, OffsetAndMetadata>> expectedResult = new HashMap<>();    expectedResult.put(group, groupCommit);    assertThat(producer.consumerGroupOffsetsHistory(), equalTo(Collections.singletonList(expectedResult)));}
f6941
0
shouldThrowOnInitTransactionIfProducerIsClosed
public void kafkatest_f6942_0()
{    buildMockProducer(true);    producer.close();    try {        producer.initTransactions();        fail("Should have thrown as producer is already closed");    } catch (IllegalStateException e) {    }}
f6942
0
shouldBeFlushedWithAutoCompleteIfBufferedRecords
public void kafkatest_f6951_0()
{    buildMockProducer(true);    producer.send(record1);    assertTrue(producer.flushed());}
f6951
0
shouldNotBeFlushedWithNoAutoCompleteIfBufferedRecords
public void kafkatest_f6952_0()
{    buildMockProducer(false);    producer.send(record1);    assertFalse(producer.flushed());}
f6952
0
testError
public void kafkatest_f6961_0() throws Exception
{    FutureRecordMetadata future = new FutureRecordMetadata(asyncRequest(baseOffset, new CorruptRecordException(), 50L), relOffset, RecordBatch.NO_TIMESTAMP, 0L, 0, 0, Time.SYSTEM);    future.get();}
f6961
0
testBlocking
public void kafkatest_f6962_0() throws Exception
{    FutureRecordMetadata future = new FutureRecordMetadata(asyncRequest(baseOffset, null, 50L), relOffset, RecordBatch.NO_TIMESTAMP, 0L, 0, 0, Time.SYSTEM);    assertEquals(baseOffset + relOffset, future.get().offset());}
f6962
0
testMatching
public void kafkatest_f6971_0()
{    assertEquals(ACL1, ACL1);    final AclBinding acl1Copy = new AclBinding(new ResourcePattern(ResourceType.TOPIC, "mytopic", PatternType.LITERAL), new AccessControlEntry("User:ANONYMOUS", "", AclOperation.ALL, AclPermissionType.ALLOW));    assertEquals(ACL1, acl1Copy);    assertEquals(acl1Copy, ACL1);    assertEquals(ACL2, ACL2);    assertNotEquals(ACL1, ACL2);    assertNotEquals(ACL2, ACL1);    assertTrue(AclBindingFilter.ANY.matches(ACL1));    assertNotEquals(AclBindingFilter.ANY, ACL1);    assertTrue(AclBindingFilter.ANY.matches(ACL2));    assertNotEquals(AclBindingFilter.ANY, ACL2);    assertTrue(AclBindingFilter.ANY.matches(ACL3));    assertNotEquals(AclBindingFilter.ANY, ACL3);    assertEquals(AclBindingFilter.ANY, AclBindingFilter.ANY);    assertTrue(ANY_ANONYMOUS.matches(ACL1));    assertNotEquals(ANY_ANONYMOUS, ACL1);    assertFalse(ANY_ANONYMOUS.matches(ACL2));    assertNotEquals(ANY_ANONYMOUS, ACL2);    assertTrue(ANY_ANONYMOUS.matches(ACL3));    assertNotEquals(ANY_ANONYMOUS, ACL3);    assertFalse(ANY_DENY.matches(ACL1));    assertFalse(ANY_DENY.matches(ACL2));    assertTrue(ANY_DENY.matches(ACL3));    assertTrue(ANY_MYTOPIC.matches(ACL1));    assertTrue(ANY_MYTOPIC.matches(ACL2));    assertFalse(ANY_MYTOPIC.matches(ACL3));    assertTrue(ANY_ANONYMOUS.matches(UNKNOWN_ACL));    assertTrue(ANY_DENY.matches(UNKNOWN_ACL));    assertEquals(UNKNOWN_ACL, UNKNOWN_ACL);    assertFalse(ANY_MYTOPIC.matches(UNKNOWN_ACL));}
f6971
0
testUnknowns
public void kafkatest_f6972_0()
{    assertFalse(ACL1.isUnknown());    assertFalse(ACL2.isUnknown());    assertFalse(ACL3.isUnknown());    assertFalse(ANY_ANONYMOUS.isUnknown());    assertFalse(ANY_DENY.isUnknown());    assertFalse(ANY_MYTOPIC.isUnknown());    assertTrue(UNKNOWN_ACL.isUnknown());}
f6972
0
testName
public void kafkatest_f6981_0() throws Exception
{    for (AclOperationTestInfo info : INFOS) {        assertEquals("AclOperation.fromString(" + info.name + ") was supposed to be " + info.operation, info.operation, AclOperation.fromString(info.name));    }    assertEquals(AclOperation.UNKNOWN, AclOperation.fromString("something"));}
f6981
0
testExhaustive
public void kafkatest_f6982_0() throws Exception
{    assertEquals(INFOS.length, AclOperation.values().length);    for (int i = 0; i < INFOS.length; i++) {        assertEquals(INFOS[i].operation, AclOperation.values()[i]);    }}
f6982
0
shouldNotMatchIfDifferentNameCase
public void kafkatest_f6991_0()
{    assertFalse(new ResourcePatternFilter(TOPIC, "NAME", LITERAL).matches(new ResourcePattern(TOPIC, "Name", LITERAL)));}
f6991
0
shouldNotMatchIfDifferentPatternType
public void kafkatest_f6992_0()
{    assertFalse(new ResourcePatternFilter(TOPIC, "Name", LITERAL).matches(new ResourcePattern(TOPIC, "Name", PREFIXED)));}
f6992
0
shouldMatchLiteralWildcardIfExactMatch
public void kafkatest_f7001_0()
{    assertTrue(new ResourcePatternFilter(TOPIC, "*", LITERAL).matches(new ResourcePattern(TOPIC, "*", LITERAL)));}
f7001
0
shouldNotMatchLiteralWildcardAgainstOtherName
public void kafkatest_f7002_0()
{    assertFalse(new ResourcePatternFilter(TOPIC, "Name", LITERAL).matches(new ResourcePattern(TOPIC, "*", LITERAL)));}
f7002
0
shouldThrowIfResourceTypeIsAny
public void kafkatest_f7011_0()
{    new ResourcePattern(ResourceType.ANY, "name", PatternType.LITERAL);}
f7011
0
shouldThrowIfPatternTypeIsMatch
public void kafkatest_f7012_0()
{    new ResourcePattern(ResourceType.TOPIC, "name", PatternType.MATCH);}
f7012
0
testEmptyList
public void kafkatest_f7021_0()
{    AbstractConfig conf;    ConfigDef configDef = new ConfigDef().define("a", Type.LIST, "", new ConfigDef.NonNullValidator(), Importance.HIGH, "doc");    conf = new AbstractConfig(configDef, Collections.emptyMap());    assertEquals(Collections.emptyList(), conf.getList("a"));    conf = new AbstractConfig(configDef, Collections.singletonMap("a", ""));    assertEquals(Collections.emptyList(), conf.getList("a"));    conf = new AbstractConfig(configDef, Collections.singletonMap("a", "b,c,d"));    assertEquals(Arrays.asList("b", "c", "d"), conf.getList("a"));}
f7021
0
testOriginalsWithPrefix
public void kafkatest_f7022_0()
{    Properties props = new Properties();    props.put("foo.bar", "abc");    props.put("setting", "def");    TestConfig config = new TestConfig(props);    Map<String, Object> originalsWithPrefix = config.originalsWithPrefix("foo.");    assertTrue(config.unused().contains("foo.bar"));    originalsWithPrefix.get("bar");    assertFalse(config.unused().contains("foo.bar"));    Map<String, Object> expected = new HashMap<>();    expected.put("bar", "abc");    assertEquals(expected, originalsWithPrefix);}
f7022
0
convertPropertiesToMap
public Map<String, ?> kafkatest_f7031_0(Map<?, ?> props)
{    for (Map.Entry<?, ?> entry : props.entrySet()) {        if (!(entry.getKey() instanceof String))            throw new ConfigException(entry.getKey().toString(), entry.getValue(), "Key must be a string.");    }    return (Map<String, ?>) props;}
f7031
0
testOriginalsWithConfigProvidersProps
public void kafkatest_f7032_0()
{    Properties props = new Properties();    // Test Case: Valid Test Case for ConfigProviders as part of config.properties    props.put("config.providers", "file");    props.put("config.providers.file.class", MockFileConfigProvider.class.getName());    props.put("prefix.ssl.truststore.location.number", 5);    props.put("sasl.kerberos.service.name", "service name");    props.put("sasl.kerberos.key", "${file:/usr/kerberos:key}");    props.put("sasl.kerberos.password", "${file:/usr/kerberos:password}");    TestIndirectConfigResolution config = new TestIndirectConfigResolution(props);    assertEquals(config.originals().get("sasl.kerberos.key"), "testKey");    assertEquals(config.originals().get("sasl.kerberos.password"), "randomPassword");    assertEquals(config.originals().get("prefix.ssl.truststore.location.number"), 5);    assertEquals(config.originals().get("sasl.kerberos.service.name"), "service name");}
f7032
0
checkInstances
 void kafkatest_f7041_0(Class<?> expectedClassPropClass, Class<?>... expectedListPropClasses)
{    assertEquals(expectedClassPropClass, getConfiguredInstance("class.prop", MetricsReporter.class).getClass());    List<?> list = getConfiguredInstances("list.prop", MetricsReporter.class);    for (int i = 0; i < list.size(); i++) assertEquals(expectedListPropClasses[i], list.get(i).getClass());}
f7041
0
testOverrides
 static void kafkatest_f7042_0()
{    ClassTestConfig testConfig1 = new ClassTestConfig(RESTRICTED_CLASS, Arrays.asList(VISIBLE_CLASS, RESTRICTED_CLASS));    testConfig1.checkInstances(RESTRICTED_CLASS, VISIBLE_CLASS, RESTRICTED_CLASS);    ClassTestConfig testConfig2 = new ClassTestConfig(RESTRICTED_CLASS.getName(), Arrays.asList(VISIBLE_CLASS.getName(), RESTRICTED_CLASS.getName()));    testConfig2.checkInstances(RESTRICTED_CLASS, VISIBLE_CLASS, RESTRICTED_CLASS);    ClassTestConfig testConfig3 = new ClassTestConfig(RESTRICTED_CLASS.getName(), VISIBLE_CLASS.getName() + "," + RESTRICTED_CLASS.getName());    testConfig3.checkInstances(RESTRICTED_CLASS, VISIBLE_CLASS, RESTRICTED_CLASS);}
f7042
0
testBadInputs
public void kafkatest_f7051_0()
{    testBadInputs(Type.INT, "hello", "42.5", 42.5, Long.MAX_VALUE, Long.toString(Long.MAX_VALUE), new Object());    testBadInputs(Type.LONG, "hello", "42.5", Long.toString(Long.MAX_VALUE) + "00", new Object());    testBadInputs(Type.DOUBLE, "hello", new Object());    testBadInputs(Type.STRING, new Object());    testBadInputs(Type.LIST, 53, new Object());    testBadInputs(Type.BOOLEAN, "hello", "truee", "fals");    testBadInputs(Type.CLASS, "ClassDoesNotExist");}
f7051
0
testBadInputs
private void kafkatest_f7052_0(Type type, Object... values)
{    for (Object value : values) {        Map<String, Object> m = new HashMap<String, Object>();        m.put("name", value);        ConfigDef def = new ConfigDef().define("name", type, Importance.HIGH, "docs");        try {            def.parse(m);            fail("Expected a config exception on bad input for value " + value);        } catch (ConfigException e) {        // this is good        }    }}
f7052
0
testValidate
public void kafkatest_f7061_0()
{    Map<String, ConfigValue> expected = new HashMap<>();    String errorMessageB = "Missing required configuration \"b\" which has no default value.";    String errorMessageC = "Missing required configuration \"c\" which has no default value.";    ConfigValue configA = new ConfigValue("a", 1, Arrays.<Object>asList(1, 2, 3), Collections.<String>emptyList());    ConfigValue configB = new ConfigValue("b", null, Arrays.<Object>asList(4, 5), Arrays.asList(errorMessageB, errorMessageB));    ConfigValue configC = new ConfigValue("c", null, Arrays.<Object>asList(4, 5), Arrays.asList(errorMessageC));    ConfigValue configD = new ConfigValue("d", 10, Arrays.<Object>asList(1, 2, 3), Collections.<String>emptyList());    expected.put("a", configA);    expected.put("b", configB);    expected.put("c", configC);    expected.put("d", configD);    ConfigDef def = new ConfigDef().define("a", Type.INT, Importance.HIGH, "docs", "group", 1, Width.SHORT, "a", Arrays.asList("b", "c"), new IntegerRecommender(false)).define("b", Type.INT, Importance.HIGH, "docs", "group", 2, Width.SHORT, "b", new IntegerRecommender(true)).define("c", Type.INT, Importance.HIGH, "docs", "group", 3, Width.SHORT, "c", new IntegerRecommender(true)).define("d", Type.INT, Importance.HIGH, "docs", "group", 4, Width.SHORT, "d", Arrays.asList("b"), new IntegerRecommender(false));    Map<String, String> props = new HashMap<>();    props.put("a", "1");    props.put("d", "10");    List<ConfigValue> configs = def.validate(props);    for (ConfigValue config : configs) {        String name = config.name();        ConfigValue expectedConfig = expected.get(name);        assertEquals(expectedConfig, config);    }}
f7061
0
testValidateMissingConfigKey
public void kafkatest_f7062_0()
{    Map<String, ConfigValue> expected = new HashMap<>();    String errorMessageB = "Missing required configuration \"b\" which has no default value.";    String errorMessageC = "Missing required configuration \"c\" which has no default value.";    String errorMessageD = "d is referred in the dependents, but not defined.";    ConfigValue configA = new ConfigValue("a", 1, Arrays.<Object>asList(1, 2, 3), Collections.<String>emptyList());    ConfigValue configB = new ConfigValue("b", null, Arrays.<Object>asList(4, 5), Arrays.asList(errorMessageB));    ConfigValue configC = new ConfigValue("c", null, Arrays.<Object>asList(4, 5), Arrays.asList(errorMessageC));    ConfigValue configD = new ConfigValue("d", null, Collections.emptyList(), Arrays.asList(errorMessageD));    configD.visible(false);    expected.put("a", configA);    expected.put("b", configB);    expected.put("c", configC);    expected.put("d", configD);    ConfigDef def = new ConfigDef().define("a", Type.INT, Importance.HIGH, "docs", "group", 1, Width.SHORT, "a", Arrays.asList("b", "c", "d"), new IntegerRecommender(false)).define("b", Type.INT, Importance.HIGH, "docs", "group", 2, Width.SHORT, "b", new IntegerRecommender(true)).define("c", Type.INT, Importance.HIGH, "docs", "group", 3, Width.SHORT, "c", new IntegerRecommender(true));    Map<String, String> props = new HashMap<>();    props.put("a", "1");    List<ConfigValue> configs = def.validate(props);    for (ConfigValue config : configs) {        String name = config.name();        ConfigValue expectedConfig = expected.get(name);        assertEquals(expectedConfig, config);    }}
f7062
0
visible
public boolean kafkatest_f7071_0(String name, Map<String, Object> parsedConfig)
{    return true;}
f7071
0
testValidators
private void kafkatest_f7072_0(Type type, Validator validator, Object defaultVal, Object[] okValues, Object[] badValues)
{    ConfigDef def = new ConfigDef().define("name", type, defaultVal, validator, Importance.HIGH, "docs");    for (Object value : okValues) {        Map<String, Object> m = new HashMap<String, Object>();        m.put("name", value);        def.parse(m);    }    for (Object value : badValues) {        Map<String, Object> m = new HashMap<String, Object>();        m.put("name", value);        try {            def.parse(m);            fail("Expected a config exception due to invalid value " + value);        } catch (ConfigException e) {        // this is good        }    }}
f7072
0
testConvertValueToStringPassword
public void kafkatest_f7081_0()
{    assertEquals(Password.HIDDEN, ConfigDef.convertToString(new Password("foobar"), Type.PASSWORD));    assertEquals("foobar", ConfigDef.convertToString("foobar", Type.PASSWORD));    assertNull(ConfigDef.convertToString(null, Type.PASSWORD));}
f7081
0
testConvertValueToStringList
public void kafkatest_f7082_0()
{    assertEquals("a,bc,d", ConfigDef.convertToString(Arrays.asList("a", "bc", "d"), Type.LIST));    assertNull(ConfigDef.convertToString(null, Type.LIST));}
f7082
0
testReplaceVariable
public void kafkatest_f7091_0() throws Exception
{    ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, "${test:testPath:testKey}"));    Map<String, String> data = result.data();    Map<String, Long> ttls = result.ttls();    assertEquals(TEST_RESULT, data.get(MY_KEY));    assertTrue(ttls.isEmpty());}
f7091
0
testReplaceVariableWithTTL
public void kafkatest_f7092_0() throws Exception
{    ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, "${test:testPath:testKeyWithTTL}"));    Map<String, String> data = result.data();    Map<String, Long> ttls = result.ttls();    assertEquals(TEST_RESULT_WITH_TTL, data.get(MY_KEY));    assertEquals(1L, ttls.get(TEST_PATH).longValue());}
f7092
0
setup
public void kafkatest_f7103_0()
{    configProvider = new TestFileConfigProvider();}
f7103
0
testGetAllKeysAtPath
public void kafkatest_f7104_0() throws Exception
{    ConfigData configData = configProvider.get("dummy");    Map<String, String> result = new HashMap<>();    result.put("testKey", "testResult");    result.put("testKey2", "testResult2");    assertEquals(result, configData.data());    assertEquals(null, configData.ttl());}
f7104
0
configure
public void kafkatest_f7113_0(Map<String, ?> configs)
{    this.vaultConfigs = configs;    configured = true;}
f7113
0
configured
public boolean kafkatest_f7114_0()
{    return configured;}
f7114
0
testSaslLoginRefreshMinPeriodSecondsMaxValueIsReallyMaximum
public void kafkatest_f7123_0()
{    Map<Object, Object> props = new HashMap<>();    props.put(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS, "901");    new ConfigDef().withClientSaslSupport().parse(props);}
f7123
0
testSaslLoginRefreshBufferSecondsMinValueIsReallyMinimum
public void kafkatest_f7124_0()
{    Map<Object, Object> props = new HashMap<>();    props.put(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS, "-1");    new ConfigDef().withClientSaslSupport().parse(props);}
f7124
0
shouldThrowNpeWhenAddingNullHeader
public void kafkatest_f7133_0()
{    new RecordHeaders().add(null);}
f7133
0
getCount
private int kafkatest_f7134_0(Headers headers)
{    int count = 0;    Iterator<Header> headerIterator = headers.iterator();    while (headerIterator.hasNext()) {        headerIterator.next();        count++;    }    return count;}
f7134
0
testRemove
public void kafkatest_f7143_0()
{    PartitionStates<String> states = new PartitionStates<>();    LinkedHashMap<TopicPartition, String> map = createMap();    states.set(map);    states.remove(new TopicPartition("foo", 2));    LinkedHashMap<TopicPartition, String> expected = new LinkedHashMap<>();    expected.put(new TopicPartition("foo", 0), "foo 0");    expected.put(new TopicPartition("blah", 2), "blah 2");    expected.put(new TopicPartition("blah", 1), "blah 1");    expected.put(new TopicPartition("baz", 2), "baz 2");    expected.put(new TopicPartition("baz", 3), "baz 3");    checkState(states, expected);    states.remove(new TopicPartition("blah", 1));    expected = new LinkedHashMap<>();    expected.put(new TopicPartition("foo", 0), "foo 0");    expected.put(new TopicPartition("blah", 2), "blah 2");    expected.put(new TopicPartition("baz", 2), "baz 2");    expected.put(new TopicPartition("baz", 3), "baz 3");    checkState(states, expected);    states.remove(new TopicPartition("baz", 3));    expected = new LinkedHashMap<>();    expected.put(new TopicPartition("foo", 0), "foo 0");    expected.put(new TopicPartition("blah", 2), "blah 2");    expected.put(new TopicPartition("baz", 2), "baz 2");    checkState(states, expected);}
f7143
0
shouldAcceptValidTopicNames
public void kafkatest_f7144_0()
{    String maxLengthString = TestUtils.randomString(249);    String[] validTopicNames = { "valid", "TOPIC", "nAmEs", "ar6", "VaL1d", "_0-9_.", "...", maxLengthString };    for (String topicName : validTopicNames) {        Topic.validate(topicName);    }}
f7144
0
run
public void kafkatest_f7153_0()
{    try {        T value = future.get();        assertEquals(expected, value);    } catch (Throwable testException) {        this.testException = testException;    }}
f7153
0
testAllOfFutures
public void kafkatest_f7154_0() throws Exception
{    final int numThreads = 5;    final List<KafkaFutureImpl<Integer>> futures = new ArrayList<>();    for (int i = 0; i < numThreads; i++) {        futures.add(new KafkaFutureImpl<>());    }    KafkaFuture<Void> allFuture = KafkaFuture.allOf(futures.toArray(new KafkaFuture[0]));    final List<CompleterThread> completerThreads = new ArrayList<>();    final List<WaiterThread> waiterThreads = new ArrayList<>();    for (int i = 0; i < numThreads; i++) {        completerThreads.add(new CompleterThread<>(futures.get(i), i));        waiterThreads.add(new WaiterThread<>(futures.get(i), i));    }    assertFalse(allFuture.isDone());    for (int i = 0; i < numThreads; i++) {        waiterThreads.get(i).start();    }    for (int i = 0; i < numThreads - 1; i++) {        completerThreads.get(i).start();    }    assertFalse(allFuture.isDone());    completerThreads.get(numThreads - 1).start();    allFuture.get();    assertTrue(allFuture.isDone());    for (int i = 0; i < numThreads; i++) {        assertEquals(Integer.valueOf(i), futures.get(i).get());    }    for (int i = 0; i < numThreads; i++) {        completerThreads.get(i).join();        waiterThreads.get(i).join();        assertEquals(null, completerThreads.get(i).testException);        assertEquals(null, waiterThreads.get(i).testException);    }}
f7154
0
testAllocationZero
public void kafkatest_f7163_0() throws Exception
{    GarbageCollectedMemoryPool pool = new GarbageCollectedMemoryPool(1000, 10, true, null);    pool.tryAllocate(0);}
f7163
0
testAllocationNegative
public void kafkatest_f7164_0() throws Exception
{    GarbageCollectedMemoryPool pool = new GarbageCollectedMemoryPool(1000, 10, false, null);    pool.tryAllocate(-1);}
f7164
0
testAddOffsetsToTxnVersions
public void kafkatest_f7173_0() throws Exception
{    testAllMessageRoundTrips(new AddOffsetsToTxnRequestData().setTransactionalId("foobar").setProducerId(0xbadcafebadcafeL).setProducerEpoch((short) 123).setGroupId("baaz"));    testAllMessageRoundTrips(new AddOffsetsToTxnResponseData().setThrottleTimeMs(42).setErrorCode((short) 0));}
f7173
0
testAddPartitionsToTxnVersions
public void kafkatest_f7174_0() throws Exception
{    testAllMessageRoundTrips(new AddPartitionsToTxnRequestData().setTransactionalId("blah").setProducerId(0xbadcafebadcafeL).setProducerEpoch((short) 30000).setTopics(new AddPartitionsToTxnTopicCollection(singletonList(new AddPartitionsToTxnTopic().setName("Topic").setPartitions(singletonList(1))).iterator())));}
f7174
0
testOffsetCommitDefaultGroupInstanceId
public void kafkatest_f7183_0() throws Exception
{    testAllMessageRoundTrips(new OffsetCommitRequestData().setTopics(new ArrayList<>()).setGroupId("groupId"));    Supplier<OffsetCommitRequestData> request = () -> new OffsetCommitRequestData().setGroupId("groupId").setMemberId(memberId).setTopics(new ArrayList<>()).setGenerationId(15);    testAllMessageRoundTripsFromVersion((short) 1, request.get());    testAllMessageRoundTripsFromVersion((short) 1, request.get().setGroupInstanceId(null));    testAllMessageRoundTripsFromVersion((short) 7, request.get().setGroupInstanceId(instanceId));}
f7183
0
testOffsetForLeaderEpochVersions
public void kafkatest_f7184_0() throws Exception
{    // Version 2 adds optional current leader epoch    OffsetForLeaderEpochRequestData.OffsetForLeaderPartition partitionDataNoCurrentEpoch = new OffsetForLeaderEpochRequestData.OffsetForLeaderPartition().setPartitionIndex(0).setLeaderEpoch(3);    OffsetForLeaderEpochRequestData.OffsetForLeaderPartition partitionDataWithCurrentEpoch = new OffsetForLeaderEpochRequestData.OffsetForLeaderPartition().setPartitionIndex(0).setLeaderEpoch(3).setCurrentLeaderEpoch(5);    testAllMessageRoundTrips(new OffsetForLeaderEpochRequestData().setTopics(singletonList(new OffsetForLeaderEpochRequestData.OffsetForLeaderTopic().setName("foo").setPartitions(singletonList(partitionDataNoCurrentEpoch)))));    testAllMessageRoundTripsBeforeVersion((short) 2, partitionDataWithCurrentEpoch, partitionDataNoCurrentEpoch);    testAllMessageRoundTripsFromVersion((short) 2, partitionDataWithCurrentEpoch);    // Version 3 adds the optional replica Id field    testAllMessageRoundTripsFromVersion((short) 3, new OffsetForLeaderEpochRequestData().setReplicaId(5));    testAllMessageRoundTripsBeforeVersion((short) 3, new OffsetForLeaderEpochRequestData().setReplicaId(5), new OffsetForLeaderEpochRequestData());    testAllMessageRoundTripsBeforeVersion((short) 3, new OffsetForLeaderEpochRequestData().setReplicaId(5), new OffsetForLeaderEpochRequestData().setReplicaId(-2));}
f7184
0
testAllMessageRoundTripsBetweenVersions
private void kafkatest_f7193_0(short startVersion, short endVersion, Message message, Message expected) throws Exception
{    for (short version = startVersion; version < endVersion; version++) {        testMessageRoundTrip(version, message, expected);    }}
f7193
0
testAllMessageRoundTripsFromVersion
private void kafkatest_f7194_0(short fromVersion, Message message) throws Exception
{    for (short version = fromVersion; version < message.highestSupportedVersion(); version++) {        testEquivalentMessageRoundTrip(version, message);    }}
f7194
0
toString
public String kafkatest_f7203_0()
{    return name + "[" + type + "]";}
f7203
0
compareTypes
private static void kafkatest_f7204_0(Schema schemaA, Schema schemaB)
{    compareTypes(new NamedType("schemaA", schemaA), new NamedType("schemaB", schemaB));}
f7204
0
shouldRoundTripFieldThroughStruct
public void kafkatest_f7213_0()
{    final UUID uuid = UUID.randomUUID();    final TestUUIDData out = new TestUUIDData();    out.setProcessId(uuid);    final Struct struct = out.toStruct((short) 1);    final TestUUIDData in = new TestUUIDData();    in.fromStruct(struct, (short) 1);    Assert.assertEquals(uuid, in.processId());}
f7213
0
shouldRoundTripFieldThroughBuffer
public void kafkatest_f7214_0()
{    final UUID uuid = UUID.randomUUID();    final TestUUIDData out = new TestUUIDData();    out.setProcessId(uuid);    final ByteBuffer buffer = ByteBuffer.allocate(out.size((short) 1));    out.write(new ByteBufferAccessor(buffer), (short) 1);    buffer.rewind();    final TestUUIDData in = new TestUUIDData();    in.read(new ByteBufferAccessor(buffer), (short) 1);    Assert.assertEquals(uuid, in.processId());}
f7214
0
testGetAttributesWithUnknown
public void kafkatest_f7228_0() throws Exception
{    sensor.record(3.5);    sensor.record(4.0);    AttributeList attributeList = getAttributes(countMetricName, countMetricName.name(), sumMetricName.name(), "name");    List<Attribute> attributes = attributeList.asList();    assertEquals(2, attributes.size());    for (Attribute attribute : attributes) {        if (countMetricName.name().equals(attribute.getName()))            assertEquals(2.0, attribute.getValue());        else if (sumMetricName.name().equals(attribute.getName()))            assertEquals(7.5, attribute.getValue());        else            fail("Unexpected attribute returned: " + attribute.getName());    }}
f7228
0
testInvoke
public void kafkatest_f7229_0() throws Exception
{    try {        mBeanServer.invoke(objectName(countMetricName), "something", null, null);        fail("invoke should have failed");    } catch (RuntimeMBeanException e) {        assertThat(e.getCause(), instanceOf(UnsupportedOperationException.class));    }}
f7229
0
testMetricName
public void kafkatest_f7238_0()
{    MetricName n1 = metrics.metricName("name", "group", "description", "key1", "value1", "key2", "value2");    Map<String, String> tags = new HashMap<String, String>();    tags.put("key1", "value1");    tags.put("key2", "value2");    MetricName n2 = metrics.metricName("name", "group", "description", tags);    assertEquals("metric names created in two different ways should be equal", n1, n2);    try {        metrics.metricName("name", "group", "description", "key1");        fail("Creating MetricName with an odd number of keyValue should fail");    } catch (IllegalArgumentException e) {    // this is expected    }}
f7238
0
testSimpleStats
public void kafkatest_f7239_0() throws Exception
{    verifyStats(m -> (double) m.metricValue());}
f7239
0
testTimeWindowing
public void kafkatest_f7248_0()
{    WindowedCount count = new WindowedCount();    MetricConfig config = new MetricConfig().timeWindow(1, TimeUnit.MILLISECONDS).samples(2);    count.record(config, 1.0, time.milliseconds());    time.sleep(1);    count.record(config, 1.0, time.milliseconds());    assertEquals(2.0, count.measure(config, time.milliseconds()), EPS);    time.sleep(1);    // oldest event times out    count.record(config, 1.0, time.milliseconds());    assertEquals(2.0, count.measure(config, time.milliseconds()), EPS);}
f7248
0
testOldDataHasNoEffect
public void kafkatest_f7249_0()
{    Max max = new Max();    long windowMs = 100;    int samples = 2;    MetricConfig config = new MetricConfig().timeWindow(windowMs, TimeUnit.MILLISECONDS).samples(samples);    max.record(config, 50, time.milliseconds());    time.sleep(samples * windowMs);    assertEquals(Double.NaN, max.measure(config, time.milliseconds()), EPS);}
f7249
0
testSimpleRate
public void kafkatest_f7258_0()
{    SimpleRate rate = new SimpleRate();    // Given    MetricConfig config = new MetricConfig().timeWindow(1, TimeUnit.SECONDS).samples(10);    // In the first window the rate is a fraction of the whole (1s) window    // So when we record 1000 at t0, the rate should be 1000 until the window completes, or more data is recorded.    record(rate, config, 1000);    assertEquals(1000, measure(rate, config), 0);    time.sleep(100);    // 1000B / 0.1s    assertEquals(1000, measure(rate, config), 0);    time.sleep(100);    // 1000B / 0.2s    assertEquals(1000, measure(rate, config), 0);    time.sleep(200);    // 1000B / 0.4s    assertEquals(1000, measure(rate, config), 0);    // In the second (and subsequent) window(s), the rate will be in proportion to the elapsed time    // So the rate will degrade over time, as the time between measurement and the initial recording grows.    time.sleep(600);    // 1000B / 1.0s    assertEquals(1000, measure(rate, config), 0);    time.sleep(200);    // 1000B / 1.2s    assertEquals(1000 / 1.2, measure(rate, config), 0);    time.sleep(200);    // 1000B / 1.4s    assertEquals(1000 / 1.4, measure(rate, config), 0);    // Adding another value, inside the same window should double the rate    record(rate, config, 1000);    // 2000B / 1.4s    assertEquals(2000 / 1.4, measure(rate, config), 0);    // Going over the next window, should not change behaviour    time.sleep(1100);    // 2000B / 2.5s    assertEquals(2000 / 2.5, measure(rate, config), 0);    record(rate, config, 1000);    // 3000B / 2.5s    assertEquals(3000 / 2.5, measure(rate, config), 0);    // Sleeping for another 6.5 windows also should be the same    time.sleep(6500);    // 3000B / 9s    assertEquals(3000 / 9, measure(rate, config), 1);    record(rate, config, 1000);    // 4000B / 9s    assertEquals(4000 / 9, measure(rate, config), 1);    // Going over the 10 window boundary should cause the first window's values (1000) will be purged.    // So the rate is calculated based on the oldest reading, which is inside the second window, at 1.4s    time.sleep(1500);    assertEquals((4000 - 1000) / (10.5 - 1.4), measure(rate, config), 1);    record(rate, config, 1000);    assertEquals((5000 - 1000) / (10.5 - 1.4), measure(rate, config), 1);}
f7258
0
record
private void kafkatest_f7259_0(Rate rate, MetricConfig config, int value)
{    rate.record(config, value, time.milliseconds());}
f7259
0
forId
 static StatType kafkatest_f7271_0(int id)
{    for (StatType statType : StatType.values()) {        if (statType.id == id)            return statType;    }    return null;}
f7271
0
createSensor
private Sensor kafkatest_f7272_0(StatType statType, int index)
{    Sensor sensor = metrics.sensor("kafka.requests." + index);    Map<String, String> tags = Collections.singletonMap("tag", "tag" + index);    switch(statType) {        case AVG:            sensor.add(metrics.metricName("test.metric.avg", "avg", tags), new Avg());            break;        case TOTAL:            sensor.add(metrics.metricName("test.metric.total", "total", tags), new CumulativeSum());            break;        case COUNT:            sensor.add(metrics.metricName("test.metric.count", "count", tags), new WindowedCount());            break;        case MAX:            sensor.add(metrics.metricName("test.metric.max", "max", tags), new Max());            break;        case MIN:            sensor.add(metrics.metricName("test.metric.min", "min", tags), new Min());            break;        case RATE:            sensor.add(metrics.metricName("test.metric.rate", "rate", tags), new Rate());            break;        case SIMPLE_RATE:            sensor.add(metrics.metricName("test.metric.simpleRate", "simpleRate", tags), new SimpleRate());            break;        case SUM:            sensor.add(metrics.metricName("test.metric.sum", "sum", tags), new WindowedSum());            break;        case VALUE:            sensor.add(metrics.metricName("test.metric.value", "value", tags), new Value());            break;        case PERCENTILES:            sensor.add(metrics.metricName("test.metric.percentiles", "percentiles", tags), new Percentiles(100, -100, 100, Percentiles.BucketSizing.CONSTANT, new Percentile(metrics.metricName("test.median", "percentiles"), 50.0), new Percentile(metrics.metricName("test.perc99_9", "percentiles"), 99.9)));            break;        case METER:            sensor.add(new Meter(metrics.metricName("test.metric.meter.rate", "meter", tags), metrics.metricName("test.metric.meter.total", "meter", tags)));            break;        default:            throw new IllegalStateException("Invalid stat type " + statType);    }    return sensor;}
f7272
0
tearDown
public void kafkatest_f7281_0()
{    metrics.close();}
f7281
0
testFrequencyCenterValueAboveMax
public void kafkatest_f7282_0()
{    new Frequencies(4, 1.0, 4.0, freq("1", 1.0), freq("2", 20.0));}
f7282
0
testConstantBinSchemeWithPositiveRange
public void kafkatest_f7291_0()
{    ConstantBinScheme scheme = new ConstantBinScheme(5, 0, 5);    assertEquals("A value below the lower bound should map to the first bin", 0, scheme.toBin(-1.0));    assertEquals("A value above the upper bound should map to the last bin", 4, scheme.toBin(5.01));    assertEquals("Check boundary of bucket 0", 0, scheme.toBin(-0.0001));    assertEquals("Check boundary of bucket 0", 0, scheme.toBin(0.0000));    assertEquals("Check boundary of bucket 0", 0, scheme.toBin(0.0001));    assertEquals("Check boundary of bucket 0", 0, scheme.toBin(0.9999));    assertEquals("Check boundary of bucket 1", 1, scheme.toBin(1.0000));    assertEquals("Check boundary of bucket 1", 1, scheme.toBin(1.0001));    assertEquals("Check boundary of bucket 1", 1, scheme.toBin(1.9999));    assertEquals("Check boundary of bucket 2", 2, scheme.toBin(2.0000));    assertEquals("Check boundary of bucket 2", 2, scheme.toBin(2.0001));    assertEquals("Check boundary of bucket 2", 2, scheme.toBin(2.9999));    assertEquals("Check boundary of bucket 3", 3, scheme.toBin(3.0000));    assertEquals("Check boundary of bucket 3", 3, scheme.toBin(3.0001));    assertEquals("Check boundary of bucket 3", 3, scheme.toBin(3.9999));    assertEquals("Check boundary of bucket 4", 4, scheme.toBin(4.0000));    assertEquals("Check boundary of bucket 4", 4, scheme.toBin(4.9999));    assertEquals("Check boundary of bucket 4", 4, scheme.toBin(5.0000));    assertEquals("Check boundary of bucket 4", 4, scheme.toBin(5.0001));    assertEquals(Float.NEGATIVE_INFINITY, scheme.fromBin(-1), 0.001d);    assertEquals(Float.POSITIVE_INFINITY, scheme.fromBin(5), 0.001d);    assertEquals(0.0, scheme.fromBin(0), 0.001d);    assertEquals(1.0, scheme.fromBin(1), 0.001d);    assertEquals(2.0, scheme.fromBin(2), 0.001d);    assertEquals(3.0, scheme.fromBin(3), 0.001d);    assertEquals(4.0, scheme.fromBin(4), 0.001d);    checkBinningConsistency(scheme);}
f7291
0
testLinearBinScheme
public void kafkatest_f7292_0()
{    LinearBinScheme scheme = new LinearBinScheme(10, 10);    assertEquals(Float.NEGATIVE_INFINITY, scheme.fromBin(-1), 0.001d);    assertEquals(Float.POSITIVE_INFINITY, scheme.fromBin(11), 0.001d);    assertEquals(0.0, scheme.fromBin(0), 0.001d);    assertEquals(0.2222, scheme.fromBin(1), 0.001d);    assertEquals(0.6666, scheme.fromBin(2), 0.001d);    assertEquals(1.3333, scheme.fromBin(3), 0.001d);    assertEquals(2.2222, scheme.fromBin(4), 0.001d);    assertEquals(3.3333, scheme.fromBin(5), 0.001d);    assertEquals(4.6667, scheme.fromBin(6), 0.001d);    assertEquals(6.2222, scheme.fromBin(7), 0.001d);    assertEquals(8.0000, scheme.fromBin(8), 0.001d);    assertEquals(10.000, scheme.fromBin(9), 0.001d);    assertEquals(0, scheme.toBin(0.0000));    assertEquals(0, scheme.toBin(0.2221));    assertEquals(1, scheme.toBin(0.2223));    assertEquals(2, scheme.toBin(0.6667));    assertEquals(3, scheme.toBin(1.3334));    assertEquals(4, scheme.toBin(2.2223));    assertEquals(5, scheme.toBin(3.3334));    assertEquals(6, scheme.toBin(4.6667));    assertEquals(7, scheme.toBin(6.2223));    assertEquals(8, scheme.toBin(8.0000));    assertEquals(9, scheme.toBin(10.000));    assertEquals(9, scheme.toBin(10.001));    assertEquals(Float.POSITIVE_INFINITY, scheme.fromBin(10), 0.001d);    checkBinningConsistency(scheme);}
f7292
0
testCreateConfigurableKafkaPrincipalBuilder
public void kafkatest_f7301_0()
{    Map<String, Object> configs = new HashMap<>();    configs.put(BrokerSecurityConfigs.PRINCIPAL_BUILDER_CLASS_CONFIG, ConfigurableKafkaPrincipalBuilder.class);    KafkaPrincipalBuilder builder = ChannelBuilders.createPrincipalBuilder(configs, null, null, null, null);    assertTrue(builder instanceof ConfigurableKafkaPrincipalBuilder);    assertTrue(((ConfigurableKafkaPrincipalBuilder) builder).configured);}
f7301
0
configure
public void kafkatest_f7302_0(Map<String, ?> configs)
{    configured = true;}
f7302
0
close
public void kafkatest_f7312_0() throws IOException, InterruptedException
{    closing = true;    this.serverSocket.close();    closeConnections();    for (Thread t : threads) t.join();    join();}
f7312
0
createEchoServer
public static NioEchoServer kafkatest_f7313_0(ListenerName listenerName, SecurityProtocol securityProtocol, AbstractConfig serverConfig, CredentialCache credentialCache, Time time) throws Exception
{    return createEchoServer(listenerName, securityProtocol, serverConfig, credentialCache, 100, time);}
f7313
0
metricNameSuffix
public String kafkatest_f7322_0()
{    return metricNameSuffix;}
f7322
0
port
public int kafkatest_f7323_0()
{    return port;}
f7323
0
run
public void kafkatest_f7332_0()
{    try {        acceptorThread.start();        while (serverSocketChannel.isOpen()) {            selector.poll(100);            synchronized (newChannels) {                for (SocketChannel socketChannel : newChannels) {                    String id = id(socketChannel);                    selector.register(id, socketChannel);                    socketChannels.add(socketChannel);                }                newChannels.clear();            }            if (closeKafkaChannels) {                for (KafkaChannel channel : selector.channels()) selector.close(channel.id());            }            List<NetworkReceive> completedReceives = selector.completedReceives();            for (NetworkReceive rcv : completedReceives) {                KafkaChannel channel = channel(rcv.source());                if (!maybeBeginServerReauthentication(channel, rcv, time)) {                    String channelId = channel.id();                    selector.mute(channelId);                    NetworkSend send = new NetworkSend(rcv.source(), rcv.payload());                    if (outputChannel == null)                        selector.send(send);                    else {                        for (ByteBuffer buffer : send.buffers) outputChannel.write(buffer);                        selector.unmute(channelId);                    }                }            }            for (Send send : selector.completedSends()) {                selector.unmute(send.destination());                numSent += 1;            }        }    } catch (IOException e) {    // ignore    }}
f7332
0
numSent
public int kafkatest_f7333_0()
{    return numSent;}
f7333
0
run
public void kafkatest_f7342_0()
{    try {        java.nio.channels.Selector acceptSelector = java.nio.channels.Selector.open();        serverSocketChannel.register(acceptSelector, SelectionKey.OP_ACCEPT);        while (serverSocketChannel.isOpen()) {            if (acceptSelector.select(1000) > 0) {                Iterator<SelectionKey> it = acceptSelector.selectedKeys().iterator();                while (it.hasNext()) {                    SelectionKey key = it.next();                    if (key.isAcceptable()) {                        SocketChannel socketChannel = ((ServerSocketChannel) key.channel()).accept();                        socketChannel.configureBlocking(false);                        newChannels.add(socketChannel);                        selector.wakeup();                    }                    it.remove();                }            }        }    } catch (IOException e) {    // ignore    }}
f7342
0
run
public void kafkatest_f7343_0()
{    try (Socket connection = new Socket(serverAddress.getAddress(), serverAddress.getPort());        OutputStream os = connection.getOutputStream()) {        os.write(payload);        os.flush();    } catch (Exception e) {        e.printStackTrace(System.err);    }}
f7343
0
conditionMet
public boolean kafkatest_f7352_0()
{    try {        selector.poll(1000L);        return selector.disconnected().containsKey(node);    } catch (IOException e) {        throw new RuntimeException(e);    }}
f7352
0
testCantSendWithInProgress
public void kafkatest_f7353_0() throws Exception
{    String node = "0";    blockingConnect(node);    selector.send(createSend(node, "test1"));    try {        selector.send(createSend(node, "test2"));        fail("IllegalStateException not thrown when sending a request with one in flight");    } catch (IllegalStateException e) {    // Expected exception    }    selector.poll(0);    assertTrue("Channel not closed", selector.disconnected().containsKey(node));    assertEquals(ChannelState.FAILED_SEND, selector.disconnected().get(node));}
f7353
0
testMute
public void kafkatest_f7362_0() throws Exception
{    blockingConnect("0");    blockingConnect("1");    selector.send(createSend("0", "hello"));    selector.send(createSend("1", "hi"));    selector.mute("1");    while (selector.completedReceives().isEmpty()) selector.poll(5);    assertEquals("We should have only one response", 1, selector.completedReceives().size());    assertEquals("The response should not be from the muted node", "0", selector.completedReceives().get(0).source());    selector.unmute("1");    do {        selector.poll(5);    } while (selector.completedReceives().isEmpty());    assertEquals("We should have only one response", 1, selector.completedReceives().size());    assertEquals("The response should be from the previously muted node", "1", selector.completedReceives().get(0).source());}
f7362
0
registerFailure
public void kafkatest_f7363_0() throws Exception
{    ChannelBuilder channelBuilder = new PlaintextChannelBuilder(null) {        @Override        public KafkaChannel buildChannel(String id, SelectionKey key, int maxReceiveSize, MemoryPool memoryPool) throws KafkaException {            throw new RuntimeException("Test exception");        }        @Override        public void close() {        }    };    Selector selector = new Selector(5000, new Metrics(), new MockTime(), "MetricGroup", channelBuilder, new LogContext());    SocketChannel socketChannel = SocketChannel.open();    socketChannel.configureBlocking(false);    try {        selector.register("1", socketChannel);        fail("Register did not fail");    } catch (IOException e) {        assertTrue("Unexpected exception: " + e, e.getCause().getMessage().contains("Test exception"));        assertFalse("Socket not closed", socketChannel.isOpen());    }    selector.close();}
f7363
0
registerChannel
protected SelectionKey kafkatest_f7373_0(String id, SocketChannel socketChannel, int interestedOps) throws IOException
{    SelectionKey key = super.registerChannel(id, socketChannel, interestedOps);    key.cancel();    if (throwIOException.get())        throw new IOException("Test exception");    return key;}
f7373
0
verifyImmediatelyConnectedException
private void kafkatest_f7374_0(Selector selector, String id) throws Exception
{    try {        selector.connect(id, new InetSocketAddress("localhost", server.port), BUFFER_SIZE, BUFFER_SIZE);        fail("Expected exception not thrown");    } catch (Exception e) {        verifyEmptyImmediatelyConnectedKeys(selector);        assertNull("Channel not removed", selector.channel(id));        ensureEmptySelectorFields(selector);    }}
f7374
0
testOutboundConnectionsCountInConnectionCreationMetric
public void kafkatest_f7383_0() throws Exception
{    // create connections    int expectedConnections = 5;    InetSocketAddress addr = new InetSocketAddress("localhost", server.port);    for (int i = 0; i < expectedConnections; i++) connect(Integer.toString(i), addr);    // Poll continuously, as we cannot guarantee that the first call will see all connections    int seenConnections = 0;    for (int i = 0; i < 10; i++) {        selector.poll(100L);        seenConnections += selector.connected().size();        if (seenConnections == expectedConnections)            break;    }    assertEquals((double) expectedConnections, getMetric("connection-creation-total").metricValue());    assertEquals((double) expectedConnections, getMetric("connection-count").metricValue());}
f7383
0
testInboundConnectionsCountInConnectionCreationMetric
public void kafkatest_f7384_0() throws Exception
{    int conns = 5;    try (ServerSocketChannel ss = ServerSocketChannel.open()) {        ss.bind(new InetSocketAddress(0));        InetSocketAddress serverAddress = (InetSocketAddress) ss.getLocalAddress();        for (int i = 0; i < conns; i++) {            Thread sender = createSender(serverAddress, randomPayload(1));            sender.start();            SocketChannel channel = ss.accept();            channel.configureBlocking(false);            selector.register(Integer.toString(i), channel);        }    }    assertEquals((double) conns, getMetric("connection-creation-total").metricValue());    assertEquals((double) conns, getMetric("connection-count").metricValue());}
f7384
0
asString
protected String kafkatest_f7393_0(NetworkReceive receive)
{    return new String(Utils.toArray(receive.payload()));}
f7393
0
sendAndReceive
private void kafkatest_f7394_0(String node, String requestPrefix, int startIndex, int endIndex) throws Exception
{    int requests = startIndex;    int responses = startIndex;    selector.send(createSend(node, requestPrefix + "-" + startIndex));    requests++;    while (responses < endIndex) {        // do the i/o        selector.poll(0L);        assertEquals("No disconnects should have occurred.", 0, selector.disconnected().size());        // handle requests and responses of the fast node        for (NetworkReceive receive : selector.completedReceives()) {            assertEquals(requestPrefix + "-" + responses, asString(receive));            responses++;        }        for (int i = 0; i < selector.completedSends().size() && requests < endIndex; i++, requests++) {            selector.send(createSend(node, requestPrefix + "-" + requests));        }    }}
f7394
0
tearDown
public void kafkatest_f7403_0() throws Exception
{    this.selector.close();    this.server.close();    this.metrics.close();}
f7403
0
securityProtocol
public SecurityProtocol kafkatest_f7404_0()
{    return SecurityProtocol.PLAINTEXT;}
f7404
0
testRenegotiationFails
public void kafkatest_f7413_0() throws Exception
{    String node = "0";    // create connections    InetSocketAddress addr = new InetSocketAddress("localhost", server.port);    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    // send echo requests and receive responses    while (!selector.isChannelReady(node)) {        selector.poll(1000L);    }    selector.send(createSend(node, node + "-" + 0));    selector.poll(0L);    server.renegotiate();    selector.send(createSend(node, node + "-" + 1));    long expiryTime = System.currentTimeMillis() + 2000;    List<String> disconnected = new ArrayList<>();    while (!disconnected.contains(node) && System.currentTimeMillis() < expiryTime) {        selector.poll(10);        disconnected.addAll(selector.disconnected().keySet());    }    assertTrue("Renegotiation should cause disconnection", disconnected.contains(node));}
f7413
0
testMuteOnOOM
public void kafkatest_f7414_0() throws Exception
{    // clean up default selector, replace it with one that uses a finite mem pool    selector.close();    MemoryPool pool = new SimpleMemoryPool(900, 900, false, null);    // the initial channel builder is for clients, we need a server one    File trustStoreFile = File.createTempFile("truststore", ".jks");    Map<String, Object> sslServerConfigs = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile, "server");    channelBuilder = new SslChannelBuilder(Mode.SERVER, null, false);    channelBuilder.configure(sslServerConfigs);    selector = new Selector(NetworkReceive.UNLIMITED, 5000, metrics, time, "MetricGroup", new HashMap<String, String>(), true, false, channelBuilder, pool, new LogContext());    try (ServerSocketChannel ss = ServerSocketChannel.open()) {        ss.bind(new InetSocketAddress(0));        InetSocketAddress serverAddress = (InetSocketAddress) ss.getLocalAddress();        SslSender sender1 = createSender(serverAddress, randomPayload(900));        SslSender sender2 = createSender(serverAddress, randomPayload(900));        sender1.start();        sender2.start();        // not defined if its 1 or 2        SocketChannel channelX = ss.accept();        channelX.configureBlocking(false);        SocketChannel channelY = ss.accept();        channelY.configureBlocking(false);        selector.register("clientX", channelX);        selector.register("clientY", channelY);        boolean handshaked = false;        NetworkReceive firstReceive = null;        long deadline = System.currentTimeMillis() + 5000;        // 2. a single payload is actually read out completely (the other is too big to fit)        while (System.currentTimeMillis() < deadline) {            selector.poll(10);            List<NetworkReceive> completed = selector.completedReceives();            if (firstReceive == null) {                if (!completed.isEmpty()) {                    assertEquals("expecting a single request", 1, completed.size());                    firstReceive = completed.get(0);                    assertTrue(selector.isMadeReadProgressLastPoll());                    assertEquals(0, pool.availableMemory());                }            } else {                assertTrue("only expecting single request", completed.isEmpty());            }            handshaked = sender1.waitForHandshake(1) && sender2.waitForHandshake(1);            if (handshaked && firstReceive != null && selector.isOutOfMemory())                break;        }        assertTrue("could not initiate connections within timeout", handshaked);        selector.poll(10);        assertTrue(selector.completedReceives().isEmpty());        assertEquals(0, pool.availableMemory());        assertNotNull("First receive not complete", firstReceive);        assertTrue("Selector not out of memory", selector.isOutOfMemory());        firstReceive.close();        // memory has been released back to pool        assertEquals(900, pool.availableMemory());        List<NetworkReceive> completed = Collections.emptyList();        deadline = System.currentTimeMillis() + 5000;        while (System.currentTimeMillis() < deadline && completed.isEmpty()) {            selector.poll(1000);            completed = selector.completedReceives();        }        assertEquals("could not read remaining request within timeout", 1, completed.size());        assertEquals(0, pool.availableMemory());        assertFalse(selector.isOutOfMemory());    }}
f7414
0
checkServerTrusted
public void kafkatest_f7423_0(X509Certificate[] x509Certificates, String s) throws CertificateException
{// nop}
f7423
0
getAcceptedIssuers
public X509Certificate[] kafkatest_f7424_0()
{    return new X509Certificate[0];}
f7424
0
testInvalidEndpointIdentification
public void kafkatest_f7433_0() throws Exception
{    String node = "0";    serverCertStores = new CertStores(true, "server", "notahost");    clientCertStores = new CertStores(false, "client", "localhost");    sslServerConfigs = serverCertStores.getTrustingConfig(clientCertStores);    sslClientConfigs = clientCertStores.getTrustingConfig(serverCertStores);    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, "HTTPS");    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(selector, node, ChannelState.State.AUTHENTICATION_FAILED);    server.verifyAuthenticationMetrics(0, 1);}
f7433
0
testEndpointIdentificationDisabled
public void kafkatest_f7434_0() throws Exception
{    serverCertStores = new CertStores(true, "server", "notahost");    clientCertStores = new CertStores(false, "client", "localhost");    sslServerConfigs = serverCertStores.getTrustingConfig(clientCertStores);    sslClientConfigs = clientCertStores.getTrustingConfig(serverCertStores);    SecurityProtocol securityProtocol = SecurityProtocol.SSL;    server = createEchoServer(SecurityProtocol.SSL);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    // Disable endpoint validation, connection should succeed    String node = "1";    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, "");    createSelector(sslClientConfigs);    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);    // Disable endpoint validation using null value, connection should succeed    String node2 = "2";    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, null);    createSelector(sslClientConfigs);    selector.connect(node2, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node2, 100, 10);    // Connection should fail with endpoint validation enabled    String node3 = "3";    sslClientConfigs.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, "HTTPS");    createSelector(sslClientConfigs);    selector.connect(node3, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(selector, node3, ChannelState.State.AUTHENTICATION_FAILED);    selector.close();}
f7434
0
testInvalidSecureRandomImplementation
public void kafkatest_f7443_0() throws Exception
{    try (SslChannelBuilder channelBuilder = new SslChannelBuilder(Mode.CLIENT, null, false)) {        sslClientConfigs.put(SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG, "invalid");        channelBuilder.configure(sslClientConfigs);        fail("SSL channel configured with invalid SecureRandom implementation");    } catch (KafkaException e) {    // Expected exception    }}
f7443
0
testInvalidTruststorePassword
public void kafkatest_f7444_0() throws Exception
{    try (SslChannelBuilder channelBuilder = new SslChannelBuilder(Mode.CLIENT, null, false)) {        sslClientConfigs.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "invalid");        channelBuilder.configure(sslClientConfigs);        fail("SSL channel configured with invalid truststore password");    } catch (KafkaException e) {    // Expected exception    }}
f7444
0
conditionMet
public boolean kafkatest_f7453_0()
{    return server.numSent() >= 2;}
f7453
0
testNetReadBufferResize
public void kafkatest_f7454_0() throws Exception
{    String node = "0";    server = createEchoServer(SecurityProtocol.SSL);    createSelector(sslClientConfigs, 10, null, null);    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, node, 64000, 10);}
f7454
0
testGracefulRemoteCloseDuringHandshakeRead
public void kafkatest_f7463_0() throws Exception
{    server = createEchoServer(SecurityProtocol.SSL);    testIOExceptionsDuringHandshake(FailureAction.NO_OP, server::closeKafkaChannels);}
f7463
0
testGracefulRemoteCloseDuringHandshakeWrite
public void kafkatest_f7464_0() throws Exception
{    server = createEchoServer(SecurityProtocol.SSL);    testIOExceptionsDuringHandshake(server::closeKafkaChannels, FailureAction.NO_OP);}
f7464
0
testServerKeystoreDynamicUpdate
public void kafkatest_f7473_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SSL;    TestSecurityConfig config = new TestSecurityConfig(sslServerConfigs);    ListenerName listenerName = ListenerName.forSecurityProtocol(securityProtocol);    ChannelBuilder serverChannelBuilder = ChannelBuilders.serverChannelBuilder(listenerName, false, securityProtocol, config, null, null, time);    server = new NioEchoServer(listenerName, securityProtocol, config, "localhost", serverChannelBuilder, null, time);    server.start();    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    // Verify that client with matching truststore can authenticate, send and receive    String oldNode = "0";    Selector oldClientSelector = createSelector(sslClientConfigs);    oldClientSelector.connect(oldNode, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, oldNode, 100, 10);    CertStores newServerCertStores = new CertStores(true, "server", "localhost");    Map<String, Object> newKeystoreConfigs = newServerCertStores.keyStoreProps();    assertTrue("SslChannelBuilder not reconfigurable", serverChannelBuilder instanceof ListenerReconfigurable);    ListenerReconfigurable reconfigurableBuilder = (ListenerReconfigurable) serverChannelBuilder;    assertEquals(listenerName, reconfigurableBuilder.listenerName());    reconfigurableBuilder.validateReconfiguration(newKeystoreConfigs);    reconfigurableBuilder.reconfigure(newKeystoreConfigs);    // Verify that new client with old truststore fails    oldClientSelector.connect("1", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(oldClientSelector, "1", ChannelState.State.AUTHENTICATION_FAILED);    // Verify that new client with new truststore can authenticate, send and receive    sslClientConfigs = clientCertStores.getTrustingConfig(newServerCertStores);    Selector newClientSelector = createSelector(sslClientConfigs);    newClientSelector.connect("2", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(newClientSelector, "2", 100, 10);    // Verify that old client continues to work    NetworkTestUtils.checkClientConnection(oldClientSelector, oldNode, 100, 10);    CertStores invalidCertStores = new CertStores(true, "server", "127.0.0.1");    Map<String, Object> invalidConfigs = invalidCertStores.getTrustingConfig(clientCertStores);    verifyInvalidReconfigure(reconfigurableBuilder, invalidConfigs, "keystore with different SubjectAltName");    Map<String, Object> missingStoreConfigs = new HashMap<>();    missingStoreConfigs.put(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, "PKCS12");    missingStoreConfigs.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, "some.keystore.path");    missingStoreConfigs.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, new Password("some.keystore.password"));    missingStoreConfigs.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, new Password("some.key.password"));    verifyInvalidReconfigure(reconfigurableBuilder, missingStoreConfigs, "keystore not found");    // Verify that new connections continue to work with the server with previously configured keystore after failed reconfiguration    newClientSelector.connect("3", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(newClientSelector, "3", 100, 10);}
f7473
0
testServerTruststoreDynamicUpdate
public void kafkatest_f7474_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SSL;    sslServerConfigs.put(BrokerSecurityConfigs.SSL_CLIENT_AUTH_CONFIG, "required");    TestSecurityConfig config = new TestSecurityConfig(sslServerConfigs);    ListenerName listenerName = ListenerName.forSecurityProtocol(securityProtocol);    ChannelBuilder serverChannelBuilder = ChannelBuilders.serverChannelBuilder(listenerName, false, securityProtocol, config, null, null, time);    server = new NioEchoServer(listenerName, securityProtocol, config, "localhost", serverChannelBuilder, null, time);    server.start();    InetSocketAddress addr = new InetSocketAddress("localhost", server.port());    // Verify that client with matching keystore can authenticate, send and receive    String oldNode = "0";    Selector oldClientSelector = createSelector(sslClientConfigs);    oldClientSelector.connect(oldNode, addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(selector, oldNode, 100, 10);    CertStores newClientCertStores = new CertStores(true, "client", "localhost");    sslClientConfigs = newClientCertStores.getTrustingConfig(serverCertStores);    Map<String, Object> newTruststoreConfigs = newClientCertStores.trustStoreProps();    assertTrue("SslChannelBuilder not reconfigurable", serverChannelBuilder instanceof ListenerReconfigurable);    ListenerReconfigurable reconfigurableBuilder = (ListenerReconfigurable) serverChannelBuilder;    assertEquals(listenerName, reconfigurableBuilder.listenerName());    reconfigurableBuilder.validateReconfiguration(newTruststoreConfigs);    reconfigurableBuilder.reconfigure(newTruststoreConfigs);    // Verify that new client with old truststore fails    oldClientSelector.connect("1", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.waitForChannelClose(oldClientSelector, "1", ChannelState.State.AUTHENTICATION_FAILED);    // Verify that new client with new truststore can authenticate, send and receive    Selector newClientSelector = createSelector(sslClientConfigs);    newClientSelector.connect("2", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(newClientSelector, "2", 100, 10);    // Verify that old client continues to work    NetworkTestUtils.checkClientConnection(oldClientSelector, oldNode, 100, 10);    Map<String, Object> invalidConfigs = new HashMap<>(newTruststoreConfigs);    invalidConfigs.put(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, "INVALID_TYPE");    verifyInvalidReconfigure(reconfigurableBuilder, invalidConfigs, "invalid truststore type");    Map<String, Object> missingStoreConfigs = new HashMap<>();    missingStoreConfigs.put(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, "PKCS12");    missingStoreConfigs.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, "some.truststore.path");    missingStoreConfigs.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, new Password("some.truststore.password"));    verifyInvalidReconfigure(reconfigurableBuilder, missingStoreConfigs, "truststore not found");    // Verify that new connections continue to work with the server with previously configured keystore after failed reconfiguration    newClientSelector.connect("3", addr, BUFFER_SIZE, BUFFER_SIZE);    NetworkTestUtils.checkClientConnection(newClientSelector, "3", 100, 10);}
f7474
0
netReadBufferSize
protected int kafkatest_f7483_0()
{    ByteBuffer netReadBuffer = netReadBuffer();    // netReadBufferSize() is invoked in SSLTransportLayer.read() prior to the read    // operation. To avoid the read buffer being expanded too early, increase buffer size    // only when read buffer is full. This ensures that BUFFER_UNDERFLOW is always    // triggered in testNetReadBufferResize().    boolean updateBufSize = netReadBuffer != null && !netReadBuffer().hasRemaining();    return netReadBufSize.updateAndGet(super.netReadBufferSize(), updateBufSize);}
f7483
0
netWriteBufferSize
protected int kafkatest_f7484_0()
{    return netWriteBufSize.updateAndGet(super.netWriteBufferSize(), true);}
f7484
0
testForIdWithInvalidIdHigh
public void kafkatest_f7493_0()
{    ApiKeys.forId(10000);}
f7493
0
schemaVersionOutOfRange
public void kafkatest_f7494_0()
{    ApiKeys.PRODUCE.requestSchema((short) ApiKeys.PRODUCE.requestSchemas.length);}
f7494
0
testSimpleUtf8Lengths
public void kafkatest_f7503_0()
{    validateUtf8Length("");    validateUtf8Length("abc");    validateUtf8Length("This is a test string.");}
f7503
0
testMultibyteUtf8Lengths
public void kafkatest_f7504_0()
{    validateUtf8Length("A\u00ea\u00f1\u00fcC");    validateUtf8Length("\ud801\udc00");    validateUtf8Length("M\u00fcO");}
f7504
0
checkNullableDefault
private void kafkatest_f7513_0(Type type, Object defaultValue)
{    // Should use default even if the field allows null values    Schema schema = new Schema(new Field("field", type, "doc", defaultValue));    Struct struct = new Struct(schema);    assertEquals("Should get the default value", defaultValue, struct.get("field"));    // should be valid even with missing value    struct.validate();}
f7513
0
testReadArraySizeTooLarge
public void kafkatest_f7514_0()
{    Type type = new ArrayOf(Type.INT8);    int size = 10;    ByteBuffer invalidBuffer = ByteBuffer.allocate(4 + size);    invalidBuffer.putInt(Integer.MAX_VALUE);    for (int i = 0; i < size; i++) invalidBuffer.put((byte) i);    invalidBuffer.rewind();    try {        type.read(invalidBuffer);        fail("Array size not validated");    } catch (SchemaException e) {    // Expected exception    }}
f7514
0
testStructEquals
public void kafkatest_f7523_0()
{    Schema schema = new Schema(new Field("field1", Type.NULLABLE_STRING), new Field("field2", Type.NULLABLE_STRING));    Struct emptyStruct1 = new Struct(schema);    Struct emptyStruct2 = new Struct(schema);    assertEquals(emptyStruct1, emptyStruct2);    Struct mostlyEmptyStruct = new Struct(schema).set("field1", "foo");    assertNotEquals(emptyStruct1, mostlyEmptyStruct);    assertNotEquals(mostlyEmptyStruct, emptyStruct1);}
f7523
0
testReadIgnoringExtraDataAtTheEnd
public void kafkatest_f7524_0()
{    Schema oldSchema = new Schema(new Field("field1", Type.NULLABLE_STRING), new Field("field2", Type.NULLABLE_STRING));    Schema newSchema = new Schema(new Field("field1", Type.NULLABLE_STRING));    String value = "foo bar baz";    Struct oldFormat = new Struct(oldSchema).set("field1", value).set("field2", "fine to ignore");    ByteBuffer buffer = ByteBuffer.allocate(oldSchema.sizeOf(oldFormat));    oldFormat.writeTo(buffer);    buffer.flip();    Struct newFormat = newSchema.read(buffer);    assertEquals(value, newFormat.get("field1"));}
f7524
0
testSetLogAppendTimeNotAllowedV0
public void kafkatest_f7533_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V0, 0L, CompressionType.GZIP, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    long logAppendTime = 15L;    ByteBufferLegacyRecordBatch batch = new ByteBufferLegacyRecordBatch(records.buffer());    batch.setMaxTimestamp(TimestampType.LOG_APPEND_TIME, logAppendTime);}
f7533
0
testSetCreateTimeNotAllowedV0
public void kafkatest_f7534_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V0, 0L, CompressionType.GZIP, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    long createTime = 15L;    ByteBufferLegacyRecordBatch batch = new ByteBufferLegacyRecordBatch(records.buffer());    batch.setMaxTimestamp(TimestampType.CREATE_TIME, createTime);}
f7534
0
iteratorRaisesOnInvalidMagic
public void kafkatest_f7543_0()
{    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    builder.append(15L, "a".getBytes(), "1".getBytes());    builder.append(20L, "b".getBytes(), "2".getBytes());    builder.close();    int position = buffer.position();    builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 2L);    builder.append(30L, "c".getBytes(), "3".getBytes());    builder.append(40L, "d".getBytes(), "4".getBytes());    builder.close();    buffer.flip();    buffer.put(position + DefaultRecordBatch.MAGIC_OFFSET, (byte) 37);    ByteBufferLogInputStream logInputStream = new ByteBufferLogInputStream(buffer, Integer.MAX_VALUE);    assertNotNull(logInputStream.nextBatch());    logInputStream.nextBatch();}
f7543
0
iteratorRaisesOnTooLargeRecords
public void kafkatest_f7544_0()
{    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    builder.append(15L, "a".getBytes(), "1".getBytes());    builder.append(20L, "b".getBytes(), "2".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 2L);    builder.append(30L, "c".getBytes(), "3".getBytes());    builder.append(40L, "d".getBytes(), "4".getBytes());    builder.close();    buffer.flip();    ByteBufferLogInputStream logInputStream = new ByteBufferLogInputStream(buffer, 25);    assertNotNull(logInputStream.nextBatch());    logInputStream.nextBatch();}
f7544
0
testSizeInBytes
public void kafkatest_f7553_0()
{    Header[] headers = new Header[] { new RecordHeader("foo", "value".getBytes()), new RecordHeader("bar", (byte[]) null) };    long timestamp = System.currentTimeMillis();    SimpleRecord[] records = new SimpleRecord[] { new SimpleRecord(timestamp, "key".getBytes(), "value".getBytes()), new SimpleRecord(timestamp + 30000, null, "value".getBytes()), new SimpleRecord(timestamp + 60000, "key".getBytes(), null), new SimpleRecord(timestamp + 60000, "key".getBytes(), "value".getBytes(), headers) };    int actualSize = MemoryRecords.withRecords(CompressionType.NONE, records).sizeInBytes();    assertEquals(actualSize, DefaultRecordBatch.sizeInBytes(Arrays.asList(records)));}
f7553
0
testInvalidRecordSize
public void kafkatest_f7554_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V2, 0L, CompressionType.NONE, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    ByteBuffer buffer = records.buffer();    buffer.putInt(DefaultRecordBatch.LENGTH_OFFSET, 10);    DefaultRecordBatch batch = new DefaultRecordBatch(buffer);    assertFalse(batch.isValid());    batch.ensureValid();}
f7554
0
testSetNoTimestampTypeNotAllowed
public void kafkatest_f7563_0()
{    MemoryRecords records = MemoryRecords.withRecords(RecordBatch.MAGIC_VALUE_V2, 0L, CompressionType.NONE, TimestampType.CREATE_TIME, new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()));    DefaultRecordBatch batch = new DefaultRecordBatch(records.buffer());    batch.setMaxTimestamp(TimestampType.NO_TIMESTAMP_TYPE, RecordBatch.NO_TIMESTAMP);}
f7563
0
testReadAndWriteControlBatch
public void kafkatest_f7564_0()
{    long producerId = 1L;    short producerEpoch = 0;    int coordinatorEpoch = 15;    ByteBuffer buffer = ByteBuffer.allocate(128);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, CompressionType.NONE, TimestampType.CREATE_TIME, 0L, RecordBatch.NO_TIMESTAMP, producerId, producerEpoch, RecordBatch.NO_SEQUENCE, true, true, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.remaining());    EndTransactionMarker marker = new EndTransactionMarker(ControlRecordType.COMMIT, coordinatorEpoch);    builder.appendEndTxnMarker(System.currentTimeMillis(), marker);    MemoryRecords records = builder.build();    List<MutableRecordBatch> batches = TestUtils.toList(records.batches());    assertEquals(1, batches.size());    MutableRecordBatch batch = batches.get(0);    assertTrue(batch.isControlBatch());    List<Record> logRecords = TestUtils.toList(records.records());    assertEquals(1, logRecords.size());    Record commitRecord = logRecords.get(0);    assertEquals(marker, EndTransactionMarker.deserialize(commitRecord));}
f7564
0
testInvalidKeySize
public void kafkatest_f7573_0()
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    // use a key size larger than the full message    int keySize = 105;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    ByteUtils.writeVarint(keySize, buf);    buf.position(buf.limit());    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7573
0
testInvalidKeySizePartial
public void kafkatest_f7574_0() throws IOException
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    // use a key size larger than the full message    int keySize = 105;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    ByteUtils.writeVarint(keySize, buf);    buf.position(buf.limit());    buf.flip();    DataInputStream inputStream = new DataInputStream(new ByteBufferInputStream(buf));    DefaultRecord.readPartiallyFrom(inputStream, skipArray, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7574
0
testInvalidHeaderValue
public void kafkatest_f7583_0()
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    // null key    ByteUtils.writeVarint(-1, buf);    // null value    ByteUtils.writeVarint(-1, buf);    ByteUtils.writeVarint(1, buf);    ByteUtils.writeVarint(1, buf);    buf.put((byte) 1);    // header value too long    ByteUtils.writeVarint(105, buf);    buf.position(buf.limit());    buf.flip();    DefaultRecord.readFrom(buf, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7583
0
testInvalidHeaderValuePartial
public void kafkatest_f7584_0() throws IOException
{    byte attributes = 0;    long timestampDelta = 2;    int offsetDelta = 1;    int sizeOfBodyInBytes = 100;    ByteBuffer buf = ByteBuffer.allocate(sizeOfBodyInBytes + ByteUtils.sizeOfVarint(sizeOfBodyInBytes));    ByteUtils.writeVarint(sizeOfBodyInBytes, buf);    buf.put(attributes);    ByteUtils.writeVarlong(timestampDelta, buf);    ByteUtils.writeVarint(offsetDelta, buf);    // null key    ByteUtils.writeVarint(-1, buf);    // null value    ByteUtils.writeVarint(-1, buf);    ByteUtils.writeVarint(1, buf);    ByteUtils.writeVarint(1, buf);    buf.put((byte) 1);    // header value too long    ByteUtils.writeVarint(105, buf);    buf.position(buf.limit());    buf.flip();    DataInputStream inputStream = new DataInputStream(new ByteBufferInputStream(buf));    DefaultRecord.readPartiallyFrom(inputStream, skipArray, 0L, 0L, RecordBatch.NO_SEQUENCE, null);}
f7584
0
testSerde
public void kafkatest_f7593_0()
{    int coordinatorEpoch = 79;    EndTransactionMarker marker = new EndTransactionMarker(ControlRecordType.COMMIT, coordinatorEpoch);    ByteBuffer buffer = marker.serializeValue();    EndTransactionMarker deserialized = EndTransactionMarker.deserializeValue(ControlRecordType.COMMIT, buffer);    assertEquals(coordinatorEpoch, deserialized.coordinatorEpoch());}
f7593
0
testDeserializeNewerVersion
public void kafkatest_f7594_0()
{    int coordinatorEpoch = 79;    ByteBuffer buffer = ByteBuffer.allocate(8);    buffer.putShort((short) 5);    buffer.putInt(coordinatorEpoch);    // unexpected data    buffer.putShort((short) 0);    buffer.flip();    EndTransactionMarker deserialized = EndTransactionMarker.deserializeValue(ControlRecordType.COMMIT, buffer);    assertEquals(coordinatorEpoch, deserialized.coordinatorEpoch());}
f7594
0
assertNoProducerData
private void kafkatest_f7603_0(RecordBatch batch)
{    assertEquals(RecordBatch.NO_PRODUCER_ID, batch.producerId());    assertEquals(RecordBatch.NO_PRODUCER_EPOCH, batch.producerEpoch());    assertEquals(RecordBatch.NO_SEQUENCE, batch.baseSequence());    assertEquals(RecordBatch.NO_SEQUENCE, batch.lastSequence());    assertFalse(batch.isTransactional());}
f7603
0
assertGenericRecordBatchData
private void kafkatest_f7604_0(RecordBatch batch, long baseOffset, long maxTimestamp, SimpleRecord... records)
{    assertEquals(magic, batch.magic());    assertEquals(compression, batch.compressionType());    if (magic == MAGIC_VALUE_V0) {        assertEquals(NO_TIMESTAMP_TYPE, batch.timestampType());    } else {        assertEquals(CREATE_TIME, batch.timestampType());        assertEquals(maxTimestamp, batch.maxTimestamp());    }    assertEquals(baseOffset + records.length - 1, batch.lastOffset());    if (magic >= MAGIC_VALUE_V2)        assertEquals(Integer.valueOf(records.length), batch.countOrNull());    assertEquals(baseOffset, batch.baseOffset());    assertTrue(batch.isValid());    List<Record> batchRecords = TestUtils.toList(batch);    for (int i = 0; i < records.length; i++) {        assertEquals(baseOffset + i, batchRecords.get(i).offset());        assertEquals(records[i].key(), batchRecords.get(i).key());        assertEquals(records[i].value(), batchRecords.get(i).value());        if (magic == MAGIC_VALUE_V0)            assertEquals(NO_TIMESTAMP, batchRecords.get(i).timestamp());        else            assertEquals(records[i].timestamp(), batchRecords.get(i).timestamp());    }}
f7604
0
testIterationDoesntChangePosition
public void kafkatest_f7613_0() throws IOException
{    long position = fileRecords.channel().position();    Iterator<Record> records = fileRecords.records().iterator();    for (byte[] value : values) {        assertTrue(records.hasNext());        assertEquals(records.next().value(), ByteBuffer.wrap(value));    }    assertEquals(position, fileRecords.channel().position());}
f7613
0
testRead
public void kafkatest_f7614_0() throws IOException
{    FileRecords read = fileRecords.slice(0, fileRecords.sizeInBytes());    assertEquals(fileRecords.sizeInBytes(), read.sizeInBytes());    TestUtils.checkEquals(fileRecords.batches(), read.batches());    List<RecordBatch> items = batches(read);    RecordBatch first = items.get(0);    // read from second message until the end    read = fileRecords.slice(first.sizeInBytes(), fileRecords.sizeInBytes() - first.sizeInBytes());    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read from second message and size is past the end of the file    read = fileRecords.slice(first.sizeInBytes(), fileRecords.sizeInBytes());    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read from second message and position + size overflows    read = fileRecords.slice(first.sizeInBytes(), Integer.MAX_VALUE);    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read from second message and size is past the end of the file on a view/slice    read = fileRecords.slice(1, fileRecords.sizeInBytes() - 1).slice(first.sizeInBytes() - 1, fileRecords.sizeInBytes());    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read from second message and position + size overflows on a view/slice    read = fileRecords.slice(1, fileRecords.sizeInBytes() - 1).slice(first.sizeInBytes() - 1, Integer.MAX_VALUE);    assertEquals(fileRecords.sizeInBytes() - first.sizeInBytes(), read.sizeInBytes());    assertEquals("Read starting from the second message", items.subList(1, items.size()), batches(read));    // read a single message starting from second message    RecordBatch second = items.get(1);    read = fileRecords.slice(first.sizeInBytes(), second.sizeInBytes());    assertEquals(second.sizeInBytes(), read.sizeInBytes());    assertEquals("Read a single message starting from the second message", Collections.singletonList(second), batches(read));}
f7614
0
testPreallocateClearShutdown
public void kafkatest_f7623_0() throws IOException
{    File temp = tempFile();    FileRecords fileRecords = FileRecords.open(temp, false, 1024 * 1024, true);    append(fileRecords, values);    int oldPosition = (int) fileRecords.channel().position();    int oldSize = fileRecords.sizeInBytes();    assertEquals(this.fileRecords.sizeInBytes(), oldPosition);    assertEquals(this.fileRecords.sizeInBytes(), oldSize);    fileRecords.close();    File tempReopen = new File(temp.getAbsolutePath());    FileRecords setReopen = FileRecords.open(tempReopen, true, 1024 * 1024, true);    int position = (int) setReopen.channel().position();    int size = setReopen.sizeInBytes();    assertEquals(oldPosition, position);    assertEquals(oldPosition, size);    assertEquals(oldPosition, tempReopen.length());}
f7623
0
testFormatConversionWithPartialMessage
public void kafkatest_f7624_0() throws IOException
{    RecordBatch batch = batches(fileRecords).get(1);    int start = fileRecords.searchForOffsetWithSize(1, 0).position;    int size = batch.sizeInBytes();    FileRecords slice = fileRecords.slice(start, size - 1);    Records messageV0 = slice.downConvert(RecordBatch.MAGIC_VALUE_V0, 0, time).records();    assertTrue("No message should be there", batches(messageV0).isEmpty());    assertEquals("There should be " + (size - 1) + " bytes", size - 1, messageV0.sizeInBytes());    // Lazy down-conversion will not return any messages for a partial input batch    TopicPartition tp = new TopicPartition("topic-1", 0);    LazyDownConversionRecords lazyRecords = new LazyDownConversionRecords(tp, slice, RecordBatch.MAGIC_VALUE_V0, 0, Time.SYSTEM);    Iterator<ConvertedRecords<?>> it = lazyRecords.iterator(16 * 1024L);    assertTrue("No messages should be returned", !it.hasNext());}
f7624
0
verifyConvertedRecords
private void kafkatest_f7633_0(List<SimpleRecord> initialRecords, List<Long> initialOffsets, List<Records> convertedRecordsList, CompressionType compressionType, byte magicByte)
{    int i = 0;    for (Records convertedRecords : convertedRecordsList) {        for (RecordBatch batch : convertedRecords.batches()) {            assertTrue("Magic byte should be lower than or equal to " + magicByte, batch.magic() <= magicByte);            if (batch.magic() == RecordBatch.MAGIC_VALUE_V0)                assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());            else                assertEquals(TimestampType.CREATE_TIME, batch.timestampType());            assertEquals("Compression type should not be affected by conversion", compressionType, batch.compressionType());            for (Record record : batch) {                assertTrue("Inner record should have magic " + magicByte, record.hasMagic(batch.magic()));                assertEquals("Offset should not change", initialOffsets.get(i).longValue(), record.offset());                assertEquals("Key should not change", utf8(initialRecords.get(i).key()), utf8(record.key()));                assertEquals("Value should not change", utf8(initialRecords.get(i).value()), utf8(record.value()));                assertFalse(record.hasTimestampType(TimestampType.LOG_APPEND_TIME));                if (batch.magic() == RecordBatch.MAGIC_VALUE_V0) {                    assertEquals(RecordBatch.NO_TIMESTAMP, record.timestamp());                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                    assertTrue(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                } else if (batch.magic() == RecordBatch.MAGIC_VALUE_V1) {                    assertEquals("Timestamp should not change", initialRecords.get(i).timestamp(), record.timestamp());                    assertTrue(record.hasTimestampType(TimestampType.CREATE_TIME));                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                } else {                    assertEquals("Timestamp should not change", initialRecords.get(i).timestamp(), record.timestamp());                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                    assertArrayEquals("Headers should not change", initialRecords.get(i).headers(), record.headers());                }                i += 1;            }        }    }    assertEquals(initialOffsets.size(), i);}
f7633
0
batches
private static List<RecordBatch> kafkatest_f7634_0(Records buffer)
{    return TestUtils.toList(buffer.batches());}
f7634
0
testBadBlockSize
public void kafkatest_f7643_0() throws Exception
{    if (!close || (useBrokenFlagDescriptorChecksum && !ignoreFlagDescriptorChecksum))        return;    byte[] compressed = compressedBytes();    ByteBuffer buffer = ByteBuffer.wrap(compressed).order(ByteOrder.LITTLE_ENDIAN);    int blockSize = buffer.getInt(7);    blockSize = (blockSize & LZ4_FRAME_INCOMPRESSIBLE_MASK) | (1 << 24 & ~LZ4_FRAME_INCOMPRESSIBLE_MASK);    buffer.putInt(7, blockSize);    IOException e = assertThrows(IOException.class, () -> testDecompression(buffer));    assertThat(e.getMessage(), CoreMatchers.containsString("exceeded max"));}
f7643
0
testCompression
public void kafkatest_f7644_0() throws Exception
{    byte[] compressed = compressedBytes();    // Check magic bytes stored as little-endian    int offset = 0;    assertEquals(0x04, compressed[offset++]);    assertEquals(0x22, compressed[offset++]);    assertEquals(0x4D, compressed[offset++]);    assertEquals(0x18, compressed[offset++]);    // Check flg descriptor    byte flg = compressed[offset++];    // 2-bit version must be 01    int version = (flg >>> 6) & 3;    assertEquals(1, version);    // Reserved bits should always be 0    int reserved = flg & 3;    assertEquals(0, reserved);    // Check block descriptor    byte bd = compressed[offset++];    // Block max-size    int blockMaxSize = (bd >>> 4) & 7;    // Only supported values are 4 (64KB), 5 (256KB), 6 (1MB), 7 (4MB)    assertTrue(blockMaxSize >= 4);    assertTrue(blockMaxSize <= 7);    // Multiple reserved bit ranges in block descriptor    reserved = bd & 15;    assertEquals(0, reserved);    reserved = (bd >>> 7) & 1;    assertEquals(0, reserved);    // If flg descriptor sets content size flag    // there are 8 additional bytes before checksum    boolean contentSize = ((flg >>> 3) & 1) != 0;    if (contentSize)        offset += 8;    // Checksum applies to frame descriptor: flg, bd, and optional contentsize    // so initial offset should be 4 (for magic bytes)    int off = 4;    int len = offset - 4;    // including magic bytes    if (this.useBrokenFlagDescriptorChecksum) {        off = 0;        len = offset;    }    int hash = XXHashFactory.fastestInstance().hash32().hash(compressed, off, len, 0);    byte hc = compressed[offset++];    assertEquals((byte) ((hash >> 8) & 0xFF), hc);    // Check EndMark, data block with size `0` expressed as a 32-bits value    if (this.close) {        offset = compressed.length - 4;        assertEquals(0, compressed[offset++]);        assertEquals(0, compressed[offset++]);        assertEquals(0, compressed[offset++]);        assertEquals(0, compressed[offset++]);    }}
f7644
0
testConversion
public void kafkatest_f7653_0() throws IOException
{    doTestConversion(false);}
f7653
0
testConversionWithOverflow
public void kafkatest_f7654_0() throws IOException
{    doTestConversion(true);}
f7654
0
testWriteEmptyRecordSet
public void kafkatest_f7663_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V0;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    Supplier<MemoryRecordsBuilder> builderSupplier = () -> new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    if (compressionType != CompressionType.ZSTD) {        MemoryRecords records = builderSupplier.get().build();        assertEquals(0, records.sizeInBytes());        assertEquals(bufferOffset, buffer.position());    } else {        Exception e = assertThrows(IllegalArgumentException.class, () -> builderSupplier.get().build());        assertEquals(e.getMessage(), "ZStandard compression is not supported for magic " + magic);    }}
f7663
0
testWriteTransactionalRecordSet
public void kafkatest_f7664_0()
{    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    long pid = 9809;    short epoch = 15;    int sequence = 2342;    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, 0L, 0L, pid, epoch, sequence, true, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    builder.append(System.currentTimeMillis(), "foo".getBytes(), "bar".getBytes());    MemoryRecords records = builder.build();    List<MutableRecordBatch> batches = Utils.toList(records.batches().iterator());    assertEquals(1, batches.size());    assertTrue(batches.get(0).isTransactional());}
f7664
0
testWriteEndTxnMarkerNonControlBatch
public void kafkatest_f7673_0()
{    ByteBuffer buffer = ByteBuffer.allocate(128);    buffer.position(bufferOffset);    long pid = 9809;    short epoch = 15;    int sequence = RecordBatch.NO_SEQUENCE;    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.CURRENT_MAGIC_VALUE, compressionType, TimestampType.CREATE_TIME, 0L, 0L, pid, epoch, sequence, true, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    builder.appendEndTxnMarker(RecordBatch.NO_TIMESTAMP, new EndTransactionMarker(ControlRecordType.ABORT, 0));}
f7673
0
testCompressionRateV0
public void kafkatest_f7674_0()
{    byte magic = RecordBatch.MAGIC_VALUE_V0;    assumeAtLeastV2OrNotZstd(magic);    ByteBuffer buffer = ByteBuffer.allocate(1024);    buffer.position(bufferOffset);    LegacyRecord[] records = new LegacyRecord[] { LegacyRecord.create(magic, 0L, "a".getBytes(), "1".getBytes()), LegacyRecord.create(magic, 1L, "b".getBytes(), "2".getBytes()), LegacyRecord.create(magic, 2L, "c".getBytes(), "3".getBytes()) };    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType, TimestampType.CREATE_TIME, 0L, 0L, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());    int uncompressedSize = 0;    for (LegacyRecord record : records) {        uncompressedSize += record.sizeInBytes() + Records.LOG_OVERHEAD;        builder.append(record);    }    MemoryRecords built = builder.build();    if (compressionType == CompressionType.NONE) {        assertEquals(1.0, builder.compressionRatio(), 0.00001);    } else {        int compressedSize = built.sizeInBytes() - Records.LOG_OVERHEAD - LegacyRecord.RECORD_OVERHEAD_V0;        double computedCompressionRate = (double) compressedSize / uncompressedSize;        assertEquals(computedCompressionRate, builder.compressionRatio(), 0.00001);    }}
f7674
0
convertV2ToV1UsingMixedCreateAndLogAppendTime
public void kafkatest_f7683_0()
{    ByteBuffer buffer = ByteBuffer.allocate(512);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, compressionType, TimestampType.LOG_APPEND_TIME, 0L);    builder.append(10L, "1".getBytes(), "a".getBytes());    builder.close();    int sizeExcludingTxnMarkers = buffer.position();    MemoryRecords.writeEndTransactionalMarker(buffer, 1L, System.currentTimeMillis(), 0, 15L, (short) 0, new EndTransactionMarker(ControlRecordType.ABORT, 0));    int position = buffer.position();    builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, compressionType, TimestampType.CREATE_TIME, 1L);    builder.append(12L, "2".getBytes(), "b".getBytes());    builder.append(13L, "3".getBytes(), "c".getBytes());    builder.close();    sizeExcludingTxnMarkers += buffer.position() - position;    MemoryRecords.writeEndTransactionalMarker(buffer, 14L, System.currentTimeMillis(), 0, 1L, (short) 0, new EndTransactionMarker(ControlRecordType.COMMIT, 0));    buffer.flip();    Supplier<ConvertedRecords<MemoryRecords>> convertedRecordsSupplier = () -> MemoryRecords.readableRecords(buffer).downConvert(RecordBatch.MAGIC_VALUE_V1, 0, time);    if (compressionType != CompressionType.ZSTD) {        ConvertedRecords<MemoryRecords> convertedRecords = convertedRecordsSupplier.get();        MemoryRecords records = convertedRecords.records();        // Transactional markers are skipped when down converting to V1, so exclude them from size        verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 3, records.sizeInBytes(), sizeExcludingTxnMarkers);        List<? extends RecordBatch> batches = Utils.toList(records.batches().iterator());        if (compressionType != CompressionType.NONE) {            assertEquals(2, batches.size());            assertEquals(TimestampType.LOG_APPEND_TIME, batches.get(0).timestampType());            assertEquals(TimestampType.CREATE_TIME, batches.get(1).timestampType());        } else {            assertEquals(3, batches.size());            assertEquals(TimestampType.LOG_APPEND_TIME, batches.get(0).timestampType());            assertEquals(TimestampType.CREATE_TIME, batches.get(1).timestampType());            assertEquals(TimestampType.CREATE_TIME, batches.get(2).timestampType());        }        List<Record> logRecords = Utils.toList(records.records().iterator());        assertEquals(3, logRecords.size());        assertEquals(ByteBuffer.wrap("1".getBytes()), logRecords.get(0).key());        assertEquals(ByteBuffer.wrap("2".getBytes()), logRecords.get(1).key());        assertEquals(ByteBuffer.wrap("3".getBytes()), logRecords.get(2).key());    } else {        Exception e = assertThrows(UnsupportedCompressionTypeException.class, convertedRecordsSupplier::get);        assertEquals("Down-conversion of zstandard-compressed batches is not supported", e.getMessage());    }}
f7683
0
convertToV1WithMixedV0AndV2Data
public void kafkatest_f7684_0()
{    assumeAtLeastV2OrNotZstd(RecordBatch.MAGIC_VALUE_V0);    assumeAtLeastV2OrNotZstd(RecordBatch.MAGIC_VALUE_V1);    ByteBuffer buffer = ByteBuffer.allocate(512);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V0, compressionType, TimestampType.NO_TIMESTAMP_TYPE, 0L);    builder.append(RecordBatch.NO_TIMESTAMP, "1".getBytes(), "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, RecordBatch.MAGIC_VALUE_V2, compressionType, TimestampType.CREATE_TIME, 1L);    builder.append(11L, "2".getBytes(), "b".getBytes());    builder.append(12L, "3".getBytes(), "c".getBytes());    builder.close();    buffer.flip();    ConvertedRecords<MemoryRecords> convertedRecords = MemoryRecords.readableRecords(buffer).downConvert(RecordBatch.MAGIC_VALUE_V1, 0, time);    MemoryRecords records = convertedRecords.records();    verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 2, records.sizeInBytes(), buffer.limit());    List<? extends RecordBatch> batches = Utils.toList(records.batches().iterator());    if (compressionType != CompressionType.NONE) {        assertEquals(2, batches.size());        assertEquals(RecordBatch.MAGIC_VALUE_V0, batches.get(0).magic());        assertEquals(0, batches.get(0).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(1).magic());        assertEquals(1, batches.get(1).baseOffset());    } else {        assertEquals(3, batches.size());        assertEquals(RecordBatch.MAGIC_VALUE_V0, batches.get(0).magic());        assertEquals(0, batches.get(0).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(1).magic());        assertEquals(1, batches.get(1).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(2).magic());        assertEquals(2, batches.get(2).baseOffset());    }    List<Record> logRecords = Utils.toList(records.records().iterator());    assertEquals("1", utf8(logRecords.get(0).key()));    assertEquals("2", utf8(logRecords.get(1).key()));    assertEquals("3", utf8(logRecords.get(2).key()));    convertedRecords = MemoryRecords.readableRecords(buffer).downConvert(RecordBatch.MAGIC_VALUE_V1, 2L, time);    records = convertedRecords.records();    batches = Utils.toList(records.batches().iterator());    logRecords = Utils.toList(records.records().iterator());    if (compressionType != CompressionType.NONE) {        assertEquals(2, batches.size());        assertEquals(RecordBatch.MAGIC_VALUE_V0, batches.get(0).magic());        assertEquals(0, batches.get(0).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(1).magic());        assertEquals(1, batches.get(1).baseOffset());        assertEquals("1", utf8(logRecords.get(0).key()));        assertEquals("2", utf8(logRecords.get(1).key()));        assertEquals("3", utf8(logRecords.get(2).key()));        verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 2, records.sizeInBytes(), buffer.limit());    } else {        assertEquals(2, batches.size());        assertEquals(RecordBatch.MAGIC_VALUE_V0, batches.get(0).magic());        assertEquals(0, batches.get(0).baseOffset());        assertEquals(RecordBatch.MAGIC_VALUE_V1, batches.get(1).magic());        assertEquals(2, batches.get(1).baseOffset());        assertEquals("1", utf8(logRecords.get(0).key()));        assertEquals("3", utf8(logRecords.get(1).key()));        verifyRecordsProcessingStats(convertedRecords.recordConversionStats(), 3, 1, records.sizeInBytes(), buffer.limit());    }}
f7684
0
testIterator
public void kafkatest_f7693_0()
{    assumeAtLeastV2OrNotZstd();    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compression, TimestampType.CREATE_TIME, firstOffset, logAppendTime, pid, epoch, firstSequence, false, false, partitionLeaderEpoch, buffer.limit());    SimpleRecord[] records = new SimpleRecord[] { new SimpleRecord(1L, "a".getBytes(), "1".getBytes()), new SimpleRecord(2L, "b".getBytes(), "2".getBytes()), new SimpleRecord(3L, "c".getBytes(), "3".getBytes()), new SimpleRecord(4L, null, "4".getBytes()), new SimpleRecord(5L, "d".getBytes(), null), new SimpleRecord(6L, (byte[]) null, null) };    for (SimpleRecord record : records) builder.append(record);    MemoryRecords memoryRecords = builder.build();    for (int iteration = 0; iteration < 2; iteration++) {        int total = 0;        for (RecordBatch batch : memoryRecords.batches()) {            assertTrue(batch.isValid());            assertEquals(compression, batch.compressionType());            assertEquals(firstOffset + total, batch.baseOffset());            if (magic >= RecordBatch.MAGIC_VALUE_V2) {                assertEquals(pid, batch.producerId());                assertEquals(epoch, batch.producerEpoch());                assertEquals(firstSequence + total, batch.baseSequence());                assertEquals(partitionLeaderEpoch, batch.partitionLeaderEpoch());                assertEquals(records.length, batch.countOrNull().intValue());                assertEquals(TimestampType.CREATE_TIME, batch.timestampType());                assertEquals(records[records.length - 1].timestamp(), batch.maxTimestamp());            } else {                assertEquals(RecordBatch.NO_PRODUCER_ID, batch.producerId());                assertEquals(RecordBatch.NO_PRODUCER_EPOCH, batch.producerEpoch());                assertEquals(RecordBatch.NO_SEQUENCE, batch.baseSequence());                assertEquals(RecordBatch.NO_PARTITION_LEADER_EPOCH, batch.partitionLeaderEpoch());                assertNull(batch.countOrNull());                if (magic == RecordBatch.MAGIC_VALUE_V0)                    assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());                else                    assertEquals(TimestampType.CREATE_TIME, batch.timestampType());            }            int recordCount = 0;            for (Record record : batch) {                assertTrue(record.isValid());                assertTrue(record.hasMagic(batch.magic()));                assertFalse(record.isCompressed());                assertEquals(firstOffset + total, record.offset());                assertEquals(records[total].key(), record.key());                assertEquals(records[total].value(), record.value());                if (magic >= RecordBatch.MAGIC_VALUE_V2)                    assertEquals(firstSequence + total, record.sequence());                assertFalse(record.hasTimestampType(TimestampType.LOG_APPEND_TIME));                if (magic == RecordBatch.MAGIC_VALUE_V0) {                    assertEquals(RecordBatch.NO_TIMESTAMP, record.timestamp());                    assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                    assertTrue(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                } else {                    assertEquals(records[total].timestamp(), record.timestamp());                    assertFalse(record.hasTimestampType(TimestampType.NO_TIMESTAMP_TYPE));                    if (magic < RecordBatch.MAGIC_VALUE_V2)                        assertTrue(record.hasTimestampType(TimestampType.CREATE_TIME));                    else                        assertFalse(record.hasTimestampType(TimestampType.CREATE_TIME));                }                total++;                recordCount++;            }            assertEquals(batch.baseOffset() + recordCount - 1, batch.lastOffset());        }    }}
f7693
0
testHasRoomForMethod
public void kafkatest_f7694_0()
{    assumeAtLeastV2OrNotZstd();    MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), magic, compression, TimestampType.CREATE_TIME, 0L);    builder.append(0L, "a".getBytes(), "1".getBytes());    assertTrue(builder.hasRoomFor(1L, "b".getBytes(), "2".getBytes(), Record.EMPTY_HEADERS));    builder.close();    assertFalse(builder.hasRoomFor(1L, "b".getBytes(), "2".getBytes(), Record.EMPTY_HEADERS));}
f7694
0
shouldRetainRecord
protected boolean kafkatest_f7703_0(RecordBatch recordBatch, Record record)
{    return false;}
f7703
0
testEmptyBatchDeletion
public void kafkatest_f7704_0()
{    if (magic >= RecordBatch.MAGIC_VALUE_V2) {        for (final BatchRetention deleteRetention : Arrays.asList(BatchRetention.DELETE, BatchRetention.DELETE_EMPTY)) {            ByteBuffer buffer = ByteBuffer.allocate(DefaultRecordBatch.RECORD_BATCH_OVERHEAD);            long producerId = 23L;            short producerEpoch = 5;            long baseOffset = 3L;            int baseSequence = 10;            int partitionLeaderEpoch = 293;            long timestamp = System.currentTimeMillis();            DefaultRecordBatch.writeEmptyHeader(buffer, RecordBatch.MAGIC_VALUE_V2, producerId, producerEpoch, baseSequence, baseOffset, baseOffset, partitionLeaderEpoch, TimestampType.CREATE_TIME, timestamp, false, false);            buffer.flip();            ByteBuffer filtered = ByteBuffer.allocate(2048);            MemoryRecords records = MemoryRecords.readableRecords(buffer);            MemoryRecords.FilterResult filterResult = records.filterTo(new TopicPartition("foo", 0), new MemoryRecords.RecordFilter() {                @Override                protected BatchRetention checkBatchRetention(RecordBatch batch) {                    return deleteRetention;                }                @Override                protected boolean shouldRetainRecord(RecordBatch recordBatch, Record record) {                    return false;                }            }, filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);            // Verify filter result            assertEquals(0, filterResult.outputBuffer().position());            // Verify filtered records            filtered.flip();            MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);            assertEquals(0, filteredRecords.sizeInBytes());        }    }}
f7704
0
testFilterToWithUndersizedBuffer
public void kafkatest_f7713_0()
{    assumeAtLeastV2OrNotZstd();    ByteBuffer buffer = ByteBuffer.allocate(1024);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 0L);    builder.append(10L, null, "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 1L);    builder.append(11L, "1".getBytes(), new byte[128]);    builder.append(12L, "2".getBytes(), "c".getBytes());    builder.append(13L, null, "d".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 4L);    builder.append(14L, null, "e".getBytes());    builder.append(15L, "5".getBytes(), "f".getBytes());    builder.append(16L, "6".getBytes(), "g".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 7L);    builder.append(17L, "7".getBytes(), new byte[128]);    builder.close();    buffer.flip();    ByteBuffer output = ByteBuffer.allocate(64);    List<Record> records = new ArrayList<>();    while (buffer.hasRemaining()) {        output.rewind();        MemoryRecords.FilterResult result = MemoryRecords.readableRecords(buffer).filterTo(new TopicPartition("foo", 0), new RetainNonNullKeysFilter(), output, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);        buffer.position(buffer.position() + result.bytesRead());        result.outputBuffer().flip();        if (output != result.outputBuffer())            assertEquals(0, output.position());        MemoryRecords filtered = MemoryRecords.readableRecords(result.outputBuffer());        records.addAll(TestUtils.toList(filtered.records()));    }    assertEquals(5, records.size());    for (Record record : records) assertNotNull(record.key());}
f7713
0
testFilterTo
public void kafkatest_f7714_0()
{    assumeAtLeastV2OrNotZstd();    ByteBuffer buffer = ByteBuffer.allocate(2048);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 0L);    builder.append(10L, null, "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 1L);    builder.append(11L, "1".getBytes(), "b".getBytes());    builder.append(12L, null, "c".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 3L);    builder.append(13L, null, "d".getBytes());    builder.append(20L, "4".getBytes(), "e".getBytes());    builder.append(15L, "5".getBytes(), "f".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, magic, compression, TimestampType.CREATE_TIME, 6L);    builder.append(16L, "6".getBytes(), "g".getBytes());    builder.close();    buffer.flip();    ByteBuffer filtered = ByteBuffer.allocate(2048);    MemoryRecords.FilterResult result = MemoryRecords.readableRecords(buffer).filterTo(new TopicPartition("foo", 0), new RetainNonNullKeysFilter(), filtered, Integer.MAX_VALUE, BufferSupplier.NO_CACHING);    filtered.flip();    assertEquals(7, result.messagesRead());    assertEquals(4, result.messagesRetained());    assertEquals(buffer.limit(), result.bytesRead());    assertEquals(filtered.limit(), result.bytesRetained());    if (magic > RecordBatch.MAGIC_VALUE_V0) {        assertEquals(20L, result.maxTimestamp());        if (compression == CompressionType.NONE && magic < RecordBatch.MAGIC_VALUE_V2)            assertEquals(4L, result.shallowOffsetOfMaxTimestamp());        else            assertEquals(5L, result.shallowOffsetOfMaxTimestamp());    }    MemoryRecords filteredRecords = MemoryRecords.readableRecords(filtered);    List<MutableRecordBatch> batches = TestUtils.toList(filteredRecords.batches());    final List<Long> expectedEndOffsets;    final List<Long> expectedStartOffsets;    final List<Long> expectedMaxTimestamps;    if (magic < RecordBatch.MAGIC_VALUE_V2 && compression == CompressionType.NONE) {        expectedEndOffsets = asList(1L, 4L, 5L, 6L);        expectedStartOffsets = asList(1L, 4L, 5L, 6L);        expectedMaxTimestamps = asList(11L, 20L, 15L, 16L);    } else if (magic < RecordBatch.MAGIC_VALUE_V2) {        expectedEndOffsets = asList(1L, 5L, 6L);        expectedStartOffsets = asList(1L, 4L, 6L);        expectedMaxTimestamps = asList(11L, 20L, 16L);    } else {        expectedEndOffsets = asList(2L, 5L, 6L);        expectedStartOffsets = asList(1L, 3L, 6L);        expectedMaxTimestamps = asList(11L, 20L, 16L);    }    assertEquals(expectedEndOffsets.size(), batches.size());    for (int i = 0; i < expectedEndOffsets.size(); i++) {        RecordBatch batch = batches.get(i);        assertEquals(expectedStartOffsets.get(i).longValue(), batch.baseOffset());        assertEquals(expectedEndOffsets.get(i).longValue(), batch.lastOffset());        assertEquals(magic, batch.magic());        assertEquals(compression, batch.compressionType());        if (magic >= RecordBatch.MAGIC_VALUE_V1) {            assertEquals(expectedMaxTimestamps.get(i).longValue(), batch.maxTimestamp());            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());        } else {            assertEquals(RecordBatch.NO_TIMESTAMP, batch.maxTimestamp());            assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());        }    }    List<Record> records = TestUtils.toList(filteredRecords.records());    assertEquals(4, records.size());    Record first = records.get(0);    assertEquals(1L, first.offset());    if (magic > RecordBatch.MAGIC_VALUE_V0)        assertEquals(11L, first.timestamp());    assertEquals("1", Utils.utf8(first.key(), first.keySize()));    assertEquals("b", Utils.utf8(first.value(), first.valueSize()));    Record second = records.get(1);    assertEquals(4L, second.offset());    if (magic > RecordBatch.MAGIC_VALUE_V0)        assertEquals(20L, second.timestamp());    assertEquals("4", Utils.utf8(second.key(), second.keySize()));    assertEquals("e", Utils.utf8(second.value(), second.valueSize()));    Record third = records.get(2);    assertEquals(5L, third.offset());    if (magic > RecordBatch.MAGIC_VALUE_V0)        assertEquals(15L, third.timestamp());    assertEquals("5", Utils.utf8(third.key(), third.keySize()));    assertEquals("f", Utils.utf8(third.value(), third.valueSize()));    Record fourth = records.get(3);    assertEquals(6L, fourth.offset());    if (magic > RecordBatch.MAGIC_VALUE_V0)        assertEquals(16L, fourth.timestamp());    assertEquals("6", Utils.utf8(fourth.key(), fourth.keySize()));    assertEquals("g", Utils.utf8(fourth.value(), fourth.valueSize()));}
f7714
0
write
public long kafkatest_f7723_0(ByteBuffer[] srcs) throws IOException
{    // which allows us to test the MultiRecordsSend behavior on a per-send basis.    if (!buffer().hasRemaining())        return 0;    return super.write(srcs);}
f7723
0
testCompressedIterationWithNullValue
public void kafkatest_f7724_0() throws Exception
{    ByteBuffer buffer = ByteBuffer.allocate(128);    DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));    AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);    LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null, CompressionType.GZIP, TimestampType.CREATE_TIME);    buffer.flip();    MemoryRecords records = MemoryRecords.readableRecords(buffer);    if (records.records().iterator().hasNext())        fail("Iteration should have caused invalid record error");}
f7724
0
shouldCreateApiResponseOnlyWithKeysSupportedByMagicValue
public void kafkatest_f7733_0()
{    final ApiVersionsResponse response = ApiVersionsResponse.apiVersionsResponse(10, RecordBatch.MAGIC_VALUE_V1);    verifyApiKeysForMagic(response, RecordBatch.MAGIC_VALUE_V1);    assertEquals(10, response.throttleTimeMs());}
f7733
0
shouldCreateApiResponseThatHasAllApiKeysSupportedByBroker
public void kafkatest_f7734_0()
{    assertEquals(apiKeysInResponse(ApiVersionsResponse.defaultApiVersionsResponse()), Utils.mkSet(ApiKeys.values()));}
f7734
0
close
public void kafkatest_f7743_0() throws IOException
{    buf.flip();    closed = true;}
f7743
0
buffer
public ByteBuffer kafkatest_f7744_0()
{    return buf;}
f7744
0
assertRequestEquals
private static void kafkatest_f7753_0(final CreateAclsRequest original, final CreateAclsRequest actual)
{    assertEquals("Number of Acls wrong", original.aclCreations().size(), actual.aclCreations().size());    for (int idx = 0; idx != original.aclCreations().size(); ++idx) {        final AclBinding originalBinding = original.aclCreations().get(idx).acl();        final AclBinding actualBinding = actual.aclCreations().get(idx).acl();        assertEquals(originalBinding, actualBinding);    }}
f7753
0
aclCreations
private static List<AclCreation> kafkatest_f7754_0(final AclBinding... acls)
{    return Arrays.stream(acls).map(AclCreation::new).collect(Collectors.toList());}
f7754
0
shouldThrowOnIfUnknown
public void kafkatest_f7763_0()
{    new DeleteAclsResponse(10, aclResponses(UNKNOWN_RESPONSE)).toStruct(V1);}
f7763
0
shouldRoundTripV0
public void kafkatest_f7764_0()
{    final DeleteAclsResponse original = new DeleteAclsResponse(10, aclResponses(LITERAL_RESPONSE));    final Struct struct = original.toStruct(V0);    final DeleteAclsResponse result = new DeleteAclsResponse(struct);    assertResponseEquals(original, result);}
f7764
0
shouldThrowIfUnknown
public void kafkatest_f7773_0()
{    new DescribeAclsRequest(UNKNOWN_FILTER, V0);}
f7773
0
shouldRoundTripLiteralV0
public void kafkatest_f7774_0()
{    final DescribeAclsRequest original = new DescribeAclsRequest(LITERAL_FILTER, V0);    final Struct struct = original.toStruct();    final DescribeAclsRequest result = new DescribeAclsRequest(struct, V0);    assertRequestEquals(original, result);}
f7774
0
shouldRoundTripV1
public void kafkatest_f7783_0()
{    final DescribeAclsResponse original = new DescribeAclsResponse(100, ApiError.NONE, aclBindings(LITERAL_ACL1, PREFIXED_ACL1));    final Struct struct = original.toStruct(V1);    final DescribeAclsResponse result = new DescribeAclsResponse(struct);    assertResponseEquals(original, result);}
f7783
0
assertResponseEquals
private static void kafkatest_f7784_0(final DescribeAclsResponse original, final DescribeAclsResponse actual)
{    final Set<AclBinding> originalBindings = new HashSet<>(original.acls());    final Set<AclBinding> actualBindings = new HashSet<>(actual.acls());    assertEquals(originalBindings, actualBindings);}
f7784
0
testErrorCountsWithTopLevelError
public void kafkatest_f7793_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.NOT_LEADER_FOR_PARTITION);    LeaderAndIsrResponse response = new LeaderAndIsrResponse(Errors.UNKNOWN_SERVER_ERROR, errors);    assertEquals(Collections.singletonMap(Errors.UNKNOWN_SERVER_ERROR, 2), response.errorCounts());}
f7793
0
testErrorCountsNoTopLevelError
public void kafkatest_f7794_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.CLUSTER_AUTHORIZATION_FAILED);    LeaderAndIsrResponse response = new LeaderAndIsrResponse(Errors.NONE, errors);    Map<Errors, Integer> errorCounts = response.errorCounts();    assertEquals(2, errorCounts.size());    assertEquals(1, errorCounts.get(Errors.NONE).intValue());    assertEquals(1, errorCounts.get(Errors.CLUSTER_AUTHORIZATION_FAILED).intValue());}
f7794
0
testShouldThrottle
public void kafkatest_f7803_0()
{    LeaveGroupResponse response = new LeaveGroupResponse(new LeaveGroupResponseData());    for (short version = 0; version <= ApiKeys.LEAVE_GROUP.latestVersion(); version++) {        if (version >= 2) {            assertTrue(response.shouldClientThrottle(version));        } else {            assertFalse(response.shouldClientThrottle(version));        }    }}
f7803
0
testEqualityWithStruct
public void kafkatest_f7804_0()
{    LeaveGroupResponseData responseData = new LeaveGroupResponseData().setErrorCode(Errors.NONE.code()).setThrottleTimeMs(throttleTimeMs);    for (short version = 0; version <= ApiKeys.LEAVE_GROUP.latestVersion(); version++) {        LeaveGroupResponse primaryResponse = new LeaveGroupResponse(responseData.toStruct(version), version);        LeaveGroupResponse secondaryResponse = new LeaveGroupResponse(responseData.toStruct(version), version);        assertEquals(primaryResponse, primaryResponse);        assertEquals(primaryResponse, secondaryResponse);        assertEquals(primaryResponse.hashCode(), secondaryResponse.hashCode());    }}
f7804
0
setUp
public void kafkatest_f7813_0()
{    expectedErrorCounts = new HashMap<>();    expectedErrorCounts.put(errorOne, 1);    expectedErrorCounts.put(errorTwo, 1);    errorsMap = new HashMap<>();    errorsMap.put(tp1, errorOne);    errorsMap.put(tp2, errorTwo);}
f7813
0
testConstructorWithErrorResponse
public void kafkatest_f7814_0()
{    OffsetCommitResponse response = new OffsetCommitResponse(throttleTimeMs, errorsMap);    assertEquals(expectedErrorCounts, response.errorCounts());    assertEquals(throttleTimeMs, response.throttleTimeMs());}
f7814
0
testNullableMetadata
public void kafkatest_f7823_0()
{    partitionDataMap.clear();    partitionDataMap.put(new TopicPartition(topicOne, partitionOne), new PartitionData(offset, leaderEpochOne, null, Errors.UNKNOWN_TOPIC_OR_PARTITION));    OffsetFetchResponse response = new OffsetFetchResponse(throttleTimeMs, Errors.GROUP_AUTHORIZATION_FAILED, partitionDataMap);    OffsetFetchResponseData expectedData = new OffsetFetchResponseData().setErrorCode(Errors.GROUP_AUTHORIZATION_FAILED.code()).setThrottleTimeMs(throttleTimeMs).setTopics(Collections.singletonList(new OffsetFetchResponseTopic().setName(topicOne).setPartitions(Collections.singletonList(new OffsetFetchResponsePartition().setPartitionIndex(partitionOne).setCommittedOffset(offset).setCommittedLeaderEpoch(leaderEpochOne.orElse(-1)).setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code()).setMetadata(null)))));    assertEquals(expectedData, response.data);}
f7823
0
testUseDefaultLeaderEpoch
public void kafkatest_f7824_0()
{    final Optional<Integer> emptyLeaderEpoch = Optional.empty();    partitionDataMap.clear();    partitionDataMap.put(new TopicPartition(topicOne, partitionOne), new PartitionData(offset, emptyLeaderEpoch, metadata, Errors.UNKNOWN_TOPIC_OR_PARTITION));    OffsetFetchResponse response = new OffsetFetchResponse(throttleTimeMs, Errors.NOT_COORDINATOR, partitionDataMap);    OffsetFetchResponseData expectedData = new OffsetFetchResponseData().setErrorCode(Errors.NOT_COORDINATOR.code()).setThrottleTimeMs(throttleTimeMs).setTopics(Collections.singletonList(new OffsetFetchResponseTopic().setName(topicOne).setPartitions(Collections.singletonList(new OffsetFetchResponsePartition().setPartitionIndex(partitionOne).setCommittedOffset(offset).setCommittedLeaderEpoch(RecordBatch.NO_PARTITION_LEADER_EPOCH).setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code()).setMetadata(metadata)))));    assertEquals(expectedData, response.data);}
f7824
0
testV3AndAboveShouldContainOnlyOneRecordBatch
public void kafkatest_f7833_0()
{    ByteBuffer buffer = ByteBuffer.allocate(256);    MemoryRecordsBuilder builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 0L);    builder.append(10L, null, "a".getBytes());    builder.close();    builder = MemoryRecords.builder(buffer, CompressionType.NONE, TimestampType.CREATE_TIME, 1L);    builder.append(11L, "1".getBytes(), "b".getBytes());    builder.append(12L, null, "c".getBytes());    builder.close();    buffer.flip();    Map<TopicPartition, MemoryRecords> produceData = new HashMap<>();    produceData.put(new TopicPartition("test", 0), MemoryRecords.readableRecords(buffer));    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forCurrentMagic((short) 1, 5000, produceData);    assertThrowsInvalidRecordExceptionForAllVersions(requestBuilder);}
f7833
0
testV3AndAboveCannotHaveNoRecordBatches
public void kafkatest_f7834_0()
{    Map<TopicPartition, MemoryRecords> produceData = new HashMap<>();    produceData.put(new TopicPartition("test", 0), MemoryRecords.EMPTY);    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forCurrentMagic((short) 1, 5000, produceData);    assertThrowsInvalidRecordExceptionForAllVersions(requestBuilder);}
f7834
0
testSerdeUnsupportedApiVersionRequest
public void kafkatest_f7843_0() throws Exception
{    int correlationId = 23423;    RequestHeader header = new RequestHeader(ApiKeys.API_VERSIONS, Short.MAX_VALUE, "", correlationId);    RequestContext context = new RequestContext(header, "0", InetAddress.getLocalHost(), KafkaPrincipal.ANONYMOUS, new ListenerName("ssl"), SecurityProtocol.SASL_SSL);    assertEquals(0, context.apiVersion());    // Write some garbage to the request buffer. This should be ignored since we will treat    // the unknown version type as v0 which has an empty request body.    ByteBuffer requestBuffer = ByteBuffer.allocate(8);    requestBuffer.putInt(3709234);    requestBuffer.putInt(29034);    requestBuffer.flip();    RequestAndSize requestAndSize = context.parseRequest(requestBuffer);    assertTrue(requestAndSize.request instanceof ApiVersionsRequest);    ApiVersionsRequest request = (ApiVersionsRequest) requestAndSize.request;    assertTrue(request.hasUnsupportedRequestVersion());    Send send = context.buildResponse(new ApiVersionsResponse(0, Errors.UNSUPPORTED_VERSION, Collections.<ApiVersionsResponse.ApiVersion>emptyList()));    ByteBufferChannel channel = new ByteBufferChannel(256);    send.writeTo(channel);    ByteBuffer responseBuffer = channel.buffer();    responseBuffer.flip();    // strip off the size    responseBuffer.getInt();    ResponseHeader responseHeader = ResponseHeader.parse(responseBuffer);    assertEquals(correlationId, responseHeader.correlationId());    Struct struct = ApiKeys.API_VERSIONS.parseResponse((short) 0, responseBuffer);    ApiVersionsResponse response = (ApiVersionsResponse) AbstractResponse.parseResponse(ApiKeys.API_VERSIONS, struct, (short) 0);    assertEquals(Errors.UNSUPPORTED_VERSION, response.error());    assertTrue(response.apiVersions().isEmpty());}
f7843
0
testSerdeControlledShutdownV0
public void kafkatest_f7844_0()
{    // Verify that version 0 of controlled shutdown does not include the clientId field    int correlationId = 2342;    ByteBuffer rawBuffer = ByteBuffer.allocate(32);    rawBuffer.putShort(ApiKeys.CONTROLLED_SHUTDOWN.id);    rawBuffer.putShort((short) 0);    rawBuffer.putInt(correlationId);    rawBuffer.flip();    RequestHeader deserialized = RequestHeader.parse(rawBuffer);    assertEquals(ApiKeys.CONTROLLED_SHUTDOWN, deserialized.apiKey());    assertEquals(0, deserialized.apiVersion());    assertEquals(correlationId, deserialized.correlationId());    assertEquals("", deserialized.clientId());    Struct serialized = deserialized.toStruct();    ByteBuffer serializedBuffer = toBuffer(serialized);    assertEquals(ApiKeys.CONTROLLED_SHUTDOWN.id, serializedBuffer.getShort(0));    assertEquals(0, serializedBuffer.getShort(2));    assertEquals(correlationId, serializedBuffer.getInt(4));    assertEquals(8, serializedBuffer.limit());}
f7844
0
checkRequest
private void kafkatest_f7853_0(AbstractRequest req, boolean checkEqualityAndHashCode)
{    // in the request is a HashMap with multiple elements since ordering of the elements may vary)    try {        Struct struct = req.toStruct();        AbstractRequest deserialized = AbstractRequest.parseRequest(req.api, req.version(), struct);        Struct struct2 = deserialized.toStruct();        if (checkEqualityAndHashCode) {            assertEquals(struct, struct2);            assertEquals(struct.hashCode(), struct2.hashCode());        }    } catch (Exception e) {        throw new RuntimeException("Failed to deserialize request " + req + " with type " + req.getClass(), e);    }}
f7853
0
checkResponse
private void kafkatest_f7854_0(AbstractResponse response, int version, boolean checkEqualityAndHashCode)
{    // in the response is a HashMap with multiple elements since ordering of the elements may vary)    try {        Struct struct = response.toStruct((short) version);        AbstractResponse deserialized = (AbstractResponse) deserialize(response, struct, (short) version);        Struct struct2 = deserialized.toStruct((short) version);        if (checkEqualityAndHashCode) {            assertEquals(struct, struct2);            assertEquals(struct.hashCode(), struct2.hashCode());        }    } catch (Exception e) {        throw new RuntimeException("Failed to deserialize response " + response + " with type " + response.getClass(), e);    }}
f7854
0
verifyFetchResponseFullWrites
public void kafkatest_f7863_0() throws Exception
{    verifyFetchResponseFullWrite(ApiKeys.FETCH.latestVersion(), createFetchResponse(123));    verifyFetchResponseFullWrite(ApiKeys.FETCH.latestVersion(), createFetchResponse(Errors.FETCH_SESSION_ID_NOT_FOUND, 123));    for (short version = 0; version <= ApiKeys.FETCH.latestVersion(); version++) {        verifyFetchResponseFullWrite(version, createFetchResponse());    }}
f7863
0
verifyFetchResponseFullWrite
private void kafkatest_f7864_0(short apiVersion, FetchResponse fetchResponse) throws Exception
{    int correlationId = 15;    Send send = fetchResponse.toSend("1", new ResponseHeader(correlationId), apiVersion);    ByteBufferChannel channel = new ByteBufferChannel(send.size());    send.writeTo(channel);    channel.close();    ByteBuffer buf = channel.buffer();    // read the size    int size = buf.getInt();    assertTrue(size > 0);    // read the header    ResponseHeader responseHeader = ResponseHeader.parse(channel.buffer());    assertEquals(correlationId, responseHeader.correlationId());    // read the body    Struct responseBody = ApiKeys.FETCH.responseSchema(apiVersion).read(buf);    assertEquals(fetchResponse.toStruct(apiVersion), responseBody);    assertEquals(size, responseHeader.sizeOf() + responseBody.sizeOf());}
f7864
0
createResponseHeader
private ResponseHeader kafkatest_f7873_0()
{    return new ResponseHeader(10);}
f7873
0
createFindCoordinatorRequest
private FindCoordinatorRequest kafkatest_f7874_0(int version)
{    return new FindCoordinatorRequest.Builder(new FindCoordinatorRequestData().setKeyType(CoordinatorType.GROUP.id()).setKey("test-group")).build((short) version);}
f7874
0
createHeartBeatResponse
private HeartbeatResponse kafkatest_f7883_0()
{    return new HeartbeatResponse(new HeartbeatResponseData().setErrorCode(Errors.NONE.code()));}
f7883
0
createJoinGroupRequest
private JoinGroupRequest kafkatest_f7884_0(int version)
{    JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols = new JoinGroupRequestData.JoinGroupRequestProtocolCollection(Collections.singleton(new JoinGroupRequestData.JoinGroupRequestProtocol().setName("consumer-range").setMetadata(new byte[0])).iterator());    if (version <= 4) {        return new JoinGroupRequest.Builder(new JoinGroupRequestData().setGroupId("group1").setSessionTimeoutMs(30000).setMemberId("consumer1").setGroupInstanceId(null).setProtocolType("consumer").setProtocols(protocols).setRebalanceTimeoutMs(// v1 and above contains rebalance timeout        60000)).build((short) version);    } else {        return new JoinGroupRequest.Builder(new JoinGroupRequestData().setGroupId("group1").setSessionTimeoutMs(30000).setMemberId("consumer1").setGroupInstanceId(// v5 and above could set group instance id        "groupInstanceId").setProtocolType("consumer").setProtocols(protocols).setRebalanceTimeoutMs(// v1 and above contains rebalance timeout        60000)).build((short) version);    }}
f7884
0
createDeleteGroupsResponse
private DeleteGroupsResponse kafkatest_f7893_0()
{    DeletableGroupResultCollection result = new DeletableGroupResultCollection();    result.add(new DeletableGroupResult().setGroupId("test-group").setErrorCode(Errors.NONE.code()));    return new DeleteGroupsResponse(new DeleteGroupsResponseData().setResults(result));}
f7893
0
createListOffsetRequest
private ListOffsetRequest kafkatest_f7894_0(int version)
{    if (version == 0) {        Map<TopicPartition, ListOffsetRequest.PartitionData> offsetData = Collections.singletonMap(new TopicPartition("test", 0), new ListOffsetRequest.PartitionData(1000000L, 10));        return ListOffsetRequest.Builder.forConsumer(false, IsolationLevel.READ_UNCOMMITTED).setTargetTimes(offsetData).build((short) version);    } else if (version == 1) {        Map<TopicPartition, ListOffsetRequest.PartitionData> offsetData = Collections.singletonMap(new TopicPartition("test", 0), new ListOffsetRequest.PartitionData(1000000L, Optional.empty()));        return ListOffsetRequest.Builder.forConsumer(true, IsolationLevel.READ_UNCOMMITTED).setTargetTimes(offsetData).build((short) version);    } else if (version == 2) {        Map<TopicPartition, ListOffsetRequest.PartitionData> offsetData = Collections.singletonMap(new TopicPartition("test", 0), new ListOffsetRequest.PartitionData(1000000L, Optional.of(5)));        return ListOffsetRequest.Builder.forConsumer(true, IsolationLevel.READ_COMMITTED).setTargetTimes(offsetData).build((short) version);    } else {        throw new IllegalArgumentException("Illegal ListOffsetRequest version " + version);    }}
f7894
0
createProduceResponse
private ProduceResponse kafkatest_f7903_0()
{    Map<TopicPartition, ProduceResponse.PartitionResponse> responseData = new HashMap<>();    responseData.put(new TopicPartition("test", 0), new ProduceResponse.PartitionResponse(Errors.NONE, 10000, RecordBatch.NO_TIMESTAMP, 100));    return new ProduceResponse(responseData, 0);}
f7903
0
createStopReplicaRequest
private StopReplicaRequest kafkatest_f7904_0(int version, boolean deletePartitions)
{    Set<TopicPartition> partitions = Utils.mkSet(new TopicPartition("test", 0));    return new StopReplicaRequest.Builder((short) version, 0, 1, 0, deletePartitions, partitions).build();}
f7904
0
createSaslHandshakeRequest
private SaslHandshakeRequest kafkatest_f7913_0()
{    return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism("PLAIN")).build();}
f7913
0
createSaslHandshakeResponse
private SaslHandshakeResponse kafkatest_f7914_0()
{    return new SaslHandshakeResponse(new SaslHandshakeResponseData().setErrorCode(Errors.NONE.code()).setMechanisms(Collections.singletonList("GSSAPI")));}
f7914
0
createDeleteTopicsResponse
private DeleteTopicsResponse kafkatest_f7923_0()
{    DeleteTopicsResponseData data = new DeleteTopicsResponseData();    data.responses().add(new DeletableTopicResult().setName("t1").setErrorCode(Errors.INVALID_TOPIC_EXCEPTION.code()));    data.responses().add(new DeletableTopicResult().setName("t2").setErrorCode(Errors.TOPIC_AUTHORIZATION_FAILED.code()));    return new DeleteTopicsResponse(data);}
f7923
0
createInitPidRequest
private InitProducerIdRequest kafkatest_f7924_0()
{    InitProducerIdRequestData requestData = new InitProducerIdRequestData().setTransactionalId(null).setTransactionTimeoutMs(100);    return new InitProducerIdRequest.Builder(requestData).build();}
f7924
0
createAddOffsetsToTxnResponse
private AddOffsetsToTxnResponse kafkatest_f7933_0()
{    return new AddOffsetsToTxnResponse(0, Errors.NONE);}
f7933
0
createEndTxnRequest
private EndTxnRequest kafkatest_f7934_0()
{    return new EndTxnRequest.Builder("tid", 21L, (short) 42, TransactionResult.COMMIT).build();}
f7934
0
createCreateAclsResponse
private CreateAclsResponse kafkatest_f7943_0()
{    return new CreateAclsResponse(0, Arrays.asList(new AclCreationResponse(ApiError.NONE), new AclCreationResponse(new ApiError(Errors.INVALID_REQUEST, "Foo bar"))));}
f7943
0
createDeleteAclsRequest
private DeleteAclsRequest kafkatest_f7944_0()
{    List<AclBindingFilter> filters = new ArrayList<>();    filters.add(new AclBindingFilter(new ResourcePatternFilter(ResourceType.ANY, null, PatternType.LITERAL), new AccessControlEntryFilter("User:ANONYMOUS", null, AclOperation.ANY, AclPermissionType.ANY)));    filters.add(new AclBindingFilter(new ResourcePatternFilter(ResourceType.ANY, null, PatternType.LITERAL), new AccessControlEntryFilter("User:bob", null, AclOperation.ANY, AclPermissionType.ANY)));    return new DeleteAclsRequest.Builder(filters).build();}
f7944
0
createCreatePartitionsResponse
private CreatePartitionsResponse kafkatest_f7953_0()
{    Map<String, ApiError> results = new HashMap<>();    results.put("my_topic", ApiError.fromThrowable(new InvalidReplicaAssignmentException("The assigned brokers included an unknown broker")));    results.put("my_topic", ApiError.NONE);    return new CreatePartitionsResponse(42, results);}
f7953
0
createCreateTokenRequest
private CreateDelegationTokenRequest kafkatest_f7954_0()
{    List<CreatableRenewers> renewers = new ArrayList<>();    renewers.add(new CreatableRenewers().setPrincipalType("User").setPrincipalName("user1"));    renewers.add(new CreatableRenewers().setPrincipalType("User").setPrincipalName("user2"));    return new CreateDelegationTokenRequest.Builder(new CreateDelegationTokenRequestData().setRenewers(renewers).setMaxLifetimeMs(System.currentTimeMillis())).build();}
f7954
0
createElectLeadersRequest
private ElectLeadersRequest kafkatest_f7963_0()
{    List<TopicPartition> partitions = asList(new TopicPartition("data", 1), new TopicPartition("data", 2));    return new ElectLeadersRequest.Builder(ElectionType.PREFERRED, partitions, 100).build((short) 1);}
f7963
0
createElectLeadersResponse
private ElectLeadersResponse kafkatest_f7964_0()
{    String topic = "myTopic";    List<ReplicaElectionResult> electionResults = new ArrayList<>();    ReplicaElectionResult electionResult = new ReplicaElectionResult();    electionResult.setTopic(topic);    // Add partition 1 result    PartitionResult partitionResult = new PartitionResult();    partitionResult.setPartitionId(0);    partitionResult.setErrorCode(ApiError.NONE.error().code());    partitionResult.setErrorMessage(ApiError.NONE.message());    electionResult.partitionResult().add(partitionResult);    // Add partition 2 result    partitionResult = new PartitionResult();    partitionResult.setPartitionId(1);    partitionResult.setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code());    partitionResult.setErrorMessage(Errors.UNKNOWN_TOPIC_OR_PARTITION.message());    electionResult.partitionResult().add(partitionResult);    return new ElectLeadersResponse(200, Errors.NONE.code(), electionResults);}
f7964
0
testErrorCountsFromGetErrorResponse
public void kafkatest_f7973_0()
{    StopReplicaRequest request = new StopReplicaRequest.Builder(ApiKeys.STOP_REPLICA.latestVersion(), 15, 20, 0, false, Utils.mkSet(new TopicPartition("foo", 0), new TopicPartition("foo", 1))).build();    StopReplicaResponse response = request.getErrorResponse(0, Errors.CLUSTER_AUTHORIZATION_FAILED.exception());    assertEquals(Collections.singletonMap(Errors.CLUSTER_AUTHORIZATION_FAILED, 2), response.errorCounts());}
f7973
0
testErrorCountsWithTopLevelError
public void kafkatest_f7974_0()
{    Map<TopicPartition, Errors> errors = new HashMap<>();    errors.put(new TopicPartition("foo", 0), Errors.NONE);    errors.put(new TopicPartition("foo", 1), Errors.NOT_LEADER_FOR_PARTITION);    StopReplicaResponse response = new StopReplicaResponse(Errors.UNKNOWN_SERVER_ERROR, errors);    assertEquals(Collections.singletonMap(Errors.UNKNOWN_SERVER_ERROR, 2), response.errorCounts());}
f7974
0
shouldNotMatchIfDifferentName
public void kafkatest_f7983_0()
{    assertFalse(new ResourceFilter(TOPIC, "Different").matches(new Resource(TOPIC, "Name")));}
f7983
0
shouldNotMatchIfDifferentNameCase
public void kafkatest_f7984_0()
{    assertFalse(new ResourceFilter(TOPIC, "NAME").matches(new Resource(TOPIC, "Name")));}
f7984
0
testName
public void kafkatest_f7993_0()
{    for (AclResourceTypeTestInfo info : INFOS) {        assertEquals("ResourceType.fromString(" + info.name + ") was supposed to be " + info.resourceType, info.resourceType, ResourceType.fromString(info.name));    }    assertEquals(ResourceType.UNKNOWN, ResourceType.fromString("something"));}
f7993
0
testExhaustive
public void kafkatest_f7994_0()
{    assertEquals(INFOS.length, ResourceType.values().length);    for (int i = 0; i < INFOS.length; i++) {        assertEquals(INFOS[i].resourceType, ResourceType.values()[i]);    }}
f7994
0
getName
public String kafkatest_f8003_0()
{    return name;}
f8003
0
testEqualsAndHashCode
public void kafkatest_f8004_0()
{    String name = "KafkaUser";    KafkaPrincipal principal1 = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, name);    KafkaPrincipal principal2 = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, name);    Assert.assertEquals(principal1.hashCode(), principal2.hashCode());    Assert.assertEquals(principal1, principal2);}
f8004
0
setUp
public void kafkatest_f8013_0()
{    dynamicPlainContext = new Password(PlainLoginModule.class.getName() + " required user=\"plainuser\" password=\"plain-secret\";");    dynamicDigestContext = new Password(TestDigestLoginModule.class.getName() + " required user=\"digestuser\" password=\"digest-secret\";");    TestJaasConfig.createConfiguration("SCRAM-SHA-256", Collections.singletonList("SCRAM-SHA-256"));}
f8013
0
tearDown
public void kafkatest_f8014_0()
{    LoginManager.closeAll();}
f8014
0
poll
private void kafkatest_f8023_0(Selector selector)
{    try {        selector.poll(50);    } catch (IOException e) {        Assert.fail("Caught unexpected exception " + e);    }}
f8023
0
configureMechanisms
private TestJaasConfig kafkatest_f8024_0(String clientMechanism, List<String> serverMechanisms)
{    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, clientMechanism);    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, serverMechanisms);    if (serverMechanisms.contains("DIGEST-MD5")) {        saslServerConfigs.put("digest-md5." + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, TestDigestLoginModule.DigestServerCallbackHandler.class.getName());    }    return TestJaasConfig.createConfiguration(clientMechanism, serverMechanisms);}
f8024
0
testValidSaslPlainOverSsl
public void kafkatest_f8033_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    server = createEchoServer(securityProtocol);    checkAuthenticationAndReauthentication(securityProtocol, node);}
f8033
0
testValidSaslPlainOverPlaintext
public void kafkatest_f8034_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    server = createEchoServer(securityProtocol);    checkAuthenticationAndReauthentication(securityProtocol, node);}
f8034
0
testMechanismPluggability
public void kafkatest_f8044_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("DIGEST-MD5", Arrays.asList("DIGEST-MD5"));    configureDigestMd5ServerCallback(securityProtocol);    server = createEchoServer(securityProtocol);    createAndCheckClientConnection(securityProtocol, node);}
f8044
0
testMultipleServerMechanisms
public void kafkatest_f8045_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("DIGEST-MD5", Arrays.asList("DIGEST-MD5", "PLAIN", "SCRAM-SHA-256"));    configureDigestMd5ServerCallback(securityProtocol);    server = createEchoServer(securityProtocol);    updateScramCredentialCache(TestJaasConfig.USERNAME, TestJaasConfig.PASSWORD);    String node1 = "1";    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "PLAIN");    createAndCheckClientConnection(securityProtocol, node1);    server.verifyAuthenticationMetrics(1, 0);    Selector selector2 = null;    Selector selector3 = null;    try {        String node2 = "2";        saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "DIGEST-MD5");        createSelector(securityProtocol, saslClientConfigs);        selector2 = selector;        InetSocketAddress addr = new InetSocketAddress("127.0.0.1", server.port());        selector.connect(node2, addr, BUFFER_SIZE, BUFFER_SIZE);        NetworkTestUtils.checkClientConnection(selector, node2, 100, 10);        // keeps it from being closed when next one is created        selector = null;        server.verifyAuthenticationMetrics(2, 0);        String node3 = "3";        saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "SCRAM-SHA-256");        createSelector(securityProtocol, saslClientConfigs);        selector3 = selector;        selector.connect(node3, new InetSocketAddress("127.0.0.1", server.port()), BUFFER_SIZE, BUFFER_SIZE);        NetworkTestUtils.checkClientConnection(selector, node3, 100, 10);        server.verifyAuthenticationMetrics(3, 0);        /*             * Now re-authenticate the connections. First we have to sleep long enough so             * that the next write will cause re-authentication, which we expect to succeed.             */        delay((long) (CONNECTIONS_MAX_REAUTH_MS_VALUE * 1.1));        server.verifyReauthenticationMetrics(0, 0);        NetworkTestUtils.checkClientConnection(selector2, node2, 100, 10);        server.verifyReauthenticationMetrics(1, 0);        NetworkTestUtils.checkClientConnection(selector3, node3, 100, 10);        server.verifyReauthenticationMetrics(2, 0);    } finally {        if (selector2 != null)            selector2.close();        if (selector3 != null)            selector3.close();    }}
f8045
0
token
public TokenInformation kafkatest_f8054_0(String tokenId)
{    TokenInformation baseTokenInfo = super.token(tokenId);    long thisLifetimeMs = System.currentTimeMillis() + tokenLifetime.apply(++callNum).longValue();    TokenInformation retvalTokenInfo = new TokenInformation(baseTokenInfo.tokenId(), baseTokenInfo.owner(), baseTokenInfo.renewers(), baseTokenInfo.issueTimestamp(), thisLifetimeMs, thisLifetimeMs);    return retvalTokenInfo;}
f8054
0
testUnauthenticatedApiVersionsRequestOverPlaintextHandshakeVersion0
public void kafkatest_f8055_0() throws Exception
{    testUnauthenticatedApiVersionsRequest(SecurityProtocol.SASL_PLAINTEXT, (short) 0);}
f8055
0
testDisallowedKafkaRequestsBeforeAuthentication
public void kafkatest_f8064_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_PLAINTEXT;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    server = createEchoServer(securityProtocol);    // Send metadata request before Kafka SASL handshake request    String node1 = "invalid1";    createClientConnection(SecurityProtocol.PLAINTEXT, node1);    MetadataRequest metadataRequest1 = new MetadataRequest.Builder(Collections.singletonList("sometopic"), true).build();    RequestHeader metadataRequestHeader1 = new RequestHeader(ApiKeys.METADATA, metadataRequest1.version(), "someclient", 1);    selector.send(metadataRequest1.toSend(node1, metadataRequestHeader1));    NetworkTestUtils.waitForChannelClose(selector, node1, ChannelState.READY.state());    selector.close();    // Test good connection still works    createAndCheckClientConnection(securityProtocol, "good1");    // Send metadata request after Kafka SASL handshake request    String node2 = "invalid2";    createClientConnection(SecurityProtocol.PLAINTEXT, node2);    sendHandshakeRequestReceiveResponse(node2, (short) 1);    MetadataRequest metadataRequest2 = new MetadataRequest.Builder(Collections.singletonList("sometopic"), true).build();    RequestHeader metadataRequestHeader2 = new RequestHeader(ApiKeys.METADATA, metadataRequest2.version(), "someclient", 2);    selector.send(metadataRequest2.toSend(node2, metadataRequestHeader2));    NetworkTestUtils.waitForChannelClose(selector, node2, ChannelState.READY.state());    selector.close();    // Test good connection still works    createAndCheckClientConnection(securityProtocol, "good2");}
f8064
0
testInvalidLoginModule
public void kafkatest_f8065_0() throws Exception
{    TestJaasConfig jaasConfig = configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    jaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_CLIENT, "InvalidLoginModule", TestJaasConfig.defaultClientOptions());    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    server = createEchoServer(securityProtocol);    try {        createSelector(securityProtocol, saslClientConfigs);        fail("SASL/PLAIN channel created without valid login module");    } catch (KafkaException e) {    // Expected exception    }}
f8065
0
testInvalidMechanism
public void kafkatest_f8074_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("PLAIN", Arrays.asList("PLAIN"));    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "INVALID");    server = createEchoServer(securityProtocol);    createAndCheckClientConnectionFailure(securityProtocol, node);    server.verifyAuthenticationMetrics(0, 1);    server.verifyReauthenticationMetrics(0, 0);}
f8074
0
testClientDynamicJaasConfiguration
public void kafkatest_f8075_0() throws Exception
{    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, "PLAIN");    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, Arrays.asList("PLAIN"));    Map<String, Object> serverOptions = new HashMap<>();    serverOptions.put("user_user1", "user1-secret");    serverOptions.put("user_user2", "user2-secret");    TestJaasConfig staticJaasConfig = new TestJaasConfig();    staticJaasConfig.createOrUpdateEntry(TestJaasConfig.LOGIN_CONTEXT_SERVER, PlainLoginModule.class.getName(), serverOptions);    staticJaasConfig.setClientOptions("PLAIN", "user1", "invalidpassword");    Configuration.setConfiguration(staticJaasConfig);    server = createEchoServer(securityProtocol);    // Check that client using static Jaas config does not connect since password is invalid    createAndCheckClientConnectionFailure(securityProtocol, "1");    // Check that 'user1' can connect with a Jaas config property override    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "user1", "user1-secret"));    createAndCheckClientConnection(securityProtocol, "2");    // Check that invalid password specified as Jaas config property results in connection failure    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "user1", "user2-secret"));    createAndCheckClientConnectionFailure(securityProtocol, "3");    // Check that another user 'user2' can also connect with a Jaas config override without any changes to static configuration    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, TestJaasConfig.jaasConfigProperty("PLAIN", "user2", "user2-secret"));    createAndCheckClientConnection(securityProtocol, "4");    // Check that clients specifying multiple login modules fail even if the credentials are valid    String module1 = TestJaasConfig.jaasConfigProperty("PLAIN", "user1", "user1-secret").value();    String module2 = TestJaasConfig.jaasConfigProperty("PLAIN", "user2", "user2-secret").value();    saslClientConfigs.put(SaslConfigs.SASL_JAAS_CONFIG, new Password(module1 + " " + module2));    try {        createClientConnection(securityProtocol, "1");        fail("Connection created with multiple login modules in sasl.jaas.config");    } catch (IllegalArgumentException e) {    // Expected    }}
f8075
0
oldSaslScramSslServerWithoutSaslAuthenticateHeader
public void kafkatest_f8084_0() throws Exception
{    verifySaslAuthenticateHeaderInterop(false, true, SecurityProtocol.SASL_SSL, "SCRAM-SHA-512");}
f8084
0
oldSaslScramSslClientWithoutSaslAuthenticateHeader
public void kafkatest_f8085_0() throws Exception
{    verifySaslAuthenticateHeaderInterop(true, false, SecurityProtocol.SASL_SSL, "SCRAM-SHA-512");}
f8085
0
testValidSaslOauthBearerMechanism
public void kafkatest_f8094_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    configureMechanisms("OAUTHBEARER", Arrays.asList("OAUTHBEARER"));    server = createEchoServer(securityProtocol);    createAndCheckClientConnection(securityProtocol, node);}
f8094
0
testCannotReauthenticateWithDifferentPrincipal
public void kafkatest_f8095_0() throws Exception
{    String node = "0";    SecurityProtocol securityProtocol = SecurityProtocol.SASL_SSL;    saslClientConfigs.put(SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS, AlternateLoginCallbackHandler.class.getName());    configureMechanisms(OAuthBearerLoginModule.OAUTHBEARER_MECHANISM, Arrays.asList(OAuthBearerLoginModule.OAUTHBEARER_MECHANISM));    server = createEchoServer(securityProtocol);    // initial authentication must succeed    createClientConnection(securityProtocol, node);    checkClientConnection(node);    // ensure metrics are as expected before trying to re-authenticate    server.verifyAuthenticationMetrics(1, 0);    server.verifyReauthenticationMetrics(0, 0);    /*         * Now re-authenticate with a different principal and ensure it fails. We first         * have to sleep long enough for the background refresh thread to replace the         * original token with a new one.         */    delay(1000L);    try {        checkClientConnection(node);        fail("Re-authentication with a different principal should have failed but did not");    } catch (AssertionError e) {        // ignore, expected        server.verifyReauthenticationMetrics(0, 1);    }}
f8095
0
createClientConnection
private void kafkatest_f8104_0(SecurityProtocol securityProtocol, String saslMechanism, String node, boolean enableSaslAuthenticateHeader) throws Exception
{    if (enableSaslAuthenticateHeader)        createClientConnection(securityProtocol, node);    else        createClientConnectionWithoutSaslAuthenticateHeader(securityProtocol, saslMechanism, node);}
f8104
0
startServerWithoutSaslAuthenticateHeader
private NioEchoServer kafkatest_f8105_0(final SecurityProtocol securityProtocol, String saslMechanism) throws Exception
{    final ListenerName listenerName = ListenerName.forSecurityProtocol(securityProtocol);    final Map<String, ?> configs = Collections.emptyMap();    final JaasContext jaasContext = JaasContext.loadServerContext(listenerName, saslMechanism, configs);    final Map<String, JaasContext> jaasContexts = Collections.singletonMap(saslMechanism, jaasContext);    boolean isScram = ScramMechanism.isScram(saslMechanism);    if (isScram)        ScramCredentialUtils.createCache(credentialCache, Arrays.asList(saslMechanism));    SaslChannelBuilder serverChannelBuilder = new SaslChannelBuilder(Mode.SERVER, jaasContexts, securityProtocol, listenerName, false, saslMechanism, true, credentialCache, null, time) {        @Override        protected SaslServerAuthenticator buildServerAuthenticator(Map<String, ?> configs, Map<String, AuthenticateCallbackHandler> callbackHandlers, String id, TransportLayer transportLayer, Map<String, Subject> subjects, Map<String, Long> connectionsMaxReauthMsByMechanism) {            return new SaslServerAuthenticator(configs, callbackHandlers, id, subjects, null, listenerName, securityProtocol, transportLayer, connectionsMaxReauthMsByMechanism, time) {                @Override                protected ApiVersionsResponse apiVersionsResponse() {                    List<ApiVersion> apiVersions = new ArrayList<>(ApiVersionsResponse.defaultApiVersionsResponse().apiVersions());                    for (Iterator<ApiVersion> it = apiVersions.iterator(); it.hasNext(); ) {                        ApiVersion apiVersion = it.next();                        if (apiVersion.apiKey == ApiKeys.SASL_AUTHENTICATE.id) {                            it.remove();                            break;                        }                    }                    return new ApiVersionsResponse(0, Errors.NONE, apiVersions);                }                @Override                protected void enableKafkaSaslAuthenticateHeaders(boolean flag) {                // Don't enable Kafka SASL_AUTHENTICATE headers                }            };        }    };    serverChannelBuilder.configure(saslServerConfigs);    server = new NioEchoServer(listenerName, securityProtocol, new TestSecurityConfig(saslServerConfigs), "localhost", serverChannelBuilder, credentialCache, time);    server.start();    return server;}
f8105
0
authenticateUsingSaslPlainAndCheckConnection
private void kafkatest_f8114_0(String node, boolean enableSaslAuthenticateHeader) throws Exception
{    // Authenticate using PLAIN username/password    String authString = "\u0000" + TestJaasConfig.USERNAME + "\u0000" + TestJaasConfig.PASSWORD;    ByteBuffer authBuf = ByteBuffer.wrap(authString.getBytes("UTF-8"));    if (enableSaslAuthenticateHeader) {        SaslAuthenticateRequestData data = new SaslAuthenticateRequestData().setAuthBytes(authBuf.array());        SaslAuthenticateRequest request = new SaslAuthenticateRequest.Builder(data).build();        sendKafkaRequestReceiveResponse(node, ApiKeys.SASL_AUTHENTICATE, request);    } else {        selector.send(new NetworkSend(node, authBuf));        waitForResponse();    }    // Check send/receive on the manually authenticated connection    NetworkTestUtils.checkClientConnection(selector, node, 100, 10);}
f8114
0
configureMechanisms
private TestJaasConfig kafkatest_f8115_0(String clientMechanism, List<String> serverMechanisms)
{    saslClientConfigs.put(SaslConfigs.SASL_MECHANISM, clientMechanism);    saslServerConfigs.put(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG, serverMechanisms);    saslServerConfigs.put(BrokerSecurityConfigs.CONNECTIONS_MAX_REAUTH_MS, CONNECTIONS_MAX_REAUTH_MS_VALUE);    if (serverMechanisms.contains("DIGEST-MD5")) {        saslServerConfigs.put("digest-md5." + BrokerSecurityConfigs.SASL_SERVER_CALLBACK_HANDLER_CLASS, TestDigestLoginModule.DigestServerCallbackHandler.class.getName());    }    return TestJaasConfig.createConfiguration(clientMechanism, serverMechanisms);}
f8115
0
createAndCheckClientConnection
private void kafkatest_f8124_0(SecurityProtocol securityProtocol, String node) throws Exception
{    try {        createClientConnection(securityProtocol, node);        checkClientConnection(node);    } finally {        closeClientConnectionIfNecessary();    }}
f8124
0
createAndCheckClientAuthenticationFailure
private void kafkatest_f8125_0(SecurityProtocol securityProtocol, String node, String mechanism, String expectedErrorMessage) throws Exception
{    ChannelState finalState = createAndCheckClientConnectionFailure(securityProtocol, node);    Exception exception = finalState.exception();    assertTrue("Invalid exception class " + exception.getClass(), exception instanceof SaslAuthenticationException);    String expectedExceptionMessage = expectedErrorMessage != null ? expectedErrorMessage : "Authentication failed during authentication due to invalid credentials with SASL mechanism " + mechanism;    assertEquals(expectedExceptionMessage, exception.getMessage());}
f8125
0
buildSaslHandshakeRequest
private SaslHandshakeRequest kafkatest_f8134_0(String mechanism, short version)
{    return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism(mechanism)).build(version);}
f8134
0
updateScramCredentialCache
private void kafkatest_f8135_0(String username, String password) throws NoSuchAlgorithmException
{    for (String mechanism : (List<String>) saslServerConfigs.get(BrokerSecurityConfigs.SASL_ENABLED_MECHANISMS_CONFIG)) {        ScramMechanism scramMechanism = ScramMechanism.forMechanismName(mechanism);        if (scramMechanism != null) {            ScramFormatter formatter = new ScramFormatter(scramMechanism);            ScramCredential credential = formatter.generateCredential(password, 4096);            credentialCache.cache(scramMechanism.mechanismName(), ScramCredential.class).put(username, credential);        }    }}
f8135
0
serviceName
public String kafkatest_f8145_0()
{    return "kafka";}
f8145
0
configure
public void kafkatest_f8147_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{    if (configured)        throw new IllegalStateException("Login callback handler configured twice");    configured = true;}
f8147
0
buildClientAuthenticator
protected SaslClientAuthenticator kafkatest_f8157_0(Map<String, ?> configs, AuthenticateCallbackHandler callbackHandler, String id, String serverHost, String servicePrincipal, TransportLayer transportLayer, Subject subject)
{    if (++numInvocations == 1)        return new SaslClientAuthenticator(configs, callbackHandler, id, subject, servicePrincipal, serverHost, "DIGEST-MD5", true, transportLayer, time);    else        return new SaslClientAuthenticator(configs, callbackHandler, id, subject, servicePrincipal, serverHost, "PLAIN", true, transportLayer, time) {            @Override            protected SaslHandshakeRequest createSaslHandshakeRequest(short version) {                return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism("PLAIN")).build(version);            }        };}
f8157
0
createSaslHandshakeRequest
protected SaslHandshakeRequest kafkatest_f8158_0(short version)
{    return new SaslHandshakeRequest.Builder(new SaslHandshakeRequestData().setMechanism("PLAIN")).build(version);}
f8158
0
createOrUpdateEntry
public void kafkatest_f8169_0(String name, String loginModule, Map<String, Object> options)
{    AppConfigurationEntry entry = new AppConfigurationEntry(loginModule, LoginModuleControlFlag.REQUIRED, options);    entryMap.put(name, new AppConfigurationEntry[] { entry });}
f8169
0
addEntry
public void kafkatest_f8170_0(String name, String loginModule, Map<String, Object> options)
{    AppConfigurationEntry entry = new AppConfigurationEntry(loginModule, LoginModuleControlFlag.REQUIRED, options);    AppConfigurationEntry[] existing = entryMap.get(name);    AppConfigurationEntry[] newEntries = existing == null ? new AppConfigurationEntry[1] : Arrays.copyOf(existing, existing.length + 1);    newEntries[newEntries.length - 1] = entry;    entryMap.put(name, newEntries);}
f8170
0
testControlFlag
public void kafkatest_f8179_0() throws Exception
{    LoginModuleControlFlag[] controlFlags = new LoginModuleControlFlag[] { LoginModuleControlFlag.REQUIRED, LoginModuleControlFlag.REQUISITE, LoginModuleControlFlag.SUFFICIENT, LoginModuleControlFlag.OPTIONAL };    Map<String, Object> options = new HashMap<>();    options.put("propName", "propValue");    for (LoginModuleControlFlag controlFlag : controlFlags) {        checkConfiguration("test.testControlFlag", controlFlag, options);    }}
f8179
0
testSingleOption
public void kafkatest_f8180_0() throws Exception
{    Map<String, Object> options = new HashMap<>();    options.put("propName", "propValue");    checkConfiguration("test.testSingleOption", LoginModuleControlFlag.REQUISITE, options);}
f8180
0
testNumericOptionWithoutQuotes
public void kafkatest_f8189_0() throws Exception
{    checkInvalidConfiguration("test.testNumericOptionWithoutQuotes required option1=3;");}
f8189
0
testInvalidControlFlag
public void kafkatest_f8190_0() throws Exception
{    checkInvalidConfiguration("test.testInvalidControlFlag { option1=3;");}
f8190
0
writeConfiguration
private void kafkatest_f8199_0(List<String> lines) throws IOException
{    Files.write(jaasConfigFile.toPath(), lines, StandardCharsets.UTF_8);    Configuration.setConfiguration(null);}
f8199
0
checkConfiguration
private void kafkatest_f8200_0(String loginModule, LoginModuleControlFlag controlFlag, Map<String, Object> options) throws Exception
{    String jaasConfigProp = jaasConfigProp(loginModule, controlFlag, options);    checkConfiguration(jaasConfigProp, loginModule, controlFlag, options);}
f8200
0
fromGoodConfig
public void kafkatest_f8209_0()
{    ExpiringCredentialRefreshConfig expiringCredentialRefreshConfig = new ExpiringCredentialRefreshConfig(new ConfigDef().withClientSaslSupport().parse(Collections.emptyMap()), true);    assertEquals(Double.valueOf(SaslConfigs.DEFAULT_LOGIN_REFRESH_WINDOW_FACTOR), Double.valueOf(expiringCredentialRefreshConfig.loginRefreshWindowFactor()));    assertEquals(Double.valueOf(SaslConfigs.DEFAULT_LOGIN_REFRESH_WINDOW_JITTER), Double.valueOf(expiringCredentialRefreshConfig.loginRefreshWindowJitter()));    assertEquals(Short.valueOf(SaslConfigs.DEFAULT_LOGIN_REFRESH_MIN_PERIOD_SECONDS), Short.valueOf(expiringCredentialRefreshConfig.loginRefreshMinPeriodSeconds()));    assertEquals(Short.valueOf(SaslConfigs.DEFAULT_LOGIN_REFRESH_BUFFER_SECONDS), Short.valueOf(expiringCredentialRefreshConfig.loginRefreshBufferSeconds()));    assertTrue(expiringCredentialRefreshConfig.loginRefreshReloginAllowedBeforeLogout());}
f8209
0
getAppConfigurationEntry
public AppConfigurationEntry[] kafkatest_f8210_0(String name)
{    // match any name    return new AppConfigurationEntry[0];}
f8210
0
expireTimeMs
public long kafkatest_f8219_0()
{    return expireTimeMs;}
f8219
0
absoluteLastRefreshTimeMs
public Long kafkatest_f8220_0()
{    return absoluteLastRefreshTimeMs;}
f8220
0
getSubject
public Subject kafkatest_f8229_0()
{    return testLoginContext.getSubject();}
f8229
0
refresherThreadStarted
public void kafkatest_f8230_0()
{    refresherThreadStartedFuture.complete(null);}
f8230
0
testRefreshWithPreExpirationBufferIntrusion
public void kafkatest_f8239_0() throws Exception
{    int numExpectedRefreshes = 1;    boolean clientReloginAllowedBeforeLogout = true;    Subject subject = new Subject();    final LoginContext mockLoginContext = mock(LoginContext.class);    when(mockLoginContext.getSubject()).thenReturn(subject);    MockTime mockTime = new MockTime();    long startMs = mockTime.milliseconds();    /*         * Identify the lifetime of each expiring credential         */    long lifetimeMinutes = 10L;    /*         * Identify the point at which refresh will occur in that lifetime         */    long refreshEveryMinutes = 8L;    /*         * Set an absolute last refresh time that will cause the login thread to exit         * after a certain number of re-logins (by adding an extra half of a refresh         * interval).         */    long absoluteLastRefreshMs = startMs + (1 + numExpectedRefreshes) * 1000 * 60 * refreshEveryMinutes - 1000 * 60 * refreshEveryMinutes / 2;    /*         * Identify a minimum period that will cause the refresh time to be delayed a         * bit.         */    int bufferIntrusionSeconds = 1;    short bufferSeconds = (short) ((lifetimeMinutes - refreshEveryMinutes) * 60 + bufferIntrusionSeconds);    short minPeriodSeconds = (short) 0;    /*         * Define some listeners so we can keep track of who gets done and when. All         * added listeners should end up done except the last, extra one, which should         * not.         */    MockScheduler mockScheduler = new MockScheduler(mockTime);    List<KafkaFutureImpl<Long>> waiters = addWaiters(mockScheduler, 1000 * (60 * refreshEveryMinutes - bufferIntrusionSeconds), numExpectedRefreshes + 1);    // Create the ExpiringCredentialRefreshingLogin instance under test    TestLoginContextFactory testLoginContextFactory = new TestLoginContextFactory();    TestExpiringCredentialRefreshingLogin testExpiringCredentialRefreshingLogin = new TestExpiringCredentialRefreshingLogin(refreshConfigThatPerformsReloginEveryGivenPercentageOfLifetime(1.0 * refreshEveryMinutes / lifetimeMinutes, minPeriodSeconds, bufferSeconds, clientReloginAllowedBeforeLogout), testLoginContextFactory, mockTime, 1000 * 60 * lifetimeMinutes, absoluteLastRefreshMs, clientReloginAllowedBeforeLogout);    testLoginContextFactory.configure(mockLoginContext, testExpiringCredentialRefreshingLogin);    /*         * Perform the login, wait up to a certain amount of time for the refresher         * thread to exit, and make sure the correct calls happened at the correct times         */    long expectedFinalMs = startMs + numExpectedRefreshes * 1000 * (60 * refreshEveryMinutes - bufferIntrusionSeconds);    assertFalse(testLoginContextFactory.refresherThreadStartedFuture().isDone());    assertFalse(testLoginContextFactory.refresherThreadDoneFuture().isDone());    testExpiringCredentialRefreshingLogin.login();    assertTrue(testLoginContextFactory.refresherThreadStartedFuture().isDone());    testLoginContextFactory.refresherThreadDoneFuture().get(1L, TimeUnit.SECONDS);    assertEquals(expectedFinalMs, mockTime.milliseconds());    for (int i = 0; i < numExpectedRefreshes; ++i) {        KafkaFutureImpl<Long> waiter = waiters.get(i);        assertTrue(waiter.isDone());        assertEquals((i + 1) * 1000 * (60 * refreshEveryMinutes - bufferIntrusionSeconds), waiter.get().longValue() - startMs);    }    assertFalse(waiters.get(numExpectedRefreshes).isDone());    InOrder inOrder = inOrder(mockLoginContext);    inOrder.verify(mockLoginContext).login();    for (int i = 0; i < numExpectedRefreshes; ++i) {        inOrder.verify(mockLoginContext).login();        inOrder.verify(mockLoginContext).logout();    }}
f8239
0
testLoginExceptionCausesCorrectLogout
public void kafkatest_f8240_0() throws Exception
{    int numExpectedRefreshes = 3;    boolean clientReloginAllowedBeforeLogout = true;    Subject subject = new Subject();    final LoginContext mockLoginContext = mock(LoginContext.class);    when(mockLoginContext.getSubject()).thenReturn(subject);    Mockito.doNothing().doThrow(new LoginException()).doNothing().when(mockLoginContext).login();    MockTime mockTime = new MockTime();    long startMs = mockTime.milliseconds();    /*         * Identify the lifetime of each expiring credential         */    long lifetimeMinutes = 100L;    /*         * Identify the point at which refresh will occur in that lifetime         */    long refreshEveryMinutes = 80L;    /*         * Set an absolute last refresh time that will cause the login thread to exit         * after a certain number of re-logins (by adding an extra half of a refresh         * interval).         */    long absoluteLastRefreshMs = startMs + (1 + numExpectedRefreshes) * 1000 * 60 * refreshEveryMinutes - 1000 * 60 * refreshEveryMinutes / 2;    /*         * Identify buffer time on either side for the refresh algorithm         */    short minPeriodSeconds = (short) 0;    short bufferSeconds = minPeriodSeconds;    // Create the ExpiringCredentialRefreshingLogin instance under test    TestLoginContextFactory testLoginContextFactory = new TestLoginContextFactory();    TestExpiringCredentialRefreshingLogin testExpiringCredentialRefreshingLogin = new TestExpiringCredentialRefreshingLogin(refreshConfigThatPerformsReloginEveryGivenPercentageOfLifetime(1.0 * refreshEveryMinutes / lifetimeMinutes, minPeriodSeconds, bufferSeconds, clientReloginAllowedBeforeLogout), testLoginContextFactory, mockTime, 1000 * 60 * lifetimeMinutes, absoluteLastRefreshMs, clientReloginAllowedBeforeLogout);    testLoginContextFactory.configure(mockLoginContext, testExpiringCredentialRefreshingLogin);    /*         * Perform the login and wait up to a certain amount of time for the refresher         * thread to exit.  A timeout indicates the thread died due to logout()         * being invoked on an instance where the login() invocation had failed.         */    assertFalse(testLoginContextFactory.refresherThreadStartedFuture().isDone());    assertFalse(testLoginContextFactory.refresherThreadDoneFuture().isDone());    testExpiringCredentialRefreshingLogin.login();    assertTrue(testLoginContextFactory.refresherThreadStartedFuture().isDone());    testLoginContextFactory.refresherThreadDoneFuture().get(1L, TimeUnit.SECONDS);}
f8240
0
testRfc7688Example
public void kafkatest_f8249_0() throws Exception
{    String message = "n,a=user@example.com,\u0001host=server.example.com\u0001port=143\u0001" + "auth=Bearer vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg\u0001\u0001";    OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse(message.getBytes(StandardCharsets.UTF_8));    assertEquals("vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg", response.tokenValue());    assertEquals("user@example.com", response.authorizationId());    assertEquals("server.example.com", response.extensions().map().get("host"));    assertEquals("143", response.extensions().map().get("port"));}
f8249
0
testNoExtensionsFromByteArray
public void kafkatest_f8250_0() throws Exception
{    String message = "n,a=user@example.com,\u0001" + "auth=Bearer vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg\u0001\u0001";    OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse(message.getBytes(StandardCharsets.UTF_8));    assertEquals("vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg", response.tokenValue());    assertEquals("user@example.com", response.authorizationId());    assertTrue(response.extensions().map().isEmpty());}
f8250
0
principalName
public String kafkatest_f8259_0()
{    return "principalName";}
f8259
0
startTimeMs
public Long kafkatest_f8260_0()
{    return null;}
f8260
0
unrecognizedExtensionsAreNotSaved
public void kafkatest_f8270_0() throws Exception
{    saslServer = new OAuthBearerSaslServer(EXTENSIONS_VALIDATOR_CALLBACK_HANDLER);    Map<String, String> customExtensions = new HashMap<>();    customExtensions.put("firstKey", "value1");    customExtensions.put("secondKey", "value1");    customExtensions.put("thirdKey", "value1");    byte[] nextChallenge = saslServer.evaluateResponse(clientInitialResponse(null, false, customExtensions));    assertTrue("Next challenge is not empty", nextChallenge.length == 0);    assertNull("Extensions not recognized by the server must be ignored", saslServer.getNegotiatedProperty("thirdKey"));}
f8270
0
throwsAuthenticationExceptionOnInvalidExtensions
public void kafkatest_f8271_0() throws Exception
{    OAuthBearerUnsecuredValidatorCallbackHandler invalidHandler = new OAuthBearerUnsecuredValidatorCallbackHandler() {        @Override        public void handle(Callback[] callbacks) throws UnsupportedCallbackException {            for (Callback callback : callbacks) {                if (callback instanceof OAuthBearerValidatorCallback) {                    OAuthBearerValidatorCallback validationCallback = (OAuthBearerValidatorCallback) callback;                    validationCallback.token(new OAuthBearerTokenMock());                } else if (callback instanceof OAuthBearerExtensionsValidatorCallback) {                    OAuthBearerExtensionsValidatorCallback extensionsCallback = (OAuthBearerExtensionsValidatorCallback) callback;                    extensionsCallback.error("firstKey", "is not valid");                    extensionsCallback.error("secondKey", "is not valid either");                } else                    throw new UnsupportedCallbackException(callback);            }        }    };    saslServer = new OAuthBearerSaslServer(invalidHandler);    Map<String, String> customExtensions = new HashMap<>();    customExtensions.put("firstKey", "value");    customExtensions.put("secondKey", "value");    saslServer.evaluateResponse(clientInitialResponse(null, false, customExtensions));}
f8271
0
invalidScope
public void kafkatest_f8280_0()
{    for (String invalidScope : new String[] { "\"foo", "\\foo" }) {        try {            OAuthBearerScopeUtils.parseScope(invalidScope);            fail("did not detect invalid scope: " + invalidScope);        } catch (OAuthBearerConfigException expected) {        // empty        }    }}
f8280
0
validClaims
public void kafkatest_f8281_0() throws OAuthBearerIllegalTokenException
{    double issuedAtSeconds = 100.1;    double expirationTimeSeconds = 300.3;    StringBuilder sb = new StringBuilder("{");    appendJsonText(sb, "sub", "SUBJECT");    appendCommaJsonText(sb, "iat", issuedAtSeconds);    appendCommaJsonText(sb, "exp", expirationTimeSeconds);    sb.append("}");    String compactSerialization = HEADER_COMPACT_SERIALIZATION + Base64.getUrlEncoder().withoutPadding().encodeToString(sb.toString().getBytes(StandardCharsets.UTF_8)) + ".";    OAuthBearerUnsecuredJws testJwt = new OAuthBearerUnsecuredJws(compactSerialization, "sub", "scope");    assertEquals(compactSerialization, testJwt.value());    assertEquals("sub", testJwt.principalClaimName());    assertEquals(1, testJwt.header().size());    assertEquals("none", testJwt.header().get("alg"));    assertEquals("scope", testJwt.scopeClaimName());    assertEquals(expirationTimeSeconds, testJwt.expirationTime());    assertTrue(testJwt.isClaimType("exp", Number.class));    assertEquals(issuedAtSeconds, testJwt.issuedAt());    assertEquals("SUBJECT", testJwt.subject());}
f8281
0
escape
private static String kafkatest_f8290_0(String jsonStringValue)
{    return jsonStringValue.replace("\"", "\\\"").replace("\\", "\\\\");}
f8290
0
addsExtensions
public void kafkatest_f8291_0() throws IOException, UnsupportedCallbackException
{    Map<String, String> options = new HashMap<>();    options.put("unsecuredLoginExtension_testId", "1");    OAuthBearerUnsecuredLoginCallbackHandler callbackHandler = createCallbackHandler(options, new MockTime());    SaslExtensionsCallback callback = new SaslExtensionsCallback();    callbackHandler.handle(new Callback[] { callback });    assertEquals("1", callback.extensions().map().get("testId"));}
f8291
0
tooEarlyExpirationTime
public void kafkatest_f8300_0() throws IOException, UnsupportedCallbackException
{    String claimsJson = "{" + PRINCIPAL_CLAIM_TEXT + comma(ISSUED_AT_CLAIM_TEXT) + comma(TOO_EARLY_EXPIRATION_TIME_CLAIM_TEXT) + "}";    confirmFailsValidation(UNSECURED_JWT_HEADER_JSON, claimsJson, MODULE_OPTIONS_MAP_NO_SCOPE_REQUIRED);}
f8300
0
includesRequiredScope
public void kafkatest_f8301_0()
{    String claimsJson = "{" + SUB_CLAIM_TEXT + comma(EXPIRATION_TIME_CLAIM_TEXT) + comma(SCOPE_CLAIM_TEXT) + "}";    Object validationResult = validationResult(UNSECURED_JWT_HEADER_JSON, claimsJson, MODULE_OPTIONS_MAP_REQUIRE_EXISTING_SCOPE);    assertTrue(validationResult instanceof OAuthBearerValidatorCallback);    assertTrue(((OAuthBearerValidatorCallback) validationResult).token() instanceof OAuthBearerUnsecuredJws);}
f8301
0
expClaimText
private static String kafkatest_f8310_0(long lifetimeSeconds)
{    return claimOrHeaderText("exp", MOCK_TIME.milliseconds() / 1000.0 + lifetimeSeconds);}
f8310
0
validateClaimForExistenceAndType
public void kafkatest_f8311_0() throws OAuthBearerIllegalTokenException
{    String claimName = "foo";    for (Boolean exists : new Boolean[] { null, Boolean.TRUE, Boolean.FALSE }) {        boolean useErrorValue = exists == null;        for (Boolean required : new boolean[] { true, false }) {            StringBuilder sb = new StringBuilder("{");            appendJsonText(sb, "exp", 100);            appendCommaJsonText(sb, "sub", "principalName");            if (useErrorValue)                appendCommaJsonText(sb, claimName, 1);            else if (exists != null && exists.booleanValue())                appendCommaJsonText(sb, claimName, claimName);            sb.append("}");            String compactSerialization = HEADER_COMPACT_SERIALIZATION + Base64.getUrlEncoder().withoutPadding().encodeToString(sb.toString().getBytes(StandardCharsets.UTF_8)) + ".";            OAuthBearerUnsecuredJws testJwt = new OAuthBearerUnsecuredJws(compactSerialization, "sub", "scope");            OAuthBearerValidationResult result = OAuthBearerValidationUtils.validateClaimForExistenceAndType(testJwt, required, claimName, String.class);            if (useErrorValue || required && !exists.booleanValue())                assertTrue(isFailureWithMessageAndNoFailureScope(result));            else                assertTrue(isSuccess(result));        }    }}
f8311
0
appendCommaJsonText
private static void kafkatest_f8320_0(StringBuilder sb, String claimName, Number claimValue)
{    sb.append(',').append(QUOTE).append(escape(claimName)).append(QUOTE).append(":").append(claimValue);}
f8320
0
appendCommaJsonText
private static void kafkatest_f8321_0(StringBuilder sb, String claimName, String claimValue)
{    sb.append(',').append(QUOTE).append(escape(claimName)).append(QUOTE).append(":").append(QUOTE).append(escape(claimValue)).append(QUOTE);}
f8321
0
configure
public void kafkatest_f8330_0(Map<String, ?> configs, String saslMechanism, List<AppConfigurationEntry> jaasConfigEntries)
{// empty}
f8330
0
close
public void kafkatest_f8331_0()
{// empty}
f8331
0
value
public String kafkatest_f8340_0()
{    return null;}
f8340
0
startTimeMs
public Long kafkatest_f8341_0()
{    return null;}
f8341
0
scope
public Set<String> kafkatest_f8350_0()
{    return Collections.emptySet();}
f8350
0
principalName
public String kafkatest_f8351_0()
{    return "principalName";}
f8351
0
value
public String kafkatest_f8360_0()
{    return "value";}
f8360
0
startTimeMs
public Long kafkatest_f8361_0()
{    return null;}
f8361
0
authorizatonIdNotEqualsAuthenticationId
public void kafkatest_f8370_0() throws Exception
{    saslServer.evaluateResponse(saslMessage(USER_B, USER_A, PASSWORD_A));}
f8370
0
emptyTokens
public void kafkatest_f8371_0()
{    Exception e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("", "", "")));    assertEquals("Authentication failed: username not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("", "", "p")));    assertEquals("Authentication failed: username not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("", "u", "")));    assertEquals("Authentication failed: password not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("a", "", "")));    assertEquals("Authentication failed: username not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("a", "", "p")));    assertEquals("Authentication failed: username not specified", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(saslMessage("a", "u", "")));    assertEquals("Authentication failed: password not specified", e.getMessage());    String nul = "\u0000";    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(String.format("%s%s%s%s%s%s", "a", nul, "u", nul, "p", nul).getBytes(StandardCharsets.UTF_8)));    assertEquals("Invalid SASL/PLAIN response: expected 3 tokens, got 4", e.getMessage());    e = assertThrows(SaslAuthenticationException.class, () -> saslServer.evaluateResponse(String.format("%s%s%s", "", nul, "u").getBytes(StandardCharsets.UTF_8)));    assertEquals("Invalid SASL/PLAIN response: expected 3 tokens, got 2", e.getMessage());}
f8371
0
missingFields
public void kafkatest_f8380_0()
{    String cred = ScramCredentialUtils.credentialToString(formatter.generateCredential("password", 2048));    ScramCredentialUtils.credentialFromString(cred.substring(cred.indexOf(',')));}
f8380
0
extraneousFields
public void kafkatest_f8381_0()
{    String cred = ScramCredentialUtils.credentialToString(formatter.generateCredential("password", 2048));    ScramCredentialUtils.credentialFromString(cred + ",a=test");}
f8381
0
validClientFinalMessage
public void kafkatest_f8390_0() throws SaslException
{    String nonce = formatter.secureRandomString();    String channelBinding = randomBytesAsString();    String proof = randomBytesAsString();    ClientFinalMessage m = new ClientFinalMessage(toBytes(channelBinding), nonce);    assertNull("Invalid proof", m.proof());    m.proof(toBytes(proof));    checkClientFinalMessage(m, channelBinding, nonce, proof);    // Default format used by Kafka client: channel-binding, nonce and proof are specified    String str = String.format("c=%s,r=%s,p=%s", channelBinding, nonce, proof);    m = createScramMessage(ClientFinalMessage.class, str);    checkClientFinalMessage(m, channelBinding, nonce, proof);    m = new ClientFinalMessage(m.toBytes());    checkClientFinalMessage(m, channelBinding, nonce, proof);    // Optional extension specified    for (String extension : VALID_EXTENSIONS) {        str = String.format("c=%s,r=%s,%s,p=%s", channelBinding, nonce, extension, proof);        checkClientFinalMessage(createScramMessage(ClientFinalMessage.class, str), channelBinding, nonce, proof);    }}
f8390
0
invalidClientFinalMessage
public void kafkatest_f8391_0()
{    String nonce = formatter.secureRandomString();    String channelBinding = randomBytesAsString();    String proof = randomBytesAsString();    // Invalid channel binding    String invalid = String.format("c=ab,r=%s,p=%s", nonce, proof);    checkInvalidScramMessage(ClientFirstMessage.class, invalid);    // Invalid proof    invalid = String.format("c=%s,r=%s,p=123", channelBinding, nonce);    checkInvalidScramMessage(ClientFirstMessage.class, invalid);    // Invalid extensions    for (String extension : INVALID_EXTENSIONS) {        invalid = String.format("c=%s,r=%s,%s,p=%s", channelBinding, nonce, extension, proof);        checkInvalidScramMessage(ClientFinalMessage.class, invalid);    }}
f8391
0
createScramMessage
private T kafkatest_f8400_0(Class<T> clazz, String message) throws SaslException
{    byte[] bytes = message.getBytes(StandardCharsets.UTF_8);    if (clazz == ClientFirstMessage.class)        return (T) new ClientFirstMessage(bytes);    else if (clazz == ServerFirstMessage.class)        return (T) new ServerFirstMessage(bytes);    else if (clazz == ClientFinalMessage.class)        return (T) new ClientFinalMessage(bytes);    else if (clazz == ServerFinalMessage.class)        return (T) new ServerFinalMessage(bytes);    else        throw new IllegalArgumentException("Unknown message type: " + clazz);}
f8400
0
checkInvalidScramMessage
private void kafkatest_f8401_0(Class<T> clazz, String message)
{    try {        createScramMessage(clazz, message);        fail("Exception not throws for invalid message of type " + clazz + " : " + message);    } catch (SaslException e) {    // Expected exception    }}
f8401
0
getServerAliases
public String[] kafkatest_f8412_0(String s, Principal[] principals)
{    return new String[] { ALIAS };}
f8412
0
chooseServerAlias
public String kafkatest_f8413_0(String s, Principal[] principals, Socket socket)
{    return ALIAS;}
f8413
0
testSslFactoryWithCustomKeyManagerConfiguration
public void kafkatest_f8430_0()
{    TestProviderCreator testProviderCreator = new TestProviderCreator();    Map<String, Object> serverSslConfig = TestSslUtils.createSslConfig(TestKeyManagerFactory.ALGORITHM, TestTrustManagerFactory.ALGORITHM);    serverSslConfig.put(SecurityConfig.SECURITY_PROVIDERS_CONFIG, testProviderCreator.getClass().getName());    SslFactory sslFactory = new SslFactory(Mode.SERVER);    sslFactory.configure(serverSslConfig);    assertNotNull("SslEngineBuilder not created", sslFactory.sslEngineBuilder());    Security.removeProvider(testProviderCreator.getProvider().getName());}
f8430
0
testSslFactoryWithoutProviderClassConfiguration
public void kafkatest_f8431_0()
{    // An exception is thrown as the algorithm is not registered through a provider    Map<String, Object> serverSslConfig = TestSslUtils.createSslConfig(TestKeyManagerFactory.ALGORITHM, TestTrustManagerFactory.ALGORITHM);    SslFactory sslFactory = new SslFactory(Mode.SERVER);    sslFactory.configure(serverSslConfig);}
f8431
0
testKeystoreVerifiableUsingTruststore
public void kafkatest_f8440_0() throws Exception
{    File trustStoreFile1 = File.createTempFile("truststore1", ".jks");    Map<String, Object> sslConfig1 = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile1, "server");    SslFactory sslFactory = new SslFactory(Mode.SERVER, null, true);    sslFactory.configure(sslConfig1);    File trustStoreFile2 = File.createTempFile("truststore2", ".jks");    Map<String, Object> sslConfig2 = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile2, "server");    // listener to stores that may not work with other brokers where the update hasn't yet been performed.    try {        sslFactory.validateReconfiguration(sslConfig2);        fail("ValidateReconfiguration did not fail as expected");    } catch (ConfigException e) {    // Expected exception    }}
f8440
0
testCertificateEntriesValidation
public void kafkatest_f8441_0() throws Exception
{    File trustStoreFile = File.createTempFile("truststore", ".jks");    Map<String, Object> serverSslConfig = TestSslUtils.createSslConfig(false, true, Mode.SERVER, trustStoreFile, "server");    Map<String, Object> newCnConfig = TestSslUtils.createSslConfig(false, true, Mode.SERVER, File.createTempFile("truststore", ".jks"), "server", "Another CN");    KeyStore ks1 = sslKeyStore(serverSslConfig).load();    KeyStore ks2 = sslKeyStore(serverSslConfig).load();    assertEquals(SslFactory.CertificateEntries.create(ks1), SslFactory.CertificateEntries.create(ks2));    // Use different alias name, validation should succeed    ks2.setCertificateEntry("another", ks1.getCertificate("localhost"));    assertEquals(SslFactory.CertificateEntries.create(ks1), SslFactory.CertificateEntries.create(ks2));    KeyStore ks3 = sslKeyStore(newCnConfig).load();    assertNotEquals(SslFactory.CertificateEntries.create(ks1), SslFactory.CertificateEntries.create(ks3));}
f8441
0
testRulesSplitting
public void kafkatest_f8450_0()
{    // seeing is believing    testRulesSplitting("[]", "");    testRulesSplitting("[DEFAULT]", "DEFAULT");    testRulesSplitting("[RULE:/]", "RULE://");    testRulesSplitting("[RULE:/.*]", "RULE:/.*/");    testRulesSplitting("[RULE:/.*/L]", "RULE:/.*/L");    testRulesSplitting("[RULE:/, DEFAULT]", "RULE://,DEFAULT");    testRulesSplitting("[RULE:/, DEFAULT]", "  RULE:// ,  DEFAULT  ");    testRulesSplitting("[RULE:   /     , DEFAULT]", "  RULE:   /     / ,  DEFAULT  ");    testRulesSplitting("[RULE:  /     /U, DEFAULT]", "  RULE:  /     /U   ,DEFAULT  ");    testRulesSplitting("[RULE:([A-Z]*)/$1/U, RULE:([a-z]+)/$1, DEFAULT]", "  RULE:([A-Z]*)/$1/U   ,RULE:([a-z]+)/$1/,   DEFAULT  ");    // empty rules are ignored    testRulesSplitting("[]", ",   , , ,      , , ,   ");    testRulesSplitting("[RULE:/, DEFAULT]", ",,RULE://,,,DEFAULT,,");    testRulesSplitting("[RULE: /   , DEFAULT]", ",  , RULE: /   /    ,,,   DEFAULT, ,   ");    testRulesSplitting("[RULE:   /  /U, DEFAULT]", "     ,  , RULE:   /  /U    ,,  ,DEFAULT, ,");    // escape sequences    testRulesSplitting("[RULE:\\/\\\\\\(\\)\\n\\t/\\/\\/]", "RULE:\\/\\\\\\(\\)\\n\\t/\\/\\//");    testRulesSplitting("[RULE:\\**\\/+/*/L, RULE:\\/*\\**/**]", "RULE:\\**\\/+/*/L,RULE:\\/*\\**/**/");    // rules rule    testRulesSplitting("[RULE:,RULE:,/,RULE:,\\//U, RULE:,/RULE:,, RULE:,RULE:,/L,RULE:,/L, RULE:, DEFAULT, /DEFAULT, DEFAULT]", "RULE:,RULE:,/,RULE:,\\//U,RULE:,/RULE:,/,RULE:,RULE:,/L,RULE:,/L,RULE:, DEFAULT, /DEFAULT/,DEFAULT");}
f8450
0
testCommaWithWhitespace
public void kafkatest_f8451_0() throws Exception
{    String rules = "RULE:^CN=((\\\\, *|\\w)+)(,.*|$)/$1/,DEFAULT";    SslPrincipalMapper mapper = SslPrincipalMapper.fromRules(rules);    assertEquals("Tkac\\, Adam", mapper.getName("CN=Tkac\\, Adam,OU=ITZ,DC=geodis,DC=cz"));}
f8451
0
floatSerdeShouldPreserveNaNValues
public void kafkatest_f8460_0()
{    int someNaNAsIntBits = 0x7f800001;    float someNaN = Float.intBitsToFloat(someNaNAsIntBits);    int anotherNaNAsIntBits = 0x7f800002;    float anotherNaN = Float.intBitsToFloat(anotherNaNAsIntBits);    try (Serde<Float> serde = Serdes.Float()) {        // Because of NaN semantics we must assert based on the raw int bits.        Float roundtrip = serde.deserializer().deserialize(topic, serde.serializer().serialize(topic, someNaN));        assertThat(Float.floatToRawIntBits(roundtrip), equalTo(someNaNAsIntBits));        Float otherRoundtrip = serde.deserializer().deserialize(topic, serde.serializer().serialize(topic, anotherNaN));        assertThat(Float.floatToRawIntBits(otherRoundtrip), equalTo(anotherNaNAsIntBits));    }}
f8460
0
getStringSerde
private Serde<String> kafkatest_f8461_0(String encoder)
{    Map<String, Object> serializerConfigs = new HashMap<String, Object>();    serializerConfigs.put("key.serializer.encoding", encoder);    Serializer<String> serializer = Serdes.String().serializer();    serializer.configure(serializerConfigs, true);    Map<String, Object> deserializerConfigs = new HashMap<String, Object>();    deserializerConfigs.put("key.deserializer.encoding", encoder);    Deserializer<String> deserializer = Serdes.String().deserializer();    deserializer.configure(deserializerConfigs, true);    return Serdes.serdeFrom(serializer, deserializer);}
f8461
0
testRegisterAppInfoRegistersMetrics
public void kafkatest_f8470_0() throws JMException
{    registerAppInfo();}
f8470
0
testUnregisterAppInfoUnregistersMetrics
public void kafkatest_f8471_0() throws JMException
{    registerAppInfo();    AppInfoParser.unregisterAppInfo(METRICS_PREFIX, METRICS_ID, metrics);    assertFalse(mBeanServer.isRegistered(expectedAppObjectName()));    assertNull(metrics.metric(metrics.metricName("commit-id", "app-info")));    assertNull(metrics.metric(metrics.metricName("version", "app-info")));    assertNull(metrics.metric(metrics.metricName("start-time-ms", "app-info")));}
f8471
0
testWriteByteBuffer
public void kafkatest_f8480_0() throws IOException
{    testWriteByteBuffer(ByteBuffer.allocate(16));}
f8480
0
testWriteDirectByteBuffer
public void kafkatest_f8481_0() throws IOException
{    testWriteByteBuffer(ByteBuffer.allocateDirect(16));}
f8481
0
testVarlongSerde
public void kafkatest_f8490_0() throws Exception
{    assertVarlongSerde(0, new byte[] { x00 });    assertVarlongSerde(-1, new byte[] { x01 });    assertVarlongSerde(1, new byte[] { x02 });    assertVarlongSerde(63, new byte[] { x7E });    assertVarlongSerde(-64, new byte[] { x7F });    assertVarlongSerde(64, new byte[] { x80, x01 });    assertVarlongSerde(-65, new byte[] { x81, x01 });    assertVarlongSerde(8191, new byte[] { xFE, x7F });    assertVarlongSerde(-8192, new byte[] { xFF, x7F });    assertVarlongSerde(8192, new byte[] { x80, x80, x01 });    assertVarlongSerde(-8193, new byte[] { x81, x80, x01 });    assertVarlongSerde(1048575, new byte[] { xFE, xFF, x7F });    assertVarlongSerde(-1048576, new byte[] { xFF, xFF, x7F });    assertVarlongSerde(1048576, new byte[] { x80, x80, x80, x01 });    assertVarlongSerde(-1048577, new byte[] { x81, x80, x80, x01 });    assertVarlongSerde(134217727, new byte[] { xFE, xFF, xFF, x7F });    assertVarlongSerde(-134217728, new byte[] { xFF, xFF, xFF, x7F });    assertVarlongSerde(134217728, new byte[] { x80, x80, x80, x80, x01 });    assertVarlongSerde(-134217729, new byte[] { x81, x80, x80, x80, x01 });    assertVarlongSerde(Integer.MAX_VALUE, new byte[] { xFE, xFF, xFF, xFF, x0F });    assertVarlongSerde(Integer.MIN_VALUE, new byte[] { xFF, xFF, xFF, xFF, x0F });    assertVarlongSerde(17179869183L, new byte[] { xFE, xFF, xFF, xFF, x7F });    assertVarlongSerde(-17179869184L, new byte[] { xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(17179869184L, new byte[] { x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-17179869185L, new byte[] { x81, x80, x80, x80, x80, x01 });    assertVarlongSerde(2199023255551L, new byte[] { xFE, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(-2199023255552L, new byte[] { xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(2199023255552L, new byte[] { x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-2199023255553L, new byte[] { x81, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(281474976710655L, new byte[] { xFE, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(-281474976710656L, new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(281474976710656L, new byte[] { x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-281474976710657L, new byte[] { x81, x80, x80, x80, x80, x80, x80, 1 });    assertVarlongSerde(36028797018963967L, new byte[] { xFE, xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(-36028797018963968L, new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(36028797018963968L, new byte[] { x80, x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-36028797018963969L, new byte[] { x81, x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(4611686018427387903L, new byte[] { xFE, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(-4611686018427387904L, new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x7F });    assertVarlongSerde(4611686018427387904L, new byte[] { x80, x80, x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(-4611686018427387905L, new byte[] { x81, x80, x80, x80, x80, x80, x80, x80, x80, x01 });    assertVarlongSerde(Long.MAX_VALUE, new byte[] { xFE, xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x01 });    assertVarlongSerde(Long.MIN_VALUE, new byte[] { xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF, x01 });}
f8490
0
testInvalidVarint
public void kafkatest_f8491_0()
{    // varint encoding has one overflow byte    ByteBuffer buf = ByteBuffer.wrap(new byte[] { xFF, xFF, xFF, xFF, xFF, x01 });    ByteUtils.readVarint(buf);}
f8491
0
testUpdateLong
public void kafkatest_f8500_0()
{    final long value = Integer.MAX_VALUE + 1;    final ByteBuffer buffer = ByteBuffer.allocate(8);    buffer.putLong(value);    Checksum crc1 = new Crc32();    Checksum crc2 = new Crc32();    Checksums.updateLong(crc1, value);    crc2.update(buffer.array(), buffer.arrayOffset(), 8);    assertEquals("Crc values should be the same", crc1.getValue(), crc2.getValue());}
f8500
0
doTestUpdateByteBufferWithOffsetPosition
private void kafkatest_f8501_0(byte[] bytes, ByteBuffer buffer, int offset)
{    buffer.put(bytes);    buffer.flip();    buffer.position(offset);    Checksum bufferCrc = Crc32C.create();    Checksums.update(bufferCrc, buffer, buffer.remaining());    assertEquals(Crc32C.compute(bytes, offset, buffer.remaining()), bufferCrc.getValue());    assertEquals(offset, buffer.position());}
f8501
0
shouldForbidConditionalRemove
public void kafkatest_f8510_0()
{    final FixedOrderMap<String, Integer> map = new FixedOrderMap<>();    map.put("a", 0);    try {        map.remove("a", 0);        fail("expected exception");    } catch (final RuntimeException e) {        assertThat(e, CoreMatchers.instanceOf(UnsupportedOperationException.class));    }    assertThat(map.get("a"), is(0));}
f8510
0
shouldForbidConditionalClear
public void kafkatest_f8511_0()
{    final FixedOrderMap<String, Integer> map = new FixedOrderMap<>();    map.put("a", 0);    try {        map.clear();        fail("expected exception");    } catch (final RuntimeException e) {        assertThat(e, CoreMatchers.instanceOf(UnsupportedOperationException.class));    }    assertThat(map.get("a"), is(0));}
f8511
0
testInsertDelete
public void kafkatest_f8520_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>(100);    assertTrue(coll.add(new TestElement(1)));    TestElement second = new TestElement(2);    assertTrue(coll.add(second));    assertTrue(coll.add(new TestElement(3)));    assertFalse(coll.add(new TestElement(3)));    assertEquals(3, coll.size());    assertTrue(coll.contains(new TestElement(1)));    assertFalse(coll.contains(new TestElement(4)));    TestElement secondAgain = coll.find(new TestElement(2));    assertTrue(second == secondAgain);    assertTrue(coll.remove(new TestElement(1)));    assertFalse(coll.remove(new TestElement(1)));    assertEquals(2, coll.size());    coll.clear();    assertEquals(0, coll.size());}
f8520
0
expectTraversal
 static void kafkatest_f8521_0(Iterator<TestElement> iterator, Integer... sequence)
{    int i = 0;    while (iterator.hasNext()) {        TestElement element = iterator.next();        Assert.assertTrue("Iterator yieled " + (i + 1) + " elements, but only " + sequence.length + " were expected.", i < sequence.length);        Assert.assertEquals("Iterator value number " + (i + 1) + " was incorrect.", sequence[i].intValue(), element.val);        i = i + 1;    }    Assert.assertTrue("Iterator yieled " + (i + 1) + " elements, but " + sequence.length + " were expected.", i == sequence.length);}
f8521
0
testListIteratorTraversal
public void kafkatest_f8530_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>();    coll.add(new TestElement(1));    coll.add(new TestElement(2));    coll.add(new TestElement(3));    ListIterator<TestElement> iter = coll.valuesList().listIterator();    // Step the iterator forward to the end of the list    assertTrue(iter.hasNext());    assertFalse(iter.hasPrevious());    assertEquals(0, iter.nextIndex());    assertEquals(-1, iter.previousIndex());    assertEquals(1, iter.next().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    assertEquals(2, iter.next().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(2, iter.nextIndex());    assertEquals(1, iter.previousIndex());    assertEquals(3, iter.next().val);    assertFalse(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(3, iter.nextIndex());    assertEquals(2, iter.previousIndex());    // Step back to the middle of the list    assertEquals(3, iter.previous().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(2, iter.nextIndex());    assertEquals(1, iter.previousIndex());    assertEquals(2, iter.previous().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    // Step forward one and then back one, return value should remain the same    assertEquals(2, iter.next().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(2, iter.nextIndex());    assertEquals(1, iter.previousIndex());    assertEquals(2, iter.previous().val);    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    // Step back to the front of the list    assertEquals(1, iter.previous().val);    assertTrue(iter.hasNext());    assertFalse(iter.hasPrevious());    assertEquals(0, iter.nextIndex());    assertEquals(-1, iter.previousIndex());}
f8530
0
testListIteratorRemove
public void kafkatest_f8531_0()
{    ImplicitLinkedHashCollection<TestElement> coll = new ImplicitLinkedHashCollection<>();    coll.add(new TestElement(1));    coll.add(new TestElement(2));    coll.add(new TestElement(3));    coll.add(new TestElement(4));    coll.add(new TestElement(5));    ListIterator<TestElement> iter = coll.valuesList().listIterator();    try {        iter.remove();        fail("Calling remove() without calling next() or previous() should raise an exception");    } catch (IllegalStateException e) {    // expected    }    // Remove after next()    iter.next();    iter.next();    iter.next();    iter.remove();    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(2, iter.nextIndex());    assertEquals(1, iter.previousIndex());    try {        iter.remove();        fail("Calling remove() twice without calling next() or previous() in between should raise an exception");    } catch (IllegalStateException e) {    // expected    }    // Remove after previous()    assertEquals(2, iter.previous().val);    iter.remove();    assertTrue(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    // Remove the first element of the list    assertEquals(1, iter.previous().val);    iter.remove();    assertTrue(iter.hasNext());    assertFalse(iter.hasPrevious());    assertEquals(0, iter.nextIndex());    assertEquals(-1, iter.previousIndex());    // Remove the last element of the list    assertEquals(4, iter.next().val);    assertEquals(5, iter.next().val);    iter.remove();    assertFalse(iter.hasNext());    assertTrue(iter.hasPrevious());    assertEquals(1, iter.nextIndex());    assertEquals(0, iter.previousIndex());    // Remove the final remaining element of the list    assertEquals(4, iter.previous().val);    iter.remove();    assertFalse(iter.hasNext());    assertFalse(iter.hasPrevious());    assertEquals(0, iter.nextIndex());    assertEquals(-1, iter.previousIndex());}
f8531
0
testFindFindAllContainsRemoveOnEmptyCollection
public void kafkatest_f8540_0()
{    ImplicitLinkedHashMultiCollection<TestElement> coll = new ImplicitLinkedHashMultiCollection<>();    assertNull(coll.find(new TestElement(2)));    assertFalse(coll.contains(new TestElement(2)));    assertFalse(coll.remove(new TestElement(2)));    assertTrue(coll.findAll(new TestElement(2)).isEmpty());}
f8540
0
testInsertDelete
public void kafkatest_f8541_0()
{    ImplicitLinkedHashMultiCollection<TestElement> multiSet = new ImplicitLinkedHashMultiCollection<>(100);    TestElement e1 = new TestElement(1);    TestElement e2 = new TestElement(1);    TestElement e3 = new TestElement(2);    multiSet.mustAdd(e1);    multiSet.mustAdd(e2);    multiSet.mustAdd(e3);    assertFalse(multiSet.add(e3));    assertEquals(3, multiSet.size());    expectExactTraversal(multiSet.findAll(e1).iterator(), e1, e2);    expectExactTraversal(multiSet.findAll(e3).iterator(), e3);    multiSet.remove(e2);    expectExactTraversal(multiSet.findAll(e1).iterator(), e1);    assertTrue(multiSet.contains(e2));}
f8541
0
testLoadKerberosLoginModule
public void kafkatest_f8550_0() throws ClassNotFoundException
{    String clazz = Java.isIbmJdk() ? "com.ibm.security.auth.module.Krb5LoginModule" : "com.sun.security.auth.module.Krb5LoginModule";    Class.forName(clazz);}
f8550
0
testJavaVersion
public void kafkatest_f8551_0()
{    Java.Version v = Java.parseVersion("9");    assertEquals(9, v.majorVersion);    assertEquals(0, v.minorVersion);    assertTrue(v.isJava9Compatible());    v = Java.parseVersion("9.0.1");    assertEquals(9, v.majorVersion);    assertEquals(0, v.minorVersion);    assertTrue(v.isJava9Compatible());    // Azul Zulu    v = Java.parseVersion("9.0.0.15");    assertEquals(9, v.majorVersion);    assertEquals(0, v.minorVersion);    assertTrue(v.isJava9Compatible());    v = Java.parseVersion("9.1");    assertEquals(9, v.majorVersion);    assertEquals(1, v.minorVersion);    assertTrue(v.isJava9Compatible());    v = Java.parseVersion("1.8.0_152");    assertEquals(1, v.majorVersion);    assertEquals(8, v.minorVersion);    assertFalse(v.isJava9Compatible());    v = Java.parseVersion("1.7.0_80");    assertEquals(1, v.majorVersion);    assertEquals(7, v.minorVersion);    assertFalse(v.isJava9Compatible());}
f8551
0
addListener
public void kafkatest_f8560_0(MockTimeListener listener)
{    listeners.add(listener);}
f8560
0
milliseconds
public long kafkatest_f8561_0()
{    maybeSleep(autoTickMs);    return timeMs.get();}
f8561
0
createTime
protected Time kafkatest_f8570_0()
{    return new MockTime();}
f8570
0
testSanitize
public void kafkatest_f8571_0()
{    String principal = "CN=Some characters !@#$%&*()_-+=';:,/~";    String sanitizedPrincipal = Sanitizer.sanitize(principal);    assertTrue(sanitizedPrincipal.replace('%', '_').matches("[a-zA-Z0-9\\._\\-]+"));    assertEquals(principal, Sanitizer.desanitize(sanitizedPrincipal));}
f8571
0
getProviderIndexFromName
private int kafkatest_f8580_0(String providerName, Provider[] providers)
{    for (int index = 0; index < providers.length; index++) {        if (providers[index].getName().equals(providerName)) {            return index;        }    }    return -1;}
f8580
0
testAddCustomSecurityProvider
public void kafkatest_f8581_0()
{    String customProviderClasses = testScramSaslServerProviderCreator.getClass().getName() + "," + testPlainSaslServerProviderCreator.getClass().getName();    Map<String, String> configs = new HashMap<>();    configs.put(SecurityConfig.SECURITY_PROVIDERS_CONFIG, customProviderClasses);    SecurityUtils.addConfiguredSecurityProviders(configs);    Provider[] providers = Security.getProviders();    int testScramSaslServerProviderIndex = getProviderIndexFromName(testScramSaslServerProvider.getName(), providers);    int testPlainSaslServerProviderIndex = getProviderIndexFromName(testPlainSaslServerProvider.getName(), providers);    // validations    MatcherAssert.assertThat(testScramSaslServerProvider.getName() + " testProvider not found at expected index", testScramSaslServerProviderIndex == 0);    MatcherAssert.assertThat(testPlainSaslServerProvider.getName() + " testProvider not found at expected index", testPlainSaslServerProviderIndex == 1);}
f8581
0
createTime
protected Time kafkatest_f8590_0()
{    return Time.SYSTEM;}
f8590
0
testTimerUpdate
public void kafkatest_f8591_0()
{    Timer timer = time.timer(500);    assertEquals(500, timer.remainingMs());    assertEquals(0, timer.elapsedMs());    time.sleep(100);    timer.update();    assertEquals(400, timer.remainingMs());    assertEquals(100, timer.elapsedMs());    time.sleep(400);    timer.update(time.milliseconds());    assertEquals(0, timer.remainingMs());    assertEquals(500, timer.elapsedMs());    assertTrue(timer.isExpired());    // Going over the expiration is fine and the elapsed time can exceed    // the initial timeout. However, remaining time should be stuck at 0.    time.sleep(200);    timer.update(time.milliseconds());    assertTrue(timer.isExpired());    assertEquals(0, timer.remainingMs());    assertEquals(700, timer.elapsedMs());}
f8591
0
testGetHost
public void kafkatest_f8600_0()
{    assertEquals("127.0.0.1", getHost("127.0.0.1:8000"));    assertEquals("mydomain.com", getHost("PLAINTEXT://mydomain.com:8080"));    assertEquals("MyDomain.com", getHost("PLAINTEXT://MyDomain.com:8080"));    assertEquals("My_Domain.com", getHost("PLAINTEXT://My_Domain.com:8080"));    assertEquals("::1", getHost("[::1]:1234"));    assertEquals("2001:db8:85a3:8d3:1319:8a2e:370:7348", getHost("PLAINTEXT://[2001:db8:85a3:8d3:1319:8a2e:370:7348]:5678"));    assertEquals("2001:DB8:85A3:8D3:1319:8A2E:370:7348", getHost("PLAINTEXT://[2001:DB8:85A3:8D3:1319:8A2E:370:7348]:5678"));    assertEquals("fe80::b1da:69ca:57f7:63d8%3", getHost("PLAINTEXT://[fe80::b1da:69ca:57f7:63d8%3]:5678"));}
f8600
0
testHostPattern
public void kafkatest_f8601_0()
{    assertTrue(validHostPattern("127.0.0.1"));    assertTrue(validHostPattern("mydomain.com"));    assertTrue(validHostPattern("MyDomain.com"));    assertTrue(validHostPattern("My_Domain.com"));    assertTrue(validHostPattern("::1"));    assertTrue(validHostPattern("2001:db8:85a3:8d3:1319:8a2e:370"));}
f8601
0
toArrayDirectByteBuffer
public void kafkatest_f8610_0()
{    byte[] input = { 0, 1, 2, 3, 4 };    ByteBuffer buffer = ByteBuffer.allocateDirect(5);    buffer.put(input);    buffer.rewind();    assertArrayEquals(input, Utils.toArray(buffer));    assertEquals(0, buffer.position());    assertArrayEquals(new byte[] { 1, 2 }, Utils.toArray(buffer, 1, 2));    assertEquals(0, buffer.position());    buffer.position(2);    assertArrayEquals(new byte[] { 2, 3, 4 }, Utils.toArray(buffer));    assertEquals(2, buffer.position());}
f8610
0
utf8ByteArraySerde
public void kafkatest_f8611_0()
{    String utf8String = "A\u00ea\u00f1\u00fcC";    byte[] utf8Bytes = utf8String.getBytes(StandardCharsets.UTF_8);    assertArrayEquals(utf8Bytes, Utils.utf8(utf8String));    assertEquals(utf8Bytes.length, Utils.utf8Length(utf8String));    assertEquals(utf8String, Utils.utf8(utf8Bytes));}
f8611
0
testReadFullyWithPartialFileChannelReads
public void kafkatest_f8620_0() throws IOException
{    FileChannel channelMock = mock(FileChannel.class);    final int bufferSize = 100;    String expectedBufferContent = fileChannelMockExpectReadWithRandomBytes(channelMock, bufferSize);    ByteBuffer buffer = ByteBuffer.allocate(bufferSize);    Utils.readFully(channelMock, buffer, 0L);    assertEquals("The buffer should be populated correctly.", expectedBufferContent, new String(buffer.array()));    assertFalse("The buffer should be filled", buffer.hasRemaining());    verify(channelMock, atLeastOnce()).read(any(), anyLong());}
f8620
0
testReadFullyIfEofIsReached
public void kafkatest_f8621_0() throws IOException
{    final FileChannel channelMock = mock(FileChannel.class);    final int bufferSize = 100;    final String fileChannelContent = "abcdefghkl";    ByteBuffer buffer = ByteBuffer.allocate(bufferSize);    when(channelMock.read(any(), anyLong())).then(invocation -> {        ByteBuffer bufferArg = invocation.getArgument(0);        bufferArg.put(fileChannelContent.getBytes());        return -1;    });    Utils.readFully(channelMock, buffer, 0L);    assertEquals("abcdefghkl", new String(buffer.array(), 0, buffer.position()));    assertEquals(fileChannelContent.length(), buffer.position());    assertTrue(buffer.hasRemaining());    verify(channelMock, atLeastOnce()).read(any(), anyLong());}
f8621
0
receive
public NetworkReceive kafkatest_f8630_0()
{    return receive;}
f8630
0
main
public static void kafkatest_f8631_0(String[] args)
{    long iters = Long.parseLong(args[0]);    Metrics metrics = new Metrics();    try {        Sensor parent = metrics.sensor("parent");        Sensor child = metrics.sensor("child", parent);        for (Sensor sensor : Arrays.asList(parent, child)) {            sensor.add(metrics.metricName(sensor.name() + ".avg", "grp1"), new Avg());            sensor.add(metrics.metricName(sensor.name() + ".count", "grp1"), new WindowedCount());            sensor.add(metrics.metricName(sensor.name() + ".max", "grp1"), new Max());            sensor.add(new Percentiles(1024, 0.0, iters, BucketSizing.CONSTANT, new Percentile(metrics.metricName(sensor.name() + ".median", "grp1"), 50.0), new Percentile(metrics.metricName(sensor.name() + ".p_99", "grp1"), 99.0)));        }        long start = System.nanoTime();        for (int i = 0; i < iters; i++) parent.record(i);        double ellapsed = (System.nanoTime() - start) / (double) iters;        System.out.println(String.format("%.2f ns per metric recording.", ellapsed));    } finally {        metrics.close();    }}
f8631
0
systemNanos
private static long kafkatest_f8640_0(int iters)
{    long total = 0;    for (int i = 0; i < iters; i++) total += System.currentTimeMillis();    return total;}
f8640
0
onUpdate
public void kafkatest_f8641_0(ClusterResource clusterResource)
{    IS_ON_UPDATE_CALLED.set(true);    this.clusterResource = clusterResource;}
f8641
0
configure
public void kafkatest_f8650_0(Map<String, ?> configs, boolean isKey)
{    this.configs = configs;    this.isKey = isKey;}
f8650
0
deserialize
public byte[] kafkatest_f8651_0(String topic, byte[] data)
{    // This will ensure that we get the cluster metadata when deserialize is called for the first time    // as subsequent compareAndSet operations will fail.    clusterIdBeforeDeserialize.compareAndSet(noClusterId, clusterMeta.get());    return data;}
f8651
0
configure
public void kafkatest_f8663_0(Map<String, ?> configs)
{    // ensure this method is called and expected configs are passed in    Object o = configs.get(APPEND_STRING_PROP);    if (o == null)        throw new ConfigException("Mock producer interceptor expects configuration " + APPEND_STRING_PROP);    if (o instanceof String)        appendStr = (String) o;    // clientId also must be in configs    Object clientIdValue = configs.get(ProducerConfig.CLIENT_ID_CONFIG);    if (clientIdValue == null)        throw new ConfigException("Mock producer interceptor expects configuration " + ProducerConfig.CLIENT_ID_CONFIG);}
f8663
0
onSend
public ProducerRecord<String, String> kafkatest_f8664_0(ProducerRecord<String, String> record)
{    ONSEND_COUNT.incrementAndGet();    return new ProducerRecord<>(record.topic(), record.partition(), record.key(), record.value().concat(appendStr));}
f8664
0
clear
public void kafkatest_f8675_0()
{    this.completedSends.clear();    this.completedReceives.clear();    this.disconnected.clear();    this.connected.clear();}
f8675
0
send
public void kafkatest_f8676_0(Send send)
{    this.initiatedSends.add(send);}
f8676
0
delayedReceive
public void kafkatest_f8685_0(DelayedReceive receive)
{    this.delayedReceives.add(receive);}
f8685
0
disconnected
public Map<String, ChannelState> kafkatest_f8686_0()
{    return disconnected;}
f8686
0
createEmptyKeyStore
private static KeyStore kafkatest_f8699_0() throws GeneralSecurityException, IOException
{    KeyStore ks = KeyStore.getInstance("JKS");    // initialize    ks.load(null, null);    return ks;}
f8699
0
saveKeyStore
private static void kafkatest_f8700_0(KeyStore ks, String filename, Password password) throws GeneralSecurityException, IOException
{    try (OutputStream out = Files.newOutputStream(Paths.get(filename))) {        ks.store(out, password.value().toCharArray());    }}
f8700
0
sanDnsName
public CertificateBuilder kafkatest_f8709_0(String hostName) throws IOException
{    subjectAltName = new GeneralNames(new GeneralName(GeneralName.dNSName, hostName)).getEncoded();    return this;}
f8709
0
sanIpAddress
public CertificateBuilder kafkatest_f8710_0(InetAddress hostAddress) throws IOException
{    subjectAltName = new GeneralNames(new GeneralName(GeneralName.iPAddress, new DEROctetString(hostAddress.getAddress()))).getEncoded();    return this;}
f8710
0
metadataUpdateWith
public static MetadataResponse kafkatest_f8719_0(final String clusterId, final int numNodes, final Map<String, Errors> topicErrors, final Map<String, Integer> topicPartitionCounts, final Function<TopicPartition, Integer> epochSupplier)
{    return metadataUpdateWith(clusterId, numNodes, topicErrors, topicPartitionCounts, epochSupplier, MetadataResponse.PartitionMetadata::new);}
f8719
0
metadataUpdateWith
public static MetadataResponse kafkatest_f8720_0(final String clusterId, final int numNodes, final Map<String, Errors> topicErrors, final Map<String, Integer> topicPartitionCounts, final Function<TopicPartition, Integer> epochSupplier, final PartitionMetadataSupplier partitionSupplier)
{    final List<Node> nodes = new ArrayList<>(numNodes);    for (int i = 0; i < numNodes; i++) nodes.add(new Node(i, "localhost", 1969 + i));    List<MetadataResponse.TopicMetadata> topicMetadata = new ArrayList<>();    for (Map.Entry<String, Integer> topicPartitionCountEntry : topicPartitionCounts.entrySet()) {        String topic = topicPartitionCountEntry.getKey();        int numPartitions = topicPartitionCountEntry.getValue();        List<MetadataResponse.PartitionMetadata> partitionMetadata = new ArrayList<>(numPartitions);        for (int i = 0; i < numPartitions; i++) {            TopicPartition tp = new TopicPartition(topic, i);            Node leader = nodes.get(i % nodes.size());            List<Node> replicas = Collections.singletonList(leader);            partitionMetadata.add(partitionSupplier.supply(Errors.NONE, i, leader, Optional.ofNullable(epochSupplier.apply(tp)), replicas, replicas, replicas));        }        topicMetadata.add(new MetadataResponse.TopicMetadata(Errors.NONE, topic, Topic.isInternal(topic), partitionMetadata));    }    for (Map.Entry<String, Errors> topicErrorEntry : topicErrors.entrySet()) {        String topic = topicErrorEntry.getKey();        topicMetadata.add(new MetadataResponse.TopicMetadata(topicErrorEntry.getValue(), topic, Topic.isInternal(topic), Collections.emptyList()));    }    return MetadataResponse.prepareResponse(nodes, clusterId, 0, topicMetadata);}
f8720
0
producerConfig
public static Properties kafkatest_f8729_0(final String bootstrapServers, final Class keySerializer, final Class valueSerializer, final Properties additional)
{    final Properties properties = new Properties();    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);    properties.put(ProducerConfig.ACKS_CONFIG, "all");    properties.put(ProducerConfig.RETRIES_CONFIG, 0);    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);    properties.putAll(additional);    return properties;}
f8729
0
producerConfig
public static Properties kafkatest_f8730_0(final String bootstrapServers, final Class keySerializer, final Class valueSerializer)
{    return producerConfig(bootstrapServers, keySerializer, valueSerializer, new Properties());}
f8730
0
checkEquals
public static void kafkatest_f8739_0(Iterable<T> it1, Iterable<T> it2)
{    assertEquals(toList(it1), toList(it2));}
f8739
0
checkEquals
public static void kafkatest_f8740_0(Iterator<T> it1, Iterator<T> it2)
{    assertEquals(Utils.toList(it1), Utils.toList(it2));}
f8740
0
initialize
public void kafkatest_f8749_0(ConnectorContext ctx)
{    context = ctx;}
f8749
0
initialize
public void kafkatest_f8750_0(ConnectorContext ctx, List<Map<String, String>> taskConfigs)
{    context = ctx;// Ignore taskConfigs. May result in more churn of tasks during recovery if updated configs// are very different, but reduces the difficulty of implementing a Connector}
f8750
0
timestamp
public Long kafkatest_f8759_0()
{    return timestamp;}
f8759
0
headers
public Headers kafkatest_f8760_0()
{    return headers;}
f8760
0
type
public Type kafkatest_f8769_0()
{    return type;}
f8769
0
isOptional
public boolean kafkatest_f8770_0()
{    return optional;}
f8770
0
valueSchema
public Schema kafkatest_f8779_0()
{    if (type != Type.MAP && type != Type.ARRAY)        throw new DataException("Cannot look up value schema on non-array and non-map type");    return valueSchema;}
f8779
0
validateValue
public static void kafkatest_f8780_0(Schema schema, Object value)
{    validateValue(null, schema, value);}
f8780
0
builder
public static SchemaBuilder kafkatest_f8789_0()
{    return SchemaBuilder.int32().name(LOGICAL_NAME).version(1);}
f8789
0
fromLogical
public static int kafkatest_f8790_0(Schema schema, java.util.Date value)
{    if (!(LOGICAL_NAME.equals(schema.name())))        throw new DataException("Requested conversion of Date object but the schema does not match.");    Calendar calendar = Calendar.getInstance(UTC);    calendar.setTime(value);    if (calendar.get(Calendar.HOUR_OF_DAY) != 0 || calendar.get(Calendar.MINUTE) != 0 || calendar.get(Calendar.SECOND) != 0 || calendar.get(Calendar.MILLISECOND) != 0) {        throw new DataException("Kafka Connect Date type should not have any time fields set to non-zero values.");    }    long unixMillis = calendar.getTimeInMillis();    return (int) (unixMillis / MILLIS_PER_DAY);}
f8790
0
schema
public Schema kafkatest_f8799_0()
{    return schema;}
f8799
0
equals
public boolean kafkatest_f8800_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Field field = (Field) o;    return Objects.equals(index, field.index) && Objects.equals(name, field.name) && Objects.equals(schema, field.schema);}
f8800
0
toString
public String kafkatest_f8809_0()
{    return "SchemaAndValue{" + "schema=" + schema + ", value=" + value + '}';}
f8809
0
isOptional
public boolean kafkatest_f8810_0()
{    return optional == null ? false : optional;}
f8810
0
doc
public String kafkatest_f8819_0()
{    return doc;}
f8819
0
doc
public SchemaBuilder kafkatest_f8820_0(String doc)
{    checkCanSet(DOC_FIELD, this.doc, doc);    this.doc = doc;    return this;}
f8820
0
int64
public static SchemaBuilder kafkatest_f8829_0()
{    return new SchemaBuilder(Type.INT64);}
f8829
0
float32
public static SchemaBuilder kafkatest_f8830_0()
{    return new SchemaBuilder(Type.FLOAT32);}
f8830
0
array
public static SchemaBuilder kafkatest_f8839_0(Schema valueSchema)
{    if (null == valueSchema)        throw new SchemaBuilderException("valueSchema cannot be null.");    SchemaBuilder builder = new SchemaBuilder(Type.ARRAY);    builder.valueSchema = valueSchema;    return builder;}
f8839
0
map
public static SchemaBuilder kafkatest_f8840_0(Schema keySchema, Schema valueSchema)
{    if (null == keySchema)        throw new SchemaBuilderException("keySchema cannot be null.");    if (null == valueSchema)        throw new SchemaBuilderException("valueSchema cannot be null.");    SchemaBuilder builder = new SchemaBuilder(Type.MAP);    builder.keySchema = keySchema;    builder.valueSchema = valueSchema;    return builder;}
f8840
0
projectStruct
private static Object kafkatest_f8849_0(Schema source, Struct sourceStruct, Schema target) throws SchemaProjectorException
{    Struct targetStruct = new Struct(target);    for (Field targetField : target.fields()) {        String fieldName = targetField.name();        Field sourceField = source.field(fieldName);        if (sourceField != null) {            Object sourceFieldValue = sourceStruct.get(fieldName);            try {                Object targetFieldValue = project(sourceField.schema(), sourceFieldValue, targetField.schema());                targetStruct.put(fieldName, targetFieldValue);            } catch (SchemaProjectorException e) {                throw new SchemaProjectorException("Error projecting " + sourceField.name(), e);            }        } else if (targetField.schema().isOptional()) {        // Ignore missing field        } else if (targetField.schema().defaultValue() != null) {            targetStruct.put(fieldName, targetField.schema().defaultValue());        } else {            throw new SchemaProjectorException("Required field `" + fieldName + "` is missing from source schema: " + source);        }    }    return targetStruct;}
f8849
0
checkMaybeCompatible
private static void kafkatest_f8850_0(Schema source, Schema target)
{    if (source.type() != target.type() && !isPromotable(source.type(), target.type())) {        throw new SchemaProjectorException("Schema type mismatch. source type: " + source.type() + " and target type: " + target.type());    } else if (!Objects.equals(source.name(), target.name())) {        throw new SchemaProjectorException("Schema name mismatch. source name: " + source.name() + " and target name: " + target.name());    } else if (!Objects.equals(source.parameters(), target.parameters())) {        throw new SchemaProjectorException("Schema parameters not equal. source parameters: " + source.parameters() + " and target parameters: " + target.parameters());    }}
f8850
0
getInt8
public Byte kafkatest_f8859_0(String fieldName)
{    return (Byte) getCheckType(fieldName, Schema.Type.INT8);}
f8859
0
getInt16
public Short kafkatest_f8860_0(String fieldName)
{    return (Short) getCheckType(fieldName, Schema.Type.INT16);}
f8860
0
getMap
public Map<K, V> kafkatest_f8869_0(String fieldName)
{    return (Map<K, V>) getCheckType(fieldName, Schema.Type.MAP);}
f8869
0
getStruct
public Struct kafkatest_f8870_0(String fieldName)
{    return (Struct) getCheckType(fieldName, Schema.Type.STRUCT);}
f8870
0
builder
public static SchemaBuilder kafkatest_f8879_0()
{    return SchemaBuilder.int32().name(LOGICAL_NAME).version(1);}
f8879
0
fromLogical
public static int kafkatest_f8880_0(Schema schema, java.util.Date value)
{    if (!(LOGICAL_NAME.equals(schema.name())))        throw new DataException("Requested conversion of Time object but the schema does not match.");    Calendar calendar = Calendar.getInstance(UTC);    calendar.setTime(value);    long unixMillis = calendar.getTimeInMillis();    if (unixMillis < 0 || unixMillis > MILLIS_PER_DAY) {        throw new DataException("Kafka Connect Time type should not have any date fields set to non-zero values.");    }    return (int) unixMillis;}
f8880
0
convertToLong
public static Long kafkatest_f8889_0(Schema schema, Object value) throws DataException
{    return (Long) convertTo(Schema.OPTIONAL_INT64_SCHEMA, schema, value);}
f8889
0
convertToFloat
public static Float kafkatest_f8890_0(Schema schema, Object value) throws DataException
{    return (Float) convertTo(Schema.OPTIONAL_FLOAT32_SCHEMA, schema, value);}
f8890
0
convertToDecimal
public static BigDecimal kafkatest_f8899_0(Schema schema, Object value, int scale)
{    return (BigDecimal) convertTo(Decimal.schema(scale), schema, value);}
f8899
0
inferSchema
public static Schema kafkatest_f8900_0(Object value)
{    if (value instanceof String) {        return Schema.STRING_SCHEMA;    }    if (value instanceof Boolean) {        return Schema.BOOLEAN_SCHEMA;    }    if (value instanceof Byte) {        return Schema.INT8_SCHEMA;    }    if (value instanceof Short) {        return Schema.INT16_SCHEMA;    }    if (value instanceof Integer) {        return Schema.INT32_SCHEMA;    }    if (value instanceof Long) {        return Schema.INT64_SCHEMA;    }    if (value instanceof Float) {        return Schema.FLOAT32_SCHEMA;    }    if (value instanceof Double) {        return Schema.FLOAT64_SCHEMA;    }    if (value instanceof byte[] || value instanceof ByteBuffer) {        return Schema.BYTES_SCHEMA;    }    if (value instanceof List) {        List<?> list = (List<?>) value;        if (list.isEmpty()) {            return null;        }        SchemaDetector detector = new SchemaDetector();        for (Object element : list) {            if (!detector.canDetect(element)) {                return null;            }        }        return SchemaBuilder.array(detector.schema()).build();    }    if (value instanceof Map) {        Map<?, ?> map = (Map<?, ?>) value;        if (map.isEmpty()) {            return null;        }        SchemaDetector keyDetector = new SchemaDetector();        SchemaDetector valueDetector = new SchemaDetector();        for (Map.Entry<?, ?> entry : map.entrySet()) {            if (!keyDetector.canDetect(entry.getKey()) || !valueDetector.canDetect(entry.getValue())) {                return null;            }        }        return SchemaBuilder.map(keyDetector.schema(), valueDetector.schema()).build();    }    if (value instanceof Struct) {        return ((Struct) value).schema();    }    return null;}
f8900
0
parse
protected static SchemaAndValuef8909_1Parser parser, boolean embedded) throws NoSuchElementException
{    if (!parser.hasNext()) {        return null;    }    if (embedded) {        if (parser.canConsume(NULL_VALUE)) {            return null;        }        if (parser.canConsume(QUOTE_DELIMITER)) {            StringBuilder sb = new StringBuilder();            while (parser.hasNext()) {                if (parser.canConsume(QUOTE_DELIMITER)) {                    break;                }                sb.append(parser.next());            }            return new SchemaAndValue(Schema.STRING_SCHEMA, sb.toString());        }    }    if (parser.canConsume(TRUE_LITERAL)) {        return TRUE_SCHEMA_AND_VALUE;    }    if (parser.canConsume(FALSE_LITERAL)) {        return FALSE_SCHEMA_AND_VALUE;    }    int startPosition = parser.mark();    try {        if (parser.canConsume(ARRAY_BEGIN_DELIMITER)) {            List<Object> result = new ArrayList<>();            Schema elementSchema = null;            while (parser.hasNext()) {                if (parser.canConsume(ARRAY_END_DELIMITER)) {                    Schema listSchema = null;                    if (elementSchema != null) {                        listSchema = SchemaBuilder.array(elementSchema).schema();                    }                    result = alignListEntriesWithSchema(listSchema, result);                    return new SchemaAndValue(listSchema, result);                }                if (parser.canConsume(COMMA_DELIMITER)) {                    throw new DataException("Unable to parse an empty array element: " + parser.original());                }                SchemaAndValue element = parse(parser, true);                elementSchema = commonSchemaFor(elementSchema, element);                result.add(element.value());                parser.canConsume(COMMA_DELIMITER);            }            // Missing either a comma or an end delimiter            if (COMMA_DELIMITER.equals(parser.previous())) {                throw new DataException("Array is missing element after ',': " + parser.original());            }            throw new DataException("Array is missing terminating ']': " + parser.original());        }        if (parser.canConsume(MAP_BEGIN_DELIMITER)) {            Map<Object, Object> result = new LinkedHashMap<>();            Schema keySchema = null;            Schema valueSchema = null;            while (parser.hasNext()) {                if (parser.canConsume(MAP_END_DELIMITER)) {                    Schema mapSchema = null;                    if (keySchema != null && valueSchema != null) {                        mapSchema = SchemaBuilder.map(keySchema, valueSchema).schema();                    }                    result = alignMapKeysAndValuesWithSchema(mapSchema, result);                    return new SchemaAndValue(mapSchema, result);                }                if (parser.canConsume(COMMA_DELIMITER)) {                    throw new DataException("Unable to parse a map entry has no key or value: " + parser.original());                }                SchemaAndValue key = parse(parser, true);                if (key == null || key.value() == null) {                    throw new DataException("Map entry may not have a null key: " + parser.original());                }                if (!parser.canConsume(ENTRY_DELIMITER)) {                    throw new DataException("Map entry is missing '=': " + parser.original());                }                SchemaAndValue value = parse(parser, true);                Object entryValue = value != null ? value.value() : null;                result.put(key.value(), entryValue);                parser.canConsume(COMMA_DELIMITER);                keySchema = commonSchemaFor(keySchema, key);                valueSchema = commonSchemaFor(valueSchema, value);            }            // Missing either a comma or an end delimiter            if (COMMA_DELIMITER.equals(parser.previous())) {                throw new DataException("Map is missing element after ',': " + parser.original());            }            throw new DataException("Map is missing terminating ']': " + parser.original());        }    } catch (DataException e) {         reverting to string", e);        parser.rewindTo(startPosition);    }    String token = parser.next().trim();    // original can be empty string but is handled right away; no way for token to be empty here    assert !token.isEmpty();    char firstChar = token.charAt(0);    boolean firstCharIsDigit = Character.isDigit(firstChar);    if (firstCharIsDigit || firstChar == '+' || firstChar == '-') {        try {            // Try to parse as a number ...            BigDecimal decimal = new BigDecimal(token);            try {                return new SchemaAndValue(Schema.INT8_SCHEMA, decimal.byteValueExact());            } catch (ArithmeticException e) {            // continue            }            try {                return new SchemaAndValue(Schema.INT16_SCHEMA, decimal.shortValueExact());            } catch (ArithmeticException e) {            // continue            }            try {                return new SchemaAndValue(Schema.INT32_SCHEMA, decimal.intValueExact());            } catch (ArithmeticException e) {            // continue            }            try {                return new SchemaAndValue(Schema.INT64_SCHEMA, decimal.longValueExact());            } catch (ArithmeticException e) {            // continue            }            double dValue = decimal.doubleValue();            if (dValue != Double.NEGATIVE_INFINITY && dValue != Double.POSITIVE_INFINITY) {                return new SchemaAndValue(Schema.FLOAT64_SCHEMA, dValue);            }            Schema schema = Decimal.schema(decimal.scale());            return new SchemaAndValue(schema, decimal);        } catch (NumberFormatException e) {        // can't parse as a number        }    }    if (firstCharIsDigit) {        // Check for a date, time, or timestamp ...        int tokenLength = token.length();        if (tokenLength == ISO_8601_DATE_LENGTH) {            try {                return new SchemaAndValue(Date.SCHEMA, new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN).parse(token));            } catch (ParseException e) {            // not a valid date            }        } else if (tokenLength == ISO_8601_TIME_LENGTH) {            try {                return new SchemaAndValue(Time.SCHEMA, new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN).parse(token));            } catch (ParseException e) {            // not a valid date            }        } else if (tokenLength == ISO_8601_TIMESTAMP_LENGTH) {            try {                return new SchemaAndValue(Timestamp.SCHEMA, new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN).parse(token));            } catch (ParseException e) {            // not a valid date            }        }    }    // so this is not embedded and we can use the original string...    return new SchemaAndValue(Schema.STRING_SCHEMA, parser.original());}
protected static SchemaAndValuef8909
1
commonSchemaFor
protected static Schema kafkatest_f8910_0(Schema previous, SchemaAndValue latest)
{    if (latest == null) {        return previous;    }    if (previous == null) {        return latest.schema();    }    Schema newSchema = latest.schema();    Type previousType = previous.type();    Type newType = newSchema.type();    if (previousType != newType) {        switch(previous.type()) {            case INT8:                if (newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {                    return newSchema;                }                break;            case INT16:                if (newType == Type.INT8) {                    return previous;                }                if (newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {                    return newSchema;                }                break;            case INT32:                if (newType == Type.INT8 || newType == Type.INT16) {                    return previous;                }                if (newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {                    return newSchema;                }                break;            case INT64:                if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32) {                    return previous;                }                if (newType == Type.FLOAT32 || newType == Type.FLOAT64) {                    return newSchema;                }                break;            case FLOAT32:                if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64) {                    return previous;                }                if (newType == Type.FLOAT64) {                    return newSchema;                }                break;            case FLOAT64:                if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32) {                    return previous;                }                break;        }        return null;    }    if (previous.isOptional() == newSchema.isOptional()) {        // Use the optional one        return previous.isOptional() ? previous : newSchema;    }    if (!previous.equals(newSchema)) {        return null;    }    return previous;}
f8910
0
hasNext
public boolean kafkatest_f8919_0()
{    return nextToken != null || canConsumeNextToken();}
f8919
0
canConsumeNextToken
protected boolean kafkatest_f8920_0()
{    return iter.getEndIndex() > iter.getIndex();}
f8920
0
schema
public Schema kafkatest_f8929_0()
{    Schema schema = schemaAndValue.schema();    if (schema == null && value() instanceof Struct) {        schema = ((Struct) value()).schema();    }    return schema;}
f8929
0
rename
public Header kafkatest_f8930_0(String key)
{    Objects.requireNonNull(key, "Null header keys are not permitted");    if (this.key.equals(key)) {        return this;    }    return new ConnectHeader(key, schemaAndValue);}
f8930
0
isEmpty
public boolean kafkatest_f8939_0()
{    return headers == null ? true : headers.isEmpty();}
f8939
0
clear
public Headers kafkatest_f8940_0()
{    if (headers != null) {        headers.clear();    }    return this;}
f8940
0
addShort
public Headers kafkatest_f8949_0(String key, short value)
{    return addWithoutValidating(key, value, Schema.INT16_SCHEMA);}
f8949
0
addInt
public Headers kafkatest_f8950_0(String key, int value)
{    return addWithoutValidating(key, value, Schema.INT32_SCHEMA);}
f8950
0
addTime
public Headers kafkatest_f8959_0(String key, java.util.Date value)
{    if (value != null) {        // Check that this is a time ...        Time.fromLogical(Time.SCHEMA, value);    }    return addWithoutValidating(key, value, Time.SCHEMA);}
f8959
0
addTimestamp
public Headers kafkatest_f8960_0(String key, java.util.Date value)
{    if (value != null) {        // Check that this is a timestamp ...        Timestamp.fromLogical(Timestamp.SCHEMA, value);    }    return addWithoutValidating(key, value, Timestamp.SCHEMA);}
f8960
0
hashCode
public int kafkatest_f8969_0()
{    return isEmpty() ? EMPTY_HASH : Objects.hash(headers);}
f8969
0
equals
public boolean kafkatest_f8970_0(Object obj)
{    if (obj == this) {        return true;    }    if (obj instanceof Headers) {        Headers that = (Headers) obj;        Iterator<Header> thisIter = this.iterator();        Iterator<Header> thatIter = that.iterator();        while (thisIter.hasNext() && thatIter.hasNext()) {            if (!Objects.equals(thisIter.next(), thatIter.next()))                return false;        }        return !thisIter.hasNext() && !thatIter.hasNext();    }    return false;}
f8970
0
traceMessage
public String kafkatest_f8979_0()
{    return traceMessage;}
f8979
0
connectorConfig
 Map<String, String> kafkatest_f8980_0(String connName)
{    throw new UnsupportedOperationException();}
f8980
0
hashCode
public int kafkatest_f8989_0()
{    return Objects.hash(taskId);}
f8989
0
kafkaOffset
public long kafkatest_f8990_0()
{    return kafkaOffset;}
f8990
0
open
public void kafkatest_f9000_0(Collection<TopicPartition> partitions)
{    this.onPartitionsAssigned(partitions);}
f9000
0
close
public void kafkatest_f9002_0(Collection<TopicPartition> partitions)
{    this.onPartitionsRevoked(partitions);}
f9002
0
commit
public void kafkatest_f9012_0() throws InterruptedException
{// This space intentionally left blank.}
f9012
0
commitRecord
public void kafkatest_f9013_0(SourceRecord record) throws InterruptedException
{// This space intentionally left blank.}
f9013
0
close
public void kafkatest_f9022_0() throws IOException
{// do nothing}
f9022
0
config
public ConfigDef kafkatest_f9023_0()
{    return StringConverterConfig.configDef();}
f9023
0
encoding
public String kafkatest_f9032_0()
{    return getString(ENCODING_CONFIG);}
f9032
0
groupPartitions
public static List<List<T>> kafkatest_f9033_0(List<T> elements, int numGroups)
{    if (numGroups <= 0)        throw new IllegalArgumentException("Number of groups must be positive.");    List<List<T>> result = new ArrayList<>(numGroups);    // Each group has either n+1 or n raw partitions    int perGroup = elements.size() / numGroups;    int leftover = elements.size() - (numGroups * perGroup);    int assigned = 0;    for (int group = 0; group < numGroups; group++) {        int numThisGroup = group < leftover ? perGroup + 1 : perGroup;        List<T> groupList = new ArrayList<>(numThisGroup);        for (int i = 0; i < numThisGroup; i++) {            groupList.add(elements.get(assigned));            assigned++;        }        result.add(groupList);    }    return result;}
f9033
0
testFieldsOnStructSchema
public void kafkatest_f9042_0()
{    Schema schema = SchemaBuilder.struct().field("foo", Schema.BOOLEAN_SCHEMA).field("bar", Schema.INT32_SCHEMA).build();    assertEquals(2, schema.fields().size());    // Validate field lookup by name    Field foo = schema.field("foo");    assertEquals(0, foo.index());    Field bar = schema.field("bar");    assertEquals(1, bar.index());    // Any other field name should fail    assertNull(schema.field("other"));}
f9042
0
testFieldsOnlyValidForStructs
public void kafkatest_f9043_0()
{    Schema.INT8_SCHEMA.fields();}
f9043
0
testValidateValueMismatchBoolean
public void kafkatest_f9052_0()
{    ConnectSchema.validateValue(Schema.BOOLEAN_SCHEMA, 1.f);}
f9052
0
testValidateValueMismatchString
public void kafkatest_f9053_0()
{    // CharSequence is a similar type (supertype of String), but we restrict to String.    CharBuffer cbuf = CharBuffer.wrap("abc");    ConnectSchema.validateValue(Schema.STRING_SCHEMA, cbuf);}
f9053
0
testValidateValueMismatchStructWrongNestedSchema
public void kafkatest_f9062_0()
{    // Top-level schema  matches, but nested does not.    ConnectSchema.validateValue(PARENT_STRUCT_SCHEMA, new Struct(PARENT_STRUCT_SCHEMA).put("nested", new Struct(SchemaBuilder.struct().field("x", Schema.INT32_SCHEMA).build()).put("x", 1)));}
f9062
0
testValidateValueMismatchDecimal
public void kafkatest_f9063_0()
{    ConnectSchema.validateValue(Decimal.schema(2), new BigInteger("156"));}
f9063
0
testEmptyStruct
public void kafkatest_f9072_0()
{    final ConnectSchema emptyStruct = new ConnectSchema(Schema.Type.STRUCT, false, null, null, null, null);    assertEquals(0, emptyStruct.fields().size());    new Struct(emptyStruct);}
f9072
0
testBuilder
public void kafkatest_f9073_0()
{    Schema plain = Date.SCHEMA;    assertEquals(Date.LOGICAL_NAME, plain.name());    assertEquals(1, (Object) plain.version());}
f9073
0
type
public Type kafkatest_f9082_0()
{    return null;}
f9082
0
isOptional
public boolean kafkatest_f9083_0()
{    return false;}
f9083
0
field
public Field kafkatest_f9092_0(String fieldName)
{    return null;}
f9092
0
schema
public Schema kafkatest_f9093_0()
{    return null;}
f9093
0
testInt64BuilderInvalidDefault
public void kafkatest_f9102_0()
{    SchemaBuilder.int64().defaultValue("invalid");}
f9102
0
testFloatBuilder
public void kafkatest_f9103_0()
{    Schema schema = SchemaBuilder.float32().build();    assertTypeAndDefault(schema, Schema.Type.FLOAT32, false, null);    assertNoMetadata(schema);    schema = SchemaBuilder.float32().name(NAME).optional().defaultValue(12.f).version(VERSION).doc(DOC).build();    assertTypeAndDefault(schema, Schema.Type.FLOAT32, true, 12.f);    assertMetadata(schema, NAME, VERSION, DOC, NO_PARAMS);}
f9103
0
testBytesBuilderInvalidDefault
public void kafkatest_f9112_0()
{    SchemaBuilder.bytes().defaultValue("a string, not bytes");}
f9112
0
testParameters
public void kafkatest_f9113_0()
{    Map<String, String> expectedParameters = new HashMap<>();    expectedParameters.put("foo", "val");    expectedParameters.put("bar", "baz");    Schema schema = SchemaBuilder.string().parameter("foo", "val").parameter("bar", "baz").build();    assertTypeAndDefault(schema, Schema.Type.STRING, false, null);    assertMetadata(schema, null, null, null, expectedParameters);    schema = SchemaBuilder.string().parameters(expectedParameters).build();    assertTypeAndDefault(schema, Schema.Type.STRING, false, null);    assertMetadata(schema, null, null, null, expectedParameters);}
f9113
0
testDefaultFieldsSameValueOverwriting
public void kafkatest_f9122_0()
{    final SchemaBuilder schemaBuilder = SchemaBuilder.string().name("testing").version(123);    schemaBuilder.name("testing");    schemaBuilder.version(123);    assertEquals("testing", schemaBuilder.name());}
f9122
0
testDefaultFieldsDifferentValueOverwriting
public void kafkatest_f9123_0()
{    final SchemaBuilder schemaBuilder = SchemaBuilder.string().name("testing").version(123);    schemaBuilder.name("testing");    schemaBuilder.version(456);}
f9123
0
assertNoMetadata
private void kafkatest_f9132_0(Schema schema)
{    assertMetadata(schema, null, null, null, null);}
f9132
0
testPrimitiveTypeProjection
public void kafkatest_f9133_0()
{    Object projected;    projected = SchemaProjector.project(Schema.BOOLEAN_SCHEMA, false, Schema.BOOLEAN_SCHEMA);    assertEquals(false, projected);    byte[] bytes = { (byte) 1, (byte) 2 };    projected = SchemaProjector.project(Schema.BYTES_SCHEMA, bytes, Schema.BYTES_SCHEMA);    assertEquals(bytes, projected);    projected = SchemaProjector.project(Schema.STRING_SCHEMA, "abc", Schema.STRING_SCHEMA);    assertEquals("abc", projected);    projected = SchemaProjector.project(Schema.BOOLEAN_SCHEMA, false, Schema.OPTIONAL_BOOLEAN_SCHEMA);    assertEquals(false, projected);    projected = SchemaProjector.project(Schema.BYTES_SCHEMA, bytes, Schema.OPTIONAL_BYTES_SCHEMA);    assertEquals(bytes, projected);    projected = SchemaProjector.project(Schema.STRING_SCHEMA, "abc", Schema.OPTIONAL_STRING_SCHEMA);    assertEquals("abc", projected);    try {        SchemaProjector.project(Schema.OPTIONAL_BOOLEAN_SCHEMA, false, Schema.BOOLEAN_SCHEMA);        fail("Cannot project optional schema to schema with no default value.");    } catch (DataException e) {    // expected    }    try {        SchemaProjector.project(Schema.OPTIONAL_BYTES_SCHEMA, bytes, Schema.BYTES_SCHEMA);        fail("Cannot project optional schema to schema with no default value.");    } catch (DataException e) {    // expected    }    try {        SchemaProjector.project(Schema.OPTIONAL_STRING_SCHEMA, "abc", Schema.STRING_SCHEMA);        fail("Cannot project optional schema to schema with no default value.");    } catch (DataException e) {    // expected    }}
f9133
0
testMapProjection
public void kafkatest_f9142_0()
{    Schema source = SchemaBuilder.map(Schema.INT32_SCHEMA, Schema.INT32_SCHEMA).optional().build();    Schema target = SchemaBuilder.map(Schema.INT32_SCHEMA, Schema.INT32_SCHEMA).defaultValue(Collections.singletonMap(1, 2)).build();    Object projected = SchemaProjector.project(source, Collections.singletonMap(3, 4), target);    assertEquals(Collections.singletonMap(3, 4), projected);    projected = SchemaProjector.project(source, null, target);    assertEquals(Collections.singletonMap(1, 2), projected);    Schema promotedTarget = SchemaBuilder.map(Schema.INT64_SCHEMA, Schema.FLOAT32_SCHEMA).defaultValue(Collections.singletonMap(3L, 4.5F)).build();    projected = SchemaProjector.project(source, Collections.singletonMap(3, 4), promotedTarget);    assertEquals(Collections.singletonMap(3L, 4.F), projected);    projected = SchemaProjector.project(source, null, promotedTarget);    assertEquals(Collections.singletonMap(3L, 4.5F), projected);    Schema noDefaultValueTarget = SchemaBuilder.map(Schema.INT32_SCHEMA, Schema.INT32_SCHEMA).build();    try {        SchemaProjector.project(source, null, noDefaultValueTarget);        fail("Reader does not provide a default value.");    } catch (SchemaProjectorException e) {    // expected    }    Schema nonPromotableTarget = SchemaBuilder.map(Schema.BOOLEAN_SCHEMA, Schema.STRING_SCHEMA).build();    try {        SchemaProjector.project(source, null, nonPromotableTarget);        fail("Neither source type matches target type nor source type can be promoted to target type");    } catch (SchemaProjectorException e) {    // expected    }}
f9142
0
testMaybeCompatible
public void kafkatest_f9143_0()
{    Schema source = SchemaBuilder.int32().name("source").build();    Schema target = SchemaBuilder.int32().name("target").build();    try {        SchemaProjector.project(source, 12, target);        fail("Source name and target name mismatch.");    } catch (SchemaProjectorException e) {    // expected    }    Schema targetWithParameters = SchemaBuilder.int32().parameters(Collections.singletonMap("key", "value"));    try {        SchemaProjector.project(source, 34, targetWithParameters);        fail("Source parameters and target parameters mismatch.");    } catch (SchemaProjectorException e) {    // expected    }}
f9143
0
testInvalidMapKeyElements
public void kafkatest_f9152_0()
{    new Struct(NESTED_SCHEMA).put("map", Collections.singletonMap("should fail because keys should be int8s", (byte) 12));}
f9152
0
testInvalidStructFieldSchema
public void kafkatest_f9153_0()
{    new Struct(NESTED_SCHEMA).put("nested", new Struct(MAP_SCHEMA));}
f9153
0
testValidateStructWithNullValue
public void kafkatest_f9162_0()
{    Schema schema = SchemaBuilder.struct().field("one", Schema.STRING_SCHEMA).field("two", Schema.STRING_SCHEMA).field("three", Schema.STRING_SCHEMA).build();    Struct struct = new Struct(schema);    Exception e = assertThrows(DataException.class, struct::validate);    assertEquals("Invalid value: null used for required field: \"one\", schema type: STRING", e.getMessage());}
f9162
0
testValidateFieldWithInvalidValueType
public void kafkatest_f9163_0()
{    String fieldName = "field";    FakeSchema fakeSchema = new FakeSchema();    Exception e = assertThrows(DataException.class, () -> ConnectSchema.validateValue(fieldName, fakeSchema, new Object()));    assertEquals("Invalid Java object for schema type null: class java.lang.Object for field: \"field\"", e.getMessage());    e = assertThrows(DataException.class, () -> ConnectSchema.validateValue(fieldName, Schema.INT8_SCHEMA, new Object()));    assertEquals("Invalid Java object for schema type INT8: class java.lang.Object for field: \"field\"", e.getMessage());}
f9163
0
testFromLogical
public void kafkatest_f9172_0()
{    assertEquals(0, Time.fromLogical(Time.SCHEMA, EPOCH.getTime()));    assertEquals(10000, Time.fromLogical(Time.SCHEMA, EPOCH_PLUS_TEN_THOUSAND_MILLIS.getTime()));}
f9172
0
testFromLogicalInvalidSchema
public void kafkatest_f9173_0()
{    Time.fromLogical(Time.builder().name("invalid").build(), EPOCH.getTime());}
f9173
0
shouldConvertEmptyString
public void kafkatest_f9182_0()
{    assertRoundTrip(Schema.STRING_SCHEMA, "");}
f9182
0
shouldConvertStringWithQuotesAndOtherDelimiterCharacters
public void kafkatest_f9183_0()
{    assertRoundTrip(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA, "three\"blind\\\"mice");    assertRoundTrip(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA, "string with delimiters: <>?,./\\=+-!@#$%^&*(){}[]|;':");}
f9183
0
shouldParseStringOfMapWithIntValuesWithWhitespaceAsMap
public void kafkatest_f9192_0()
{    SchemaAndValue result = roundTrip(STRING_INT_MAP_SCHEMA, " { \"foo\" :  1234567890 , \"bar\" : 0,  \"baz\" : -987654321 }  ");    assertEquals(STRING_INT_MAP_SCHEMA, result.schema());    assertEquals(STRING_INT_MAP, result.value());}
f9192
0
shouldConvertListWithStringValues
public void kafkatest_f9193_0()
{    assertRoundTrip(STRING_LIST_SCHEMA, STRING_LIST_SCHEMA, STRING_LIST);}
f9193
0
shouldFailToParseStringOfMalformedMap
public void kafkatest_f9202_0()
{    Values.convertToList(Schema.STRING_SCHEMA, " { \"foo\" :  1234567890 , \"a\", \"bar\" : 0,  \"baz\" : -987654321 }  ");}
f9202
0
shouldFailToParseStringOfMapWithIntValuesWithOnlyBlankEntries
public void kafkatest_f9203_0()
{    Values.convertToList(Schema.STRING_SCHEMA, " { ,,  , , }  ");}
f9203
0
shouldConvertTimestampValues
public void kafkatest_f9212_0()
{    java.util.Date current = new java.util.Date();    long currentMillis = current.getTime() % MILLIS_PER_DAY;    // java.util.Date - just copy    java.util.Date ts1 = Values.convertToTimestamp(Timestamp.SCHEMA, current);    assertEquals(current, ts1);    // java.util.Date as a Timestamp - discard the day's milliseconds and keep the date    java.util.Date currentDate = new java.util.Date(current.getTime() - currentMillis);    ts1 = Values.convertToTimestamp(Date.SCHEMA, currentDate);    assertEquals(currentDate, ts1);    // java.util.Date as a Time - discard the date and keep the day's milliseconds    ts1 = Values.convertToTimestamp(Time.SCHEMA, currentMillis);    assertEquals(new java.util.Date(currentMillis), ts1);    // ISO8601 strings - currently broken because tokenization breaks at colon    // Millis as string    java.util.Date ts3 = Values.convertToTimestamp(Timestamp.SCHEMA, Long.toString(current.getTime()));    assertEquals(current, ts3);    // Millis as long    java.util.Date ts4 = Values.convertToTimestamp(Timestamp.SCHEMA, current.getTime());    assertEquals(current, ts4);}
f9212
0
assertParsed
protected void kafkatest_f9214_0(String input)
{    assertParsed(input, input);}
f9214
0
populate
protected void kafkatest_f9223_0(Headers headers)
{    headers.addBoolean(key, true);    headers.addInt(key, 0);    headers.addString(other, "other value");    headers.addString(key, null);    headers.addString(key, "third");}
f9223
0
shouldBeEquals
public void kafkatest_f9224_0()
{    Headers other = new ConnectHeaders();    assertEquals(headers, other);    assertEquals(headers.hashCode(), other.hashCode());    populate(headers);    assertNotEquals(headers, other);    assertNotEquals(headers.hashCode(), other.hashCode());    populate(other);    assertEquals(headers, other);    assertEquals(headers.hashCode(), other.hashCode());    headers.addString("wow", "some value");    assertNotEquals(headers, other);}
f9224
0
shouldRemoveAllHeadersWithSameKey
public void kafkatest_f9233_0()
{    populate(headers);    iter = headers.allWithName(key);    assertContainsHeader(key, Schema.BOOLEAN_SCHEMA, true);    assertContainsHeader(key, Schema.INT32_SCHEMA, 0);    assertContainsHeader(key, Schema.STRING_SCHEMA, "third");    assertOnlySingleHeader(other, Schema.STRING_SCHEMA, "other value");    headers.remove(key);    assertNoHeaderWithKey(key);    assertOnlySingleHeader(other, Schema.STRING_SCHEMA, "other value");}
f9233
0
shouldRemoveAllHeaders
public void kafkatest_f9234_0()
{    populate(headers);    iter = headers.allWithName(key);    assertContainsHeader(key, Schema.BOOLEAN_SCHEMA, true);    assertContainsHeader(key, Schema.INT32_SCHEMA, 0);    assertContainsHeader(key, Schema.STRING_SCHEMA, "third");    assertOnlySingleHeader(other, Schema.STRING_SCHEMA, "other value");    headers.clear();    assertNoHeaderWithKey(key);    assertNoHeaderWithKey(other);    assertEquals(0, headers.size());    assertTrue(headers.isEmpty());}
f9234
0
shouldValidateBuildInTypes
public void kafkatest_f9243_0()
{    assertSchemaMatches(Schema.OPTIONAL_BOOLEAN_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_BYTES_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_INT8_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_INT16_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_INT32_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_INT64_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_FLOAT32_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_FLOAT64_SCHEMA, null);    assertSchemaMatches(Schema.OPTIONAL_STRING_SCHEMA, null);    assertSchemaMatches(Schema.BOOLEAN_SCHEMA, true);    assertSchemaMatches(Schema.BYTES_SCHEMA, new byte[] {});    assertSchemaMatches(Schema.INT8_SCHEMA, (byte) 0);    assertSchemaMatches(Schema.INT16_SCHEMA, (short) 0);    assertSchemaMatches(Schema.INT32_SCHEMA, 0);    assertSchemaMatches(Schema.INT64_SCHEMA, 0L);    assertSchemaMatches(Schema.FLOAT32_SCHEMA, 1.0f);    assertSchemaMatches(Schema.FLOAT64_SCHEMA, 1.0d);    assertSchemaMatches(Schema.STRING_SCHEMA, "value");    assertSchemaMatches(SchemaBuilder.array(Schema.STRING_SCHEMA), new ArrayList<String>());    assertSchemaMatches(SchemaBuilder.array(Schema.STRING_SCHEMA), Collections.singletonList("value"));    assertSchemaMatches(SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.INT32_SCHEMA), new HashMap<String, Integer>());    assertSchemaMatches(SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.INT32_SCHEMA), Collections.singletonMap("a", 0));    Schema emptyStructSchema = SchemaBuilder.struct();    assertSchemaMatches(emptyStructSchema, new Struct(emptyStructSchema));    Schema structSchema = SchemaBuilder.struct().field("foo", Schema.OPTIONAL_BOOLEAN_SCHEMA).field("bar", Schema.STRING_SCHEMA).schema();    assertSchemaMatches(structSchema, new Struct(structSchema).put("foo", true).put("bar", "v"));}
f9243
0
shouldValidateLogicalTypes
public void kafkatest_f9244_0()
{    assertSchemaMatches(Decimal.schema(3), new BigDecimal(100.00));    assertSchemaMatches(Time.SCHEMA, new java.util.Date());    assertSchemaMatches(Date.SCHEMA, new java.util.Date());    assertSchemaMatches(Timestamp.SCHEMA, new java.util.Date());}
f9244
0
assertSchemaDoesNotMatch
protected void kafkatest_f9253_0(Schema schema, Object value)
{    try {        assertSchemaMatches(schema, value);        fail("Should have failed to validate value '" + value + "' and schema: " + schema);    } catch (DataException e) {    // expected    }}
f9253
0
attemptAndFailToAddHeader
protected void kafkatest_f9254_0(String key, Schema schema, Object value)
{    try {        headers.add(key, value, schema);        fail("Should have failed to add header with key '" + key + "', value '" + value + "', and schema: " + schema);    } catch (DataException e) {    // expected    }}
f9254
0
beforeEach
public void kafkatest_f9263_0()
{    key = "key";    withString("value");}
f9263
0
withValue
protected Header kafkatest_f9264_0(Schema schema, Object value)
{    header = new ConnectHeader(key, new SchemaAndValue(schema, value));    return header;}
f9264
0
shouldCreateSinkRecordWithEmptyHeaders
public void kafkatest_f9273_0()
{    assertEquals(TOPIC_NAME, record.topic());    assertEquals(PARTITION_NUMBER, record.kafkaPartition());    assertEquals(Schema.STRING_SCHEMA, record.keySchema());    assertEquals("key", record.key());    assertEquals(Schema.BOOLEAN_SCHEMA, record.valueSchema());    assertEquals(false, record.value());    assertEquals(KAFKA_OFFSET, record.kafkaOffset());    assertEquals(KAFKA_TIMESTAMP, record.timestamp());    assertEquals(TS_TYPE, record.timestampType());    assertNotNull(record.headers());    assertTrue(record.headers().isEmpty());}
f9273
0
shouldDuplicateRecordAndCloneHeaders
public void kafkatest_f9274_0()
{    SinkRecord duplicate = record.newRecord(TOPIC_NAME, PARTITION_NUMBER, Schema.STRING_SCHEMA, "key", Schema.BOOLEAN_SCHEMA, false, KAFKA_TIMESTAMP);    assertEquals(TOPIC_NAME, duplicate.topic());    assertEquals(PARTITION_NUMBER, duplicate.kafkaPartition());    assertEquals(Schema.STRING_SCHEMA, duplicate.keySchema());    assertEquals("key", duplicate.key());    assertEquals(Schema.BOOLEAN_SCHEMA, duplicate.valueSchema());    assertEquals(false, duplicate.value());    assertEquals(KAFKA_OFFSET, duplicate.kafkaOffset());    assertEquals(KAFKA_TIMESTAMP, duplicate.timestamp());    assertEquals(TS_TYPE, duplicate.timestampType());    assertNotNull(duplicate.headers());    assertTrue(duplicate.headers().isEmpty());    assertNotSame(record.headers(), duplicate.headers());    assertEquals(record.headers(), duplicate.headers());}
f9274
0
shouldFindByName
public void kafkatest_f9283_0()
{    for (ConverterType type : ConverterType.values()) {        assertEquals(type, ConverterType.withName(type.getName()));    }}
f9283
0
beforeEach
public void kafkatest_f9284_0()
{    converter = new SimpleHeaderConverter();}
f9284
0
shouldParseStringOfMapWithShortValuesWithoutWhitespaceAsMap
public void kafkatest_f9293_0()
{    SchemaAndValue result = roundTrip(Schema.STRING_SCHEMA, "{\"foo\":12345,\"bar\":0,\"baz\":-4321}");    assertEquals(STRING_SHORT_MAP_SCHEMA, result.schema());    assertEquals(STRING_SHORT_MAP, result.value());}
f9293
0
shouldParseStringOfMapWithShortValuesWithWhitespaceAsMap
public void kafkatest_f9294_0()
{    SchemaAndValue result = roundTrip(Schema.STRING_SCHEMA, " { \"foo\" :  12345 , \"bar\" : 0,  \"baz\" : -4321 }  ");    assertEquals(STRING_SHORT_MAP_SCHEMA, result.schema());    assertEquals(STRING_SHORT_MAP, result.value());}
f9294
0
shouldConvertEmptyListToListWithoutSchema
public void kafkatest_f9303_0()
{    assertRoundTrip(null, new ArrayList<>());}
f9303
0
roundTrip
protected SchemaAndValue kafkatest_f9304_0(Schema schema, Object input)
{    byte[] serialized = converter.fromConnectHeader(TOPIC, HEADER, schema, input);    return converter.toConnectHeader(TOPIC, HEADER, serialized);}
f9304
0
testBytesToStringNonUtf8Encoding
public void kafkatest_f9313_0() throws UnsupportedEncodingException
{    converter.configure(Collections.singletonMap("converter.encoding", "UTF-16"), true);    SchemaAndValue data = converter.toConnectData(TOPIC, SAMPLE_STRING.getBytes("UTF-16"));    assertEquals(Schema.OPTIONAL_STRING_SCHEMA, data.schema());    assertEquals(SAMPLE_STRING, data.value());}
f9313
0
testStringHeaderValueToBytes
public void kafkatest_f9314_0() throws UnsupportedEncodingException
{    assertArrayEquals(SAMPLE_STRING.getBytes("UTF8"), converter.fromConnectHeader(TOPIC, "hdr", Schema.STRING_SCHEMA, SAMPLE_STRING));}
f9314
0
initialize
public voidf9325_1Subject subject, CallbackHandler callbackHandler, Map<String, ?> sharedState, Map<String, ?> options)
{    this.callbackHandler = callbackHandler;    fileName = (String) options.get(FILE_OPTIONS);    if (fileName == null || fileName.trim().isEmpty()) {        throw new ConfigException("Property Credentials file must be specified");    }    if (!credentialPropertiesMap.containsKey(fileName)) {        Properties credentialProperties = new Properties();        try {            try (InputStream inputStream = Files.newInputStream(Paths.get(fileName))) {                credentialProperties.load(inputStream);            }            credentialPropertiesMap.putIfAbsent(fileName, credentialProperties);        } catch (IOException e) {                        throw new ConfigException("Error loading Property Credentials file");        }    }}
public voidf9325
1
login
public boolean kafkatest_f9326_0() throws LoginException
{    Callback[] callbacks = configureCallbacks();    try {        callbackHandler.handle(callbacks);    } catch (Exception e) {        throw new LoginException(e.getMessage());    }    String username = ((NameCallback) callbacks[0]).getName();    char[] passwordChars = ((PasswordCallback) callbacks[1]).getPassword();    String password = passwordChars != null ? new String(passwordChars) : null;    Properties credentialProperties = credentialPropertiesMap.get(fileName);    authenticated = credentialProperties.isEmpty() || (password != null && password.equals(credentialProperties.get(username)));    return authenticated;}
f9326
0
testBadPassword
public void kafkatest_f9335_0() throws IOException
{    setMock("Basic", "user", "password1", true);    jaasBasicAuthFilter.filter(requestContext);}
f9335
0
testUnknownBearer
public void kafkatest_f9336_0() throws IOException
{    setMock("Unknown", "user", "password", true);    jaasBasicAuthFilter.filter(requestContext);}
f9336
0
version
public String kafkatest_f9345_0()
{    return AppInfoParser.getVersion();}
f9345
0
start
public void kafkatest_f9346_0(Map<String, String> props)
{    AbstractConfig parsedConfig = new AbstractConfig(CONFIG_DEF, props);    filename = parsedConfig.getString(FILE_CONFIG);}
f9346
0
stop
public void kafkatest_f9355_0()
{    if (outputStream != null && outputStream != System.out)        outputStream.close();}
f9355
0
logFilename
private String kafkatest_f9356_0()
{    return filename == null ? "stdout" : filename;}
f9356
0
poll
public List<SourceRecord>f9365_1) throws InterruptedException
{    if (stream == null) {        try {            stream = Files.newInputStream(Paths.get(filename));            Map<String, Object> offset = context.offsetStorageReader().offset(Collections.singletonMap(FILENAME_FIELD, filename));            if (offset != null) {                Object lastRecordedOffset = offset.get(POSITION_FIELD);                if (lastRecordedOffset != null && !(lastRecordedOffset instanceof Long))                    throw new ConnectException("Offset position is the incorrect type");                if (lastRecordedOffset != null) {                                        long skipLeft = (Long) lastRecordedOffset;                    while (skipLeft > 0) {                        try {                            long skipped = stream.skip(skipLeft);                            skipLeft -= skipped;                        } catch (IOException e) {                                                        throw new ConnectException(e);                        }                    }                                    }                streamOffset = (lastRecordedOffset != null) ? (Long) lastRecordedOffset : 0L;            } else {                streamOffset = 0L;            }            reader = new BufferedReader(new InputStreamReader(stream, StandardCharsets.UTF_8));                    } catch (NoSuchFileException e) {                        synchronized (this) {                this.wait(1000);            }            return null;        } catch (IOException e) {                        throw new ConnectException(e);        }    }    // is available.    try {        final BufferedReader readerCopy;        synchronized (this) {            readerCopy = reader;        }        if (readerCopy == null)            return null;        ArrayList<SourceRecord> records = null;        int nread = 0;        while (readerCopy.ready()) {            nread = readerCopy.read(buffer, offset, buffer.length - offset);            log.trace("Read {} bytes from {}", nread, logFilename());            if (nread > 0) {                offset += nread;                if (offset == buffer.length) {                    char[] newbuf = new char[buffer.length * 2];                    System.arraycopy(buffer, 0, newbuf, 0, buffer.length);                    buffer = newbuf;                }                String line;                do {                    line = extractLine();                    if (line != null) {                        log.trace("Read a line from {}", logFilename());                        if (records == null)                            records = new ArrayList<>();                        records.add(new SourceRecord(offsetKey(filename), offsetValue(streamOffset), topic, null, null, null, VALUE_SCHEMA, line, System.currentTimeMillis()));                        if (records.size() >= batchSize) {                            return records;                        }                    }                } while (line != null);            }        }        if (nread <= 0)            synchronized (this) {                this.wait(1000);            }        return records;    } catch (IOException e) {    // Underlying stream was killed, probably as a result of calling stop. Allow to return    // null, and driving thread will handle any shutdown if necessary.    }    return null;}
public List<SourceRecord>f9365
1
extractLine
private String kafkatest_f9366_0()
{    int until = -1, newStart = -1;    for (int i = 0; i < offset; i++) {        if (buffer[i] == '\n') {            until = i;            newStart = i + 1;            break;        } else if (buffer[i] == '\r') {            // We need to check for \r\n, so we must skip this if we can't check the next char            if (i + 1 >= offset)                return null;            until = i;            newStart = (buffer[i + 1] == '\n') ? i + 2 : i + 1;            break;        }    }    if (until != -1) {        String result = new String(buffer, 0, until);        System.arraycopy(buffer, newStart, buffer, 0, buffer.length - newStart);        offset = offset - newStart;        if (streamOffset != null)            streamOffset += newStart;        return result;    } else {        return null;    }}
f9366
0
testTaskClass
public void kafkatest_f9375_0()
{    replayAll();    connector.start(sinkProperties);    assertEquals(FileStreamSinkTask.class, connector.taskClass());    verifyAll();}
f9375
0
setup
public void kafkatest_f9376_0() throws Exception
{    os = new ByteArrayOutputStream();    printStream = new PrintStream(os);    task = new FileStreamSinkTask(printStream);    File outputDir = topDir.newFolder("file-stream-sink-" + UUID.randomUUID().toString());    outputFile = outputDir.getCanonicalPath() + "/connect.output";}
f9376
0
testMissingTopic
public void kafkatest_f9385_0()
{    sourceProperties.remove(FileStreamSourceConnector.TOPIC_CONFIG);    connector.start(sourceProperties);}
f9385
0
testBlankTopic
public void kafkatest_f9386_0()
{    // Because of trimming this tests is same as testing for empty string.    sourceProperties.put(FileStreamSourceConnector.TOPIC_CONFIG, "     ");    connector.start(sourceProperties);}
f9386
0
expectOffsetLookupReturnNone
private void kafkatest_f9395_0()
{    EasyMock.expect(context.offsetStorageReader()).andReturn(offsetStorageReader);    EasyMock.expect(offsetStorageReader.offset(EasyMock.<Map<String, String>>anyObject())).andReturn(null);}
f9395
0
convert
public Object kafkatest_f9396_0(Schema schema, JsonNode value)
{    return value.booleanValue();}
f9396
0
convert
public Object kafkatest_f9405_0(Schema schema, JsonNode value)
{    Schema elemSchema = schema == null ? null : schema.valueSchema();    ArrayList<Object> result = new ArrayList<>();    for (JsonNode elem : value) {        result.add(convertToConnect(elemSchema, elem));    }    return result;}
f9405
0
convert
public Object kafkatest_f9406_0(Schema schema, JsonNode value)
{    Schema keySchema = schema == null ? null : schema.keySchema();    Schema valueSchema = schema == null ? null : schema.valueSchema();    // If the map uses strings for keys, it should be encoded in the natural JSON format. If it uses other    // primitive types or a complex type as a key, it will be encoded as a list of pairs. If we don't have a    // schema, we default to encoding in a Map.    Map<Object, Object> result = new HashMap<>();    if (schema == null || keySchema.type() == Schema.Type.STRING) {        if (!value.isObject())            throw new DataException("Maps with string fields should be encoded as JSON objects, but found " + value.getNodeType());        Iterator<Map.Entry<String, JsonNode>> fieldIt = value.fields();        while (fieldIt.hasNext()) {            Map.Entry<String, JsonNode> entry = fieldIt.next();            result.put(entry.getKey(), convertToConnect(valueSchema, entry.getValue()));        }    } else {        if (!value.isArray())            throw new DataException("Maps with non-string fields should be encoded as JSON array of tuples, but found " + value.getNodeType());        for (JsonNode entry : value) {            if (!entry.isArray())                throw new DataException("Found invalid map entry instead of array tuple: " + entry.getNodeType());            if (entry.size() != 2)                throw new DataException("Found invalid map entry, expected length 2 but found :" + entry.size());            result.put(convertToConnect(keySchema, entry.get(0)), convertToConnect(valueSchema, entry.get(1)));        }    }    return result;}
f9406
0
convert
public Object kafkatest_f9415_0(Schema schema, Object value)
{    if (!(value instanceof java.util.Date))        throw new DataException("Invalid type for Timestamp, expected Date but was " + value.getClass());    return Timestamp.fromLogical(schema, (java.util.Date) value);}
f9415
0
config
public ConfigDef kafkatest_f9416_0()
{    return JsonConverterConfig.configDef();}
f9416
0
asConnectSchema
public Schema kafkatest_f9425_0(JsonNode jsonSchema)
{    if (jsonSchema.isNull())        return null;    Schema cached = toConnectSchemaCache.get(jsonSchema);    if (cached != null)        return cached;    JsonNode schemaTypeNode = jsonSchema.get(JsonSchema.SCHEMA_TYPE_FIELD_NAME);    if (schemaTypeNode == null || !schemaTypeNode.isTextual())        throw new DataException("Schema must contain 'type' field");    final SchemaBuilder builder;    switch(schemaTypeNode.textValue()) {        case JsonSchema.BOOLEAN_TYPE_NAME:            builder = SchemaBuilder.bool();            break;        case JsonSchema.INT8_TYPE_NAME:            builder = SchemaBuilder.int8();            break;        case JsonSchema.INT16_TYPE_NAME:            builder = SchemaBuilder.int16();            break;        case JsonSchema.INT32_TYPE_NAME:            builder = SchemaBuilder.int32();            break;        case JsonSchema.INT64_TYPE_NAME:            builder = SchemaBuilder.int64();            break;        case JsonSchema.FLOAT_TYPE_NAME:            builder = SchemaBuilder.float32();            break;        case JsonSchema.DOUBLE_TYPE_NAME:            builder = SchemaBuilder.float64();            break;        case JsonSchema.BYTES_TYPE_NAME:            builder = SchemaBuilder.bytes();            break;        case JsonSchema.STRING_TYPE_NAME:            builder = SchemaBuilder.string();            break;        case JsonSchema.ARRAY_TYPE_NAME:            JsonNode elemSchema = jsonSchema.get(JsonSchema.ARRAY_ITEMS_FIELD_NAME);            if (elemSchema == null || elemSchema.isNull())                throw new DataException("Array schema did not specify the element type");            builder = SchemaBuilder.array(asConnectSchema(elemSchema));            break;        case JsonSchema.MAP_TYPE_NAME:            JsonNode keySchema = jsonSchema.get(JsonSchema.MAP_KEY_FIELD_NAME);            if (keySchema == null)                throw new DataException("Map schema did not specify the key type");            JsonNode valueSchema = jsonSchema.get(JsonSchema.MAP_VALUE_FIELD_NAME);            if (valueSchema == null)                throw new DataException("Map schema did not specify the value type");            builder = SchemaBuilder.map(asConnectSchema(keySchema), asConnectSchema(valueSchema));            break;        case JsonSchema.STRUCT_TYPE_NAME:            builder = SchemaBuilder.struct();            JsonNode fields = jsonSchema.get(JsonSchema.STRUCT_FIELDS_FIELD_NAME);            if (fields == null || !fields.isArray())                throw new DataException("Struct schema's \"fields\" argument is not an array.");            for (JsonNode field : fields) {                JsonNode jsonFieldName = field.get(JsonSchema.STRUCT_FIELD_NAME_FIELD_NAME);                if (jsonFieldName == null || !jsonFieldName.isTextual())                    throw new DataException("Struct schema's field name not specified properly");                builder.field(jsonFieldName.asText(), asConnectSchema(field));            }            break;        default:            throw new DataException("Unknown schema type: " + schemaTypeNode.textValue());    }    JsonNode schemaOptionalNode = jsonSchema.get(JsonSchema.SCHEMA_OPTIONAL_FIELD_NAME);    if (schemaOptionalNode != null && schemaOptionalNode.isBoolean() && schemaOptionalNode.booleanValue())        builder.optional();    else        builder.required();    JsonNode schemaNameNode = jsonSchema.get(JsonSchema.SCHEMA_NAME_FIELD_NAME);    if (schemaNameNode != null && schemaNameNode.isTextual())        builder.name(schemaNameNode.textValue());    JsonNode schemaVersionNode = jsonSchema.get(JsonSchema.SCHEMA_VERSION_FIELD_NAME);    if (schemaVersionNode != null && schemaVersionNode.isIntegralNumber()) {        builder.version(schemaVersionNode.intValue());    }    JsonNode schemaDocNode = jsonSchema.get(JsonSchema.SCHEMA_DOC_FIELD_NAME);    if (schemaDocNode != null && schemaDocNode.isTextual())        builder.doc(schemaDocNode.textValue());    JsonNode schemaParamsNode = jsonSchema.get(JsonSchema.SCHEMA_PARAMETERS_FIELD_NAME);    if (schemaParamsNode != null && schemaParamsNode.isObject()) {        Iterator<Map.Entry<String, JsonNode>> paramsIt = schemaParamsNode.fields();        while (paramsIt.hasNext()) {            Map.Entry<String, JsonNode> entry = paramsIt.next();            JsonNode paramValue = entry.getValue();            if (!paramValue.isTextual())                throw new DataException("Schema parameters must have string values.");            builder.parameter(entry.getKey(), paramValue.textValue());        }    }    JsonNode schemaDefaultNode = jsonSchema.get(JsonSchema.SCHEMA_DEFAULT_FIELD_NAME);    if (schemaDefaultNode != null)        builder.defaultValue(convertToConnect(builder, schemaDefaultNode));    Schema result = builder.build();    toConnectSchemaCache.put(jsonSchema, result);    return result;}
f9425
0
convertToJsonWithEnvelope
private JsonNode kafkatest_f9426_0(Schema schema, Object value)
{    return new JsonSchema.Envelope(asJsonSchema(schema), convertToJson(schema, value)).toJsonNode();}
f9426
0
toJsonNode
public ObjectNode kafkatest_f9435_0()
{    return envelope(schema, payload);}
f9435
0
serialize
public byte[] kafkatest_f9436_0(String topic, JsonNode data)
{    if (data == null)        return null;    try {        return objectMapper.writeValueAsBytes(data);    } catch (Exception e) {        throw new SerializationException("Error serializing JSON message", e);    }}
f9436
0
doubleToConnect
public void kafkatest_f9445_0()
{    assertEquals(new SchemaAndValue(Schema.FLOAT64_SCHEMA, 12.34), converter.toConnectData(TOPIC, "{ \"schema\": { \"type\": \"double\" }, \"payload\": 12.34 }".getBytes()));}
f9445
0
bytesToConnect
public void kafkatest_f9446_0() throws UnsupportedEncodingException
{    ByteBuffer reference = ByteBuffer.wrap("test-string".getBytes("UTF-8"));    String msg = "{ \"schema\": { \"type\": \"bytes\" }, \"payload\": \"dGVzdC1zdHJpbmc=\" }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    ByteBuffer converted = ByteBuffer.wrap((byte[]) schemaAndValue.value());    assertEquals(reference, converted);}
f9446
0
decimalToConnectOptional
public void kafkatest_f9455_0()
{    Schema schema = Decimal.builder(2).optional().schema();    String msg = "{ \"schema\": { \"type\": \"bytes\", \"name\": \"org.apache.kafka.connect.data.Decimal\", \"version\": 1, \"optional\": true, \"parameters\": { \"scale\": \"2\" } }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertNull(schemaAndValue.value());}
f9455
0
decimalToConnectWithDefaultValue
public void kafkatest_f9456_0()
{    BigDecimal reference = new BigDecimal(new BigInteger("156"), 2);    Schema schema = Decimal.builder(2).defaultValue(reference).build();    String msg = "{ \"schema\": { \"type\": \"bytes\", \"name\": \"org.apache.kafka.connect.data.Decimal\", \"version\": 1, \"default\": \"AJw=\", \"parameters\": { \"scale\": \"2\" } }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertEquals(reference, schemaAndValue.value());}
f9456
0
timeToConnectOptionalWithDefaultValue
public void kafkatest_f9465_0()
{    java.util.Date reference = new java.util.Date(0);    Schema schema = Time.builder().optional().defaultValue(reference).schema();    String msg = "{ \"schema\": { \"type\": \"int32\", \"name\": \"org.apache.kafka.connect.data.Time\", \"version\": 1, \"optional\": true, \"default\": 0 }, \"payload\": null }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    assertEquals(schema, schemaAndValue.schema());    assertEquals(reference, schemaAndValue.value());}
f9465
0
timestampToConnect
public void kafkatest_f9466_0()
{    Schema schema = Timestamp.SCHEMA;    GregorianCalendar calendar = new GregorianCalendar(1970, Calendar.JANUARY, 1, 0, 0, 0);    calendar.setTimeZone(TimeZone.getTimeZone("UTC"));    calendar.add(Calendar.MILLISECOND, 2000000000);    calendar.add(Calendar.MILLISECOND, 2000000000);    java.util.Date reference = calendar.getTime();    String msg = "{ \"schema\": { \"type\": \"int64\", \"name\": \"org.apache.kafka.connect.data.Timestamp\", \"version\": 1 }, \"payload\": 4000000000 }";    SchemaAndValue schemaAndValue = converter.toConnectData(TOPIC, msg.getBytes());    java.util.Date converted = (java.util.Date) schemaAndValue.value();    assertEquals(schema, schemaAndValue.schema());    assertEquals(reference, converted);}
f9466
0
intToJson
public void kafkatest_f9475_0()
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Schema.INT32_SCHEMA, 12));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"int32\", \"optional\": false }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertEquals(12, converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).intValue());}
f9475
0
longToJson
public void kafkatest_f9476_0()
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Schema.INT64_SCHEMA, 4398046511104L));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"int64\", \"optional\": false }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertEquals(4398046511104L, converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).longValue());}
f9476
0
structSchemaIdentical
public void kafkatest_f9485_0()
{    Schema schema = SchemaBuilder.struct().field("field1", Schema.BOOLEAN_SCHEMA).field("field2", Schema.STRING_SCHEMA).field("field3", Schema.STRING_SCHEMA).field("field4", Schema.BOOLEAN_SCHEMA).build();    Schema inputSchema = SchemaBuilder.struct().field("field1", Schema.BOOLEAN_SCHEMA).field("field2", Schema.STRING_SCHEMA).field("field3", Schema.STRING_SCHEMA).field("field4", Schema.BOOLEAN_SCHEMA).build();    Struct input = new Struct(inputSchema).put("field1", true).put("field2", "string2").put("field3", "string3").put("field4", false);    assertStructSchemaEqual(schema, input);}
f9485
0
decimalToJson
public void kafkatest_f9486_0() throws IOException
{    JsonNode converted = parse(converter.fromConnectData(TOPIC, Decimal.schema(2), new BigDecimal(new BigInteger("156"), 2)));    validateEnvelope(converted);    assertEquals(parse("{ \"type\": \"bytes\", \"optional\": false, \"name\": \"org.apache.kafka.connect.data.Decimal\", \"version\": 1, \"parameters\": { \"scale\": \"2\" } }"), converted.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertArrayEquals(new byte[] { 0, -100 }, converted.get(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME).binaryValue());}
f9486
0
nullValueToJson
public void kafkatest_f9495_0()
{    // This characterizes the production of tombstone messages when Json schemas is not enabled    Map<String, Boolean> props = Collections.singletonMap("schemas.enable", false);    converter.configure(props, true);    byte[] converted = converter.fromConnectData(TOPIC, null, null);    assertNull(converted);}
f9495
0
mismatchSchemaJson
public void kafkatest_f9496_0()
{    // If we have mismatching schema info, we should properly convert to a DataException    converter.fromConnectData(TOPIC, Schema.FLOAT64_SCHEMA, true);}
f9496
0
validateEnvelope
private void kafkatest_f9505_0(JsonNode env)
{    assertNotNull(env);    assertTrue(env.isObject());    assertEquals(2, env.size());    assertTrue(env.has(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertTrue(env.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME).isObject());    assertTrue(env.has(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME));}
f9505
0
validateEnvelopeNullSchema
private void kafkatest_f9506_0(JsonNode env)
{    assertNotNull(env);    assertTrue(env.isObject());    assertEquals(2, env.size());    assertTrue(env.has(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME));    assertTrue(env.get(JsonSchema.ENVELOPE_SCHEMA_FIELD_NAME).isNull());    assertTrue(env.has(JsonSchema.ENVELOPE_PAYLOAD_FIELD_NAME));}
f9506
0
policyName
protected String kafkatest_f9516_0()
{    return "All";}
f9516
0
isAllowed
protected boolean kafkatest_f9517_0(ConfigValue configValue)
{    return true;}
f9517
0
fromConnectData
public byte[] kafkatest_f9528_0(String topic, Schema schema, Object value)
{    if (schema != null && schema.type() != Schema.Type.BYTES)        throw new DataException("Invalid schema type for ByteArrayConverter: " + schema.type().toString());    if (value != null && !(value instanceof byte[]))        throw new DataException("ByteArrayConverter is not compatible with objects of type " + value.getClass());    return (byte[]) value;}
f9528
0
toConnectData
public SchemaAndValue kafkatest_f9529_0(String topic, byte[] value)
{    return new SchemaAndValue(Schema.OPTIONAL_BYTES_SCHEMA, value);}
f9529
0
toConnectData
public SchemaAndValue kafkatest_f9538_0(String topic, byte[] value)
{    try {        return new SchemaAndValue(schema, deserializer.deserialize(topic, value));    } catch (SerializationException e) {        throw new DataException("Failed to deserialize " + typeName + ": ", e);    }}
f9538
0
fromConnectHeader
public byte[] kafkatest_f9539_0(String topic, String headerKey, Schema schema, Object value)
{    return fromConnectData(topic, schema, value);}
f9539
0
onShutdown
public void kafkatest_f9549_0(String connector)
{    statusBackingStore.putSafe(new ConnectorStatus(connector, ConnectorStatus.State.UNASSIGNED, workerId, generation()));}
f9549
0
onFailure
public void kafkatest_f9550_0(String connector, Throwable cause)
{    statusBackingStore.putSafe(new ConnectorStatus(connector, ConnectorStatus.State.FAILED, trace(cause), workerId, generation()));}
f9550
0
plugins
public Plugins kafkatest_f9559_0()
{    return worker.getPlugins();}
f9559
0
connectors
public Collection<String> kafkatest_f9560_0()
{    return configBackingStore.snapshot().connectors();}
f9560
0
convertConfigKey
private static ConfigKeyInfo kafkatest_f9569_0(ConfigKey configKey)
{    return convertConfigKey(configKey, "");}
f9569
0
convertConfigKey
private static ConfigKeyInfo kafkatest_f9570_0(ConfigKey configKey, String prefix)
{    String name = prefix + configKey.name;    Type type = configKey.type;    String typeName = configKey.type.name();    boolean required = false;    String defaultValue;    if (ConfigDef.NO_DEFAULT_VALUE.equals(configKey.defaultValue)) {        defaultValue = null;        required = true;    } else {        defaultValue = ConfigDef.convertToString(configKey.defaultValue, type);    }    String importance = configKey.importance.name();    String documentation = configKey.documentation;    String group = configKey.group;    int orderInGroup = configKey.orderInGroup;    String width = configKey.width.name();    String displayName = configKey.displayName;    List<String> dependents = configKey.dependents;    return new ConfigKeyInfo(name, typeName, required, defaultValue, importance, documentation, group, orderInGroup, width, displayName, dependents);}
f9570
0
state
public State kafkatest_f9579_0()
{    return state;}
f9579
0
trace
public String kafkatest_f9580_0()
{    return trace;}
f9580
0
restUrl
public URI kafkatest_f9589_0()
{    return rest.serverUrl();}
f9589
0
run
public voidf9590_1)
{    try {        startLatch.await();        Connect.this.stop();    } catch (InterruptedException e) {            }}
public voidf9590
1
tags
public Map<String, String> kafkatest_f9599_0()
{    return tags;}
f9599
0
includes
public boolean kafkatest_f9600_0(MetricName metricName)
{    return metricName != null && groupName.equals(metricName.group()) && tags.equals(metricName.tags());}
f9600
0
addValueMetric
public void kafkatest_f9609_0(MetricNameTemplate nameTemplate, final LiteralSupplier<T> supplier)
{    MetricName metricName = metricName(nameTemplate);    if (metrics().metric(metricName) == null) {        metrics().addMetric(metricName, new Gauge<T>() {            @Override            public T value(MetricConfig config, long now) {                return supplier.metricValue(now);            }        });    }}
f9609
0
value
public T kafkatest_f9610_0(MetricConfig config, long now)
{    return supplier.metricValue(now);}
f9610
0
tags
 static Map<String, String> kafkatest_f9619_0(String... keyValue)
{    if ((keyValue.length % 2) != 0)        throw new IllegalArgumentException("keyValue needs to be specified in pairs");    Map<String, String> tags = new LinkedHashMap<>();    for (int i = 0; i < keyValue.length; i += 2) {        tags.put(keyValue[i], keyValue[i + 1]);    }    return tags;}
f9619
0
main
public static void kafkatest_f9620_0(String[] args)
{    ConnectMetricsRegistry metrics = new ConnectMetricsRegistry();    System.out.println(Metrics.toHtmlTable(JMX_PREFIX, metrics.getAllTemplates()));}
f9620
0
workerGroupName
public String kafkatest_f9629_0()
{    return WORKER_GROUP_NAME;}
f9629
0
workerRebalanceGroupName
public String kafkatest_f9630_0()
{    return WORKER_REBALANCE_GROUP_NAME;}
f9630
0
errorToleranceType
public ToleranceType kafkatest_f9639_0()
{    String tolerance = getString(ERRORS_TOLERANCE_CONFIG);    for (ToleranceType type : ToleranceType.values()) {        if (type.name().equalsIgnoreCase(tolerance)) {            return type;        }    }    return ERRORS_TOLERANCE_DEFAULT;}
f9639
0
enableErrorLog
public boolean kafkatest_f9640_0()
{    return getBoolean(ERRORS_LOG_ENABLE_CONFIG);}
f9640
0
contains
public boolean kafkatest_f9649_0(String connector)
{    return connectorConfigs.containsKey(connector);}
f9649
0
connectors
public Set<String> kafkatest_f9650_0()
{    return connectorConfigs.keySet();}
f9650
0
inconsistentConnectors
public Set<String> kafkatest_f9659_0()
{    return inconsistentConnectors;}
f9659
0
toString
public String kafkatest_f9660_0()
{    return "ClusterConfigState{" + "offset=" + offset + ", connectorTaskCounts=" + connectorTaskCounts + ", connectorConfigs=" + connectorConfigs + ", taskConfigs=" + taskConfigs + ", inconsistentConnectors=" + inconsistentConnectors + '}';}
f9660
0
offset
public long kafkatest_f9669_0()
{    return offset;}
f9669
0
toString
public String kafkatest_f9670_0()
{    return "WorkerState{" + "url='" + url + '\'' + ", offset=" + offset + '}';}
f9670
0
asMap
protected Map<String, Collection<Integer>> kafkatest_f9679_0()
{    // Using LinkedHashMap preserves the ordering, which is helpful for tests and debugging    Map<String, Collection<Integer>> taskMap = new LinkedHashMap<>();    for (String connectorId : new HashSet<>(connectorIds)) {        Collection<Integer> connectorTasks = taskMap.get(connectorId);        if (connectorTasks == null) {            connectorTasks = new ArrayList<>();            taskMap.put(connectorId, connectorTasks);        }        connectorTasks.add(CONNECTOR_TASK);    }    for (ConnectorTaskId taskId : taskIds) {        String connectorId = taskId.connector();        Collection<Integer> connectorTasks = taskMap.get(connectorId);        if (connectorTasks == null) {            connectorTasks = new ArrayList<>();            taskMap.put(connectorId, connectorTasks);        }        connectorTasks.add(taskId.task());    }    return taskMap;}
f9679
0
checkVersionCompatibility
private static void kafkatest_f9680_0(short version)
{    // check for invalid versions    if (version < CONNECT_PROTOCOL_V0)        throw new SchemaException("Unsupported subscription version: " + version);// otherwise, assume versions can be parsed as V0}
f9680
0
start
public void kafkatest_f9689_0()
{    this.herderExecutor.submit(this);}
f9689
0
run
public voidf9690_1)
{    try {                startServices();                while (!stopping.get()) {            tick();        }        halt();                herderMetrics.close();    } catch (Throwable t) {                Exit.exit(1);    }}
public voidf9690
1
connectors
public void kafkatest_f9699_0(final Callback<Collection<String>> callback)
{    log.trace("Submitting connector listing request");    addRequest(new Callable<Void>() {        @Override        public Void call() throws Exception {            if (checkRebalanceNeeded(callback))                return null;            callback.onCompletion(null, configState.connectors());            return null;        }    }, forwardErrorCallback(callback));}
f9699
0
call
public Void kafkatest_f9700_0() throws Exception
{    if (checkRebalanceNeeded(callback))        return null;    callback.onCompletion(null, configState.connectors());    return null;}
f9700
0
putConnectorConfig
public void kafkatest_f9709_0(final String connName, final Map<String, String> config, final boolean allowReplace, final Callback<Created<ConnectorInfo>> callback)
{    log.trace("Submitting connector config write request {}", connName);    addRequest(new Callable<Void>() {        @Override        public Void call() throws Exception {            if (maybeAddConfigErrors(validateConnectorConfig(config), callback)) {                return null;            }            log.trace("Handling connector config request {}", connName);            if (!isLeader()) {                callback.onCompletion(new NotLeaderException("Only the leader can set connector configs.", leaderUrl()), null);                return null;            }            boolean exists = configState.contains(connName);            if (!allowReplace && exists) {                callback.onCompletion(new AlreadyExistsException("Connector " + connName + " already exists"), null);                return null;            }            log.trace("Submitting connector config {} {} {}", connName, allowReplace, configState.connectors());            configBackingStore.putConnectorConfig(connName, config);            // Note that we use the updated connector config despite the fact that we don't have an updated            // snapshot yet. The existing task info should still be accurate.            ConnectorInfo info = new ConnectorInfo(connName, config, configState.tasks(connName), // validateConnectorConfig have checked the existence of CONNECTOR_CLASS_CONFIG            connectorTypeForClass(config.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG)));            callback.onCompletion(null, new Created<>(!exists, info));            return null;        }    }, forwardErrorCallback(callback));}
f9709
0
call
public Void kafkatest_f9710_0() throws Exception
{    if (maybeAddConfigErrors(validateConnectorConfig(config), callback)) {        return null;    }    log.trace("Handling connector config request {}", connName);    if (!isLeader()) {        callback.onCompletion(new NotLeaderException("Only the leader can set connector configs.", leaderUrl()), null);        return null;    }    boolean exists = configState.contains(connName);    if (!allowReplace && exists) {        callback.onCompletion(new AlreadyExistsException("Connector " + connName + " already exists"), null);        return null;    }    log.trace("Submitting connector config {} {} {}", connName, allowReplace, configState.connectors());    configBackingStore.putConnectorConfig(connName, config);    // Note that we use the updated connector config despite the fact that we don't have an updated    // snapshot yet. The existing task info should still be accurate.    ConnectorInfo info = new ConnectorInfo(connName, config, configState.tasks(connName), // validateConnectorConfig have checked the existence of CONNECTOR_CLASS_CONFIG    connectorTypeForClass(config.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG)));    callback.onCompletion(null, new Created<>(!exists, info));    return null;}
f9710
0
restartConnector
public HerderRequest kafkatest_f9719_0(final long delayMs, final String connName, final Callback<Void> callback)
{    return addRequest(delayMs, new Callable<Void>() {        @Override        public Void call() throws Exception {            if (checkRebalanceNeeded(callback))                return null;            if (!configState.connectors().contains(connName)) {                callback.onCompletion(new NotFoundException("Unknown connector: " + connName), null);                return null;            }            if (assignment.connectors().contains(connName)) {                try {                    worker.stopConnector(connName);                    if (startConnector(connName))                        callback.onCompletion(null, null);                    else                        callback.onCompletion(new ConnectException("Failed to start connector: " + connName), null);                } catch (Throwable t) {                    callback.onCompletion(t, null);                }            } else if (isLeader()) {                callback.onCompletion(new NotAssignedException("Cannot restart connector since it is not assigned to this member", member.ownerUrl(connName)), null);            } else {                callback.onCompletion(new NotLeaderException("Cannot restart connector since it is not assigned to this member", leaderUrl()), null);            }            return null;        }    }, forwardErrorCallback(callback));}
f9719
0
call
public Void kafkatest_f9720_0() throws Exception
{    if (checkRebalanceNeeded(callback))        return null;    if (!configState.connectors().contains(connName)) {        callback.onCompletion(new NotFoundException("Unknown connector: " + connName), null);        return null;    }    if (assignment.connectors().contains(connName)) {        try {            worker.stopConnector(connName);            if (startConnector(connName))                callback.onCompletion(null, null);            else                callback.onCompletion(new ConnectException("Failed to start connector: " + connName), null);        } catch (Throwable t) {            callback.onCompletion(t, null);        }    } else if (isLeader()) {        callback.onCompletion(new NotAssignedException("Cannot restart connector since it is not assigned to this member", member.ownerUrl(connName)), null);    } else {        callback.onCompletion(new NotLeaderException("Cannot restart connector since it is not assigned to this member", leaderUrl()), null);    }    return null;}
f9720
0
startAndStop
private void kafkatest_f9729_0(Collection<Callable<Void>> callables)
{    try {        startAndStopExecutor.invokeAll(callables);    } catch (InterruptedException e) {    // ignore    }}
f9729
0
startWork
private voidf9730_1)
{    // Start assigned connectors and tasks        List<Callable<Void>> callables = new ArrayList<>();    for (String connectorName : assignmentDifference(assignment.connectors(), runningAssignment.connectors())) {        callables.add(getConnectorStartingCallable(connectorName));    }    // These tasks have been stopped by this worker due to task reconfiguration. In order to    // restart them, they are removed just before the overall task startup from the set of    // currently running tasks. Therefore, they'll be restarted only if they are included in    // the assignment that was just received after rebalancing.    runningAssignment.tasks().removeAll(tasksToRestart);    tasksToRestart.clear();    for (ConnectorTaskId taskId : assignmentDifference(assignment.tasks(), runningAssignment.tasks())) {        callables.add(getTaskStartingCallable(taskId));    }    startAndStop(callables);    runningAssignment = member.currentProtocolVersion() == CONNECT_PROTOCOL_V0 ? ExtendedAssignment.empty() : assignment;    }
private voidf9730
1
call
public Voidf9739_1) throws Exception
{    try {        startConnector(connectorName);    } catch (Throwable t) {                onFailure(connectorName, t);    }    return null;}
public Voidf9739
1
getConnectorStoppingCallable
private Callable<Void>f9740_1final String connectorName)
{    return new Callable<Void>() {        @Override        public Void call() throws Exception {            try {                worker.stopConnector(connectorName);            } catch (Throwable t) {                            }            return null;        }    };}
private Callable<Void>f9740
1
addRequest
 DistributedHerderRequest kafkatest_f9749_0(Callable<Void> action, Callback<Void> callback)
{    return addRequest(0, action, callback);}
f9749
0
addRequest
 DistributedHerderRequest kafkatest_f9750_0(long delayMs, Callable<Void> action, Callback<Void> callback)
{    DistributedHerderRequest req = new DistributedHerderRequest(time.milliseconds() + delayMs, requestSeqNum.incrementAndGet(), action, callback);    requests.add(req);    if (peekWithoutException() == req)        member.wakeup();    return req;}
f9750
0
compareTo
public int kafkatest_f9759_0(DistributedHerderRequest o)
{    final int cmp = Long.compare(at, o.at);    return cmp == 0 ? Long.compare(seq, o.seq) : cmp;}
f9759
0
equals
public boolean kafkatest_f9760_0(Object o)
{    if (this == o)        return true;    if (!(o instanceof DistributedHerderRequest))        return false;    DistributedHerderRequest other = (DistributedHerderRequest) o;    return compareTo(other) == 0;}
f9760
0
metricValue
public Double kafkatest_f9769_0(long now)
{    return (double) generation;}
f9769
0
metricValue
public Double kafkatest_f9770_0(long now)
{    return rebalancing ? 1.0d : 0.0d;}
f9770
0
fillAssignmentsAndSerialize
private Map<String, ByteBuffer>f9779_1Collection<String> members, short error, String leaderId, String leaderUrl, long maxOffset, Map<String, Collection<String>> connectorAssignments, Map<String, Collection<ConnectorTaskId>> taskAssignments)
{    Map<String, ByteBuffer> groupAssignment = new HashMap<>();    for (String member : members) {        Collection<String> connectors = connectorAssignments.get(member);        if (connectors == null) {            connectors = Collections.emptyList();        }        Collection<ConnectorTaskId> tasks = taskAssignments.get(member);        if (tasks == null) {            tasks = Collections.emptyList();        }        Assignment assignment = new Assignment(error, leaderId, leaderUrl, maxOffset, connectors, tasks);                groupAssignment.put(member, ConnectProtocol.serializeAssignment(assignment));    }        return groupAssignment;}
private Map<String, ByteBuffer>f9779
1
findMaxMemberConfigOffset
private longf9780_1Map<String, ExtendedWorkerState> memberConfigs, WorkerCoordinator coordinator)
{    // The new config offset is the maximum seen by any member. We always perform assignment using this offset,    // even if some members have fallen behind. The config offset used to generate the assignment is included in    // the response so members that have fallen behind will not use the assignment until they have caught up.    Long maxOffset = null;    for (Map.Entry<String, ExtendedWorkerState> stateEntry : memberConfigs.entrySet()) {        long memberRootOffset = stateEntry.getValue().offset();        if (maxOffset == null)            maxOffset = memberRootOffset;        else            maxOffset = Math.max(maxOffset, memberRootOffset);    }        return maxOffset;}
private longf9780
1
toStruct
public Struct kafkatest_f9789_0()
{    Collection<Struct> assigned = taskAssignments(asMap());    Collection<Struct> revoked = taskAssignments(revokedAsMap());    return new Struct(ASSIGNMENT_V1).set(ERROR_KEY_NAME, error()).set(LEADER_KEY_NAME, leader()).set(LEADER_URL_KEY_NAME, leaderUrl()).set(CONFIG_OFFSET_KEY_NAME, offset()).set(ASSIGNMENT_KEY_NAME, assigned != null ? assigned.toArray() : null).set(REVOKED_KEY_NAME, revoked != null ? revoked.toArray() : null).set(SCHEDULED_DELAY_KEY_NAME, delay);}
f9789
0
fromStruct
public static ExtendedAssignment kafkatest_f9790_0(short version, Struct struct)
{    return struct == null ? null : new ExtendedAssignment(version, struct.getShort(ERROR_KEY_NAME), struct.getString(LEADER_KEY_NAME), struct.getString(LEADER_URL_KEY_NAME), struct.getLong(CONFIG_OFFSET_KEY_NAME), extractConnectors(struct, ASSIGNMENT_KEY_NAME), extractTasks(struct, ASSIGNMENT_KEY_NAME), extractConnectors(struct, REVOKED_KEY_NAME), extractTasks(struct, REVOKED_KEY_NAME), struct.getInt(SCHEDULED_DELAY_KEY_NAME));}
f9790
0
computeDeleted
private Map<String, ConnectorsAndTasks>f9799_1ConnectorsAndTasks deleted, Map<String, Collection<String>> connectorAssignments, Map<String, Collection<ConnectorTaskId>> taskAssignments)
{    // Connector to worker reverse lookup map    Map<String, String> connectorOwners = WorkerCoordinator.invertAssignment(connectorAssignments);    // Task to worker reverse lookup map    Map<ConnectorTaskId, String> taskOwners = WorkerCoordinator.invertAssignment(taskAssignments);    Map<String, ConnectorsAndTasks> toRevoke = new HashMap<>();    // Add the connectors that have been deleted to the revoked set    deleted.connectors().forEach(c -> toRevoke.computeIfAbsent(connectorOwners.get(c), v -> new ConnectorsAndTasks.Builder().build()).connectors().add(c));    // Add the tasks that have been deleted to the revoked set    deleted.tasks().forEach(t -> toRevoke.computeIfAbsent(taskOwners.get(t), v -> new ConnectorsAndTasks.Builder().build()).tasks().add(t));        return toRevoke;}
private Map<String, ConnectorsAndTasks>f9799
1
computePreviousAssignment
private ConnectorsAndTasks kafkatest_f9800_0(Map<String, ConnectorsAndTasks> toRevoke, Map<String, Collection<String>> connectorAssignments, Map<String, Collection<ConnectorTaskId>> taskAssignments, ConnectorsAndTasks lostAssignments)
{    ConnectorsAndTasks previousAssignment = new ConnectorsAndTasks.Builder().with(connectorAssignments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet()), taskAssignments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet())).build();    for (ConnectorsAndTasks revoked : toRevoke.values()) {        previousAssignment.connectors().removeAll(revoked.connectors());        previousAssignment.tasks().removeAll(revoked.tasks());        previousRevocation.connectors().addAll(revoked.connectors());        previousRevocation.tasks().addAll(revoked.tasks());    }    // Depends on the previous assignment's collections being sets at the moment.    // TODO: make it independent    previousAssignment.connectors().addAll(lostAssignments.connectors());    previousAssignment.tasks().addAll(lostAssignments.tasks());    return previousAssignment;}
f9800
0
assignment
private ConnectorsAndTasksf9809_1Map<String, ExtendedWorkerState> memberConfigs)
{        Set<String> connectors = memberConfigs.values().stream().flatMap(state -> state.assignment().connectors().stream()).collect(Collectors.toSet());    Set<ConnectorTaskId> tasks = memberConfigs.values().stream().flatMap(state -> state.assignment().tasks().stream()).collect(Collectors.toSet());    return new ConnectorsAndTasks.Builder().with(connectors, tasks).build();}
private ConnectorsAndTasksf9809
1
calculateDelay
private int kafkatest_f9810_0(long now)
{    long diff = scheduledRebalance - now;    return diff > 0 ? (int) Math.min(diff, maxDelay) : 0;}
f9810
0
checkVersionCompatibility
private static void kafkatest_f9819_0(short version)
{    // check for invalid versions    if (version < CONNECT_PROTOCOL_V0)        throw new SchemaException("Unsupported subscription version: " + version);// otherwise, assume versions can be parsed}
f9819
0
forwardUrl
public String kafkatest_f9820_0()
{    return forwardUrl;}
f9820
0
rejoinNeededOrPending
protected boolean kafkatest_f9829_0()
{    return super.rejoinNeededOrPending() || (assignmentSnapshot == null || assignmentSnapshot.failed()) || rejoinRequested;}
f9829
0
memberId
public String kafkatest_f9830_0()
{    Generation generation = generation();    if (generation != null)        return generation.memberId;    return JoinGroupRequest.UNKNOWN_MEMBER_ID;}
f9830
0
currentProtocolVersion
public short kafkatest_f9839_0()
{    return currentConnectProtocol == EAGER ? (short) 0 : (short) 1;}
f9839
0
measure
public double kafkatest_f9840_0(MetricConfig config, long now)
{    return assignmentSnapshot.connectors().size();}
f9840
0
tasks
public Collection<ConnectorTaskId> kafkatest_f9849_0()
{    return tasks;}
f9849
0
size
public int kafkatest_f9850_0()
{    return connectors.size() + tasks.size();}
f9850
0
connectorsSize
public int kafkatest_f9859_0()
{    return connectors.size();}
f9859
0
tasksSize
public int kafkatest_f9860_0()
{    return tasks.size();}
f9860
0
hashCode
public int kafkatest_f9869_0()
{    return Objects.hash(worker, connectors, tasks);}
f9869
0
stop
public void kafkatest_f9870_0()
{    if (stopped)        return;    stop(false);}
f9870
0
currentProtocolVersion
public short kafkatest_f9879_0()
{    return coordinator.currentProtocolVersion();}
f9879
0
stop
private voidf9880_1boolean swallowException)
{    log.trace("Stopping the Connect group member.");    AtomicReference<Throwable> firstException = new AtomicReference<>();    this.stopped = true;    Utils.closeQuietly(coordinator, "coordinator", firstException);    Utils.closeQuietly(metrics, "consumer metrics", firstException);    Utils.closeQuietly(client, "consumer network client", firstException);    AppInfoParser.unregisterAppInfo(JMX_PREFIX, clientId, metrics);    if (firstException.get() != null && !swallowException)        throw new KafkaException("Failed to stop the Connect group member", firstException.get());    else        }
private voidf9880
1
recordError
public void kafkatest_f9889_0()
{    recordProcessingErrors.record();}
f9889
0
recordSkipped
public void kafkatest_f9890_0()
{    recordsSkipped.record();}
f9890
0
reset
private void kafkatest_f9899_0()
{    attempt = 0;    position = null;    klass = null;    error = null;}
f9899
0
consumerRecord
public void kafkatest_f9900_0(ConsumerRecord<byte[], byte[]> consumedMessage)
{    this.consumedMessage = consumedMessage;    reset();}
f9900
0
report
public void kafkatest_f9909_0()
{    for (ErrorReporter reporter : reporters) {        reporter.report(this);    }}
f9909
0
toString
public String kafkatest_f9910_0()
{    return toString(false);}
f9910
0
execAndRetry
protected V kafkatest_f9919_0(Operation<V> operation) throws Exception
{    int attempt = 0;    long startTime = time.milliseconds();    long deadline = startTime + errorRetryTimeout;    do {        try {            attempt++;            return operation.call();        } catch (RetriableException e) {            log.trace("Caught a retriable exception while executing {} operation with {}", context.stage(), context.executingClass());            errorHandlingMetrics.recordFailure();            if (checkRetry(startTime)) {                backoff(attempt, deadline);                if (Thread.currentThread().isInterrupted()) {                    log.trace("Thread was interrupted. Marking operation as failed.");                    context.error(e);                    return null;                }                errorHandlingMetrics.recordRetry();            } else {                log.trace("Can't retry. start={}, attempt={}, deadline={}", startTime, attempt, deadline);                context.error(e);                return null;            }        } finally {            context.attempt(attempt);        }    } while (true);}
f9919
0
execAndHandleError
protected V kafkatest_f9920_0(Operation<V> operation, Class<? extends Exception> tolerated)
{    try {        V result = execAndRetry(operation);        if (context.failed()) {            markAsFailed();            errorHandlingMetrics.recordSkipped();        }        return result;    } catch (Exception e) {        errorHandlingMetrics.recordFailure();        markAsFailed();        context.error(e);        if (!tolerated.isAssignableFrom(e.getClass())) {            throw new ConnectException("Unhandled exception in error handler", e);        }        if (!withinToleranceLimits()) {            throw new ConnectException("Tolerance exceeded in error handler", e);        }        errorHandlingMetrics.recordSkipped();        return null;    }}
f9920
0
consumerRecord
public void kafkatest_f9929_0(ConsumerRecord<byte[], byte[]> consumedMessage)
{    this.context.consumerRecord(consumedMessage);}
f9929
0
failed
public boolean kafkatest_f9930_0()
{    return this.context.failed();}
f9930
0
result
public T kafkatest_f9939_0()
{    return result;}
f9939
0
equals
public boolean kafkatest_f9940_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    Created<?> created1 = (Created<?>) o;    return Objects.equals(created, created1.created) && Objects.equals(result, created1.result);}
f9940
0
restExtensions
public Set<PluginDesc<ConnectRestExtension>> kafkatest_f9949_0()
{    return restExtensions;}
f9949
0
connectorClientConfigPolicies
public Set<PluginDesc<ConnectorClientConfigOverridePolicy>> kafkatest_f9950_0()
{    return connectorClientConfigPolicies;}
f9950
0
loadJdbcDrivers
private voidf9959_1final ClassLoader loader)
{    // Apply here what java.sql.DriverManager does to discover and register classes    // implementing the java.sql.Driver interface.    AccessController.doPrivileged(new PrivilegedAction<Void>() {        @Override        public Void run() {            ServiceLoader<Driver> loadedDrivers = ServiceLoader.load(Driver.class, loader);            Iterator<Driver> driversIterator = loadedDrivers.iterator();            try {                while (driversIterator.hasNext()) {                    Driver driver = driversIterator.next();                                    }            } catch (Throwable t) {                            }            return null;        }    });}
private voidf9959
1
run
public Voidf9960_1)
{    ServiceLoader<Driver> loadedDrivers = ServiceLoader.load(Driver.class, loader);    Iterator<Driver> driversIterator = loadedDrivers.iterator();    try {        while (driversIterator.hasNext()) {            Driver driver = driversIterator.next();                    }    } catch (Throwable t) {            }    return null;}
public Voidf9960
1
scan
protected voidf9969_1URL url)
{    try {        super.scan(url);    } catch (ReflectionsException e) {        Logger log = Reflections.log;        if (log != null && log.isWarnEnabled()) {                    }    }}
protected voidf9969
1
getResource
public URL kafkatest_f9970_0(String name)
{    if (serviceLoaderManifestForPlugin(name)) {        // This will enable thePluginClassLoader to limit its resource search only to its own URL paths.        return null;    } else {        return super.getResource(name);    }}
f9970
0
version
public String kafkatest_f9979_0()
{    return version;}
f9979
0
type
public PluginType kafkatest_f9980_0()
{    return type;}
f9980
0
pluginClass
protected static Class<? extends U> kafkatest_f9989_0(DelegatingClassLoader loader, String classOrAlias, Class<U> pluginClass) throws ClassNotFoundException
{    Class<?> klass = loader.loadClass(classOrAlias, false);    if (pluginClass.isAssignableFrom(klass)) {        return (Class<? extends U>) klass;    }    throw new ClassNotFoundException("Requested class: " + classOrAlias + " does not extend " + pluginClass.getSimpleName());}
f9989
0
isInternalConverter
protected static boolean kafkatest_f9990_0(String classPropertyName)
{    return classPropertyName.equals(WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG) || classPropertyName.equals(WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG);}
f9990
0
configProviders
public Set<PluginDesc<ConfigProvider>> kafkatest_f9999_0()
{    return delegatingLoader.configProviders();}
f9999
0
newConnector
public Connector kafkatest_f10000_0(String connectorClassOrAlias)
{    Class<? extends Connector> klass = connectorClass(connectorClassOrAlias);    return newPlugin(klass);}
f10000
0
newTranformations
public Transformation<R> kafkatest_f10009_0(String transformationClassOrAlias)
{    return null;}
f10009
0
connectors
public Collection<PluginDesc<Connector>> kafkatest_f10010_0()
{    return connectors;}
f10010
0
simpleName
public String kafkatest_f10019_0()
{    return klass.getSimpleName();}
f10019
0
toString
public String kafkatest_f10020_0()
{    return super.toString().toLowerCase(Locale.ROOT);}
f10020
0
prunedName
public static String kafkatest_f10029_0(PluginDesc<?> plugin)
{    // It's currently simpler to switch on type than do pattern matching.    switch(plugin.type()) {        case SOURCE:        case SINK:        case CONNECTOR:            return prunePluginName(plugin, "Connector");        default:            return prunePluginName(plugin, plugin.type().simpleName());    }}
f10029
0
isAliasUnique
public static boolean kafkatest_f10030_0(PluginDesc<U> alias, Collection<PluginDesc<U>> plugins)
{    boolean matched = false;    for (PluginDesc<U> plugin : plugins) {        if (simpleName(alias).equals(simpleName(plugin)) || prunedName(alias).equals(prunedName(plugin))) {            if (matched) {                return false;            }            matched = true;        }    }    return true;}
f10030
0
register
public ResourceConfig kafkatest_f10039_0(Class<?> componentClass, Class<?>... contracts)
{    if (allowedToRegister(componentClass)) {        resourceConfig.register(componentClass, contracts);    }    return resourceConfig;}
f10039
0
register
public ResourceConfig kafkatest_f10040_0(Class<?> componentClass, int priority)
{    if (allowedToRegister(componentClass)) {        resourceConfig.register(componentClass, priority);    }    return resourceConfig;}
f10040
0
hashCode
public int kafkatest_f10049_0()
{    return Objects.hash(configKey, configValue);}
f10049
0
toString
public String kafkatest_f10050_0()
{    return "[" + configKey.toString() + "," + configValue.toString() + "]";}
f10050
0
type
public String kafkatest_f10059_0()
{    return type;}
f10059
0
required
public boolean kafkatest_f10060_0()
{    return required;}
f10060
0
equals
public boolean kafkatest_f10069_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    ConfigKeyInfo that = (ConfigKeyInfo) o;    return Objects.equals(name, that.name) && Objects.equals(type, that.type) && Objects.equals(required, that.required) && Objects.equals(defaultValue, that.defaultValue) && Objects.equals(importance, that.importance) && Objects.equals(documentation, that.documentation) && Objects.equals(group, that.group) && Objects.equals(orderInGroup, that.orderInGroup) && Objects.equals(width, that.width) && Objects.equals(displayName, that.displayName) && Objects.equals(dependents, that.dependents);}
f10069
0
hashCode
public int kafkatest_f10070_0()
{    return Objects.hash(name, type, required, defaultValue, importance, documentation, group, orderInGroup, width, displayName, dependents);}
f10070
0
toString
public String kafkatest_f10079_0()
{    StringBuffer sb = new StringBuffer();    sb.append("[").append(name).append(",").append(value).append(",").append(recommendedValues).append(",").append(errors).append(",").append(visible).append("]");    return sb.toString();}
f10079
0
name
public String kafkatest_f10080_0()
{    return name;}
f10080
0
equals
public boolean kafkatest_f10089_0(Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    ConnectorPluginInfo that = (ConnectorPluginInfo) o;    return Objects.equals(className, that.className) && type == that.type && Objects.equals(version, that.version);}
f10089
0
hashCode
public int kafkatest_f10090_0()
{    return Objects.hash(className, type, version);}
f10090
0
id
public int kafkatest_f10099_0()
{    return id;}
f10099
0
compareTo
public int kafkatest_f10100_0(TaskState that)
{    return Integer.compare(this.id, that.id);}
f10100
0
hashCode
public int kafkatest_f10109_0()
{    return Objects.hash(name, config);}
f10109
0
errorCode
public int kafkatest_f10110_0()
{    return errorCode;}
f10110
0
equals
public boolean kafkatest_f10119_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    TaskInfo taskInfo = (TaskInfo) o;    return Objects.equals(id, taskInfo.id) && Objects.equals(config, taskInfo.config);}
f10119
0
hashCode
public int kafkatest_f10120_0()
{    return Objects.hash(id, config);}
f10120
0
createConnector
public Response kafkatest_f10129_0(@QueryParam("forward") final Boolean forward, @Context final HttpHeaders headers, final CreateConnectorRequest createRequest) throws Throwable
{    // Trim leading and trailing whitespaces from the connector name, replace null with empty string    // if no name element present to keep validation within validator (NonEmptyStringWithoutControlChars    // allows null values)    String name = createRequest.name() == null ? "" : createRequest.name().trim();    Map<String, String> configs = createRequest.config();    checkAndPutConnectorConfigName(name, configs);    FutureCallback<Herder.Created<ConnectorInfo>> cb = new FutureCallback<>();    herder.putConnectorConfig(name, configs, false, cb);    Herder.Created<ConnectorInfo> info = completeOrForwardRequest(cb, "/connectors", "POST", headers, createRequest, new TypeReference<ConnectorInfo>() {    }, new CreatedConnectorInfoTranslator(), forward);    URI location = UriBuilder.fromUri("/connectors").path(name).build();    return Response.created(location).entity(info.result()).build();}
f10129
0
getConnector
public ConnectorInfo kafkatest_f10130_0(@PathParam("connector") final String connector, @Context final HttpHeaders headers, kafkatest_f10130_0("forward") final Boolean forward) throws Throwable
{    FutureCallback<ConnectorInfo> cb = new FutureCallback<>();    herder.connectorInfo(connector, cb);    return completeOrForwardRequest(cb, "/connectors/" + connector, "GET", headers, null, forward);}
f10130
0
getTaskStatus
public ConnectorStateInfo.TaskState kafkatest_f10139_0(@PathParam("connector") final String connector, @Context final HttpHeaders headers, kafkatest_f10139_0("task") final Integer task) throws Throwable
{    return herder.taskStatus(new ConnectorTaskId(connector, task));}
f10139
0
restartTask
public void kafkatest_f10140_0(@PathParam("connector") final String connector, kafkatest_f10140_0("task") final Integer task, @Context final HttpHeaders headers, kafkatest_f10140_0("forward") final Boolean forward) throws Throwable
{    FutureCallback<Void> cb = new FutureCallback<>();    ConnectorTaskId taskId = new ConnectorTaskId(connector, task);    herder.restartTask(taskId, cb);    completeOrForwardRequest(cb, "/connectors/" + connector + "/tasks/" + task + "/restart", "POST", headers, null, forward);}
f10140
0
httpRequest
public static HttpResponse<T>f10149_1String url, String method, HttpHeaders headers, Object requestBodyData, TypeReference<T> responseFormat, WorkerConfig config)
{    HttpClient client;    if (url.startsWith("https://")) {        client = new HttpClient(SSLUtils.createClientSideSslContextFactory(config));    } else {        client = new HttpClient();    }    client.setFollowRedirects(false);    try {        client.start();    } catch (Exception e) {                throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR, "Failed to start RestClient: " + e.getMessage(), e);    }    try {        String serializedBody = requestBodyData == null ? null : JSON_SERDE.writeValueAsString(requestBodyData);        log.trace("Sending {} with input {} to {}", method, serializedBody, url);        Request req = client.newRequest(url);        req.method(method);        req.accept("application/json");        req.agent("kafka-connect");        addHeadersToRequest(headers, req);        if (serializedBody != null) {            req.content(new StringContentProvider(serializedBody, StandardCharsets.UTF_8), "application/json");        }        ContentResponse res = req.send();        int responseCode = res.getStatus();                if (responseCode == HttpStatus.NO_CONTENT_204) {            return new HttpResponse<>(responseCode, convertHttpFieldsToMap(res.getHeaders()), null);        } else if (responseCode >= 400) {            ErrorMessage errorMessage = JSON_SERDE.readValue(res.getContentAsString(), ErrorMessage.class);            throw new ConnectRestException(responseCode, errorMessage.errorCode(), errorMessage.message());        } else if (responseCode >= 200 && responseCode < 300) {            T result = JSON_SERDE.readValue(res.getContentAsString(), responseFormat);            return new HttpResponse<>(responseCode, convertHttpFieldsToMap(res.getHeaders()), result);        } else {            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR, Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), "Unexpected status code when handling forwarded request: " + responseCode);        }    } catch (IOException | InterruptedException | TimeoutException | ExecutionException e) {                throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR, "IO Error trying to forward REST request: " + e.getMessage(), e);    } finally {        if (client != null)            try {                client.stop();            } catch (Exception e) {                            }    }}
public static HttpResponse<T>f10149
1
addHeadersToRequest
private static void kafkatest_f10150_0(HttpHeaders headers, Request req)
{    if (headers != null) {        String credentialAuthorization = headers.getHeaderString(HttpHeaders.AUTHORIZATION);        if (credentialAuthorization != null) {            req.header(HttpHeaders.AUTHORIZATION, credentialAuthorization);        }    }}
f10150
0
initializeResources
public voidf10159_1Herder herder)
{        ResourceConfig resourceConfig = new ResourceConfig();    resourceConfig.register(new JacksonJsonProvider());    resourceConfig.register(new RootResource(herder));    resourceConfig.register(new ConnectorsResource(herder, config));    resourceConfig.register(new ConnectorPluginsResource(herder));    resourceConfig.register(ConnectExceptionMapper.class);    resourceConfig.property(ServerProperties.WADL_FEATURE_DISABLE, true);    registerRestExtensions(herder, resourceConfig);    ServletContainer servletContainer = new ServletContainer(resourceConfig);    ServletHolder servletHolder = new ServletHolder(servletContainer);    ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);    context.setContextPath("/");    context.addServlet(servletHolder, "/*");    String allowedOrigins = config.getString(WorkerConfig.ACCESS_CONTROL_ALLOW_ORIGIN_CONFIG);    if (allowedOrigins != null && !allowedOrigins.trim().isEmpty()) {        FilterHolder filterHolder = new FilterHolder(new CrossOriginFilter());        filterHolder.setName("cross-origin");        filterHolder.setInitParameter(CrossOriginFilter.ALLOWED_ORIGINS_PARAM, allowedOrigins);        String allowedMethods = config.getString(WorkerConfig.ACCESS_CONTROL_ALLOW_METHODS_CONFIG);        if (allowedMethods != null && !allowedOrigins.trim().isEmpty()) {            filterHolder.setInitParameter(CrossOriginFilter.ALLOWED_METHODS_PARAM, allowedMethods);        }        context.addFilter(filterHolder, "/*", EnumSet.of(DispatcherType.REQUEST));    }    RequestLogHandler requestLogHandler = new RequestLogHandler();    Slf4jRequestLogWriter slf4jRequestLogWriter = new Slf4jRequestLogWriter();    slf4jRequestLogWriter.setLoggerName(RestServer.class.getCanonicalName());    CustomRequestLog requestLog = new CustomRequestLog(slf4jRequestLogWriter, CustomRequestLog.EXTENDED_NCSA_FORMAT + " %msT");    requestLogHandler.setRequestLog(requestLog);    handlers.setHandlers(new Handler[] { context, new DefaultHandler(), requestLogHandler });    try {        context.start();    } catch (Exception e) {        throw new ConnectException("Unable to initialize REST resources", e);    }     server is started and ready to handle requests");}
public voidf10159
1
serverUrl
public URI kafkatest_f10160_0()
{    return jettyServer.getURI();}
f10160
0
configureSslContextFactoryKeyStore
protected static void kafkatest_f10169_0(SslContextFactory ssl, Map<String, Object> sslConfigValues)
{    ssl.setKeyStoreType((String) getOrDefault(sslConfigValues, SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, SslConfigs.DEFAULT_SSL_KEYSTORE_TYPE));    String sslKeystoreLocation = (String) sslConfigValues.get(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG);    if (sslKeystoreLocation != null)        ssl.setKeyStorePath(sslKeystoreLocation);    Password sslKeystorePassword = (Password) sslConfigValues.get(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG);    if (sslKeystorePassword != null)        ssl.setKeyStorePassword(sslKeystorePassword.value());    Password sslKeyPassword = (Password) sslConfigValues.get(SslConfigs.SSL_KEY_PASSWORD_CONFIG);    if (sslKeyPassword != null)        ssl.setKeyManagerPassword(sslKeyPassword.value());}
f10169
0
getOrDefault
protected static Object kafkatest_f10170_0(Map<String, Object> configMap, String key, Object defaultValue)
{    if (configMap.containsKey(key))        return configMap.get(key);    return defaultValue;}
f10170
0
dlqTopicName
public String kafkatest_f10179_0()
{    return getString(DLQ_TOPIC_NAME_CONFIG);}
f10179
0
dlqTopicReplicationFactor
public short kafkatest_f10180_0()
{    return getShort(DLQ_TOPIC_REPLICATION_FACTOR_CONFIG);}
f10180
0
commit
private voidf10189_1WorkerSourceTask workerTask)
{        try {        if (workerTask.commitOffsets()) {            return;        }            } catch (Throwable t) {        // We're very careful about exceptions here since any uncaught exceptions in the commit        // thread would cause the fixed interval schedule on the ExecutorService to stop running        // for that task            }}
private voidf10189
1
start
public synchronized voidf10190_1)
{        startServices();    }
public synchronized voidf10190
1
deleteConnectorConfig
public synchronized void kafkatest_f10199_0(String connName, Callback<Created<ConnectorInfo>> callback)
{    try {        if (!configState.contains(connName)) {            // Deletion, must already exist            callback.onCompletion(new NotFoundException("Connector " + connName + " not found", null), null);            return;        }        removeConnectorTasks(connName);        worker.stopConnector(connName);        configBackingStore.removeConnectorConfig(connName);        onDeletion(connName);        callback.onCompletion(null, new Created<ConnectorInfo>(false, null));    } catch (ConnectException e) {        callback.onCompletion(e, null);    }}
f10199
0
putConnectorConfig
public synchronized void kafkatest_f10200_0(String connName, final Map<String, String> config, boolean allowReplace, final Callback<Created<ConnectorInfo>> callback)
{    try {        if (maybeAddConfigErrors(validateConnectorConfig(config), callback)) {            return;        }        boolean created = false;        if (configState.contains(connName)) {            if (!allowReplace) {                callback.onCompletion(new AlreadyExistsException("Connector " + connName + " already exists"), null);                return;            }            worker.stopConnector(connName);        } else {            created = true;        }        configBackingStore.putConnectorConfig(connName, config);        if (!startConnector(connName)) {            callback.onCompletion(new ConnectException("Failed to start connector: " + connName), null);            return;        }        updateConnectorTasks(connName);        callback.onCompletion(null, new Created<>(created, createConnectorInfo(connName)));    } catch (ConnectException e) {        callback.onCompletion(e, null);    }}
f10200
0
recomputeTaskConfigs
private List<Map<String, String>> kafkatest_f10209_0(String connName)
{    Map<String, String> config = configState.connectorConfig(connName);    ConnectorConfig connConfig = worker.isSinkConnector(connName) ? new SinkConnectorConfig(plugins(), config) : new SourceConnectorConfig(plugins(), config);    return worker.connectorTaskConfigs(connName, connConfig);}
f10209
0
createConnectorTasks
private void kafkatest_f10210_0(String connName, TargetState initialState)
{    Map<String, String> connConfigs = configState.connectorConfig(connName);    for (ConnectorTaskId taskId : configState.tasks(connName)) {        Map<String, String> taskConfigMap = configState.taskConfig(taskId);        worker.startTask(taskId, configState, connConfigs, taskConfigMap, this, initialState);    }}
f10210
0
hashCode
public int kafkatest_f10219_0()
{    return Objects.hash(seq);}
f10219
0
changeState
public synchronized void kafkatest_f10220_0(State newState, long now)
{    // JDK8: remove synchronization by using lastState.getAndUpdate(oldState->oldState.newState(newState, now));    lastState.set(lastState.get().newState(newState, now));}
f10220
0
toString
public String kafkatest_f10229_0()
{    StringJoiner chain = new StringJoiner(", ", getClass().getName() + "{", "}");    for (Transformation<R> transformation : transformations) {        chain.add(transformation.getClass().getName());    }    return chain.toString();}
f10229
0
initConfigTransformer
private WorkerConfigTransformer kafkatest_f10230_0()
{    final List<String> providerNames = config.getList(WorkerConfig.CONFIG_PROVIDERS_CONFIG);    Map<String, ConfigProvider> providerMap = new HashMap<>();    for (String providerName : providerNames) {        ConfigProvider configProvider = plugins.newConfigProvider(config, WorkerConfig.CONFIG_PROVIDERS_CONFIG + "." + providerName, ClassLoaderUsage.PLUGINS);        providerMap.put(providerName, configProvider);    }    return new WorkerConfigTransformer(this, providerMap);}
f10230
0
stopConnector
public booleanf10239_1String connName)
{    try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {                WorkerConnector workerConnector = connectors.remove(connName);        if (workerConnector == null) {                        return false;        }        ClassLoader savedLoader = plugins.currentThreadLoader();        try {            savedLoader = plugins.compareAndSwapLoaders(workerConnector.connector());            workerConnector.shutdown();        } finally {            Plugins.compareAndSwapLoaders(savedLoader);        }            }    return true;}
public booleanf10239
1
connectorNames
public Set<String> kafkatest_f10240_0()
{    return connectors.keySet();}
f10240
0
sinkTaskReporters
private List<ErrorReporter> kafkatest_f10249_0(ConnectorTaskId id, SinkConnectorConfig connConfig, ErrorHandlingMetrics errorHandlingMetrics, Class<? extends Connector> connectorClass)
{    ArrayList<ErrorReporter> reporters = new ArrayList<>();    LogReporter logReporter = new LogReporter(id, connConfig, errorHandlingMetrics);    reporters.add(logReporter);    // check if topic for dead letter queue exists    String topic = connConfig.dlqTopicName();    if (topic != null && !topic.isEmpty()) {        Map<String, Object> producerProps = producerConfigs(id, "connector-dlq-producer-" + id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);        Map<String, Object> adminProps = adminConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);        DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(adminProps, id, connConfig, producerProps, errorHandlingMetrics);        reporters.add(reporter);    }    return reporters;}
f10249
0
sourceTaskReporters
private List<ErrorReporter> kafkatest_f10250_0(ConnectorTaskId id, ConnectorConfig connConfig, ErrorHandlingMetrics errorHandlingMetrics)
{    List<ErrorReporter> reporters = new ArrayList<>();    LogReporter logReporter = new LogReporter(id, connConfig, errorHandlingMetrics);    reporters.add(logReporter);    return reporters;}
f10250
0
getInternalKeyConverter
public Converter kafkatest_f10259_0()
{    return internalKeyConverter;}
f10259
0
getInternalValueConverter
public Converter kafkatest_f10260_0()
{    return internalValueConverter;}
f10260
0
close
 void kafkatest_f10269_0()
{    metricGroup.close();}
f10269
0
recordConnectorStartupFailure
 void kafkatest_f10270_0()
{    connectorStartupAttempts.record(1.0);    connectorStartupFailures.record(1.0);    connectorStartupResults.record(0.0);}
f10270
0
postProcessParsedConfig
protected Map<String, Object> kafkatest_f10279_0(final Map<String, Object> parsedValues)
{    return CommonClientConfigs.postProcessReconnectBackoffConfigs(this, parsedValues);}
f10279
0
pluginLocations
public static List<String> kafkatest_f10280_0(Map<String, String> props)
{    String locationList = props.get(WorkerConfig.PLUGIN_PATH_CONFIG);    return locationList == null ? new ArrayList<String>() : Arrays.asList(COMMA_WITH_WHITESPACE.split(locationList.trim(), -1));}
f10280
0
doStart
private booleanf10289_1)
{    try {        switch(state) {            case STARTED:                return false;            case INIT:            case STOPPED:                connector.start(config);                this.state = State.STARTED;                return true;            default:                throw new IllegalArgumentException("Cannot start connector in state " + state);        }    } catch (Throwable t) {                onFailure(t);        return false;    }}
private booleanf10289
1
onFailure
private void kafkatest_f10290_0(Throwable t)
{    statusListener.onFailure(connName, t);    this.state = State.FAILED;}
f10290
0
connectorType
protected String kafkatest_f10299_0()
{    if (isSinkConnector())        return "sink";    if (isSourceConnector())        return "source";    return "unknown";}
f10299
0
connector
public Connector kafkatest_f10300_0()
{    return connector;}
f10300
0
onFailure
public void kafkatest_f10309_0(String connector, Throwable cause)
{    state = AbstractStatus.State.FAILED;    delegate.onFailure(connector, cause);}
f10309
0
onDeletion
public void kafkatest_f10310_0(String connector)
{    state = AbstractStatus.State.DESTROYED;    delegate.onDeletion(connector);}
f10310
0
addSystemInfo
protected void kafkatest_f10319_0()
{    String[] osInfo = { OS.getName(), OS.getArch(), OS.getVersion() };    values.put("os.spec", Utils.join(osInfo, ", "));    values.put("os.vcpus", String.valueOf(OS.getAvailableProcessors()));}
f10319
0
initialize
public voidf10320_1TaskConfig taskConfig)
{    try {        this.taskConfig = taskConfig.originalsStrings();        this.context = new WorkerSinkTaskContext(consumer, this, configState);    } catch (Throwable t) {                onFailure(t);    }}
public voidf10320
1
initializeAndStart
protected voidf10329_1)
{    SinkConnectorConfig.validate(taskConfig);    if (SinkConnectorConfig.hasTopicsConfig(taskConfig)) {        String[] topics = taskConfig.get(SinkTask.TOPICS_CONFIG).split(",");        consumer.subscribe(Arrays.asList(topics), new HandleRebalance());            } else {        String topicsRegexStr = taskConfig.get(SinkTask.TOPICS_REGEX_CONFIG);        Pattern pattern = Pattern.compile(topicsRegexStr);        consumer.subscribe(pattern, new HandleRebalance());            }    task.initialize(context);    task.start(taskConfig);    }
protected voidf10329
1
poll
protected void kafkatest_f10330_0(long timeoutMs)
{    rewind();    long retryTimeout = context.timeout();    if (retryTimeout > 0) {        timeoutMs = Math.min(timeoutMs, retryTimeout);        context.timeout(-1L);    }    log.trace("{} Polling consumer with timeout {} ms", this, timeoutMs);    ConsumerRecords<byte[], byte[]> msgs = pollConsumer(timeoutMs);    assert messageBatch.isEmpty() || msgs.isEmpty();    log.trace("{} Polling returned {} messages", this, msgs.count());    convertMessages(msgs);    deliverMessages();}
f10330
0
convertMessages
private void kafkatest_f10339_0(ConsumerRecords<byte[], byte[]> msgs)
{    origOffsets.clear();    for (ConsumerRecord<byte[], byte[]> msg : msgs) {        log.trace("{} Consuming and converting message in topic '{}' partition {} at offset {} and timestamp {}", this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp());        retryWithToleranceOperator.consumerRecord(msg);        SinkRecord transRecord = convertAndTransformRecord(msg);        origOffsets.put(new TopicPartition(msg.topic(), msg.partition()), new OffsetAndMetadata(msg.offset() + 1));        if (transRecord != null) {            messageBatch.add(transRecord);        } else {            log.trace("{} Converters and transformations returned null, possibly because of too many retries, so " + "dropping record in topic '{}' partition {} at offset {}", this, msg.topic(), msg.partition(), msg.offset());        }    }    sinkTaskMetricsGroup.recordConsumedOffsets(origOffsets);}
f10339
0
convertAndTransformRecord
private SinkRecord kafkatest_f10340_0(final ConsumerRecord<byte[], byte[]> msg)
{    SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(() -> keyConverter.toConnectData(msg.topic(), msg.key()), Stage.KEY_CONVERTER, keyConverter.getClass());    SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(() -> valueConverter.toConnectData(msg.topic(), msg.value()), Stage.VALUE_CONVERTER, valueConverter.getClass());    Headers headers = retryWithToleranceOperator.execute(() -> convertHeadersFor(msg), Stage.HEADER_CONVERTER, headerConverter.getClass());    if (retryWithToleranceOperator.failed()) {        return null;    }    Long timestamp = ConnectUtils.checkAndConvertTimestamp(msg.timestamp());    SinkRecord origRecord = new SinkRecord(msg.topic(), msg.partition(), keyAndSchema.schema(), keyAndSchema.value(), valueAndSchema.schema(), valueAndSchema.value(), msg.offset(), timestamp, msg.timestampType(), headers);    log.trace("{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}", this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());    return transformationChain.apply(origRecord);}
f10340
0
recordCommitFailure
protected void kafkatest_f10349_0(long duration, Throwable error)
{    super.recordCommitFailure(duration, error);}
f10349
0
recordCommitSuccess
protected void kafkatest_f10350_0(long duration)
{    super.recordCommitSuccess(duration);    sinkTaskMetricsGroup.recordOffsetCommitSuccess();}
f10350
0
recordPut
 void kafkatest_f10359_0(long duration)
{    putBatchTime.record(duration);}
f10359
0
recordPartitionCount
 void kafkatest_f10360_0(int assignedPartitionCount)
{    partitionCount.record(assignedPartitionCount);}
f10360
0
configs
public Map<String, String> kafkatest_f10369_0()
{    return configState.taskConfig(sinkTask.id());}
f10369
0
offset
public voidf10370_1Map<TopicPartition, Long> offsets)
{        this.offsets.putAll(offsets);}
public voidf10370
1
pausedPartitions
public Set<TopicPartition> kafkatest_f10379_0()
{    return pausedPartitions;}
f10379
0
requestCommit
public voidf10380_1)
{        commitRequested = true;}
public voidf10380
1
execute
public voidf10389_1)
{    try {        task.initialize(new WorkerSourceTaskContext(offsetReader, this, configState));        task.start(taskConfig);                synchronized (this) {            if (startedShutdownBeforeStartCompleted) {                tryStop();                return;            }            finishedStart = true;        }        while (!isStopping()) {            if (shouldPause()) {                onPause();                if (awaitUnpause()) {                    onResume();                }                continue;            }            maybeThrowProducerSendException();            if (toSend == null) {                log.trace("{} Nothing to send to Kafka. Polling source for additional records", this);                long start = time.milliseconds();                toSend = poll();                if (toSend != null) {                    recordPollReturned(toSend.size(), time.milliseconds() - start);                }            }            if (toSend == null)                continue;            log.trace("{} About to send {} records to Kafka", this, toSend.size());            if (!sendRecords())                stopRequestedLatch.await(SEND_FAILED_BACKOFF_MS, TimeUnit.MILLISECONDS);        }    } catch (InterruptedException e) {    // Ignore and allow to exit.    } finally {        // It should still be safe to commit offsets since any exception would have        // simply resulted in not getting more records but all the existing records should be ok to flush        // and commit offsets. Worst case, task.flush() will also throw an exception causing the offset commit        // to fail.        commitOffsets();    }}
public voidf10389
1
maybeThrowProducerSendException
private void kafkatest_f10390_0()
{    if (producerSendException.get() != null) {        throw new ConnectException("Unrecoverable exception from producer send callback", producerSendException.get());    }}
f10390
0
onCompletion
public voidf10399_1Throwable error, Void result)
{    if (error != null) {            } else {        log.trace("{} Finished flushing offsets to storage", WorkerSourceTask.this);    }}
public voidf10399
1
commitSourceTask
private voidf10400_1)
{    try {        this.task.commit();    } catch (Throwable t) {            }}
private voidf10400
1
finishedAllWrites
private void kafkatest_f10409_0()
{    if (!completed) {        metricsGroup.recordWrite(batchSize - counter);        completed = true;    }}
f10409
0
close
 void kafkatest_f10410_0()
{    metricGroup.close();}
f10410
0
stop
public void kafkatest_f10419_0()
{    triggerStop();}
f10419
0
cancel
public void kafkatest_f10420_0()
{    cancelled = true;}
f10420
0
run
public void kafkatest_f10429_0()
{    // Clear all MDC parameters, in case this thread is being reused    LoggingContext.clear();    try (LoggingContext loggingContext = LoggingContext.forTask(id())) {        ClassLoader savedLoader = Plugins.compareAndSwapLoaders(loader);        String savedName = Thread.currentThread().getName();        try {            Thread.currentThread().setName(THREAD_NAME_PREFIX + id);            doRun();            onShutdown();        } catch (Throwable t) {            onFailure(t);            if (t instanceof Error)                throw (Error) t;        } finally {            try {                Thread.currentThread().setName(savedName);                Plugins.compareAndSwapLoaders(savedLoader);                shutdownLatch.countDown();            } finally {                try {                    releaseResources();                } finally {                    taskMetricsGroup.close();                }            }        }    }}
f10429
0
shouldPause
public boolean kafkatest_f10430_0()
{    return this.targetState == TargetState.PAUSED;}
f10430
0
measure
public double kafkatest_f10439_0(MetricConfig config, long now)
{    return taskStateTimer.durationRatio(matchingState, now);}
f10439
0
close
 void kafkatest_f10440_0()
{    metricGroup.close();}
f10440
0
state
public State kafkatest_f10449_0()
{    return taskStateTimer.currentState();}
f10449
0
metricGroup
protected MetricGroup kafkatest_f10450_0()
{    return metricGroup;}
f10450
0
COMMIT_TASKS_KEY
public static String kafkatest_f10459_0(String connectorName)
{    return COMMIT_TASKS_PREFIX + connectorName;}
f10459
0
setUpdateListener
public void kafkatest_f10460_0(UpdateListener listener)
{    this.updateListener = listener;}
f10460
0
putTaskConfigs
public voidf10469_1String connector, List<Map<String, String>> configs)
{    // any outstanding lagging data to consume.    try {        configLog.readToEnd().get(READ_TO_END_TIMEOUT_MS, TimeUnit.MILLISECONDS);    } catch (InterruptedException | ExecutionException | TimeoutException e) {                throw new ConnectException("Error writing root configuration to Kafka", e);    }    int taskCount = configs.size();    // Start sending all the individual updates    int index = 0;    for (Map<String, String> taskConfig : configs) {        Struct connectConfig = new Struct(TASK_CONFIGURATION_V0);        connectConfig.put("properties", taskConfig);        byte[] serializedConfig = converter.fromConnectData(topic, TASK_CONFIGURATION_V0, connectConfig);                ConnectorTaskId connectorTaskId = new ConnectorTaskId(connector, index);        configLog.send(TASK_KEY(connectorTaskId), serializedConfig);        index++;    }    // the end of the log    try {        // Read to end to ensure all the task configs have been written        if (taskCount > 0) {            configLog.readToEnd().get(READ_TO_END_TIMEOUT_MS, TimeUnit.MILLISECONDS);        }        // Write the commit message        Struct connectConfig = new Struct(CONNECTOR_TASKS_COMMIT_V0);        connectConfig.put("tasks", taskCount);        byte[] serializedConfig = converter.fromConnectData(topic, CONNECTOR_TASKS_COMMIT_V0, connectConfig);                configLog.send(COMMIT_TASKS_KEY(connector), serializedConfig);        // Read to end to ensure all the commit messages have been written        configLog.readToEnd().get(READ_TO_END_TIMEOUT_MS, TimeUnit.MILLISECONDS);    } catch (InterruptedException | ExecutionException | TimeoutException e) {                throw new ConnectException("Error writing root configuration to Kafka", e);    }}
public voidf10469
1
refresh
public void kafkatest_f10470_0(long timeout, TimeUnit unit) throws TimeoutException
{    try {        configLog.readToEnd().get(timeout, unit);    } catch (InterruptedException | ExecutionException e) {        throw new ConnectException("Error trying to read to end of config log", e);    }}
f10470
0
intValue
private static int kafkatest_f10479_0(Object value)
{    if (value instanceof Integer)        return (int) value;    else if (value instanceof Long)        return (int) (long) value;    else        throw new ConnectException("Expected integer value to be either Integer or Long");}
f10479
0
configure
public void kafkatest_f10480_0(final WorkerConfig config)
{    String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);    if (topic == null || topic.trim().length() == 0)        throw new ConfigException("Offset storage topic must be specified");    data = new HashMap<>();    Map<String, Object> originals = config.originals();    Map<String, Object> producerProps = new HashMap<>(originals);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());    producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);    Map<String, Object> consumerProps = new HashMap<>(originals);    consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());    consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());    Map<String, Object> adminProps = new HashMap<>(originals);    NewTopic topicDescription = TopicAdmin.defineTopic(topic).compacted().partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).build();    offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);}
f10480
0
onCompletion
public synchronized void kafkatest_f10489_0(RecordMetadata metadata, Exception exception)
{    if (exception != null) {        if (!completed) {            this.exception = exception;            callback.onCompletion(exception, null);            completed = true;            this.notify();        }        return;    }    numLeft -= 1;    if (numLeft == 0) {        callback.onCompletion(null, null);        completed = true;        this.notify();    }}
f10489
0
cancel
public synchronized boolean kafkatest_f10490_0(boolean mayInterruptIfRunning)
{    return false;}
f10490
0
start
public void kafkatest_f10499_0()
{    kafkaLog.start();    // read to the end on startup to ensure that api requests see the most recent states    kafkaLog.readToEnd();}
f10499
0
stop
public void kafkatest_f10500_0()
{    kafkaLog.stop();}
f10500
0
onCompletion
public voidf10509_1RecordMetadata metadata, Exception exception)
{    if (exception == null)        return;    if (exception instanceof RetriableException) {        synchronized (KafkaStatusBackingStore.this) {            if (entry.isDeleted() || status.generation() != generation || (safeWrite && !entry.canWriteSafely(status, sequence)))                return;        }        kafkaLog.send(key, value, this);    } else {            }}
public voidf10509
1
getOrAdd
private synchronized CacheEntry<ConnectorStatus> kafkatest_f10510_0(String connector)
{    CacheEntry<ConnectorStatus> entry = connectors.get(connector);    if (entry == null) {        entry = new CacheEntry<>();        connectors.put(connector, entry);    }    return entry;}
f10510
0
parseTaskStatus
private TaskStatusf10519_1ConnectorTaskId taskId, byte[] data)
{    try {        SchemaAndValue schemaAndValue = converter.toConnectData(topic, data);        if (!(schemaAndValue.value() instanceof Map)) {                        return null;        }        @SuppressWarnings("unchecked")        Map<String, Object> statusMap = (Map<String, Object>) schemaAndValue.value();        TaskStatus.State state = TaskStatus.State.valueOf((String) statusMap.get(STATE_KEY_NAME));        String trace = (String) statusMap.get(TRACE_KEY_NAME);        String workerUrl = (String) statusMap.get(WORKER_ID_KEY_NAME);        int generation = ((Long) statusMap.get(GENERATION_KEY_NAME)).intValue();        return new TaskStatus(taskId, state, workerUrl, generation, trace);    } catch (Exception e) {                return null;    }}
private TaskStatusf10519
1
serialize
private byte[] kafkatest_f10520_0(AbstractStatus status)
{    Struct struct = new Struct(STATUS_SCHEMA_V0);    struct.put(STATE_KEY_NAME, status.state().name());    if (status.trace() != null)        struct.put(TRACE_KEY_NAME, status.trace());    struct.put(WORKER_ID_KEY_NAME, status.workerId());    struct.put(GENERATION_KEY_NAME, status.generation());    return converter.fromConnectData(topic, STATUS_SCHEMA_V0, struct);}
f10520
0
delete
public void kafkatest_f10529_0()
{    this.deleted = true;}
f10529
0
isDeleted
public boolean kafkatest_f10530_0()
{    return deleted;}
f10530
0
putTargetState
public synchronized void kafkatest_f10542_0(String connector, TargetState state)
{    ConnectorState connectorState = connectors.get(connector);    if (connectorState == null)        throw new IllegalArgumentException("No connector `" + connector + "` configured");    connectorState.targetState = state;    if (updateListener != null)        updateListener.onConnectorTargetStateChange(connector);}
f10542
0
setUpdateListener
public synchronized void kafkatest_f10543_0(UpdateListener listener)
{    this.updateListener = listener;}
f10543
0
putSafe
public synchronized void kafkatest_f10557_0(ConnectorStatus status)
{    put(status);}
f10557
0
put
public synchronized void kafkatest_f10558_0(TaskStatus status)
{    if (status.state() == TaskStatus.State.DESTROYED)        tasks.remove(status.id().connector(), status.id().task());    else        tasks.put(status.id().connector(), status.id().task(), status);}
f10558
0
flushing
private boolean kafkatest_f10568_0()
{    return toFlush != null;}
f10568
0
beginFlush
public synchronized booleanf10569_1)
{    if (flushing()) {                throw new ConnectException("OffsetStorageWriter is already flushing");    }    if (data.isEmpty())        return false;    assert !flushing();    toFlush = data;    data = new HashMap<>();    return true;}
public synchronized booleanf10569
1
run
public voidf10578_1)
{        context.raiseError(new RuntimeException());}
public voidf10578
1
taskClass
public Class<? extends Task> kafkatest_f10579_0()
{    throw new UnsupportedOperationException();}
f10579
0
start
public void kafkatest_f10588_0(Map<String, String> props)
{    delegate.start(props);}
f10588
0
taskClass
public Class<? extends Task> kafkatest_f10589_0()
{    return MockSinkTask.class;}
f10589
0
initialize
public void kafkatest_f10600_0(ConnectorContext ctx, List<Map<String, String>> taskConfigs)
{    delegate.initialize(ctx, taskConfigs);}
f10600
0
reconfigure
public void kafkatest_f10601_0(Map<String, String> props)
{    delegate.reconfigure(props);}
f10601
0
start
public voidf10610_1Map<String, String> config)
{    this.mockMode = config.get(MockConnector.MOCK_MODE_KEY);    if (MockConnector.TASK_FAILURE.equals(mockMode)) {        this.startTimeMs = System.currentTimeMillis();        String delayMsString = config.get(MockConnector.DELAY_MS_KEY);        this.failureDelayMs = MockConnector.DEFAULT_FAILURE_DELAY_MS;        if (delayMsString != null)            failureDelayMs = Long.parseLong(delayMsString);            }}
public voidf10610
1
poll
public List<SourceRecord>f10611_1) throws InterruptedException
{    if (MockConnector.TASK_FAILURE.equals(mockMode)) {        long now = System.currentTimeMillis();        if (now > startTimeMs + failureDelayMs) {                        throw new RuntimeException();        }    }    return Collections.emptyList();}
public List<SourceRecord>f10611
1
stop
public void kafkatest_f10622_0()
{    throttler.wakeup();}
f10622
0
printTransformationHtml
private static void kafkatest_f10623_0(PrintStream out, DocInfo docInfo)
{    out.println("<div id=\"" + docInfo.transformationName + "\">");    out.print("<h5>");    out.print(docInfo.transformationName);    out.println("</h5>");    out.println(docInfo.overview);    out.println("<p/>");    out.println(docInfo.configDef.toHtml());    out.println("</div>");}
f10623
0
start
public void kafkatest_f10633_0(Map<String, String> props)
{    try {        name = props.get(NAME_CONFIG);        id = Integer.parseInt(props.get(ID_CONFIG));    } catch (NumberFormatException e) {        throw new ConnectException("Invalid VerifiableSourceTask configuration", e);    }}
f10633
0
put
public void kafkatest_f10634_0(Collection<SinkRecord> records)
{    long nowMs = System.currentTimeMillis();    for (SinkRecord record : records) {        Map<String, Object> data = new HashMap<>();        data.put("name", name);        // VerifiableSourceTask's input task (source partition)        data.put("task", record.key());        data.put("sinkTask", id);        data.put("topic", record.topic());        data.put("time_ms", nowMs);        data.put("seqno", record.value());        data.put("offset", record.kafkaOffset());        String dataJson;        try {            dataJson = JSON_SERDE.writeValueAsString(data);        } catch (JsonProcessingException e) {            dataJson = "Bad data can't be written as json: " + e.getMessage();        }        System.out.println(dataJson);        unflushed.add(data);    }}
f10634
0
poll
public List<SourceRecord> kafkatest_f10645_0() throws InterruptedException
{    long sendStartMs = System.currentTimeMillis();    if (throttler.shouldThrottle(seqno - startingSeqno, sendStartMs))        throttler.throttle();    long nowMs = System.currentTimeMillis();    Map<String, Object> data = new HashMap<>();    data.put("name", name);    data.put("task", id);    data.put("topic", this.topic);    data.put("time_ms", nowMs);    data.put("seqno", seqno);    String dataJson;    try {        dataJson = JSON_SERDE.writeValueAsString(data);    } catch (JsonProcessingException e) {        dataJson = "Bad data can't be written as json: " + e.getMessage();    }    System.out.println(dataJson);    Map<String, Long> ccOffset = Collections.singletonMap(SEQNO_FIELD, seqno);    SourceRecord srcRecord = new SourceRecord(partition, ccOffset, topic, Schema.INT32_SCHEMA, id, Schema.INT64_SCHEMA, seqno);    List<SourceRecord> result = Collections.singletonList(srcRecord);    seqno++;    return result;}
f10645
0
commitRecord
public void kafkatest_f10646_0(SourceRecord record) throws InterruptedException
{    Map<String, Object> data = new HashMap<>();    data.put("name", name);    data.put("task", id);    data.put("topic", this.topic);    data.put("time_ms", System.currentTimeMillis());    data.put("seqno", record.value());    data.put("committed", true);    String dataJson;    try {        dataJson = JSON_SERDE.writeValueAsString(data);    } catch (JsonProcessingException e) {        dataJson = "Bad data can't be written as json: " + e.getMessage();    }    System.out.println(dataJson);}
f10646
0
lookupKafkaClusterId
public static Stringf10655_1WorkerConfig config)
{        try (Admin adminClient = Admin.create(config.originals())) {        return lookupKafkaClusterId(adminClient);    }}
public static Stringf10655
1
lookupKafkaClusterId
 static Stringf10656_1Admin adminClient)
{        try {        KafkaFuture<String> clusterIdFuture = adminClient.describeCluster().clusterId();        if (clusterIdFuture == null) {                        return null;        }                String kafkaClusterId = clusterIdFuture.get();                return kafkaClusterId;    } catch (InterruptedException e) {        throw new ConnectException("Unexpectedly interrupted when looking up Kafka cluster info", e);    } catch (ExecutionException e) {        throw new ConnectException("Failed to connect to and describe Kafka cluster. " + "Check worker's broker connection and security properties.", e);    }}
 static Stringf10656
1
start
public voidf10666_1)
{        initializer.run();    producer = createProducer();    consumer = createConsumer();    List<TopicPartition> partitions = new ArrayList<>();    // We expect that the topics will have been created either manually by the user or automatically by the herder    List<PartitionInfo> partitionInfos = null;    long started = time.milliseconds();    while (partitionInfos == null && time.milliseconds() - started < CREATE_TOPIC_TIMEOUT_MS) {        partitionInfos = consumer.partitionsFor(topic);        Utils.sleep(Math.min(time.milliseconds() - started, 1000));    }    if (partitionInfos == null)        throw new ConnectException("Could not look up partition metadata for offset backing store topic in" + " allotted period. This could indicate a connectivity issue, unavailable topic partitions, or if" + " this is your first use of the topic it may have taken too long to create.");    for (PartitionInfo partition : partitionInfos) partitions.add(new TopicPartition(partition.topic(), partition.partition()));    consumer.assign(partitions);    // Always consume from the beginning of all partitions. Necessary to ensure that we don't use committed offsets    // when a 'group.id' is specified (if offsets happen to have been committed unexpectedly).    consumer.seekToBeginning(partitions);    readToLogEnd();    thread = new WorkThread();    thread.start();        }
public voidf10666
1
stop
public voidf10667_1)
{        synchronized (this) {        stopRequested = true;    }    consumer.wakeup();    try {        thread.join();    } catch (InterruptedException e) {        throw new ConnectException("Failed to stop KafkaBasedLog. Exiting without cleanly shutting " + "down it's producer and consumer.", e);    }    try {        producer.close();    } catch (KafkaException e) {            }    try {        consumer.close();    } catch (KafkaException e) {            }    }
public voidf10667
1
readToLogEnd
private void kafkatest_f10676_0()
{    log.trace("Reading to end of offset log");    Set<TopicPartition> assignment = consumer.assignment();    Map<TopicPartition, Long> endOffsets = consumer.endOffsets(assignment);    log.trace("Reading to end of log offsets {}", endOffsets);    while (!endOffsets.isEmpty()) {        Iterator<Map.Entry<TopicPartition, Long>> it = endOffsets.entrySet().iterator();        while (it.hasNext()) {            Map.Entry<TopicPartition, Long> entry = it.next();            if (consumer.position(entry.getKey()) >= entry.getValue())                it.remove();            else {                poll(Integer.MAX_VALUE);                break;            }        }    }}
f10676
0
run
public voidf10677_1)
{    try {        log.trace("{} started execution", this);        while (true) {            int numCallbacks;            synchronized (KafkaBasedLog.this) {                if (stopRequested)                    break;                numCallbacks = readLogEndOffsetCallbacks.size();            }            if (numCallbacks > 0) {                try {                    readToLogEnd();                    log.trace("Finished read to end log for topic {}", topic);                } catch (TimeoutException e) {                                        continue;                } catch (WakeupException e) {                    // called. Both are handled by restarting this loop.                    continue;                }            }            synchronized (KafkaBasedLog.this) {                // since it is possible for another write + readToEnd to sneak in the meantime                for (int i = 0; i < numCallbacks; i++) {                    Callback<Void> cb = readLogEndOffsetCallbacks.poll();                    cb.onCompletion(null, null);                }            }            try {                poll(Integer.MAX_VALUE);            } catch (WakeupException e) {                // See previous comment, both possible causes of this wakeup are handled by starting this loop again                continue;            }        }    } catch (Throwable t) {            }}
public voidf10677
1
registerUrlTypes
public static void kafkatest_f10686_0()
{    final List<UrlType> urlTypes = new LinkedList<>();    urlTypes.add(new EmptyUrlType(ENDINGS));    urlTypes.addAll(Arrays.asList(Vfs.DefaultUrlTypes.values()));    Vfs.setDefaultURLTypes(urlTypes);}
f10686
0
matches
public boolean kafkatest_f10687_0(URL url)
{    final String protocol = url.getProtocol();    final String externalForm = url.toExternalForm();    if (!protocol.equals(FILE_PROTOCOL)) {        return false;    }    for (String ending : endings) {        if (externalForm.endsWith(ending)) {            return true;        }    }    return false;}
f10687
0
shutdown
public void kafkatest_f10697_0(long gracefulTimeout, TimeUnit unit) throws InterruptedException
{    boolean success = gracefulShutdown(gracefulTimeout, unit);    if (!success)        forceShutdown();}
f10697
0
gracefulShutdown
public boolean kafkatest_f10698_0(long timeout, TimeUnit unit) throws InterruptedException
{    startGracefulShutdown();    return awaitShutdown(timeout, unit);}
f10698
0
row
public Map<C, V> kafkatest_f10707_0(R row)
{    Map<C, V> columns = table.get(row);    if (columns == null)        return Collections.emptyMap();    return Collections.unmodifiableMap(columns);}
f10707
0
partitions
public NewTopicBuilder kafkatest_f10708_0(int numPartitions)
{    this.numPartitions = numPartitions;    return this;}
f10708
0
createTopics
public Set<String>f10717_1NewTopic... topics)
{    Map<String, NewTopic> topicsByName = new HashMap<>();    if (topics != null) {        for (NewTopic topic : topics) {            if (topic != null)                topicsByName.put(topic.name(), topic);        }    }    if (topicsByName.isEmpty())        return Collections.emptySet();    String bootstrapServers = bootstrapServers();    String topicNameList = Utils.join(topicsByName.keySet(), "', '");    // Attempt to create any missing topics    CreateTopicsOptions args = new CreateTopicsOptions().validateOnly(false);    Map<String, KafkaFuture<Void>> newResults = admin.createTopics(topicsByName.values(), args).values();    // Iterate over each future so that we can handle individual failures like when some topics already exist    Set<String> newlyCreatedTopicNames = new HashSet<>();    for (Map.Entry<String, KafkaFuture<Void>> entry : newResults.entrySet()) {        String topic = entry.getKey();        try {            entry.getValue().get();                        newlyCreatedTopicNames.add(topic);        } catch (ExecutionException e) {            Throwable cause = e.getCause();            if (cause instanceof TopicExistsException) {                                continue;            }            if (cause instanceof UnsupportedVersionException) {                                return Collections.emptySet();            }            if (cause instanceof ClusterAuthorizationException) {                                return Collections.emptySet();            }            if (cause instanceof TopicAuthorizationException) {                                return Collections.emptySet();            }            if (cause instanceof TimeoutException) {                // Timed out waiting for the operation to complete                throw new ConnectException("Timed out while checking for or creating topic(s) '" + topicNameList + "'." + " This could indicate a connectivity issue, unavailable topic partitions, or if" + " this is your first use of the topic it may have taken too long to create.", cause);            }            throw new ConnectException("Error while attempting to create/find topic(s) '" + topicNameList + "'", e);        } catch (InterruptedException e) {            Thread.interrupted();            throw new ConnectException("Interrupted while attempting to create/find topic(s) '" + topicNameList + "'", e);        }    }    return newlyCreatedTopicNames;}
public Set<String>f10717
1
close
public void kafkatest_f10718_0()
{    admin.close();}
f10718
0
policyToTest
protected ConnectorClientConfigOverridePolicy kafkatest_f10727_0()
{    return noneConnectorClientConfigOverridePolicy;}
f10727
0
testPrincipalOnly
public void kafkatest_f10728_0()
{    Map<String, Object> clientConfig = Collections.singletonMap(SaslConfigs.SASL_JAAS_CONFIG, "test");    testValidOverride(clientConfig);}
f10728
0
testToConnect
public void kafkatest_f10737_0()
{    SchemaAndValue data = converter.toConnectData(TOPIC, SAMPLE_BYTES);    assertEquals(Schema.OPTIONAL_BYTES_SCHEMA, data.schema());    assertTrue(Arrays.equals(SAMPLE_BYTES, (byte[]) data.value()));}
f10737
0
testToConnectNull
public void kafkatest_f10738_0()
{    SchemaAndValue data = converter.toConnectData(TOPIC, null);    assertEquals(Schema.OPTIONAL_BYTES_SCHEMA, data.schema());    assertNull(data.value());}
f10738
0
samples
public Integer[] kafkatest_f10747_0()
{    return new Integer[] { Integer.MIN_VALUE, 1234, Integer.MAX_VALUE };}
f10747
0
schema
protected Schema kafkatest_f10748_0()
{    return Schema.OPTIONAL_INT32_SCHEMA;}
f10748
0
testDeserializingDataWithTooManyBytes
public void kafkatest_f10757_0()
{    converter.toConnectData(TOPIC, new byte[10]);}
f10757
0
testDeserializingHeaderWithTooManyBytes
public void kafkatest_f10758_0()
{    converter.toConnectHeader(TOPIC, HEADER_NAME, new byte[10]);}
f10758
0
testCreateWithOverridesForNonePolicy
public void kafkatest_f10768_0() throws Exception
{    Map<String, String> props = basicConnectorConfig();    props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + SaslConfigs.SASL_JAAS_CONFIG, "sasl");    assertFailCreateConnector("None", props);}
f10768
0
testCreateWithNotAllowedOverridesForPrincipalPolicy
public void kafkatest_f10769_0() throws Exception
{    Map<String, String> props = basicConnectorConfig();    props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + SaslConfigs.SASL_JAAS_CONFIG, "sasl");    props.put(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX + ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");    assertFailCreateConnector("Principal", props);}
f10769
0
tasks
public Collection<TaskHandle> kafkatest_f10778_0()
{    return taskHandles.values();}
f10778
0
deleteTask
public voidf10779_1String taskId)
{        taskHandles.remove(taskId);}
public voidf10779
1
recordConnectorStart
public void kafkatest_f10788_0()
{    startAndStopCounter.recordStart();}
f10788
0
recordConnectorStop
public void kafkatest_f10789_0()
{    startAndStopCounter.recordStop();}
f10789
0
assertWorkersUp
private Optional<Boolean>f10798_1int numWorkers)
{    try {        int numUp = connect.activeWorkers().size();        return Optional.of(numUp >= numWorkers);    } catch (Exception e) {                return Optional.empty();    }}
private Optional<Boolean>f10798
1
assertConnectorAndTasksRunning
private Optional<Boolean>f10799_1String connectorName, int numTasks)
{    try {        ConnectorStateInfo info = connect.connectorStatus(connectorName);        boolean result = info != null && info.tasks().size() == numTasks && info.connector().state().equals(AbstractStatus.State.RUNNING.toString()) && info.tasks().stream().allMatch(s -> s.state().equals(AbstractStatus.State.RUNNING.toString()));        return Optional.of(result);    } catch (Exception e) {                return Optional.empty();    }}
private Optional<Boolean>f10799
1
close
public void kafkatest_f10810_0()
{    // delete connector handle    RuntimeHandles.get().deleteConnector(CONNECTOR_NAME);    // stop all Connect, Kafka and Zk threads.    connect.stop();}
f10810
0
testSinkConnector
public void kafkatest_f10811_0() throws Exception
{    // create test topic    connect.kafka().createTopic("test-topic", NUM_TOPIC_PARTITIONS);    // setup up props for the sink connector    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSinkConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put(TOPICS_CONFIG, "test-topic");    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    // expect all records to be consumed by the connector    connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);    // expect all records to be consumed by the connector    connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);    // start a sink connector    connect.configureConnector(CONNECTOR_NAME, props);    waitForCondition(this::checkForPartitionAssignment, CONNECTOR_SETUP_DURATION_MS, "Connector tasks were not assigned a partition each.");    // produce some messages into source topic partitions    for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {        connect.kafka().produce("test-topic", i % NUM_TOPIC_PARTITIONS, "key", "simple-message-value-" + i);    }    // consume all records from the source topic or fail, to ensure that they were correctly produced.    assertEquals("Unexpected number of records consumed", NUM_RECORDS_PRODUCED, connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, "test-topic").count());    // wait for the connector tasks to consume all records.    connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);    // wait for the connector tasks to commit all records.    connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);    // delete connector    connect.deleteConnector(CONNECTOR_NAME);}
f10811
0
start
public voidf10820_1Map<String, String> props)
{    taskId = props.get("task.id");    connectorName = props.get("connector.name");    taskHandle = RuntimeHandles.get().connectorHandle(connectorName).taskHandle(taskId);        taskHandle.recordTaskStart();}
public voidf10820
1
open
public voidf10821_1Collection<TopicPartition> partitions)
{        assignments.addAll(partitions);    taskHandle.partitionsAssigned(partitions.size());}
public voidf10821
1
version
public String kafkatest_f10830_0()
{    return "unknown";}
f10830
0
start
public voidf10831_1Map<String, String> props)
{    taskId = props.get("task.id");    connectorName = props.get("connector.name");    topicName = props.getOrDefault(TOPIC_CONFIG, "sequential-topic");    throughput = Long.valueOf(props.getOrDefault("throughput", "-1"));    batchSize = Integer.valueOf(props.getOrDefault("messages.per.poll", "1"));    taskHandle = RuntimeHandles.get().connectorHandle(connectorName).taskHandle(taskId);    Map<String, Object> offset = Optional.ofNullable(context.offsetStorageReader().offset(Collections.singletonMap("task.id", taskId))).orElse(Collections.emptyMap());    startingSeqno = Optional.ofNullable((Long) offset.get("saved")).orElse(0L);        throttler = new ThroughputThrottler(throughput, System.currentTimeMillis());    taskHandle.recordTaskStart();}
public voidf10831
1
testDeleteConnector
public void kafkatest_f10840_0() throws Exception
{    // create test topic    connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);    // setup up props for the source connector    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put("throughput", String.valueOf(1));    props.put("messages.per.poll", String.valueOf(10));    props.put(TOPIC_CONFIG, TOPIC_NAME);    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    waitForCondition(() -> this.assertWorkersUp(3), WORKER_SETUP_DURATION_MS, "Connect workers did not start in time.");    // start a source connector    IntStream.range(0, 4).forEachOrdered(i -> {        try {            connect.configureConnector(CONNECTOR_NAME + i, props);        } catch (IOException e) {            throw new ConnectException(e);        }    });    waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(true), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not start in time.");    // delete connector    connect.deleteConnector(CONNECTOR_NAME + 3);    waitForCondition(() -> !this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(true), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not stop in time.");    waitForCondition(this::assertConnectorAndTasksAreUnique, WORKER_SETUP_DURATION_MS, "Connect and tasks are imbalanced between the workers.");}
f10840
0
testAddingWorker
public void kafkatest_f10841_0() throws Exception
{    // create test topic    connect.kafka().createTopic(TOPIC_NAME, NUM_TOPIC_PARTITIONS);    // setup up props for the source connector    Map<String, String> props = new HashMap<>();    props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());    props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));    props.put("throughput", String.valueOf(1));    props.put("messages.per.poll", String.valueOf(10));    props.put(TOPIC_CONFIG, TOPIC_NAME);    props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());    waitForCondition(() -> this.assertWorkersUp(3), WORKER_SETUP_DURATION_MS, "Connect workers did not start in time.");    // start a source connector    IntStream.range(0, 4).forEachOrdered(i -> {        try {            connect.configureConnector(CONNECTOR_NAME + i, props);        } catch (IOException e) {            throw new ConnectException(e);        }    });    waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not start in time.");    connect.addWorker();    waitForCondition(() -> this.assertWorkersUp(4), WORKER_SETUP_DURATION_MS, "Connect workers did not start in time.");    waitForCondition(() -> this.assertConnectorAndTasksRunning(CONNECTOR_NAME + 3, NUM_TASKS).orElse(false), CONNECTOR_SETUP_DURATION_MS, "Connector tasks did not start in time.");    waitForCondition(this::assertConnectorAndTasksAreUnique, WORKER_SETUP_DURATION_MS, "Connect and tasks are imbalanced between the workers.");}
f10841
0
version
public String kafkatest_f10852_0()
{    return "test";}
f10852
0
isRegistered
public boolean kafkatest_f10853_0()
{    return true;}
f10853
0
expectedRestarts
public StartAndStopLatch kafkatest_f10862_0(int expectedStarts, int expectedStops, List<StartAndStopLatch> dependents)
{    StartAndStopLatch latch = new StartAndStopLatch(expectedStarts, expectedStops, this::remove, dependents, clock);    restartLatches.add(latch);    return latch;}
f10862
0
expectedRestarts
public StartAndStopLatch kafkatest_f10863_0(int expectedRestarts)
{    return expectedRestarts(expectedRestarts, expectedRestarts);}
f10863
0
shouldRecordStarts
public void kafkatest_f10872_0()
{    assertEquals(0, counter.starts());    counter.recordStart();    assertEquals(1, counter.starts());    counter.recordStart();    assertEquals(2, counter.starts());    assertEquals(2, counter.starts());}
f10872
0
shouldRecordStops
public void kafkatest_f10873_0()
{    assertEquals(0, counter.stops());    counter.recordStop();    assertEquals(1, counter.stops());    counter.recordStop();    assertEquals(2, counter.stops());    assertEquals(2, counter.stops());}
f10873
0
shouldReturnFalseWhenAwaitingForStartToNeverComplete
public void kafkatest_f10882_0() throws Throwable
{    latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);    future = asyncAwait(100);    clock.sleep(10);    assertFalse(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
f10882
0
shouldReturnFalseWhenAwaitingForStopToNeverComplete
public void kafkatest_f10883_0() throws Throwable
{    latch = new StartAndStopLatch(1, 1, this::complete, dependents, clock);    future = asyncAwait(100);    latch.recordStart();    clock.sleep(10);    assertFalse(future.get(200, TimeUnit.MILLISECONDS));    assertTrue(future.isDone());}
f10883
0
commit
public void kafkatest_f10892_0()
{    if (recordsToCommitLatch != null) {        recordsToCommitLatch.countDown();    }    connectorHandle.commit();}
f10892
0
commit
public void kafkatest_f10893_0(int batchSize)
{    if (recordsToCommitLatch != null) {        IntStream.range(0, batchSize).forEach(i -> recordsToCommitLatch.countDown());    }    connectorHandle.commit(batchSize);}
f10893
0
recordTaskStart
public void kafkatest_f10902_0()
{    startAndStopCounter.recordStart();}
f10902
0
recordTaskStop
public void kafkatest_f10903_0()
{    startAndStopCounter.recordStop();}
f10903
0
testConfigValidationEmptyConfig
public void kafkatest_f10912_0()
{    AbstractHerder herder = createConfigValidationHerder(TestSourceConnector.class, noneConnectorClientConfigOverridePolicy);    replayAll();    herder.validateConnectorConfig(new HashMap<String, String>());    verifyAll();}
f10912
0
testConfigValidationMissingName
public void kafkatest_f10913_0()
{    AbstractHerder herder = createConfigValidationHerder(TestSourceConnector.class, noneConnectorClientConfigOverridePolicy);    replayAll();    Map<String, String> config = Collections.singletonMap(ConnectorConfig.CONNECTOR_CLASS_CONFIG, TestSourceConnector.class.getName());    ConfigInfos result = herder.validateConnectorConfig(config);    // We expect there to be errors due to the missing name and .... Note that these assertions depend heavily on    // the config fields for SourceConnectorConfig, but we expect these to change rarely.    assertEquals(TestSourceConnector.class.getName(), result.name());    assertEquals(Arrays.asList(ConnectorConfig.COMMON_GROUP, ConnectorConfig.TRANSFORMS_GROUP, ConnectorConfig.ERROR_GROUP), result.groups());    assertEquals(2, result.errorCount());    // Base connector config has 13 fields, connector's configs add 2    assertEquals(15, result.values().size());    // Missing name should generate an error    assertEquals(ConnectorConfig.NAME_CONFIG, result.values().get(0).configValue().name());    assertEquals(1, result.values().get(0).configValue().errors().size());    // "required" config from connector should generate an error    assertEquals("required", result.values().get(13).configValue().name());    assertEquals(1, result.values().get(13).configValue().errors().size());    verifyAll();}
f10913
0
createConfigValidationHerder
private AbstractHerder kafkatest_f10922_0(Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy)
{    ConfigBackingStore configStore = strictMock(ConfigBackingStore.class);    StatusBackingStore statusStore = strictMock(StatusBackingStore.class);    AbstractHerder herder = partialMockBuilder(AbstractHerder.class).withConstructor(Worker.class, String.class, String.class, StatusBackingStore.class, ConfigBackingStore.class, ConnectorClientConfigOverridePolicy.class).withArgs(worker, workerId, kafkaClusterId, statusStore, configStore, connectorClientConfigOverridePolicy).addMockedMethod("generation").createMock();    EasyMock.expect(herder.generation()).andStubReturn(generation);    // Call to validateConnectorConfig    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andStubReturn(plugins);    final Connector connector;    try {        connector = connectorClass.newInstance();    } catch (InstantiationException | IllegalAccessException e) {        throw new RuntimeException("Couldn't create connector", e);    }    EasyMock.expect(plugins.newConnector(connectorClass.getName())).andReturn(connector);    EasyMock.expect(plugins.compareAndSwapLoaders(connector)).andReturn(classLoader);    return herder;}
f10922
0
apply
public R kafkatest_f10924_0(R record)
{    return record;}
f10924
0
testGettingGroupWithTags
public void kafkatest_f10934_0()
{    MetricGroup group1 = metrics.group("name", "k1", "v1", "k2", "v2");    assertEquals("v1", group1.tags().get("k1"));    assertEquals("v2", group1.tags().get("k2"));    assertEquals(2, group1.tags().size());}
f10934
0
testGettingGroupMultipleTimes
public void kafkatest_f10935_0()
{    MetricGroup group1 = metrics.group("name");    MetricGroup group2 = metrics.group("name");    assertNotNull(group1);    assertSame(group1, group2);    MetricGroup group3 = metrics.group("other");    assertNotNull(group3);    assertNotSame(group1, group3);    // Now with tags    MetricGroup group4 = metrics.group("name", "k1", "v1");    assertNotNull(group4);    assertNotSame(group1, group4);    assertNotSame(group2, group4);    assertNotSame(group3, group4);    MetricGroup group5 = metrics.group("name", "k1", "v1");    assertSame(group4, group5);}
f10935
0
apply
public R kafkatest_f10944_0(R record)
{    return null;}
f10944
0
close
public void kafkatest_f10945_0()
{    magicNumber = 0;}
f10945
0
multipleTransformsOneDangling
public void kafkatest_f10954_0()
{    Map<String, String> props = new HashMap<>();    props.put("name", "test");    props.put("connector.class", TestConnector.class.getName());    props.put("transforms", "a, b");    props.put("transforms.a.type", SimpleTransformation.class.getName());    props.put("transforms.a.magic.number", "42");    new ConnectorConfig(MOCK_PLUGINS, props);}
f10954
0
multipleTransforms
public void kafkatest_f10955_0()
{    Map<String, String> props = new HashMap<>();    props.put("name", "test");    props.put("connector.class", TestConnector.class.getName());    props.put("transforms", "a, b");    props.put("transforms.a.type", SimpleTransformation.class.getName());    props.put("transforms.a.magic.number", "42");    props.put("transforms.b.type", SimpleTransformation.class.getName());    props.put("transforms.b.magic.number", "84");    final ConnectorConfig config = new ConnectorConfig(MOCK_PLUGINS, props);    final List<Transformation<R>> transformations = config.transformations();    assertEquals(2, transformations.size());    assertEquals(42, ((SimpleTransformation) transformations.get(0)).magicNumber);    assertEquals(84, ((SimpleTransformation) transformations.get(1)).magicNumber);}
f10955
0
testEagerToCoopAssignment
public void kafkatest_f10964_0()
{    ConnectProtocol.Assignment assignment = new ConnectProtocol.Assignment(ConnectProtocol.Assignment.NO_ERROR, "leader", LEADER_URL, 1L, Arrays.asList(connectorId1, connectorId3), Arrays.asList(taskId2x0));    ByteBuffer leaderBuf = ConnectProtocol.serializeAssignment(assignment);    ConnectProtocol.Assignment leaderAssignment = IncrementalCooperativeConnectProtocol.deserializeAssignment(leaderBuf);    assertEquals(false, leaderAssignment.failed());    assertEquals("leader", leaderAssignment.leader());    assertEquals(1, leaderAssignment.offset());    assertEquals(Arrays.asList(connectorId1, connectorId3), leaderAssignment.connectors());    assertEquals(Collections.singletonList(taskId2x0), leaderAssignment.tasks());    ConnectProtocol.Assignment assignment2 = new ConnectProtocol.Assignment(ConnectProtocol.Assignment.NO_ERROR, "member", LEADER_URL, 1L, Arrays.asList(connectorId2), Arrays.asList(taskId1x0, taskId3x0));    ByteBuffer memberBuf = ConnectProtocol.serializeAssignment(assignment2);    ConnectProtocol.Assignment memberAssignment = IncrementalCooperativeConnectProtocol.deserializeAssignment(memberBuf);    assertEquals(false, memberAssignment.failed());    assertEquals("member", memberAssignment.leader());    assertEquals(1, memberAssignment.offset());    assertEquals(Collections.singletonList(connectorId2), memberAssignment.connectors());    assertEquals(Arrays.asList(taskId1x0, taskId3x0), memberAssignment.tasks());}
f10964
0
testCoopToEagerAssignment
public void kafkatest_f10965_0()
{    ExtendedAssignment assignment = new ExtendedAssignment(CONNECT_PROTOCOL_V1, ConnectProtocol.Assignment.NO_ERROR, "leader", LEADER_URL, 1L, Arrays.asList(connectorId1, connectorId3), Arrays.asList(taskId2x0), Collections.emptyList(), Collections.emptyList(), 0);    ByteBuffer leaderBuf = IncrementalCooperativeConnectProtocol.serializeAssignment(assignment);    ConnectProtocol.Assignment leaderAssignment = ConnectProtocol.deserializeAssignment(leaderBuf);    assertEquals(false, leaderAssignment.failed());    assertEquals("leader", leaderAssignment.leader());    assertEquals(1, leaderAssignment.offset());    assertEquals(Arrays.asList(connectorId1, connectorId3), leaderAssignment.connectors());    assertEquals(Collections.singletonList(taskId2x0), leaderAssignment.tasks());    ExtendedAssignment assignment2 = new ExtendedAssignment(CONNECT_PROTOCOL_V1, ConnectProtocol.Assignment.NO_ERROR, "member", LEADER_URL, 1L, Arrays.asList(connectorId2), Arrays.asList(taskId1x0, taskId3x0), Collections.emptyList(), Collections.emptyList(), 0);    ByteBuffer memberBuf = IncrementalCooperativeConnectProtocol.serializeAssignment(assignment2);    ConnectProtocol.Assignment memberAssignment = ConnectProtocol.deserializeAssignment(memberBuf);    assertEquals(false, memberAssignment.failed());    assertEquals("member", memberAssignment.leader());    assertEquals(1, memberAssignment.offset());    assertEquals(Collections.singletonList(connectorId2), memberAssignment.connectors());    assertEquals(Arrays.asList(taskId1x0, taskId3x0), memberAssignment.tasks());}
f10965
0
testHaltCleansUpWorker
public void kafkatest_f10974_0()
{    EasyMock.expect(worker.connectorNames()).andReturn(Collections.singleton(CONN1));    worker.stopConnector(CONN1);    PowerMock.expectLastCall().andReturn(true);    EasyMock.expect(worker.taskIds()).andReturn(Collections.singleton(TASK1));    worker.stopAndAwaitTask(TASK1);    PowerMock.expectLastCall();    member.stop();    PowerMock.expectLastCall();    configBackingStore.stop();    PowerMock.expectLastCall();    statusBackingStore.stop();    PowerMock.expectLastCall();    worker.stop();    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.halt();    PowerMock.verifyAll();}
f10974
0
testCreateConnector
public void kafkatest_f10975_0() throws Exception
{    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    member.wakeup();    PowerMock.expectLastCall();    // config validation    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    EasyMock.expect(connectorMock.config()).andReturn(new ConfigDef());    EasyMock.expect(connectorMock.validate(CONN2_CONFIG)).andReturn(new Config(Collections.<ConfigValue>emptyList()));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    // CONN2 is new, should succeed    configBackingStore.putConnectorConfig(CONN2, CONN2_CONFIG);    PowerMock.expectLastCall();    ConnectorInfo info = new ConnectorInfo(CONN2, CONN2_CONFIG, Collections.<ConnectorTaskId>emptyList(), ConnectorType.SOURCE);    putConnectorCallback.onCompletion(null, new Herder.Created<>(true, info));    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // No immediate action besides this -- change will be picked up via the config log    PowerMock.replayAll();    herder.putConnectorConfig(CONN2, CONN2_CONFIG, false, putConnectorCallback);    herder.tick();    time.sleep(1000L);    assertStatistics(3, 1, 100, 1000L);    PowerMock.verifyAll();}
f10975
0
testRestartConnectorRedirectToOwner
public void kafkatest_f10984_0() throws Exception
{    // get the initial assignment    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList());    expectPostRebalanceCatchup(SNAPSHOT);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // now handle the connector restart    member.wakeup();    PowerMock.expectLastCall();    member.ensureActive();    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    String ownerUrl = "ownerUrl";    EasyMock.expect(member.ownerUrl(CONN1)).andReturn(ownerUrl);    PowerMock.replayAll();    herder.tick();    time.sleep(1000L);    assertStatistics(3, 1, 100, 1000L);    FutureCallback<Void> callback = new FutureCallback<>();    herder.restartConnector(CONN1, callback);    herder.tick();    time.sleep(2000L);    assertStatistics(3, 1, 100, 3000L);    try {        callback.get(1000L, TimeUnit.MILLISECONDS);        fail("Expected NotLeaderException to be raised");    } catch (ExecutionException e) {        assertTrue(e.getCause() instanceof NotAssignedException);        NotAssignedException notAssignedException = (NotAssignedException) e.getCause();        assertEquals(ownerUrl, notAssignedException.forwardUrl());    }    PowerMock.verifyAll();}
f10984
0
testRestartTask
public void kafkatest_f10985_0() throws Exception
{    EasyMock.expect(worker.connectorTaskConfigs(CONN1, conn1SinkConfig)).andStubReturn(TASK_CONFIGS);    // get the initial assignment    EasyMock.expect(member.memberId()).andStubReturn("leader");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));    expectPostRebalanceCatchup(SNAPSHOT);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    // now handle the task restart    member.wakeup();    PowerMock.expectLastCall();    member.ensureActive();    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    worker.stopAndAwaitTask(TASK0);    PowerMock.expectLastCall();    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    PowerMock.replayAll();    herder.tick();    FutureCallback<Void> callback = new FutureCallback<>();    herder.restartTask(TASK0, callback);    herder.tick();    callback.get(1000L, TimeUnit.MILLISECONDS);    PowerMock.verifyAll();}
f10985
0
testUnknownConnectorPaused
public void kafkatest_f10994_0() throws Exception
{    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    EasyMock.expect(worker.connectorNames()).andStubReturn(Collections.singleton(CONN1));    // join    expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));    expectPostRebalanceCatchup(SNAPSHOT);    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // state change is ignored since we have no target state    member.wakeup();    member.ensureActive();    PowerMock.expectLastCall();    EasyMock.expect(configBackingStore.snapshot()).andReturn(SNAPSHOT);    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    // join    herder.tick();    configUpdateListener.onConnectorTargetStateChange("unknown-connector");    // continue    herder.tick();    PowerMock.verifyAll();}
f10994
0
testConnectorPausedRunningTaskOnly
public void kafkatest_f10995_0() throws Exception
{    // even if we don't own the connector, we should still propagate target state    // changes to the worker so that tasks will transition correctly    EasyMock.expect(member.memberId()).andStubReturn("member");    EasyMock.expect(member.currentProtocolVersion()).andStubReturn(CONNECT_PROTOCOL_V0);    EasyMock.expect(worker.connectorNames()).andStubReturn(Collections.<String>emptySet());    // join    expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));    expectPostRebalanceCatchup(SNAPSHOT);    worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));    PowerMock.expectLastCall().andReturn(true);    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    // handle the state change    member.wakeup();    member.ensureActive();    PowerMock.expectLastCall();    EasyMock.expect(configBackingStore.snapshot()).andReturn(SNAPSHOT_PAUSED_CONN1);    PowerMock.expectLastCall();    worker.setTargetState(CONN1, TargetState.PAUSED);    PowerMock.expectLastCall();    member.poll(EasyMock.anyInt());    PowerMock.expectLastCall();    PowerMock.replayAll();    // join    herder.tick();    // state changes to paused    configUpdateListener.onConnectorTargetStateChange(CONN1);    // apply state change    herder.tick();    PowerMock.verifyAll();}
f10995
0
expectRebalance
private void kafkatest_f11004_0(final Collection<String> revokedConnectors, final List<ConnectorTaskId> revokedTasks, final short error, final long offset, final List<String> assignedConnectors, final List<ConnectorTaskId> assignedTasks)
{    expectRebalance(revokedConnectors, revokedTasks, error, offset, assignedConnectors, assignedTasks, 0);}
f11004
0
expectRebalance
private void kafkatest_f11005_0(final Collection<String> revokedConnectors, final List<ConnectorTaskId> revokedTasks, final short error, final long offset, final List<String> assignedConnectors, final List<ConnectorTaskId> assignedTasks, int delay)
{    member.ensureActive();    PowerMock.expectLastCall().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            ExtendedAssignment assignment;            if (!revokedConnectors.isEmpty() || !revokedTasks.isEmpty()) {                rebalanceListener.onRevoked("leader", revokedConnectors, revokedTasks);            }            if (connectProtocolVersion == CONNECT_PROTOCOL_V0) {                assignment = new ExtendedAssignment(connectProtocolVersion, error, "leader", "leaderUrl", offset, assignedConnectors, assignedTasks, Collections.emptyList(), Collections.emptyList(), 0);            } else {                assignment = new ExtendedAssignment(connectProtocolVersion, error, "leader", "leaderUrl", offset, assignedConnectors, assignedTasks, new ArrayList<>(revokedConnectors), new ArrayList<>(revokedTasks), delay);            }            rebalanceListener.onAssigned(assignment, 3);            time.sleep(100L);            return null;        }    });    if (!revokedConnectors.isEmpty()) {        for (String connector : revokedConnectors) {            worker.stopConnector(connector);            PowerMock.expectLastCall().andReturn(true);        }    }    if (!revokedTasks.isEmpty()) {        worker.stopAndAwaitTask(EasyMock.anyObject(ConnectorTaskId.class));        PowerMock.expectLastCall();    }    if (!revokedConnectors.isEmpty()) {        statusBackingStore.flush();        PowerMock.expectLastCall();    }    member.wakeup();    PowerMock.expectLastCall();}
f11005
0
testTaskAssignmentWhenWorkerLeavesPermanently
public void kafkatest_f11014_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    when(coordinator.configSnapshot()).thenReturn(configState);    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    // First assignment with 2 workers and 2 connectors configured but not yet assigned    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(2, 8, 0, 0, "worker1", "worker2");    // Second assignment with only one worker remaining in the group. The worker that left the    // group was a follower. No re-assignments take place immediately and the count    // down for the rebalance delay starts    applyAssignments(returnedAssignments);    assignments.remove("worker2");    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1");    time.sleep(rebalanceDelay / 2);    // Third (incidental) assignment with still only one worker in the group. Max delay has not    // been reached yet    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay / 2, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1");    time.sleep(rebalanceDelay / 2 + 1);    // Fourth assignment after delay expired    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(1, 4, 0, 0, "worker1");    verify(coordinator, times(rebalanceNum)).configSnapshot();    verify(coordinator, times(rebalanceNum)).leaderState(any());}
f11014
0
testTaskAssignmentWhenWorkerBounces
public void kafkatest_f11015_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    when(coordinator.configSnapshot()).thenReturn(configState);    doReturn(Collections.EMPTY_MAP).when(assignor).serializeAssignments(assignmentsCapture.capture());    // First assignment with 2 workers and 2 connectors configured but not yet assigned    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(2, 8, 0, 0, "worker1", "worker2");    // Second assignment with only one worker remaining in the group. The worker that left the    // group was a follower. No re-assignments take place immediately and the count    // down for the rebalance delay starts    applyAssignments(returnedAssignments);    assignments.remove("worker2");    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1");    time.sleep(rebalanceDelay / 2);    // Third (incidental) assignment with still only one worker in the group. Max delay has not    // been reached yet    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay / 2, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1");    time.sleep(rebalanceDelay / 4);    // Fourth assignment with the second worker returning before the delay expires    // Since the delay is still active, lost assignments are not reassigned yet    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    memberConfigs.put("worker2", new ExtendedWorkerState(leaderUrl, offset, null));    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(rebalanceDelay / 4, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(0, 0, 0, 0, "worker1", "worker2");    time.sleep(rebalanceDelay / 4);    // Fifth assignment with the same two workers. The delay has expired, so the lost    // assignments ought to be assigned to the worker that has appeared as returned.    applyAssignments(returnedAssignments);    memberConfigs = memberConfigs(leader, offset, assignments);    assignor.performTaskAssignment(leader, offset, memberConfigs, coordinator);    ++rebalanceNum;    returnedAssignments = assignmentsCapture.getValue();    assertDelay(0, returnedAssignments);    expectedMemberConfigs = memberConfigs(leader, offset, returnedAssignments);    assertNoReassignments(memberConfigs, expectedMemberConfigs);    assertAssignment(1, 4, 0, 0, "worker1", "worker2");    verify(coordinator, times(rebalanceNum)).configSnapshot();    verify(coordinator, times(rebalanceNum)).leaderState(any());}
f11015
0
testLostAssignmentHandlingWhenWorkerBounces
public void kafkatest_f11024_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    assertTrue(assignor.candidateWorkersForReassignment.isEmpty());    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    Map<String, WorkerLoad> configuredAssignment = new HashMap<>();    configuredAssignment.put("worker0", workerLoad("worker0", 0, 2, 0, 4));    configuredAssignment.put("worker2", workerLoad("worker2", 4, 2, 8, 4));    ConnectorsAndTasks newSubmissions = new ConnectorsAndTasks.Builder().build();    // No lost assignments    assignor.handleLostAssignments(new ConnectorsAndTasks.Builder().build(), newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    String flakyWorker = "worker1";    WorkerLoad lostLoad = workerLoad(flakyWorker, 2, 2, 4, 4);    ConnectorsAndTasks lostAssignments = new ConnectorsAndTasks.Builder().withCopies(lostLoad.connectors(), lostLoad.tasks()).build();    // Lost assignments detected - No candidate worker has appeared yet (worker with no assignments)    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay / 2);    rebalanceDelay /= 2;    // A new worker (probably returning worker) has joined    configuredAssignment.put(flakyWorker, new WorkerLoad.Builder(flakyWorker).build());    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.singleton(flakyWorker), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay);    // The new worker has still no assignments    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertTrue("Wrong assignment of lost connectors", configuredAssignment.getOrDefault(flakyWorker, new WorkerLoad.Builder(flakyWorker).build()).connectors().containsAll(lostAssignments.connectors()));    assertTrue("Wrong assignment of lost tasks", configuredAssignment.getOrDefault(flakyWorker, new WorkerLoad.Builder(flakyWorker).build()).tasks().containsAll(lostAssignments.tasks()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);}
f11024
0
testLostAssignmentHandlingWhenWorkerLeavesPermanently
public void kafkatest_f11025_0()
{    // Customize assignor for this test case    time = new MockTime();    initAssignor();    assertTrue(assignor.candidateWorkersForReassignment.isEmpty());    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    Map<String, WorkerLoad> configuredAssignment = new HashMap<>();    configuredAssignment.put("worker0", workerLoad("worker0", 0, 2, 0, 4));    configuredAssignment.put("worker2", workerLoad("worker2", 4, 2, 8, 4));    ConnectorsAndTasks newSubmissions = new ConnectorsAndTasks.Builder().build();    // No lost assignments    assignor.handleLostAssignments(new ConnectorsAndTasks.Builder().build(), newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);    String removedWorker = "worker1";    WorkerLoad lostLoad = workerLoad(removedWorker, 2, 2, 4, 4);    ConnectorsAndTasks lostAssignments = new ConnectorsAndTasks.Builder().withCopies(lostLoad.connectors(), lostLoad.tasks()).build();    // Lost assignments detected - No candidate worker has appeared yet (worker with no assignments)    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay / 2);    rebalanceDelay /= 2;    // No new worker has joined    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(time.milliseconds() + rebalanceDelay, assignor.scheduledRebalance);    assertEquals(rebalanceDelay, assignor.delay);    time.sleep(rebalanceDelay);    assignor.handleLostAssignments(lostAssignments, newSubmissions, new ArrayList<>(configuredAssignment.values()));    assertTrue("Wrong assignment of lost connectors", newSubmissions.connectors().containsAll(lostAssignments.connectors()));    assertTrue("Wrong assignment of lost tasks", newSubmissions.tasks().containsAll(lostAssignments.tasks()));    assertThat("Wrong set of workers for reassignments", Collections.emptySet(), is(assignor.candidateWorkersForReassignment));    assertEquals(0, assignor.scheduledRebalance);    assertEquals(0, assignor.delay);}
f11025
0
memberConfigs
private static Map<String, ExtendedWorkerState> kafkatest_f11034_0(String givenLeader, long givenOffset, int start, int connectorNum)
{    return IntStream.range(start, connectorNum + 1).mapToObj(i -> new SimpleEntry<>("worker" + i, new ExtendedWorkerState(expectedLeaderUrl(givenLeader), givenOffset, null))).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
f11034
0
connectorTaskCounts
private static Map<String, Integer> kafkatest_f11035_0(int start, int connectorNum, int taskCounts)
{    return IntStream.range(start, connectorNum + 1).mapToObj(i -> new SimpleEntry<>("connector" + i, taskCounts)).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
f11035
0
assertDelay
private void kafkatest_f11044_0(int expectedDelay, Map<String, ExtendedAssignment> newAssignments)
{    newAssignments.values().stream().forEach(a -> assertEquals("Wrong rebalance delay in " + a, expectedDelay, a.delay()));}
f11044
0
assertNoReassignments
private void kafkatest_f11045_0(Map<String, ExtendedWorkerState> existingAssignments, Map<String, ExtendedWorkerState> newAssignments)
{    assertNoDuplicateInAssignment(existingAssignments);    assertNoDuplicateInAssignment(newAssignments);    List<String> existingConnectors = existingAssignments.values().stream().flatMap(a -> a.assignment().connectors().stream()).collect(Collectors.toList());    List<String> newConnectors = newAssignments.values().stream().flatMap(a -> a.assignment().connectors().stream()).collect(Collectors.toList());    List<ConnectorTaskId> existingTasks = existingAssignments.values().stream().flatMap(a -> a.assignment().tasks().stream()).collect(Collectors.toList());    List<ConnectorTaskId> newTasks = newAssignments.values().stream().flatMap(a -> a.assignment().tasks().stream()).collect(Collectors.toList());    existingConnectors.retainAll(newConnectors);    assertThat("Found connectors in new assignment that already exist in current assignment", Collections.emptyList(), is(existingConnectors));    existingTasks.retainAll(newTasks);    assertThat("Found tasks in new assignment that already exist in current assignment", Collections.emptyList(), is(existingConnectors));}
f11045
0
testTaskAssignmentWhenWorkerJoins
public void kafkatest_f11054_0()
{    when(configStorage.snapshot()).thenReturn(configState1);    coordinator.metadata();    ++configStorageCalls;    List<JoinGroupResponseMember> responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, null);    addJoinGroupResponseMember(responseMembers, memberId, offset, null);    Map<String, ByteBuffer> result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    ExtendedAssignment leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId1), 4, Collections.emptyList(), 0, leaderAssignment);    ExtendedAssignment memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId2), 4, Collections.emptyList(), 0, memberAssignment);    coordinator.metadata();    ++configStorageCalls;    responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, leaderAssignment);    addJoinGroupResponseMember(responseMembers, memberId, offset, memberAssignment);    addJoinGroupResponseMember(responseMembers, anotherMemberId, offset, null);    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 2, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, memberAssignment);    ExtendedAssignment anotherMemberAssignment = deserializeAssignment(result, anotherMemberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, anotherMemberAssignment);    verify(configStorage, times(configStorageCalls)).snapshot();}
f11054
0
testTaskAssignmentWhenWorkerLeavesPermanently
public void kafkatest_f11055_0()
{    when(configStorage.snapshot()).thenReturn(configState1);    // First assignment distributes configured connectors and tasks    coordinator.metadata();    ++configStorageCalls;    List<JoinGroupResponseMember> responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, null);    addJoinGroupResponseMember(responseMembers, memberId, offset, null);    addJoinGroupResponseMember(responseMembers, anotherMemberId, offset, null);    Map<String, ByteBuffer> result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    ExtendedAssignment leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId1), 3, Collections.emptyList(), 0, leaderAssignment);    ExtendedAssignment memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.singletonList(connectorId2), 3, Collections.emptyList(), 0, memberAssignment);    ExtendedAssignment anotherMemberAssignment = deserializeAssignment(result, anotherMemberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 2, Collections.emptyList(), 0, anotherMemberAssignment);    // Second rebalance detects a worker is missing    coordinator.metadata();    ++configStorageCalls;    // Mark everyone as in sync with configState1    responseMembers = new ArrayList<>();    addJoinGroupResponseMember(responseMembers, leaderId, offset, leaderAssignment);    addJoinGroupResponseMember(responseMembers, memberId, offset, memberAssignment);    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, memberAssignment);    rebalanceDelay /= 2;    time.sleep(rebalanceDelay);    // A third rebalance before the delay expires won't change the assignments    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 0, Collections.emptyList(), 0, rebalanceDelay, memberAssignment);    time.sleep(rebalanceDelay + 1);    // A rebalance after the delay expires re-assigns the lost tasks    result = coordinator.performAssignment(leaderId, compatibility.protocol(), responseMembers);    leaderAssignment = deserializeAssignment(result, leaderId);    assertAssignment(leaderId, offset, Collections.emptyList(), 1, Collections.emptyList(), 0, leaderAssignment);    memberAssignment = deserializeAssignment(result, memberId);    assertAssignment(leaderId, offset, Collections.emptyList(), 1, Collections.emptyList(), 0, memberAssignment);    verify(configStorage, times(configStorageCalls)).snapshot();}
f11055
0
testMetadata
public void kafkatest_f11064_0()
{    EasyMock.expect(configStorage.snapshot()).andReturn(configState1);    PowerMock.replayAll();    JoinGroupRequestData.JoinGroupRequestProtocolCollection serialized = coordinator.metadata();    assertEquals(expectedMetadataSize, serialized.size());    Iterator<JoinGroupRequestData.JoinGroupRequestProtocol> protocolIterator = serialized.iterator();    assertTrue(protocolIterator.hasNext());    JoinGroupRequestData.JoinGroupRequestProtocol defaultMetadata = protocolIterator.next();    assertEquals(compatibility.protocol(), defaultMetadata.name());    ConnectProtocol.WorkerState state = ConnectProtocol.deserializeMetadata(ByteBuffer.wrap(defaultMetadata.metadata()));    assertEquals(1, state.offset());    PowerMock.verifyAll();}
f11064
0
testNormalJoinGroupLeader
public void kafkatest_f11065_0()
{    EasyMock.expect(configStorage.snapshot()).andReturn(configState1);    PowerMock.replayAll();    final String consumerId = "leader";    client.prepareResponse(FindCoordinatorResponse.prepareResponse(Errors.NONE, node));    coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));    // normal join group    Map<String, Long> memberConfigOffsets = new HashMap<>();    memberConfigOffsets.put("leader", 1L);    memberConfigOffsets.put("member", 1L);    client.prepareResponse(joinGroupLeaderResponse(1, consumerId, memberConfigOffsets, Errors.NONE));    client.prepareResponse(new MockClient.RequestMatcher() {        @Override        public boolean matches(AbstractRequest body) {            SyncGroupRequest sync = (SyncGroupRequest) body;            return sync.data.memberId().equals(consumerId) && sync.data.generationId() == 1 && sync.groupAssignments().containsKey(consumerId);        }    }, syncGroupResponse(ConnectProtocol.Assignment.NO_ERROR, "leader", 1L, Collections.singletonList(connectorId1), Collections.<ConnectorTaskId>emptyList(), Errors.NONE));    coordinator.ensureActiveGroup();    assertFalse(coordinator.rejoinNeededOrPending());    assertEquals(0, rebalanceListener.revokedCount);    assertEquals(1, rebalanceListener.assignedCount);    assertFalse(rebalanceListener.assignment.failed());    assertEquals(1L, rebalanceListener.assignment.offset());    assertEquals("leader", rebalanceListener.assignment.leader());    assertEquals(Collections.singletonList(connectorId1), rebalanceListener.assignment.connectors());    assertEquals(Collections.emptyList(), rebalanceListener.assignment.tasks());    PowerMock.verifyAll();}
f11065
0
testLeaderPerformAssignmentSingleTaskConnectors
public void kafkatest_f11074_0() throws Exception
{    // Since all the protocol responses are mocked, the other tests validate doSync runs, but don't validate its    // output. So we test it directly here.    EasyMock.expect(configStorage.snapshot()).andReturn(configStateSingleTaskConnectors);    PowerMock.replayAll();    // Prime the current configuration state    coordinator.metadata();    // Mark everyone as in sync with configState1    List<JoinGroupResponseData.JoinGroupResponseMember> responseMembers = new ArrayList<>();    responseMembers.add(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId("leader").setMetadata(ConnectProtocol.serializeMetadata(new ConnectProtocol.WorkerState(LEADER_URL, 1L)).array()));    responseMembers.add(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId("member").setMetadata(ConnectProtocol.serializeMetadata(new ConnectProtocol.WorkerState(MEMBER_URL, 1L)).array()));    Map<String, ByteBuffer> result = coordinator.performAssignment("leader", WorkerCoordinator.DEFAULT_SUBPROTOCOL, responseMembers);    // Round robin assignment when there are the same number of connectors and tasks should result in each being    // evenly distributed across the workers, i.e. round robin assignment of connectors first, then followed by tasks    ConnectProtocol.Assignment leaderAssignment = ConnectProtocol.deserializeAssignment(result.get("leader"));    assertEquals(false, leaderAssignment.failed());    assertEquals("leader", leaderAssignment.leader());    assertEquals(1, leaderAssignment.offset());    assertEquals(Arrays.asList(connectorId1, connectorId3), leaderAssignment.connectors());    assertEquals(Arrays.asList(taskId2x0), leaderAssignment.tasks());    ConnectProtocol.Assignment memberAssignment = ConnectProtocol.deserializeAssignment(result.get("member"));    assertEquals(false, memberAssignment.failed());    assertEquals("leader", memberAssignment.leader());    assertEquals(1, memberAssignment.offset());    assertEquals(Collections.singletonList(connectorId2), memberAssignment.connectors());    assertEquals(Arrays.asList(taskId1x0, taskId3x0), memberAssignment.tasks());    PowerMock.verifyAll();}
f11074
0
joinGroupLeaderResponse
private JoinGroupResponse kafkatest_f11075_0(int generationId, String memberId, Map<String, Long> configOffsets, Errors error)
{    List<JoinGroupResponseData.JoinGroupResponseMember> metadata = new ArrayList<>();    for (Map.Entry<String, Long> configStateEntry : configOffsets.entrySet()) {        // We need a member URL, but it doesn't matter for the purposes of this test. Just set it to the member ID        String memberUrl = configStateEntry.getKey();        long configOffset = configStateEntry.getValue();        ByteBuffer buf = ConnectProtocol.serializeMetadata(new ConnectProtocol.WorkerState(memberUrl, configOffset));        metadata.add(new JoinGroupResponseData.JoinGroupResponseMember().setMemberId(configStateEntry.getKey()).setMetadata(buf.array()));    }    return new JoinGroupResponse(new JoinGroupResponseData().setErrorCode(error.code()).setGenerationId(generationId).setProtocolName(WorkerCoordinator.DEFAULT_SUBPROTOCOL).setLeader(memberId).setMemberId(memberId).setMembers(metadata));}
f11075
0
testErrorHandlingInSourceTasks
public void kafkatest_f11084_0() throws Exception
{    Map<String, String> reportProps = new HashMap<>();    reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, "true");    reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, "true");    LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);    RetryWithToleranceOperator retryWithToleranceOperator = operator();    retryWithToleranceOperator.metrics(errorHandlingMetrics);    retryWithToleranceOperator.reporters(singletonList(reporter));    createSourceTask(initialState, retryWithToleranceOperator);    // valid json    Schema valSchema = SchemaBuilder.struct().field("val", Schema.INT32_SCHEMA).build();    Struct struct1 = new Struct(valSchema).put("val", 1234);    SourceRecord record1 = new SourceRecord(emptyMap(), emptyMap(), TOPIC, PARTITION1, valSchema, struct1);    Struct struct2 = new Struct(valSchema).put("val", 6789);    SourceRecord record2 = new SourceRecord(emptyMap(), emptyMap(), TOPIC, PARTITION1, valSchema, struct2);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(false);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(false);    EasyMock.expect(workerSourceTask.isStopping()).andReturn(true);    EasyMock.expect(workerSourceTask.commitOffsets()).andReturn(true);    offsetWriter.offset(EasyMock.anyObject(), EasyMock.anyObject());    EasyMock.expectLastCall().times(2);    sourceTask.initialize(EasyMock.anyObject());    EasyMock.expectLastCall();    sourceTask.start(EasyMock.anyObject());    EasyMock.expectLastCall();    EasyMock.expect(sourceTask.poll()).andReturn(singletonList(record1));    EasyMock.expect(sourceTask.poll()).andReturn(singletonList(record2));    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andReturn(null).times(2);    PowerMock.replayAll();    workerSourceTask.initialize(TASK_CONFIG);    workerSourceTask.execute();    // two records were consumed from Kafka    assertSourceMetricValue("source-record-poll-total", 2.0);    // only one was written to the task    assertSourceMetricValue("source-record-write-total", 0.0);    // one record completely failed (converter issues)    assertErrorHandlingMetricValue("total-record-errors", 0.0);    // 2 failures in the transformation, and 1 in the converter    assertErrorHandlingMetricValue("total-record-failures", 4.0);    // one record completely failed (converter issues), and thus was skipped    assertErrorHandlingMetricValue("total-records-skipped", 0.0);    PowerMock.verifyAll();}
f11084
0
connConfig
private ConnectorConfig kafkatest_f11085_0(Map<String, String> connProps)
{    Map<String, String> props = new HashMap<>();    props.put(ConnectorConfig.NAME_CONFIG, "test");    props.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, SinkTask.class.getName());    props.putAll(connProps);    return new ConnectorConfig(plugins, props);}
f11085
0
createSourceTask
private void kafkatest_f11094_0(TargetState initialState, RetryWithToleranceOperator retryWithToleranceOperator, Converter converter)
{    TransformationChain<SourceRecord> sourceTransforms = new TransformationChain<>(singletonList(new FaultyPassthrough<SourceRecord>()), retryWithToleranceOperator);    workerSourceTask = PowerMock.createPartialMock(WorkerSourceTask.class, new String[] { "commitOffsets", "isStopping" }, taskId, sourceTask, statusListener, initialState, converter, converter, headerConverter, sourceTransforms, producer, offsetReader, offsetWriter, workerConfig, ClusterConfigState.EMPTY, metrics, pluginLoader, time, retryWithToleranceOperator);}
f11094
0
records
private ConsumerRecords<byte[], byte[]> kafkatest_f11095_0(ConsumerRecord<byte[], byte[]> record)
{    return new ConsumerRecords<>(Collections.singletonMap(new TopicPartition(record.topic(), record.partition()), singletonList(record)));}
f11095
0
testDLQConfigWithEmptyTopicName
public void kafkatest_f11104_0()
{    DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(emptyMap()), TASK_ID, errorHandlingMetrics);    ProcessingContext context = processingContext();    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andThrow(new RuntimeException());    replay(producer);    // since topic name is empty, this method should be a NOOP.    // if it attempts to log to the DLQ via the producer, the send mock will throw a RuntimeException.    deadLetterQueueReporter.report(context);}
f11104
0
testDLQConfigWithValidTopicName
public void kafkatest_f11105_0()
{    DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);    ProcessingContext context = processingContext();    EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject())).andReturn(metadata);    replay(producer);    deadLetterQueueReporter.report(context);    PowerMock.verifyAll();}
f11105
0
testDlqHeaderIsAppended
public void kafkatest_f11114_0()
{    Map<String, String> props = new HashMap<>();    props.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);    props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, "true");    DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);    ProcessingContext context = new ProcessingContext();    context.consumerRecord(new ConsumerRecord<>("source-topic", 7, 10, "source-key".getBytes(), "source-value".getBytes()));    context.currentContext(Stage.TRANSFORMATION, Transformation.class);    context.error(new ConnectException("Test Exception"));    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(DLQ_TOPIC, "source-key".getBytes(), "source-value".getBytes());    producerRecord.headers().add(ERROR_HEADER_ORIG_TOPIC, "dummy".getBytes());    deadLetterQueueReporter.populateContextHeaders(producerRecord, context);    int appearances = 0;    for (Header header : producerRecord.headers()) {        if (ERROR_HEADER_ORIG_TOPIC.equalsIgnoreCase(header.key())) {            appearances++;        }    }    assertEquals("source-topic", headerValue(producerRecord, ERROR_HEADER_ORIG_TOPIC));    assertEquals(2, appearances);}
f11114
0
headerValue
private String kafkatest_f11115_0(ProducerRecord<byte[], byte[]> producerRecord, String headerSuffix)
{    return new String(producerRecord.headers().lastHeader(headerSuffix).value());}
f11115
0
testHandleExceptionInTaskPoll
public void kafkatest_f11124_0()
{    testHandleExceptionInStage(Stage.TASK_POLL, new org.apache.kafka.connect.errors.RetriableException("Test"));}
f11124
0
testThrowExceptionInTaskPut
public void kafkatest_f11125_0()
{    testHandleExceptionInStage(Stage.TASK_PUT, new Exception());}
f11125
0
testExecAndHandleNonRetriableErrorThrice
public void kafkatest_f11134_0() throws Exception
{    execAndHandleNonRetriableError(3, 0, new Exception("Non Retriable Test"));}
f11134
0
execAndHandleRetriableError
public void kafkatest_f11135_0(int numRetriableExceptionsThrown, long expectedWait, Exception e) throws Exception
{    MockTime time = new MockTime(0, 0, 0);    RetryWithToleranceOperator retryWithToleranceOperator = new RetryWithToleranceOperator(6000, ERRORS_RETRY_MAX_DELAY_DEFAULT, ALL, time);    retryWithToleranceOperator.metrics(errorHandlingMetrics);    EasyMock.expect(mockOperation.call()).andThrow(e).times(numRetriableExceptionsThrown);    EasyMock.expect(mockOperation.call()).andReturn("Success");    replay(mockOperation);    String result = retryWithToleranceOperator.execAndHandleError(mockOperation, Exception.class);    assertFalse(retryWithToleranceOperator.failed());    assertEquals("Success", result);    assertEquals(expectedWait, time.hiResClockMs());    PowerMock.verifyAll();}
f11135
0
setUp
public void kafkatest_f11144_0()
{    expectedConnectors = Arrays.asList("sink1", "source1", "source2");    connectClusterState = new ConnectClusterStateImpl(herderRequestTimeoutMs, new ConnectClusterDetailsImpl(KAFKA_CLUSTER_ID), herder);}
f11144
0
connectors
public void kafkatest_f11145_0()
{    Capture<Callback<Collection<String>>> callback = EasyMock.newCapture();    herder.connectors(EasyMock.capture(callback));    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() {            callback.getValue().onCompletion(null, expectedConnectors);            return null;        }    });    EasyMock.replay(herder);    assertEquals(expectedConnectors, connectClusterState.connectors());}
f11145
0
setUp
public void kafkatest_f11154_0() throws Exception
{    // Fairly simple use case, thus no need to create a random directory here yet.    URL location = Paths.get("/tmp").toUri().toURL();    // Normally parent will be a DelegatingClassLoader.    pluginLoader = new PluginClassLoader(location, new URL[0], systemLoader);}
f11154
0
testRegularPluginDesc
public void kafkatest_f11155_0()
{    PluginDesc<Connector> connectorDesc = new PluginDesc<>(Connector.class, regularVersion, pluginLoader);    assertPluginDesc(connectorDesc, Connector.class, regularVersion, pluginLoader.location());    PluginDesc<Converter> converterDesc = new PluginDesc<>(Converter.class, snaphotVersion, pluginLoader);    assertPluginDesc(converterDesc, Converter.class, snaphotVersion, pluginLoader.location());    PluginDesc<Transformation> transformDesc = new PluginDesc<>(Transformation.class, noVersion, pluginLoader);    assertPluginDesc(transformDesc, Transformation.class, noVersion, pluginLoader.location());}
f11155
0
createConfig
protected void kafkatest_f11164_0()
{    this.config = new TestableWorkerConfig(props);}
f11164
0
shouldInstantiateAndConfigureConverters
public void kafkatest_f11165_0()
{    instantiateAndConfigureConverter(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);    // Validate extra configs got passed through to overridden converters    assertEquals("true", converter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));    assertEquals("foo1", converter.configs.get("extra.config"));    instantiateAndConfigureConverter(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);    // Validate extra configs got passed through to overridden converters    assertEquals("true", converter.configs.get(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG));    assertEquals("foo2", converter.configs.get("extra.config"));}
f11165
0
config
public ConfigDef kafkatest_f11174_0()
{    return JsonConverterConfig.configDef();}
f11174
0
configure
public void kafkatest_f11175_0(Map<String, ?> configs)
{    this.configs = configs;    // requires the `converter.type` config be set    new JsonConverterConfig(configs);}
f11175
0
version
public String kafkatest_f11187_0()
{    return "test";}
f11187
0
configure
public void kafkatest_f11188_0(Map<String, ?> configs)
{    this.configs = configs;    super.configure(configs);}
f11188
0
testEmptyStructurePluginUrls
public void kafkatest_f11197_0() throws Exception
{    createBasicDirectoryLayout();    assertEquals(Collections.<Path>emptyList(), PluginUtils.pluginUrls(pluginPath));}
f11197
0
testPluginUrlsWithJars
public void kafkatest_f11198_0() throws Exception
{    createBasicDirectoryLayout();    List<Path> expectedUrls = createBasicExpectedUrls();    assertUrls(expectedUrls, PluginUtils.pluginUrls(pluginPath));}
f11198
0
assertUrls
private void kafkatest_f11207_0(List<Path> expected, List<Path> actual)
{    Collections.sort(expected);    // not sorting 'actual' because it should be returned sorted from withing the PluginUtils.    assertEquals(expected, actual);}
f11207
0
time
public MockTime kafkatest_f11208_0()
{    return (MockTime) super.time();}
f11208
0
metricChange
public void kafkatest_f11217_0(KafkaMetric metric)
{    metricsByName.put(metric.metricName(), metric);}
f11217
0
metricRemoval
public void kafkatest_f11218_0(KafkaMetric metric)
{// don't remove metrics, or else we won't be able to access them after the metric metricGroup is closed}
f11218
0
testValidateConfigWithAlias
public void kafkatest_f11227_0() throws Throwable
{    herder.validateConnectorConfig(EasyMock.eq(props));    PowerMock.expectLastCall().andAnswer((IAnswer<ConfigInfos>) () -> {        ConfigDef connectorConfigDef = ConnectorConfig.configDef();        List<ConfigValue> connectorConfigValues = connectorConfigDef.validate(props);        Connector connector = new ConnectorPluginsResourceTestConnector();        Config config = connector.validate(props);        ConfigDef configDef = connector.config();        Map<String, ConfigDef.ConfigKey> configKeys = configDef.configKeys();        List<ConfigValue> configValues = config.configValues();        Map<String, ConfigDef.ConfigKey> resultConfigKeys = new HashMap<>(configKeys);        resultConfigKeys.putAll(connectorConfigDef.configKeys());        configValues.addAll(connectorConfigValues);        return AbstractHerder.generateResult(ConnectorPluginsResourceTestConnector.class.getName(), resultConfigKeys, configValues, Collections.singletonList("Test"));    });    PowerMock.replayAll();    // make a request to connector-plugins resource using a valid alias.    ConfigInfos configInfos = connectorPluginsResource.validateConfigs("ConnectorPluginsResourceTest", props);    assertEquals(CONFIG_INFOS.name(), configInfos.name());    assertEquals(0, configInfos.errorCount());    assertEquals(CONFIG_INFOS.groups(), configInfos.groups());    assertEquals(new HashSet<>(CONFIG_INFOS.values()), new HashSet<>(configInfos.values()));    PowerMock.verifyAll();}
f11227
0
testValidateConfigWithNonExistentName
public void kafkatest_f11228_0() throws Throwable
{    herder.validateConnectorConfig(EasyMock.eq(props));    PowerMock.expectLastCall().andAnswer((IAnswer<ConfigInfos>) () -> {        ConfigDef connectorConfigDef = ConnectorConfig.configDef();        List<ConfigValue> connectorConfigValues = connectorConfigDef.validate(props);        Connector connector = new ConnectorPluginsResourceTestConnector();        Config config = connector.validate(props);        ConfigDef configDef = connector.config();        Map<String, ConfigDef.ConfigKey> configKeys = configDef.configKeys();        List<ConfigValue> configValues = config.configValues();        Map<String, ConfigDef.ConfigKey> resultConfigKeys = new HashMap<>(configKeys);        resultConfigKeys.putAll(connectorConfigDef.configKeys());        configValues.addAll(connectorConfigValues);        return AbstractHerder.generateResult(ConnectorPluginsResourceTestConnector.class.getName(), resultConfigKeys, configValues, Collections.singletonList("Test"));    });    PowerMock.replayAll();    // make a request to connector-plugins resource using a non-loaded connector with the same    // simple name but different package.    String customClassname = "com.custom.package." + ConnectorPluginsResourceTestConnector.class.getSimpleName();    connectorPluginsResource.validateConfigs(customClassname, props);    PowerMock.verifyAll();}
f11228
0
taskConfigs
public List<Map<String, String>> kafkatest_f11238_0(int maxTasks)
{    return null;}
f11238
0
config
public ConfigDef kafkatest_f11240_0()
{    return CONFIG_DEF;}
f11240
0
testExpandConnectorsInfo
public void kafkatest_f11249_0() throws Throwable
{    EasyMock.expect(herder.connectors()).andReturn(Arrays.asList(CONNECTOR2_NAME, CONNECTOR_NAME));    ConnectorInfo connector = EasyMock.mock(ConnectorInfo.class);    ConnectorInfo connector2 = EasyMock.mock(ConnectorInfo.class);    EasyMock.expect(herder.connectorInfo(CONNECTOR2_NAME)).andReturn(connector2);    EasyMock.expect(herder.connectorInfo(CONNECTOR_NAME)).andReturn(connector);    forward = EasyMock.mock(UriInfo.class);    MultivaluedMap<String, String> queryParams = new MultivaluedHashMap<>();    queryParams.putSingle("expand", "info");    EasyMock.expect(forward.getQueryParameters()).andReturn(queryParams).anyTimes();    EasyMock.replay(forward);    PowerMock.replayAll();    Map<String, Map<String, Object>> expanded = (Map<String, Map<String, Object>>) connectorsResource.listConnectors(forward, NULL_HEADERS).getEntity();    // Ordering isn't guaranteed, compare sets    assertEquals(new HashSet<>(Arrays.asList(CONNECTOR_NAME, CONNECTOR2_NAME)), expanded.keySet());    assertEquals(connector2, expanded.get(CONNECTOR2_NAME).get("info"));    assertEquals(connector, expanded.get(CONNECTOR_NAME).get("info"));    PowerMock.verifyAll();}
f11249
0
testFullExpandConnectors
public void kafkatest_f11250_0() throws Throwable
{    EasyMock.expect(herder.connectors()).andReturn(Arrays.asList(CONNECTOR2_NAME, CONNECTOR_NAME));    ConnectorInfo connectorInfo = EasyMock.mock(ConnectorInfo.class);    ConnectorInfo connectorInfo2 = EasyMock.mock(ConnectorInfo.class);    EasyMock.expect(herder.connectorInfo(CONNECTOR2_NAME)).andReturn(connectorInfo2);    EasyMock.expect(herder.connectorInfo(CONNECTOR_NAME)).andReturn(connectorInfo);    ConnectorStateInfo connector = EasyMock.mock(ConnectorStateInfo.class);    ConnectorStateInfo connector2 = EasyMock.mock(ConnectorStateInfo.class);    EasyMock.expect(herder.connectorStatus(CONNECTOR2_NAME)).andReturn(connector2);    EasyMock.expect(herder.connectorStatus(CONNECTOR_NAME)).andReturn(connector);    forward = EasyMock.mock(UriInfo.class);    MultivaluedMap<String, String> queryParams = new MultivaluedHashMap<>();    queryParams.put("expand", Arrays.asList("info", "status"));    EasyMock.expect(forward.getQueryParameters()).andReturn(queryParams).anyTimes();    EasyMock.replay(forward);    PowerMock.replayAll();    Map<String, Map<String, Object>> expanded = (Map<String, Map<String, Object>>) connectorsResource.listConnectors(forward, NULL_HEADERS).getEntity();    // Ordering isn't guaranteed, compare sets    assertEquals(new HashSet<>(Arrays.asList(CONNECTOR_NAME, CONNECTOR2_NAME)), expanded.keySet());    assertEquals(connectorInfo2, expanded.get(CONNECTOR2_NAME).get("info"));    assertEquals(connectorInfo, expanded.get(CONNECTOR_NAME).get("info"));    assertEquals(connector2, expanded.get(CONNECTOR2_NAME).get("status"));    assertEquals(connector, expanded.get(CONNECTOR_NAME).get("status"));    PowerMock.verifyAll();}
f11250
0
testCreateConnectorNoName
public void kafkatest_f11259_0() throws Throwable
{    // Clone CONNECTOR_CONFIG_WITHOUT_NAME Map, as createConnector changes it (puts the name in it) and this    // will affect later tests    Map<String, String> inputConfig = getConnectorConfig(CONNECTOR_CONFIG_WITHOUT_NAME);    final CreateConnectorRequest bodyIn = new CreateConnectorRequest(null, inputConfig);    final CreateConnectorRequest bodyOut = new CreateConnectorRequest("", CONNECTOR_CONFIG_WITH_EMPTY_NAME);    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.putConnectorConfig(EasyMock.eq(bodyOut.name()), EasyMock.eq(bodyOut.config()), EasyMock.eq(false), EasyMock.capture(cb));    expectAndCallbackResult(cb, new Herder.Created<>(true, new ConnectorInfo(bodyOut.name(), bodyOut.config(), CONNECTOR_TASK_NAMES, ConnectorType.SOURCE)));    PowerMock.replayAll();    connectorsResource.createConnector(FORWARD, NULL_HEADERS, bodyIn);    PowerMock.verifyAll();}
f11259
0
testDeleteConnector
public void kafkatest_f11260_0() throws Throwable
{    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.deleteConnectorConfig(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(cb));    expectAndCallbackResult(cb, null);    PowerMock.replayAll();    connectorsResource.destroyConnector(CONNECTOR_NAME, NULL_HEADERS, FORWARD);    PowerMock.verifyAll();}
f11260
0
testPutConnectorConfigWithSpecialCharsInName
public void kafkatest_f11269_0() throws Throwable
{    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.putConnectorConfig(EasyMock.eq(CONNECTOR_NAME_SPECIAL_CHARS), EasyMock.eq(CONNECTOR_CONFIG_SPECIAL_CHARS), EasyMock.eq(true), EasyMock.capture(cb));    expectAndCallbackResult(cb, new Herder.Created<>(true, new ConnectorInfo(CONNECTOR_NAME_SPECIAL_CHARS, CONNECTOR_CONFIG_SPECIAL_CHARS, CONNECTOR_TASK_NAMES, ConnectorType.SINK)));    PowerMock.replayAll();    String rspLocation = connectorsResource.putConnectorConfig(CONNECTOR_NAME_SPECIAL_CHARS, NULL_HEADERS, FORWARD, CONNECTOR_CONFIG_SPECIAL_CHARS).getLocation().toString();    String decoded = new URI(rspLocation).getPath();    Assert.assertEquals("/connectors/" + CONNECTOR_NAME_SPECIAL_CHARS, decoded);    PowerMock.verifyAll();}
f11269
0
testPutConnectorConfigWithControlSequenceInName
public void kafkatest_f11270_0() throws Throwable
{    final Capture<Callback<Herder.Created<ConnectorInfo>>> cb = Capture.newInstance();    herder.putConnectorConfig(EasyMock.eq(CONNECTOR_NAME_CONTROL_SEQUENCES1), EasyMock.eq(CONNECTOR_CONFIG_CONTROL_SEQUENCES), EasyMock.eq(true), EasyMock.capture(cb));    expectAndCallbackResult(cb, new Herder.Created<>(true, new ConnectorInfo(CONNECTOR_NAME_CONTROL_SEQUENCES1, CONNECTOR_CONFIG_CONTROL_SEQUENCES, CONNECTOR_TASK_NAMES, ConnectorType.SINK)));    PowerMock.replayAll();    String rspLocation = connectorsResource.putConnectorConfig(CONNECTOR_NAME_CONTROL_SEQUENCES1, NULL_HEADERS, FORWARD, CONNECTOR_CONFIG_CONTROL_SEQUENCES).getLocation().toString();    String decoded = new URI(rspLocation).getPath();    Assert.assertEquals("/connectors/" + CONNECTOR_NAME_CONTROL_SEQUENCES1, decoded);    PowerMock.verifyAll();}
f11270
0
testRestartConnectorOwnerRedirect
public void kafkatest_f11279_0() throws Throwable
{    final Capture<Callback<Void>> cb = Capture.newInstance();    herder.restartConnector(EasyMock.eq(CONNECTOR_NAME), EasyMock.capture(cb));    String ownerUrl = "http://owner:8083";    expectAndCallbackException(cb, new NotAssignedException("not owner test", ownerUrl));    EasyMock.expect(RestClient.httpRequest(EasyMock.eq("http://owner:8083/connectors/" + CONNECTOR_NAME + "/restart?forward=false"), EasyMock.eq("POST"), EasyMock.isNull(), EasyMock.isNull(), EasyMock.<TypeReference>anyObject(), EasyMock.anyObject(WorkerConfig.class))).andReturn(new RestClient.HttpResponse<>(202, new HashMap<String, String>(), null));    PowerMock.replayAll();    connectorsResource.restartConnector(CONNECTOR_NAME, NULL_HEADERS, true);    PowerMock.verifyAll();}
f11279
0
testRestartTaskNotFound
public void kafkatest_f11280_0() throws Throwable
{    ConnectorTaskId taskId = new ConnectorTaskId(CONNECTOR_NAME, 0);    final Capture<Callback<Void>> cb = Capture.newInstance();    herder.restartTask(EasyMock.eq(taskId), EasyMock.capture(cb));    expectAndCallbackException(cb, new NotFoundException("not found"));    PowerMock.replayAll();    connectorsResource.restartTask(CONNECTOR_NAME, 0, NULL_HEADERS, FORWARD);    PowerMock.verifyAll();}
f11280
0
testRootGet
public void kafkatest_f11289_0()
{    EasyMock.expect(herder.kafkaClusterId()).andReturn(MockAdminClient.DEFAULT_CLUSTER_ID);    replayAll();    ServerInfo info = rootResource.serverInfo();    assertEquals(AppInfoParser.getVersion(), info.version());    assertEquals(AppInfoParser.getCommitId(), info.commit());    assertEquals(MockAdminClient.DEFAULT_CLUSTER_ID, info.clusterId());    verifyAll();}
f11289
0
tearDown
public void kafkatest_f11290_0()
{    server.stop();}
f11290
0
testGetOrDefault
public void kafkatest_f11299_0()
{    String existingKey = "exists";    String missingKey = "missing";    String value = "value";    String defaultValue = "default";    Map<String, Object> map = new HashMap<>();    map.put("exists", "value");    Assert.assertEquals(SSLUtils.getOrDefault(map, existingKey, defaultValue), value);    Assert.assertEquals(SSLUtils.getOrDefault(map, missingKey, defaultValue), defaultValue);}
f11299
0
testCreateServerSideSslContextFactory
public void kafkatest_f11300_0()
{    Map<String, String> configMap = new HashMap<>(DEFAULT_CONFIG);    configMap.put("ssl.keystore.location", "/path/to/keystore");    configMap.put("ssl.keystore.password", "123456");    configMap.put("ssl.key.password", "123456");    configMap.put("ssl.truststore.location", "/path/to/truststore");    configMap.put("ssl.truststore.password", "123456");    configMap.put("ssl.provider", "SunJSSE");    configMap.put("ssl.cipher.suites", "SSL_RSA_WITH_RC4_128_SHA,SSL_RSA_WITH_RC4_128_MD5");    configMap.put("ssl.secure.random.implementation", "SHA1PRNG");    configMap.put("ssl.client.auth", "required");    configMap.put("ssl.endpoint.identification.algorithm", "HTTPS");    configMap.put("ssl.keystore.type", "JKS");    configMap.put("ssl.protocol", "TLS");    configMap.put("ssl.truststore.type", "JKS");    configMap.put("ssl.enabled.protocols", "TLSv1.2,TLSv1.1,TLSv1");    configMap.put("ssl.keymanager.algorithm", "SunX509");    configMap.put("ssl.trustmanager.algorithm", "PKIX");    DistributedConfig config = new DistributedConfig(configMap);    SslContextFactory ssl = SSLUtils.createServerSideSslContextFactory(config);    Assert.assertEquals("file:///path/to/keystore", ssl.getKeyStorePath());    Assert.assertEquals("file:///path/to/truststore", ssl.getTrustStorePath());    Assert.assertEquals("SunJSSE", ssl.getProvider());    Assert.assertArrayEquals(new String[] { "SSL_RSA_WITH_RC4_128_SHA", "SSL_RSA_WITH_RC4_128_MD5" }, ssl.getIncludeCipherSuites());    Assert.assertEquals("SHA1PRNG", ssl.getSecureRandomAlgorithm());    Assert.assertTrue(ssl.getNeedClientAuth());    Assert.assertFalse(ssl.getWantClientAuth());    Assert.assertEquals("JKS", ssl.getKeyStoreType());    Assert.assertEquals("JKS", ssl.getTrustStoreType());    Assert.assertEquals("TLS", ssl.getProtocol());    Assert.assertArrayEquals(new String[] { "TLSv1.2", "TLSv1.1", "TLSv1" }, ssl.getIncludeProtocols());    Assert.assertEquals("SunX509", ssl.getKeyManagerFactoryAlgorithm());    Assert.assertEquals("PKIX", ssl.getTrustManagerFactoryAlgorithm());}
f11300
0
testCreateSourceConnector
public void kafkatest_f11309_0() throws Exception
{    connector = PowerMock.createMock(BogusSourceConnector.class);    expectAdd(SourceSink.SOURCE);    Map<String, String> config = connectorConfig(SourceSink.SOURCE);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    expectConfigValidation(connectorMock, true, config);    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, config, false, createCallback);    PowerMock.verifyAll();}
f11309
0
testCreateConnectorFailedBasicValidation
public void kafkatest_f11310_0()
{    // Basic validation should be performed and return an error, but should still evaluate the connector's config    connector = PowerMock.createMock(BogusSourceConnector.class);    Map<String, String> config = connectorConfig(SourceSink.SOURCE);    config.remove(ConnectorConfig.NAME_CONFIG);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    EasyMock.expect(connectorMock.config()).andStubReturn(new ConfigDef());    ConfigValue validatedValue = new ConfigValue("foo.bar");    EasyMock.expect(connectorMock.validate(config)).andReturn(new Config(singletonList(validatedValue)));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);    createCallback.onCompletion(EasyMock.<BadRequestException>anyObject(), EasyMock.<Herder.Created<ConnectorInfo>>isNull());    PowerMock.expectLastCall();    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, config, false, createCallback);    PowerMock.verifyAll();}
f11310
0
testCreateAndStop
public void kafkatest_f11319_0() throws Exception
{    connector = PowerMock.createMock(BogusSourceConnector.class);    expectAdd(SourceSink.SOURCE);    Map<String, String> connectorConfig = connectorConfig(SourceSink.SOURCE);    Connector connectorMock = PowerMock.createMock(SourceConnector.class);    expectConfigValidation(connectorMock, true, connectorConfig);    // herder.stop() should stop any running connectors and tasks even if destroyConnector was not invoked    expectStop();    statusBackingStore.stop();    EasyMock.expectLastCall();    worker.stop();    EasyMock.expectLastCall();    PowerMock.replayAll();    herder.putConnectorConfig(CONNECTOR_NAME, connectorConfig, false, createCallback);    herder.stop();    PowerMock.verifyAll();}
f11319
0
testAccessors
public void kafkatest_f11320_0() throws Exception
{    Map<String, String> connConfig = connectorConfig(SourceSink.SOURCE);    System.out.println(connConfig);    Callback<Collection<String>> listConnectorsCb = PowerMock.createMock(Callback.class);    Callback<ConnectorInfo> connectorInfoCb = PowerMock.createMock(Callback.class);    Callback<Map<String, String>> connectorConfigCb = PowerMock.createMock(Callback.class);    Callback<List<TaskInfo>> taskConfigsCb = PowerMock.createMock(Callback.class);    // Check accessors with empty worker    listConnectorsCb.onCompletion(null, Collections.EMPTY_SET);    EasyMock.expectLastCall();    connectorInfoCb.onCompletion(EasyMock.<NotFoundException>anyObject(), EasyMock.<ConnectorInfo>isNull());    EasyMock.expectLastCall();    connectorConfigCb.onCompletion(EasyMock.<NotFoundException>anyObject(), EasyMock.<Map<String, String>>isNull());    EasyMock.expectLastCall();    taskConfigsCb.onCompletion(EasyMock.<NotFoundException>anyObject(), EasyMock.<List<TaskInfo>>isNull());    EasyMock.expectLastCall();    // Create connector    connector = PowerMock.createMock(BogusSourceConnector.class);    expectAdd(SourceSink.SOURCE);    expectConfigValidation(connector, true, connConfig);    // Validate accessors with 1 connector    listConnectorsCb.onCompletion(null, singleton(CONNECTOR_NAME));    EasyMock.expectLastCall();    ConnectorInfo connInfo = new ConnectorInfo(CONNECTOR_NAME, connConfig, Arrays.asList(new ConnectorTaskId(CONNECTOR_NAME, 0)), ConnectorType.SOURCE);    connectorInfoCb.onCompletion(null, connInfo);    EasyMock.expectLastCall();    connectorConfigCb.onCompletion(null, connConfig);    EasyMock.expectLastCall();    TaskInfo taskInfo = new TaskInfo(new ConnectorTaskId(CONNECTOR_NAME, 0), taskConfig(SourceSink.SOURCE));    taskConfigsCb.onCompletion(null, Arrays.asList(taskInfo));    EasyMock.expectLastCall();    PowerMock.replayAll();    // All operations are synchronous for StandaloneHerder, so we don't need to actually wait after making each call    herder.connectors(listConnectorsCb);    herder.connectorInfo(CONNECTOR_NAME, connectorInfoCb);    herder.connectorConfig(CONNECTOR_NAME, connectorConfigCb);    herder.taskConfigs(CONNECTOR_NAME, taskConfigsCb);    herder.putConnectorConfig(CONNECTOR_NAME, connConfig, false, createCallback);    EasyMock.reset(transformer);    EasyMock.expect(transformer.transform(EasyMock.eq(CONNECTOR_NAME), EasyMock.anyObject())).andThrow(new AssertionError("Config transformation should not occur when requesting connector or task info")).anyTimes();    EasyMock.replay(transformer);    herder.connectors(listConnectorsCb);    herder.connectorInfo(CONNECTOR_NAME, connectorInfoCb);    herder.connectorConfig(CONNECTOR_NAME, connectorConfigCb);    herder.taskConfigs(CONNECTOR_NAME, taskConfigsCb);    PowerMock.verifyAll();}
f11320
0
expectConfigValidation
private void kafkatest_f11329_0(Connector connectorMock, boolean shouldCreateConnector, Map<String, String>... configs)
{    // config validation    EasyMock.expect(worker.configTransformer()).andReturn(transformer).times(2);    final Capture<Map<String, String>> configCapture = EasyMock.newCapture();    EasyMock.expect(transformer.transform(EasyMock.capture(configCapture))).andAnswer(configCapture::getValue);    EasyMock.expect(worker.getPlugins()).andReturn(plugins).times(3);    EasyMock.expect(plugins.compareAndSwapLoaders(connectorMock)).andReturn(delegatingLoader);    if (shouldCreateConnector) {        EasyMock.expect(worker.getPlugins()).andReturn(plugins);        EasyMock.expect(plugins.newConnector(EasyMock.anyString())).andReturn(connectorMock);    }    EasyMock.expect(connectorMock.config()).andStubReturn(new ConfigDef());    for (Map<String, String> config : configs) EasyMock.expect(connectorMock.validate(config)).andReturn(new Config(Collections.<ConfigValue>emptyList()));    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader);}
f11329
0
setUp
public void kafkatest_f11330_0()
{    time = new MockTime();    time.sleep(1000L);    tracker = new StateTracker();    state = State.UNASSIGNED;}
f11330
0
taskClass
public Class<? extends Task> kafkatest_f11342_0()
{    return null;}
f11342
0
taskConfigs
public List<Map<String, String>> kafkatest_f11343_0(int maxTasks)
{    return null;}
f11343
0
testEmbeddedConfigReplaceField
public void kafkatest_f11353_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", ReplaceField.Value.class.getName());    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
f11353
0
testEmbeddedConfigSetSchemaMetadata
public void kafkatest_f11354_0()
{    // Validate that we can construct a Connector config containing the extended config for the transform    HashMap<String, String> connProps = new HashMap<>();    connProps.put("name", "foo");    connProps.put("connector.class", MockConnector.class.getName());    connProps.put("transforms", "example");    connProps.put("transforms.example.type", SetSchemaMetadata.Value.class.getName());    // Safe when we're only constructing the config    Plugins plugins = null;    new ConnectorConfig(plugins, connProps);}
f11354
0
testTransformNullConfiguration
public void kafkatest_f11363_0()
{    assertNull(configTransformer.transform(MY_CONNECTOR, null));}
f11363
0
get
public ConfigData kafkatest_f11365_0(String path)
{    return null;}
f11365
0
testStartupAndPause
public void kafkatest_f11375_0()
{    connector.version();    expectLastCall().andReturn(VERSION);    connector.initialize(EasyMock.notNull(ConnectorContext.class));    expectLastCall();    connector.start(CONFIG);    expectLastCall();    listener.onStartup(CONNECTOR);    expectLastCall();    connector.stop();    expectLastCall();    listener.onPause(CONNECTOR);    expectLastCall();    listener.onShutdown(CONNECTOR);    expectLastCall();    replayAll();    WorkerConnector workerConnector = new WorkerConnector(CONNECTOR, connector, ctx, metrics, listener);    workerConnector.initialize(connectorConfig);    assertInitializedMetric(workerConnector);    workerConnector.transitionTo(TargetState.STARTED);    assertRunningMetric(workerConnector);    workerConnector.transitionTo(TargetState.PAUSED);    assertPausedMetric(workerConnector);    workerConnector.shutdown();    assertStoppedMetric(workerConnector);    verifyAll();}
f11375
0
testOnResume
public void kafkatest_f11376_0()
{    connector.version();    expectLastCall().andReturn(VERSION);    connector.initialize(EasyMock.notNull(ConnectorContext.class));    expectLastCall();    listener.onPause(CONNECTOR);    expectLastCall();    connector.start(CONFIG);    expectLastCall();    listener.onResume(CONNECTOR);    expectLastCall();    connector.stop();    expectLastCall();    listener.onShutdown(CONNECTOR);    expectLastCall();    replayAll();    WorkerConnector workerConnector = new WorkerConnector(CONNECTOR, connector, ctx, metrics, listener);    workerConnector.initialize(connectorConfig);    assertInitializedMetric(workerConnector);    workerConnector.transitionTo(TargetState.PAUSED);    assertPausedMetric(workerConnector);    workerConnector.transitionTo(TargetState.STARTED);    assertRunningMetric(workerConnector);    workerConnector.shutdown();    assertStoppedMetric(workerConnector);    verifyAll();}
f11376
0
assertStoppedMetric
protected void kafkatest_f11385_0(WorkerConnector workerConnector)
{    assertTrue(workerConnector.metrics().isUnassigned());    assertFalse(workerConnector.metrics().isFailed());    assertFalse(workerConnector.metrics().isPaused());    assertFalse(workerConnector.metrics().isRunning());}
f11385
0
assertInitializedMetric
protected void kafkatest_f11386_0(WorkerConnector workerConnector)
{    assertTrue(workerConnector.metrics().isUnassigned());    assertFalse(workerConnector.metrics().isFailed());    assertFalse(workerConnector.metrics().isPaused());    assertFalse(workerConnector.metrics().isRunning());    MetricGroup metricGroup = workerConnector.metrics().metricGroup();    String status = metrics.currentMetricValueAsString(metricGroup, "status");    String type = metrics.currentMetricValueAsString(metricGroup, "connector-type");    String clazz = metrics.currentMetricValueAsString(metricGroup, "connector-class");    String version = metrics.currentMetricValueAsString(metricGroup, "connector-version");    assertEquals(type, "unknown");    assertNotNull(clazz);    assertEquals(VERSION, version);}
f11386
0
testWakeupInCommitSyncCausesRetry
public void kafkatest_f11395_0() throws Exception
{    createTask(initialState);    expectInitializeTask();    expectPollInitialAssignment();    expectConsumerPoll(1);    expectConversionAndTransformation(1);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    final List<TopicPartition> partitions = asList(TOPIC_PARTITION, TOPIC_PARTITION2);    final Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();    offsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    offsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    sinkTask.preCommit(offsets);    EasyMock.expectLastCall().andReturn(offsets);    // first one raises wakeup    consumer.commitSync(EasyMock.<Map<TopicPartition, OffsetAndMetadata>>anyObject());    EasyMock.expectLastCall().andThrow(new WakeupException());    // we should retry and complete the commit    consumer.commitSync(EasyMock.<Map<TopicPartition, OffsetAndMetadata>>anyObject());    EasyMock.expectLastCall();    sinkTask.close(new HashSet<>(partitions));    EasyMock.expectLastCall();    EasyMock.expect(consumer.position(TOPIC_PARTITION)).andReturn(FIRST_OFFSET);    EasyMock.expect(consumer.position(TOPIC_PARTITION2)).andReturn(FIRST_OFFSET);    sinkTask.open(partitions);    EasyMock.expectLastCall();    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            rebalanceListener.getValue().onPartitionsRevoked(partitions);            rebalanceListener.getValue().onPartitionsAssigned(partitions);            return ConsumerRecords.empty();        }    });    EasyMock.expect(consumer.assignment()).andReturn(new HashSet<>(partitions));    consumer.resume(Collections.singleton(TOPIC_PARTITION));    EasyMock.expectLastCall();    consumer.resume(Collections.singleton(TOPIC_PARTITION2));    EasyMock.expectLastCall();    statusListener.onResume(taskId);    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    time.sleep(30000L);    workerTask.initializeAndStart();    time.sleep(30000L);    // poll for initial assignment    workerTask.iteration();    time.sleep(30000L);    // first record delivered    workerTask.iteration();    // now rebalance with the wakeup triggered    workerTask.iteration();    time.sleep(30000L);    assertSinkMetricValue("partition-count", 2);    assertSinkMetricValue("sink-record-read-total", 1.0);    assertSinkMetricValue("sink-record-send-total", 1.0);    assertSinkMetricValue("sink-record-active-count", 0.0);    assertSinkMetricValue("sink-record-active-count-max", 1.0);    assertSinkMetricValue("sink-record-active-count-avg", 0.33333);    assertSinkMetricValue("offset-commit-seq-no", 1.0);    assertSinkMetricValue("offset-commit-completion-total", 1.0);    assertSinkMetricValue("offset-commit-skip-total", 0.0);    assertTaskMetricValue("status", "running");    assertTaskMetricValue("running-ratio", 1.0);    assertTaskMetricValue("pause-ratio", 0.0);    assertTaskMetricValue("batch-size-max", 1.0);    assertTaskMetricValue("batch-size-avg", 1.0);    assertTaskMetricValue("offset-commit-max-time-ms", 0.0);    assertTaskMetricValue("offset-commit-avg-time-ms", 0.0);    assertTaskMetricValue("offset-commit-failure-percentage", 0.0);    assertTaskMetricValue("offset-commit-success-percentage", 1.0);    PowerMock.verifyAll();}
f11395
0
answer
public ConsumerRecords<byte[], byte[]> kafkatest_f11396_0() throws Throwable
{    rebalanceListener.getValue().onPartitionsRevoked(partitions);    rebalanceListener.getValue().onPartitionsAssigned(partitions);    return ConsumerRecords.empty();}
f11396
0
testCommitWithOutOfOrderCallback
public void kafkatest_f11405_0() throws Exception
{    createTask(initialState);    expectInitializeTask();    // iter 1    expectPollInitialAssignment();    // iter 2    expectConsumerPoll(1);    expectConversionAndTransformation(4);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    final Map<TopicPartition, OffsetAndMetadata> workerStartingOffsets = new HashMap<>();    workerStartingOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET));    workerStartingOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    final Map<TopicPartition, OffsetAndMetadata> workerCurrentOffsets = new HashMap<>();    workerCurrentOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    workerCurrentOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    final List<TopicPartition> originalPartitions = asList(TOPIC_PARTITION, TOPIC_PARTITION2);    final List<TopicPartition> rebalancedPartitions = asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3);    final Map<TopicPartition, OffsetAndMetadata> rebalanceOffsets = new HashMap<>();    rebalanceOffsets.put(TOPIC_PARTITION, workerCurrentOffsets.get(TOPIC_PARTITION));    rebalanceOffsets.put(TOPIC_PARTITION2, workerCurrentOffsets.get(TOPIC_PARTITION2));    rebalanceOffsets.put(TOPIC_PARTITION3, new OffsetAndMetadata(FIRST_OFFSET));    final Map<TopicPartition, OffsetAndMetadata> postRebalanceCurrentOffsets = new HashMap<>();    postRebalanceCurrentOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 3));    postRebalanceCurrentOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET));    postRebalanceCurrentOffsets.put(TOPIC_PARTITION3, new OffsetAndMetadata(FIRST_OFFSET + 2));    // iter 3 - note that we return the current offset to indicate they should be committed    sinkTask.preCommit(workerCurrentOffsets);    EasyMock.expectLastCall().andReturn(workerCurrentOffsets);    // We need to delay the result of trying to commit offsets to Kafka via the consumer.commitAsync    // method. We do this so that we can test that the callback is not called until after the rebalance    // changes the lastCommittedOffsets. To fake this for tests we have the commitAsync build a function    // that will call the callback with the appropriate parameters, and we'll run that function later.    final AtomicReference<Runnable> asyncCallbackRunner = new AtomicReference<>();    final AtomicBoolean asyncCallbackRan = new AtomicBoolean();    consumer.commitAsync(EasyMock.eq(workerCurrentOffsets), EasyMock.<OffsetCommitCallback>anyObject());    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @SuppressWarnings("unchecked")        @Override        public Void answer() throws Throwable {            // Grab the arguments passed to the consumer.commitAsync method            final Object[] args = EasyMock.getCurrentArguments();            final Map<TopicPartition, OffsetAndMetadata> offsets = (Map<TopicPartition, OffsetAndMetadata>) args[0];            final OffsetCommitCallback callback = (OffsetCommitCallback) args[1];            asyncCallbackRunner.set(new Runnable() {                @Override                public void run() {                    callback.onComplete(offsets, null);                    asyncCallbackRan.set(true);                }            });            return null;        }    });    // Expect the next poll to discover and perform the rebalance, THEN complete the previous callback handler,    // and then return one record for TP1 and one for TP3.    final AtomicBoolean rebalanced = new AtomicBoolean();    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            // Rebalance always begins with revoking current partitions ...            rebalanceListener.getValue().onPartitionsRevoked(originalPartitions);            // Respond to the rebalance            Map<TopicPartition, Long> offsets = new HashMap<>();            offsets.put(TOPIC_PARTITION, rebalanceOffsets.get(TOPIC_PARTITION).offset());            offsets.put(TOPIC_PARTITION2, rebalanceOffsets.get(TOPIC_PARTITION2).offset());            offsets.put(TOPIC_PARTITION3, rebalanceOffsets.get(TOPIC_PARTITION3).offset());            sinkTaskContext.getValue().offset(offsets);            rebalanceListener.getValue().onPartitionsAssigned(rebalancedPartitions);            rebalanced.set(true);            // Run the previous async commit handler            asyncCallbackRunner.get().run();            // And prep the two records to return            long timestamp = RecordBatch.NO_TIMESTAMP;            TimestampType timestampType = TimestampType.NO_TIMESTAMP_TYPE;            List<ConsumerRecord<byte[], byte[]>> records = new ArrayList<>();            records.add(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturnedTp1 + 1, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));            records.add(new ConsumerRecord<>(TOPIC, PARTITION3, FIRST_OFFSET + recordsReturnedTp3 + 1, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));            recordsReturnedTp1 += 1;            recordsReturnedTp3 += 1;            return new ConsumerRecords<>(Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), records));        }    });    // onPartitionsRevoked    sinkTask.preCommit(workerCurrentOffsets);    EasyMock.expectLastCall().andReturn(workerCurrentOffsets);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    sinkTask.close(workerCurrentOffsets.keySet());    EasyMock.expectLastCall();    consumer.commitSync(workerCurrentOffsets);    EasyMock.expectLastCall();    // onPartitionsAssigned - step 1    final long offsetTp1 = rebalanceOffsets.get(TOPIC_PARTITION).offset();    final long offsetTp2 = rebalanceOffsets.get(TOPIC_PARTITION2).offset();    final long offsetTp3 = rebalanceOffsets.get(TOPIC_PARTITION3).offset();    EasyMock.expect(consumer.position(TOPIC_PARTITION)).andReturn(offsetTp1);    EasyMock.expect(consumer.position(TOPIC_PARTITION2)).andReturn(offsetTp2);    EasyMock.expect(consumer.position(TOPIC_PARTITION3)).andReturn(offsetTp3);    // onPartitionsAssigned - step 2    sinkTask.open(rebalancedPartitions);    EasyMock.expectLastCall();    // onPartitionsAssigned - step 3 rewind    consumer.seek(TOPIC_PARTITION, offsetTp1);    EasyMock.expectLastCall();    consumer.seek(TOPIC_PARTITION2, offsetTp2);    EasyMock.expectLastCall();    consumer.seek(TOPIC_PARTITION3, offsetTp3);    EasyMock.expectLastCall();    // iter 4 - note that we return the current offset to indicate they should be committed    sinkTask.preCommit(postRebalanceCurrentOffsets);    EasyMock.expectLastCall().andReturn(postRebalanceCurrentOffsets);    final Capture<OffsetCommitCallback> callback = EasyMock.newCapture();    consumer.commitAsync(EasyMock.eq(postRebalanceCurrentOffsets), EasyMock.capture(callback));    EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callback.getValue().onComplete(postRebalanceCurrentOffsets, null);            return null;        }    });    // no actual consumer.commit() triggered    expectConsumerPoll(1);    sinkTask.put(EasyMock.<Collection<SinkRecord>>anyObject());    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    workerTask.initializeAndStart();    // iter 1 -- initial assignment    workerTask.iteration();    assertEquals(workerStartingOffsets, Whitebox.getInternalState(workerTask, "currentOffsets"));    assertEquals(workerStartingOffsets, Whitebox.getInternalState(workerTask, "lastCommittedOffsets"));    time.sleep(WorkerConfig.OFFSET_COMMIT_TIMEOUT_MS_DEFAULT);    // iter 2 -- deliver 2 records    workerTask.iteration();    sinkTaskContext.getValue().requestCommit();    // iter 3 -- commit in progress    workerTask.iteration();    assertSinkMetricValue("partition-count", 3);    assertSinkMetricValue("sink-record-read-total", 3.0);    assertSinkMetricValue("sink-record-send-total", 3.0);    assertSinkMetricValue("sink-record-active-count", 4.0);    assertSinkMetricValue("sink-record-active-count-max", 4.0);    assertSinkMetricValue("sink-record-active-count-avg", 0.71429);    assertSinkMetricValue("offset-commit-seq-no", 2.0);    assertSinkMetricValue("offset-commit-completion-total", 1.0);    assertSinkMetricValue("offset-commit-skip-total", 1.0);    assertTaskMetricValue("status", "running");    assertTaskMetricValue("running-ratio", 1.0);    assertTaskMetricValue("pause-ratio", 0.0);    assertTaskMetricValue("batch-size-max", 2.0);    assertTaskMetricValue("batch-size-avg", 1.0);    assertTaskMetricValue("offset-commit-max-time-ms", 0.0);    assertTaskMetricValue("offset-commit-avg-time-ms", 0.0);    assertTaskMetricValue("offset-commit-failure-percentage", 0.0);    assertTaskMetricValue("offset-commit-success-percentage", 1.0);    assertTrue(asyncCallbackRan.get());    assertTrue(rebalanced.get());    // Check that the offsets were not reset by the out-of-order async commit callback    assertEquals(postRebalanceCurrentOffsets, Whitebox.getInternalState(workerTask, "currentOffsets"));    assertEquals(rebalanceOffsets, Whitebox.getInternalState(workerTask, "lastCommittedOffsets"));    time.sleep(WorkerConfig.OFFSET_COMMIT_TIMEOUT_MS_DEFAULT);    sinkTaskContext.getValue().requestCommit();    // iter 4 -- commit in progress    workerTask.iteration();    // Check that the offsets were not reset by the out-of-order async commit callback    assertEquals(postRebalanceCurrentOffsets, Whitebox.getInternalState(workerTask, "currentOffsets"));    assertEquals(postRebalanceCurrentOffsets, Whitebox.getInternalState(workerTask, "lastCommittedOffsets"));    assertSinkMetricValue("partition-count", 3);    assertSinkMetricValue("sink-record-read-total", 4.0);    assertSinkMetricValue("sink-record-send-total", 4.0);    assertSinkMetricValue("sink-record-active-count", 0.0);    assertSinkMetricValue("sink-record-active-count-max", 4.0);    assertSinkMetricValue("sink-record-active-count-avg", 0.5555555);    assertSinkMetricValue("offset-commit-seq-no", 3.0);    assertSinkMetricValue("offset-commit-completion-total", 2.0);    assertSinkMetricValue("offset-commit-skip-total", 1.0);    assertTaskMetricValue("status", "running");    assertTaskMetricValue("running-ratio", 1.0);    assertTaskMetricValue("pause-ratio", 0.0);    assertTaskMetricValue("batch-size-max", 2.0);    assertTaskMetricValue("batch-size-avg", 1.0);    assertTaskMetricValue("offset-commit-max-time-ms", 0.0);    assertTaskMetricValue("offset-commit-avg-time-ms", 0.0);    assertTaskMetricValue("offset-commit-failure-percentage", 0.0);    assertTaskMetricValue("offset-commit-success-percentage", 1.0);    PowerMock.verifyAll();}
f11405
0
answer
public Void kafkatest_f11406_0() throws Throwable
{    // Grab the arguments passed to the consumer.commitAsync method    final Object[] args = EasyMock.getCurrentArguments();    final Map<TopicPartition, OffsetAndMetadata> offsets = (Map<TopicPartition, OffsetAndMetadata>) args[0];    final OffsetCommitCallback callback = (OffsetCommitCallback) args[1];    asyncCallbackRunner.set(new Runnable() {        @Override        public void run() {            callback.onComplete(offsets, null);            asyncCallbackRan.set(true);        }    });    return null;}
f11406
0
testMetricsGroup
public void kafkatest_f11415_0()
{    SinkTaskMetricsGroup group = new SinkTaskMetricsGroup(taskId, metrics);    SinkTaskMetricsGroup group1 = new SinkTaskMetricsGroup(taskId1, metrics);    for (int i = 0; i != 10; ++i) {        group.recordRead(1);        group.recordSend(2);        group.recordPut(3);        group.recordPartitionCount(4);        group.recordOffsetSequenceNumber(5);    }    Map<TopicPartition, OffsetAndMetadata> committedOffsets = new HashMap<>();    committedOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 1));    group.recordCommittedOffsets(committedOffsets);    Map<TopicPartition, OffsetAndMetadata> consumedOffsets = new HashMap<>();    consumedOffsets.put(TOPIC_PARTITION, new OffsetAndMetadata(FIRST_OFFSET + 10));    group.recordConsumedOffsets(consumedOffsets);    for (int i = 0; i != 20; ++i) {        group1.recordRead(1);        group1.recordSend(2);        group1.recordPut(30);        group1.recordPartitionCount(40);        group1.recordOffsetSequenceNumber(50);    }    committedOffsets = new HashMap<>();    committedOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET + 2));    committedOffsets.put(TOPIC_PARTITION3, new OffsetAndMetadata(FIRST_OFFSET + 3));    group1.recordCommittedOffsets(committedOffsets);    consumedOffsets = new HashMap<>();    consumedOffsets.put(TOPIC_PARTITION2, new OffsetAndMetadata(FIRST_OFFSET + 20));    consumedOffsets.put(TOPIC_PARTITION3, new OffsetAndMetadata(FIRST_OFFSET + 30));    group1.recordConsumedOffsets(consumedOffsets);    assertEquals(0.333, metrics.currentMetricValueAsDouble(group.metricGroup(), "sink-record-read-rate"), 0.001d);    assertEquals(0.667, metrics.currentMetricValueAsDouble(group.metricGroup(), "sink-record-send-rate"), 0.001d);    assertEquals(9, metrics.currentMetricValueAsDouble(group.metricGroup(), "sink-record-active-count"), 0.001d);    assertEquals(4, metrics.currentMetricValueAsDouble(group.metricGroup(), "partition-count"), 0.001d);    assertEquals(5, metrics.currentMetricValueAsDouble(group.metricGroup(), "offset-commit-seq-no"), 0.001d);    assertEquals(3, metrics.currentMetricValueAsDouble(group.metricGroup(), "put-batch-max-time-ms"), 0.001d);    // Close the group    group.close();    for (MetricName metricName : group.metricGroup().metrics().metrics().keySet()) {        // Metrics for this group should no longer exist        assertFalse(group.metricGroup().groupId().includes(metricName));    }    // Sensors for this group should no longer exist    assertNull(group.metricGroup().metrics().getSensor("source-record-poll"));    assertNull(group.metricGroup().metrics().getSensor("source-record-write"));    assertNull(group.metricGroup().metrics().getSensor("poll-batch-time"));    assertEquals(0.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), "sink-record-read-rate"), 0.001d);    assertEquals(1.333, metrics.currentMetricValueAsDouble(group1.metricGroup(), "sink-record-send-rate"), 0.001d);    assertEquals(45, metrics.currentMetricValueAsDouble(group1.metricGroup(), "sink-record-active-count"), 0.001d);    assertEquals(40, metrics.currentMetricValueAsDouble(group1.metricGroup(), "partition-count"), 0.001d);    assertEquals(50, metrics.currentMetricValueAsDouble(group1.metricGroup(), "offset-commit-seq-no"), 0.001d);    assertEquals(30, metrics.currentMetricValueAsDouble(group1.metricGroup(), "put-batch-max-time-ms"), 0.001d);}
f11415
0
expectInitializeTask
private void kafkatest_f11416_0() throws Exception
{    consumer.subscribe(EasyMock.eq(asList(TOPIC)), EasyMock.capture(rebalanceListener));    PowerMock.expectLastCall();    sinkTask.initialize(EasyMock.capture(sinkTaskContext));    PowerMock.expectLastCall();    sinkTask.start(TASK_PROPS);    PowerMock.expectLastCall();}
f11416
0
expectConsumerPoll
private void kafkatest_f11425_0(final int numMessages, final long timestamp, final TimestampType timestampType)
{    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            List<ConsumerRecord<byte[], byte[]>> records = new ArrayList<>();            for (int i = 0; i < numMessages; i++) records.add(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturnedTp1 + i, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));            recordsReturnedTp1 += numMessages;            return new ConsumerRecords<>(numMessages > 0 ? Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), records) : Collections.<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>emptyMap());        }    });}
f11425
0
answer
public ConsumerRecords<byte[], byte[]> kafkatest_f11426_0() throws Throwable
{    List<ConsumerRecord<byte[], byte[]>> records = new ArrayList<>();    for (int i = 0; i < numMessages; i++) records.add(new ConsumerRecord<>(TOPIC, PARTITION, FIRST_OFFSET + recordsReturnedTp1 + i, timestamp, timestampType, 0L, 0, 0, RAW_KEY, RAW_VALUE));    recordsReturnedTp1 += numMessages;    return new ConsumerRecords<>(numMessages > 0 ? Collections.singletonMap(new TopicPartition(TOPIC, PARTITION), records) : Collections.<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>emptyMap());}
f11426
0
taskMetricValue
private double kafkatest_f11435_0(String metricName)
{    MetricGroup taskGroup = workerTask.taskMetricsGroup().metricGroup();    double value = metrics.currentMetricValueAsDouble(taskGroup, metricName);    System.out.println("** " + metricName + "=" + value);    return value;}
f11435
0
assertMetrics
private void kafkatest_f11436_0(int minimumPollCountExpected)
{    MetricGroup sinkTaskGroup = workerTask.sinkTaskMetricsGroup().metricGroup();    MetricGroup taskGroup = workerTask.taskMetricsGroup().metricGroup();    double readRate = metrics.currentMetricValueAsDouble(sinkTaskGroup, "sink-record-read-rate");    double readTotal = metrics.currentMetricValueAsDouble(sinkTaskGroup, "sink-record-read-total");    double sendRate = metrics.currentMetricValueAsDouble(sinkTaskGroup, "sink-record-send-rate");    double sendTotal = metrics.currentMetricValueAsDouble(sinkTaskGroup, "sink-record-send-total");}
f11436
0
testAssignmentPauseResume
public void kafkatest_f11445_0() throws Exception
{    // Just validate that the calls are passed through to the consumer, and that where appropriate errors are    // converted    expectInitializeTask();    expectPollInitialAssignment();    expectOnePoll().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            assertEquals(new HashSet<>(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3)), sinkTaskContext.getValue().assignment());            return null;        }    });    EasyMock.expect(consumer.assignment()).andReturn(new HashSet<>(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3)));    expectOnePoll().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            try {                sinkTaskContext.getValue().pause(UNASSIGNED_TOPIC_PARTITION);                fail("Trying to pause unassigned partition should have thrown an Connect exception");            } catch (ConnectException e) {            // expected            }            sinkTaskContext.getValue().pause(TOPIC_PARTITION, TOPIC_PARTITION2);            return null;        }    });    consumer.pause(Arrays.asList(UNASSIGNED_TOPIC_PARTITION));    PowerMock.expectLastCall().andThrow(new IllegalStateException("unassigned topic partition"));    consumer.pause(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2));    PowerMock.expectLastCall();    expectOnePoll().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            try {                sinkTaskContext.getValue().resume(UNASSIGNED_TOPIC_PARTITION);                fail("Trying to resume unassigned partition should have thrown an Connect exception");            } catch (ConnectException e) {            // expected            }            sinkTaskContext.getValue().resume(TOPIC_PARTITION, TOPIC_PARTITION2);            return null;        }    });    consumer.resume(Arrays.asList(UNASSIGNED_TOPIC_PARTITION));    PowerMock.expectLastCall().andThrow(new IllegalStateException("unassigned topic partition"));    consumer.resume(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2));    PowerMock.expectLastCall();    expectStopTask();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    workerTask.initializeAndStart();    workerTask.iteration();    workerTask.iteration();    workerTask.iteration();    workerTask.iteration();    workerTask.stop();    workerTask.close();    PowerMock.verifyAll();}
f11445
0
answer
public Object kafkatest_f11446_0() throws Throwable
{    assertEquals(new HashSet<>(Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3)), sinkTaskContext.getValue().assignment());    return null;}
f11446
0
expectPollInitialAssignment
private void kafkatest_f11455_0() throws Exception
{    final List<TopicPartition> partitions = Arrays.asList(TOPIC_PARTITION, TOPIC_PARTITION2, TOPIC_PARTITION3);    sinkTask.open(partitions);    EasyMock.expectLastCall();    EasyMock.expect(consumer.poll(Duration.ofMillis(EasyMock.anyLong()))).andAnswer(new IAnswer<ConsumerRecords<byte[], byte[]>>() {        @Override        public ConsumerRecords<byte[], byte[]> answer() throws Throwable {            rebalanceListener.getValue().onPartitionsAssigned(partitions);            return ConsumerRecords.empty();        }    });    EasyMock.expect(consumer.position(TOPIC_PARTITION)).andReturn(FIRST_OFFSET);    EasyMock.expect(consumer.position(TOPIC_PARTITION2)).andReturn(FIRST_OFFSET);    EasyMock.expect(consumer.position(TOPIC_PARTITION3)).andReturn(FIRST_OFFSET);    sinkTask.put(Collections.<SinkRecord>emptyList());    EasyMock.expectLastCall();}
f11455
0
answer
public ConsumerRecords<byte[], byte[]> kafkatest_f11456_0() throws Throwable
{    rebalanceListener.getValue().onPartitionsAssigned(partitions);    return ConsumerRecords.empty();}
f11456
0
answer
public Object kafkatest_f11465_0() throws Throwable
{    time.sleep(consumerCommitDelayMs);    if (invokeCallback)        capturedCallback.getValue().onComplete(offsetsToCommit, consumerCommitError);    return null;}
f11465
0
setup
public void kafkatest_f11466_0()
{    super.setup();    Map<String, String> workerProps = new HashMap<>();    workerProps.put("key.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("value.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.key.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.value.converter", "org.apache.kafka.connect.json.JsonConverter");    workerProps.put("internal.key.converter.schemas.enable", "false");    workerProps.put("internal.value.converter.schemas.enable", "false");    workerProps.put("offset.storage.file.filename", "/tmp/connect.offsets");    plugins = new Plugins(workerProps);    config = new StandaloneConfig(workerProps);    producerCallbacks = EasyMock.newCapture();    metrics = new MockConnectMetrics();}
f11466
0
answer
public List<SourceRecord> kafkatest_f11475_0() throws Throwable
{    pollLatch.countDown();    throw exception;}
f11475
0
testCommit
public void kafkatest_f11476_0() throws Exception
{    // Test that the task commits properly when prompted    createWorkerTask();    sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));    EasyMock.expectLastCall();    sourceTask.start(TASK_PROPS);    EasyMock.expectLastCall();    statusListener.onStartup(taskId);    EasyMock.expectLastCall();    // We'll wait for some data, then trigger a flush    final CountDownLatch pollLatch = expectPolls(1);    expectOffsetFlush(true);    sourceTask.stop();    EasyMock.expectLastCall();    expectOffsetFlush(true);    statusListener.onShutdown(taskId);    EasyMock.expectLastCall();    producer.close(EasyMock.anyObject(Duration.class));    EasyMock.expectLastCall();    transformationChain.close();    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    Future<?> taskFuture = executor.submit(workerTask);    assertTrue(awaitLatch(pollLatch));    assertTrue(workerTask.commitOffsets());    workerTask.stop();    assertTrue(workerTask.awaitStop(1000));    taskFuture.get();    assertPollMetrics(1);    PowerMock.verifyAll();}
f11476
0
testSlowTaskStart
public void kafkatest_f11485_0() throws Exception
{    final CountDownLatch startupLatch = new CountDownLatch(1);    final CountDownLatch finishStartupLatch = new CountDownLatch(1);    createWorkerTask();    sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));    EasyMock.expectLastCall();    sourceTask.start(TASK_PROPS);    EasyMock.expectLastCall().andAnswer(new IAnswer<Object>() {        @Override        public Object answer() throws Throwable {            startupLatch.countDown();            assertTrue(awaitLatch(finishStartupLatch));            return null;        }    });    statusListener.onStartup(taskId);    EasyMock.expectLastCall();    sourceTask.stop();    EasyMock.expectLastCall();    expectOffsetFlush(true);    statusListener.onShutdown(taskId);    EasyMock.expectLastCall();    producer.close(EasyMock.anyObject(Duration.class));    EasyMock.expectLastCall();    transformationChain.close();    EasyMock.expectLastCall();    PowerMock.replayAll();    workerTask.initialize(TASK_CONFIG);    Future<?> workerTaskFuture = executor.submit(workerTask);    // Stopping immediately while the other thread has work to do should result in no polling, no offset commits,    // exiting the work thread immediately, and the stop() method will be invoked in the background thread since it    // cannot be invoked immediately in the thread trying to stop the task.    assertTrue(awaitLatch(startupLatch));    workerTask.stop();    finishStartupLatch.countDown();    assertTrue(workerTask.awaitStop(1000));    workerTaskFuture.get();    PowerMock.verifyAll();}
f11485
0
answer
public Object kafkatest_f11486_0() throws Throwable
{    startupLatch.countDown();    assertTrue(awaitLatch(finishStartupLatch));    return null;}
f11486
0
expectSendRecordTaskCommitRecordSucceed
private Capture<ProducerRecord<byte[], byte[]>> kafkatest_f11495_0(boolean anyTimes, boolean isRetry) throws InterruptedException
{    return expectSendRecord(anyTimes, isRetry, true, true);}
f11495
0
expectSendRecordTaskCommitRecordFail
private Capture<ProducerRecord<byte[], byte[]>> kafkatest_f11496_0(boolean anyTimes, boolean isRetry) throws InterruptedException
{    return expectSendRecord(anyTimes, isRetry, true, false);}
f11496
0
expectOffsetFlush
private void kafkatest_f11505_0(boolean succeed) throws Exception
{    EasyMock.expect(offsetWriter.beginFlush()).andReturn(true);    Future<Void> flushFuture = PowerMock.createMock(Future.class);    EasyMock.expect(offsetWriter.doFlush(EasyMock.anyObject(Callback.class))).andReturn(flushFuture);    // Should throw for failure    IExpectationSetters<Void> futureGetExpect = EasyMock.expect(flushFuture.get(EasyMock.anyLong(), EasyMock.anyObject(TimeUnit.class)));    if (succeed) {        sourceTask.commit();        EasyMock.expectLastCall();        futureGetExpect.andReturn(null);    } else {        futureGetExpect.andThrow(new TimeoutException());        offsetWriter.cancelFlush();        PowerMock.expectLastCall();    }}
f11505
0
assertPollMetrics
private void kafkatest_f11506_0(int minimumPollCountExpected)
{    MetricGroup sourceTaskGroup = workerTask.sourceTaskMetricsGroup().metricGroup();    MetricGroup taskGroup = workerTask.taskMetricsGroup().metricGroup();    double pollRate = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-poll-rate");    double pollTotal = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-poll-total");    if (minimumPollCountExpected > 0) {        assertEquals(RECORDS.size(), metrics.currentMetricValueAsDouble(taskGroup, "batch-size-max"), 0.000001d);        assertEquals(RECORDS.size(), metrics.currentMetricValueAsDouble(taskGroup, "batch-size-avg"), 0.000001d);        assertTrue(pollRate > 0.0d);    } else {        assertTrue(pollRate == 0.0d);    }    assertTrue(pollTotal >= minimumPollCountExpected);    double writeRate = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-write-rate");    double writeTotal = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-write-total");    if (minimumPollCountExpected > 0) {        assertTrue(writeRate > 0.0d);    } else {        assertTrue(writeRate == 0.0d);    }    assertTrue(writeTotal >= minimumPollCountExpected);    double pollBatchTimeMax = metrics.currentMetricValueAsDouble(sourceTaskGroup, "poll-batch-max-time-ms");    double pollBatchTimeAvg = metrics.currentMetricValueAsDouble(sourceTaskGroup, "poll-batch-avg-time-ms");    if (minimumPollCountExpected > 0) {        assertTrue(pollBatchTimeMax >= 0.0d);    }    assertTrue(Double.isNaN(pollBatchTimeAvg) || pollBatchTimeAvg > 0.0d);    double activeCount = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-active-count");    double activeCountMax = metrics.currentMetricValueAsDouble(sourceTaskGroup, "source-record-active-count-max");    assertEquals(0, activeCount, 0.000001d);    if (minimumPollCountExpected > 0) {        assertEquals(RECORDS.size(), activeCountMax, 0.000001d);    }}
f11506
0
updateMetricsOnListenerEventsForStartupPauseResumeAndFailure
public void kafkatest_f11515_0()
{    ConnectorTaskId taskId = new ConnectorTaskId("foo", 0);    MockConnectMetrics metrics = new MockConnectMetrics();    MockTime time = metrics.time();    ConnectException error = new ConnectException("error");    TaskMetricsGroup group = new TaskMetricsGroup(taskId, metrics, statusListener);    statusListener.onStartup(taskId);    expectLastCall();    statusListener.onPause(taskId);    expectLastCall();    statusListener.onResume(taskId);    expectLastCall();    statusListener.onPause(taskId);    expectLastCall();    statusListener.onResume(taskId);    expectLastCall();    statusListener.onFailure(taskId, error);    expectLastCall();    statusListener.onShutdown(taskId);    expectLastCall();    replay(statusListener);    time.sleep(1000L);    group.onStartup(taskId);    assertRunningMetric(group);    time.sleep(2000L);    group.onPause(taskId);    assertPausedMetric(group);    time.sleep(3000L);    group.onResume(taskId);    assertRunningMetric(group);    time.sleep(4000L);    group.onPause(taskId);    assertPausedMetric(group);    time.sleep(5000L);    group.onResume(taskId);    assertRunningMetric(group);    time.sleep(6000L);    group.onFailure(taskId, error);    assertFailedMetric(group);    time.sleep(7000L);    group.onShutdown(taskId);    assertStoppedMetric(group);    verify(statusListener);    long totalTime = 27000L;    double pauseTimeRatio = (double) (3000L + 5000L) / totalTime;    double runningTimeRatio = (double) (2000L + 4000L + 6000L) / totalTime;    assertEquals(pauseTimeRatio, metrics.currentMetricValueAsDouble(group.metricGroup(), "pause-ratio"), 0.000001d);    assertEquals(runningTimeRatio, metrics.currentMetricValueAsDouble(group.metricGroup(), "running-ratio"), 0.000001d);}
f11515
0
assertFailedMetric
protected void kafkatest_f11516_0(TaskMetricsGroup metricsGroup)
{    assertEquals(AbstractStatus.State.FAILED, metricsGroup.state());}
f11516
0
testStopInvalidConnector
public void kafkatest_f11525_0()
{    expectConverters();    expectStartStorage();    PowerMock.replayAll();    worker = new Worker(WORKER_ID, new MockTime(), plugins, config, offsetBackingStore, noneConnectorClientConfigOverridePolicy);    worker.start();    worker.stopConnector(CONNECTOR_ID);    PowerMock.verifyAll();}
f11525
0
testReconfigureConnectorTasks
public void kafkatest_f11526_0()
{    expectConverters();    expectStartStorage();    EasyMock.expect(plugins.currentThreadLoader()).andReturn(delegatingLoader).times(3);    EasyMock.expect(plugins.newConnector(WorkerTestConnector.class.getName())).andReturn(connector);    EasyMock.expect(connector.version()).andReturn("1.0");    Map<String, String> props = new HashMap<>();    props.put(SinkConnectorConfig.TOPICS_CONFIG, "foo,bar");    props.put(ConnectorConfig.TASKS_MAX_CONFIG, "1");    props.put(ConnectorConfig.NAME_CONFIG, CONNECTOR_ID);    props.put(ConnectorConfig.CONNECTOR_CLASS_CONFIG, WorkerTestConnector.class.getName());    EasyMock.expect(connector.version()).andReturn("1.0");    EasyMock.expect(plugins.compareAndSwapLoaders(connector)).andReturn(delegatingLoader).times(3);    connector.initialize(anyObject(ConnectorContext.class));    EasyMock.expectLastCall();    connector.start(props);    EasyMock.expectLastCall();    EasyMock.expect(Plugins.compareAndSwapLoaders(delegatingLoader)).andReturn(pluginLoader).times(3);    connectorStatusListener.onStartup(CONNECTOR_ID);    EasyMock.expectLastCall();    // Reconfigure    EasyMock.<Class<? extends Task>>expect(connector.taskClass()).andReturn(TestSourceTask.class);    Map<String, String> taskProps = new HashMap<>();    taskProps.put("foo", "bar");    EasyMock.expect(connector.taskConfigs(2)).andReturn(Arrays.asList(taskProps, taskProps));    // Remove    connector.stop();    EasyMock.expectLastCall();    connectorStatusListener.onShutdown(CONNECTOR_ID);    EasyMock.expectLastCall();    expectStopStorage();    PowerMock.replayAll();    worker = new Worker(WORKER_ID, new MockTime(), plugins, config, offsetBackingStore, noneConnectorClientConfigOverridePolicy);    worker.start();    assertStatistics(worker, 0, 0);    assertEquals(Collections.emptySet(), worker.connectorNames());    worker.startConnector(CONNECTOR_ID, props, ctx, connectorStatusListener, TargetState.STARTED);    assertStatistics(worker, 1, 0);    assertEquals(new HashSet<>(Arrays.asList(CONNECTOR_ID)), worker.connectorNames());    try {        worker.startConnector(CONNECTOR_ID, props, ctx, connectorStatusListener, TargetState.STARTED);        fail("Should have thrown exception when trying to add connector with same name.");    } catch (ConnectException e) {    // expected    }    Map<String, String> connProps = new HashMap<>(props);    connProps.put(ConnectorConfig.TASKS_MAX_CONFIG, "2");    ConnectorConfig connConfig = new SinkConnectorConfig(plugins, connProps);    List<Map<String, String>> taskConfigs = worker.connectorTaskConfigs(CONNECTOR_ID, connConfig);    Map<String, String> expectedTaskProps = new HashMap<>();    expectedTaskProps.put("foo", "bar");    expectedTaskProps.put(TaskConfig.TASK_CLASS_CONFIG, TestSourceTask.class.getName());    expectedTaskProps.put(SinkTask.TOPICS_CONFIG, "foo,bar");    assertEquals(2, taskConfigs.size());    assertEquals(expectedTaskProps, taskConfigs.get(0));    assertEquals(expectedTaskProps, taskConfigs.get(1));    assertStatistics(worker, 1, 0);    assertStartupStatistics(worker, 1, 0, 0, 0);    worker.stopConnector(CONNECTOR_ID);    assertStatistics(worker, 0, 0);    assertStartupStatistics(worker, 1, 0, 0, 0);    assertEquals(Collections.emptySet(), worker.connectorNames());    // Nothing should be left, so this should effectively be a nop    worker.stop();    assertStatistics(worker, 0, 0);    PowerMock.verifyAll();}
f11526
0
testConsumerConfigsWithOverrides
public void kafkatest_f11535_0()
{    Map<String, String> props = new HashMap<>(workerProps);    props.put("consumer.auto.offset.reset", "latest");    props.put("consumer.max.poll.records", "1000");    props.put("consumer.client.id", "consumer-test-id");    WorkerConfig configWithOverrides = new StandaloneConfig(props);    Map<String, String> expectedConfigs = new HashMap<>(defaultConsumerConfigs);    expectedConfigs.put("group.id", "connect-test");    expectedConfigs.put("auto.offset.reset", "latest");    expectedConfigs.put("max.poll.records", "1000");    expectedConfigs.put("client.id", "consumer-test-id");    EasyMock.expect(connectorConfig.originalsWithPrefix(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX)).andReturn(new HashMap<>());    PowerMock.replayAll();    assertEquals(expectedConfigs, Worker.consumerConfigs(new ConnectorTaskId("test", 1), configWithOverrides, connectorConfig, null, noneConnectorClientConfigOverridePolicy));}
f11535
0
testConsumerConfigsWithClientOverrides
public void kafkatest_f11536_0()
{    Map<String, String> props = new HashMap<>(workerProps);    props.put("consumer.auto.offset.reset", "latest");    props.put("consumer.max.poll.records", "5000");    WorkerConfig configWithOverrides = new StandaloneConfig(props);    Map<String, String> expectedConfigs = new HashMap<>(defaultConsumerConfigs);    expectedConfigs.put("group.id", "connect-test");    expectedConfigs.put("auto.offset.reset", "latest");    expectedConfigs.put("max.poll.records", "5000");    expectedConfigs.put("max.poll.interval.ms", "1000");    expectedConfigs.put("client.id", "connector-consumer-test-1");    Map<String, Object> connConfig = new HashMap<String, Object>();    connConfig.put("max.poll.records", "5000");    connConfig.put("max.poll.interval.ms", "1000");    EasyMock.expect(connectorConfig.originalsWithPrefix(ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX)).andReturn(connConfig);    PowerMock.replayAll();    assertEquals(expectedConfigs, Worker.consumerConfigs(new ConnectorTaskId("test", 1), configWithOverrides, connectorConfig, null, allConnectorClientConfigOverridePolicy));}
f11536
0
expectConverters
private void kafkatest_f11545_0(Boolean expectDefaultConverters)
{    expectConverters(JsonConverter.class, expectDefaultConverters);}
f11545
0
expectConverters
private void kafkatest_f11546_0(Class<? extends Converter> converterClass, Boolean expectDefaultConverters)
{    // As default converters are instantiated when a task starts, they are expected only if the `startTask` method is called    if (expectDefaultConverters) {        // Instantiate and configure default        EasyMock.expect(plugins.newConverter(config, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS)).andReturn(keyConverter);        EasyMock.expect(plugins.newConverter(config, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS)).andReturn(valueConverter);        EasyMock.expectLastCall();    }    // internal    Converter internalKeyConverter = PowerMock.createMock(converterClass);    Converter internalValueConverter = PowerMock.createMock(converterClass);    // Instantiate and configure internal    EasyMock.expect(plugins.newConverter(config, WorkerConfig.INTERNAL_KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS)).andReturn(internalKeyConverter);    EasyMock.expect(plugins.newConverter(config, WorkerConfig.INTERNAL_VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS)).andReturn(internalValueConverter);    EasyMock.expectLastCall();}
f11546
0
version
public String kafkatest_f11557_0()
{    return "1.0";}
f11557
0
poll
public List<SourceRecord> kafkatest_f11559_0() throws InterruptedException
{    return null;}
f11559
0
emptyWorkerLoad
public static WorkerLoad kafkatest_f11569_0(String worker)
{    return new WorkerLoad.Builder(worker).build();}
f11569
0
workerLoad
public WorkerLoad kafkatest_f11570_0(String worker, int connectorStart, int connectorNum, int taskStart, int taskNum)
{    return new WorkerLoad.Builder(worker).with(newConnectors(connectorStart, connectorStart + connectorNum), newTasks(taskStart, taskStart + taskNum)).build();}
f11570
0
taskConfigs
public static Map<ConnectorTaskId, Map<String, String>> kafkatest_f11579_0(int start, int connectorNum, int taskNum)
{    return IntStream.range(start, taskNum + 1).mapToObj(i -> new SimpleEntry<>(new ConnectorTaskId("connector" + i / connectorNum + 1, i), new HashMap<String, String>())).collect(Collectors.toMap(SimpleEntry::getKey, SimpleEntry::getValue));}
f11579
0
expectedLeaderUrl
public static String kafkatest_f11580_0(String givenLeader)
{    return "http://" + givenLeader + ":8083";}
f11580
0
expectSuccessfulSetCallback
private Callback<Void> kafkatest_f11589_0()
{    @SuppressWarnings("unchecked")    Callback<Void> setCallback = PowerMock.createMock(Callback.class);    setCallback.onCompletion(EasyMock.isNull(Throwable.class), EasyMock.isNull(Void.class));    PowerMock.expectLastCall();    return setCallback;}
f11589
0
expectSuccessfulGetCallback
private Callback<Map<ByteBuffer, ByteBuffer>> kafkatest_f11590_0()
{    Callback<Map<ByteBuffer, ByteBuffer>> getCallback = PowerMock.createMock(Callback.class);    getCallback.onCompletion(EasyMock.isNull(Throwable.class), EasyMock.anyObject(Map.class));    PowerMock.expectLastCall();    return getCallback;}
f11590
0
testBackgroundConnectorDeletion
public void kafkatest_f11599_0() throws Exception
{    // verify that we handle connector deletions correctly when they come up through the log    expectConfigure();    List<ConsumerRecord<String, byte[]>> existingRecords = Arrays.asList(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0)), new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(1)), new ConsumerRecord<>(TOPIC, 0, 2, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(1), CONFIGS_SERIALIZED.get(2)), new ConsumerRecord<>(TOPIC, 0, 3, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, COMMIT_TASKS_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(3)));    LinkedHashMap<byte[], Struct> deserialized = new LinkedHashMap<>();    deserialized.put(CONFIGS_SERIALIZED.get(0), CONNECTOR_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(1), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(2), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(3), TASKS_COMMIT_STRUCT_TWO_TASK_CONNECTOR);    logOffset = 5;    expectStart(existingRecords, deserialized);    LinkedHashMap<String, byte[]> serializedData = new LinkedHashMap<>();    serializedData.put(CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0));    serializedData.put(TARGET_STATE_KEYS.get(0), CONFIGS_SERIALIZED.get(1));    Map<String, Struct> deserializedData = new HashMap<>();    deserializedData.put(CONNECTOR_CONFIG_KEYS.get(0), null);    deserializedData.put(TARGET_STATE_KEYS.get(0), null);    expectRead(serializedData, deserializedData);    configUpdateListener.onConnectorConfigRemove(CONNECTOR_IDS.get(0));    EasyMock.expectLastCall();    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    configStorage.start();    // Should see a single connector with initial state paused    ClusterConfigState configState = configStorage.snapshot();    assertEquals(TargetState.STARTED, configState.targetState(CONNECTOR_IDS.get(0)));    configStorage.refresh(0, TimeUnit.SECONDS);    configStorage.stop();    PowerMock.verifyAll();}
f11599
0
testRestoreTargetStateUnexpectedDeletion
public void kafkatest_f11600_0() throws Exception
{    expectConfigure();    List<ConsumerRecord<String, byte[]>> existingRecords = Arrays.asList(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, CONNECTOR_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(0)), new ConsumerRecord<>(TOPIC, 0, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(1)), new ConsumerRecord<>(TOPIC, 0, 2, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TASK_CONFIG_KEYS.get(1), CONFIGS_SERIALIZED.get(2)), new ConsumerRecord<>(TOPIC, 0, 3, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TARGET_STATE_KEYS.get(0), CONFIGS_SERIALIZED.get(3)), new ConsumerRecord<>(TOPIC, 0, 4, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, COMMIT_TASKS_CONFIG_KEYS.get(0), CONFIGS_SERIALIZED.get(4)));    LinkedHashMap<byte[], Struct> deserialized = new LinkedHashMap<>();    deserialized.put(CONFIGS_SERIALIZED.get(0), CONNECTOR_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(1), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(2), TASK_CONFIG_STRUCTS.get(0));    deserialized.put(CONFIGS_SERIALIZED.get(3), null);    deserialized.put(CONFIGS_SERIALIZED.get(4), TASKS_COMMIT_STRUCT_TWO_TASK_CONNECTOR);    logOffset = 5;    expectStart(existingRecords, deserialized);    // Shouldn't see any callbacks since this is during startup    expectStop();    PowerMock.replayAll();    configStorage.setupAndCreateKafkaBasedLog(TOPIC, DEFAULT_DISTRIBUTED_CONFIG);    configStorage.start();    // The target state deletion should reset the state to STARTED    ClusterConfigState configState = configStorage.snapshot();    // Should always be next to be read, even if uncommitted    assertEquals(5, configState.offset());    assertEquals(Arrays.asList(CONNECTOR_IDS.get(0)), new ArrayList<>(configState.connectors()));    assertEquals(TargetState.STARTED, configState.targetState(CONNECTOR_IDS.get(0)));    configStorage.stop();    PowerMock.verifyAll();}
f11600
0
expectRead
private void kafkatest_f11609_0(LinkedHashMap<String, byte[]> serializedValues, Map<String, Struct> deserializedValues)
{    expectReadToEnd(serializedValues);    for (Map.Entry<String, Struct> deserializedValueEntry : deserializedValues.entrySet()) {        byte[] serializedValue = serializedValues.get(deserializedValueEntry.getKey());        EasyMock.expect(converter.toConnectData(EasyMock.eq(TOPIC), EasyMock.aryEq(serializedValue))).andReturn(new SchemaAndValue(null, structToMap(deserializedValueEntry.getValue())));    }}
f11609
0
expectRead
private void kafkatest_f11610_0(final String key, final byte[] serializedValue, Struct deserializedValue)
{    LinkedHashMap<String, byte[]> serializedData = new LinkedHashMap<>();    serializedData.put(key, serializedValue);    expectRead(serializedData, Collections.singletonMap(key, deserializedValue));}
f11610
0
setUp
public void kafkatest_f11619_0() throws Exception
{    store = PowerMock.createPartialMockAndInvokeDefaultConstructor(KafkaOffsetBackingStore.class, "createKafkaBasedLog");}
f11619
0
testStartStop
public void kafkatest_f11620_0() throws Exception
{    expectConfigure();    expectStart(Collections.emptyList());    expectStop();    PowerMock.replayAll();    store.configure(DEFAULT_DISTRIBUTED_CONFIG);    assertEquals(TOPIC, capturedTopic.getValue());    assertEquals("org.apache.kafka.common.serialization.ByteArraySerializer", capturedProducerProps.getValue().get(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.ByteArraySerializer", capturedProducerProps.getValue().get(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.ByteArrayDeserializer", capturedConsumerProps.getValue().get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG));    assertEquals("org.apache.kafka.common.serialization.ByteArrayDeserializer", capturedConsumerProps.getValue().get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG));    assertEquals(TOPIC, capturedNewTopic.getValue().name());    assertEquals(TOPIC_PARTITIONS, capturedNewTopic.getValue().numPartitions());    assertEquals(TOPIC_REPLICATION_FACTOR, capturedNewTopic.getValue().replicationFactor());    store.start();    store.stop();    PowerMock.verifyAll();}
f11620
0
testGetSetNull
public void kafkatest_f11629_0() throws Exception
{    expectConfigure();    expectStart(Collections.emptyList());    // Set offsets    Capture<org.apache.kafka.clients.producer.Callback> callback0 = EasyMock.newCapture();    storeLog.send(EasyMock.isNull(byte[].class), EasyMock.aryEq(TP0_VALUE.array()), EasyMock.capture(callback0));    PowerMock.expectLastCall();    Capture<org.apache.kafka.clients.producer.Callback> callback1 = EasyMock.newCapture();    storeLog.send(EasyMock.aryEq(TP1_KEY.array()), EasyMock.isNull(byte[].class), EasyMock.capture(callback1));    PowerMock.expectLastCall();    // Second get() should get the produced data and return the new values    final Capture<Callback<Void>> secondGetReadToEndCallback = EasyMock.newCapture();    storeLog.readToEnd(EasyMock.capture(secondGetReadToEndCallback));    PowerMock.expectLastCall().andAnswer(() -> {        capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, (byte[]) null, TP0_VALUE.array()));        capturedConsumedCallback.getValue().onCompletion(null, new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY.array(), (byte[]) null));        secondGetReadToEndCallback.getValue().onCompletion(null, null);        return null;    });    expectStop();    PowerMock.replayAll();    store.configure(DEFAULT_DISTRIBUTED_CONFIG);    store.start();    // Set offsets using null keys and values    Map<ByteBuffer, ByteBuffer> toSet = new HashMap<>();    toSet.put(null, TP0_VALUE);    toSet.put(TP1_KEY, null);    final AtomicBoolean invoked = new AtomicBoolean(false);    Future<Void> setFuture = store.set(toSet, new Callback<Void>() {        @Override        public void onCompletion(Throwable error, Void result) {            invoked.set(true);        }    });    assertFalse(setFuture.isDone());    // Out of order callbacks shouldn't matter, should still require all to be invoked before invoking the callback    // for the store's set callback    callback1.getValue().onCompletion(null, null);    assertFalse(invoked.get());    callback0.getValue().onCompletion(null, null);    setFuture.get(10000, TimeUnit.MILLISECONDS);    assertTrue(invoked.get());    // Getting data should read to end of our published data and return it    final AtomicBoolean secondGetInvokedAndPassed = new AtomicBoolean(false);    store.get(Arrays.asList(null, TP1_KEY), new Callback<Map<ByteBuffer, ByteBuffer>>() {        @Override        public void onCompletion(Throwable error, Map<ByteBuffer, ByteBuffer> result) {            assertEquals(TP0_VALUE, result.get(null));            assertNull(result.get(TP1_KEY));            secondGetInvokedAndPassed.set(true);        }    }).get(10000, TimeUnit.MILLISECONDS);    assertTrue(secondGetInvokedAndPassed.get());    store.stop();    PowerMock.verifyAll();}
f11629
0
onCompletion
public void kafkatest_f11630_0(Throwable error, Void result)
{    invoked.set(true);}
f11630
0
putConnectorState
public void kafkatest_f11639_0()
{    KafkaBasedLog<String, byte[]> kafkaBasedLog = mock(KafkaBasedLog.class);    Converter converter = mock(Converter.class);    KafkaStatusBackingStore store = new KafkaStatusBackingStore(new MockTime(), converter, STATUS_TOPIC, kafkaBasedLog);    byte[] value = new byte[0];    expect(converter.fromConnectData(eq(STATUS_TOPIC), anyObject(Schema.class), anyObject(Struct.class))).andStubReturn(value);    final Capture<Callback> callbackCapture = newCapture();    kafkaBasedLog.send(eq("status-connector-conn"), eq(value), capture(callbackCapture));    expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callbackCapture.getValue().onCompletion(null, null);            return null;        }    });    replayAll();    ConnectorStatus status = new ConnectorStatus(CONNECTOR, ConnectorStatus.State.RUNNING, WORKER_ID, 0);    store.put(status);    // state is not visible until read back from the log    assertEquals(null, store.get(CONNECTOR));    verifyAll();}
f11639
0
answer
public Void kafkatest_f11640_0() throws Throwable
{    callbackCapture.getValue().onCompletion(null, null);    return null;}
f11640
0
answer
public Void kafkatest_f11649_0() throws Throwable
{    callbackCapture.getValue().onCompletion(null, null);    store.read(consumerRecord(1, "status-connector-conn", value));    return null;}
f11649
0
putConnectorStateShouldOverride
public void kafkatest_f11650_0()
{    final byte[] value = new byte[0];    String otherWorkerId = "anotherhost:8083";    KafkaBasedLog<String, byte[]> kafkaBasedLog = mock(KafkaBasedLog.class);    Converter converter = mock(Converter.class);    final KafkaStatusBackingStore store = new KafkaStatusBackingStore(new MockTime(), converter, STATUS_TOPIC, kafkaBasedLog);    // the persisted came from a different host and has a newer generation    Map<String, Object> firstStatusRead = new HashMap<>();    firstStatusRead.put("worker_id", otherWorkerId);    firstStatusRead.put("state", "RUNNING");    firstStatusRead.put("generation", 1L);    Map<String, Object> secondStatusRead = new HashMap<>();    secondStatusRead.put("worker_id", WORKER_ID);    secondStatusRead.put("state", "UNASSIGNED");    secondStatusRead.put("generation", 0L);    expect(converter.toConnectData(STATUS_TOPIC, value)).andReturn(new SchemaAndValue(null, firstStatusRead)).andReturn(new SchemaAndValue(null, secondStatusRead));    expect(converter.fromConnectData(eq(STATUS_TOPIC), anyObject(Schema.class), anyObject(Struct.class))).andStubReturn(value);    final Capture<Callback> callbackCapture = newCapture();    kafkaBasedLog.send(eq("status-connector-conn"), eq(value), capture(callbackCapture));    expectLastCall().andAnswer(new IAnswer<Void>() {        @Override        public Void answer() throws Throwable {            callbackCapture.getValue().onCompletion(null, null);            store.read(consumerRecord(1, "status-connector-conn", value));            return null;        }    });    replayAll();    store.read(consumerRecord(0, "status-connector-conn", value));    ConnectorStatus status = new ConnectorStatus(CONNECTOR, ConnectorStatus.State.UNASSIGNED, WORKER_ID, 0);    store.put(status);    assertEquals(status, store.get(CONNECTOR));    verifyAll();}
f11650
0
deleteConnectorStatus
public void kafkatest_f11659_0()
{    MemoryStatusBackingStore store = new MemoryStatusBackingStore();    store.put(new ConnectorStatus("connector", ConnectorStatus.State.RUNNING, "localhost:8083", 0));    store.put(new ConnectorStatus("connector", ConnectorStatus.State.DESTROYED, "localhost:8083", 0));    assertNull(store.get("connector"));}
f11659
0
deleteTaskStatus
public void kafkatest_f11660_0()
{    MemoryStatusBackingStore store = new MemoryStatusBackingStore();    ConnectorTaskId taskId = new ConnectorTaskId("connector", 0);    store.put(new TaskStatus(taskId, ConnectorStatus.State.RUNNING, "localhost:8083", 0));    store.put(new TaskStatus(taskId, ConnectorStatus.State.DESTROYED, "localhost:8083", 0));    assertNull(store.get(taskId));}
f11660
0
testCancelBeforeAwaitFlush
public void kafkatest_f11669_0()
{    PowerMock.replayAll();    writer.offset(OFFSET_KEY, OFFSET_VALUE);    assertTrue(writer.beginFlush());    writer.cancelFlush();    PowerMock.verifyAll();}
f11669
0
testCancelAfterAwaitFlush
public void kafkatest_f11670_0() throws Exception
{    @SuppressWarnings("unchecked")    Callback<Void> callback = PowerMock.createMock(Callback.class);    CountDownLatch allowStoreCompleteCountdown = new CountDownLatch(1);    // In this test, the write should be cancelled so the callback will not be invoked and is not    // passed to the expectStore call    expectStore(OFFSET_KEY, OFFSET_KEY_SERIALIZED, OFFSET_VALUE, OFFSET_VALUE_SERIALIZED, null, false, allowStoreCompleteCountdown);    PowerMock.replayAll();    writer.offset(OFFSET_KEY, OFFSET_VALUE);    assertTrue(writer.beginFlush());    // Start the flush, then immediately cancel before allowing the mocked store request to finish    Future<Void> flushFuture = writer.doFlush(callback);    writer.cancelFlush();    allowStoreCompleteCountdown.countDown();    flushFuture.get(1000, TimeUnit.MILLISECONDS);    PowerMock.verifyAll();}
f11670
0
addWorker
public WorkerHandlef11679_1)
{    WorkerHandle worker = WorkerHandle.start(workerNamePrefix + nextWorkerId.getAndIncrement(), workerProps);    connectCluster.add(worker);        return worker;}
public WorkerHandlef11679
1
removeWorker
public void kafkatest_f11680_0()
{    WorkerHandle toRemove = null;    for (Iterator<WorkerHandle> it = connectCluster.iterator(); it.hasNext(); toRemove = it.next()) {    }    removeWorker(toRemove);}
f11680
0
connectorStatus
public ConnectorStateInfof11689_1String connectorName)
{    ObjectMapper mapper = new ObjectMapper();    try {        String url = endpointForResource(String.format("connectors/%s/status", connectorName));        return mapper.readerFor(ConnectorStateInfo.class).readValue(executeGet(url));    } catch (IOException e) {                throw new ConnectException("Could not read connector state", e);    }}
public ConnectorStateInfof11689
1
endpointForResource
public String kafkatest_f11690_0(String resource) throws IOException
{    String url = connectCluster.stream().map(WorkerHandle::url).filter(Objects::nonNull).findFirst().orElseThrow(() -> new IOException("Connect workers have not been provisioned")).toString();    return url + resource;}
f11690
0
numWorkers
public Builder kafkatest_f11699_0(int numWorkers)
{    this.numWorkers = numWorkers;    return this;}
f11699
0
numBrokers
public Builder kafkatest_f11700_0(int numBrokers)
{    this.numBrokers = numBrokers;    return this;}
f11700
0
createLogDir
private String kafkatest_f11709_0() throws IOException
{    TemporaryFolder tmpFolder = new TemporaryFolder();    tmpFolder.create();    return tmpFolder.newFolder().getAbsolutePath();}
f11709
0
bootstrapServers
public String kafkatest_f11710_0()
{    return Arrays.stream(brokers).map(this::address).collect(Collectors.joining(","));}
f11710
0
createAdminClient
public Admin kafkatest_f11719_0()
{    final Properties adminClientConfig = new Properties();    adminClientConfig.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers());    final Object listeners = brokerConfig.get(KafkaConfig$.MODULE$.ListenersProp());    if (listeners != null && listeners.toString().contains("SSL")) {        adminClientConfig.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));        adminClientConfig.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());        adminClientConfig.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SSL");    }    return Admin.create(adminClientConfig);}
f11719
0
consume
public ConsumerRecords<byte[], byte[]>f11720_1int n, long maxDuration, String... topics)
{    Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> records = new HashMap<>();    int consumedRecords = 0;    try (KafkaConsumer<byte[], byte[]> consumer = createConsumerAndSubscribeTo(Collections.emptyMap(), topics)) {        final long startMillis = System.currentTimeMillis();        long allowedDuration = maxDuration;        while (allowedDuration > 0) {                        ConsumerRecords<byte[], byte[]> rec = consumer.poll(Duration.ofMillis(allowedDuration));            if (rec.isEmpty()) {                allowedDuration = maxDuration - (System.currentTimeMillis() - startMillis);                continue;            }            for (TopicPartition partition : rec.partitions()) {                final List<ConsumerRecord<byte[], byte[]>> r = rec.records(partition);                records.computeIfAbsent(partition, t -> new ArrayList<>()).addAll(r);                consumedRecords += r.size();            }            if (consumedRecords >= n) {                return new ConsumerRecords<>(records);            }            allowedDuration = maxDuration - (System.currentTimeMillis() - startMillis);        }    }    throw new RuntimeException("Could not find enough records. found " + consumedRecords + ", expected " + n);}
public ConsumerRecords<byte[], byte[]>f11720
1
equals
public boolean kafkatest_f11729_0(Object o)
{    if (this == o) {        return true;    }    if (!(o instanceof WorkerHandle)) {        return false;    }    WorkerHandle that = (WorkerHandle) o;    return Objects.equals(workerName, that.workerName) && Objects.equals(worker, that.worker);}
f11729
0
hashCode
public int kafkatest_f11730_0()
{    return Objects.hash(workerName, worker);}
f11730
0
run
public void kafkatest_f11739_0()
{    consumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE));}
f11739
0
run
public void kafkatest_f11740_0()
{    consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY, TP1_VALUE));}
f11740
0
run
public void kafkatest_f11749_0()
{    consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP1_KEY, TP1_VALUE_NEW));}
f11749
0
testPollConsumerError
public void kafkatest_f11750_0() throws Exception
{    expectStart();    expectStop();    PowerMock.replayAll();    final CountDownLatch finishedLatch = new CountDownLatch(1);    Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(TP0, 1L);    endOffsets.put(TP1, 1L);    consumer.updateEndOffsets(endOffsets);    consumer.schedulePollTask(new Runnable() {        @Override        public void run() {            // Trigger exception            consumer.schedulePollTask(new Runnable() {                @Override                public void run() {                    consumer.setPollException(Errors.COORDINATOR_NOT_AVAILABLE.exception());                }            });            // Should keep polling until it reaches current log end offset for all partitions            consumer.scheduleNopPollTask();            consumer.scheduleNopPollTask();            consumer.schedulePollTask(new Runnable() {                @Override                public void run() {                    consumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE_NEW));                    consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 0, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, TP0_KEY, TP0_VALUE_NEW));                }            });            consumer.schedulePollTask(new Runnable() {                @Override                public void run() {                    finishedLatch.countDown();                }            });        }    });    store.start();    assertTrue(finishedLatch.await(10000, TimeUnit.MILLISECONDS));    assertEquals(CONSUMER_ASSIGNMENT, consumer.assignment());    assertEquals(1L, consumer.position(TP0));    store.stop();    assertFalse(Whitebox.<Thread>getInternalState(store, "thread").isAlive());    assertTrue(consumer.closed());    PowerMock.verifyAll();}
f11750
0
run
public void kafkatest_f11759_0()
{    finishedLatch.countDown();}
f11759
0
testProducerError
public void kafkatest_f11760_0() throws Exception
{    expectStart();    TestFuture<RecordMetadata> tp0Future = new TestFuture<>();    ProducerRecord<String, String> tp0Record = new ProducerRecord<>(TOPIC, TP0_KEY, TP0_VALUE);    Capture<org.apache.kafka.clients.producer.Callback> callback0 = EasyMock.newCapture();    EasyMock.expect(producer.send(EasyMock.eq(tp0Record), EasyMock.capture(callback0))).andReturn(tp0Future);    expectStop();    PowerMock.replayAll();    Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(TP0, 0L);    endOffsets.put(TP1, 0L);    consumer.updateEndOffsets(endOffsets);    store.start();    assertEquals(CONSUMER_ASSIGNMENT, consumer.assignment());    assertEquals(0L, consumer.position(TP0));    assertEquals(0L, consumer.position(TP1));    final AtomicReference<Throwable> setException = new AtomicReference<>();    store.send(TP0_KEY, TP0_VALUE, new org.apache.kafka.clients.producer.Callback() {        @Override        public void onCompletion(RecordMetadata metadata, Exception exception) {            // Should only be invoked once            assertNull(setException.get());            setException.set(exception);        }    });    KafkaException exc = new LeaderNotAvailableException("Error");    tp0Future.resolve(exc);    callback0.getValue().onCompletion(null, exc);    assertNotNull(setException.get());    store.stop();    assertFalse(Whitebox.<Thread>getInternalState(store, "thread").isAlive());    assertTrue(consumer.closed());    PowerMock.verifyAll();}
f11760
0
shouldNotAllowNullTaskIdForOffsetContext
public void kafkatest_f11769_0()
{    LoggingContext.forOffsets(null);}
f11769
0
shouldCreateAndCloseLoggingContextEvenWithNullContextMap
public voidf11770_1)
{    MDC.clear();    assertMdc(null, null, null);    try (LoggingContext loggingContext = LoggingContext.forConnector(CONNECTOR_NAME)) {        assertMdc(CONNECTOR_NAME, null, Scope.WORKER);            }    assertMdc(null, null, null);}
public voidf11770
1
testGracefulShutdown
public void kafkatest_f11779_0() throws InterruptedException
{    ShutdownableThread thread = new ShutdownableThread("graceful") {        @Override        public void execute() {            while (getRunning()) {                try {                    Thread.sleep(1);                } catch (InterruptedException e) {                // Ignore                }            }        }    };    thread.start();    Thread.sleep(10);    assertTrue(thread.gracefulShutdown(1000, TimeUnit.MILLISECONDS));}
f11779
0
execute
public void kafkatest_f11780_0()
{    while (getRunning()) {        try {            Thread.sleep(1);        } catch (InterruptedException e) {        // Ignore        }    }}
f11780
0
isCancelled
public boolean kafkatest_f11789_0()
{    return false;}
f11789
0
isDone
public boolean kafkatest_f11790_0()
{    return resolved;}
f11790
0
teardown
public void kafkatest_f11799_0()
{    backgroundThreadExceptionHandler.verifyNoExceptions();    ShutdownableThread.funcaughtExceptionHandler = null;}
f11799
0
returnNullWithApiVersionMismatch
public void kafkatest_f11800_0()
{    final NewTopic newTopic = TopicAdmin.defineTopic("myTopic").partitions(1).compacted().build();    Cluster cluster = createCluster(1);    try (AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(new MockTime(), cluster)) {        env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());        env.kafkaClient().prepareResponse(createTopicResponseWithUnsupportedVersion(newTopic));        TopicAdmin admin = new TopicAdmin(null, env.adminClient());        boolean created = admin.createTopic(newTopic);        assertFalse(created);    }}
f11800
0
createTopicResponseWithClusterAuthorizationException
private CreateTopicsResponse kafkatest_f11809_0(NewTopic... topics)
{    return createTopicResponse(new ApiError(Errors.CLUSTER_AUTHORIZATION_FAILED, "Not authorized to create topic(s)"), topics);}
f11809
0
createTopicResponseWithTopicAuthorizationException
private CreateTopicsResponse kafkatest_f11810_0(NewTopic... topics)
{    return createTopicResponse(new ApiError(Errors.TOPIC_AUTHORIZATION_FAILED, "Not authorized to create topic(s)"), topics);}
f11810
0
getOrBuildSchema
private Schema kafkatest_f11820_0(Schema valueSchema)
{    Schema updatedSchema = schemaUpdateCache.get(valueSchema);    if (updatedSchema != null)        return updatedSchema;    final SchemaBuilder builder;    if (wholeValueCastType != null) {        builder = SchemaUtil.copySchemaBasics(valueSchema, convertFieldType(wholeValueCastType));    } else {        builder = SchemaUtil.copySchemaBasics(valueSchema, SchemaBuilder.struct());        for (Field field : valueSchema.fields()) {            if (casts.containsKey(field.name())) {                SchemaBuilder fieldBuilder = convertFieldType(casts.get(field.name()));                if (field.schema().isOptional())                    fieldBuilder.optional();                if (field.schema().defaultValue() != null) {                    Schema fieldSchema = field.schema();                    fieldBuilder.defaultValue(castValueToType(fieldSchema, fieldSchema.defaultValue(), fieldBuilder.type()));                }                builder.field(field.name(), fieldBuilder.build());            } else {                builder.field(field.name(), field.schema());            }        }    }    if (valueSchema.isOptional())        builder.optional();    if (valueSchema.defaultValue() != null)        builder.defaultValue(castValueToType(valueSchema, valueSchema.defaultValue(), builder.type()));    updatedSchema = builder.build();    schemaUpdateCache.put(valueSchema, updatedSchema);    return updatedSchema;}
f11820
0
convertFieldType
private SchemaBuilder kafkatest_f11821_0(Schema.Type type)
{    switch(type) {        case INT8:            return SchemaBuilder.int8();        case INT16:            return SchemaBuilder.int16();        case INT32:            return SchemaBuilder.int32();        case INT64:            return SchemaBuilder.int64();        case FLOAT32:            return SchemaBuilder.float32();        case FLOAT64:            return SchemaBuilder.float64();        case BOOLEAN:            return SchemaBuilder.bool();        case STRING:            return SchemaBuilder.string();        default:            throw new DataException("Unexpected type in Cast transformation: " + type);    }}
f11821
0
castToString
private static String kafkatest_f11830_0(Object value)
{    if (value instanceof java.util.Date) {        java.util.Date dateValue = (java.util.Date) value;        return Values.dateFormatFor(dateValue).format(dateValue);    } else {        return value.toString();    }}
f11830
0
parseFieldTypes
private static Map<String, Schema.Type> kafkatest_f11831_0(List<String> mappings)
{    final Map<String, Schema.Type> m = new HashMap<>();    boolean isWholeValueCast = false;    for (String mapping : mappings) {        final String[] parts = mapping.split(":");        if (parts.length > 2) {            throw new ConfigException(ReplaceField.ConfigName.RENAME, mappings, "Invalid rename mapping: " + mapping);        }        if (parts.length == 1) {            Schema.Type targetType = Schema.Type.valueOf(parts[0].trim().toUpperCase(Locale.ROOT));            m.put(WHOLE_VALUE_CAST, validCastType(targetType, FieldType.OUTPUT));            isWholeValueCast = true;        } else {            Schema.Type type;            try {                type = Schema.Type.valueOf(parts[1].trim().toUpperCase(Locale.ROOT));            } catch (IllegalArgumentException e) {                throw new ConfigException("Invalid type found in casting spec: " + parts[1].trim(), e);            }            m.put(parts[0].trim(), validCastType(type, FieldType.OUTPUT));        }    }    if (isWholeValueCast && mappings.size() > 1) {        throw new ConfigException("Cast transformations that specify a type to cast the entire value to " + "may ony specify a single cast in their spec");    }    return m;}
f11831
0
apply
public R kafkatest_f11840_0(R record)
{    final Schema schema = operatingSchema(record);    if (schema == null) {        final Map<String, Object> value = requireMapOrNull(operatingValue(record), PURPOSE);        return newRecord(record, null, value == null ? null : value.get(fieldName));    } else {        final Struct value = requireStructOrNull(operatingValue(record), PURPOSE);        return newRecord(record, schema.field(fieldName).schema(), value == null ? null : value.get(fieldName));    }}
f11840
0
config
public ConfigDef kafkatest_f11842_0()
{    return CONFIG_DEF;}
f11842
0
config
public ConfigDef kafkatest_f11852_0()
{    return CONFIG_DEF;}
f11852
0
applySchemaless
private R kafkatest_f11853_0(R record)
{    final Map<String, Object> value = requireMap(operatingValue(record), PURPOSE);    final Map<String, Object> newValue = new LinkedHashMap<>();    applySchemaless(value, "", newValue);    return newRecord(record, null, newValue);}
f11853
0
newRecord
protected R kafkatest_f11862_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());}
f11862
0
operatingSchema
protected Schema kafkatest_f11863_0(R record)
{    return record.valueSchema();}
f11863
0
newRecord
protected R kafkatest_f11872_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());}
f11872
0
operatingSchema
protected Schema kafkatest_f11873_0(R record)
{    return record.valueSchema();}
f11873
0
close
public void kafkatest_f11882_0()
{    schemaUpdateCache = null;}
f11882
0
config
public ConfigDef kafkatest_f11883_0()
{    return CONFIG_DEF;}
f11883
0
applySchemaless
private R kafkatest_f11892_0(R record)
{    final Map<String, Object> value = requireMap(operatingValue(record), PURPOSE);    final HashMap<String, Object> updatedValue = new HashMap<>(value);    for (String field : maskedFields) {        updatedValue.put(field, masked(value.get(field)));    }    return newRecord(record, updatedValue);}
f11892
0
applyWithSchema
private R kafkatest_f11893_0(R record)
{    final Struct value = requireStruct(operatingValue(record), PURPOSE);    final Struct updatedValue = new Struct(value.schema());    for (Field field : value.schema().fields()) {        final Object origFieldValue = value.get(field);        updatedValue.put(field, maskedFields.contains(field.name()) ? masked(origFieldValue) : origFieldValue);    }    return newRecord(record, updatedValue);}
f11893
0
configure
public void kafkatest_f11903_0(Map<String, ?> props)
{    final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);    regex = Pattern.compile(config.getString(ConfigName.REGEX));    replacement = config.getString(ConfigName.REPLACEMENT);}
f11903
0
apply
public R kafkatest_f11904_0(R record)
{    final Matcher matcher = regex.matcher(record.topic());    if (matcher.matches()) {        final String topic = matcher.replaceFirst(replacement);        return record.newRecord(topic, record.kafkaPartition(), record.keySchema(), record.key(), record.valueSchema(), record.value(), record.timestamp());    }    return record;}
f11904
0
reverseRenamed
 String kafkatest_f11914_0(String fieldName)
{    final String mapping = reverseRenames.get(fieldName);    return mapping == null ? fieldName : mapping;}
f11914
0
apply
public R kafkatest_f11915_0(R record)
{    if (operatingSchema(record) == null) {        return applySchemaless(record);    } else {        return applyWithSchema(record);    }}
f11915
0
operatingSchema
protected Schema kafkatest_f11924_0(R record)
{    return record.valueSchema();}
f11924
0
operatingValue
protected Object kafkatest_f11925_0(R record)
{    return record.value();}
f11925
0
updateSchemaIn
protected static Object kafkatest_f11935_0(Object keyOrValue, Schema updatedSchema)
{    if (keyOrValue instanceof Struct) {        Struct origStruct = (Struct) keyOrValue;        Struct newStruct = new Struct(updatedSchema);        for (Field field : updatedSchema.fields()) {            // assume both schemas have exact same fields with same names and schemas ...            newStruct.put(field, origStruct.get(field));        }        return newStruct;    }    return keyOrValue;}
f11935
0
toRaw
public Date kafkatest_f11936_0(Config config, Object orig)
{    if (!(orig instanceof String))        throw new DataException("Expected string timestamp to be a String, but found " + orig.getClass());    try {        return config.format.parse((String) orig);    } catch (ParseException e) {        throw new DataException("Could not parse timestamp: value (" + orig + ") does not match pattern (" + config.format.toPattern() + ")", e);    }}
f11936
0
toRaw
public Date kafkatest_f11945_0(Config config, Object orig)
{    if (!(orig instanceof Date))        throw new DataException("Expected Time to be a java.util.Date, but found " + orig.getClass());    // Already represented as a java.util.Date and Connect Times are a subset of valid java.util.Date values    return (Date) orig;}
f11945
0
typeSchema
public Schema kafkatest_f11946_0(boolean isOptional)
{    return isOptional ? OPTIONAL_TIME_SCHEMA : Time.SCHEMA;}
f11946
0
operatingValue
protected Object kafkatest_f11956_0(R record)
{    return record.key();}
f11956
0
newRecord
protected R kafkatest_f11957_0(R record, Schema updatedSchema, Object updatedValue)
{    return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());}
f11957
0
convertTimestamp
private Object kafkatest_f11966_0(Object timestamp, String timestampFormat)
{    if (timestamp == null) {        return null;    }    if (timestampFormat == null) {        timestampFormat = inferTimestampType(timestamp);    }    TimestampTranslator sourceTranslator = TRANSLATORS.get(timestampFormat);    if (sourceTranslator == null) {        throw new ConnectException("Unsupported timestamp type: " + timestampFormat);    }    Date rawTimestamp = sourceTranslator.toRaw(config, timestamp);    TimestampTranslator targetTranslator = TRANSLATORS.get(config.type);    if (targetTranslator == null) {        throw new ConnectException("Unsupported timestamp type: " + config.type);    }    return targetTranslator.toType(config, rawTimestamp);}
f11966
0
convertTimestamp
private Object kafkatest_f11967_0(Object timestamp)
{    return convertTimestamp(timestamp, null);}
f11967
0
toString
public String kafkatest_f11976_0()
{    return "valid regex";}
f11976
0
requireSchema
public static void kafkatest_f11977_0(Schema schema, String purpose)
{    if (schema == null) {        throw new DataException("Schema required for [" + purpose + "]");    }}
f11977
0
configure
public void kafkatest_f11986_0(Map<String, ?> configs)
{    final SimpleConfig config = new SimpleConfig(CONFIG_DEF, configs);    fields = config.getList(FIELDS_CONFIG);    valueToKeySchemaCache = new SynchronizedCache<>(new LRUCache<Schema, Schema>(16));}
f11986
0
apply
public R kafkatest_f11987_0(R record)
{    if (record.valueSchema() == null) {        return applySchemaless(record);    } else {        return applyWithSchema(record);    }}
f11987
0
testUnsupportedTargetType
public void kafkatest_f11996_0()
{    xformKey.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "foo:bytes"));}
f11996
0
testConfigInvalidMap
public void kafkatest_f11997_0()
{    xformKey.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "foo:int8:extra"));}
f11997
0
castWholeRecordValueWithSchemaBooleanTrue
public void kafkatest_f12006_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "boolean"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, Schema.INT32_SCHEMA, 42));    assertEquals(Schema.Type.BOOLEAN, transformed.valueSchema().type());    assertEquals(true, transformed.value());}
f12006
0
castWholeRecordValueWithSchemaBooleanFalse
public void kafkatest_f12007_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "boolean"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, Schema.INT32_SCHEMA, 0));    assertEquals(Schema.Type.BOOLEAN, transformed.valueSchema().type());    assertEquals(false, transformed.value());}
f12007
0
castWholeRecordValueSchemalessInt64
public void kafkatest_f12016_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "int64"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, null, 42));    assertNull(transformed.valueSchema());    assertEquals((long) 42, transformed.value());}
f12016
0
castWholeRecordValueSchemalessFloat32
public void kafkatest_f12017_0()
{    xformValue.configure(Collections.singletonMap(Cast.SPEC_CONFIG, "float32"));    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, null, 42));    assertNull(transformed.valueSchema());    assertEquals(42.f, transformed.value());}
f12017
0
schemaless
public void kafkatest_f12026_0()
{    xform.configure(Collections.singletonMap("field", "magic"));    final SinkRecord record = new SinkRecord("test", 0, null, Collections.singletonMap("magic", 42), null, null, 0);    final SinkRecord transformedRecord = xform.apply(record);    assertNull(transformedRecord.keySchema());    assertEquals(42, transformedRecord.key());}
f12026
0
testNullSchemaless
public void kafkatest_f12027_0()
{    xform.configure(Collections.singletonMap("field", "magic"));    final Map<String, Object> key = null;    final SinkRecord record = new SinkRecord("test", 0, null, key, null, null, 0);    final SinkRecord transformedRecord = xform.apply(record);    assertNull(transformedRecord.keySchema());    assertNull(transformedRecord.key());}
f12027
0
testOptionalStruct
public void kafkatest_f12036_0()
{    xformValue.configure(Collections.<String, String>emptyMap());    SchemaBuilder builder = SchemaBuilder.struct().optional();    builder.field("opt_int32", Schema.OPTIONAL_INT32_SCHEMA);    Schema schema = builder.build();    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, schema, null));    assertEquals(Schema.Type.STRUCT, transformed.valueSchema().type());    assertNull(transformed.value());}
f12036
0
testOptionalNestedStruct
public void kafkatest_f12037_0()
{    xformValue.configure(Collections.<String, String>emptyMap());    SchemaBuilder builder = SchemaBuilder.struct().optional();    builder.field("opt_int32", Schema.OPTIONAL_INT32_SCHEMA);    Schema supportedTypesSchema = builder.build();    builder = SchemaBuilder.struct();    builder.field("B", supportedTypesSchema);    Schema oneLevelNestedSchema = builder.build();    Struct oneLevelNestedStruct = new Struct(oneLevelNestedSchema);    oneLevelNestedStruct.put("B", null);    SourceRecord transformed = xformValue.apply(new SourceRecord(null, null, "topic", 0, oneLevelNestedSchema, oneLevelNestedStruct));    assertEquals(Schema.Type.STRUCT, transformed.valueSchema().type());    Struct transformedStruct = (Struct) transformed.value();    assertNull(transformedStruct.get("B.opt_int32"));}
f12037
0
topLevelStructRequired
public void kafkatest_f12046_0()
{    xform.configure(Collections.singletonMap("topic.field", "topic_field"));    xform.apply(new SourceRecord(null, null, "", 0, Schema.INT32_SCHEMA, 42));}
f12046
0
copySchemaAndInsertConfiguredFields
public void kafkatest_f12047_0()
{    final Map<String, Object> props = new HashMap<>();    props.put("topic.field", "topic_field!");    props.put("partition.field", "partition_field");    props.put("timestamp.field", "timestamp_field?");    props.put("static.field", "instance_id");    props.put("static.value", "my-instance-id");    xform.configure(props);    final Schema simpleStructSchema = SchemaBuilder.struct().name("name").version(1).doc("doc").field("magic", Schema.OPTIONAL_INT64_SCHEMA).build();    final Struct simpleStruct = new Struct(simpleStructSchema).put("magic", 42L);    final SourceRecord record = new SourceRecord(null, null, "test", 0, simpleStructSchema, simpleStruct);    final SourceRecord transformedRecord = xform.apply(record);    assertEquals(simpleStructSchema.name(), transformedRecord.valueSchema().name());    assertEquals(simpleStructSchema.version(), transformedRecord.valueSchema().version());    assertEquals(simpleStructSchema.doc(), transformedRecord.valueSchema().doc());    assertEquals(Schema.OPTIONAL_INT64_SCHEMA, transformedRecord.valueSchema().field("magic").schema());    assertEquals(42L, ((Struct) transformedRecord.value()).getInt64("magic").longValue());    assertEquals(Schema.STRING_SCHEMA, transformedRecord.valueSchema().field("topic_field").schema());    assertEquals("test", ((Struct) transformedRecord.value()).getString("topic_field"));    assertEquals(Schema.OPTIONAL_INT32_SCHEMA, transformedRecord.valueSchema().field("partition_field").schema());    assertEquals(0, ((Struct) transformedRecord.value()).getInt32("partition_field").intValue());    assertEquals(Timestamp.builder().optional().build(), transformedRecord.valueSchema().field("timestamp_field").schema());    assertEquals(null, ((Struct) transformedRecord.value()).getInt64("timestamp_field"));    assertEquals(Schema.OPTIONAL_STRING_SCHEMA, transformedRecord.valueSchema().field("instance_id").schema());    assertEquals("my-instance-id", ((Struct) transformedRecord.value()).getString("instance_id"));    // Exercise caching    final SourceRecord transformedRecord2 = xform.apply(new SourceRecord(null, null, "test", 1, simpleStructSchema, new Struct(simpleStructSchema)));    assertSame(transformedRecord.valueSchema(), transformedRecord2.valueSchema());}
f12047
0
identity
public void kafkatest_f12056_0()
{    assertEquals("orig", apply("(.*)", "$1", "orig"));}
f12056
0
addPrefix
public void kafkatest_f12057_0()
{    assertEquals("prefix-orig", apply("(.*)", "prefix-$1", "orig"));}
f12057
0
schemaNameAndVersionUpdate
public void kafkatest_f12066_0()
{    final Map<String, String> props = new HashMap<>();    props.put("schema.name", "foo");    props.put("schema.version", "42");    xform.configure(props);    final SinkRecord record = new SinkRecord("", 0, null, null, SchemaBuilder.struct().build(), null, 0);    final SinkRecord updatedRecord = xform.apply(record);    assertEquals("foo", updatedRecord.valueSchema().name());    assertEquals(new Integer(42), updatedRecord.valueSchema().version());}
f12066
0
schemaNameAndVersionUpdateWithStruct
public void kafkatest_f12067_0()
{    final String fieldName1 = "f1";    final String fieldName2 = "f2";    final String fieldValue1 = "value1";    final int fieldValue2 = 1;    final Schema schema = SchemaBuilder.struct().name("my.orig.SchemaDefn").field(fieldName1, Schema.STRING_SCHEMA).field(fieldName2, Schema.INT32_SCHEMA).build();    final Struct value = new Struct(schema).put(fieldName1, fieldValue1).put(fieldName2, fieldValue2);    final Map<String, String> props = new HashMap<>();    props.put("schema.name", "foo");    props.put("schema.version", "42");    xform.configure(props);    final SinkRecord record = new SinkRecord("", 0, null, null, schema, value, 0);    final SinkRecord updatedRecord = xform.apply(record);    assertEquals("foo", updatedRecord.valueSchema().name());    assertEquals(new Integer(42), updatedRecord.valueSchema().version());    // Make sure the struct's schema and fields all point to the new schema    assertMatchingSchema((Struct) updatedRecord.value(), updatedRecord.valueSchema());}
f12067
0
testConfigInvalidFormat
public void kafkatest_f12076_0()
{    Map<String, String> config = new HashMap<>();    config.put(TimestampConverter.TARGET_TYPE_CONFIG, "string");    config.put(TimestampConverter.FORMAT_CONFIG, "bad-format");    xformValue.configure(config);}
f12076
0
testSchemalessIdentity
public void kafkatest_f12077_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformValue.apply(createRecordSchemaless(DATE_PLUS_TIME.getTime()));    assertNull(transformed.valueSchema());    assertEquals(DATE_PLUS_TIME.getTime(), transformed.value());}
f12077
0
testWithSchemaIdentity
public void kafkatest_f12086_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Timestamp.SCHEMA, DATE_PLUS_TIME.getTime()));    assertEquals(Timestamp.SCHEMA, transformed.valueSchema());    assertEquals(DATE_PLUS_TIME.getTime(), transformed.value());}
f12086
0
testWithSchemaTimestampToDate
public void kafkatest_f12087_0()
{    xformValue.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Date"));    SourceRecord transformed = xformValue.apply(createRecordWithSchema(Timestamp.SCHEMA, DATE_PLUS_TIME.getTime()));    assertEquals(Date.SCHEMA, transformed.valueSchema());    assertEquals(DATE.getTime(), transformed.value());}
f12087
0
testSchemalessNullValueConversion
private void kafkatest_f12096_0(String targetType)
{    Map<String, String> config = new HashMap<>();    config.put(TimestampConverter.TARGET_TYPE_CONFIG, targetType);    config.put(TimestampConverter.FORMAT_CONFIG, STRING_DATE_FMT);    xformValue.configure(config);    SourceRecord transformed = xformValue.apply(createRecordSchemaless(null));    assertNull(transformed.valueSchema());    assertNull(transformed.value());}
f12096
0
testSchemalessNullFieldConversion
private void kafkatest_f12097_0(String targetType)
{    Map<String, String> config = new HashMap<>();    config.put(TimestampConverter.TARGET_TYPE_CONFIG, targetType);    config.put(TimestampConverter.FORMAT_CONFIG, STRING_DATE_FMT);    config.put(TimestampConverter.FIELD_CONFIG, "ts");    xformValue.configure(config);    SourceRecord transformed = xformValue.apply(createRecordSchemaless(null));    assertNull(transformed.valueSchema());    assertNull(transformed.value());}
f12097
0
testWithSchemaNullValueToTime
public void kafkatest_f12106_0()
{    testWithSchemaNullValueConversion("Time", Schema.OPTIONAL_INT64_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullValueConversion("Time", TimestampConverter.OPTIONAL_TIME_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullValueConversion("Time", TimestampConverter.OPTIONAL_DATE_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullValueConversion("Time", Schema.OPTIONAL_STRING_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullValueConversion("Time", TimestampConverter.OPTIONAL_TIMESTAMP_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);}
f12106
0
testWithSchemaNullFieldToTime
public void kafkatest_f12107_0()
{    testWithSchemaNullFieldConversion("Time", Schema.OPTIONAL_INT64_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullFieldConversion("Time", TimestampConverter.OPTIONAL_TIME_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullFieldConversion("Time", TimestampConverter.OPTIONAL_DATE_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullFieldConversion("Time", Schema.OPTIONAL_STRING_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);    testWithSchemaNullFieldConversion("Time", TimestampConverter.OPTIONAL_TIMESTAMP_SCHEMA, TimestampConverter.OPTIONAL_TIME_SCHEMA);}
f12107
0
testKey
public void kafkatest_f12116_0()
{    xformKey.configure(Collections.singletonMap(TimestampConverter.TARGET_TYPE_CONFIG, "Timestamp"));    SourceRecord transformed = xformKey.apply(new SourceRecord(null, null, "topic", 0, null, DATE_PLUS_TIME.getTime(), null, null));    assertNull(transformed.keySchema());    assertEquals(DATE_PLUS_TIME.getTime(), transformed.key());}
f12116
0
createRecordWithSchema
private SourceRecord kafkatest_f12117_0(Schema schema, Object value)
{    return new SourceRecord(null, null, "topic", 0, schema, value);}
f12117
0
withSchema
public void kafkatest_f12126_0()
{    xform.configure(Collections.singletonMap("fields", "a,b"));    final Schema valueSchema = SchemaBuilder.struct().field("a", Schema.INT32_SCHEMA).field("b", Schema.INT32_SCHEMA).field("c", Schema.INT32_SCHEMA).build();    final Struct value = new Struct(valueSchema);    value.put("a", 1);    value.put("b", 2);    value.put("c", 3);    final SinkRecord record = new SinkRecord("", 0, null, null, valueSchema, value, 0);    final SinkRecord transformedRecord = xform.apply(record);    final Schema expectedKeySchema = SchemaBuilder.struct().field("a", Schema.INT32_SCHEMA).field("b", Schema.INT32_SCHEMA).build();    final Struct expectedKey = new Struct(expectedKeySchema).put("a", 1).put("b", 2);    assertEquals(expectedKeySchema, transformedRecord.keySchema());    assertEquals(expectedKey, transformedRecord.key());}
f12126
0
run
public int kafkatest_f12127_0(final String[] args)
{    return run(args, new Properties());}
f12127
0
getTopicPartitionOffsetFromResetPlan
private Map<TopicPartition, Long> kafkatest_f12136_0(final String resetPlanPath) throws IOException, ParseException
{    final String resetPlanCsv = Utils.readFileAsString(resetPlanPath);    return parseResetPlan(resetPlanCsv);}
f12136
0
resetByDuration
private void kafkatest_f12137_0(final Consumer<byte[], byte[]> client, final Set<TopicPartition> inputTopicPartitions, final Duration duration)
{    final Instant now = Instant.now();    final long timestamp = now.minus(duration).toEpochMilli();    final Map<TopicPartition, Long> topicPartitionsAndTimes = new HashMap<>(inputTopicPartitions.size());    for (final TopicPartition topicPartition : inputTopicPartitions) {        topicPartitionsAndTimes.put(topicPartition, timestamp);    }    final Map<TopicPartition, OffsetAndTimestamp> topicPartitionsAndOffset = client.offsetsForTimes(topicPartitionsAndTimes);    for (final TopicPartition topicPartition : inputTopicPartitions) {        client.seek(topicPartition, topicPartitionsAndOffset.get(topicPartition).offset());    }}
f12137
0
maybeDeleteInternalTopics
private void kafkatest_f12146_0(final Admin adminClient, final boolean dryRun)
{    System.out.println("Deleting all internal/auto-created topics for application " + options.valueOf(applicationIdOption));    final List<String> topicsToDelete = new ArrayList<>();    for (final String listing : allTopics) {        if (isInternalTopic(listing)) {            if (!dryRun) {                topicsToDelete.add(listing);            } else {                System.out.println("Topic: " + listing);            }        }    }    if (!dryRun) {        doDelete(topicsToDelete, adminClient);    }    System.out.println("Done.");}
f12146
0
doDelete
public void kafkatest_f12147_0(final List<String> topicsToDelete, final Admin adminClient)
{    boolean hasDeleteErrors = false;    final DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(topicsToDelete);    final Map<String, KafkaFuture<Void>> results = deleteTopicsResult.values();    for (final Map.Entry<String, KafkaFuture<Void>> entry : results.entrySet()) {        try {            entry.getValue().get(30, TimeUnit.SECONDS);        } catch (final Exception e) {            System.err.println("ERROR: deleting topic " + entry.getKey());            e.printStackTrace(System.err);            hasDeleteErrors = true;        }    }    if (hasDeleteErrors) {        throw new RuntimeException("Encountered an error deleting one or more topics");    }}
f12147
0
name
 String kafkatest_f12156_0()
{    if (requestSpec != null) {        return MessageGenerator.stripSuffix(requestSpec.name(), MessageGenerator.REQUEST_SUFFIX);    } else if (responseSpec != null) {        return MessageGenerator.stripSuffix(responseSpec.name(), MessageGenerator.RESPONSE_SUFFIX);    } else {        throw new RuntimeException("Neither requestSpec nor responseSpec is defined " + "for API key " + apiKey);    }}
f12156
0
requestSchema
 String kafkatest_f12157_0()
{    if (requestSpec == null) {        return "null";    } else {        return String.format("%sData.SCHEMAS", requestSpec.name());    }}
f12157
0
generateNewApiMessageMethod
private void kafkatest_f12166_0(String type)
{    headerGenerator.addImport(MessageGenerator.API_MESSAGE_CLASS);    buffer.printf("public ApiMessage new%s() {%n", MessageGenerator.capitalizeFirst(type));    buffer.incrementIndent();    buffer.printf("switch (apiKey) {%n");    buffer.incrementIndent();    for (Map.Entry<Short, ApiData> entry : apis.entrySet()) {        buffer.printf("case %d:%n", entry.getKey());        buffer.incrementIndent();        buffer.printf("return new %s%sData();%n", entry.getValue().name(), MessageGenerator.capitalizeFirst(type));        buffer.decrementIndent();    }    buffer.printf("default:%n");    buffer.incrementIndent();    headerGenerator.addImport(MessageGenerator.UNSUPPORTED_VERSION_EXCEPTION_CLASS);    buffer.printf("throw new UnsupportedVersionException(\"Unsupported %s API key \"" + " + apiKey);%n", type);    buffer.decrementIndent();    buffer.decrementIndent();    buffer.printf("}%n");    buffer.decrementIndent();    buffer.printf("}%n");}
f12166
0
generateAccessor
private void kafkatest_f12167_0(String name, String type)
{    buffer.printf("public %s %s() {%n", type, name);    buffer.incrementIndent();    buffer.printf("return this.%s;%n", name);    buffer.decrementIndent();    buffer.printf("}%n");}
f12167
0
equals
public boolean kafkatest_f12176_0(Object other)
{    if (!(other instanceof CodeBuffer)) {        return false;    }    CodeBuffer o = (CodeBuffer) other;    return lines.equals(o.lines);}
f12176
0
hashCode
public int kafkatest_f12177_0()
{    return lines.hashCode();}
f12177
0
typeString
public String kafkatest_f12186_0()
{    return type.toString();}
f12186
0
type
public FieldType kafkatest_f12187_0()
{    return type;}
f12187
0
isInteger
public boolean kafkatest_f12196_0()
{    return true;}
f12196
0
fixedLength
public Optional<Integer> kafkatest_f12197_0()
{    return Optional.of(1);}
f12197
0
fixedLength
public Optional<Integer> kafkatest_f12206_0()
{    return Optional.of(8);}
f12206
0
toString
public String kafkatest_f12207_0()
{    return NAME;}
f12207
0
isStruct
public boolean kafkatest_f12216_0()
{    return true;}
f12216
0
toString
public String kafkatest_f12217_0()
{    return type;}
f12217
0
isArray
 boolean kafkatest_f12226_0()
{    return false;}
f12226
0
isStructArray
 boolean kafkatest_f12227_0()
{    return false;}
f12227
0
generate
 void kafkatest_f12236_0(MessageSpec message) throws Exception
{    if (message.struct().versions().contains(Short.MAX_VALUE)) {        throw new RuntimeException("Message " + message.name() + " does " + "not specify a maximum version.");    }    structRegistry.register(message);    schemaGenerator.generateSchemas(message);    generateClass(Optional.of(message), message.name() + "Data", message.struct(), message.struct().versions());    headerGenerator.generate();}
f12236
0
write
 void kafkatest_f12237_0(Writer writer) throws Exception
{    headerGenerator.buffer().write(writer);    buffer.write(writer);}
f12237
0
generateHashSetFindAllMethod
private void kafkatest_f12246_0(String className, StructSpec struct)
{    headerGenerator.addImport(MessageGenerator.LIST_CLASS);    buffer.printf("public List<%s> findAll(%s) {%n", className, commaSeparatedHashSetFieldAndTypes(struct));    buffer.incrementIndent();    generateKeyElement(className, struct);    headerGenerator.addImport(MessageGenerator.IMPLICIT_LINKED_HASH_MULTI_COLLECTION_CLASS);    buffer.printf("return findAll(key);%n");    buffer.decrementIndent();    buffer.printf("}%n");    buffer.printf("%n");}
f12246
0
generateKeyElement
private void kafkatest_f12247_0(String className, StructSpec struct)
{    buffer.printf("%s key = new %s();%n", className, className);    for (FieldSpec field : struct.fields()) {        if (field.mapKey()) {            buffer.printf("key.set%s(%s);%n", field.capitalizedCamelCaseName(), field.camelCaseName());        }    }}
f12247
0
generateClassConstructors
private void kafkatest_f12256_0(String className, StructSpec struct)
{    headerGenerator.addImport(MessageGenerator.READABLE_CLASS);    buffer.printf("public %s(Readable readable, short version) {%n", className);    buffer.incrementIndent();    initializeArrayDefaults(struct);    buffer.printf("read(readable, version);%n");    buffer.decrementIndent();    buffer.printf("}%n");    buffer.printf("%n");    headerGenerator.addImport(MessageGenerator.STRUCT_CLASS);    buffer.printf("public %s(Struct struct, short version) {%n", className);    buffer.incrementIndent();    initializeArrayDefaults(struct);    buffer.printf("fromStruct(struct, version);%n");    buffer.decrementIndent();    buffer.printf("}%n");    buffer.printf("%n");    buffer.printf("public %s() {%n", className);    buffer.incrementIndent();    for (FieldSpec field : struct.fields()) {        buffer.printf("this.%s = %s;%n", field.camelCaseName(), fieldDefault(field));    }    buffer.decrementIndent();    buffer.printf("}%n");}
f12256
0
initializeArrayDefaults
private void kafkatest_f12257_0(StructSpec struct)
{    for (FieldSpec field : struct.fields()) {        if (field.type().isArray()) {            buffer.printf("this.%s = %s;%n", field.camelCaseName(), fieldDefault(field));        }    }}
f12257
0
generateClassWriter
private void kafkatest_f12266_0(String className, StructSpec struct, Versions parentVersions)
{    headerGenerator.addImport(MessageGenerator.WRITABLE_CLASS);    buffer.printf("@Override%n");    buffer.printf("public void write(Writable writable, short version) {%n");    buffer.incrementIndent();    if (generateInverseVersionCheck(parentVersions, struct.versions())) {        buffer.incrementIndent();        headerGenerator.addImport(MessageGenerator.UNSUPPORTED_VERSION_EXCEPTION_CLASS);        buffer.printf("throw new UnsupportedVersionException(\"Can't write " + "version \" + version + \" of %s\");%n", className);        buffer.decrementIndent();        buffer.printf("}%n");    }    Versions curVersions = parentVersions.intersect(struct.versions());    if (curVersions.empty()) {        throw new RuntimeException("Version ranges " + parentVersions + " and " + struct.versions() + " have no versions in common.");    }    for (FieldSpec field : struct.fields()) {        generateFieldWriter(field, curVersions);    }    buffer.decrementIndent();    buffer.printf("}%n");}
f12266
0
writeFieldToWritable
private String kafkatest_f12267_0(FieldType type, boolean nullable, String name)
{    if (type instanceof FieldType.BoolFieldType) {        return String.format("writable.writeByte(%s ? (byte) 1 : (byte) 0)", name);    } else if (type instanceof FieldType.Int8FieldType) {        return String.format("writable.writeByte(%s)", name);    } else if (type instanceof FieldType.Int16FieldType) {        return String.format("writable.writeShort(%s)", name);    } else if (type instanceof FieldType.Int32FieldType) {        return String.format("writable.writeInt(%s)", name);    } else if (type instanceof FieldType.Int64FieldType) {        return String.format("writable.writeLong(%s)", name);    } else if (type instanceof FieldType.UUIDFieldType) {        return String.format("writable.writeUUID(%s)", name);    } else if (type instanceof FieldType.StringFieldType) {        if (nullable) {            return String.format("writable.writeNullableString(%s)", name);        } else {            return String.format("writable.writeString(%s)", name);        }    } else if (type instanceof FieldType.BytesFieldType) {        if (nullable) {            return String.format("writable.writeNullableBytes(%s)", name);        } else {            return String.format("writable.writeBytes(%s)", name);        }    } else if (type instanceof FieldType.StructType) {        return String.format("%s.write(writable, version)", name);    } else {        throw new RuntimeException("Unsupported field type " + type);    }}
f12267
0
generateFieldEquals
private void kafkatest_f12276_0(FieldSpec field)
{    if (field.type().isString() || field.type().isArray() || field.type().isStruct()) {        buffer.printf("if (this.%s == null) {%n", field.camelCaseName());        buffer.incrementIndent();        buffer.printf("if (other.%s != null) return false;%n", field.camelCaseName());        buffer.decrementIndent();        buffer.printf("} else {%n");        buffer.incrementIndent();        buffer.printf("if (!this.%s.equals(other.%s)) return false;%n", field.camelCaseName(), field.camelCaseName());        buffer.decrementIndent();        buffer.printf("}%n");    } else if (field.type().isBytes()) {        // Arrays#equals handles nulls.        headerGenerator.addImport(MessageGenerator.ARRAYS_CLASS);        buffer.printf("if (!Arrays.equals(this.%s, other.%s)) return false;%n", field.camelCaseName(), field.camelCaseName());    } else {        buffer.printf("if (%s != other.%s) return false;%n", field.camelCaseName(), field.camelCaseName());    }}
f12276
0
generateClassHashCode
private void kafkatest_f12277_0(StructSpec struct, boolean onlyMapKeys)
{    buffer.printf("@Override%n");    buffer.printf("public int hashCode() {%n");    buffer.incrementIndent();    buffer.printf("int hashCode = 0;%n");    for (FieldSpec field : struct.fields()) {        if ((!onlyMapKeys) || field.mapKey()) {            generateFieldHashCode(field);        }    }    buffer.printf("return hashCode;%n");    buffer.decrementIndent();    buffer.printf("}%n");}
f12277
0
validateNullDefault
private void kafkatest_f12286_0(FieldSpec field)
{    if (!(field.nullableVersions().contains(field.versions()))) {        throw new RuntimeException("null cannot be the default for field " + field.name() + ", because not all versions of this field are " + "nullable.");    }}
f12286
0
generateFieldAccessor
private void kafkatest_f12287_0(FieldSpec field)
{    buffer.printf("%n");    generateAccessor(fieldAbstractJavaType(field), field.camelCaseName(), field.camelCaseName());}
f12287
0
stripSuffix
 static String kafkatest_f12296_0(String str, String suffix)
{    if (str.endsWith(suffix)) {        return str.substring(0, str.length() - suffix.length());    } else {        throw new RuntimeException("String " + str + " does not end with the " + "expected suffix " + suffix);    }}
f12296
0
main
public static void kafkatest_f12297_0(String[] args) throws Exception
{    if (args.length == 0) {        System.out.println(USAGE);        System.exit(0);    } else if (args.length != 2) {        System.out.println(USAGE);        System.exit(1);    }    processDirectories(args[0], args[1]);}
f12297
0
generateSchemas
 void kafkatest_f12306_0(MessageSpec message) throws Exception
{    // Generate schemas for inline structures    generateSchemas(message.generatedClassName(), message.struct(), message.struct().versions());    // Generate schemas for common structures    for (Iterator<StructSpec> iter = structRegistry.commonStructs(); iter.hasNext(); ) {        StructSpec struct = iter.next();        generateSchemas(struct.name(), struct, struct.versions());    }}
f12306
0
generateSchemas
 void kafkatest_f12307_0(String className, StructSpec struct, Versions parentVersions) throws Exception
{    Versions versions = parentVersions.intersect(struct.versions());    MessageInfo messageInfo = messages.get(className);    if (messageInfo != null) {        return;    }    messageInfo = new MessageInfo(versions);    messages.put(className, messageInfo);    // Process the leaf classes first.    for (FieldSpec field : struct.fields()) {        if (field.type().isStructArray()) {            FieldType.ArrayType arrayType = (FieldType.ArrayType) field.type();            generateSchemas(arrayType.elementType().toString(), structRegistry.findStruct(field), versions);        } else if (field.type().isStruct()) {            generateSchemas(field.type().toString(), structRegistry.findStruct(field), versions);        }    }    CodeBuffer prev = null;    for (short v = versions.lowest(); v <= versions.highest(); v++) {        CodeBuffer cur = new CodeBuffer();        generateSchemaForVersion(struct, v, cur);        // create a new map entry.        if (!cur.equals(prev)) {            messageInfo.schemaForVersion.put(v, cur);        }        prev = cur;    }}
f12307
0
isStructArrayWithKeys
 boolean kafkatest_f12316_0(FieldSpec field)
{    if (!field.type().isArray()) {        return false;    }    FieldType.ArrayType arrayType = (FieldType.ArrayType) field.type();    if (!arrayType.isStructArray()) {        return false;    }    StructSpec struct = structSpecs.get(arrayType.elementName());    if (struct == null) {        throw new RuntimeException("Unable to locate a specification for the structure " + arrayType.elementName());    }    return struct.hasKeys();}
f12316
0
commonStructNames
 Set<String> kafkatest_f12317_0()
{    return commonStructNames;}
f12317
0
parse
public static Versions kafkatest_f12326_0(String input, Versions defaultVersions)
{    if (input == null) {        return defaultVersions;    }    String trimmedInput = input.trim();    if (trimmedInput.length() == 0) {        return defaultVersions;    }    if (trimmedInput.equals(NONE_STRING)) {        return NONE;    }    if (trimmedInput.endsWith("+")) {        return new Versions(Short.parseShort(trimmedInput.substring(0, trimmedInput.length() - 1)), Short.MAX_VALUE);    } else {        int dashIndex = trimmedInput.indexOf("-");        if (dashIndex < 0) {            short version = Short.parseShort(trimmedInput);            return new Versions(version, version);        }        return new Versions(Short.parseShort(trimmedInput.substring(0, dashIndex)), Short.parseShort(trimmedInput.substring(dashIndex + 1)));    }}
f12326
0
lowest
public short kafkatest_f12327_0()
{    return lowest;}
f12327
0
testUnknownEntityType
public void kafkatest_f12336_0()
{    for (FieldType type : new FieldType[] { FieldType.StringFieldType.INSTANCE, FieldType.Int8FieldType.INSTANCE, FieldType.Int16FieldType.INSTANCE, FieldType.Int32FieldType.INSTANCE, FieldType.Int64FieldType.INSTANCE, new FieldType.ArrayType(FieldType.StringFieldType.INSTANCE) }) {        EntityType.UNKNOWN.verifyTypeMatches("unknown", type);    }}
f12336
0
testVerifyTypeMatches
public void kafkatest_f12337_0()
{    EntityType.TRANSACTIONAL_ID.verifyTypeMatches("transactionalIdField", FieldType.StringFieldType.INSTANCE);    EntityType.TRANSACTIONAL_ID.verifyTypeMatches("transactionalIdField", new FieldType.ArrayType(FieldType.StringFieldType.INSTANCE));    EntityType.PRODUCER_ID.verifyTypeMatches("producerIdField", FieldType.Int64FieldType.INSTANCE);    EntityType.PRODUCER_ID.verifyTypeMatches("producerIdField", new FieldType.ArrayType(FieldType.Int64FieldType.INSTANCE));    EntityType.GROUP_ID.verifyTypeMatches("groupIdField", FieldType.StringFieldType.INSTANCE);    EntityType.GROUP_ID.verifyTypeMatches("groupIdField", new FieldType.ArrayType(FieldType.StringFieldType.INSTANCE));    EntityType.TOPIC_NAME.verifyTypeMatches("topicNameField", FieldType.StringFieldType.INSTANCE);    EntityType.TOPIC_NAME.verifyTypeMatches("topicNameField", new FieldType.ArrayType(FieldType.StringFieldType.INSTANCE));    EntityType.BROKER_ID.verifyTypeMatches("brokerIdField", FieldType.Int32FieldType.INSTANCE);    EntityType.BROKER_ID.verifyTypeMatches("brokerIdField", new FieldType.ArrayType(FieldType.Int32FieldType.INSTANCE));}
f12337
0
testToSnakeCase
public void kafkatest_f12346_0() throws Exception
{    assertEquals("", MessageGenerator.toSnakeCase(""));    assertEquals("foo_bar_baz", MessageGenerator.toSnakeCase("FooBarBaz"));    assertEquals("foo_bar_baz", MessageGenerator.toSnakeCase("fooBarBaz"));    assertEquals("fortran", MessageGenerator.toSnakeCase("FORTRAN"));}
f12346
0
stripSuffixTest
public void kafkatest_f12347_0() throws Exception
{    assertEquals("FooBa", MessageGenerator.stripSuffix("FooBar", "r"));    assertEquals("", MessageGenerator.stripSuffix("FooBar", "FooBar"));    assertEquals("Foo", MessageGenerator.stripSuffix("FooBar", "Bar"));    try {        MessageGenerator.stripSuffix("FooBar", "Baz");        fail("expected exception");    } catch (RuntimeException e) {    }}
f12347
0
testContains
public void kafkatest_f12356_0()
{    assertTrue(newVersions(2, 3).contains((short) 3));    assertTrue(newVersions(2, 3).contains((short) 2));    assertFalse(newVersions(0, 1).contains((short) 2));    assertTrue(newVersions(0, Short.MAX_VALUE).contains((short) 100));    assertFalse(newVersions(2, Short.MAX_VALUE).contains((short) 0));    assertTrue(newVersions(2, 3).contains(newVersions(2, 3)));    assertTrue(newVersions(2, 3).contains(newVersions(2, 2)));    assertFalse(newVersions(2, 3).contains(newVersions(2, 4)));    assertTrue(newVersions(2, 3).contains(Versions.NONE));    assertTrue(Versions.ALL.contains(newVersions(1, 2)));}
f12356
0
setUp
public void kafkatest_f12357_0()
{    for (int i = 0; i < DISTINCT_KEYS; ++i) {        keys[i] = KEY + i;        values[i] = VALUE + i;    }    lruCache = new LRUCache<>(100);}
f12357
0
measureSkipIteratorForVariableBatchSize
public void kafkatest_f12366_0(Blackhole bh) throws IOException
{    for (int i = 0; i < batchCount; ++i) {        for (MutableRecordBatch batch : MemoryRecords.readableRecords(batchBuffers[i].duplicate()).batches()) {            try (CloseableIterator<Record> iterator = batch.skipKeyValueIterator(bufferSupplier)) {                while (iterator.hasNext()) bh.consume(iterator.next());            }        }    }}
f12366
0
getProducer
public Producer<byte[], byte[]> kafkatest_f12367_0()
{    return producer;}
f12367
0
getCompressionType
public String kafkatest_f12376_0()
{    return compressionType;}
f12376
0
setCompressionType
public void kafkatest_f12377_0(String compressionType)
{    this.compressionType = compressionType;}
f12377
0
getSecurityProtocol
public String kafkatest_f12386_0()
{    return securityProtocol;}
f12386
0
setSecurityProtocol
public void kafkatest_f12387_0(String securityProtocol)
{    this.securityProtocol = securityProtocol;}
f12387
0
getSslKeystoreLocation
public String kafkatest_f12396_0()
{    return sslKeystoreLocation;}
f12396
0
getSslKeystoreType
public String kafkatest_f12397_0()
{    return sslKeystoreType;}
f12397
0
getMaxBlockMs
public int kafkatest_f12406_0()
{    return maxBlockMs;}
f12406
0
setMaxBlockMs
public void kafkatest_f12407_0(int maxBlockMs)
{    this.maxBlockMs = maxBlockMs;}
f12407
0
testSetSaslMechanism
public void kafkatest_f12416_0()
{    Properties props = getLog4jConfig(false);    props.put("log4j.appender.KAFKA.SaslMechanism", "PLAIN");    PropertyConfigurator.configure(props);    MockKafkaLog4jAppender mockKafkaLog4jAppender = getMockKafkaLog4jAppender();    assertThat(mockKafkaLog4jAppender.getProducerProperties().getProperty(SaslConfigs.SASL_MECHANISM), equalTo("PLAIN"));}
f12416
0
testSaslMechanismNotSet
public void kafkatest_f12417_0()
{    testProducerPropertyNotSet(SaslConfigs.SASL_MECHANISM);}
f12417
0
replaceProducerWithMocked
private void kafkatest_f12426_0(MockKafkaLog4jAppender mockKafkaLog4jAppender, boolean success)
{    @SuppressWarnings("unchecked")    MockProducer<byte[], byte[]> producer = EasyMock.niceMock(MockProducer.class);    @SuppressWarnings("unchecked")    Future<RecordMetadata> futureMock = EasyMock.niceMock(Future.class);    try {        if (!success)            EasyMock.expect(futureMock.get()).andThrow(new ExecutionException("simulated timeout", new TimeoutException()));    } catch (InterruptedException | ExecutionException e) {    // just mocking    }    EasyMock.expect(producer.send(EasyMock.anyObject())).andReturn(futureMock);    EasyMock.replay(producer, futureMock);    // reconfiguring mock appender    mockKafkaLog4jAppender.setKafkaProducer(producer);    mockKafkaLog4jAppender.activateOptions();}
f12426
0
getMockKafkaLog4jAppender
private MockKafkaLog4jAppender kafkatest_f12427_0()
{    return (MockKafkaLog4jAppender) Logger.getRootLogger().getAppender("KAFKA");}
f12427
0
extract
public long kafkatest_f12436_0(final ConsumerRecord<Object, Object> record, final long partitionTime)
{    if (record.value() instanceof PageViewTypedDemo.PageView) {        return ((PageViewTypedDemo.PageView) record.value()).timestamp;    }    if (record.value() instanceof PageViewTypedDemo.UserProfile) {        return ((PageViewTypedDemo.UserProfile) record.value()).timestamp;    }    if (record.value() instanceof JsonNode) {        return ((JsonNode) record.value()).get("timestamp").longValue();    }    throw new IllegalArgumentException("JsonTimestampExtractor cannot recognize the record value " + record.value());}
f12436
0
deserialize
public T kafkatest_f12438_0(final String topic, final byte[] data)
{    if (data == null) {        return null;    }    try {        return (T) OBJECT_MAPPER.readValue(data, JSONSerdeCompatible.class);    } catch (final IOException e) {        throw new SerializationException(e);    }}
f12438
0
main
public static void kafkatest_f12448_0(final String[] args)
{    final Properties props = new Properties();    props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-temperature");    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> source = builder.stream("iot-temperature");    final KStream<Windowed<String>, String> max = source.selectKey((key, value) -> "temp").groupByKey().windowedBy(TimeWindows.of(Duration.ofSeconds(TEMPERATURE_WINDOW_SIZE))).reduce((value1, value2) -> {        if (Integer.parseInt(value1) > Integer.parseInt(value2)) {            return value1;        } else {            return value2;        }    }).toStream().filter((key, value) -> Integer.parseInt(value) > TEMPERATURE_THRESHOLD);    final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);    // need to override key serde to Windowed<String> type    max.to("iot-temperature-max", Produced.with(windowedSerde, Serdes.String()));    final KafkaStreams streams = new KafkaStreams(builder.build(), props);    final CountDownLatch latch = new CountDownLatch(1);    // attach shutdown handler to catch control-c    Runtime.getRuntime().addShutdownHook(new Thread("streams-temperature-shutdown-hook") {        @Override        public void run() {            streams.close();            latch.countDown();        }    });    try {        streams.start();        latch.await();    } catch (final Throwable e) {        System.exit(1);    }    System.exit(0);}
f12448
0
run
public void kafkatest_f12449_0()
{    streams.close();    latch.countDown();}
f12449
0
handle
public ProductionExceptionHandlerResponse kafkatest_f12459_0(final ProducerRecord<byte[], byte[]> record, final Exception exception)
{    return ProductionExceptionHandlerResponse.FAIL;}
f12459
0
configure
public void kafkatest_f12460_0(final Map<String, ?> configs)
{// ignore}
f12460
0
getAdminClient
 AdminClient kafkatest_f12469_0(final Map<String, Object> config)
{    throw new UnsupportedOperationException("Direct use of this method is deprecated. " + "Implementations of KafkaClientSupplier should implement the getAdmin() method instead. " + "The method will be removed in a future release.");}
f12469
0
getAdmin
 Admin kafkatest_f12470_0(final Map<String, Object> config)
{    return getAdminClient(config);}
f12470
0
setUncaughtExceptionHandler
public void kafkatest_f12479_0(final Thread.UncaughtExceptionHandler eh)
{    synchronized (stateLock) {        if (state == State.CREATED) {            for (final StreamThread thread : threads) {                thread.setUncaughtExceptionHandler(eh);            }            if (globalStreamThread != null) {                globalStreamThread.setUncaughtExceptionHandler(eh);            }        } else {            throw new IllegalStateException("Can only set UncaughtExceptionHandler in CREATED state. " + "Current state is: " + state);        }    }}
f12479
0
setGlobalStateRestoreListener
public void kafkatest_f12480_0(final StateRestoreListener globalStateRestoreListener)
{    synchronized (stateLock) {        if (state == State.CREATED) {            this.globalStateRestoreListener = globalStateRestoreListener;        } else {            throw new IllegalStateException("Can only set GlobalStateRestoreListener in CREATED state. " + "Current state is: " + state);        }    }}
f12480
0
parseHostInfo
private static HostInfo kafkatest_f12489_0(final String endPoint)
{    if (endPoint == null || endPoint.trim().isEmpty()) {        return StreamsMetadataState.UNKNOWN_HOST;    }    final String host = getHost(endPoint);    final Integer port = getPort(endPoint);    if (host == null || port == null) {        throw new ConfigException(String.format("Error parsing host address %s. Expected format host:port.", endPoint));    }    return new HostInfo(host, port);}
f12489
0
start
public synchronized voidf12490_1) throws IllegalStateException, StreamsException
{    if (setState(State.REBALANCING)) {                if (globalStreamThread != null) {            globalStreamThread.start();        }        for (final StreamThread thread : threads) {            thread.start();        }        final Long cleanupDelay = config.getLong(StreamsConfig.STATE_CLEANUP_DELAY_MS_CONFIG);        stateDirCleaner.scheduleAtFixedRate(() -> {            // we do not use lock here since we only read on the value and act on it            if (state == State.RUNNING) {                stateDirectory.cleanRemovedTasks(cleanupDelay);            }        }, cleanupDelay, cleanupDelay, TimeUnit.MILLISECONDS);    } else {        throw new IllegalStateException("The client is either already started or already stopped, cannot re-start");    }}
public synchronized voidf12490
1
metadataForKey
public StreamsMetadata kafkatest_f12499_0(final String storeName, final K key, final StreamPartitioner<? super K, ?> partitioner)
{    validateIsRunning();    return streamsMetadataState.getMetadataWithKey(storeName, key, partitioner);}
f12499
0
store
public T kafkatest_f12500_0(final String storeName, final QueryableStoreType<T> queryableStoreType)
{    validateIsRunning();    return queryableStoreProvider.getStore(storeName, queryableStoreType);}
f12500
0
with
public static Consumed<K, V> kafkatest_f12509_0(final Topology.AutoOffsetReset resetPolicy)
{    return new Consumed<>(null, null, null, resetPolicy, null);}
f12509
0
as
public static Consumed<K, V> kafkatest_f12510_0(final String processorName)
{    return new Consumed<>(null, null, null, null, processorName);}
f12510
0
keySerde
public static Grouped kafkatest_f12519_0(final Serde<K> keySerde)
{    return new Grouped<>(null, keySerde, null);}
f12519
0
valueSerde
public static Grouped kafkatest_f12520_0(final Serde<V> valueSerde)
{    return new Grouped<>(null, null, valueSerde);}
f12520
0
withKey
 static ValueMapperWithKey<K, V, VR> kafkatest_f12529_0(final ValueMapper<V, VR> valueMapper)
{    Objects.requireNonNull(valueMapper, "valueMapper can't be null");    return (readOnlyKey, value) -> valueMapper.apply(value);}
f12529
0
toValueTransformerWithKeySupplier
 static ValueTransformerWithKeySupplier<K, V, VR> kafkatest_f12530_0(final ValueTransformerSupplier<V, VR> valueTransformerSupplier)
{    Objects.requireNonNull(valueTransformerSupplier, "valueTransformerSupplier can't be null");    return () -> {        final ValueTransformer<V, VR> valueTransformer = valueTransformerSupplier.get();        return new ValueTransformerWithKey<K, V, VR>() {            @Override            public void init(final ProcessorContext context) {                valueTransformer.init(context);            }            @Override            public VR transform(final K readOnlyKey, final V value) {                return valueTransformer.transform(value);            }            @Override            public void close() {                valueTransformer.close();            }        };    };}
f12530
0
inner
public Deserializer<T> kafkatest_f12539_0()
{    return inner;}
f12539
0
setInner
public void kafkatest_f12540_0(final Deserializer<T> inner)
{    this.inner = inner;}
f12540
0
keySerde
public Serde<K> kafkatest_f12549_0()
{    return keySerde;}
f12549
0
keyDeserializer
public Deserializer<K> kafkatest_f12550_0()
{    return keySerde == null ? null : keySerde.deserializer();}
f12550
0
deserializeParts
public Change<T> kafkatest_f12559_0(final String topic, final Change<byte[]> serialChange)
{    if (serialChange == null) {        return null;    }    final Deserializer<T> innerDeserializer = innerSerde().deserializer();    final T oldValue = serialChange.oldValue == null ? null : innerDeserializer.deserialize(topic, serialChange.oldValue);    final T newValue = serialChange.newValue == null ? null : innerDeserializer.deserialize(topic, serialChange.newValue);    return new Change<>(newValue, oldValue);}
f12559
0
mergeChangeArraysIntoSingleLegacyFormattedArray
public static byte[] kafkatest_f12560_0(final Change<byte[]> serialChange)
{    if (serialChange == null) {        return null;    }    final int oldSize = serialChange.oldValue == null ? -1 : serialChange.oldValue.length;    final int newSize = serialChange.newValue == null ? -1 : serialChange.newValue.length;    final ByteBuffer buffer = ByteBuffer.allocate(Integer.BYTES * 2 + Math.max(0, oldSize) + Math.max(0, newSize));    buffer.putInt(oldSize);    if (serialChange.oldValue != null) {        buffer.put(serialChange.oldValue);    }    buffer.putInt(newSize);    if (serialChange.newValue != null) {        buffer.put(serialChange.newValue);    }    return buffer.array();}
f12560
0
otherJoinSideNodeName
 String kafkatest_f12569_0()
{    return otherJoinSideNodeName;}
f12569
0
toString
public String kafkatest_f12570_0()
{    return "BaseJoinProcessorNode{" + "joinThisProcessorParameters=" + joinThisProcessorParameters + ", joinOtherProcessorParameters=" + joinOtherProcessorParameters + ", joinMergeProcessorParameters=" + joinMergeProcessorParameters + ", valueJoiner=" + valueJoiner + ", thisJoinSideNodeName='" + thisJoinSideNodeName + '\'' + ", otherJoinSideNodeName='" + otherJoinSideNodeName + '\'' + "} " + super.toString();}
f12570
0
getValueDeserializer
 Deserializer<V> kafkatest_f12579_0()
{    final Deserializer<? extends V> valueDeserializer = valueSerde == null ? null : valueSerde.deserializer();    return unsafeCastChangedToValueDeserializer(valueDeserializer);}
f12579
0
unsafeCastChangedToValueDeserializer
private Deserializer<V> kafkatest_f12580_0(final Deserializer<? extends V> valueDeserializer)
{    return (Deserializer<V>) new ChangedDeserializer<>(valueDeserializer);}
f12580
0
withRepartitionTopic
public GroupedTableOperationRepartitionNodeBuilder<K, V> kafkatest_f12589_0(final String repartitionTopic)
{    this.repartitionTopic = repartitionTopic;    return this;}
f12589
0
withProcessorParameters
public GroupedTableOperationRepartitionNodeBuilder<K, V> kafkatest_f12590_0(final ProcessorParameters processorParameters)
{    this.processorParameters = processorParameters;    return this;}
f12590
0
toString
public String kafkatest_f12599_0()
{    return "KTableKTableJoinNode{" + "joinThisStoreNames=" + Arrays.toString(joinThisStoreNames()) + ", joinOtherStoreNames=" + Arrays.toString(joinOtherStoreNames()) + "} " + super.toString();}
f12599
0
kTableKTableJoinNodeBuilder
public static KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12600_0()
{    return new KTableKTableJoinNodeBuilder<>();}
f12600
0
withJoinOtherStoreNames
public KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12609_0(final String[] joinOtherStoreNames)
{    this.joinOtherStoreNames = joinOtherStoreNames;    return this;}
f12609
0
withQueryableStoreName
public KTableKTableJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12610_0(final String queryableStoreName)
{    this.queryableStoreName = queryableStoreName;    return this;}
f12610
0
writeToTopology
public void kafkatest_f12619_0(final InternalTopologyBuilder topologyBuilder)
{    final Serializer<K> keySerializer = keySerde != null ? keySerde.serializer() : null;    final Deserializer<K> keyDeserializer = keySerde != null ? keySerde.deserializer() : null;    topologyBuilder.addInternalTopic(repartitionTopic);    topologyBuilder.addProcessor(processorParameters.processorName(), processorParameters.processorSupplier(), parentNodeNames());    topologyBuilder.addSink(sinkName, repartitionTopic, keySerializer, getValueSerializer(), null, processorParameters.processorName());    topologyBuilder.addSource(null, sourceName, new FailOnInvalidTimestamp(), keyDeserializer, getValueDeserializer(), repartitionTopic);}
f12619
0
optimizableRepartitionNodeBuilder
public static OptimizableRepartitionNodeBuilder<K, V> kafkatest_f12620_0()
{    return new OptimizableRepartitionNodeBuilder<>();}
f12620
0
processorParameters
public ProcessorParameters kafkatest_f12629_0()
{    return processorParameters;}
f12629
0
toString
public String kafkatest_f12630_0()
{    return "ProcessorNode{" + "processorParameters=" + processorParameters + "} " + super.toString();}
f12630
0
parentNodes
public Collection<StreamsGraphNode> kafkatest_f12639_0()
{    return parentNodes;}
f12639
0
parentNodeNames
 String[] kafkatest_f12640_0()
{    final String[] parentNames = new String[parentNodes.size()];    int index = 0;    for (final StreamsGraphNode parentNode : parentNodes) {        parentNames[index++] = parentNode.nodeName();    }    return parentNames;}
f12640
0
isMergeNode
public boolean kafkatest_f12649_0()
{    return mergeNode;}
f12649
0
setMergeNode
public void kafkatest_f12650_0(final boolean mergeNode)
{    this.mergeNode = mergeNode;}
f12650
0
writeToTopology
public void kafkatest_f12659_0(final InternalTopologyBuilder topologyBuilder)
{    final Serializer<K> keySerializer = producedInternal.keySerde() == null ? null : producedInternal.keySerde().serializer();    final Serializer<V> valSerializer = producedInternal.valueSerde() == null ? null : producedInternal.valueSerde().serializer();    final StreamPartitioner<? super K, ? super V> partitioner = producedInternal.streamPartitioner();    final String[] parentNames = parentNodeNames();    if (partitioner == null && keySerializer instanceof WindowedSerializer) {        @SuppressWarnings("unchecked")        final StreamPartitioner<K, V> windowedPartitioner = (StreamPartitioner<K, V>) new WindowedStreamPartitioner<Object, V>((WindowedSerializer) keySerializer);        topologyBuilder.addSink(nodeName(), topicNameExtractor, keySerializer, valSerializer, windowedPartitioner, parentNames);    } else {        topologyBuilder.addSink(nodeName(), topicNameExtractor, keySerializer, valSerializer, partitioner, parentNames);    }}
f12659
0
getTopicNames
public Collection<String> kafkatest_f12660_0()
{    return new ArrayList<>(topicNames);}
f12660
0
streamStreamJoinNodeBuilder
public static StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12669_0()
{    return new StreamStreamJoinNodeBuilder<>();}
f12669
0
withValueJoiner
public StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12670_0(final ValueJoiner<? super V1, ? super V2, ? extends VR> valueJoiner)
{    this.valueJoiner = valueJoiner;    return this;}
f12670
0
withJoined
public StreamStreamJoinNodeBuilder<K, V1, V2, VR> kafkatest_f12679_0(final Joined<K, V1, V2> joined)
{    this.joined = joined;    return this;}
f12679
0
build
public StreamStreamJoinNode<K, V1, V2, VR> kafkatest_f12680_0()
{    return new StreamStreamJoinNode<>(nodeName, valueJoiner, joinThisProcessorParameters, joinOtherProcessorParameters, joinMergeProcessorParameters, thisWindowedStreamProcessorParameters, otherWindowedStreamProcessorParameters, thisWindowStoreBuilder, otherWindowStoreBuilder, joined);}
f12680
0
withSourceName
public TableSourceNodeBuilder<K, V> kafkatest_f12689_0(final String sourceName)
{    this.sourceName = sourceName;    return this;}
f12689
0
withTopic
public TableSourceNodeBuilder<K, V> kafkatest_f12690_0(final String topic)
{    this.topic = topic;    return this;}
f12690
0
name
public String kafkatest_f12699_0()
{    return name;}
f12699
0
build
 KTable<KR, VR> kafkatest_f12700_0(final String functionName, final StoreBuilder<? extends StateStore> storeBuilder, final KStreamAggProcessorSupplier<K, KR, V, VR> aggregateSupplier, final String queryableStoreName, final Serde<KR> keySerde, final Serde<VR> valSerde)
{    assert queryableStoreName == null || queryableStoreName.equals(storeBuilder.name());    final String aggFunctionName = builder.newProcessorName(functionName);    String sourceName = this.name;    StreamsGraphNode parentNode = streamsGraphNode;    if (repartitionRequired) {        final OptimizableRepartitionNodeBuilder<K, V> repartitionNodeBuilder = optimizableRepartitionNodeBuilder();        final String repartitionTopicPrefix = userProvidedRepartitionTopicName != null ? userProvidedRepartitionTopicName : storeBuilder.name();        sourceName = createRepartitionSource(repartitionTopicPrefix, repartitionNodeBuilder);        // the existing repartition node, otherwise we create a new one.        if (repartitionNode == null || userProvidedRepartitionTopicName == null) {            repartitionNode = repartitionNodeBuilder.build();        }        builder.addGraphNode(parentNode, repartitionNode);        parentNode = repartitionNode;    }    final StatefulProcessorNode<K, V> statefulProcessorNode = new StatefulProcessorNode<>(aggFunctionName, new ProcessorParameters<>(aggregateSupplier, aggFunctionName), storeBuilder);    builder.addGraphNode(parentNode, statefulProcessorNode);    return new KTableImpl<>(aggFunctionName, keySerde, valSerde, sourceName.equals(this.name) ? sourceNodes : Collections.singleton(sourceName), queryableStoreName, aggregateSupplier, statefulProcessorNode, builder);}
f12700
0
addStateStore
public synchronized void kafkatest_f12709_0(final StoreBuilder builder)
{    addGraphNode(root, new StateStoreNode(builder));}
f12709
0
addGlobalStore
public synchronized void kafkatest_f12710_0(final StoreBuilder<KeyValueStore> storeBuilder, final String sourceName, final String topic, final ConsumedInternal consumed, final String processorName, final ProcessorSupplier stateUpdateSupplier)
{    final StreamsGraphNode globalStoreNode = new GlobalStoreNode(storeBuilder, sourceName, topic, consumed, processorName, stateUpdateSupplier);    addGraphNode(root, globalStoreNode);}
f12710
0
maybeOptimizeRepartitionOperations
private voidf12719_1)
{    maybeUpdateKeyChangingRepartitionNodeMap();    final Iterator<Entry<StreamsGraphNode, LinkedHashSet<OptimizableRepartitionNode>>> entryIterator = keyChangingOperationsToOptimizableRepartitionNodes.entrySet().iterator();    while (entryIterator.hasNext()) {        final Map.Entry<StreamsGraphNode, LinkedHashSet<OptimizableRepartitionNode>> entry = entryIterator.next();        final StreamsGraphNode keyChangingNode = entry.getKey();        if (entry.getValue().isEmpty()) {            continue;        }        final GroupedInternal groupedInternal = new GroupedInternal(getRepartitionSerdes(entry.getValue()));        final String repartitionTopicName = getFirstRepartitionTopicName(entry.getValue());        // passing in the name of the first repartition topic, re-used to create the optimized repartition topic        final StreamsGraphNode optimizedSingleRepartition = createRepartitionNode(repartitionTopicName, groupedInternal.keySerde(), groupedInternal.valueSerde());        // re-use parent buildPriority to make sure the single repartition graph node is evaluated before downstream nodes        optimizedSingleRepartition.setBuildPriority(keyChangingNode.buildPriority());        for (final OptimizableRepartitionNode repartitionNodeToBeReplaced : entry.getValue()) {            final StreamsGraphNode keyChangingNodeChild = findParentNodeMatching(repartitionNodeToBeReplaced, gn -> gn.parentNodes().contains(keyChangingNode));            if (keyChangingNodeChild == null) {                throw new StreamsException(String.format("Found a null keyChangingChild node for %s", repartitionNodeToBeReplaced));            }                        // need to add children of key-changing node as children of optimized repartition            // in order to process records from re-partitioning            optimizedSingleRepartition.addChild(keyChangingNodeChild);                        // now remove children from key-changing node            keyChangingNode.removeChild(keyChangingNodeChild);            // now need to get children of repartition node so we can remove repartition node            final Collection<StreamsGraphNode> repartitionNodeToBeReplacedChildren = repartitionNodeToBeReplaced.children();            final Collection<StreamsGraphNode> parentsOfRepartitionNodeToBeReplaced = repartitionNodeToBeReplaced.parentNodes();            for (final StreamsGraphNode repartitionNodeToBeReplacedChild : repartitionNodeToBeReplacedChildren) {                for (final StreamsGraphNode parentNode : parentsOfRepartitionNodeToBeReplaced) {                    parentNode.addChild(repartitionNodeToBeReplacedChild);                }            }            for (final StreamsGraphNode parentNode : parentsOfRepartitionNodeToBeReplaced) {                parentNode.removeChild(repartitionNodeToBeReplaced);            }            repartitionNodeToBeReplaced.clearChildren();                    }        keyChangingNode.addChild(optimizedSingleRepartition);        entryIterator.remove();    }}
private voidf12719
1
maybeUpdateKeyChangingRepartitionNodeMap
private void kafkatest_f12720_0()
{    final Map<StreamsGraphNode, Set<StreamsGraphNode>> mergeNodesToKeyChangers = new HashMap<>();    for (final StreamsGraphNode mergeNode : mergeNodes) {        mergeNodesToKeyChangers.put(mergeNode, new LinkedHashSet<>());        final Collection<StreamsGraphNode> keys = keyChangingOperationsToOptimizableRepartitionNodes.keySet();        for (final StreamsGraphNode key : keys) {            final StreamsGraphNode maybeParentKey = findParentNodeMatching(mergeNode, node -> node.parentNodes().contains(key));            if (maybeParentKey != null) {                mergeNodesToKeyChangers.get(mergeNode).add(key);            }        }    }    for (final Map.Entry<StreamsGraphNode, Set<StreamsGraphNode>> entry : mergeNodesToKeyChangers.entrySet()) {        final StreamsGraphNode mergeKey = entry.getKey();        final Collection<StreamsGraphNode> keyChangingParents = entry.getValue();        final LinkedHashSet<OptimizableRepartitionNode> repartitionNodes = new LinkedHashSet<>();        for (final StreamsGraphNode keyChangingParent : keyChangingParents) {            repartitionNodes.addAll(keyChangingOperationsToOptimizableRepartitionNodes.get(keyChangingParent));            keyChangingOperationsToOptimizableRepartitionNodes.remove(keyChangingParent);        }        keyChangingOperationsToOptimizableRepartitionNodes.put(mergeKey, repartitionNodes);    }}
f12720
0
otherValueSerde
public Serde<VO> kafkatest_f12729_0()
{    return otherValueSerde;}
f12729
0
name
public String kafkatest_f12730_0()
{    return name;}
f12730
0
windowedBy
public SessionWindowedKStream<K, V> kafkatest_f12739_0(final SessionWindows windows)
{    return new SessionWindowedKStreamImpl<>(windows, builder, sourceNodes, name, keySerde, valSerde, aggregateBuilder, streamsGraphNode);}
f12739
0
doAggregate
private KTable<K, T> kafkatest_f12740_0(final KStreamAggProcessorSupplier<K, K, V, T> aggregateSupplier, final String functionName, final MaterializedInternal<K, T, KeyValueStore<Bytes, byte[]>> materializedInternal)
{    return aggregateBuilder.build(functionName, new TimestampedKeyValueStoreMaterializer<>(materializedInternal).materialize(), aggregateSupplier, materializedInternal.queryableStoreName(), materializedInternal.keySerde(), materializedInternal.valueSerde());}
f12740
0
get
public Processor<K, V> kafkatest_f12749_0()
{    return new KStreamAggregateProcessor();}
f12749
0
enableSendingOldValues
public void kafkatest_f12750_0()
{    sendOldValues = true;}
f12750
0
process
public void kafkatest_f12760_0(final K key, final V value)
{    for (int i = 0; i < predicates.length; i++) {        if (predicates[i].test(key, value)) {            // use forward with child here and then break the loop            // so that no record is going to be piped to multiple streams            context().forward(key, value, To.child(childNodes[i]));            break;        }    }}
f12760
0
get
public Processor<K, V> kafkatest_f12761_0()
{    return new KStreamFilterProcessor();}
f12761
0
close
public void kafkatest_f12770_0()
{    transformer.close();}
f12770
0
get
public Processor<KIn, VIn> kafkatest_f12771_0()
{    return new KStreamFlatTransformValuesProcessor<>(valueTransformerSupplier.get());}
f12771
0
selectKey
public KStream<KR, V> kafkatest_f12780_0(final KeyValueMapper<? super K, ? super V, ? extends KR> mapper)
{    return selectKey(mapper, NamedInternal.empty());}
f12780
0
selectKey
public KStream<KR, V> kafkatest_f12781_0(final KeyValueMapper<? super K, ? super V, ? extends KR> mapper, final Named named)
{    Objects.requireNonNull(mapper, "mapper can't be null");    Objects.requireNonNull(named, "named can't be null");    final ProcessorGraphNode<K, V> selectKeyProcessorNode = internalSelectKey(mapper, new NamedInternal(named));    selectKeyProcessorNode.keyChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, selectKeyProcessorNode);    // key serde cannot be preserved    return new KStreamImpl<>(selectKeyProcessorNode.nodeName(), null, valSerde, sourceNodes, true, selectKeyProcessorNode, builder);}
f12781
0
flatMap
public KStream<KR, VR> kafkatest_f12790_0(final KeyValueMapper<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends KR, ? extends VR>>> mapper)
{    return flatMap(mapper, NamedInternal.empty());}
f12790
0
flatMap
public KStream<KR, VR> kafkatest_f12791_0(final KeyValueMapper<? super K, ? super V, ? extends Iterable<? extends KeyValue<? extends KR, ? extends VR>>> mapper, final Named named)
{    Objects.requireNonNull(mapper, "mapper can't be null");    Objects.requireNonNull(named, "named can't be null");    final String name = new NamedInternal(named).orElseGenerateWithPrefix(builder, FLATMAP_NAME);    final ProcessorParameters<? super K, ? super V> processorParameters = new ProcessorParameters<>(new KStreamFlatMap<>(mapper), name);    final ProcessorGraphNode<? super K, ? super V> flatMapNode = new ProcessorGraphNode<>(name, processorParameters);    flatMapNode.keyChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, flatMapNode);    // key and value serde cannot be preserved    return new KStreamImpl<>(name, null, null, sourceNodes, true, flatMapNode, builder);}
f12791
0
merge
public KStream<K, V> kafkatest_f12800_0(final KStream<K, V> stream, final Named processorName)
{    Objects.requireNonNull(stream);    return merge(builder, stream, new NamedInternal(processorName));}
f12800
0
merge
private KStream<K, V> kafkatest_f12801_0(final InternalStreamsBuilder builder, final KStream<K, V> stream, final NamedInternal processorName)
{    final KStreamImpl<K, V> streamImpl = (KStreamImpl<K, V>) stream;    final String name = processorName.orElseGenerateWithPrefix(builder, MERGE_NAME);    final Set<String> allSourceNodes = new HashSet<>();    final boolean requireRepartitioning = streamImpl.repartitionRequired || repartitionRequired;    allSourceNodes.addAll(sourceNodes);    allSourceNodes.addAll(streamImpl.sourceNodes);    final ProcessorParameters<? super K, ? super V> processorParameters = new ProcessorParameters<>(new KStreamPassThrough<>(), name);    final ProcessorGraphNode<? super K, ? super V> mergeNode = new ProcessorGraphNode<>(name, processorParameters);    mergeNode.setMergeNode(true);    builder.addGraphNode(Arrays.asList(this.streamsGraphNode, streamImpl.streamsGraphNode), mergeNode);    // drop the serde as we cannot safely use either one to represent both streams    return new KStreamImpl<>(name, null, null, allSourceNodes, requireRepartitioning, mergeNode, builder);}
f12801
0
to
public void kafkatest_f12810_0(final TopicNameExtractor<K, V> topicExtractor)
{    to(topicExtractor, Produced.with(keySerde, valSerde, null));}
f12810
0
to
public void kafkatest_f12811_0(final TopicNameExtractor<K, V> topicExtractor, final Produced<K, V> produced)
{    Objects.requireNonNull(topicExtractor, "topic extractor can't be null");    Objects.requireNonNull(produced, "Produced can't be null");    final ProducedInternal<K, V> producedInternal = new ProducedInternal<>(produced);    if (producedInternal.keySerde() == null) {        producedInternal.withKeySerde(keySerde);    }    if (producedInternal.valueSerde() == null) {        producedInternal.withValueSerde(valSerde);    }    to(topicExtractor, producedInternal);}
f12811
0
transformValues
public KStream<K, VR> kafkatest_f12820_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> valueTransformerSupplier, final Named named, final String... stateStoreNames)
{    Objects.requireNonNull(valueTransformerSupplier, "valueTransformSupplier can't be null");    Objects.requireNonNull(named, "named can't be null");    return doTransformValues(valueTransformerSupplier, new NamedInternal(named), stateStoreNames);}
f12820
0
doTransformValues
private KStream<K, VR> kafkatest_f12821_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> valueTransformerWithKeySupplier, final NamedInternal named, final String... stateStoreNames)
{    final String name = named.orElseGenerateWithPrefix(builder, TRANSFORMVALUES_NAME);    final StatefulProcessorNode<? super K, ? super V> transformNode = new StatefulProcessorNode<>(name, new ProcessorParameters<>(new KStreamTransformValues<>(valueTransformerWithKeySupplier), name), stateStoreNames);    transformNode.setValueChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, transformNode);    // cannot inherit value serde    return new KStreamImpl<>(name, keySerde, null, sourceNodes, repartitionRequired, transformNode, builder);}
f12821
0
join
public KStream<K, VR> kafkatest_f12830_0(final KStream<K, VO> otherStream, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final JoinWindows windows, final Joined<K, V, VO> joined)
{    return doJoin(otherStream, joiner, windows, joined, new KStreamImplJoin(false, false));}
f12830
0
outerJoin
public KStream<K, VR> kafkatest_f12831_0(final KStream<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final JoinWindows windows)
{    return outerJoin(other, joiner, windows, Joined.with(null, null, null));}
f12831
0
leftJoin
public KStream<K, VR> kafkatest_f12840_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner)
{    return leftJoin(other, joiner, Joined.with(null, null, null));}
f12840
0
leftJoin
public KStream<K, VR> kafkatest_f12841_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Joined<K, V, VO> joined)
{    Objects.requireNonNull(other, "other can't be null");    Objects.requireNonNull(joiner, "joiner can't be null");    Objects.requireNonNull(joined, "joined can't be null");    final JoinedInternal<K, V, VO> joinedInternal = new JoinedInternal<>(joined);    final String internalName = joinedInternal.name();    if (repartitionRequired) {        final KStreamImpl<K, V> thisStreamRepartitioned = repartitionForJoin(internalName != null ? internalName : name, joined.keySerde(), joined.valueSerde());        return thisStreamRepartitioned.doStreamTableJoin(other, joiner, joined, true);    } else {        return doStreamTableJoin(other, joiner, joined, true);    }}
f12841
0
groupBy
public KGroupedStream<KR, V> kafkatest_f12850_0(final KeyValueMapper<? super K, ? super V, KR> selector, final Grouped<KR, V> grouped)
{    Objects.requireNonNull(selector, "selector can't be null");    Objects.requireNonNull(grouped, "grouped can't be null");    final GroupedInternal<KR, V> groupedInternal = new GroupedInternal<>(grouped);    final ProcessorGraphNode<K, V> selectKeyMapNode = internalSelectKey(selector, new NamedInternal(groupedInternal.name()));    selectKeyMapNode.keyChangingOperation(true);    builder.addGraphNode(this.streamsGraphNode, selectKeyMapNode);    return new KGroupedStreamImpl<>(selectKeyMapNode.nodeName(), sourceNodes, groupedInternal, true, selectKeyMapNode, builder);}
f12850
0
groupByKey
public KGroupedStream<K, V> kafkatest_f12851_0()
{    return groupByKey(Grouped.with(keySerde, valSerde));}
f12851
0
init
public void kafkatest_f12860_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    otherWindow = (WindowStore<K, V2>) context.getStateStore(otherWindowName);}
f12860
0
process
public voidf12861_1final K key, final V1 value)
{    // thus, to be consistent and to avoid ambiguous null semantics, null values are ignored    if (key == null || value == null) {                skippedRecordsSensor.record();        return;    }    boolean needOuterJoin = outer;    final long inputRecordTimestamp = context().timestamp();    final long timeFrom = Math.max(0L, inputRecordTimestamp - joinBeforeMs);    final long timeTo = Math.max(0L, inputRecordTimestamp + joinAfterMs);    try (final WindowStoreIterator<V2> iter = otherWindow.fetch(key, timeFrom, timeTo)) {        while (iter.hasNext()) {            needOuterJoin = false;            final KeyValue<Long, V2> otherRecord = iter.next();            context().forward(key, joiner.apply(value, otherRecord.value), To.all().withTimestamp(Math.max(inputRecordTimestamp, otherRecord.key)));        }        if (needOuterJoin) {            context().forward(key, joiner.apply(value, null));        }    }}
public voidf12861
1
process
public void kafkatest_f12870_0(final K readOnlyKey, final V value)
{    final V1 newValue = mapper.apply(readOnlyKey, value);    context().forward(readOnlyKey, newValue);}
f12870
0
get
public Processor<K, V> kafkatest_f12871_0()
{    return new KStreamPassThroughProcessor<>();}
f12871
0
init
public void kafkatest_f12880_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    store = (TimestampedKeyValueStore<K, V>) context.getStateStore(storeName);    tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);}
f12880
0
process
public voidf12881_1final K key, final V value)
{    // If the key or value is null we don't need to proceed    if (key == null || value == null) {                skippedRecordsSensor.record();        return;    }    final ValueAndTimestamp<V> oldAggAndTimestamp = store.get(key);    final V oldAgg = getValueOrNull(oldAggAndTimestamp);    final V newAgg;    final long newTimestamp;    if (oldAgg == null) {        newAgg = value;        newTimestamp = context().timestamp();    } else {        newAgg = reducer.apply(oldAgg, value);        newTimestamp = Math.max(context().timestamp(), oldAggAndTimestamp.timestamp());    }    store.put(key, ValueAndTimestamp.make(newAgg, newTimestamp));    tupleForwarder.maybeForward(key, newAgg, sendOldValues ? oldAgg : null, newTimestamp);}
public voidf12881
1
init
public void kafkatest_f12891_0(final ProcessorContext context)
{    super.init(context);    internalProcessorContext = (InternalProcessorContext) context;    metrics = (StreamsMetricsImpl) context.metrics();    lateRecordDropSensor = Sensors.lateRecordDropSensor(internalProcessorContext);    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    store = (SessionStore<K, Agg>) context.getStateStore(storeName);    tupleForwarder = new SessionTupleForwarder<>(store, context, new SessionCacheFlushListener<>(context), sendOldValues);}
f12891
0
process
public voidf12892_1final K key, final V value)
{    // the record with the table    if (key == null) {                skippedRecordsSensor.record();        return;    }    final long timestamp = context().timestamp();    observedStreamTime = Math.max(observedStreamTime, timestamp);    final long closeTime = observedStreamTime - windows.gracePeriodMs();    final List<KeyValue<Windowed<K>, Agg>> merged = new ArrayList<>();    final SessionWindow newSessionWindow = new SessionWindow(timestamp, timestamp);    SessionWindow mergedWindow = newSessionWindow;    Agg agg = initializer.apply();    try (final KeyValueIterator<Windowed<K>, Agg> iterator = store.findSessions(key, timestamp - windows.inactivityGap(), timestamp + windows.inactivityGap())) {        while (iterator.hasNext()) {            final KeyValue<Windowed<K>, Agg> next = iterator.next();            merged.add(next);            agg = sessionMerger.apply(key, agg, next.value);            mergedWindow = mergeSessionWindow(mergedWindow, (SessionWindow) next.key.window());        }    }    if (mergedWindow.end() < closeTime) {                lateRecordDropSensor.record();    } else {        if (!mergedWindow.equals(newSessionWindow)) {            for (final KeyValue<Windowed<K>, Agg> session : merged) {                store.remove(session.key);                tupleForwarder.maybeForward(session.key, null, sendOldValues ? session.value : null);            }        }        agg = aggregator.apply(key, value, agg);        final Windowed<K> sessionKey = new Windowed<>(key, mergedWindow);        store.put(sessionKey, agg);        tupleForwarder.maybeForward(sessionKey, agg, null);    }}
public voidf12892
1
process
public void kafkatest_f12902_0(final K key, final V value)
{    context.forward(key, valueTransformer.transform(key, value));}
f12902
0
close
public void kafkatest_f12903_0()
{    valueTransformer.close();}
f12903
0
init
public void kafkatest_f12912_0(final ProcessorContext context)
{    windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);}
f12912
0
get
public ValueAndTimestamp<Agg> kafkatest_f12913_0(final Windowed<K> windowedKey)
{    final K key = windowedKey.key();    final W window = (W) windowedKey.window();    return windowStore.fetch(key, window.start());}
f12913
0
computeValue
private ValueAndTimestamp<V> kafkatest_f12923_0(final K key, final ValueAndTimestamp<V> valueAndTimestamp)
{    ValueAndTimestamp<V> newValueAndTimestamp = null;    if (valueAndTimestamp != null) {        final V value = valueAndTimestamp.value();        if (filterNot ^ predicate.test(key, value)) {            newValueAndTimestamp = valueAndTimestamp;        }    }    return newValueAndTimestamp;}
f12923
0
init
public void kafkatest_f12924_0(final ProcessorContext context)
{    super.init(context);    if (queryableName != null) {        store = (TimestampedKeyValueStore<K, V>) context.getStateStore(queryableName);        tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);    }}
f12924
0
doFilter
private KTable<K, V> kafkatest_f12933_0(final Predicate<? super K, ? super V> predicate, final Named named, final MaterializedInternal<K, V, KeyValueStore<Bytes, byte[]>> materializedInternal, final boolean filterNot)
{    final Serde<K> keySerde;    final Serde<V> valueSerde;    final String queryableStoreName;    final StoreBuilder<TimestampedKeyValueStore<K, V>> storeBuilder;    if (materializedInternal != null) {        // materialize the store; but we still need to burn one index BEFORE generating the processor to keep compatibility.        if (materializedInternal.storeName() == null) {            builder.newStoreName(FILTER_NAME);        }        // we can inherit parent key and value serde if user do not provide specific overrides, more specifically:        // we preserve the key following the order of 1) materialized, 2) parent        keySerde = materializedInternal.keySerde() != null ? materializedInternal.keySerde() : this.keySerde;        // we preserve the value following the order of 1) materialized, 2) parent        valueSerde = materializedInternal.valueSerde() != null ? materializedInternal.valueSerde() : this.valSerde;        queryableStoreName = materializedInternal.queryableStoreName();        // only materialize if materialized is specified and it has queryable name        storeBuilder = queryableStoreName != null ? (new TimestampedKeyValueStoreMaterializer<>(materializedInternal)).materialize() : null;    } else {        keySerde = this.keySerde;        valueSerde = this.valSerde;        queryableStoreName = null;        storeBuilder = null;    }    final String name = new NamedInternal(named).orElseGenerateWithPrefix(builder, FILTER_NAME);    final KTableProcessorSupplier<K, V, V> processorSupplier = new KTableFilter<>(this, predicate, filterNot, queryableStoreName);    final ProcessorParameters<K, V> processorParameters = unsafeCastProcessorParametersToCompletelyDifferentType(new ProcessorParameters<>(processorSupplier, name));    final StreamsGraphNode tableNode = new TableProcessorNode<>(name, processorParameters, storeBuilder);    builder.addGraphNode(this.streamsGraphNode, tableNode);    return new KTableImpl<>(name, keySerde, valueSerde, sourceNodes, queryableStoreName, processorSupplier, tableNode, builder);}
f12933
0
filter
public KTable<K, V> kafkatest_f12934_0(final Predicate<? super K, ? super V> predicate)
{    Objects.requireNonNull(predicate, "predicate can't be null");    return doFilter(predicate, NamedInternal.empty(), null, false);}
f12934
0
mapValues
public KTable<K, VR> kafkatest_f12943_0(final ValueMapper<? super V, ? extends VR> mapper)
{    Objects.requireNonNull(mapper, "mapper can't be null");    return doMapValues(withKey(mapper), NamedInternal.empty(), null);}
f12943
0
mapValues
public KTable<K, VR> kafkatest_f12944_0(final ValueMapper<? super V, ? extends VR> mapper, final Named named)
{    Objects.requireNonNull(mapper, "mapper can't be null");    return doMapValues(withKey(mapper), named, null);}
f12944
0
transformValues
public KTable<K, VR> kafkatest_f12953_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> transformerSupplier, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized, final String... stateStoreNames)
{    return transformValues(transformerSupplier, materialized, NamedInternal.empty(), stateStoreNames);}
f12953
0
transformValues
public KTable<K, VR> kafkatest_f12954_0(final ValueTransformerWithKeySupplier<? super K, ? super V, ? extends VR> transformerSupplier, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized, final Named named, final String... stateStoreNames)
{    Objects.requireNonNull(materialized, "materialized can't be null");    Objects.requireNonNull(named, "named can't be null");    final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized);    return doTransformValues(transformerSupplier, materializedInternal, new NamedInternal(named), stateStoreNames);}
f12954
0
join
public KTable<K, R> kafkatest_f12963_0(final KTable<K, V1> other, final ValueJoiner<? super V, ? super V1, ? extends R> joiner, final Named named)
{    return doJoin(other, joiner, named, null, false, false);}
f12963
0
join
public KTable<K, VR> kafkatest_f12964_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized)
{    return join(other, joiner, NamedInternal.empty(), materialized);}
f12964
0
leftJoin
public KTable<K, VR> kafkatest_f12973_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Named named, final Materialized<K, VR, KeyValueStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(materialized, "materialized can't be null");    final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(materialized, builder, MERGE_NAME);    return doJoin(other, joiner, named, materializedInternal, true, false);}
f12973
0
doJoin
private KTable<K, VR> kafkatest_f12974_0(final KTable<K, VO> other, final ValueJoiner<? super V, ? super VO, ? extends VR> joiner, final Named joinName, final MaterializedInternal<K, VR, KeyValueStore<Bytes, byte[]>> materializedInternal, final boolean leftOuter, final boolean rightOuter)
{    Objects.requireNonNull(other, "other can't be null");    Objects.requireNonNull(joiner, "joiner can't be null");    Objects.requireNonNull(joinName, "joinName can't be null");    final NamedInternal renamed = new NamedInternal(joinName);    final String joinMergeName = renamed.orElseGenerateWithPrefix(builder, MERGE_NAME);    final Set<String> allSourceNodes = ensureJoinableWith((AbstractStream<K, VO>) other);    if (leftOuter) {        enableSendingOldValues();    }    if (rightOuter) {        ((KTableImpl) other).enableSendingOldValues();    }    final KTableKTableAbstractJoin<K, VR, V, VO> joinThis;    final KTableKTableAbstractJoin<K, VR, VO, V> joinOther;    if (!leftOuter) {        // inner        joinThis = new KTableKTableInnerJoin<>(this, (KTableImpl<K, ?, VO>) other, joiner);        joinOther = new KTableKTableInnerJoin<>((KTableImpl<K, ?, VO>) other, this, reverseJoiner(joiner));    } else if (!rightOuter) {        // left        joinThis = new KTableKTableLeftJoin<>(this, (KTableImpl<K, ?, VO>) other, joiner);        joinOther = new KTableKTableRightJoin<>((KTableImpl<K, ?, VO>) other, this, reverseJoiner(joiner));    } else {        // outer        joinThis = new KTableKTableOuterJoin<>(this, (KTableImpl<K, ?, VO>) other, joiner);        joinOther = new KTableKTableOuterJoin<>((KTableImpl<K, ?, VO>) other, this, reverseJoiner(joiner));    }    final String joinThisName = renamed.suffixWithOrElseGet("-join-this", builder, JOINTHIS_NAME);    final String joinOtherName = renamed.suffixWithOrElseGet("-join-other", builder, JOINOTHER_NAME);    final ProcessorParameters<K, Change<V>> joinThisProcessorParameters = new ProcessorParameters<>(joinThis, joinThisName);    final ProcessorParameters<K, Change<VO>> joinOtherProcessorParameters = new ProcessorParameters<>(joinOther, joinOtherName);    final Serde<K> keySerde;    final Serde<VR> valueSerde;    final String queryableStoreName;    final StoreBuilder<TimestampedKeyValueStore<K, VR>> storeBuilder;    if (materializedInternal != null) {        keySerde = materializedInternal.keySerde() != null ? materializedInternal.keySerde() : this.keySerde;        valueSerde = materializedInternal.valueSerde();        queryableStoreName = materializedInternal.storeName();        storeBuilder = new TimestampedKeyValueStoreMaterializer<>(materializedInternal).materialize();    } else {        keySerde = this.keySerde;        valueSerde = null;        queryableStoreName = null;        storeBuilder = null;    }    final KTableKTableJoinNode<K, V, VO, VR> kTableKTableJoinNode = KTableKTableJoinNode.<K, V, VO, VR>kTableKTableJoinNodeBuilder().withNodeName(joinMergeName).withJoinThisProcessorParameters(joinThisProcessorParameters).withJoinOtherProcessorParameters(joinOtherProcessorParameters).withThisJoinSideNodeName(name).withOtherJoinSideNodeName(((KTableImpl) other).name).withJoinThisStoreNames(valueGetterSupplier().storeNames()).withJoinOtherStoreNames(((KTableImpl) other).valueGetterSupplier().storeNames()).withKeySerde(keySerde).withValueSerde(valueSerde).withQueryableStoreName(queryableStoreName).withStoreBuilder(storeBuilder).build();    builder.addGraphNode(this.streamsGraphNode, kTableKTableJoinNode);    // we can inherit parent key serde if user do not provide specific overrides    return new KTableImpl<K, Change<VR>, VR>(kTableKTableJoinNode.nodeName(), kTableKTableJoinNode.keySerde(), kTableKTableJoinNode.valueSerde(), allSourceNodes, kTableKTableJoinNode.queryableStoreName(), kTableKTableJoinNode.joinMerger(), kTableKTableJoinNode, builder);}
f12974
0
storeNames
public String[] kafkatest_f12983_0()
{    final String[] storeNames1 = valueGetterSupplier1.storeNames();    final String[] storeNames2 = valueGetterSupplier2.storeNames();    final Set<String> stores = new HashSet<>(storeNames1.length + storeNames2.length);    Collections.addAll(stores, storeNames1);    Collections.addAll(stores, storeNames2);    return stores.toArray(new String[stores.size()]);}
f12983
0
get
public Processor<K, Change<V1>> kafkatest_f12984_0()
{    return new KTableKTableJoinProcessor(valueGetterSupplier2.get());}
f12984
0
getQueryableName
public String kafkatest_f12993_0()
{    return queryableName;}
f12993
0
get
public Processor<K, Change<V>> kafkatest_f12994_0()
{    return new KTableKTableJoinMergeProcessor();}
f12994
0
get
public Processor<K, Change<V1>> kafkatest_f13003_0()
{    return new KTableKTableLeftJoinProcessor(valueGetterSupplier2.get());}
f13003
0
view
public KTableValueGetterSupplier<K, R> kafkatest_f13004_0()
{    return new KTableKTableLeftJoinValueGetterSupplier(valueGetterSupplier1, valueGetterSupplier2);}
f13004
0
view
public KTableValueGetterSupplier<K, R> kafkatest_f13013_0()
{    return new KTableKTableOuterJoinValueGetterSupplier(valueGetterSupplier1, valueGetterSupplier2);}
f13013
0
get
public KTableValueGetter<K, R> kafkatest_f13014_0()
{    return new KTableKTableOuterJoinValueGetter(valueGetterSupplier1.get(), valueGetterSupplier2.get());}
f13014
0
get
public KTableValueGetter<K, R> kafkatest_f13023_0()
{    return new KTableKTableRightJoinValueGetter(valueGetterSupplier1.get(), valueGetterSupplier2.get());}
f13023
0
init
public void kafkatest_f13024_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    valueGetter.init(context);}
f13024
0
storeNames
public String[] kafkatest_f13033_0()
{    return parentValueGetterSupplier.storeNames();}
f13033
0
enableSendingOldValues
public void kafkatest_f13034_0()
{    parent.enableSendingOldValues();    sendOldValues = true;}
f13034
0
storeNames
public String[] kafkatest_f13043_0()
{    return new String[] { storeName };}
f13043
0
init
public void kafkatest_f13044_0(final ProcessorContext context)
{    store = (TimestampedKeyValueStore<K, V>) context.getStateStore(storeName);}
f13044
0
get
public KTableValueGetter<K, KeyValue<K1, V1>> kafkatest_f13054_0()
{    return new KTableMapValueGetter(parentValueGetterSupplier.get());}
f13054
0
storeNames
public String[] kafkatest_f13055_0()
{    throw new StreamsException("Underlying state store not accessible due to repartitioning.");}
f13055
0
materialize
public void kafkatest_f13064_0()
{    this.queryableName = storeName;}
f13064
0
init
public void kafkatest_f13065_0(final ProcessorContext context)
{    super.init(context);    metrics = (StreamsMetricsImpl) context.metrics();    skippedRecordsSensor = ThreadMetrics.skipRecordSensor(metrics);    if (queryableName != null) {        store = (TimestampedKeyValueStore<K, V>) context.getStateStore(queryableName);        tupleForwarder = new TimestampedTupleForwarder<>(store, context, new TimestampedCacheFlushListener<>(context), sendOldValues);    }}
f13065
0
storeNames
public String[] kafkatest_f13075_0()
{    return parentValueGetterSupplier.storeNames();}
f13075
0
enableSendingOldValues
public void kafkatest_f13076_0()
{    parent.enableSendingOldValues();    sendOldValues = true;}
f13076
0
storeSupplier
public StoreSupplier<S> kafkatest_f13085_0()
{    return storeSupplier;}
f13085
0
keySerde
public Serde<K> kafkatest_f13086_0()
{    return keySerde;}
f13086
0
empty
public static NamedInternal kafkatest_f13095_0()
{    return new NamedInternal((String) null);}
f13095
0
with
public static NamedInternal kafkatest_f13096_0(final String name)
{    return new NamedInternal(name);}
f13096
0
close
public void kafkatest_f13105_0()
{    if (closable) {        printWriter.close();    } else {        printWriter.flush();    }}
f13105
0
keySerde
public Serde<K> kafkatest_f13106_0()
{    return keySerde;}
f13106
0
count
public KTable<Windowed<K>, Long> kafkatest_f13115_0()
{    return doCount(Materialized.with(keySerde, Serdes.Long()));}
f13115
0
count
public KTable<Windowed<K>, Long> kafkatest_f13116_0(final Materialized<K, Long, SessionStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(materialized, "materialized can't be null");    // we used to burn a topology name here, so we have to keep doing it for compatibility    if (new MaterializedInternal<>(materialized).storeName() == null) {        builder.newStoreName(AGGREGATE_NAME);    }    return doCount(materialized);}
f13116
0
withNoBound
public Suppressed.StrictBufferConfig kafkatest_f13125_0()
{    return new StrictBufferConfigImpl(Long.MAX_VALUE, Long.MAX_VALUE, // doesn't matter, given the bounds    SHUT_DOWN);}
f13125
0
shutDownWhenFull
public Suppressed.StrictBufferConfig kafkatest_f13126_0()
{    return new StrictBufferConfigImpl(maxRecords(), maxBytes(), SHUT_DOWN);}
f13126
0
toString
public String kafkatest_f13135_0()
{    return "EagerBufferConfigImpl{maxRecords=" + maxRecords + ", maxBytes=" + maxBytes + '}';}
f13135
0
buildFinalResultsSuppression
public SuppressedInternal<K> kafkatest_f13136_0(final Duration gracePeriod)
{    return new SuppressedInternal<>(name, gracePeriod, bufferConfig, TimeDefinitions.WindowEndTimeDefinition.instance(), true);}
f13136
0
init
public void kafkatest_f13145_0(final ProcessorContext context)
{    parentGetter.init(context);    // the main processor is responsible for the buffer's lifecycle    buffer = requireNonNull((TimeOrderedKeyValueBuffer<K, V>) context.getStateStore(storeName));}
f13145
0
get
public ValueAndTimestamp<V> kafkatest_f13146_0(final K key)
{    final Maybe<ValueAndTimestamp<V>> maybeValue = buffer.priorValueForBuffered(key);    if (maybeValue.isDefined()) {        return maybeValue.getNullableValue();    } else {        // not buffered, so the suppressed view is equal to the parent view        return parentGetter.get(key);    }}
f13146
0
emit
private void kafkatest_f13155_0(final TimeOrderedKeyValueBuffer.Eviction<K, V> toEmit)
{    if (shouldForward(toEmit.value())) {        final ProcessorRecordContext prevRecordContext = internalProcessorContext.recordContext();        internalProcessorContext.setRecordContext(toEmit.recordContext());        try {            internalProcessorContext.forward(toEmit.key(), toEmit.value());            suppressionEmitSensor.record();        } finally {            internalProcessorContext.setRecordContext(prevRecordContext);        }    }}
f13155
0
shouldForward
private boolean kafkatest_f13156_0(final Change<V> value)
{    return value.newValue != null || !safeToDropTombstones;}
f13156
0
withName
public Suppressed<K> kafkatest_f13166_0(final String name)
{    return new SuppressedInternal<>(name, timeToWaitForMoreEvents, bufferConfig, timeDefinition, safeToDropTombstones);}
f13166
0
name
public String kafkatest_f13167_0()
{    return name;}
f13167
0
time
public long kafkatest_f13176_0(final ProcessorContext context, final K key)
{    return context.timestamp();}
f13176
0
type
public TimeDefinitionType kafkatest_f13177_0()
{    return TimeDefinitionType.RECORD_TIME;}
f13177
0
count
public KTable<Windowed<K>, Long> kafkatest_f13186_0()
{    return doCount(Materialized.with(keySerde, Serdes.Long()));}
f13186
0
count
public KTable<Windowed<K>, Long> kafkatest_f13187_0(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized)
{    Objects.requireNonNull(materialized, "materialized can't be null");    // we used to burn a topology name here, so we have to keep doing it for compatibility    if (new MaterializedInternal<>(materialized).storeName() == null) {        builder.newStoreName(AGGREGATE_NAME);    }    return doCount(materialized);}
f13187
0
init
public void kafkatest_f13196_0(final ProcessorContext context)
{    transformer.init(context);}
f13196
0
transform
public Iterable<KeyValue<KOut, VOut>> kafkatest_f13197_0(final KIn key, final VIn value)
{    final KeyValue<KOut, VOut> pair = transformer.transform(key, value);    if (pair != null) {        return Collections.singletonList(pair);    }    return Collections.emptyList();}
f13197
0
named
public static Joined<K, V, VO> kafkatest_f13206_0(final String name)
{    return new Joined<>(null, null, null, name);}
f13206
0
as
public static Joined<K, V, VO> kafkatest_f13207_0(final String name)
{    return new Joined<>(null, null, null, name);}
f13207
0
of
public static JoinWindows kafkatest_f13216_0(final long timeDifferenceMs) throws IllegalArgumentException
{    // This is a static factory method, so we initialize grace and retention to the defaults.    return new JoinWindows(timeDifferenceMs, timeDifferenceMs, -1L, DEFAULT_RETENTION_MS);}
f13216
0
of
public static JoinWindows kafkatest_f13217_0(final Duration timeDifference) throws IllegalArgumentException
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeDifference, "timeDifference");    return of(ApiUtils.validateMillisecondDuration(timeDifference, msgPrefix));}
f13217
0
until
public JoinWindows kafkatest_f13226_0(final long durationMs) throws IllegalArgumentException
{    if (durationMs < size()) {        throw new IllegalArgumentException("Window retention time (durationMs) cannot be smaller than the window size.");    }    return new JoinWindows(beforeMs, afterMs, graceMs, durationMs, segments);}
f13226
0
maintainMs
public long kafkatest_f13227_0()
{    return Math.max(maintainDurationMs, size());}
f13227
0
withValueSerde
public Materialized<K, V, S> kafkatest_f13236_0(final Serde<V> valueSerde)
{    this.valueSerde = valueSerde;    return this;}
f13236
0
withKeySerde
public Materialized<K, V, S> kafkatest_f13237_0(final Serde<K> keySerde)
{    this.keySerde = keySerde;    return this;}
f13237
0
containsValidPattern
private static boolean kafkatest_f13246_0(final String topic)
{    for (int i = 0; i < topic.length(); ++i) {        final char c = topic.charAt(i);        // We don't use Character.isLetterOrDigit(c) because it's slower        final boolean validLetterOrDigit = (c >= 'a' && c <= 'z') || (c >= '0' && c <= '9') || (c >= 'A' && c <= 'Z');        final boolean validChar = validLetterOrDigit || c == '.' || c == '_' || c == '-';        if (!validChar) {            return false;        }    }    return true;}
f13246
0
apply
public String kafkatest_f13247_0(final K key, final V value)
{    return String.format("%s, %s", key, value);}
f13247
0
keySerde
public static Produced<K, V> kafkatest_f13256_0(final Serde<K> keySerde)
{    return new Produced<>(keySerde, null, null, null);}
f13256
0
valueSerde
public static Produced<K, V> kafkatest_f13257_0(final Serde<V> valueSerde)
{    return new Produced<>(null, valueSerde, null, null);}
f13257
0
withKeySerde
public Serialized<K, V> kafkatest_f13266_0(final Serde<K> keySerde)
{    return new Serialized<>(keySerde, null);}
f13266
0
withValueSerde
public Serialized<K, V> kafkatest_f13267_0(final Serde<V> valueSerde)
{    return new Serialized<>(null, valueSerde);}
f13267
0
innerSerializer
 Serializer<T> kafkatest_f13276_0()
{    return inner;}
f13276
0
with
public static SessionWindows kafkatest_f13277_0(final long inactivityGapMs)
{    if (inactivityGapMs <= 0) {        throw new IllegalArgumentException("Gap time (inactivityGapMs) cannot be zero or negative.");    }    return new SessionWindows(inactivityGapMs, DEFAULT_RETENTION_MS, -1);}
f13277
0
toString
public String kafkatest_f13286_0()
{    return "SessionWindows{" + "gapMs=" + gapMs + ", maintainDurationMs=" + maintainDurationMs + ", graceMs=" + graceMs + '}';}
f13286
0
maxRecords
 static EagerBufferConfig kafkatest_f13287_0(final long recordLimit)
{    return new EagerBufferConfigImpl(recordLimit, Long.MAX_VALUE);}
f13287
0
setIsChangelogTopic
public void kafkatest_f13296_0(final boolean isChangelogTopic)
{    this.isChangelogTopic = isChangelogTopic;}
f13296
0
innerDeserializer
 Deserializer<T> kafkatest_f13297_0()
{    return inner;}
f13297
0
advanceBy
public TimeWindows kafkatest_f13306_0(final Duration advance)
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(advance, "advance");    return advanceBy(ApiUtils.validateMillisecondDuration(advance, msgPrefix));}
f13306
0
windowsFor
public Map<Long, TimeWindow> kafkatest_f13307_0(final long timestamp)
{    long windowStart = (Math.max(0, timestamp - sizeMs + advanceMs) / advanceMs) * advanceMs;    final Map<Long, TimeWindow> windows = new LinkedHashMap<>();    while (windowStart <= timestamp) {        final TimeWindow window = new TimeWindow(windowStart, windowStart + sizeMs);        windows.put(windowStart, window);        windowStart += advanceMs;    }    return windows;}
f13307
0
of
public static UnlimitedWindows kafkatest_f13316_0()
{    return new UnlimitedWindows(DEFAULT_START_TIMESTAMP_MS);}
f13316
0
startOn
public UnlimitedWindows kafkatest_f13317_0(final long startMs) throws IllegalArgumentException
{    if (startMs < 0) {        throw new IllegalArgumentException("Window start time (startMs) cannot be negative.");    }    return new UnlimitedWindows(startMs);}
f13317
0
toString
public String kafkatest_f13326_0()
{    return "UnlimitedWindows{" + "startMs=" + startMs + ", segments=" + segments + '}';}
f13326
0
start
public long kafkatest_f13327_0()
{    return startMs;}
f13327
0
toString
public String kafkatest_f13336_0()
{    return "[" + key + "@" + window.start() + "/" + window.end() + "]";}
f13336
0
equals
public boolean kafkatest_f13337_0(final Object obj)
{    if (obj == this) {        return true;    }    if (!(obj instanceof Windowed)) {        return false;    }    final Windowed<?> that = (Windowed) obj;    return window.equals(that.window) && key.equals(that.key);}
f13337
0
maintainMs
public long kafkatest_f13346_0()
{    return maintainDurationMs;}
f13346
0
segments
protected Windows<W> kafkatest_f13347_0(final int segments) throws IllegalArgumentException
{    if (segments < 2) {        throw new IllegalArgumentException("Number of segments must be at least 2.");    }    this.segments = segments;    return this;}
f13347
0
applicationId
public String kafkatest_f13362_0()
{    return applicationId;}
f13362
0
taskId
public TaskId kafkatest_f13363_0()
{    return taskId;}
f13363
0
headers
public Headers kafkatest_f13372_0()
{    if (recordContext == null) {        throw new IllegalStateException("This should not happen as headers() should only be called while a record is processed");    }    return recordContext.headers();}
f13372
0
timestamp
public long kafkatest_f13373_0()
{    if (recordContext == null) {        throw new IllegalStateException("This should not happen as timestamp() should only be called while a record is processed");    }    return recordContext.timestamp();}
f13373
0
uninitialize
public void kafkatest_f13382_0()
{    initialized = false;}
f13382
0
id
public TaskId kafkatest_f13383_0()
{    return id;}
f13383
0
flushState
 void kafkatest_f13392_0()
{    stateMgr.flush();}
f13392
0
registerStateStores
 void kafkatest_f13393_0()
{    if (topology.stateStores().isEmpty()) {        return;    }    try {        if (!stateDirectory.lock(id)) {            throw new LockException(String.format("%sFailed to lock the state directory for task %s", logPrefix, id));        }    } catch (final IOException e) {        throw new StreamsException(String.format("%sFatal error while trying to lock the state directory for task %s", logPrefix, id));    }    log.trace("Initializing state stores");    for (final StateStore store : topology.stateStores()) {        log.trace("Initializing store {}", store.name());        processorContext.uninitialize();        store.init(processorContext, store);    }}
f13393
0
restoringTaskFor
public StreamTask kafkatest_f13402_0(final TopicPartition partition)
{    return restoringByPartition.get(partition);}
f13402
0
allTasks
 List<StreamTask> kafkatest_f13403_0()
{    final List<StreamTask> tasks = super.allTasks();    tasks.addAll(restoring.values());    return tasks;}
f13403
0
punctuate
 intf13412_1)
{    int punctuated = 0;    final Iterator<Map.Entry<TaskId, StreamTask>> it = running.entrySet().iterator();    while (it.hasNext()) {        final StreamTask task = it.next().getValue();        try {            if (task.maybePunctuateStreamTime()) {                punctuated++;            }            if (task.maybePunctuateSystemTime()) {                punctuated++;            }        } catch (final TaskMigratedException e) {                        final RuntimeException fatalException = closeZombieTask(task);            if (fatalException != null) {                throw fatalException;            }            it.remove();            throw e;        } catch (final KafkaException e) {                        throw e;        }    }    return punctuated;}
 intf13412
1
clear
 void kafkatest_f13413_0()
{    super.clear();    restoring.clear();    restoringByPartition.clear();    restoredPartitions.clear();}
f13413
0
suspendTasks
private RuntimeExceptionf13422_1final Collection<T> tasks)
{    final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);    for (final Iterator<T> it = tasks.iterator(); it.hasNext(); ) {        final T task = it.next();        try {            task.suspend();            suspended.put(task.id(), task);        } catch (final TaskMigratedException closeAsZombieAndSwallow) {            // as we suspend a task, we are either shutting down or rebalancing, thus, we swallow and move on                        firstException.compareAndSet(null, closeZombieTask(task));            it.remove();        } catch (final RuntimeException e) {                        firstException.compareAndSet(null, e);            try {                task.close(false, false);            } catch (final RuntimeException f) {                            }        }    }    return firstException.get();}
private RuntimeExceptionf13422
1
closeZombieTask
 RuntimeExceptionf13423_1final T task)
{    try {        task.close(false, true);    } catch (final RuntimeException e) {         ignore and proceed.", taskTypeName, task.id(), e.toString());        return e;    }    return null;}
 RuntimeExceptionf13423
1
describe
 void kafkatest_f13432_0(final StringBuilder builder, final Collection<T> tasks, final String indent, final String name)
{    builder.append(indent).append(name);    for (final T t : tasks) {        builder.append(indent).append(t.toString(indent + "\t\t"));    }    builder.append("\n");}
f13432
0
allTasks
 List<T> kafkatest_f13433_0()
{    final List<T> tasks = new ArrayList<>();    tasks.addAll(running.values());    tasks.addAll(suspended.values());    tasks.addAll(created.values());    return tasks;}
f13433
0
errCode
public int kafkatest_f13442_0()
{    return errCode;}
f13442
0
latestSupportedVersion
public int kafkatest_f13443_0()
{    return latestSupportedVersion;}
f13443
0
encodePartitionsByHostAsDictionary
private void kafkatest_f13452_0(final DataOutputStream out) throws IOException
{    // Build a dictionary to encode topicNames    int topicIndex = 0;    final Map<String, Integer> topicNameDict = new HashMap<>();    for (final Map.Entry<HostInfo, Set<TopicPartition>> entry : partitionsByHost.entrySet()) {        for (final TopicPartition topicPartition : entry.getValue()) {            if (!topicNameDict.containsKey(topicPartition.topic())) {                topicNameDict.put(topicPartition.topic(), topicIndex++);            }        }    }    // write the topic name dictionary out    out.writeInt(topicNameDict.size());    for (final Map.Entry<String, Integer> entry : topicNameDict.entrySet()) {        out.writeInt(entry.getValue());        out.writeUTF(entry.getKey());    }    // encode partitions by host    out.writeInt(partitionsByHost.size());    // Write the topic index, partition    for (final Map.Entry<HostInfo, Set<TopicPartition>> entry : partitionsByHost.entrySet()) {        writeHostInfo(out, entry.getKey());        out.writeInt(entry.getValue().size());        for (final TopicPartition partition : entry.getValue()) {            out.writeInt(topicNameDict.get(partition.topic()));            out.writeInt(partition.partition());        }    }}
f13452
0
writeHostInfo
private void kafkatest_f13453_0(final DataOutputStream out, final HostInfo hostInfo) throws IOException
{    out.writeUTF(hostInfo.host());    out.writeInt(hostInfo.port());}
f13453
0
decodeVersionTwoData
private static void kafkatest_f13462_0(final AssignmentInfo assignmentInfo, final DataInputStream in) throws IOException
{    decodeActiveTasks(assignmentInfo, in);    decodeStandbyTasks(assignmentInfo, in);    decodePartitionsByHost(assignmentInfo, in);}
f13462
0
decodePartitionsByHost
private static void kafkatest_f13463_0(final AssignmentInfo assignmentInfo, final DataInputStream in) throws IOException
{    assignmentInfo.partitionsByHost = new HashMap<>();    final int numEntries = in.readInt();    for (int i = 0; i < numEntries; i++) {        final HostInfo hostInfo = new HostInfo(in.readUTF(), in.readInt());        assignmentInfo.partitionsByHost.put(hostInfo, readTopicPartitions(in));    }}
f13463
0
toString
public String kafkatest_f13472_0()
{    return "[version=" + usedVersion + ", supported version=" + latestSupportedVersion + ", active tasks=" + activeTasks + ", standby tasks=" + standbyTasks + ", partitions by host=" + partitionsByHost + "]";}
f13472
0
getAssignmentErrorCode
public AtomicIntegerf13473_1final Map<String, ?> configs)
{    final Object ai = configs.get(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE);    if (ai == null) {        final KafkaException fatalException = new KafkaException("assignmentErrorCode is not specified");                throw fatalException;    }    if (!(ai instanceof AtomicInteger)) {        final KafkaException fatalException = new KafkaException(String.format("%s is not an instance of %s", ai.getClass().getName(), AtomicInteger.class.getName()));                throw fatalException;    }    return (AtomicInteger) ai;}
public AtomicIntegerf13473
1
code
public int kafkatest_f13482_0()
{    return code;}
f13482
0
copy
public ClientState kafkatest_f13483_0()
{    return new ClientState(new HashSet<>(activeTasks), new HashSet<>(standbyTasks), new HashSet<>(assignedTasks), new HashSet<>(prevActiveTasks), new HashSet<>(prevStandbyTasks), new HashSet<>(prevAssignedTasks), capacity);}
f13483
0
addPreviousActiveTasks
public void kafkatest_f13492_0(final Set<TaskId> prevTasks)
{    prevActiveTasks.addAll(prevTasks);    prevAssignedTasks.addAll(prevTasks);}
f13492
0
addPreviousStandbyTasks
public void kafkatest_f13493_0(final Set<TaskId> standbyTasks)
{    prevStandbyTasks.addAll(standbyTasks);    prevAssignedTasks.addAll(standbyTasks);}
f13493
0
capacity
 int kafkatest_f13502_0()
{    return capacity;}
f13502
0
hasUnfulfilledQuota
 boolean kafkatest_f13503_0(final int tasksPerThread)
{    return activeTasks.size() < capacity * tasksPerThread;}
f13503
0
findClientsWithoutAssignedTask
private Set<ID> kafkatest_f13512_0(final TaskId taskId)
{    final Set<ID> clientIds = new HashSet<>();    for (final Map.Entry<ID, ClientState> client : clients.entrySet()) {        if (!client.getValue().hasAssignedTask(taskId)) {            clientIds.add(client.getKey());        }    }    return clientIds;}
f13512
0
findClient
private ClientState kafkatest_f13513_0(final TaskId taskId, final Set<ID> clientsWithin)
{    // optimize the case where there is only 1 id to search within.    if (clientsWithin.size() == 1) {        return clients.get(clientsWithin.iterator().next());    }    final ClientState previous = findClientsWithPreviousAssignedTask(taskId, clientsWithin);    if (previous == null) {        return leastLoaded(taskId, clientsWithin);    }    if (shouldBalanceLoad(previous)) {        final ClientState standby = findLeastLoadedClientWithPreviousStandByTask(taskId, clientsWithin);        if (standby == null || shouldBalanceLoad(standby)) {            return leastLoaded(taskId, clientsWithin);        }        return standby;    }    return previous;}
f13513
0
hasNewPair
 boolean kafkatest_f13522_0(final TaskId task1, final Set<TaskId> taskIds)
{    if (pairs.size() == maxPairs) {        return false;    }    for (final TaskId taskId : taskIds) {        if (!pairs.contains(pair(task1, taskId))) {            return true;        }    }    return false;}
f13522
0
addPairs
 void kafkatest_f13523_0(final TaskId taskId, final Set<TaskId> assigned)
{    for (final TaskId id : assigned) {        pairs.add(pair(id, taskId));    }}
f13523
0
userEndPoint
public String kafkatest_f13532_0()
{    return userEndPoint;}
f13532
0
encode
public ByteBuffer kafkatest_f13533_0()
{    final ByteBuffer buf;    switch(usedVersion) {        case 1:            buf = encodeVersionOne();            break;        case 2:            buf = encodeVersionTwo();            break;        case 3:            buf = encodeVersionThree();            break;        case 4:            buf = encodeVersionFour();            break;        case 5:            buf = encodeVersionFive();            break;        default:            throw new IllegalStateException("Unknown metadata version: " + usedVersion + "; latest supported version: " + LATEST_SUPPORTED_VERSION);    }    buf.rewind();    return buf;}
f13533
0
encodeVersionThreeFourAndFive
private ByteBuffer kafkatest_f13542_0(final int usedVersion)
{    final byte[] endPointBytes = prepareUserEndPoint();    final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeFourAndFiveByteLength(endPointBytes));    // used version    buf.putInt(usedVersion);    // supported version    buf.putInt(LATEST_SUPPORTED_VERSION);    encodeClientUUID(buf);    encodeTasks(buf, prevTasks);    encodeTasks(buf, standbyTasks);    encodeUserEndPoint(buf, endPointBytes);    return buf;}
f13542
0
encodeVersionThree
private ByteBuffer kafkatest_f13543_0()
{    return encodeVersionThreeFourAndFive(3);}
f13543
0
decodeUserEndPoint
private static void kafkatest_f13552_0(final SubscriptionInfo subscriptionInfo, final ByteBuffer data)
{    final int bytesLength = data.getInt();    if (bytesLength != 0) {        final byte[] bytes = new byte[bytesLength];        data.get(bytes);        subscriptionInfo.userEndPoint = new String(bytes, StandardCharsets.UTF_8);    }}
f13552
0
decodeVersionThreeData
private static void kafkatest_f13553_0(final SubscriptionInfo subscriptionInfo, final ByteBuffer data)
{    decodeClientUUID(subscriptionInfo, data);    decodeTasks(subscriptionInfo, data);    decodeUserEndPoint(subscriptionInfo, data);}
f13553
0
restoreAll
public void kafkatest_f13562_0(final Collection<KeyValue<byte[], byte[]>> records)
{    throw new UnsupportedOperationException();}
f13562
0
restore
public void kafkatest_f13563_0(final byte[] key, final byte[] value)
{    throw new UnsupportedOperationException("Single restore functionality shouldn't be called directly but " + "through the delegated StateRestoreCallback instance");}
f13563
0
taskId
public TaskId kafkatest_f13573_0()
{    return delegate.taskId();}
f13573
0
keySerde
public Serde<?> kafkatest_f13574_0()
{    return delegate.keySerde();}
f13574
0
forward
public void kafkatest_f13583_0(final K key, final V value, final To to)
{    throw new StreamsException("ProcessorContext#forward() not supported.");}
f13583
0
forward
public void kafkatest_f13584_0(final K key, final V value, final int childIndex)
{    throw new StreamsException("ProcessorContext#forward() not supported.");}
f13584
0
appConfigsWithPrefix
public Map<String, Object> kafkatest_f13593_0(final String prefix)
{    return delegate.appConfigsWithPrefix(prefix);}
f13593
0
getStateStore
public StateStore kafkatest_f13594_0(final String name)
{    final StateStore store = stateManager.getGlobalStore(name);    if (store instanceof TimestampedKeyValueStore) {        return new TimestampedKeyValueStoreReadWriteDecorator((TimestampedKeyValueStore) store);    } else if (store instanceof KeyValueStore) {        return new KeyValueStoreReadWriteDecorator((KeyValueStore) store);    } else if (store instanceof TimestampedWindowStore) {        return new TimestampedWindowStoreReadWriteDecorator((TimestampedWindowStore) store);    } else if (store instanceof WindowStore) {        return new WindowStoreReadWriteDecorator((WindowStore) store);    } else if (store instanceof SessionStore) {        return new SessionStoreReadWriteDecorator((SessionStore) store);    }    return store;}
f13594
0
initialize
public Set<String>f13603_1)
{    try {        if (!stateDirectory.lockGlobalState()) {            throw new LockException(String.format("Failed to lock the global state directory: %s", baseDir));        }    } catch (final IOException e) {        throw new LockException(String.format("Failed to lock the global state directory: %s", baseDir), e);    }    try {        checkpointFileCache.putAll(checkpointFile.read());    } catch (final IOException e) {        try {            stateDirectory.unlockGlobalState();        } catch (final IOException e1) {                    }        throw new StreamsException("Failed to read checkpoints for global state globalStores", e);    }    final List<StateStore> stateStores = topology.globalStateStores();    for (final StateStore stateStore : stateStores) {        globalStoreNames.add(stateStore.name());        stateStore.init(globalProcessorContext, stateStore);    }    return Collections.unmodifiableSet(globalStoreNames);}
public Set<String>f13603
1
reinitializeStateStoresForPartitions
public void kafkatest_f13604_0(final Collection<TopicPartition> partitions, final InternalProcessorContext processorContext)
{    StateManagerUtil.reinitializeStateStoresForPartitions(log, eosEnabled, baseDir, globalStores, topology.storeToChangelogTopic(), partitions, processorContext, checkpointFile, checkpointFileCache);    globalConsumer.assign(partitions);    globalConsumer.seekToBeginning(partitions);}
f13604
0
checkpoint
public voidf13613_1final Map<TopicPartition, Long> offsets)
{    checkpointFileCache.putAll(offsets);    final Map<TopicPartition, Long> filteredOffsets = new HashMap<>();    // Skip non persistent store    for (final Map.Entry<TopicPartition, Long> topicPartitionOffset : checkpointFileCache.entrySet()) {        final String topic = topicPartitionOffset.getKey().topic();        if (!globalNonPersistentStoresTopics.contains(topic)) {            filteredOffsets.put(topicPartitionOffset.getKey(), topicPartitionOffset.getValue());        }    }    try {        checkpointFile.write(filteredOffsets);    } catch (final IOException e) {            }}
public voidf13613
1
checkpointed
public Map<TopicPartition, Long> kafkatest_f13614_0()
{    return Collections.unmodifiableMap(checkpointFileCache);}
f13614
0
state
public State kafkatest_f13623_0()
{    // we do not need to use the stat lock since the variable is volatile    return state;}
f13623
0
setState
private voidf13624_1final State newState)
{    final State oldState = state;    synchronized (stateLock) {        if (state == State.PENDING_SHUTDOWN && newState == State.PENDING_SHUTDOWN) {            // will be refused but we do not throw exception here            return;        } else if (state == State.DEAD) {            // will be refused but we do not throw exception here            return;        } else if (!state.isValidTransition(newState)) {                        throw new StreamsException(logPrefix + "Unexpected state transition from " + oldState + " to " + newState);        } else {                    }        state = newState;    }    if (stateListener != null) {        stateListener.onChange(this, state, oldState);    }}
private voidf13624
1
consumerMetrics
public Map<MetricName, Metric> kafkatest_f13633_0()
{    return Collections.unmodifiableMap(globalConsumer.metrics());}
f13633
0
name
public String kafkatest_f13634_0()
{    return name;}
f13634
0
users
private Set<String> kafkatest_f13643_0()
{    return users;}
f13643
0
loggingEnabled
public boolean kafkatest_f13644_0()
{    return builder.loggingEnabled();}
f13644
0
isMatch
private boolean kafkatest_f13653_0(final String topic)
{    return pattern.matcher(topic).matches();}
f13653
0
describe
 Source kafkatest_f13654_0()
{    return new Source(name, topics.size() == 0 ? null : new HashSet<>(topics), pattern);}
f13654
0
addProcessor
public final void kafkatest_f13663_0(final String name, final ProcessorSupplier supplier, final String... predecessorNames)
{    Objects.requireNonNull(name, "name must not be null");    Objects.requireNonNull(supplier, "supplier must not be null");    Objects.requireNonNull(predecessorNames, "predecessor names must not be null");    if (nodeFactories.containsKey(name)) {        throw new TopologyException("Processor " + name + " is already added.");    }    if (predecessorNames.length == 0) {        throw new TopologyException("Processor " + name + " must have at least one parent");    }    for (final String predecessor : predecessorNames) {        Objects.requireNonNull(predecessor, "predecessor name must not be null");        if (predecessor.equals(name)) {            throw new TopologyException("Processor " + name + " cannot be a predecessor of itself.");        }        if (!nodeFactories.containsKey(predecessor)) {            throw new TopologyException("Predecessor processor " + predecessor + " is not added yet for " + name);        }    }    nodeFactories.put(name, new ProcessorNodeFactory(name, predecessorNames, supplier));    nodeGrouper.add(name);    nodeGrouper.unite(name, predecessorNames);    nodeGroups = null;}
f13663
0
addStateStore
public final void kafkatest_f13664_0(final StoreBuilder<?> storeBuilder, final String... processorNames)
{    addStateStore(storeBuilder, false, processorNames);}
f13664
0
connectProcessorAndStateStore
private void kafkatest_f13673_0(final String processorName, final String stateStoreName)
{    if (globalStateBuilders.containsKey(stateStoreName)) {        throw new TopologyException("Global StateStore " + stateStoreName + " can be used by a Processor without being specified; it should not be explicitly passed.");    }    if (!stateFactories.containsKey(stateStoreName)) {        throw new TopologyException("StateStore " + stateStoreName + " is not added yet.");    }    if (!nodeFactories.containsKey(processorName)) {        throw new TopologyException("Processor " + processorName + " is not added yet.");    }    final StateStoreFactory stateStoreFactory = stateFactories.get(stateStoreName);    final Iterator<String> iter = stateStoreFactory.users().iterator();    if (iter.hasNext()) {        final String user = iter.next();        nodeGrouper.unite(user, processorName);    }    stateStoreFactory.users().add(processorName);    final NodeFactory nodeFactory = nodeFactories.get(processorName);    if (nodeFactory instanceof ProcessorNodeFactory) {        final ProcessorNodeFactory processorNodeFactory = (ProcessorNodeFactory) nodeFactory;        processorNodeFactory.addStateStore(stateStoreName);        connectStateStoreNameToSourceTopicsOrPattern(stateStoreName, processorNodeFactory);    } else {        throw new TopologyException("cannot connect a state store " + stateStoreName + " to a source node or a sink node.");    }}
f13673
0
findSourcesForProcessorPredecessors
private Set<SourceNodeFactory> kafkatest_f13674_0(final String[] predecessors)
{    final Set<SourceNodeFactory> sourceNodes = new HashSet<>();    for (final String predecessor : predecessors) {        final NodeFactory nodeFactory = nodeFactories.get(predecessor);        if (nodeFactory instanceof SourceNodeFactory) {            sourceNodes.add((SourceNodeFactory) nodeFactory);        } else if (nodeFactory instanceof ProcessorNodeFactory) {            sourceNodes.addAll(findSourcesForProcessorPredecessors(((ProcessorNodeFactory) nodeFactory).predecessors));        }    }    return sourceNodes;}
f13674
0
globalNodeGroups
private Set<String> kafkatest_f13683_0()
{    final Set<String> globalGroups = new HashSet<>();    for (final Map.Entry<Integer, Set<String>> nodeGroup : nodeGroups().entrySet()) {        final Set<String> nodes = nodeGroup.getValue();        for (final String node : nodes) {            if (isGlobalSource(node)) {                globalGroups.addAll(nodes);            }        }    }    return globalGroups;}
f13683
0
build
private ProcessorTopology kafkatest_f13684_0(final Set<String> nodeGroup)
{    Objects.requireNonNull(applicationId, "topology has not completed optimization");    final Map<String, ProcessorNode> processorMap = new LinkedHashMap<>();    final Map<String, SourceNode> topicSourceMap = new HashMap<>();    final Map<String, SinkNode> topicSinkMap = new HashMap<>();    final Map<String, StateStore> stateStoreMap = new LinkedHashMap<>();    final Set<String> repartitionTopics = new HashSet<>();    // also make sure the state store map values following the insertion ordering    for (final NodeFactory factory : nodeFactories.values()) {        if (nodeGroup == null || nodeGroup.contains(factory.name)) {            final ProcessorNode node = factory.build();            processorMap.put(node.name(), node);            if (factory instanceof ProcessorNodeFactory) {                buildProcessorNode(processorMap, stateStoreMap, (ProcessorNodeFactory) factory, node);            } else if (factory instanceof SourceNodeFactory) {                buildSourceNode(topicSourceMap, repartitionTopics, (SourceNodeFactory) factory, (SourceNode) node);            } else if (factory instanceof SinkNodeFactory) {                buildSinkNode(processorMap, topicSinkMap, repartitionTopics, (SinkNodeFactory) factory, (SinkNode) node);            } else {                throw new TopologyException("Unknown definition class: " + factory.getClass().getName());            }        }    }    return new ProcessorTopology(new ArrayList<>(processorMap.values()), topicSourceMap, topicSinkMap, new ArrayList<>(stateStoreMap.values()), new ArrayList<>(globalStateStores.values()), storeToChangelogTopic, repartitionTopics);}
f13684
0
createChangelogTopicConfig
private InternalTopicConfig kafkatest_f13693_0(final StateStoreFactory factory, final String name)
{    if (factory.isWindowStore()) {        final WindowedChangelogTopicConfig config = new WindowedChangelogTopicConfig(name, factory.logConfig());        config.setRetentionMs(factory.retentionPeriod());        return config;    } else {        return new UnwindowedChangelogTopicConfig(name, factory.logConfig());    }}
f13693
0
earliestResetTopicsPattern
public synchronized Pattern kafkatest_f13694_0()
{    return resetTopicsPattern(earliestResetTopics, earliestResetPatterns);}
f13694
0
sourceTopicPattern
 synchronized Pattern kafkatest_f13703_0()
{    if (topicPattern == null) {        final List<String> allSourceTopics = new ArrayList<>();        if (!nodeToSourceTopics.isEmpty()) {            for (final List<String> topics : nodeToSourceTopics.values()) {                allSourceTopics.addAll(maybeDecorateInternalSourceTopics(topics));            }            allSourceTopics.removeAll(globalTopics);        }        Collections.sort(allSourceTopics);        topicPattern = buildPatternForOffsetResetTopics(allSourceTopics, nodeToSourcePatterns.values());    }    return topicPattern;}
f13703
0
updateSubscriptions
 synchronized voidf13704_1final SubscriptionUpdates subscriptionUpdates, final String logPrefix)
{        this.subscriptionUpdates = subscriptionUpdates;    setRegexMatchedTopicsToSourceNodes();    setRegexMatchedTopicToStateStore();}
 synchronized voidf13704
1
source
public TopologyDescription.Source kafkatest_f13713_0()
{    return source;}
f13713
0
processor
public TopologyDescription.Processor kafkatest_f13714_0()
{    return processor;}
f13714
0
topics
public String kafkatest_f13723_0()
{    return topics.toString();}
f13723
0
topicSet
public Set<String> kafkatest_f13724_0()
{    return topics;}
f13724
0
hashCode
public int kafkatest_f13733_0()
{    // omit successor as it might change and alter the hash code    return Objects.hash(name, stores);}
f13733
0
topic
public String kafkatest_f13734_0()
{    if (topicNameExtractor instanceof StaticTopicNameExtractor) {        return ((StaticTopicNameExtractor) topicNameExtractor).topicName;    } else {        return null;    }}
f13734
0
toString
public String kafkatest_f13743_0()
{    return "Sub-topology: " + id + "\n" + nodesAsString() + "\n";}
f13743
0
nodesAsString
private String kafkatest_f13744_0()
{    final StringBuilder sb = new StringBuilder();    for (final TopologyDescription.Node node : nodes) {        sb.append("    ");        sb.append(node);        sb.append('\n');    }    return sb.toString();}
f13744
0
addGlobalStore
public void kafkatest_f13753_0(final TopologyDescription.GlobalStore globalStore)
{    globalStores.add(globalStore);}
f13753
0
subtopologies
public Set<TopologyDescription.Subtopology> kafkatest_f13754_0()
{    return Collections.unmodifiableSet(subtopologies);}
f13754
0
toString
public String kafkatest_f13763_0()
{    return String.format("SubscriptionUpdates{updatedTopicSubscriptions=%s}", updatedTopicSubscriptions);}
f13763
0
updateSubscribedTopics
 voidf13764_1final Set<String> topics, final String logPrefix)
{    final SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();        // update the topic groups with the returned subscription set for regex pattern subscriptions    subscriptionUpdates.updateTopics(topics);    updateSubscriptions(subscriptionUpdates, logPrefix);}
 voidf13764
1
removeAllThreadLevelSensors
public final void kafkatest_f13773_0()
{    synchronized (threadLevelSensors) {        while (!threadLevelSensors.isEmpty()) {            metrics.removeSensor(threadLevelSensors.pop());        }    }}
f13773
0
taskLevelTagMap
public Map<String, String> kafkatest_f13774_0(final String taskName)
{    final Map<String, String> tagMap = threadLevelTagMap();    tagMap.put(TASK_ID_TAG, taskName);    return tagMap;}
f13774
0
removeAllCacheLevelSensors
public final void kafkatest_f13783_0(final String taskName, final String cacheName)
{    final String key = cacheSensorPrefix(taskName, cacheName);    synchronized (cacheLevelSensors) {        final Deque<String> strings = cacheLevelSensors.remove(key);        while (strings != null && !strings.isEmpty()) {            metrics.removeSensor(strings.pop());        }    }}
f13783
0
cacheSensorPrefix
private String kafkatest_f13784_0(final String taskName, final String cacheName)
{    return taskSensorPrefix(taskName) + SENSOR_PREFIX_DELIMITER + "cache" + SENSOR_PREFIX_DELIMITER + cacheName;}
f13784
0
tagMap
public final Map<String, String> kafkatest_f13793_0(final String... tags)
{    final Map<String, String> tagMap = new LinkedHashMap<>();    tagMap.put("client-id", threadName);    if (tags != null) {        if ((tags.length % 2) != 0) {            throw new IllegalArgumentException("Tags needs to be specified in key-value pairs");        }        for (int i = 0; i < tags.length; i += 2) {            tagMap.put(tags[i], tags[i + 1]);        }    }    return tagMap;}
f13793
0
constructTags
private Map<String, String> kafkatest_f13794_0(final String scopeName, final String entityName, final String... tags)
{    final String[] updatedTags = Arrays.copyOf(tags, tags.length + 2);    updatedTags[tags.length] = scopeName + "-id";    updatedTags[tags.length + 1] = entityName;    return tagMap(updatedTags);}
f13794
0
addRateOfSumAndSumMetricsToSensor
public static void kafkatest_f13803_0(final Sensor sensor, final String group, final Map<String, String> tags, final String operation, final String descriptionOfRate, final String descriptionOfTotal)
{    addRateOfSumMetricToSensor(sensor, group, tags, operation, descriptionOfRate);    addSumMetricToSensor(sensor, group, tags, operation, descriptionOfTotal);}
f13803
0
addRateOfSumMetricToSensor
public static void kafkatest_f13804_0(final Sensor sensor, final String group, final Map<String, String> tags, final String operation, final String description)
{    sensor.add(new MetricName(operation + RATE_SUFFIX, group, description, tags), new Rate(TimeUnit.SECONDS, new WindowedSum()));}
f13804
0
commitSensor
public static Sensor kafkatest_f13813_0(final StreamsMetricsImpl streamsMetrics)
{    final Sensor commitSensor = streamsMetrics.threadLevelSensor(COMMIT, Sensor.RecordingLevel.INFO);    final Map<String, String> tagMap = streamsMetrics.threadLevelTagMap();    addAvgAndMaxToSensor(commitSensor, THREAD_LEVEL_GROUP, tagMap, COMMIT_LATENCY);    addInvocationRateAndCountToSensor(commitSensor, THREAD_LEVEL_GROUP, tagMap, COMMIT, COMMIT_TOTAL_DESCRIPTION, COMMIT_RATE_DESCRIPTION);    return commitSensor;}
f13813
0
pollSensor
public static Sensor kafkatest_f13814_0(final StreamsMetricsImpl streamsMetrics)
{    final Sensor pollSensor = streamsMetrics.threadLevelSensor(POLL, Sensor.RecordingLevel.INFO);    final Map<String, String> tagMap = streamsMetrics.threadLevelTagMap();    addAvgAndMaxToSensor(pollSensor, THREAD_LEVEL_GROUP, tagMap, POLL_LATENCY);    addInvocationRateAndCountToSensor(pollSensor, THREAD_LEVEL_GROUP, tagMap, POLL, POLL_TOTAL_DESCRIPTION, POLL_RATE_DESCRIPTION);    return pollSensor;}
f13814
0
addRawRecords
 int kafkatest_f13823_0(final TopicPartition partition, final Iterable<ConsumerRecord<byte[], byte[]>> rawRecords)
{    final RecordQueue recordQueue = partitionQueues.get(partition);    final int oldSize = recordQueue.size();    final int newSize = recordQueue.addRawRecords(rawRecords);    // add this record queue to be considered for processing in the future if it was empty before    if (oldSize == 0 && newSize > 0) {        nonEmptyQueuesByTime.offer(recordQueue);        // processed next, and hence the stream-time will be updated when we retrieved records by then        if (nonEmptyQueuesByTime.size() == this.partitionQueues.size()) {            allBuffered = true;        }    }    totalBuffered += newSize - oldSize;    return newSize;}
f13823
0
partitions
public Set<TopicPartition> kafkatest_f13824_0()
{    return Collections.unmodifiableSet(partitionQueues.keySet());}
f13824
0
getStateStore
public StateStore kafkatest_f13833_0(final String name)
{    if (currentNode() == null) {        throw new StreamsException("Accessing from an unknown node");    }    final StateStore global = stateManager.getGlobalStore(name);    if (global != null) {        if (global instanceof TimestampedKeyValueStore) {            return new TimestampedKeyValueStoreReadOnlyDecorator((TimestampedKeyValueStore) global);        } else if (global instanceof KeyValueStore) {            return new KeyValueStoreReadOnlyDecorator((KeyValueStore) global);        } else if (global instanceof TimestampedWindowStore) {            return new TimestampedWindowStoreReadOnlyDecorator((TimestampedWindowStore) global);        } else if (global instanceof WindowStore) {            return new WindowStoreReadOnlyDecorator((WindowStore) global);        } else if (global instanceof SessionStore) {            return new SessionStoreReadOnlyDecorator((SessionStore) global);        }        return global;    }    if (!currentNode().stateStores.contains(name)) {        throw new StreamsException("Processor " + currentNode().name() + " has no access to StateStore " + name + " as the store is not connected to the processor. If you add stores manually via '.addStateStore()' " + "make sure to connect the added store to the processor by providing the processor name to " + "'.addStateStore()' or connect them via '.connectProcessorAndStateStores()'. " + "DSL users need to provide the store name to '.process()', '.transform()', or '.transformValues()' " + "to connect the store to the corresponding operator. If you do not add stores manually, " + "please file a bug report at https://issues.apache.org/jira/projects/KAFKA.");    }    final StateStore store = stateManager.getStore(name);    if (store instanceof TimestampedKeyValueStore) {        return new TimestampedKeyValueStoreReadWriteDecorator((TimestampedKeyValueStore) store);    } else if (store instanceof KeyValueStore) {        return new KeyValueStoreReadWriteDecorator((KeyValueStore) store);    } else if (store instanceof TimestampedWindowStore) {        return new TimestampedWindowStoreReadWriteDecorator((TimestampedWindowStore) store);    } else if (store instanceof WindowStore) {        return new WindowStoreReadWriteDecorator((WindowStore) store);    } else if (store instanceof SessionStore) {        return new SessionStoreReadWriteDecorator((SessionStore) store);    }    return store;}
f13833
0
forward
public void kafkatest_f13834_0(final K key, final V value)
{    forward(key, value, SEND_TO_ALL);}
f13834
0
init
public void kafkatest_f13843_0(final ProcessorContext context, final StateStore root)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
f13843
0
close
public void kafkatest_f13844_0()
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
f13844
0
put
public void kafkatest_f13853_0(final K key, final V value)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
f13853
0
put
public void kafkatest_f13854_0(final K key, final V value, final long windowStartTimestamp)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
f13854
0
put
public void kafkatest_f13863_0(final Windowed<K> sessionKey, final AGG aggregate)
{    throw new UnsupportedOperationException(ERROR_MESSAGE);}
f13863
0
fetchSession
public AGG kafkatest_f13864_0(final K key, final long startTime, final long endTime)
{    return wrapped().fetchSession(key, startTime, endTime);}
f13864
0
put
public void kafkatest_f13873_0(final K key, final V value)
{    wrapped().put(key, value);}
f13873
0
putIfAbsent
public V kafkatest_f13874_0(final K key, final V value)
{    return wrapped().putIfAbsent(key, value);}
f13874
0
all
public KeyValueIterator<Windowed<K>, V> kafkatest_f13883_0()
{    return wrapped().all();}
f13883
0
findSessions
public KeyValueIterator<Windowed<K>, AGG> kafkatest_f13884_0(final K key, final long earliestSessionEndTime, final long latestSessionStartTime)
{    return wrapped().findSessions(key, earliestSessionEndTime, latestSessionStartTime);}
f13884
0
children
public List<ProcessorNode<?, ?>> kafkatest_f13893_0()
{    return children;}
f13893
0
getChild
 ProcessorNode kafkatest_f13894_0(final String childName)
{    return childByName.get(childName);}
f13894
0
removeAllSensors
private void kafkatest_f13903_0()
{    metrics.removeAllNodeLevelSensors(taskName, processorNodeName);}
f13903
0
createTaskAndNodeLatencyAndThroughputSensors
private static Sensor kafkatest_f13904_0(final String operation, final StreamsMetricsImpl metrics, final String taskName, final String processorNodeName, final Map<String, String> taskTags, final Map<String, String> nodeTags)
{    final Sensor parent = metrics.taskLevelSensor(taskName, operation, Sensor.RecordingLevel.DEBUG);    addAvgAndMaxLatencyToSensor(parent, PROCESSOR_NODE_METRICS_GROUP, taskTags, operation);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(parent, PROCESSOR_NODE_METRICS_GROUP, taskTags, operation);    final Sensor sensor = metrics.nodeLevelSensor(taskName, processorNodeName, operation, Sensor.RecordingLevel.DEBUG, parent);    addAvgAndMaxLatencyToSensor(sensor, PROCESSOR_NODE_METRICS_GROUP, nodeTags, operation);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(sensor, PROCESSOR_NODE_METRICS_GROUP, nodeTags, operation);    return sensor;}
f13904
0
equals
public boolean kafkatest_f13913_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final ProcessorRecordContext that = (ProcessorRecordContext) o;    return timestamp == that.timestamp && offset == that.offset && partition == that.partition && Objects.equals(topic, that.topic) && Objects.equals(headers, that.headers);}
f13913
0
hashCode
public int kafkatest_f13914_0()
{    throw new UnsupportedOperationException("ProcessorRecordContext is unsafe for use in Hash collections");}
f13914
0
putOffsetLimit
 void kafkatest_f13923_0(final TopicPartition partition, final long limit)
{    log.trace("Updating store offset limit for partition {} to {}", partition, limit);    offsetLimits.put(partition, limit);}
f13923
0
offsetLimit
 long kafkatest_f13924_0(final TopicPartition partition)
{    final Long limit = offsetLimits.get(partition);    return limit != null ? limit : Long.MAX_VALUE;}
f13924
0
changelogPartitions
 Collection<TopicPartition> kafkatest_f13933_0()
{    return unmodifiableList(changelogPartitions);}
f13933
0
ensureStoresRegistered
 void kafkatest_f13934_0()
{    for (final Map.Entry<String, Optional<StateStore>> entry : registeredStores.entrySet()) {        if (!entry.getValue().isPresent()) {            throw new IllegalStateException("store [" + entry.getKey() + "] has not been correctly registered. This is a bug in Kafka Streams.");        }    }}
f13934
0
processors
public List<ProcessorNode> kafkatest_f13943_0()
{    return processorNodes;}
f13943
0
stateStores
public List<StateStore> kafkatest_f13944_0()
{    return stateStores;}
f13944
0
processorConnectedStateStores
public Set<String> kafkatest_f13953_0(final String processorName)
{    for (final ProcessorNode<?, ?> node : processorNodes) {        if (node.name().equals(processorName)) {            return node.stateStores;        }    }    return Collections.emptySet();}
f13953
0
schedule
public Cancellable kafkatest_f13954_0(final PunctuationSchedule sched)
{    synchronized (pq) {        pq.add(sched);    }    return sched.cancellable();}
f13954
0
equals
public boolean kafkatest_f13963_0(final Object other)
{    return super.equals(other);}
f13963
0
hashCode
public int kafkatest_f13964_0()
{    return super.hashCode();}
f13964
0
restore
 void kafkatest_f13973_0(final byte[] key, final byte[] value)
{    throw new UnsupportedOperationException();}
f13973
0
init
public void kafkatest_f13974_0(final Producer<byte[], byte[]> producer)
{    this.producer = producer;}
f13974
0
offsets
public Map<TopicPartition, Long> kafkatest_f13983_0()
{    return Collections.unmodifiableMap(offsets);}
f13983
0
producer
 Producer<byte[], byte[]> kafkatest_f13984_0()
{    return producer;}
f13984
0
headRecordTimestamp
public long kafkatest_f13993_0()
{    return headRecord == null ? UNKNOWN : headRecord.timestamp;}
f13993
0
partitionTime
 long kafkatest_f13994_0()
{    return partitionTime;}
f13994
0
process
public void kafkatest_f14003_0(final K key, final V value)
{    final RecordCollector collector = ((RecordCollector.Supplier) context).recordCollector();    final long timestamp = context.timestamp();    if (timestamp < 0) {        throw new StreamsException("Invalid (negative) timestamp of " + timestamp + " for output record <" + key + ":" + value + ">.");    }    final String topic = topicExtractor.extract(key, value, this.context.recordContext());    try {        collector.send(topic, key, value, context.headers(), timestamp, keySerializer, valSerializer, partitioner);    } catch (final ClassCastException e) {        final String keyClass = key == null ? "unknown because key is null" : key.getClass().getName();        final String valueClass = value == null ? "unknown because value is null" : value.getClass().getName();        throw new StreamsException(String.format("A serializer (key: %s / value: %s) is not compatible to the actual key or value type " + "(key type: %s / value type: %s). Change the default Serdes in StreamConfig or " + "provide correct Serdes via method parameters.", keySerializer.getClass().getName(), valSerializer.getClass().getName(), keyClass, valueClass), e);    }}
f14003
0
toString
public String kafkatest_f14004_0()
{    return toString("");}
f14004
0
compareTo
public int kafkatest_f14013_0(final Object other)
{    final long otherTimestamp = ((Stamped<?>) other).timestamp;    if (timestamp < otherTimestamp) {        return -1;    } else if (timestamp > otherTimestamp) {        return 1;    }    return 0;}
f14013
0
equals
public boolean kafkatest_f14014_0(final Object other)
{    if (other == null || getClass() != other.getClass()) {        return false;    }    final long otherTimestamp = ((Stamped<?>) other).timestamp;    return timestamp == otherTimestamp;}
f14014
0
offsets
public Map<TopicPartition, Long> kafkatest_f14028_0()
{    return Collections.emptyMap();}
f14028
0
getStateMgr
 StateManager kafkatest_f14029_0()
{    return stateManager;}
f14029
0
forward
public void kafkatest_f14038_0(final K key, final V value, final int childIndex)
{    throw new UnsupportedOperationException("this should not happen: forward() not supported in standby tasks.");}
f14038
0
forward
public void kafkatest_f14039_0(final K key, final V value, final String childName)
{    throw new UnsupportedOperationException("this should not happen: forward() not supported in standby tasks.");}
f14039
0
initializeTopology
public void kafkatest_f14048_0()
{// no-op}
f14048
0
resume
public voidf14049_1)
{        allowUpdateOfOffsetLimit();}
public voidf14049
1
allowUpdateOfOffsetLimit
 void kafkatest_f14058_0()
{    updateableOffsetLimits.addAll(offsetLimits.keySet());}
f14058
0
directoryForTask
public File kafkatest_f14059_0(final TaskId taskId)
{    final File taskDir = new File(stateDir, taskId.toString());    if (createStateDirectory && !taskDir.exists() && !taskDir.mkdir()) {        throw new ProcessorStateException(String.format("task directory [%s] doesn't exist and couldn't be created", taskDir.getPath()));    }    return taskDir;}
f14059
0
cleanRemovedTasks
private synchronized voidf14068_1final long cleanupDelayMs, final boolean manualUserCall) throws Exception
{    final File[] taskDirs = listTaskDirectories();    if (taskDirs == null || taskDirs.length == 0) {        // nothing to do        return;    }    for (final File taskDir : taskDirs) {        final String dirName = taskDir.getName();        final TaskId id = TaskId.parse(dirName);        if (!locks.containsKey(id)) {            try {                if (lock(id)) {                    final long now = time.milliseconds();                    final long lastModifiedMs = taskDir.lastModified();                    if (now > lastModifiedMs + cleanupDelayMs || manualUserCall) {                        if (!manualUserCall) {                                                    } else {                                                    }                        Utils.delete(taskDir);                    }                }            } catch (final OverlappingFileLockException e) {                // locked by another thread                if (manualUserCall) {                                        throw e;                }            } catch (final IOException e) {                                if (manualUserCall) {                    throw e;                }            } finally {                try {                    unlock(id);                } catch (final IOException e) {                                        if (manualUserCall) {                        throw e;                    }                }            }        }    }}
private synchronized voidf14068
1
listTaskDirectories
 File[] kafkatest_f14069_0()
{    return !stateDir.exists() ? new File[0] : stateDir.listFiles(pathname -> pathname.isDirectory() && PATH_NAME.matcher(pathname.getName()).matches());}
f14069
0
checkpoint
 long kafkatest_f14078_0()
{    return checkpointOffset;}
f14078
0
setCheckpointOffset
 void kafkatest_f14079_0(final long checkpointOffset)
{    this.checkpointOffset = checkpointOffset;}
f14079
0
setEndingOffset
 void kafkatest_f14088_0(final long endingOffset)
{    this.endingOffset = Math.min(offsetLimit, endingOffset);}
f14088
0
startingOffset
 long kafkatest_f14089_0()
{    return startingOffset;}
f14089
0
restore
public Collection<TopicPartition>f14098_1final RestoringTasks active)
{    if (!needsInitializing.isEmpty()) {        initialize(active);    }    if (needsRestoring.isEmpty()) {        restoreConsumer.unsubscribe();        return completed();    }    try {        final ConsumerRecords<byte[], byte[]> records = restoreConsumer.poll(pollTime);        for (final TopicPartition partition : needsRestoring) {            final StateRestorer restorer = stateRestorers.get(partition);            final long pos = processNext(records.records(partition), restorer, restoreToOffsets.get(partition));            restorer.setRestoredOffset(pos);            if (restorer.hasCompleted(pos, restoreToOffsets.get(partition))) {                restorer.restoreDone();                restoreToOffsets.remove(partition);                completedRestorers.add(partition);            }        }    } catch (final InvalidOffsetException recoverableException) {                final Set<TopicPartition> partitions = recoverableException.partitions();        for (final TopicPartition partition : partitions) {            final StreamTask task = active.restoringTaskFor(partition);                        needsInitializing.remove(partition);            needsRestoring.remove(partition);            final StateRestorer restorer = stateRestorers.get(partition);            restorer.setCheckpointOffset(StateRestorer.NO_CHECKPOINT);            task.reinitializeStateStoresForPartitions(recoverableException.partitions());        }        restoreConsumer.seekToBeginning(partitions);    }    needsRestoring.removeAll(completedRestorers);    if (needsRestoring.isEmpty()) {        restoreConsumer.unsubscribe();    }    return completed();}
public Collection<TopicPartition>f14098
1
initialize
private voidf14099_1final RestoringTasks active)
{    if (!restoreConsumer.subscription().isEmpty()) {        throw new StreamsException("Restore consumer should not be subscribed to any topics (" + restoreConsumer.subscription() + ")");    }    // first refresh the changelog partition information from brokers, since initialize is only called when    // the needsInitializing map is not empty, meaning we do not know the metadata for some of them yet    refreshChangelogInfo();    final Set<TopicPartition> initializable = new HashSet<>();    for (final TopicPartition topicPartition : needsInitializing) {        if (hasPartition(topicPartition)) {            initializable.add(topicPartition);        }    }    // try to fetch end offsets for the initializable restorers and remove any partitions    // where we already have all of the data    final Map<TopicPartition, Long> endOffsets;    try {        endOffsets = restoreConsumer.endOffsets(initializable);    } catch (final TimeoutException e) {        // if timeout exception gets thrown we just give up this time and retry in the next run loop         will fall back to partition by partition fetching", initializable);        return;    }    endOffsets.forEach((partition, endOffset) -> {        if (endOffset != null) {            final StateRestorer restorer = stateRestorers.get(partition);            final long offsetLimit = restorer.offsetLimit();            restoreToOffsets.put(partition, Math.min(endOffset, offsetLimit));        } else {             removing this partition from the current run loop");            initializable.remove(partition);        }    });    final Iterator<TopicPartition> iter = initializable.iterator();    while (iter.hasNext()) {        final TopicPartition topicPartition = iter.next();        final Long restoreOffset = restoreToOffsets.get(topicPartition);        final StateRestorer restorer = stateRestorers.get(topicPartition);        if (restorer.checkpoint() >= restoreOffset) {            restorer.setRestoredOffset(restorer.checkpoint());            iter.remove();            completedRestorers.add(topicPartition);        } else if (restoreOffset == 0) {            restorer.setRestoredOffset(0);            iter.remove();            completedRestorers.add(topicPartition);        } else {            restorer.setEndingOffset(restoreOffset);        }        needsInitializing.remove(topicPartition);    }    // set up restorer for those initializable    if (!initializable.isEmpty()) {        startRestoration(initializable, active);    }}
private voidf14099
1
toString
public String kafkatest_f14108_0()
{    return toString("");}
f14108
0
toString
public String kafkatest_f14109_0(final String indent)
{    final StringBuilder builder = new StringBuilder();    builder.append(indent).append("GlobalMetadata: ").append(allMetadata).append("\n");    builder.append(indent).append("GlobalStores: ").append(globalStores).append("\n");    builder.append(indent).append("My HostInfo: ").append(thisHost).append("\n");    builder.append(indent).append(clusterMetadata).append("\n");    return builder.toString();}
f14109
0
getSourceTopicsInfo
private SourceTopicsInfo kafkatest_f14118_0(final String storeName)
{    final List<String> sourceTopics = builder.stateStoreNameToSourceTopics().get(storeName);    if (sourceTopics == null || sourceTopics.isEmpty()) {        return null;    }    return new SourceTopicsInfo(sourceTopics);}
f14118
0
isInitialized
private boolean kafkatest_f14119_0()
{    return clusterMetadata != null && !clusterMetadata.topics().isEmpty();}
f14119
0
name
public String kafkatest_f14128_0()
{    return "stream";}
f14128
0
subscriptionUserData
public ByteBuffer kafkatest_f14129_0(final Set<String> topics)
{    // Adds the following information to subscription    // 1. Client UUID (a unique id assigned to an instance of KafkaStreams)    // 2. Task ids of previously running tasks    // 3. Task ids of valid local states on the client's state directory.    final Set<TaskId> previousActiveTasks = taskManager.prevActiveTaskIds();    final Set<TaskId> standbyTasks = taskManager.cachedTasksIds();    standbyTasks.removeAll(previousActiveTasks);    final SubscriptionInfo data = new SubscriptionInfo(usedSubscriptionMetadataVersion, taskManager.processId(), previousActiveTasks, standbyTasks, userEndPoint);    taskManager.updateSubscriptionsFromMetadata(topics);    return data.encode();}
f14129
0
processVersionTwoAssignment
public static void kafkatest_f14138_0(final String logPrefix, final AssignmentInfo info, final List<TopicPartition> partitions, final Map<TaskId, Set<TopicPartition>> activeTasks, final Map<TopicPartition, PartitionInfo> topicToPartitionInfo)
{    processVersionOneAssignment(logPrefix, info, partitions, activeTasks);    // process partitions by host    final Map<HostInfo, Set<TopicPartition>> partitionsByHost = info.partitionsByHost();    for (final Set<TopicPartition> value : partitionsByHost.values()) {        for (final TopicPartition topicPartition : value) {            topicToPartitionInfo.put(topicPartition, new PartitionInfo(topicPartition.topic(), topicPartition.partition(), null, new Node[0], new Node[0]));        }    }}
f14138
0
prepareTopic
private voidf14139_1final Map<String, InternalTopicConfig> topicPartitions)
{        // first construct the topics to make ready    final Map<String, InternalTopicConfig> topicsToMakeReady = new HashMap<>();    for (final InternalTopicConfig topic : topicPartitions.values()) {        final Optional<Integer> numPartitions = topic.numberOfPartitions();        if (!numPartitions.isPresent()) {            throw new StreamsException(String.format("%sTopic [%s] number of partitions not defined", logPrefix, topic.name()));        }        topic.setNumberOfPartitions(numPartitions.get());        topicsToMakeReady.put(topic.name(), topic);    }    if (!topicsToMakeReady.isEmpty()) {        internalTopicManager.makeReady(topicsToMakeReady);    }    }
private voidf14139
1
getStacktraceString
private Stringf14148_1final KafkaException e)
{    String stacktrace = null;    try (final StringWriter stringWriter = new StringWriter();        final PrintWriter printWriter = new PrintWriter(stringWriter)) {        e.printStackTrace(printWriter);        stacktrace = stringWriter.toString();    } catch (final IOException ioe) {            }    return stacktrace;}
private Stringf14148
1
punctuate
public void kafkatest_f14149_0(final ProcessorNode node, final long timestamp, final PunctuationType type, final Punctuator punctuator)
{    if (processorContext.currentNode() != null) {        throw new IllegalStateException(String.format("%sCurrent node is not null", logPrefix));    }    updateProcessorContext(new StampedRecord(DUMMY_RECORD, timestamp), node);    if (log.isTraceEnabled()) {        log.trace("Punctuating processor {} with timestamp {} and punctuation type {}", node.name(), timestamp, type);    }    try {        node.punctuate(timestamp, punctuator);    } catch (final ProducerFencedException fatal) {        throw new TaskMigratedException(this, fatal);    } catch (final KafkaException e) {        throw new StreamsException(String.format("%sException caught while punctuating processor '%s'", logPrefix, node.name()), e);    } finally {        processorContext.setCurrentNode(null);    }}
f14149
0
suspend
 void kafkatest_f14158_0(final boolean clean, final boolean isZombie)
{    try {        // should we call this only on clean suspend?        closeTopology();    } catch (final RuntimeException fatal) {        if (clean) {            throw fatal;        }    }    if (clean) {        TaskMigratedException taskMigratedException = null;        try {            commit(false);        } finally {            if (eosEnabled) {                stateMgr.checkpoint(activeTaskCheckpointableOffsets());                try {                    recordCollector.close();                } catch (final ProducerFencedException e) {                    taskMigratedException = new TaskMigratedException(this, e);                } finally {                    producer = null;                }            }        }        if (taskMigratedException != null) {            throw taskMigratedException;        }    } else {        maybeAbortTransactionAndCloseRecordCollector(isZombie);    }}
f14158
0
maybeAbortTransactionAndCloseRecordCollector
private voidf14159_1final boolean isZombie)
{    if (eosEnabled && !isZombie) {        try {            if (transactionInFlight) {                producer.abortTransaction();            }            transactionInFlight = false;        } catch (final ProducerFencedException ignore) {        /* TODO                 * this should actually never happen atm as we guard the call to #abortTransaction                 * -> the reason for the guard is a "bug" in the Producer -- it throws IllegalStateException                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got                 * fixed and fall-back to this catch-and-swallow code                 */        // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens        }    }    if (eosEnabled) {        try {            recordCollector.close();        } catch (final Throwable e) {                    } finally {            producer = null;        }    }}
private voidf14159
1
maybePunctuateSystemTime
public boolean kafkatest_f14168_0()
{    final long systemTime = time.milliseconds();    final boolean punctuated = systemTimePunctuationQueue.mayPunctuate(systemTime, PunctuationType.WALL_CLOCK_TIME, this);    if (punctuated) {        commitNeeded = true;    }    return punctuated;}
f14168
0
requestCommit
 void kafkatest_f14169_0()
{    commitRequested = true;}
f14169
0
setState
 Statef14178_1final State newState)
{    final State oldState;    synchronized (stateLock) {        oldState = state;        if (state == State.PENDING_SHUTDOWN && newState != State.DEAD) {                        // refused but we do not throw exception here            return null;        } else if (state == State.DEAD) {                        // will be refused but we do not throw exception here            return null;        } else if (state == State.PARTITIONS_REVOKED && newState == State.PARTITIONS_REVOKED) {                        // refused but we do not throw exception here            return null;        } else if (!state.isValidTransition(newState)) {                        throw new StreamsException(logPrefix + "Unexpected state transition from " + oldState + " to " + newState);        } else {                    }        state = newState;        if (newState == State.RUNNING) {            updateThreadMetadata(taskManager.activeTasks(), taskManager.standbyTasks());        } else {            updateThreadMetadata(Collections.emptyMap(), Collections.emptyMap());        }    }    if (stateListener != null) {        stateListener.onChange(this, state, oldState);    }    return oldState;}
 Statef14178
1
isRunningAndNotRebalancing
public boolean kafkatest_f14179_0()
{    // we do not need to grab stateLock since it is a single read    return state == State.RUNNING;}
f14179
0
close
public voidf14189_1)
{    if (threadProducer != null) {        try {            threadProducer.close();        } catch (final Throwable e) {                    }    }}
public voidf14189
1
createTask
 StandbyTask kafkatest_f14190_0(final Consumer<byte[], byte[]> consumer, final TaskId taskId, final Set<TopicPartition> partitions)
{    createTaskSensor.record();    final ProcessorTopology topology = builder.build(taskId.topicGroupId);    if (!topology.stateStores().isEmpty() && !topology.storeToChangelogTopic().isEmpty()) {        return new StandbyTask(taskId, partitions, topology, consumer, storeChangelogReader, config, streamsMetrics, stateDirectory);    } else {        log.trace("Skipped standby task {} with assigned partitions {} " + "since it does not have any state stores to materialize", taskId, partitions);        return null;    }}
f14190
0
runLoop
private voidf14199_1)
{    consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);    while (isRunning()) {        try {            runOnce();            if (assignmentErrorCode.get() == AssignorError.VERSION_PROBING.code()) {                                assignmentErrorCode.set(AssignorError.NONE.code());                enforceRebalance();            }        } catch (final TaskMigratedException ignoreAndRejoinGroup) {                        enforceRebalance();        }    }}
private voidf14199
1
enforceRebalance
private void kafkatest_f14200_0()
{    consumer.unsubscribe();    consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);}
f14200
0
advanceNowAndComputeLatency
private long kafkatest_f14209_0()
{    final long previous = now;    now = time.milliseconds();    return Math.max(now - previous, 0);}
f14209
0
shutdown
public voidf14210_1)
{        final State oldState = setState(State.PENDING_SHUTDOWN);    if (oldState == State.CREATED) {        // The thread may not have been started. Take responsibility for shutting down        completeShutdown(true);    }}
public voidf14210
1
producerMetrics
public Map<MetricName, Metric> kafkatest_f14219_0()
{    final LinkedHashMap<MetricName, Metric> result = new LinkedHashMap<>();    if (producer != null) {        final Map<MetricName, ? extends Metric> producerMetrics = producer.metrics();        if (producerMetrics != null) {            result.putAll(producerMetrics);        }    } else {        // all the active tasks and add their metrics to the output metrics map.        for (final StreamTask task : taskManager.activeTasks().values()) {            final Map<MetricName, ? extends Metric> taskProducerMetrics = task.getProducer().metrics();            result.putAll(taskProducerMetrics);        }    }    return result;}
f14219
0
consumerMetrics
public Map<MetricName, Metric> kafkatest_f14220_0()
{    final Map<MetricName, ? extends Metric> consumerMetrics = consumer.metrics();    final Map<MetricName, ? extends Metric> restoreConsumerMetrics = restoreConsumer.metrics();    final LinkedHashMap<MetricName, Metric> result = new LinkedHashMap<>();    result.putAll(consumerMetrics);    result.putAll(restoreConsumerMetrics);    return result;}
f14220
0
addStreamTasks
private voidf14229_1final Collection<TopicPartition> assignment)
{    if (assignedActiveTasks == null || assignedActiveTasks.isEmpty()) {        return;    }    final Map<TaskId, Set<TopicPartition>> newTasks = new HashMap<>();    // collect newly assigned tasks and reopen re-assigned tasks        for (final Map.Entry<TaskId, Set<TopicPartition>> entry : assignedActiveTasks.entrySet()) {        final TaskId taskId = entry.getKey();        final Set<TopicPartition> partitions = entry.getValue();        if (assignment.containsAll(partitions)) {            try {                if (!active.maybeResumeSuspendedTask(taskId, partitions)) {                    newTasks.put(taskId, partitions);                }            } catch (final StreamsException e) {                                throw e;            }        } else {                    }    }    if (newTasks.isEmpty()) {        return;    }    // CANNOT FIND RETRY AND BACKOFF LOGIC    // create all newly assigned tasks (guard against race condition with other thread via backoff and retry)    // -> other thread will call removeSuspendedTasks(); eventually        for (final StreamTask task : taskCreator.createTasks(consumer, newTasks)) {        active.addNewTask(task);    }}
private voidf14229
1
addStandbyTasks
private voidf14230_1)
{    if (assignedStandbyTasks == null || assignedStandbyTasks.isEmpty()) {        return;    }        final Map<TaskId, Set<TopicPartition>> newStandbyTasks = new HashMap<>();    // collect newly assigned standby tasks and reopen re-assigned standby tasks    for (final Map.Entry<TaskId, Set<TopicPartition>> entry : assignedStandbyTasks.entrySet()) {        final TaskId taskId = entry.getKey();        final Set<TopicPartition> partitions = entry.getValue();        if (!standby.maybeResumeSuspendedTask(taskId, partitions)) {            newStandbyTasks.put(taskId, partitions);        }    }    if (newStandbyTasks.isEmpty()) {        return;    }    // create all newly assigned standby tasks (guard against race condition with other thread via backoff and retry)    // -> other thread will call removeSuspendedStandbyTasks(); eventually    log.trace("New standby tasks to be created: {}", newStandbyTasks);    for (final StandbyTask task : standbyTaskCreator.createTasks(consumer, newStandbyTasks)) {        standby.addNewTask(task);    }}
private voidf14230
1
getAdminClient
 Admin kafkatest_f14239_0()
{    return adminClient;}
f14239
0
suspendedActiveTaskIds
 Set<TaskId> kafkatest_f14240_0()
{    return active.previousTaskIds();}
f14240
0
hasStandbyRunningTasks
 boolean kafkatest_f14249_0()
{    return standby.hasRunningTasks();}
f14249
0
assignStandbyPartitions
private void kafkatest_f14250_0()
{    final Collection<StandbyTask> running = standby.running();    final Map<TopicPartition, Long> checkpointedOffsets = new HashMap<>();    for (final StandbyTask standbyTask : running) {        checkpointedOffsets.putAll(standbyTask.checkpointedOffsets());    }    restoreConsumer.assign(checkpointedOffsets.keySet());    for (final Map.Entry<TopicPartition, Long> entry : checkpointedOffsets.entrySet()) {        final TopicPartition partition = entry.getKey();        final long offset = entry.getValue();        if (offset >= 0) {            restoreConsumer.seek(partition, offset);        } else {            restoreConsumer.seekToBeginning(singleton(partition));        }    }}
f14250
0
maybeCommitActiveTasksPerUserRequested
 int kafkatest_f14259_0()
{    return active.maybeCommitPerUserRequested();}
f14259
0
maybePurgeCommitedRecords
 voidf14260_1)
{    // newer offsets anyways.    if (deleteRecordsResult == null || deleteRecordsResult.all().isDone()) {        if (deleteRecordsResult != null && deleteRecordsResult.all().isCompletedExceptionally()) {                    }        final Map<TopicPartition, RecordsToDelete> recordsToDelete = new HashMap<>();        for (final Map.Entry<TopicPartition, Long> entry : active.recordsToDelete().entrySet()) {            recordsToDelete.put(entry.getKey(), RecordsToDelete.beforeOffset(entry.getValue()));        }        if (!recordsToDelete.isEmpty()) {            deleteRecordsResult = adminClient.deleteRecords(recordsToDelete);            log.trace("Sent delete-records request: {}", recordsToDelete);        }    }}
 voidf14260
1
getProperties
public Map<String, String> kafkatest_f14269_0(final Map<String, String> defaultProperties, final long additionalRetentionMs)
{    // internal topic config overridden rule: library overrides < global config overrides < per-topic config overrides    final Map<String, String> topicConfig = new HashMap<>(UNWINDOWED_STORE_CHANGELOG_TOPIC_DEFAULT_OVERRIDES);    topicConfig.putAll(defaultProperties);    topicConfig.putAll(topicConfigs);    return topicConfig;}
f14269
0
equals
public boolean kafkatest_f14270_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final UnwindowedChangelogTopicConfig that = (UnwindowedChangelogTopicConfig) o;    return Objects.equals(name, that.name) && Objects.equals(topicConfigs, that.topicConfigs);}
f14270
0
toString
public String kafkatest_f14279_0()
{    return topicGroupId + "_" + partition;}
f14279
0
parse
public static TaskId kafkatest_f14280_0(final String taskIdStr)
{    final int index = taskIdStr.indexOf('_');    if (index <= 0 || index + 1 >= taskIdStr.length()) {        throw new TaskIdFormatException(taskIdStr);    }    try {        final int topicGroupId = Integer.parseInt(taskIdStr.substring(0, index));        final int partition = Integer.parseInt(taskIdStr.substring(index + 1));        return new TaskId(topicGroupId, partition);    } catch (final Exception e) {        throw new TaskIdFormatException(taskIdStr);    }}
f14280
0
topicPartitions
public Set<TopicPartition> kafkatest_f14289_0()
{    return topicPartitions;}
f14289
0
equals
public boolean kafkatest_f14290_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final TaskMetadata that = (TaskMetadata) o;    return Objects.equals(taskId, that.taskId) && Objects.equals(topicPartitions, that.topicPartitions);}
f14290
0
producerClientIds
public Set<String> kafkatest_f14299_0()
{    return producerClientIds;}
f14299
0
adminClientId
public String kafkatest_f14300_0()
{    return adminClientId;}
f14300
0
hashCode
public int kafkatest_f14309_0()
{    throw new UnsupportedOperationException("To is unsafe for use in Hash collections");}
f14309
0
onInvalidTimestamp
public long kafkatest_f14310_0(final ConsumerRecord<Object, Object> record, final long recordTimestamp, final long partitionTime) throws StreamsException
{    if (partitionTime < 0) {        throw new StreamsException("Could not infer new timestamp for input record " + record + " because partition time is unknown.");    }    return partitionTime;}
f14310
0
next
public KeyValue<K, V> kafkatest_f14319_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    final Bytes nextCacheKey = cacheIterator.hasNext() ? cacheIterator.peekNextKey() : null;    final KS nextStoreKey = storeIterator.hasNext() ? storeIterator.peekNextKey() : null;    if (nextCacheKey == null) {        return nextStoreValue(nextStoreKey);    }    if (nextStoreKey == null) {        return nextCacheValue(nextCacheKey);    }    final int comparison = compare(nextCacheKey, nextStoreKey);    if (comparison > 0) {        return nextStoreValue(nextStoreKey);    } else if (comparison < 0) {        return nextCacheValue(nextCacheKey);    } else {        // skip the same keyed element        storeIterator.next();        return nextCacheValue(nextCacheKey);    }}
f14319
0
nextStoreValue
private KeyValue<K, V> kafkatest_f14320_0(final KS nextStoreKey)
{    final KeyValue<KS, VS> next = storeIterator.next();    if (!next.key.equals(nextStoreKey)) {        throw new IllegalStateException("Next record key is not the peeked key value; this should not happen");    }    return deserializeStorePair(next);}
f14320
0
put
public voidf14329_1final Bytes key, final byte[] value)
{    final long timestamp = keySchema.segmentTimestamp(key);    observedStreamTime = Math.max(observedStreamTime, timestamp);    final long segmentId = segments.segmentId(timestamp);    final S segment = segments.getOrCreateSegmentIfLive(segmentId, context, observedStreamTime);    if (segment == null) {        expiredRecordSensor.record();            } else {        segment.put(key, value);    }}
public voidf14329
1
get
public byte[] kafkatest_f14330_0(final Bytes key)
{    final S segment = segments.getSegmentForTimestamp(keySchema.segmentTimestamp(key));    if (segment == null) {        return null;    }    return segment.get(key);}
f14330
0
getWriteBatches
 Map<S, WriteBatch> kafkatest_f14339_0(final Collection<KeyValue<byte[], byte[]>> records)
{    // advance stream time to the max timestamp in the batch    for (final KeyValue<byte[], byte[]> record : records) {        final long timestamp = keySchema.segmentTimestamp(Bytes.wrap(record.key));        observedStreamTime = Math.max(observedStreamTime, timestamp);    }    final Map<S, WriteBatch> writeBatchMap = new HashMap<>();    for (final KeyValue<byte[], byte[]> record : records) {        final long timestamp = keySchema.segmentTimestamp(Bytes.wrap(record.key));        final long segmentId = segments.segmentId(timestamp);        final S segment = segments.getOrCreateSegmentIfLive(segmentId, context, observedStreamTime);        if (segment != null) {            // will only close the database and open it again with bulk loading enabled.            if (!bulkLoadSegments.contains(segment)) {                segment.toggleDbForBulkLoading(true);                // If the store does not exist yet, the getOrCreateSegmentIfLive will call openDB that                // makes the open flag for the newly created store.                // if the store does exist already, then toggleDbForBulkLoading will make sure that                // the store is already open here.                bulkLoadSegments = new HashSet<>(segments.allSegments());            }            try {                final WriteBatch batch = writeBatchMap.computeIfAbsent(segment, s -> new WriteBatch());                segment.addToBatch(record, batch);            } catch (final RocksDBException e) {                throw new ProcessorStateException("Error restoring batch to store " + this.name, e);            }        }    }    return writeBatchMap;}
f14339
0
toggleForBulkLoading
private void kafkatest_f14340_0(final boolean prepareForBulkload)
{    for (final S segment : segments.allSegments()) {        segment.toggleDbForBulkLoading(prepareForBulkload);    }}
f14340
0
segments
public List<S> kafkatest_f14349_0(final long timeFrom, final long timeTo)
{    final List<S> result = new ArrayList<>();    final NavigableMap<Long, S> segmentsInRange = segments.subMap(segmentId(timeFrom), true, segmentId(timeTo), true);    for (final S segment : segmentsInRange.values()) {        if (segment.isOpen()) {            result.add(segment);        }    }    return result;}
f14349
0
allSegments
public List<S> kafkatest_f14350_0()
{    final List<S> result = new ArrayList<>();    for (final S segment : segments.values()) {        if (segment.isOpen()) {            result.add(segment);        }    }    return result;}
f14350
0
withLoggingDisabled
public StoreBuilder<T> kafkatest_f14359_0()
{    enableLogging = false;    logConfig.clear();    return this;}
f14359
0
logConfig
public Map<String, String> kafkatest_f14360_0()
{    return logConfig;}
f14360
0
priorValue
 byte[] kafkatest_f14369_0()
{    return priorValue;}
f14369
0
oldValue
 byte[] kafkatest_f14370_0()
{    return oldValue;}
f14370
0
hashCode
public int kafkatest_f14379_0()
{    int result = Objects.hash(recordContext);    result = 31 * result + Arrays.hashCode(priorValue);    result = 31 * result + Arrays.hashCode(oldValue);    result = 31 * result + Arrays.hashCode(newValue);    return result;}
f14379
0
toString
public String kafkatest_f14380_0()
{    return "BufferValue{" + "priorValue=" + Arrays.toString(priorValue) + ", oldValue=" + Arrays.toString(oldValue) + ", newValue=" + Arrays.toString(newValue) + ", recordContext=" + recordContext + '}';}
f14380
0
delete
public byte[] kafkatest_f14389_0(final Bytes key)
{    Objects.requireNonNull(key, "key cannot be null");    validateStoreOpen();    lock.writeLock().lock();    try {        return deleteInternal(key);    } finally {        lock.writeLock().unlock();    }}
f14389
0
deleteInternal
private byte[] kafkatest_f14390_0(final Bytes key)
{    final byte[] v = getInternal(key);    putInternal(key, null);    return v;}
f14390
0
initInternal
private void kafkatest_f14399_0(final InternalProcessorContext context)
{    this.context = context;    cacheName = context.taskId() + "-" + name();    cache = context.getCache();    cache.addDirtyEntryFlushListener(cacheName, entries -> {        for (final ThreadCache.DirtyEntry entry : entries) {            putAndMaybeForward(entry, context);        }    });}
f14399
0
putAndMaybeForward
private void kafkatest_f14400_0(final ThreadCache.DirtyEntry entry, final InternalProcessorContext context)
{    final Bytes binaryKey = cacheFunction.key(entry.key());    final Windowed<Bytes> bytesKey = SessionKeySchema.from(binaryKey);    if (flushListener != null) {        final byte[] newValueBytes = entry.newValue();        final byte[] oldValueBytes = newValueBytes == null || sendOldValues ? wrapped().fetchSession(bytesKey.key(), bytesKey.window().start(), bytesKey.window().end()) : null;        // we can skip flushing to downstream as well as writing to underlying store        if (newValueBytes != null || oldValueBytes != null) {            // we need to get the old values if needed, and then put to store, and then flush            wrapped().put(bytesKey, entry.newValue());            final ProcessorRecordContext current = context.recordContext();            context.setRecordContext(entry.entry().context());            try {                flushListener.apply(binaryKey.get(), newValueBytes, sendOldValues ? oldValueBytes : null, entry.entry().context().timestamp());            } finally {                context.setRecordContext(current);            }        }    } else {        wrapped().put(bytesKey, entry.newValue());    }}
f14400
0
flush
public void kafkatest_f14409_0()
{    cache.flush(cacheName);    super.flush();}
f14409
0
close
public void kafkatest_f14410_0()
{    flush();    cache.close(cacheName);    super.close();}
f14410
0
setCacheKeyRange
private void kafkatest_f14419_0(final long lowerRangeEndTime, final long upperRangeEndTime)
{    if (cacheFunction.segmentId(lowerRangeEndTime) != cacheFunction.segmentId(upperRangeEndTime)) {        throw new IllegalStateException("Error iterating over segments: segment interval has changed");    }    if (keyFrom == keyTo) {        cacheKeyFrom = cacheFunction.cacheKey(segmentLowerRangeFixedSize(keyFrom, lowerRangeEndTime));        cacheKeyTo = cacheFunction.cacheKey(segmentUpperRangeFixedSize(keyTo, upperRangeEndTime));    } else {        cacheKeyFrom = cacheFunction.cacheKey(keySchema.lowerRange(keyFrom, lowerRangeEndTime), currentSegmentId);        cacheKeyTo = cacheFunction.cacheKey(keySchema.upperRange(keyTo, latestSessionStartTime), currentSegmentId);    }}
f14419
0
segmentLowerRangeFixedSize
private Bytes kafkatest_f14420_0(final Bytes key, final long segmentBeginTime)
{    final Windowed<Bytes> sessionKey = new Windowed<>(key, new SessionWindow(0, Math.max(0, segmentBeginTime)));    return SessionKeySchema.toBinary(sessionKey);}
f14420
0
fetch
public synchronized WindowStoreIterator<byte[]> kafkatest_f14429_0(final Bytes key, final long timeFrom, final long timeTo)
{    // since this function may not access the underlying inner store, we need to validate    // if store is open outside as well.    validateStoreOpen();    final WindowStoreIterator<byte[]> underlyingIterator = wrapped().fetch(key, timeFrom, timeTo);    if (cache == null) {        return underlyingIterator;    }    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> cacheIterator = wrapped().persistent() ? new CacheIteratorWrapper(key, timeFrom, timeTo) : cache.range(name, cacheFunction.cacheKey(keySchema.lowerRangeFixedSize(key, timeFrom)), cacheFunction.cacheKey(keySchema.upperRangeFixedSize(key, timeTo)));    final HasNextCondition hasNextCondition = keySchema.hasNextCondition(key, key, timeFrom, timeTo);    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> filteredCacheIterator = new FilteredCacheIterator(cacheIterator, hasNextCondition, cacheFunction);    return new MergedSortedCacheWindowStoreIterator(filteredCacheIterator, underlyingIterator);}
f14429
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]>f14430_1final Bytes from, final Bytes to, final long timeFrom, final long timeTo)
{    if (from.compareTo(to) > 0) {                return KeyValueIterators.emptyIterator();    }    // since this function may not access the underlying inner store, we need to validate    // if store is open outside as well.    validateStoreOpen();    final KeyValueIterator<Windowed<Bytes>, byte[]> underlyingIterator = wrapped().fetch(from, to, timeFrom, timeTo);    if (cache == null) {        return underlyingIterator;    }    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> cacheIterator = wrapped().persistent() ? new CacheIteratorWrapper(from, to, timeFrom, timeTo) : cache.range(name, cacheFunction.cacheKey(keySchema.lowerRange(from, timeFrom)), cacheFunction.cacheKey(keySchema.upperRange(to, timeTo)));    final HasNextCondition hasNextCondition = keySchema.hasNextCondition(from, to, timeFrom, timeTo);    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> filteredCacheIterator = new FilteredCacheIterator(cacheIterator, hasNextCondition, cacheFunction);    return new MergedSortedCacheWindowStoreKeyValueIterator(filteredCacheIterator, underlyingIterator, bytesSerdes, windowSize, cacheFunction);}
public KeyValueIterator<Windowed<Bytes>, byte[]>f14430
1
close
public void kafkatest_f14439_0()
{    current.close();}
f14439
0
currentSegmentBeginTime
private long kafkatest_f14440_0()
{    return currentSegmentId * segmentInterval;}
f14440
0
putIfAbsent
public byte[] kafkatest_f14449_0(final Bytes key, final byte[] value)
{    final byte[] previous = wrapped().putIfAbsent(key, value);    if (previous == null) {        // then it was absent        log(key, value);    }    return previous;}
f14449
0
putAll
public void kafkatest_f14450_0(final List<KeyValue<Bytes, byte[]>> entries)
{    wrapped().putAll(entries);    for (final KeyValue<Bytes, byte[]> entry : entries) {        log(entry.key, entry.value);    }}
f14450
0
remove
public void kafkatest_f14459_0(final Windowed<Bytes> sessionKey)
{    wrapped().remove(sessionKey);    changeLogger.logChange(SessionKeySchema.toBinary(sessionKey), null);}
f14459
0
put
public void kafkatest_f14460_0(final Windowed<Bytes> sessionKey, final byte[] aggregate)
{    wrapped().put(sessionKey, aggregate);    changeLogger.logChange(SessionKeySchema.toBinary(sessionKey), aggregate);}
f14460
0
fetch
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14469_0(final Bytes keyFrom, final Bytes keyTo, final long from, final long to)
{    return wrapped().fetch(keyFrom, keyTo, from, to);}
f14469
0
all
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14470_0()
{    return wrapped().all();}
f14470
0
next
public KeyValue<K, V> kafkatest_f14479_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return current.next();}
f14479
0
get
public V kafkatest_f14480_0(final K key)
{    Objects.requireNonNull(key);    final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);    for (final ReadOnlyKeyValueStore<K, V> store : stores) {        try {            final V result = store.get(key);            if (result != null) {                return result;            }        } catch (final InvalidStateStoreException e) {            throw new InvalidStateStoreException("State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.");        }    }    return null;}
f14480
0
fetch
public WindowStoreIterator<V> kafkatest_f14489_0(final K key, final long timeFrom, final long timeTo)
{    Objects.requireNonNull(key, "key can't be null");    final List<ReadOnlyWindowStore<K, V>> stores = provider.stores(storeName, windowStoreType);    for (final ReadOnlyWindowStore<K, V> windowStore : stores) {        try {            final WindowStoreIterator<V> result = windowStore.fetch(key, timeFrom, timeTo);            if (!result.hasNext()) {                result.close();            } else {                return result;            }        } catch (final InvalidStateStoreException e) {            throw new InvalidStateStoreException("State store is not available anymore and may have been migrated to another instance; " + "please re-discover its location from the state metadata.");        }    }    return KeyValueIterators.emptyWindowStoreIterator();}
f14489
0
fetch
public WindowStoreIterator<V> kafkatest_f14490_0(final K key, final Instant from, final Instant to) throws IllegalArgumentException
{    return fetch(key, ApiUtils.validateMillisecondInstant(from, prepareMillisCheckFailMsgPrefix(from, "from")), ApiUtils.validateMillisecondInstant(to, prepareMillisCheckFailMsgPrefix(to, "to")));}
f14490
0
serialize
 ByteBuffer kafkatest_f14499_0(final int endPadding)
{    final byte[] serializedContext = recordContext.serialize();    final int sizeOfContext = serializedContext.length;    final int sizeOfValueLength = Integer.BYTES;    final int sizeOfValue = value == null ? 0 : value.length;    final ByteBuffer buffer = ByteBuffer.allocate(sizeOfContext + sizeOfValueLength + sizeOfValue + endPadding);    buffer.put(serializedContext);    if (value == null) {        buffer.putInt(-1);    } else {        buffer.putInt(value.length);        buffer.put(value);    }    return buffer;}
f14499
0
deserialize
 static ContextualRecord kafkatest_f14500_0(final ByteBuffer buffer)
{    final ProcessorRecordContext context = ProcessorRecordContext.deserialize(buffer);    final int valueLength = buffer.getInt();    if (valueLength == -1) {        return new ContextualRecord(null, context);    } else {        final byte[] value = new byte[valueLength];        buffer.get(value);        return new ContextualRecord(value, context);    }}
f14500
0
peekNext
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f14509_0()
{    return cachedPair(cacheIterator.peekNext());}
f14509
0
close
public void kafkatest_f14510_0()
{    cacheIterator.close();}
f14510
0
peekNext
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f14519_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return cacheIterator.peekNext();}
f14519
0
stores
public List<T> kafkatest_f14520_0(final String storeName, final QueryableStoreType<T> queryableStoreType)
{    final StateStore store = globalStateStores.get(storeName);    if (store == null || !queryableStoreType.accepts(store)) {        return Collections.emptyList();    }    if (!store.isOpen()) {        throw new InvalidStateStoreException("the state store, " + storeName + ", is not open.");    }    if (store instanceof TimestampedKeyValueStore && queryableStoreType instanceof QueryableStoreTypes.KeyValueStoreType) {        return (List<T>) Collections.singletonList(new ReadOnlyKeyValueStoreFacade((TimestampedKeyValueStore<Object, Object>) store));    } else if (store instanceof TimestampedWindowStore && queryableStoreType instanceof QueryableStoreTypes.WindowStoreType) {        return (List<T>) Collections.singletonList(new ReadOnlyWindowStoreFacade((TimestampedWindowStore<Object, Object>) store));    }    return (List<T>) Collections.singletonList(store);}
f14520
0
delete
public synchronized byte[] kafkatest_f14529_0(final Bytes key)
{    final byte[] oldValue = map.remove(key);    size -= oldValue == null ? 0 : 1;    return oldValue;}
f14529
0
range
public synchronized KeyValueIterator<Bytes, byte[]>f14530_1final Bytes from, final Bytes to)
{    if (from.compareTo(to) > 0) {                return KeyValueIterators.emptyIterator();    }    return new DelegatingPeekingKeyValueIterator<>(name, new InMemoryKeyValueIterator(map.subMap(from, true, to, true).keySet()));}
public synchronized KeyValueIterator<Bytes, byte[]>f14530
1
name
public String kafkatest_f14539_0()
{    return name;}
f14539
0
get
public SessionStore<Bytes, byte[]> kafkatest_f14540_0()
{    return new InMemorySessionStore(name, retentionPeriod, metricsScope());}
f14540
0
findSessions
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f14549_0(final Bytes key, final long earliestSessionEndTime, final long latestSessionStartTime)
{    Objects.requireNonNull(key, "key cannot be null");    removeExpiredSegments();    return registerNewIterator(key, key, latestSessionStartTime, endTimeMap.tailMap(earliestSessionEndTime, true).entrySet().iterator());}
f14549
0
findSessions
public KeyValueIterator<Windowed<Bytes>, byte[]>f14550_1final Bytes keyFrom, final Bytes keyTo, final long earliestSessionEndTime, final long latestSessionStartTime)
{    Objects.requireNonNull(keyFrom, "from key cannot be null");    Objects.requireNonNull(keyTo, "to key cannot be null");    removeExpiredSegments();    if (keyFrom.compareTo(keyTo) > 0) {                return KeyValueIterators.emptyIterator();    }    return registerNewIterator(keyFrom, keyTo, latestSessionStartTime, endTimeMap.tailMap(earliestSessionEndTime, true).entrySet().iterator());}
public KeyValueIterator<Windowed<Bytes>, byte[]>f14550
1
hasNext
public boolean kafkatest_f14559_0()
{    if (next != null) {        return true;    } else if (recordIterator == null) {        return false;    } else {        next = getNext();        return next != null;    }}
f14559
0
peekNextKey
public Windowed<Bytes> kafkatest_f14560_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return next.key;}
f14560
0
withCachingDisabled
public StoreBuilder<InMemoryTimeOrderedKeyValueBuffer<K, V>> kafkatest_f14569_0()
{    return this;}
f14569
0
withLoggingEnabled
public StoreBuilder<InMemoryTimeOrderedKeyValueBuffer<K, V>> kafkatest_f14570_0(final Map<String, String> config)
{    throw new UnsupportedOperationException();}
f14570
0
init
public void kafkatest_f14579_0(final ProcessorContext context, final StateStore root)
{    final InternalProcessorContext internalProcessorContext = (InternalProcessorContext) context;    bufferSizeSensor = Sensors.createBufferSizeSensor(this, internalProcessorContext);    bufferCountSensor = Sensors.createBufferCountSensor(this, internalProcessorContext);    context.register(root, (RecordBatchingStateRestoreCallback) this::restoreBatch);    if (loggingEnabled) {        collector = ((RecordCollector.Supplier) context).recordCollector();        changelogTopic = ProcessorStateManager.storeChangelogTopic(context.applicationId(), storeName);    }    updateBufferMetrics();    open = true;    partition = context.taskId().partition;}
f14579
0
isOpen
public boolean kafkatest_f14580_0()
{    return open;}
f14580
0
put
public void kafkatest_f14589_0(final long time, final K key, final Change<V> value, final ProcessorRecordContext recordContext)
{    requireNonNull(value, "value cannot be null");    requireNonNull(recordContext, "recordContext cannot be null");    final Bytes serializedKey = Bytes.wrap(keySerde.serializer().serialize(changelogTopic, key));    final Change<byte[]> serialChange = valueSerde.serializeParts(changelogTopic, value);    final BufferValue buffered = getBuffered(serializedKey);    final byte[] serializedPriorValue;    if (buffered == null) {        final V priorValue = value.oldValue;        serializedPriorValue = (priorValue == null) ? null : valueSerde.innerSerde().serializer().serialize(changelogTopic, priorValue);    } else {        serializedPriorValue = buffered.priorValue();    }    cleanPut(time, serializedKey, new BufferValue(serializedPriorValue, serialChange.oldValue, serialChange.newValue, recordContext));    dirtyKeys.add(serializedKey);    updateBufferMetrics();}
f14589
0
getBuffered
private BufferValue kafkatest_f14590_0(final Bytes key)
{    final BufferKey bufferKey = index.get(key);    return bufferKey == null ? null : sortedMap.get(bufferKey);}
f14590
0
get
public WindowStore<Bytes, byte[]> kafkatest_f14599_0()
{    return new InMemoryWindowStore(name, retentionPeriod, windowSize, retainDuplicates, metricsScope());}
f14599
0
metricsScope
public String kafkatest_f14600_0()
{    return "in-memory-window";}
f14600
0
put
public voidf14609_1final Bytes key, final byte[] value, final long windowStartTimestamp)
{    removeExpiredSegments();    maybeUpdateSeqnumForDups();    observedStreamTime = Math.max(observedStreamTime, windowStartTimestamp);    final Bytes keyBytes = retainDuplicates ? wrapForDups(key, seqnum) : key;    if (windowStartTimestamp <= observedStreamTime - retentionPeriod) {        expiredRecordSensor.record();            } else {        if (value != null) {            segmentMap.computeIfAbsent(windowStartTimestamp, t -> new ConcurrentSkipListMap<>());            segmentMap.get(windowStartTimestamp).put(keyBytes, value);        } else {            segmentMap.computeIfPresent(windowStartTimestamp, (t, kvMap) -> {                kvMap.remove(keyBytes);                if (kvMap.isEmpty()) {                    segmentMap.remove(windowStartTimestamp);                }                return kvMap;            });        }    }}
public voidf14609
1
fetch
public byte[] kafkatest_f14610_0(final Bytes key, final long windowStartTimestamp)
{    Objects.requireNonNull(key, "key cannot be null");    removeExpiredSegments();    if (windowStartTimestamp <= observedStreamTime - retentionPeriod) {        return null;    }    final ConcurrentNavigableMap<Bytes, byte[]> kvMap = segmentMap.get(windowStartTimestamp);    if (kvMap == null) {        return null;    } else {        return kvMap.get(key);    }}
f14610
0
removeExpiredSegments
private void kafkatest_f14619_0()
{    long minLiveTime = Math.max(0L, observedStreamTime - retentionPeriod + 1);    for (final InMemoryWindowStoreIteratorWrapper it : openIterators) {        minLiveTime = Math.min(minLiveTime, it.minTime());    }    segmentMap.headMap(minLiveTime, false).clear();}
f14619
0
maybeUpdateSeqnumForDups
private void kafkatest_f14620_0()
{    if (retainDuplicates) {        seqnum = (seqnum + 1) & 0x7FFFFFFF;    }}
f14620
0
minTime
 Long kafkatest_f14629_0()
{    return currentTime;}
f14629
0
peekNextKey
public Long kafkatest_f14630_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return super.currentTime;}
f14630
0
close
public void kafkatest_f14639_0()
{    innerIterator.close();}
f14639
0
peekNextKey
public K kafkatest_f14641_0()
{    throw new NoSuchElementException();}
f14641
0
equals
public boolean kafkatest_f14650_0(final Object obj)
{    if (obj == null || getClass() != obj.getClass()) {        return false;    }    final KeyValueSegment segment = (KeyValueSegment) obj;    return id == segment.id;}
f14650
0
hashCode
public int kafkatest_f14651_0()
{    return Objects.hash(id);}
f14651
0
delete
public byte[] kafkatest_f14660_0(final Bytes key)
{    return convertToTimestampedFormat(store.delete(key));}
f14660
0
name
public String kafkatest_f14661_0()
{    return store.name();}
f14661
0
approximateNumEntries
public long kafkatest_f14670_0()
{    return store.approximateNumEntries();}
f14670
0
close
public void kafkatest_f14671_0()
{    innerIterator.close();}
f14671
0
equals
public boolean kafkatest_f14680_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final LRUCacheEntry that = (LRUCacheEntry) o;    return sizeBytes == that.sizeBytes && isDirty() == that.isDirty() && Objects.equals(record, that.record);}
f14680
0
hashCode
public int kafkatest_f14681_0()
{    return Objects.hash(record, sizeBytes, isDirty());}
f14681
0
setWhenEldestRemoved
 void kafkatest_f14690_0(final EldestEntryRemovalListener listener)
{    this.listener = listener;}
f14690
0
name
public String kafkatest_f14691_0()
{    return this.name;}
f14691
0
range
public KeyValueIterator<Bytes, byte[]> kafkatest_f14700_0(final Bytes from, final Bytes to)
{    throw new UnsupportedOperationException("MemoryLRUCache does not support range() function.");}
f14700
0
all
public KeyValueIterator<Bytes, byte[]> kafkatest_f14701_0()
{    throw new UnsupportedOperationException("MemoryLRUCache does not support all() function.");}
f14701
0
next
public KeyValue<Bytes, byte[]> kafkatest_f14710_0()
{    lastKey = keys.next();    return new KeyValue<>(lastKey, entries.get(lastKey));}
f14710
0
close
public void kafkatest_f14711_0()
{// do nothing}
f14711
0
deserializeCacheValue
 byte[] kafkatest_f14720_0(final LRUCacheEntry cacheEntry)
{    return cacheEntry.value();}
f14720
0
deserializeStoreKey
public Windowed<Bytes> kafkatest_f14721_0(final Windowed<Bytes> key)
{    return key;}
f14721
0
deserializeCacheKey
 Windowed<Bytes> kafkatest_f14730_0(final Bytes cacheKey)
{    final byte[] binaryKey = cacheFunction.key(cacheKey).get();    return WindowKeySchema.fromStoreKey(binaryKey, windowSize, serdes.keyDeserializer(), serdes.topic());}
f14730
0
deserializeCacheValue
 byte[] kafkatest_f14731_0(final LRUCacheEntry cacheEntry)
{    return cacheEntry.value();}
f14731
0
delete
public V kafkatest_f14740_0(final K key)
{    try {        if (deleteTime.shouldRecord()) {            return measureLatency(() -> outerValue(wrapped().delete(keyBytes(key))), deleteTime);        } else {            return outerValue(wrapped().delete(keyBytes(key)));        }    } catch (final ProcessorStateException e) {        final String message = String.format(e.getMessage(), key);        throw new ProcessorStateException(message, e);    }}
f14740
0
range
public KeyValueIterator<K, V> kafkatest_f14741_0(final K from, final K to)
{    return new MeteredKeyValueIterator(wrapped().range(Bytes.wrap(serdes.rawKey(from)), Bytes.wrap(serdes.rawKey(to))), rangeTime);}
f14741
0
hasNext
public boolean kafkatest_f14750_0()
{    return iter.hasNext();}
f14750
0
next
public KeyValue<K, V> kafkatest_f14751_0()
{    final KeyValue<Bytes, byte[]> keyValue = iter.next();    return KeyValue.pair(serdes.keyFrom(keyValue.key.get()), outerValue(keyValue.value));}
f14751
0
fetch
public KeyValueIterator<Windowed<K>, V> kafkatest_f14760_0(final K from, final K to)
{    Objects.requireNonNull(from, "from cannot be null");    Objects.requireNonNull(to, "to cannot be null");    return new MeteredWindowedKeyValueIterator<>(wrapped().fetch(keyBytes(from), keyBytes(to)), fetchTime, metrics, serdes, time);}
f14760
0
findSessions
public KeyValueIterator<Windowed<K>, V> kafkatest_f14761_0(final K key, final long earliestSessionEndTime, final long latestSessionStartTime)
{    Objects.requireNonNull(key, "key cannot be null");    final Bytes bytesKey = keyBytes(key);    return new MeteredWindowedKeyValueIterator<>(wrapped().findSessions(bytesKey, earliestSessionEndTime, latestSessionStartTime), fetchTime, metrics, serdes, time);}
f14761
0
windowedKey
private Windowed<K> kafkatest_f14770_0(final Windowed<Bytes> bytesKey)
{    final K key = serdes.keyFrom(bytesKey.key().get());    return new Windowed<>(key, bytesKey.window());}
f14770
0
close
public void kafkatest_f14771_0()
{    try {        iter.close();    } finally {        metrics.recordLatency(sensor, startNs, time.nanoseconds());    }}
f14771
0
fetch
public KeyValueIterator<Windowed<K>, V> kafkatest_f14780_0(final K from, final K to, final long timeFrom, final long timeTo)
{    return new MeteredWindowedKeyValueIterator<>(wrapped().fetch(keyBytes(from), keyBytes(to), timeFrom, timeTo), fetchTime, metrics, serdes, time);}
f14780
0
fetchAll
public KeyValueIterator<Windowed<K>, V> kafkatest_f14781_0(final long timeFrom, final long timeTo)
{    return new MeteredWindowedKeyValueIterator<>(wrapped().fetchAll(timeFrom, timeTo), fetchTime, metrics, serdes, time);}
f14781
0
taskName
public String kafkatest_f14790_0()
{    return taskName;}
f14790
0
metricsScope
public String kafkatest_f14791_0()
{    return metricsScope;}
f14791
0
memtableMinFlushTimeSensor
public static Sensor kafkatest_f14800_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, MEMTABLE_FLUSH_TIME_MIN);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), MEMTABLE_FLUSH_TIME_MIN, MEMTABLE_FLUSH_TIME_MIN_DESCRIPTION);    return sensor;}
f14800
0
memtableMaxFlushTimeSensor
public static Sensor kafkatest_f14801_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, MEMTABLE_FLUSH_TIME_MAX);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), MEMTABLE_FLUSH_TIME_MAX, MEMTABLE_FLUSH_TIME_MAX_DESCRIPTION);    return sensor;}
f14801
0
compactionTimeMaxSensor
public static Sensor kafkatest_f14810_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, COMPACTION_TIME_MAX);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), COMPACTION_TIME_MAX, COMPACTION_TIME_MAX_DESCRIPTION);    return sensor;}
f14810
0
numberOfOpenFilesSensor
public static Sensor kafkatest_f14811_0(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext)
{    final Sensor sensor = createSensor(streamsMetrics, metricContext, NUMBER_OF_OPEN_FILES);    addValueMetricToSensor(sensor, STATE_LEVEL_GROUP, streamsMetrics.storeLevelTagMap(metricContext.taskName(), metricContext.metricsScope(), metricContext.storeName()), NUMBER_OF_OPEN_FILES, NUMBER_OF_OPEN_FILES_DESCRIPTION);    return sensor;}
f14811
0
createBufferSizeSensor
public static Sensor kafkatest_f14820_0(final StateStore store, final InternalProcessorContext context)
{    return getBufferSizeOrCountSensor(store, context, "size");}
f14820
0
createBufferCountSensor
public static Sensor kafkatest_f14821_0(final StateStore store, final InternalProcessorContext context)
{    return getBufferSizeOrCountSensor(store, context, "count");}
f14821
0
flush
 synchronized void kafkatest_f14830_0()
{    flush(null);}
f14830
0
flush
private void kafkatest_f14831_0(final LRUNode evicted)
{    numFlushes++;    if (log.isTraceEnabled()) {        log.trace("Named cache {} stats on flush: #hits={}, #misses={}, #overwrites={}, #flushes={}", name, hits(), misses(), overwrites(), flushes());    }    if (listener == null) {        throw new IllegalArgumentException("No listener for namespace " + name + " registered with cache");    }    if (dirtyKeys.isEmpty()) {        return;    }    final List<ThreadCache.DirtyEntry> entries = new ArrayList<>();    final List<Bytes> deleted = new ArrayList<>();    // flushed entries and remove from dirtyKeys.    if (evicted != null) {        entries.add(new ThreadCache.DirtyEntry(evicted.key, evicted.entry.value(), evicted.entry));        dirtyKeys.remove(evicted.key);    }    for (final Bytes key : dirtyKeys) {        final LRUNode node = getInternal(key);        if (node == null) {            throw new IllegalStateException("Key = " + key + " found in dirty key set, but entry is null");        }        entries.add(new ThreadCache.DirtyEntry(key, node.entry.value(), node.entry));        node.entry.markClean();        if (node.entry.value() == null) {            deleted.add(node.key);        }    }    // clear dirtyKeys before the listener is applied as it may be re-entrant.    dirtyKeys.clear();    listener.apply(entries);    for (final Bytes key : deleted) {        delete(key);    }}
f14831
0
putAll
 synchronized void kafkatest_f14840_0(final List<KeyValue<byte[], LRUCacheEntry>> entries)
{    for (final KeyValue<byte[], LRUCacheEntry> entry : entries) {        put(Bytes.wrap(entry.key), entry.value);    }}
f14840
0
delete
 synchronized LRUCacheEntry kafkatest_f14841_0(final Bytes key)
{    final LRUNode node = cache.remove(key);    if (node == null) {        return null;    }    remove(node);    dirtyKeys.remove(key);    currentSizeBytes -= node.size();    return node.entry();}
f14841
0
tail
 synchronized LRUNode kafkatest_f14850_0()
{    return tail;}
f14850
0
close
 synchronized void kafkatest_f14851_0()
{    head = tail = null;    listener = null;    currentSizeBytes = 0;    dirtyKeys.clear();    cache.clear();    namedCacheMetrics.removeAllSensors();}
f14851
0
writeIntLine
private void kafkatest_f14860_0(final BufferedWriter writer, final int number) throws IOException
{    writer.write(Integer.toString(number));    writer.newLine();}
f14860
0
writeEntry
private void kafkatest_f14861_0(final BufferedWriter writer, final TopicPartition part, final long offset) throws IOException
{    writer.write(part.topic());    writer.write(' ');    writer.write(Integer.toString(part.partition()));    writer.write(' ');    writer.write(Long.toString(offset));    writer.newLine();}
f14861
0
range
public KeyValueIterator<K, V> kafkatest_f14870_0(final K from, final K to)
{    return new KeyValueIteratorFacade<>(inner.range(from, to));}
f14870
0
all
public KeyValueIterator<K, V> kafkatest_f14871_0()
{    return new KeyValueIteratorFacade<>(inner.all());}
f14871
0
all
public KeyValueIterator<Windowed<K>, V> kafkatest_f14880_0()
{    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<V>> innerIterator = inner.all();    return new KeyValueIteratorFacade<>(innerIterator);}
f14880
0
close
public void kafkatest_f14881_0()
{    innerIterator.close();}
f14881
0
setEnv
public Options kafkatest_f14890_0(final Env env)
{    dbOptions.setEnv(env);    return this;}
f14890
0
getEnv
public Env kafkatest_f14891_0()
{    return dbOptions.getEnv();}
f14891
0
optimizeUniversalStyleCompaction
public Options kafkatest_f14900_0(final long memtableMemoryBudget)
{    columnFamilyOptions.optimizeUniversalStyleCompaction(memtableMemoryBudget);    return this;}
f14900
0
setComparator
public Options kafkatest_f14901_0(final BuiltinComparator builtinComparator)
{    columnFamilyOptions.setComparator(builtinComparator);    return this;}
f14901
0
setErrorIfExists
public Options kafkatest_f14910_0(final boolean errorIfExists)
{    dbOptions.setErrorIfExists(errorIfExists);    return this;}
f14910
0
paranoidChecks
public boolean kafkatest_f14911_0()
{    final boolean columnFamilyParanoidFileChecks = columnFamilyOptions.paranoidFileChecks();    final boolean dbOptionsParanoidChecks = dbOptions.paranoidChecks();    if (columnFamilyParanoidFileChecks != dbOptionsParanoidChecks) {        throw new IllegalStateException("Config for paranoid checks for RockDB and ColumnFamilies should be the same.");    }    return dbOptionsParanoidChecks;}
f14911
0
setUseFsync
public Options kafkatest_f14920_0(final boolean useFsync)
{    dbOptions.setUseFsync(useFsync);    return this;}
f14920
0
setDbPaths
public Options kafkatest_f14921_0(final Collection<DbPath> dbPaths)
{    dbOptions.setDbPaths(dbPaths);    return this;}
f14921
0
setStatistics
public Options kafkatest_f14930_0(final Statistics statistics)
{    dbOptions.setStatistics(statistics);    return this;}
f14930
0
statistics
public Statistics kafkatest_f14931_0()
{    return dbOptions.statistics();}
f14931
0
setMaxBackgroundJobs
public Options kafkatest_f14940_0(final int maxBackgroundJobs)
{    dbOptions.setMaxBackgroundJobs(maxBackgroundJobs);    return this;}
f14940
0
maxLogFileSize
public long kafkatest_f14941_0()
{    return dbOptions.maxLogFileSize();}
f14941
0
setMaxManifestFileSize
public Options kafkatest_f14950_0(final long maxManifestFileSize)
{    dbOptions.setMaxManifestFileSize(maxManifestFileSize);    return this;}
f14950
0
setMaxTableFilesSizeFIFO
public Options kafkatest_f14951_0(final long maxTableFilesSize)
{    columnFamilyOptions.setMaxTableFilesSizeFIFO(maxTableFilesSize);    return this;}
f14951
0
setManifestPreallocationSize
public Options kafkatest_f14960_0(final long size)
{    dbOptions.setManifestPreallocationSize(size);    return this;}
f14960
0
setUseDirectReads
public Options kafkatest_f14961_0(final boolean useDirectReads)
{    dbOptions.setUseDirectReads(useDirectReads);    return this;}
f14961
0
setAllowMmapWrites
public Options kafkatest_f14970_0(final boolean allowMmapWrites)
{    dbOptions.setAllowMmapWrites(allowMmapWrites);    return this;}
f14970
0
isFdCloseOnExec
public boolean kafkatest_f14971_0()
{    return dbOptions.isFdCloseOnExec();}
f14971
0
accessHintOnCompactionStart
public AccessHint kafkatest_f14980_0()
{    return dbOptions.accessHintOnCompactionStart();}
f14980
0
setNewTableReaderForCompactionInputs
public Options kafkatest_f14981_0(final boolean newTableReaderForCompactionInputs)
{    dbOptions.setNewTableReaderForCompactionInputs(newTableReaderForCompactionInputs);    return this;}
f14981
0
setUseAdaptiveMutex
public Options kafkatest_f14990_0(final boolean useAdaptiveMutex)
{    dbOptions.setUseAdaptiveMutex(useAdaptiveMutex);    return this;}
f14990
0
bytesPerSync
public long kafkatest_f14991_0()
{    return dbOptions.bytesPerSync();}
f14991
0
allowConcurrentMemtableWrite
public boolean kafkatest_f15000_0()
{    return dbOptions.allowConcurrentMemtableWrite();}
f15000
0
setEnableWriteThreadAdaptiveYield
public Options kafkatest_f15001_0(final boolean enableWriteThreadAdaptiveYield)
{    dbOptions.setEnableWriteThreadAdaptiveYield(enableWriteThreadAdaptiveYield);    return this;}
f15001
0
walRecoveryMode
public WALRecoveryMode kafkatest_f15010_0()
{    return dbOptions.walRecoveryMode();}
f15010
0
setAllow2pc
public Options kafkatest_f15011_0(final boolean allow2pc)
{    dbOptions.setAllow2pc(allow2pc);    return this;}
f15011
0
avoidFlushDuringRecovery
public boolean kafkatest_f15020_0()
{    return dbOptions.avoidFlushDuringRecovery();}
f15020
0
setAvoidFlushDuringShutdown
public Options kafkatest_f15021_0(final boolean avoidFlushDuringShutdown)
{    dbOptions.setAvoidFlushDuringShutdown(avoidFlushDuringShutdown);    return this;}
f15021
0
memTableFactoryName
public String kafkatest_f15030_0()
{    return columnFamilyOptions.memTableFactoryName();}
f15030
0
tableFormatConfig
public TableFormatConfig kafkatest_f15031_0()
{    return columnFamilyOptions.tableFormatConfig();}
f15031
0
setBottommostCompressionType
public Options kafkatest_f15040_0(final CompressionType bottommostCompressionType)
{    columnFamilyOptions.setBottommostCompressionType(bottommostCompressionType);    return this;}
f15040
0
bottommostCompressionType
public CompressionType kafkatest_f15041_0()
{    return columnFamilyOptions.bottommostCompressionType();}
f15041
0
levelZeroSlowdownWritesTrigger
public int kafkatest_f15050_0()
{    return columnFamilyOptions.levelZeroSlowdownWritesTrigger();}
f15050
0
setLevelZeroSlowdownWritesTrigger
public Options kafkatest_f15051_0(final int numFiles)
{    columnFamilyOptions.setLevelZeroSlowdownWritesTrigger(numFiles);    return this;}
f15051
0
setLevelCompactionDynamicLevelBytes
public Options kafkatest_f15060_0(final boolean enableLevelCompactionDynamicLevelBytes)
{    columnFamilyOptions.setLevelCompactionDynamicLevelBytes(enableLevelCompactionDynamicLevelBytes);    return this;}
f15060
0
levelCompactionDynamicLevelBytes
public boolean kafkatest_f15061_0()
{    return columnFamilyOptions.levelCompactionDynamicLevelBytes();}
f15061
0
maxSequentialSkipInIterations
public long kafkatest_f15070_0()
{    return columnFamilyOptions.maxSequentialSkipInIterations();}
f15070
0
setMaxSequentialSkipInIterations
public Options kafkatest_f15071_0(final long maxSequentialSkipInIterations)
{    columnFamilyOptions.setMaxSequentialSkipInIterations(maxSequentialSkipInIterations);    return this;}
f15071
0
maxSuccessiveMerges
public long kafkatest_f15080_0()
{    return columnFamilyOptions.maxSuccessiveMerges();}
f15080
0
setMaxSuccessiveMerges
public Options kafkatest_f15081_0(final long maxSuccessiveMerges)
{    columnFamilyOptions.setMaxSuccessiveMerges(maxSuccessiveMerges);    return this;}
f15081
0
setHardPendingCompactionBytesLimit
public Options kafkatest_f15090_0(final long hardPendingCompactionBytesLimit)
{    columnFamilyOptions.setHardPendingCompactionBytesLimit(hardPendingCompactionBytesLimit);    return this;}
f15090
0
hardPendingCompactionBytesLimit
public long kafkatest_f15091_0()
{    return columnFamilyOptions.hardPendingCompactionBytesLimit();}
f15091
0
setParanoidFileChecks
public Options kafkatest_f15100_0(final boolean paranoidFileChecks)
{    columnFamilyOptions.setParanoidFileChecks(paranoidFileChecks);    return this;}
f15100
0
paranoidFileChecks
public boolean kafkatest_f15101_0()
{    return columnFamilyOptions.paranoidFileChecks();}
f15101
0
setCompactionOptionsFIFO
public Options kafkatest_f15110_0(final CompactionOptionsFIFO compactionOptionsFIFO)
{    columnFamilyOptions.setCompactionOptionsFIFO(compactionOptionsFIFO);    return this;}
f15110
0
compactionOptionsFIFO
public CompactionOptionsFIFO kafkatest_f15111_0()
{    return columnFamilyOptions.compactionOptionsFIFO();}
f15111
0
makeNext
public KeyValue<Bytes, byte[]> kafkatest_f15120_0()
{    if (!iter.isValid()) {        return allDone();    } else {        next = getKeyValue();        iter.next();        return next;    }}
f15120
0
getKeyValue
private KeyValue<Bytes, byte[]> kafkatest_f15121_0()
{    return new KeyValue<>(new Bytes(iter.key()), iter.value());}
f15121
0
metricsScope
public String kafkatest_f15130_0()
{    return "rocksdb-session";}
f15130
0
segmentIntervalMs
public long kafkatest_f15131_0()
{    // Selected somewhat arbitrarily. Profiling may reveal a different value is preferable.    return Math.max(retentionPeriod / 2, 60_000L);}
f15131
0
openDB
 void kafkatest_f15140_0(final ProcessorContext context)
{    // initialize the default rocksdb options    final DBOptions dbOptions = new DBOptions();    final ColumnFamilyOptions columnFamilyOptions = new ColumnFamilyOptions();    userSpecifiedOptions = new RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter(dbOptions, columnFamilyOptions);    final BlockBasedTableConfig tableConfig = new BlockBasedTableConfig();    cache = new LRUCache(BLOCK_CACHE_SIZE);    tableConfig.setBlockCache(cache);    tableConfig.setBlockSize(BLOCK_SIZE);    filter = new BloomFilter();    tableConfig.setFilter(filter);    userSpecifiedOptions.optimizeFiltersForHits();    userSpecifiedOptions.setTableFormatConfig(tableConfig);    userSpecifiedOptions.setWriteBufferSize(WRITE_BUFFER_SIZE);    userSpecifiedOptions.setCompressionType(COMPRESSION_TYPE);    userSpecifiedOptions.setCompactionStyle(COMPACTION_STYLE);    userSpecifiedOptions.setMaxWriteBufferNumber(MAX_WRITE_BUFFERS);    userSpecifiedOptions.setCreateIfMissing(true);    userSpecifiedOptions.setErrorIfExists(false);    userSpecifiedOptions.setInfoLogLevel(InfoLogLevel.ERROR_LEVEL);    // this is the recommended way to increase parallelism in RocksDb    // note that the current implementation of setIncreaseParallelism affects the number    // of compaction threads but not flush threads (the latter remains one). Also    // the parallelism value needs to be at least two because of the code in    // https://github.com/facebook/rocksdb/blob/62ad0a9b19f0be4cefa70b6b32876e764b7f3c11/util/options.cc#L580    // subtracts one from the value passed to determine the number of compaction threads    // (this could be a bug in the RocksDB code and their devs have been contacted).    userSpecifiedOptions.setIncreaseParallelism(Math.max(Runtime.getRuntime().availableProcessors(), 2));    wOptions = new WriteOptions();    wOptions.setDisableWAL(true);    fOptions = new FlushOptions();    fOptions.setWaitForFlush(true);    final Map<String, Object> configs = context.appConfigs();    final Class<RocksDBConfigSetter> configSetterClass = (Class<RocksDBConfigSetter>) configs.get(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG);    if (configSetterClass != null) {        configSetter = Utils.newInstance(configSetterClass);        configSetter.setConfig(name, userSpecifiedOptions, configs);    }    if (prepareForBulkload) {        userSpecifiedOptions.prepareForBulkLoad();    }    dbDir = new File(new File(context.stateDir(), parentDir), name);    try {        Files.createDirectories(dbDir.getParentFile().toPath());        Files.createDirectories(dbDir.getAbsoluteFile().toPath());    } catch (final IOException fatal) {        throw new ProcessorStateException(fatal);    }    setUpMetrics(context, configs);    openRocksDB(dbOptions, columnFamilyOptions);    open = true;}
f15140
0
setUpMetrics
private void kafkatest_f15141_0(final ProcessorContext context, final Map<String, Object> configs)
{    if (userSpecifiedOptions.statistics() == null && RecordingLevel.forName((String) configs.get(METRICS_RECORDING_LEVEL_CONFIG)) == RecordingLevel.DEBUG) {        // metrics recorder will clean up statistics object        final Statistics statistics = new Statistics();        userSpecifiedOptions.setStatistics(statistics);        metricsRecorder.addStatistics(name, statistics, (StreamsMetricsImpl) context.metrics(), context.taskId());        removeStatisticsFromMetricsRecorder = true;    }}
f15141
0
putIfAbsent
public synchronized byte[] kafkatest_f15150_0(final Bytes key, final byte[] value)
{    Objects.requireNonNull(key, "key cannot be null");    final byte[] originalValue = get(key);    if (originalValue == null) {        put(key, value);    }    return originalValue;}
f15150
0
putAll
public void kafkatest_f15151_0(final List<KeyValue<Bytes, byte[]>> entries)
{    try (final WriteBatch batch = new WriteBatch()) {        dbAccessor.prepareBatch(entries, batch);        write(batch);    } catch (final RocksDBException e) {        throw new ProcessorStateException("Error while batch writing to store " + name, e);    }}
f15151
0
addToBatch
public void kafkatest_f15160_0(final KeyValue<byte[], byte[]> record, final WriteBatch batch) throws RocksDBException
{    dbAccessor.addToBatch(record.key, record.value, batch);}
f15160
0
write
public void kafkatest_f15161_0(final WriteBatch batch) throws RocksDBException
{    db.write(wOptions, batch);}
f15161
0
all
public KeyValueIterator<Bytes, byte[]> kafkatest_f15170_0()
{    final RocksIterator innerIterWithTimestamp = db.newIterator(columnFamily);    innerIterWithTimestamp.seekToFirst();    return new RocksDbIterator(name, innerIterWithTimestamp, openIterators);}
f15170
0
approximateNumEntries
public long kafkatest_f15171_0() throws RocksDBException
{    return db.getLongProperty(columnFamily, "rocksdb.estimate-num-keys");}
f15171
0
getOptions
public Options kafkatest_f15180_0()
{    return userSpecifiedOptions;}
f15180
0
openRocksDB
 void kafkatest_f15181_0(final DBOptions dbOptions, final ColumnFamilyOptions columnFamilyOptions)
{    final List<ColumnFamilyDescriptor> columnFamilyDescriptors = asList(new ColumnFamilyDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY, columnFamilyOptions), new ColumnFamilyDescriptor("keyValueWithTimestamp".getBytes(StandardCharsets.UTF_8), columnFamilyOptions));    final List<ColumnFamilyHandle> columnFamilies = new ArrayList<>(columnFamilyDescriptors.size());    try {        db = RocksDB.open(dbOptions, dbDir.getAbsolutePath(), columnFamilyDescriptors, columnFamilies);        setDbAccessor(columnFamilies.get(0), columnFamilies.get(1));    } catch (final RocksDBException e) {        if ("Column family not found: : keyValueWithTimestamp".equals(e.getMessage())) {            try {                db = RocksDB.open(dbOptions, dbDir.getAbsolutePath(), columnFamilyDescriptors.subList(0, 1), columnFamilies);                columnFamilies.add(db.createColumnFamily(columnFamilyDescriptors.get(1)));            } catch (final RocksDBException fatal) {                throw new ProcessorStateException("Error opening store " + name + " at location " + dbDir.toString(), fatal);            }            setDbAccessor(columnFamilies.get(0), columnFamilies.get(1));        } else {            throw new ProcessorStateException("Error opening store " + name + " at location " + dbDir.toString(), e);        }    }}
f15181
0
flush
public void kafkatest_f15190_0() throws RocksDBException
{    db.flush(fOptions, oldColumnFamily);    db.flush(fOptions, newColumnFamily);}
f15190
0
prepareBatchForRestore
public void kafkatest_f15191_0(final Collection<KeyValue<byte[], byte[]>> records, final WriteBatch batch) throws RocksDBException
{    for (final KeyValue<byte[], byte[]> record : records) {        addToBatch(record.key, record.value, batch);    }}
f15191
0
makeNext
public KeyValue<Bytes, byte[]> kafkatest_f15200_0()
{    final KeyValue<Bytes, byte[]> next = super.makeNext();    if (next == null) {        return allDone();    } else {        if (comparator.compare(next.key.get(), upperBoundKey) <= 0) {            return next;        } else {            return allDone();        }    }}
f15200
0
name
public String kafkatest_f15201_0()
{    return name;}
f15201
0
put
public void kafkatest_f15210_0(final Bytes key, final byte[] value)
{    put(key, value, context.timestamp());}
f15210
0
put
public void kafkatest_f15211_0(final Bytes key, final byte[] value, final long windowStartTimestamp)
{    maybeUpdateSeqnumForDups();    wrapped().put(WindowKeySchema.toStoreKeyBinary(key, windowStartTimestamp, seqnum), value);}
f15211
0
cacheKey
 Bytes kafkatest_f15220_0(final Bytes key, final long segmentId)
{    final byte[] keyBytes = key.get();    final ByteBuffer buf = ByteBuffer.allocate(SEGMENT_ID_BYTES + keyBytes.length);    buf.putLong(segmentId).put(keyBytes);    return Bytes.wrap(buf.array());}
f15220
0
bytesFromCacheKey
 static byte[] kafkatest_f15221_0(final Bytes cacheKey)
{    final byte[] binaryKey = new byte[cacheKey.get().length - SEGMENT_ID_BYTES];    System.arraycopy(cacheKey.get(), SEGMENT_ID_BYTES, binaryKey, 0, binaryKey.length);    return binaryKey;}
f15221
0
next
public KeyValue<Bytes, byte[]> kafkatest_f15230_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return currentIterator.next();}
f15230
0
upperRangeFixedSize
public Bytes kafkatest_f15231_0(final Bytes key, final long to)
{    final Windowed<Bytes> sessionKey = new Windowed<>(key, new SessionWindow(to, Long.MAX_VALUE));    return SessionKeySchema.toBinary(sessionKey);}
f15231
0
extractEndTimestamp
 static long kafkatest_f15240_0(final byte[] binaryKey)
{    return ByteBuffer.wrap(binaryKey).getLong(binaryKey.length - 2 * TIMESTAMP_SIZE);}
f15240
0
extractStartTimestamp
 static long kafkatest_f15241_0(final byte[] binaryKey)
{    return ByteBuffer.wrap(binaryKey).getLong(binaryKey.length - TIMESTAMP_SIZE);}
f15241
0
maybeWrapCaching
private SessionStore<Bytes, byte[]> kafkatest_f15250_0(final SessionStore<Bytes, byte[]> inner)
{    if (!enableCaching) {        return inner;    }    return new CachingSessionStore(inner, storeSupplier.segmentIntervalMs());}
f15250
0
maybeWrapLogging
private SessionStore<Bytes, byte[]> kafkatest_f15251_0(final SessionStore<Bytes, byte[]> inner)
{    if (!enableLogging) {        return inner;    }    return new ChangeLoggingSessionBytesStore(inner);}
f15251
0
nameSpaceFromTaskIdAndStore
public static String kafkatest_f15260_0(final String taskIDString, final String underlyingStoreName)
{    return taskIDString + "-" + underlyingStoreName;}
f15260
0
taskIDfromCacheName
public static String kafkatest_f15261_0(final String cacheName)
{    final String[] tokens = cacheName.split("-", 2);    return tokens[0];}
f15261
0
range
public MemoryLRUCacheBytesIterator kafkatest_f15270_0(final String namespace, final Bytes from, final Bytes to)
{    final NamedCache cache = getCache(namespace);    if (cache == null) {        return new MemoryLRUCacheBytesIterator(Collections.<Bytes>emptyIterator(), new NamedCache(namespace, this.metrics));    }    return new MemoryLRUCacheBytesIterator(cache.keyRange(from, to), cache);}
f15270
0
all
public MemoryLRUCacheBytesIterator kafkatest_f15271_0(final String namespace)
{    final NamedCache cache = getCache(namespace);    if (cache == null) {        return new MemoryLRUCacheBytesIterator(Collections.<Bytes>emptyIterator(), new NamedCache(namespace, this.metrics));    }    return new MemoryLRUCacheBytesIterator(cache.allKeys(), cache);}
f15271
0
peekNext
public KeyValue<Bytes, LRUCacheEntry> kafkatest_f15280_0()
{    if (!hasNext()) {        throw new NoSuchElementException();    }    return nextEntry;}
f15280
0
hasNext
public boolean kafkatest_f15281_0()
{    if (nextEntry != null) {        return true;    }    while (keys.hasNext() && nextEntry == null) {        internalNext();    }    return nextEntry != null;}
f15281
0
recordContext
public ProcessorRecordContext kafkatest_f15290_0()
{    return recordContext;}
f15290
0
toString
public String kafkatest_f15291_0()
{    return "Eviction{key=" + key + ", value=" + value + ", recordContext=" + recordContext + '}';}
f15291
0
putAll
public void kafkatest_f15300_0(final List<KeyValue<Bytes, byte[]>> entries)
{    wrapped.putAll(entries);}
f15300
0
delete
public byte[] kafkatest_f15301_0(final Bytes key)
{    return wrapped.delete(key);}
f15301
0
persistent
public boolean kafkatest_f15310_0()
{    return false;}
f15310
0
destroy
public void kafkatest_f15311_0() throws IOException
{    Utils.delete(dbDir);}
f15311
0
maybeWrapCaching
private WindowStore<Bytes, byte[]> kafkatest_f15320_0(final WindowStore<Bytes, byte[]> inner)
{    if (!enableCaching) {        return inner;    }    return new CachingWindowStore(inner, storeSupplier.windowSize(), storeSupplier.segmentIntervalMs());}
f15320
0
maybeWrapLogging
private WindowStore<Bytes, byte[]> kafkatest_f15321_0(final WindowStore<Bytes, byte[]> inner)
{    if (!enableLogging) {        return inner;    }    return new ChangeLoggingTimestampedWindowBytesStore(inner, storeSupplier.retainDuplicates());}
f15321
0
all
public KeyValueIterator<Windowed<Bytes>, byte[]> kafkatest_f15330_0()
{    return wrapped.all();}
f15330
0
flush
public void kafkatest_f15331_0()
{    wrapped.flush();}
f15331
0
rawTimestamp
private static byte[] kafkatest_f15340_0(final byte[] rawValueAndTimestamp)
{    return ByteBuffer.allocate(8).put(rawValueAndTimestamp, 0, 8).array();}
f15340
0
timestamp
 static long kafkatest_f15341_0(final byte[] rawValueAndTimestamp)
{    return LONG_DESERIALIZER.deserialize(null, rawTimestamp(rawValueAndTimestamp));}
f15341
0
upperRange
public Bytes kafkatest_f15350_0(final Bytes key, final long to)
{    final byte[] maxSuffix = ByteBuffer.allocate(SUFFIX_SIZE).putLong(to).putInt(Integer.MAX_VALUE).array();    return OrderedBytes.upperRange(key, maxSuffix);}
f15350
0
lowerRange
public Bytes kafkatest_f15351_0(final Bytes key, final long from)
{    return OrderedBytes.lowerRange(key, MIN_SUFFIX);}
f15351
0
extractWindow
private static Window kafkatest_f15360_0(final byte[] binaryKey, final long windowSize)
{    final ByteBuffer buffer = ByteBuffer.wrap(binaryKey);    final long start = buffer.getLong(binaryKey.length - TIMESTAMP_SIZE);    return timeWindowForSize(start, windowSize);}
f15360
0
toStoreKeyBinary
public static Bytes kafkatest_f15361_0(final Bytes key, final long timestamp, final int seqnum)
{    final byte[] serializedKey = key.get();    return toStoreKeyBinary(serializedKey, timestamp, seqnum);}
f15361
0
fromStoreKey
public static Windowed<K> kafkatest_f15370_0(final byte[] binaryKey, final long windowSize, final Deserializer<K> deserializer, final String topic)
{    final K key = deserializer.deserialize(topic, extractStoreKeyBytes(binaryKey));    final Window window = extractStoreWindow(binaryKey, windowSize);    return new Windowed<>(key, window);}
f15370
0
fromStoreKey
public static Windowed<K> kafkatest_f15371_0(final Windowed<Bytes> windowedKey, final Deserializer<K> deserializer, final String topic)
{    final K key = deserializer.deserialize(topic, windowedKey.key().get());    return new Windowed<>(key, windowedKey.window());}
f15371
0
peekNextKey
public Long kafkatest_f15380_0()
{    return WindowKeySchema.extractStoreTimestamp(bytesIterator.peekNextKey().get());}
f15380
0
hasNext
public boolean kafkatest_f15381_0()
{    return bytesIterator.hasNext();}
f15381
0
fetch
public byte[] kafkatest_f15390_0(final Bytes key, final long time)
{    return convertToTimestampedFormat(store.fetch(key, time));}
f15390
0
fetch
public WindowStoreIterator<byte[]> kafkatest_f15391_0(final Bytes key, final long timeFrom, final long timeTo)
{    return new WindowToTimestampedWindowIteratorAdapter(store.fetch(key, timeFrom, timeTo));}
f15391
0
flush
public void kafkatest_f15400_0()
{    store.flush();}
f15400
0
close
public void kafkatest_f15401_0()
{    store.close();}
f15401
0
setFlushListener
public boolean kafkatest_f15410_0(final CacheFlushListener<K, V> listener, final boolean sendOldValues)
{    if (wrapped instanceof CachedStateStore) {        return ((CachedStateStore<K, V>) wrapped).setFlushListener(listener, sendOldValues);    }    return false;}
f15410
0
name
public String kafkatest_f15411_0()
{    return wrapped.name();}
f15411
0
timestampedKeyValueStore
public static QueryableStoreType<ReadOnlyKeyValueStore<K, ValueAndTimestamp<V>>> kafkatest_f15420_0()
{    return new TimestampedKeyValueStoreType<>();}
f15420
0
windowStore
public static QueryableStoreType<ReadOnlyWindowStore<K, V>> kafkatest_f15421_0()
{    return new WindowStoreType<>();}
f15421
0
close
 voidf15430_1final String storeName, final Options options)
{    }
 voidf15430
1
withBuiltinTypes
public static StateSerdes<K, V> kafkatest_f15431_0(final String topic, final Class<K> keyClass, final Class<V> valueClass)
{    return new StateSerdes<>(topic, Serdes.serdeFrom(keyClass), Serdes.serdeFrom(valueClass));}
f15431
0
valueFrom
public V kafkatest_f15440_0(final byte[] rawValue)
{    return valueSerde.deserializer().deserialize(topic, rawValue);}
f15440
0
rawKey
public byte[] kafkatest_f15441_0(final K key)
{    try {        return keySerde.serializer().serialize(topic, key);    } catch (final ClassCastException e) {        final String keyClass = key == null ? "unknown because key is null" : key.getClass().getName();        throw new StreamsException(String.format("A serializer (%s) is not compatible to the actual key type " + "(key type: %s). Change the default Serdes in StreamConfig or " + "provide correct Serdes via method parameters.", keySerializer().getClass().getName(), keyClass), e);    }}
f15441
0
name
public String kafkatest_f15450_0()
{    return name;}
f15450
0
get
public KeyValueStore<Bytes, byte[]> kafkatest_f15451_0()
{    return new MemoryNavigableLRUCache(name, maxCacheSize);}
f15451
0
persistentSessionStore
public static SessionBytesStoreSupplier kafkatest_f15460_0(final String name, final Duration retentionPeriod)
{    final String msgPrefix = prepareMillisCheckFailMsgPrefix(retentionPeriod, "retentionPeriod");    return persistentSessionStore(name, ApiUtils.validateMillisecondDuration(retentionPeriod, msgPrefix));}
f15460
0
inMemorySessionStore
public static SessionBytesStoreSupplier kafkatest_f15461_0(final String name, final Duration retentionPeriod)
{    Objects.requireNonNull(name, "name cannot be null");    final String msgPrefix = prepareMillisCheckFailMsgPrefix(retentionPeriod, "retentionPeriod");    final long retentionPeriodMs = ApiUtils.validateMillisecondDuration(retentionPeriod, msgPrefix);    if (retentionPeriodMs < 0) {        throw new IllegalArgumentException("retentionPeriod cannot be negative");    }    return new InMemorySessionBytesStoreSupplier(name, retentionPeriodMs);}
f15461
0
host
public String kafkatest_f15470_0()
{    return hostInfo.host();}
f15470
0
port
public int kafkatest_f15471_0()
{    return hostInfo.port();}
f15471
0
toString
public String kafkatest_f15480_0()
{    return "<" + value + "," + timestamp + ">";}
f15480
0
equals
public boolean kafkatest_f15481_0(final Object o)
{    if (this == o) {        return true;    }    if (o == null || getClass() != o.getClass()) {        return false;    }    final ValueAndTimestamp<?> that = (ValueAndTimestamp<?>) o;    return timestamp == that.timestamp && Objects.equals(value, that.value);}
f15481
0
stream
public synchronized KStream<K, V> kafkatest_f15490_0(final Pattern topicPattern)
{    return stream(topicPattern, Consumed.with(null, null));}
f15490
0
stream
public synchronized KStream<K, V> kafkatest_f15491_0(final Pattern topicPattern, final Consumed<K, V> consumed)
{    Objects.requireNonNull(topicPattern, "topicPattern can't be null");    Objects.requireNonNull(consumed, "consumed can't be null");    return internalStreamsBuilder.stream(topicPattern, new ConsumedInternal<>(consumed));}
f15491
0
addStateStore
public synchronized StreamsBuilder kafkatest_f15500_0(final StoreBuilder builder)
{    Objects.requireNonNull(builder, "builder can't be null");    internalStreamsBuilder.addStateStore(builder);    return this;}
f15500
0
addGlobalStore
public synchronized StreamsBuilder kafkatest_f15501_0(final StoreBuilder storeBuilder, final String topic, final String sourceName, final Consumed consumed, final String processorName, final ProcessorSupplier stateUpdateSupplier)
{    Objects.requireNonNull(storeBuilder, "storeBuilder can't be null");    Objects.requireNonNull(consumed, "consumed can't be null");    internalStreamsBuilder.addGlobalStore(storeBuilder, sourceName, topic, new ConsumedInternal<>(consumed), processorName, stateUpdateSupplier);    return this;}
f15501
0
adminClientPrefix
public static String kafkatest_f15510_0(final String adminClientProp)
{    return ADMIN_CLIENT_PREFIX + adminClientProp;}
f15510
0
topicPrefix
public static String kafkatest_f15511_0(final String topicProp)
{    return TOPIC_PREFIX + topicProp;}
f15511
0
getProducerConfigs
public Map<String, Object> kafkatest_f15520_0(final String clientId)
{    final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(PRODUCER_PREFIX, ProducerConfig.configNames());    checkIfUnexpectedUserSpecifiedConsumerConfig(clientProvidedProps, NON_CONFIGURABLE_PRODUCER_EOS_CONFIGS);    // generate producer configs from original properties and overridden maps    final Map<String, Object> props = new HashMap<>(eosEnabled ? PRODUCER_EOS_OVERRIDES : PRODUCER_DEFAULT_OVERRIDES);    props.putAll(getClientCustomProps());    props.putAll(clientProvidedProps);    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, originals().get(BOOTSTRAP_SERVERS_CONFIG));    // add client id with stream client id prefix    props.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);    return props;}
f15520
0
getAdminConfigs
public Map<String, Object> kafkatest_f15521_0(final String clientId)
{    final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(ADMIN_CLIENT_PREFIX, AdminClientConfig.configNames());    final Map<String, Object> props = new HashMap<>();    props.putAll(getClientCustomProps());    props.putAll(clientProvidedProps);    // add client id with stream client id prefix    props.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);    return props;}
f15521
0
main
public static void kafkatest_f15530_0(final String[] args)
{    System.out.println(CONFIG.toHtml());}
f15530
0
addSource
public synchronized Topology kafkatest_f15531_0(final String name, final String... topics)
{    internalTopologyBuilder.addSource(null, name, null, null, null, topics);    return this;}
f15531
0
addSource
public synchronized Topology kafkatest_f15540_0(final String name, final Deserializer keyDeserializer, final Deserializer valueDeserializer, final Pattern topicPattern)
{    internalTopologyBuilder.addSource(null, name, null, keyDeserializer, valueDeserializer, topicPattern);    return this;}
f15540
0
addSource
public synchronized Topology kafkatest_f15541_0(final AutoOffsetReset offsetReset, final String name, final Deserializer keyDeserializer, final Deserializer valueDeserializer, final String... topics)
{    internalTopologyBuilder.addSource(offsetReset, name, null, keyDeserializer, valueDeserializer, topics);    return this;}
f15541
0
addSink
public synchronized Topology kafkatest_f15550_0(final String name, final TopicNameExtractor<K, V> topicExtractor, final StreamPartitioner<? super K, ? super V> partitioner, final String... parentNames)
{    internalTopologyBuilder.addSink(name, topicExtractor, null, null, partitioner, parentNames);    return this;}
f15550
0
addSink
public synchronized Topology kafkatest_f15551_0(final String name, final TopicNameExtractor<K, V> topicExtractor, final Serializer<K> keySerializer, final Serializer<V> valueSerializer, final String... parentNames)
{    internalTopologyBuilder.addSink(name, topicExtractor, keySerializer, valueSerializer, null, parentNames);    return this;}
f15551
0
verifyInEquality
public static void kafkatest_f15560_0(final T o1, final T o2)
{    // making sure we don't get an NPE in the test    if (o1 == null && o2 == null) {        throw new AssertionError("Both o1 and o2 were null.");    } else if (o1 == null) {        return;    } else if (o2 == null) {        return;    }    verifyGeneralEqualityProperties(o1, o2);    // these two objects should NOT equal each other    if (o1.equals(o2)) {        throw new AssertionError(String.format("o1[%s] was equal to o2[%s].", o1, o2));    }    if (o2.equals(o1)) {        throw new AssertionError(String.format("o2[%s] was equal to o1[%s].", o2, o1));    }    verifyHashCodeConsistency(o1, o2);    // since these objects are NOT equal, their hashcode SHOULD PROBABLY not be the same    if (o1.hashCode() == o2.hashCode()) {        throw new AssertionError(String.format("o1[%s].hash[%d] was equal to o2[%s].hash[%d], even though !o1.equals(o2). " + "This is NOT A BUG, but it is undesirable for hash collection performance.", o1, o1.hashCode(), o2, o2.hashCode()));    }}
f15560
0
verifyGeneralEqualityProperties
private static void kafkatest_f15561_0(final T o1, final T o2)
{    // objects should equal themselves    if (!o1.equals(o1)) {        throw new AssertionError(String.format("o1[%s] was not equal to itself.", o1));    }    if (!o2.equals(o2)) {        throw new AssertionError(String.format("o2[%s] was not equal to itself.", o2));    }    // non-null objects should not equal null    if (o1.equals(null)) {        throw new AssertionError(String.format("o1[%s] was equal to null.", o1));    }    if (o2.equals(null)) {        throw new AssertionError(String.format("o2[%s] was equal to null.", o2));    }    // objects should not equal some random object    if (o1.equals(new Object())) {        throw new AssertionError(String.format("o1[%s] was equal to an anonymous Object.", o1));    }    if (o2.equals(new Object())) {        throw new AssertionError(String.format("o2[%s] was equal to an anonymous Object.", o2));    }}
f15561
0
checkResult
private void kafkatest_f15570_0(final String outputTopic, final KeyValueTimestamp<Long, String> expectedFinalResult, final int expectedTotalNumRecords) throws InterruptedException
{    final List<KeyValueTimestamp<Long, String>> result = IntegrationTestUtils.waitUntilMinKeyValueWithTimestampRecordsReceived(RESULT_CONSUMER_CONFIG, outputTopic, expectedTotalNumRecords, 30 * 1000L);    assertThat(result.get(result.size() - 1), is(expectedFinalResult));}
f15570
0
runTest
 void kafkatest_f15571_0(final List<List<KeyValueTimestamp<Long, String>>> expectedResult) throws Exception
{    runTest(expectedResult, null);}
f15571
0
conditionMet
public boolean kafkatest_f15580_0()
{    try {        final ConsumerGroupDescription groupDescription = adminClient.describeConsumerGroups(Collections.singletonList(appID)).describedGroups().get(appID).get();        return groupDescription.members().isEmpty();    } catch (final ExecutionException | InterruptedException e) {        return false;    }}
f15580
0
prepareTest
 void kafkatest_f15581_0() throws Exception
{    prepareConfigs();    prepareEnvironment();    // busy wait until cluster (ie, ConsumerGroupCoordinator) is available    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Test consumer group " + appID + " still active even after waiting " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    cluster.deleteAndRecreateTopics(INPUT_TOPIC, OUTPUT_TOPIC, OUTPUT_TOPIC_2, OUTPUT_TOPIC_2_RERUN);    add10InputElements();}
f15581
0
testReprocessingFromDateTimeAfterResetWithoutIntermediateUserTopic
 void kafkatest_f15590_0() throws Exception
{    appID = testId + "-from-datetime";    streamsConfig.put(StreamsConfig.APPLICATION_ID_CONFIG, appID);    // RUN    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.start();    final List<KeyValue<Long, Long>> result = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultConsumerConfig, OUTPUT_TOPIC, 10);    streams.close();    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * STREAMS_CONSUMER_TIMEOUT, "Streams Application consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * STREAMS_CONSUMER_TIMEOUT) + " ms.");    // RESET    final File resetFile = File.createTempFile("reset", ".csv");    try (final BufferedWriter writer = new BufferedWriter(new FileWriter(resetFile))) {        writer.write(INPUT_TOPIC + ",0,1");    }    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.cleanUp();    final SimpleDateFormat format = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSS");    final Calendar calendar = Calendar.getInstance();    calendar.add(Calendar.DATE, -1);    cleanGlobal(false, "--to-datetime", format.format(calendar.getTime()));    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Reset Tool consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    assertInternalTopicsGotDeleted(null);    resetFile.deleteOnExit();    // RE-RUN    streams.start();    final List<KeyValue<Long, Long>> resultRerun = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultConsumerConfig, OUTPUT_TOPIC, 10);    streams.close();    assertThat(resultRerun, equalTo(result));    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Reset Tool consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    cleanGlobal(false, null, null);}
f15590
0
testReprocessingByDurationAfterResetWithoutIntermediateUserTopic
 void kafkatest_f15591_0() throws Exception
{    appID = testId + "-from-duration";    streamsConfig.put(StreamsConfig.APPLICATION_ID_CONFIG, appID);    // RUN    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.start();    final List<KeyValue<Long, Long>> result = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultConsumerConfig, OUTPUT_TOPIC, 10);    streams.close();    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * STREAMS_CONSUMER_TIMEOUT, "Streams Application consumer group " + appID + "  did not time out after " + (TIMEOUT_MULTIPLIER * STREAMS_CONSUMER_TIMEOUT) + " ms.");    // RESET    final File resetFile = File.createTempFile("reset", ".csv");    try (final BufferedWriter writer = new BufferedWriter(new FileWriter(resetFile))) {        writer.write(INPUT_TOPIC + ",0,1");    }    streams = new KafkaStreams(setupTopologyWithoutIntermediateUserTopic(), streamsConfig);    streams.cleanUp();    cleanGlobal(false, "--by-duration", "PT1M");    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Reset Tool consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    assertInternalTopicsGotDeleted(null);    resetFile.deleteOnExit();    // RE-RUN    streams.start();    final List<KeyValue<Long, Long>> resultRerun = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultConsumerConfig, OUTPUT_TOPIC, 10);    streams.close();    assertThat(resultRerun, equalTo(result));    TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT, "Reset Tool consumer group " + appID + " did not time out after " + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + " ms.");    cleanGlobal(false, null, null);}
f15591
0
shouldBeAbleToCommitMultiplePartitionOffsets
public void kafkatest_f15600_0() throws Exception
{    runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, null, SINGLE_PARTITION_OUTPUT_TOPIC);}
f15600
0
shouldBeAbleToRunWithTwoSubtopologies
public void kafkatest_f15601_0() throws Exception
{    runSimpleCopyTest(1, SINGLE_PARTITION_INPUT_TOPIC, SINGLE_PARTITION_THROUGH_TOPIC, SINGLE_PARTITION_OUTPUT_TOPIC);}
f15601
0
shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances
public void kafkatest_f15610_0() throws Exception
{    try (final KafkaStreams streams1 = getKafkaStreams(false, "appDir1", 1);        final KafkaStreams streams2 = getKafkaStreams(false, "appDir2", 1)) {        streams1.start();        streams2.start();        final List<KeyValue<Long, Long>> committedDataBeforeGC = prepareData(0L, 10L, 0L, 1L);        final List<KeyValue<Long, Long>> uncommittedDataBeforeGC = prepareData(10L, 15L, 0L, 1L);        final List<KeyValue<Long, Long>> dataBeforeGC = new ArrayList<>();        dataBeforeGC.addAll(committedDataBeforeGC);        dataBeforeGC.addAll(uncommittedDataBeforeGC);        final List<KeyValue<Long, Long>> dataToTriggerFirstRebalance = prepareData(15L, 20L, 0L, 1L);        final List<KeyValue<Long, Long>> dataAfterSecondRebalance = prepareData(20L, 30L, 0L, 1L);        writeInputData(committedDataBeforeGC);        TestUtils.waitForCondition(() -> commitRequested.get() == 2, MAX_WAIT_TIME_MS, "SteamsTasks did not request commit.");        writeInputData(uncommittedDataBeforeGC);        final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeGC.size(), null);        final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeGC.size(), CONSUMER_GROUP_ID);        checkResultPerKey(committedRecords, committedDataBeforeGC);        checkResultPerKey(uncommittedRecords, dataBeforeGC);        gcInjected.set(true);        writeInputData(dataToTriggerFirstRebalance);        TestUtils.waitForCondition(() -> streams1.allMetadata().size() == 1 && streams2.allMetadata().size() == 1 && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2 || streams2.allMetadata().iterator().next().topicPartitions().size() == 2), MAX_WAIT_TIME_MS, "Should have rebalanced.");        final List<KeyValue<Long, Long>> committedRecordsAfterRebalance = readResult(uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size(), CONSUMER_GROUP_ID);        final List<KeyValue<Long, Long>> expectedCommittedRecordsAfterRebalance = new ArrayList<>();        expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeGC);        expectedCommittedRecordsAfterRebalance.addAll(dataToTriggerFirstRebalance);        checkResultPerKey(committedRecordsAfterRebalance, expectedCommittedRecordsAfterRebalance);        doGC = false;        TestUtils.waitForCondition(() -> streams1.allMetadata().size() == 1 && streams2.allMetadata().size() == 1 && streams1.allMetadata().iterator().next().topicPartitions().size() == 1 && streams2.allMetadata().iterator().next().topicPartitions().size() == 1, MAX_WAIT_TIME_MS, "Should have rebalanced.");        writeInputData(dataAfterSecondRebalance);        final List<KeyValue<Long, Long>> allCommittedRecords = readResult(committedDataBeforeGC.size() + uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size() + dataAfterSecondRebalance.size(), CONSUMER_GROUP_ID + "_ALL");        final List<KeyValue<Long, Long>> allExpectedCommittedRecordsAfterRecovery = new ArrayList<>();        allExpectedCommittedRecordsAfterRecovery.addAll(committedDataBeforeGC);        allExpectedCommittedRecordsAfterRecovery.addAll(uncommittedDataBeforeGC);        allExpectedCommittedRecordsAfterRecovery.addAll(dataToTriggerFirstRebalance);        allExpectedCommittedRecordsAfterRecovery.addAll(dataAfterSecondRebalance);        checkResultPerKey(allCommittedRecords, allExpectedCommittedRecordsAfterRecovery);    }}
f15610
0
prepareData
private List<KeyValue<Long, Long>> kafkatest_f15611_0(final long fromInclusive, final long toExclusive, final Long... keys)
{    final List<KeyValue<Long, Long>> data = new ArrayList<>();    for (final Long k : keys) {        for (long v = fromInclusive; v < toExclusive; ++v) {            data.add(new KeyValue<>(k, v));        }    }    return data;}
f15611
0
verifyStateStore
private void kafkatest_f15621_0(final KafkaStreams streams, final Set<KeyValue<Long, Long>> expectedStoreContent)
{    ReadOnlyKeyValueStore<Long, Long> store = null;    final long maxWaitingTime = System.currentTimeMillis() + 300000L;    while (System.currentTimeMillis() < maxWaitingTime) {        try {            store = streams.store(storeName, QueryableStoreTypes.keyValueStore());            break;        } catch (final InvalidStateStoreException okJustRetry) {            try {                Thread.sleep(5000L);            } catch (final Exception ignore) {            }        }    }    assertNotNull(store);    final KeyValueIterator<Long, Long> it = store.all();    while (it.hasNext()) {        assertTrue(expectedStoreContent.remove(it.next()));    }    assertTrue(expectedStoreContent.isEmpty());}
f15621
0
startKafkaCluster
public static void kafkatest_f15622_0() throws InterruptedException
{    CLUSTER.createTopics(TOPIC_1_0, TOPIC_2_0, TOPIC_A_0, TOPIC_C_0, TOPIC_Y_0, TOPIC_Z_0, TOPIC_1_1, TOPIC_2_1, TOPIC_A_1, TOPIC_C_1, TOPIC_Y_1, TOPIC_Z_1, TOPIC_1_2, TOPIC_2_2, TOPIC_A_2, TOPIC_C_2, TOPIC_Y_2, TOPIC_Z_2, NOOP, DEFAULT_OUTPUT_TOPIC, OUTPUT_TOPIC_0, OUTPUT_TOPIC_1, OUTPUT_TOPIC_2);}
f15622
0
shouldThrowStreamsExceptionNoResetSpecified
public void kafkatest_f15631_0() throws InterruptedException
{    final Properties props = new Properties();    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    props.put(ConsumerConfig.METADATA_MAX_AGE_CONFIG, "1000");    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "none");    final Properties localConfig = StreamsTestUtils.getStreamsConfig("testAutoOffsetWithNone", CLUSTER.bootstrapServers(), STRING_SERDE_CLASSNAME, STRING_SERDE_CLASSNAME, props);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> exceptionStream = builder.stream(NOOP);    exceptionStream.to(DEFAULT_OUTPUT_TOPIC, Produced.with(stringSerde, stringSerde));    final KafkaStreams streams = new KafkaStreams(builder.build(), localConfig);    final TestingUncaughtExceptionHandler uncaughtExceptionHandler = new TestingUncaughtExceptionHandler();    streams.setUncaughtExceptionHandler(uncaughtExceptionHandler);    streams.start();    TestUtils.waitForCondition(() -> uncaughtExceptionHandler.correctExceptionThrown, "The expected NoOffsetForPartitionException was never thrown");    streams.close();}
f15631
0
uncaughtException
public void kafkatest_f15632_0(final Thread t, final Throwable e)
{    assertThat(e.getClass().getSimpleName(), is("StreamsException"));    assertThat(e.getCause().getClass().getSimpleName(), is("NoOffsetForPartitionException"));    correctExceptionThrown = true;}
f15632
0
produceTopicValues
private void kafkatest_f15641_0(final String topic) throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronously(topic, Arrays.asList(new KeyValue<>("a", 1L), new KeyValue<>("b", 2L), new KeyValue<>("c", 3L), new KeyValue<>("d", 4L), new KeyValue<>("e", 5L)), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, LongSerializer.class, new Properties()), mockTime);}
f15641
0
produceAbortedMessages
private void kafkatest_f15642_0() throws Exception
{    final Properties properties = new Properties();    properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "someid");    properties.put(ProducerConfig.RETRIES_CONFIG, 1);    IntegrationTestUtils.produceAbortedKeyValuesSynchronouslyWithTimestamp(globalTableTopic, Arrays.asList(new KeyValue<>(1L, "A"), new KeyValue<>(2L, "B"), new KeyValue<>(3L, "C"), new KeyValue<>(4L, "D")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), LongSerializer.class, StringSerializer.class, properties), mockTime.milliseconds());}
f15642
0
createTopics
private void kafkatest_f15651_0() throws Exception
{    streamTopic = "stream-" + testNo;    globalTableTopic = "globalTable-" + testNo;    CLUSTER.createTopics(streamTopic);    CLUSTER.createTopic(globalTableTopic, 2, 1);}
f15651
0
startStreams
private void kafkatest_f15652_0()
{    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();}
f15652
0
init
public void kafkatest_f15661_0(final ProcessorContext context)
{    super.init(context);    store = (KeyValueStore<String, Long>) context.getStateStore(storeName);}
f15661
0
process
public void kafkatest_f15662_0(final String key, final Long value)
{    firstRecordProcessed = true;}
f15662
0
shouldCompactAndDeleteTopicsForWindowStoreChangelogs
public void kafkatest_f15671_0() throws Exception
{    final String appID = APP_ID + "-compact-delete";    streamsProp.put(StreamsConfig.APPLICATION_ID_CONFIG, appID);    //     // Step 1: Configure and start a simple word count topology    //     final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> textLines = builder.stream(DEFAULT_INPUT_TOPIC);    final int durationMs = 2000;    textLines.flatMapValues(value -> Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+"))).groupBy(MockMapper.selectValueMapper()).windowedBy(TimeWindows.of(ofSeconds(1L)).grace(ofMillis(0L))).count(Materialized.<String, Long, WindowStore<Bytes, byte[]>>as("CountWindows").withRetention(ofSeconds(2L)));    final KafkaStreams streams = new KafkaStreams(builder.build(), streamsProp);    streams.start();    //     // Step 2: Produce some input data to the input topic.    //     produceData(Arrays.asList("hello", "world", "world", "hello world"));    //     // Step 3: Verify the state changelog topics are compact    //     waitForCompletion(streams, 2, 30000);    streams.close();    final Properties properties = getTopicProperties(ProcessorStateManager.storeChangelogTopic(appID, "CountWindows"));    final List<String> policies = Arrays.asList(properties.getProperty(LogConfig.CleanupPolicyProp()).split(","));    assertEquals(2, policies.size());    assertTrue(policies.contains(LogConfig.Compact()));    assertTrue(policies.contains(LogConfig.Delete()));    // retention should be 1 day + the window duration    final long retention = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS) + durationMs;    assertEquals(retention, Long.parseLong(properties.getProperty(LogConfig.RetentionMsProp())));    final Properties repartitionProps = getTopicProperties(appID + "-CountWindows-repartition");    assertEquals(LogConfig.Delete(), repartitionProps.getProperty(LogConfig.CleanupPolicyProp()));    assertEquals(3, repartitionProps.size());}
f15671
0
before
public void kafkatest_f15672_0() throws InterruptedException
{    builder = new StreamsBuilder();    createTopics();    streamsConfiguration = new Properties();    final String applicationId = "kgrouped-stream-test-" + testNo.incrementAndGet();    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL_MS);    streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024L);    final KeyValueMapper<Integer, String, String> mapper = MockMapper.selectValueMapper();    stream = builder.stream(streamOneInput, Consumed.with(Serdes.Integer(), Serdes.String()));    groupedStream = stream.groupBy(mapper, Grouped.with(Serdes.String(), Serdes.String()));    reducer = (value1, value2) -> value1 + ":" + value2;}
f15672
0
before
public void kafkatest_f15681_0() throws InterruptedException
{    builder = new StreamsBuilder();    createTopics();    streamsConfiguration = new Properties();    final String applicationId = "kgrouped-stream-test-" + testNo.incrementAndGet();    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());    streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    final KeyValueMapper<Integer, String, String> mapper = MockMapper.selectValueMapper();    stream = builder.stream(streamOneInput, Consumed.with(Serdes.Integer(), Serdes.String()));    groupedStream = stream.groupBy(mapper, Grouped.with(Serdes.String(), Serdes.String()));    reducer = (value1, value2) -> value1 + ":" + value2;    initializer = () -> 0;    aggregator = (aggKey, value, aggregate) -> aggregate + value.length();}
f15681
0
whenShuttingDown
public void kafkatest_f15682_0() throws IOException
{    if (kafkaStreams != null) {        kafkaStreams.close();    }    IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);}
f15682
0
shouldGroupByKey
public void kafkatest_f15691_0() throws Exception
{    final long timestamp = mockTime.milliseconds();    produceMessages(timestamp);    produceMessages(timestamp);    stream.groupByKey(Grouped.with(Serdes.Integer(), Serdes.String())).windowedBy(TimeWindows.of(ofMillis(500L))).count().toStream((windowedKey, value) -> windowedKey.key() + "@" + windowedKey.window().start()).to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));    startStreams();    final List<KeyValueTimestamp<String, Long>> results = receiveMessages(new StringDeserializer(), new LongDeserializer(), 10);    results.sort(KStreamAggregationIntegrationTest::compare);    final long window = timestamp / 500 * 500;    assertThat(results, is(Arrays.asList(new KeyValueTimestamp("1@" + window, 1L, timestamp), new KeyValueTimestamp("1@" + window, 2L, timestamp), new KeyValueTimestamp("2@" + window, 1L, timestamp), new KeyValueTimestamp("2@" + window, 2L, timestamp), new KeyValueTimestamp("3@" + window, 1L, timestamp), new KeyValueTimestamp("3@" + window, 2L, timestamp), new KeyValueTimestamp("4@" + window, 1L, timestamp), new KeyValueTimestamp("4@" + window, 2L, timestamp), new KeyValueTimestamp("5@" + window, 1L, timestamp), new KeyValueTimestamp("5@" + window, 2L, timestamp))));}
f15691
0
shouldCountSessionWindows
public void kafkatest_f15692_0() throws Exception
{    final long sessionGap = 5 * 60 * 1000L;    final List<KeyValue<String, String>> t1Messages = Arrays.asList(new KeyValue<>("bob", "start"), new KeyValue<>("penny", "start"), new KeyValue<>("jo", "pause"), new KeyValue<>("emily", "pause"));    final long t1 = mockTime.milliseconds() - TimeUnit.MILLISECONDS.convert(1, TimeUnit.HOURS);    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, t1Messages, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t1);    final long t2 = t1 + (sessionGap / 2);    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Collections.singletonList(new KeyValue<>("emily", "resume")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t2);    final long t3 = t1 + sessionGap + 1;    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Arrays.asList(new KeyValue<>("bob", "pause"), new KeyValue<>("penny", "stop")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t3);    final long t4 = t3 + (sessionGap / 2);    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Arrays.asList(// bobs session continues    new KeyValue<>("bob", "resume"), // jo's starts new session    new KeyValue<>("jo", "resume")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t4);    final long t5 = t4 - 1;    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(userSessionsStream, Collections.singletonList(// jo has late arrival    new KeyValue<>("jo", "late")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), t5);    final Map<Windowed<String>, KeyValue<Long, Long>> results = new HashMap<>();    final CountDownLatch latch = new CountDownLatch(13);    builder.stream(userSessionsStream, Consumed.with(Serdes.String(), Serdes.String())).groupByKey(Grouped.with(Serdes.String(), Serdes.String())).windowedBy(SessionWindows.with(ofMillis(sessionGap))).count().toStream().transform(() -> new Transformer<Windowed<String>, Long, KeyValue<Object, Object>>() {        private ProcessorContext context;        @Override        public void init(final ProcessorContext context) {            this.context = context;        }        @Override        public KeyValue<Object, Object> transform(final Windowed<String> key, final Long value) {            results.put(key, KeyValue.pair(value, context.timestamp()));            latch.countDown();            return null;        }        @Override        public void close() {        }    });    startStreams();    latch.await(30, TimeUnit.SECONDS);    assertThat(results.get(new Windowed<>("bob", new SessionWindow(t1, t1))), equalTo(KeyValue.pair(1L, t1)));    assertThat(results.get(new Windowed<>("penny", new SessionWindow(t1, t1))), equalTo(KeyValue.pair(1L, t1)));    assertThat(results.get(new Windowed<>("jo", new SessionWindow(t1, t1))), equalTo(KeyValue.pair(1L, t1)));    assertThat(results.get(new Windowed<>("jo", new SessionWindow(t5, t4))), equalTo(KeyValue.pair(2L, t4)));    assertThat(results.get(new Windowed<>("emily", new SessionWindow(t1, t2))), equalTo(KeyValue.pair(2L, t2)));    assertThat(results.get(new Windowed<>("bob", new SessionWindow(t3, t4))), equalTo(KeyValue.pair(2L, t4)));    assertThat(results.get(new Windowed<>("penny", new SessionWindow(t3, t3))), equalTo(KeyValue.pair(1L, t3)));}
f15692
0
produceMessages
private void kafkatest_f15704_0(final long timestamp) throws Exception
{    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(streamOneInput, Arrays.asList(new KeyValue<>(1, "A"), new KeyValue<>(2, "B"), new KeyValue<>(3, "C"), new KeyValue<>(4, "D"), new KeyValue<>(5, "E")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, StringSerializer.class, new Properties()), timestamp);}
f15704
0
createTopics
private void kafkatest_f15705_0() throws InterruptedException
{    streamOneInput = "stream-one-" + testNo;    outputTopic = "output-" + testNo;    userSessionsStream = userSessionsStream + "-" + testNo;    CLUSTER.createTopic(streamOneInput, 3, 1);    CLUSTER.createTopics(userSessionsStream, outputTopic);}
f15705
0
init
public void kafkatest_f15714_0(final ProcessorContext context)
{    state = (KeyValueStore<Integer, Integer>) context.getStateStore(stateStoreName);}
f15714
0
transform
public KeyValue<Integer, Integer> kafkatest_f15715_0(final Integer key, final Integer value)
{    state.putIfAbsent(key, 0);    Integer storedValue = state.get(key);    final KeyValue<Integer, Integer> result = new KeyValue<>(key + 1, value + storedValue++);    state.put(key, storedValue);    return result;}
f15715
0
transform
public Integer kafkatest_f15727_0(final Integer value)
{    state.putIfAbsent(value, 0);    Integer counter = state.get(value);    state.put(value, ++counter);    return counter;}
f15727
0
shouldFlatTransformValuesWithKey
public void kafkatest_f15729_0()
{    stream.flatTransformValues(() -> new ValueTransformerWithKey<Integer, Integer, Iterable<Integer>>() {        private KeyValueStore<Integer, Integer> state;        @Override        public void init(final ProcessorContext context) {            state = (KeyValueStore<Integer, Integer>) context.getStateStore("myTransformState");        }        @Override        public Iterable<Integer> transform(final Integer key, final Integer value) {            final List<Integer> result = new ArrayList<>();            state.putIfAbsent(key, 0);            Integer storedValue = state.get(key);            for (int i = 0; i < 3; i++) {                result.add(value + storedValue++);            }            state.put(key, storedValue);            return result;        }        @Override        public void close() {        }    }, "myTransformState").foreach(action);    final List<KeyValue<Integer, Integer>> expected = Arrays.asList(KeyValue.pair(1, 1), KeyValue.pair(1, 2), KeyValue.pair(1, 3), KeyValue.pair(2, 2), KeyValue.pair(2, 3), KeyValue.pair(2, 4), KeyValue.pair(3, 3), KeyValue.pair(3, 4), KeyValue.pair(3, 5), KeyValue.pair(2, 4), KeyValue.pair(2, 5), KeyValue.pair(2, 6), KeyValue.pair(2, 9), KeyValue.pair(2, 10), KeyValue.pair(2, 11), KeyValue.pair(1, 6), KeyValue.pair(1, 7), KeyValue.pair(1, 8));    verifyResult(expected);}
f15729
0
shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosDisabled
public void kafkatest_f15740_0() throws Exception
{    try {        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        streamsOne.start();        produceKeyValues("a", "b", "c");        assertNumberValuesRead(readKeyValues, expectedInitialResultsMap, "Table did not read all values");        streamsOne.close();        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        // the state restore listener will append one record to the log        streamsOne.setGlobalStateRestoreListener(new UpdatingSourceTopicOnRestoreStartStateRestoreListener());        streamsOne.start();        produceKeyValues("f", "g", "h");        assertNumberValuesRead(readKeyValues, expectedResultsWithDataWrittenDuringRestoreMap, "Table did not get all values after restart");    } finally {        streamsOne.close(Duration.ofSeconds(5));    }}
f15740
0
shouldRestoreAndProgressWhenTopicWrittenToDuringRestorationWithEosEnabled
public void kafkatest_f15741_0() throws Exception
{    try {        STREAMS_CONFIG.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        streamsOne.start();        produceKeyValues("a", "b", "c");        assertNumberValuesRead(readKeyValues, expectedInitialResultsMap, "Table did not read all values");        streamsOne.close();        streamsOne = new KafkaStreams(streamsBuilder.build(), STREAMS_CONFIG);        // the state restore listener will append one record to the log        streamsOne.setGlobalStateRestoreListener(new UpdatingSourceTopicOnRestoreStartStateRestoreListener());        streamsOne.start();        produceKeyValues("f", "g", "h");        assertNumberValuesRead(readKeyValues, expectedResultsWithDataWrittenDuringRestoreMap, "Table did not get all values after restart");    } finally {        streamsOne.close(Duration.ofSeconds(5));    }}
f15741
0
produceRecordsForTwoSegments
private void kafkatest_f15752_0(final Duration segmentInterval) throws Exception
{    final MockTime mockTime = new MockTime(Math.max(segmentInterval.toMillis(), 60_000L));    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(STREAM_INPUT, Collections.singletonList(new KeyValue<>(1, "A")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, StringSerializer.class, new Properties()), mockTime.milliseconds());    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(STREAM_INPUT, Collections.singletonList(new KeyValue<>(1, "B")), TestUtils.producerConfig(CLUSTER.bootstrapServers(), IntegerSerializer.class, StringSerializer.class, new Properties()), mockTime.milliseconds());}
f15752
0
waitUntilAllRecordsAreConsumed
private void kafkatest_f15753_0() throws Exception
{    IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(TestUtils.consumerConfig(CLUSTER.bootstrapServers(), "consumerApp", LongDeserializer.class, LongDeserializer.class, new Properties()), STREAM_OUTPUT_1, 2);}
f15753
0
checkRocksDBMetricsByTag
private void kafkatest_f15762_0(final String tag)
{    final List<Metric> listMetricStore = new ArrayList<Metric>(kafkaStreams.metrics().values()).stream().filter(m -> m.metricName().group().equals("stream-state-metrics") && m.metricName().tags().containsKey(tag)).collect(Collectors.toList());    checkMetricByName(listMetricStore, BYTES_WRITTEN_RATE, 1);    checkMetricByName(listMetricStore, BYTES_WRITTEN_TOTAL, 1);    checkMetricByName(listMetricStore, BYTES_READ_RATE, 1);    checkMetricByName(listMetricStore, BYTES_READ_TOTAL, 1);    checkMetricByName(listMetricStore, MEMTABLE_BYTES_FLUSHED_RATE, 1);    checkMetricByName(listMetricStore, MEMTABLE_BYTES_FLUSHED_TOTAL, 1);    checkMetricByName(listMetricStore, MEMTABLE_HIT_RATIO, 1);    checkMetricByName(listMetricStore, MEMTABLE_FLUSH_TIME_AVG, 1);    checkMetricByName(listMetricStore, MEMTABLE_FLUSH_TIME_MIN, 1);    checkMetricByName(listMetricStore, MEMTABLE_FLUSH_TIME_MAX, 1);    checkMetricByName(listMetricStore, WRITE_STALL_DURATION_AVG, 1);    checkMetricByName(listMetricStore, WRITE_STALL_DURATION_TOTAL, 1);    checkMetricByName(listMetricStore, BLOCK_CACHE_DATA_HIT_RATIO, 1);    checkMetricByName(listMetricStore, BLOCK_CACHE_INDEX_HIT_RATIO, 1);    checkMetricByName(listMetricStore, BLOCK_CACHE_FILTER_HIT_RATIO, 1);    checkMetricByName(listMetricStore, BYTES_READ_DURING_COMPACTION_RATE, 1);    checkMetricByName(listMetricStore, BYTES_WRITTEN_DURING_COMPACTION_RATE, 1);    checkMetricByName(listMetricStore, COMPACTION_TIME_AVG, 1);    checkMetricByName(listMetricStore, COMPACTION_TIME_MIN, 1);    checkMetricByName(listMetricStore, COMPACTION_TIME_MAX, 1);    checkMetricByName(listMetricStore, NUMBER_OF_OPEN_FILES, 1);    checkMetricByName(listMetricStore, NUMBER_OF_FILE_ERRORS, 1);}
f15762
0
checkKeyValueStoreMetricsByGroup
private void kafkatest_f15763_0(final String group)
{    final List<Metric> listMetricStore = new ArrayList<Metric>(kafkaStreams.metrics().values()).stream().filter(m -> m.metricName().group().equals(group)).collect(Collectors.toList());    checkMetricByName(listMetricStore, PUT_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, PUT_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, GET_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, GET_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, DELETE_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, DELETE_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, PUT_ALL_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, PUT_ALL_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, ALL_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, ALL_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, RANGE_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, RANGE_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, FLUSH_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, FLUSH_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, RESTORE_LATENCY_AVG, 2);    checkMetricByName(listMetricStore, RESTORE_LATENCY_MAX, 2);    checkMetricByName(listMetricStore, PUT_RATE, 2);    checkMetricByName(listMetricStore, PUT_TOTAL, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_RATE, 2);    checkMetricByName(listMetricStore, PUT_IF_ABSENT_TOTAL, 2);    checkMetricByName(listMetricStore, GET_RATE, 2);    checkMetricByName(listMetricStore, DELETE_RATE, 2);    checkMetricByName(listMetricStore, DELETE_TOTAL, 2);    checkMetricByName(listMetricStore, PUT_ALL_RATE, 2);    checkMetricByName(listMetricStore, PUT_ALL_TOTAL, 2);    checkMetricByName(listMetricStore, ALL_RATE, 2);    checkMetricByName(listMetricStore, ALL_TOTAL, 2);    checkMetricByName(listMetricStore, RANGE_RATE, 2);    checkMetricByName(listMetricStore, RANGE_TOTAL, 2);    checkMetricByName(listMetricStore, FLUSH_RATE, 2);    checkMetricByName(listMetricStore, FLUSH_TOTAL, 2);    checkMetricByName(listMetricStore, RESTORE_RATE, 2);    checkMetricByName(listMetricStore, RESTORE_TOTAL, 2);}
f15763
0
shouldApplyUpdatesToStandbyStore
public void kafkatest_f15772_0() throws Exception
{    final int batch1NumMessages = 100;    final int batch2NumMessages = 100;    final int key = 1;    final Semaphore semaphore = new Semaphore(0);    final StreamsBuilder builder = new StreamsBuilder();    builder.table(INPUT_TOPIC_NAME, Consumed.with(Serdes.Integer(), Serdes.Integer()), Materialized.<Integer, Integer, KeyValueStore<Bytes, byte[]>>as(TABLE_NAME).withCachingDisabled()).toStream().peek((k, v) -> semaphore.release());    final KafkaStreams kafkaStreams1 = createKafkaStreams(builder, streamsConfiguration());    final KafkaStreams kafkaStreams2 = createKafkaStreams(builder, streamsConfiguration());    final List<KafkaStreams> kafkaStreamsList = Arrays.asList(kafkaStreams1, kafkaStreams2);    final AtomicLong restoreStartOffset = new AtomicLong(-1L);    final AtomicLong restoreEndOffset = new AtomicLong(-1L);    kafkaStreamsList.forEach(kafkaStreams -> {        kafkaStreams.setGlobalStateRestoreListener(createTrackingRestoreListener(restoreStartOffset, restoreEndOffset));        kafkaStreams.start();    });    waitForKafkaStreamssToEnterRunningState(kafkaStreamsList, 60, TimeUnit.SECONDS);    produceValueRange(key, 0, batch1NumMessages);    // Assert that all messages in the first batch were processed in a timely manner    assertThat(semaphore.tryAcquire(batch1NumMessages, 60, TimeUnit.SECONDS), is(equalTo(true)));    final ReadOnlyKeyValueStore<Integer, Integer> store1 = kafkaStreams1.store(TABLE_NAME, QueryableStoreTypes.keyValueStore());    final ReadOnlyKeyValueStore<Integer, Integer> store2 = kafkaStreams2.store(TABLE_NAME, QueryableStoreTypes.keyValueStore());    final boolean kafkaStreams1WasFirstActive;    if (store1.get(key) != null) {        kafkaStreams1WasFirstActive = true;    } else {        // Assert that data from the job was sent to the store        assertThat(store2.get(key), is(notNullValue()));        kafkaStreams1WasFirstActive = false;    }    // Assert that no restore has occurred, ensures that when we check later that the restore    // notification actually came from after the rebalance.    assertThat(restoreStartOffset.get(), is(equalTo(-1L)));    // Assert that the current value in store reflects all messages being processed    assertThat(kafkaStreams1WasFirstActive ? store1.get(key) : store2.get(key), is(equalTo(batch1NumMessages - 1)));    if (kafkaStreams1WasFirstActive) {        kafkaStreams1.close();    } else {        kafkaStreams2.close();    }    final ReadOnlyKeyValueStore<Integer, Integer> newActiveStore = kafkaStreams1WasFirstActive ? store2 : store1;    retryOnExceptionWithTimeout(100, 60 * 1000, TimeUnit.MILLISECONDS, () -> {        // Assert that after failover we have recovered to the last store write        assertThat(newActiveStore.get(key), is(equalTo(batch1NumMessages - 1)));    });    final int totalNumMessages = batch1NumMessages + batch2NumMessages;    produceValueRange(key, batch1NumMessages, totalNumMessages);    // Assert that all messages in the second batch were processed in a timely manner    assertThat(semaphore.tryAcquire(batch2NumMessages, 60, TimeUnit.SECONDS), is(equalTo(true)));    // Assert that either restore was unnecessary or we restored from an offset later than 0    assertThat(restoreStartOffset.get(), is(anyOf(greaterThan(0L), equalTo(-1L))));    // Assert that either restore was unnecessary or we restored to the last offset before we closed the kafkaStreams    assertThat(restoreEndOffset.get(), is(anyOf(equalTo(batch1NumMessages - 1), equalTo(-1L))));    // Assert that the current value in store reflects all messages being processed    assertThat(newActiveStore.get(key), is(equalTo(totalNumMessages - 1)));}
f15772
0
produceValueRange
private void kafkatest_f15773_0(final int key, final int start, final int endExclusive) throws Exception
{    final Properties producerProps = new Properties();    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster.bootstrapServers());    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);    IntegrationTestUtils.produceKeyValuesSynchronously(INPUT_TOPIC_NAME, IntStream.range(start, endExclusive).mapToObj(i -> KeyValue.pair(key, i)).collect(Collectors.toList()), producerProps, mockTime);}
f15773
0
createTopics
public static void kafkatest_f15784_0() throws Exception
{    CLUSTER.createTopic(INPUT_TOPIC, 1, 1);}
f15784
0
setup
public void kafkatest_f15785_0()
{    // create admin client for verification    final Properties adminConfig = new Properties();    adminConfig.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    adminClient = Admin.create(adminConfig);    final Properties streamsConfiguration = new Properties();    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, APPLICATION_ID);    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, PURGE_INTERVAL_MS);    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory(APPLICATION_ID).getPath());    streamsConfiguration.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_MS_CONFIG), PURGE_INTERVAL_MS);    streamsConfiguration.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG), PURGE_SEGMENT_BYTES);    // we cannot allow batch size larger than segment size    streamsConfiguration.put(StreamsConfig.producerPrefix(ProducerConfig.BATCH_SIZE_CONFIG), PURGE_SEGMENT_BYTES / 2);    final StreamsBuilder builder = new StreamsBuilder();    builder.stream(INPUT_TOPIC).groupBy(MockMapper.selectKeyKeyValueMapper()).count();    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration, time);}
f15785
0
close
public void kafkatest_f15794_0()
{    if (!closed) {        myStream.close();        closed = true;    }}
f15794
0
isClosed
public boolean kafkatest_f15795_0()
{    return closed;}
f15795
0
shouldBeAbleToQueryFilterState
public void kafkatest_f15804_0() throws Exception
{    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Long().getClass());    final StreamsBuilder builder = new StreamsBuilder();    final String[] keys = { "hello", "goodbye", "welcome", "go", "kafka" };    final Set<KeyValue<String, Long>> batch1 = new HashSet<>(Arrays.asList(new KeyValue<>(keys[0], 1L), new KeyValue<>(keys[1], 1L), new KeyValue<>(keys[2], 3L), new KeyValue<>(keys[3], 5L), new KeyValue<>(keys[4], 2L)));    final Set<KeyValue<String, Long>> expectedBatch1 = new HashSet<>(Collections.singleton(new KeyValue<>(keys[4], 2L)));    IntegrationTestUtils.produceKeyValuesSynchronously(streamOne, batch1, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, LongSerializer.class, new Properties()), mockTime);    final Predicate<String, Long> filterPredicate = (key, value) -> key.contains("kafka");    final KTable<String, Long> t1 = builder.table(streamOne);    final KTable<String, Long> t2 = t1.filter(filterPredicate, Materialized.as("queryFilter"));    t1.filterNot(filterPredicate, Materialized.as("queryFilterNot"));    t2.toStream().to(outputTopic);    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();    waitUntilAtLeastNumRecordProcessed(outputTopic, 1);    final ReadOnlyKeyValueStore<String, Long> myFilterStore = kafkaStreams.store("queryFilter", QueryableStoreTypes.keyValueStore());    final ReadOnlyKeyValueStore<String, Long> myFilterNotStore = kafkaStreams.store("queryFilterNot", QueryableStoreTypes.keyValueStore());    for (final KeyValue<String, Long> expectedEntry : expectedBatch1) {        TestUtils.waitForCondition(() -> expectedEntry.value.equals(myFilterStore.get(expectedEntry.key)), "Cannot get expected result");    }    for (final KeyValue<String, Long> batchEntry : batch1) {        if (!expectedBatch1.contains(batchEntry)) {            TestUtils.waitForCondition(() -> myFilterStore.get(batchEntry.key) == null, "Cannot get null result");        }    }    for (final KeyValue<String, Long> expectedEntry : expectedBatch1) {        TestUtils.waitForCondition(() -> myFilterNotStore.get(expectedEntry.key) == null, "Cannot get null result");    }    for (final KeyValue<String, Long> batchEntry : batch1) {        if (!expectedBatch1.contains(batchEntry)) {            TestUtils.waitForCondition(() -> batchEntry.value.equals(myFilterNotStore.get(batchEntry.key)), "Cannot get expected result");        }    }}
f15804
0
shouldBeAbleToQueryMapValuesState
public void kafkatest_f15805_0() throws Exception
{    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());    final StreamsBuilder builder = new StreamsBuilder();    final String[] keys = { "hello", "goodbye", "welcome", "go", "kafka" };    final Set<KeyValue<String, String>> batch1 = new HashSet<>(Arrays.asList(new KeyValue<>(keys[0], "1"), new KeyValue<>(keys[1], "1"), new KeyValue<>(keys[2], "3"), new KeyValue<>(keys[3], "5"), new KeyValue<>(keys[4], "2")));    IntegrationTestUtils.produceKeyValuesSynchronously(streamOne, batch1, TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class, new Properties()), mockTime);    final KTable<String, String> t1 = builder.table(streamOne);    t1.mapValues((ValueMapper<String, Long>) Long::valueOf, Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as("queryMapValues").withValueSerde(Serdes.Long())).toStream().to(outputTopic, Produced.with(Serdes.String(), Serdes.Long()));    kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);    kafkaStreams.start();    waitUntilAtLeastNumRecordProcessed(outputTopic, 5);    final ReadOnlyKeyValueStore<String, Long> myMapStore = kafkaStreams.store("queryMapValues", QueryableStoreTypes.keyValueStore());    for (final KeyValue<String, String> batchEntry : batch1) {        assertEquals(Long.valueOf(batchEntry.value), myMapStore.get(batchEntry.key));    }}
f15805
0
waitUntilAtLeastNumRecordProcessed
private void kafkatest_f15814_0(final String topic, final int numRecs) throws Exception
{    final Properties config = new Properties();    config.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    config.setProperty(ConsumerConfig.GROUP_ID_CONFIG, "queryable-state-consumer");    config.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    config.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());    config.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, LongDeserializer.class.getName());    IntegrationTestUtils.waitUntilMinValuesRecordsReceived(config, topic, numRecs, 120 * 1000);}
f15814
0
fetch
private Set<KeyValue<String, Long>> kafkatest_f15815_0(final ReadOnlyWindowStore<String, Long> store, final String key)
{    final WindowStoreIterator<Long> fetch = store.fetch(key, ofEpochMilli(0), ofEpochMilli(System.currentTimeMillis()));    if (fetch.hasNext()) {        final KeyValue<Long, Long> next = fetch.next();        return Collections.singleton(KeyValue.pair(key, next.value));    }    return Collections.emptySet();}
f15815
0
testRegexMatchesTopicsAWhenCreated
public void kafkatest_f15824_0() throws Exception
{    final Serde<String> stringSerde = Serdes.String();    final List<String> expectedFirstAssignment = Collections.singletonList("TEST-TOPIC-1");    final List<String> expectedSecondAssignment = Arrays.asList("TEST-TOPIC-1", "TEST-TOPIC-2");    CLUSTER.createTopic("TEST-TOPIC-1");    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> pattern1Stream = builder.stream(Pattern.compile("TEST-TOPIC-\\d"));    pattern1Stream.to(outputTopic, Produced.with(stringSerde, stringSerde));    final List<String> assignedTopics = new CopyOnWriteArrayList<>();    streams = new KafkaStreams(builder.build(), streamsConfiguration, new DefaultKafkaClientSupplier() {        @Override        public Consumer<byte[], byte[]> getConsumer(final Map<String, Object> config) {            return new KafkaConsumer<byte[], byte[]>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer()) {                @Override                public void subscribe(final Pattern topics, final ConsumerRebalanceListener listener) {                    super.subscribe(topics, new TheConsumerRebalanceListener(assignedTopics, listener));                }            };        }    });    streams.start();    TestUtils.waitForCondition(() -> assignedTopics.equals(expectedFirstAssignment), STREAM_TASKS_NOT_UPDATED);    CLUSTER.createTopic("TEST-TOPIC-2");    TestUtils.waitForCondition(() -> assignedTopics.equals(expectedSecondAssignment), STREAM_TASKS_NOT_UPDATED);}
f15824
0
getConsumer
public Consumer<byte[], byte[]> kafkatest_f15825_0(final Map<String, Object> config)
{    return new KafkaConsumer<byte[], byte[]>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer()) {        @Override        public void subscribe(final Pattern topics, final ConsumerRebalanceListener listener) {            super.subscribe(topics, new TheConsumerRebalanceListener(assignedTopics, listener));        }    };}
f15825
0
getConsumer
public Consumer<byte[], byte[]> kafkatest_f15834_0(final Map<String, Object> config)
{    return new KafkaConsumer<byte[], byte[]>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer()) {        @Override        public void subscribe(final Pattern topics, final ConsumerRebalanceListener listener) {            super.subscribe(topics, new TheConsumerRebalanceListener(leaderAssignment, listener));        }    };}
f15834
0
subscribe
public void kafkatest_f15835_0(final Pattern topics, final ConsumerRebalanceListener listener)
{    super.subscribe(topics, new TheConsumerRebalanceListener(leaderAssignment, listener));}
f15835
0
shouldSendCorrectResults_NO_OPTIMIZATION
public void kafkatest_f15844_0() throws Exception
{    runIntegrationTest(StreamsConfig.NO_OPTIMIZATION, FOUR_REPARTITION_TOPICS);}
f15844
0
runIntegrationTest
private void kafkatest_f15845_0(final String optimizationConfig, final int expectedNumberRepartitionTopics) throws Exception
{    final Initializer<Integer> initializer = () -> 0;    final Aggregator<String, String, Integer> aggregator = (k, v, agg) -> agg + v.length();    final Reducer<String> reducer = (v1, v2) -> v1 + ":" + v2;    final List<String> processorValueCollector = new ArrayList<>();    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> sourceStream = builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()));    final KStream<String, String> mappedStream = sourceStream.map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v));    mappedStream.filter((k, v) -> k.equals("B")).mapValues(v -> v.toUpperCase(Locale.getDefault())).process(() -> new SimpleProcessor(processorValueCollector));    final KStream<String, Long> countStream = mappedStream.groupByKey().count(Materialized.with(Serdes.String(), Serdes.Long())).toStream();    countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()));    mappedStream.groupByKey().aggregate(initializer, aggregator, Materialized.with(Serdes.String(), Serdes.Integer())).toStream().to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()));    // adding operators for case where the repartition node is further downstream    mappedStream.filter((k, v) -> true).peek((k, v) -> System.out.println(k + ":" + v)).groupByKey().reduce(reducer, Materialized.with(Serdes.String(), Serdes.String())).toStream().to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));    mappedStream.filter((k, v) -> k.equals("A")).join(countStream, (v1, v2) -> v1 + ":" + v2.toString(), JoinWindows.of(ofMillis(5000)), Joined.with(Serdes.String(), Serdes.String(), Serdes.Long())).to(JOINED_TOPIC);    streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);    final Properties producerConfig = TestUtils.producerConfig(CLUSTER.bootstrapServers(), StringSerializer.class, StringSerializer.class);    IntegrationTestUtils.produceKeyValuesSynchronously(INPUT_TOPIC, getKeyValues(), producerConfig, mockTime);    final Properties consumerConfig1 = TestUtils.consumerConfig(CLUSTER.bootstrapServers(), StringDeserializer.class, LongDeserializer.class);    final Properties consumerConfig2 = TestUtils.consumerConfig(CLUSTER.bootstrapServers(), StringDeserializer.class, IntegerDeserializer.class);    final Properties consumerConfig3 = TestUtils.consumerConfig(CLUSTER.bootstrapServers(), StringDeserializer.class, StringDeserializer.class);    final Topology topology = builder.build(streamsConfiguration);    final String topologyString = topology.describe().toString();    if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {        assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);    } else {        assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);    }    /*           confirming number of expected repartition topics here         */    assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));    final KafkaStreams streams = new KafkaStreams(topology, streamsConfiguration);    streams.start();    final List<KeyValue<String, Long>> expectedCountKeyValues = Arrays.asList(KeyValue.pair("A", 3L), KeyValue.pair("B", 3L), KeyValue.pair("C", 3L));    IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig1, COUNT_TOPIC, expectedCountKeyValues);    final List<KeyValue<String, Integer>> expectedAggKeyValues = Arrays.asList(KeyValue.pair("A", 9), KeyValue.pair("B", 9), KeyValue.pair("C", 9));    IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig2, AGGREGATION_TOPIC, expectedAggKeyValues);    final List<KeyValue<String, String>> expectedReduceKeyValues = Arrays.asList(KeyValue.pair("A", "foo:bar:baz"), KeyValue.pair("B", "foo:bar:baz"), KeyValue.pair("C", "foo:bar:baz"));    IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig3, REDUCE_TOPIC, expectedReduceKeyValues);    final List<KeyValue<String, String>> expectedJoinKeyValues = Arrays.asList(KeyValue.pair("A", "foo:3"), KeyValue.pair("A", "bar:3"), KeyValue.pair("A", "baz:3"));    IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(consumerConfig3, JOINED_TOPIC, expectedJoinKeyValues);    final List<String> expectedCollectedProcessorValues = Arrays.asList("FOO", "BAR", "BAZ");    assertThat(3, equalTo(processorValueCollector.size()));    assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));    streams.close(ofSeconds(5));}
f15845
0
getCountOfRepartitionTopicsFound
private int kafkatest_f15854_0(final String topologyString)
{    final Matcher matcher = repartitionTopicPattern.matcher(topologyString);    final List<String> repartitionTopicsFound = new ArrayList<>();    while (matcher.find()) {        repartitionTopicsFound.add(matcher.group());    }    return repartitionTopicsFound.size();}
f15854
0
getKeyValues
private List<KeyValue<String, String>> kafkatest_f15855_0()
{    final List<KeyValue<String, String>> keyValueList = new ArrayList<>();    final String[] keys = new String[] { "X", "Y", "Z" };    final String[] values = new String[] { "A:foo", "B:foo", "C:foo" };    for (final String key : keys) {        for (final String value : values) {            keyValueList.add(KeyValue.pair(key, value));        }    }    return keyValueList;}
f15855
0
shouldNotAllowToResetWhileStreamsRunning
public void kafkatest_f15864_0() throws Exception
{    super.shouldNotAllowToResetWhileStreamsIsRunning();}
f15864
0
shouldNotAllowToResetWhenInputTopicAbsent
public void kafkatest_f15865_0() throws Exception
{    super.shouldNotAllowToResetWhenInputTopicAbsent();}
f15865
0
shutdown
public void kafkatest_f15874_0()
{    if (kafkaStreams != null) {        kafkaStreams.close(Duration.ofSeconds(30));    }}
f15874
0
shouldRestoreStateFromSourceTopic
public void kafkatest_f15875_0() throws Exception
{    final AtomicInteger numReceived = new AtomicInteger(0);    final StreamsBuilder builder = new StreamsBuilder();    final Properties props = props(APPID);    props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    // restoring from 1000 to 4000 (committed), and then process from 4000 to 5000 on each of the two partitions    final int offsetLimitDelta = 1000;    final int offsetCheckpointed = 1000;    createStateForRestoration(INPUT_STREAM);    setCommittedOffset(INPUT_STREAM, offsetLimitDelta);    final StateDirectory stateDirectory = new StateDirectory(new StreamsConfig(props), new MockTime(), true);    new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 0)), ".checkpoint")).write(Collections.singletonMap(new TopicPartition(INPUT_STREAM, 0), (long) offsetCheckpointed));    new OffsetCheckpoint(new File(stateDirectory.directoryForTask(new TaskId(0, 1)), ".checkpoint")).write(Collections.singletonMap(new TopicPartition(INPUT_STREAM, 1), (long) offsetCheckpointed));    final CountDownLatch startupLatch = new CountDownLatch(1);    final CountDownLatch shutdownLatch = new CountDownLatch(1);    builder.table(INPUT_STREAM, Materialized.<Integer, Integer, KeyValueStore<Bytes, byte[]>>as("store").withKeySerde(Serdes.Integer()).withValueSerde(Serdes.Integer())).toStream().foreach((key, value) -> {        if (numReceived.incrementAndGet() == 2 * offsetLimitDelta) {            shutdownLatch.countDown();        }    });    kafkaStreams = new KafkaStreams(builder.build(props), props);    kafkaStreams.setStateListener((newState, oldState) -> {        if (newState == KafkaStreams.State.RUNNING && oldState == KafkaStreams.State.REBALANCING) {            startupLatch.countDown();        }    });    final AtomicLong restored = new AtomicLong(0);    kafkaStreams.setGlobalStateRestoreListener(new StateRestoreListener() {        @Override        public void onRestoreStart(final TopicPartition topicPartition, final String storeName, final long startingOffset, final long endingOffset) {        }        @Override        public void onBatchRestored(final TopicPartition topicPartition, final String storeName, final long batchEndOffset, final long numRestored) {        }        @Override        public void onRestoreEnd(final TopicPartition topicPartition, final String storeName, final long totalRestored) {            restored.addAndGet(totalRestored);        }    });    kafkaStreams.start();    assertTrue(startupLatch.await(30, TimeUnit.SECONDS));    assertThat(restored.get(), equalTo((long) numberOfKeys - offsetLimitDelta * 2 - offsetCheckpointed * 2));    assertTrue(shutdownLatch.await(30, TimeUnit.SECONDS));    assertThat(numReceived.get(), equalTo(offsetLimitDelta * 2));}
f15875
0
setCommittedOffset
private void kafkatest_f15889_0(final String topic, final int limitDelta)
{    final Properties consumerConfig = new Properties();    consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, APPID);    consumerConfig.put(ConsumerConfig.CLIENT_ID_CONFIG, "commit-consumer");    consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);    consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);    final Consumer<Integer, Integer> consumer = new KafkaConsumer<>(consumerConfig);    final List<TopicPartition> partitions = Arrays.asList(new TopicPartition(topic, 0), new TopicPartition(topic, 1));    consumer.assign(partitions);    consumer.seekToEnd(partitions);    for (final TopicPartition partition : partitions) {        final long position = consumer.position(partition);        consumer.seek(partition, position - limitDelta);    }    consumer.commitSync();    consumer.close();}
f15889
0
run
public void kafkatest_f15890_0()
{    try {        final Map<String, Set<Integer>> allData = generate(bootstrapServers, numKeys, maxRecordsPerKey, Duration.ofSeconds(20));        result = verify(bootstrapServers, allData, maxRecordsPerKey);    } catch (final Exception ex) {        this.exception = ex;    }}
f15890
0
shouldCreateStandByTasksForMaterializedAndOptimizedSourceTables
public void kafkatest_f15901_0() throws Exception
{    final Properties streamsConfiguration1 = streamsConfiguration();    streamsConfiguration1.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    final Properties streamsConfiguration2 = streamsConfiguration();    streamsConfiguration2.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    final StreamsBuilder builder = new StreamsBuilder();    builder.table(INPUT_TOPIC, Consumed.with(Serdes.Integer(), Serdes.Integer()), Materialized.as("source-table"));    createClients(builder.build(streamsConfiguration1), streamsConfiguration1, builder.build(streamsConfiguration2), streamsConfiguration2);    setStateListenersForVerification(thread -> !thread.standbyTasks().isEmpty() && !thread.activeTasks().isEmpty());    startClients();    waitUntilBothClientAreOK("At least one client did not reach state RUNNING with active tasks and stand-by tasks");}
f15901
0
createClients
private void kafkatest_f15902_0(final Topology topology1, final Properties streamsConfiguration1, final Topology topology2, final Properties streamsConfiguration2)
{    client1 = new KafkaStreams(topology1, streamsConfiguration1);    client2 = new KafkaStreams(topology2, streamsConfiguration2);}
f15902
0
shouldMigrateInMemoryKeyValueStoreToTimestampedKeyValueStoreUsingPapi
public void kafkatest_f15911_0() throws Exception
{    shouldMigrateKeyValueStoreToTimestampedKeyValueStoreUsingPapi(false);}
f15911
0
shouldMigratePersistentKeyValueStoreToTimestampedKeyValueStoreUsingPapi
public void kafkatest_f15912_0() throws Exception
{    shouldMigrateKeyValueStoreToTimestampedKeyValueStoreUsingPapi(true);}
f15912
0
shouldMigratePersistentWindowStoreToTimestampedWindowStoreUsingPapi
public void kafkatest_f15921_0() throws Exception
{    final StreamsBuilder streamsBuilderForOldStore = new StreamsBuilder();    streamsBuilderForOldStore.addStateStore(Stores.windowStoreBuilder(Stores.persistentWindowStore(STORE_NAME, Duration.ofMillis(1000L), Duration.ofMillis(1000L), false), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(WindowedProcessor::new, STORE_NAME);    final StreamsBuilder streamsBuilderForNewStore = new StreamsBuilder();    streamsBuilderForNewStore.addStateStore(Stores.timestampedWindowStoreBuilder(Stores.persistentTimestampedWindowStore(STORE_NAME, Duration.ofMillis(1000L), Duration.ofMillis(1000L), false), Serdes.Integer(), Serdes.Long())).<Integer, Integer>stream(inputStream).process(TimestampedWindowedProcessor::new, STORE_NAME);    final Properties props = props();    shouldMigrateWindowStoreToTimestampedWindowStoreUsingPapi(new KafkaStreams(streamsBuilderForOldStore.build(), props), new KafkaStreams(streamsBuilderForNewStore.build(), props), true);}
f15921
0
shouldMigrateWindowStoreToTimestampedWindowStoreUsingPapi
private void kafkatest_f15922_0(final KafkaStreams kafkaStreamsOld, final KafkaStreams kafkaStreamsNew, final boolean persistentStore) throws Exception
{    kafkaStreams = kafkaStreamsOld;    kafkaStreams.start();    processWindowedKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 1L)));    processWindowedKeyValueAndVerifyPlainCount(1, singletonList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L)));    final long lastUpdateKeyOne = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processWindowedKeyValueAndVerifyPlainCount(2, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L)));    final long lastUpdateKeyTwo = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processWindowedKeyValueAndVerifyPlainCount(3, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L)));    final long lastUpdateKeyThree = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 1L)));    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 2L)));    processWindowedKeyValueAndVerifyPlainCount(4, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), 3L)));    final long lastUpdateKeyFour = persistentStore ? -1L : CLUSTER.time.milliseconds() - 1L;    kafkaStreams.close();    kafkaStreams = null;    kafkaStreams = kafkaStreamsNew;    kafkaStreams.start();    verifyWindowedCountWithTimestamp(new Windowed<>(1, new TimeWindow(0L, 1000L)), 2L, lastUpdateKeyOne);    verifyWindowedCountWithTimestamp(new Windowed<>(2, new TimeWindow(0L, 1000L)), 1L, lastUpdateKeyTwo);    verifyWindowedCountWithTimestamp(new Windowed<>(3, new TimeWindow(0L, 1000L)), 1L, lastUpdateKeyThree);    verifyWindowedCountWithTimestamp(new Windowed<>(4, new TimeWindow(0L, 1000L)), 3L, lastUpdateKeyFour);    final long currentTime = CLUSTER.time.milliseconds();    processKeyValueAndVerifyWindowedCountWithTimestamp(1, currentTime + 42L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyTwo)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, lastUpdateKeyFour))));    processKeyValueAndVerifyWindowedCountWithTimestamp(2, currentTime + 45L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, lastUpdateKeyFour))));    // can process "out of order" record for different key    processKeyValueAndVerifyWindowedCountWithTimestamp(4, currentTime + 21L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(4L, currentTime + 21L))));    processKeyValueAndVerifyWindowedCountWithTimestamp(4, currentTime + 42L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(5L, currentTime + 42L))));    // out of order (same key) record should not reduce result timestamp    processKeyValueAndVerifyWindowedCountWithTimestamp(4, currentTime + 10L, asList(KeyValue.pair(new Windowed<>(1, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(3L, currentTime + 42L)), KeyValue.pair(new Windowed<>(2, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(2L, currentTime + 45L)), KeyValue.pair(new Windowed<>(3, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(1L, lastUpdateKeyThree)), KeyValue.pair(new Windowed<>(4, new TimeWindow(0L, 1000L)), ValueAndTimestamp.make(6L, currentTime + 42L))));    // test new segment    processKeyValueAndVerifyWindowedCountWithTimestamp(10, currentTime + 100001L, singletonList(KeyValue.pair(new Windowed<>(10, new TimeWindow(100000L, 101000L)), ValueAndTimestamp.make(1L, currentTime + 100001L))));    kafkaStreams.close();}
f15922
0
process
public void kafkatest_f15932_0(final Integer key, final Integer value)
{    final long newCount;    final ValueAndTimestamp<Long> oldCountWithTimestamp = store.get(key);    final long newTimestamp;    if (oldCountWithTimestamp == null) {        newCount = 1L;        newTimestamp = context.timestamp();    } else {        newCount = oldCountWithTimestamp.value() + 1L;        newTimestamp = Math.max(oldCountWithTimestamp.timestamp(), context.timestamp());    }    store.put(key, ValueAndTimestamp.make(newCount, newTimestamp));}
f15932
0
init
public void kafkatest_f15934_0(final ProcessorContext context)
{    store = (WindowStore<Integer, Long>) context.getStateStore(STORE_NAME);}
f15934
0
testOuter
public void kafkatest_f15945_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer");    final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null", 3L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b", 6L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-a", 9L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-b", 9L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c", 10L)), null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-d", 14L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-a", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-b", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-c", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d", 15L)));    leftStream.outerJoin(rightStream, valueJoiner, JoinWindows.of(ofSeconds(10))).to(OUTPUT_TOPIC);    runTest(expectedResult);}
f15945
0
testOuterRepartitioned
public void kafkatest_f15946_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer");    final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null", 3L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b", 6L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-a", 9L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-b", 9L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c", 10L)), null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-d", 14L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-a", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-b", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-c", 15L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d", 15L)));    leftStream.map(MockMapper.noOpKeyValueMapper()).outerJoin(rightStream.flatMap(MockMapper.noOpFlatKeyValueMapper()).selectKey(MockMapper.selectKeyKeyValueMapper()), valueJoiner, JoinWindows.of(ofSeconds(10))).to(OUTPUT_TOPIC);    runTest(expectedResult);}
f15946
0
init
public void kafkatest_f15955_0(final ProcessorContext context)
{    this.context = context;}
f15955
0
transform
public KeyValue<String, Long>f15956_1final String key, final Long value)
{    try {        assertThat(context.topic(), equalTo(topic));    } catch (final Throwable e) {        firstException.compareAndSet(null, e);            }    return new KeyValue<>(key, value);}
public KeyValue<String, Long>f15956
1
waitForAnyRecord
private static boolean kafkatest_f15966_0(final String topic)
{    final Properties properties = new Properties();    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());    properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);    properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);    properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);    try (final Consumer<Object, Object> consumer = new KafkaConsumer<>(properties)) {        final List<TopicPartition> partitions = consumer.partitionsFor(topic).stream().map(pi -> new TopicPartition(pi.topic(), pi.partition())).collect(Collectors.toList());        consumer.assign(partitions);        consumer.seekToBeginning(partitions);        final long start = System.currentTimeMillis();        while ((System.currentTimeMillis() - start) < DEFAULT_TIMEOUT) {            final ConsumerRecords<Object, Object> records = consumer.poll(ofMillis(500));            if (!records.isEmpty()) {                return true;            }        }        return false;    }}
f15966
0
shouldShutdownWhenRecordConstraintIsViolated
public void kafkatest_f15967_0() throws InterruptedException
{    final String testId = "-shouldShutdownWhenRecordConstraintIsViolated";    final String appId = getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;    final String input = "input" + testId;    final String outputSuppressed = "output-suppressed" + testId;    final String outputRaw = "output-raw" + testId;    cleanStateBeforeTest(CLUSTER, input, outputRaw, outputSuppressed);    final StreamsBuilder builder = new StreamsBuilder();    final KTable<String, Long> valueCounts = buildCountsTable(input, builder);    valueCounts.suppress(untilTimeLimit(ofMillis(MAX_VALUE), maxRecords(1L).shutDownWhenFull())).toStream().to(outputSuppressed, Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().to(outputRaw, Produced.with(STRING_SERDE, Serdes.Long()));    final Properties streamsConfig = getStreamsConfig(appId);    final KafkaStreams driver = IntegrationTestUtils.getStartedStreams(streamsConfig, builder, true);    try {        produceSynchronously(input, asList(new KeyValueTimestamp<>("k1", "v1", scaledTime(0L)), new KeyValueTimestamp<>("k1", "v2", scaledTime(1L)), new KeyValueTimestamp<>("k2", "v1", scaledTime(2L)), new KeyValueTimestamp<>("x", "x", scaledTime(3L))));        verifyErrorShutdown(driver);    } finally {        driver.close();        cleanStateAfterTest(CLUSTER, driver);    }}
f15967
0
testLeft
public void kafkatest_f15976_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-left");    if (cacheEnabled) {        leftTable.leftJoin(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(false)).to(OUTPUT_TOPIC);        runTest(expectedFinalJoinResult, storeName);    } else {        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null", 3L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a", 5L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 7L)), null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null", 9L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c", 10L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null", 11L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 12L)), null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d", 15L)));        leftTable.leftJoin(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
f15976
0
testOuter
public void kafkatest_f15977_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-outer");    if (cacheEnabled) {        leftTable.outerJoin(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(false)).to(OUTPUT_TOPIC);        runTest(expectedFinalJoinResult, storeName);    } else {        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null", 3L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a", 5L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-b", 7L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 8L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null", 9L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c", 10L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null", 11L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 12L)), null, Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-d", 14L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d", 15L)));        leftTable.outerJoin(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
f15977
0
testOuterOuter
public void kafkatest_f15986_0() throws Exception
{    STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appID + "-inner-outer");    if (cacheEnabled) {        leftTable.outerJoin(rightTable, valueJoiner).outerJoin(rightTable, valueJoiner, materialized).toStream().peek(new CountingPeek(true)).to(OUTPUT_TOPIC);        runTest(expectedFinalMultiJoinResult, storeName);    } else {        final List<List<KeyValueTimestamp<Long, String>>> expectedResult = Arrays.asList(null, null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-null-null", 3L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "A-a-a", 4L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-a-a", 5L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "B-b-b", 6L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-b-b", 7L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 8L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 8L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null-null", 9L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-c-c", 10L)), Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null-null", 11L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "C-null-null", 11L)), Collections.singletonList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, null, 12L)), null, null, Arrays.asList(new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-d-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "null-d-d", 14L), new KeyValueTimestamp<>(ANY_UNIQUE_KEY, "D-d-d", 15L)));        leftTable.outerJoin(rightTable, valueJoiner).outerJoin(rightTable, valueJoiner, materialized).toStream().to(OUTPUT_TOPIC);        runTest(expectedResult, storeName);    }}
f15986
0
start
public voidf15987_1) throws IOException, InterruptedException
{            zookeeper = new EmbeddedZookeeper();        brokerConfig.put(KafkaConfig$.MODULE$.ZkConnectProp(), zKConnectString());    brokerConfig.put(KafkaConfig$.MODULE$.PortProp(), DEFAULT_BROKER_PORT);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.DeleteTopicEnableProp(), true);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.LogCleanerDedupeBufferSizeProp(), 2 * 1024 * 1024L);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.GroupMinSessionTimeoutMsProp(), 0);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.GroupInitialRebalanceDelayMsProp(), 0);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.OffsetsTopicReplicationFactorProp(), (short) 1);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.OffsetsTopicPartitionsProp(), 5);    putIfAbsent(brokerConfig, KafkaConfig$.MODULE$.AutoCreateTopicsEnableProp(), true);    for (int i = 0; i < brokers.length; i++) {        brokerConfig.put(KafkaConfig$.MODULE$.BrokerIdProp(), i);                brokers[i] = new KafkaEmbedded(brokerConfig, time);            }}
public voidf15987
1
createTopic
public void kafkatest_f15996_0(final String topic, final int partitions, final int replication) throws InterruptedException
{    createTopic(topic, partitions, replication, Collections.emptyMap());}
f15996
0
createTopic
public void kafkatest_f15997_0(final String topic, final int partitions, final int replication, final Map<String, String> topicConfig) throws InterruptedException
{    brokers[0].createTopic(topic, partitions, replication, topicConfig);    final List<TopicPartition> topicPartitions = new ArrayList<>();    for (int partition = 0; partition < partitions; partition++) {        topicPartitions.add(new TopicPartition(topic, partition));    }    IntegrationTestUtils.waitForTopicPartitions(brokers(), topicPartitions, TOPIC_CREATION_TIMEOUT);}
f15997
0
deleteAndRecreateTopics
public void kafkatest_f16006_0(final long timeoutMs, final String... topics) throws InterruptedException
{    deleteTopicsAndWait(timeoutMs, topics);    createTopics(topics);}
f16006
0
waitForRemainingTopics
public void kafkatest_f16007_0(final long timeoutMs, final String... topics) throws InterruptedException
{    TestUtils.waitForCondition(new TopicsRemainingCondition(topics), timeoutMs, "Topics are not expected after " + timeoutMs + " milli seconds.");}
f16007
0
cleanStateAfterTest
public static void kafkatest_f16016_0(final EmbeddedKafkaCluster cluster, final KafkaStreams driver)
{    driver.cleanUp();    try {        cluster.deleteAllTopicsAndWait(DEFAULT_TIMEOUT);    } catch (final InterruptedException e) {        throw new RuntimeException(e);    }}
f16016
0
produceKeyValuesSynchronously
public static void kafkatest_f16017_0(final String topic, final Collection<KeyValue<K, V>> records, final Properties producerConfig, final Time time) throws ExecutionException, InterruptedException
{    produceKeyValuesSynchronously(topic, records, producerConfig, time, false);}
f16017
0
produceValuesSynchronously
public static void kafkatest_f16026_0(final String topic, final Collection<V> records, final Properties producerConfig, final Time time) throws ExecutionException, InterruptedException
{    produceValuesSynchronously(topic, records, producerConfig, time, false);}
f16026
0
produceValuesSynchronously
public static void kafkatest_f16027_0(final String topic, final Collection<V> records, final Properties producerConfig, final Time time, final boolean enableTransactions) throws ExecutionException, InterruptedException
{    final Collection<KeyValue<Object, V>> keyedRecords = new ArrayList<>();    for (final V value : records) {        final KeyValue<Object, V> kv = new KeyValue<>(null, value);        keyedRecords.add(kv);    }    produceKeyValuesSynchronously(topic, keyedRecords, producerConfig, time, enableTransactions);}
f16027
0
waitUntilFinalKeyValueRecordsReceived
public static List<KeyValue<K, V>> kafkatest_f16036_0(final Properties consumerConfig, final String topic, final List<KeyValue<K, V>> expectedRecords, final long waitTime) throws InterruptedException
{    return waitUntilFinalKeyValueRecordsReceived(consumerConfig, topic, expectedRecords, waitTime, false);}
f16036
0
waitUntilFinalKeyValueTimestampRecordsReceived
public static List<KeyValueTimestamp<K, V>> kafkatest_f16037_0(final Properties consumerConfig, final String topic, final List<KeyValueTimestamp<K, V>> expectedRecords, final long waitTime) throws InterruptedException
{    return waitUntilFinalKeyValueRecordsReceived(consumerConfig, topic, expectedRecords, waitTime, true);}
f16037
0
printRecords
private static String kafkatest_f16046_0(final List<ConsumerRecord<K, V>> result)
{    final StringBuilder resultStr = new StringBuilder();    resultStr.append("[\n");    for (final ConsumerRecord<?, ?> record : result) {        resultStr.append("  ").append(record.toString()).append("\n");    }    resultStr.append("]");    return resultStr.toString();}
f16046
0
readValues
public static List<V> kafkatest_f16047_0(final String topic, final Properties consumerConfig, final long waitTime, final int maxMessages)
{    final List<V> returnList;    try (final Consumer<Object, V> consumer = createConsumer(consumerConfig)) {        returnList = readValues(topic, consumer, waitTime, maxMessages);    }    return returnList;}
f16047
0
effectiveConfigFrom
private Properties kafkatest_f16056_0(final Properties initialConfig)
{    final Properties effectiveConfig = new Properties();    effectiveConfig.put(KafkaConfig$.MODULE$.BrokerIdProp(), 0);    effectiveConfig.put(KafkaConfig$.MODULE$.HostNameProp(), "localhost");    effectiveConfig.put(KafkaConfig$.MODULE$.PortProp(), "9092");    effectiveConfig.put(KafkaConfig$.MODULE$.NumPartitionsProp(), 1);    effectiveConfig.put(KafkaConfig$.MODULE$.AutoCreateTopicsEnableProp(), true);    effectiveConfig.put(KafkaConfig$.MODULE$.MessageMaxBytesProp(), 1000000);    effectiveConfig.put(KafkaConfig$.MODULE$.ControlledShutdownEnableProp(), true);    effectiveConfig.put(KafkaConfig$.MODULE$.ZkSessionTimeoutMsProp(), 10000);    effectiveConfig.putAll(initialConfig);    effectiveConfig.setProperty(KafkaConfig$.MODULE$.LogDirProp(), logDir.getAbsolutePath());    return effectiveConfig;}
f16056
0
brokerList
public String kafkatest_f16057_0()
{    final Object listenerConfig = effectiveConfig.get(KafkaConfig$.MODULE$.InterBrokerListenerNameProp());    return kafka.config().hostName() + ":" + kafka.boundPort(new ListenerName(listenerConfig != null ? listenerConfig.toString() : "PLAINTEXT"));}
f16057
0
shouldThrowNullPointerExceptionForNullDuration
public void kafkatest_f16066_0()
{    final String nullDurationPrefix = prepareMillisCheckFailMsgPrefix(null, "nullDuration");    try {        validateMillisecondDuration(null, nullDurationPrefix);        fail("Expected exception when null passed to duration.");    } catch (final IllegalArgumentException e) {        assertThat(e.getMessage(), containsString(nullDurationPrefix));    }}
f16066
0
shouldThrowArithmeticExceptionForMaxDuration
public void kafkatest_f16067_0()
{    final Duration maxDurationInDays = Duration.ofDays(MAX_ACCEPTABLE_DAYS_FOR_DURATION);    final String maxDurationPrefix = prepareMillisCheckFailMsgPrefix(maxDurationInDays, "maxDuration");    try {        validateMillisecondDuration(maxDurationInDays, maxDurationPrefix);        fail("Expected exception when maximum days passed for duration, because of long overflow");    } catch (final IllegalArgumentException e) {        assertThat(e.getMessage(), containsString(maxDurationPrefix));    }}
f16067
0
testInvalidSocketSendBufferSize
public void kafkatest_f16076_0()
{    props.put(CommonClientConfigs.SEND_BUFFER_CONFIG, -2);    final KafkaStreams streams = new KafkaStreams(builder.build(), props);    streams.close();}
f16076
0
testInvalidSocketReceiveBufferSize
public void kafkatest_f16077_0()
{    props.put(CommonClientConfigs.RECEIVE_BUFFER_CONFIG, -2);    final KafkaStreams streams = new KafkaStreams(builder.build(), props);    streams.close();}
f16077
0
testInitializesAndDestroysMetricsReporters
public void kafkatest_f16086_0()
{    final int oldInitCount = MockMetricsReporter.INIT_COUNT.get();    try (final KafkaStreams streams = new KafkaStreams(builder.build(), props)) {        final int newInitCount = MockMetricsReporter.INIT_COUNT.get();        final int initDiff = newInitCount - oldInitCount;        assertTrue("some reporters should be initialized by calling on construction", initDiff > 0);        streams.start();        final int oldCloseCount = MockMetricsReporter.CLOSE_COUNT.get();        streams.close();        assertEquals(oldCloseCount + initDiff, MockMetricsReporter.CLOSE_COUNT.get());    }}
f16086
0
testCloseIsIdempotent
public void kafkatest_f16087_0()
{    globalStreams.close();    final int closeCount = MockMetricsReporter.CLOSE_COUNT.get();    globalStreams.close();    Assert.assertEquals("subsequent close() calls should do nothing", closeCount, MockMetricsReporter.CLOSE_COUNT.get());}
f16087
0
shouldNotGetAllTasksWithStoreWhenNotRunning
public void kafkatest_f16096_0()
{    globalStreams.allMetadataForStore("store");}
f16096
0
shouldNotGetTaskWithKeyAndSerializerWhenNotRunning
public void kafkatest_f16097_0()
{    globalStreams.metadataForKey("store", "key", Serdes.String().serializer());}
f16097
0
statelessTopologyShouldNotCreateStateDirectory
public void kafkatest_f16106_0() throws Exception
{    final String inputTopic = testName.getMethodName() + "-input";    final String outputTopic = testName.getMethodName() + "-output";    CLUSTER.createTopics(inputTopic, outputTopic);    final Topology topology = new Topology();    topology.addSource("source", Serdes.String().deserializer(), Serdes.String().deserializer(), inputTopic).addProcessor("process", () -> new AbstractProcessor<String, String>() {        @Override        public void process(final String key, final String value) {            if (value.length() % 2 == 0) {                context().forward(key, key + value);            }        }    }, "source").addSink("sink", outputTopic, new StringSerializer(), new StringSerializer(), "process");    startStreamsAndCheckDirExists(topology, Collections.singleton(inputTopic), outputTopic, false);}
f16106
0
process
public void kafkatest_f16107_0(final String key, final String value)
{    if (value.length() % 2 == 0) {        context().forward(key, key + value);    }}
f16107
0
shouldHaveSameEqualsAndHashCode
public void kafkatest_f16116_0()
{    final KeyValue<String, Long> kv = KeyValue.pair("key1", 1L);    final KeyValue<String, Long> copyOfKV = KeyValue.pair(kv.key, kv.value);    // Reflexive    assertTrue(kv.equals(kv));    assertTrue(kv.hashCode() == kv.hashCode());    // Symmetric    assertTrue(kv.equals(copyOfKV));    assertTrue(kv.hashCode() == copyOfKV.hashCode());    assertTrue(copyOfKV.hashCode() == kv.hashCode());    // Transitive    final KeyValue<String, Long> copyOfCopyOfKV = KeyValue.pair(copyOfKV.key, copyOfKV.value);    assertTrue(copyOfKV.equals(copyOfCopyOfKV));    assertTrue(copyOfKV.hashCode() == copyOfCopyOfKV.hashCode());    assertTrue(kv.equals(copyOfCopyOfKV));    assertTrue(kv.hashCode() == copyOfCopyOfKV.hashCode());    // Inequality scenarios    assertFalse("must be false for null", kv.equals(null));    assertFalse("must be false if key is non-null and other key is null", kv.equals(KeyValue.pair(null, kv.value)));    assertFalse("must be false if value is non-null and other value is null", kv.equals(KeyValue.pair(kv.key, null)));    final KeyValue<Long, Long> differentKeyType = KeyValue.pair(1L, kv.value);    assertFalse("must be false for different key types", kv.equals(differentKeyType));    final KeyValue<String, String> differentValueType = KeyValue.pair(kv.key, "anyString");    assertFalse("must be false for different value types", kv.equals(differentValueType));    final KeyValue<Long, String> differentKeyValueTypes = KeyValue.pair(1L, "anyString");    assertFalse("must be false for different key and value types", kv.equals(differentKeyValueTypes));    assertFalse("must be false for different types of objects", kv.equals(new Object()));    final KeyValue<String, Long> differentKey = KeyValue.pair(kv.key + "suffix", kv.value);    assertFalse("must be false if key is different", kv.equals(differentKey));    assertFalse("must be false if key is different", differentKey.equals(kv));    final KeyValue<String, Long> differentValue = KeyValue.pair(kv.key, kv.value + 1L);    assertFalse("must be false if value is different", kv.equals(differentValue));    assertFalse("must be false if value is different", differentValue.equals(kv));    final KeyValue<String, Long> differentKeyAndValue = KeyValue.pair(kv.key + "suffix", kv.value + 1L);    assertFalse("must be false if key and value are different", kv.equals(differentKeyAndValue));    assertFalse("must be false if key and value are different", differentKeyAndValue.equals(kv));}
f16116
0
key
public K kafkatest_f16117_0()
{    return key;}
f16117
0
randomFilter
 KStream<K, V> kafkatest_f16126_0()
{    final String name = builder.newProcessorName("RANDOM-FILTER-");    final ProcessorGraphNode<K, V> processorNode = new ProcessorGraphNode<>(name, new ProcessorParameters<>(new ExtendedKStreamDummy<>(), name));    builder.addGraphNode(this.streamsGraphNode, processorNode);    return new KStreamImpl<>(name, null, null, sourceNodes, false, processorNode, builder);}
f16126
0
get
public Processor<K, V> kafkatest_f16127_0()
{    return new ExtendedKStreamDummyProcessor();}
f16127
0
shouldInnerJoinWithStream
public void kafkatest_f16136_0()
{    final MockProcessorSupplier<String, String> supplier = new MockProcessorSupplier<>();    stream.join(global, keyValueMapper, MockValueJoiner.TOSTRING_JOINER).process(supplier);    final Map<String, ValueAndTimestamp<String>> expected = new HashMap<>();    expected.put("1", ValueAndTimestamp.make("a+A", 2L));    expected.put("2", ValueAndTimestamp.make("b+B", 10L));    verifyJoin(expected, supplier);}
f16136
0
verifyJoin
private void kafkatest_f16137_0(final Map<String, ValueAndTimestamp<String>> expected, final MockProcessorSupplier<String, String> supplier)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());    final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        // write some data to the global table        driver.pipeInput(recordFactory.create(globalTopic, "a", "A", 1L));        driver.pipeInput(recordFactory.create(globalTopic, "b", "B", 5L));        // write some data to the stream        driver.pipeInput(recordFactory.create(streamTopic, "1", "a", 2L));        driver.pipeInput(recordFactory.create(streamTopic, "2", "b", 10L));        driver.pipeInput(recordFactory.create(streamTopic, "3", "c", 3L));    }    assertEquals(expected, supplier.theCapturedProcessor().lastValueAndTimestampPerKey);}
f16137
0
shouldBeAbleToProcessNestedMultipleKeyChangingNodes
public void kafkatest_f16152_0()
{    final Properties properties = new Properties();    properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "test-application");    properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");    properties.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> inputStream = builder.stream("inputTopic");    final KStream<String, String> changedKeyStream = inputStream.selectKey((k, v) -> v.substring(0, 5));    // first repartition    changedKeyStream.groupByKey(Grouped.as("count-repartition")).count(Materialized.as("count-store")).toStream().to("count-topic", Produced.with(Serdes.String(), Serdes.Long()));    // second repartition    changedKeyStream.groupByKey(Grouped.as("windowed-repartition")).windowedBy(TimeWindows.of(Duration.ofSeconds(5))).count(Materialized.as("windowed-count-store")).toStream().map((k, v) -> KeyValue.pair(k.key(), v)).to("windowed-count", Produced.with(Serdes.String(), Serdes.Long()));    builder.build(properties);}
f16152
0
shouldNotOptimizeWithValueOrKeyChangingOperatorsAfterInitialKeyChange
public void kafkatest_f16153_0()
{    final Topology attemptedOptimize = getTopologyWithChangingValuesAfterChangingKey(StreamsConfig.OPTIMIZE);    final Topology noOptimization = getTopologyWithChangingValuesAfterChangingKey(StreamsConfig.NO_OPTIMIZATION);    assertEquals(attemptedOptimize.describe().toString(), noOptimization.describe().toString());    assertEquals(2, getCountOfRepartitionTopicsFound(attemptedOptimize.describe().toString()));    assertEquals(2, getCountOfRepartitionTopicsFound(noOptimization.describe().toString()));}
f16153
0
apply
public String kafkatest_f16165_0(final String value)
{    return value;}
f16165
0
test
public boolean kafkatest_f16166_0(final String key, final String value)
{    return true;}
f16166
0
shouldMapStateStoresToCorrectSourceTopics
public void kafkatest_f16175_0()
{    final KStream<String, String> playEvents = builder.stream(Collections.singleton("events"), consumed);    final MaterializedInternal<String, String, KeyValueStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(Materialized.as("table-store"), builder, storePrefix);    final KTable<String, String> table = builder.table("table-topic", consumed, materializedInternal);    final KStream<String, String> mapped = playEvents.map(MockMapper.<String, String>selectValueKeyValueMapper());    mapped.leftJoin(table, MockValueJoiner.TOSTRING_JOINER).groupByKey().count(Materialized.as("count"));    builder.buildAndOptimizeTopology();    builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(APP_ID)));    assertEquals(Collections.singletonList("table-topic"), builder.internalTopologyBuilder.stateStoreNameToSourceTopics().get("table-store"));    assertEquals(Collections.singletonList(APP_ID + "-KSTREAM-MAP-0000000003-repartition"), builder.internalTopologyBuilder.stateStoreNameToSourceTopics().get("count"));}
f16175
0
shouldAddTopicToEarliestAutoOffsetResetList
public void kafkatest_f16176_0()
{    final String topicName = "topic-1";    final ConsumedInternal consumed = new ConsumedInternal<>(Consumed.with(AutoOffsetReset.EARLIEST));    builder.stream(Collections.singleton(topicName), consumed);    builder.buildAndOptimizeTopology();    assertTrue(builder.internalTopologyBuilder.earliestResetTopicsPattern().matcher(topicName).matches());    assertFalse(builder.internalTopologyBuilder.latestResetTopicsPattern().matcher(topicName).matches());}
f16176
0
shouldUseProvidedTimestampExtractor
public void kafkatest_f16185_0()
{    final ConsumedInternal consumed = new ConsumedInternal<>(Consumed.with(new MockTimestampExtractor()));    builder.stream(Collections.singleton("topic"), consumed);    builder.buildAndOptimizeTopology();    final ProcessorTopology processorTopology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(APP_ID))).build(null);    assertThat(processorTopology.source("topic").getTimestampExtractor(), instanceOf(MockTimestampExtractor.class));}
f16185
0
ktableShouldHaveNullTimestampExtractorWhenNoneSupplied
public void kafkatest_f16186_0()
{    builder.table("topic", consumed, materialized);    builder.buildAndOptimizeTopology();    final ProcessorTopology processorTopology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(APP_ID))).build(null);    assertNull(processorTopology.source("topic").getTimestampExtractor());}
f16186
0
shouldNotHaveNullInitializerOnAggregate
public void kafkatest_f16195_0()
{    groupedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as("store"));}
f16195
0
shouldNotHaveNullAdderOnAggregate
public void kafkatest_f16196_0()
{    groupedStream.aggregate(MockInitializer.STRING_INIT, null, Materialized.as("store"));}
f16196
0
doCountSessionWindows
private void kafkatest_f16205_0(final MockProcessorSupplier<Windowed<String>, Long> supplier)
{    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 10));        driver.pipeInput(recordFactory.create(TOPIC, "2", "2", 15));        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 30));        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 70));        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 100));        driver.pipeInput(recordFactory.create(TOPIC, "1", "1", 90));    }    final Map<Windowed<String>, ValueAndTimestamp<Long>> result = supplier.theCapturedProcessor().lastValueAndTimestampPerKey;    assertEquals(ValueAndTimestamp.make(2L, 30L), result.get(new Windowed<>("1", new SessionWindow(10L, 30L))));    assertEquals(ValueAndTimestamp.make(1L, 15L), result.get(new Windowed<>("2", new SessionWindow(15L, 15L))));    assertEquals(ValueAndTimestamp.make(3L, 100L), result.get(new Windowed<>("1", new SessionWindow(70L, 100L))));}
f16205
0
shouldCountSessionWindows
public void kafkatest_f16206_0()
{    final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();    final KTable<Windowed<String>, Long> table = groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).count(Materialized.as("session-store"));    table.toStream().process(supplier);    doCountSessionWindows(supplier);    assertEquals(table.queryableStoreName(), "session-store");}
f16206
0
shouldNotAcceptNullInitializerWhenAggregatingSessionWindows
public void kafkatest_f16215_0()
{    groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).aggregate(null, MockAggregator.TOSTRING_ADDER, (aggKey, aggOne, aggTwo) -> null, Materialized.as("storeName"));}
f16215
0
shouldNotAcceptNullAggregatorWhenAggregatingSessionWindows
public void kafkatest_f16216_0()
{    groupedStream.windowedBy(SessionWindows.with(ofMillis(30))).aggregate(MockInitializer.STRING_INIT, null, (aggKey, aggOne, aggTwo) -> null, Materialized.as("storeName"));}
f16216
0
shouldLogAndMeasureSkipsInAggregate
public void kafkatest_f16225_0()
{    groupedStream.count(Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>as("count").withKeySerde(Serdes.String()));    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);        LogCaptureAppender.unregister(appender);        final Map<MetricName, ? extends Metric> metrics = driver.metrics();        assertEquals(1.0, getMetricByName(metrics, "skipped-records-total", "stream-metrics").metricValue());        assertNotEquals(0.0, getMetricByName(metrics, "skipped-records-rate", "stream-metrics").metricValue());        assertThat(appender.getMessages(), hasItem("Skipping record due to null key or value. key=[3] value=[null] topic=[topic] partition=[0] offset=[6]"));    }}
f16225
0
shouldReduceAndMaterializeResults
public void kafkatest_f16226_0()
{    groupedStream.reduce(MockReducer.STRING_ADDER, Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as("reduce").withKeySerde(Serdes.String()).withValueSerde(Serdes.String()));    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        processData(driver);        {            final KeyValueStore<String, String> reduced = driver.getKeyValueStore("reduce");            assertThat(reduced.get("1"), equalTo("A+C+D"));            assertThat(reduced.get("2"), equalTo("B"));            assertThat(reduced.get("3"), equalTo("E+F"));        }        {            final KeyValueStore<String, ValueAndTimestamp<String>> reduced = driver.getTimestampedKeyValueStore("reduce");            assertThat(reduced.get("1"), equalTo(ValueAndTimestamp.make("A+C+D", 10L)));            assertThat(reduced.get("2"), equalTo(ValueAndTimestamp.make("B", 1L)));            assertThat(reduced.get("3"), equalTo(ValueAndTimestamp.make("E+F", 9L)));        }    }}
f16226
0
shouldNotAllowInvalidStoreNameOnAggregate
public void kafkatest_f16235_0()
{    groupedTable.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, Materialized.as(INVALID_STORE_NAME));}
f16235
0
shouldNotAllowNullInitializerOnAggregate
public void kafkatest_f16236_0()
{    groupedTable.aggregate(null, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, Materialized.as("store"));}
f16236
0
shouldReduceWithInternalStoreName
public void kafkatest_f16245_0()
{    final KeyValueMapper<String, Number, KeyValue<String, Integer>> intProjection = (key, value) -> KeyValue.pair(key, value.intValue());    final KTable<String, Integer> reduced = builder.table(topic, Consumed.with(Serdes.String(), Serdes.Double()), Materialized.<String, Double, KeyValueStore<Bytes, byte[]>>as("store").withKeySerde(Serdes.String()).withValueSerde(Serdes.Double())).groupBy(intProjection).reduce(MockReducer.INTEGER_ADDER, MockReducer.INTEGER_SUBTRACTOR);    final MockProcessorSupplier<String, Integer> supplier = getReducedResults(reduced);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        assertReduced(supplier.theCapturedProcessor().lastValueAndTimestampPerKey, topic, driver);        assertNull(reduced.queryableStoreName());    }}
f16245
0
shouldReduceAndMaterializeResults
public void kafkatest_f16246_0()
{    final KeyValueMapper<String, Number, KeyValue<String, Integer>> intProjection = (key, value) -> KeyValue.pair(key, value.intValue());    final KTable<String, Integer> reduced = builder.table(topic, Consumed.with(Serdes.String(), Serdes.Double())).groupBy(intProjection).reduce(MockReducer.INTEGER_ADDER, MockReducer.INTEGER_SUBTRACTOR, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>as("reduce").withKeySerde(Serdes.String()).withValueSerde(Serdes.Integer()));    final MockProcessorSupplier<String, Integer> supplier = getReducedResults(reduced);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        assertReduced(supplier.theCapturedProcessor().lastValueAndTimestampPerKey, topic, driver);        {            final KeyValueStore<String, Integer> reduce = driver.getKeyValueStore("reduce");            assertThat(reduce.get("A"), equalTo(5));            assertThat(reduce.get("B"), equalTo(6));        }        {            final KeyValueStore<String, ValueAndTimestamp<Integer>> reduce = driver.getTimestampedKeyValueStore("reduce");            assertThat(reduce.get("A"), equalTo(ValueAndTimestamp.make(5, 50L)));            assertThat(reduce.get("B"), equalTo(ValueAndTimestamp.make(6, 30L)));        }    }}
f16246
0
shouldThrowNullPointerOnAggregateWhenSubtractorIsNull
public void kafkatest_f16255_0()
{    groupedTable.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, null, Materialized.as("store"));}
f16255
0
shouldThrowNullPointerOnAggregateWhenMaterializedIsNull
public void kafkatest_f16256_0()
{    groupedTable.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, MockAggregator.TOSTRING_REMOVER, (Materialized) null);}
f16256
0
testFlatMapValuesWithKeys
public void kafkatest_f16265_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final ValueMapperWithKey<Integer, Number, Iterable<String>> mapper = (readOnlyKey, value) -> {        final ArrayList<String> result = new ArrayList<>();        result.add("v" + value);        result.add("k" + readOnlyKey);        return result;    };    final int[] expectedKeys = { 0, 1, 2, 3 };    final KStream<Integer, Integer> stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.Integer()));    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream.flatMapValues(mapper).process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        for (final int expectedKey : expectedKeys) {            // passing the timestamp to recordFactory.create to disambiguate the call            driver.pipeInput(recordFactory.create(topicName, expectedKey, expectedKey, 0L));        }    }    final KeyValueTimestamp[] expected = { new KeyValueTimestamp<>(0, "v0", 0), new KeyValueTimestamp<>(0, "k0", 0), new KeyValueTimestamp<>(1, "v1", 0), new KeyValueTimestamp<>(1, "k1", 0), new KeyValueTimestamp<>(2, "v2", 0), new KeyValueTimestamp<>(2, "k2", 0), new KeyValueTimestamp<>(3, "v3", 0), new KeyValueTimestamp<>(3, "k3", 0) };    assertArrayEquals(expected, supplier.theCapturedProcessor().processed.toArray());}
f16265
0
setUp
public void kafkatest_f16266_0()
{    inputKey = 1;    inputValue = 10;    transformer = mock(Transformer.class);    context = strictMock(ProcessorContext.class);    processor = new KStreamFlatTransformProcessor<>(transformer);}
f16266
0
shouldTransformInputRecordToMultipleOutputValues
public void kafkatest_f16275_0()
{    final Iterable<String> outputValues = Arrays.asList("Hello", "Blue", "Planet");    processor.init(context);    EasyMock.reset(valueTransformer);    EasyMock.expect(valueTransformer.transform(inputKey, inputValue)).andReturn(outputValues);    for (final String outputValue : outputValues) {        context.forward(inputKey, outputValue);    }    replayAll();    processor.process(inputKey, inputValue);    verifyAll();}
f16275
0
shouldEmitNoRecordIfTransformReturnsEmptyList
public void kafkatest_f16276_0()
{    processor.init(context);    EasyMock.reset(valueTransformer);    EasyMock.expect(valueTransformer.transform(inputKey, inputValue)).andReturn(Collections.<String>emptyList());    replayAll();    processor.process(inputKey, inputValue);    verifyAll();}
f16276
0
pushToGlobalTable
private void kafkatest_f16285_0(final int messageCount, final String valuePrefix)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(globalTableTopic, "FKey" + expectedKeys[i], valuePrefix + expectedKeys[i]));    }}
f16285
0
pushNullValueToGlobalTable
private void kafkatest_f16286_0(final int messageCount)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(globalTableTopic, "FKey" + expectedKeys[i], (String) null));    }}
f16286
0
pushToStream
private void kafkatest_f16295_0(final int messageCount, final String valuePrefix, final boolean includeForeignKey)
{    final ConsumerRecordFactory<Integer, String> recordFactory = new ConsumerRecordFactory<>(new IntegerSerializer(), new StringSerializer(), 0L, 1L);    for (int i = 0; i < messageCount; i++) {        String value = valuePrefix + expectedKeys[i];        if (includeForeignKey) {            value = value + ",FKey" + expectedKeys[i];        }        driver.pipeInput(recordFactory.create(streamTopic, expectedKeys[i], value));    }}
f16295
0
pushToGlobalTable
private void kafkatest_f16296_0(final int messageCount, final String valuePrefix)
{    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer(), 0L, 1L);    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(globalTableTopic, "FKey" + expectedKeys[i], valuePrefix + expectedKeys[i]));    }}
f16296
0
testNumProcesses
public void kafkatest_f16305_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> source1 = builder.stream(Arrays.asList("topic-1", "topic-2"), stringConsumed);    final KStream<String, String> source2 = builder.stream(Arrays.asList("topic-3", "topic-4"), stringConsumed);    final KStream<String, String> stream1 = source1.filter((key, value) -> true).filterNot((key, value) -> false);    final KStream<String, Integer> stream2 = stream1.mapValues(Integer::new);    final KStream<String, Integer> stream3 = source2.flatMapValues((ValueMapper<String, Iterable<Integer>>) value -> Collections.singletonList(new Integer(value)));    final KStream<String, Integer>[] streams2 = stream2.branch((key, value) -> (value % 2) == 0, (key, value) -> true);    final KStream<String, Integer>[] streams3 = stream3.branch((key, value) -> (value % 2) == 0, (key, value) -> true);    final int anyWindowSize = 1;    final Joined<String, Integer, Integer> joined = Joined.with(Serdes.String(), Serdes.Integer(), Serdes.Integer());    final KStream<String, Integer> stream4 = streams2[0].join(streams3[0], (value1, value2) -> value1 + value2, JoinWindows.of(ofMillis(anyWindowSize)), joined);    streams2[1].join(streams3[1], (value1, value2) -> value1 + value2, JoinWindows.of(ofMillis(anyWindowSize)), joined);    stream4.to("topic-5");    streams2[1].through("topic-6").process(new MockProcessorSupplier<>());    assertEquals(// sources    2 + // stream1    2 + // stream2    1 + // stream3    1 + 1 + // streams2    2 + 1 + // streams3    2 + // stream2-stream3 joins    5 * 2 + // to    1 + // through    2 + // process    1, TopologyWrapper.getInternalTopologyBuilder(builder.build()).setApplicationId("X").build(null).processors().size());}
f16305
0
shouldPreserveSerdesForOperators
public void kafkatest_f16306_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> stream1 = builder.stream(Collections.singleton("topic-1"), stringConsumed);    final KTable<String, String> table1 = builder.table("topic-2", stringConsumed);    final GlobalKTable<String, String> table2 = builder.globalTable("topic-2", stringConsumed);    final ConsumedInternal<String, String> consumedInternal = new ConsumedInternal<>(stringConsumed);    final KeyValueMapper<String, String, String> selector = (key, value) -> key;    final KeyValueMapper<String, String, Iterable<KeyValue<String, String>>> flatSelector = (key, value) -> Collections.singleton(new KeyValue<>(key, value));    final ValueMapper<String, String> mapper = value -> value;    final ValueMapper<String, Iterable<String>> flatMapper = Collections::singleton;    final ValueJoiner<String, String, String> joiner = (value1, value2) -> value1;    final TransformerSupplier<String, String, KeyValue<String, String>> transformerSupplier = () -> new Transformer<String, String, KeyValue<String, String>>() {        @Override        public void init(final ProcessorContext context) {        }        @Override        public KeyValue<String, String> transform(final String key, final String value) {            return new KeyValue<>(key, value);        }        @Override        public void close() {        }    };    final ValueTransformerSupplier<String, String> valueTransformerSupplier = () -> new ValueTransformer<String, String>() {        @Override        public void init(final ProcessorContext context) {        }        @Override        public String transform(final String value) {            return value;        }        @Override        public void close() {        }    };    assertEquals(((AbstractStream) stream1.filter((key, value) -> false)).keySerde(), consumedInternal.keySerde());    assertEquals(((AbstractStream) stream1.filter((key, value) -> false)).valueSerde(), consumedInternal.valueSerde());    assertEquals(((AbstractStream) stream1.filterNot((key, value) -> false)).keySerde(), consumedInternal.keySerde());    assertEquals(((AbstractStream) stream1.filterNot((key, value) -> false)).valueSerde(), consumedInternal.valueSerde());    assertNull(((AbstractStream) stream1.selectKey(selector)).keySerde());    assertEquals(((AbstractStream) stream1.selectKey(selector)).valueSerde(), consumedInternal.valueSerde());    assertNull(((AbstractStream) stream1.map(KeyValue::new)).keySerde());    assertNull(((AbstractStream) stream1.map(KeyValue::new)).valueSerde());    assertEquals(((AbstractStream) stream1.mapValues(mapper)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.mapValues(mapper)).valueSerde());    assertNull(((AbstractStream) stream1.flatMap(flatSelector)).keySerde());    assertNull(((AbstractStream) stream1.flatMap(flatSelector)).valueSerde());    assertEquals(((AbstractStream) stream1.flatMapValues(flatMapper)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.flatMapValues(flatMapper)).valueSerde());    assertNull(((AbstractStream) stream1.transform(transformerSupplier)).keySerde());    assertNull(((AbstractStream) stream1.transform(transformerSupplier)).valueSerde());    assertEquals(((AbstractStream) stream1.transformValues(valueTransformerSupplier)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.transformValues(valueTransformerSupplier)).valueSerde());    assertNull(((AbstractStream) stream1.merge(stream1)).keySerde());    assertNull(((AbstractStream) stream1.merge(stream1)).valueSerde());    assertEquals(((AbstractStream) stream1.through("topic-3")).keySerde(), consumedInternal.keySerde());    assertEquals(((AbstractStream) stream1.through("topic-3")).valueSerde(), consumedInternal.valueSerde());    assertEquals(((AbstractStream) stream1.through("topic-3", Produced.with(mySerde, mySerde))).keySerde(), mySerde);    assertEquals(((AbstractStream) stream1.through("topic-3", Produced.with(mySerde, mySerde))).valueSerde(), mySerde);    assertEquals(((AbstractStream) stream1.groupByKey()).keySerde(), consumedInternal.keySerde());    assertEquals(((AbstractStream) stream1.groupByKey()).valueSerde(), consumedInternal.valueSerde());    assertEquals(((AbstractStream) stream1.groupByKey(Grouped.with(mySerde, mySerde))).keySerde(), mySerde);    assertEquals(((AbstractStream) stream1.groupByKey(Grouped.with(mySerde, mySerde))).valueSerde(), mySerde);    assertNull(((AbstractStream) stream1.groupBy(selector)).keySerde());    assertEquals(((AbstractStream) stream1.groupBy(selector)).valueSerde(), consumedInternal.valueSerde());    assertEquals(((AbstractStream) stream1.groupBy(selector, Grouped.with(mySerde, mySerde))).keySerde(), mySerde);    assertEquals(((AbstractStream) stream1.groupBy(selector, Grouped.with(mySerde, mySerde))).valueSerde(), mySerde);    assertNull(((AbstractStream) stream1.join(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).keySerde());    assertNull(((AbstractStream) stream1.join(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).valueSerde());    assertEquals(((AbstractStream) stream1.join(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.join(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertNull(((AbstractStream) stream1.leftJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).keySerde());    assertNull(((AbstractStream) stream1.leftJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).valueSerde());    assertEquals(((AbstractStream) stream1.leftJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.leftJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertNull(((AbstractStream) stream1.outerJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).keySerde());    assertNull(((AbstractStream) stream1.outerJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)))).valueSerde());    assertEquals(((AbstractStream) stream1.outerJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.outerJoin(stream1, joiner, JoinWindows.of(Duration.ofMillis(100L)), Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertEquals(((AbstractStream) stream1.join(table1, joiner)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.join(table1, joiner)).valueSerde());    assertEquals(((AbstractStream) stream1.join(table1, joiner, Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.join(table1, joiner, Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertEquals(((AbstractStream) stream1.leftJoin(table1, joiner)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.leftJoin(table1, joiner)).valueSerde());    assertEquals(((AbstractStream) stream1.leftJoin(table1, joiner, Joined.with(mySerde, mySerde, mySerde))).keySerde(), mySerde);    assertNull(((AbstractStream) stream1.leftJoin(table1, joiner, Joined.with(mySerde, mySerde, mySerde))).valueSerde());    assertEquals(((AbstractStream) stream1.join(table2, selector, joiner)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.join(table2, selector, joiner)).valueSerde());    assertEquals(((AbstractStream) stream1.leftJoin(table2, selector, joiner)).keySerde(), consumedInternal.keySerde());    assertNull(((AbstractStream) stream1.leftJoin(table2, selector, joiner)).valueSerde());}
f16306
0
shouldPropagateRepartitionFlagAfterGlobalKTableJoin
public void kafkatest_f16319_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final GlobalKTable<String, String> globalKTable = builder.globalTable("globalTopic");    final KeyValueMapper<String, String, String> kvMappper = (k, v) -> k + v;    final ValueJoiner<String, String, String> valueJoiner = (v1, v2) -> v1 + v2;    builder.<String, String>stream("topic").selectKey((k, v) -> v).join(globalKTable, kvMappper, valueJoiner).groupByKey().count();    final Pattern repartitionTopicPattern = Pattern.compile("Sink: .*-repartition");    final String topology = builder.build().describe().toString();    final Matcher matcher = repartitionTopicPattern.matcher(topology);    assertTrue(matcher.find());    final String match = matcher.group();    assertThat(match, notNullValue());    assertTrue(match.endsWith("repartition"));}
f16319
0
testToWithNullValueSerdeDoesntNPE
public void kafkatest_f16320_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final Consumed<String, String> consumed = Consumed.with(Serdes.String(), Serdes.String());    final KStream<String, String> inputStream = builder.stream(Collections.singleton("input"), consumed);    inputStream.to("output", Produced.with(Serdes.String(), Serdes.String()));}
f16320
0
shouldNotAllowNullMapperOnFlatMapValuesWithKey
public void kafkatest_f16329_0()
{    testStream.flatMapValues((ValueMapperWithKey<? super String, ? super String, ? extends Iterable<? extends String>>) null);}
f16329
0
shouldHaveAtLeastOnPredicateWhenBranching
public void kafkatest_f16330_0()
{    testStream.branch();}
f16330
0
shouldNotAllowNullValueTransformerWithKeySupplierOnFlatTransformValues
public void kafkatest_f16339_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> testStream.flatTransformValues((ValueTransformerWithKeySupplier) null));    assertEquals("valueTransformerSupplier can't be null", e.getMessage());}
f16339
0
shouldNotAllowNullValueTransformerSupplierOnFlatTransformValues
public void kafkatest_f16340_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> testStream.flatTransformValues((ValueTransformerSupplier) null));    assertEquals("valueTransformerSupplier can't be null", e.getMessage());}
f16340
0
shouldNotAllowNullTableOnJoinWithGlobalTable
public void kafkatest_f16349_0()
{    testStream.join((GlobalKTable) null, MockMapper.selectValueMapper(), MockValueJoiner.TOSTRING_JOINER);}
f16349
0
shouldNotAllowNullMapperOnJoinWithGlobalTable
public void kafkatest_f16350_0()
{    testStream.join(builder.globalTable("global", stringConsumed), null, MockValueJoiner.TOSTRING_JOINER);}
f16350
0
shouldThrowNullPointerOnJoinWithTableWhenJoinedIsNull
public void kafkatest_f16359_0()
{    final KTable<String, String> table = builder.table("blah", stringConsumed);    try {        testStream.join(table, MockValueJoiner.TOSTRING_JOINER, null);        fail("Should have thrown NullPointerException");    } catch (final NullPointerException e) {    // ok    }}
f16359
0
shouldThrowNullPointerOnJoinWithStreamWhenJoinedIsNull
public void kafkatest_f16360_0()
{    testStream.join(testStream, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(10)), null);}
f16360
0
testWindowing
public void kafkatest_f16369_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KStream<Integer, String> stream1;    final KStream<Integer, String> stream2;    final KStream<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream1 = builder.stream(topic1, consumed);    stream2 = builder.stream(topic2, consumed);    joined = stream1.join(stream2, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(100)), Joined.with(Serdes.Integer(), Serdes.String(), Serdes.String()));    joined.process(supplier);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> processor = supplier.theCapturedProcessor();        long time = 0L;        // w2 = {}        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "A" + expectedKeys[i], time));        }        processor.checkAndClearProcessResult(EMPTY);        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0) }        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "a" + expectedKeys[i], time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+a0", 0), new KeyValueTimestamp<>(1, "A1+a1", 0));        // push four items to the primary stream with larger and increasing timestamp; this should produce no items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0) }        time = 1000L;        for (int i = 0; i < expectedKeys.length; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "B" + expectedKeys[i], time + i));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items to the other stream with fixed larger timestamp; this should produce four items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100) }        time += 100L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "b" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+b0", 1100), new KeyValueTimestamp<>(1, "B1+b1", 1100), new KeyValueTimestamp<>(2, "B2+b2", 1100), new KeyValueTimestamp<>(3, "B3+b3", 1100));        // push four items to the other stream with incremented timestamp; this should produce three items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "c" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(1, "B1+c1", 1101), new KeyValueTimestamp<>(2, "B2+c2", 1101), new KeyValueTimestamp<>(3, "B3+c3", 1101));        // push four items to the other stream with incremented timestamp; this should produce two items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "d" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(2, "B2+d2", 1102), new KeyValueTimestamp<>(3, "B3+d3", 1102));        // push four items to the other stream with incremented timestamp; this should produce one item        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "e" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(3, "B3+e3", 1103));        // push four items to the other stream with incremented timestamp; this should produce no items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "f" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items to the other stream with timestamp before the window bound; this should produce no items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899) }        time = 1000L - 100L - 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "g" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items to the other stream with with incremented timestamp; this should produce one item        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "h" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+h0", 1000));        // push four items to the other stream with with incremented timestamp; this should produce two items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "i" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+i0", 1000), new KeyValueTimestamp<>(1, "B1+i1", 1001));        // push four items to the other stream with with incremented timestamp; this should produce three items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901),        // 0:j0 (ts: 902), 1:j1 (ts: 902), 2:j2 (ts: 902), 3:j3 (ts: 902) }        time += 1;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "j" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+j0", 1000), new KeyValueTimestamp<>(1, "B1+j1", 1001), new KeyValueTimestamp<>(2, "B2+j2", 1002));        // push four items to the other stream with with incremented timestamp; this should produce four items        // w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003)  }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901),        // 0:j0 (ts: 902), 1:j1 (ts: 902), 2:j2 (ts: 902), 3:j3 (ts: 902) }        // --> w1 = { 0:A0 (ts: 0), 1:A1 (ts: 0),        // 0:B0 (ts: 1000), 1:B1 (ts: 1001), 2:B2 (ts: 1002), 3:B3 (ts: 1003) }        // w2 = { 0:a0 (ts: 0), 1:a1 (ts: 0),        // 0:b0 (ts: 1100), 1:b1 (ts: 1100), 2:b2 (ts: 1100), 3:b3 (ts: 1100),        // 0:c0 (ts: 1101), 1:c1 (ts: 1101), 2:c2 (ts: 1101), 3:c3 (ts: 1101),        // 0:d0 (ts: 1102), 1:d1 (ts: 1102), 2:d2 (ts: 1102), 3:d3 (ts: 1102),        // 0:e0 (ts: 1103), 1:e1 (ts: 1103), 2:e2 (ts: 1103), 3:e3 (ts: 1103),        // 0:f0 (ts: 1104), 1:f1 (ts: 1104), 2:f2 (ts: 1104), 3:f3 (ts: 1104),        // 0:g0 (ts: 899), 1:g1 (ts: 899), 2:g2 (ts: 899), 3:g3 (ts: 899),        // 0:h0 (ts: 900), 1:h1 (ts: 900), 2:h2 (ts: 900), 3:h3 (ts: 900),        // 0:i0 (ts: 901), 1:i1 (ts: 901), 2:i2 (ts: 901), 3:i3 (ts: 901),        // 0:j0 (ts: 902), 1:j1 (ts: 902), 2:j2 (ts: 902), 3:j3 (ts: 902) }        // 0:k0 (ts: 903), 1:k1 (ts: 903), 2:k2 (ts: 903), 3:k3 (ts: 903) }        time += 1;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "k" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "B0+k0", 1000), new KeyValueTimestamp<>(1, "B1+k1", 1001), new KeyValueTimestamp<>(2, "B2+k2", 1002), new KeyValueTimestamp<>(3, "B3+k3", 1003));        // advance time to not join with existing data        // we omit above exiting data, even if it's still in the window        //         // push four items with increasing timestamps to the other stream. the primary window is empty; this should produce no items        // w1 = {}        // w2 = {}        // --> w1 = {}        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time = 2000L;        for (int i = 0; i < expectedKeys.length; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "l" + expectedKeys[i], time + i));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with larger timestamps to the primary stream; this should produce four items        // w1 = {}        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time = 2000L + 100L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "C" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "C0+l0", 2100), new KeyValueTimestamp<>(1, "C1+l1", 2100), new KeyValueTimestamp<>(2, "C2+l2", 2100), new KeyValueTimestamp<>(3, "C3+l3", 2100));        // push four items with increase timestamps to the primary stream; this should produce three items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "D" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(1, "D1+l1", 2101), new KeyValueTimestamp<>(2, "D2+l2", 2101), new KeyValueTimestamp<>(3, "D3+l3", 2101));        // push four items with increase timestamps to the primary stream; this should produce two items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "E" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(2, "E2+l2", 2102), new KeyValueTimestamp<>(3, "E3+l3", 2102));        // push four items with increase timestamps to the primary stream; this should produce one item        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "F" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(3, "F3+l3", 2103));        // push four items with increase timestamps (now out of window) to the primary stream; this should produce no items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "G" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with smaller timestamps (before window) to the primary stream; this should produce no items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time = 2000L - 100L - 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "H" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with increased timestamps to the primary stream; this should produce one item        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "I" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "I0+l0", 2000));        // push four items with increased timestamps to the primary stream; this should produce two items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "J" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "J0+l0", 2000), new KeyValueTimestamp<>(1, "J1+l1", 2001));        // push four items with increased timestamps to the primary stream; this should produce three items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901),        // 0:K0 (ts: 1902), 1:K1 (ts: 1902), 2:K2 (ts: 1902), 3:K3 (ts: 1902) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "K" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "K0+l0", 2000), new KeyValueTimestamp<>(1, "K1+l1", 2001), new KeyValueTimestamp<>(2, "K2+l2", 2002));        // push four items with increased timestamps to the primary stream; this should produce four items        // w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901) }        // 0:K0 (ts: 1902), 1:K1 (ts: 1902), 2:K2 (ts: 1902), 3:K3 (ts: 1902) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        // --> w1 = { 0:C0 (ts: 2100), 1:C1 (ts: 2100), 2:C2 (ts: 2100), 3:C3 (ts: 2100),        // 0:D0 (ts: 2101), 1:D1 (ts: 2101), 2:D2 (ts: 2101), 3:D3 (ts: 2101),        // 0:E0 (ts: 2102), 1:E1 (ts: 2102), 2:E2 (ts: 2102), 3:E3 (ts: 2102),        // 0:F0 (ts: 2103), 1:F1 (ts: 2103), 2:F2 (ts: 2103), 3:F3 (ts: 2103),        // 0:G0 (ts: 2104), 1:G1 (ts: 2104), 2:G2 (ts: 2104), 3:G3 (ts: 2104),        // 0:H0 (ts: 1899), 1:H1 (ts: 1899), 2:H2 (ts: 1899), 3:H3 (ts: 1899),        // 0:I0 (ts: 1900), 1:I1 (ts: 1900), 2:I2 (ts: 1900), 3:I3 (ts: 1900),        // 0:J0 (ts: 1901), 1:J1 (ts: 1901), 2:J2 (ts: 1901), 3:J3 (ts: 1901),        // 0:K0 (ts: 1902), 1:K1 (ts: 1902), 2:K2 (ts: 1902), 3:K3 (ts: 1902),        // 0:L0 (ts: 1903), 1:L1 (ts: 1903), 2:L2 (ts: 1903), 3:L3 (ts: 1903) }        // w2 = { 0:l0 (ts: 2000), 1:l1 (ts: 2001), 2:l2 (ts: 2002), 3:l3 (ts: 2003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "L" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "L0+l0", 2000), new KeyValueTimestamp<>(1, "L1+l1", 2001), new KeyValueTimestamp<>(2, "L2+l2", 2002), new KeyValueTimestamp<>(3, "L3+l3", 2003));    }}
f16369
0
testAsymmetricWindowingAfter
public void kafkatest_f16370_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KStream<Integer, String> stream1;    final KStream<Integer, String> stream2;    final KStream<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier = new MockProcessorSupplier<>();    stream1 = builder.stream(topic1, consumed);    stream2 = builder.stream(topic2, consumed);    joined = stream1.join(stream2, MockValueJoiner.TOSTRING_JOINER, JoinWindows.of(ofMillis(0)).after(ofMillis(100)), Joined.with(Serdes.Integer(), Serdes.String(), Serdes.String()));    joined.process(supplier);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> processor = supplier.theCapturedProcessor();        long time = 1000L;        // w2 = {}        for (int i = 0; i < expectedKeys.length; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "A" + expectedKeys[i], time + i));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items smaller timestamps (out of window) to the secondary stream; this should produce no items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = {}        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999) }        time = 1000L - 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "a" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);        // push four items with increased timestamps to the secondary stream; this should produce one item        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "b" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+b0", 1000));        // push four items with increased timestamps to the secondary stream; this should produce two items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "c" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+c0", 1001), new KeyValueTimestamp<>(1, "A1+c1", 1001));        // push four items with increased timestamps to the secondary stream; this should produce three items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "d" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+d0", 1002), new KeyValueTimestamp<>(1, "A1+d1", 1002), new KeyValueTimestamp<>(2, "A2+d2", 1002));        // push four items with increased timestamps to the secondary stream; this should produce four items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "e" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+e0", 1003), new KeyValueTimestamp<>(1, "A1+e1", 1003), new KeyValueTimestamp<>(2, "A2+e2", 1003), new KeyValueTimestamp<>(3, "A3+e3", 1003));        // push four items with larger timestamps to the secondary stream; this should produce four items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100) }        time = 1000 + 100L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "f" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(0, "A0+f0", 1100), new KeyValueTimestamp<>(1, "A1+f1", 1100), new KeyValueTimestamp<>(2, "A2+f2", 1100), new KeyValueTimestamp<>(3, "A3+f3", 1100));        // push four items with increased timestamps to the secondary stream; this should produce three items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "g" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(1, "A1+g1", 1101), new KeyValueTimestamp<>(2, "A2+g2", 1101), new KeyValueTimestamp<>(3, "A3+g3", 1101));        // push four items with increased timestamps to the secondary stream; this should produce two items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "h" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(2, "A2+h2", 1102), new KeyValueTimestamp<>(3, "A3+h3", 1102));        // push four items with increased timestamps to the secondary stream; this should produce one item        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102),        // 0:i0 (ts: 1103), 1:i1 (ts: 1103), 2:i2 (ts: 1103), 3:i3 (ts: 1103) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "i" + expectedKey, time));        }        processor.checkAndClearProcessResult(new KeyValueTimestamp<>(3, "A3+i3", 1103));        // push four items with increased timestamps (no out of window) to the secondary stream; this should produce no items        // w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102),        // 0:i0 (ts: 1103), 1:i1 (ts: 1103), 2:i2 (ts: 1103), 3:i3 (ts: 1103) }        // --> w1 = { 0:A0 (ts: 1000), 1:A1 (ts: 1001), 2:A2 (ts: 1002), 3:A3 (ts: 1003) }        // w2 = { 0:a0 (ts: 999), 1:a1 (ts: 999), 2:a2 (ts: 999), 3:a3 (ts: 999),        // 0:b0 (ts: 1000), 1:b1 (ts: 1000), 2:b2 (ts: 1000), 3:b3 (ts: 1000),        // 0:c0 (ts: 1001), 1:c1 (ts: 1001), 2:c2 (ts: 1001), 3:c3 (ts: 1001),        // 0:d0 (ts: 1002), 1:d1 (ts: 1002), 2:d2 (ts: 1002), 3:d3 (ts: 1002),        // 0:e0 (ts: 1003), 1:e1 (ts: 1003), 2:e2 (ts: 1003), 3:e3 (ts: 1003),        // 0:f0 (ts: 1100), 1:f1 (ts: 1100), 2:f2 (ts: 1100), 3:f3 (ts: 1100),        // 0:g0 (ts: 1101), 1:g1 (ts: 1101), 2:g2 (ts: 1101), 3:g3 (ts: 1101),        // 0:h0 (ts: 1102), 1:h1 (ts: 1102), 2:h2 (ts: 1102), 3:h3 (ts: 1102),        // 0:i0 (ts: 1103), 1:i1 (ts: 1103), 2:i2 (ts: 1103), 3:i3 (ts: 1103),        // 0:j0 (ts: 1104), 1:j1 (ts: 1104), 2:j2 (ts: 1104), 3:j3 (ts: 1104) }        time += 1L;        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "j" + expectedKey, time));        }        processor.checkAndClearProcessResult(EMPTY);    }}
f16370
0
pushToTable
private void kafkatest_f16379_0(final int messageCount, final String valuePrefix)
{    final Random r = new Random(System.currentTimeMillis());    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(tableTopic, expectedKeys[i], valuePrefix + expectedKeys[i], r.nextInt(Integer.MAX_VALUE)));    }}
f16379
0
pushNullValueToTable
private void kafkatest_f16380_0()
{    for (int i = 0; i < 2; i++) {        driver.pipeInput(recordFactory.create(tableTopic, expectedKeys[i], null));    }}
f16380
0
cleanup
public void kafkatest_f16389_0()
{    driver.close();}
f16389
0
pushToStream
private void kafkatest_f16390_0(final int messageCount, final String valuePrefix)
{    for (int i = 0; i < messageCount; i++) {        driver.pipeInput(recordFactory.create(streamTopic, expectedKeys[i], valuePrefix + expectedKeys[i], i));    }}
f16390
0
testTypeVariance
public void kafkatest_f16399_0()
{    new StreamsBuilder().<Integer, String>stream("numbers").map((key, value) -> KeyValue.pair(key, key + ":" + value)).to("strings");}
f16399
0
testFlatMapValues
public void kafkatest_f16400_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = { 1, 10, 100, 1000 };    final KStream<Integer, String> stream = builder.stream(topicName, Consumed.with(Serdes.Integer(), Serdes.String()));    stream.mapValues(CharSequence::length).process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topicName, expectedKey, Integer.toString(expectedKey), expectedKey / 2L));        }    }    final KeyValueTimestamp[] expected = { new KeyValueTimestamp<>(1, 1, 0), new KeyValueTimestamp<>(10, 2, 5), new KeyValueTimestamp<>(100, 3, 50), new KeyValueTimestamp<>(1000, 4, 500) };    assertArrayEquals(expected, supplier.theCapturedProcessor().processed.toArray());}
f16400
0
initializeStore
public void kafkatest_f16409_0()
{    final File stateDir = TestUtils.tempDirectory();    metrics = new Metrics();    final MockStreamsMetrics metrics = new MockStreamsMetrics(KStreamSessionWindowAggregateProcessorTest.this.metrics);    ThreadMetrics.skipRecordSensor(metrics);    context = new InternalMockProcessorContext(stateDir, Serdes.String(), Serdes.String(), metrics, new StreamsConfig(StreamsTestUtils.getStreamsConfig()), NoOpRecordCollector::new, new ThreadCache(new LogContext("testCache "), 100000, metrics)) {        @Override        public <K, V> void forward(final K key, final V value, final To to) {            toInternal.update(to);            results.add(new KeyValueTimestamp<>(key, value, toInternal.timestamp()));        }    };    initStore(true);    processor.init(context);}
f16409
0
forward
public void kafkatest_f16410_0(final K key, final V value, final To to)
{    toInternal.update(to);    results.add(new KeyValueTimestamp<>(key, value, toInternal.timestamp()));}
f16410
0
shouldGetAggregatedValuesFromValueGetter
public void kafkatest_f16419_0()
{    final KTableValueGetter<Windowed<String>, Long> getter = sessionAggregator.view().get();    getter.init(context);    context.setTime(0);    processor.process("a", "1");    context.setTime(GAP_MS + 1);    processor.process("a", "1");    processor.process("a", "2");    final long t0 = getter.get(new Windowed<>("a", new SessionWindow(0, 0))).value();    final long t1 = getter.get(new Windowed<>("a", new SessionWindow(GAP_MS + 1, GAP_MS + 1))).value();    assertEquals(1L, t0);    assertEquals(2L, t1);}
f16419
0
shouldImmediatelyForwardNewSessionWhenNonCachedStore
public void kafkatest_f16420_0()
{    initStore(false);    processor.init(context);    context.setTime(0);    processor.process("a", "1");    processor.process("b", "1");    processor.process("c", "1");    assertEquals(Arrays.asList(new KeyValueTimestamp<>(new Windowed<>("a", new SessionWindow(0, 0)), new Change<>(1L, null), 0L), new KeyValueTimestamp<>(new Windowed<>("b", new SessionWindow(0, 0)), new Change<>(1L, null), 0L), new KeyValueTimestamp<>(new Windowed<>("c", new SessionWindow(0, 0)), new Change<>(1L, null), 0L)), results);}
f16420
0
init
public void kafkatest_f16430_0(final ProcessorContext context)
{    context.schedule(Duration.ofMillis(1), PunctuationType.WALL_CLOCK_TIME, timestamp -> context.forward(-1, (int) timestamp));}
f16430
0
transform
public KeyValue<Integer, Integer> kafkatest_f16431_0(final Number key, final Number value)
{    total += value.intValue();    return KeyValue.pair(key.intValue() * 2, total);}
f16431
0
shouldLogAndMeterWhenSkippingExpiredWindow
public void kafkatest_f16445_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic = "topic";    final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));    stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String())).windowedBy(TimeWindows.of(ofMillis(10)).advanceBy(ofMillis(5)).until(100)).aggregate(() -> "", MockAggregator.toStringInstance("+"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as("topic1-Canonicalized").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()).toStream().map((key, value) -> new KeyValue<>(key.toString(), value)).to("output");    LogCaptureAppender.setClassLoggerToDebug(KStreamWindowAggregate.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, "k", "100", 100L));        driver.pipeInput(recordFactory.create(topic, "k", "0", 0L));        driver.pipeInput(recordFactory.create(topic, "k", "1", 1L));        driver.pipeInput(recordFactory.create(topic, "k", "2", 2L));        driver.pipeInput(recordFactory.create(topic, "k", "3", 3L));        driver.pipeInput(recordFactory.create(topic, "k", "4", 4L));        driver.pipeInput(recordFactory.create(topic, "k", "5", 5L));        driver.pipeInput(recordFactory.create(topic, "k", "6", 6L));        LogCaptureAppender.unregister(appender);        assertLatenessMetrics(driver, // how many events get dropped        is(7.0), // k:0 is 100ms late, since its time is 0, but it arrives at stream time 100.        is(100.0), // (0 + 100 + 99 + 98 + 97 + 96 + 95 + 94) / 8        is(84.875));        assertThat(appender.getMessages(), hasItems("Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[0] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[1] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[2] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[3] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[4] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[5] window=[0,10) expiration=[10] streamTime=[100]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[6] window=[0,10) expiration=[10] streamTime=[100]"));        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@95/105]", "+100", 100);        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@100/110]", "+100", 100);        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@5/15]", "+5", 5);        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@5/15]", "+5+6", 6);        assertThat(driver.readOutput("output"), nullValue());    }}
f16445
0
shouldLogAndMeterWhenSkippingExpiredWindowByGrace
public void kafkatest_f16446_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic = "topic";    final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));    stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String())).windowedBy(TimeWindows.of(ofMillis(10)).advanceBy(ofMillis(10)).grace(ofMillis(90L))).aggregate(() -> "", MockAggregator.toStringInstance("+"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as("topic1-Canonicalized").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()).toStream().map((key, value) -> new KeyValue<>(key.toString(), value)).to("output");    LogCaptureAppender.setClassLoggerToDebug(KStreamWindowAggregate.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, "k", "100", 200L));        driver.pipeInput(recordFactory.create(topic, "k", "0", 100L));        driver.pipeInput(recordFactory.create(topic, "k", "1", 101L));        driver.pipeInput(recordFactory.create(topic, "k", "2", 102L));        driver.pipeInput(recordFactory.create(topic, "k", "3", 103L));        driver.pipeInput(recordFactory.create(topic, "k", "4", 104L));        driver.pipeInput(recordFactory.create(topic, "k", "5", 105L));        driver.pipeInput(recordFactory.create(topic, "k", "6", 6L));        LogCaptureAppender.unregister(appender);        assertLatenessMetrics(driver, is(7.0), is(194.0), is(97.375));        assertThat(appender.getMessages(), hasItems("Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[100,110) expiration=[110] streamTime=[200]", "Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[6] window=[0,10) expiration=[110] streamTime=[200]"));        OutputVerifier.compareKeyValueTimestamp(getOutput(driver), "[k@200/210]", "+100", 200);        assertThat(driver.readOutput("output"), nullValue());    }}
f16446
0
setUp
public void kafkatest_f16455_0()
{    // disable caching at the config level    props.setProperty(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, "0");}
f16455
0
doTestKTable
private void kafkatest_f16456_0(final StreamsBuilder builder, final KTable<String, Integer> table2, final KTable<String, Integer> table3, final String topic)
{    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();    table2.toStream().process(supplier);    table3.toStream().process(supplier);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, "A", 1, 10L));        driver.pipeInput(recordFactory.create(topic, "B", 2, 5L));        driver.pipeInput(recordFactory.create(topic, "C", 3, 8L));        driver.pipeInput(recordFactory.create(topic, "D", 4, 14L));        driver.pipeInput(recordFactory.create(topic, "A", null, 18L));        driver.pipeInput(recordFactory.create(topic, "B", null, 15L));    }    final List<MockProcessor<String, Integer>> processors = supplier.capturedProcessors(2);    processors.get(0).checkAndClearProcessResult(new KeyValueTimestamp<>("A", null, 10), new KeyValueTimestamp<>("B", 2, 5), new KeyValueTimestamp<>("C", null, 8), new KeyValueTimestamp<>("D", 4, 14), new KeyValueTimestamp<>("A", null, 18), new KeyValueTimestamp<>("B", null, 15));    processors.get(1).checkAndClearProcessResult(new KeyValueTimestamp<>("A", 1, 10), new KeyValueTimestamp<>("B", null, 5), new KeyValueTimestamp<>("C", 3, 8), new KeyValueTimestamp<>("D", null, 14), new KeyValueTimestamp<>("A", null, 18), new KeyValueTimestamp<>("B", null, 15));}
f16456
0
shouldSendOldValuesWhenEnabledWithoutMaterialization
public void kafkatest_f16465_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTableImpl<String, Integer, Integer> table1 = (KTableImpl<String, Integer, Integer>) builder.table(topic1, consumed);    final KTableImpl<String, Integer, Integer> table2 = (KTableImpl<String, Integer, Integer>) table1.filter(predicate);    doTestSendingOldValue(builder, table1, table2, topic1);}
f16465
0
shouldSendOldValuesWhenEnabledOnMaterialization
public void kafkatest_f16466_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final KTableImpl<String, Integer, Integer> table1 = (KTableImpl<String, Integer, Integer>) builder.table(topic1, consumed);    final KTableImpl<String, Integer, Integer> table2 = (KTableImpl<String, Integer, Integer>) table1.filter(predicate, Materialized.as("store2"));    doTestSendingOldValue(builder, table1, table2, topic1);}
f16466
0
testStateStoreLazyEval
public void kafkatest_f16477_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final String topic2 = "topic2";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    builder.table(topic2, consumed);    final KTableImpl<String, String, Integer> table1Mapped = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new);    table1Mapped.filter((key, value) -> (value % 2) == 0);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        assertEquals(0, driver.getAllStateStores().size());    }}
f16477
0
testStateStore
public void kafkatest_f16478_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final String topic2 = "topic2";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    final KTableImpl<String, String, String> table2 = (KTableImpl<String, String, String>) builder.table(topic2, consumed);    final KTableImpl<String, String, Integer> table1Mapped = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new);    final KTableImpl<String, Integer, Integer> table1MappedFiltered = (KTableImpl<String, Integer, Integer>) table1Mapped.filter((key, value) -> (value % 2) == 0);    table2.join(table1MappedFiltered, (v1, v2) -> v1 + v2);    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        assertEquals(2, driver.getAllStateStores().size());    }}
f16478
0
shouldNotAllowNullOtherTableOnJoin
public void kafkatest_f16487_0()
{    table.join(null, MockValueJoiner.TOSTRING_JOINER);}
f16487
0
shouldAllowNullStoreInJoin
public void kafkatest_f16488_0()
{    table.join(table, MockValueJoiner.TOSTRING_JOINER);}
f16488
0
shouldThrowNullPointerOnLeftJoinWhenMaterializedIsNull
public void kafkatest_f16497_0()
{    table.leftJoin(table, MockValueJoiner.TOSTRING_JOINER, (Materialized) null);}
f16497
0
shouldThrowNullPointerOnOuterJoinWhenMaterializedIsNull
public void kafkatest_f16498_0()
{    table.outerJoin(table, MockValueJoiner.TOSTRING_JOINER, (Materialized) null);}
f16498
0
shouldLogAndMeterSkippedRecordsDueToNullLeftKey
public void kafkatest_f16507_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final Processor<String, Change<String>> join = new KTableKTableInnerJoin<>((KTableImpl<String, String, String>) builder.table("left", Consumed.with(Serdes.String(), Serdes.String())), (KTableImpl<String, String, String>) builder.table("right", Consumed.with(Serdes.String(), Serdes.String())), null).get();    final MockProcessorContext context = new MockProcessorContext();    context.setRecordMetadata("left", -1, -2, null, -3);    join.init(context);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    join.process(null, new Change<>("new", "old"));    LogCaptureAppender.unregister(appender);    assertEquals(1.0, getMetricByName(context.metrics().metrics(), "skipped-records-total", "stream-metrics").metricValue());    assertThat(appender.getMessages(), hasItem("Skipping record due to null key. change=[(new<-old)] topic=[left] partition=[-1] offset=[-2]"));}
f16507
0
doTestNotSendingOldValues
private void kafkatest_f16508_0(final StreamsBuilder builder, final int[] expectedKeys, final KTable<Integer, String> table1, final KTable<Integer, String> table2, final MockProcessorSupplier<Integer, String> supplier, final KTable<Integer, String> joined)
{    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        final MockProcessor<Integer, String> proc = supplier.theCapturedProcessor();        assertFalse(((KTableImpl<?, ?, ?>) table1).sendingOldValueEnabled());        assertFalse(((KTableImpl<?, ?, ?>) table2).sendingOldValueEnabled());        assertFalse(((KTableImpl<?, ?, ?>) joined).sendingOldValueEnabled());        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        proc.checkAndClearProcessResult(EMPTY);        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("X0+Y0", null), 5), new KeyValueTimestamp<>(1, new Change<>("X1+Y1", null), 10));        // push all four items to the primary stream. this should produce two items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+Y0", null), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+Y1", null), 10));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+YY0", null), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+YY1", null), 7), new KeyValueTimestamp<>(2, new Change<>("XX2+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("XX3+YY3", null), 15));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXX0+YY0", null), 6), new KeyValueTimestamp<>(1, new Change<>("XXX1+YY1", null), 6), new KeyValueTimestamp<>(2, new Change<>("XXX2+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("XXX3+YY3", null), 15));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>(null, null), 6), new KeyValueTimestamp<>(1, new Change<>(null, null), 7));        // push all four items to the primary stream. this should produce two items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(2, new Change<>("XXXX2+YY2", null), 13), new KeyValueTimestamp<>(3, new Change<>("XXXX3+YY3", null), 15));        // push four items to the primary stream with null. this should produce two items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(2, new Change<>(null, null), 10), new KeyValueTimestamp<>(3, new Change<>(null, null), 20));    }}
f16508
0
testJoin
public void kafkatest_f16517_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KTable<Integer, String> table1;    final KTable<Integer, String> table2;    final KTable<Integer, String> joined;    table1 = builder.table(topic1, consumed);    table2 = builder.table(topic2, consumed);    joined = table1.outerJoin(table2, MockValueJoiner.TOSTRING_JOINER);    joined.toStream().to(output);    final Collection<Set<String>> copartitionGroups = TopologyWrapper.getInternalTopologyBuilder(builder.build()).copartitionGroups();    assertEquals(1, copartitionGroups.size());    assertEquals(new HashSet<>(Arrays.asList(topic1, topic2)), copartitionGroups.iterator().next());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        assertOutputKeyValueTimestamp(driver, 0, "X0+null", 5L);        assertOutputKeyValueTimestamp(driver, 1, "X1+null", 6L);        assertNull(driver.readOutput(output));        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        assertOutputKeyValueTimestamp(driver, 0, "X0+Y0", 5L);        assertOutputKeyValueTimestamp(driver, 1, "X1+Y1", 10L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        assertOutputKeyValueTimestamp(driver, 0, "XX0+Y0", 7L);        assertOutputKeyValueTimestamp(driver, 1, "XX1+Y1", 10L);        assertOutputKeyValueTimestamp(driver, 2, "XX2+null", 7L);        assertOutputKeyValueTimestamp(driver, 3, "XX3+null", 7L);        assertNull(driver.readOutput(output));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XX0+YY0", 7L);        assertOutputKeyValueTimestamp(driver, 1, "XX1+YY1", 7L);        assertOutputKeyValueTimestamp(driver, 2, "XX2+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "XX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXX0+YY0", 6L);        assertOutputKeyValueTimestamp(driver, 1, "XXX1+YY1", 6L);        assertOutputKeyValueTimestamp(driver, 2, "XXX2+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "XXX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXX0+null", 6L);        assertOutputKeyValueTimestamp(driver, 1, "XXX1+null", 7L);        assertNull(driver.readOutput(output));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, "XXXX0+null", 13L);        assertOutputKeyValueTimestamp(driver, 1, "XXXX1+null", 13L);        assertOutputKeyValueTimestamp(driver, 2, "XXXX2+YY2", 13L);        assertOutputKeyValueTimestamp(driver, 3, "XXXX3+YY3", 15L);        assertNull(driver.readOutput(output));        // push four items to the primary stream with null. this should produce four items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        assertOutputKeyValueTimestamp(driver, 0, null, 0L);        assertOutputKeyValueTimestamp(driver, 1, null, 42L);        assertOutputKeyValueTimestamp(driver, 2, "null+YY2", 10L);        assertOutputKeyValueTimestamp(driver, 3, "null+YY3", 20L);        assertNull(driver.readOutput(output));    }}
f16517
0
testNotSendingOldValue
public void kafkatest_f16518_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final int[] expectedKeys = new int[] { 0, 1, 2, 3 };    final KTable<Integer, String> table1;    final KTable<Integer, String> table2;    final KTable<Integer, String> joined;    final MockProcessorSupplier<Integer, String> supplier;    table1 = builder.table(topic1, consumed);    table2 = builder.table(topic2, consumed);    joined = table1.outerJoin(table2, MockValueJoiner.TOSTRING_JOINER);    supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build().addProcessor("proc", supplier, ((KTableImpl<?, ?, ?>) joined).name);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        final MockProcessor<Integer, String> proc = supplier.theCapturedProcessor();        assertTrue(((KTableImpl<?, ?, ?>) table1).sendingOldValueEnabled());        assertTrue(((KTableImpl<?, ?, ?>) table2).sendingOldValueEnabled());        assertFalse(((KTableImpl<?, ?, ?>) joined).sendingOldValueEnabled());        // push two items to the primary stream. the other table is empty        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic1, expectedKeys[i], "X" + expectedKeys[i], 5L + i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic1, null, "SomeVal", 42L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right:        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("X0+null", null), 5), new KeyValueTimestamp<>(1, new Change<>("X1+null", null), 6));        // push two items to the other stream. this should produce two items.        for (int i = 0; i < 2; i++) {            driver.pipeInput(recordFactory.create(topic2, expectedKeys[i], "Y" + expectedKeys[i], 10L * i));        }        // pass tuple with null key, it will be discarded in join process        driver.pipeInput(recordFactory.create(topic2, null, "AnotherVal", 73L));        // left: X0:0 (ts: 5), X1:1 (ts: 6)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("X0+Y0", null), 5), new KeyValueTimestamp<>(1, new Change<>("X1+Y1", null), 10));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XX" + expectedKey, 7L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: Y0:0 (ts: 0), Y1:1 (ts: 10)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+Y0", null), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+Y1", null), 10), new KeyValueTimestamp<>(2, new Change<>("XX2+null", null), 7), new KeyValueTimestamp<>(3, new Change<>("XX3+null", null), 7));        // push all items to the other stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic2, expectedKey, "YY" + expectedKey, expectedKey * 5L));        }        // left: XX0:0 (ts: 7), XX1:1 (ts: 7), XX2:2 (ts: 7), XX3:3 (ts: 7)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XX0+YY0", null), 7), new KeyValueTimestamp<>(1, new Change<>("XX1+YY1", null), 7), new KeyValueTimestamp<>(2, new Change<>("XX2+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("XX3+YY3", null), 15));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXX" + expectedKey, 6L));        }        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY0:0 (ts: 0), YY1:1 (ts: 5), YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXX0+YY0", null), 6), new KeyValueTimestamp<>(1, new Change<>("XXX1+YY1", null), 6), new KeyValueTimestamp<>(2, new Change<>("XXX2+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("XXX3+YY3", null), 15));        // push two items with null to the other stream as deletes. this should produce two item.        driver.pipeInput(recordFactory.create(topic2, expectedKeys[0], null, 5L));        driver.pipeInput(recordFactory.create(topic2, expectedKeys[1], null, 7L));        // left: XXX0:0 (ts: 6), XXX1:1 (ts: 6), XXX2:2 (ts: 6), XXX3:3 (ts: 6)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXX0+null", null), 6), new KeyValueTimestamp<>(1, new Change<>("XXX1+null", null), 7));        // push all four items to the primary stream. this should produce four items.        for (final int expectedKey : expectedKeys) {            driver.pipeInput(recordFactory.create(topic1, expectedKey, "XXXX" + expectedKey, 13L));        }        // left: XXXX0:0 (ts: 13), XXXX1:1 (ts: 13), XXXX2:2 (ts: 13), XXXX3:3 (ts: 13)        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>("XXXX0+null", null), 13), new KeyValueTimestamp<>(1, new Change<>("XXXX1+null", null), 13), new KeyValueTimestamp<>(2, new Change<>("XXXX2+YY2", null), 13), new KeyValueTimestamp<>(3, new Change<>("XXXX3+YY3", null), 15));        // push four items to the primary stream with null. this should produce four items.        driver.pipeInput(recordFactory.create(topic1, expectedKeys[0], null, 0L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[1], null, 42L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[2], null, 5L));        driver.pipeInput(recordFactory.create(topic1, expectedKeys[3], null, 20L));        // left:        // right: YY2:2 (ts: 10), YY3:3 (ts: 15)        proc.checkAndClearProcessResult(new KeyValueTimestamp<>(0, new Change<>(null, null), 0), new KeyValueTimestamp<>(1, new Change<>(null, null), 42), new KeyValueTimestamp<>(2, new Change<>("null+YY2", null), 10), new KeyValueTimestamp<>(3, new Change<>("null+YY3", null), 20));    }}
f16518
0
doTestValueGetter
private void kafkatest_f16527_0(final StreamsBuilder builder, final String topic1, final KTableImpl<String, String, Integer> table2, final KTableImpl<String, String, Integer> table3)
{    final Topology topology = builder.build();    final KTableValueGetterSupplier<String, Integer> getterSupplier2 = table2.valueGetterSupplier();    final KTableValueGetterSupplier<String, Integer> getterSupplier3 = table3.valueGetterSupplier();    final InternalTopologyBuilder topologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology);    topologyBuilder.connectProcessorAndStateStores(table2.name, getterSupplier2.storeNames());    topologyBuilder.connectProcessorAndStateStores(table3.name, getterSupplier3.storeNames());    try (final TopologyTestDriverWrapper driver = new TopologyTestDriverWrapper(builder.build(), props)) {        final KTableValueGetter<String, Integer> getter2 = getterSupplier2.get();        final KTableValueGetter<String, Integer> getter3 = getterSupplier3.get();        getter2.init(driver.setCurrentNodeForProcessorContext(table2.name));        getter3.init(driver.setCurrentNodeForProcessorContext(table3.name));        driver.pipeInput(recordFactory.create(topic1, "A", "01", 50L));        driver.pipeInput(recordFactory.create(topic1, "B", "01", 10L));        driver.pipeInput(recordFactory.create(topic1, "C", "01", 30L));        assertEquals(ValueAndTimestamp.make(1, 50L), getter2.get("A"));        assertEquals(ValueAndTimestamp.make(1, 10L), getter2.get("B"));        assertEquals(ValueAndTimestamp.make(1, 30L), getter2.get("C"));        assertEquals(ValueAndTimestamp.make(-1, 50L), getter3.get("A"));        assertEquals(ValueAndTimestamp.make(-1, 10L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(-1, 30L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", "02", 25L));        driver.pipeInput(recordFactory.create(topic1, "B", "02", 20L));        assertEquals(ValueAndTimestamp.make(2, 25L), getter2.get("A"));        assertEquals(ValueAndTimestamp.make(2, 20L), getter2.get("B"));        assertEquals(ValueAndTimestamp.make(1, 30L), getter2.get("C"));        assertEquals(ValueAndTimestamp.make(-2, 25L), getter3.get("A"));        assertEquals(ValueAndTimestamp.make(-2, 20L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(-1, 30L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", "03", 35L));        assertEquals(ValueAndTimestamp.make(3, 35L), getter2.get("A"));        assertEquals(ValueAndTimestamp.make(2, 20L), getter2.get("B"));        assertEquals(ValueAndTimestamp.make(1, 30L), getter2.get("C"));        assertEquals(ValueAndTimestamp.make(-3, 35L), getter3.get("A"));        assertEquals(ValueAndTimestamp.make(-2, 20L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(-1, 30L), getter3.get("C"));        driver.pipeInput(recordFactory.create(topic1, "A", (String) null, 1L));        assertNull(getter2.get("A"));        assertEquals(ValueAndTimestamp.make(2, 20L), getter2.get("B"));        assertEquals(ValueAndTimestamp.make(1, 30L), getter2.get("C"));        assertNull(getter3.get("A"));        assertEquals(ValueAndTimestamp.make(-2, 20L), getter3.get("B"));        assertEquals(ValueAndTimestamp.make(-1, 30L), getter3.get("C"));    }}
f16527
0
testQueryableValueGetter
public void kafkatest_f16528_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    final String storeName2 = "store2";    final String storeName3 = "store3";    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, consumed);    final KTableImpl<String, String, Integer> table2 = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>as(storeName2).withValueSerde(Serdes.Integer()));    final KTableImpl<String, String, Integer> table3 = (KTableImpl<String, String, Integer>) table1.mapValues(value -> new Integer(value) * (-1), Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>as(storeName3).withValueSerde(Serdes.Integer()));    final KTableImpl<String, String, Integer> table4 = (KTableImpl<String, String, Integer>) table1.mapValues(Integer::new);    assertEquals(storeName2, table2.queryableStoreName());    assertEquals(storeName3, table3.queryableStoreName());    assertNull(table4.queryableStoreName());    doTestValueGetter(builder, topic1, table2, table3);}
f16528
0
testNotSendingOldValue
public void kafkatest_f16537_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    @SuppressWarnings("unchecked")    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, stringConsumed);    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build().addProcessor("proc1", supplier, table1.name);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        final MockProcessor<String, Integer> proc1 = supplier.theCapturedProcessor();        driver.pipeInput(recordFactory.create(topic1, "A", "01", 10L));        driver.pipeInput(recordFactory.create(topic1, "B", "01", 20L));        driver.pipeInput(recordFactory.create(topic1, "C", "01", 15L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("01", null), 10), new KeyValueTimestamp<>("B", new Change<>("01", null), 20), new KeyValueTimestamp<>("C", new Change<>("01", null), 15));        driver.pipeInput(recordFactory.create(topic1, "A", "02", 8L));        driver.pipeInput(recordFactory.create(topic1, "B", "02", 22L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("02", null), 8), new KeyValueTimestamp<>("B", new Change<>("02", null), 22));        driver.pipeInput(recordFactory.create(topic1, "A", "03", 12L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("03", null), 12));        driver.pipeInput(recordFactory.create(topic1, "A", (String) null, 15L));        driver.pipeInput(recordFactory.create(topic1, "B", (String) null, 20L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(null, null), 15), new KeyValueTimestamp<>("B", new Change<>(null, null), 20));    }}
f16537
0
testSendingOldValue
public void kafkatest_f16538_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final String topic1 = "topic1";    @SuppressWarnings("unchecked")    final KTableImpl<String, String, String> table1 = (KTableImpl<String, String, String>) builder.table(topic1, stringConsumed);    table1.enableSendingOldValues();    assertTrue(table1.sendingOldValueEnabled());    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();    final Topology topology = builder.build().addProcessor("proc1", supplier, table1.name);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, props)) {        final MockProcessor<String, Integer> proc1 = supplier.theCapturedProcessor();        driver.pipeInput(recordFactory.create(topic1, "A", "01", 10L));        driver.pipeInput(recordFactory.create(topic1, "B", "01", 20L));        driver.pipeInput(recordFactory.create(topic1, "C", "01", 15L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("01", null), 10), new KeyValueTimestamp<>("B", new Change<>("01", null), 20), new KeyValueTimestamp<>("C", new Change<>("01", null), 15));        driver.pipeInput(recordFactory.create(topic1, "A", "02", 8L));        driver.pipeInput(recordFactory.create(topic1, "B", "02", 22L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("02", "01"), 8), new KeyValueTimestamp<>("B", new Change<>("02", "01"), 22));        driver.pipeInput(recordFactory.create(topic1, "A", "03", 12L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>("03", "02"), 12));        driver.pipeInput(recordFactory.create(topic1, "A", (String) null, 15L));        driver.pipeInput(recordFactory.create(topic1, "B", (String) null, 20L));        proc1.checkAndClearProcessResult(new KeyValueTimestamp<>("A", new Change<>(null, "03"), 15), new KeyValueTimestamp<>("B", new Change<>(null, "02"), 20));    }}
f16538
0
shouldTransformOnGetIfNotMaterialized
public void kafkatest_f16547_0()
{    final KTableTransformValues<String, String, String> transformValues = new KTableTransformValues<>(parent, new ExclamationValueTransformerSupplier(), null);    expect(parent.valueGetterSupplier()).andReturn(parentGetterSupplier);    expect(parentGetterSupplier.get()).andReturn(parentGetter);    expect(parentGetter.get("Key")).andReturn(ValueAndTimestamp.make("Value", -1L));    replay(parent, parentGetterSupplier, parentGetter);    final KTableValueGetter<String, String> getter = transformValues.view().get();    getter.init(context);    final String result = getter.get("Key").value();    assertThat(result, is("Key->Value!"));}
f16547
0
shouldGetFromStateStoreIfMaterialized
public void kafkatest_f16548_0()
{    final KTableTransformValues<String, String, String> transformValues = new KTableTransformValues<>(parent, new ExclamationValueTransformerSupplier(), QUERYABLE_NAME);    expect(context.getStateStore(QUERYABLE_NAME)).andReturn(stateStore);    expect(stateStore.get("Key")).andReturn(ValueAndTimestamp.make("something", 0L));    replay(context, stateStore);    final KTableValueGetter<String, String> getter = transformValues.view().get();    getter.init(context);    final String result = getter.get("Key").value();    assertThat(result, is("something"));}
f16548
0
shouldCalculateCorrectOldValuesIfNotStatefulEvenIfNotMaterialized
public void kafkatest_f16557_0()
{    builder.table(INPUT_TOPIC, CONSUMED).transformValues(new StatelessTransformerSupplier()).groupBy(toForceSendingOfOldValues(), Grouped.with(Serdes.String(), Serdes.Integer())).reduce(MockReducer.INTEGER_ADDER, MockReducer.INTEGER_SUBTRACTOR).mapValues(mapBackToStrings()).toStream().process(capture);    driver = new TopologyTestDriver(builder.build(), props());    driver.pipeInput(recordFactory.create(INPUT_TOPIC, "A", "a", 5L));    driver.pipeInput(recordFactory.create(INPUT_TOPIC, "A", "aa", 15L));    driver.pipeInput(recordFactory.create(INPUT_TOPIC, "A", "aaa", 10));    assertThat(output(), hasItems(new KeyValueTimestamp<>("A", "1", 5), new KeyValueTimestamp<>("A", "0", 15), new KeyValueTimestamp<>("A", "2", 15), new KeyValueTimestamp<>("A", "0", 15), new KeyValueTimestamp<>("A", "3", 15)));}
f16557
0
output
private ArrayList<KeyValueTimestamp<Object, Object>> kafkatest_f16558_0()
{    return capture.capturedProcessors(1).get(0).processed;}
f16558
0
get
public ValueTransformerWithKey<String, String, String> kafkatest_f16568_0()
{    return null;}
f16568
0
get
public ValueTransformerWithKey<String, String, Integer> kafkatest_f16569_0()
{    return new StatefulTransformer();}
f16569
0
shouldSuffixNameOrReturnProviderValue
public void kafkatest_f16582_0()
{    final String name = "foo";    final TestNameProvider provider = new TestNameProvider();    assertEquals(name + TEST_SUFFIX, NamedInternal.with(name).suffixWithOrElseGet(TEST_SUFFIX, provider, TEST_PREFIX));    // 1, not 0, indicates that the named call still burned an index number.    assertEquals("prefix-PROCESSOR-1", NamedInternal.with(null).suffixWithOrElseGet(TEST_SUFFIX, provider, TEST_PREFIX));}
f16582
0
shouldGenerateWithPrefixGivenEmptyName
public void kafkatest_f16583_0()
{    final String prefix = "KSTREAM-MAP-";    assertEquals(prefix + "PROCESSOR-0", NamedInternal.with(null).orElseGenerateWithPrefix(new TestNameProvider(), prefix));}
f16583
0
shouldCountSessionWindowedWithCachingDisabled
public void kafkatest_f16592_0()
{    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    shouldCountSessionWindowed();}
f16592
0
shouldCountSessionWindowedWithCachingEnabled
public void kafkatest_f16593_0()
{    shouldCountSessionWindowed();}
f16593
0
shouldThrowNullPointerOnAggregateIfMergerIsNull
public void kafkatest_f16602_0()
{    stream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, null);}
f16602
0
shouldThrowNullPointerOnReduceIfReducerIsNull
public void kafkatest_f16603_0()
{    stream.reduce(null);}
f16603
0
shouldNotOverlapIfOtherWindowIsBeforeThisWindow
public void kafkatest_f16612_0()
{    /*         * This:        [-------]         * Other: [---]         */    assertFalse(window.overlap(new SessionWindow(0, 25)));    assertFalse(window.overlap(new SessionWindow(0, start - 1)));    assertFalse(window.overlap(new SessionWindow(start - 1, start - 1)));}
f16612
0
shouldOverlapIfOtherWindowEndIsWithinThisWindow
public void kafkatest_f16613_0()
{    /*         * This:        [-------]         * Other: [---------]         */    assertTrue(window.overlap(new SessionWindow(0, start)));    assertTrue(window.overlap(new SessionWindow(0, start + 1)));    assertTrue(window.overlap(new SessionWindow(0, 75)));    assertTrue(window.overlap(new SessionWindow(0, end - 1)));    assertTrue(window.overlap(new SessionWindow(0, end)));    assertTrue(window.overlap(new SessionWindow(start - 1, start)));    assertTrue(window.overlap(new SessionWindow(start - 1, start + 1)));    assertTrue(window.overlap(new SessionWindow(start - 1, 75)));    assertTrue(window.overlap(new SessionWindow(start - 1, end - 1)));    assertTrue(window.overlap(new SessionWindow(start - 1, end)));}
f16613
0
windowedZeroTimeLimitShouldImmediatelyEmit
public void kafkatest_f16622_0()
{    final Harness<Windowed<String>, Long> harness = new Harness<>(untilTimeLimit(ZERO, unbounded()), timeWindowedSerdeFrom(String.class, 100L), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = ARBITRARY_LONG;    context.setRecordMetadata("", 0, 0L, null, timestamp);    final Windowed<String> key = new Windowed<>("hey", new TimeWindow(0L, 100L));    final Change<Long> value = ARBITRARY_CHANGE;    harness.processor.process(key, value);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
f16622
0
intermediateSuppressionShouldBufferAndEmitLater
public void kafkatest_f16623_0()
{    final Harness<String, Long> harness = new Harness<>(untilTimeLimit(ofMillis(1), unbounded()), String(), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 0L;    context.setRecordMetadata("topic", 0, 0, null, timestamp);    final String key = "hey";    final Change<Long> value = new Change<>(null, 1L);    harness.processor.process(key, value);    assertThat(context.forwarded(), hasSize(0));    context.setRecordMetadata("topic", 0, 1, null, 1L);    harness.processor.process("tick", new Change<>(null, null));    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
f16623
0
suppressShouldEmitWhenOverRecordCapacity
public void kafkatest_f16632_0()
{    final Harness<String, Long> harness = new Harness<>(untilTimeLimit(Duration.ofDays(100), maxRecords(1)), String(), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 100L;    context.setRecordMetadata("", 0, 0L, null, timestamp);    final String key = "hey";    final Change<Long> value = new Change<>(null, ARBITRARY_LONG);    harness.processor.process(key, value);    context.setRecordMetadata("", 0, 1L, null, timestamp + 1);    harness.processor.process("dummyKey", value);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
f16632
0
suppressShouldEmitWhenOverByteCapacity
public void kafkatest_f16633_0()
{    final Harness<String, Long> harness = new Harness<>(untilTimeLimit(Duration.ofDays(100), maxBytes(60L)), String(), Long());    final MockInternalProcessorContext context = harness.context;    final long timestamp = 100L;    context.setRecordMetadata("", 0, 0L, null, timestamp);    final String key = "hey";    final Change<Long> value = new Change<>(null, ARBITRARY_LONG);    harness.processor.process(key, value);    context.setRecordMetadata("", 0, 1L, null, timestamp + 1);    harness.processor.process("dummyKey", value);    assertThat(context.forwarded(), hasSize(1));    final MockProcessorContext.CapturedForward capturedForward = context.forwarded().get(0);    assertThat(capturedForward.keyValue(), is(new KeyValue<>(key, value)));    assertThat(capturedForward.timestamp(), is(timestamp));}
f16633
0
shouldSuppressIntermediateEventsWithTimeLimit
public void kafkatest_f16642_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<String, Long> valueCounts = builder.table("input", Consumed.with(STRING_SERDE, STRING_SERDE), Materialized.<String, String, KeyValueStore<Bytes, byte[]>>with(STRING_SERDE, STRING_SERDE).withCachingDisabled().withLoggingDisabled()).groupBy((k, v) -> new KeyValue<>(v, k), Grouped.with(STRING_SERDE, STRING_SERDE)).count();    valueCounts.suppress(untilTimeLimit(ofMillis(2L), unbounded())).toStream().to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().to("output-raw", Produced.with(STRING_SERDE, Serdes.Long()));    final Topology topology = builder.build();    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(STRING_SERIALIZER, STRING_SERIALIZER);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, config)) {        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v2", 1L));        driver.pipeInput(recordFactory.create("input", "k2", "v1", 2L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("v1", 1L, 0L), new KeyValueTimestamp<>("v1", 0L, 1L), new KeyValueTimestamp<>("v2", 1L, 1L), new KeyValueTimestamp<>("v1", 1L, 2L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("v1", 1L, 2L)));        // inserting a dummy "tick" record just to advance stream time        driver.pipeInput(recordFactory.create("input", "tick", "tick", 3L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("tick", 1L, 3L)));        // the stream time is now 3, so it's time to emit this record        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("v2", 1L, 1L)));        driver.pipeInput(recordFactory.create("input", "tick", "tick", 4L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("tick", 0L, 4L), new KeyValueTimestamp<>("tick", 1L, 4L)));        // tick is still buffered, since it was first inserted at time 3, and it is only time 4 right now.        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), emptyList());    }}
f16642
0
shouldSuppressIntermediateEventsWithRecordLimit
public void kafkatest_f16643_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KTable<String, Long> valueCounts = builder.table("input", Consumed.with(STRING_SERDE, STRING_SERDE), Materialized.<String, String, KeyValueStore<Bytes, byte[]>>with(STRING_SERDE, STRING_SERDE).withCachingDisabled().withLoggingDisabled()).groupBy((k, v) -> new KeyValue<>(v, k), Grouped.with(STRING_SERDE, STRING_SERDE)).count(Materialized.with(STRING_SERDE, Serdes.Long()));    valueCounts.suppress(untilTimeLimit(ofMillis(Long.MAX_VALUE), maxRecords(1L).emitEarlyWhenFull())).toStream().to("output-suppressed", Produced.with(STRING_SERDE, Serdes.Long()));    valueCounts.toStream().to("output-raw", Produced.with(STRING_SERDE, Serdes.Long()));    final Topology topology = builder.build();    System.out.println(topology.describe());    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(STRING_SERIALIZER, STRING_SERIALIZER);    try (final TopologyTestDriver driver = new TopologyTestDriver(topology, config)) {        driver.pipeInput(recordFactory.create("input", "k1", "v1", 0L));        driver.pipeInput(recordFactory.create("input", "k1", "v2", 1L));        driver.pipeInput(recordFactory.create("input", "k2", "v1", 2L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(new KeyValueTimestamp<>("v1", 1L, 0L), new KeyValueTimestamp<>("v1", 0L, 1L), new KeyValueTimestamp<>("v2", 1L, 1L), new KeyValueTimestamp<>("v1", 1L, 2L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), asList(// consecutive updates to v1 get suppressed into only the latter.        new KeyValueTimestamp<>("v1", 0L, 1L), new KeyValueTimestamp<>("v2", 1L, 1L)));        driver.pipeInput(recordFactory.create("input", "x", "x", 3L));        verify(drainProducerRecords(driver, "output-raw", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(new KeyValueTimestamp<>("x", 1L, 3L)));        verify(drainProducerRecords(driver, "output-suppressed", STRING_DESERIALIZER, LONG_DESERIALIZER), singletonList(// now we see that last update to v1, but we won't see the update to x until it gets evicted        new KeyValueTimestamp<>("v1", 1L, 2L)));    }}
f16643
0
drainProducerRecords
private static List<ProducerRecord<K, V>> kafkatest_f16652_0(final TopologyTestDriver driver, final String topic, final Deserializer<K> keyDeserializer, final Deserializer<V> valueDeserializer)
{    final List<ProducerRecord<K, V>> result = new LinkedList<>();    for (ProducerRecord<K, V> next = driver.readOutput(topic, keyDeserializer, valueDeserializer); next != null; next = driver.readOutput(topic, keyDeserializer, valueDeserializer)) {        result.add(next);    }    return new ArrayList<>(result);}
f16652
0
printRecords
private static String kafkatest_f16653_0(final List<ProducerRecord<K, V>> result)
{    final StringBuilder resultStr = new StringBuilder();    resultStr.append("[\n");    for (final ProducerRecord<?, ?> record : result) {        resultStr.append("  ").append(record).append("\n");    }    resultStr.append("]");    return resultStr.toString();}
f16653
0
shouldForwardRecordsIfWrappedStateStoreDoesNotCache
public void kafkatest_f16662_0()
{    shouldForwardRecordsIfWrappedStateStoreDoesNotCache(false);    shouldForwardRecordsIfWrappedStateStoreDoesNotCache(true);}
f16662
0
shouldForwardRecordsIfWrappedStateStoreDoesNotCache
private void kafkatest_f16663_0(final boolean sendOldValues)
{    final WrappedStateStore<StateStore, String, String> store = mock(WrappedStateStore.class);    final ProcessorContext context = mock(ProcessorContext.class);    expect(store.setFlushListener(null, sendOldValues)).andReturn(false);    if (sendOldValues) {        context.forward("key1", new Change<>("newValue1", "oldValue1"));        context.forward("key2", new Change<>("newValue2", "oldValue2"), To.all().withTimestamp(42L));    } else {        context.forward("key1", new Change<>("newValue1", null));        context.forward("key2", new Change<>("newValue2", null), To.all().withTimestamp(42L));    }    expectLastCall();    replay(store, context);    final TimestampedTupleForwarder<String, String> forwarder = new TimestampedTupleForwarder<>(store, context, null, sendOldValues);    forwarder.maybeForward("key1", "newValue1", "oldValue1");    forwarder.maybeForward("key2", "newValue2", "oldValue2", 42L);    verify(store, context);}
f16663
0
shouldThrowNullPointerOnAggregateIfInitializerIsNull
public void kafkatest_f16672_0()
{    windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER);}
f16672
0
shouldThrowNullPointerOnAggregateIfAggregatorIsNull
public void kafkatest_f16673_0()
{    windowedStream.aggregate(MockInitializer.STRING_INIT, null);}
f16673
0
endMustBeLargerThanStart
public void kafkatest_f16682_0()
{    new TimeWindow(start, start);}
f16682
0
shouldNotOverlapIfOtherWindowIsBeforeThisWindow
public void kafkatest_f16683_0()
{    /*         * This:        [-------)         * Other: [-----)         */    assertFalse(window.overlap(new TimeWindow(0, 25)));    assertFalse(window.overlap(new TimeWindow(0, start - 1)));    assertFalse(window.overlap(new TimeWindow(0, start)));}
f16683
0
shouldCallInitOfAdapteeTransformer
public void kafkatest_f16692_0()
{    EasyMock.expect(transformerSupplier.get()).andReturn(transformer);    transformer.init(context);    replayAll();    final TransformerSupplierAdapter<String, String, Integer, Integer> adapter = new TransformerSupplierAdapter<>(transformerSupplier);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adaptedTransformer = adapter.get();    adaptedTransformer.init(context);    verifyAll();}
f16692
0
shouldCallCloseOfAdapteeTransformer
public void kafkatest_f16693_0()
{    EasyMock.expect(transformerSupplier.get()).andReturn(transformer);    transformer.close();    replayAll();    final TransformerSupplierAdapter<String, String, Integer, Integer> adapter = new TransformerSupplierAdapter<>(transformerSupplier);    final Transformer<String, String, Iterable<KeyValue<Integer, Integer>>> adaptedTransformer = adapter.get();    adaptedTransformer.close();    verifyAll();}
f16693
0
endTimeShouldNotBeBeforeStart
public void kafkatest_f16702_0()
{    final JoinWindows windowSpec = JoinWindows.of(ofMillis(ANY_SIZE));    try {        windowSpec.after(ofMillis(-ANY_SIZE - 1));        fail("window end time should not be before window start time");    } catch (final IllegalArgumentException e) {    // expected    }}
f16702
0
startTimeShouldNotBeAfterEnd
public void kafkatest_f16703_0()
{    final JoinWindows windowSpec = JoinWindows.of(ofMillis(ANY_SIZE));    try {        windowSpec.before(ofMillis(-ANY_SIZE - 1));        fail("window start time should not be after window end time");    } catch (final IllegalArgumentException e) {    // expected    }}
f16703
0
shouldThrowNullPointerIfKeyValueBytesStoreSupplierIsNull
public void kafkatest_f16712_0()
{    Materialized.as((KeyValueBytesStoreSupplier) null);}
f16712
0
shouldThrowNullPointerIfSessionBytesStoreSupplierIsNull
public void kafkatest_f16713_0()
{    Materialized.as((SessionBytesStoreSupplier) null);}
f16713
0
apply
public String kafkatest_f16722_0(final String key, final Integer value)
{    return String.format("%s -> %d", key, value);}
f16722
0
shouldThrowNullPointerExceptionIfFilePathIsNull
public void kafkatest_f16723_0()
{    Printed.toFile(null);}
f16723
0
shouldNotFailWithSameRepartitionTopicNameUsingSameSessionWindowStream
public void kafkatest_f16732_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KGroupedStream<String, String> kGroupedStream = builder.<String, String>stream("topic").selectKey((k, v) -> k).groupByKey(Grouped.as("grouping"));    final SessionWindowedKStream<String, String> sessionWindowedKStream = kGroupedStream.windowedBy(SessionWindows.with(Duration.ofMillis(10L)));    sessionWindowedKStream.count().toStream().to("output-one");    sessionWindowedKStream.reduce((v, v2) -> v + v2).toStream().to("output-two");    kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(30L))).count().toStream().to("output-two");    final String topologyString = builder.build().describe().toString();    assertThat(1, is(getCountOfRepartitionTopicsFound(topologyString, repartitionTopicPattern)));    assertTrue(topologyString.contains("grouping-repartition"));}
f16732
0
shouldNotFailWithSameRepartitionTopicNameUsingSameKGroupedTable
public void kafkatest_f16733_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final KGroupedTable<String, String> kGroupedTable = builder.<String, String>table("topic").groupBy(KeyValue::pair, Grouped.as("grouping"));    kGroupedTable.count().toStream().to("output-count");    kGroupedTable.reduce((v, v2) -> v2, (v, v2) -> v2).toStream().to("output-reduce");    final String topologyString = builder.build().describe().toString();    assertThat(1, is(getCountOfRepartitionTopicsFound(topologyString, repartitionTopicPattern)));    assertTrue(topologyString.contains("grouping-repartition"));}
f16733
0
shouldKeepRepartitionTopicNameForGroupByKeyNoWindows
public void kafkatest_f16742_0()
{    final String expectedNoWindowRepartitionTopic = "(topic: kstream-grouping-repartition)";    final String noWindowGroupingRepartitionTopology = buildStreamGroupByKeyNoWindows(false, true);    assertTrue(noWindowGroupingRepartitionTopology.contains(expectedNoWindowRepartitionTopic));    final String noWindowGroupingUpdatedTopology = buildStreamGroupByKeyNoWindows(true, true);    assertTrue(noWindowGroupingUpdatedTopology.contains(expectedNoWindowRepartitionTopic));}
f16742
0
shouldKeepRepartitionTopicNameForGroupByNoWindows
public void kafkatest_f16743_0()
{    final String expectedNoWindowRepartitionTopic = "(topic: kstream-grouping-repartition)";    final String noWindowGroupingRepartitionTopology = buildStreamGroupByKeyNoWindows(false, false);    assertTrue(noWindowGroupingRepartitionTopology.contains(expectedNoWindowRepartitionTopic));    final String noWindowGroupingUpdatedTopology = buildStreamGroupByKeyNoWindows(true, false);    assertTrue(noWindowGroupingUpdatedTopology.contains(expectedNoWindowRepartitionTopic));}
f16743
0
getCountOfRepartitionTopicsFound
private int kafkatest_f16752_0(final String topologyString, final Pattern repartitionTopicPattern)
{    final Matcher matcher = repartitionTopicPattern.matcher(topologyString);    final List<String> repartitionTopicsFound = new ArrayList<>();    while (matcher.find()) {        repartitionTopicsFound.add(matcher.group());    }    return repartitionTopicsFound.size();}
f16752
0
buildTopology
private Topology kafkatest_f16753_0(final String optimizationConfig)
{    final Initializer<Integer> initializer = () -> 0;    final Aggregator<String, String, Integer> aggregator = (k, v, agg) -> agg + v.length();    final Reducer<String> reducer = (v1, v2) -> v1 + ":" + v2;    final List<String> processorValueCollector = new ArrayList<>();    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, String> sourceStream = builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()));    final KStream<String, String> mappedStream = sourceStream.map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v));    mappedStream.filter((k, v) -> k.equals("B")).mapValues(v -> v.toUpperCase(Locale.getDefault())).process(() -> new SimpleProcessor(processorValueCollector));    final KStream<String, Long> countStream = mappedStream.groupByKey(Grouped.as(firstRepartitionTopicName)).count(Materialized.with(Serdes.String(), Serdes.Long())).toStream();    countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()));    mappedStream.groupByKey(Grouped.as(secondRepartitionTopicName)).aggregate(initializer, aggregator, Materialized.with(Serdes.String(), Serdes.Integer())).toStream().to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()));    // adding operators for case where the repartition node is further downstream    mappedStream.filter((k, v) -> true).peek((k, v) -> System.out.println(k + ":" + v)).groupByKey(Grouped.as(thirdRepartitionTopicName)).reduce(reducer, Materialized.with(Serdes.String(), Serdes.String())).toStream().to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));    mappedStream.filter((k, v) -> k.equals("A")).join(countStream, (v1, v2) -> v1 + ":" + v2.toString(), JoinWindows.of(Duration.ofMillis(5000L)), Joined.with(Serdes.String(), Serdes.String(), Serdes.Long(), fourthRepartitionTopicName)).to(JOINED_TOPIC);    final Properties properties = new Properties();    properties.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);    return builder.build(properties);}
f16753
0
shouldSetWindowGraceTime
public void kafkatest_f16762_0()
{    final long anyRetentionTime = 42L;    assertEquals(anyRetentionTime, SessionWindows.with(ofMillis(1)).grace(ofMillis(anyRetentionTime)).gracePeriodMs());}
f16762
0
gracePeriodShouldEnforceBoundaries
public void kafkatest_f16763_0()
{    SessionWindows.with(ofMillis(3L)).grace(ofMillis(0));    try {        SessionWindows.with(ofMillis(3L)).grace(ofMillis(-1L));        fail("should not accept negatives");    } catch (final IllegalArgumentException e) {    // expected    }}
f16763
0
finalEventsShouldAcceptStrictBuffersAndSetBounds
public void kafkatest_f16772_0()
{    assertThat(untilWindowCloses(unbounded()), is(new FinalResultsSuppressionBuilder<>(null, unbounded())));    assertThat(untilWindowCloses(maxRecords(2L).shutDownWhenFull()), is(new FinalResultsSuppressionBuilder<>(null, new StrictBufferConfigImpl(2L, MAX_VALUE, SHUT_DOWN))));    assertThat(untilWindowCloses(maxBytes(2L).shutDownWhenFull()), is(new FinalResultsSuppressionBuilder<>(null, new StrictBufferConfigImpl(MAX_VALUE, 2L, SHUT_DOWN))));    assertThat(untilWindowCloses(unbounded()).withName("name"), is(new FinalResultsSuppressionBuilder<>("name", unbounded())));    assertThat(untilWindowCloses(maxRecords(2L).shutDownWhenFull()).withName("name"), is(new FinalResultsSuppressionBuilder<>("name", new StrictBufferConfigImpl(2L, MAX_VALUE, SHUT_DOWN))));    assertThat(untilWindowCloses(maxBytes(2L).shutDownWhenFull()).withName("name"), is(new FinalResultsSuppressionBuilder<>("name", new StrictBufferConfigImpl(MAX_VALUE, 2L, SHUT_DOWN))));}
f16772
0
setUp
public void kafkatest_f16773_0()
{    props.put(StreamsConfig.DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS, Serdes.StringSerde.class.getName());    props.put(StreamsConfig.DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS, Serdes.ByteArraySerde.class.getName());}
f16773
0
shouldUseWindowSizeAsRentitionTimeIfWindowSizeIsLargerThanDefaultRetentionTime
public void kafkatest_f16782_0()
{    final long windowSize = 2 * TimeWindows.of(ofMillis(1)).maintainMs();    assertEquals(windowSize, TimeWindows.of(ofMillis(windowSize)).maintainMs());}
f16782
0
windowSizeMustNotBeZero
public void kafkatest_f16783_0()
{    TimeWindows.of(ofMillis(0));}
f16783
0
shouldComputeWindowsForTumblingWindows
public void kafkatest_f16792_0()
{    final TimeWindows windows = TimeWindows.of(ofMillis(12L));    final Map<Long, TimeWindow> matched = windows.windowsFor(21L);    assertEquals(1, matched.size());    assertEquals(new TimeWindow(12L, 24L), matched.get(12L));}
f16792
0
equalsAndHashcodeShouldBeValidForPositiveCases
public void kafkatest_f16793_0()
{    verifyEquality(TimeWindows.of(ofMillis(3)), TimeWindows.of(ofMillis(3)));    verifyEquality(TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)), TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)));    verifyEquality(TimeWindows.of(ofMillis(3)).grace(ofMillis(1)), TimeWindows.of(ofMillis(3)).grace(ofMillis(1)));    verifyEquality(TimeWindows.of(ofMillis(3)).grace(ofMillis(4)), TimeWindows.of(ofMillis(3)).grace(ofMillis(4)));    verifyEquality(TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)).grace(ofMillis(1)).grace(ofMillis(4)), TimeWindows.of(ofMillis(3)).advanceBy(ofMillis(1)).grace(ofMillis(1)).grace(ofMillis(4)));}
f16793
0
equalsAndHashcodeShouldBeValidForNegativeCases
public void kafkatest_f16802_0()
{    verifyInEquality(UnlimitedWindows.of().startOn(ofEpochMilli(9)), UnlimitedWindows.of().startOn(ofEpochMilli(1)));}
f16802
0
shouldWrapForTimeWindowedSerde
public void kafkatest_f16803_0()
{    final Serde<Windowed<String>> serde = WindowedSerdes.timeWindowedSerdeFrom(String.class);    assertTrue(serde.serializer() instanceof TimeWindowedSerializer);    assertTrue(serde.deserializer() instanceof TimeWindowedDeserializer);    assertTrue(((TimeWindowedSerializer) serde.serializer()).innerSerializer() instanceof StringSerializer);    assertTrue(((TimeWindowedDeserializer) serde.deserializer()).innerDeserializer() instanceof StringDeserializer);}
f16803
0
sessionWindowedDeserializerShouldThrowNpeIfNotInitializedProperly
public void kafkatest_f16812_0()
{    final SessionWindowedDeserializer<byte[]> deserializer = new SessionWindowedDeserializer<>();    final NullPointerException exception = assertThrows(NullPointerException.class, () -> deserializer.deserialize("topic", new byte[0]));    assertThat(exception.getMessage(), equalTo("Inner deserializer is `null`. User code must use constructor " + "`SessionWindowedDeserializer(final Deserializer<T> inner)` instead of the no-arg constructor."));}
f16812
0
timeWindowedSerializerShouldNotThrowOnCloseIfNotInitializedProperly
public void kafkatest_f16813_0()
{    new TimeWindowedSerializer<>().close();}
f16813
0
retentionTimeMustNotBeNegative
public void kafkatest_f16822_0()
{    new TestWindows().until(-1);}
f16822
0
overlap
public boolean kafkatest_f16823_0(final Window other)
{    return false;}
f16823
0
run
private void kafkatest_f16832_0()
{    switch(testName) {        // loading phases        case "load-one":            produce(LOADING_PRODUCER_CLIENT_ID, SOURCE_TOPIC_ONE, numRecords, keySkew, valueSize);            break;        case "load-two":            produce(LOADING_PRODUCER_CLIENT_ID, SOURCE_TOPIC_ONE, numRecords, keySkew, valueSize);            produce(LOADING_PRODUCER_CLIENT_ID, SOURCE_TOPIC_TWO, numRecords, keySkew, valueSize);            break;        // testing phases        case "consume":            consume(SOURCE_TOPIC_ONE);            break;        case "consumeproduce":            consumeAndProduce(SOURCE_TOPIC_ONE);            break;        case "streamcount":            countStreamsNonWindowed(SOURCE_TOPIC_ONE);            break;        case "streamcountwindowed":            countStreamsWindowed(SOURCE_TOPIC_ONE);            break;        case "streamprocess":            processStream(SOURCE_TOPIC_ONE);            break;        case "streamprocesswithsink":            processStreamWithSink(SOURCE_TOPIC_ONE);            break;        case "streamprocesswithstatestore":            processStreamWithStateStore(SOURCE_TOPIC_ONE);            break;        case "streamprocesswithwindowstore":            processStreamWithWindowStore(SOURCE_TOPIC_ONE);            break;        case "streamtablejoin":            streamTableJoin(SOURCE_TOPIC_ONE, SOURCE_TOPIC_TWO);            break;        case "streamstreamjoin":            streamStreamJoin(SOURCE_TOPIC_ONE, SOURCE_TOPIC_TWO);            break;        case "tabletablejoin":            tableTableJoin(SOURCE_TOPIC_ONE, SOURCE_TOPIC_TWO);            break;        case "yahoo":            yahooBenchmark(YAHOO_CAMPAIGNS_TOPIC, YAHOO_EVENTS_TOPIC);            break;        default:            throw new RuntimeException("Unknown test name " + testName);    }}
f16832
0
main
public static void kafkatest_f16833_0(final String[] args) throws IOException
{    if (args.length < 5) {        System.err.println("Not enough parameters are provided; expecting propFileName, testName, numRecords, keySkew, valueSize");        System.exit(1);    }    final String propFileName = args[0];    final String testName = args[1].toLowerCase(Locale.ROOT);    final int numRecords = Integer.parseInt(args[2]);    // 0d means even distribution    final double keySkew = Double.parseDouble(args[3]);    final int valueSize = Integer.parseInt(args[4]);    final Properties props = Utils.loadProps(propFileName);    final String kafka = props.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);    if (kafka == null) {        System.err.println("No bootstrap kafka servers specified in " + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);        System.exit(1);    }    // Note: this output is needed for automated tests and must not be removed    System.out.println("StreamsTest instance started");    System.out.println("testName=" + testName);    System.out.println("streamsProperties=" + props);    System.out.println("numRecords=" + numRecords);    System.out.println("keySkew=" + keySkew);    System.out.println("valueSize=" + valueSize);    final SimpleBenchmark benchmark = new SimpleBenchmark(props, testName, numRecords, keySkew, valueSize);    benchmark.run();}
f16833
0
processStreamWithStateStore
private void kafkatest_f16842_0(final String topic)
{    final CountDownLatch latch = new CountDownLatch(1);    setStreamProperties("simple-benchmark-streams-with-store");    final StreamsBuilder builder = new StreamsBuilder();    final StoreBuilder<KeyValueStore<Integer, byte[]>> storeBuilder = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore("store"), INTEGER_SERDE, BYTE_SERDE);    builder.addStateStore(storeBuilder.withCachingEnabled());    final KStream<Integer, byte[]> source = builder.stream(topic);    source.peek(new CountDownAction(latch)).process(new ProcessorSupplier<Integer, byte[]>() {        @Override        public Processor<Integer, byte[]> get() {            return new AbstractProcessor<Integer, byte[]>() {                KeyValueStore<Integer, byte[]> store;                @SuppressWarnings("unchecked")                @Override                public void init(final ProcessorContext context) {                    super.init(context);                    store = (KeyValueStore<Integer, byte[]>) context.getStateStore("store");                }                @Override                public void process(final Integer key, final byte[] value) {                    store.get(key);                    store.put(key, value);                }            };        }    }, "store");    final KafkaStreams streams = createKafkaStreamsWithExceptionHandler(builder, props);    runGenericBenchmark(streams, "Streams Stateful Performance [records/latency/rec-sec/MB-sec joined]: ", latch);}
f16842
0
get
public Processor<Integer, byte[]> kafkatest_f16843_0()
{    return new AbstractProcessor<Integer, byte[]>() {        KeyValueStore<Integer, byte[]> store;        @SuppressWarnings("unchecked")        @Override        public void init(final ProcessorContext context) {            super.init(context);            store = (KeyValueStore<Integer, byte[]>) context.getStateStore("store");        }        @Override        public void process(final Integer key, final byte[] value) {            store.get(key);            store.put(key, value);        }    };}
f16843
0
streamTableJoin
private void kafkatest_f16852_0(final String kStreamTopic, final String kTableTopic)
{    final CountDownLatch latch = new CountDownLatch(1);    setStreamProperties("simple-benchmark-stream-table-join");    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Integer, byte[]> input1 = builder.stream(kStreamTopic);    final KTable<Integer, byte[]> input2 = builder.table(kTableTopic);    input1.leftJoin(input2, VALUE_JOINER).foreach(new CountDownAction(latch));    final KafkaStreams streams = createKafkaStreamsWithExceptionHandler(builder, props);    // run benchmark    runGenericBenchmark(streams, "Streams KStreamKTable LeftJoin Performance [records/latency/rec-sec/MB-sec joined]: ", latch);}
f16852
0
streamStreamJoin
private void kafkatest_f16853_0(final String kStreamTopic1, final String kStreamTopic2)
{    final CountDownLatch latch = new CountDownLatch(1);    setStreamProperties("simple-benchmark-stream-stream-join");    final StreamsBuilder builder = new StreamsBuilder();    final KStream<Integer, byte[]> input1 = builder.stream(kStreamTopic1);    final KStream<Integer, byte[]> input2 = builder.stream(kStreamTopic2);    input1.leftJoin(input2, VALUE_JOINER, JoinWindows.of(ofMillis(STREAM_STREAM_JOIN_WINDOW))).foreach(new CountDownAction(latch));    final KafkaStreams streams = createKafkaStreamsWithExceptionHandler(builder, props);    // run benchmark    runGenericBenchmark(streams, "Streams KStreamKStream LeftJoin Performance [records/latency/rec-sec/MB-sec  joined]: ", latch);}
f16853
0
getAllPartitions
private List<TopicPartition> kafkatest_f16862_0(final KafkaConsumer<?, ?> consumer, final String... topics)
{    final ArrayList<TopicPartition> partitions = new ArrayList<>();    for (final String topic : topics) {        for (final PartitionInfo info : consumer.partitionsFor(topic)) {            partitions.add(new TopicPartition(info.topic(), info.partition()));        }    }    return partitions;}
f16862
0
yahooBenchmark
private void kafkatest_f16863_0(final String campaignsTopic, final String eventsTopic)
{    final YahooBenchmark benchmark = new YahooBenchmark(this, campaignsTopic, eventsTopic);    benchmark.run();}
f16863
0
shouldComputeGroupingForTwoGroups
public void kafkatest_f16872_0()
{    final PartitionGrouper grouper = new DefaultPartitionGrouper();    final Map<TaskId, Set<TopicPartition>> expectedPartitionsForTask = new HashMap<>();    final Map<Integer, Set<String>> topicGroups = new HashMap<>();    int topicGroupId = 0;    topicGroups.put(topicGroupId, mkSet("topic1"));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 0), mkSet(new TopicPartition("topic1", 0)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 1), mkSet(new TopicPartition("topic1", 1)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 2), mkSet(new TopicPartition("topic1", 2)));    topicGroups.put(++topicGroupId, mkSet("topic2"));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 0), mkSet(new TopicPartition("topic2", 0)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 1), mkSet(new TopicPartition("topic2", 1)));    assertEquals(expectedPartitionsForTask, grouper.partitionGroups(topicGroups, metadata));}
f16872
0
shouldComputeGroupingForSingleGroupWithMultipleTopics
public void kafkatest_f16873_0()
{    final PartitionGrouper grouper = new DefaultPartitionGrouper();    final Map<TaskId, Set<TopicPartition>> expectedPartitionsForTask = new HashMap<>();    final Map<Integer, Set<String>> topicGroups = new HashMap<>();    final int topicGroupId = 0;    topicGroups.put(topicGroupId, mkSet("topic1", "topic2"));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 0), mkSet(new TopicPartition("topic1", 0), new TopicPartition("topic2", 0)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 1), mkSet(new TopicPartition("topic1", 1), new TopicPartition("topic2", 1)));    expectedPartitionsForTask.put(new TaskId(topicGroupId, 2), mkSet(new TopicPartition("topic1", 2)));    assertEquals(expectedPartitionsForTask, grouper.partitionGroups(topicGroups, metadata));}
f16873
0
shouldReturnTopicFromRecordContext
public void kafkatest_f16882_0()
{    assertThat(context.topic(), equalTo(recordContext.topic()));}
f16882
0
shouldReturnNullIfTopicEqualsNonExistTopic
public void kafkatest_f16883_0()
{    context.setRecordContext(new ProcessorRecordContext(0, 0, 0, AbstractProcessorContext.NONEXIST_TOPIC, null));    assertThat(context.topic(), nullValue());}
f16883
0
shouldThrowIllegalStateExceptionOnHeadersIfNoRecordContext
public void kafkatest_f16892_0()
{    context.setRecordContext(null);    try {        context.headers();    } catch (final IllegalStateException e) {    // pass    }}
f16892
0
appConfigsShouldReturnParsedValues
public void kafkatest_f16893_0()
{    assertThat(context.appConfigs().get(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG), equalTo(RocksDBConfigSetter.class));}
f16893
0
createTask
private AbstractTask kafkatest_f16907_0(final Consumer consumer, final Map<StateStore, String> stateStoresToChangelogTopics)
{    return createTask(consumer, stateStoresToChangelogTopics, stateDirectory);}
f16907
0
createTask
private AbstractTask kafkatest_f16908_0(final Consumer consumer, final Map<StateStore, String> stateStoresToChangelogTopics, final StateDirectory stateDirectory)
{    final Properties properties = new Properties();    properties.put(StreamsConfig.APPLICATION_ID_CONFIG, "app");    properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummyhost:9092");    final StreamsConfig config = new StreamsConfig(properties);    final Map<String, String> storeNamesToChangelogTopics = new HashMap<>(stateStoresToChangelogTopics.size());    for (final Map.Entry<StateStore, String> e : stateStoresToChangelogTopics.entrySet()) {        storeNamesToChangelogTopics.put(e.getKey().name(), e.getValue());    }    return new AbstractTask(id, storeTopicPartitions, withLocalStores(new ArrayList<>(stateStoresToChangelogTopics.keySet()), storeNamesToChangelogTopics), consumer, new StoreChangelogReader(consumer, Duration.ZERO, new MockStateRestoreListener(), new LogContext("stream-task-test ")), false, stateDirectory, config) {        @Override        public void resume() {        }        @Override        public void commit() {        }        @Override        public void suspend() {        }        @Override        public void close(final boolean clean, final boolean isZombie) {        }        @Override        public void closeSuspended(final boolean clean, final boolean isZombie, final RuntimeException e) {        }        @Override        public boolean initializeStateStores() {            return false;        }        @Override        public void initializeTopology() {        }    };}
f16908
0
shouldClosedUnInitializedTasksOnSuspend
public void kafkatest_f16923_0()
{    t1.close(false, false);    EasyMock.expectLastCall();    EasyMock.replay(t1);    assignedTasks.addNewTask(t1);    assertThat(assignedTasks.suspend(), nullValue());    EasyMock.verify(t1);}
f16923
0
shouldNotSuspendSuspendedTasks
public void kafkatest_f16924_0()
{    mockRunningTaskSuspension();    EasyMock.replay(t1);    assertThat(suspendTask(), nullValue());    assertThat(assignedTasks.suspend(), nullValue());    EasyMock.verify(t1);}
f16924
0
shouldCommitRunningTasksIfNeeded
public void kafkatest_f16933_0()
{    mockTaskInitialization();    EasyMock.expect(t1.commitRequested()).andReturn(true);    EasyMock.expect(t1.commitNeeded()).andReturn(true);    t1.commit();    EasyMock.expectLastCall();    EasyMock.replay(t1);    addAndInitTask();    assertThat(assignedTasks.maybeCommitPerUserRequested(), equalTo(1));    EasyMock.verify(t1);}
f16933
0
shouldCloseTaskOnMaybeCommitIfTaskMigratedException
public void kafkatest_f16934_0()
{    mockTaskInitialization();    EasyMock.expect(t1.commitRequested()).andReturn(true);    EasyMock.expect(t1.commitNeeded()).andReturn(true);    t1.commit();    EasyMock.expectLastCall().andThrow(new TaskMigratedException());    t1.close(false, true);    EasyMock.expectLastCall();    EasyMock.replay(t1);    addAndInitTask();    try {        assignedTasks.maybeCommitPerUserRequested();        fail("Should have thrown TaskMigratedException.");    } catch (final TaskMigratedException expected) {    /* ignore */    }    assertThat(assignedTasks.runningTaskIds(), equalTo(Collections.EMPTY_SET));    EasyMock.verify(t1);}
f16934
0
addAndInitTask
private void kafkatest_f16943_0()
{    assignedTasks.addNewTask(t1);    assignedTasks.initializeNewTasks();}
f16943
0
suspendTask
private RuntimeException kafkatest_f16944_0()
{    addAndInitTask();    return assignedTasks.suspend();}
f16944
0
shouldEncodeAndDecodeVersion5
public void kafkatest_f16953_0()
{    final AssignmentInfo info = new AssignmentInfo(5, activeTasks, standbyTasks, globalAssignment, 2);    final AssignmentInfo expectedInfo = new AssignmentInfo(5, LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, globalAssignment, 2);    assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));}
f16953
0
shouldHaveNotReachedCapacityWhenAssignedTasksLessThanCapacity
public void kafkatest_f16954_0()
{    assertFalse(client.reachedCapacity());}
f16954
0
shouldHaveMoreAvailableCapacityWhenCapacityTheSameButFewerAssignedTasks
public void kafkatest_f16963_0()
{    final ClientState c2 = new ClientState(1);    client.assign(new TaskId(0, 1), true);    assertTrue(c2.hasMoreAvailableCapacityThan(client));    assertFalse(client.hasMoreAvailableCapacityThan(c2));}
f16963
0
shouldHaveMoreAvailableCapacityWhenCapacityHigherAndSameAssignedTaskCount
public void kafkatest_f16964_0()
{    final ClientState c2 = new ClientState(2);    assertTrue(c2.hasMoreAvailableCapacityThan(client));    assertFalse(client.hasMoreAvailableCapacityThan(c2));}
f16964
0
shouldAssignTopicGroupIdEvenlyAcrossClientsWithStandByTasks
public void kafkatest_f16973_0()
{    createClient(p1, 2);    createClient(p2, 2);    createClient(p3, 2);    final StickyTaskAssignor taskAssignor = createTaskAssignor(task20, task11, task12, task10, task21, task22);    taskAssignor.assign(1);    assertActiveTaskTopicGroupIdsEvenlyDistributed();}
f16973
0
shouldNotMigrateActiveTaskToOtherProcess
public void kafkatest_f16974_0()
{    createClientWithPreviousActiveTasks(p1, 1, task00);    createClientWithPreviousActiveTasks(p2, 1, task01);    final StickyTaskAssignor firstAssignor = createTaskAssignor(task00, task01, task02);    firstAssignor.assign(0);    assertThat(clients.get(p1).activeTasks(), hasItems(task00));    assertThat(clients.get(p2).activeTasks(), hasItems(task01));    assertThat(allActiveTasks(), equalTo(Arrays.asList(task00, task01, task02)));    clients.clear();    // flip the previous active tasks assignment around.    createClientWithPreviousActiveTasks(p1, 1, task01);    createClientWithPreviousActiveTasks(p2, 1, task02);    final StickyTaskAssignor secondAssignor = createTaskAssignor(task00, task01, task02);    secondAssignor.assign(0);    assertThat(clients.get(p1).activeTasks(), hasItems(task01));    assertThat(clients.get(p2).activeTasks(), hasItems(task02));    assertThat(allActiveTasks(), equalTo(Arrays.asList(task00, task01, task02)));}
f16974
0
shouldNotAssignStandbyTaskReplicasWhenNoClientAvailableWithoutHavingTheTaskAssigned
public void kafkatest_f16983_0()
{    createClient(p1, 1);    final StickyTaskAssignor taskAssignor = createTaskAssignor(task00);    taskAssignor.assign(1);    assertThat(clients.get(p1).standbyTasks().size(), equalTo(0));}
f16983
0
shouldAssignActiveAndStandbyTasks
public void kafkatest_f16984_0()
{    createClient(p1, 1);    createClient(p2, 1);    createClient(p3, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task01, task02);    taskAssignor.assign(1);    assertThat(allActiveTasks(), equalTo(Arrays.asList(task00, task01, task02)));    assertThat(allStandbyTasks(), equalTo(Arrays.asList(task00, task01, task02)));}
f16984
0
shouldReBalanceTasksAcrossAllClientsWhenCapacityAndTaskCountTheSame
public void kafkatest_f16993_0()
{    createClientWithPreviousActiveTasks(p3, 1, task00, task01, task02, task03);    createClient(p1, 1);    createClient(p2, 1);    createClient(p4, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task02, task01, task03);    taskAssignor.assign(0);    assertThat(clients.get(p1).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p2).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p3).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p4).assignedTaskCount(), equalTo(1));}
f16993
0
shouldReBalanceTasksAcrossClientsWhenCapacityLessThanTaskCount
public void kafkatest_f16994_0()
{    createClientWithPreviousActiveTasks(p3, 1, task00, task01, task02, task03);    createClient(p1, 1);    createClient(p2, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task02, task01, task03);    taskAssignor.assign(0);    assertThat(clients.get(p3).assignedTaskCount(), equalTo(2));    assertThat(clients.get(p1).assignedTaskCount(), equalTo(1));    assertThat(clients.get(p2).assignedTaskCount(), equalTo(1));}
f16994
0
shouldAssignTasksToNewClientWithoutFlippingAssignmentBetweenExistingAndBouncedClients
public void kafkatest_f17003_0()
{    final TaskId task06 = new TaskId(0, 6);    final ClientState c1 = createClientWithPreviousActiveTasks(p1, 1, task00, task01, task02, task06);    final ClientState c2 = createClient(p2, 1);    c2.addPreviousStandbyTasks(Utils.mkSet(task03, task04, task05));    final ClientState newClient = createClient(p3, 1);    final StickyTaskAssignor<Integer> taskAssignor = createTaskAssignor(task00, task01, task02, task03, task04, task05, task06);    taskAssignor.assign(0);    assertThat(c1.activeTasks(), not(hasItem(task03)));    assertThat(c1.activeTasks(), not(hasItem(task04)));    assertThat(c1.activeTasks(), not(hasItem(task05)));    assertThat(c1.activeTaskCount(), equalTo(3));    assertThat(c2.activeTasks(), not(hasItems(task00)));    assertThat(c2.activeTasks(), not(hasItems(task01)));    assertThat(c2.activeTasks(), not(hasItems(task02)));    assertThat(c2.activeTaskCount(), equalTo(2));    assertThat(newClient.activeTaskCount(), equalTo(2));}
f17003
0
createTaskAssignor
private StickyTaskAssignor<Integer> kafkatest_f17004_0(final TaskId... tasks)
{    final List<TaskId> taskIds = Arrays.asList(tasks);    Collections.shuffle(taskIds);    return new StickyTaskAssignor<>(clients, new HashSet<>(taskIds));}
f17004
0
shouldThrowForUnknownVersion1
public void kafkatest_f17013_0()
{    new SubscriptionInfo(0, processId, activeTasks, standbyTasks, "localhost:80");}
f17013
0
shouldThrowForUnknownVersion2
public void kafkatest_f17014_0()
{    new SubscriptionInfo(LATEST_SUPPORTED_VERSION + 1, processId, activeTasks, standbyTasks, "localhost:80");}
f17014
0
shouldNotifyRestoreStartNonBatchMode
public void kafkatest_f17023_0()
{    setUpCompositeRestoreListener(stateRestoreCallback);    compositeRestoreListener.onRestoreStart(topicPartition, storeName, startOffset, endOffset);    assertStateRestoreListenerOnStartNotification(stateRestoreCallback);    assertStateRestoreListenerOnStartNotification(reportingStoreListener);}
f17023
0
shouldNotifyRestoreStartBatchMode
public void kafkatest_f17024_0()
{    setUpCompositeRestoreListener(batchingStateRestoreCallback);    compositeRestoreListener.onRestoreStart(topicPartition, storeName, startOffset, endOffset);    assertStateRestoreListenerOnStartNotification(batchingStateRestoreCallback);    assertStateRestoreListenerOnStartNotification(reportingStoreListener);}
f17024
0
assertStateRestoreListenerOnStartNotification
private void kafkatest_f17033_0(final MockStateRestoreListener restoreListener)
{    assertTrue(restoreListener.storeNameCalledStates.containsKey(RESTORE_START));    assertThat(restoreListener.restoreTopicPartition, is(topicPartition));    assertThat(restoreListener.restoreStartOffset, is(startOffset));    assertThat(restoreListener.restoreEndOffset, is(endOffset));}
f17033
0
assertStateRestoreListenerOnBatchCompleteNotification
private void kafkatest_f17034_0(final MockStateRestoreListener restoreListener)
{    assertTrue(restoreListener.storeNameCalledStates.containsKey(RESTORE_BATCH));    assertThat(restoreListener.restoreTopicPartition, is(topicPartition));    assertThat(restoreListener.restoredBatchOffset, is(batchOffset));    assertThat(restoreListener.numBatchRestored, is(numberRestored));}
f17034
0
shouldEnforceCopartitioningOnRepartitionTopics
public void kafkatest_f17043_0()
{    final InternalTopicConfig config = createTopicConfig("repartitioned", 10);    validator.enforce(Utils.mkSet("first", "second", config.name()), Collections.singletonMap(config.name(), config), cluster.withPartitions(partitions));    assertThat(config.numberOfPartitions(), equalTo(Optional.of(2)));}
f17043
0
shouldSetNumPartitionsToMaximumPartitionsWhenAllTopicsAreRepartitionTopics
public void kafkatest_f17044_0()
{    final InternalTopicConfig one = createTopicConfig("one", 1);    final InternalTopicConfig two = createTopicConfig("two", 15);    final InternalTopicConfig three = createTopicConfig("three", 5);    final Map<String, InternalTopicConfig> repartitionTopicConfig = new HashMap<>();    repartitionTopicConfig.put(one.name(), one);    repartitionTopicConfig.put(two.name(), two);    repartitionTopicConfig.put(three.name(), three);    validator.enforce(Utils.mkSet(one.name(), two.name(), three.name()), repartitionTopicConfig, cluster);    assertThat(one.numberOfPartitions(), equalTo(Optional.of(15)));    assertThat(two.numberOfPartitions(), equalTo(Optional.of(15)));    assertThat(three.numberOfPartitions(), equalTo(Optional.of(15)));}
f17044
0
shouldForwardToSingleChild
public void kafkatest_f17053_0()
{    child.process(null, null);    expectLastCall();    replay(child, recordContext);    globalContext.forward(null, null);    verify(child, recordContext);}
f17053
0
shouldFailToForwardUsingToParameter
public void kafkatest_f17054_0()
{    globalContext.forward(null, null, To.all());}
f17054
0
shouldNotAllowInitForTimestampedWindowStore
public void kafkatest_f17063_0()
{    final StateStore store = globalContext.getStateStore(GLOBAL_TIMESTAMPED_WINDOW_STORE_NAME);    try {        store.init(null, null);        fail("Should have thrown UnsupportedOperationException.");    } catch (final UnsupportedOperationException expected) {    }}
f17063
0
shouldNotAllowInitForSessionStore
public void kafkatest_f17064_0()
{    final StateStore store = globalContext.getStateStore(GLOBAL_SESSION_STORE_NAME);    try {        store.init(null, null);        fail("Should have thrown UnsupportedOperationException.");    } catch (final UnsupportedOperationException expected) {    }}
f17064
0
shouldLockGlobalStateDirectory
public void kafkatest_f17073_0()
{    stateManager.initialize();    assertTrue(new File(stateDirectory.globalStateDir(), ".lock").exists());}
f17073
0
shouldThrowLockExceptionIfCantGetLock
public void kafkatest_f17074_0() throws IOException
{    final StateDirectory stateDir = new StateDirectory(streamsConfig, time, true);    try {        stateDir.lockGlobalState();        stateManager.initialize();    } finally {        stateDir.unlockGlobalState();    }}
f17074
0
shouldNotConvertValuesIfStoreDoesNotImplementTimestampedBytesStore
public void kafkatest_f17083_0()
{    initializeConsumer(1, 0, t1);    stateManager.initialize();    stateManager.register(store1, stateRestoreCallback);    final KeyValue<byte[], byte[]> restoredRecord = stateRestoreCallback.restored.get(0);    assertEquals(3, restoredRecord.key.length);    assertEquals(5, restoredRecord.value.length);}
f17083
0
shouldNotConvertValuesIfInnerStoreDoesNotImplementTimestampedBytesStore
public void kafkatest_f17084_0()
{    initializeConsumer(1, 0, t1);    stateManager.initialize();    stateManager.register(new WrappedStateStore<NoOpReadOnlyStore<Object, Object>, Object, Object>(store1) {    }, stateRestoreCallback);    final KeyValue<byte[], byte[]> restoredRecord = stateRestoreCallback.restored.get(0);    assertEquals(3, restoredRecord.key.length);    assertEquals(5, restoredRecord.value.length);}
f17084
0
shouldThrowProcessorStateStoreExceptionIfStoreFlushFailed
public void kafkatest_f17093_0()
{    stateManager.initialize();    // register the stores    initializeConsumer(1, 0, t1);    stateManager.register(new NoOpReadOnlyStore(store1.name()) {        @Override        public void flush() {            throw new RuntimeException("KABOOM!");        }    }, stateRestoreCallback);    stateManager.flush();}
f17093
0
flush
public void kafkatest_f17094_0()
{    throw new RuntimeException("KABOOM!");}
f17094
0
close
public void kafkatest_f17103_0()
{    super.close();    throw new RuntimeException("KABOOM!");}
f17103
0
shouldReleaseLockIfExceptionWhenLoadingCheckpoints
public void kafkatest_f17104_0() throws IOException
{    writeCorruptCheckpoint();    try {        stateManager.initialize();    } catch (final StreamsException e) {    // expected    }    final StateDirectory stateDir = new StateDirectory(streamsConfig, new MockTime(), true);    try {        // should be able to get the lock now as it should've been released        assertTrue(stateDir.lockGlobalState());    } finally {        stateDir.unlockGlobalState();    }}
f17104
0
shouldRetryWhenEndOffsetsThrowsTimeoutException
public void kafkatest_f17113_0()
{    final int retries = 2;    final AtomicInteger numberOfCalls = new AtomicInteger(0);    consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {        @Override        public synchronized Map<TopicPartition, Long> endOffsets(final Collection<org.apache.kafka.common.TopicPartition> partitions) {            numberOfCalls.incrementAndGet();            throw new TimeoutException();        }    };    streamsConfig = new StreamsConfig(new Properties() {        {            put(StreamsConfig.APPLICATION_ID_CONFIG, "appId");            put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummy:1234");            put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());            put(StreamsConfig.RETRIES_CONFIG, retries);        }    });    try {        new GlobalStateManagerImpl(new LogContext("mock"), topology, consumer, stateDirectory, stateRestoreListener, streamsConfig);    } catch (final StreamsException expected) {        assertEquals(numberOfCalls.get(), retries);    }}
f17113
0
endOffsets
public synchronized Map<TopicPartition, Long> kafkatest_f17114_0(final Collection<org.apache.kafka.common.TopicPartition> partitions)
{    numberOfCalls.incrementAndGet();    throw new TimeoutException();}
f17114
0
shouldInitializeStateManager
public void kafkatest_f17123_0()
{    final Map<TopicPartition, Long> startingOffsets = globalStateTask.initialize();    assertTrue(stateMgr.initialized);    assertEquals(offsets, startingOffsets);}
f17123
0
shouldInitializeContext
public void kafkatest_f17124_0()
{    globalStateTask.initialize();    assertTrue(context.initialized);}
f17124
0
shouldFlushStateManagerWithOffsets
public void kafkatest_f17133_0() throws IOException
{    final Map<TopicPartition, Long> expectedOffsets = new HashMap<>();    expectedOffsets.put(t1, 52L);    expectedOffsets.put(t2, 100L);    globalStateTask.initialize();    globalStateTask.update(new ConsumerRecord<>(topic1, 1, 51, "foo".getBytes(), "foo".getBytes()));    globalStateTask.flushState();    assertEquals(expectedOffsets, stateMgr.checkpointed());}
f17133
0
shouldCheckpointOffsetsWhenStateIsFlushed
public void kafkatest_f17134_0()
{    final Map<TopicPartition, Long> expectedOffsets = new HashMap<>();    expectedOffsets.put(t1, 102L);    expectedOffsets.put(t2, 100L);    globalStateTask.initialize();    globalStateTask.update(new ConsumerRecord<>(topic1, 1, 101, "foo".getBytes(), "foo".getBytes()));    globalStateTask.flushState();    assertThat(stateMgr.checkpointed(), equalTo(expectedOffsets));}
f17134
0
shouldCloseStateStoresOnClose
public void kafkatest_f17143_0() throws Exception
{    initializeConsumer();    globalStreamThread.start();    final StateStore globalStore = builder.globalStateStores().get(GLOBAL_STORE_NAME);    assertTrue(globalStore.isOpen());    globalStreamThread.shutdown();    globalStreamThread.join();    assertFalse(globalStore.isOpen());}
f17143
0
shouldTransitionToDeadOnClose
public void kafkatest_f17144_0() throws Exception
{    initializeConsumer();    globalStreamThread.start();    globalStreamThread.shutdown();    globalStreamThread.join();    assertEquals(GlobalStreamThread.State.DEAD, globalStreamThread.state());}
f17144
0
shouldUseSuppliedConfigs
public void kafkatest_f17153_0()
{    final Map<String, String> configs = new HashMap<>();    configs.put("retention.ms", "1000");    configs.put("retention.bytes", "10000");    final UnwindowedChangelogTopicConfig topicConfig = new UnwindowedChangelogTopicConfig("name", configs);    final Map<String, String> properties = topicConfig.getProperties(Collections.<String, String>emptyMap(), 0);    assertEquals("1000", properties.get("retention.ms"));    assertEquals("10000", properties.get("retention.bytes"));}
f17153
0
shouldUseSuppliedConfigsForRepartitionConfig
public void kafkatest_f17154_0()
{    final Map<String, String> configs = new HashMap<>();    configs.put("retention.ms", "1000");    final RepartitionTopicConfig topicConfig = new RepartitionTopicConfig("name", configs);    assertEquals("1000", topicConfig.getProperties(Collections.<String, String>emptyMap(), 0).get(TopicConfig.RETENTION_MS_CONFIG));}
f17154
0
shouldLogWhenTopicNotFoundAndNotThrowException
public void kafkatest_f17163_0()
{    LogCaptureAppender.setClassLoggerToDebug(InternalTopicManager.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    mockAdminClient.addTopic(false, topic, Collections.singletonList(new TopicPartitionInfo(0, broker1, cluster, Collections.emptyList())), null);    final InternalTopicConfig internalTopicConfig = new RepartitionTopicConfig(topic, Collections.emptyMap());    internalTopicConfig.setNumberOfPartitions(1);    final InternalTopicConfig internalTopicConfigII = new RepartitionTopicConfig("internal-topic", Collections.emptyMap());    internalTopicConfigII.setNumberOfPartitions(1);    final Map<String, InternalTopicConfig> topicConfigMap = new HashMap<>();    topicConfigMap.put(topic, internalTopicConfig);    topicConfigMap.put("internal-topic", internalTopicConfigII);    internalTopicManager.makeReady(topicConfigMap);    boolean foundExpectedMessage = false;    for (final String message : appender.getMessages()) {        foundExpectedMessage |= message.contains("Topic internal-topic is unknown or not found, hence not existed yet.");    }    assertTrue(foundExpectedMessage);}
f17163
0
shouldExhaustRetriesOnMarkedForDeletionTopic
public void kafkatest_f17164_0()
{    mockAdminClient.addTopic(false, topic, Collections.singletonList(new TopicPartitionInfo(0, broker1, cluster, Collections.emptyList())), null);    mockAdminClient.markTopicForDeletion(topic);    final InternalTopicConfig internalTopicConfig = new RepartitionTopicConfig(topic, Collections.emptyMap());    internalTopicConfig.setNumberOfPartitions(1);    try {        internalTopicManager.makeReady(Collections.singletonMap(topic, internalTopicConfig));        fail("Should have thrown StreamsException.");    } catch (final StreamsException expected) {        assertNull(expected.getCause());        assertTrue(expected.getMessage().startsWith("Could not create topics after 1 retries"));    }}
f17164
0
testAddProcessorWithSameName
public void kafkatest_f17173_0()
{    builder.addSource(null, "source", null, null, null, "topic-1");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    try {        builder.addProcessor("processor", new MockProcessorSupplier(), "source");        fail("Should throw TopologyException with processor name conflict");    } catch (final TopologyException expected) {    /* ok */    }}
f17173
0
testAddProcessorWithWrongParent
public void kafkatest_f17174_0()
{    builder.addProcessor("processor", new MockProcessorSupplier(), "source");}
f17174
0
testAddSinkConnectedWithParent
public void kafkatest_f17183_0()
{    builder.addSource(null, "source", null, null, null, "source-topic");    builder.addSink("sink", "dest-topic", null, null, null, "source");    final Map<Integer, Set<String>> nodeGroups = builder.nodeGroups();    final Set<String> nodeGroup = nodeGroups.get(0);    assertTrue(nodeGroup.contains("sink"));    assertTrue(nodeGroup.contains("source"));}
f17183
0
testAddSinkConnectedWithMultipleParent
public void kafkatest_f17184_0()
{    builder.addSource(null, "source", null, null, null, "source-topic");    builder.addSource(null, "sourceII", null, null, null, "source-topicII");    builder.addSink("sink", "dest-topic", null, null, null, "source", "sourceII");    final Map<Integer, Set<String>> nodeGroups = builder.nodeGroups();    final Set<String> nodeGroup = nodeGroups.get(0);    assertTrue(nodeGroup.contains("sink"));    assertTrue(nodeGroup.contains("source"));    assertTrue(nodeGroup.contains("sourceII"));}
f17184
0
testAddStateStoreWithSource
public void kafkatest_f17193_0()
{    builder.addSource(null, "source-1", null, null, null, "topic-1");    try {        builder.addStateStore(storeBuilder, "source-1");        fail("Should throw TopologyException with store cannot be added to source");    } catch (final TopologyException expected) {    /* ok */    }}
f17193
0
testAddStateStoreWithSink
public void kafkatest_f17194_0()
{    builder.addSource(null, "source-1", null, null, null, "topic-1");    builder.addSink("sink-1", "topic-1", null, null, null, "source-1");    try {        builder.addStateStore(storeBuilder, "sink-1");        fail("Should throw TopologyException with store cannot be added to sink");    } catch (final TopologyException expected) {    /* ok */    }}
f17194
0
shouldNotAllowNullTopicChooserWhenAddingSink
public void kafkatest_f17203_0()
{    builder.addSink("name", (TopicNameExtractor<Object, Object>) null, null, null, null);}
f17203
0
shouldNotAllowNullNameWhenAddingProcessor
public void kafkatest_f17204_0()
{    builder.addProcessor(null, () -> null);}
f17204
0
shouldAssociateStateStoreNameWhenStateStoreSupplierIsInternal
public void kafkatest_f17213_0()
{    builder.addSource(null, "source", null, null, null, "topic");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    builder.addStateStore(storeBuilder, "processor");    final Map<String, List<String>> stateStoreNameToSourceTopic = builder.stateStoreNameToSourceTopics();    assertEquals(1, stateStoreNameToSourceTopic.size());    assertEquals(Collections.singletonList("topic"), stateStoreNameToSourceTopic.get("store"));}
f17213
0
shouldAssociateStateStoreNameWhenStateStoreSupplierIsExternal
public void kafkatest_f17214_0()
{    builder.addSource(null, "source", null, null, null, "topic");    builder.addProcessor("processor", new MockProcessorSupplier(), "source");    builder.addStateStore(storeBuilder, "processor");    final Map<String, List<String>> stateStoreNameToSourceTopic = builder.stateStoreNameToSourceTopics();    assertEquals(1, stateStoreNameToSourceTopic.size());    assertEquals(Collections.singletonList("topic"), stateStoreNameToSourceTopic.get("store"));}
f17214
0
shouldConnectRegexMatchedTopicsToStateStore
public void kafkatest_f17223_0() throws Exception
{    builder.addSource(null, "ingest", null, null, null, Pattern.compile("topic-\\d+"));    builder.addProcessor("my-processor", new MockProcessorSupplier(), "ingest");    builder.addStateStore(storeBuilder, "my-processor");    final InternalTopologyBuilder.SubscriptionUpdates subscriptionUpdates = new InternalTopologyBuilder.SubscriptionUpdates();    final Field updatedTopicsField = subscriptionUpdates.getClass().getDeclaredField("updatedTopicSubscriptions");    updatedTopicsField.setAccessible(true);    final Set<String> updatedTopics = (Set<String>) updatedTopicsField.get(subscriptionUpdates);    updatedTopics.add("topic-2");    updatedTopics.add("topic-3");    updatedTopics.add("topic-A");    builder.updateSubscriptions(subscriptionUpdates, "test-thread");    builder.setApplicationId("test-app");    final Map<String, List<String>> stateStoreAndTopics = builder.stateStoreNameToSourceTopics();    final List<String> topics = stateStoreAndTopics.get(storeBuilder.name());    assertEquals("Expected to contain two topics", 2, topics.size());    assertTrue(topics.contains("topic-2"));    assertTrue(topics.contains("topic-3"));    assertFalse(topics.contains("topic-A"));}
f17223
0
shouldNotAllowToAddGlobalStoreWithSourceNameEqualsProcessorName
public void kafkatest_f17224_0()
{    final String sameNameForSourceAndProcessor = "sameName";    builder.addGlobalStore((StoreBuilder<KeyValueStore>) storeBuilder, sameNameForSourceAndProcessor, null, null, null, "anyTopicName", sameNameForSourceAndProcessor, new MockProcessorSupplier());}
f17224
0
sourceShouldNotBeEqualForDifferentPattern
public void kafkatest_f17233_0()
{    final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source("name", null, Pattern.compile("topic"));    final InternalTopologyBuilder.Source differentPattern = new InternalTopologyBuilder.Source("name", null, Pattern.compile("topic2"));    final InternalTopologyBuilder.Source overlappingPattern = new InternalTopologyBuilder.Source("name", null, Pattern.compile("top*"));    assertThat(base, not(equalTo(differentPattern)));    assertThat(base, not(equalTo(overlappingPattern)));}
f17233
0
shouldGetThreadLevelSensor
public void kafkatest_f17234_0()
{    final Metrics metrics = mock(Metrics.class);    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, THREAD_NAME, VERSION);    final String sensorName = "sensor1";    final String expectedFullSensorName = INTERNAL_PREFIX + SENSOR_PREFIX_DELIMITER + THREAD_NAME + SENSOR_NAME_DELIMITER + sensorName;    final RecordingLevel recordingLevel = RecordingLevel.DEBUG;    final Sensor[] parents = {};    EasyMock.expect(metrics.sensor(expectedFullSensorName, recordingLevel, parents)).andReturn(null);    replayAll();    final Sensor sensor = streamsMetrics.threadLevelSensor(sensorName, recordingLevel);    verifyAll();    assertNull(sensor);}
f17234
0
shouldAddAmountRateAndSum
public void kafkatest_f17243_0()
{    StreamsMetricsImpl.addRateOfSumAndSumMetricsToSensor(sensor, group, tags, metricNamePrefix, description1, description2);    final double valueToRecord1 = 18.0;    final double valueToRecord2 = 72.0;    final long defaultWindowSizeInSeconds = Duration.ofMillis(new MetricConfig().timeWindowMs()).getSeconds();    final double expectedRateMetricValue = (valueToRecord1 + valueToRecord2) / defaultWindowSizeInSeconds;    verifyMetric(metricNamePrefix + "-rate", description1, valueToRecord1, valueToRecord2, expectedRateMetricValue);    // values are recorded once for each metric verification    final double expectedSumMetricValue = 2 * valueToRecord1 + 2 * valueToRecord2;    verifyMetric(metricNamePrefix + "-total", description2, valueToRecord1, valueToRecord2, expectedSumMetricValue);    // one metric is added automatically in the constructor of Metrics    assertThat(metrics.metrics().size(), equalTo(2 + 1));}
f17243
0
shouldAddSum
public void kafkatest_f17244_0()
{    StreamsMetricsImpl.addSumMetricToSensor(sensor, group, tags, metricNamePrefix, description1);    final double valueToRecord1 = 18.0;    final double valueToRecord2 = 42.0;    final double expectedSumMetricValue = valueToRecord1 + valueToRecord2;    verifyMetric(metricNamePrefix + "-total", description1, valueToRecord1, valueToRecord2, expectedSumMetricValue);    // one metric is added automatically in the constructor of Metrics    assertThat(metrics.metrics().size(), equalTo(1 + 1));}
f17244
0
shouldGetCommitSensor
public void kafkatest_f17253_0()
{    final String operation = "commit";    final String operationLatency = operation + StreamsMetricsImpl.LATENCY_SUFFIX;    final String totalDescription = "The total number of commit calls";    final String rateDescription = "The average per-second number of commit calls";    mockStatic(StreamsMetricsImpl.class);    expect(streamsMetrics.threadLevelSensor(operation, RecordingLevel.INFO)).andReturn(dummySensor);    expect(streamsMetrics.threadLevelTagMap()).andReturn(dummyTagMap);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operation, totalDescription, rateDescription);    StreamsMetricsImpl.addAvgAndMaxToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operationLatency);    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = ThreadMetrics.commitSensor(streamsMetrics);    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(dummySensor));}
f17253
0
shouldGetPollSensor
public void kafkatest_f17254_0()
{    final String operation = "poll";    final String operationLatency = operation + StreamsMetricsImpl.LATENCY_SUFFIX;    final String totalDescription = "The total number of poll calls";    final String rateDescription = "The average per-second number of poll calls";    mockStatic(StreamsMetricsImpl.class);    expect(streamsMetrics.threadLevelSensor(operation, RecordingLevel.INFO)).andReturn(dummySensor);    expect(streamsMetrics.threadLevelTagMap()).andReturn(dummyTagMap);    StreamsMetricsImpl.addInvocationRateAndCountToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operation, totalDescription, rateDescription);    StreamsMetricsImpl.addAvgAndMaxToSensor(dummySensor, THREAD_LEVEL_GROUP, dummyTagMap, operationLatency);    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = ThreadMetrics.pollSensor(streamsMetrics);    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(dummySensor));}
f17254
0
reset
public void kafkatest_f17263_0()
{    registered.clear();}
f17263
0
wasRegistered
public boolean kafkatest_f17264_0(final TopicPartition partition)
{    return registered.contains(partition);}
f17264
0
globalWindowStoreShouldBeReadOnly
public void kafkatest_f17273_0()
{    doTest("GlobalWindowStore", (Consumer<WindowStore<String, Long>>) store -> {        verifyStoreCannotBeInitializedOrClosed(store);        checkThrowsUnsupportedOperation(store::flush, "flush()");        checkThrowsUnsupportedOperation(() -> store.put("1", 1L, 1L), "put()");        checkThrowsUnsupportedOperation(() -> store.put("1", 1L), "put()");        assertEquals(iters.get(0), store.fetchAll(0L, 0L));        assertEquals(windowStoreIter, store.fetch(KEY, 0L, 1L));        assertEquals(iters.get(1), store.fetch(KEY, KEY, 0L, 1L));        assertEquals((Long) VALUE, store.fetch(KEY, 1L));        assertEquals(iters.get(2), store.all());    });}
f17273
0
globalTimestampedWindowStoreShouldBeReadOnly
public void kafkatest_f17274_0()
{    doTest("GlobalTimestampedWindowStore", (Consumer<TimestampedWindowStore<String, Long>>) store -> {        verifyStoreCannotBeInitializedOrClosed(store);        checkThrowsUnsupportedOperation(store::flush, "flush()");        checkThrowsUnsupportedOperation(() -> store.put("1", ValueAndTimestamp.make(1L, 1L), 1L), "put() [with timestamp]");        checkThrowsUnsupportedOperation(() -> store.put("1", ValueAndTimestamp.make(1L, 1L)), "put() [no timestamp]");        assertEquals(timestampedIters.get(0), store.fetchAll(0L, 0L));        assertEquals(windowStoreIter, store.fetch(KEY, 0L, 1L));        assertEquals(timestampedIters.get(1), store.fetch(KEY, KEY, 0L, 1L));        assertEquals(VALUE_AND_TIMESTAMP, store.fetch(KEY, 1L));        assertEquals(timestampedIters.get(2), store.all());    });}
f17274
0
windowStoreMock
private WindowStore<String, Long> kafkatest_f17283_0()
{    final WindowStore<String, Long> windowStore = mock(WindowStore.class);    initStateStoreMock(windowStore);    expect(windowStore.fetchAll(anyLong(), anyLong())).andReturn(iters.get(0));    expect(windowStore.fetch(anyString(), anyString(), anyLong(), anyLong())).andReturn(iters.get(1));    expect(windowStore.fetch(anyString(), anyLong(), anyLong())).andReturn(windowStoreIter);    expect(windowStore.fetch(anyString(), anyLong())).andReturn(VALUE);    expect(windowStore.all()).andReturn(iters.get(2));    windowStore.put(anyString(), anyLong());    expectLastCall().andAnswer(() -> {        putExecuted = true;        return null;    });    replay(windowStore);    return windowStore;}
f17283
0
timestampedWindowStoreMock
private TimestampedWindowStore<String, Long> kafkatest_f17284_0()
{    final TimestampedWindowStore<String, Long> windowStore = mock(TimestampedWindowStore.class);    initStateStoreMock(windowStore);    expect(windowStore.fetchAll(anyLong(), anyLong())).andReturn(timestampedIters.get(0));    expect(windowStore.fetch(anyString(), anyString(), anyLong(), anyLong())).andReturn(timestampedIters.get(1));    expect(windowStore.fetch(anyString(), anyLong(), anyLong())).andReturn(windowStoreIter);    expect(windowStore.fetch(anyString(), anyLong())).andReturn(VALUE_AND_TIMESTAMP);    expect(windowStore.all()).andReturn(timestampedIters.get(2));    windowStore.put(anyString(), anyObject(ValueAndTimestamp.class));    expectLastCall().andAnswer(() -> {        putExecuted = true;        return null;    });    windowStore.put(anyString(), anyObject(ValueAndTimestamp.class), anyLong());    expectLastCall().andAnswer(() -> {        putWithTimestampExecuted = true;        return null;    });    replay(windowStore);    return windowStore;}
f17284
0
prepare
public void kafkatest_f17293_0()
{    final StreamsConfig streamsConfig = mock(StreamsConfig.class);    expect(streamsConfig.getString(StreamsConfig.APPLICATION_ID_CONFIG)).andReturn("add-id");    expect(streamsConfig.defaultValueSerde()).andReturn(Serdes.ByteArray());    expect(streamsConfig.defaultKeySerde()).andReturn(Serdes.ByteArray());    replay(streamsConfig);    context = new ProcessorContextImpl(mock(TaskId.class), mock(StreamTask.class), streamsConfig, mock(RecordCollector.class), mock(ProcessorStateManager.class), mock(StreamsMetricsImpl.class), mock(ThreadCache.class));}
f17293
0
shouldNotAllowToScheduleZeroMillisecondPunctuation
public void kafkatest_f17294_0()
{    try {        context.schedule(Duration.ofMillis(0L), null, null);        fail("Should have thrown IllegalArgumentException");    } catch (final IllegalArgumentException expected) {        assertThat(expected.getMessage(), equalTo("The minimum supported scheduling interval is 1 millisecond."));    }}
f17294
0
shouldEstimateEmptyHeaderAsZeroLength
public void kafkatest_f17306_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(42L, 73L, 0, null, new RecordHeaders());    assertEquals(MIN_SIZE, context.residentMemorySizeEstimate());}
f17306
0
shouldEstimateTopicLength
public void kafkatest_f17307_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(42L, 73L, 0, "topic", null);    assertEquals(MIN_SIZE + 5L, context.residentMemorySizeEstimate());}
f17307
0
testRegisterNonPersistentStore
public void kafkatest_f17316_0() throws IOException
{    final MockKeyValueStore nonPersistentStore = // non persistent store    new MockKeyValueStore(nonPersistentStoreName, false);    final ProcessorStateManager stateMgr = new ProcessorStateManager(new TaskId(0, 2), noPartitions, false, stateDirectory, mkMap(mkEntry(persistentStoreName, persistentStoreTopicName), mkEntry(nonPersistentStoreName, nonPersistentStoreTopicName)), changelogReader, false, logContext);    try {        stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);        assertTrue(changelogReader.wasRegistered(new TopicPartition(nonPersistentStoreTopicName, 2)));    } finally {        stateMgr.close(true);    }}
f17316
0
testChangeLogOffsets
public void kafkatest_f17317_0() throws IOException
{    final TaskId taskId = new TaskId(0, 0);    final long storeTopic1LoadedCheckpoint = 10L;    final String storeName1 = "store1";    final String storeName2 = "store2";    final String storeName3 = "store3";    final String storeTopicName1 = ProcessorStateManager.storeChangelogTopic(applicationId, storeName1);    final String storeTopicName2 = ProcessorStateManager.storeChangelogTopic(applicationId, storeName2);    final String storeTopicName3 = ProcessorStateManager.storeChangelogTopic(applicationId, storeName3);    final Map<String, String> storeToChangelogTopic = new HashMap<>();    storeToChangelogTopic.put(storeName1, storeTopicName1);    storeToChangelogTopic.put(storeName2, storeTopicName2);    storeToChangelogTopic.put(storeName3, storeTopicName3);    final OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(stateDirectory.directoryForTask(taskId), StateManagerUtil.CHECKPOINT_FILE_NAME));    checkpoint.write(singletonMap(new TopicPartition(storeTopicName1, 0), storeTopic1LoadedCheckpoint));    final TopicPartition partition1 = new TopicPartition(storeTopicName1, 0);    final TopicPartition partition2 = new TopicPartition(storeTopicName2, 0);    final TopicPartition partition3 = new TopicPartition(storeTopicName3, 1);    final MockKeyValueStore store1 = new MockKeyValueStore(storeName1, true);    final MockKeyValueStore store2 = new MockKeyValueStore(storeName2, true);    final MockKeyValueStore store3 = new MockKeyValueStore(storeName3, true);    // if there is a source partition, inherit the partition id    final Set<TopicPartition> sourcePartitions = Utils.mkSet(new TopicPartition(storeTopicName3, 1));    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, sourcePartitions, // standby    true, stateDirectory, storeToChangelogTopic, changelogReader, false, logContext);    try {        stateMgr.register(store1, store1.stateRestoreCallback);        stateMgr.register(store2, store2.stateRestoreCallback);        stateMgr.register(store3, store3.stateRestoreCallback);        final Map<TopicPartition, Long> changeLogOffsets = stateMgr.checkpointed();        assertEquals(3, changeLogOffsets.size());        assertTrue(changeLogOffsets.containsKey(partition1));        assertTrue(changeLogOffsets.containsKey(partition2));        assertTrue(changeLogOffsets.containsKey(partition3));        assertEquals(storeTopic1LoadedCheckpoint, (long) changeLogOffsets.get(partition1));        assertEquals(-1L, (long) changeLogOffsets.get(partition2));        assertEquals(-1L, (long) changeLogOffsets.get(partition3));    } finally {        stateMgr.close(true);    }}
f17317
0
shouldOverrideRestoredOffsetsWithProcessedOffsets
public void kafkatest_f17326_0() throws IOException
{    final Map<TopicPartition, Long> offsets = singletonMap(persistentStorePartition, 99L);    checkpoint.write(offsets);    final MockKeyValueStore persistentStore = new MockKeyValueStore(persistentStoreName, true);    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, singletonMap(persistentStoreName, persistentStorePartition.topic()), changelogReader, false, logContext);    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    // should ignore irrelevant topic partitions    changelogReader.setRestoredOffsets(mkMap(mkEntry(persistentStorePartition, 110L), mkEntry(new TopicPartition("sillytopic", 5000), 1234L)));    // should ignore irrelevant topic partitions    stateMgr.checkpoint(mkMap(mkEntry(persistentStorePartition, 220L), mkEntry(new TopicPartition("ignoreme", 42), 9000L)));    stateMgr.close(true);    final Map<TopicPartition, Long> read = checkpoint.read();    // the checkpoint gets incremented to be the log position _after_ the committed offset    assertThat(read, equalTo(singletonMap(persistentStorePartition, 221L)));}
f17326
0
shouldWriteCheckpointForPersistentLogEnabledStore
public void kafkatest_f17327_0() throws IOException
{    final ProcessorStateManager stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, singletonMap(persistentStore.name(), persistentStoreTopicName), changelogReader, false, logContext);    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    stateMgr.checkpoint(singletonMap(persistentStorePartition, 10L));    final Map<TopicPartition, Long> read = checkpoint.read();    assertThat(read, equalTo(singletonMap(persistentStorePartition, 11L)));}
f17327
0
close
public void kafkatest_f17336_0()
{    throw new RuntimeException("KABOOM!");}
f17336
0
shouldLogAWarningIfCheckpointThrowsAnIOException
public void kafkatest_f17337_0()
{    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    final ProcessorStateManager stateMgr;    try {        stateMgr = new ProcessorStateManager(taskId, noPartitions, false, stateDirectory, singletonMap(persistentStore.name(), persistentStoreTopicName), changelogReader, false, logContext);    } catch (final IOException e) {        e.printStackTrace();        throw new AssertionError(e);    }    stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);    stateDirectory.clean();    stateMgr.checkpoint(singletonMap(persistentStorePartition, 10L));    LogCaptureAppender.unregister(appender);    boolean foundExpectedLogMessage = false;    for (final LogCaptureAppender.Event event : appender.getEvents()) {        if ("WARN".equals(event.getLevel()) && event.getMessage().startsWith("process-state-manager-test Failed to write offset checkpoint file to [") && event.getMessage().endsWith(".checkpoint]") && event.getThrowableInfo().get().startsWith("java.io.FileNotFoundException: ")) {            foundExpectedLogMessage = true;            break;        }    }    assertTrue(foundExpectedLogMessage);}
f17337
0
shouldSuccessfullyReInitializeStateStoresWithEosEnable
public void kafkatest_f17346_0() throws Exception
{    shouldSuccessfullyReInitializeStateStores(true);}
f17346
0
shouldSuccessfullyReInitializeStateStores
private void kafkatest_f17347_0(final boolean eosEnabled) throws Exception
{    final String store2Name = "store2";    final String store2Changelog = "store2-changelog";    final TopicPartition store2Partition = new TopicPartition(store2Changelog, 0);    final List<TopicPartition> changelogPartitions = asList(changelogTopicPartition, store2Partition);    final Map<String, String> storeToChangelog = mkMap(mkEntry(storeName, changelogTopic), mkEntry(store2Name, store2Changelog));    final MockKeyValueStore stateStore = new MockKeyValueStore(storeName, true);    final MockKeyValueStore stateStore2 = new MockKeyValueStore(store2Name, true);    final ProcessorStateManager stateManager = new ProcessorStateManager(taskId, changelogPartitions, false, stateDirectory, storeToChangelog, changelogReader, eosEnabled, logContext);    stateManager.register(stateStore, stateStore.stateRestoreCallback);    stateManager.register(stateStore2, stateStore2.stateRestoreCallback);    stateStore.initialized = false;    stateStore2.initialized = false;    stateManager.reinitializeStateStoresForPartitions(changelogPartitions, new NoOpProcessorContext() {        @Override        public void register(final StateStore store, final StateRestoreCallback stateRestoreCallback) {            stateManager.register(store, stateRestoreCallback);        }    });    assertTrue(stateStore.initialized);    assertTrue(stateStore2.initialized);}
f17347
0
testTopologyMetadata
public void kafkatest_f17356_0()
{    topology.addSource("source-1", "topic-1");    topology.addSource("source-2", "topic-2", "topic-3");    topology.addProcessor("processor-1", new MockProcessorSupplier<>(), "source-1");    topology.addProcessor("processor-2", new MockProcessorSupplier<>(), "source-1", "source-2");    topology.addSink("sink-1", "topic-3", "processor-1");    topology.addSink("sink-2", "topic-4", "processor-1", "processor-2");    final ProcessorTopology processorTopology = topology.getInternalBuilder("X").build();    assertEquals(6, processorTopology.processors().size());    assertEquals(2, processorTopology.sources().size());    assertEquals(3, processorTopology.sourceTopics().size());    assertNotNull(processorTopology.source("topic-1"));    assertNotNull(processorTopology.source("topic-2"));    assertNotNull(processorTopology.source("topic-3"));    assertEquals(processorTopology.source("topic-2"), processorTopology.source("topic-3"));}
f17356
0
testDrivingSimpleTopology
public void kafkatest_f17357_0()
{    final int partition = 10;    driver = new TopologyTestDriver(createSimpleTopology(partition), props);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key1", "value1"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key1", "value1", partition);    assertNoOutputRecord(OUTPUT_TOPIC_2);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key2", "value2"));    assertNextOutputRecord(OUTPUT_TOPIC_1, "key2", "value2", partition);    assertNoOutputRecord(OUTPUT_TOPIC_2);    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key3", "value3"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key4", "value4"));    driver.pipeInput(recordFactory.create(INPUT_TOPIC_1, "key5", "value5"));    assertNoOutputRecord(OUTPUT_TOPIC_2);    assertNextOutputRecord(OUTPUT_TOPIC_1, "key3", "value3", partition);    assertNextOutputRecord(OUTPUT_TOPIC_1, "key4", "value4", partition);    assertNextOutputRecord(OUTPUT_TOPIC_1, "key5", "value5", partition);}
f17357
0
shouldCreateStringWithSourceAndTopics
public void kafkatest_f17366_0()
{    topology.addSource("source", "topic1", "topic2");    final ProcessorTopology processorTopology = topology.getInternalBuilder().build();    final String result = processorTopology.toString();    assertThat(result, containsString("source:\n\t\ttopics:\t\t[topic1, topic2]\n"));}
f17366
0
shouldCreateStringWithMultipleSourcesAndTopics
public void kafkatest_f17367_0()
{    topology.addSource("source", "topic1", "topic2");    topology.addSource("source2", "t", "t1", "t2");    final ProcessorTopology processorTopology = topology.getInternalBuilder().build();    final String result = processorTopology.toString();    assertThat(result, containsString("source:\n\t\ttopics:\t\t[topic1, topic2]\n"));    assertThat(result, containsString("source2:\n\t\ttopics:\t\t[t, t1, t2]\n"));}
f17367
0
inMemoryStoreShouldNotResultInPersistentLocalStore
public void kafkatest_f17376_0()
{    final ProcessorTopology processorTopology = createLocalStoreTopology(Stores.inMemoryKeyValueStore("my-store"));    assertFalse(processorTopology.hasPersistentLocalStore());}
f17376
0
persistentLocalStoreShouldBeDetected
public void kafkatest_f17377_0()
{    final ProcessorTopology processorTopology = createLocalStoreTopology(Stores.persistentKeyValueStore("my-store"));    assertTrue(processorTopology.hasPersistentLocalStore());}
f17377
0
assertNextOutputRecord
private void kafkatest_f17386_0(final String topic, final String key, final String value, final Headers headers, final Integer partition, final Long timestamp)
{    final ProducerRecord<String, String> record = driver.readOutput(topic, STRING_DESERIALIZER, STRING_DESERIALIZER);    assertEquals(topic, record.topic());    assertEquals(key, record.key());    assertEquals(value, record.value());    assertEquals(partition, record.partition());    assertEquals(timestamp, record.timestamp());    assertEquals(headers, record.headers());}
f17386
0
assertNoOutputRecord
private void kafkatest_f17387_0(final String topic)
{    assertNull(driver.readOutput(topic));}
f17387
0
createInternalRepartitioningWithValueTimestampTopology
private Topology kafkatest_f17396_0()
{    topology.addSource("source", INPUT_TOPIC_1).addProcessor("processor", define(new ValueTimestampProcessor()), "source").addSink("sink0", THROUGH_TOPIC_1, "processor").addSource("source1", THROUGH_TOPIC_1).addSink("sink1", OUTPUT_TOPIC_1, "source1");    // use wrapper to get the internal topology builder to add internal topic    final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology);    internalTopologyBuilder.addInternalTopic(THROUGH_TOPIC_1);    return topology;}
f17396
0
createForwardToSourceTopology
private Topology kafkatest_f17397_0()
{    return topology.addSource("source-1", INPUT_TOPIC_1).addSink("sink-1", OUTPUT_TOPIC_1, "source-1").addSource("source-2", OUTPUT_TOPIC_1).addSink("sink-2", OUTPUT_TOPIC_2, "source-2");}
f17397
0
process
public void kafkatest_f17406_0(final String key, final String value)
{    for (int i = 0; i != numChildren; ++i) {        context().forward(key, value + "(" + (i + 1) + ")", "sink" + i);    }}
f17406
0
init
public void kafkatest_f17407_0(final ProcessorContext context)
{    super.init(context);    store = (KeyValueStore<String, String>) context.getStateStore(storeName);}
f17407
0
testPunctuationIntervalCancelFromPunctuator
public void kafkatest_f17416_0()
{    final PunctuationSchedule sched = new PunctuationSchedule(node, 0L, 100L, punctuator);    final long now = sched.timestamp - 100L;    final Cancellable cancellable = queue.schedule(sched);    final ProcessorNodePunctuator processorNodePunctuator = new ProcessorNodePunctuator() {        @Override        public void punctuate(final ProcessorNode node, final long time, final PunctuationType type, final Punctuator punctuator) {            punctuator.punctuate(time);            // simulate scheduler cancelled from within punctuator            cancellable.cancel();        }    };    queue.mayPunctuate(now, PunctuationType.STREAM_TIME, processorNodePunctuator);    assertEquals(0, node.mockProcessor.punctuatedStreamTime.size());    queue.mayPunctuate(now + 100L, PunctuationType.STREAM_TIME, processorNodePunctuator);    assertEquals(1, node.mockProcessor.punctuatedStreamTime.size());    queue.mayPunctuate(now + 200L, PunctuationType.STREAM_TIME, processorNodePunctuator);    assertEquals(1, node.mockProcessor.punctuatedStreamTime.size());}
f17416
0
punctuate
public void kafkatest_f17417_0(final ProcessorNode node, final long time, final PunctuationType type, final Punctuator punctuator)
{    punctuator.punctuate(time);    // simulate scheduler cancelled from within punctuator    cancellable.cancel();}
f17417
0
shouldThrowStreamsExceptionOnSubsequentCallIfASendFailsWithDefaultExceptionHandler
public void kafkatest_f17429_0()
{    final RecordCollector collector = new RecordCollectorImpl("test", logContext, new DefaultProductionExceptionHandler(), new Metrics().sensor("skipped-records"));    collector.init(new MockProducer(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {        @Override        public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {            callback.onCompletion(null, new Exception());            return null;        }    });    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);    try {        collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);        fail("Should have thrown StreamsException");    } catch (final StreamsException expected) {    /* ok */    }}
f17429
0
send
public synchronized Future<RecordMetadata> kafkatest_f17430_0(final ProducerRecord record, final Callback callback)
{    callback.onCompletion(null, new Exception());    return null;}
f17430
0
shouldThrowStreamsExceptionOnCloseIfASendFailedWithDefaultExceptionHandler
public void kafkatest_f17439_0()
{    final RecordCollector collector = new RecordCollectorImpl("test", logContext, new DefaultProductionExceptionHandler(), new Metrics().sensor("skipped-records"));    collector.init(new MockProducer(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {        @Override        public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {            callback.onCompletion(null, new Exception());            return null;        }    });    collector.send("topic1", "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);    try {        collector.close();        fail("Should have thrown StreamsException");    } catch (final StreamsException expected) {    /* ok */    }}
f17439
0
send
public synchronized Future<RecordMetadata> kafkatest_f17440_0(final ProducerRecord record, final Callback callback)
{    callback.onCompletion(null, new Exception());    return null;}
f17440
0
configure
public void kafkatest_f17449_0(final Map<String, ?> configs, final boolean isKey)
{    this.isKey = isKey;    super.configure(configs, isKey);}
f17449
0
serialize
public byte[] kafkatest_f17450_0(final String topic, final Headers headers, final String data)
{    if (isKey) {        headers.add(new RecordHeader("key", "key".getBytes()));    } else {        headers.add(new RecordHeader("value", "value".getBytes()));    }    return serialize(topic, data);}
f17450
0
shouldThrowStreamsExceptionWhenValueDeserializationFails
public void kafkatest_f17459_0()
{    final byte[] value = Serdes.Long().serializer().serialize("foo", 1L);    final List<ConsumerRecord<byte[], byte[]>> records = Collections.singletonList(new ConsumerRecord<>("topic", 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, value));    queue.addRawRecords(records);}
f17459
0
shouldNotThrowStreamsExceptionWhenKeyDeserializationFailsWithSkipHandler
public void kafkatest_f17460_0()
{    final byte[] key = Serdes.Long().serializer().serialize("foo", 1L);    final List<ConsumerRecord<byte[], byte[]>> records = Collections.singletonList(new ConsumerRecord<>("topic", 1, 1, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, key, recordValue));    queueThatSkipsDeserializeErrors.addRawRecords(records);    assertEquals(0, queueThatSkipsDeserializeErrors.size());}
f17460
0
shouldThrowStreamsExceptionOnKeyValueTypeSerializerMismatch
public void kafkatest_f17469_0()
{    final String keyOfDifferentTypeThanSerializer = "key with different type";    final String valueOfDifferentTypeThanSerializer = "value with different type";    // When/Then    context.setTime(0);    try {        illTypedSink.process(keyOfDifferentTypeThanSerializer, valueOfDifferentTypeThanSerializer);        fail("Should have thrown StreamsException");    } catch (final StreamsException e) {        assertThat(e.getCause(), instanceOf(ClassCastException.class));    }}
f17469
0
shouldHandleNullKeysWhenThrowingStreamsExceptionOnKeyValueTypeSerializerMismatch
public void kafkatest_f17470_0()
{    final String invalidValueToTriggerSerializerMismatch = "";    // When/Then    context.setTime(1);    try {        illTypedSink.process(null, invalidValueToTriggerSerializerMismatch);        fail("Should have thrown StreamsException");    } catch (final StreamsException e) {        assertThat(e.getCause(), instanceOf(ClassCastException.class));        assertThat(e.getMessage(), containsString("unknown because key is null"));    }}
f17470
0
testStorePartitions
public void kafkatest_f17479_0() throws IOException
{    final StreamsConfig config = createConfig(baseDir);    task = new StandbyTask(taskId, topicPartitions, topology, consumer, changelogReader, config, streamsMetrics, stateDirectory);    task.initializeStateStores();    assertEquals(Utils.mkSet(partition2, partition1), new HashSet<>(task.checkpointedOffsets().keySet()));}
f17479
0
testUpdateNonInitializedStore
public void kafkatest_f17480_0() throws IOException
{    final StreamsConfig config = createConfig(baseDir);    task = new StandbyTask(taskId, topicPartitions, topology, consumer, changelogReader, config, streamsMetrics, stateDirectory);    restoreStateConsumer.assign(new ArrayList<>(task.checkpointedOffsets().keySet()));    try {        task.update(partition1, singletonList(new ConsumerRecord<>(partition1.topic(), partition1.partition(), 10, 0L, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, recordValue)));        fail("expected an exception");    } catch (final NullPointerException npe) {        assertThat(npe.getMessage(), containsString("stateRestoreCallback must not be null"));    }}
f17480
0
committed
public synchronized OffsetAndMetadata kafkatest_f17489_0(final TopicPartition partition)
{    committedCallCount.getAndIncrement();    return super.committed(partition);}
f17489
0
shouldGetConsumerCommittedOffsetsOncePerCommit
public void kafkatest_f17490_0() throws IOException
{    final AtomicInteger committedCallCount = new AtomicInteger();    final Consumer<byte[], byte[]> consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {        @Override        public synchronized OffsetAndMetadata committed(final TopicPartition partition) {            committedCallCount.getAndIncrement();            return super.committed(partition);        }    };    consumer.assign(Collections.singletonList(globalTopicPartition));    consumer.commitSync(mkMap(mkEntry(globalTopicPartition, new OffsetAndMetadata(0L))));    task = new StandbyTask(taskId, ktablePartitions, ktableTopology, consumer, changelogReader, createConfig(baseDir), streamsMetrics, stateDirectory);    task.initializeStateStores();    task.update(globalTopicPartition, Collections.singletonList(makeConsumerRecord(globalTopicPartition, 1, 1)));    assertThat(committedCallCount.get(), equalTo(1));    task.update(globalTopicPartition, Collections.singletonList(makeConsumerRecord(globalTopicPartition, 1, 1)));    // We should not make another consumer.committed() call until we commit    assertThat(committedCallCount.get(), equalTo(1));    task.commit();    task.update(globalTopicPartition, Collections.singletonList(makeConsumerRecord(globalTopicPartition, 1, 1)));    // We committed so we're allowed to make another consumer.committed() call    assertThat(committedCallCount.get(), equalTo(2));}
f17490
0
setupCloseTaskMetric
private MetricName kafkatest_f17499_0()
{    final MetricName metricName = new MetricName("name", "group", "description", Collections.emptyMap());    final Sensor sensor = streamsMetrics.threadLevelSensor("task-closed", Sensor.RecordingLevel.INFO);    sensor.add(metricName, new CumulativeSum());    return metricName;}
f17499
0
verifyCloseTaskMetric
private void kafkatest_f17500_0(final double expected, final StreamsMetricsImpl streamsMetrics, final MetricName metricName)
{    final KafkaMetric metric = (KafkaMetric) streamsMetrics.metrics().get(metricName);    final double totalCloses = metric.measurable().measure(metric.config(), System.currentTimeMillis());    assertThat(totalCloses, equalTo(expected));}
f17500
0
shouldNotFlushOffsetsWhenFlushIntervalHasNotLapsed
public void kafkatest_f17509_0()
{    stateConsumer.initialize();    consumer.addRecord(new ConsumerRecord<>("topic-one", 1, 20L, new byte[0], new byte[0]));    time.sleep(FLUSH_INTERVAL / 2);    stateConsumer.pollAndUpdate();    assertFalse(stateMaintainer.flushed);}
f17509
0
shouldCloseConsumer
public void kafkatest_f17510_0() throws IOException
{    stateConsumer.close();    assertTrue(consumer.closed());}
f17510
0
shouldCreateBaseDirectory
public void kafkatest_f17519_0()
{    assertTrue(stateDir.exists());    assertTrue(stateDir.isDirectory());    assertTrue(appDir.exists());    assertTrue(appDir.isDirectory());}
f17519
0
shouldCreateTaskStateDirectory
public void kafkatest_f17520_0()
{    final TaskId taskId = new TaskId(0, 0);    final File taskDirectory = directory.directoryForTask(taskId);    assertTrue(taskDirectory.exists());    assertTrue(taskDirectory.isDirectory());}
f17520
0
shouldNotRemoveNonTaskDirectoriesAndFiles
public void kafkatest_f17529_0()
{    final File otherDir = TestUtils.tempDirectory(stateDir.toPath(), "foo");    directory.cleanRemovedTasks(0);    assertTrue(otherDir.exists());}
f17529
0
shouldListAllTaskDirectories
public void kafkatest_f17530_0()
{    TestUtils.tempDirectory(stateDir.toPath(), "foo");    final File taskDir1 = directory.directoryForTask(new TaskId(0, 0));    final File taskDir2 = directory.directoryForTask(new TaskId(0, 1));    final List<File> dirs = Arrays.asList(directory.listTaskDirectories());    assertEquals(2, dirs.size());    assertTrue(dirs.contains(taskDir1));    assertTrue(dirs.contains(taskDir2));}
f17530
0
shouldNotCreateGlobalStateDirectory
public void kafkatest_f17539_0() throws Exception
{    initializeStateDirectory(false);    final File globalStateDir = directory.globalStateDir();    assertFalse(globalStateDir.exists());}
f17539
0
shouldLockTaskStateDirectoryWhenDirectoryCreationDisabled
public void kafkatest_f17540_0() throws Exception
{    initializeStateDirectory(false);    final TaskId taskId = new TaskId(0, 0);    assertTrue(directory.lock(taskId));}
f17540
0
shouldConvertToKeyValueBatches
public void kafkatest_f17554_0()
{    final ArrayList<KeyValue<byte[], byte[]>> actual = new ArrayList<>();    final BatchingStateRestoreCallback callback = new BatchingStateRestoreCallback() {        @Override        public void restoreAll(final Collection<KeyValue<byte[], byte[]>> records) {            actual.addAll(records);        }        @Override        public void restore(final byte[] key, final byte[] value) {        // unreachable        }    };    final RecordBatchingStateRestoreCallback adapted = adapt(callback);    final byte[] key1 = { 1 };    final byte[] value1 = { 2 };    final byte[] key2 = { 3 };    final byte[] value2 = { 4 };    adapted.restoreBatch(asList(new ConsumerRecord<>("topic1", 0, 0L, key1, value1), new ConsumerRecord<>("topic2", 1, 1L, key2, value2)));    assertThat(actual, is(asList(new KeyValue<>(key1, value1), new KeyValue<>(key2, value2))));}
f17554
0
restoreAll
public void kafkatest_f17555_0(final Collection<KeyValue<byte[], byte[]>> records)
{    actual.addAll(records);}
f17555
0
shouldBeCompletedIfOffsetAndOffsetLimitAreZero
public void kafkatest_f17564_0()
{    final StateRestorer restorer = new StateRestorer(new TopicPartition("topic", 1), compositeRestoreListener, null, 0, true, "storeName", identity());    assertTrue(restorer.hasCompleted(0, 10));}
f17564
0
shouldSetRestoredOffsetToMinOfLimitAndOffset
public void kafkatest_f17565_0()
{    restorer.setRestoredOffset(20);    assertThat(restorer.restoredOffset(), equalTo(20L));    restorer.setRestoredOffset(100);    assertThat(restorer.restoredOffset(), equalTo(OFFSET_LIMIT));}
f17565
0
partitions
public Set<TopicPartition> kafkatest_f17574_0()
{    return Collections.singleton(topicPartition);}
f17574
0
shouldRecoverFromOffsetOutOfRangeExceptionAndRestoreFromStart
public void kafkatest_f17575_0()
{    final int messages = 10;    final int startOffset = 5;    final long expiredCheckpoint = 1L;    assignPartition(messages, topicPartition);    consumer.updateBeginningOffsets(Collections.singletonMap(topicPartition, (long) startOffset));    consumer.updateEndOffsets(Collections.singletonMap(topicPartition, (long) (messages + startOffset)));    addRecords(messages, topicPartition, startOffset);    consumer.assign(Collections.emptyList());    final StateRestorer stateRestorer = new StateRestorer(topicPartition, restoreListener, expiredCheckpoint, Long.MAX_VALUE, true, "storeName", identity());    changelogReader.register(stateRestorer);    EasyMock.expect(active.restoringTaskFor(topicPartition)).andStubReturn(task);    EasyMock.replay(active, task);    // first restore call "fails" since OffsetOutOfRangeException but we should not die with an exception    assertEquals(0, changelogReader.restore(active).size());    // the starting offset for stateRestorer is set to NO_CHECKPOINT    assertThat(stateRestorer.checkpoint(), equalTo(-1L));    // restore the active task again    changelogReader.register(stateRestorer);    // the restored task should return completed partition without Exception.    assertEquals(1, changelogReader.restore(active).size());    // the restored size should be equal to message length.    assertThat(callback.restored.size(), equalTo(messages));}
f17575
0
shouldNotRestoreAnythingWhenPartitionIsEmpty
public void kafkatest_f17584_0()
{    final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, null, Long.MAX_VALUE, true, "storeName", identity());    setupConsumer(0, topicPartition);    changelogReader.register(restorer);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(0));    assertThat(restorer.restoredOffset(), equalTo(0L));}
f17584
0
shouldNotRestoreAnythingWhenCheckpointAtEndOffset
public void kafkatest_f17585_0()
{    final long endOffset = 10L;    setupConsumer(endOffset, topicPartition);    final StateRestorer restorer = new StateRestorer(topicPartition, restoreListener, endOffset, Long.MAX_VALUE, true, "storeName", identity());    changelogReader.register(restorer);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(0));    assertThat(restorer.restoredOffset(), equalTo(endOffset));}
f17585
0
shouldNotThrowTaskMigratedExceptionIfEndOffsetGetsExceededDuringRestoreForSourceTopic
public void kafkatest_f17594_0()
{    final int messages = 10;    setupConsumer(messages, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 5, true, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andReturn(task);    replay(active);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(5));}
f17594
0
shouldNotThrowTaskMigratedExceptionIfEndOffsetNotExceededDuringRestoreForSourceTopic
public void kafkatest_f17595_0()
{    final int messages = 10;    setupConsumer(messages, topicPartition);    changelogReader.register(new StateRestorer(topicPartition, restoreListener, null, 10, true, "storeName", identity()));    expect(active.restoringTaskFor(topicPartition)).andReturn(task);    replay(active);    changelogReader.restore(active);    assertThat(callback.restored.size(), equalTo(10));}
f17595
0
before
public void kafkatest_f17604_0()
{    builder = new StreamsBuilder();    final KStream<Object, Object> one = builder.stream("topic-one");    one.groupByKey().count(Materialized.<Object, Long, KeyValueStore<Bytes, byte[]>>as("table-one"));    final KStream<Object, Object> two = builder.stream("topic-two");    two.groupByKey().count(Materialized.<Object, Long, KeyValueStore<Bytes, byte[]>>as("table-two"));    builder.stream("topic-three").groupByKey().count(Materialized.<Object, Long, KeyValueStore<Bytes, byte[]>>as("table-three"));    one.merge(two).groupByKey().count(Materialized.<Object, Long, KeyValueStore<Bytes, byte[]>>as("merged-table"));    builder.stream("topic-four").mapValues(new ValueMapper<Object, Object>() {        @Override        public Object apply(final Object value) {            return value;        }    });    builder.globalTable("global-topic", Consumed.with(null, null), Materialized.<Object, Object, KeyValueStore<Bytes, byte[]>>as(globalTable));    TopologyWrapper.getInternalTopologyBuilder(builder.build()).setApplicationId("appId");    topic1P0 = new TopicPartition("topic-one", 0);    topic1P1 = new TopicPartition("topic-one", 1);    topic2P0 = new TopicPartition("topic-two", 0);    topic2P1 = new TopicPartition("topic-two", 1);    topic3P0 = new TopicPartition("topic-three", 0);    topic4P0 = new TopicPartition("topic-four", 0);    hostOne = new HostInfo("host-one", 8080);    hostTwo = new HostInfo("host-two", 9090);    hostThree = new HostInfo("host-three", 7070);    hostToPartitions = new HashMap<>();    hostToPartitions.put(hostOne, Utils.mkSet(topic1P0, topic2P1, topic4P0));    hostToPartitions.put(hostTwo, Utils.mkSet(topic2P0, topic1P1));    hostToPartitions.put(hostThree, Collections.singleton(topic3P0));    final List<PartitionInfo> partitionInfos = Arrays.asList(new PartitionInfo("topic-one", 0, null, null, null), new PartitionInfo("topic-one", 1, null, null, null), new PartitionInfo("topic-two", 0, null, null, null), new PartitionInfo("topic-two", 1, null, null, null), new PartitionInfo("topic-three", 0, null, null, null), new PartitionInfo("topic-four", 0, null, null, null));    cluster = new Cluster(null, Collections.<Node>emptyList(), partitionInfos, Collections.<String>emptySet(), Collections.<String>emptySet());    metadataState = new StreamsMetadataState(TopologyWrapper.getInternalTopologyBuilder(builder.build()), hostOne);    metadataState.onChange(hostToPartitions, cluster);    partitioner = new StreamPartitioner<String, Object>() {        @Override        public Integer partition(final String topic, final String key, final Object value, final int numPartitions) {            return 1;        }    };}
f17604
0
apply
public Object kafkatest_f17605_0(final Object value)
{    return value;}
f17605
0
shouldGetInstanceWithKey
public void kafkatest_f17614_0()
{    final TopicPartition tp4 = new TopicPartition("topic-three", 1);    hostToPartitions.put(hostTwo, Utils.mkSet(topic2P0, tp4));    metadataState.onChange(hostToPartitions, cluster.withPartitions(Collections.singletonMap(tp4, new PartitionInfo("topic-three", 1, null, null, null))));    final StreamsMetadata expected = new StreamsMetadata(hostThree, Utils.mkSet(globalTable, "table-three"), Collections.singleton(topic3P0));    final StreamsMetadata actual = metadataState.getMetadataWithKey("table-three", "the-key", Serdes.String().serializer());    assertEquals(expected, actual);}
f17614
0
shouldGetInstanceWithKeyAndCustomPartitioner
public void kafkatest_f17615_0()
{    final TopicPartition tp4 = new TopicPartition("topic-three", 1);    hostToPartitions.put(hostTwo, Utils.mkSet(topic2P0, tp4));    metadataState.onChange(hostToPartitions, cluster.withPartitions(Collections.singletonMap(tp4, new PartitionInfo("topic-three", 1, null, null, null))));    final StreamsMetadata expected = new StreamsMetadata(hostTwo, Utils.mkSet(globalTable, "table-two", "table-three", "merged-table"), Utils.mkSet(topic2P0, tp4));    final StreamsMetadata actual = metadataState.getMetadataWithKey("table-three", "the-key", partitioner);    assertEquals(expected, actual);}
f17615
0
shouldHaveGlobalStoreInAllMetadata
public void kafkatest_f17624_0()
{    final Collection<StreamsMetadata> metadata = metadataState.getAllMetadataForStore(globalTable);    assertEquals(3, metadata.size());    for (final StreamsMetadata streamsMetadata : metadata) {        assertTrue(streamsMetadata.stateStoreNames().contains(globalTable));    }}
f17624
0
shouldGetMyMetadataForGlobalStoreWithKey
public void kafkatest_f17625_0()
{    final StreamsMetadata metadata = metadataState.getMetadataWithKey(globalTable, "key", Serdes.String().serializer());    assertEquals(hostOne, metadata.hostInfo());}
f17625
0
shouldInterleaveTasksByGroupId
public void kafkatest_f17634_0()
{    final TaskId taskIdA0 = new TaskId(0, 0);    final TaskId taskIdA1 = new TaskId(0, 1);    final TaskId taskIdA2 = new TaskId(0, 2);    final TaskId taskIdA3 = new TaskId(0, 3);    final TaskId taskIdB0 = new TaskId(1, 0);    final TaskId taskIdB1 = new TaskId(1, 1);    final TaskId taskIdB2 = new TaskId(1, 2);    final TaskId taskIdC0 = new TaskId(2, 0);    final TaskId taskIdC1 = new TaskId(2, 1);    final List<TaskId> expectedSubList1 = asList(taskIdA0, taskIdA3, taskIdB2);    final List<TaskId> expectedSubList2 = asList(taskIdA1, taskIdB0, taskIdC0);    final List<TaskId> expectedSubList3 = asList(taskIdA2, taskIdB1, taskIdC1);    final List<List<TaskId>> embeddedList = asList(expectedSubList1, expectedSubList2, expectedSubList3);    final List<TaskId> tasks = asList(taskIdC0, taskIdC1, taskIdB0, taskIdB1, taskIdB2, taskIdA0, taskIdA1, taskIdA2, taskIdA3);    Collections.shuffle(tasks);    final List<List<TaskId>> interleavedTaskIds = StreamsPartitionAssignor.interleaveTasksByGroupId(tasks, 3);    assertThat(interleavedTaskIds, equalTo(embeddedList));}
f17634
0
testSubscription
public void kafkatest_f17635_0()
{    builder.addSource(null, "source1", null, null, null, "topic1");    builder.addSource(null, "source2", null, null, null, "topic2");    builder.addProcessor("processor", new MockProcessorSupplier(), "source1", "source2");    final Set<TaskId> prevTasks = Utils.mkSet(new TaskId(0, 1), new TaskId(1, 1), new TaskId(2, 1));    final Set<TaskId> cachedTasks = Utils.mkSet(new TaskId(0, 1), new TaskId(1, 1), new TaskId(2, 1), new TaskId(0, 2), new TaskId(1, 2), new TaskId(2, 2));    final UUID processId = UUID.randomUUID();    createMockTaskManager(prevTasks, cachedTasks, processId, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    final Set<String> topics = Utils.mkSet("topic1", "topic2");    final ConsumerPartitionAssignor.Subscription subscription = new ConsumerPartitionAssignor.Subscription(new ArrayList<>(topics), partitionAssignor.subscriptionUserData(topics));    Collections.sort(subscription.topics());    assertEquals(asList("topic1", "topic2"), subscription.topics());    final Set<TaskId> standbyTasks = new HashSet<>(cachedTasks);    standbyTasks.removeAll(prevTasks);    final SubscriptionInfo info = new SubscriptionInfo(processId, prevTasks, standbyTasks, null);    assertEquals(info.encode(), subscription.userData());}
f17635
0
testOnAssignment
public void kafkatest_f17644_0()
{    createMockTaskManager();    final Map<HostInfo, Set<TopicPartition>> hostState = Collections.singletonMap(new HostInfo("localhost", 9090), Utils.mkSet(t3p0, t3p3));    taskManager.setPartitionsByHostState(hostState);    EasyMock.expectLastCall();    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    activeTasks.put(task0, Utils.mkSet(t3p0));    activeTasks.put(task3, Utils.mkSet(t3p3));    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    standbyTasks.put(task1, Utils.mkSet(t3p1));    standbyTasks.put(task2, Utils.mkSet(t3p2));    taskManager.setAssignmentMetadata(activeTasks, standbyTasks);    EasyMock.expectLastCall();    final Capture<Cluster> capturedCluster = EasyMock.newCapture();    taskManager.setClusterMetadata(EasyMock.capture(capturedCluster));    EasyMock.expectLastCall();    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    final List<TaskId> activeTaskList = asList(task0, task3);    final AssignmentInfo info = new AssignmentInfo(activeTaskList, standbyTasks, hostState);    final ConsumerPartitionAssignor.Assignment assignment = new ConsumerPartitionAssignor.Assignment(asList(t3p0, t3p3), info.encode());    partitionAssignor.onAssignment(assignment, null);    EasyMock.verify(taskManager);    assertEquals(Collections.singleton(t3p0.topic()), capturedCluster.getValue().topics());    assertEquals(2, capturedCluster.getValue().partitionsForTopic(t3p0.topic()).size());}
f17644
0
testAssignWithInternalTopics
public void kafkatest_f17645_0()
{    builder.setApplicationId(applicationId);    builder.addInternalTopic("topicX");    builder.addSource(null, "source1", null, null, null, "topic1");    builder.addProcessor("processor1", new MockProcessorSupplier(), "source1");    builder.addSink("sink1", "topicX", null, null, null, "processor1");    builder.addSource(null, "source2", null, null, null, "topicX");    builder.addProcessor("processor2", new MockProcessorSupplier(), "source2");    final List<String> topics = asList("topic1", applicationId + "-topicX");    final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);    final UUID uuid1 = UUID.randomUUID();    createMockTaskManager(emptyTasks, emptyTasks, uuid1, builder);    EasyMock.replay(taskManager);    configurePartitionAssignor(Collections.emptyMap());    final MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);    partitionAssignor.setInternalTopicManager(internalTopicManager);    subscriptions.put("consumer10", new ConsumerPartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));    partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();    // check prepared internal topics    assertEquals(1, internalTopicManager.readyTopics.size());    assertEquals(allTasks.size(), (long) internalTopicManager.readyTopics.get(applicationId + "-topicX"));}
f17645
0
shouldNotAddStandbyTaskPartitionsToPartitionsForHost
public void kafkatest_f17654_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("topic1").groupByKey().count();    final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(builder.build());    internalTopologyBuilder.setApplicationId(applicationId);    final UUID uuid = UUID.randomUUID();    createMockTaskManager(emptyTasks, emptyTasks, uuid, internalTopologyBuilder);    EasyMock.replay(taskManager);    final Map<String, Object> props = new HashMap<>();    props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);    props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint);    configurePartitionAssignor(props);    partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));    subscriptions.put("consumer1", new ConsumerPartitionAssignor.Subscription(Collections.singletonList("topic1"), new SubscriptionInfo(uuid, emptyTasks, emptyTasks, userEndPoint).encode()));    subscriptions.put("consumer2", new ConsumerPartitionAssignor.Subscription(Collections.singletonList("topic1"), new SubscriptionInfo(UUID.randomUUID(), emptyTasks, emptyTasks, "other:9090").encode()));    final Set<TopicPartition> allPartitions = Utils.mkSet(t1p0, t1p1, t1p2);    final Map<String, ConsumerPartitionAssignor.Assignment> assign = partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();    final ConsumerPartitionAssignor.Assignment consumer1Assignment = assign.get("consumer1");    final AssignmentInfo assignmentInfo = AssignmentInfo.decode(consumer1Assignment.userData());    final Set<TopicPartition> consumer1partitions = assignmentInfo.partitionsByHost().get(new HostInfo("localhost", 8080));    final Set<TopicPartition> consumer2Partitions = assignmentInfo.partitionsByHost().get(new HostInfo("other", 9090));    final HashSet<TopicPartition> allAssignedPartitions = new HashSet<>(consumer1partitions);    allAssignedPartitions.addAll(consumer2Partitions);    assertThat(consumer1partitions, not(allPartitions));    assertThat(consumer2Partitions, not(allPartitions));    assertThat(allAssignedPartitions, equalTo(allPartitions));}
f17654
0
shouldThrowKafkaExceptionIfTaskMangerNotConfigured
public void kafkatest_f17655_0()
{    final Map<String, Object> config = configProps();    config.remove(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);    try {        partitionAssignor.configure(config);        fail("Should have thrown KafkaException");    } catch (final KafkaException expected) {        assertThat(expected.getMessage(), equalTo("TaskManager is not specified"));    }}
f17655
0
shouldDownGradeSubscriptionToVersion2For0101
public void kafkatest_f17664_0()
{    shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0101);}
f17664
0
shouldDownGradeSubscriptionToVersion2For0102
public void kafkatest_f17665_0()
{    shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0102);}
f17665
0
shouldThrowIfPreVersionProbingSubscriptionAndFutureSubscriptionIsMixed
private void kafkatest_f17674_0(final int oldVersion)
{    subscriptions.put("consumer1", new ConsumerPartitionAssignor.Subscription(Collections.singletonList("topic1"), new SubscriptionInfo(oldVersion, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()));    subscriptions.put("future-consumer", new ConsumerPartitionAssignor.Subscription(Collections.singletonList("topic1"), encodeFutureSubscription()));    createMockTaskManager(emptyTasks, emptyTasks, UUID.randomUUID(), builder);    EasyMock.replay(taskManager);    partitionAssignor.configure(configProps());    try {        partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)).groupAssignment();        fail("Should have thrown IllegalStateException");    } catch (final IllegalStateException expected) {    // pass    }}
f17674
0
createAssignment
private ConsumerPartitionAssignor.Assignment kafkatest_f17675_0(final Map<HostInfo, Set<TopicPartition>> firstHostState)
{    final AssignmentInfo info = new AssignmentInfo(Collections.emptyList(), Collections.emptyMap(), firstHostState);    return new ConsumerPartitionAssignor.Assignment(Collections.emptyList(), info.encode());}
f17675
0
setup
public void kafkatest_f17684_0()
{    consumer.assign(asList(partition1, partition2));    stateDirectory = new StateDirectory(createConfig(false), new MockTime(), true);}
f17684
0
cleanup
public void kafkatest_f17685_0() throws IOException
{    try {        if (task != null) {            try {                task.close(true, false);            } catch (final Exception e) {            // swallow            }        }    } finally {        Utils.delete(BASE_DIR);    }}
f17685
0
testPauseResume
public void kafkatest_f17694_0()
{    task = createStatelessTask(createConfig(false));    task.addRecords(partition1, asList(getConsumerRecord(partition1, 10), getConsumerRecord(partition1, 20)));    task.addRecords(partition2, asList(getConsumerRecord(partition2, 35), getConsumerRecord(partition2, 45), getConsumerRecord(partition2, 55), getConsumerRecord(partition2, 65)));    assertTrue(task.process());    assertEquals(1, source1.numReceived);    assertEquals(0, source2.numReceived);    assertEquals(1, consumer.paused().size());    assertTrue(consumer.paused().contains(partition2));    task.addRecords(partition1, asList(getConsumerRecord(partition1, 30), getConsumerRecord(partition1, 40), getConsumerRecord(partition1, 50)));    assertEquals(2, consumer.paused().size());    assertTrue(consumer.paused().contains(partition1));    assertTrue(consumer.paused().contains(partition2));    assertTrue(task.process());    assertEquals(2, source1.numReceived);    assertEquals(0, source2.numReceived);    assertEquals(1, consumer.paused().size());    assertTrue(consumer.paused().contains(partition2));    assertTrue(task.process());    assertEquals(3, source1.numReceived);    assertEquals(0, source2.numReceived);    assertEquals(1, consumer.paused().size());    assertTrue(consumer.paused().contains(partition2));    assertTrue(task.process());    assertEquals(3, source1.numReceived);    assertEquals(1, source2.numReceived);    assertEquals(0, consumer.paused().size());}
f17694
0
shouldPunctuateOnceStreamTimeAfterGap
public void kafkatest_f17695_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    task.addRecords(partition1, asList(getConsumerRecord(partition1, 20), getConsumerRecord(partition1, 142), getConsumerRecord(partition1, 155), getConsumerRecord(partition1, 160)));    task.addRecords(partition2, asList(getConsumerRecord(partition2, 25), getConsumerRecord(partition2, 145), getConsumerRecord(partition2, 159), getConsumerRecord(partition2, 161)));    // st: -1    // punctuate at 20    assertFalse(task.maybePunctuateStreamTime());    // st: 20    assertTrue(task.process());    assertEquals(7, task.numBuffered());    assertEquals(1, source1.numReceived);    assertEquals(0, source2.numReceived);    assertTrue(task.maybePunctuateStreamTime());    // st: 25    assertTrue(task.process());    assertEquals(6, task.numBuffered());    assertEquals(1, source1.numReceived);    assertEquals(1, source2.numReceived);    assertFalse(task.maybePunctuateStreamTime());    // st: 142    // punctuate at 142    assertTrue(task.process());    assertEquals(5, task.numBuffered());    assertEquals(2, source1.numReceived);    assertEquals(1, source2.numReceived);    assertTrue(task.maybePunctuateStreamTime());    // st: 145    // only one punctuation after 100ms gap    assertTrue(task.process());    assertEquals(4, task.numBuffered());    assertEquals(2, source1.numReceived);    assertEquals(2, source2.numReceived);    assertFalse(task.maybePunctuateStreamTime());    // st: 155    // punctuate at 155    assertTrue(task.process());    assertEquals(3, task.numBuffered());    assertEquals(3, source1.numReceived);    assertEquals(2, source2.numReceived);    assertTrue(task.maybePunctuateStreamTime());    // st: 159    assertTrue(task.process());    assertEquals(2, task.numBuffered());    assertEquals(3, source1.numReceived);    assertEquals(3, source2.numReceived);    assertFalse(task.maybePunctuateStreamTime());    // st: 160, aligned at 0    assertTrue(task.process());    assertEquals(1, task.numBuffered());    assertEquals(4, source1.numReceived);    assertEquals(3, source2.numReceived);    assertTrue(task.maybePunctuateStreamTime());    // st: 161    assertTrue(task.process());    assertEquals(0, task.numBuffered());    assertEquals(4, source1.numReceived);    assertEquals(4, source2.numReceived);    assertFalse(task.maybePunctuateStreamTime());    processorStreamTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.STREAM_TIME, 20L, 142L, 155L, 160L);}
f17695
0
shouldPunctuateOnceSystemTimeAfterGap
public void kafkatest_f17704_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    final long now = time.milliseconds();    time.sleep(100);    assertTrue(task.maybePunctuateSystemTime());    assertFalse(task.maybePunctuateSystemTime());    time.sleep(10);    assertTrue(task.maybePunctuateSystemTime());    time.sleep(12);    assertTrue(task.maybePunctuateSystemTime());    time.sleep(7);    assertFalse(task.maybePunctuateSystemTime());    // punctuate at now + 130    time.sleep(1);    assertTrue(task.maybePunctuateSystemTime());    // punctuate at now + 235    time.sleep(105);    assertTrue(task.maybePunctuateSystemTime());    assertFalse(task.maybePunctuateSystemTime());    // punctuate at now + 240, still aligned on the initial punctuation    time.sleep(5);    assertTrue(task.maybePunctuateSystemTime());    assertFalse(task.maybePunctuateSystemTime());    processorSystemTime.mockProcessor.checkAndClearPunctuateResult(PunctuationType.WALL_CLOCK_TIME, now + 100, now + 110, now + 122, now + 130, now + 235, now + 240);}
f17704
0
shouldWrapKafkaExceptionsWithStreamsExceptionAndAddContext
public void kafkatest_f17705_0()
{    task = createTaskThatThrowsException(false);    task.initializeStateStores();    task.initializeTopology();    task.addRecords(partition2, singletonList(getConsumerRecord(partition2, 0)));    try {        task.process();        fail("Should've thrown StreamsException");    } catch (final Exception e) {        assertThat(task.processorContext.currentNode(), nullValue());    }}
f17705
0
shouldThrowIllegalStateExceptionIfCurrentNodeIsNotNullWhenPunctuateCalled
public void kafkatest_f17714_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    task.processorContext.setCurrentNode(processorStreamTime);    try {        task.punctuate(processorStreamTime, 10, PunctuationType.STREAM_TIME, punctuator);        fail("Should throw illegal state exception as current node is not null");    } catch (final IllegalStateException e) {    // pass    }}
f17714
0
shouldCallPunctuateOnPassedInProcessorNode
public void kafkatest_f17715_0()
{    task = createStatelessTask(createConfig(false));    task.initializeStateStores();    task.initializeTopology();    task.punctuate(processorStreamTime, 5, PunctuationType.STREAM_TIME, punctuator);    assertThat(punctuatedAt, equalTo(5L));    task.punctuate(processorStreamTime, 10, PunctuationType.STREAM_TIME, punctuator);    assertThat(punctuatedAt, equalTo(10L));}
f17715
0
shouldNotCloseProducerOnErrorDuringUncleanCloseWithEosDisabled
public void kafkatest_f17724_0()
{    task = createTaskThatThrowsException(false);    task.close(false, false);    task = null;    assertFalse(producer.closed());}
f17724
0
shouldCommitTransactionAndCloseProducerOnCleanCloseWithEosEnabled
public void kafkatest_f17725_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    task.close(true, false);    task = null;    assertTrue(producer.transactionCommitted());    assertFalse(producer.transactionInFlight());    assertTrue(producer.closed());}
f17725
0
shouldThrowExceptionIfAnyExceptionsRaisedDuringCloseButStillCloseAllProcessorNodesTopology
public void kafkatest_f17734_0()
{    task = createTaskThatThrowsException(false);    task.initializeStateStores();    task.initializeTopology();    try {        task.close(true, false);        fail("should have thrown runtime exception");    } catch (final RuntimeException expected) {        task = null;    }    assertTrue(processorSystemTime.closed);    assertTrue(processorStreamTime.closed);    assertTrue(source1.closed);}
f17734
0
shouldInitAndBeginTransactionOnCreateIfEosEnabled
public void kafkatest_f17735_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    assertTrue(producer.transactionInitialized());    assertTrue(producer.transactionInFlight());}
f17735
0
shouldStartNewTransactionOnResumeIfEosEnabled
public void kafkatest_f17744_0()
{    task = createStatelessTask(createConfig(true));    task.initializeTopology();    task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0)));    task.process();    task.suspend();    task.resume();    task.initializeTopology();    assertTrue(producer.transactionInFlight());}
f17744
0
shouldNotStartNewTransactionOnResumeIfEosDisabled
public void kafkatest_f17745_0()
{    task = createStatelessTask(createConfig(false));    task.addRecords(partition1, singletonList(getConsumerRecord(partition1, 0)));    task.process();    task.suspend();    task.resume();    assertFalse(producer.transactionInFlight());}
f17745
0
shouldNotViolateAtLeastOnceWhenExceptionOccursDuringTaskSuspension
public void kafkatest_f17754_0()
{    final StreamTask task = createTaskThatThrowsException(false);    task.initializeStateStores();    task.initializeTopology();    try {        task.suspend();        fail("should have thrown an exception");    } catch (final Exception e) {    // all good    }}
f17754
0
shouldCloseStateManagerIfFailureOnTaskClose
public void kafkatest_f17755_0()
{    task = createStatefulTaskThatThrowsExceptionOnClose();    task.initializeStateStores();    task.initializeTopology();    try {        task.close(true, false);        fail("should have thrown an exception");    } catch (final Exception e) {    // all good    }    task = null;    assertFalse(stateStore.isOpen());}
f17755
0
shouldThrowProcessorStateExceptionOnInitializeOffsetsWhenKafkaException
public void kafkatest_f17764_0()
{    final Consumer<byte[], byte[]> consumer = mockConsumerWithCommittedException(new KafkaException("message"));    final AbstractTask task = createOptimizedStatefulTask(createConfig(false), consumer);    task.initializeStateStores();}
f17764
0
shouldThrowWakeupExceptionOnInitializeOffsetsWhenWakeupException
public void kafkatest_f17765_0()
{    final Consumer<byte[], byte[]> consumer = mockConsumerWithCommittedException(new WakeupException());    final AbstractTask task = createOptimizedStatefulTask(createConfig(false), consumer);    task.initializeStateStores();}
f17765
0
getConsumerRecord
private ConsumerRecord<byte[], byte[]> kafkatest_f17774_0(final TopicPartition topicPartition, final long offset)
{    return new ConsumerRecord<>(topicPartition.topic(), topicPartition.partition(), offset, // use the offset as the timestamp    offset, TimestampType.CREATE_TIME, 0L, 0, 0, recordKey, recordValue);}
f17774
0
setUp
public void kafkatest_f17775_0()
{    processId = UUID.randomUUID();    internalTopologyBuilder = InternalStreamsBuilderTest.internalTopologyBuilder(internalStreamsBuilder);    internalTopologyBuilder.setApplicationId(applicationId);    streamsMetadataState = new StreamsMetadataState(internalTopologyBuilder, StreamsMetadataState.UNKNOWN_HOST);}
f17775
0
shouldNotCauseExceptionIfNothingCommitted
public void kafkatest_f17784_0()
{    final long commitInterval = 1000L;    final Properties props = configProps(false);    props.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);    props.setProperty(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Long.toString(commitInterval));    final StreamsConfig config = new StreamsConfig(props);    final Consumer<byte[], byte[]> consumer = EasyMock.createNiceMock(Consumer.class);    final TaskManager taskManager = mockTaskManagerCommit(consumer, 1, 0);    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId, StreamsConfig.METRICS_LATEST);    final StreamThread thread = new StreamThread(mockTime, config, null, consumer, consumer, null, taskManager, streamsMetrics, internalTopologyBuilder, clientId, new LogContext(""), new AtomicInteger());    thread.setNow(mockTime.milliseconds());    thread.maybeCommit();    mockTime.sleep(commitInterval - 10L);    thread.setNow(mockTime.milliseconds());    thread.maybeCommit();    EasyMock.verify(taskManager);}
f17784
0
shouldCommitAfterTheCommitInterval
public void kafkatest_f17785_0()
{    final long commitInterval = 1000L;    final Properties props = configProps(false);    props.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);    props.setProperty(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Long.toString(commitInterval));    final StreamsConfig config = new StreamsConfig(props);    final Consumer<byte[], byte[]> consumer = EasyMock.createNiceMock(Consumer.class);    final TaskManager taskManager = mockTaskManagerCommit(consumer, 2, 1);    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId, StreamsConfig.METRICS_LATEST);    final StreamThread thread = new StreamThread(mockTime, config, null, consumer, consumer, null, taskManager, streamsMetrics, internalTopologyBuilder, clientId, new LogContext(""), new AtomicInteger());    thread.setNow(mockTime.milliseconds());    thread.maybeCommit();    mockTime.sleep(commitInterval + 1);    thread.setNow(mockTime.milliseconds());    thread.maybeCommit();    EasyMock.verify(taskManager);}
f17785
0
mockRunOnce
private void kafkatest_f17794_0(final boolean shutdownOnPoll)
{    final Collection<TopicPartition> assignedPartitions = Collections.singletonList(t1p1);    class MockStreamThreadConsumer<K, V> extends MockConsumer<K, V> {        private StreamThread streamThread;        private MockStreamThreadConsumer(final OffsetResetStrategy offsetResetStrategy) {            super(offsetResetStrategy);        }        @Override        public synchronized ConsumerRecords<K, V> poll(final Duration timeout) {            assertNotNull(streamThread);            if (shutdownOnPoll) {                streamThread.shutdown();            }            streamThread.rebalanceListener.onPartitionsAssigned(assignedPartitions);            return super.poll(timeout);        }        private void setStreamThread(final StreamThread streamThread) {            this.streamThread = streamThread;        }    }    final MockStreamThreadConsumer<byte[], byte[]> mockStreamThreadConsumer = new MockStreamThreadConsumer<>(OffsetResetStrategy.EARLIEST);    final TaskManager taskManager = new TaskManager(new MockChangelogReader(), processId, "log-prefix", mockStreamThreadConsumer, streamsMetadataState, null, null, null, new AssignedStreamsTasks(new LogContext()), new AssignedStandbyTasks(new LogContext()));    taskManager.setConsumer(mockStreamThreadConsumer);    taskManager.setAssignmentMetadata(Collections.emptyMap(), Collections.emptyMap());    final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId, StreamsConfig.METRICS_LATEST);    final StreamThread thread = new StreamThread(mockTime, config, null, mockStreamThreadConsumer, mockStreamThreadConsumer, null, taskManager, streamsMetrics, internalTopologyBuilder, clientId, new LogContext(""), new AtomicInteger()).updateThreadMetadata(getSharedAdminClientId(clientId));    mockStreamThreadConsumer.setStreamThread(thread);    mockStreamThreadConsumer.assign(assignedPartitions);    mockStreamThreadConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));    addRecord(mockStreamThreadConsumer, 1L, 0L);    thread.setState(StreamThread.State.STARTING);    thread.setState(StreamThread.State.PARTITIONS_REVOKED);    thread.runOnce();}
f17794
0
poll
public synchronized ConsumerRecords<K, V> kafkatest_f17795_0(final Duration timeout)
{    assertNotNull(streamThread);    if (shutdownOnPoll) {        streamThread.shutdown();    }    streamThread.rebalanceListener.onPartitionsAssigned(assignedPartitions);    return super.poll(timeout);}
f17795
0
shouldReturnStandbyTaskMetadataWhileRunningState
public void kafkatest_f17804_0()
{    internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).groupByKey().count(Materialized.as("count-one"));    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread(clientId, config, false);    final MockConsumer<byte[], byte[]> restoreConsumer = clientSupplier.restoreConsumer;    restoreConsumer.updatePartitions("stream-thread-test-count-one-changelog", singletonList(new PartitionInfo("stream-thread-test-count-one-changelog", 0, null, new Node[0], new Node[0])));    final HashMap<TopicPartition, Long> offsets = new HashMap<>();    offsets.put(new TopicPartition("stream-thread-test-count-one-changelog", 1), 0L);    restoreConsumer.updateEndOffsets(offsets);    restoreConsumer.updateBeginningOffsets(offsets);    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(null);    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    // assign single partition    standbyTasks.put(task1, Collections.singleton(t1p1));    thread.taskManager().setAssignmentMetadata(Collections.emptyMap(), standbyTasks);    thread.rebalanceListener.onPartitionsAssigned(Collections.emptyList());    thread.runOnce();    final ThreadMetadata threadMetadata = thread.threadMetadata();    assertEquals(StreamThread.State.RUNNING.name(), threadMetadata.threadState());    assertTrue(threadMetadata.standbyTasks().contains(new TaskMetadata(task1.toString(), Utils.mkSet(t1p1))));    assertTrue(threadMetadata.activeTasks().isEmpty());}
f17804
0
shouldUpdateStandbyTask
public void kafkatest_f17805_0() throws Exception
{    final String storeName1 = "count-one";    final String storeName2 = "table-two";    final String changelogName1 = applicationId + "-" + storeName1 + "-changelog";    final String changelogName2 = applicationId + "-" + storeName2 + "-changelog";    final TopicPartition partition1 = new TopicPartition(changelogName1, 1);    final TopicPartition partition2 = new TopicPartition(changelogName2, 1);    internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).groupByKey().count(Materialized.as(storeName1));    final MaterializedInternal<Object, Object, KeyValueStore<Bytes, byte[]>> materialized = new MaterializedInternal<>(Materialized.as(storeName2), internalStreamsBuilder, "");    internalStreamsBuilder.table(topic2, new ConsumedInternal<>(), materialized);    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread(clientId, config, false);    final MockConsumer<byte[], byte[]> restoreConsumer = clientSupplier.restoreConsumer;    restoreConsumer.updatePartitions(changelogName1, singletonList(new PartitionInfo(changelogName1, 1, null, new Node[0], new Node[0])));    restoreConsumer.assign(Utils.mkSet(partition1, partition2));    restoreConsumer.updateEndOffsets(Collections.singletonMap(partition1, 10L));    restoreConsumer.updateBeginningOffsets(Collections.singletonMap(partition1, 0L));    restoreConsumer.updateEndOffsets(Collections.singletonMap(partition2, 10L));    restoreConsumer.updateBeginningOffsets(Collections.singletonMap(partition2, 0L));    // let the store1 be restored from 0 to 10; store2 be restored from 5 (checkpointed) to 10    final OffsetCheckpoint checkpoint = new OffsetCheckpoint(new File(stateDirectory.directoryForTask(task3), CHECKPOINT_FILE_NAME));    checkpoint.write(Collections.singletonMap(partition2, 5L));    for (long i = 0L; i < 10L; i++) {        restoreConsumer.addRecord(new ConsumerRecord<>(changelogName1, 1, i, ("K" + i).getBytes(), ("V" + i).getBytes()));        restoreConsumer.addRecord(new ConsumerRecord<>(changelogName2, 1, i, ("K" + i).getBytes(), ("V" + i).getBytes()));    }    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(null);    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    // assign single partition    standbyTasks.put(task1, Collections.singleton(t1p1));    standbyTasks.put(task3, Collections.singleton(t2p1));    thread.taskManager().setAssignmentMetadata(Collections.emptyMap(), standbyTasks);    thread.rebalanceListener.onPartitionsAssigned(Collections.emptyList());    thread.runOnce();    final StandbyTask standbyTask1 = thread.taskManager().standbyTask(partition1);    final StandbyTask standbyTask2 = thread.taskManager().standbyTask(partition2);    final KeyValueStore<Object, Long> store1 = (KeyValueStore<Object, Long>) standbyTask1.getStore(storeName1);    final KeyValueStore<Object, Long> store2 = (KeyValueStore<Object, Long>) standbyTask2.getStore(storeName2);    assertEquals(10L, store1.approximateNumEntries());    assertEquals(5L, store2.approximateNumEntries());    assertEquals(0, thread.standbyRecords().size());}
f17805
0
shouldAlwaysReturnEmptyTasksMetadataWhileRebalancingStateAndTasksNotRunning
public void kafkatest_f17816_0()
{    internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).groupByKey().count(Materialized.as("count-one"));    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread(clientId, config, false);    final MockConsumer<byte[], byte[]> restoreConsumer = clientSupplier.restoreConsumer;    restoreConsumer.updatePartitions("stream-thread-test-count-one-changelog", asList(new PartitionInfo("stream-thread-test-count-one-changelog", 0, null, new Node[0], new Node[0]), new PartitionInfo("stream-thread-test-count-one-changelog", 1, null, new Node[0], new Node[0])));    final HashMap<TopicPartition, Long> offsets = new HashMap<>();    offsets.put(new TopicPartition("stream-thread-test-count-one-changelog", 0), 0L);    offsets.put(new TopicPartition("stream-thread-test-count-one-changelog", 1), 0L);    restoreConsumer.updateEndOffsets(offsets);    restoreConsumer.updateBeginningOffsets(offsets);    clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));    final List<TopicPartition> assignedPartitions = new ArrayList<>();    thread.setState(StreamThread.State.STARTING);    thread.rebalanceListener.onPartitionsRevoked(assignedPartitions);    assertThreadMetadataHasEmptyTasksWithState(thread.threadMetadata(), StreamThread.State.PARTITIONS_REVOKED);    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    // assign single partition    assignedPartitions.add(t1p1);    activeTasks.put(task1, Collections.singleton(t1p1));    standbyTasks.put(task2, Collections.singleton(t1p2));    thread.taskManager().setAssignmentMetadata(activeTasks, standbyTasks);    thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);    assertThreadMetadataHasEmptyTasksWithState(thread.threadMetadata(), StreamThread.State.PARTITIONS_ASSIGNED);}
f17816
0
shouldRecoverFromInvalidOffsetExceptionOnRestoreAndFinishRestore
public void kafkatest_f17817_0() throws Exception
{    internalStreamsBuilder.stream(Collections.singleton("topic"), consumed).groupByKey().count(Materialized.as("count"));    internalStreamsBuilder.buildAndOptimizeTopology();    final StreamThread thread = createStreamThread("clientId", config, false);    final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;    final MockConsumer<byte[], byte[]> mockRestoreConsumer = (MockConsumer<byte[], byte[]>) thread.restoreConsumer;    final TopicPartition topicPartition = new TopicPartition("topic", 0);    final Set<TopicPartition> topicPartitionSet = Collections.singleton(topicPartition);    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    activeTasks.put(new TaskId(0, 0), topicPartitionSet);    thread.taskManager().setAssignmentMetadata(activeTasks, Collections.emptyMap());    mockConsumer.updatePartitions("topic", singletonList(new PartitionInfo("topic", 0, null, new Node[0], new Node[0])));    mockConsumer.updateBeginningOffsets(Collections.singletonMap(topicPartition, 0L));    mockRestoreConsumer.updatePartitions("stream-thread-test-count-changelog", singletonList(new PartitionInfo("stream-thread-test-count-changelog", 0, null, new Node[0], new Node[0])));    final TopicPartition changelogPartition = new TopicPartition("stream-thread-test-count-changelog", 0);    final Set<TopicPartition> changelogPartitionSet = Collections.singleton(changelogPartition);    mockRestoreConsumer.updateBeginningOffsets(Collections.singletonMap(changelogPartition, 0L));    mockRestoreConsumer.updateEndOffsets(Collections.singletonMap(changelogPartition, 2L));    mockConsumer.schedulePollTask(() -> {        thread.setState(StreamThread.State.PARTITIONS_REVOKED);        thread.rebalanceListener.onPartitionsAssigned(topicPartitionSet);    });    try {        thread.start();        TestUtils.waitForCondition(() -> mockRestoreConsumer.assignment().size() == 1, "Never restore first record");        mockRestoreConsumer.addRecord(new ConsumerRecord<>("stream-thread-test-count-changelog", 0, 0L, "K1".getBytes(), "V1".getBytes()));        TestUtils.waitForCondition(() -> mockRestoreConsumer.position(changelogPartition) == 1L, "Never restore first record");        mockRestoreConsumer.setPollException(new InvalidOffsetException("Try Again!") {            @Override            public Set<TopicPartition> partitions() {                return changelogPartitionSet;            }        });        mockRestoreConsumer.addRecord(new ConsumerRecord<>("stream-thread-test-count-changelog", 0, 0L, "K1".getBytes(), "V1".getBytes()));        mockRestoreConsumer.addRecord(new ConsumerRecord<>("stream-thread-test-count-changelog", 0, 1L, "K2".getBytes(), "V2".getBytes()));        TestUtils.waitForCondition(() -> {            mockRestoreConsumer.assign(changelogPartitionSet);            return mockRestoreConsumer.position(changelogPartition) == 2L;        }, "Never finished restore");    } finally {        thread.shutdown();        thread.join(10000);    }}
f17817
0
setUp
public void kafkatest_f17826_0()
{    taskManager = new TaskManager(changeLogReader, UUID.randomUUID(), "", restoreConsumer, streamsMetadataState, activeTaskCreator, standbyTaskCreator, adminClient, active, standby);    taskManager.setConsumer(consumer);}
f17826
0
replay
private void kafkatest_f17827_0()
{    EasyMock.replay(changeLogReader, restoreConsumer, consumer, activeTaskCreator, standbyTaskCreator, active, standby, adminClient);}
f17827
0
shouldNotAddResumedActiveTasks
public void kafkatest_f17836_0()
{    checkOrder(active, true);    expect(active.maybeResumeSuspendedTask(taskId0, taskId0Partitions)).andReturn(true);    replay();    taskManager.setAssignmentMetadata(taskId0Assignment, Collections.<TaskId, Set<TopicPartition>>emptyMap());    taskManager.createTasks(taskId0Partitions);    // should be no calls to activeTaskCreator and no calls to active.addNewTasks(..)    verify(active, activeTaskCreator);}
f17836
0
shouldAddNonResumedStandbyTasks
public void kafkatest_f17837_0()
{    mockStandbyTaskExpectations();    expect(standby.maybeResumeSuspendedTask(taskId0, taskId0Partitions)).andReturn(false);    standby.addNewTask(EasyMock.same(standbyTask));    replay();    taskManager.setAssignmentMetadata(Collections.<TaskId, Set<TopicPartition>>emptyMap(), taskId0Assignment);    taskManager.createTasks(taskId0Partitions);    verify(standbyTaskCreator, active);}
f17837
0
shouldUnassignChangelogPartitionsOnShutdown
public void kafkatest_f17846_0()
{    restoreConsumer.unsubscribe();    expectLastCall();    replay();    taskManager.shutdown(true);    verify(restoreConsumer);}
f17846
0
shouldInitializeNewActiveTasks
public void kafkatest_f17847_0()
{    active.updateRestored(EasyMock.<Collection<TopicPartition>>anyObject());    expectLastCall();    replay();    taskManager.updateNewAndRestoringTasks();    verify(active);}
f17847
0
shouldSeekToBeginningIfOffsetIsLessThan0
public void kafkatest_f17856_0()
{    mockAssignStandbyPartitions(-1L);    restoreConsumer.seekToBeginning(taskId0Partitions);    expectLastCall();    replay();    taskManager.updateNewAndRestoringTasks();    verify(restoreConsumer);}
f17856
0
shouldCommitActiveAndStandbyTasks
public void kafkatest_f17857_0()
{    expect(active.commit()).andReturn(1);    expect(standby.commit()).andReturn(2);    replay();    assertThat(taskManager.commitAll(), equalTo(3));    verify(active, standby);}
f17857
0
shouldNotResumeConsumptionUntilAllStoresRestored
public void kafkatest_f17866_0()
{    expect(active.allTasksRunning()).andReturn(false);    final Consumer<byte[], byte[]> consumer = EasyMock.createStrictMock(Consumer.class);    taskManager.setConsumer(consumer);    EasyMock.replay(active, consumer);    // shouldn't invoke `resume` method in consumer    taskManager.updateNewAndRestoringTasks();    EasyMock.verify(consumer);}
f17866
0
shouldUpdateTasksFromPartitionAssignment
public void kafkatest_f17867_0()
{    final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();    final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();    taskManager.setAssignmentMetadata(activeTasks, standbyTasks);    assertTrue(taskManager.assignedActiveTasks().isEmpty());    // assign two active tasks with two partitions each    activeTasks.put(task01, new HashSet<>(asList(t1p1, t2p1)));    activeTasks.put(task02, new HashSet<>(asList(t1p2, t2p2)));    // assign one standby task with two partitions    standbyTasks.put(task03, new HashSet<>(asList(t1p3, t2p3)));    taskManager.setAssignmentMetadata(activeTasks, standbyTasks);    assertThat(taskManager.assignedActiveTasks(), equalTo(activeTasks));    assertThat(taskManager.assignedStandbyTasks(), equalTo(standbyTasks));}
f17867
0
setClassLoggerToDebug
public static void kafkatest_f17876_0(final Class<?> clazz)
{    Logger.getLogger(clazz).setLevel(Level.DEBUG);}
f17876
0
unregister
public static void kafkatest_f17877_0(final LogCaptureAppender logCaptureAppender)
{    Logger.getRootLogger().removeAppender(logCaptureAppender);}
f17877
0
shouldCreateKeyValueStoreWithTheProvidedInnerStore
public void kafkatest_f17887_0()
{    final KeyValueBytesStoreSupplier supplier = EasyMock.createNiceMock(KeyValueBytesStoreSupplier.class);    final InMemoryKeyValueStore store = new InMemoryKeyValueStore("name");    EasyMock.expect(supplier.name()).andReturn("name").anyTimes();    EasyMock.expect(supplier.get()).andReturn(store);    EasyMock.replay(supplier);    final MaterializedInternal<String, Integer, KeyValueStore<Bytes, byte[]>> materialized = new MaterializedInternal<>(Materialized.as(supplier), nameProvider, storePrefix);    final TimestampedKeyValueStoreMaterializer<String, Integer> materializer = new TimestampedKeyValueStoreMaterializer<>(materialized);    final StoreBuilder<TimestampedKeyValueStore<String, Integer>> builder = materializer.materialize();    final TimestampedKeyValueStore<String, Integer> built = builder.build();    assertThat(store.name(), CoreMatchers.equalTo(built.name()));}
f17887
0
extractMetadataTimestamp
public void kafkatest_f17888_0()
{    testExtractMetadataTimestamp(new LogAndSkipOnInvalidTimestamp());}
f17888
0
after
public void kafkatest_f17899_0()
{    store.close();    driver.clear();}
f17899
0
getContents
private static Map<Integer, String> kafkatest_f17900_0(final KeyValueIterator<Integer, String> iter)
{    final HashMap<Integer, String> result = new HashMap<>();    while (iter.hasNext()) {        final KeyValue<Integer, String> entry = iter.next();        result.put(entry.key, entry.value);    }    return result;}
f17900
0
testPutIfAbsent
public void kafkatest_f17909_0()
{    // Verify that the store reads and writes correctly ...    assertNull(store.putIfAbsent(0, "zero"));    assertNull(store.putIfAbsent(1, "one"));    assertNull(store.putIfAbsent(2, "two"));    assertNull(store.putIfAbsent(4, "four"));    assertEquals("four", store.putIfAbsent(4, "unexpected value"));    assertEquals(4, driver.sizeOf(store));    assertEquals("zero", store.get(0));    assertEquals("one", store.get(1));    assertEquals("two", store.get(2));    assertNull(store.get(3));    assertEquals("four", store.get(4));    // Flush the store and verify all current entries were properly flushed ...    store.flush();    assertEquals("zero", driver.flushedEntryStored(0));    assertEquals("one", driver.flushedEntryStored(1));    assertEquals("two", driver.flushedEntryStored(2));    assertEquals("four", driver.flushedEntryStored(4));    assertFalse(driver.flushedEntryRemoved(0));    assertFalse(driver.flushedEntryRemoved(1));    assertFalse(driver.flushedEntryRemoved(2));    assertFalse(driver.flushedEntryRemoved(4));}
f17909
0
shouldThrowNullPointerExceptionOnPutNullKey
public void kafkatest_f17910_0()
{    store.put(null, "anyValue");}
f17910
0
shouldThrowNullPointerExceptionOnRangeNullToKey
public void kafkatest_f17919_0()
{    store.range(2, null);}
f17919
0
testSize
public void kafkatest_f17920_0()
{    assertEquals("A newly created store should have no entries", 0, store.approximateNumEntries());    store.put(0, "zero");    store.put(1, "one");    store.put(2, "two");    store.put(4, "four");    store.put(5, "five");    store.flush();    assertEquals(5, store.approximateNumEntries());}
f17920
0
shouldPutAndFetch
public void kafkatest_f17929_0()
{    final String key = "a";    bytesStore.put(serializeKey(new Windowed<>(key, windows[0])), serializeValue(10));    bytesStore.put(serializeKey(new Windowed<>(key, windows[1])), serializeValue(50));    bytesStore.put(serializeKey(new Windowed<>(key, windows[2])), serializeValue(100));    final KeyValueIterator<Bytes, byte[]> values = bytesStore.fetch(Bytes.wrap(key.getBytes()), 0, 500);    final List<KeyValue<Windowed<String>, Long>> expected = Arrays.asList(KeyValue.pair(new Windowed<>(key, windows[0]), 10L), KeyValue.pair(new Windowed<>(key, windows[1]), 50L));    assertEquals(expected, toList(values));}
f17929
0
shouldFindValuesWithinRange
public void kafkatest_f17930_0()
{    final String key = "a";    bytesStore.put(serializeKey(new Windowed<>(key, windows[0])), serializeValue(10));    bytesStore.put(serializeKey(new Windowed<>(key, windows[1])), serializeValue(50));    bytesStore.put(serializeKey(new Windowed<>(key, windows[2])), serializeValue(100));    final KeyValueIterator<Bytes, byte[]> results = bytesStore.fetch(Bytes.wrap(key.getBytes()), 1, 999);    final List<KeyValue<Windowed<String>, Long>> expected = Arrays.asList(KeyValue.pair(new Windowed<>(key, windows[0]), 10L), KeyValue.pair(new Windowed<>(key, windows[1]), 50L));    assertEquals(expected, toList(results));}
f17930
0
shouldRestoreToByteStore
public void kafkatest_f17939_0()
{    // 0 segments initially.    assertEquals(0, bytesStore.getSegments().size());    final String key = "a";    final Collection<KeyValue<byte[], byte[]>> records = new ArrayList<>();    records.add(new KeyValue<>(serializeKey(new Windowed<>(key, windows[0])).get(), serializeValue(50L)));    records.add(new KeyValue<>(serializeKey(new Windowed<>(key, windows[3])).get(), serializeValue(100L)));    bytesStore.restoreAllInternal(records);    // 2 segments are created during restoration.    assertEquals(2, bytesStore.getSegments().size());    // Bulk loading is enabled during recovery.    for (final S segment : bytesStore.getSegments()) {        assertThat(getOptions(segment).level0FileNumCompactionTrigger(), equalTo(1 << 30));    }    final List<KeyValue<Windowed<String>, Long>> expected = new ArrayList<>();    expected.add(new KeyValue<>(new Windowed<>(key, windows[0]), 50L));    expected.add(new KeyValue<>(new Windowed<>(key, windows[3]), 100L));    final List<KeyValue<Windowed<String>, Long>> results = toList(bytesStore.all());    assertEquals(expected, results);}
f17939
0
shouldRespectBulkLoadOptionsDuringInit
public void kafkatest_f17940_0()
{    bytesStore.init(context, bytesStore);    final String key = "a";    bytesStore.put(serializeKey(new Windowed<>(key, windows[0])), serializeValue(50L));    bytesStore.put(serializeKey(new Windowed<>(key, windows[3])), serializeValue(100L));    assertEquals(2, bytesStore.getSegments().size());    final StateRestoreListener restoreListener = context.getRestoreListener(bytesStore.name());    restoreListener.onRestoreStart(null, bytesStore.name(), 0L, 0L);    for (final S segment : bytesStore.getSegments()) {        assertThat(getOptions(segment).level0FileNumCompactionTrigger(), equalTo(1 << 30));    }    restoreListener.onRestoreEnd(null, bytesStore.name(), 0L);    for (final S segment : bytesStore.getSegments()) {        assertThat(getOptions(segment).level0FileNumCompactionTrigger(), equalTo(4));    }}
f17940
0
shouldStoreDifferentValues
public void kafkatest_f17949_0()
{    final byte[] priorValue = { (byte) 0 };    final byte[] oldValue = { (byte) 1 };    final BufferValue bufferValue = new BufferValue(priorValue, oldValue, null, null);    assertSame(priorValue, bufferValue.priorValue());    assertSame(oldValue, bufferValue.oldValue());    assertNotEquals(bufferValue.priorValue(), bufferValue.oldValue());}
f17949
0
shouldStoreDifferentValuesWithPriorNull
public void kafkatest_f17950_0()
{    final byte[] priorValue = null;    final byte[] oldValue = { (byte) 1 };    final BufferValue bufferValue = new BufferValue(priorValue, oldValue, null, null);    assertNull(bufferValue.priorValue());    assertSame(oldValue, bufferValue.oldValue());    assertNotEquals(bufferValue.priorValue(), bufferValue.oldValue());}
f17950
0
shouldDeserializeOld
public void kafkatest_f17959_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(0L, 0L, 0, "topic", null);    final byte[] serializedContext = context.serialize();    final byte[] oldValue = { (byte) 5 };    final ByteBuffer serialValue = ByteBuffer.allocate(serializedContext.length + Integer.BYTES * 3 + oldValue.length).put(serializedContext).putInt(-1).putInt(1).put(oldValue).putInt(-1);    serialValue.position(0);    assertThat(BufferValue.deserialize(serialValue), is(new BufferValue(null, oldValue, null, context)));}
f17959
0
shouldDeserializeNew
public void kafkatest_f17960_0()
{    final ProcessorRecordContext context = new ProcessorRecordContext(0L, 0L, 0, "topic", null);    final byte[] serializedContext = context.serialize();    final byte[] newValue = { (byte) 5 };    final ByteBuffer serialValue = ByteBuffer.allocate(serializedContext.length + Integer.BYTES * 3 + newValue.length).put(serializedContext).putInt(-1).putInt(-1).putInt(1).put(newValue);    serialValue.position(0);    assertThat(BufferValue.deserialize(serialValue), is(new BufferValue(null, null, newValue, context)));}
f17960
0
bytesValue
private byte[] kafkatest_f17969_0(final String value)
{    return value.getBytes();}
f17969
0
bytesKey
private Bytes kafkatest_f17970_0(final String key)
{    return Bytes.wrap(key.getBytes());}
f17970
0
shouldNotShowItemsDeletedFromCacheButFlushedToStoreBeforeDelete
public void kafkatest_f17979_0()
{    store.put(bytesKey("a"), bytesValue("a"));    store.flush();    store.delete(bytesKey("a"));    assertNull(store.get(bytesKey("a")));    assertFalse(store.range(bytesKey("a"), bytesKey("b")).hasNext());    assertFalse(store.all().hasNext());}
f17979
0
shouldClearNamespaceCacheOnClose
public void kafkatest_f17980_0()
{    store.put(bytesKey("a"), bytesValue("a"));    assertEquals(1, cache.size());    store.close();    assertEquals(0, cache.size());}
f17980
0
shouldThrowNullPointerExceptionOnPutIfAbsentWithNullKey
public void kafkatest_f17989_0()
{    store.putIfAbsent(null, bytesValue("c"));}
f17989
0
shouldThrowNullPointerExceptionOnPutAllWithNullKey
public void kafkatest_f17990_0()
{    final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();    entries.add(new KeyValue<>(null, bytesValue("a")));    try {        store.putAll(entries);        fail("Should have thrown NullPointerException while putAll null key");    } catch (final NullPointerException expected) {    }}
f17990
0
shouldPutFetchAllKeysFromCache
public void kafkatest_f17999_0()
{    cachingStore.put(new Windowed<>(keyA, new SessionWindow(0, 0)), "1".getBytes());    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(0, 0)), "1".getBytes());    cachingStore.put(new Windowed<>(keyB, new SessionWindow(0, 0)), "1".getBytes());    assertEquals(3, cache.size());    final KeyValueIterator<Windowed<Bytes>, byte[]> all = cachingStore.findSessions(keyA, keyB, 0, 0);    verifyWindowedKeyValue(all.next(), new Windowed<>(keyA, new SessionWindow(0, 0)), "1");    verifyWindowedKeyValue(all.next(), new Windowed<>(keyAA, new SessionWindow(0, 0)), "1");    verifyWindowedKeyValue(all.next(), new Windowed<>(keyB, new SessionWindow(0, 0)), "1");    assertFalse(all.hasNext());}
f17999
0
shouldPutFetchRangeFromCache
public void kafkatest_f18000_0()
{    cachingStore.put(new Windowed<>(keyA, new SessionWindow(0, 0)), "1".getBytes());    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(0, 0)), "1".getBytes());    cachingStore.put(new Windowed<>(keyB, new SessionWindow(0, 0)), "1".getBytes());    assertEquals(3, cache.size());    final KeyValueIterator<Windowed<Bytes>, byte[]> some = cachingStore.findSessions(keyAA, keyB, 0, 0);    verifyWindowedKeyValue(some.next(), new Windowed<>(keyAA, new SessionWindow(0, 0)), "1");    verifyWindowedKeyValue(some.next(), new Windowed<>(keyB, new SessionWindow(0, 0)), "1");    assertFalse(some.hasNext());}
f18000
0
shouldNotForwardChangedValuesDuringFlushWhenSendOldValuesDisabled
public void kafkatest_f18009_0()
{    final Windowed<Bytes> a = new Windowed<>(keyA, new SessionWindow(0, 0));    final Windowed<String> aDeserialized = new Windowed<>("a", new SessionWindow(0, 0));    final CacheFlushListenerStub<Windowed<String>, String> flushListener = new CacheFlushListenerStub<>(new SessionWindowedDeserializer<>(new StringDeserializer()), new StringDeserializer());    cachingStore.setFlushListener(flushListener, false);    cachingStore.put(a, "1".getBytes());    cachingStore.flush();    cachingStore.put(a, "2".getBytes());    cachingStore.flush();    cachingStore.remove(a);    cachingStore.flush();    assertEquals(asList(new KeyValueTimestamp<>(aDeserialized, new Change<>("1", null), DEFAULT_TIMESTAMP), new KeyValueTimestamp<>(aDeserialized, new Change<>("2", null), DEFAULT_TIMESTAMP), new KeyValueTimestamp<>(aDeserialized, new Change<>(null, null), DEFAULT_TIMESTAMP)), flushListener.forwarded);    flushListener.forwarded.clear();    cachingStore.put(a, "1".getBytes());    cachingStore.put(a, "2".getBytes());    cachingStore.remove(a);    cachingStore.flush();    assertEquals(Collections.emptyList(), flushListener.forwarded);    flushListener.forwarded.clear();}
f18009
0
shouldReturnSameResultsForSingleKeyFindSessionsAndEqualKeyRangeFindSessions
public void kafkatest_f18010_0()
{    cachingStore.put(new Windowed<>(keyA, new SessionWindow(0, 1)), "1".getBytes());    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(2, 3)), "2".getBytes());    cachingStore.put(new Windowed<>(keyAA, new SessionWindow(4, 5)), "3".getBytes());    cachingStore.put(new Windowed<>(keyB, new SessionWindow(6, 7)), "4".getBytes());    final KeyValueIterator<Windowed<Bytes>, byte[]> singleKeyIterator = cachingStore.findSessions(keyAA, 0L, 10L);    final KeyValueIterator<Windowed<Bytes>, byte[]> keyRangeIterator = cachingStore.findSessions(keyAA, keyAA, 0L, 10L);    assertEquals(singleKeyIterator.next(), keyRangeIterator.next());    assertEquals(singleKeyIterator.next(), keyRangeIterator.next());    assertFalse(singleKeyIterator.hasNext());    assertFalse(keyRangeIterator.hasNext());}
f18010
0
shouldThrowNullPointerExceptionOnFetchNullFromKey
public void kafkatest_f18019_0()
{    cachingStore.fetch(null, keyA);}
f18019
0
shouldThrowNullPointerExceptionOnFetchNullToKey
public void kafkatest_f18020_0()
{    cachingStore.fetch(keyA, null);}
f18020
0
closeStore
public void kafkatest_f18029_0()
{    cachingStore.close();}
f18029
0
shouldNotReturnDuplicatesInRanges
public void kafkatest_f18030_0()
{    final StreamsBuilder builder = new StreamsBuilder();    final StoreBuilder<WindowStore<String, String>> storeBuilder = Stores.windowStoreBuilder(Stores.persistentWindowStore("store-name", ofHours(1L), ofMinutes(1L), false), Serdes.String(), Serdes.String()).withCachingEnabled();    builder.addStateStore(storeBuilder);    builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String())).transform(() -> new Transformer<String, String, KeyValue<String, String>>() {        private WindowStore<String, String> store;        private int numRecordsProcessed;        @SuppressWarnings("unchecked")        @Override        public void init(final ProcessorContext processorContext) {            this.store = (WindowStore<String, String>) processorContext.getStateStore("store-name");            int count = 0;            final KeyValueIterator<Windowed<String>, String> all = store.all();            while (all.hasNext()) {                count++;                all.next();            }            assertThat(count, equalTo(0));        }        @Override        public KeyValue<String, String> transform(final String key, final String value) {            int count = 0;            final KeyValueIterator<Windowed<String>, String> all = store.all();            while (all.hasNext()) {                count++;                all.next();            }            assertThat(count, equalTo(numRecordsProcessed));            store.put(value, value);            numRecordsProcessed++;            return new KeyValue<>(key, value);        }        @Override        public void close() {        }    }, "store-name");    final String bootstrapServers = "localhost:9092";    final Properties streamsConfiguration = new Properties();    streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, "test-app");    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 10 * 1000);    final long initialWallClockTime = 0L;    final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), streamsConfiguration, initialWallClockTime);    final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(Serdes.String().serializer(), Serdes.String().serializer(), initialWallClockTime);    for (int i = 0; i < 5; i++) {        driver.pipeInput(recordFactory.create(topic, UUID.randomUUID().toString(), UUID.randomUUID().toString()));    }    driver.advanceWallClockTime(10 * 1000L);    recordFactory.advanceTimeMs(10 * 1000L);    for (int i = 0; i < 5; i++) {        driver.pipeInput(recordFactory.create(topic, UUID.randomUUID().toString(), UUID.randomUUID().toString()));    }    driver.advanceWallClockTime(10 * 1000L);    recordFactory.advanceTimeMs(10 * 1000L);    for (int i = 0; i < 5; i++) {        driver.pipeInput(recordFactory.create(topic, UUID.randomUUID().toString(), UUID.randomUUID().toString()));    }    driver.advanceWallClockTime(10 * 1000L);    recordFactory.advanceTimeMs(10 * 1000L);    for (int i = 0; i < 5; i++) {        driver.pipeInput(recordFactory.create(topic, UUID.randomUUID().toString(), UUID.randomUUID().toString()));    }}
f18030
0
shouldGetAllFromCache
public void kafkatest_f18040_0()
{    cachingStore.put(bytesKey("a"), bytesValue("a"));    cachingStore.put(bytesKey("b"), bytesValue("b"));    cachingStore.put(bytesKey("c"), bytesValue("c"));    cachingStore.put(bytesKey("d"), bytesValue("d"));    cachingStore.put(bytesKey("e"), bytesValue("e"));    cachingStore.put(bytesKey("f"), bytesValue("f"));    cachingStore.put(bytesKey("g"), bytesValue("g"));    cachingStore.put(bytesKey("h"), bytesValue("h"));    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.all();    final String[] array = { "a", "b", "c", "d", "e", "f", "g", "h" };    for (final String s : array) {        verifyWindowedKeyValue(iterator.next(), new Windowed<>(bytesKey(s), new TimeWindow(DEFAULT_TIMESTAMP, DEFAULT_TIMESTAMP + WINDOW_SIZE)), s);    }    assertFalse(iterator.hasNext());}
f18040
0
shouldFetchAllWithinTimestampRange
public void kafkatest_f18041_0()
{    final String[] array = { "a", "b", "c", "d", "e", "f", "g", "h" };    for (int i = 0; i < array.length; i++) {        context.setTime(i);        cachingStore.put(bytesKey(array[i]), bytesValue(array[i]));    }    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator = cachingStore.fetchAll(ofEpochMilli(0), ofEpochMilli(7));    for (int i = 0; i < array.length; i++) {        final String str = array[i];        verifyWindowedKeyValue(iterator.next(), new Windowed<>(bytesKey(str), new TimeWindow(i, i + WINDOW_SIZE)), str);    }    assertFalse(iterator.hasNext());    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator1 = cachingStore.fetchAll(ofEpochMilli(2), ofEpochMilli(4));    for (int i = 2; i <= 4; i++) {        final String str = array[i];        verifyWindowedKeyValue(iterator1.next(), new Windowed<>(bytesKey(str), new TimeWindow(i, i + WINDOW_SIZE)), str);    }    assertFalse(iterator1.hasNext());    final KeyValueIterator<Windowed<Bytes>, byte[]> iterator2 = cachingStore.fetchAll(ofEpochMilli(5), ofEpochMilli(7));    for (int i = 5; i <= 7; i++) {        final String str = array[i];        verifyWindowedKeyValue(iterator2.next(), new Windowed<>(bytesKey(str), new TimeWindow(i, i + WINDOW_SIZE)), str);    }    assertFalse(iterator2.hasNext());}
f18041
0
shouldIterateCacheAndStore
public void kafkatest_f18050_0()
{    final Bytes key = Bytes.wrap("1".getBytes());    underlying.put(WindowKeySchema.toStoreKeyBinary(key, DEFAULT_TIMESTAMP, 0), "a".getBytes());    cachingStore.put(key, bytesValue("b"), DEFAULT_TIMESTAMP + WINDOW_SIZE);    final WindowStoreIterator<byte[]> fetch = cachingStore.fetch(bytesKey("1"), ofEpochMilli(DEFAULT_TIMESTAMP), ofEpochMilli(DEFAULT_TIMESTAMP + WINDOW_SIZE));    verifyKeyValue(fetch.next(), DEFAULT_TIMESTAMP, "a");    verifyKeyValue(fetch.next(), DEFAULT_TIMESTAMP + WINDOW_SIZE, "b");    assertFalse(fetch.hasNext());}
f18050
0
shouldIterateCacheAndStoreKeyRange
public void kafkatest_f18051_0()
{    final Bytes key = Bytes.wrap("1".getBytes());    underlying.put(WindowKeySchema.toStoreKeyBinary(key, DEFAULT_TIMESTAMP, 0), "a".getBytes());    cachingStore.put(key, bytesValue("b"), DEFAULT_TIMESTAMP + WINDOW_SIZE);    final KeyValueIterator<Windowed<Bytes>, byte[]> fetchRange = cachingStore.fetch(key, bytesKey("2"), ofEpochMilli(DEFAULT_TIMESTAMP), ofEpochMilli(DEFAULT_TIMESTAMP + WINDOW_SIZE));    verifyWindowedKeyValue(fetchRange.next(), new Windowed<>(key, new TimeWindow(DEFAULT_TIMESTAMP, DEFAULT_TIMESTAMP + WINDOW_SIZE)), "a");    verifyWindowedKeyValue(fetchRange.next(), new Windowed<>(key, new TimeWindow(DEFAULT_TIMESTAMP + WINDOW_SIZE, DEFAULT_TIMESTAMP + WINDOW_SIZE + WINDOW_SIZE)), "b");    assertFalse(fetchRange.hasNext());}
f18051
0
shouldNotThrowNullPointerExceptionOnPutNullValue
public void kafkatest_f18060_0()
{    cachingStore.put(bytesKey("a"), null);}
f18060
0
shouldThrowNullPointerExceptionOnFetchNullKey
public void kafkatest_f18061_0()
{    cachingStore.fetch(null, ofEpochMilli(1L), ofEpochMilli(2L));}
f18061
0
shouldWriteKeyValueBytesToInnerStoreOnPut
public void kafkatest_f18070_0()
{    store.put(hi, there);    assertThat(inner.get(hi), equalTo(there));}
f18070
0
shouldLogChangeOnPut
public void kafkatest_f18071_0()
{    store.put(hi, there);    assertThat(sent.get(hi), equalTo(there));}
f18071
0
shouldNotWriteToChangeLogOnPutIfAbsentWhenValueForKeyExists
public void kafkatest_f18080_0()
{    store.put(hi, there);    store.putIfAbsent(hi, world);    assertThat(sent.get(hi), equalTo(there));}
f18080
0
shouldReturnCurrentValueOnPutIfAbsent
public void kafkatest_f18081_0()
{    store.put(hi, there);    assertThat(store.putIfAbsent(hi, world), equalTo(there));}
f18081
0
shouldDelegateToUnderlyingStoreWhenFetching
public void kafkatest_f18090_0()
{    EasyMock.expect(inner.fetch(bytesKey)).andReturn(KeyValueIterators.<Windowed<Bytes>, byte[]>emptyIterator());    init();    store.fetch(bytesKey);    EasyMock.verify(inner);}
f18090
0
shouldDelegateToUnderlyingStoreWhenFetchingRange
public void kafkatest_f18091_0()
{    EasyMock.expect(inner.fetch(bytesKey, bytesKey)).andReturn(KeyValueIterators.<Windowed<Bytes>, byte[]>emptyIterator());    init();    store.fetch(bytesKey, bytesKey);    EasyMock.verify(inner);}
f18091
0
shouldLogChangeOnPut
public void kafkatest_f18100_0()
{    store.put(hi, rawThere);    final ValueAndTimestamp<byte[]> logged = sent.get(hi);    assertThat(logged.value(), equalTo(there.value()));    assertThat(logged.timestamp(), equalTo(there.timestamp()));}
f18100
0
shouldWriteAllKeyValueToInnerStoreOnPutAll
public void kafkatest_f18101_0()
{    store.putAll(Arrays.asList(KeyValue.pair(hi, rawThere), KeyValue.pair(hello, rawWorld)));    assertThat(root.get(hi), equalTo(rawThere));    assertThat(root.get(hello), equalTo(rawWorld));}
f18101
0
shouldReturnCurrentValueOnPutIfAbsent
public void kafkatest_f18110_0()
{    store.put(hi, rawThere);    assertThat(store.putIfAbsent(hi, rawWorld), equalTo(rawThere));}
f18110
0
shouldReturnNullOnPutIfAbsentWhenNoPreviousValue
public void kafkatest_f18111_0()
{    assertThat(store.putIfAbsent(hi, rawThere), is(nullValue()));}
f18111
0
shouldRetainDuplicatesWhenSet
public void kafkatest_f18120_0()
{    store = new ChangeLoggingTimestampedWindowBytesStore(inner, true);    inner.put(bytesKey, valueAndTimestamp, 0);    EasyMock.expectLastCall().times(2);    init();    store.put(bytesKey, valueAndTimestamp);    store.put(bytesKey, valueAndTimestamp);    assertArrayEquals(value, (byte[]) sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 1)).value());    assertEquals(42L, sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 1)).timestamp());    assertArrayEquals(value, (byte[]) sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 2)).value());    assertEquals(42L, sent.get(WindowKeySchema.toStoreKeyBinary(bytesKey, 0, 2)).timestamp());    EasyMock.verify(inner);}
f18120
0
send
public void kafkatest_f18121_0(final String topic, final K key, final V value, final Headers headers, final Integer partition, final Long timestamp, final Serializer<K> keySerializer, final Serializer<V> valueSerializer)
{    sent.put(key, value);}
f18121
0
shouldReturnNullIfKeyDoesntExist
public void kafkatest_f18130_0()
{    assertNull(theStore.get("whatever"));}
f18130
0
shouldThrowNullPointerExceptionOnGetNullKey
public void kafkatest_f18131_0()
{    theStore.get(null);}
f18131
0
shouldFindValueForKeyWhenMultiStores
public void kafkatest_f18140_0()
{    final KeyValueStore<String, String> cache = newStoreInstance();    stubProviderTwo.addStore(storeName, cache);    cache.put("key-two", "key-two-value");    stubOneUnderlying.put("key-one", "key-one-value");    assertEquals("key-two-value", theStore.get("key-two"));    assertEquals("key-one-value", theStore.get("key-one"));}
f18140
0
shouldSupportRange
public void kafkatest_f18141_0()
{    stubOneUnderlying.put("a", "a");    stubOneUnderlying.put("b", "b");    stubOneUnderlying.put("c", "c");    final List<KeyValue<String, String>> results = toList(theStore.range("a", "b"));    assertTrue(results.contains(new KeyValue<>("a", "a")));    assertTrue(results.contains(new KeyValue<>("b", "b")));    assertEquals(2, results.size());}
f18141
0
approximateNumEntries
public long kafkatest_f18150_0()
{    return Long.MAX_VALUE;}
f18150
0
shouldReturnLongMaxValueOnUnderflow
public void kafkatest_f18151_0()
{    stubProviderTwo.addStore(storeName, new NoOpReadOnlyStore<Object, Object>() {        @Override        public long approximateNumEntries() {            return Long.MAX_VALUE;        }    });    stubProviderTwo.addStore(storeNameA, new NoOpReadOnlyStore<Object, Object>() {        @Override        public long approximateNumEntries() {            return Long.MAX_VALUE;        }    });    assertEquals(Long.MAX_VALUE, theStore.approximateNumEntries());}
f18151
0
shouldThrowInvalidStateStoreExceptionOnRebalance
public void kafkatest_f18160_0()
{    final CompositeReadOnlySessionStore<String, String> store = new CompositeReadOnlySessionStore<>(new StateStoreProviderStub(true), QueryableStoreTypes.sessionStore(), "whateva");    store.fetch("a");}
f18160
0
shouldThrowInvalidStateStoreExceptionIfSessionFetchThrows
public void kafkatest_f18161_0()
{    underlyingSessionStore.setOpen(false);    try {        sessionStore.fetch("key");        fail("Should have thrown InvalidStateStoreException with session store");    } catch (final InvalidStateStoreException e) {    }}
f18161
0
shouldFindValueForKeyWhenMultiStores
public void kafkatest_f18170_0()
{    final ReadOnlyWindowStoreStub<String, String> secondUnderlying = new ReadOnlyWindowStoreStub<>(WINDOW_SIZE);    stubProviderTwo.addStore(storeName, secondUnderlying);    underlyingWindowStore.put("key-one", "value-one", 0L);    secondUnderlying.put("key-two", "value-two", 10L);    final List<KeyValue<Long, String>> keyOneResults = StreamsTestUtils.toList(windowStore.fetch("key-one", ofEpochMilli(0L), ofEpochMilli(1L)));    final List<KeyValue<Long, String>> keyTwoResults = StreamsTestUtils.toList(windowStore.fetch("key-two", ofEpochMilli(10L), ofEpochMilli(11L)));    assertEquals(Collections.singletonList(KeyValue.pair(0L, "value-one")), keyOneResults);    assertEquals(Collections.singletonList(KeyValue.pair(10L, "value-two")), keyTwoResults);}
f18170
0
shouldNotGetValuesFromOtherStores
public void kafkatest_f18171_0()
{    otherUnderlyingStore.put("some-key", "some-value", 0L);    underlyingWindowStore.put("some-key", "my-value", 1L);    final List<KeyValue<Long, String>> results = StreamsTestUtils.toList(windowStore.fetch("some-key", ofEpochMilli(0L), ofEpochMilli(2L)));    assertEquals(Collections.singletonList(new KeyValue<>(1L, "my-value")), results);}
f18171
0
shouldFetchAllAcrossStores
public void kafkatest_f18180_0()
{    final ReadOnlyWindowStoreStub<String, String> secondUnderlying = new ReadOnlyWindowStoreStub<>(WINDOW_SIZE);    stubProviderTwo.addStore(storeName, secondUnderlying);    underlyingWindowStore.put("a", "a", 0L);    secondUnderlying.put("b", "b", 10L);    final List<KeyValue<Windowed<String>, String>> results = StreamsTestUtils.toList(windowStore.fetchAll(ofEpochMilli(0), ofEpochMilli(10)));    assertThat(results, equalTo(Arrays.asList(KeyValue.pair(new Windowed<>("a", new TimeWindow(0, WINDOW_SIZE)), "a"), KeyValue.pair(new Windowed<>("b", new TimeWindow(10, 10 + WINDOW_SIZE)), "b"))));}
f18180
0
shouldThrowNPEIfKeyIsNull
public void kafkatest_f18181_0()
{    windowStore.fetch(null, ofEpochMilli(0), ofEpochMilli(0));}
f18181
0
key
public Bytes kafkatest_f18190_0(final Bytes cacheKey)
{    return cacheKey;}
f18190
0
cacheKey
public Bytes kafkatest_f18191_0(final Bytes key)
{    return key;}
f18191
0
shouldThrowUnsupportedOperationExeceptionOnRemove
public void kafkatest_f18200_0()
{    allIterator.remove();}
f18200
0
before
public void kafkatest_f18201_0()
{    stores.put("kv-store", Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore("kv-store"), Serdes.String(), Serdes.String()).build());    stores.put("ts-kv-store", Stores.timestampedKeyValueStoreBuilder(Stores.inMemoryKeyValueStore("ts-kv-store"), Serdes.String(), Serdes.String()).build());    stores.put("w-store", Stores.windowStoreBuilder(Stores.inMemoryWindowStore("w-store", Duration.ofMillis(10L), Duration.ofMillis(2L), false), Serdes.String(), Serdes.String()).build());    stores.put("ts-w-store", Stores.timestampedWindowStoreBuilder(Stores.inMemoryWindowStore("ts-w-store", Duration.ofMillis(10L), Duration.ofMillis(2L), false), Serdes.String(), Serdes.String()).build());    final ProcessorContextImpl mockContext = mock(ProcessorContextImpl.class);    expect(mockContext.applicationId()).andReturn("appId").anyTimes();    expect(mockContext.metrics()).andReturn(new StreamsMetricsImpl(new Metrics(), "threadName", StreamsConfig.METRICS_LATEST)).anyTimes();    expect(mockContext.taskId()).andReturn(new TaskId(0, 0)).anyTimes();    expect(mockContext.recordCollector()).andReturn(null).anyTimes();    replay(mockContext);    for (final StateStore store : stores.values()) {        store.init(mockContext, null);    }}
f18201
0
createKeyValueStore
protected KeyValueStore<K, V> kafkatest_f18210_0(final ProcessorContext context)
{    final StoreBuilder storeBuilder = Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore("my-store"), (Serde<K>) context.keySerde(), (Serde<V>) context.valueSerde()).withLoggingEnabled(Collections.singletonMap("retention.ms", "1000"));    final StateStore store = storeBuilder.build();    store.init(context, store);    return (KeyValueStore<K, V>) store;}
f18210
0
createKeyValueStore
protected KeyValueStore<K, V> kafkatest_f18211_0(final ProcessorContext context)
{    final StoreBuilder storeBuilder = Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore("my-store"), (Serde<K>) context.keySerde(), (Serde<V>) context.valueSerde());    final StateStore store = storeBuilder.build();    store.init(context, store);    return (KeyValueStore<K, V>) store;}
f18211
0
setClassLoggerToDebug
 void kafkatest_f18220_0()
{    LogCaptureAppender.setClassLoggerToDebug(InMemorySessionStore.class);}
f18220
0
shouldRemoveExpired
public void kafkatest_f18221_0()
{    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 0)), 1L);    sessionStore.put(new Windowed<>("aa", new SessionWindow(0, 10)), 2L);    sessionStore.put(new Windowed<>("a", new SessionWindow(10, 20)), 3L);    // Advance stream time to expire the first record    sessionStore.put(new Windowed<>("aa", new SessionWindow(10, RETENTION_PERIOD)), 4L);    try (final KeyValueIterator<Windowed<String>, Long> iterator = sessionStore.findSessions("a", "b", 0L, Long.MAX_VALUE)) {        assertEquals(valuesToSet(iterator), new HashSet<>(Arrays.asList(2L, 3L, 4L)));    }}
f18221
0
testExpiration
public void kafkatest_f18230_0()
{    long currentTime = 0;    setCurrentTime(currentTime);    windowStore.put(1, "one");    currentTime += RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "two");    currentTime += RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "three");    currentTime += RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "four");    // increase current time to the full RETENTION_PERIOD to expire first record    currentTime = currentTime + RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "five");    KeyValueIterator<Windowed<Integer>, String> iterator = windowStore.fetchAll(0L, currentTime);    // effect of this put (expires next oldest record, adds new one) should not be reflected in the already fetched results    currentTime = currentTime + RETENTION_PERIOD / 4;    setCurrentTime(currentTime);    windowStore.put(1, "six");    // should only have middle 4 values, as (only) the first record was expired at the time of the fetch    // and the last was inserted after the fetch    assertEquals(windowedPair(1, "two", RETENTION_PERIOD / 4), iterator.next());    assertEquals(windowedPair(1, "three", RETENTION_PERIOD / 2), iterator.next());    assertEquals(windowedPair(1, "four", 3 * (RETENTION_PERIOD / 4)), iterator.next());    assertEquals(windowedPair(1, "five", RETENTION_PERIOD), iterator.next());    assertFalse(iterator.hasNext());    iterator = windowStore.fetchAll(0L, currentTime);    // If we fetch again after the last put, the second oldest record should have expired and newest should appear in results    assertEquals(windowedPair(1, "three", RETENTION_PERIOD / 2), iterator.next());    assertEquals(windowedPair(1, "four", 3 * (RETENTION_PERIOD / 4)), iterator.next());    assertEquals(windowedPair(1, "five", RETENTION_PERIOD), iterator.next());    assertEquals(windowedPair(1, "six", 5 * (RETENTION_PERIOD / 4)), iterator.next());    assertFalse(iterator.hasNext());}
f18230
0
setup
public void kafkatest_f18231_0()
{    keyValueIteratorFacade = new KeyValueIteratorFacade<>(mockedKeyValueIterator);}
f18231
0
shouldGetSegmentNameFromId
public void kafkatest_f18240_0()
{    assertEquals("test.0", segments.segmentName(0));    assertEquals("test." + SEGMENT_INTERVAL, segments.segmentName(1));    assertEquals("test." + 2 * SEGMENT_INTERVAL, segments.segmentName(2));}
f18240
0
shouldCreateSegments
public void kafkatest_f18241_0()
{    final KeyValueSegment segment1 = segments.getOrCreateSegmentIfLive(0, context, -1L);    final KeyValueSegment segment2 = segments.getOrCreateSegmentIfLive(1, context, -1L);    final KeyValueSegment segment3 = segments.getOrCreateSegmentIfLive(2, context, -1L);    assertTrue(new File(context.stateDir(), "test/test.0").isDirectory());    assertTrue(new File(context.stateDir(), "test/test." + SEGMENT_INTERVAL).isDirectory());    assertTrue(new File(context.stateDir(), "test/test." + 2 * SEGMENT_INTERVAL).isDirectory());    assertTrue(segment1.isOpen());    assertTrue(segment2.isOpen());    assertTrue(segment3.isOpen());}
f18241
0
shouldRollSegments
public void kafkatest_f18250_0()
{    updateStreamTimeAndCreateSegment(0);    verifyCorrectSegments(0, 1);    updateStreamTimeAndCreateSegment(1);    verifyCorrectSegments(0, 2);    updateStreamTimeAndCreateSegment(2);    verifyCorrectSegments(0, 3);    updateStreamTimeAndCreateSegment(3);    verifyCorrectSegments(0, 4);    updateStreamTimeAndCreateSegment(4);    verifyCorrectSegments(0, 5);    updateStreamTimeAndCreateSegment(5);    verifyCorrectSegments(1, 5);    updateStreamTimeAndCreateSegment(6);    verifyCorrectSegments(2, 5);}
f18250
0
futureEventsShouldNotCauseSegmentRoll
public void kafkatest_f18251_0()
{    updateStreamTimeAndCreateSegment(0);    verifyCorrectSegments(0, 1);    updateStreamTimeAndCreateSegment(1);    verifyCorrectSegments(0, 2);    updateStreamTimeAndCreateSegment(2);    verifyCorrectSegments(0, 3);    updateStreamTimeAndCreateSegment(3);    verifyCorrectSegments(0, 4);    final long streamTime = updateStreamTimeAndCreateSegment(4);    verifyCorrectSegments(0, 5);    segments.getOrCreateSegmentIfLive(5, context, streamTime);    verifyCorrectSegments(0, 6);    segments.getOrCreateSegmentIfLive(6, context, streamTime);    verifyCorrectSegments(0, 7);}
f18251
0
shouldCompareSegmentIdOnly
public void kafkatest_f18260_0()
{    final KeyValueSegment segment1 = new KeyValueSegment("a", "C", 50L, metricsRecorder);    final KeyValueSegment segment2 = new KeyValueSegment("b", "B", 100L, metricsRecorder);    final KeyValueSegment segment3 = new KeyValueSegment("c", "A", 0L, metricsRecorder);    assertThat(segment1.compareTo(segment1), equalTo(0));    assertThat(segment1.compareTo(segment2), equalTo(-1));    assertThat(segment2.compareTo(segment1), equalTo(1));    assertThat(segment1.compareTo(segment3), equalTo(1));    assertThat(segment3.compareTo(segment1), equalTo(-1));    assertThat(segment2.compareTo(segment3), equalTo(1));    assertThat(segment3.compareTo(segment2), equalTo(-1));}
f18260
0
setUp
public void kafkatest_f18261_0()
{    EasyMock.expect(supplier.get()).andReturn(inner);    EasyMock.expect(supplier.name()).andReturn("name");    EasyMock.replay(supplier);    builder = new KeyValueStoreBuilder<>(supplier, Serdes.String(), Serdes.String(), new MockTime());}
f18261
0
shouldThrowNullPointerIfValueSerdeIsNull
public void kafkatest_f18270_0()
{    new KeyValueStoreBuilder<>(supplier, Serdes.String(), null, new MockTime());}
f18270
0
shouldThrowNullPointerIfTimeIsNull
public void kafkatest_f18271_0()
{    new KeyValueStoreBuilder<>(supplier, Serdes.String(), Serdes.String(), null);}
f18271
0
shouldSkipLargerDeletedCacheValue
public void kafkatest_f18280_0() throws Exception
{    final byte[][] bytes = { { 0 }, { 1 } };    store.put(Bytes.wrap(bytes[0]), bytes[0]);    cache.put(namespace, Bytes.wrap(bytes[1]), new LRUCacheEntry(null));    final MergedSortedCacheKeyValueBytesStoreIterator iterator = createIterator();    assertArrayEquals(bytes[0], iterator.next().key.get());    assertFalse(iterator.hasNext());}
f18280
0
shouldSkipSmallerDeletedCachedValue
public void kafkatest_f18281_0() throws Exception
{    final byte[][] bytes = { { 0 }, { 1 } };    cache.put(namespace, Bytes.wrap(bytes[0]), new LRUCacheEntry(null));    store.put(Bytes.wrap(bytes[1]), bytes[1]);    final MergedSortedCacheKeyValueBytesStoreIterator iterator = createIterator();    assertArrayEquals(bytes[1], iterator.next().key.get());    assertFalse(iterator.hasNext());}
f18281
0
shouldGetNextFromStore
public void kafkatest_f18290_0()
{    final MergedSortedCacheSessionStoreIterator mergeIterator = createIterator(storeKvs, Collections.emptyIterator());    assertThat(mergeIterator.next(), equalTo(KeyValue.pair(new Windowed<>(storeKey, storeWindow), storeKey.get())));}
f18290
0
shouldPeekNextKeyFromStore
public void kafkatest_f18291_0()
{    final MergedSortedCacheSessionStoreIterator mergeIterator = createIterator(storeKvs, Collections.emptyIterator());    assertThat(mergeIterator.peekNextKey(), equalTo(new Windowed<>(storeKey, storeWindow)));}
f18291
0
shouldPeekNextCacheKey
public void kafkatest_f18300_0()
{    windowStoreKvPairs.add(KeyValue.pair(0L, "a".getBytes()));    cache.put(namespace, SINGLE_SEGMENT_CACHE_FUNCTION.cacheKey(WindowKeySchema.toStoreKeyBinary("a", 10L, 0, stateSerdes)), new LRUCacheEntry("b".getBytes()));    final Bytes fromBytes = WindowKeySchema.toStoreKeyBinary("a", 0, 0, stateSerdes);    final Bytes toBytes = WindowKeySchema.toStoreKeyBinary("a", 100, 0, stateSerdes);    final KeyValueIterator<Long, byte[]> storeIterator = new DelegatingPeekingKeyValueIterator<>("store", new KeyValueIteratorStub<>(windowStoreKvPairs.iterator()));    final ThreadCache.MemoryLRUCacheBytesIterator cacheIterator = cache.range(namespace, SINGLE_SEGMENT_CACHE_FUNCTION.cacheKey(fromBytes), SINGLE_SEGMENT_CACHE_FUNCTION.cacheKey(toBytes));    final MergedSortedCacheWindowStoreIterator iterator = new MergedSortedCacheWindowStoreIterator(cacheIterator, storeIterator);    assertThat(iterator.peekNextKey(), equalTo(0L));    iterator.next();    assertThat(iterator.peekNextKey(), equalTo(10L));    iterator.close();}
f18300
0
segmentId
public long kafkatest_f18301_0(final Bytes key)
{    return 0;}
f18301
0
convertWindowedKey
private Windowed<String> kafkatest_f18310_0(final Windowed<Bytes> bytesWindowed)
{    final String key = deserializer.deserialize("", bytesWindowed.key().get());    return new Windowed<>(key, bytesWindowed.window());}
f18310
0
createIterator
private MergedSortedCacheWindowStoreKeyValueIterator kafkatest_f18311_0(final Iterator<KeyValue<Windowed<Bytes>, byte[]>> storeKvs, final Iterator<KeyValue<Bytes, LRUCacheEntry>> cacheKvs)
{    final DelegatingPeekingKeyValueIterator<Windowed<Bytes>, byte[]> storeIterator = new DelegatingPeekingKeyValueIterator<>("store", new KeyValueIteratorStub<>(storeKvs));    final PeekingKeyValueIterator<Bytes, LRUCacheEntry> cacheIterator = new DelegatingPeekingKeyValueIterator<>("cache", new KeyValueIteratorStub<>(cacheKvs));    return new MergedSortedCacheWindowStoreKeyValueIterator(cacheIterator, storeIterator, new StateSerdes<>("name", Serdes.Bytes(), Serdes.ByteArray()), WINDOW_SIZE, SINGLE_SEGMENT_CACHE_FUNCTION);}
f18311
0
shouldDeleteFromInnerStoreAndRecordDeleteMetric
public void kafkatest_f18320_0()
{    expect(inner.delete(keyBytes)).andReturn(valueBytes);    init();    metered.delete(key);    final KafkaMetric metric = metric("delete-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18320
0
shouldGetRangeFromInnerStoreAndRecordRangeMetric
public void kafkatest_f18321_0()
{    expect(inner.range(keyBytes, keyBytes)).andReturn(new KeyValueIteratorStub<>(Collections.singletonList(byteKeyValuePair).iterator()));    init();    final KeyValueIterator<String, String> iterator = metered.range(key, key);    assertThat(iterator.next().value, equalTo(value));    assertFalse(iterator.hasNext());    iterator.close();    final KafkaMetric metric = metric("range-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18321
0
testMetrics
public void kafkatest_f18330_0()
{    init();    final JmxReporter reporter = new JmxReporter("kafka.streams");    metrics.addReporter(reporter);    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", taskId.toString(), "scope", "metered")));    assertTrue(reporter.containsMbean(String.format("kafka.streams:type=stream-%s-state-metrics,client-id=%s,task-id=%s,%s-state-id=%s", "scope", "test", taskId.toString(), "scope", "all")));}
f18330
0
shouldWriteBytesToInnerStoreAndRecordPutMetric
public void kafkatest_f18331_0()
{    inner.put(eq(windowedKeyBytes), aryEq(keyBytes));    expectLastCall();    init();    metered.put(new Windowed<>(key, new SessionWindow(0, 0)), key);    final KafkaMetric metric = metric("put-rate");    assertTrue(((Double) metric.metricValue()) > 0);    verify(inner);}
f18331
0
shouldThrowNullPointerOnRemoveIfKeyIsNull
public void kafkatest_f18340_0()
{    metered.remove(null);}
f18340
0
shouldThrowNullPointerOnFetchIfKeyIsNull
public void kafkatest_f18341_0()
{    metered.fetch(null);}
f18341
0
before
public void kafkatest_f18350_0()
{    metered = new MeteredTimestampedKeyValueStore<>(inner, "scope", new MockTime(), Serdes.String(), new ValueAndTimestampSerde<>(Serdes.String()));    metrics.config().recordLevel(Sensor.RecordingLevel.DEBUG);    expect(context.metrics()).andReturn(new MockStreamsMetrics(metrics));    expect(context.taskId()).andReturn(taskId);    expect(inner.name()).andReturn("metered").anyTimes();}
f18350
0
init
private void kafkatest_f18351_0()
{    replay(inner, context);    metered.init(context, metered);}
f18351
0
shouldGetAllFromInnerStoreAndRecordAllMetric
public void kafkatest_f18360_0()
{    expect(inner.all()).andReturn(new KeyValueIteratorStub<>(Collections.singletonList(byteKeyValueTimestampPair).iterator()));    init();    final KeyValueIterator<String, ValueAndTimestamp<String>> iterator = metered.all();    assertThat(iterator.next().value, equalTo(valueAndTimestamp));    assertFalse(iterator.hasNext());    iterator.close();    final KafkaMetric metric = metric(new MetricName("all-rate", "stream-scope-state-metrics", "", tags));    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18360
0
shouldFlushInnerWhenFlushTimeRecords
public void kafkatest_f18361_0()
{    inner.flush();    expectLastCall().once();    init();    metered.flush();    final KafkaMetric metric = metric("flush-rate");    assertTrue((Double) metric.metricValue() > 0);    verify(inner);}
f18361
0
shouldNotThrowExceptionIfSerdesCorrectlySetFromProcessorContext
public void kafkatest_f18370_0()
{    EasyMock.expect(innerStoreMock.name()).andStubReturn("mocked-store");    EasyMock.replay(innerStoreMock);    final MeteredTimestampedWindowStore<String, Long> store = new MeteredTimestampedWindowStore<>(innerStoreMock, // any size    10L, "scope", new MockTime(), null, null);    store.init(context, innerStoreMock);    try {        store.put("key", ValueAndTimestamp.make(42L, 60000));    } catch (final StreamsException exception) {        if (exception.getCause() instanceof ClassCastException) {            fail("Serdes are not correctly set from processor context.");        }        throw exception;    }}
f18370
0
shouldNotThrowExceptionIfSerdesCorrectlySetFromConstructorParameters
public void kafkatest_f18371_0()
{    EasyMock.expect(innerStoreMock.name()).andStubReturn("mocked-store");    EasyMock.replay(innerStoreMock);    final MeteredTimestampedWindowStore<String, Long> store = new MeteredTimestampedWindowStore<>(innerStoreMock, // any size    10L, "scope", new MockTime(), Serdes.String(), new ValueAndTimestampSerde<>(Serdes.Long()));    store.init(context, innerStoreMock);    try {        store.put("key", ValueAndTimestamp.make(42L, 60000));    } catch (final StreamsException exception) {        if (exception.getCause() instanceof ClassCastException) {            fail("Serdes are not correctly set from constructor parameters.");        }        throw exception;    }}
f18371
0
shouldNotThrowNullPointerExceptionIfFetchReturnsNull
public void kafkatest_f18380_0()
{    expect(innerStoreMock.fetch(Bytes.wrap("a".getBytes()), 0)).andReturn(null);    replay(innerStoreMock);    store.init(context, store);    assertNull(store.fetch("a", 0));}
f18380
0
shouldSetFlushListenerOnWrappedCachingStore
public void kafkatest_f18381_0()
{    final CachedWindowStore cachedWindowStore = mock(CachedWindowStore.class);    expect(cachedWindowStore.setFlushListener(anyObject(CacheFlushListener.class), eq(false))).andReturn(true);    replay(cachedWindowStore);    final MeteredWindowStore<String, String> metered = new MeteredWindowStore<>(cachedWindowStore, // any size    10L, "scope", new MockTime(), Serdes.String(), new SerdeThatDoesntHandleNull());    assertTrue(metered.setFlushListener(null, false));    verify(cachedWindowStore);}
f18381
0
shouldGetMemtableHitRatioSensor
public void kafkatest_f18390_0()
{    final String metricNamePrefix = "memtable-hit-ratio";    final String description = "Ratio of memtable hits relative to all lookups to the memtable";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::memtableHitRatioSensor);}
f18390
0
shouldGetMemtableBytesFlushedSensor
public void kafkatest_f18391_0()
{    final String metricNamePrefix = "memtable-bytes-flushed";    final String descriptionOfTotal = "Total number of bytes flushed from the memtable to disk";    final String descriptionOfRate = "Average number of bytes flushed per second from the memtable to disk";    verifyRateAndTotalSensor(metricNamePrefix, descriptionOfTotal, descriptionOfRate, RocksDBMetrics::memtableBytesFlushedSensor);}
f18391
0
shouldGetBytesWrittenDuringCompactionSensor
public void kafkatest_f18400_0()
{    final String metricNamePrefix = "bytes-written-compaction";    final String description = "Average number of bytes written per second during compaction";    verifyRateSensor(metricNamePrefix, description, RocksDBMetrics::bytesWrittenDuringCompactionSensor);}
f18400
0
shouldGetCompactionTimeAvgSensor
public void kafkatest_f18401_0()
{    final String metricNamePrefix = "compaction-time-avg";    final String description = "Average time spent on compaction in ms";    verifyValueSensor(metricNamePrefix, description, RocksDBMetrics::compactionTimeAvgSensor);}
f18401
0
replayCallAndVerify
private void kafkatest_f18410_0(final SensorCreator sensorCreator)
{    replayAll();    replay(StreamsMetricsImpl.class);    final Sensor sensor = sensorCreator.sensor(streamsMetrics, new RocksDBMetricContext(taskName, storeType, storeName));    verifyAll();    verify(StreamsMetricsImpl.class);    assertThat(sensor, is(this.sensor));}
f18410
0
setUp
public void kafkatest_f18411_0()
{    innerMetrics = new Metrics();    metrics = new MockStreamsMetrics(innerMetrics);    cache = new NamedCache(taskIDString + "-" + underlyingStoreName, metrics);}
f18411
0
shouldEvictEldestEntry
public void kafkatest_f18420_0()
{    cache.put(Bytes.wrap(new byte[] { 0 }), new LRUCacheEntry(new byte[] { 10 }));    cache.put(Bytes.wrap(new byte[] { 1 }), new LRUCacheEntry(new byte[] { 20 }));    cache.put(Bytes.wrap(new byte[] { 2 }), new LRUCacheEntry(new byte[] { 30 }));    cache.evict();    assertNull(cache.get(Bytes.wrap(new byte[] { 0 })));    assertEquals(2, cache.size());}
f18420
0
shouldFlushDirtEntriesOnEviction
public void kafkatest_f18421_0()
{    final List<ThreadCache.DirtyEntry> flushed = new ArrayList<>();    cache.put(Bytes.wrap(new byte[] { 0 }), new LRUCacheEntry(new byte[] { 10 }, headers, true, 0, 0, 0, ""));    cache.put(Bytes.wrap(new byte[] { 1 }), new LRUCacheEntry(new byte[] { 20 }));    cache.put(Bytes.wrap(new byte[] { 2 }), new LRUCacheEntry(new byte[] { 30 }, headers, true, 0, 0, 0, ""));    cache.setListener(new ThreadCache.DirtyEntryFlushListener() {        @Override        public void apply(final List<ThreadCache.DirtyEntry> dirty) {            flushed.addAll(dirty);        }    });    cache.evict();    assertEquals(2, flushed.size());    assertEquals(Bytes.wrap(new byte[] { 0 }), flushed.get(0).key());    assertEquals(headers, flushed.get(0).entry().context().headers());    assertArrayEquals(new byte[] { 10 }, flushed.get(0).newValue());    assertEquals(Bytes.wrap(new byte[] { 2 }), flushed.get(1).key());    assertArrayEquals(new byte[] { 30 }, flushed.get(1).newValue());    assertEquals(cache.flushes(), 1);}
f18421
0
apply
public void kafkatest_f18430_0(final List<ThreadCache.DirtyEntry> dirty)
{    cache.put(key, clean);}
f18430
0
shouldReturnNullIfKeyIsNull
public void kafkatest_f18431_0()
{    assertNull(cache.get(null));}
f18431
0
shouldThrowExceptionWhenLookingForKVStoreWithDifferentType
public void kafkatest_f18440_0()
{    storeProvider.getStore(keyValueStore, QueryableStoreTypes.windowStore());}
f18440
0
shouldFindGlobalStores
public void kafkatest_f18441_0()
{    globalStateStores.put("global", new NoOpReadOnlyStore<>());    assertNotNull(storeProvider.getStore("global", QueryableStoreTypes.keyValueStore()));}
f18441
0
shouldReturnPlainKeyValuePairsOnSingleKeyFetchInstantParameters
public void kafkatest_f18450_0()
{    expect(mockedWindowTimestampIterator.next()).andReturn(KeyValue.pair(21L, ValueAndTimestamp.make("value1", 22L))).andReturn(KeyValue.pair(42L, ValueAndTimestamp.make("value2", 23L)));    expect(mockedWindowTimestampStore.fetch("key1", Instant.ofEpochMilli(21L), Instant.ofEpochMilli(42L))).andReturn(mockedWindowTimestampIterator);    replay(mockedWindowTimestampIterator, mockedWindowTimestampStore);    final WindowStoreIterator<String> iterator = readOnlyWindowStoreFacade.fetch("key1", Instant.ofEpochMilli(21L), Instant.ofEpochMilli(42L));    assertThat(iterator.next(), is(KeyValue.pair(21L, "value1")));    assertThat(iterator.next(), is(KeyValue.pair(42L, "value2")));    verify(mockedWindowTimestampIterator, mockedWindowTimestampStore);}
f18450
0
shouldReturnPlainKeyValuePairsOnRangeFetchLongParameters
public void kafkatest_f18451_0()
{    expect(mockedKeyValueWindowTimestampIterator.next()).andReturn(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), ValueAndTimestamp.make("value1", 22L))).andReturn(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), ValueAndTimestamp.make("value2", 100L)));    expect(mockedWindowTimestampStore.fetch("key1", "key2", 21L, 42L)).andReturn(mockedKeyValueWindowTimestampIterator);    replay(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);    final KeyValueIterator<Windowed<String>, String> iterator = readOnlyWindowStoreFacade.fetch("key1", "key2", 21L, 42L);    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key1", new TimeWindow(21L, 22L)), "value1")));    assertThat(iterator.next(), is(KeyValue.pair(new Windowed<>("key2", new TimeWindow(42L, 43L)), "value2")));    verify(mockedKeyValueWindowTimestampIterator, mockedWindowTimestampStore);}
f18451
0
peekNextKey
public Windowed<K> kafkatest_f18461_0()
{    throw new UnsupportedOperationException("peekNextKey() not supported in " + getClass().getName());}
f18461
0
hasNext
public boolean kafkatest_f18462_0()
{    return iterator.hasNext();}
f18462
0
hasNext
public boolean kafkatest_f18473_0()
{    return iterator.hasNext();}
f18473
0
next
public KeyValue<Windowed<K>, V> kafkatest_f18474_0()
{    return iterator.next();}
f18474
0
next
public KeyValue<Long, E> kafkatest_f18487_0()
{    return underlying.next();}
f18487
0
shouldPreserveNullValueOnConversion
public void kafkatest_f18488_0()
{    final ConsumerRecord<byte[], byte[]> nullValueRecord = new ConsumerRecord<>("", 0, 0L, new byte[0], null);    assertNull(timestampedValueConverter.convert(nullValueRecord).value());}
f18488
0
createCompactionFilter
public AbstractCompactionFilter<?> kafkatest_f18498_0(final Context context)
{    return null;}
f18498
0
name
public String kafkatest_f18499_0()
{    return "AbstractCompactionFilterFactory";}
f18499
0
getOptions
 Options kafkatest_f18508_0(final KeyValueSegment segment)
{    return segment.getOptions();}
f18508
0
buildSessionStore
 SessionStore<K, V> kafkatest_f18509_0(final long retentionPeriod, final Serde<K> keySerde, final Serde<V> valueSerde)
{    return Stores.sessionStoreBuilder(Stores.persistentSessionStore(STORE_NAME, ofMillis(retentionPeriod)), keySerde, valueSerde).build();}
f18509
0
setConfig
public void kafkatest_f18518_0(final String storeName, final Options options, final Map<String, Object> configs)
{    options.setStatistics(new Statistics());}
f18518
0
close
public void kafkatest_f18519_0(final String storeName, final Options options)
{    options.statistics().close();}
f18519
0
shouldRestoreAll
public void kafkatest_f18528_0()
{    final List<KeyValue<byte[], byte[]>> entries = getKeyValueEntries();    rocksDBStore.init(context, rocksDBStore);    context.restore(rocksDBStore.name(), entries);    assertEquals("a", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "1")))));    assertEquals("b", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "2")))));    assertEquals("c", stringDeserializer.deserialize(null, rocksDBStore.get(new Bytes(stringSerializer.serialize(null, "3")))));}
f18528
0
shouldPutOnlyIfAbsentValue
public void kafkatest_f18529_0()
{    rocksDBStore.init(context, rocksDBStore);    final Bytes keyBytes = new Bytes(stringSerializer.serialize(null, "one"));    final byte[] valueBytes = stringSerializer.serialize(null, "A");    final byte[] valueBytesUpdate = stringSerializer.serialize(null, "B");    rocksDBStore.putIfAbsent(keyBytes, valueBytes);    rocksDBStore.putIfAbsent(keyBytes, valueBytesUpdate);    final String retrievedValue = stringDeserializer.deserialize(null, rocksDBStore.get(keyBytes));    assertEquals("A", retrievedValue);}
f18529
0
shouldThrowProcessorStateExceptionOnPutDeletedDir
public void kafkatest_f18538_0() throws IOException
{    rocksDBStore.init(context, rocksDBStore);    Utils.delete(dir);    rocksDBStore.put(new Bytes(stringSerializer.serialize(null, "anyKey")), stringSerializer.serialize(null, "anyValue"));    rocksDBStore.flush();}
f18538
0
shouldHandleToggleOfEnablingBloomFilters
public void kafkatest_f18539_0()
{    final Properties props = StreamsTestUtils.getStreamsConfig();    props.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG, TestingBloomFilterRocksDBConfigSetter.class);    rocksDBStore = getRocksDBStore();    dir = TestUtils.tempDirectory();    context = new InternalMockProcessorContext(dir, Serdes.String(), Serdes.String(), new StreamsConfig(props));    enableBloomFilters = false;    rocksDBStore.init(context, rocksDBStore);    final List<String> expectedValues = new ArrayList<>();    expectedValues.add("a");    expectedValues.add("b");    expectedValues.add("c");    final List<KeyValue<byte[], byte[]>> keyValues = getKeyValueEntries();    for (final KeyValue<byte[], byte[]> keyValue : keyValues) {        rocksDBStore.put(new Bytes(keyValue.key), keyValue.value);    }    int expectedIndex = 0;    for (final KeyValue<byte[], byte[]> keyValue : keyValues) {        final byte[] valBytes = rocksDBStore.get(new Bytes(keyValue.key));        assertThat(new String(valBytes, UTF_8), is(expectedValues.get(expectedIndex++)));    }    assertFalse(TestingBloomFilterRocksDBConfigSetter.bloomFiltersSet);    rocksDBStore.close();    expectedIndex = 0;    // reopen with Bloom Filters enabled    // should open fine without errors    enableBloomFilters = true;    rocksDBStore.init(context, rocksDBStore);    for (final KeyValue<byte[], byte[]> keyValue : keyValues) {        final byte[] valBytes = rocksDBStore.get(new Bytes(keyValue.key));        assertThat(new String(valBytes, UTF_8), is(expectedValues.get(expectedIndex++)));    }    assertTrue(TestingBloomFilterRocksDBConfigSetter.bloomFiltersSet);}
f18539
0
shouldOpenNewStoreInRegularMode
public void kafkatest_f18548_0()
{    LogCaptureAppender.setClassLoggerToDebug(RocksDBTimestampedStore.class);    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    rocksDBStore.init(context, rocksDBStore);    assertThat(appender.getMessages(), hasItem("Opening store " + DB_NAME + " in regular mode"));    LogCaptureAppender.unregister(appender);    try (final KeyValueIterator<Bytes, byte[]> iterator = rocksDBStore.all()) {        assertThat(iterator.hasNext(), is(false));    }}
f18548
0
shouldOpenExistingStoreInRegularMode
public void kafkatest_f18549_0() throws Exception
{    LogCaptureAppender.setClassLoggerToDebug(RocksDBTimestampedStore.class);    // prepare store    rocksDBStore.init(context, rocksDBStore);    rocksDBStore.put(new Bytes("key".getBytes()), "timestamped".getBytes());    rocksDBStore.close();    // re-open store    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    rocksDBStore = getRocksDBStore();    rocksDBStore.init(context, rocksDBStore);    assertThat(appender.getMessages(), hasItem("Opening store " + DB_NAME + " in regular mode"));    LogCaptureAppender.unregister(appender);    rocksDBStore.close();    // verify store    final DBOptions dbOptions = new DBOptions();    final ColumnFamilyOptions columnFamilyOptions = new ColumnFamilyOptions();    final List<ColumnFamilyDescriptor> columnFamilyDescriptors = asList(new ColumnFamilyDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY, columnFamilyOptions), new ColumnFamilyDescriptor("keyValueWithTimestamp".getBytes(StandardCharsets.UTF_8), columnFamilyOptions));    final List<ColumnFamilyHandle> columnFamilies = new ArrayList<>(columnFamilyDescriptors.size());    RocksDB db = null;    ColumnFamilyHandle noTimestampColumnFamily = null, withTimestampColumnFamily = null;    try {        db = RocksDB.open(dbOptions, new File(new File(context.stateDir(), "rocksdb"), DB_NAME).getAbsolutePath(), columnFamilyDescriptors, columnFamilies);        noTimestampColumnFamily = columnFamilies.get(0);        withTimestampColumnFamily = columnFamilies.get(1);        assertThat(db.get(noTimestampColumnFamily, "key".getBytes()), new IsNull<>());        assertThat(db.getLongProperty(noTimestampColumnFamily, "rocksdb.estimate-num-keys"), is(0L));        assertThat(db.get(withTimestampColumnFamily, "key".getBytes()).length, is(11));        assertThat(db.getLongProperty(withTimestampColumnFamily, "rocksdb.estimate-num-keys"), is(1L));    } finally {        // Order of closing must follow: ColumnFamilyHandle > RocksDB > DBOptions > ColumnFamilyOptions        if (noTimestampColumnFamily != null) {            noTimestampColumnFamily.close();        }        if (withTimestampColumnFamily != null) {            withTimestampColumnFamily.close();        }        if (db != null) {            db.close();        }        dbOptions.close();        columnFamilyOptions.close();    }}
f18549
0
testRolling
public void kafkatest_f18558_0()
{    // to validate segments    final long startTime = SEGMENT_INTERVAL * 2;    final long increment = SEGMENT_INTERVAL / 2;    setCurrentTime(startTime);    windowStore.put(0, "zero");    assertEquals(Utils.mkSet(segments.segmentName(2)), segmentDirs(baseDir));    setCurrentTime(startTime + increment);    windowStore.put(1, "one");    assertEquals(Utils.mkSet(segments.segmentName(2)), segmentDirs(baseDir));    setCurrentTime(startTime + increment * 2);    windowStore.put(2, "two");    assertEquals(Utils.mkSet(segments.segmentName(2), segments.segmentName(3)), segmentDirs(baseDir));    setCurrentTime(startTime + increment * 4);    windowStore.put(4, "four");    assertEquals(Utils.mkSet(segments.segmentName(2), segments.segmentName(3), segments.segmentName(4)), segmentDirs(baseDir));    setCurrentTime(startTime + increment * 5);    windowStore.put(5, "five");    assertEquals(Utils.mkSet(segments.segmentName(2), segments.segmentName(3), segments.segmentName(4)), segmentDirs(baseDir));    assertEquals(new HashSet<>(Collections.singletonList("zero")), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("one")), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("two")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    setCurrentTime(startTime + increment * 6);    windowStore.put(6, "six");    assertEquals(Utils.mkSet(segments.segmentName(3), segments.segmentName(4), segments.segmentName(5)), segmentDirs(baseDir));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("two")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("six")), toSet(windowStore.fetch(6, ofEpochMilli(startTime + increment * 6 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 6 + WINDOW_SIZE))));    setCurrentTime(startTime + increment * 7);    windowStore.put(7, "seven");    assertEquals(Utils.mkSet(segments.segmentName(3), segments.segmentName(4), segments.segmentName(5)), segmentDirs(baseDir));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("two")), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("six")), toSet(windowStore.fetch(6, ofEpochMilli(startTime + increment * 6 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 6 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("seven")), toSet(windowStore.fetch(7, ofEpochMilli(startTime + increment * 7 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 7 + WINDOW_SIZE))));    setCurrentTime(startTime + increment * 8);    windowStore.put(8, "eight");    assertEquals(Utils.mkSet(segments.segmentName(4), segments.segmentName(5), segments.segmentName(6)), segmentDirs(baseDir));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(0, ofEpochMilli(startTime - WINDOW_SIZE), ofEpochMilli(startTime + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(1, ofEpochMilli(startTime + increment - WINDOW_SIZE), ofEpochMilli(startTime + increment + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(2, ofEpochMilli(startTime + increment * 2 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 2 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.emptyList()), toSet(windowStore.fetch(3, ofEpochMilli(startTime + increment * 3 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 3 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("four")), toSet(windowStore.fetch(4, ofEpochMilli(startTime + increment * 4 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 4 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("five")), toSet(windowStore.fetch(5, ofEpochMilli(startTime + increment * 5 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 5 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("six")), toSet(windowStore.fetch(6, ofEpochMilli(startTime + increment * 6 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 6 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("seven")), toSet(windowStore.fetch(7, ofEpochMilli(startTime + increment * 7 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 7 + WINDOW_SIZE))));    assertEquals(new HashSet<>(Collections.singletonList("eight")), toSet(windowStore.fetch(8, ofEpochMilli(startTime + increment * 8 - WINDOW_SIZE), ofEpochMilli(startTime + increment * 8 + WINDOW_SIZE))));    // check segment directories    windowStore.flush();    assertEquals(Utils.mkSet(segments.segmentName(4), segments.segmentName(5), segments.segmentName(6)), segmentDirs(baseDir));}
f18558
0
testSegmentMaintenance
public void kafkatest_f18559_0()
{    windowStore = buildWindowStore(RETENTION_PERIOD, WINDOW_SIZE, true, Serdes.Integer(), Serdes.String());    windowStore.init(context, windowStore);    context.setTime(0L);    setCurrentTime(0);    windowStore.put(0, "v");    assertEquals(Utils.mkSet(segments.segmentName(0L)), segmentDirs(baseDir));    setCurrentTime(SEGMENT_INTERVAL - 1);    windowStore.put(0, "v");    windowStore.put(0, "v");    assertEquals(Utils.mkSet(segments.segmentName(0L)), segmentDirs(baseDir));    setCurrentTime(SEGMENT_INTERVAL);    windowStore.put(0, "v");    assertEquals(Utils.mkSet(segments.segmentName(0L), segments.segmentName(1L)), segmentDirs(baseDir));    WindowStoreIterator iter;    int fetchedCount;    iter = windowStore.fetch(0, ofEpochMilli(0L), ofEpochMilli(SEGMENT_INTERVAL * 4));    fetchedCount = 0;    while (iter.hasNext()) {        iter.next();        fetchedCount++;    }    assertEquals(4, fetchedCount);    assertEquals(Utils.mkSet(segments.segmentName(0L), segments.segmentName(1L)), segmentDirs(baseDir));    setCurrentTime(SEGMENT_INTERVAL * 3);    windowStore.put(0, "v");    iter = windowStore.fetch(0, ofEpochMilli(0L), ofEpochMilli(SEGMENT_INTERVAL * 4));    fetchedCount = 0;    while (iter.hasNext()) {        iter.next();        fetchedCount++;    }    assertEquals(2, fetchedCount);    assertEquals(Utils.mkSet(segments.segmentName(1L), segments.segmentName(3L)), segmentDirs(baseDir));    setCurrentTime(SEGMENT_INTERVAL * 5);    windowStore.put(0, "v");    iter = windowStore.fetch(0, ofEpochMilli(SEGMENT_INTERVAL * 4), ofEpochMilli(SEGMENT_INTERVAL * 10));    fetchedCount = 0;    while (iter.hasNext()) {        iter.next();        fetchedCount++;    }    assertEquals(1, fetchedCount);    assertEquals(Utils.mkSet(segments.segmentName(3L), segments.segmentName(5L)), segmentDirs(baseDir));}
f18559
0
closeSegments
public void kafkatest_f18568_0()
{    if (iterator != null) {        iterator.close();        iterator = null;    }    segmentOne.close();    segmentTwo.close();}
f18568
0
shouldIterateOverAllSegments
public void kafkatest_f18569_0()
{    iterator = new SegmentIterator<>(Arrays.asList(segmentOne, segmentTwo).iterator(), hasNextCondition, Bytes.wrap("a".getBytes()), Bytes.wrap("z".getBytes()));    assertTrue(iterator.hasNext());    assertEquals("a", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("a", "1"), toStringKeyValue(iterator.next()));    assertTrue(iterator.hasNext());    assertEquals("b", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("b", "2"), toStringKeyValue(iterator.next()));    assertTrue(iterator.hasNext());    assertEquals("c", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("c", "3"), toStringKeyValue(iterator.next()));    assertTrue(iterator.hasNext());    assertEquals("d", new String(iterator.peekNextKey().get()));    assertEquals(KeyValue.pair("d", "4"), toStringKeyValue(iterator.next()));    assertFalse(iterator.hasNext());}
f18569
0
createRecordCollector
private RecordCollectorImpl kafkatest_f18578_0(final String name)
{    return new RecordCollectorImpl(name, new LogContext(name), new DefaultProductionExceptionHandler(), new Metrics().sensor("skipped-records")) {        @Override        public <K1, V1> void send(final String topic, final K1 key, final V1 value, final Headers headers, final Integer partition, final Long timestamp, final Serializer<K1> keySerializer, final Serializer<V1> valueSerializer) {            changeLog.add(new KeyValue<>(keySerializer.serialize(topic, headers, key), valueSerializer.serialize(topic, headers, value)));        }    };}
f18578
0
send
public void kafkatest_f18579_0(final String topic, final K1 key, final V1 value, final Headers headers, final Integer partition, final Long timestamp, final Serializer<K1> keySerializer, final Serializer<V1> valueSerializer)
{    changeLog.add(new KeyValue<>(keySerializer.serialize(topic, headers, key), valueSerializer.serialize(topic, headers, value)));}
f18579
0
shouldRemove
public void kafkatest_f18588_0()
{    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 1000)), 1L);    sessionStore.put(new Windowed<>("a", new SessionWindow(1500, 2500)), 2L);    sessionStore.remove(new Windowed<>("a", new SessionWindow(0, 1000)));    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 0L, 1000L)) {        assertFalse(results.hasNext());    }    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 1500L, 2500L)) {        assertTrue(results.hasNext());    }}
f18588
0
shouldRemoveOnNullAggValue
public void kafkatest_f18589_0()
{    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 1000)), 1L);    sessionStore.put(new Windowed<>("a", new SessionWindow(1500, 2500)), 2L);    sessionStore.put(new Windowed<>("a", new SessionWindow(0, 1000)), null);    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 0L, 1000L)) {        assertFalse(results.hasNext());    }    try (final KeyValueIterator<Windowed<String>, Long> results = sessionStore.findSessions("a", 1500L, 2500L)) {        assertTrue(results.hasNext());    }}
f18589
0
shouldNotThrowExceptionRemovingNonexistentKey
public void kafkatest_f18598_0()
{    sessionStore.remove(new Windowed<>("a", new SessionWindow(0, 1)));}
f18598
0
shouldThrowNullPointerExceptionOnFindSessionsNullKey
public void kafkatest_f18599_0()
{    sessionStore.findSessions(null, 1L, 2L);}
f18599
0
valuesToSet
protected static Set<V> kafkatest_f18608_0(final Iterator<KeyValue<K, V>> iterator)
{    final Set<V> results = new HashSet<>();    while (iterator.hasNext()) {        results.add(iterator.next().value);    }    return results;}
f18608
0
toSet
protected static Set<KeyValue<K, V>> kafkatest_f18609_0(final Iterator<KeyValue<K, V>> iterator)
{    final Set<KeyValue<K, V>> results = new HashSet<>();    while (iterator.hasNext()) {        results.add(iterator.next());    }    return results;}
f18609
0
testLowerBoundMatchesTrailingZeros
public void kafkatest_f18618_0()
{    final Bytes lower = sessionKeySchema.lowerRange(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), Long.MAX_VALUE);    assertThat("appending zeros to key should still be in range", lower.compareTo(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }), new SessionWindow(Long.MAX_VALUE, Long.MAX_VALUE)))) < 0);    assertThat(lower, equalTo(SessionKeySchema.toBinary(new Windowed<>(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), new SessionWindow(0, 0)))));}
f18618
0
shouldSerializeDeserialize
public void kafkatest_f18619_0()
{    final byte[] bytes = keySerde.serializer().serialize(topic, windowedKey);    final Windowed<String> result = keySerde.deserializer().deserialize(topic, bytes);    assertEquals(windowedKey, result);}
f18619
0
shouldExtractKeyFromBinary
public void kafkatest_f18628_0()
{    final byte[] serialized = SessionKeySchema.toBinary(windowedKey, serde.serializer(), "dummy");    assertEquals(windowedKey, SessionKeySchema.from(serialized, serde.deserializer(), "dummy"));}
f18628
0
shouldExtractBytesKeyFromBinary
public void kafkatest_f18629_0()
{    final Bytes bytesKey = Bytes.wrap(key.getBytes());    final Windowed<Bytes> windowedBytesKey = new Windowed<>(bytesKey, window);    final Bytes serialized = SessionKeySchema.toBinary(windowedBytesKey);    assertEquals(windowedBytesKey, SessionKeySchema.from(serialized));}
f18629
0
shouldThrowNullPointerIfInnerIsNull
public void kafkatest_f18638_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> new SessionStoreBuilder<>(null, Serdes.String(), Serdes.String(), new MockTime()));    assertThat(e.getMessage(), equalTo("supplier cannot be null"));}
f18638
0
shouldThrowNullPointerIfKeySerdeIsNull
public void kafkatest_f18639_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> new SessionStoreBuilder<>(supplier, null, Serdes.String(), new MockTime()));    assertThat(e.getMessage(), equalTo("name cannot be null"));}
f18639
0
cleanUp
public void kafkatest_f18648_0() throws IOException
{    Utils.delete(stateDir);}
f18648
0
shouldFindKeyValueStores
public void kafkatest_f18649_0()
{    mockThread(true);    final List<ReadOnlyKeyValueStore<String, String>> kvStores = provider.stores("kv-store", QueryableStoreTypes.keyValueStore());    assertEquals(2, kvStores.size());    for (final ReadOnlyKeyValueStore<String, String> store : kvStores) {        assertThat(store, instanceOf(ReadOnlyKeyValueStore.class));        assertThat(store, not(instanceOf(TimestampedKeyValueStore.class)));    }}
f18649
0
shouldThrowInvalidStoreExceptionIfTsKVStoreClosed
public void kafkatest_f18658_0()
{    mockThread(true);    taskOne.getStore("timestamped-kv-store").close();    provider.stores("timestamped-kv-store", QueryableStoreTypes.timestampedKeyValueStore());}
f18658
0
shouldThrowInvalidStoreExceptionIfWindowStoreClosed
public void kafkatest_f18659_0()
{    mockThread(true);    taskOne.getStore("window-store").close();    provider.stores("window-store", QueryableStoreTypes.windowStore());}
f18659
0
checkOverheads
private void kafkatest_f18668_0(final double entryFactor, final double systemFactor, final long desiredCacheSize, final int keySizeBytes, final int valueSizeBytes)
{    final Runtime runtime = Runtime.getRuntime();    final long numElements = desiredCacheSize / memoryCacheEntrySize(new byte[keySizeBytes], new byte[valueSizeBytes], "");    System.gc();    final long prevRuntimeMemory = runtime.totalMemory() - runtime.freeMemory();    final ThreadCache cache = new ThreadCache(logContext, desiredCacheSize, new MockStreamsMetrics(new Metrics()));    final long size = cache.sizeBytes();    assertEquals(size, 0);    for (int i = 0; i < numElements; i++) {        final String keyStr = "K" + i;        final Bytes key = Bytes.wrap(keyStr.getBytes());        final byte[] value = new byte[valueSizeBytes];        cache.put(namespace, key, new LRUCacheEntry(value, null, true, 1L, 1L, 1, ""));    }    System.gc();    final double ceiling = desiredCacheSize + desiredCacheSize * entryFactor;    final long usedRuntimeMemory = runtime.totalMemory() - runtime.freeMemory() - prevRuntimeMemory;    assertTrue((double) cache.sizeBytes() <= ceiling);    assertTrue("Used memory size " + usedRuntimeMemory + " greater than expected " + cache.sizeBytes() * systemFactor, cache.sizeBytes() * systemFactor >= usedRuntimeMemory);}
f18668
0
cacheOverheadsSmallValues
public void kafkatest_f18669_0()
{    final Runtime runtime = Runtime.getRuntime();    final double factor = 0.05;    // if I ask for a cache size of 10 MB, accept an overhead of 3x, i.e., 30 MBs might be allocated    final double systemFactor = 3;    final long desiredCacheSize = Math.min(100 * 1024 * 1024L, runtime.maxMemory());    final int keySizeBytes = 8;    final int valueSizeBytes = 100;    checkOverheads(factor, systemFactor, desiredCacheSize, keySizeBytes, valueSizeBytes);}
f18669
0
shouldPeekNextKey
public void kafkatest_f18678_0()
{    final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics()));    final Bytes theByte = Bytes.wrap(new byte[] { 0 });    cache.put(namespace, theByte, dirtyEntry(theByte.get()));    final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.range(namespace, theByte, Bytes.wrap(new byte[] { 1 }));    assertEquals(theByte, iterator.peekNextKey());    assertEquals(theByte, iterator.peekNextKey());}
f18678
0
shouldGetSameKeyAsPeekNext
public void kafkatest_f18679_0()
{    final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics()));    final Bytes theByte = Bytes.wrap(new byte[] { 0 });    cache.put(namespace, theByte, dirtyEntry(theByte.get()));    final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.range(namespace, theByte, Bytes.wrap(new byte[] { 1 }));    assertEquals(iterator.peekNextKey(), iterator.next().key);}
f18679
0
shouldEvictImmediatelyIfCacheSizeIsZero
public void kafkatest_f18688_0()
{    final ThreadCache cache = new ThreadCache(logContext, 0, new MockStreamsMetrics(new Metrics()));    shouldEvictImmediatelyIfCacheSizeIsZeroOrVerySmall(cache);}
f18688
0
shouldEvictAfterPutAll
public void kafkatest_f18689_0()
{    final List<ThreadCache.DirtyEntry> received = new ArrayList<>();    final ThreadCache cache = new ThreadCache(logContext, 1, new MockStreamsMetrics(new Metrics()));    cache.addDirtyEntryFlushListener(namespace, received::addAll);    cache.putAll(namespace, Arrays.asList(KeyValue.pair(Bytes.wrap(new byte[] { 0 }), dirtyEntry(new byte[] { 5 })), KeyValue.pair(Bytes.wrap(new byte[] { 1 }), dirtyEntry(new byte[] { 6 }))));    assertEquals(cache.evicts(), 2);    assertEquals(received.size(), 2);}
f18689
0
dirtyEntry
private LRUCacheEntry kafkatest_f18698_0(final byte[] key)
{    return new LRUCacheEntry(key, null, true, -1, -1, -1, "");}
f18698
0
cleanEntry
private LRUCacheEntry kafkatest_f18699_0(final byte[] key)
{    return new LRUCacheEntry(key);}
f18699
0
shouldRespectEvictionPredicate
public void kafkatest_f18708_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    putRecord(buffer, context, 0L, 0L, "asdf", "eyt");    putRecord(buffer, context, 1L, 0L, "zxcv", "rtg");    assertThat(buffer.numRecords(), is(2));    final List<Eviction<String, String>> evicted = new LinkedList<>();    buffer.evictWhile(() -> buffer.numRecords() > 1, evicted::add);    assertThat(buffer.numRecords(), is(1));    assertThat(evicted, is(singletonList(new Eviction<>("asdf", new Change<>("eyt", null), getContext(0L)))));    cleanup(context, buffer);}
f18708
0
shouldTrackCount
public void kafkatest_f18709_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    putRecord(buffer, context, 0L, 0L, "asdf", "oin");    assertThat(buffer.numRecords(), is(1));    putRecord(buffer, context, 1L, 0L, "asdf", "wekjn");    assertThat(buffer.numRecords(), is(1));    putRecord(buffer, context, 0L, 0L, "zxcv", "24inf");    assertThat(buffer.numRecords(), is(2));    cleanup(context, buffer);}
f18709
0
shouldRestoreV2Format
public void kafkatest_f18718_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    final RecordBatchingStateRestoreCallback stateRestoreCallback = (RecordBatchingStateRestoreCallback) context.stateRestoreCallback(testName);    context.setRecordContext(new ProcessorRecordContext(0, 0, 0, "", null));    final RecordHeaders v2FlagHeaders = new RecordHeaders(new Header[] { new RecordHeader("v", new byte[] { (byte) 2 }) });    final byte[] todeleteValue = getBufferValue("doomed", 0).serialize(0).array();    final byte[] asdfValue = getBufferValue("qwer", 1).serialize(0).array();    final FullChangeSerde<String> fullChangeSerde = FullChangeSerde.wrap(Serdes.String());    final byte[] zxcvValue1 = new BufferValue(Serdes.String().serializer().serialize(null, "previous"), Serdes.String().serializer().serialize(null, "IGNORED"), Serdes.String().serializer().serialize(null, "3o4im"), getContext(2L)).serialize(0).array();    final FullChangeSerde<String> fullChangeSerde1 = FullChangeSerde.wrap(Serdes.String());    final byte[] zxcvValue2 = new BufferValue(Serdes.String().serializer().serialize(null, "previous"), Serdes.String().serializer().serialize(null, "3o4im"), Serdes.String().serializer().serialize(null, "next"), getContext(3L)).serialize(0).array();    stateRestoreCallback.restoreBatch(asList(new ConsumerRecord<>("changelog-topic", 0, 0, 999, TimestampType.CREATE_TIME, -1L, -1, -1, "todelete".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + todeleteValue.length).put(todeleteValue).putLong(0L).array(), v2FlagHeaders), new ConsumerRecord<>("changelog-topic", 0, 1, 9999, TimestampType.CREATE_TIME, -1L, -1, -1, "asdf".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + asdfValue.length).put(asdfValue).putLong(2L).array(), v2FlagHeaders), new ConsumerRecord<>("changelog-topic", 0, 2, 99, TimestampType.CREATE_TIME, -1L, -1, -1, "zxcv".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + zxcvValue1.length).put(zxcvValue1).putLong(1L).array(), v2FlagHeaders), new ConsumerRecord<>("changelog-topic", 0, 2, 100, TimestampType.CREATE_TIME, -1L, -1, -1, "zxcv".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + zxcvValue2.length).put(zxcvValue2).putLong(1L).array(), v2FlagHeaders)));    assertThat(buffer.numRecords(), is(3));    assertThat(buffer.minTimestamp(), is(0L));    assertThat(buffer.bufferSize(), is(142L));    stateRestoreCallback.restoreBatch(singletonList(new ConsumerRecord<>("changelog-topic", 0, 3, 3, TimestampType.CREATE_TIME, -1L, -1, -1, "todelete".getBytes(UTF_8), null)));    assertThat(buffer.numRecords(), is(2));    assertThat(buffer.minTimestamp(), is(1L));    assertThat(buffer.bufferSize(), is(95L));    assertThat(buffer.priorValueForBuffered("todelete"), is(Maybe.undefined()));    assertThat(buffer.priorValueForBuffered("asdf"), is(Maybe.defined(null)));    assertThat(buffer.priorValueForBuffered("zxcv"), is(Maybe.defined(ValueAndTimestamp.make("previous", -1))));    // flush the buffer into a list in buffer order so we can make assertions about the contents.    final List<Eviction<String, String>> evicted = new LinkedList<>();    buffer.evictWhile(() -> true, evicted::add);    // Several things to note:    // * The buffered records are ordered according to their buffer time (serialized in the value of the changelog)    // * The record timestamps are properly restored, and not conflated with the record's buffer time.    // * The keys and values are properly restored    // * The record topic is set to the original input topic, *not* the changelog topic    // * The record offset preserves the original input record's offset, *not* the offset of the changelog record    assertThat(evicted, is(asList(new Eviction<>("zxcv", new Change<>("next", "3o4im"), getContext(3L)), new Eviction<>("asdf", new Change<>("qwer", null), getContext(1L)))));    cleanup(context, buffer);}
f18718
0
shouldNotRestoreUnrecognizedVersionRecord
public void kafkatest_f18719_0()
{    final TimeOrderedKeyValueBuffer<String, String> buffer = bufferSupplier.apply(testName);    final MockInternalProcessorContext context = makeContext();    buffer.init(context, buffer);    final RecordBatchingStateRestoreCallback stateRestoreCallback = (RecordBatchingStateRestoreCallback) context.stateRestoreCallback(testName);    context.setRecordContext(new ProcessorRecordContext(0, 0, 0, "", null));    final RecordHeaders unknownFlagHeaders = new RecordHeaders(new Header[] { new RecordHeader("v", new byte[] { (byte) -1 }) });    final byte[] todeleteValue = getBufferValue("doomed", 0).serialize(0).array();    try {        stateRestoreCallback.restoreBatch(singletonList(new ConsumerRecord<>("changelog-topic", 0, 0, 999, TimestampType.CREATE_TIME, -1L, -1, -1, "todelete".getBytes(UTF_8), ByteBuffer.allocate(Long.BYTES + todeleteValue.length).putLong(0L).put(todeleteValue).array(), unknownFlagHeaders)));        fail("expected an exception");    } catch (final IllegalArgumentException expected) {    // nothing to do.    } finally {        cleanup(context, buffer);    }}
f18719
0
shouldHaveCachingStoreWhenEnabled
public void kafkatest_f18728_0()
{    final TimestampedKeyValueStore<String, String> store = builder.withCachingEnabled().build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredTimestampedKeyValueStore.class));    assertThat(wrapped, instanceOf(CachingKeyValueStore.class));}
f18728
0
shouldHaveChangeLoggingStoreWhenLoggingEnabled
public void kafkatest_f18729_0()
{    final TimestampedKeyValueStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredTimestampedKeyValueStore.class));    assertThat(wrapped, instanceOf(ChangeLoggingTimestampedKeyValueBytesStore.class));    assertThat(((WrappedStateStore) wrapped).wrapped(), CoreMatchers.equalTo(inner));}
f18729
0
createContext
public void kafkatest_f18738_0()
{    stateDirectory = TestUtils.tempDirectory();    context = new InternalMockProcessorContext(stateDirectory, Serdes.String(), Serdes.Long(), new NoOpRecordCollector(), new ThreadCache(new LogContext("testCache "), 0, new MockStreamsMetrics(new Metrics())));    segments = new TimestampedSegments(storeName, METRICS_SCOPE, RETENTION_PERIOD, SEGMENT_INTERVAL);}
f18738
0
close
public void kafkatest_f18739_0()
{    segments.close();}
f18739
0
shouldCloseAllOpenSegments
public void kafkatest_f18748_0()
{    final TimestampedSegment first = segments.getOrCreateSegmentIfLive(0, context, -1L);    final TimestampedSegment second = segments.getOrCreateSegmentIfLive(1, context, -1L);    final TimestampedSegment third = segments.getOrCreateSegmentIfLive(2, context, -1L);    segments.close();    assertFalse(first.isOpen());    assertFalse(second.isOpen());    assertFalse(third.isOpen());}
f18748
0
shouldOpenExistingSegments
public void kafkatest_f18749_0()
{    segments = new TimestampedSegments("test", METRICS_SCOPE, 4, 1);    segments.getOrCreateSegmentIfLive(0, context, -1L);    segments.getOrCreateSegmentIfLive(1, context, -1L);    segments.getOrCreateSegmentIfLive(2, context, -1L);    segments.getOrCreateSegmentIfLive(3, context, -1L);    segments.getOrCreateSegmentIfLive(4, context, -1L);    // close existing.    segments.close();    segments = new TimestampedSegments("test", METRICS_SCOPE, 4, 1);    segments.openExisting(context, -1L);    assertTrue(segments.getSegmentForTimestamp(0).isOpen());    assertTrue(segments.getSegmentForTimestamp(1).isOpen());    assertTrue(segments.getSegmentForTimestamp(2).isOpen());    assertTrue(segments.getSegmentForTimestamp(3).isOpen());    assertTrue(segments.getSegmentForTimestamp(4).isOpen());}
f18749
0
verifyCorrectSegments
private void kafkatest_f18758_0(final long first, final int numSegments)
{    final List<TimestampedSegment> result = this.segments.segments(0, Long.MAX_VALUE);    assertEquals(numSegments, result.size());    for (int i = 0; i < numSegments; i++) {        assertEquals(i + first, result.get(i).id);    }}
f18758
0
shouldDeleteStateDirectoryOnDestroy
public void kafkatest_f18759_0() throws Exception
{    final TimestampedSegment segment = new TimestampedSegment("segment", "window", 0L, metricsRecorder);    final String directoryPath = TestUtils.tempDirectory().getAbsolutePath();    final File directory = new File(directoryPath);    final ProcessorContext mockContext = mock(ProcessorContext.class);    expect(mockContext.appConfigs()).andReturn(mkMap(mkEntry(METRICS_RECORDING_LEVEL_CONFIG, "INFO")));    expect(mockContext.stateDir()).andReturn(directory);    replay(mockContext);    segment.openDB(mockContext);    assertTrue(new File(directoryPath, "window").exists());    assertTrue(new File(directoryPath + File.separator + "window", "segment").exists());    assertTrue(new File(directoryPath + File.separator + "window", "segment").list().length > 0);    segment.destroy();    assertFalse(new File(directoryPath + File.separator + "window", "segment").exists());    assertTrue(new File(directoryPath, "window").exists());}
f18759
0
shouldHaveChangeLoggingStoreWhenLoggingEnabled
public void kafkatest_f18768_0()
{    final TimestampedWindowStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredTimestampedWindowStore.class));    assertThat(wrapped, instanceOf(ChangeLoggingTimestampedWindowBytesStore.class));    assertThat(((WrappedStateStore) wrapped).wrapped(), CoreMatchers.equalTo(inner));}
f18768
0
shouldHaveCachingAndChangeLoggingWhenBothEnabled
public void kafkatest_f18769_0()
{    final TimestampedWindowStore<String, String> store = builder.withLoggingEnabled(Collections.emptyMap()).withCachingEnabled().build();    final WrappedStateStore caching = (WrappedStateStore) ((WrappedStateStore) store).wrapped();    final WrappedStateStore changeLogging = (WrappedStateStore) caching.wrapped();    assertThat(store, instanceOf(MeteredTimestampedWindowStore.class));    assertThat(caching, instanceOf(CachingWindowStore.class));    assertThat(changeLogging, instanceOf(ChangeLoggingTimestampedWindowBytesStore.class));    assertThat(changeLogging.wrapped(), CoreMatchers.equalTo(inner));}
f18769
0
setup
public void kafkatest_f18778_0()
{    windowStore = buildWindowStore(RETENTION_PERIOD, WINDOW_SIZE, false, Serdes.Integer(), Serdes.String());    final RecordCollector recordCollector = createRecordCollector(windowStore.name());    recordCollector.init(producer);    context = new InternalMockProcessorContext(baseDir, Serdes.String(), Serdes.Integer(), recordCollector, new ThreadCache(new LogContext("testCache"), 0, new MockStreamsMetrics(new Metrics())));    windowStore.init(context, windowStore);}
f18778
0
after
public void kafkatest_f18779_0()
{    windowStore.close();}
f18779
0
shouldFetchAndIterateOverExactKeys
public void kafkatest_f18788_0()
{    final long windowSize = 0x7a00000000000000L;    final long retentionPeriod = 0x7a00000000000000L;    final WindowStore<String, String> windowStore = buildWindowStore(retentionPeriod, windowSize, false, Serdes.String(), Serdes.String());    windowStore.init(context, windowStore);    windowStore.put("a", "0001", 0);    windowStore.put("aa", "0002", 0);    windowStore.put("a", "0003", 1);    windowStore.put("aa", "0004", 1);    windowStore.put("a", "0005", 0x7a00000000000000L - 1);    final Set expected = new HashSet<>(asList("0001", "0003", "0005"));    assertThat(toSet(windowStore.fetch("a", ofEpochMilli(0), ofEpochMilli(Long.MAX_VALUE))), equalTo(expected));    Set<KeyValue<Windowed<String>, String>> set = toSet(windowStore.fetch("a", "a", ofEpochMilli(0), ofEpochMilli(Long.MAX_VALUE)));    assertThat(set, equalTo(new HashSet<>(asList(windowedPair("a", "0001", 0, windowSize), windowedPair("a", "0003", 1, windowSize), windowedPair("a", "0005", 0x7a00000000000000L - 1, windowSize)))));    set = toSet(windowStore.fetch("aa", "aa", ofEpochMilli(0), ofEpochMilli(Long.MAX_VALUE)));    assertThat(set, equalTo(new HashSet<>(asList(windowedPair("aa", "0002", 0, windowSize), windowedPair("aa", "0004", 1, windowSize)))));}
f18788
0
testDeleteAndUpdate
public void kafkatest_f18789_0()
{    final long currentTime = 0;    setCurrentTime(currentTime);    windowStore.put(1, "one");    windowStore.put(1, "one v2");    WindowStoreIterator<String> iterator = windowStore.fetch(1, 0, currentTime);    assertEquals(new KeyValue<>(currentTime, "one v2"), iterator.next());    windowStore.put(1, null);    iterator = windowStore.fetch(1, 0, currentTime);    assertFalse(iterator.hasNext());}
f18789
0
shouldLogAndMeasureExpiredRecords
public void kafkatest_f18798_0()
{    setClassLoggerToDebug();    final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();    // Advance stream time by inserting record with large enough timestamp that records with timestamp 0 are expired    windowStore.put(1, "initial record", 2 * RETENTION_PERIOD);    // Try inserting a record with timestamp 0 -- should be dropped    windowStore.put(1, "late record", 0L);    windowStore.put(1, "another on-time record", RETENTION_PERIOD + 1);    LogCaptureAppender.unregister(appender);    final Map<MetricName, ? extends Metric> metrics = context.metrics().metrics();    final String metricScope = getMetricsScope();    final Metric dropTotal = metrics.get(new MetricName("expired-window-record-drop-total", "stream-" + metricScope + "-metrics", "The total number of occurrence of expired-window-record-drop operations.", mkMap(mkEntry("client-id", "mock"), mkEntry("task-id", "0_0"), mkEntry(metricScope + "-id", windowStore.name()))));    final Metric dropRate = metrics.get(new MetricName("expired-window-record-drop-rate", "stream-" + metricScope + "-metrics", "The average number of occurrence of expired-window-record-drop operation per second.", mkMap(mkEntry("client-id", "mock"), mkEntry("task-id", "0_0"), mkEntry(metricScope + "-id", windowStore.name()))));    assertEquals(1.0, dropTotal.metricValue());    assertNotEquals(0.0, dropRate.metricValue());    final List<String> messages = appender.getMessages();    assertThat(messages, hasItem("Skipping record for expired segment."));}
f18798
0
shouldNotThrowExceptionWhenFetchRangeIsExpired
public void kafkatest_f18799_0()
{    windowStore.put(1, "one", 0L);    windowStore.put(1, "two", 4 * RETENTION_PERIOD);    final WindowStoreIterator<String> iterator = windowStore.fetch(1, 0L, 10L);    assertFalse(iterator.hasNext());}
f18799
0
entriesByKey
private Map<Integer, Set<String>> kafkatest_f18808_0(final List<KeyValue<byte[], byte[]>> changeLog, kafkatest_f18808_0("SameParameterValue") final long startTime)
{    final HashMap<Integer, Set<String>> entriesByKey = new HashMap<>();    for (final KeyValue<byte[], byte[]> entry : changeLog) {        final long timestamp = WindowKeySchema.extractStoreTimestamp(entry.key);        final Integer key = WindowKeySchema.extractStoreKey(entry.key, serdes);        final String value = entry.value == null ? null : serdes.valueFrom(entry.value);        final Set<String> entries = entriesByKey.computeIfAbsent(key, k -> new HashSet<>());        entries.add(value + "@" + (timestamp - startTime));    }    return entriesByKey;}
f18808
0
windowedPair
protected static KeyValue<Windowed<K>, V> kafkatest_f18809_0(final K key, final V value, final long timestamp)
{    return windowedPair(key, value, timestamp, WINDOW_SIZE);}
f18809
0
testLowerBoundWithZeroTimestamp
public void kafkatest_f18818_0()
{    final Bytes lower = windowKeySchema.lowerRange(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), 0);    assertThat(lower, equalTo(WindowKeySchema.toStoreKeyBinary(new byte[] { 0xA, 0xB, 0xC }, 0, 0)));}
f18818
0
testLowerBoundWithMonZeroTimestamp
public void kafkatest_f18819_0()
{    final Bytes lower = windowKeySchema.lowerRange(Bytes.wrap(new byte[] { 0xA, 0xB, 0xC }), 42);    assertThat(lower, equalTo(WindowKeySchema.toStoreKeyBinary(new byte[] { 0xA, 0xB, 0xC }, 0, 0)));}
f18819
0
shouldConvertToBinaryAndBack
public void kafkatest_f18828_0()
{    final Bytes serialized = WindowKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);    final Windowed<String> result = WindowKeySchema.fromStoreKey(serialized.get(), endTime - startTime, stateSerdes.keyDeserializer(), stateSerdes.topic());    assertEquals(windowedKey, result);}
f18828
0
shouldExtractEndTimeFromBinary
public void kafkatest_f18829_0()
{    final Bytes serialized = WindowKeySchema.toStoreKeyBinary(windowedKey, 0, stateSerdes);    assertEquals(0, WindowKeySchema.extractStoreSequence(serialized.get()));}
f18829
0
shouldNotHaveChangeLoggingStoreWhenDisabled
public void kafkatest_f18838_0()
{    final WindowStore<String, String> store = builder.withLoggingDisabled().build();    final StateStore next = ((WrappedStateStore) store).wrapped();    assertThat(next, CoreMatchers.equalTo(inner));}
f18838
0
shouldHaveCachingStoreWhenEnabled
public void kafkatest_f18839_0()
{    final WindowStore<String, String> store = builder.withCachingEnabled().build();    final StateStore wrapped = ((WrappedStateStore) store).wrapped();    assertThat(store, instanceOf(MeteredWindowStore.class));    assertThat(wrapped, instanceOf(CachingWindowStore.class));}
f18839
0
shouldFindWindowStores
public void kafkatest_f18848_0()
{    final List<ReadOnlyWindowStore<Object, Object>> windowStores = wrappingStoreProvider.stores("window", windowStore());    assertEquals(2, windowStores.size());}
f18848
0
shouldThrowInvalidStoreExceptionIfNoStoreOfTypeFound
public void kafkatest_f18849_0()
{    wrappingStoreProvider.stores("doesn't exist", QueryableStoreTypes.keyValueStore());}
f18849
0
restoredEntries
public Iterable<KeyValue<byte[], byte[]>> kafkatest_f18858_0()
{    return restorableEntries;}
f18858
0
addEntryToRestoreLog
public void kafkatest_f18859_0(final K key, final V value)
{    restorableEntries.add(new KeyValue<>(stateSerdes.rawKey(key), stateSerdes.rawValue(value)));}
f18859
0
peekNextKey
public Long kafkatest_f18869_0()
{    throw new NoSuchElementException();}
f18869
0
hasNext
public boolean kafkatest_f18870_0()
{    return false;}
f18870
0
fetch
public KeyValueIterator kafkatest_f18882_0(final Object from, final Object to, final Instant fromTime, final Instant toTime) throws IllegalArgumentException
{    return EMPTY_WINDOW_STORE_ITERATOR;}
f18882
0
all
public WindowStoreIterator<KeyValue> kafkatest_f18883_0()
{    return EMPTY_WINDOW_STORE_ITERATOR;}
f18883
0
shouldThrowIfTopicNameIsNull
public void kafkatest_f18892_0()
{    new StateSerdes<>(null, Serdes.ByteArray(), Serdes.ByteArray());}
f18892
0
shouldThrowIfKeyClassIsNull
public void kafkatest_f18893_0()
{    new StateSerdes<>("anyName", null, Serdes.ByteArray());}
f18893
0
shouldThrowIfILruMapStoreCapacityIsNegative
public void kafkatest_f18902_0()
{    final Exception e = assertThrows(IllegalArgumentException.class, () -> Stores.lruMap("anyName", -1));    assertEquals("maxCacheSize cannot be negative", e.getMessage());}
f18902
0
shouldThrowIfIPersistentWindowStoreStoreNameIsNull
public void kafkatest_f18903_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> Stores.persistentWindowStore(null, ZERO, ZERO, false));    assertEquals("name cannot be null", e.getMessage());}
f18903
0
shouldThrowIfSupplierIsNullForWindowStoreBuilder
public void kafkatest_f18912_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> Stores.windowStoreBuilder(null, Serdes.ByteArray(), Serdes.ByteArray()));    assertEquals("supplier cannot be null", e.getMessage());}
f18912
0
shouldThrowIfSupplierIsNullForKeyValueStoreBuilder
public void kafkatest_f18913_0()
{    final Exception e = assertThrows(NullPointerException.class, () -> Stores.keyValueStoreBuilder(null, Serdes.ByteArray(), Serdes.ByteArray()));    assertEquals("supplier cannot be null", e.getMessage());}
f18913
0
shouldBuildKeyValueStore
public void kafkatest_f18922_0()
{    final KeyValueStore<String, String> store = Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore("name"), Serdes.String(), Serdes.String()).build();    assertThat(store, not(nullValue()));}
f18922
0
shouldBuildTimestampedKeyValueStore
public void kafkatest_f18923_0()
{    final TimestampedKeyValueStore<String, String> store = Stores.timestampedKeyValueStoreBuilder(Stores.persistentTimestampedKeyValueStore("name"), Serdes.String(), Serdes.String()).build();    assertThat(store, not(nullValue()));}
f18923
0
shouldAllowJoinUnmaterializedFilteredKTable
public void kafkatest_f18932_0()
{    final KTable<Bytes, String> filteredKTable = builder.<Bytes, String>table(TABLE_TOPIC).filter(MockPredicate.allGoodPredicate());    builder.<Bytes, String>stream(STREAM_TOPIC).join(filteredKTable, MockValueJoiner.TOSTRING_JOINER);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertThat(topology.stateStores().size(), equalTo(1));    assertThat(topology.processorConnectedStateStores("KSTREAM-JOIN-0000000005"), equalTo(Collections.singleton(topology.stateStores().get(0).name())));    assertTrue(topology.processorConnectedStateStores("KTABLE-FILTER-0000000003").isEmpty());}
f18932
0
shouldAllowJoinMaterializedFilteredKTable
public void kafkatest_f18933_0()
{    final KTable<Bytes, String> filteredKTable = builder.<Bytes, String>table(TABLE_TOPIC).filter(MockPredicate.allGoodPredicate(), Materialized.as("store"));    builder.<Bytes, String>stream(STREAM_TOPIC).join(filteredKTable, MockValueJoiner.TOSTRING_JOINER);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertThat(topology.stateStores().size(), equalTo(1));    assertThat(topology.processorConnectedStateStores("KSTREAM-JOIN-0000000005"), equalTo(Collections.singleton("store")));    assertThat(topology.processorConnectedStateStores("KTABLE-FILTER-0000000003"), equalTo(Collections.singleton("store")));}
f18933
0
shouldUseSerdesDefinedInMaterializedToConsumeTable
public void kafkatest_f18942_0()
{    final Map<Long, String> results = new HashMap<>();    final String topic = "topic";    final ForeachAction<Long, String> action = results::put;    builder.table(topic, Materialized.<Long, String, KeyValueStore<Bytes, byte[]>>as("store").withKeySerde(Serdes.Long()).withValueSerde(Serdes.String())).toStream().foreach(action);    final ConsumerRecordFactory<Long, String> recordFactory = new ConsumerRecordFactory<>(new LongSerializer(), new StringSerializer());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, 1L, "value1"));        driver.pipeInput(recordFactory.create(topic, 2L, "value2"));        final KeyValueStore<Long, String> store = driver.getKeyValueStore("store");        assertThat(store.get(1L), equalTo("value1"));        assertThat(store.get(2L), equalTo("value2"));        assertThat(results.get(1L), equalTo("value1"));        assertThat(results.get(2L), equalTo("value2"));    }}
f18942
0
shouldUseSerdesDefinedInMaterializedToConsumeGlobalTable
public void kafkatest_f18943_0()
{    final String topic = "topic";    builder.globalTable(topic, Materialized.<Long, String, KeyValueStore<Bytes, byte[]>>as("store").withKeySerde(Serdes.Long()).withValueSerde(Serdes.String()));    final ConsumerRecordFactory<Long, String> recordFactory = new ConsumerRecordFactory<>(new LongSerializer(), new StringSerializer());    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {        driver.pipeInput(recordFactory.create(topic, 1L, "value1"));        driver.pipeInput(recordFactory.create(topic, 2L, "value2"));        final KeyValueStore<Long, String> store = driver.getKeyValueStore("store");        assertThat(store.get(1L), equalTo("value1"));        assertThat(store.get(2L), equalTo("value2"));    }}
f18943
0
shouldUseSpecifiedNameForSinkProcessor
public void kafkatest_f18952_0()
{    final String expected = "sink-processor";    final KStream<Object, Object> stream = builder.stream(STREAM_TOPIC);    stream.to(STREAM_TOPIC_TWO, Produced.as(expected));    stream.to(STREAM_TOPIC_TWO);    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", expected, "KSTREAM-SINK-0000000002");}
f18952
0
shouldUseSpecifiedNameForMapOperation
public void kafkatest_f18953_0()
{    builder.stream(STREAM_TOPIC).map(KeyValue::pair, Named.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", STREAM_OPERATION_NAME);}
f18953
0
shouldUseSpecifiedNameForJoinOperationBetweenKStreamAndKTable
public void kafkatest_f18962_0()
{    final KStream<String, String> streamOne = builder.stream(STREAM_TOPIC);    final KTable<String, String> streamTwo = builder.table("table-topic");    streamOne.join(streamTwo, (value1, value2) -> value1, Joined.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", "KSTREAM-SOURCE-0000000002", "KTABLE-SOURCE-0000000003", STREAM_OPERATION_NAME);}
f18962
0
shouldUseSpecifiedNameForLeftJoinOperationBetweenKStreamAndKTable
public void kafkatest_f18963_0()
{    final KStream<String, String> streamOne = builder.stream(STREAM_TOPIC);    final KTable<String, String> streamTwo = builder.table(STREAM_TOPIC_TWO);    streamOne.leftJoin(streamTwo, (value1, value2) -> value1, Joined.as(STREAM_OPERATION_NAME));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000000", "KSTREAM-SOURCE-0000000002", "KTABLE-SOURCE-0000000003", STREAM_OPERATION_NAME);}
f18963
0
shouldUseSpecifiedNameForToStream
public void kafkatest_f18972_0()
{    builder.table(STREAM_TOPIC).toStream(Named.as("to-stream"));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000001", "KTABLE-SOURCE-0000000002", "to-stream");}
f18972
0
shouldUseSpecifiedNameForToStreamWithMapper
public void kafkatest_f18973_0()
{    builder.table(STREAM_TOPIC).toStream(KeyValue::pair, Named.as("to-stream"));    builder.build();    final ProcessorTopology topology = builder.internalTopologyBuilder.rewriteTopology(new StreamsConfig(props)).build();    assertSpecifiedNameForOperation(topology, "KSTREAM-SOURCE-0000000001", "KTABLE-SOURCE-0000000002", "to-stream", "KSTREAM-KEY-SELECT-0000000004");}
f18973
0
consumerConfigMustContainStreamPartitionAssignorConfig
public void kafkatest_f18982_0()
{    props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 42);    props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);    props.put(StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG, 7L);    props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, "dummy:host");    props.put(StreamsConfig.RETRIES_CONFIG, 10);    props.put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), 5);    props.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG), 100);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> returnedProps = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertEquals(42, returnedProps.get(StreamsConfig.REPLICATION_FACTOR_CONFIG));    assertEquals(1, returnedProps.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG));    assertEquals(StreamsPartitionAssignor.class.getName(), returnedProps.get(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG));    assertEquals(7L, returnedProps.get(StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG));    assertEquals("dummy:host", returnedProps.get(StreamsConfig.APPLICATION_SERVER_CONFIG));    assertNull(returnedProps.get(StreamsConfig.RETRIES_CONFIG));    assertEquals(5, returnedProps.get(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG)));    assertEquals(100, returnedProps.get(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG)));}
f18982
0
consumerConfigShouldContainAdminClientConfigsForRetriesAndRetryBackOffMsWithAdminPrefix
public void kafkatest_f18983_0()
{    props.put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG), 20);    props.put(StreamsConfig.adminClientPrefix(StreamsConfig.RETRY_BACKOFF_MS_CONFIG), 200L);    props.put(StreamsConfig.RETRIES_CONFIG, 10);    props.put(StreamsConfig.RETRY_BACKOFF_MS_CONFIG, 100L);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> returnedProps = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertEquals(20, returnedProps.get(StreamsConfig.adminClientPrefix(StreamsConfig.RETRIES_CONFIG)));    assertEquals(200L, returnedProps.get(StreamsConfig.adminClientPrefix(StreamsConfig.RETRY_BACKOFF_MS_CONFIG)));}
f18983
0
shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig
public void kafkatest_f18992_0()
{    props.put(producerPrefix("interceptor.statsd.host"), "host");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(clientId);    assertEquals("host", producerConfigs.get("interceptor.statsd.host"));}
f18992
0
shouldSupportPrefixedProducerConfigs
public void kafkatest_f18993_0()
{    props.put(producerPrefix(ProducerConfig.BUFFER_MEMORY_CONFIG), 10);    props.put(producerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> configs = streamsConfig.getProducerConfigs(clientId);    assertEquals(10, configs.get(ProducerConfig.BUFFER_MEMORY_CONFIG));    assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));}
f18993
0
shouldOverrideStreamsDefaultConsumerConfigs
public void kafkatest_f19002_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), "latest");    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "10");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertEquals("latest", consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));    assertEquals("10", consumerConfigs.get(ConsumerConfig.MAX_POLL_RECORDS_CONFIG));}
f19002
0
shouldOverrideStreamsDefaultProducerConfigs
public void kafkatest_f19003_0()
{    props.put(StreamsConfig.producerPrefix(ProducerConfig.LINGER_MS_CONFIG), "10000");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(clientId);    assertEquals("10000", producerConfigs.get(ProducerConfig.LINGER_MS_CONFIG));}
f19003
0
shouldResetToDefaultIfGlobalConsumerAutoCommitIsOverridden
public void kafkatest_f19012_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), "true");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getGlobalConsumerConfigs(clientId);    assertEquals("false", consumerConfigs.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));}
f19012
0
testGetGlobalConsumerConfigsWithGlobalConsumerOverridenPrefix
public void kafkatest_f19013_0()
{    props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "5");    props.put(StreamsConfig.globalConsumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), "50");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> returnedProps = streamsConfig.getGlobalConsumerConfigs(clientId);    assertEquals("50", returnedProps.get(ConsumerConfig.MAX_POLL_RECORDS_CONFIG));}
f19013
0
shouldResetToDefaultIfConsumerIsolationLevelIsOverriddenIfEosEnabled
public void kafkatest_f19022_0()
{    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "anyValue");    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertThat(consumerConfigs.get(ConsumerConfig.ISOLATION_LEVEL_CONFIG), equalTo(READ_COMMITTED.name().toLowerCase(Locale.ROOT)));}
f19022
0
shouldAllowSettingConsumerIsolationLevelIfEosDisabled
public void kafkatest_f19023_0()
{    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, READ_UNCOMMITTED.name().toLowerCase(Locale.ROOT));    final StreamsConfig streamsConfig = new StreamsConfig(props);    final Map<String, Object> consumerConfigs = streamsConfig.getMainConsumerConfigs(groupId, clientId, threadIdx);    assertThat(consumerConfigs.get(ConsumerConfig.ISOLATION_LEVEL_CONFIG), equalTo(READ_UNCOMMITTED.name().toLowerCase(Locale.ROOT)));}
f19023
0
shouldSpecifyCorrectKeySerdeClassOnError
public void kafkatest_f19032_0()
{    final Properties props = getStreamsConfig();    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);    final StreamsConfig config = new StreamsConfig(props);    try {        config.defaultKeySerde();        fail("Test should throw a StreamsException");    } catch (final StreamsException e) {        assertEquals("Failed to configure key serde class org.apache.kafka.streams.StreamsConfigTest$MisconfiguredSerde", e.getMessage());    }}
f19032
0
shouldSpecifyCorrectValueSerdeClassOnError
public void kafkatest_f19033_0()
{    final Properties props = getStreamsConfig();    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);    final StreamsConfig config = new StreamsConfig(props);    try {        config.defaultValueSerde();        fail("Test should throw a StreamsException");    } catch (final StreamsException e) {        assertEquals("Failed to configure value serde class org.apache.kafka.streams.StreamsConfigTest$MisconfiguredSerde", e.getMessage());    }}
f19033
0
deserializer
public Deserializer kafkatest_f19042_0()
{    return null;}
f19042
0
extract
public long kafkatest_f19043_0(final ConsumerRecord<Object, Object> record, final long partitionTime)
{    return 0;}
f19043
0
createKafkaStreams
private KafkaStreams kafkatest_f19052_0(final Properties props)
{    props.put(StreamsConfig.APPLICATION_ID_CONFIG, APP_ID);    props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);    props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);    props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);    props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Integer().getClass());    final StreamsBuilder builder = new StreamsBuilder();    final KStream<String, Integer> data = builder.stream("data");    data.to("echo");    data.process(SmokeTestUtil.printProcessorSupplier("data"));    final KGroupedStream<String, Integer> groupedData = data.groupByKey();    // min    groupedData.aggregate(new Initializer<Integer>() {        @Override        public Integer apply() {            return Integer.MAX_VALUE;        }    }, new Aggregator<String, Integer, Integer>() {        @Override        public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {            return (value < aggregate) ? value : aggregate;        }    }, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>with(null, intSerde)).toStream().to("min", Produced.with(stringSerde, intSerde));    // sum    groupedData.aggregate(new Initializer<Long>() {        @Override        public Long apply() {            return 0L;        }    }, new Aggregator<String, Integer, Long>() {        @Override        public Long apply(final String aggKey, final Integer value, final Long aggregate) {            return (long) value + aggregate;        }    }, Materialized.<String, Long, KeyValueStore<Bytes, byte[]>>with(null, longSerde)).toStream().to("sum", Produced.with(stringSerde, longSerde));    if (withRepartitioning) {        final KStream<String, Integer> repartitionedData = data.through("repartition");        repartitionedData.process(SmokeTestUtil.printProcessorSupplier("repartition"));        final KGroupedStream<String, Integer> groupedDataAfterRepartitioning = repartitionedData.groupByKey();        // max        groupedDataAfterRepartitioning.aggregate(new Initializer<Integer>() {            @Override            public Integer apply() {                return Integer.MIN_VALUE;            }        }, new Aggregator<String, Integer, Integer>() {            @Override            public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {                return (value > aggregate) ? value : aggregate;            }        }, Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>with(null, intSerde)).toStream().to("max", Produced.with(stringSerde, intSerde));        // count        groupedDataAfterRepartitioning.count().toStream().to("cnt", Produced.with(stringSerde, longSerde));    }    return new KafkaStreams(builder.build(), props);}
f19052
0
apply
public Integer kafkatest_f19053_0()
{    return Integer.MAX_VALUE;}
f19053
0
verify
public static void kafkatest_f19062_0(final String kafka, final boolean withRepartitioning)
{    final Properties props = new Properties();    props.put(ConsumerConfig.CLIENT_ID_CONFIG, "verifier");    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));    final Map<TopicPartition, Long> committedOffsets;    try (final Admin adminClient = Admin.create(props)) {        ensureStreamsApplicationDown(adminClient);        committedOffsets = getCommittedOffsets(adminClient, withRepartitioning);    }    final String[] allInputTopics;    final String[] allOutputTopics;    if (withRepartitioning) {        allInputTopics = new String[] { "data", "repartition" };        allOutputTopics = new String[] { "echo", "min", "sum", "repartition", "max", "cnt" };    } else {        allInputTopics = new String[] { "data" };        allOutputTopics = new String[] { "echo", "min", "sum" };    }    final Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> inputRecordsPerTopicPerPartition;    try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {        final List<TopicPartition> partitions = getAllPartitions(consumer, allInputTopics);        consumer.assign(partitions);        consumer.seekToBeginning(partitions);        inputRecordsPerTopicPerPartition = getRecords(consumer, committedOffsets, withRepartitioning, true);    } catch (final Exception e) {        e.printStackTrace(System.err);        System.out.println("FAILED");        return;    }    final Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> outputRecordsPerTopicPerPartition;    try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {        final List<TopicPartition> partitions = getAllPartitions(consumer, allOutputTopics);        consumer.assign(partitions);        consumer.seekToBeginning(partitions);        outputRecordsPerTopicPerPartition = getRecords(consumer, consumer.endOffsets(partitions), withRepartitioning, false);    } catch (final Exception e) {        e.printStackTrace(System.err);        System.out.println("FAILED");        return;    }    verifyReceivedAllRecords(inputRecordsPerTopicPerPartition.get("data"), outputRecordsPerTopicPerPartition.get("echo"));    if (withRepartitioning) {        verifyReceivedAllRecords(inputRecordsPerTopicPerPartition.get("data"), outputRecordsPerTopicPerPartition.get("repartition"));    }    verifyMin(inputRecordsPerTopicPerPartition.get("data"), outputRecordsPerTopicPerPartition.get("min"));    verifySum(inputRecordsPerTopicPerPartition.get("data"), outputRecordsPerTopicPerPartition.get("sum"));    if (withRepartitioning) {        verifyMax(inputRecordsPerTopicPerPartition.get("repartition"), outputRecordsPerTopicPerPartition.get("max"));        verifyCnt(inputRecordsPerTopicPerPartition.get("repartition"), outputRecordsPerTopicPerPartition.get("cnt"));    }    try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {        final List<TopicPartition> partitions = getAllPartitions(consumer, allOutputTopics);        consumer.assign(partitions);        consumer.seekToBeginning(partitions);        verifyAllTransactionFinished(consumer, kafka, withRepartitioning);    } catch (final Exception e) {        e.printStackTrace(System.err);        System.out.println("FAILED");        return;    }    // do not modify: required test output    System.out.println("ALL-RECORDS-DELIVERED");    System.out.flush();}
f19062
0
ensureStreamsApplicationDown
private static void kafkatest_f19063_0(final Admin adminClient)
{    final long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;    ConsumerGroupDescription description;    do {        description = getConsumerGroupDescription(adminClient);        if (System.currentTimeMillis() > maxWaitTime && !description.members().isEmpty()) {            throw new RuntimeException("Streams application not down after " + (MAX_IDLE_TIME_MS / 1000) + " seconds. " + "Group: " + description);        }        sleep(1000);    } while (!description.members().isEmpty());}
f19063
0
verifyCnt
private static void kafkatest_f19072_0(final Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> inputPerTopicPerPartition, final Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> cntPerTopicPerPartition)
{    final StringDeserializer stringDeserializer = new StringDeserializer();    final LongDeserializer longDeserializer = new LongDeserializer();    final HashMap<String, Long> currentSumPerKey = new HashMap<>();    for (final Map.Entry<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> partitionRecords : cntPerTopicPerPartition.entrySet()) {        final TopicPartition inputTopicPartition = new TopicPartition("repartition", partitionRecords.getKey().partition());        final List<ConsumerRecord<byte[], byte[]>> partitionInput = inputPerTopicPerPartition.get(inputTopicPartition);        final List<ConsumerRecord<byte[], byte[]>> partitionCnt = partitionRecords.getValue();        if (partitionInput.size() != partitionCnt.size()) {            throw new RuntimeException("Result verification failed: expected " + partitionInput.size() + " records for " + partitionRecords.getKey() + " but received " + partitionCnt.size());        }        final Iterator<ConsumerRecord<byte[], byte[]>> inputRecords = partitionInput.iterator();        for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionCnt) {            final ConsumerRecord<byte[], byte[]> input = inputRecords.next();            final String receivedKey = stringDeserializer.deserialize(receivedRecord.topic(), receivedRecord.key());            final long receivedValue = longDeserializer.deserialize(receivedRecord.topic(), receivedRecord.value());            final String key = stringDeserializer.deserialize(input.topic(), input.key());            Long cnt = currentSumPerKey.get(key);            if (cnt == null) {                cnt = 0L;            }            currentSumPerKey.put(key, ++cnt);            if (!receivedKey.equals(key) || receivedValue != cnt) {                throw new RuntimeException("Result verification failed for " + receivedRecord + " expected <" + key + "," + cnt + "> but was <" + receivedKey + "," + receivedValue + ">");            }        }    }}
f19072
0
verifyAllTransactionFinished
private static void kafkatest_f19073_0(final KafkaConsumer<byte[], byte[]> consumer, final String kafka, final boolean withRepartitioning)
{    final String[] topics;    if (withRepartitioning) {        topics = new String[] { "echo", "min", "sum", "repartition", "max", "cnt" };    } else {        topics = new String[] { "echo", "min", "sum" };    }    final List<TopicPartition> partitions = getAllPartitions(consumer, topics);    consumer.assign(partitions);    consumer.seekToEnd(partitions);    for (final TopicPartition tp : partitions) {        System.out.println(tp + " at position " + consumer.position(tp));    }    final Properties producerProps = new Properties();    producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, "VerifyProducer");    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);    producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);    try (final KafkaProducer<String, String> producer = new KafkaProducer<>(producerProps)) {        for (final TopicPartition tp : partitions) {            final ProducerRecord<String, String> record = new ProducerRecord<>(tp.topic(), tp.partition(), "key", "value");            producer.send(record, (metadata, exception) -> {                if (exception != null) {                    exception.printStackTrace(System.err);                    System.err.flush();                    Exit.exit(1);                }            });        }    }    final StringDeserializer stringDeserializer = new StringDeserializer();    long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;    while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {        final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));        if (records.isEmpty()) {            System.out.println("No data received.");            for (final TopicPartition tp : partitions) {                System.out.println(tp + " at position " + consumer.position(tp));            }        }        for (final ConsumerRecord<byte[], byte[]> record : records) {            maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;            final String topic = record.topic();            final TopicPartition tp = new TopicPartition(topic, record.partition());            try {                final String key = stringDeserializer.deserialize(topic, record.key());                final String value = stringDeserializer.deserialize(topic, record.value());                if (!("key".equals(key) && "value".equals(value) && partitions.remove(tp))) {                    throw new RuntimeException("Post transactions verification failed. Received unexpected verification record: " + "Expected record <'key','value'> from one of " + partitions + " but got" + " <" + key + "," + value + "> [" + record.topic() + ", " + record.partition() + "]");                } else {                    System.out.println("Verifying " + tp + " successful.");                }            } catch (final SerializationException e) {                throw new RuntimeException("Post transactions verification failed. Received unexpected verification record: " + "Expected record <'key','value'> from one of " + partitions + " but got " + record, e);            }        }    }    if (!partitions.isEmpty()) {        throw new RuntimeException("Could not read all verification records. Did not receive any new record within the last " + (MAX_IDLE_TIME_MS / 1000) + " sec.");    }}
f19073
0
closeAsync
public void kafkatest_f19082_0()
{    streams.close(Duration.ZERO);}
f19082
0
close
public void kafkatest_f19083_0()
{    streams.close(Duration.ofSeconds(5));    // do not remove these printouts since they are needed for health scripts    if (!uncaughtException) {        System.out.println(name + ": SMOKE-TEST-CLIENT-CLOSED");    }    try {        thread.join();    } catch (final Exception ex) {        // do not remove these printouts since they are needed for health scripts        System.out.println(name + ": SMOKE-TEST-CLIENT-EXCEPTION");    // ignore    }}
f19083
0
generatorProperties
private static Properties kafkatest_f19092_0(final String kafka)
{    final Properties producerProps = new Properties();    producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, "SmokeTest");    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);    producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);    producerProps.put(ProducerConfig.ACKS_CONFIG, "all");    return producerProps;}
f19092
0
onCompletion
public void kafkatest_f19093_0(final RecordMetadata metadata, final Exception exception)
{    if (exception != null) {        if (exception instanceof TimeoutException) {            needRetry.add(originalRecord);        } else {            exception.printStackTrace();            Exit.exit(1);        }    }}
f19093
0
indent
private static String kafkatest_f19102_0(@SuppressWarnings("SameParameterValue") final String prefix, final Iterable<ConsumerRecord<String, Number>> list)
{    final StringBuilder stringBuilder = new StringBuilder();    for (final ConsumerRecord<String, Number> record : list) {        stringBuilder.append(prefix).append(record).append('\n');    }    return stringBuilder.toString();}
f19102
0
getSum
private static Long kafkatest_f19103_0(final String key)
{    final int min = getMin(key).intValue();    final int max = getMax(key).intValue();    return ((long) min + max) * (max - min + 1L) / 2L;}
f19103
0
init
public void kafkatest_f19112_0(final ProcessorContext context)
{    super.init(context);    System.out.println("[DEV] initializing processor: topic=" + topic + " taskId=" + context.taskId());    numRecordsProcessed = 0;}
f19112
0
process
public void kafkatest_f19113_0(final Object key, final Object value)
{    numRecordsProcessed++;    if (numRecordsProcessed % 100 == 0) {        System.out.printf("%s: %s%n", name, Instant.now());        System.out.println("processed " + numRecordsProcessed + " records from topic=" + topic);    }}
f19113
0
apply
public Long kafkatest_f19122_0(final String aggKey, final Long value, final Long aggregate)
{    return aggregate - value;}
f19122
0
createDir
 static File kafkatest_f19123_0(final File parent, final String child)
{    final File dir = new File(parent, child);    dir.mkdir();    return dir;}
f19123
0
updatedConfigs
private static Map<String, String> kafkatest_f19132_0(final String formattedConfigs)
{    final String[] parts = formattedConfigs.split(",");    final Map<String, String> updatedConfigs = new HashMap<>();    for (final String part : parts) {        final String[] keyValue = part.split("=");        updatedConfigs.put(keyValue[KEY], keyValue[VALUE]);    }    return updatedConfigs;}
f19132
0
main
public static void kafkatest_f19133_0(final String[] args) throws IOException
{    if (args.length < 2) {        System.err.println("StreamsEosTest are expecting two parameters: propFile, command; but only see " + args.length + " parameter");        System.exit(1);    }    final String propFileName = args[0];    final String command = args[1];    final Properties streamsProperties = Utils.loadProps(propFileName);    final String kafka = streamsProperties.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);    if (kafka == null) {        System.err.println("No bootstrap kafka servers specified in " + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);        System.exit(1);    }    System.out.println("StreamsTest instance started");    System.out.println("kafka=" + kafka);    System.out.println("props=" + streamsProperties);    System.out.println("command=" + command);    System.out.flush();    if (command == null || propFileName == null) {        System.exit(-1);    }    switch(command) {        case "run":            EosTestDriver.generate(kafka);            break;        case "process":            new EosTestClient(streamsProperties, false).start();            break;        case "process-complex":            new EosTestClient(streamsProperties, true).start();            break;        case "verify":            EosTestDriver.verify(kafka, false);            break;        case "verify-complex":            EosTestDriver.verify(kafka, true);            break;        default:            System.out.println("unknown command: " + command);            System.out.flush();            System.exit(-1);    }}
f19133
0
main
public static void kafkatest_f19142_0(final String[] args) throws Exception
{    if (args.length < 1) {        System.err.println("StreamsUpgradeTest requires one argument (properties-file) but no provided: ");    }    final String propFileName = args.length > 0 ? args[0] : null;    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started (StreamsUpgradeTest trunk)");    System.out.println("props=" + streamsProperties);    final StreamsBuilder builder = new StreamsBuilder();    final KStream dataStream = builder.stream("data");    dataStream.process(SmokeTestUtil.printProcessorSupplier("data"));    dataStream.to("echo");    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsUpgradeTest");    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    final KafkaClientSupplier kafkaClientSupplier;    if (streamsProperties.containsKey("test.future.metadata")) {        streamsProperties.remove("test.future.metadata");        kafkaClientSupplier = new FutureKafkaClientSupplier();    } else {        kafkaClientSupplier = new DefaultKafkaClientSupplier();    }    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder.build(), config, kafkaClientSupplier);    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread(() -> {        System.out.println("closing Kafka Streams instance");        System.out.flush();        streams.close();        System.out.println("UPGRADE-TEST-CLIENT-CLOSED");        System.out.flush();    }));}
f19142
0
getConsumer
public Consumer<byte[], byte[]> kafkatest_f19143_0(final Map<String, Object> config)
{    config.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, FutureStreamsPartitionAssignor.class.getName());    return new KafkaConsumer<>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());}
f19143
0
shouldParseCorrectMap
public void kafkatest_f19152_0()
{    final String formattedConfigs = "foo=foo1,bar=bar1,baz=baz1";    final Map<String, String> parsedMap = SystemTestUtil.parseConfigs(formattedConfigs);    final TreeMap<String, String> sortedParsedMap = new TreeMap<>(parsedMap);    assertEquals(sortedParsedMap, expectedParsedMap);}
f19152
0
shouldThrowExceptionOnNull
public void kafkatest_f19153_0()
{    SystemTestUtil.parseConfigs(null);}
f19153
0
testShiftOffsetByWhenBeforeBeginningOffset
public void kafkatest_f19162_0()
{    final Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(topicPartition, 4L);    consumer.updateEndOffsets(endOffsets);    final Map<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(topicPartition, 0L);    consumer.updateBeginningOffsets(beginningOffsets);    streamsResetter.shiftOffsetsBy(consumer, inputTopicPartitions, -3L);    final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(500));    assertEquals(5, records.count());}
f19162
0
testShiftOffsetByWhenAfterEndOffset
public void kafkatest_f19163_0()
{    final Map<TopicPartition, Long> endOffsets = new HashMap<>();    endOffsets.put(topicPartition, 3L);    consumer.updateEndOffsets(endOffsets);    final Map<TopicPartition, Long> beginningOffsets = new HashMap<>();    beginningOffsets.put(topicPartition, 0L);    consumer.updateBeginningOffsets(beginningOffsets);    streamsResetter.shiftOffsetsBy(consumer, inputTopicPartitions, 5L);    final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(500));    assertEquals(2, records.count());}
f19163
0
invokeGetDateTimeMethod
private void kafkatest_f19172_0(final SimpleDateFormat format) throws ParseException
{    final Date checkpoint = new Date();    final StreamsResetter streamsResetter = new StreamsResetter();    final String formattedCheckpoint = format.format(checkpoint);    streamsResetter.getDateTime(formattedCheckpoint);}
f19172
0
shouldNotAllowNullNameWhenAddingSourceWithTopic
public void kafkatest_f19173_0()
{    topology.addSource((String) null, "topic");}
f19173
0
shouldNotAllowNullTopicChooserWhenAddingSink
public void kafkatest_f19182_0()
{    topology.addSink("name", (TopicNameExtractor<Object, Object>) null);}
f19182
0
shouldNotAllowNullProcessorNameWhenConnectingProcessorAndStateStores
public void kafkatest_f19183_0()
{    topology.connectProcessorAndStateStores(null, "store");}
f19183
0
shouldNotAllowToAddProcessorWithEmptyParents
public void kafkatest_f19192_0()
{    topology.addSource("source", "topic-1");    try {        topology.addProcessor("processor", new MockProcessorSupplier());        fail("Should throw TopologyException for processor without at least one parent node");    } catch (final TopologyException expected) {    }}
f19192
0
shouldNotAllowToAddProcessorWithNullParents
public void kafkatest_f19193_0()
{    topology.addSource("source", "topic-1");    try {        topology.addProcessor("processor", new MockProcessorSupplier(), (String) null);        fail("Should throw NullPointerException for processor when null parent names are provided");    } catch (final NullPointerException expected) {    }}
f19193
0
shouldNotAllowToAddStateStoreToNonExistingProcessor
public void kafkatest_f19202_0()
{    mockStoreBuilder();    EasyMock.replay(storeBuilder);    topology.addStateStore(storeBuilder, "no-such-processor");}
f19202
0
shouldNotAllowToAddStateStoreToSource
public void kafkatest_f19203_0()
{    mockStoreBuilder();    EasyMock.replay(storeBuilder);    topology.addSource("source-1", "topic-1");    try {        topology.addStateStore(storeBuilder, "source-1");        fail("Should have thrown TopologyException for adding store to source node");    } catch (final TopologyException expected) {    }}
f19203
0
sinkShouldReturnNullTopicWithDynamicRouting
public void kafkatest_f19214_0()
{    final TopologyDescription.Sink expectedSinkNode = new InternalTopologyBuilder.Sink("sink", (key, value, record) -> record.topic() + "-" + key);    assertThat(expectedSinkNode.topic(), equalTo(null));}
f19214
0
sinkShouldReturnTopicNameExtractorWithDynamicRouting
public void kafkatest_f19215_0()
{    final TopicNameExtractor topicNameExtractor = (key, value, record) -> record.topic() + "-" + key;    final TopologyDescription.Sink expectedSinkNode = new InternalTopologyBuilder.Sink("sink", topicNameExtractor);    assertThat(expectedSinkNode.topicNameExtractor(), equalTo(topicNameExtractor));}
f19215
0
processorWithMultipleSourcesShouldHaveSingleSubtopology
public void kafkatest_f19224_0()
{    final TopologyDescription.Source expectedSourceNode1 = addSource("source1", "topic0");    final TopologyDescription.Source expectedSourceNode2 = addSource("source2", Pattern.compile("topic[1-9]"));    final TopologyDescription.Processor expectedProcessorNode = addProcessor("processor", expectedSourceNode1, expectedSourceNode2);    final Set<TopologyDescription.Node> allNodes = new HashSet<>();    allNodes.add(expectedSourceNode1);    allNodes.add(expectedSourceNode2);    allNodes.add(expectedProcessorNode);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, allNodes));    assertThat(topology.describe(), equalTo(expectedDescription));}
f19224
0
multipleSourcesWithProcessorsShouldHaveDistinctSubtopologies
public void kafkatest_f19225_0()
{    final TopologyDescription.Source expectedSourceNode1 = addSource("source1", "topic1");    final TopologyDescription.Processor expectedProcessorNode1 = addProcessor("processor1", expectedSourceNode1);    final TopologyDescription.Source expectedSourceNode2 = addSource("source2", "topic2");    final TopologyDescription.Processor expectedProcessorNode2 = addProcessor("processor2", expectedSourceNode2);    final TopologyDescription.Source expectedSourceNode3 = addSource("source3", "topic3");    final TopologyDescription.Processor expectedProcessorNode3 = addProcessor("processor3", expectedSourceNode3);    final Set<TopologyDescription.Node> allNodes1 = new HashSet<>();    allNodes1.add(expectedSourceNode1);    allNodes1.add(expectedProcessorNode1);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(0, allNodes1));    final Set<TopologyDescription.Node> allNodes2 = new HashSet<>();    allNodes2.add(expectedSourceNode2);    allNodes2.add(expectedProcessorNode2);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(1, allNodes2));    final Set<TopologyDescription.Node> allNodes3 = new HashSet<>();    allNodes3.add(expectedSourceNode3);    allNodes3.add(expectedProcessorNode3);    expectedDescription.addSubtopology(new InternalTopologyBuilder.Subtopology(2, allNodes3));    assertThat(topology.describe(), equalTo(expectedDescription));}
f19225
0
kGroupedStreamZeroArgCountShouldPreserveTopologyStructure
public void kafkatest_f19234_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("input-topic").groupByKey().count();    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])\n" + "      --> KSTREAM-AGGREGATE-0000000002\n" + "    Processor: KSTREAM-AGGREGATE-0000000002 (stores: [KSTREAM-AGGREGATE-STATE-STORE-0000000001])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000000\n\n", describe.toString());}
f19234
0
kGroupedStreamNamedMaterializedCountShouldPreserveTopologyStructure
public void kafkatest_f19235_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.stream("input-topic").groupByKey().count(Materialized.as("count-store"));    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])\n" + "      --> KSTREAM-AGGREGATE-0000000001\n" + "    Processor: KSTREAM-AGGREGATE-0000000001 (stores: [count-store])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000000\n\n", describe.toString());}
f19235
0
tableNamedMaterializedCountShouldPreserveTopologyStructure
public void kafkatest_f19244_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.table("input-topic").groupBy((key, value) -> null).count(Materialized.as("count-store"));    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000001 (topics: [input-topic])\n" + "      --> KTABLE-SOURCE-0000000002\n" + "    Processor: KTABLE-SOURCE-0000000002 (stores: [input-topic-STATE-STORE-0000000000])\n" + "      --> KTABLE-SELECT-0000000003\n" + "      <-- KSTREAM-SOURCE-0000000001\n" + "    Processor: KTABLE-SELECT-0000000003 (stores: [])\n" + "      --> KSTREAM-SINK-0000000004\n" + "      <-- KTABLE-SOURCE-0000000002\n" + "    Sink: KSTREAM-SINK-0000000004 (topic: count-store-repartition)\n" + "      <-- KTABLE-SELECT-0000000003\n" + "\n" + "  Sub-topology: 1\n" + "    Source: KSTREAM-SOURCE-0000000005 (topics: [count-store-repartition])\n" + "      --> KTABLE-AGGREGATE-0000000006\n" + "    Processor: KTABLE-AGGREGATE-0000000006 (stores: [count-store])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000005\n" + "\n", describe.toString());}
f19244
0
tableAnonymousMaterializedCountShouldPreserveTopologyStructure
public void kafkatest_f19245_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.table("input-topic").groupBy((key, value) -> null).count(Materialized.with(null, Serdes.Long()));    final TopologyDescription describe = builder.build().describe();    assertEquals("Topologies:\n" + "   Sub-topology: 0\n" + "    Source: KSTREAM-SOURCE-0000000001 (topics: [input-topic])\n" + "      --> KTABLE-SOURCE-0000000002\n" + "    Processor: KTABLE-SOURCE-0000000002 (stores: [input-topic-STATE-STORE-0000000000])\n" + "      --> KTABLE-SELECT-0000000003\n" + "      <-- KSTREAM-SOURCE-0000000001\n" + "    Processor: KTABLE-SELECT-0000000003 (stores: [])\n" + "      --> KSTREAM-SINK-0000000005\n" + "      <-- KTABLE-SOURCE-0000000002\n" + "    Sink: KSTREAM-SINK-0000000005 (topic: KTABLE-AGGREGATE-STATE-STORE-0000000004-repartition)\n" + "      <-- KTABLE-SELECT-0000000003\n" + "\n" + "  Sub-topology: 1\n" + "    Source: KSTREAM-SOURCE-0000000006 (topics: [KTABLE-AGGREGATE-STATE-STORE-0000000004-repartition])\n" + "      --> KTABLE-AGGREGATE-0000000007\n" + "    Processor: KTABLE-AGGREGATE-0000000007 (stores: [KTABLE-AGGREGATE-STATE-STORE-0000000004])\n" + "      --> none\n" + "      <-- KSTREAM-SOURCE-0000000006\n" + "\n", describe.toString());}
f19245
0
addProcessor
private TopologyDescription.Processor kafkatest_f19254_0(final String processorName, final TopologyDescription.Node... parents)
{    return addProcessorWithNewStore(processorName, new String[0], parents);}
f19254
0
addProcessorWithNewStore
private TopologyDescription.Processor kafkatest_f19255_0(final String processorName, final String[] storeNames, final TopologyDescription.Node... parents)
{    return addProcessorWithStore(processorName, storeNames, true, parents);}
f19255
0
getInternalBuilder
public InternalTopologyBuilder kafkatest_f19264_0(final String applicationId)
{    return internalTopologyBuilder.rewriteTopology(new StreamsConfig(StreamsTestUtils.getStreamsConfig(applicationId)));}
f19264
0
name
public String kafkatest_f19265_0()
{    return this.name;}
f19265
0
delete
public synchronized V kafkatest_f19274_0(final K key)
{    return this.map.remove(key);}
f19274
0
range
public synchronized KeyValueIterator<K, V> kafkatest_f19275_0(final K from, final K to)
{    return new DelegatingPeekingKeyValueIterator<>(name, new GenericInMemoryKeyValueIterator<>(this.map.subMap(from, true, to, true).entrySet().iterator()));}
f19275
0
name
public String kafkatest_f19284_0()
{    return this.name;}
f19284
0
init
public void kafkatest_f19285_0(final ProcessorContext context, final StateStore root)
{    if (root != null) {        context.register(root, null);    }    this.open = true;}
f19285
0
range
public synchronized KeyValueIterator<K, ValueAndTimestamp<V>> kafkatest_f19294_0(final K from, final K to)
{    return new DelegatingPeekingKeyValueIterator<>(name, new GenericInMemoryKeyValueIterator<>(this.map.subMap(from, true, to, true).entrySet().iterator()));}
f19294
0
all
public synchronized KeyValueIterator<K, ValueAndTimestamp<V>> kafkatest_f19295_0()
{    final TreeMap<K, ValueAndTimestamp<V>> copy = new TreeMap<>(this.map);    return new DelegatingPeekingKeyValueIterator<>(name, new GenericInMemoryKeyValueIterator<>(copy.entrySet().iterator()));}
f19295
0
baseDir
public File kafkatest_f19306_0()
{    return null;}
f19306
0
close
public void kafkatest_f19309_0(final boolean clean) throws IOException
{    this.offsets.putAll(offsets);    closed = true;}
f19309
0
valueSerde
public Serde<?> kafkatest_f19318_0()
{    return valSerde;}
f19318
0
stateDir
public File kafkatest_f19320_0()
{    if (stateDir == null) {        throw new UnsupportedOperationException("State directory not specified");    }    return stateDir;}
f19320
0
setTime
public void kafkatest_f19330_0(final long timestamp)
{    if (recordContext != null) {        recordContext = new ProcessorRecordContext(timestamp, recordContext.offset(), recordContext.partition(), recordContext.topic(), recordContext.headers());    }    this.timestamp = timestamp;}
f19330
0
timestamp
public long kafkatest_f19331_0()
{    if (recordContext == null) {        return timestamp;    }    return recordContext.timestamp();}
f19331
0
close
public void kafkatest_f19340_0()
{// no-op}
f19340
0
peekNextKey
public K kafkatest_f19341_0()
{    return null;}
f19341
0
setClusterForAdminClient
public void kafkatest_f19350_0(final Cluster cluster)
{    this.cluster = cluster;}
f19350
0
getAdmin
public Admin kafkatest_f19351_0(final Map<String, Object> config)
{    return new MockAdminClient(cluster.nodes(), cluster.nodeById(0));}
f19351
0
collected
public List<ProducerRecord<byte[], byte[]>> kafkatest_f19363_0()
{    return unmodifiableList(collected);}
f19363
0
metrics
public StreamsMetricsImpl kafkatest_f19364_0()
{    return (StreamsMetricsImpl) super.metrics();}
f19364
0
stateRestoreCallback
public StateRestoreCallback kafkatest_f19375_0(final String storeName)
{    return restoreCallbacks.get(storeName);}
f19375
0
makeReady
public void kafkatest_f19376_0(final Map<String, InternalTopicConfig> topics)
{    for (final InternalTopicConfig topic : topics.values()) {        final String topicName = topic.name();        final int numberOfPartitions = topic.numberOfPartitions().get();        readyTopics.put(topicName, numberOfPartitions);        final List<PartitionInfo> partitions = new ArrayList<>();        for (int i = 0; i < numberOfPartitions; i++) {            partitions.add(new PartitionInfo(topicName, i, null, null, null));        }        restoreConsumer.updatePartitions(topicName, partitions);    }}
f19376
0
restore
public void kafkatest_f19385_0(final byte[] key, final byte[] value)
{    keys.add(deserializer.deserialize("", key));    values.add(value);}
f19385
0
putIfAbsent
public Object kafkatest_f19387_0(final Object key, final Object value)
{    return null;}
f19387
0
apply
public KeyValue<V, V> kafkatest_f19397_0(final K key, final V value)
{    return KeyValue.pair(value, value);}
f19397
0
apply
public V kafkatest_f19398_0(final K key, final V value)
{    return value;}
f19398
0
test
public boolean kafkatest_f19407_0(final K key, final V value)
{    return true;}
f19407
0
allGoodPredicate
public static Predicate<K, V> kafkatest_f19408_0()
{    return new AllGoodPredicate<>();}
f19408
0
close
public void kafkatest_f19417_0()
{    super.close();    this.closed = true;}
f19417
0
get
public Processor<K, V> kafkatest_f19418_0()
{    final MockProcessor<K, V> processor = new MockProcessor<>(punctuationType, scheduleInterval);    processors.add(processor);    return processor;}
f19418
0
reset
public void kafkatest_f19427_0()
{    assignedPartition = null;    seekOffset = -1L;    endOffset = 0L;    recordBuffer.clear();}
f19427
0
bufferRecord
public void kafkatest_f19428_0(final ConsumerRecord<K, V> record)
{    recordBuffer.add(new ConsumerRecord<>(record.topic(), record.partition(), record.offset(), record.timestamp(), record.timestampType(), 0L, 0, 0, keySerializer.serialize(record.topic(), record.headers(), record.key()), valueSerializer.serialize(record.topic(), record.headers(), record.value()), record.headers()));    endOffset = record.offset();    super.updateEndOffsets(Collections.singletonMap(assignedPartition, endOffset));}
f19428
0
close
public void kafkatest_f19437_0()
{    super.close();    this.closed = true;}
f19437
0
restore
public void kafkatest_f19438_0(final byte[] key, final byte[] value)
{    restored.add(KeyValue.pair(key, value));}
f19438
0
getStateStore
public StateStore kafkatest_f19447_0(final String name)
{    return null;}
f19447
0
schedule
public Cancellable kafkatest_f19448_0(final long interval, final PunctuationType type, final Punctuator callback)
{    return null;}
f19448
0
all
public KeyValueIterator<K, V> kafkatest_f19459_0()
{    return null;}
f19459
0
approximateNumEntries
public long kafkatest_f19460_0()
{    return 0L;}
f19460
0
fetch
public KeyValueIterator<Windowed<K>, V> kafkatest_f19474_0(final K key)
{    if (!open) {        throw new InvalidStateStoreException("not open");    }    if (!sessions.containsKey(key)) {        return new KeyValueIteratorStub<>(Collections.<KeyValue<Windowed<K>, V>>emptyIterator());    }    return new KeyValueIteratorStub<>(sessions.get(key).iterator());}
f19474
0
fetch
public KeyValueIterator<Windowed<K>, V> kafkatest_f19475_0(final K from, final K to)
{    if (!open) {        throw new InvalidStateStoreException("not open");    }    if (sessions.subMap(from, true, to, true).isEmpty()) {        return new KeyValueIteratorStub<>(Collections.<KeyValue<Windowed<K>, V>>emptyIterator());    }    final Iterator<List<KeyValue<Windowed<K>, V>>> keysIterator = sessions.subMap(from, true, to, true).values().iterator();    return new KeyValueIteratorStub<>(new Iterator<KeyValue<Windowed<K>, V>>() {        Iterator<KeyValue<Windowed<K>, V>> it;        @Override        public boolean hasNext() {            while (it == null || !it.hasNext()) {                if (!keysIterator.hasNext()) {                    return false;                }                it = keysIterator.next().iterator();            }            return true;        }        @Override        public KeyValue<Windowed<K>, V> next() {            return it.next();        }    });}
f19475
0
fetch
public KeyValueIterator<Bytes, byte[]> kafkatest_f19487_0(final Bytes key, final long from, final long to)
{    return fetch(key, key, from, to);}
f19487
0
fetch
public KeyValueIterator<Bytes, byte[]> kafkatest_f19488_0(final Bytes keyFrom, final Bytes keyTo, final long from, final long to)
{    fetchCalled = true;    return new KeyValueIteratorStub<>(Collections.<KeyValue<Bytes, byte[]>>emptyIterator());}
f19488
0
persistent
public boolean kafkatest_f19497_0()
{    return false;}
f19497
0
init
public void kafkatest_f19498_0(final ProcessorContext context)
{    SingletonNoOpValueTransformer.this.context = context;}
f19498
0
toList
public static List<KeyValue<K, V>> kafkatest_f19508_0(final Iterator<KeyValue<K, V>> iterator)
{    final List<KeyValue<K, V>> results = new ArrayList<>();    while (iterator.hasNext()) {        results.add(iterator.next());    }    return results;}
f19508
0
valuesToList
public static List<V> kafkatest_f19509_0(final Iterator<KeyValue<K, V>> iterator)
{    final List<V> results = new ArrayList<>();    while (iterator.hasNext()) {        results.add(iterator.next().value);    }    return results;}
f19509
0
delete
public V kafkatest_f19518_0(final K key)
{    return getValueOrNull(inner.delete(key));}
f19518
0
flush
public void kafkatest_f19519_0()
{    inner.flush();}
f19519
0
close
public void kafkatest_f19528_0()
{    inner.close();}
f19528
0
name
public String kafkatest_f19529_0()
{    return inner.name();}
f19529
0
timestamp
public long kafkatest_f19538_0()
{    return timestamp;}
f19538
0
keyValue
public KeyValue kafkatest_f19539_0()
{    return keyValue;}
f19539
0
setRecordMetadata
public void kafkatest_f19548_0(final String topic, final int partition, final long offset, final Headers headers, final long timestamp)
{    this.topic = topic;    this.partition = partition;    this.offset = offset;    this.headers = headers;    this.timestamp = timestamp;}
f19548
0
setTopic
public void kafkatest_f19549_0(final String topic)
{    this.topic = topic;}
f19549
0
timestamp
public long kafkatest_f19558_0()
{    if (timestamp == null) {        throw new IllegalStateException("Timestamp must be set before use via setRecordMetadata() or setTimestamp().");    }    return timestamp;}
f19558
0
register
public void kafkatest_f19559_0(final StateStore store, final StateRestoreCallback stateRestoreCallbackIsIgnoredInMock)
{    stateStores.put(store.name(), store);}
f19559
0
forwarded
public List<CapturedForward> kafkatest_f19568_0()
{    return new LinkedList<>(capturedForwards);}
f19568
0
forwarded
public List<CapturedForward> kafkatest_f19569_0(final String childName)
{    final LinkedList<CapturedForward> result = new LinkedList<>();    for (final CapturedForward capture : capturedForwards) {        if (capture.childName() == null || capture.childName().equals(childName)) {            result.add(capture);        }    }    return result;}
f19569
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19578_0(final K key, final V value, final long timestampMs)
{    return create(key, value, new RecordHeaders(), timestampMs);}
f19578
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19579_0(final K key, final V value, final Headers headers, final long timestampMs)
{    if (topicName == null) {        throw new IllegalStateException("ConsumerRecordFactory was created without defaultTopicName. " + "Use #create(String topicName, K key, V value, long timestampMs) instead.");    }    return create(topicName, key, value, headers, timestampMs);}
f19579
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19588_0(final String topicName, final V value, final Headers headers)
{    return create(topicName, null, value, headers);}
f19588
0
create
public ConsumerRecord<byte[], byte[]> kafkatest_f19589_0(final String topicName, final V value)
{    return create(topicName, null, value, new RecordHeaders());}
f19589
0
compareValue
public static void kafkatest_f19598_0(final ProducerRecord<K, V> record, final V expectedValue) throws AssertionError
{    Objects.requireNonNull(record);    final V recordValue = record.value();    final AssertionError error = new AssertionError("Expected value=" + expectedValue + " but was value=" + recordValue);    if (recordValue != null) {        if (!recordValue.equals(expectedValue)) {            throw error;        }    } else if (expectedValue != null) {        throw error;    }}
f19598
0
compareValue
public static void kafkatest_f19599_0(final ProducerRecord<K, V> record, final ProducerRecord<K, V> expectedRecord) throws AssertionError
{    Objects.requireNonNull(expectedRecord);    compareValue(record, expectedRecord.value());}
f19599
0
compareKeyValueHeaders
public static void kafkatest_f19608_0(final ProducerRecord<K, V> record, final K expectedKey, final V expectedValue, final Headers expectedHeaders) throws AssertionError
{    Objects.requireNonNull(record);    final K recordKey = record.key();    final V recordValue = record.value();    final Headers recordHeaders = record.headers();    final AssertionError error = new AssertionError("Expected <" + expectedKey + ", " + expectedValue + "> with headers=" + expectedHeaders + " but was <" + recordKey + ", " + recordValue + "> with headers=" + recordHeaders);    if (recordKey != null) {        if (!recordKey.equals(expectedKey)) {            throw error;        }    } else if (expectedKey != null) {        throw error;    }    if (recordValue != null) {        if (!recordValue.equals(expectedValue)) {            throw error;        }    } else if (expectedValue != null) {        throw error;    }    if (recordHeaders != null) {        if (!recordHeaders.equals(expectedHeaders)) {            throw error;        }    } else if (expectedHeaders != null) {        throw error;    }}
f19608
0
compareKeyValueHeaders
public static void kafkatest_f19609_0(final ProducerRecord<K, V> record, final ProducerRecord<K, V> expectedRecord) throws AssertionError
{    Objects.requireNonNull(expectedRecord);    compareKeyValueHeaders(record, expectedRecord.key(), expectedRecord.value(), expectedRecord.headers());}
f19609
0
pipeInput
public void kafkatest_f19621_0(final List<ConsumerRecord<byte[], byte[]>> records)
{    for (final ConsumerRecord<byte[], byte[]> record : records) {        pipeInput(record);    }}
f19621
0
advanceWallClockTime
public void kafkatest_f19622_0(final long advanceMs)
{    mockWallClockTime.sleep(advanceMs);    if (task != null) {        task.maybePunctuateSystemTime();        task.commit();    }    captureOutputRecords();}
f19622
0
getWindowStore
public WindowStore<K, V>f19631_1final String name)
{    final StateStore store = getStateStore(name, false);    if (store instanceof TimestampedWindowStore) {                return new WindowStoreFacade<>((TimestampedWindowStore<K, V>) store);    }    return store instanceof WindowStore ? (WindowStore<K, V>) store : null;}
public WindowStore<K, V>f19631
1
getTimestampedWindowStore
public WindowStore<K, ValueAndTimestamp<V>> kafkatest_f19632_0(final String name)
{    final StateStore store = getStateStore(name, false);    return store instanceof TimestampedWindowStore ? (TimestampedWindowStore<K, V>) store : null;}
f19632
0
position
public synchronized long kafkatest_f19643_0(final TopicPartition partition)
{    return 0L;}
f19643
0
setup
public void kafkatest_f19644_0()
{    keyValueStoreFacade = new KeyValueStoreFacade<>(mockedKeyValueTimestampStore);}
f19644
0
shouldReturnIsPersistent
public void kafkatest_f19653_0()
{    expect(mockedKeyValueTimestampStore.persistent()).andReturn(true).andReturn(false);    replay(mockedKeyValueTimestampStore);    assertThat(keyValueStoreFacade.persistent(), is(true));    assertThat(keyValueStoreFacade.persistent(), is(false));    verify(mockedKeyValueTimestampStore);}
f19653
0
shouldReturnIsOpen
public void kafkatest_f19654_0()
{    expect(mockedKeyValueTimestampStore.isOpen()).andReturn(true).andReturn(false);    replay(mockedKeyValueTimestampStore);    assertThat(keyValueStoreFacade.isOpen(), is(true));    assertThat(keyValueStoreFacade.isOpen(), is(false));    verify(mockedKeyValueTimestampStore);}
f19654
0
shouldReturnIsOpen
public void kafkatest_f19663_0()
{    expect(mockedWindowTimestampStore.isOpen()).andReturn(true).andReturn(false);    replay(mockedWindowTimestampStore);    assertThat(windowStoreFacade.isOpen(), is(true));    assertThat(windowStoreFacade.isOpen(), is(false));    verify(mockedWindowTimestampStore);}
f19663
0
shouldCaptureOutputRecords
public void kafkatest_f19664_0()
{    final AbstractProcessor<String, Long> processor = new AbstractProcessor<String, Long>() {        @Override        public void process(final String key, final Long value) {            context().forward(key + value, key.length() + value);        }    };    final MockProcessorContext context = new MockProcessorContext();    processor.init(context);    processor.process("foo", 5L);    processor.process("barbaz", 50L);    final Iterator<CapturedForward> forwarded = context.forwarded().iterator();    assertEquals(new KeyValue<>("foo5", 8L), forwarded.next().keyValue());    assertEquals(new KeyValue<>("barbaz50", 56L), forwarded.next().keyValue());    assertFalse(forwarded.hasNext());    context.resetForwards();    assertEquals(0, context.forwarded().size());}
f19664
0
process
public void kafkatest_f19673_0(final String key, final Long value)
{    context().forward(key, value, "child1");}
f19673
0
shouldCaptureCommitsAndAllowReset
public void kafkatest_f19674_0()
{    final AbstractProcessor<String, Long> processor = new AbstractProcessor<String, Long>() {        private int count = 0;        @Override        public void process(final String key, final Long value) {            if (++count > 2) {                context().commit();            }        }    };    final MockProcessorContext context = new MockProcessorContext();    processor.init(context);    processor.process("foo", 5L);    processor.process("barbaz", 50L);    assertFalse(context.committed());    processor.process("foobar", 500L);    assertTrue(context.committed());    context.resetCommit();    assertFalse(context.committed());}
f19674
0
shouldSetStartTime
public void kafkatest_f19685_0()
{    final TopologyTestDriver.MockTime time = new TopologyTestDriver.MockTime(42L);    assertEquals(42L, time.milliseconds());    assertEquals(42L * 1000L * 1000L, time.nanoseconds());}
f19685
0
shouldGetNanosAsMillis
public void kafkatest_f19686_0()
{    final TopologyTestDriver.MockTime time = new TopologyTestDriver.MockTime(42L);    assertEquals(42L, time.hiResClockMs());}
f19686
0
shouldNotAllowToCreateTopicWithNullTopicNameWithKeyValuePairs
public void kafkatest_f19695_0()
{    factory.create(null, Collections.singletonList(KeyValue.pair(rawKey, value)));}
f19695
0
shouldNotAllowToCreateTopicWithNullTopicNameWithKeyValuePairsAndCustomTimestamps
public void kafkatest_f19696_0()
{    factory.create(null, Collections.singletonList(KeyValue.pair(rawKey, value)), timestamp, 2L);}
f19696
0
shouldCreateConsumerRecordWithOtherTopicName
public void kafkatest_f19705_0()
{    verifyRecord(otherTopicName, rawKey, rawValue, 0L, factory.create(otherTopicName, rawKey, value));    factory.advanceTimeMs(3L);    verifyRecord(otherTopicName, rawKey, rawValue, 3L, factory.create(otherTopicName, rawKey, value));}
f19705
0
shouldCreateConsumerRecord
public void kafkatest_f19706_0()
{    verifyRecord(topicName, rawKey, rawValue, 0L, factory.create(rawKey, value));    factory.advanceTimeMs(3L);    verifyRecord(topicName, rawKey, rawValue, 3L, factory.create(rawKey, value));}
f19706
0
shouldNotAllowNullProducerRecordWithExpectedRecordForCompareValue
public void kafkatest_f19715_0()
{    OutputVerifier.compareValue((ProducerRecord<byte[], byte[]>) null, producerRecord);}
f19715
0
shouldNotAllowNullExpectedRecordForCompareValue
public void kafkatest_f19716_0()
{    OutputVerifier.compareValue(producerRecord, (ProducerRecord<byte[], byte[]>) null);}
f19716
0
shouldNotAllowNullExpectedRecordForCompareKeyValueTimestamp
public void kafkatest_f19725_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, null);}
f19725
0
shouldPassIfValueIsEqualForCompareValue
public void kafkatest_f19726_0()
{    OutputVerifier.compareValue(producerRecord, value);}
f19726
0
shouldFailIfValueIsDifferentWithNullReverseForCompareValueWithProducerRecord
public void kafkatest_f19735_0()
{    OutputVerifier.compareValue(nullKeyValueRecord, new ProducerRecord<>("sameTopic", Integer.MAX_VALUE, Long.MAX_VALUE, value, value));}
f19735
0
shouldPassIfKeyAndValueIsEqualForCompareKeyValue
public void kafkatest_f19736_0()
{    OutputVerifier.compareKeyValue(producerRecord, key, value);}
f19736
0
shouldPassIfKeyAndValueIsEqualWithNullForCompareKeyValueWithProducerRecord
public void kafkatest_f19745_0()
{    OutputVerifier.compareKeyValue(nullKeyValueRecord, new ProducerRecord<byte[], byte[]>("otherTopic", 0, 0L, null, null));}
f19745
0
shouldFailIfKeyIsDifferentForCompareKeyValueWithProducerRecord
public void kafkatest_f19746_0()
{    OutputVerifier.compareKeyValue(producerRecord, new ProducerRecord<>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, value, value));}
f19746
0
shouldFailIfValueIsDifferentWithNullForCompareValueTimestamp
public void kafkatest_f19755_0()
{    OutputVerifier.compareValueTimestamp(producerRecord, null, Long.MAX_VALUE);}
f19755
0
shouldFailIfValueIsDifferentWithNullReverseForCompareValueTimestamp
public void kafkatest_f19756_0()
{    OutputVerifier.compareValueTimestamp(nullKeyValueRecord, value, Long.MAX_VALUE);}
f19756
0
shouldPassIfKeyAndValueAndTimestampIsEqualForCompareKeyValueTimestamp
public void kafkatest_f19765_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, key, value, Long.MAX_VALUE);}
f19765
0
shouldPassIfKeyAndValueIsEqualWithNullForCompareKeyValueTimestamp
public void kafkatest_f19766_0()
{    OutputVerifier.compareKeyValueTimestamp(nullKeyValueRecord, null, null, Long.MAX_VALUE);}
f19766
0
shouldFailIfKeyIsDifferentForCompareKeyValueTimestampWithProducerRecord
public void kafkatest_f19775_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, new ProducerRecord<>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, value, value));}
f19775
0
shouldFailIfKeyIsDifferentWithNullForCompareKeyValueTimestampWithProducerRecord
public void kafkatest_f19776_0()
{    OutputVerifier.compareKeyValueTimestamp(producerRecord, new ProducerRecord<byte[], byte[]>("someTopic", Integer.MAX_VALUE, Long.MAX_VALUE, null, value));}
f19776
0
punctuate
public void kafkatest_f19785_0(final long timestamp)
{    punctuatedAt.add(timestamp);}
f19785
0
init
public void kafkatest_f19786_0(final ProcessorContext context)
{    initialized = true;    this.context = context;    for (final Punctuation punctuation : punctuations) {        this.context.schedule(Duration.ofMillis(punctuation.intervalMs), punctuation.punctuationType, punctuation.callback);    }}
f19786
0
setupMultipleSourceTopology
private Topology kafkatest_f19795_0(final String... sourceTopicNames)
{    final Topology topology = new Topology();    final String[] processorNames = new String[sourceTopicNames.length];    int i = 0;    for (final String sourceTopicName : sourceTopicNames) {        final String sourceName = sourceTopicName + "-source";        final String processorName = sourceTopicName + "-processor";        topology.addSource(sourceName, sourceTopicName);        processorNames[i++] = processorName;        topology.addProcessor(processorName, new MockProcessorSupplier(), sourceName);    }    topology.addSink("sink-topic", SINK_TOPIC_1, processorNames);    return topology;}
f19795
0
setupGlobalStoreTopology
private Topology kafkatest_f19796_0(final String... sourceTopicNames)
{    if (sourceTopicNames.length == 0) {        throw new IllegalArgumentException("sourceTopicNames cannot be empty");    }    final Topology topology = new Topology();    for (final String sourceTopicName : sourceTopicNames) {        topology.addGlobalStore(Stores.<Bytes, byte[]>keyValueStoreBuilder(Stores.inMemoryKeyValueStore(sourceTopicName + "-globalStore"), null, null).withLoggingDisabled(), sourceTopicName, null, null, sourceTopicName, sourceTopicName + "-processor", new MockProcessorSupplier());    }    return topology;}
f19796
0
shouldProcessConsumerRecordList
public void kafkatest_f19805_0()
{    testDriver = new TopologyTestDriver(setupMultipleSourceTopology(SOURCE_TOPIC_1, SOURCE_TOPIC_2), config);    final List<Record> processedRecords1 = mockProcessors.get(0).processedRecords;    final List<Record> processedRecords2 = mockProcessors.get(1).processedRecords;    final List<ConsumerRecord<byte[], byte[]>> testRecords = new ArrayList<>(2);    testRecords.add(consumerRecord1);    testRecords.add(consumerRecord2);    testDriver.pipeInput(testRecords);    assertEquals(1, processedRecords1.size());    assertEquals(1, processedRecords2.size());    Record record = processedRecords1.get(0);    Record expectedResult = new Record(consumerRecord1, 0L);    assertThat(record, equalTo(expectedResult));    record = processedRecords2.get(0);    expectedResult = new Record(consumerRecord2, 0L);    assertThat(record, equalTo(expectedResult));}
f19805
0
shouldForwardRecordsFromSubtopologyToSubtopology
public void kafkatest_f19806_0()
{    testDriver = new TopologyTestDriver(setupTopologyWithTwoSubtopologies(), config);    testDriver.pipeInput(consumerRecord1);    ProducerRecord outputRecord = testDriver.readOutput(SINK_TOPIC_1);    assertEquals(key1, outputRecord.key());    assertEquals(value1, outputRecord.value());    assertEquals(SINK_TOPIC_1, outputRecord.topic());    outputRecord = testDriver.readOutput(SINK_TOPIC_2);    assertEquals(key1, outputRecord.key());    assertEquals(value1, outputRecord.value());    assertEquals(SINK_TOPIC_2, outputRecord.topic());}
f19806
0
shouldThrowIfPersistentBuiltInStoreIsAccessedWithUntypedMethod
public void kafkatest_f19815_0()
{    shouldThrowIfBuiltInStoreIsAccessedWithUntypedMethod(true);}
f19815
0
shouldThrowIfBuiltInStoreIsAccessedWithUntypedMethod
private void kafkatest_f19816_0(final boolean persistent)
{    final String keyValueStoreName = "keyValueStore";    final String timestampedKeyValueStoreName = "keyValueTimestampStore";    final String windowStoreName = "windowStore";    final String timestampedWindowStoreName = "windowTimestampStore";    final String sessionStoreName = "sessionStore";    final String globalKeyValueStoreName = "globalKeyValueStore";    final String globalTimestampedKeyValueStoreName = "globalKeyValueTimestampStore";    final Topology topology = setupSingleProcessorTopology();    addStoresToTopology(topology, persistent, keyValueStoreName, timestampedKeyValueStoreName, windowStoreName, timestampedWindowStoreName, sessionStoreName, globalKeyValueStoreName, globalTimestampedKeyValueStoreName);    testDriver = new TopologyTestDriver(topology, config);    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(keyValueStoreName));        assertThat(e.getMessage(), equalTo("Store " + keyValueStoreName + " is a key-value store and should be accessed via `getKeyValueStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(timestampedKeyValueStoreName));        assertThat(e.getMessage(), equalTo("Store " + timestampedKeyValueStoreName + " is a timestamped key-value store and should be accessed via `getTimestampedKeyValueStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(windowStoreName));        assertThat(e.getMessage(), equalTo("Store " + windowStoreName + " is a window store and should be accessed via `getWindowStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(timestampedWindowStoreName));        assertThat(e.getMessage(), equalTo("Store " + timestampedWindowStoreName + " is a timestamped window store and should be accessed via `getTimestampedWindowStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(sessionStoreName));        assertThat(e.getMessage(), equalTo("Store " + sessionStoreName + " is a session store and should be accessed via `getSessionStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(globalKeyValueStoreName));        assertThat(e.getMessage(), equalTo("Store " + globalKeyValueStoreName + " is a key-value store and should be accessed via `getKeyValueStore()`"));    }    {        final IllegalArgumentException e = assertThrows(IllegalArgumentException.class, () -> testDriver.getStateStore(globalTimestampedKeyValueStoreName));        assertThat(e.getMessage(), equalTo("Store " + globalTimestampedKeyValueStoreName + " is a timestamped key-value store and should be accessed via `getTimestampedKeyValueStore()`"));    }}
f19816
0
shouldPunctuateIfEvenTimeAdvances
public void kafkatest_f19825_0()
{    setup();    testDriver.pipeInput(recordFactory.create("input-topic", "a", 1L, 9999L));    OutputVerifier.compareKeyValue(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer), "a", 21L);    testDriver.pipeInput(recordFactory.create("input-topic", "a", 1L, 9999L));    assertNull(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer));    testDriver.pipeInput(recordFactory.create("input-topic", "a", 1L, 10000L));    OutputVerifier.compareKeyValue(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer), "a", 21L);    assertNull(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer));}
f19825
0
shouldPunctuateIfWallClockTimeAdvances
public void kafkatest_f19826_0()
{    setup();    testDriver.advanceWallClockTime(60000);    OutputVerifier.compareKeyValue(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer), "a", 21L);    assertNull(testDriver.readOutput("result-topic", stringDeserializer, longDeserializer));}
f19826
0
process
public void kafkatest_f19836_0(final String key, final Long value)
{    store.put(key, value);}
f19836
0
shouldFeedStoreFromGlobalKTable
public void kafkatest_f19838_0()
{    final StreamsBuilder builder = new StreamsBuilder();    builder.globalTable("topic", Consumed.with(Serdes.String(), Serdes.String()), Materialized.as("globalStore"));    try (final TopologyTestDriver testDriver = new TopologyTestDriver(builder.build(), config)) {        final KeyValueStore<String, String> globalStore = testDriver.getKeyValueStore("globalStore");        Assert.assertNotNull(globalStore);        Assert.assertNotNull(testDriver.getAllStateStores().get("globalStore"));        final ConsumerRecordFactory<String, String> recordFactory = new ConsumerRecordFactory<>(new StringSerializer(), new StringSerializer());        testDriver.pipeInput(recordFactory.create("topic", "k1", "value1"));        // we expect to have both in the global store, the one from pipeInput and the one from the producer        Assert.assertEquals("value1", globalStore.get("k1"));    }}
f19838
0
printProcessorSupplier
private static ProcessorSupplier<K, V> kafkatest_f19847_0()
{    return new ProcessorSupplier<K, V>() {        public Processor<K, V> get() {            return new AbstractProcessor<K, V>() {                private int numRecordsProcessed = 0;                @Override                public void init(final ProcessorContext context) {                    System.out.println("[0.10.0] initializing processor: topic=data taskId=" + context.taskId());                    numRecordsProcessed = 0;                }                @Override                public void process(final K key, final V value) {                    numRecordsProcessed++;                    if (numRecordsProcessed % 100 == 0) {                        System.out.println("processed " + numRecordsProcessed + " records from topic=data");                    }                }                @Override                public void punctuate(final long timestamp) {                }                @Override                public void close() {                }            };        }    };}
f19847
0
get
public Processor<K, V> kafkatest_f19848_0()
{    return new AbstractProcessor<K, V>() {        private int numRecordsProcessed = 0;        @Override        public void init(final ProcessorContext context) {            System.out.println("[0.10.0] initializing processor: topic=data taskId=" + context.taskId());            numRecordsProcessed = 0;        }        @Override        public void process(final K key, final V value) {            numRecordsProcessed++;            if (numRecordsProcessed % 100 == 0) {                System.out.println("processed " + numRecordsProcessed + " records from topic=data");            }        }        @Override        public void punctuate(final long timestamp) {        }        @Override        public void close() {        }    };}
f19848
0
main
public static void kafkatest_f19861_0(final String[] args) throws Exception
{    if (args.length < 2) {        System.err.println("StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only " + args.length + " provided: " + (args.length > 0 ? args[0] : ""));    }    final String kafka = args[0];    final String propFileName = args.length > 1 ? args[1] : null;    final Properties streamsProperties = Utils.loadProps(propFileName);    System.out.println("StreamsTest instance started (StreamsUpgradeTest v0.10.2)");    System.out.println("kafka=" + kafka);    System.out.println("props=" + streamsProperties);    final KStreamBuilder builder = new KStreamBuilder();    final KStream dataStream = builder.stream("data");    dataStream.process(printProcessorSupplier());    dataStream.to("echo");    final Properties config = new Properties();    config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, "StreamsUpgradeTest");    config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);    config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);    config.putAll(streamsProperties);    final KafkaStreams streams = new KafkaStreams(builder, config);    streams.start();    Runtime.getRuntime().addShutdownHook(new Thread() {        @Override        public void run() {            streams.close();            System.out.println("UPGRADE-TEST-CLIENT-CLOSED");            System.out.flush();        }    });}
f19861
0
run
public void kafkatest_f19862_0()
{    streams.close();    System.out.println("UPGRADE-TEST-CLIENT-CLOSED");    System.out.flush();}
f19862
0
init
public void kafkatest_f19873_0(final ProcessorContext context)
{    System.out.println("[0.11.0] initializing processor: topic=data taskId=" + context.taskId());    numRecordsProcessed = 0;}
f19873
0
process
public void kafkatest_f19874_0(final K key, final V value)
{    numRecordsProcessed++;    if (numRecordsProcessed % 100 == 0) {        System.out.println("processed " + numRecordsProcessed + " records from topic=data");    }}
f19874
0
printProcessorSupplier
private static ProcessorSupplier<K, V> kafkatest_f19887_0()
{    return new ProcessorSupplier<K, V>() {        public Processor<K, V> get() {            return new AbstractProcessor<K, V>() {                private int numRecordsProcessed = 0;                @Override                public void init(final ProcessorContext context) {                    System.out.println("[1.1] initializing processor: topic=data taskId=" + context.taskId());                    numRecordsProcessed = 0;                }                @Override                public void process(final K key, final V value) {                    numRecordsProcessed++;                    if (numRecordsProcessed % 100 == 0) {                        System.out.println("processed " + numRecordsProcessed + " records from topic=data");                    }                }                @Override                public void punctuate(final long timestamp) {                }                @Override                public void close() {                }            };        }    };}
f19887
0
get
public Processor<K, V> kafkatest_f19888_0()
{    return new AbstractProcessor<K, V>() {        private int numRecordsProcessed = 0;        @Override        public void init(final ProcessorContext context) {            System.out.println("[1.1] initializing processor: topic=data taskId=" + context.taskId());            numRecordsProcessed = 0;        }        @Override        public void process(final K key, final V value) {            numRecordsProcessed++;            if (numRecordsProcessed % 100 == 0) {                System.out.println("processed " + numRecordsProcessed + " records from topic=data");            }        }        @Override        public void punctuate(final long timestamp) {        }        @Override        public void close() {        }    };}
f19888
0
init
public void kafkatest_f19900_0(final ProcessorContext context)
{    System.out.println("[2.1] initializing processor: topic=data taskId=" + context.taskId());    numRecordsProcessed = 0;}
f19900
0
process
public void kafkatest_f19901_0(final K key, final V value)
{    numRecordsProcessed++;    if (numRecordsProcessed % 100 == 0) {        System.out.println("processed " + numRecordsProcessed + " records from topic=data");    }}
f19901
0
main
public static void kafkatest_f19913_0(String[] args) throws Exception
{    ArgumentParser parser = ArgumentParsers.newArgumentParser("client-compatibility-test").defaultHelp(true).description("This tool is used to verify client compatibility guarantees.");    parser.addArgument("--topic").action(store()).required(true).type(String.class).dest("topic").metavar("TOPIC").help("the compatibility test will produce messages to this topic");    parser.addArgument("--bootstrap-server").action(store()).required(true).type(String.class).dest("bootstrapServer").metavar("BOOTSTRAP_SERVER").help("The server(s) to use for bootstrapping");    parser.addArgument("--offsets-for-times-supported").action(store()).required(true).type(Boolean.class).dest("offsetsForTimesSupported").metavar("OFFSETS_FOR_TIMES_SUPPORTED").help("True if KafkaConsumer#offsetsForTimes is supported by the current broker version");    parser.addArgument("--cluster-id-supported").action(store()).required(true).type(Boolean.class).dest("clusterIdSupported").metavar("CLUSTER_ID_SUPPORTED").help("True if cluster IDs are supported.  False if cluster ID always appears as null.");    parser.addArgument("--expect-record-too-large-exception").action(store()).required(true).type(Boolean.class).dest("expectRecordTooLargeException").metavar("EXPECT_RECORD_TOO_LARGE_EXCEPTION").help("True if we should expect a RecordTooLargeException when trying to read from a topic " + "that contains a message that is bigger than " + ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG + ".  This is pre-KIP-74 behavior.");    parser.addArgument("--num-cluster-nodes").action(store()).required(true).type(Integer.class).dest("numClusterNodes").metavar("NUM_CLUSTER_NODES").help("The number of cluster nodes we should expect to see from the AdminClient.");    parser.addArgument("--create-topics-supported").action(store()).required(true).type(Boolean.class).dest("createTopicsSupported").metavar("CREATE_TOPICS_SUPPORTED").help("Whether we should be able to create topics via the AdminClient.");    parser.addArgument("--describe-acls-supported").action(store()).required(true).type(Boolean.class).dest("describeAclsSupported").metavar("DESCRIBE_ACLS_SUPPORTED").help("Whether describeAcls is supported in the AdminClient.");    Namespace res = null;    try {        res = parser.parseArgs(args);    } catch (ArgumentParserException e) {        if (args.length == 0) {            parser.printHelp();            Exit.exit(0);        } else {            parser.handleError(e);            Exit.exit(1);        }    }    TestConfig testConfig = new TestConfig(res);    ClientCompatibilityTest test = new ClientCompatibilityTest(testConfig);    try {        test.run();    } catch (Throwable t) {        System.out.printf("FAILED: Caught exception %s%n%n", t.getMessage());        t.printStackTrace();        Exit.exit(1);    }    System.out.println("SUCCESS.");    Exit.exit(0);}
f19913
0
toHexString
private static String kafkatest_f19914_0(byte[] buf)
{    StringBuilder bld = new StringBuilder();    for (byte b : buf) {        bld.append(String.format("%02x", b));    }    return bld.toString();}
f19914
0
onUpdate
public void kafkatest_f19923_0(ClusterResource clusterResource)
{    if (expectClusterId) {        if (clusterResource.clusterId() == null) {            throw new RuntimeException("Expected cluster id to be supported, but it was null.");        }    } else {        if (clusterResource.clusterId() != null) {            throw new RuntimeException("Expected cluster id to be null, but it was supported.");        }    }}
f19923
0
testConsume
public voidf19924_1final long prodTimeMs) throws Throwable
{    Properties consumerProps = new Properties();    consumerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, testConfig.bootstrapServer);    consumerProps.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 512);    ClientCompatibilityTestDeserializer deserializer = new ClientCompatibilityTestDeserializer(testConfig.expectClusterId);    try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps, deserializer, deserializer)) {        final List<PartitionInfo> partitionInfos = consumer.partitionsFor(testConfig.topic);        if (partitionInfos.size() < 1)            throw new RuntimeException("Expected at least one partition for topic " + testConfig.topic);        final Map<TopicPartition, Long> timestampsToSearch = new HashMap<>();        final LinkedList<TopicPartition> topicPartitions = new LinkedList<>();        for (PartitionInfo partitionInfo : partitionInfos) {            TopicPartition topicPartition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());            timestampsToSearch.put(topicPartition, prodTimeMs);            topicPartitions.add(topicPartition);        }        final OffsetsForTime offsetsForTime = new OffsetsForTime();        tryFeature("offsetsForTimes", testConfig.offsetsForTimesSupported, () -> offsetsForTime.result = consumer.offsetsForTimes(timestampsToSearch), () ->         // Whether or not offsetsForTimes works, beginningOffsets and endOffsets        // should work.        consumer.beginningOffsets(timestampsToSearch.keySet());        consumer.endOffsets(timestampsToSearch.keySet());        consumer.assign(topicPartitions);        consumer.seekToBeginning(topicPartitions);        final Iterator<byte[]> iter = new Iterator<byte[]>() {            private static final int TIMEOUT_MS = 10000;            private Iterator<ConsumerRecord<byte[], byte[]>> recordIter = null;            private byte[] next = null;            private byte[] fetchNext() {                while (true) {                    long curTime = Time.SYSTEM.milliseconds();                    if (curTime - prodTimeMs > TIMEOUT_MS)                        throw new RuntimeException("Timed out after " + TIMEOUT_MS + " ms.");                    if (recordIter == null) {                        ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));                        recordIter = records.iterator();                    }                    if (recordIter.hasNext())                        return recordIter.next().value();                    recordIter = null;                }            }            @Override            public boolean hasNext() {                if (next != null)                    return true;                next = fetchNext();                return next != null;            }            @Override            public byte[] next() {                if (!hasNext())                    throw new NoSuchElementException();                byte[] cur = next;                next = null;                return cur;            }            @Override            public void remove() {                throw new UnsupportedOperationException();            }        };        byte[] next = iter.next();        try {            compareArrays(message1, next);                    } catch (RuntimeException e) {            throw new RuntimeException("The first message in this topic was not ours. Please use a new topic when " + "running this program.");        }        try {            next = iter.next();            if (testConfig.expectRecordTooLargeException) {                throw new RuntimeException("Expected to get a RecordTooLargeException when reading a record " + "bigger than " + ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG);            }            try {                compareArrays(message2, next);            } catch (RuntimeException e) {                System.out.println("The second message in this topic was not ours. Please use a new " + "topic when running this program.");                Exit.exit(1);            }        } catch (RecordTooLargeException e) {                        if (!testConfig.expectRecordTooLargeException)                throw new RuntimeException("Got an unexpected RecordTooLargeException when reading a record " + "bigger than " + ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG);        }            }    }
public voidf19924
1
record
public void kafkatest_f19933_0(int iter, int latency, int bytes, long time)
{    this.count++;    this.bytes += bytes;    this.totalLatency += latency;    this.maxLatency = Math.max(this.maxLatency, latency);    this.windowCount++;    this.windowBytes += bytes;    this.windowTotalLatency += latency;    this.windowMaxLatency = Math.max(windowMaxLatency, latency);    if (iter % this.sampling == 0) {        this.latencies[index] = latency;        this.index++;    }    /* maybe report the recent perf */    if (time - windowStart >= reportingInterval) {        printWindow();        newWindow();    }}
f19933
0
nextCompletion
public Callback kafkatest_f19934_0(long start, int bytes, Stats stats)
{    Callback cb = new PerfCallback(this.iteration, start, bytes, stats);    this.iteration++;    return cb;}
f19934
0
metricRemoval
public voidf19943_1KafkaMetric metric)
{    synchronized (lock) {                metrics.remove(metric.metricName());    }}
public voidf19943
1
close
public void kafkatest_f19944_0()
{    executor.shutdown();    try {        executor.awaitTermination(30, TimeUnit.SECONDS);    } catch (InterruptedException e) {        throw new KafkaException("Interrupted when shutting down PushHttpMetricsReporter", e);    }}
f19944
0
name
public String kafkatest_f19953_0()
{    return name;}
f19953
0
group
public String kafkatest_f19954_0()
{    return group;}
f19954
0
createProducer
private static KafkaProducer<String, String> kafkatest_f19963_0(Namespace parsedArgs)
{    String transactionalId = parsedArgs.getString("transactionalId");    String brokerList = parsedArgs.getString("brokerList");    Properties props = new Properties();    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);    props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");    // We set a small batch size to ensure that we have multiple inflight requests per transaction.    // If it is left at the default, each transaction will have only one batch per partition, hence not testing    // the case with multiple inflights.    props.put(ProducerConfig.BATCH_SIZE_CONFIG, "512");    props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "5");    return new KafkaProducer<>(props);}
f19963
0
createConsumer
private static KafkaConsumer<String, String> kafkatest_f19964_0(Namespace parsedArgs)
{    String consumerGroup = parsedArgs.getString("consumerGroup");    String brokerList = parsedArgs.getString("brokerList");    Integer numMessagesPerTransaction = parsedArgs.getInt("messagesPerTransaction");    Properties props = new Properties();    props.put(ConsumerConfig.GROUP_ID_CONFIG, consumerGroup);    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);    props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");    props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, numMessagesPerTransaction.toString());    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");    props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "10000");    props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, "3000");    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");    return new KafkaConsumer<>(props);}
f19964
0
addKafkaSerializerModule
private void kafkatest_f19973_0()
{    SimpleModule kafka = new SimpleModule();    kafka.addSerializer(TopicPartition.class, new JsonSerializer<TopicPartition>() {        @Override        public void serialize(TopicPartition tp, JsonGenerator gen, SerializerProvider serializers) throws IOException {            gen.writeStartObject();            gen.writeObjectField("topic", tp.topic());            gen.writeObjectField("partition", tp.partition());            gen.writeEndObject();        }    });    mapper.registerModule(kafka);}
f19973
0
serialize
public void kafkatest_f19974_0(TopicPartition tp, JsonGenerator gen, SerializerProvider serializers) throws IOException
{    gen.writeStartObject();    gen.writeObjectField("topic", tp.topic());    gen.writeObjectField("partition", tp.partition());    gen.writeEndObject();}
f19974
0
run
public voidf19983_1)
{    try {        printJson(new StartupComplete());        consumer.subscribe(Collections.singletonList(topic), this);        while (!isFinished()) {            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(Long.MAX_VALUE));            Map<TopicPartition, OffsetAndMetadata> offsets = onRecordsReceived(records);            if (!useAutoCommit) {                if (useAsyncCommit)                    consumer.commitAsync(offsets, this);                else                    commitSync(offsets);            }        }    } catch (WakeupException e) {        // ignore, we are closing        log.trace("Caught WakeupException because consumer is shutdown, ignore and terminate.", e);    } catch (Throwable t) {        // Log the error so it goes to the service log and not stdout            } finally {        consumer.close();        printJson(new ShutdownComplete());        shutdownLatch.countDown();    }}
public voidf19983
1
close
public void kafkatest_f19984_0()
{    boolean interrupted = false;    try {        consumer.wakeup();        while (true) {            try {                shutdownLatch.await();                return;            } catch (InterruptedException e) {                interrupted = true;            }        }    } finally {        if (interrupted)            Thread.currentThread().interrupt();    }}
f19984
0
count
public long kafkatest_f19993_0()
{    return count;}
f19993
0
partitions
public List<RecordSetSummary> kafkatest_f19994_0()
{    return partitionSummaries;}
f19994
0
name
public String kafkatest_f20003_0()
{    return "offsets_committed";}
f20003
0
offsets
public List<CommitData> kafkatest_f20004_0()
{    return offsets;}
f20004
0
main
public static void kafkatest_f20013_0(String[] args)
{    ArgumentParser parser = argParser();    if (args.length == 0) {        parser.printHelp();        Exit.exit(0);    }    try {        final VerifiableConsumer consumer = createFromArgs(parser, args);        Runtime.getRuntime().addShutdownHook(new Thread(() -> consumer.close()));        consumer.run();    } catch (ArgumentParserException e) {        parser.handleError(e);        Exit.exit(1);    }}
f20013
0
argParser
private static ArgumentParser kafkatest_f20014_0()
{    ArgumentParser parser = ArgumentParsers.newArgumentParser("verifiable-log4j-appender").defaultHelp(true).description("This tool produces increasing integers to the specified topic using KafkaLog4jAppender.");    parser.addArgument("--topic").action(store()).required(true).type(String.class).metavar("TOPIC").help("Produce messages to this topic.");    parser.addArgument("--broker-list").action(store()).required(true).type(String.class).metavar("HOST1:PORT1[,HOST2:PORT2[...]]").dest("brokerList").help("Comma-separated list of Kafka brokers in the form HOST1:PORT1,HOST2:PORT2,...");    parser.addArgument("--max-messages").action(store()).required(false).setDefault(-1).type(Integer.class).metavar("MAX-MESSAGES").dest("maxMessages").help("Produce this many messages. If -1, produce messages until the process is killed externally.");    parser.addArgument("--acks").action(store()).required(false).setDefault("-1").type(String.class).choices("0", "1", "-1").metavar("ACKS").help("Acks required on each produced message. See Kafka docs on request.required.acks for details.");    parser.addArgument("--security-protocol").action(store()).required(false).setDefault("PLAINTEXT").type(String.class).choices("PLAINTEXT", "SSL", "SASL_PLAINTEXT", "SASL_SSL").metavar("SECURITY-PROTOCOL").dest("securityProtocol").help("Security protocol to be used while communicating with Kafka brokers.");    parser.addArgument("--ssl-truststore-location").action(store()).required(false).type(String.class).metavar("SSL-TRUSTSTORE-LOCATION").dest("sslTruststoreLocation").help("Location of SSL truststore to use.");    parser.addArgument("--ssl-truststore-password").action(store()).required(false).type(String.class).metavar("SSL-TRUSTSTORE-PASSWORD").dest("sslTruststorePassword").help("Password for SSL truststore to use.");    parser.addArgument("--appender.config").action(store()).required(false).type(String.class).metavar("CONFIG_FILE").help("Log4jAppender config properties file.");    parser.addArgument("--sasl-kerberos-service-name").action(store()).required(false).type(String.class).metavar("SASL-KERBEROS-SERVICE-NAME").dest("saslKerberosServiceName").help("Name of sasl kerberos service.");    parser.addArgument("--client-jaas-conf-path").action(store()).required(false).type(String.class).metavar("CLIENT-JAAS-CONF-PATH").dest("clientJaasConfPath").help("Path of JAAS config file of Kafka client.");    parser.addArgument("--kerb5-conf-path").action(store()).required(false).type(String.class).metavar("KERB5-CONF-PATH").dest("kerb5ConfPath").help("Path of Kerb5 config file.");    return parser;}
f20014
0
getValue
public String kafkatest_f20023_0(long val)
{    if (this.valuePrefix != null) {        return String.format("%d.%d", this.valuePrefix, val);    }    return String.format("%d", val);}
f20023
0
getKey
public String kafkatest_f20024_0()
{    String key = null;    if (repeatingKeys != null) {        key = Integer.toString(keyCounter++);        if (keyCounter == repeatingKeys) {            keyCounter = 0;        }    }    return key;}
f20024
0
partition
public int kafkatest_f20033_0()
{    return recordMetadata.partition();}
f20033
0
offset
public long kafkatest_f20034_0()
{    return recordMetadata.offset();}
f20034
0
acked
public long kafkatest_f20043_0()
{    return this.acked;}
f20043
0
targetThroughput
public long kafkatest_f20044_0()
{    return this.targetThroughput;}
f20044
0
status
public AgentStatusResponse kafkatest_f20053_0() throws Exception
{    return new AgentStatusResponse(serverStartMs, workerManager.workerStates());}
f20053
0
uptime
public UptimeResponse kafkatest_f20054_0()
{    return new UptimeResponse(serverStartMs, time.milliseconds());}
f20054
0
target
public Builder kafkatest_f20063_0(String target)
{    this.target = target;    return this;}
f20063
0
target
public Builder kafkatest_f20064_0(String host, int port)
{    this.target = String.format("%s:%d", host, port);    return this;}
f20064
0
destroyWorker
public void kafkatest_f20073_0(DestroyWorkerRequest request) throws Exception
{    UriBuilder uriBuilder = UriBuilder.fromPath(url("/agent/worker"));    uriBuilder.queryParam("workerId", request.workerId());    HttpResponse<Empty> resp = JsonRestServer.<Empty>httpRequest(uriBuilder.build().toString(), "DELETE", null, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
f20073
0
invokeShutdown
public void kafkatest_f20074_0() throws Exception
{    HttpResponse<Empty> resp = JsonRestServer.<Empty>httpRequest(url("/agent/shutdown"), "PUT", null, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
f20074
0
stopWorker
public Empty kafkatest_f20083_0(StopWorkerRequest req) throws Throwable
{    agent().stopWorker(req);    return Empty.INSTANCE;}
f20083
0
destroyWorker
public Empty kafkatest_f20084_0(@DefaultValue("0") kafkatest_f20084_0("workerId") long workerId) throws Throwable
{    agent().destroyWorker(new DestroyWorkerRequest(workerId));    return Empty.INSTANCE;}
f20084
0
spec
 TaskSpec kafkatest_f20093_0()
{    return spec;}
f20093
0
state
 WorkerState kafkatest_f20094_0()
{    switch(state) {        case STARTING:            return new WorkerStarting(taskId, spec);        case RUNNING:            return new WorkerRunning(taskId, spec, startedMs, status.get());        case CANCELLING:        case STOPPING:            return new WorkerStopping(taskId, spec, startedMs, status.get());        case DONE:            return new WorkerDone(taskId, spec, startedMs, doneMs, status.get(), error);    }    throw new RuntimeException("unreachable");}
f20094
0
call
public Voidf20103_1) throws Exception
{    if (worker.error.isEmpty() && !failure.isEmpty()) {        worker.error = failure;    }    worker.transitionToDone();    if (worker.mustDestroy) {                workers.remove(worker.workerId);    } else {            }    return null;}
public Voidf20103
1
stopWorker
public void kafkatest_f20104_0(long workerId, boolean mustDestroy) throws Throwable
{    try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {        stateChangeExecutor.submit(new StopWorker(workerId, mustDestroy)).get();    } catch (ExecutionException e) {        throw e.getCause();    }}
f20104
0
name
public String kafkatest_f20113_0()
{    return name;}
f20113
0
hostname
public String kafkatest_f20114_0()
{    return hostname;}
f20114
0
scheduler
public Scheduler kafkatest_f20123_0()
{    return scheduler;}
f20123
0
runCommand
public String kafkatest_f20124_0(String[] command) throws IOException
{    return commandRunner.run(curNode, command);}
f20124
0
getTrogdorCoordinatorPort
public static int kafkatest_f20133_0(Node node)
{    return getIntConfig(node, Platform.Config.TROGDOR_COORDINATOR_PORT, Coordinator.DEFAULT_PORT);}
f20133
0
parse
public static Platform kafkatest_f20134_0(String curNodeName, String path) throws Exception
{    JsonNode root = JsonUtil.JSON_SERDE.readTree(new File(path));    JsonNode platformNode = root.get("platform");    if (platformNode == null) {        throw new RuntimeException("Expected to find a 'platform' field " + "in the root JSON configuration object");    }    String platformName = platformNode.textValue();    return Utils.newParameterizedInstance(platformName, String.class, curNodeName, Scheduler.class, Scheduler.SYSTEM, JsonNode.class, root);}
f20134
0
perSecToPerPeriod
public static int kafkatest_f20143_0(float perSec, long periodMs)
{    float period = ((float) periodMs) / 1000.0f;    float perPeriod = perSec * period;    perPeriod = Math.max(1.0f, perPeriod);    return (int) perPeriod;}
f20143
0
addConfigsToProperties
public static void kafkatest_f20144_0(Properties props, Map<String, String> commonConf, Map<String, String> clientConf)
{    for (Map.Entry<String, String> commonEntry : commonConf.entrySet()) {        props.setProperty(commonEntry.getKey(), commonEntry.getValue());    }    for (Map.Entry<String, String> entry : clientConf.entrySet()) {        props.setProperty(entry.getKey(), entry.getValue());    }}
f20144
0
status
public CoordinatorStatusResponse kafkatest_f20153_0() throws Exception
{    return new CoordinatorStatusResponse(startTimeMs);}
f20153
0
uptime
public UptimeResponse kafkatest_f20154_0()
{    return new UptimeResponse(startTimeMs, time.milliseconds());}
f20154
0
log
public Builder kafkatest_f20163_0(Logger log)
{    this.log = log;    return this;}
f20163
0
maxTries
public Builder kafkatest_f20164_0(int maxTries)
{    this.maxTries = maxTries;    return this;}
f20164
0
stopTask
public void kafkatest_f20173_0(StopTaskRequest request) throws Exception
{    HttpResponse<Empty> resp = JsonRestServer.httpRequest(log, url("/coordinator/task/stop"), "PUT", request, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
f20173
0
destroyTask
public void kafkatest_f20174_0(DestroyTaskRequest request) throws Exception
{    UriBuilder uriBuilder = UriBuilder.fromPath(url("/coordinator/tasks"));    uriBuilder.queryParam("taskId", request.id());    HttpResponse<Empty> resp = JsonRestServer.httpRequest(log, uriBuilder.build().toString(), "DELETE", null, new TypeReference<Empty>() {    }, maxTries);    resp.body();}
f20174
0
setCoordinator
public void kafkatest_f20183_0(Coordinator myCoordinator)
{    coordinator.set(myCoordinator);}
f20183
0
status
public CoordinatorStatusResponse kafkatest_f20184_0() throws Throwable
{    return coordinator().status();}
f20184
0
tryCreate
 voidf20193_1)
{    try {        client.createWorker(new CreateWorkerRequest(workerId, taskId, spec));    } catch (Throwable e) {            }}
 voidf20193
1
tryStop
 voidf20194_1)
{    try {        client.stopWorker(new StopWorkerRequest(workerId));    } catch (Throwable e) {            }}
 voidf20194
1
call
public Voidf20203_1) throws Exception
{    ManagedWorker worker = workers.get(workerId);    if (worker == null) {                return null;    }    if (!worker.shouldRun) {                return null;    }        worker.shouldRun = false;    rescheduleNextHeartbeat(0);    return null;}
public Voidf20203
1
destroyWorker
public void kafkatest_f20204_0(long workerId)
{    executor.submit(new DestroyWorker(workerId));}
f20204
0
getCombinedStatus
private JsonNode kafkatest_f20213_0()
{    if (workerIds.size() == 1) {        return workerStates.get(workerIds.values().iterator().next()).status();    } else {        ObjectNode objectNode = new ObjectNode(JsonNodeFactory.instance);        for (Map.Entry<String, Long> entry : workerIds.entrySet()) {            String nodeName = entry.getKey();            Long workerId = entry.getValue();            WorkerState state = workerStates.get(workerId);            JsonNode node = state.status();            if (node != null) {                objectNode.set(nodeName, node);            }        }        return objectNode;    }}
f20213
0
activeWorkerIds
 TreeMap<String, Long> kafkatest_f20214_0()
{    TreeMap<String, Long> activeWorkerIds = new TreeMap<>();    for (Map.Entry<String, Long> entry : workerIds.entrySet()) {        WorkerState workerState = workerStates.get(entry.getValue());        if (!workerState.done()) {            activeWorkerIds.put(entry.getKey(), entry.getValue());        }    }    return activeWorkerIds;}
f20214
0
call
public Voidf20223_1) throws Exception
{    try {        WorkerState prevState = workerStates.get(workerId);        if (prevState == null) {            throw new RuntimeException("Unable to find workerId " + workerId);        }        ManagedTask task = tasks.get(prevState.taskId());        if (task == null) {            throw new RuntimeException("Unable to find taskId " + prevState.taskId());        }                workerStates.put(workerId, nextState);        if (nextState.done() && (!prevState.done())) {            handleWorkerCompletion(task, nodeName, (WorkerDone) nextState);        }    } catch (Exception e) {                nodeManagers.get(nodeName).stopWorker(workerId);    }    return null;}
public Voidf20223
1
handleWorkerCompletion
private voidf20224_1ManagedTask task, String nodeName, WorkerDone state)
{    if (state.error().isEmpty()) {            } else {                task.maybeSetError(state.error());    }    TreeMap<String, Long> activeWorkerIds = task.activeWorkerIds();    if (activeWorkerIds.isEmpty()) {        task.doneMs = time.milliseconds();        task.state = TaskStateType.DONE;            } else if ((task.state == TaskStateType.RUNNING) && (!task.error.isEmpty())) {                task.state = TaskStateType.STOPPING;        for (Map.Entry<String, Long> entry : activeWorkerIds.entrySet()) {            nodeManagers.get(entry.getKey()).stopWorker(entry.getValue());        }    }}
private voidf20224
1
latencyMs
public int kafkatest_f20233_0()
{    return latencyMs;}
f20233
0
newController
public TaskController kafkatest_f20234_0(String id)
{    return topology -> nodeSpecs.keySet();}
f20234
0
mountPath
public String kafkatest_f20243_0()
{    return mountPath;}
f20243
0
prefix
public String kafkatest_f20244_0()
{    return prefix;}
f20244
0
addFault
 synchronized void kafkatest_f20253_0(KiboshFaultSpec toAdd) throws IOException
{    KiboshControlFile file = KiboshControlFile.read(controlPath);    List<KiboshFaultSpec> faults = new ArrayList<>(file.faults());    faults.add(toAdd);    new KiboshControlFile(faults).write(controlPath);}
f20253
0
removeFault
 synchronized void kafkatest_f20254_0(KiboshFaultSpec toRemove) throws IOException
{    KiboshControlFile file = KiboshControlFile.read(controlPath);    List<KiboshFaultSpec> faults = new ArrayList<>();    boolean foundToRemove = false;    for (KiboshFaultSpec fault : file.faults()) {        if (fault.equals(toRemove)) {            foundToRemove = true;        } else {            faults.add(fault);        }    }    if (!foundToRemove) {        throw new RuntimeException("Failed to find fault " + toRemove + ". ");    }    new KiboshControlFile(faults).write(controlPath);}
f20254
0
removeFault
 void kafkatest_f20263_0(String mountPath, KiboshFaultSpec spec) throws IOException
{    KiboshProcess process = findProcessObject(mountPath);    process.removeFault(spec);}
f20263
0
targetNodes
public Set<String> kafkatest_f20264_0(Topology topology)
{    return nodeNames;}
f20264
0
stop
public voidf20273_1Platform platform) throws Exception
{        this.status.update(new TextNode("removing network partition " + id));    runIptablesCommands(platform, "-D");    this.status.update(new TextNode("removed network partition " + id));}
public voidf20273
1
runIptablesCommands
private void kafkatest_f20274_0(Platform platform, String iptablesAction) throws Exception
{    Node curNode = platform.curNode();    Topology topology = platform.topology();    TreeSet<String> toBlock = new TreeSet<>();    for (Set<String> partitionSet : partitionSets) {        if (!partitionSet.contains(curNode.name())) {            for (String nodeName : partitionSet) {                toBlock.add(nodeName);            }        }    }    for (String nodeName : toBlock) {        Node node = topology.node(nodeName);        InetAddress addr = InetAddress.getByName(node.hostname());        platform.runCommand(new String[] { "sudo", "iptables", iptablesAction, "INPUT", "-p", "tcp", "-s", addr.getHostAddress(), "-j", "DROP", "-m", "comment", "--comment", nodeName });    }}
f20274
0
serverStartMs
public long kafkatest_f20283_0()
{    return serverStartMs;}
f20283
0
workers
public TreeMap<Long, WorkerState> kafkatest_f20284_0()
{    return workers;}
f20284
0
workerId
public long kafkatest_f20293_0()
{    return workerId;}
f20293
0
equals
public boolean kafkatest_f20294_0(Object o)
{    if (this == o)        return true;    if (o == null || getClass() != o.getClass())        return false;    return true;}
f20294
0
port
public int kafkatest_f20303_0()
{    return connector.getLocalPort();}
f20303
0
beginShutdown
public voidf20304_1)
{    if (!shutdownExecutor.isShutdown()) {        shutdownExecutor.submit((Callable<Void>) () -> {            try {                                jettyServer.stop();                jettyServer.join();                            } catch (Exception e) {                            } finally {                jettyServer.destroy();            }            shutdownExecutor.shutdown();            return null;        });    }}
public voidf20304
1
toString
public final String kafkatest_f20313_0()
{    return JsonUtil.toJsonString(this);}
f20313
0
toResponse
public Responsef20314_1Throwable e)
{    if (log.isDebugEnabled()) {            } else if (log.isInfoEnabled()) {            }    if (e instanceof NotFoundException) {        return buildResponse(Response.Status.NOT_FOUND, e);    } else if (e instanceof InvalidRequestException) {        return buildResponse(Response.Status.BAD_REQUEST, e);    } else if (e instanceof InvalidTypeIdException) {        return buildResponse(Response.Status.NOT_IMPLEMENTED, e);    } else if (e instanceof JsonMappingException) {        return buildResponse(Response.Status.BAD_REQUEST, e);    } else if (e instanceof ClassNotFoundException) {        return buildResponse(Response.Status.NOT_IMPLEMENTED, e);    } else if (e instanceof SerializationException) {        return buildResponse(Response.Status.BAD_REQUEST, e);    } else if (e instanceof RequestConflictException) {        return buildResponse(Response.Status.CONFLICT, e);    } else {        return buildResponse(Response.Status.INTERNAL_SERVER_ERROR, e);    }}
public Responsef20314
1
stateType
public TaskStateType kafkatest_f20323_0()
{    return TaskStateType.DONE;}
f20323
0
stateType
public TaskStateType kafkatest_f20324_0()
{    return TaskStateType.PENDING;}
f20324
0
state
public Optional<TaskStateType> kafkatest_f20333_0()
{    return state;}
f20333
0
matches
public boolean kafkatest_f20334_0(String taskId, long startMs, long endMs, TaskStateType state)
{    if ((!taskIds.isEmpty()) && (!taskIds.contains(taskId))) {        return false;    }    if ((firstStartMs > 0) && (startMs < firstStartMs)) {        return false;    }    if ((lastStartMs > 0) && ((startMs < 0) || (startMs > lastStartMs))) {        return false;    }    if ((firstEndMs > 0) && (endMs < firstEndMs)) {        return false;    }    if ((lastEndMs > 0) && ((endMs < 0) || (endMs > lastEndMs))) {        return false;    }    if (this.state.isPresent() && !this.state.get().equals(state)) {        return false;    }    return true;}
f20334
0
doneMs
public long kafkatest_f20343_0()
{    return doneMs;}
f20343
0
status
public JsonNode kafkatest_f20344_0()
{    return status;}
f20344
0
spec
public TaskSpec kafkatest_f20353_0()
{    return spec;}
f20353
0
stopping
public boolean kafkatest_f20354_0()
{    return false;}
f20354
0
get
public synchronized JsonNode kafkatest_f20363_0()
{    return status;}
f20363
0
targetNodes
public Set<String> kafkatest_f20364_0(Topology topology)
{    return Topology.Util.agentNodeNames(topology);}
f20364
0
hashCode
public final int kafkatest_f20373_0()
{    return Objects.hashCode(toString());}
f20373
0
toString
public String kafkatest_f20374_0()
{    return JsonUtil.toJsonString(this);}
f20374
0
newTaskWorker
public TaskWorker kafkatest_f20383_0(String id)
{    return new ConnectionStressWorker(id, this);}
f20383
0
start
public voidf20384_1Platform platform, WorkerStatusTracker status, KafkaFutureImpl<String> doneFuture) throws Exception
{    if (!running.compareAndSet(false, true)) {        throw new IllegalStateException("ConnectionStressWorker is already running.");    }        this.doneFuture = doneFuture;    this.status = status;    synchronized (ConnectionStressWorker.this) {        this.totalConnections = 0;        this.totalFailedConnections = 0;        this.startTimeMs = TIME.milliseconds();    }    this.statusUpdaterExecutor = Executors.newScheduledThreadPool(1, ThreadUtils.createThreadFactory("StatusUpdaterWorkerThread%d", false));    this.statusUpdaterFuture = this.statusUpdaterExecutor.scheduleAtFixedRate(new StatusUpdater(), 0, REPORT_INTERVAL_MS, TimeUnit.MILLISECONDS);    this.workerExecutor = Executors.newFixedThreadPool(spec.numThreads(), ThreadUtils.createThreadFactory("ConnectionStressWorkerThread%d", false));    for (int i = 0; i < spec.numThreads(); i++) {        this.workerExecutor.submit(new ConnectLoop());    }}
public voidf20384
1
connectsPerSec
public double kafkatest_f20394_0()
{    return connectsPerSec;}
f20394
0
stop
public voidf20395_1Platform platform) throws Exception
{    if (!running.compareAndSet(true, false)) {        throw new IllegalStateException("ConnectionStressWorker is not running.");    }        // Shut down the periodic status updater and perform a final update on the    // statistics.  We want to do this first, before deactivating any threads.    // Otherwise, if some threads take a while to terminate, this could lead    // to a misleading rate getting reported.    this.statusUpdaterFuture.cancel(false);    this.statusUpdaterExecutor.shutdown();    this.statusUpdaterExecutor.awaitTermination(1, TimeUnit.DAYS);    this.statusUpdaterExecutor = null;    new StatusUpdater().run();    doneFuture.complete("");    workerExecutor.shutdownNow();    workerExecutor.awaitTermination(1, TimeUnit.DAYS);    this.workerExecutor = null;    this.status = null;}
public voidf20395
1
threadsPerWorker
public int kafkatest_f20404_0()
{    return threadsPerWorker;}
f20404
0
consumerConf
public Map<String, String> kafkatest_f20405_0()
{    return consumerConf;}
f20405
0
run
public void kafkatest_f20414_0()
{    try {        List<Future<Void>> consumeTasks = new ArrayList<>();        for (ConsumeMessages task : consumeTasks()) {            consumeTasks.add(executor.submit(task));        }        executor.submit(new CloseStatusUpdater(consumeTasks));    } catch (Throwable e) {        WorkerUtils.abort(log, "Prepare", e, doneFuture);    }}
f20414
0
consumeTasks
private List<ConsumeMessages> kafkatest_f20415_0()
{    List<ConsumeMessages> tasks = new ArrayList<>();    String consumerGroup = consumerGroup();    int consumerCount = spec.threadsPerWorker();    Map<String, List<TopicPartition>> partitionsByTopic = spec.materializeTopics();    boolean toUseGroupPartitionAssignment = partitionsByTopic.values().stream().allMatch(List::isEmpty);    if (!toUseGroupPartitionAssignment && !toUseRandomConsumeGroup() && consumerCount > 1)        throw new ConfigException("You may not specify an explicit partition assignment when using multiple consumers in the same group." + "Please leave the consumer group unset, specify topics instead of partitions or use a single consumer.");    consumer = consumer(consumerGroup, clientId(0));    if (toUseGroupPartitionAssignment) {        Set<String> topics = partitionsByTopic.keySet();        tasks.add(new ConsumeMessages(consumer, topics));        for (int i = 0; i < consumerCount - 1; i++) {            tasks.add(new ConsumeMessages(consumer(consumerGroup(), clientId(i + 1)), topics));        }    } else {        List<TopicPartition> partitions = populatePartitionsByTopic(consumer.consumer(), partitionsByTopic).values().stream().flatMap(List::stream).collect(Collectors.toList());        tasks.add(new ConsumeMessages(consumer, partitions));        for (int i = 0; i < consumerCount - 1; i++) {            tasks.add(new ConsumeMessages(consumer(consumerGroup(), clientId(i + 1)), partitions));        }    }    return tasks;}
f20415
0
update
 synchronized void kafkatest_f20424_0()
{    workerStatus.update(JsonUtil.JSON_SERDE.valueToTree(statuses));}
f20424
0
updateConsumeStatus
 synchronized void kafkatest_f20425_0(String clientId, StatusData status)
{    statuses.put(clientId, JsonUtil.JSON_SERDE.valueToTree(status));}
f20425
0
p95LatencyMs
public int kafkatest_f20434_0()
{    return p95LatencyMs;}
f20434
0
p99LatencyMs
public int kafkatest_f20435_0()
{    return p99LatencyMs;}
f20435
0
commandNode
public String kafkatest_f20444_0()
{    return commandNode;}
f20444
0
command
public List<String> kafkatest_f20445_0()
{    return command;}
f20445
0
run
public voidf20454_1)
{    log.trace("{}: starting stderr monitor.", id);    try (BufferedReader br = new BufferedReader(new InputStreamReader(process.getErrorStream(), StandardCharsets.UTF_8))) {        String line;        while (true) {            try {                line = br.readLine();                if (line == null) {                    throw new IOException("EOF");                }            } catch (IOException e) {                                return;            }                    }    } catch (Throwable e) {            }}
public voidf20454
1
run
public voidf20455_1)
{    OutputStreamWriter stdinWriter = new OutputStreamWriter(process.getOutputStream(), StandardCharsets.UTF_8);    try {        while (true) {                        Optional<JsonNode> node = stdinQueue.take();            if (!node.isPresent()) {                log.trace("{}: StdinWriter terminating.", id);                return;            }            String inputString = JsonUtil.toJsonString(node.get());                        stdinWriter.write(inputString + "\n");            stdinWriter.flush();        }    } catch (IOException e) {            } catch (Throwable e) {            } finally {        try {            stdinWriter.close();        } catch (IOException e) {                    }    }}
public voidf20455
1
fraction
public float kafkatest_f20464_0()
{    return fraction;}
f20464
0
value
public int kafkatest_f20465_0()
{    return value;}
f20465
0
configs
public Map<String, String> kafkatest_f20474_0()
{    return configs;}
f20474
0
newTopic
public NewTopic kafkatest_f20475_0(String topicName)
{    NewTopic newTopic;    if (partitionAssignments.isEmpty()) {        int effectiveNumPartitions = numPartitions <= 0 ? DEFAULT_NUM_PARTITIONS : numPartitions;        short effectiveReplicationFactor = replicationFactor <= 0 ? DEFAULT_REPLICATION_FACTOR : replicationFactor;        newTopic = new NewTopic(topicName, effectiveNumPartitions, effectiveReplicationFactor);    } else {        newTopic = new NewTopic(topicName, partitionAssignments);    }    if (!configs.isEmpty()) {        newTopic.configs(configs);    }    return newTopic;}
f20475
0
targetMessagesPerSec
public int kafkatest_f20484_0()
{    return targetMessagesPerSec;}
f20484
0
maxMessages
public long kafkatest_f20485_0()
{    return maxMessages;}
f20485
0
useConfiguredPartitioner
public boolean kafkatest_f20494_0()
{    return useConfiguredPartitioner;}
f20494
0
skipFlush
public boolean kafkatest_f20495_0()
{    return skipFlush;}
f20495
0
sendMessage
private void kafkatest_f20504_0() throws InterruptedException
{    if (!partitionsIterator.hasNext())        partitionsIterator = activePartitions.iterator();    TopicPartition partition = partitionsIterator.next();    ProducerRecord<byte[], byte[]> record;    if (spec.useConfiguredPartitioner()) {        record = new ProducerRecord<>(partition.topic(), keys.next(), values.next());    } else {        record = new ProducerRecord<>(partition.topic(), partition.partition(), keys.next(), values.next());    }    sendFuture = producer.send(record, new SendRecordsCallback(this, Time.SYSTEM.milliseconds()));    throttle.increment();}
f20504
0
recordDuration
 void kafkatest_f20505_0(long durationMs)
{    histogram.add(durationMs);}
f20505
0
stop
public voidf20514_1Platform platform) throws Exception
{    if (!running.compareAndSet(true, false)) {        throw new IllegalStateException("ProduceBenchWorker is not running.");    }        doneFuture.complete("");    executor.shutdownNow();    executor.awaitTermination(1, TimeUnit.DAYS);    this.executor = null;    this.status = null;    this.doneFuture = null;}
public voidf20514
1
percent
public int kafkatest_f20515_0()
{    return percent;}
f20515
0
next
 synchronized ToSendTrackerResult kafkatest_f20524_0()
{    if (failed.isEmpty()) {        if (frontier >= maxMessages) {            return null;        } else {            return new ToSendTrackerResult(frontier++, true);        }    } else {        return new ToSendTrackerResult(failed.remove(0), false);    }}
f20524
0
run
public voidf20525_1)
{    long messagesSent = 0;    long uniqueMessagesSent = 0;        try {        Iterator<TopicPartition> iter = partitions.iterator();        while (true) {            final ToSendTrackerResult result = toSendTracker.next();            if (result == null) {                break;            }            throttle.increment();            final long messageIndex = result.index;            if (result.firstSend) {                toReceiveTracker.addPending(messageIndex);                uniqueMessagesSent++;            }            messagesSent++;            if (!iter.hasNext()) {                iter = partitions.iterator();            }            TopicPartition partition = iter.next();            // we explicitly specify generator position based on message index            ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(partition.topic(), partition.partition(), KEY_GENERATOR.generate(messageIndex), spec.valueGenerator().generate(messageIndex));            producer.send(record, (metadata, exception) -> {                if (exception == null) {                    try {                        lock.lock();                        unackedSends -= 1;                        if (unackedSends <= 0)                            unackedSendsAreZero.signalAll();                    } finally {                        lock.unlock();                    }                } else {                                        toSendTracker.addFailed(messageIndex);                }            });        }    } catch (Throwable e) {        WorkerUtils.abort(log, "ProducerRunnable", e, doneFuture);    } finally {        try {            lock.lock();             uniqueMessagesSent={}; " + "ackedSends={}/{}.", id, messagesSent, uniqueMessagesSent, spec.maxMessages() - unackedSends, spec.maxMessages());        } finally {            lock.unlock();        }    }}
public voidf20525
1
totalReceived
public long kafkatest_f20534_0()
{    return totalReceived;}
f20534
0
stop
public voidf20535_1Platform platform) throws Exception
{    if (!running.compareAndSet(true, false)) {        throw new IllegalStateException("RoundTripWorker is not running.");    }        doneFuture.complete("");    executor.shutdownNow();    executor.awaitTermination(1, TimeUnit.DAYS);    Utils.closeQuietly(consumer, "consumer");    Utils.closeQuietly(producer, "producer");    this.consumer = null;    this.producer = null;    this.unackedSends = null;    this.executor = null;    this.doneFuture = null;    }
public voidf20535
1
producerConf
public Map<String, String> kafkatest_f20544_0()
{    return producerConf;}
f20544
0
consumerConf
public Map<String, String> kafkatest_f20545_0()
{    return consumerConf;}
f20545
0
consumerConf
public Map<String, String> kafkatest_f20554_0()
{    return consumerConf;}
f20554
0
adminClientConf
public Map<String, String> kafkatest_f20555_0()
{    return adminClientConf;}
f20555
0
refreshRateMs
public int kafkatest_f20564_0()
{    return refreshRateMs;}
f20564
0
newController
public TaskController kafkatest_f20565_0(String id)
{    return topology -> Collections.singleton(clientNode);}
f20565
0
refresh
public voidf20574_1)
{    try {        if (this.producer == null) {            // Housekeeping to track the number of opened connections.            SustainedConnectionWorker.this.totalProducerConnections.incrementAndGet();            // Create the producer, fetch the specified topic's partitions and randomize them.            this.producer = new KafkaProducer<>(this.props, new ByteArraySerializer(), new ByteArraySerializer());            this.partitions = this.producer.partitionsFor(this.topicName).stream().map(partitionInfo -> new TopicPartition(partitionInfo.topic(), partitionInfo.partition())).collect(Collectors.toList());            Collections.shuffle(this.partitions);        }        // Create a new iterator over the partitions if the current one doesn't exist or is exhausted.        if (this.partitionsIterator == null || !this.partitionsIterator.hasNext()) {            this.partitionsIterator = this.partitions.iterator();        }        // Produce a single record and send it synchronously.        TopicPartition partition = this.partitionsIterator.next();        ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(partition.topic(), partition.partition(), keys.next(), values.next());        producer.send(record).get();    } catch (Throwable e) {        // Set the producer to be recreated on the next cycle.        this.closeQuietly();        // Housekeeping to track the number of opened connections and failed connection attempts.        SustainedConnectionWorker.this.totalProducerConnections.decrementAndGet();        SustainedConnectionWorker.this.totalProducerFailedConnections.incrementAndGet();        SustainedConnectionWorker.    }    // Schedule this again and set to not in use.    this.completeRefresh();}
public voidf20574
1
closeQuietly
protected void kafkatest_f20575_0()
{    Utils.closeQuietly(this.producer, "KafkaProducer");    this.producer = null;    this.partitions = null;    this.partitionsIterator = null;}
f20575
0
totalConsumerFailedConnections
public long kafkatest_f20584_0()
{    return totalConsumerFailedConnections;}
f20584
0
totalMetadataConnections
public long kafkatest_f20585_0()
{    return totalMetadataConnections;}
f20585
0
transactionIntervalMs
public int kafkatest_f20594_0()
{    return intervalMs;}
f20594
0
nextAction
public synchronized TransactionAction kafkatest_f20595_0()
{    if (lastTransactionStartMs == NULL_START_MS) {        lastTransactionStartMs = time.milliseconds();        return TransactionAction.BEGIN_TRANSACTION;    }    if (time.milliseconds() - lastTransactionStartMs >= intervalMs) {        lastTransactionStartMs = NULL_START_MS;        return TransactionAction.COMMIT_TRANSACTION;    }    return TransactionAction.NO_OP;}
f20595
0
messagesPerTransaction
public int kafkatest_f20604_0()
{    return messagesPerTransaction;}
f20604
0
nextAction
public synchronized TransactionAction kafkatest_f20605_0()
{    if (messagesInTransaction == -1) {        messagesInTransaction = 0;        return TransactionAction.BEGIN_TRANSACTION;    }    if (messagesInTransaction == messagesPerTransaction) {        messagesInTransaction = -1;        return TransactionAction.COMMIT_TRANSACTION;    }    messagesInTransaction += 1;    return TransactionAction.NO_OP;}
f20605
0
expectConfigure
private void kafkatest_f20614_0()
{    EasyMock.expect(executor.scheduleAtFixedRate(EasyMock.capture(reportRunnable), EasyMock.eq(5L), EasyMock.eq(5L), EasyMock.eq(TimeUnit.SECONDS))).andReturn(// return value not expected to be used    null);}
f20614
0
configure
private void kafkatest_f20615_0()
{    Map<String, String> config = new HashMap<>();    config.put(PushHttpMetricsReporter.METRICS_URL_CONFIG, URL.toString());    config.put(PushHttpMetricsReporter.METRICS_PERIOD_CONFIG, "5");    reporter.configure(config);}
f20615
0
testAgentStartShutdown
public void kafkatest_f20624_0() throws Exception
{    Agent agent = createAgent(Scheduler.SYSTEM);    agent.beginShutdown();    agent.waitForShutdown();}
f20624
0
testAgentProgrammaticShutdown
public void kafkatest_f20625_0() throws Exception
{    Agent agent = createAgent(Scheduler.SYSTEM);    AgentClient client = new AgentClient.Builder().maxTries(10).target("localhost", agent.port()).build();    client.invokeShutdown();    agent.waitForShutdown();}
f20625
0
close
public void kafkatest_f20634_0() throws Exception
{    Utils.delete(tempDir);}
f20634
0
testKiboshFaults
public void kafkatest_f20635_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    MockScheduler scheduler = new MockScheduler(time);    Agent agent = createAgent(scheduler);    AgentClient client = new AgentClient.Builder().maxTries(10).target("localhost", agent.port()).build();    new ExpectedTasks().waitFor(client);    try (MockKibosh mockKibosh = new MockKibosh()) {        Assert.assertEquals(KiboshControlFile.EMPTY, mockKibosh.read());        FilesUnreadableFaultSpec fooSpec = new FilesUnreadableFaultSpec(0, 900000, Collections.singleton("myAgent"), mockKibosh.tempDir.getPath(), "/foo", 123);        client.createWorker(new CreateWorkerRequest(0, "foo", fooSpec));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerRunning("foo", fooSpec, 0, new TextNode("Added fault foo"))).build()).waitFor(client);        Assert.assertEquals(new KiboshControlFile(Collections.<Kibosh.KiboshFaultSpec>singletonList(new KiboshFilesUnreadableFaultSpec("/foo", 123))), mockKibosh.read());        FilesUnreadableFaultSpec barSpec = new FilesUnreadableFaultSpec(0, 900000, Collections.singleton("myAgent"), mockKibosh.tempDir.getPath(), "/bar", 456);        client.createWorker(new CreateWorkerRequest(1, "bar", barSpec));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerRunning("foo", fooSpec, 0, new TextNode("Added fault foo"))).build()).addTask(new ExpectedTaskBuilder("bar").workerState(new WorkerRunning("bar", barSpec, 0, new TextNode("Added fault bar"))).build()).waitFor(client);        Assert.assertEquals(new KiboshControlFile(new ArrayList<Kibosh.KiboshFaultSpec>() {            {                add(new KiboshFilesUnreadableFaultSpec("/foo", 123));                add(new KiboshFilesUnreadableFaultSpec("/bar", 456));            }        }), mockKibosh.read());        time.sleep(1);        client.stopWorker(new StopWorkerRequest(0));        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").workerState(new WorkerDone("foo", fooSpec, 0, 1, new TextNode("Removed fault foo"), "")).build()).addTask(new ExpectedTaskBuilder("bar").workerState(new WorkerRunning("bar", barSpec, 0, new TextNode("Added fault bar"))).build()).waitFor(client);        Assert.assertEquals(new KiboshControlFile(Collections.<Kibosh.KiboshFaultSpec>singletonList(new KiboshFilesUnreadableFaultSpec("/bar", 456))), mockKibosh.read());    }}
f20635
0
taskSpec
public ExpectedTaskBuilder kafkatest_f20644_0(TaskSpec taskSpec)
{    this.taskSpec = taskSpec;    return this;}
f20644
0
taskState
public ExpectedTaskBuilder kafkatest_f20645_0(TaskState taskState)
{    this.taskState = taskState;    return this;}
f20645
0
addTask
public ExpectedTasks kafkatest_f20654_0(ExpectedTask task)
{    expected.put(task.id, task);    return this;}
f20654
0
waitFor
public ExpectedTasksf20655_1final CoordinatorClient client) throws InterruptedException
{    TestUtils.waitForCondition(() -> {        TasksResponse tasks = null;        try {            tasks = client.tasks(new TasksRequest(null, 0, 0, 0, 0, Optional.empty()));        } catch (Exception e) {                        throw new RuntimeException(e);        }        StringBuilder errors = new StringBuilder();        for (Map.Entry<String, ExpectedTask> entry : expected.entrySet()) {            String id = entry.getKey();            ExpectedTask task = entry.getValue();            String differences = task.compare(tasks.tasks().get(id));            if (differences != null) {                errors.append(differences);            }        }        String errorString = errors.toString();        if (!errorString.isEmpty()) {                                                return false;        }        return true;    }, "Timed out waiting for expected tasks " + JsonUtil.toJsonString(expected));    return this;}
public ExpectedTasksf20655
1
addAgent
public Builder kafkatest_f20664_0(String nodeName)
{    if (agentNames.contains(nodeName)) {        throw new RuntimeException("There is already an agent on node " + nodeName);    }    agentNames.add(nodeName);    return this;}
f20664
0
getOrCreate
private NodeData kafkatest_f20665_0(String nodeName, TreeMap<String, NodeData> nodes)
{    NodeData data = nodes.get(nodeName);    if (data != null)        return data;    data = new NodeData();    data.hostname = "127.0.0.1";    nodes.put(nodeName, data);    return data;}
f20665
0
testExpansions
public void kafkatest_f20674_0() throws Exception
{    HashSet<String> expected1 = new HashSet<>(Arrays.asList("foo1", "foo2", "foo3"));    assertEquals(expected1, StringExpander.expand("foo[1-3]"));    HashSet<String> expected2 = new HashSet<>(Arrays.asList("foo bar baz 0"));    assertEquals(expected2, StringExpander.expand("foo bar baz [0-0]"));    HashSet<String> expected3 = new HashSet<>(Arrays.asList("[[ wow50 ]]", "[[ wow51 ]]", "[[ wow52 ]]"));    assertEquals(expected3, StringExpander.expand("[[ wow[50-52] ]]"));    HashSet<String> expected4 = new HashSet<>(Arrays.asList("foo1bar", "foo2bar", "foo3bar"));    assertEquals(expected4, StringExpander.expand("foo[1-3]bar"));    // should expand latest range first    HashSet<String> expected5 = new HashSet<>(Arrays.asList("start[1-3]middle1epilogue", "start[1-3]middle2epilogue", "start[1-3]middle3epilogue"));    assertEquals(expected5, StringExpander.expand("start[1-3]middle[1-3]epilogue"));}
f20674
0
testDateString
public void kafkatest_f20675_0()
{    assertEquals("2019-01-08T20:59:29.85Z", dateString(1546981169850L, ZoneOffset.UTC));}
f20675
0
testExistingTopicsMustHaveRequestedNumberOfPartitions
public void kafkatest_f20684_0() throws Throwable
{    List<TopicPartitionInfo> tpInfo = new ArrayList<>();    tpInfo.add(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList()));    tpInfo.add(new TopicPartitionInfo(1, broker2, singleReplica, Collections.<Node>emptyList()));    adminClient.addTopic(false, TEST_TOPIC, tpInfo, null);    WorkerUtils.createTopics(log, adminClient, Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC), false);}
f20684
0
testExistingTopicsNotCreated
public void kafkatest_f20685_0() throws Throwable
{    final String existingTopic = "existing-topic";    List<TopicPartitionInfo> tpInfo = new ArrayList<>();    tpInfo.add(new TopicPartitionInfo(0, broker1, singleReplica, Collections.<Node>emptyList()));    tpInfo.add(new TopicPartitionInfo(1, broker2, singleReplica, Collections.<Node>emptyList()));    tpInfo.add(new TopicPartitionInfo(2, broker3, singleReplica, Collections.<Node>emptyList()));    adminClient.addTopic(false, existingTopic, tpInfo, null);    WorkerUtils.createTopics(log, adminClient, Collections.singletonMap(existingTopic, new NewTopic(existingTopic, tpInfo.size(), TEST_REPLICATION_FACTOR)), false);    assertEquals(Collections.singleton(existingTopic), adminClient.listTopics().names().get());}
f20685
0
makeExistingTopicWithOneReplica
private void kafkatest_f20694_0(String topicName, int numPartitions)
{    List<TopicPartitionInfo> tpInfo = new ArrayList<>();    int brokerIndex = 0;    for (int i = 0; i < numPartitions; ++i) {        Node broker = cluster.get(brokerIndex);        tpInfo.add(new TopicPartitionInfo(i, broker, singleReplica, Collections.<Node>emptyList()));        brokerIndex = (brokerIndex + 1) % cluster.size();    }    adminClient.addTopic(false, topicName, tpInfo, null);}
f20694
0
testVerifyTopics
public void kafkatest_f20695_0() throws Throwable
{    Map<String, NewTopic> newTopics = Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC);    WorkerUtils.createTopics(log, adminClient, newTopics, true);    adminClient.setFetchesRemainingUntilVisible(TEST_TOPIC, 2);    WorkerUtils.verifyTopics(log, adminClient, Collections.singleton(TEST_TOPIC), Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC), 3, 1);    adminClient.setFetchesRemainingUntilVisible(TEST_TOPIC, 100);    try {        WorkerUtils.verifyTopics(log, adminClient, Collections.singleton(TEST_TOPIC), Collections.singletonMap(TEST_TOPIC, NEW_TEST_TOPIC), 2, 1);        Assert.fail("expected to get UnknownTopicOrPartitionException");    } catch (UnknownTopicOrPartitionException e) {    // expected    }}
f20695
0
waitFor
public ExpectedLines kafkatest_f20704_0(final String nodeName, final CapturingCommandRunner runner) throws InterruptedException
{    TestUtils.waitForCondition(() -> linesMatch(nodeName, runner.lines(nodeName)), "failed to find the expected lines " + this.toString());    return this;}
f20704
0
linesMatch
private booleanf20705_1final String nodeName, List<String> actualLines)
{    int matchIdx = 0, i = 0;    while (true) {        if (matchIdx == expectedLines.size()) {                        return true;        }        if (i == actualLines.size()) {                        return false;        }        String actualLine = actualLines.get(i++);        String expectedLine = expectedLines.get(matchIdx);        if (expectedLine.equals(actualLine)) {            matchIdx++;        } else {            log.trace("Expected:\n'{}', Got:\n'{}'", expectedLine, actualLine);            matchIdx = 0;        }    }}
private booleanf20705
1
testTaskRequestWithFutureStartMsDoesNotGetRun
public void kafkatest_f20714_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    Scheduler scheduler = new MockScheduler(time);    try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().addCoordinator("node01").addAgent("node02").scheduler(scheduler).build()) {        NoOpTaskSpec fooSpec = new NoOpTaskSpec(1000, 500);        time.sleep(999);        CoordinatorClient coordinatorClient = cluster.coordinatorClient();        coordinatorClient.createTask(new CreateTaskRequest("fooSpec", fooSpec));        TaskState expectedState = new ExpectedTaskBuilder("fooSpec").taskState(new TaskPending(fooSpec)).build().taskState();        TaskState resp = coordinatorClient.task(new TaskRequest("fooSpec"));        assertEquals(expectedState, resp);    }}
f20714
0
testTaskRequest
public void kafkatest_f20715_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    Scheduler scheduler = new MockScheduler(time);    try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().addCoordinator("node01").addAgent("node02").scheduler(scheduler).build()) {        CoordinatorClient coordinatorClient = cluster.coordinatorClient();        NoOpTaskSpec fooSpec = new NoOpTaskSpec(1, 10);        coordinatorClient.createTask(new CreateTaskRequest("foo", fooSpec));        TaskState expectedState = new ExpectedTaskBuilder("foo").taskState(new TaskPending(fooSpec)).build().taskState();        TaskState resp = coordinatorClient.task(new TaskRequest("foo"));        assertEquals(expectedState, resp);        time.sleep(2);        new ExpectedTasks().addTask(new ExpectedTaskBuilder("foo").taskState(new TaskRunning(fooSpec, 2, new TextNode("active"))).workerState(new WorkerRunning("foo", fooSpec, 2, new TextNode("active"))).build()).waitFor(coordinatorClient).waitFor(cluster.agentClient("node02"));        try {            coordinatorClient.task(new TaskRequest("non-existent-foo"));            fail("Non existent task request should have raised a NotFoundException");        } catch (NotFoundException ignored) {        }    }}
f20715
0
testToExceptionNotFoundException
public void kafkatest_f20724_0() throws Exception
{    RestExceptionMapper.toException(Response.Status.NOT_FOUND.getStatusCode(), "Not Found");}
f20724
0
testToExceptionClassNotFoundException
public void kafkatest_f20725_0() throws Exception
{    RestExceptionMapper.toException(Response.Status.NOT_IMPLEMENTED.getStatusCode(), "Not Implemented");}
f20725
0
stop
public void kafkatest_f20734_0(Platform platform) throws Exception
{    this.future.cancel(false);    this.executor.shutdown();    this.executor.awaitTermination(1, TimeUnit.DAYS);    this.status.update(new TextNode("halted"));}
f20734
0
testTaskSpecSerialization
public void kafkatest_f20735_0() throws Exception
{    try {        JsonUtil.JSON_SERDE.readValue("{\"startMs\":123,\"durationMs\":456,\"exitMs\":1000,\"error\":\"foo\"}", SampleTaskSpec.class);        fail("Expected InvalidTypeIdException because type id is missing.");    } catch (InvalidTypeIdException e) {    }    String inputJson = "{\"class\":\"org.apache.kafka.trogdor.task.SampleTaskSpec\"," + "\"startMs\":123,\"durationMs\":456,\"nodeToExitMs\":{\"node01\":1000},\"error\":\"foo\"}";    SampleTaskSpec spec = JsonUtil.JSON_SERDE.readValue(inputJson, SampleTaskSpec.class);    assertEquals(123, spec.startMs());    assertEquals(456, spec.durationMs());    assertEquals(Long.valueOf(1000), spec.nodeToExitMs().get("node01"));    assertEquals("foo", spec.error());    String outputJson = JsonUtil.toJsonString(spec);    assertEquals(inputJson, outputJson);}
f20735
0
testProcessWithFailedExit
public void kafkatest_f20744_0() throws Exception
{    if (OperatingSystem.IS_WINDOWS)        return;    ExternalCommandWorker worker = new ExternalCommandWorkerBuilder("falseTask").command("false").build();    KafkaFutureImpl<String> doneFuture = new KafkaFutureImpl<>();    worker.start(null, new AgentWorkerStatusTracker(), doneFuture);    assertEquals("exited with return code 1", doneFuture.get());    worker.stop(null);}
f20744
0
testProcessNotFound
public void kafkatest_f20745_0() throws Exception
{    ExternalCommandWorker worker = new ExternalCommandWorkerBuilder("notFoundTask").command("/dev/null/non/existent/script/path").build();    KafkaFutureImpl<String> doneFuture = new KafkaFutureImpl<>();    worker.start(null, new AgentWorkerStatusTracker(), doneFuture);    String errorString = doneFuture.get();    assertTrue(errorString.startsWith("Unable to start process"));    worker.stop(null);}
f20745
0
testSequentialPayloadGenerator
public void kafkatest_f20754_0()
{    SequentialPayloadGenerator g4 = new SequentialPayloadGenerator(4, 1);    assertLittleEndianArrayEquals(1, g4.generate(0));    assertLittleEndianArrayEquals(2, g4.generate(1));    SequentialPayloadGenerator g8 = new SequentialPayloadGenerator(8, 0);    assertLittleEndianArrayEquals(0, g8.generate(0));    assertLittleEndianArrayEquals(1, g8.generate(1));    assertLittleEndianArrayEquals(123123123123L, g8.generate(123123123123L));    SequentialPayloadGenerator g2 = new SequentialPayloadGenerator(2, 0);    assertLittleEndianArrayEquals(0, g2.generate(0));    assertLittleEndianArrayEquals(1, g2.generate(1));    assertLittleEndianArrayEquals(1, g2.generate(1));    assertLittleEndianArrayEquals(1, g2.generate(131073));}
f20754
0
assertLittleEndianArrayEquals
private static void kafkatest_f20755_0(long expected, byte[] actual)
{    byte[] longActual = new byte[8];    System.arraycopy(actual, 0, longActual, 0, Math.min(actual.length, longActual.length));    ByteBuffer buf = ByteBuffer.wrap(longActual).order(ByteOrder.LITTLE_ENDIAN);    assertEquals(expected, buf.getLong());}
f20755
0
delay
protected synchronized void kafkatest_f20764_0(long amount) throws InterruptedException
{    time.sleep(amount);}
f20764
0
testThrottle
public void kafkatest_f20765_0() throws Exception
{    MockTime time = new MockTime(0, 0, 0);    ThrottleMock throttle = new ThrottleMock(time, 3);    Assert.assertFalse(throttle.increment());    Assert.assertEquals(0, time.milliseconds());    Assert.assertFalse(throttle.increment());    Assert.assertEquals(0, time.milliseconds());    Assert.assertFalse(throttle.increment());    Assert.assertEquals(0, time.milliseconds());    Assert.assertTrue(throttle.increment());    Assert.assertEquals(100, time.milliseconds());    time.sleep(50);    Assert.assertFalse(throttle.increment());    Assert.assertEquals(150, time.milliseconds());    Assert.assertFalse(throttle.increment());    Assert.assertEquals(150, time.milliseconds());    Assert.assertTrue(throttle.increment());    Assert.assertEquals(200, time.milliseconds());}
f20765
0
